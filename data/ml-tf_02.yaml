- en: 1 A machine-learning odyssey
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 机器学习之旅
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Machine-learning fundamentals
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: Data representation, features, and vector norms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据表示、特征和向量范数
- en: Why TensorFlow?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择TensorFlow？
- en: Have you ever wondered whether there are limits to what computer programs can
    solve? Nowadays, computers appear to do a lot more than unravel mathematical equations.
    In the past half-century, programming has become the ultimate tool for automating
    tasks and saving time. But how much can we automate, and how do we go about doing
    so?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾想过计算机程序能解决的问题是否有极限？如今，计算机似乎能做很多事情，不仅仅是解开数学方程。在过去半个世纪里，编程已经成为自动化任务和节省时间的终极工具。但我们能自动化多少，我们如何去做？
- en: Can a computer observe a photograph and say, “Aha—I see a lovely couple walking
    over a bridge under an umbrella in the rain”? Can software make medical decisions
    as accurately as trained professionals can? Can software make better predictions
    about stock market performance than humans could? The achievements of the past
    decade hint that the answer to all these questions is a resounding yes and that
    the implementations appear to have a common strategy.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机能否观察一张照片并说，“啊哈——我看到一对可爱的情侣在雨中走过一座桥下”？软件能否像训练有素的专家一样准确做出医疗决策？软件能否比人类做出更好的关于股市表现的预测？过去十年的成就暗示，所有这些问题的答案都是响亮的“是”，并且这些实现似乎有一个共同的策略。
- en: Recent theoretical advances coupled with newly available technologies have enabled
    anyone with access to a computer to attempt their own approach to solving these
    incredibly hard problems. (Okay, not just anyone, but that’s why you’re reading
    this book, right?)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的理论进步加上新技术的可用性，使得任何能够访问计算机的人都可以尝试自己解决这些极其困难的问题。（好吧，不仅仅是任何人，这就是你为什么在阅读这本书，对吧？）
- en: A programmer no longer needs to know the intricate details of a problem to solve
    it. Consider converting speech to text. A traditional approach may involve understanding
    the biological structure of human vocal cords to decipher utterances by using
    many hand-designed, domain-specific, ungeneralizable pieces of code. Nowadays,
    it’s possible to write code that looks at many examples and figures out how to
    solve the problem, given enough time and examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员不再需要了解问题的复杂细节就能解决问题。考虑将语音转换为文本。传统的方法可能涉及理解人类声带的生物结构，通过许多手设计的、特定领域的、不可通用的代码片段来解码语音。如今，人们可以编写代码，通过观察许多示例，并在足够的时间和示例的帮助下找出解决问题的方法。
- en: 'Take another example: identifying the sentiment of text in a book or a tweet
    as positive or negative. Or you may want to identify the text even more granularly,
    such as text that implies the writer’s likes or loves, things that they hate or
    is angry or sad about. Past approaches to performing this task were limited to
    scanning the text in question, looking for harsh words such as *ugly*, *stupid*,
    and *miserable* to indicate anger or sadness, or punctuation such as exclamation
    marks, which could mean happy or angry but not exactly in-between.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 再举一个例子：识别一本书或推文中文本的情感是正面还是负面。或者你可能想更细致地识别文本，比如暗示作者喜欢或热爱的事物，他们讨厌的事物，或者他们愤怒或悲伤的事物。过去执行这项任务的方法仅限于扫描相关的文本，寻找像*丑陋*、*愚蠢*和*悲惨*这样的严厉词汇来表示愤怒或悲伤，或者像感叹号这样的标点符号，这可能意味着快乐或愤怒，但并不完全介于两者之间。
- en: Algorithms learn from data, similar to the way that humans learn from experience.
    Humans learn by reading books, observing situations, studying in school, exchanging
    conversations, and browsing websites, among other means. How can a machine possibly
    develop a brain capable of learning? There’s no definitive answer, but world-class
    researchers have developed intelligent programs from different angles. Among the
    implementations, scholars have noticed recurring patterns in solving these kinds
    of problems that led to a standardized field that we today label *machine learning*
    (ML).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从数据中学习，类似于人类从经验中学习的方式。人类通过阅读书籍、观察情况、在学校学习、交流对话和浏览网站等多种方式学习。一台机器怎么可能发展出具有学习能力的“大脑”？这没有确切的答案，但世界级的学者们从不同的角度开发了智能程序。在这些实现中，学者们注意到了解决这类问题时的一些重复出现的模式，这导致了今天我们称之为*机器学习*（ML）的标准化领域。
- en: As the study of ML has matured, the tools for performing machine learning have
    become more standardized, robust, high-performing, and scalable. This is where
    TensorFlow comes in. This software library has an intuitive interface that lets
    programmers dive into using complex ML ideas.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习研究的成熟，执行机器学习的工具变得更加标准化、稳健、高性能和可扩展。这就是 TensorFlow 的作用所在。这个软件库具有直观的界面，让程序员能够深入使用复杂的
    ML 概念。
- en: 'Keeping up with the versions: TensorFlow 2 and beyond'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随版本更新：TensorFlow 2 及以上版本
- en: This book is standardized on two versions of TensorFlow from the 1.x series.
    Version 1.15, which is the latest release in the 1.x series, works well with Python
    3\. In chapters 7 and 19, you’ll read about a few examples that require Python
    2; for that reason, TensorFlow 1.14 is required.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书基于 TensorFlow 1.x 系列的两个版本进行标准化。版本 1.15，是 1.x 系列中的最新发布版本，与 Python 3 兼容良好。在第
    7 章和第 19 章中，你会读到一些需要 Python 2 的例子；因此，需要 TensorFlow 1.14。
- en: Also, a complete port of the listings and code that address TensorFlow 2 was
    released while this book was under development. (See the appendix for the details.)
    You’ll notice that 85-90% of the code for the listings that work in TensorFlow
    2 is the same. The main reason is that the data cleaning, gathering, preparation,
    and evaluation code is fully reusable because it uses accompanying ML libraries
    like Scikit and Matplotlib.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在本书开发期间，还发布了针对 TensorFlow 2 的完整代码列表和代码的移植。（详情见附录。）你会发现，85-90% 的在 TensorFlow
    2 中运行的代码列表代码是相同的。主要原因在于数据清洗、收集、准备和评估代码完全可重用，因为它使用了伴随的 ML 库，如 Scikit 和 Matplotlib。
- en: The TensorFlow 2 version of the listings incorporates new features, including
    always eager execution and updated package names for the optimizers and training.
    The new listings work well in Python 3; I welcome your feedback on them if you
    give them a try. You can find the TensorFlow 2 listing code at [https://github.com/chrismattmann/
    MLwithTensorFlow2ed/tree/master/TFv2](http://github.com/chrismattmann/MLwithTensorFlow2ed/tree/master/TFv2/).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2 版本的代码列表包含了新特性，包括始终启用急切执行和优化器及训练更新的包名。新的代码列表在 Python 3 中运行良好；如果你尝试了它们，欢迎你对它们提出反馈。你可以在
    [https://github.com/chrismattmann/MLwithTensorFlow2ed/tree/master/TFv2](http://github.com/chrismattmann/MLwithTensorFlow2ed/tree/master/TFv2/)
    找到 TensorFlow 2 的代码列表。
- en: Chapter 2 presents the ins and outs of this library, and every chapter thereafter
    explains how to use TensorFlow for the various ML applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第二章介绍了这个库的方方面面，之后的每一章都解释了如何使用 TensorFlow 进行各种机器学习应用。
- en: 1.1 Machine-learning fundamentals
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 机器学习基础
- en: Have you ever tried to explain to someone how to swim? Describing the rhythmic
    joint movements and fluid patterns is overwhelming in its complexity. Similarly,
    some software problems are too complicated for us to easily wrap our minds around.
    For this task, machine learning may be the tool to use.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有尝试向某人解释如何游泳？描述节奏性的关节运动和流体模式在复杂性上令人难以置信。同样，一些软件问题对我们来说过于复杂，难以轻易理解。对于这个任务，机器学习可能是我们使用的工具。
- en: Full speed ahead!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 全速前进！
- en: Machine learning is a relatively young technology, so imagine that you’re a
    geometer in Euclid’s era, paving the way to a newly discovered field. Or consider
    yourself to be a physicist during the time of Newton, possibly pondering something
    equivalent to general relativity for the field of machine learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一项相对较新的技术，所以想象一下，你是欧几里得时代的几何学家，正在开辟一个新领域的道路。或者，设想一下，你是牛顿时代的物理学家，可能正在思考机器学习领域的广义相对论等价物。
- en: Handcrafting carefully tuned algorithms to get the job done was once the only
    way of building software. From a simplistic point of view, traditional programming
    assumes a deterministic output for each input. Machine learning, on the other
    hand, can solve a class of problems for which the input-output correspondences
    aren’t well understood.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 精心手工调整算法以完成任务曾经是构建软件的唯一方式。从简单观点来看，传统编程假设每个输入都有一个确定性的输出。另一方面，机器学习可以解决一类输入输出对应关系不明确的难题。
- en: Machine learning is characterized by software that learns from previous experiences.
    Such a computer program improves performance as more and more examples are available.
    The hope is that if you throw enough data at this machinery, it’ll learn patterns
    and produce intelligent results for newly-fed input.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的特点是软件能够从以往的经验中学习。这样的计算机程序随着更多例子的可用而提高性能。希望如果你向这个机器投入足够的数据，它将学会模式并为新输入产生智能结果。
- en: Trusting and explaining machine-learning output
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 信任和解释机器学习输出
- en: 'Pattern detection is a trait that’s no longer unique to humans. The explosive
    growth of computer clock speed and memory led to an unusual situation: computers
    can now be used to make predictions, catch anomalies, rank items, and label images
    automatically. This new set of tools provides intelligent answers to ill-defined
    problems, but at the subtle cost of trust. Would you trust a computer algorithm
    to dispense vital medical advice such as whether to perform heart surgery or,
    more important, to explain why it gave you such vital medical advice?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 模式检测不再是人类独有的特性。计算机时钟速度和内存的爆炸性增长导致了一种异常情况：现在可以使用计算机进行预测、捕捉异常、排序项目和自动标记图像。这一套新工具为不明确的问题提供了智能答案，但代价是信任的微妙损失。你会信任一个计算机算法提供诸如是否进行心脏手术等至关重要的医疗建议吗？更重要的是，它会解释为什么给你提供了这样重要的医疗建议吗？
- en: There’s no place for mediocre machine-learning solutions. Human trust is too
    fragile, and our algorithms must be robust against doubt. Follow along closely
    and carefully in this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 没有平庸的机器学习解决方案的余地。人类的信任太脆弱了，我们的算法必须能够抵御怀疑。请仔细跟随本章的指导。
- en: Another name for machine learning is *inductive learning*, because the code
    is trying to infer structure from data alone. The process is like going on vacation
    in a foreign country and reading a local fashion magazine to figure out how to
    dress. You can develop an idea of the culture from images of people wearing local
    articles of clothing. You’re learning *inductively*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的另一个名称是*归纳学习*，因为代码试图仅从数据中推断结构。这个过程就像在外国度假时阅读当地时尚杂志来了解如何着装一样。你可以从穿着当地服装的人的图片中发展出对文化的看法。你是在*归纳性地*学习。
- en: You may never have used such an approach when programming, because inductive
    learning isn’t always necessary. Consider the task of determining whether the
    sum of two arbitrary numbers is even or odd. Sure, you can imagine training a
    machine-learning algorithm with millions of training examples (outlined in figure
    1.1), but you certainly know that this approach would be overkill. A more direct
    approach can easily do the trick.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你在编程时可能从未使用过这种方法，因为归纳学习并不总是必要的。考虑一下确定两个任意数之和是偶数还是奇数的任务。当然，你可以想象训练一个机器学习算法，使用数百万个训练示例（如图1.1所示），但你当然知道这种方法会过度。一个更直接的方法可以轻松地完成这个任务。
- en: '![CH01_F01_Mattmann2](../Images/CH01_F01_Mattmann2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F01_Mattmann2](../Images/CH01_F01_Mattmann2.png)'
- en: Figure 1.1 Each pair of integers, when summed, results in an even or odd number.
    The input and output correspondences listed are called the ground-truth dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 每对整数相加的结果是偶数或奇数。列出的输入和输出对应关系称为真实数据集。
- en: 'The sum of two odd numbers is always an even number. Convince yourself: take
    any two odd numbers, add them, and check whether the sum is an even number. Here’s
    how you can prove that fact directly:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 两个奇数之和总是偶数。请你自己验证一下：取任意两个奇数，将它们相加，检查和是否为偶数。以下是直接证明这一事实的方法：
- en: For any integer *n*, the formula 2*n* + 1 produces an odd number. Moreover,
    any odd number can be written as 2*n* + 1 for some value *n*. The number 3 can
    be written as 2(1) + 1\. And the number 5 can be written as 2(2) + 1.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于任何整数*n*，公式2*n* + 1会产生一个奇数。此外，任何奇数都可以写成2*n* + 1的形式，其中*n*是某个值。数字3可以写成2(1) +
    1。数字5可以写成2(2) + 1。
- en: Suppose that we have two odd numbers, 2*n* + 1 and 2*m* + 1, where *n* and *m*
    are integers. Adding two odd numbers yields (2*n* + 1) + (2*m* + 1) = 2*n* + 2*m*
    + 2 = 2(*n* + *m* + 1). This number is even because 2 times anything is even.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们有两个奇数，2*n* + 1和2*m* + 1，其中*n*和*m*是整数。两个奇数相加得到(2*n* + 1) + (2*m* + 1) = 2*n*
    + 2*m* + 2 = 2(*n* + *m* + 1)。这个数是偶数，因为任何数的两倍都是偶数。
- en: 'Likewise, we see that the sum of two even numbers is an even number: 2*m* +
    2*n* = 2(*m* + *n*). Last, we also deduce that the sum of an even with an odd
    is an odd number: 2*m* + (2*n* + 1) = 2(*m* + *n*) + 1\. Figure 1.2 presents this
    logic more clearly.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们看到两个偶数之和也是偶数：2*m* + 2*n* = 2(*m* + *n*)。最后，我们还推断出偶数与奇数之和是奇数：2*m* + (2*n*
    + 1) = 2(*m* + *n*) + 1。图1.2更清楚地展示了这一逻辑。
- en: '![CH01_F02_Mattmann2](../Images/CH01_F02_Mattmann2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F02_Mattmann2](../Images/CH01_F02_Mattmann2.png)'
- en: Figure 1.2 The inner logic behind how the output response corresponds to the
    input pairs
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 输出响应与输入对之间内在逻辑的示意图
- en: That’s it! With absolutely no use of machine learning, you can solve this task
    on any pair of integers someone throws at you. Directly applying mathematical
    rules can solve this problem. But in ML algorithms, we can treat the inner logic
    as a *black box*, meaning that the logic happening inside may not be obvious to
    interpret, as depicted in figure 1.3.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！在完全不使用机器学习的情况下，你可以在任何一对整数上解决这个问题。直接应用数学规则可以解决这个问题。但在机器学习算法中，我们可以将内部逻辑视为*黑盒*，这意味着内部发生的逻辑可能不明显，如图1.3所示。
- en: '![CH01_F03_Mattmann2](../Images/CH01_F03_Mattmann2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F03_Mattmann2](../Images/CH01_F03_Mattmann2.png)'
- en: Figure 1.3 An ML approach to solving problems can be thought of as tuning the
    parameters of a black box until it produces satisfactory results.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 将机器学习解决问题的方法视为调整黑盒的参数，直到它产生令人满意的结果。
- en: 1.1.1 Parameters
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 参数
- en: Sometimes, the best way to devise an algorithm that transforms an input to its
    corresponding output is too complicated. If the input is a series of numbers representing
    a grayscale image, for example, you can imagine the difficulty of writing an algorithm
    to label every object in the image. Machine learning comes in handy when the inner
    workings aren’t well understood. It provides us a toolkit for writing software
    without defining every detail of the algorithm. The programmer can leave some
    values undecided and let the machine-learning system figure out the best values
    by itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，设计一个将输入转换为相应输出的算法可能过于复杂。例如，如果输入是一系列代表灰度图像的数字，你可以想象编写一个算法来标记图像中每个对象的难度。当内部工作原理不为人所知时，机器学习就派上用场了。它为我们提供了一个工具包，可以编写软件而无需定义算法的每个细节。程序员可以留一些值未定，让机器学习系统自行找出最佳值。
- en: The undecided values are called *parameters*, and the description is referred
    to as the *model*. Your job is to write an algorithm that observes existing examples
    to figure out how to best tune parameters to achieve the best model. Wow, that’s
    a mouthful! But don’t worry; this concept will be a recurring motif.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 未确定的值被称为*参数*，描述被称为*模型*。你的任务是编写一个算法，通过观察现有示例来找出如何最佳调整参数以实现最佳模型。哇，这话说得有点多！但别担心；这个概念将会反复出现。
- en: Machine learning might solve a problem without much insight
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可能在没有太多洞察力的情况下解决问题
- en: By mastering the art of inductive problem solving, we wield a double-edged sword.
    Although ML algorithms may perform well when solving specific tasks, tracing the
    steps of deduction to understand why a result is produced may not be as clear.
    An elaborate machine-learning system learns thousands of parameters, but untangling
    the meaning behind each parameter sometimes isn’t the prime directive. With that
    in mind, I assure you that there’s a world of magic to unfold.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握归纳问题解决的艺术，我们掌握了一把双刃剑。虽然机器学习算法在解决特定任务时可能表现良好，但追溯推理步骤以了解为什么产生结果可能并不那么清晰。一个复杂的机器学习系统学习成千上万的参数，但解开每个参数背后的含义有时并不是首要任务。考虑到这一点，我向你保证，有一个充满魔法的世界等待我们去探索。
- en: '**Exercise 1.1**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1.1**'
- en: Suppose that you’ve collected three months’ worth of stock market prices. You’d
    like to predict future trends to outsmart the system for monetary gains. Without
    using ML, how would you go about solving this problem? (As you’ll see in chapter
    13, this problem becomes approachable with ML techniques.)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经收集了三个月的股票市场价格。你希望预测未来的趋势，以获得经济收益。在不使用机器学习的情况下，你将如何解决这个问题？（正如你将在第13章中看到的，这个问题可以通过机器学习技术来解决。）
- en: '**Answer**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Believe it or not, hard-designed rules are a common way to define stock market
    trading strategies. An algorithm as simple as “If the price drops 5%, buy some
    stocks” are often used. Notice that no machine learning is involved—only traditional
    logic.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，硬编码的规则是定义股票市场交易策略的常见方式。像“如果价格下跌5%，就买一些股票”这样简单的算法经常被使用。请注意，这里没有涉及机器学习——只有传统逻辑。
- en: '**Exercise 1.2**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1.2**'
- en: The National Aeronautics and Space Administration (NASA) launches satellites
    into space, and the satellites collect data that we call telemetry. Sometimes,
    anomalies in the collected data indicates that something was wrong with the instrument
    or the conditions under which the data was collected. For simplification, assume
    that telemetry data is a time-based sequence of numbers. To detect an anomaly
    today, most approaches use simple thresholds, or max or min values for those numbers,
    to trigger alarms. What’s a better way to trigger alarms and detect anomalies,
    using ML?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 美国国家航空航天局（NASA）将卫星发射到太空，卫星收集的数据我们称之为遥测数据。有时，收集到的数据中的异常表明仪器或收集数据时的条件出现了问题。为了简化，假设遥测数据是基于时间的数字序列。为了检测今天的异常，大多数方法使用简单的阈值，或者这些数字的最大或最小值来触发警报。使用机器学习来触发警报和检测异常，有什么更好的方法吗？
- en: '**Answer**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: You could record a series of nominal NASA telemetry data at each time step—say,
    5 seconds. Then take the data values, and whenever they trigger an alarm, record
    1 (anomaly); otherwise, record 0 (normal). Congrats—you’ve built a ground-truth
    dataset that you can feed into any one of the predictive models you’ll learn about
    later in this book, such as regression, or classification. You could even build
    a deep learning model. See, isn’t machine learning fun?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在每个时间步记录一系列名义上的NASA遥测数据——比如说，5秒。然后取数据值，每当它们触发警报时，记录1（异常）；否则，记录0（正常）。恭喜你——你已经建立了一个可以喂入你将在本书后面学习到的任何预测模型的真实数据集，例如回归或分类。你甚至可以构建一个深度学习模型。看，机器学习不是很有趣吗？
- en: 1.1.2 Learning and inference
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 学习和推理
- en: Suppose that you’re trying to bake desserts in an oven. If you’re new to the
    kitchen, it can take days to come up with both the right combination and perfect
    ratio of ingredients to make something that tastes great. By recording a recipe,
    you can remember how to repeat a dessert.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在尝试在烤箱里烘焙甜点。如果你对厨房不熟悉，可能需要几天时间才能找到正确的组合和完美的配料比例，以制作出美味的甜点。通过记录食谱，你可以记住如何重复制作甜点。
- en: 'Machine learning shares the idea of recipes. Typically, we examine an algorithm
    in two stages: *learning* and *inference*. The objective of the learning stage
    is to describe the data, which is called the *feature vector*, and summarize it
    in a *model*. The model is our recipe. In effect, the model is a program with
    a couple of open interpretations, and the data helps disambiguate it.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习分享了食谱的想法。通常，我们分两个阶段来检查一个算法：*学习*和*推理*。学习阶段的目标是描述数据，这被称为*特征向量*，并在*模型*中总结它。模型是我们的食谱。实际上，模型是一个具有几个开放解释的程序，数据有助于消除歧义。
- en: Note A *feature vector* is a practical simplification of data. You can think
    of it as a sufficient summary of real-world objects in a list of attributes. The
    learning and inference steps rely on the feature vector instead of the data directly.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：*特征向量*是数据的实际简化。你可以将其视为一个属性列表中现实世界对象的充分总结。学习和推理步骤依赖于特征向量而不是直接的数据。
- en: Similar to the way that recipes can be shared and used by other people, the
    learned model is reused by other software. The learning stage is the most time-consuming.
    Running an algorithm may take hours, if not days or weeks, to converge into a
    useful model, as you will see when you begin to build your own in chapter 3\.
    Figure 1.4 outlines the learning pipeline.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与食谱可以被其他人分享和使用的方式类似，学习到的模型被其他软件重用。学习阶段是最耗时的。运行一个算法可能需要数小时，甚至数天或数周才能收敛到一个有用的模型，正如你在第3章开始构建自己的模型时将会看到的。图1.4概述了学习流程。
- en: '![CH01_F04_Mattmann2](../Images/CH01_F04_Mattmann2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F04_Mattmann2](../Images/CH01_F04_Mattmann2.png)'
- en: Figure 1.4 The learning approach generally follows a structured recipe. First,
    the dataset needs to be transformed into a representation—most often, a list of
    features—that the learning algorithm can use. Then the learning algorithm chooses
    a model and efficiently searches for the model’s parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 学习方法通常遵循一个结构化的食谱。首先，数据集需要被转换成一个表示形式——通常是特征列表——学习算法可以使用。然后，学习算法选择一个模型并高效地搜索该模型的参数。
- en: The inference stage uses the model to make intelligent remarks about never-before-seen
    data. This stage is like using a recipe you found online. The process of inference
    typically takes orders of magnitude less time than learning; inference can be
    fast enough to work on real-time data. Inference is all about testing the model
    on new data and observing performance in the process, as shown in figure 1.5.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 推理阶段使用模型对从未见过的新数据进行智能评论。这个阶段就像使用你在网上找到的食谱一样。推理过程通常比学习快得多；推理可以快到足以处理实时数据。推理完全是关于在新的数据上测试模型并在过程中观察性能，如图
    1.5 所示。
- en: '![CH01_F05_Mattmann2](../Images/CH01_F05_Mattmann2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F05_Mattmann2](../Images/CH01_F05_Mattmann2.png)'
- en: Figure 1.5 The inference approach generally uses a model that has already been
    learned or given. After converting data into a usable representation, such as
    a feature vector, this approach uses the model to produce intended output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 推理方法通常使用已经学习或给出的模型。在将数据转换为可用的表示形式，例如特征向量之后，这种方法使用模型来产生预期的输出。
- en: 1.2 Data representation and features
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 数据表示和特征
- en: Data is a first-class citizen of machine learning. Computers are nothing more
    than sophisticated calculators, so the data we feed our machine-learning systems
    must be mathematical objects such as scalars, vectors, matrices, and graphs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是机器学习的第一公民。计算机不过是非常复杂的计算器，所以我们提供给机器学习系统的数据必须是数学对象，如标量、向量、矩阵和图。
- en: 'The basic theme in all forms of representation is *features*, which are observable
    properties of an object:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所有表示形式的基本主题是 *特征*，这是对象的可观察属性：
- en: '*Vectors* have a flat and simple structure, and are the typical embodiments
    of data in most real-world machine-learning applications. A *scalar* is a single
    element in the vector. Vectors have two attributes: a natural number representing
    the dimension of the vector, and a type (such as real numbers, integers, and so
    on). Examples of 2D vectors of integers are (1, 2) and (-6, 0); similarly, a scalar
    could be 1 or the character *a*. Examples of 3D vectors of real numbers are (1.1,
    2.0, 3.9) and (∏, ∏/2, ∏/3). You get the idea: a collection of numbers of the
    same type. In a program that uses machine learning, a vector measures a property
    of the data, such as color, density, loudness, or proximity—anything you can describe
    with a series of numbers, one for each thing being measured.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量* 具有扁平且简单的结构，是大多数现实世界机器学习应用中数据的典型体现。*标量* 是向量中的一个单个元素。向量有两个属性：一个表示向量维度的自然数，以及一个类型（例如实数、整数等）。整数的二维向量示例有
    (1, 2) 和 (-6, 0)；同样，标量可以是 1 或字符 *a*。实数的三维向量示例有 (1.1, 2.0, 3.9) 和 (∏, ∏/2, ∏/3)。你明白了：同一类型的数字集合。在一个使用机器学习的程序中，向量衡量数据的某个属性，如颜色、密度、响度或邻近性——任何可以用一系列数字描述的东西，每个数字对应于被测量的一个事物。'
- en: Moreover, a vector of vectors is a *matrix**.* If each feature vector describes
    the features of one object in your dataset, the matrix describes all the objects;
    each item in the outer vector is a node that’s a list of features of one object.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，向量的向量是一个 *矩阵**.* 如果每个特征向量描述了数据集中一个对象的特征，那么矩阵描述了所有对象；外部向量中的每个项目都是一个节点，它是一个对象的特征列表。
- en: Graphs, on the other hand, are more expressive. A *graph* is a collection of
    objects (*nodes*) that can be linked with *edges* to represent a network. A graphical
    structure enables representing relationships between objects, such as in a friendship
    network or a navigation route of a subway system. Consequently, they’re tremendously
    harder to manage in machine-learning applications. In this book, our input data
    will rarely involve a graphical structure.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，图（graphs）更具表现力。*图* 是一组可以与 *边* 相连的对象（*节点*），用以表示网络。图形结构能够表示对象之间的关系，例如在社交网络或地铁系统的导航路线中。因此，在机器学习应用中，它们的管理要困难得多。在这本书中，我们的输入数据很少会涉及图形结构。
- en: Feature vectors are practical simplifications of real-world data, which can
    be too complicated to deal with. Instead of attending to every little detail of
    a data item, using a feature vector is a practical simplification. A car in the
    real world, for example, is much more than the text used to describe it. A car
    salesman is trying to sell you the car, not the intangible words spoken or written.
    Those words are abstract concepts, similar to the way that feature vectors are
    summaries of the data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是现实世界数据的实用简化，这些数据可能过于复杂而难以处理。使用特征向量而不是关注数据项的每一个细节是一种实用的简化。例如，现实世界中的汽车远不止用来描述它的文本。汽车销售员试图卖给你的是汽车，而不是无形的话语或文字。这些话语是抽象概念，类似于特征向量是数据的摘要。
- en: The following scenario explains this concept further. When you’re in the market
    for a new car, keeping tabs on every minor detail of different makes and models
    is essential. After all, if you’re about to spend thousands of dollars, you may
    as well do so diligently. You’d likely record a list of the features of each car
    and compare these features. This ordered list of features is the feature vector.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下场景进一步解释了这一概念。当你打算购买一辆新车时，密切关注不同品牌和型号的每一个细节是至关重要的。毕竟，如果你即将花费数千美元，你最好勤奋地去做。你可能会记录下每辆车的特征列表并进行比较。这个有序的特征列表就是特征向量。
- en: When shopping for cars, you might find comparing mileage to be more lucrative
    than comparing something less relevant to your interest, such as weight. The number
    of features to track also must be right—not too few, or you’ll lose information
    you care about, and not too many, or they’ll be unwieldy and time consuming to
    keep track of. This tremendous effort to select both the number of measurements
    and which measurements to compare is called *feature engineering* or *feature
    selection*. Depending on which features you examine, the performance of your system
    can fluctuate dramatically. Selecting the right features to track can make up
    for a weak learning algorithm.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在购买汽车时，你可能发现比较油耗比比较一些不那么相关的因素（如重量）更有利可图。要跟踪的特征数量也必须恰到好处——既不能太少，否则你会失去你关心的信息，也不能太多，否则它们将难以管理且耗时。这种选择测量数量和比较哪些测量的巨大努力被称为*特征工程*或*特征选择*。根据你检查的特征，你系统的性能可能会大幅波动。选择正确的特征进行跟踪可以弥补一个弱学习算法的不足。
- en: When training a model to detect cars in an image, for example, you’ll gain an
    enormous performance and speed improvement if you first convert the image to grayscale.
    By providing some of your own bias when preprocessing the data, you end up helping
    the algorithm, because it won’t need to learn that colors don’t matter when detecting
    cars. The algorithm can instead focus on identifying shapes and textures, which
    will lead to much faster learning than trying to process colors as well.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当训练一个模型来检测图像中的汽车时，如果你首先将图像转换为灰度，你将获得巨大的性能和速度提升。通过在预处理数据时提供一些自己的偏见，你最终帮助了算法，因为它不需要学习颜色在检测汽车时并不重要。算法可以专注于识别形状和纹理，这将导致比尝试处理颜色更快的学习。
- en: The general rule of thumb in ML is that more data produces better results. But
    the same isn’t always true of having more features. Perhaps counterintuitively,
    if the number of features you’re tracking is too high, performance may suffer.
    Populating the space of all data with representative samples requires exponentially
    more data as the dimension of the feature vector increases. As a result, feature
    engineering, as depicted in figure 1.6, is one of the most significant problems
    in ML.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中的一般经验法则是，更多的数据会产生更好的结果。但拥有更多特征并不总是如此。也许出人意料的是，如果你跟踪的特征数量过高，性能可能会受到影响。随着特征向量维度的增加，用代表性样本填充所有数据的空间需要指数级更多的数据。因此，如图1.6所示的特征工程是机器学习中最重要的问题之一。
- en: Curse of dimensionality
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: To model real-world data accurately, we clearly need more than one or two data
    points. But how much data depends on a variety of things, including the number
    of dimensions in the feature vector. Adding too many features causes the number
    of data points required to describe the space to increase exponentially. That’s
    why we can’t design a 1,000,000-dimension feature vector to exhaust all possible
    factors and then expect the algorithm to learn a model. This phenomenon is called
    the curse of dimensionality*.*
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确地对现实世界数据进行建模，我们显然需要不止一个或两个数据点。但数据量取决于许多因素，包括特征向量的维度数。添加过多的特征会导致描述空间所需的数据点数量呈指数增长。这就是为什么我们不能设计一个包含一百万维度的特征向量来耗尽所有可能的因素，然后期望算法学习到一个模型。这种现象被称为维度诅咒**。
- en: You may not appreciate this fact right away, but something consequential happens
    when you decide which features are worth observing. For centuries, philosophers
    have pondered the meaning of *identity*; you may not immediately realize that
    you’ve come up with a definition of *identity* through your choice of specific
    features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能一开始不会意识到这一点，但当你决定哪些特征值得观察时，会发生一些重要的事情。几个世纪以来，哲学家们一直在思考*同一性*的含义；你可能不会立刻意识到，通过选择特定的特征，你已经对*同一性*给出了一种定义。
- en: '![CH01_F06_Mattmann2](../Images/CH01_F06_Mattmann2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F06_Mattmann2](../Images/CH01_F06_Mattmann2.png)'
- en: Figure 1.6 Feature engineering is the process of selecting relevant features
    for the task.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 特征工程是选择与任务相关的特征的过程。
- en: Imagine writing a machine-learning system to detect faces in an image. Suppose
    that one of the necessary features for something to be a face is the presence
    of two eyes. Implicitly, a face is now defined as something with eyes. Do you
    realize the kind of trouble that this definition can get you into? If a photo
    of a person shows them blinking, your detector won’t find a face, because it can’t
    find two eyes. The algorithm would fail to detect a face when a person is blinking.
    The definition of a face was inaccurate to begin with, and it’s apparent from
    the poor detection results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下编写一个机器学习系统来检测图像中的面部。假设面部的一个必要特征是存在两只眼睛。隐含地，面部现在被定义为有眼睛的东西。你是否意识到这种定义可能会给你带来什么样的麻烦？如果一张照片显示某人正在眨眼，你的检测器将找不到面部，因为它找不到两只眼睛。当一个人眨眼时，算法将无法检测到面部。面部的定义从一开始就不准确，从糟糕的检测结果中可以明显看出。
- en: Nowadays, especially with the tremendous speed at which capabilities like smart
    vehicles and autonomous drones are evolving, identity bias, or simply bias, in
    ML is becoming a big concern, because these capabilities can cause loss of human
    life if they screw up. Consider a smart vehicle that has never seen a person in
    a wheelchair because the training data never included those examples, so the smart
    car does not stop when the wheelchair enters the crosswalk. What if the training
    data for a company’s drone delivering your package never saw a female wearing
    a hat before, and all other training instances with things that look like hats
    were places to land? The hat—and, more important, the person—may be in grave danger!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尤其是在智能车辆和自主无人机等能力以惊人的速度发展的今天，机器学习中的身份偏差，或者简单地说偏差，已经成为一个重大的问题，因为这些能力如果出错可能会导致人员伤亡。考虑一辆从未见过轮椅中的人的智能车辆，因为训练数据中从未包含这些例子，所以当轮椅进入人行横道时，智能汽车不会停车。如果一家公司用于递送包裹的无人机训练数据从未见过戴帽子的女性，而所有其他看起来像帽子的训练实例都是着陆点，会怎样呢？帽子和，更重要的是，戴帽子的人可能会处于极大的危险中！
- en: The identity of an object is decomposed into the features from which it’s composed.
    If the features you’re tracking for one car match the corresponding features of
    another car, they may as well be indistinguishable from your perspective. You’d
    need to add another feature to the system to tell the cars apart; otherwise, you’ll
    think they’re the same item (like the drone landing on the poor lady’s hat). When
    handcrafting features, you must take great care not to fall into this philosophical
    predicament of identity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 物体的身份被分解为其组成的特征。如果你跟踪的一辆车的特征与另一辆车的对应特征相匹配，那么从你的角度来看，它们可能无法区分。你需要向系统中添加另一个特征来区分这些车辆；否则，你会认为它们是同一件物品（比如无人机落在那位不幸女士的帽子上）。在手工制作特征时，你必须非常小心，不要陷入这种关于身份的哲学困境。
- en: Exercise 1.3
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1.3
- en: 'Suppose that you’re teaching a robot how to fold clothes. The perception system
    sees a shirt lying on a table, as shown in the following figure. You’d like to
    represent the shirt as a vector of features so that you can compare it with different
    clothes. Decide which features are most useful to track. (Hint: What types of
    words do retailers use to describe their clothing online?)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在教机器人如何叠衣服。感知系统看到衬衫躺在桌子上，如图所示。你希望将衬衫表示为特征向量，以便你可以将其与不同的衣服进行比较。决定哪些特征最有用进行追踪。（提示：零售商在线描述他们的服装时使用哪些类型的词语？）
- en: '![CH01_F06_UN01_Mattmann2](../Images/CH01_F06_UN01_Mattmann2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F06_UN01_Mattmann2](../Images/CH01_F06_UN01_Mattmann2.png)'
- en: A robot is trying to fold a shirt. What are good features of the shirt to track?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一台机器人正在尝试叠衬衫。衬衫有哪些好的特征可以追踪？
- en: '**Answer**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: The width, height, x-symmetry score, y-symmetry score, and flatness are good
    features to observe when folding clothes. Color, cloth texture, and material are
    mostly irrelevant.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在叠衣服时，观察宽度、高度、x对称得分、y对称得分和平坦度是很好的特征。颜色、布料纹理和材料大多无关紧要。
- en: Exercise 1.4
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1.4
- en: Now, instead of detecting clothes, you ambitiously decide to detect arbitrary
    objects; the following figure shows some examples. What are some salient features
    that can easily differentiate objects?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你雄心勃勃地决定检测任意物体；以下图显示了几个示例。有哪些显著的特征可以轻松区分物体？
- en: '![CH01_F06_UN02_Mattmann2](../Images/CH01_F06_UN02_Mattmann2.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F06_UN02_Mattmann2](../Images/CH01_F06_UN02_Mattmann2.png)'
- en: 'Here are images of three objects: a lamp, a pair of pants, and a dog. What
    are some good features that you should record to compare and differentiate objects?'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了三个物体的图像：一盏灯、一条裤子和一只狗。你应记录哪些好的特征来比较和区分物体？
- en: '**Answer**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Observing brightness and reflection may help differentiate the lamp from the
    other two objects. The shape of pants often follows a predictable template, so
    shape would be another good feature to track. Finally, texture may be a salient
    feature for differentiating the picture of a dog from the other two classes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 观察亮度和反射可能有助于区分灯和其他两个物体。裤子的形状通常遵循一个可预测的模板，因此形状将是另一个很好的追踪特征。最后，纹理可能是区分狗的图像与其他两个类别的显著特征。
- en: Feature engineering is a refreshingly philosophical pursuit. For those who enjoy
    thought-provoking escapades into the meaning of self, I invite you to meditate
    on feature selection, because it’s still an open problem. Fortunately for the
    rest of you, to alleviate extensive debates, recent advances have made it possible
    to automatically determine which features to track. You’ll be able to try this
    process yourself in chapter 7.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是一项令人耳目一新的哲学追求。对于那些喜欢思考自我意义的探险者，我邀请你们沉思于特征选择，因为它仍然是一个未解之谜。幸运的是，对于你们其他人来说，为了缓解广泛的争论，最近的研究进展使得自动确定要追踪哪些特征成为可能。你们将在第7章中亲自尝试这个过程。
- en: Now consider the problem of a doctor looking at a set of N 244 × 244 (width
    × height) squamous-cell images like the ones shown in figure 1.7 and trying to
    determine whether they indicate the presence of cancer. Some images definitely
    indicate cancer; others do not. The doctor may have a set of historical patient
    images that he could examine and learn from over time, so that when he sees new
    images, he develops his own representation model of what cancer looks like.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑这样一个问题：一位医生正在查看一组N张244×244（宽度×高度）的鳞状细胞图像，就像图1.7中所示的那样，并试图确定它们是否表明患者存在癌症。一些图像明确表明有癌症；而另一些则没有。医生可能有一组历史患者图像，他可以随着时间的推移进行检查和学习，这样当他看到新的图像时，他就能发展出自己关于癌症外观的表征模型。
- en: '![CH01_F07_Mattmann2](../Images/CH01_F07_Mattmann2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F07_Mattmann2](../Images/CH01_F07_Mattmann2.png)'
- en: Figure 1.7 The machine-learning process. From left to right, doctors try to
    determine whether images representing biopsies of cells indicate cancer in their
    patients**.**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 机器学习过程。从左到右，医生试图确定代表细胞活检的图像是否表明他们的患者有癌症**。**
- en: Feature vectors are used in both learning and inference
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量在学习和推理中都得到应用
- en: The interplay between learning and inference provides a complete picture of
    a machine-learning system, as shown in the following figure. The first step is
    representing real-world data in a feature vector. We can represent images by a
    vector of numbers corresponding to pixel intensities, for example. (We’ll explore
    how to represent images in greater detail in future chapters.) We can show our
    learning algorithm the ground-truth labels (such as Bird or Dog) along with each
    feature vector. With enough data, the algorithm generates a learned model. We
    can use this model on other real-world data to uncover previously unknown labels.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 学习和推理之间的相互作用为机器学习系统提供了一个完整的图景，如下所示。第一步是将现实世界数据表示为特征向量。例如，我们可以通过像素强度对应的数字向量来表示图像。（我们将在未来的章节中更详细地探讨如何表示图像。）我们可以向学习算法展示与每个特征向量相关的真实标签（如鸟或狗）。有了足够的数据，算法就会生成一个学习模型。我们可以使用这个模型来处理其他现实世界数据，以揭示之前未知的标签。
- en: Feature vectors are a representation of real-world data used by both the learning
    and inference components of machine learning. The input to the algorithm isn’t
    the real-world image directly, but its feature vector.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是机器学习的学习和推理组件使用的现实世界数据的表示。算法的输入不是直接的现实世界图像，而是其特征向量。
- en: '![](../Images/Ch01-11.gif)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/Ch01-11.gif)'
- en: Feature vectors are representations of real-world data used by both the learning
    and inference components of machine learning. The input to the algorithm isn’t
    the real-world image directly, but its feature vector.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量是机器学习的学习和推理组件使用的现实世界数据的表示。算法的输入不是直接的现实世界图像，而是其特征向量。
- en: 'In machine learning, we are trying to emulate this model building process.
    First, we take N input squamous cancer cell 244 × 244 images from the historical
    patient data and prepare the problem by lining up the images with their associated
    labels (cancer or no cancer). We call this stage the data cleaning and preparation
    stage of machine learning. What follows is the process of identifying important
    features. Features include the image pixel intensities, or early value for each
    x, y, and c, or (244, 244, 3), for the image’s height, width, and three-channel
    red/green/blue (RGB) color. The model creates the mapping between those feature
    values and the desired label output: cancer or no cancer.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们试图模拟这个模型构建过程。首先，我们从历史患者数据中获取 N 张 244 × 244 的鳞状癌细胞图像，并通过将图像与其关联的标签（癌症或无癌症）排列起来来准备问题。我们称这个阶段为机器学习的数据清洗和准备阶段。接下来是识别重要特征的过程。特征包括图像像素强度，或每个
    x, y, 和 c 的早期值，或 (244, 244, 3)，代表图像的高度、宽度和三个通道的红/绿/蓝 (RGB) 颜色。模型创建这些特征值与所需标签输出（癌症或无癌症）之间的映射。
- en: 1.3 Distance metrics
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 距离度量
- en: If you have feature vectors of cars you may want to buy, you can figure out
    which two cars are most similar by defining a distance function on the feature
    vectors. Comparing similarities between objects is an essential component of machine
    learning. Feature vectors allow us to represent objects so that we may compare
    them in a variety of ways. A standard approach is to use the *Euclidian distance*,
    which is the geometric interpretation you may find most intuitive when thinking
    about points in space.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一系列你想要购买的特征向量，你可以在特征向量上定义一个距离函数，以确定哪两辆车最相似。比较对象之间的相似性是机器学习的一个基本组成部分。特征向量使我们能够以各种方式表示对象，以便进行比较。一种标准的方法是使用
    *欧几里得距离*，这是你在思考空间中的点时可能发现最直观的几何解释。
- en: 'Suppose that we have two feature vectors, *x* = (*x*[1], *x*[2], ..., *x*[n])
    and *y* = (*y*[1], *y*[2], ..., *y*[n]). The Euclidian distance ||*x* - *y* ||
    is calculated with the following equation, which scholars call the *L2 norm*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个特征向量，*x* = (*x*[1], *x*[2], ..., *x*[n]) 和 *y* = (*y*[1], *y*[2], ...,
    *y*[n])。欧几里得距离 ||*x* - *y* || 使用以下公式计算，学者们称之为 *L2 范数*：
- en: '![CH01_F07_UN03EQ01_Mattmann2](../Images/CH01_F07_UN03EQ01_Mattmann2.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F07_UN03EQ01_Mattmann2](../Images/CH01_F07_UN03EQ01_Mattmann2.png)'
- en: The Euclidian distance between (0, 1) and (1, 0) is
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 点 (0, 1) 和 (1, 0) 之间的欧几里得距离是
- en: '![CH01_F07_UN03EQ02_Mattmann2](../Images/CH01_F07_UN03EQ02_Mattmann2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F07_UN03EQ02_Mattmann2](../Images/CH01_F07_UN03EQ02_Mattmann2.png)'
- en: 'That function is only one of many possible distance functions, however. The
    L0, L1, and L-infinity norms also exist. All these norms are valid ways to measure
    distance. Here they are in more detail:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个函数只是许多可能的距离函数之一。L0、L1 和 L-infinity 范数也存在。所有这些范数都是测量距离的有效方式。以下是更详细的说明：
- en: The *L0 norm* counts the total nonzero elements of a vector. The distance between
    the origin (0, 0) and vector (0, 5) is 1, for example, because there’s only one
    nonzero element. The L0 distance between (1, 1) and (2, 2) is 2, because neither
    dimension matches up. Imagine that the first and second dimensions represent username
    and password, respectively. If the L0 distance between a login attempt and the
    true credentials is 0, the login is successful. If the distance is 1, either the
    username or password is incorrect, but not both. Finally, if the distance is 2,
    neither username nor password is found in the database.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L0范数*计算向量的总非零元素数。例如，原点(0, 0)和向量(0, 5)之间的距离是1，因为只有一个非零元素。(1, 1)和(2, 2)之间的L0距离是2，因为两个维度都不匹配。想象一下，第一和第二维度分别代表用户名和密码。如果登录尝试和真实凭证之间的L0距离是0，则登录成功。如果距离是1，则用户名或密码错误，但不是两者都错误。最后，如果距离是2，则数据库中找不到用户名或密码。'
- en: The *L1 norm*, shown in figure 1.8, is defined as Σ*x*[*n*]. The distance between
    two vectors under the L1 norm is also referred to as the *Manhattan distance*.
    Imagine living in a downtown area like Manhattan, where the streets form a grid.
    The shortest distance from one intersection to another is along the blocks. Similarly,
    the L1 distance between two vectors is along the orthogonal directions. The distance
    between (0, 1) and (1, 0) under the L1 norm is 2\. Computing the L1 distance between
    two vectors is the sum of absolute differences at each dimension, which is a useful
    measure of similarity.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1范数*，如图1.8所示，定义为Σ*x*[*n*]。在L1范数下，两个向量之间的距离也被称为*曼哈顿距离*。想象一下生活在像曼哈顿这样的市中心地区，街道形成了一个网格。从一个交叉口到另一个交叉口的最短距离是沿着街区。同样，两个向量之间的L1距离是沿着正交方向。在L1范数下，(0,
    1)和(1, 0)之间的距离是2。计算两个向量之间的L1距离是每个维度绝对差分的总和，这是一个有用的相似度度量。'
- en: '![CH01_F08_Mattmann2](../Images/CH01_F08_Mattmann2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F08_Mattmann2](../Images/CH01_F08_Mattmann2.png)'
- en: Figure 1.8 The L1 distance is called the Manhattan distance (also the taxicab
    metric) because it resembles the route of a car in a gridlike neighborhood such
    as Manhattan. If a car is traveling from point (0, 1) to point (1, 0), the shortest
    route requires a length of 2 units.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 L1距离被称为曼哈顿距离（也称为出租车距离），因为它类似于在像曼哈顿这样的网格状社区中汽车行驶的路线。如果一辆车从点(0, 1)行驶到点(1,
    0)，最短路线需要2个单位的长度。
- en: The *L2 norm*, shown in figure 1.9, is the Euclidian length of a vector, (Σ(x[n])²)^(1/2)
    It’s the most direct route you can possibly take on a geometric plane to get from
    one point to another. For the mathematically inclined, this norm implements the
    least-squares estimation as predicted by the Gauss-Markov theorem. For the rest
    of you, it’s the shortest distance between two points in space.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L2范数*，如图1.9所示，是向量的欧几里得长度，(Σ(x[n])²)^(1/2)。这是在几何平面上从一点到另一点可以采取的最直接路线。对于数学爱好者来说，这个范数实现了高斯-马尔可夫定理预测的最小二乘估计。对于其他人来说，它是空间中两点之间的最短距离。'
- en: '![CH01_F09_Mattmann2](../Images/CH01_F09_Mattmann2.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F09_Mattmann2](../Images/CH01_F09_Mattmann2.png)'
- en: Figure 1.9 The L2 norm between points (0, 1) and (1, 0) is the length of a single
    straight-line segment between both points.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 点(0, 1)和(1, 0)之间的L2范数是两点之间单一直线段的长度。
- en: The *L-N norm* generalizes this pattern, resulting in (Σ(|x[n]|)^N)^(1/N). We
    rarely use finite norms above L2, but it’s here for completeness.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L-N范数*将这种模式推广，结果为(Σ(|x[n]|)^N)^(1/N)。我们很少使用L2以上的有限范数，但这里列出是为了完整性。'
- en: The *L-infinity norm* is (Σ(|x[n]|)^∞)^(1/∞). More naturally, it’s the largest
    magnitude among each element. If the vector is (-1, -2, -3), the L-infinity norm
    is 3\. If a feature vector represents costs of various items, minimizing the L-infinity
    norm of the vector is an attempt to reduce the cost of the most expensive item.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L无穷范数*是(Σ(|x[n]|)^∞)^(1/∞)。更自然地说，它是每个元素中的最大幅度。如果向量是(-1, -2, -3)，则L无穷范数是3。如果一个特征向量代表各种物品的成本，最小化向量的L无穷范数是尝试减少最昂贵物品的成本。'
- en: 1.4 Types of learning
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 学习类型
- en: 'Now that you can compare feature vectors, you have the tools necessary to use
    data for practical algorithms. Machine learning is often split into three perspectives:
    supervised learning, unsupervised learning, and reinforcement learning. An emerging
    new area is meta-learning, sometimes called AutoML. The following sections examine
    all four types.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用特征向量进行比较，您有了使用数据为实际算法提供工具的必要条件。机器学习通常分为三个视角：监督学习、无监督学习和强化学习。一个新兴的新领域是元学习，有时称为AutoML。以下几节将检查所有四种类型。
- en: When do I use a metric other than the L2 norm in the real world?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，我什么时候会使用L2范数以外的度量标准？
- en: Let’s say you’re working for a search-engine startup trying to compete with
    Google. Your boss assigns you the task of using machine learning to personalize
    the search results for each user.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在为一家搜索引擎初创公司工作，试图与谷歌竞争。你的老板分配给你一个任务，就是使用机器学习来为每个用户个性化搜索结果。
- en: A good goal might be that users shouldn’t see five or more incorrect search
    results per month. A year’s worth of user data is a 12-dimensional vector (each
    month of the year is a dimension), indicating the number of incorrect results
    shown per month. You’re trying to satisfy the condition that the L-infinity norm
    of this vector must be less than 5.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的目标可能是用户每月不应看到五个或更多的错误搜索结果。一年的用户数据是一个12维向量（每年的每个月份是一个维度），表示每月显示的错误结果的数目。你试图满足这个条件，即这个向量的L-无穷范数必须小于5。
- en: Suppose instead that your boss changes the requirements, saying that fewer than
    five erroneous search results are allowed for the entire year. In this case, you’re
    trying to achieve an L1 norm below 5, because the sum of all errors in the entire
    space should be less than 5.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的老板改变了要求，说整个年度内允许的错误搜索结果少于五个。在这种情况下，你试图实现L1范数低于5，因为整个空间中所有错误的和应该小于5。
- en: 'Now your boss changes the requirements again: the number of months with erroneous
    search results should be fewer than 5\. In that case, you’re trying to achieve
    an L0 norm less than 5, because the number of months with a nonzero error should
    be fewer than 5.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在老板再次改变了要求：错误搜索结果的月份数量应少于5。在这种情况下，你试图实现L0范数小于5，因为非零错误的月份数量应少于5。
- en: 1.4.1 Supervised learning
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 监督学习
- en: By definition, a supervisor is someone higher up in the chain of command. When
    we’re in doubt, our supervisor dictates what to do. Likewise, *supervised learning*
    is all about learning from examples laid out by a supervisor (such as a teacher).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，一个监督者是命令链中的高级人员。当我们犹豫不决时，我们的监督者会指示我们做什么。同样，*监督学习*完全是关于从监督者（如教师）提供的示例中学习。
- en: A supervised machine-learning system needs labeled data to develop a useful
    understanding, which we call its *model*. Given many photographs of people and
    their recorded corresponding ethnicities, for example, we can train a model to
    classify the ethnicity of a never-before-seen person in an arbitrary photograph.
    Simply put, a model is a function that assigns a label to data by using a collection
    of previous examples, called a *training dataset*, as reference.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个监督机器学习系统需要标记数据来发展有用的理解，我们称之为其*模型*。例如，给定许多人的照片和记录的相应种族，我们可以训练一个模型来对任意照片中从未见过的人的种族进行分类。简单来说，模型是一个函数，通过使用一组称为*训练数据集*的先前示例作为参考，为数据分配标签。
- en: A convenient way to talk about models is through mathematical notation. Let
    *x* be an instance of data, such as a feature vector. The label associated with
    *x* is *f* (*x*), often referred to as the *ground truth* of *x*. Usually, we
    use the variable *y* = *f* (*x*) because it’s quicker to write. In the example
    of classifying the ethnicity of a person through a photograph, *x* can be a 100-dimensional
    vector of various relevant features, and *y* is one of a couple of values to represent
    the various ethnicities. Because *y* is discrete with few values, the model is
    called a *classifier*. If `y` can result in many values, and the values have a
    natural ordering, the model is called a *regressor*.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数学符号来谈论模型是一种方便的方式。设*x*为数据的一个实例，例如一个特征向量。与*x*相关联的标签是*f*(*x*)，通常被称为*x*的*真实值*。通常，我们使用变量*y*
    = *f*(*x*)，因为它写起来更快。在通过照片对人的种族进行分类的例子中，*x*可以是一个包含各种相关特征的100维向量，而*y*是代表各种种族的一组值之一。因为*y*是离散的，值很少，所以该模型被称为*分类器*。如果`y`可以产生许多值，并且这些值有自然顺序，则该模型被称为*回归器*。
- en: Let’s denote a model’s prediction of *x* as *g* (*x*). Sometimes, you can tweak
    a model to change its performance dramatically. Models have parameters that can
    be tuned by a human or automatically. We use the vector to represent the parameters.
    Putting it all together, *g* (*x*|) more completely represents the model, read
    “*g* of *x* given.”
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用*g*(*x*)表示模型对*x*的预测。有时，你可以调整模型以显著改变其性能。模型有参数，可以由人类或自动调整。我们用向量来表示参数。将所有这些放在一起，*g*(*x*|)更完整地表示了模型，读作“*g*
    of *x* given。”
- en: NOTE Models may also have *hyperparameters**,* which are extra ad-hoc properties
    of a model. The term *hyper* in *hyperparameter* may seem a bit strange at first.
    A better name could be *metaparameter**,* because the parameter is akin to metadata
    about the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** 模型也可能有*超参数*，这是模型的一些额外临时属性。*超参数*中的*超*字眼一开始可能显得有些奇怪。更好的名字可能是*元参数*，因为参数类似于关于模型的元数据。'
- en: The success of a model’s prediction *g*(*x*|) depends on how well it agrees
    with the ground truth *y*. We need a way to measure the distance between these
    two vectors. The L2 norm may be used to measure how close two vectors lie, for
    example. The distance between the ground truth and the prediction is called the
    *cost*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测 g(x|) 的成功取决于它与真实值 y 的吻合程度。我们需要一种方法来衡量这两个向量之间的距离。例如，L2 范数可以用来衡量两个向量之间的接近程度。真实值与预测值之间的距离称为*成本*。
- en: 'The essence of a supervised machine-learning algorithm is to figure out the
    parameters of a model that result in the least *cost*. Mathematically put, we’re
    looking for a θ* (pronounced theta star) that minimizes the cost among all data
    points *x* ∈ *X*. One way of formalizing this optimization problem is the following
    equation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习算法的本质是找出导致最小*成本*的模型参数。从数学上讲，我们正在寻找一个 θ*（读作 theta star），它在所有数据点 x ∈ X 中最小化成本。将这个优化问题形式化的一个方法如下方程：
- en: θ^(* )= argmin[θ]*Cost*(θ|X)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: θ^(* )= argmin[θ]*Cost*(θ|X)
- en: where
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![CH01_F09EQ05_Mattmann2](../Images/CH01_F09EQ05_Mattmann2.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F09EQ05_Mattmann2](../Images/CH01_F09EQ05_Mattmann2.png)'
- en: Clearly, brute-forcing every possible combination of x (also known as a *parameter
    space*) will eventually find the optimal solution, but at an unacceptable run
    time. A major area of research in **machine** learning is about writing algorithms
    that efficiently search this parameter space. Some of the early algorithms include
    *gradient descent**, simulated annealing**,* and *genetic algorithms**.* TensorFlow
    automatically takes care of the low-level implementation details of these algorithms,
    so I won’t get into them in too much detail.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，穷举所有可能的 x 组合（也称为*参数空间*）最终会找到最优解，但运行时间将无法接受。**机器学习**的一个主要研究领域是编写高效搜索这个参数空间的算法。一些早期的算法包括*梯度下降*、模拟退火**和*遗传算法**.*
    TensorFlow 自动处理这些算法的低级实现细节，因此我不会过多地深入这些细节。
- en: After the parameters are learned one way or another, you can finally evaluate
    the model to figure out how well the system captured patterns from the data. A
    rule of thumb is to not evaluate your model on the same data you used to train
    it, because you already know it works for the training data; you need to tell
    whether the model works for data that *wasn’t* part of the training set, to make
    sure your model is general-purpose and not *biased* to the data used to train
    it. Use the majority of the data for training and the remainder for testing. If
    you have 100 labeled data points, for example, randomly select 70 of them to train
    a model and reserve the other 30 to test it, creating a 70-30 split.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 无论通过何种方式学习参数后，你最终可以评估模型以了解系统从数据中捕获模式的效果如何。一个经验法则是不要在用于训练的数据上评估你的模型，因为你已经知道它对训练数据有效；你需要判断模型是否对*未*包含在训练集中的数据进行有效处理，以确保你的模型是通用型的，而不是对训练数据有*偏见*。使用大部分数据用于训练，其余的用于测试。例如，如果你有
    100 个标记的数据点，随机选择其中的 70 个来训练一个模型，并保留其余的 30 个来测试它，这样就创建了一个 70-30 的分割。
- en: Why split the data?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么需要分割数据？
- en: If the 70-30 split seems odd to you, think about it this way. Suppose that your
    physics teacher gives you a practice exam and tells you that the real exam will
    be no different. You might as well memorize the answers and earn a perfect score
    without understanding the concepts. Similarly, if you test your model on the training
    dataset, you’re not doing yourself any favors. You risk a false sense of security,
    because the model may merely be memorizing the results. Now, where’s the intelligence
    in that?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 70-30 的分割看起来很奇怪，可以这样思考。假设你的物理老师给你一份练习考试，并告诉你真正的考试将与这次没有区别。你不妨记住答案，而不理解概念就能得到满分。同样，如果你在训练数据集上测试你的模型，你并没有给自己带来任何好处。你可能会产生一种虚假的安全感，因为模型可能只是记住结果。那么，这种所谓的“智慧”在哪里呢？
- en: Instead of using the 70-30 split, machine-learning practitioners typically divide
    their datasets 60-20-20\. Training consumes 60% of the dataset, and testing uses
    20%, leaving the other 20% for validation, which is explained in chapter 2.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用70-30的分割不同，机器学习从业者通常将他们的数据集分成60-20-20。训练消耗60%的数据集，测试使用20%，剩下的20%用于验证，这在第2章中有解释。
- en: 1.4.2 Unsupervised learning
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 无监督学习
- en: '*Unsupervised* *learning* is about modeling data that comes without corresponding
    labels or responses. The fact that we can make any conclusions at all on raw data
    feels like magic. With enough data, it may be possible to find patterns and structure.
    Two of the most powerful tools that machine-learning practitioners use to learn
    from data alone are clustering and dimensionality reduction.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习* 是关于对没有相应标签或响应的数据进行建模。我们能够在原始数据上得出任何结论都感觉像是魔法。有了足够的数据，可能可以发现模式和结构。机器学习从业者从数据本身学习时使用的两个最强大的工具是聚类和维度降低。'
- en: '*Clustering* is the process of splitting the data into individual buckets of
    similar items. In a sense, clustering is like classifying data without knowing
    any corresponding labels. When organizing your books on three shelves, for example,
    you likely place similar genres together, or maybe you group them by the authors’
    last names. You might have a Stephen King section, another for textbooks, and
    a third for anything else. You don’t care that all the books are separated by
    the same feature, only that each book has something unique that allows you to
    organize it into one of several roughly equal, easily identifiable groups. One
    of the most popular clustering algorithms is *k-means**,* which is a specific
    instance of a more powerful technique called the *E-M algorithm**.*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚类* 是将数据分割成相似项的单独桶的过程。从某种意义上说，聚类就像在没有知道任何对应标签的情况下对数据进行分类。例如，当你将你的书组织在三个书架上时，你可能会将相似类型的书籍放在一起，或者可能按作者姓氏分组。你可能有一个斯蒂芬·金区域，另一个用于教科书，第三个用于其他任何东西。你不在乎所有书籍是否由相同的特征分开，只在乎每本书都有一些独特的东西，这让你能够将其组织到几个大致相等、易于识别的组中。最流行的聚类算法之一是
    *k-means*，它是称为 *E-M 算法* 的更强大技术的一个特定实例。'
- en: '*Dimensionality reduction* is about manipulating the data to view it from a
    much simpler perspective—the ML equivalent of the phrase “Keep it simple, stupid.”
    By getting rid of redundant features, for example, we can explain the same data
    in a lower-dimensional space and see which features matter. This simplification
    also helps in data visualization or preprocessing for performance efficiency.
    One of the earliest algorithms is *principle component analysis* (PCA), and a
    newer one is *autoencoders*, which are covered in chapter 7.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*维度降低* 是关于操纵数据以便从更简单的角度来观察它——机器学习中的“保持简单，愚蠢”这一短语的等价物。例如，通过去除冗余特征，我们可以用更低维度的空间来解释相同的数据，并看到哪些特征是重要的。这种简化也有助于数据可视化或预处理以提高性能效率。最早的算法之一是
    *主成分分析* (PCA)，而较新的一个是 *自编码器*，这些内容在第7章中有介绍。'
- en: 1.4.3 Reinforcement learning
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 强化学习
- en: Supervised and unsupervised learning seem to suggest that the existence of a
    teacher is all or nothing. But in one well-studied branch of machine learning,
    the environment acts as a teacher, providing hints as opposed to definite answers.
    The learning system receives feedback on its actions, with no concrete promise
    that it’s progressing in the right direction, which might be to solve a maze or
    accomplish an explicit goal.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习似乎暗示了教师的存在要么全部要么没有。但在机器学习的一个研究得很好的分支中，环境充当教师，提供提示而不是明确的答案。学习系统对其行为获得反馈，没有具体的承诺表明它在正确的方向上进步，这可能是指解决迷宫或完成一个明确的目标。
- en: 'Exploration vs. exploitation: The heart of reinforcement learning'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用：强化学习的核心
- en: Imagine playing a video game that you’ve never seen before. You click buttons
    on a controller and discover that a particular combination of strokes gradually
    increases your score. Brilliant! Now you repeatedly exploit this finding in the
    hope of beating the high score. In the back of your mind, however, you wonder
    whether you’re missing out on a better combination of button clicks. Should you
    exploit your current best strategy or risk exploring new options?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下玩一个你从未见过的视频游戏。你在控制器上点击按钮，发现某个特定的按键组合逐渐提高你的分数。太棒了！现在你反复利用这个发现，希望能打破高分。然而，在你的潜意识里，你却在想是否错过了更好的按键组合。你应该利用你当前的最佳策略，还是冒险探索新的选项？
- en: Unlike supervised learning, in which training data is conveniently labeled by
    a “teacher,” *reinforcement learning* trains on information gathered by observing
    how the environment reacts to actions. Reinforcement learning is a type of machine
    learning that interacts with the environment to learn which combination of actions
    yields the most favorable results. Because we’re already anthropomorphizing algorithms
    by using the words *environment* and *action*, scholars typically refer to the
    system as an autonomous *agent*. Therefore, this type of machine learning naturally
    manifests itself in the domain of robotics.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，在监督学习中，训练数据由“教师”方便地标注，*强化学习*则通过观察环境对动作的反应来收集信息进行训练。强化学习是一种与环境交互以学习哪种动作组合能产生最理想结果的机器学习方法。因为我们已经通过使用“环境”和“动作”这些词将算法拟人化了，学者们通常将这个系统称为自主的*代理*。因此，这种机器学习方法自然地体现在机器人领域。
- en: 'To reason about agents in the environment, we introduce two new concepts: states
    and actions. The status of the world frozen at a particular time is called a state.
    An agent may perform one of many actions to change the current state. To drive
    an agent to perform actions, each state yields a corresponding reward. An agent
    eventually discovers the expected total reward of each state, called the *value*
    of a state.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在环境中推理代理，我们引入了两个新概念：状态和动作。在特定时间冻结的世界状态称为状态。代理可以通过执行许多动作之一来改变当前状态。为了驱动代理执行动作，每个状态都会产生相应的奖励。代理最终会发现每个状态的预期总奖励，称为该状态的价值。
- en: 'Like any other machine-learning system, performance improves with more data.
    In this case, the data is a history of experiences. In reinforcement learning,
    we don’t know the final cost or reward of a series of actions until that series
    is executed. These situations render traditional supervised learning ineffective,
    because we don’t know exactly which action in the history of action sequences
    is to blame for ending up in a low-value state. The only information an agent
    knows for certain is the cost of a series of actions that it has already taken,
    which is incomplete. The agent’s goal is to find a sequence of actions that maximizes
    rewards. If you’re more interested in this subject, you may want to check out
    another topical book in the Manning Publications family: *Grokking Deep Reinforcement
    Learning*, by Miguel Morales (Manning, 2020; [https://www .manning.com/books/grokking-deep-reinforcement-learning](https://www.manning.com/books/grokking-deep-reinforcement-learning)).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他机器学习系统一样，性能随着数据的增加而提高。在这种情况下，数据是经验的历史。在强化学习中，我们不知道一系列动作的最终成本或奖励，直到这一系列动作被执行。这些情况使得传统的监督学习变得无效，因为我们不知道在动作序列的历史中，哪个动作导致了最终的低价值状态。代理所知道的确切信息只是它已经采取的一系列动作的成本，这是不完整的。代理的目标是找到一系列动作，以最大化奖励。如果你对这个主题更感兴趣，你可能想查看Manning
    Publications家族的另一本相关书籍：Miguel Morales所著的*Grokking Deep Reinforcement Learning*（Manning，2020；[https://www.manning.com/books/grokking-deep-reinforcement-learning](https://www.manning.com/books/grokking-deep-reinforcement-learning)）。
- en: 1.4.4 Meta-learning
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.4 元学习
- en: Relatively recently, a new area of machine learning called meta-learning has
    emerged. The idea is simple. Data scientists and ML experts spend a tremendous
    amount of time executing the steps of ML, as shown in figure 1.7\. What if those
    steps—defining and representing the problem, choosing a model, testing the model,
    and evaluating the model—could themselves be automated? Instead of being limited
    to exploring only one or a small group of models, why not have the program itself
    try all the models?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 相对较近，一个新的机器学习领域——元学习——出现了。这个想法很简单。数据科学家和机器学习专家花费大量时间执行机器学习的步骤，如图1.7所示。如果这些步骤——定义和表示问题、选择模型、测试模型和评估模型——本身可以被自动化，那会怎样？为什么不限制于只探索一个或一小组模型，而是让程序本身尝试所有模型呢？
- en: 'Many businesses separate the roles of the domain expert (refer to the doctor
    in figure 1.7), the data scientist (the person modeling the data and potentially
    extracting or choosing features that are important, such as the image RGB pixels),
    and the ML engineer (responsible for tuning, testing, and deploying the model),
    as shown in figure 1.10a. As you’ll remember from earlier in the chapter, these
    roles interact in three basic areas: data cleaning and prep, which both the domain
    expert and data scientist may help with; feature and model selection, mainly a
    data-scientist job with a little help from the ML engineer; and then train, test,
    and evaluate, mostly the job of the ML engineer with a little help from the data
    scientist. We’ve added a new wrinkle: taking our model and deploying it, which
    is what happens in the real world and is something that brings its own set of
    challenges. This scenario is one reason why you are reading the second edition
    of this book; it’s covered in chapter 2, where I discuss deploying and using TensorFlow.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 许多企业将领域专家（如图1.7中的医生）、数据科学家（负责建模数据和可能提取或选择重要特征的人员，例如图像的RGB像素）以及机器学习工程师（负责调整、测试和部署模型）的角色分开，如图1.10a所示。正如你从本章前面的内容中记得的那样，这些角色在三个基本领域进行互动：数据清洗和准备，这两个领域领域专家和数据科学家都可能提供帮助；特征和模型选择，主要是数据科学家的工作，ML工程师提供一些帮助；然后是训练、测试和评估，主要是ML工程师的工作，数据科学家提供一些帮助。我们还增加了一个新的复杂因素：将我们的模型部署出去，这在现实世界中会发生，并且带来了一组自己的挑战。这就是你正在阅读本书第二版的原因之一；它将在第2章中讨论，我将讨论部署和使用TensorFlow。
- en: 'What if instead of having **data scientists** and ML engineers pick models,
    train, evaluate, and tune them, we could have the system automatically search
    over the space of possible models, and try them all? This approach overcomes limiting
    your overall ML experience to a small number of possible solutions wherein you’ll
    likely choose the first one that performs reasonably. But what if the system could
    figure out which models are best and how to tune the models automatically? That’s
    precisely what you see in figure 1.10b: the process of meta-learning, or AutoML.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够让系统自动在可能模型的范围内搜索，并尝试所有模型，而不是让数据科学家和ML工程师选择模型、训练、评估和调整它们，会怎么样呢？这种方法克服了将你的整体ML经验限制在少量可能解决方案中的限制，你可能会选择第一个表现合理的解决方案。但如果系统能够找出哪些模型最好以及如何自动调整模型呢？这正是你在图1.10b中看到的内容：元学习的过程，或自动机器学习（AutoML）。
- en: '![CH01_F10_Mattmann2](../Images/CH01_F10_Mattmann2.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F10_Mattmann2](../Images/CH01_F10_Mattmann2.png)'
- en: Figure 1.10 Traditional ML and its evolution to meta-learning, in which the
    system does its own model selection, training, tuning, and evaluation to pick
    the best ML model among many candidates
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 传统机器学习及其演变为元学习，其中系统自行进行模型选择、训练、调整和评估，以从众多候选模型中选择最佳机器学习模型
- en: Data scientists, you’re canceled!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家，你们被取消了！
- en: Today’s cancel culture lends itself nicely to the concept of meta-learning,
    whose roots grew from the idea that data science itself—the process of creating
    and experimenting with many types of ML pipelines, including data cleaning, model
    building, and testing—can be automated. An associated Defense Advanced Research
    Projects Agency (DARPA) program, Data Driven Discovery of Models (D3M), had the
    purported goal of doing away with data scientists and instead automating their
    activities. Although the results of that DARPA program and the field of meta-learning
    thus far are promising, we’re not quite ready to fully cancel data scientists
    . . . yet. Don’t worry; you’re safe!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的取消文化非常适合元学习这一概念，其根源源于数据科学本身——创建和实验多种类型的机器学习管道的过程，包括数据清洗、模型构建和测试——可以自动化的想法。与之相关的一个高级研究计划局（DARPA）项目，数据驱动模型发现（D3M），声称目标是废除数据科学家，而是自动化他们的活动。尽管那个DARPA项目以及元学习领域的成果是有希望的，但我们还没有准备好完全取消数据科学家……至少现在还没有。别担心；你很安全！
- en: '**Exercise 1.5**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1.5**'
- en: Would you use supervised, unsupervised, reinforcement, or meta-learning to solve
    the following problems? (a) Find the best ML algorithm that takes baseball statistics
    and predicts whether a player will make the Hall of Fame. (b) Organize various
    fruits in three baskets based on no other information. (c) Predict the weather
    based on sensor data. (d) Learn to play chess well after many trial-and-error
    attempts.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你会使用监督学习、无监督学习、强化学习还是元学习来解决以下问题？（a）找到最佳机器学习算法，该算法使用棒球统计数据并预测一名球员是否会进入名人堂。（b）根据没有其他信息的情况，将各种水果组织到三个篮子里。（c）根据传感器数据预测天气。（d）在多次尝试和错误后学会下棋。
- en: '**Answer**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: (a) Meta-learning; (b) Unsupervised; (c) Supervised; (d) Reinforcement.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: （a）元学习；（b）无监督学习；（c）监督学习；（d）强化学习。
- en: 1.5 TensorFlow
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 TensorFlow
- en: Google open-sourced its machine-learning framework, TensorFlow, in late 2015
    under the Apache 2.0 license. Before that, it was used proprietarily by Google
    in speech recognition, Search, Photos, and Gmail, among other applications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在2015年底以Apache 2.0许可证开源了其机器学习框架TensorFlow。在此之前，它被谷歌在语音识别、搜索、照片和Gmail等应用中私有使用。
- en: A bit of history
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一点历史
- en: A former scalable distributed training and learning system called DistBelief
    is the primary influence on TensorFlow’s current implementation. Have you ever
    written a messy piece of code and wished you could start over? That’s the dynamic
    between DistBelief and TensorFlow. TensorFlow is not the first system Google open-sourced
    based on an internal project. Google’s famous Map-Reduce system and Google File
    System (GFS) are the basis of modern Apache data processing, web-crawling, and
    big data systems including Hadoop, Nutch, and Spark. Additionally, Google’s bigTable
    system is where the Apache Hbase project came from.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为DistBelief的早期可扩展分布式训练和学习系统是TensorFlow当前实现的主要影响。你有没有写过一团糟的代码，希望可以重新开始？这就是DistBelief和TensorFlow之间的动态。TensorFlow不是谷歌基于内部项目开源的第一个系统。谷歌著名的Map-Reduce系统和Google
    File System（GFS）是现代Apache数据处理、网络爬虫和大数据系统（包括Hadoop、Nutch和Spark）的基础。此外，谷歌的大表系统是Apache
    Hbase项目的基础。
- en: The library is implemented in C++ and has a convenient Python API, as well as
    a less-appreciated C++ API. Because of the simpler dependencies, TensorFlow can
    be quickly deployed to various architectures.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该库是用C++实现的，具有方便的Python API，以及一个不太受欢迎的C++ API。由于依赖关系更简单，TensorFlow可以快速部署到各种架构。
- en: Similar to Theano—a popular numerical computation library for Python that you
    may be familiar with ([http://deeplearning.net/software/theano](http://deeplearning.net/software/theano/))—computations
    are described as flowcharts, separating design from implementation. With little
    to no hassle, this dichotomy allows the same design to be implemented on mobile
    devices as well as large-scale training systems with thousands of processors.
    The single system spans a broad range of platforms. TensorFlow also plays nicely
    with a variety of newer, similarly-developed ML libraries, including Keras (TensorFlow
    2.0 is fully integrated with Keras), along with libraries such as PyTorch ([https://pytorch.org](https://pytorch.org/)),
    originally developed by Facebook, and richer application programming interfaces
    for ML such as Fast.Ai. You can use many toolkits to do ML, but you’re reading
    a book about TensorFlow, right? Let’s focus on it!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与Theano——一个你可能熟悉的流行的Python数值计算库——类似——计算被描述为流程图，将设计从实现中分离出来。几乎无需麻烦，这种二分法使得相同的设计可以在移动设备以及拥有数千个处理器的庞大训练系统中实现。单一系统覆盖了广泛的平台。TensorFlow也与各种新开发的类似机器学习库兼容得很好，包括Keras（TensorFlow
    2.0完全集成了Keras），以及PyTorch（[https://pytorch.org](https://pytorch.org/)）等库，这些库最初由Facebook开发，以及更丰富的机器学习应用程序编程接口，如Fast.Ai。你可以使用许多工具包来做机器学习，但你正在读一本关于TensorFlow的书，对吧？让我们专注于它！
- en: One of the fanciest properties of TensorFlow is its *automatic differentiation*
    capabilities. You can experiment with new networks without having to redefine
    many key calculations.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow最令人印象深刻的特性之一是其**自动微分**功能。你可以在不重新定义许多关键计算的情况下实验新的网络。
- en: note Automatic differentiation makes it much easier to implement *backpropagation*,
    which is a computationally-heavy calculation used in a branch of machine learning
    called *neural networks*. TensorFlow hides the nitty-gritty details of backpropagation
    so you can focus on the bigger picture. Chapter 11 covers an introduction to neural
    networks with TensorFlow.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意自动微分使得实现**反向传播**变得容易得多，这是一种在机器学习的一个分支——**神经网络**中使用的计算密集型计算。TensorFlow隐藏了反向传播的细节，这样你可以专注于更大的图景。第11章介绍了TensorFlow中的神经网络简介。
- en: All the mathematics is abstracted away and unfolded under the hood. Using TensorFlow
    is like using WolframAlpha for a calculus problem set.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数学都被抽象出来，在幕后展开。使用TensorFlow就像使用WolframAlpha来解决微积分问题集。
- en: Another feature of this library is its interactive visualization environment,
    called *TensorBoard*. This tool shows a flowchart of the way data transforms,
    displays summary logs over time, and traces performance. Figure 1.11 shows what
    TensorBoard looks like; chapter 2 covers using it.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该库的另一个特点是它的交互式可视化环境，称为**TensorBoard**。这个工具显示了数据转换的流程图，随着时间的推移显示总结日志，并追踪性能。图1.11显示了TensorBoard的外观；第2章介绍了如何使用它。
- en: '![CH01_F11_Mattmann2](../Images/CH01_F11_Mattmann2.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F11_Mattmann2](../Images/CH01_F11_Mattmann2.png)'
- en: Figure 1.11 Example of TensorBoard in action
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 TensorBoard的实际应用示例
- en: Prototyping in TensorFlow is much faster than in Theano (code initiates in a
    matter of seconds as opposed to minutes) because many of the operations come precompiled.
    It becomes easy to debug code due to subgraph execution; an entire segment of
    computation can be reused without recalculation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中进行原型设计比在Theano中快得多（代码启动只需几秒钟，而不是几分钟），因为许多操作都是预编译的。由于子图执行，调试代码变得容易；整个计算段可以重用而无需重新计算。
- en: Because TensorFlow isn’t only about neural networks, it also has out-of-the-box
    matrix computation and manipulation tools. Most libraries, such as PyTorch, Fast.Ai,
    and Caffe, are designed solely for deep neural networks, but TensorFlow is more
    flexible as well as scalable.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因为TensorFlow不仅仅关于神经网络，它还提供了现成的矩阵计算和操作工具。大多数库，如PyTorch、Fast.Ai和Caffe，都是专门为深度神经网络设计的，但TensorFlow更加灵活和可扩展。
- en: The library is well-documented and officially supported by Google. Machine learning
    is a sophisticated topic, so having an exceptionally reputable company behind
    TensorFlow is comforting.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该库得到了良好的文档记录，并得到了Google的官方支持。机器学习是一个复杂的话题，因此有一个非常值得信赖的公司支持TensorFlow是令人欣慰的。
- en: 1.6 Overview of future chapters
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 未来章节概述
- en: Chapter 2 demonstrates how to use various components of TensorFlow (see figure
    1.12). Chapters 3-10 show how to implement classic machine-learning algorithms
    in TensorFlow, and chapters 11-19 cover algorithms based on neural networks. The
    algorithms solve a wide variety of problems, such as prediction, classification,
    clustering, dimensionality reduction, and planning.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章演示了如何使用TensorFlow的各种组件（见图1.12）。第3-10章展示了如何在TensorFlow中实现经典机器学习算法，第11-19章涵盖了基于神经网络的算法。这些算法解决了广泛的问题，如预测、分类、聚类、降维和规划。
- en: '![CH01_F12_Mattmann2](../Images/CH01_F12_Mattmann2.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![CH01_F12_Mattmann2](../Images/CH01_F12_Mattmann2.png)'
- en: Figure 1.12 This chapter introduces fundamental machine-learning concepts, and
    chapter 2 begins your journey in TensorFlow. Other tools can apply machine-learning
    algorithms (such as Caffe, Theano, and Torch), but you’ll see in chapter 2 why
    TensorFlow is the way to go.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12 本章介绍了基本的机器学习概念，第2章开始了你在TensorFlow中的旅程。其他工具可以应用机器学习算法（如Caffe、Theano和Torch），但你在第2章中会看到为什么TensorFlow是最佳选择。
- en: Many algorithms can solve the same real-world problem, and many real-world problems
    can be solved by the same algorithm. Table 1.1 covers the ones laid out in this
    book.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法可以解决同一个实际世界问题，许多实际世界问题也可以由同一个算法解决。表1.1涵盖了本书中阐述的内容。
- en: Table 1.1 Many real-world problems can be solved by using the corresponding
    algorithm found in its respective chapter.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1 许多实际世界问题可以通过在其相应章节中找到的相应算法来解决。
- en: '| Real-world problem | Algorithm | Chapter(s) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 实际世界问题 | 算法 | 章节 |'
- en: '| Predicting trends, fitting a curve to data points, describing relationships
    between variables | Linear regression | 3, 4 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 预测趋势，将曲线拟合到数据点，描述变量之间的关系 | 线性回归 | 3, 4 |'
- en: '| Classifying data into two categories, finding the best way to split a dataset
    | Logistic regression | 5, 6 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 将数据分类为两个类别，找到分割数据集的最佳方式 | 逻辑回归 | 5, 6 |'
- en: '| Classifying data into multiple categories | Softmax regression | 5, 6 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 将数据分类到多个类别 | Softmax回归 | 5, 6 |'
- en: '| Revealing hidden causes of observations, finding the most likely hidden reason
    for a series of outcomes | Hidden Markov model (Viterbi) | 9, 10 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 揭示观察到的隐藏原因，找到一系列结果最可能的隐藏原因 | 隐藏马尔可夫模型（维特比） | 9, 10 |'
- en: '| Clustering data into a fixed number of categories, automatically partitioning
    data points into separate classes | k-means | 7, 8 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 将数据聚类到固定数量的类别，自动将数据点划分到不同的类别中 | k-means | 7, 8 |'
- en: '| Clustering data into arbitrary categories, visualizing high-dimensional data
    in a lower-dimensional embedding | Self-organizing map | 7, 8 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 将数据聚类到任意类别，在低维嵌入中可视化高维数据 | 自组织映射 | 7, 8 |'
- en: '| Reducing dimensionality of data, learning latent variables responsible for
    high-dimensional data | Autoencoder | 11, 12 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 降低数据的维度，学习导致高维数据的潜在变量 | 自动编码器 | 11, 12 |'
- en: '| Planning actions in an environment using neural networks (reinforcement learning)
    | Q-policy neural network | 13 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 使用神经网络在环境中规划动作（强化学习） | Q策略神经网络 | 13 |'
- en: '| Classifying data using supervised neural networks | Perceptron | 14, 15 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 使用监督神经网络对数据进行分类 | 感知器 | 14, 15 |'
- en: '| Classifying real-world images using supervised neural networks | Convolution
    neural network | 14, 15 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 使用监督神经网络对现实世界图像进行分类 | 卷积神经网络 | 14, 15 |'
- en: '| Producing patterns that match observations using neural networks | Recurrent
    neural network | 16, 17 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 使用神经网络产生与观察结果匹配的模型 | 循环神经网络 | 16, 17 |'
- en: '| Predicting natural-language responses to natural-language queries | Seq2seq
    model | 18 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 预测对自然语言查询的自然语言响应 | Seq2seq模型 | 18 |'
- en: '| Ranking items by learning their utility | Ranking | 19 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 通过学习它们的效用对项目进行排序 | 排序 | 19 |'
- en: TIP If you’re interested in the intricate architecture details of TensorFlow,
    the best available source is the official documentation at [https://www.tensorflow.org/
    tutorials/customization/basics](https://www.tensorflow.org/tutorials/customization/basics).
    This book sprints ahead and uses TensorFlow without slowing down for the breadth
    of low-level performance tuning. If you’re interested in cloud services, you may
    want to consider Google’s solution for professional-grade scale and speed ([https://cloud.google.com/products/ai](https://cloud.google.com/products/ai)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 如果你对TensorFlow的复杂架构细节感兴趣，最佳资源是官方文档中的[https://www.tensorflow.org/tutorials/customization/basics](https://www.tensorflow.org/tutorials/customization/basics)。本书迅速前进，在调整低级性能的同时，不减速地使用TensorFlow。如果你对云服务感兴趣，你可能想考虑谷歌针对专业级规模和速度的解决方案([https://cloud.google.com/products/ai](https://cloud.google.com/products/ai))。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow has become the tool of choice among professionals and researchers
    for implementing machine-learning solutions.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow已经成为专业人士和研究人员实现机器学习解决方案的首选工具。
- en: Machine learning uses examples to develop an expert system that can make useful
    statements about new inputs.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习使用示例来开发一个专家系统，该系统能对新的输入做出有用的陈述。
- en: A key property of ML is that performance tends to improve with more training
    data.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的一个关键特性是，性能往往随着更多训练数据的增加而提高。
- en: 'Over the years, scholars have crafted three major archetypes that most problems
    fit: supervised learning, unsupervised learning, and reinforcement learning. Meta-learning
    is a new area of ML that focuses on exploring the entire space of models, solutions,
    and tuning tricks automatically.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几年来，学者们已经构建了三个主要原型，大多数问题都符合这些原型：监督学习、无监督学习和强化学习。元学习是机器学习的一个新领域，它专注于自动探索整个模型、解决方案和调整技巧的空间。
- en: After a real-world problem is formulated in a machine-learning perspective,
    several algorithms become available. Of the many software libraries and frameworks
    that can accomplish an implementation, we chose TensorFlow as our silver bullet.
    Developed by Google and supported by its flourishing community, TensorFlow gives
    us a way to implement industry-standard code easily.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在从机器学习角度对现实世界问题进行表述后，会出现几种算法。在众多可以完成实现的软件库和框架中，我们选择了TensorFlow作为我们的银弹。由谷歌开发并得到其繁荣社区的支撑，TensorFlow为我们提供了一种轻松实现行业标准代码的方法。
