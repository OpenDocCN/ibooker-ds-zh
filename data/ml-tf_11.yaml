- en: 9 Hidden Markov models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 隐马尔可夫模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining interpretive models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义可解释模型
- en: Using Markov chains to model data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用马尔可夫链建模数据
- en: Inferring hidden state by using a hidden Markov model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用隐马尔可夫模型推断隐藏状态
- en: 'If a rocket blows up, someone’s probably going to get fired, so rocket scientists
    and engineers must be able to make confident decisions about all components and
    configurations. They do so by physical simulations and mathematical deduction
    from first principles. You, too, have solved science problems with pure logical
    thinking. Consider Boyle’s law: pressure and volume of a gas are inversely related
    under a fixed temperature. You can make insightful inferences from these simple
    laws about the world that have been discovered. Recently, machine learning has
    started to play the role of an important sidekick to deductive reasoning.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果火箭爆炸，可能有人会被解雇，因此火箭科学家和工程师必须能够对所有组件和配置做出自信的决定。他们通过物理模拟和从第一原理进行数学推导来实现这一点。您也通过纯粹的逻辑思维解决了科学问题。考虑波义耳定律：在固定温度下，气体的压力和体积成反比。您可以从这些简单的定律中得出关于已发现世界的深刻推论。最近，机器学习开始扮演演绎推理的重要配角。
- en: '*Rocket science* and *machine learning* aren’t phrases that usually appear
    together except unless you literally walk a week in my shoes. (Check out my author
    bio!) But nowadays, modeling real-world sensor readings by using intelligent data-driven
    algorithms is more approachable in the aerospace industry. Also, the use of machine-learning
    techniques is flourishing in the health care and automotive industries. But why?'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*火箭科学*和*机器学习*并不是通常一起出现的短语，除非你真的走过我的一周。 （查看我的作者简介！）但如今，在航空航天行业中，使用智能数据驱动算法对现实世界的传感器读数进行建模更加可行。此外，机器学习技术在医疗保健和汽车行业中也得到了蓬勃发展。但为什么？'
- en: This influx can be partly attributed to better understanding of *interpretable*
    models, which are machine-learning models in which the learned parameters have
    clear interpretations. If a rocket blows up, for example, an interpretable model
    might help trace the root cause.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这种涌入部分可以归因于对*可解释*模型更好的理解，这些模型是机器学习模型，其中学习的参数具有明确的解释。例如，如果火箭爆炸，一个可解释的模型可能有助于追踪根本原因。
- en: Exercise 9.1
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 练习9.1
- en: What makes a model interpretable may be slightly subjective. What are your criteria
    for an interpretable model?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使模型可解释的因素可能有些主观。您对可解释模型的评判标准是什么？
- en: '**Answers**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: We like to refer to mathematical proofs as the de facto explanation technique.
    If one were to convince another of the truth of a mathematical theorem, a proof
    that irrefutably traces the steps of reasoning is sufficient.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们喜欢将数学证明视为事实上的解释技术。如果有人要说服另一个人一个数学定理的真实性，一个无可辩驳地追踪推理步骤的证明就足够了。
- en: This chapter is about exposing the hidden explanations behind observations.
    Consider a puppet master pulling strings to make a puppet appear to be alive.
    Analyzing only the motions of the puppet might lead to overly complicated conclusions
    about how it’s possible for an inanimate object to move. After you notice the
    attached strings, you’ll realize that a puppet master is the best explanation
    for the lifelike motions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是关于揭示观察背后的隐藏解释。考虑一个木偶大师拉动绳子使木偶看起来像是有生命的。仅分析木偶的动作可能会导致过于复杂的结论，关于一个非生命物体如何移动。在你注意到连接的绳子后，你会意识到木偶大师是解释逼真动作的最佳解释。
- en: On that note, this chapter introduces *hidden Markov models* (HMMs), which reveal
    intuitive properties about the problem under study. The HMM is the “puppet master,”
    which explains the observations. You model observations by using Markov chains,
    which are described in section 9.2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个话题上，本章介绍了*隐马尔可夫模型*（HMMs），它揭示了关于研究问题的直观属性。HMM是“木偶大师”，它解释了观察结果。您通过使用第9.2节中描述的马尔可夫链来建模观察结果。
- en: Before reading in detail about Markov chains and HMMs, consider alternative
    models. In section 9.1, you’ll see models that may not be interpretable.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细阅读马尔可夫链和HMMs之前，考虑替代模型。在第9.1节中，你会看到可能不可解释的模型。
- en: 9.1 Example of a not-so-interpretable model
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 一个不太可解释的模型示例
- en: 'One classic example of a black-box machine-learning algorithm that’s difficult
    to interpret is image classification. In an image-classification task, the goal
    is to assign a label to each input image. More simply, image classification is
    often posed as a multiple-choice question: which one of the listed categories
    best describes the image? Machine- learning practitioners have made tremendous
    advancements in solving this problem, to the point where today’s best image classifiers
    match human-level performance on certain datasets.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒机器学习算法的一个经典例子是图像分类，这种算法难以解释。在图像分类任务中，目标是给每个输入图像分配一个标签。更简单地说，图像分类通常被表述为一个多项选择题：列出的类别中哪一个最能描述图像？机器学习从业者在这方面的进步巨大，以至于今天的最佳图像分类器在某些数据集上与人类的表现相当。
- en: 'In chapter 14, you’ll learn how to solve the problem of classifying images
    by using convolutional neural networks (CNNs), a class of machine-learning models
    that end up learning a lot of parameters. But those parameters are the problem
    with CNNs: what do each of the thousands, if not millions, of parameters mean?
    It’s difficult to ask an image classifier why it made the decision that it did.
    All we have available are the learned parameters, which may not easily explain
    the reasoning behind the classification.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在第14章中，你将学习如何使用卷积神经网络（CNNs）来解决问题，这是一种学习大量参数的机器学习模型。但那些参数正是CNN的问题：如果不说成千上万，至少是数百万个参数分别代表什么意思？很难询问图像分类器它为什么会做出这样的决定。我们所能拥有的只是学习到的参数，这些参数可能无法轻易解释分类背后的推理。
- en: Machine learning sometimes gets the reputation of being a black-box tool that
    solves a specific problem without revealing how it arrives at its conclusion.
    The purpose of this chapter is to unveil an area of machine learning with an interpretable
    model. Specifically, you’ll learn about the HMM and use TensorFlow to implement
    it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习有时会获得一个声誉，即它是一个黑盒工具，可以解决特定问题而不透露其结论是如何得出的。本章的目的是揭示机器学习中的一个具有可解释模型的领域。具体来说，你将了解HMM（隐马尔可夫模型）并使用TensorFlow来实现它。
- en: 9.2 Markov model
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 马尔可夫模型
- en: Andrey Markov was a Russian mathematician who studied the ways that systems
    change over time in the presence of randomness. Imagine gas particles bouncing
    around in the air. Tracking the position of each particle with Newtonian physics
    can get way too complicated, so introducing randomness helps simplify the physical
    model a little.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 安德烈·马尔可夫是一位俄罗斯数学家，他研究了在随机性的存在下系统随时间变化的方式。想象一下气体粒子在空气中弹跳。用牛顿物理学跟踪每个粒子的位置会变得非常复杂，因此引入随机性有助于简化物理模型。
- en: Markov realized that what helps simplify a random system even further is considering
    only a limited area around the gas particle to model it. Maybe a gas particle
    in Europe has barely any effect on a particle in the United States. So why not
    ignore it? The mathematics is simplified when you look only at a nearby neighborhood
    instead of the entire system. This notion is referred to as the *Markov property*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫意识到，进一步简化随机系统的是只考虑气体粒子周围的有限区域来对其进行建模。也许欧洲的气体粒子对美国的粒子几乎没有影响。那么为什么不去忽略它呢？当你只观察附近的邻域而不是整个系统时，数学就简化了。这种概念被称为*马尔可夫性质*。
- en: Consider modeling the weather. Meteorologists evaluate various conditions with
    thermometers, barometers, and anemometers to help predict the weather. They draw
    on brilliant insight and years of experience to do their job.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑对天气进行建模。气象学家使用温度计、气压计和风速计等工具评估各种条件，以帮助预测天气。他们凭借卓越的洞察力和多年的经验来完成他们的工作。
- en: 'Let’s use the Markov property to get started with a simple model. First, you
    identify the possible situations, or *states*, that you care to study. Figure
    9.1 shows three weather states as nodes in a graph: Cloudy, Rainy, and Sunny.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用马尔可夫性质从一个简单的模型开始。首先，你确定你想要研究的可能情况，或者说*状态*。图9.1显示了三个天气状态作为图中的节点：Cloudy（多云）、Rainy（雨天）和Sunny（晴天）。
- en: '![CH09_F01_Mattmann2](../Images/CH09_F01_Mattmann2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_Mattmann2](../Images/CH09_F01_Mattmann2.png)'
- en: Figure 9.1 Weather conditions (states) represented as nodes in a graph
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 以节点形式表示的天气条件（状态）
- en: Now that you have the states, you want to define how one state transforms into
    another. Modeling weather as a deterministic system is difficult. The conclusion
    that if it’s sunny today, it’ll certainly be sunny tomorrow isn’t obvious. Instead,
    you can introduce randomness and say that if it’s sunny today, there’s a 90% chance
    that it’ll be sunny tomorrow and a 10% chance that it’ll be cloudy. The Markov
    property comes into play when you use only today’s weather condition to predict
    tomorrow’s (instead of using history).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了状态，你想要定义一个状态如何转换到另一个状态。将天气建模为确定性系统是困难的。如果今天是晴天，明天肯定也是晴天的结论并不明显。相反，你可以引入随机性，并说如果今天是晴天，有90%的可能性明天也是晴天，有10%的可能性是多云。当你只使用今天的天气条件来预测明天的天气（而不是使用历史数据）时，马尔可夫性质就发挥作用了。
- en: Exercise 9.2
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 练习9.2
- en: A robot that decides which action to perform based on only its current state
    is said to follow the Markov property. What are the advantages and disadvantages
    of such a decision-making process?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个仅基于其当前状态来决定采取何种行动的机器人被认为是遵循马尔可夫性质。这种决策过程的优点和缺点是什么？
- en: '**Answers**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: The Markov property is computationally easy to work with, but Markov models
    aren’t able to generalize to situations that require accumulating a history of
    knowledge. Examples are models in which a trend over time is important or knowledge
    of more than one past state gives a better idea of what to expect next.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫性质在计算上很容易处理，但马尔可夫模型无法推广到需要积累知识历史的情况。例如，在时间趋势很重要或了解多个过去状态能更好地预测下一个期望的情况中的模型。
- en: Figure 9.2 demonstrates the transitions as directed edges drawn between nodes,
    with the arrow pointing toward the next future state. Each edge has a weight representing
    the probability (such as a 30% chance that if today is rainy, tomorrow will be
    cloudy). The lack of an edge between two nodes is an elegant way of showing that
    the probability of that transformation is near zero. The transition probabilities
    can be learned from historical data, but for now, let’s assume that they’re given
    to us.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2展示了节点之间绘制的有向边，箭头指向下一个未来状态。每条边都有一个权重，表示概率（例如，如果今天是雨天，明天多云的概率为30%）。两个节点之间没有边是一种优雅地表示那种转换概率几乎为零的方式。转移概率可以从历史数据中学习，但在此我们假设它们是给定的。
- en: '![CH09_F02_Mattmann2](../Images/CH09_F02_Mattmann2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F02_Mattmann2](../Images/CH09_F02_Mattmann2.png)'
- en: Figure 9.2 Transition probabilities between weather conditions are represented
    as directed edges.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2展示了不同天气条件之间的转移概率，用有向边表示。
- en: If you have three states, you can represent the transitions as a 3 × 3 matrix.
    Each element of the matrix (at row *i* and column *j* ) corresponds to the probability
    associated with the edge from node *i* to node *j*. In general, if you have *N*
    states, the *transition matrix* will be *N* × *N* in size; see figure 9.4 for
    an example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有三个状态，你可以用3×3矩阵来表示转移。矩阵中的每个元素（在第*i*行和第*j*列）对应于从节点*i*到节点*j*的边的概率。一般来说，如果你有*N*个状态，*转移矩阵*的大小将是*N*×*N*；参见图9.4的示例。
- en: We call this system a *Markov model*. Over time, a state changes using the transition
    probabilities defined in figure 9.2\. In our example, Sunny has a 90% chance of
    Sunny again tomorrow, so we show an edge of probability 0.9, looping back to itself.
    There’s a 10% chance of a sunny day being followed by a cloudy day, shown in the
    diagram as the edge 0.1, pointing from Sunny to Cloudy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这个系统为*马尔可夫模型*。随着时间的推移，状态会根据图9.2中定义的转移概率发生变化。在我们的例子中，晴天有90%的可能性再次成为晴天，所以我们显示一个概率为0.9的边，回到自身。有10%的可能性是晴天之后是多云，在图中表示为从晴天到多云的边0.1。
- en: Figure 9.3 is another way to visualize how the states change, given the transition
    probabilities. This depiction is often called a *trellis diagram*, which turns
    out to be an essential tool, as you’ll see later when we implement the TensorFlow
    algorithms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3是另一种可视化状态变化的方法，给出了状态转移概率。这种表示通常被称为*梯形图*，它最终证明是一个重要的工具，正如你将在我们实现TensorFlow算法时看到的。
- en: '![CH09_F03_Mattmann2](../Images/CH09_F03_Mattmann2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F03_Mattmann2](../Images/CH09_F03_Mattmann2.png)'
- en: Figure 9.3 A trellis representation of the Markov system changing states over
    time
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3展示了马尔可夫系统随时间变化的梯形表示
- en: You’ve seen how TensorFlow code builds a graph to represent computation. It
    may be tempting to treat each node in a Markov model as a node in TensorFlow.
    But even though figures 9.2 and 9.3 nicely illustrate state transitions, figure
    9.4 shows a more efficient way to implement them in code.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了如何使用TensorFlow代码构建图来表示计算。你可能想将马尔可夫模型中的每个节点视为TensorFlow中的一个节点。但尽管图9.2和9.3很好地说明了状态转换，图9.4展示了在代码中实现它们的更有效方法。
- en: '![CH09_F04_Mattmann2](../Images/CH09_F04_Mattmann2.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Mattmann2](../Images/CH09_F04_Mattmann2.png)'
- en: Figure 9.4 A transition matrix conveys the probabilities of a state from the
    left (rows) transitioning to a state at the top (columns).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 一个转换矩阵传达了从左侧（行）到顶部（列）状态转换的概率。
- en: Remember that nodes in a TensorFlow graph are tensors, so you can represent
    a transition matrix (let’s call it *T* ) as a node in TensorFlow. Then you can
    apply mathematical operations on the TensorFlow node to achieve interesting results.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，TensorFlow图中的节点是张量，因此你可以将转换矩阵（让我们称其为 *T*）表示为TensorFlow中的一个节点。然后，你可以在TensorFlow节点上应用数学运算以实现有趣的结果。
- en: Suppose that you prefer sunny days over rainy ones, so you have a score associated
    with each day. You represent your scores for each state in a 3 × 1 matrix called
    *s*. Then multiplying the two matrices in TensorFlow with `tf.matmul(T*s``)` gives
    the expected preference of transitioning from each state.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你更喜欢晴天而不是雨天，因此你为每一天都有一个分数。你将每个状态的分数表示在一个名为 *s* 的3 × 1矩阵中。然后，使用 `tf.matmul(T*s)`
    在TensorFlow中将这两个矩阵相乘，可以得到从每个状态转换的预期偏好。
- en: Representing a scenario in a Markov model allows you to simplify greatly how
    you view the world. But frequently, it’s difficult to measure the state of the
    world directly. Often, you have to use evidence from multiple observations to
    figure out the hidden meaning. That problem is what section 9.3 aims to solve.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫模型中表示一个场景可以极大地简化你对世界的看法。但通常，直接测量世界的状态很困难。通常，你必须使用来自多个观测的证据来弄清楚隐藏的意义。这正是9.3节旨在解决的问题。
- en: 9.3 Hidden Markov model
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 隐藏马尔可夫模型
- en: The Markov model defined in section 9.2 is convenient when all the states are
    observable, but that’s not always the case. Consider having access to only the
    temperature readings of a town. Temperature isn’t weather, but it’s related. How,
    then, can you infer the weather from this indirect set of measurements?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在9.2节中定义的马尔可夫模型在所有状态都是可观测时很方便，但情况并不总是如此。考虑只有获取一个城镇的温度读数。温度不是天气，但它与之相关。那么，你如何从这些间接的测量数据中推断出天气呢？
- en: Rainy weather most likely causes a lower temperature reading, whereas a sunny
    day most likely causes a higher temperature reading. With temperature knowledge
    and transition probabilities alone, you can still make intelligent inferences
    about the most likely weather.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 雨天天气很可能会造成温度读数较低，而晴天则很可能会造成温度读数较高。仅凭温度知识和转换概率，你仍然可以做出关于最可能天气的智能推断。
- en: Problems like this one are common in the real world. A state might leave traces
    of hints behind, and those hints are all you have available.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的问题在现实世界中很常见。一个状态可能会留下一些线索，而这些线索就是你所能拥有的全部。
- en: Models like these are HMMs because the true states of the world (such as whether
    it’s raining or sunny) aren’t directly observable. These hidden states follow
    a Markov model, and each state emits a measurable observation with a certain likelihood.
    The hidden state of Sunny, for example, might usually emit high temperature readings,
    but occasionally also emits low readings for one reason or another.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的模型是HMM，因为世界的真实状态（例如是否在下雨或晴天）是不可直接观测的。这些隐藏状态遵循马尔可夫模型，并且每个状态都会以一定的可能性发出可测量的观测值。例如，晴朗的隐藏状态通常可能会发出高温读数，但偶尔也可能因为某种原因发出低温读数。
- en: In an HMM, you have to define the emission probability, which is usually represented
    as a matrix called the *emission matrix*. The number of rows in the matrix is
    the number of states (Sunny, Cloudy, Rainy), and the number of columns is the
    number of observation types (Hot, Mild, Cold). Each element of the matrix is the
    probability associated with the emission.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在HMM中，你必须定义发射概率，这通常用一个称为 *发射矩阵* 的矩阵表示。矩阵的行数是状态的数量（晴朗、多云、雨天），列数是观测类型的数量（热、温和、冷）。矩阵中的每个元素都与发射相关的概率。
- en: The canonical way to visualize an HMM is to append observations to the trellis,
    as shown in figure 9.5.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化HMM的典型方法是将观测值附加到梯形图上，如图9.5所示。
- en: '![CH09_F05_Mattmann2](../Images/CH09_F05_Mattmann2.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F05_Mattmann2](../Images/CH09_F05_Mattmann2.png)'
- en: Figure 9.5 An HMM trellis showing how weather conditions might produce temperature
    readings
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 一个 HMM 框图，展示了天气条件可能产生的温度读数
- en: 'So that’s *almost* it. The HMM is a description of transition probabilities,
    emission probabilities, and one more thing: initial probabilities. The *initial
    probability* is the probability that each state will happen without the model’s
    previous knowledge. If you’re modeling the weather in Los Angeles, perhaps the
    initial probability of Sunny would be much greater. Or let’s say you’re modeling
    the weather in Seattle; you know that you can set the initial probability of Rainy
    to something higher.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，几乎就是这样。HMM 是对转移概率、发射概率以及另一件事的描述：初始概率。*初始概率* 是在模型没有先前知识的情况下每个状态发生的概率。如果你正在模拟洛杉矶的天气，那么晴朗的初始概率可能要大得多。或者让我们假设你正在模拟西雅图的天气；你知道可以将雨天的初始概率设置得更高。
- en: An HMM lets you understand a sequence of observations. In this weather-modeling
    scenario, you may ask about the probability of observing a certain sequence of
    temperature readings. I’ll answer this question with the forward algorithm.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: HMM 允许你理解一系列观察结果。在这个天气建模场景中，你可能想知道观察特定温度读数序列的概率。我将使用前向算法来回答这个问题。
- en: 9.4 Forward algorithm
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 前向算法
- en: The *forward algorithm* computes the probability of an observation. Many permutations
    may cause a particular observation, so enumerating all possibilities the naïve
    way will take an exponentially long time to compute.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*前向算法*计算观察到的概率。许多排列可能导致特定的观察结果，所以以天真方式枚举所有可能性将需要指数级的时间来计算。'
- en: Instead, you can solve the problem by using *dynamic programming*, a strategy
    of breaking a complex problem into simple little ones and using a lookup table
    to cache the results. In your code, you’ll save the lookup table as a NumPy array
    and feed it to a TensorFlow op to keep updating it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以通过使用 *动态规划* 来解决这个问题，这是一种将复杂问题分解成简单的小问题并使用查找表来缓存结果的策略。在你的代码中，你将查找表保存为 NumPy
    数组，并将其馈送到 TensorFlow 操作以保持更新。
- en: As shown in listing 9.1, create an `HMM` class to capture the HMM parameters,
    which include the initial probability vector, transition probability matrix, and
    emission probability matrix.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表 9.1 所示，创建一个 `HMM` 类来捕获 HMM 参数，包括初始概率向量、转移概率矩阵和发射概率矩阵。
- en: Listing 9.1 Defining the `H M M` class
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1 定义 `HMM` 类
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports the required libraries
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入所需的库
- en: ❷ Stores the parameters as method variables
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将参数存储为方法变量
- en: ❸ Double-checks that the shapes of all the matrices make sense
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 双重检查所有矩阵的形状是否合理
- en: ❹ Defines the placeholders used for the forward algorithm
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义前向算法使用的占位符
- en: Next, you’ll define a quick helper function to access a row from the emission
    matrix. The code in listing 9.2 is a helper function that obtains data efficiently
    from an arbitrary matrix. The `slice` function extracts a fraction of the original
    tensor. This function requires as input the relevant tensor, the starting location
    specified by a tensor, and the size of the slice specified by a tensor.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将定义一个快速辅助函数来访问发射矩阵中的一行。列表 9.2 中的代码是一个辅助函数，它能够有效地从任意矩阵中获取数据。`slice` 函数从原始张量中提取一部分。此函数需要输入相关的张量、由张量指定的起始位置以及由张量指定的切片大小。
- en: Listing 9.2 Creating a helper function to access an observation’s emission probability
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.2 创建一个辅助函数以访问观察的发射概率
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Where to slice the emission matrix
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如何切割发射矩阵
- en: ❷ The shape of the slice
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 切片的形状
- en: ❸ Performs the slicing operation
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行切片操作
- en: You need to define two TensorFlow ops. The first one, in listing 9.3, will be
    run only once to initialize the forward algorithm’s cache.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要定义两个 TensorFlow 操作。第一个，在列表 9.3 中，将只运行一次以初始化前向算法的缓存。
- en: Listing 9.3 Initializing the cache
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.3 初始化缓存
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The next op updates the cache at each observation, as shown in listing 9.4\.
    Running this code is often called *executing a forward step*. Although it looks
    as though this `forward_op` function takes no input, it depends on placeholder
    variables that need to be fed to the session. Specifically, `self.fwd` and `self.obs_idx`
    are the inputs to this function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个操作在每次观察时更新缓存，如列表 9.4 所示。运行此代码通常被称为 *执行前向步骤*。尽管看起来这个 `forward_op` 函数没有输入，但它依赖于需要馈送到会话中的占位符变量。具体来说，`self.fwd`
    和 `self.obs_idx` 是此函数的输入。
- en: Listing 9.4 Updating the cache
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 更新缓存
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Outside the `HMM` class, let’s define a function to run the forward algorithm,
    as shown in listing 9.5\. The forward algorithm runs the forward step for each
    observation. In the end, it outputs a probability of observations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`HMM`类之外，让我们定义一个函数来运行前向算法，如图9.5所示。前向算法为每个观察运行前向步骤。最后，它输出观察的概率。
- en: Listing 9.5 Defining the forward algorithm, given an `H M M`
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.5 定义前向算法，给定`HMM`
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the main function, let’s set up the `HMM` class by feeding it the initial
    probability vector, transition probability matrix, and emission probability matrix.
    For consistency, the example in listing 9.6 is lifted directly from the Wikipedia
    article on HMMs at [http://mng.bz/8ztL](http://mng.bz/8ztL), as shown in figure
    9.6.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在主函数中，让我们通过提供初始概率向量、转移概率矩阵和发射概率矩阵来设置`HMM`类。为了保持一致性，列表9.6中的示例直接从维基百科上的HMM文章[http://mng.bz/8ztL](http://mng.bz/8ztL)中提取，如图9.6所示。
- en: '![CH09_F06_Mattmann2](../Images/CH09_F06_Mattmann2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F06_Mattmann2](../Images/CH09_F06_Mattmann2.png)'
- en: Figure 9.6 Screenshot of HMM example scenario
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 HMM示例场景截图
- en: 'In general, the three concepts are defined as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这三个概念定义如下：
- en: '*Initial probability vector* —Starting probability of the states'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*初始概率向量* —状态开始的概率'
- en: '*Transition probability matrix* —Probabilities associated with landing on the
    next states, given the current state'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转移概率矩阵* —给定当前状态，到达下一个状态的概率'
- en: '*Emission probability matrix* —Likelihood of an observed state implying the
    state you’re interested in has occurred'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*发射概率矩阵* —观察到的状态暗示你感兴趣的状态已经发生的可能性'
- en: Given these matrices, you’ll call the forward algorithm that you’ve defined
    (listing 9.6).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些矩阵，您将调用您定义的前向算法（列表9.6）。
- en: Listing 9.6 Defining the `H M M` and calling the forward algorithm
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 定义`HMM`和调用前向算法
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When you run listing 9.6, the algorithm outputs the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行列表9.6时，算法输出以下内容：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 9.5 Viterbi decoding
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 维特比解码
- en: The *Viterbi decoding algorithm* finds the most likely sequence of hidden states,
    given a sequence of observations. It requires a caching scheme similar to the
    forward algorithm. You’ll name the cache `viterbi`. In the HMM constructor, append
    the line shown in listing 9.7.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*维特比解码算法*在给定观察序列的情况下找到最可能的隐藏状态序列。它需要一个类似于前向算法的缓存方案。您将命名缓存为`viterbi`。在HMM构造函数中，添加列表9.7中显示的行。'
- en: Listing 9.7 Adding the Viterbi cache as a member variable
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 将维特比缓存作为成员变量添加
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In listing 9.8, you’ll define a TensorFlow op to update the `viterbi` cache.
    This op will be a method in the `HMM` class.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表9.8中，您将定义一个TensorFlow操作来更新`viterbi`缓存。这个操作将是`HMM`类中的一个方法。
- en: Listing 9.8 Defining an op to update the forward cache
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 定义一个操作来更新前向缓存
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You’ll also need an op to update the back pointers (listing 9.9).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要一个操作来更新回溯指针（列表9.9）。
- en: Listing 9.9 Defining an op to update the back pointers
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.9 定义一个操作来更新回溯指针
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Finally, in listing 9.10, define the Viterbi decoding function outside the HMM.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在列表9.10中，定义维特比解码函数，位于HMM之外。
- en: Listing 9.10 Defining the Viterbi decoding algorithm
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.10 定义维特比解码算法
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can run the code in listing 9.11 in the main function to evaluate the Viterbi
    decoding of an observation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在主函数中运行列表9.11中的代码来评估观察的维特比解码。
- en: Listing 9.11 Running the Viterbi decode
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 运行维特比解码
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 9.6 Uses of HMMs
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 HMM的应用
- en: Now that you’ve implemented the forward algorithm and Viterbi algorithm, let’s
    take a look at interesting uses for your newfound power.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经实现了前向算法和维特比算法，让我们来看看您新获得的力量的一些有趣用途。
- en: 9.6.1 Modeling a video
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.1 模型视频
- en: Imagine being able to recognize a person based solely (no pun intended) on how
    they walk. Identifying people based on their gait is a pretty cool idea, but first,
    you need a model to recognize the gait. Consider an HMM in which the sequence
    of hidden states for a gait are (1) rest position, (2) right foot forward, (3)
    rest position, (4) left foot forward, and (5) rest position. The observed states
    are silhouettes of a person walking/jogging/running taken from a video clip. (A
    dataset of such examples is available at [http://mng.bz/Tqfx.](http://mng.bz/Tqfx))
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下仅根据一个人走路的方式（不是字面上的意思）来识别这个人。根据人的步态来识别人是一个相当酷的想法，但首先，你需要一个模型来识别步态。考虑一个HMM，其中步态的隐藏状态序列为（1）休息位置，（2）右脚向前，（3）休息位置，（4）左脚向前，以及（5）休息位置。观察到的状态是从视频剪辑中提取的人行走/慢跑/跑步的轮廓。（此类示例的数据集可在[http://mng.bz/Tqfx.](http://mng.bz/Tqfx)找到）
- en: 9.6.2 Modeling DNA
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.2 模型DNA
- en: DNA is a sequence of nucleotides, and we’re gradually learning more about its
    structure. One clever way to understand a long DNA string is to model the regions,
    if you know some probability about the order in which they appear. As a cloudy
    day is common after a rainy day, maybe a certain region on the DNA sequence (*start
    codon*) is more common before another region (*stop codon*).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: DNA是一系列核苷酸，我们正在逐渐了解其结构。理解长DNA字符串的一个巧妙方法是对区域进行建模，如果你知道它们出现的概率顺序。就像雨天之后通常是多云的一天，也许DNA序列上的某个区域（*起始密码子*）在另一个区域（*终止密码子*）之前更常见。
- en: 9.6.3 Modeling an image
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.3 建模图像
- en: In handwriting recognition, we aim to retrieve the plaintext from an image of
    handwritten words. One approach is to resolve characters one at a time and then
    concatenate the results. You can use the insight that characters are written in
    sequences—words—to build an HMM. Knowing the previous character probably could
    help you rule out possibilities of the next character. The hidden states are the
    plaintext, and the observations are cropped images containing individual characters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在手写识别中，我们的目标是从一个手写单词的图像中检索明文。一种方法是一次性解析字符，然后将结果连接起来。你可以利用字符是按序列书写的——单词——来构建一个HMM。知道前一个字符可能有助于你排除下一个字符的可能性。隐藏状态是明文，观察结果是包含单个字符的裁剪图像。
- en: 9.7 Application of HMMs
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 HMM的应用
- en: 'HMMs work best when you have an idea about what the hidden states are and how
    they change over time. Luckily, in the field of natural language processing, tagging
    a sentence’s parts of speech can be solved with HMMs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当你知道隐藏状态是什么以及它们如何随时间变化时，HMM工作得最好。幸运的是，在自然语言处理领域，使用HMM来标记句子的词性是可以解决的：
- en: A sequence of words in a sentence corresponds to the observations of the HMM.
    The sentence “Open the pod bay doors, HAL,” for example, has six observed words.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子中的单词序列对应于HMM的观察结果。例如，句子“打开pod舱门，HAL”有六个观察到的单词。
- en: 'The hidden states are the parts of speech: verb, noun, adjective, and so on.
    The observed word *open* in the preceding example should correspond to the hidden
    state *verb*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏状态是词性：动词、名词、形容词等等。前例中观察到的单词*open*应该对应于隐藏状态*verb*。
- en: The transition probabilities can be designed by the programmer or obtained through
    data. These probabilities represent the rules of the parts of speech. The probability
    that two verbs will occur one after another should be low, for example. By setting
    up a transition probability, you avoid having the algorithm brute-forcing all
    possibilities.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换概率可以由程序员设计或通过数据获得。这些概率代表了词性的规则。例如，两个动词连续出现的概率应该是低的。通过设置转换概率，你可以避免算法穷举所有可能性。
- en: The emitting probabilities of each word can be obtained from data. A traditional
    part-of-speech tagging dataset is called Moby; you can find it at [www.gutenberg.org/ebooks/3203](http://www.gutenberg.org/ebooks/3203).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词的发射概率可以从数据中获得。一个传统的词性标注数据集被称为Moby；你可以在[www.gutenberg.org/ebooks/3203](http://www.gutenberg.org/ebooks/3203)找到它。
- en: Note You now have what it takes to design your own experiments with HMMs. These
    models are powerful tools, and I urge you to try them on your own data. Predefine
    some transitions and emissions, and see whether you can recover hidden states.
    I hope that this chapter can get you started.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你现在已经有能力设计自己的HMM实验了。这些模型是强大的工具，我敦促你尝试在自己的数据上使用它们。预先定义一些转换和发射，看看你是否能恢复隐藏状态。我希望这一章能帮助你开始。
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A complicated, entangled system can be simplified with a Markov model.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个复杂、纠缠的系统可以用马尔可夫模型简化。
- en: HMMs are particularly useful in real-world applications because most observations
    are measurements of hidden states.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HMM在现实世界应用中特别有用，因为大多数观察都是对隐藏状态的测量。
- en: The forward and Viterbi algorithms are among the most common algorithms used
    on HMMs.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向算法和维特比算法是用于HMM的最常见算法之一。
