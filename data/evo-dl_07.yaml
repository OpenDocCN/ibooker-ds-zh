- en: 5 Automating hyperparameter optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 自动化超参数优化
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Developing a process to manually optimize hyperparameters for DL networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发一个手动优化深度学习网络超参数的过程
- en: Building automatic hyperparameter optimization with random search
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机搜索构建自动超参数优化
- en: Formalizing automatic HPO by employing a grid search algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用网格搜索算法形式化自动HPO
- en: Applying evolutionary computation to HPO using PSO
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PSO将进化计算应用于HPO
- en: Extending evolutionary HPO by using evolutionary strategies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用进化策略扩展进化超参数优化
- en: Applying DE to HPO
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将DE应用于HPO
- en: In the past few chapters, we have been exploring various forms of evolutionary
    computation, from genetic algorithms to particle swarm optimization and even advanced
    methods like evolutionary strategies and differential evolution. We continue to
    use all these EC methods through the rest of the book in some capacity to improve
    on DL. We combine these methods into a process we colloquially call *evolutionary
    deep learning* (EDL).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几章中，我们一直在探索各种进化计算形式，从遗传算法到粒子群优化，甚至更高级的方法如进化策略和差分进化。我们将在本书的其余部分以某种形式继续使用所有这些EC方法来改进深度学习。我们将这些方法结合成一个我们俗称为“进化深度学习”（EDL）的过程。
- en: However, before building a set of EDL solutions to various DL problems, we would
    be remiss if we didn’t understand the problems we are trying to solve and how
    they are solved without EC. After all, EC tools are just a few in a grand `toolbox`
    we can use to improve DL. Therefore, before we get into applying EC methods to
    HPO, we first look at the importance of hyperparameter optimization and some manual
    strategies. Second, when considering automated HPO we want to create a baseline
    by first reviewing other search methods, such as random and grid search.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在构建一系列针对各种深度学习问题的EDL解决方案之前，如果我们不了解我们试图解决的问题以及它们在没有EC的情况下是如何解决的，那么我们将犯下错误。毕竟，EC工具只是我们可以用来改进深度学习的大量“工具箱”中的一小部分。因此，在我们开始应用EC方法到HPO之前，我们首先探讨超参数优化的重要性以及一些手动策略。其次，在考虑自动HPO时，我们希望通过首先回顾其他搜索方法，如随机搜索和网格搜索，来建立一个基线。
- en: 5.1 Option selection and hyperparameter tuning
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 选项选择和超参数调整
- en: One of the most difficult problems practitioners of DL face is determining which
    options and “knobs” to dial in to improve their models. Most texts dedicated to
    teaching DL address the many options and hyperparameters but rarely detail the
    effects of changes. This is compounded by an AI/ML community showcasing state-of-the-art
    models that often omit the vast amount of work needed to attain them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习实践者面临的最困难问题之一是确定哪些选项和“旋钮”需要调整以改进他们的模型。大多数致力于教授深度学习的文本都涉及众多选项和超参数，但很少详细说明变化的影响。这种情况被一个展示最先进模型的AI/ML社区所加剧，这些模型往往省略了达到这些模型所需的大量工作。
- en: For most practitioners, learning how to use the many options and tuning hyperparameters
    comes from hours of experience building models. Without this tuning, many such
    models, as was demonstrated in the last section, could be seriously flawed. This
    becomes a problem not only for newcomers but the field of DL itself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数实践者来说，学习如何使用众多选项和调整超参数主要来自于构建模型时的数小时经验。没有这种调整，正如上一节所展示的，许多此类模型可能会存在严重缺陷。这不仅是一个新手的难题，也是深度学习领域本身的问题。
- en: We start by looking at a base deep learning model that uses PyTorch to approximate
    a function. Later examples in this book use Keras and/or PyTorch to demonstrate
    how easily these techniques can be swapped between frameworks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先研究一个使用PyTorch来近似函数的基本深度学习模型。本书后面的例子将使用Keras和/或PyTorch来展示这些技术如何在框架之间轻松互换。
- en: 5.1.1 Tuning hyperparameter strategies
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 超参数调整策略
- en: In this section, we look at some techniques and strategies to select options
    and tune hyperparameters for DL models. Some of these have been gleaned from years
    of experience, but realize such strategies will need to evolve. DL is constantly
    growing, and new model options are continually being enlisted.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨一些技术和策略来选择选项和调整深度学习模型的超参数。其中一些是从多年经验中获得的，但意识到这些策略需要发展。深度学习不断增长，新的模型选项也在不断被采用。
- en: Deep learning knowledge
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习知识
- en: This book assumes you understand basic DL principles of things like the perceptron,
    multi-layer perceptron, activation functions, and optimization methods. If you
    feel you need a refresher on DL in general, there are plenty of good resources
    online and published by Manning Publications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设你理解基本深度学习原理，例如感知器、多层感知器、激活函数和优化方法。如果你觉得需要复习一下深度学习的基础，网上和Manning Publications出版的资源有很多。
- en: A few key differences have been added to demonstrate working with hyperparameters
    and other options for the following exercise. Open the EDL_5_1_Hyperparameter_
    Tuning.ipynb notebook in your browser. Consult the appendix if you need assistance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了一些关键差异以展示如何处理超参数和其他选项，以供以下练习使用。在浏览器中打开EDL_5_1_Hyperparameter_Tuning.ipynb笔记本。如果需要帮助，请查阅附录。
- en: Start by running the whole notebook from the menu with Run > Run All. Confirm
    the output is like figure 5.1 for the initial function and predicted solution.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用菜单中的“运行”>“运行所有”来运行整个笔记本。确认输出类似于图5.1中的初始函数和预测解。
- en: '![](../Images/CH05_F01_Lanham.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F01_Lanham.png)'
- en: Figure 5.1 Data points and the solution target function
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 数据点和目标函数的解
- en: 'Next, scroll down, and look at the hyperparameters code block, as shown in
    listing 5.1\. There are two new hyperparameters added here: `batch_size` and `data_step`.
    The first hyperparameter, `batch-size`, determines the number of inputs we feed
    into the network for each forward pass. Recall that in the last exercise, this
    was `1`. The other hyperparameter, `data_step`, is not a typical hyperparameter
    but allows us to control the amount of data generated for training.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，向下滚动，查看如图5.1所示的超参数代码块。这里添加了两个新的超参数：`batch_size`和`data_step`。第一个超参数`batch-size`决定了每次前向传递我们向网络输入的输入数量。回想一下，在上一个练习中，这个值是`1`。另一个超参数`data_step`不是一个典型的超参数，但它允许我们控制为训练生成数据的数量。
- en: 'Listing 5.1 EDL_5_1_Hyperparameter_Tuning.ipynb: Hyperparameters'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1 EDL_5_1_Hyperparameter_Tuning.ipynb：超参数
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Forms the parameter for setting the name of the test
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 形成设置测试名称的参数
- en: ❷ The number of elements to feed in a single forward pass
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 单次前向传递中要输入的元素数量
- en: ❸ Controls the frequency of data samples for data generation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 控制数据生成中数据样本的频率
- en: Change the test name `hp_test` to something like `test 2`. Then modify the `middle_
    layer` value to something like `25` or larger. Run the cell, and then run the
    remaining cells in the notebook with Run > Run After.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 将测试名称`hp_test`更改为类似`test 2`的内容。然后修改`middle_layer`的值到`25`或更大。运行该单元格，然后使用“运行”>“运行后续单元格”运行笔记本中的剩余单元格。
- en: The predicted output is shown for both tests in figure 5.2, which also shows
    the output of `test 2` fitting much better. Notice the slight difference in time
    of training the model as well. This difference comes from larger models taking
    more time to train.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2显示了两个测试的预测输出，其中也显示了`test 2`的输出拟合得更好。注意训练模型的时间略有差异。这种差异来自更大的模型需要更多时间来训练。
- en: '![](../Images/CH05_F02_Lanham.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F02_Lanham.png)'
- en: Figure 5.2 Comparing differences in the hyperparameter tuning of the middle
    layer
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2比较中间层超参数调整的差异
- en: You can now go back and modify the other hyperparameters—`batch_size` and `data_step`.
    Be aware, though, that these values are linked, and if you dramatically increase
    the amount of data by decreasing `data_step` to `.1`, then you, likewise, need
    to increase the `batch_size`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以返回并修改其他超参数——`batch_size`和`data_step`。但请注意，这些值是相互关联的，如果你通过将`data_step`减小到`.1`来大幅增加数据量，那么你同样需要增加`batch_size`。
- en: Figure 5.3 shows the results of altering and not altering the batch size when
    increasing the amount of data. The results comparison shown in figure 5.3 are
    quite dramatic with respect to the amount of training time for completing 500
    epochs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3显示了在增加数据量时改变和未改变批量大小时的结果。图5.3中显示的结果与完成500个epoch的训练时间相比非常显著。
- en: '![](../Images/CH05_F03_Lanham.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F03_Lanham.png)'
- en: Figure 5.3 Comparison of changing data size, with and without batch size changes
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 比较改变数据大小，有和无批量大小变化的情况
- en: Continue to change the test name, modify hyperparameters, and then run the remaining
    cells with Run > Run After. If you find the plots are getting too messy, you can
    reset the plot results by running Run > Run All from the menu. Figure 5.4 shows
    an example of modifying the `learning_rate` from `3.5e-06` to `3.5e-01`. Your
    overall goal when tuning hyperparameters is to create the smallest model that
    will train the fastest and produce the best results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 继续更改测试名称，修改超参数，然后通过“运行”>“运行后续”来运行剩余的单元格。如果你发现图表变得过于混乱，你可以通过从菜单中运行“运行”>“运行所有”来重置图表结果。图5.4显示了将`learning_rate`从`3.5e-06`更改为`3.5e-01`的示例。你在调整超参数时的总体目标是创建一个最小的模型，该模型可以最快地训练并产生最佳结果。
- en: '![](../Images/CH05_F04_Lanham.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F04_Lanham.png)'
- en: Figure 5.4 The effects of altering the learning rate
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 改变学习率的影响
- en: 'Even with our simple example of only five hyperparameters, you may struggle
    with where to start, so a good starting point is to follow these steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们只有五个超参数的简单例子中，你也可能感到难以确定从哪里开始，因此一个好的起点是遵循以下步骤：
- en: '*Setting network size*—In our example, this was modifying the `middle_layer`
    value. Typically, you will want to start by adjusting the network size and/or
    number of layers. Be aware, though, that increasing the number of linear layers
    is less significant than increasing the number of network nodes in a layer.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*设置网络大小*—在我们的例子中，这是修改`middle_layer`值。通常，你将想从调整网络大小和/或层数开始。但请注意，增加线性层数的重要性不如增加层中的网络节点数。'
- en: 'Hyperparameter training rule #1: Network size'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '超参数训练规则 #1：网络大小'
- en: Increase the number of network layers to extract more features from data. Expand
    or decrease the model width (nodes) to tune the fit of the model.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增加网络层数以从数据中提取更多特征。扩展或减小模型宽度（节点）以调整模型的拟合度。
- en: '*Understanding data variability*—There is an expectation that DL models need
    to consume vast amounts of data. While DL models may certainly benefit from more
    data, success depends more on how variable the source data is. In our example,
    we were able to control the variability of the data using the `data_step` value.
    However, that ability to control variability of data is often not an option. In
    turn, if your model is highly variable and, thus, requires more data, you will
    likely need to increase the size of the model in layers and/or width. Pictures
    of handwritten digits, like the MNIST dataset, have much less variance than the
    images of fashion depicted in the Fashion-MNIST dataset.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*理解数据变异性*—我们期望深度学习模型需要消耗大量数据。虽然深度学习模型确实可能从更多数据中受益，但成功更多地取决于源数据的变异性。在我们的例子中，我们能够通过使用`data_step`值来控制数据的变异性。然而，控制数据变异性的能力通常并不是一个选项。反过来，如果你的模型高度变异，因此需要更多数据，你可能会需要增加模型在层数和/或宽度上的大小。例如，手写数字图片，如MNIST数据集，比Fashion-MNIST数据集中描述的时尚图片具有更少的变异性。'
- en: 'Hyperparameter training rule #2: Data variability'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '超参数训练规则 #2：数据变异性'
- en: Understand the variability of your source data. More-varied data requires larger
    models to either extract more features or learn how to fit more complex solutions.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解你的源数据的变异性。更多变性的数据需要更大的模型来提取更多特征或学习如何拟合更复杂的解决方案。
- en: '*Selecting batch size*—As we’ve seen in our example, tuning the batch size
    of a model can make it significantly more training-efficient. However, this hyperparameter
    is not a silver bullet for fixing training performance, and increasing it may
    be detrimental to end results. Instead, batch size needs to be tuned based on
    the variability of the input data. More variable input data generally benefits
    from smaller batch sizes, in the range of 16–64, while less-varied data may benefit
    from large batch sizes, in the range 64–256—or even higher.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择批量大小*—正如我们在例子中所看到的，调整模型的批量大小可以使其训练效率显著提高。然而，这个超参数并不是修复训练性能的万能药，增加它可能会对最终结果产生不利影响。相反，批量大小需要根据输入数据的变异性进行调整。更多变性的输入数据通常从较小的批量大小中受益，范围在16-64之间，而较少变性的数据可能从较大的批量大小中受益，范围在64-256之间——甚至更高。'
- en: 'Hyperparameter training rule #3: Batch size'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '超参数训练规则 #3：批量大小'
- en: If input data is highly varied, then decrease the batch size. Increase the batch
    size for less varied and more uniform datasets.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果输入数据高度变异，则减小批量大小。对于较少变异和更均匀的数据集，增加批量大小。
- en: '*Adjusting learning rate*—The learning rate controls how quickly a model learns
    and is often the first hyperparameter abused by newcomers. Much like batch size,
    learning rate is dictated by the complexity of the model, driven by the variance
    of the input data. Higher data variance requires smaller learning rates, while
    more uniform data can support high rates of learning. This is pictured well in
    figure 2.6, where we can see the model benefit from higher learning rates as a
    result of having very uniform data. Adjusting the model size may also require
    a decrease in the learning rate, due to increased model complexity.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*调整学习率*—学习率控制模型学习的速度，通常是新入门者首先滥用的超参数。与批量大小类似，学习率由模型的复杂度决定，这种复杂度是由输入数据的方差驱动的。更高的数据方差需要更小的学习率，而更均匀的数据可以支持更高的学习率。这在图2.6中得到了很好的体现，我们可以看到，由于数据非常均匀，模型从更高的学习率中受益。调整模型大小可能还需要降低学习率，因为模型复杂度增加。'
- en: 'Hyperparameter training rule #4: Learning rate'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '超参数训练规则 #4：学习率'
- en: Adjust the learning rate to match the variability of the input data. If you
    need to increase the size of a model, generally, decrease the learning rate as
    well.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调整学习率以匹配输入数据的变异性。如果你需要增加模型的大小，通常，也需要降低学习率。
- en: '*Tuning training iterations*—If you are working on smaller problems, you will
    often see the model quickly converge to some base solution. From that, you can
    then simply reduce the number of epochs (training iterations) of the model. However,
    if the model is more complex and takes much longer to train, determining total
    number of training iterations can be more problematic. Fortunately, most DL frameworks
    provide early stopping mechanisms that will watch for some arbitrary value of
    loss and, when that is achieved, will automatically stop training. As a general
    rule, then, you will often want to select the highest number of training iterations
    you think you need. Another option is to have the model save its weights periodically.
    Then, if needed, the same model may be reloaded and training continued.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*调整训练迭代次数*—如果你正在处理较小的问题，你通常会看到模型快速收敛到某个基本解决方案。从那时起，你可以简单地减少模型的epoch数（训练迭代次数）。然而，如果模型更复杂且训练时间更长，确定总的训练迭代次数可能会更成问题。幸运的是，大多数深度学习框架提供了早期停止机制，它会监视损失的一些任意值，当达到该值时，将自动停止训练。因此，一般来说，你通常会想选择你认为需要的最高训练迭代次数。另一个选项是让模型定期保存其权重。然后，如果需要，可以重新加载相同的模型并继续训练。'
- en: 'Hyperparameter training rule #5: Training iterations'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '超参数训练规则 #5：训练迭代次数'
- en: Always use the highest number of training iterations you think you will need.
    Use techniques like early stopping and/or model saving to reduce training iterations.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总是使用你认为需要的最高训练迭代次数。使用早期停止和/或模型保存等技术来减少训练迭代次数。
- en: Use these five rules to guide you when training hyperparameters, but be aware
    that the techniques mentioned are only a general guide. There may be network configurations,
    datasets, and other factors that alter those general rules. In the next section,
    we advance to the various model options you may need to decide on when building
    robust models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这五条规则来指导你在训练超参数时，但请注意，提到的技术仅是一般性指南。可能会有网络配置、数据集和其他因素改变这些一般规则。在下一节中，我们将进一步探讨在构建鲁棒模型时可能需要决定的各种模型选项。
- en: 5.1.2 Selecting model options
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 选择模型选项
- en: Aside from hyperparameters, the biggest source of tuning the model comes in
    the various options you internally decide to use. DL models provide for numerous
    options, sometimes dictated by the problem or network architecture, but often,
    subtle variations radically alter the way a model fits.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了超参数之外，调整模型的最大来源是你在内部决定使用的各种选项。深度学习模型提供了许多选项，有时由问题或网络架构决定，但通常，细微的变化会极大地改变模型拟合的方式。
- en: Model options range from activation and optimizer functions to the addition
    of the number and size of layers. As mentioned in the last section, layer depth
    is often dictated by the number of features a model needs to extract and learn.
    The type of layer, be it convolutional or recurrent networks, is often determined
    by the type of features needed to be learned. For example, we use CNN layers to
    learn clusters of features and RNN to determine how features are aligned or in
    what order.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选项的范围从激活和优化器函数到层数量和尺寸的增加。如前所述，层深度通常由模型需要提取和学习的特征数量决定。层的类型，无论是卷积还是循环网络，通常由需要学习的特点类型决定。例如，我们使用CNN层来学习特征簇，使用RNN来确定特征如何对齐或按什么顺序排列。
- en: Therefore, most DL models’ network size and layer types are driven by the variance
    of data and type of features needed to be learned. For image classification problems,
    CNN layers are used to extract visual features, like an eye or mouth. RNN layers,
    on the other hand, are used for processing language or time data, where the need
    is to understand how one feature relates to another in sequences.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，大多数深度学习模型的网络大小和层类型是由数据的方差和需要学习的特点类型驱动的。对于图像分类问题，CNN层用于提取视觉特征，如眼睛或嘴巴。另一方面，RNN层用于处理语言或时间数据，其中需要理解一个特征如何与另一个特征在序列中相关联。
- en: That means that in most cases, the options a DL practitioner needs to concern
    themselves with are the base functions of activation, optimization, and loss.
    Activation functions are typically dictated by the type of problem and form of
    data. We typically avoid altering activation functions until the final steps of
    tuning.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在大多数情况下，深度学习从业者需要关注的选项是激活、优化和损失的基础函数。激活函数通常由问题的类型和数据的形式决定。我们通常避免在调整的最后步骤之前更改激活函数。
- en: Most often, the choice of optimizer and loss function dictates how well a model
    trains, if at all. Take, for example, figure 5.5, which shows the results of selecting
    three different optimizers to train our last exercise, using a `middle_layer`
    hyperparameter of 25\. Notice in the figure that stochastic gradient descent (SGD)
    and Adagrad perform quite poorly in comparison to Adam and RMSprop.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的是，优化器和损失函数的选择决定了模型是否能够良好地训练。以图5.5为例，它显示了选择三个不同的优化器来训练我们最后一个练习的结果，使用`middle_layer`超参数为25。注意在图中，与Adam和RMSprop相比，随机梯度下降（SGD）和Adagrad的表现相当差。
- en: '![](../Images/CH05_F05_Lanham.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F05_Lanham.png)'
- en: Figure 5.5 A comparison of optimizer functions
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 优化器函数的比较
- en: 'Likewise, the form of loss function used to critique the network’s learning
    can have a substantial effect on model training. In our simple regression example,
    we have just two options: mean-squared error and mean absolute error, or L1 loss.
    Figure 5.6 shows a comparison between the two loss functions used in the last
    sample exercise. From the figure, it appears the better loss function for the
    last exercise may be L1 loss, or MAE.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，用于评估网络学习的损失函数的形式可以对模型训练产生重大影响。在我们的简单回归示例中，我们有两个选项：均方误差和平均绝对误差，或者L1损失。图5.6显示了在最后一个样本练习中使用的两个损失函数的比较。从图中可以看出，最后一个练习中更好的损失函数可能是L1损失，或MAE。
- en: '![](../Images/CH05_F06_Lanham.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F06_Lanham.png)'
- en: Figure 5.6 A comparison between loss criteria
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 损失标准的比较
- en: 'Hyperparameter training rule #6: Changing the model'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '超参数训练规则 #6：更改模型'
- en: As a general rule, anytime the model architecture or critical model options,
    like the optimization and/or loss function, change you need to retune all hyperparameters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一般规则，每当模型架构或关键模型选项，如优化和/或损失函数发生变化时，您需要重新调整所有超参数。
- en: By now, hopefully you realize that without a very keen eye and attention to
    detail, it is easy to miss finding the optimal model for your problem. In fact,
    you could spend countless hours tuning model hyperparameters, only to realize
    a better loss or optimization function may have performed far better.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，希望您已经意识到，如果没有非常敏锐的视角和对细节的关注，很容易错过找到适合您问题的最佳模型。事实上，您可能会花费无数小时调整模型超参数，最终发现更好的损失或优化函数可能表现得更好。
- en: Hyperparameter tuning and model option selection is prone to errors, even among
    the most seasoned deep learners. In chapter 4, we first introduced EDL by training
    model hyperparameters with evolutionary computation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整和模型选项选择容易出错，即使在经验丰富的深度学习者中也是如此。在第4章中，我们首先通过使用进化计算来训练模型超参数介绍了EDL。
- en: When it comes to building working DL models, you will typically define the model
    and select the options you feel most appropriate to your problem. You may then
    alter and tune the various hyperparameters, starting with the previously stated
    strategies. However, at some point, and unfortunately this happens often, you
    may decide to alter model options, like the optimizer or loss function. This,
    in turn, often requires you to loop back and retune hyperparameters based on the
    changed model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到构建工作的深度学习模型时，你通常会定义模型并选择你认为最适合你问题的选项。然后，你可能需要调整和微调各种超参数，从之前提到的策略开始。然而，不幸的是，这种情况经常发生，你可能会决定更改模型选项，如优化器或损失函数。这反过来又通常要求你根据更改的模型回过头来重新调整超参数。
- en: 5.2 Automating HPO with random search
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 使用随机搜索自动化HPO
- en: We just looked at manual HPO of DL using a function approximation problem. In
    this scenario, we provided a set of tools by which the practitioners could run
    the notebook consecutively, using different hyperparameters to generate comparisons.
    As you likely discovered from working through that exercise, manual HPO is time-consuming
    and just downright boring.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看了使用函数逼近问题进行深度学习手动HPO。在这种情况下，我们提供了一套工具，让从业者可以连续运行笔记本，使用不同的超参数生成比较。正如你很可能从完成那个练习中发现的那样，手动HPO既耗时又相当无聊。
- en: Of course, there are now numerous tools for performing HPO automatically. These
    tools range from Python packages to full systems incorporated into cloud technologies
    as part of an AutoML solution. We could certainly use any of those tools to perform
    a baseline comparison against EC methods, but for our purposes, we want to understand
    the automation and search process deeper.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现在有众多工具可以自动执行HPO。这些工具从Python包到作为AutoML解决方案一部分集成到云技术中的完整系统都有。我们当然可以使用这些工具中的任何一个来对EC方法进行基线比较，但就我们的目的而言，我们希望更深入地理解自动化和搜索过程。
- en: Random search HPO, as its name implies, is the process of sampling random values
    from a known set of hyperparameters within given ranges and then evaluating the
    effectiveness. The hope in random search is that you eventually find the best
    or desired solution. The process is comparable to someone throwing darts blindfolded,
    hoping to hit a bullseye. The blindfolded person likely won’t hit a bullseye in
    a couple throws, but over many throws, we expect they might.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索HPO，正如其名所示，是从给定范围内的已知超参数集中采样随机值并评估其有效性的过程。随机搜索的希望是最终找到最佳或期望的解决方案。这个过程可以比作某人蒙着眼睛投掷飞镖，希望击中靶心。蒙眼的人可能不会在几次投掷中击中靶心，但经过多次投掷，我们预计他们可能会。
- en: 5.2.1 Applying random search to HPO
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 将随机搜索应用于HPO
- en: Notebook EDL_5_2_RS_HPO.ipynb is an upgraded version of our previous notebook
    that automates HPO using a simple random search algorithm. Open that notebook
    in Colab, and then run all the cells via Runtime > Run All from the menu. As a
    comparison, feel free to open the EDL_5_1_Hyperparameter_Tuning.ipynb notebook.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Notebook EDL_5_2_RS_HPO.ipynb是我们之前笔记本的升级版本，它使用简单的随机搜索算法自动化HPO。在Colab中打开该笔记本，然后通过菜单中的“运行”>“运行所有”来运行所有单元格。作为比较，你可以自由打开EDL_5_1_Hyperparameter_Tuning.ipynb笔记本。
- en: Let’s start by exploring the problem function we want our DL network to approximate.
    The first cell of code revisits our polynomial function from chapter 4, as shown
    in figure 5.1\. The following listing contains the code that generates the sample
    set of input and target data points we train the network on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先探索我们希望我们的深度学习网络逼近的问题函数。代码的第一个单元格回顾了第4章中的多项式函数，如图5.1所示。以下列表包含生成训练网络的输入和目标数据点的样本集的代码。
- en: 'Listing 5.2 EDL_5_2_RS_HPO.ipynb: Defining the data'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.2 EDL_5_2_RS_HPO.ipynb：定义数据
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Defines the polynomial target function
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义多项式目标函数
- en: ❷ Sets the bounds and step for the data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置数据的界限和步长
- en: ❸ Generates and reshapes input data
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成并重塑输入数据
- en: ❹ Generates target outputs
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成目标输出
- en: ❺ Finds the number of inputs to the network
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 找到网络的输入数量
- en: Next, we revisit the base model/class we are using as the network we want to
    learn to approximate this function, as defined in the following listing. This
    is the same base model we used to evaluate our simpler example from chapter 2.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们回顾我们用作学习近似该函数的网络的基础模型/类，如下所示列表中定义。这是我们用来评估第2章中更简单示例的相同基础模型。
- en: 'Listing 5.3 EDL_5_2_RS_HPO.ipynb: Defining the model'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3 EDL_5_2_RS_HPO.ipynb：定义模型
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Defines the input nodes and size of the middle layer
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义输入节点和中层的大小
- en: ❷ Sets up the first fully connected layer
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置第一个全连接层
- en: ❸ Defines the forward function
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义前向函数
- en: Now, for the automation magic. Our process of automating HPO consists of using
    a new class to contain and manage the search. For random search, the version of
    this `Hyperparameters` class is shown in listing 5.4\. This `init` function takes
    the input hyperparameters and converts them to class properties using `update`.
    When we use this class, we first set up the base properties as inputs, and then
    for each hyperparameter property, we define a generator that provides the next
    value. Calling the `next` function on this base `Hyperparameters` object generates
    a new generated object that is used in a single evaluation. Don’t fret if this
    is not so obvious just yet; we are using some advanced functionality that is best
    explained in code to come.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于自动化魔法。我们自动化HPO的过程包括使用一个新类来包含和管理搜索。对于随机搜索，此`Hyperparameters`类的版本如列表5.4所示。此`init`函数接收输入超参数并将它们转换为类属性，使用`update`。当我们使用此类时，我们首先设置基本属性作为输入，然后对于每个超参数属性，我们定义一个提供下一个值的生成器。在基础`Hyperparameters`对象上调用`next`函数生成一个新的生成对象，用于单次评估。如果这还不那么明显，请不要担心；我们正在使用一些高级功能，这些功能最好通过即将到来的代码来解释。
- en: 'Listing 5.4 EDL_5_2_RS_HPO.ipynb: The hyperparameter class'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4 EDL_5_2_RS_HPO.ipynb：超参数类
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The init function placing the input args into the dictionary
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `init`函数将输入参数放入字典中
- en: ❷ Overrides the str function for nicer printing
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 覆盖str函数以实现更友好的打印
- en: ❸ The function to grab the next instance of a hyperparameters object
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取超参数对象下一个实例的函数
- en: ❹ Loops through the args dictionary and then calls next on the argument
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历args字典，然后对参数调用next方法
- en: ❺ Returns a new instance of the hyperparameters object
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回超参数对象的新实例
- en: The `Hyperparameters` class internally uses the Python generator pattern to
    loop through all the properties to create a new instance. For our random search
    approach, we use a function generator called `sampler`, as shown in listing 5.5\.
    The `sampler` function is meant to continuously sample from a given function within
    some range set by `min` and `max`. Python supports two forms of generators—the
    one we are using uses the yield keywork to interrupt the loop and return a value.
    To execute a generator, you need to wrap the function in `next`, as seen in the
    previous listing (listing 5.4).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hyperparameters`类内部使用Python生成器模式遍历所有属性以创建一个新实例。对于我们的随机搜索方法，我们使用一个名为`sampler`的函数生成器，如列表5.5所示。`sampler`函数旨在从由`min`和`max`设置的某个范围内连续采样给定函数。Python支持两种形式的生成器——我们使用的一种使用`yield`关键字来中断循环并返回值。要执行生成器，您需要将函数包裹在`next`中，如前一个列表（列表5.4）所示。'
- en: 'Listing 5.5 EDL_5_2_RS_HPO.ipynb: The sampler generator'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.5 EDL_5_2_RS_HPO.ipynb：采样生成器
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The input is function and range, from min to max.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入是函数和范围，从min到max。
- en: ❷ The infinite generator set up with endless loop
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用无限循环设置的无限生成器
- en: ❸ Yields by calling the function with min,max range
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过调用函数并指定min，max范围来产生
- en: We can put these pieces together when we initially set up the base or parent
    `Hyperparameters` object, as shown in listing 5.6\. In the parent object, we define
    each input equal to the `sampler` generator defined by various functions and min/max
    ranges. Notice how the sampling function we use changes from `random.ranint` to
    `random .uniform`, where both functions generate a random variable from a uniform
    distribution. Calling the next function generates a child hyperparameters object
    that can be used in an experiment evaluation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在最初设置基本或父`Hyperparameters`对象时将这些部分组合起来，如列表5.6所示。在父对象中，我们将每个输入定义为由各种函数和min/max范围定义的`sampler`生成器。注意我们使用的采样函数如何从`random.ranint`更改为`random.uniform`，其中两个函数都从均匀分布生成随机变量。调用next函数生成一个子超参数对象，可用于实验评估。
- en: 'Listing 5.6 EDL_5_2_RS_HPO.ipynb: Creating a hyperparameters parent'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.6 EDL_5_2_RS_HPO.ipynb：创建超参数父对象
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Adds input for epochs to the generator function
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将输入的epochs添加到生成器函数中
- en: ❷ Additional input using random.input base function
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用随机.input基本函数添加额外输入
- en: ❸ Samples the next object and then prints
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 采样下一个对象然后打印
- en: To understand how this works, jump down to the large block of code with the
    training function `train_function`, as shown in listing 5.7\. Inside this function,
    we first call `hp.next()` to generate a child object. We can then use the values
    in our training algorithm by just using the name as a property on the object.
    Since we are using the `sampler` function with random evaluators anytime we call
    `hp.next()`, the output is a random set of hyperparameters.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这是如何工作的，请跳转到包含训练函数`train_function`的大块代码，如列表5.7所示。在这个函数内部，我们首先调用`hp.next()`来生成一个子对象。然后，我们可以通过在对象上使用名称作为属性来使用我们的训练算法中的值。由于我们每次调用`hp.next()`时都使用`sampler`函数与随机评估器一起使用，所以输出是一组随机的超参数。
- en: 'Listing 5.7 EDL_5_2_RS_HPO.ipynb: Using a hyperparameters child'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.7 EDL_5_2_RS_HPO.ipynb：使用超参数子对象
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Generates a child hyperparameters object
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成子超参数对象
- en: ❶ Uses the hyperparameter by calling a property
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过调用属性来使用超参数
- en: Lastly, we can look at how all of this is brought together and automated to
    perform HPO, as shown in listing 5.8\. Since we have encapsulated all our random
    sampling with the HP class, the rest of the code is quite simple. Since this is
    a minimization problem, we want to tune the hyperparameters to minimize the target
    network’s loss, so we set the starting best value to max infinity. Then, in a
    loop defined by runs, we call the `train_function` using the parent hyperparameters
    object. Inside the training function, `HP` generates a new random instance of
    hyperparameters and uses those to evaluate network loss. We evaluate the overall
    `fitness` with a full prediction over all the points in the model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看如何将这些内容整合并自动化以执行HPO，如列表5.8所示。由于我们已经将所有随机采样封装在HP类中，所以其余代码相当简单。由于这是一个最小化问题，我们希望调整超参数以最小化目标网络的损失，因此我们将起始最佳值设置为最大无穷大。然后，在由运行定义的循环中，我们使用父超参数对象调用`train_function`。在训练函数内部，`HP`生成一个新的随机超参数实例，并使用这些值来评估网络损失。我们通过在模型的所有点上进行完整预测来评估整体`fitness`。
- en: 'Listing 5.8 EDL_5_2_RS_HPO.ipynb: Automatic HPO'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8 EDL_5_2_RS_HPO.ipynb：自动HPO
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Sets the initial max value for best
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置最佳值的初始最大值
- en: ❷ Runs the training function
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行训练函数
- en: ❸ Evaluates fitness over all data
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在所有数据上评估fitness
- en: ❹ Checks if this is a new best
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 检查这是否是新的最佳值
- en: Figure 5.7 shows the output of performing automatic HPO using random search.
    At the top of the charts is the best `fitness` overall, with the hyperparameters
    listed. Below that are three plots showing the loss history for the network training,
    how well the model approximates the function, and, finally, a mapping of all the
    evaluations run so far. The evaluation plot shows a grayscale output of the best
    `fitness`, where the black hexagon represents the best overall evaluated `fitness`
    thus far. At the bottom of the figure, you can see the result after running for
    10,000 runs, where the single black dot, which is hard to see, represents the
    minimum `fitness`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7展示了使用随机搜索执行自动HPO的输出。图表顶部显示的是整体最佳`fitness`，并列出了超参数。下面是三个图表，显示了网络训练的损失历史、模型对函数的近似程度，以及迄今为止所有评估的映射。评估图显示了最佳`fitness`的灰度输出，其中黑色六边形代表迄今为止评估的最佳整体`fitness`。图例底部，你可以看到运行10,000次后的结果，其中单个黑色点，难以看清，代表最小`fitness`。
- en: '![](../Images/CH05_F07_Lanham.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F07_Lanham.png)'
- en: Figure 5.7 Results of running random hyperparameter search
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7随机超参数搜索的运行结果
- en: Finally, we review the code that generates the output, as we use it for all
    the exercises in this chapter. This plotting output first draws the last run’s
    loss history and function approximation, as shown in the following listing. The
    end plot is a hexbin map of the all the runs’ history, plotted by learning rate
    and middle layer hyperparameters. As the automatic HPO is running, you will see
    this plot change over time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回顾生成输出的代码，因为我们用它来完成本章的所有练习。此绘图输出首先绘制了最后一次运行的损失历史和函数近似，如下面的列表所示。最终图是所有运行历史的六边形图，通过学习率和中间层超参数绘制。随着自动HPO的运行，你会看到这个图随时间变化。
- en: 'Listing 5.9 EDL_5_2_RS_HPO.ipynb: Plotting the output'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9 EDL_5_2_RS_HPO.ipynb：绘制输出
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Sets up the combined figure with three horizontal subplots
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置包含三个水平子图的组合图
- en: ❷ Plots the loss training history
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制损失训练历史
- en: ❸ Draws the function approximation
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制函数近似
- en: ❹ Hexbins all run history
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对所有运行历史进行六边形图绘制
- en: The results shown in figure 5.8 took several hours to generate, and you can
    clearly see where the optimum results land. In this scenario, we only use two
    hyperparameters for HPO, so we can clearly visualize the results in two dimensions.
    We could, of course, use all these techniques on more than two variables, but
    as expected, that would likely require more runs and time. In later scenarios,
    we introduce more advanced techniques to visualize and track multiple hyperparameters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8中显示的结果生成花费了好几个小时，你可以清楚地看到最佳结果落在何处。在这种情况下，我们只为HPO使用了两个超参数，因此我们可以清楚地从两个维度可视化结果。当然，我们可以在超过两个变量上使用所有这些技术，但正如预期的那样，这可能会需要更多的运行和时间。在后续场景中，我们将介绍更高级的技术来可视化和跟踪多个超参数。
- en: '![](../Images/CH05_F08_Lanham.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F08_Lanham.png)'
- en: Figure 5.8 An example output from HPO random search, from 10 to 10,000 runs
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 HPO随机搜索的一个示例输出，从10到10,000次运行
- en: 'Random search is great for finding quick answers, but the problem with this
    approach is that random methods are just that: random. We have no way of knowing
    if we are getting closer to a solution or what a possible best solution may even
    look like. There are several statistical methods that could track progress and
    promote better solutions, but these still require hundreds—or thousands—of iterations.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索对于找到快速答案很好，但这个方法的问题在于，随机方法就是随机。我们无法知道我们是否接近解决方案，甚至一个可能的最佳解决方案可能是什么样子。有一些统计方法可以跟踪进度并促进更好的解决方案，但这些仍然需要数百——或者数千——次迭代。
- en: In our simple example here, we are only managing two hyperparameters over relatively
    small ranges. That means that in a relatively short time, we could have a reasonably
    good guess. However, that guess provides us no insight on how close it is, other
    than it was the best over a certain number of random samplings. Random search
    works well for quick approximations, but there are far better methods, as discussed
    in upcoming sections.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这里的简单示例中，我们只是在相对较小的范围内管理两个超参数。这意味着在相对较短的时间内，我们可能有一个相当好的猜测。然而，这个猜测并没有给我们提供关于它有多接近的洞察，除了它是在一定数量的随机采样中最好的。随机搜索对于快速近似效果很好，但正如将在后续章节中讨论的，有更好的方法。
- en: 5.3 Grid search and HPO
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 网格搜索和HPO
- en: While random search can be an effective method to make better guesses quickly
    for finding accurate HPO, it is overly time-consuming. Generating the final output
    for figure 5.7 took over 8 hours, which is slow but can yield accurate results.
    Searching for fast and accurate automatic HPO requires more elevated techniques.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然随机搜索可以是一个快速做出更好猜测的有效方法，以找到准确的HPO，但它过于耗时。生成图5.7的最终输出花费了超过8个小时，这很慢，但可以产生准确的结果。寻找快速且准确的自动HPO需要更高级的技术。
- en: One such simple technique that works effectively for everything from archeological
    digs to search and rescue teams is *grid search*. Grid search works by laying
    out the search area or surface in a grid pattern and then methodically walking
    through every cell in the grid. Grid search is best visualized in two dimensions,
    but this technique is valid for any number of dimensional problems.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单而有效的技术，适用于从考古挖掘到搜救队的一切，就是*网格搜索*。网格搜索通过以网格模式布置搜索区域或表面，然后系统地遍历网格中的每个单元格。网格搜索最好在二维中可视化，但这种技术在任何维度的问题上都是有效的。
- en: Figure 5.9 shows a comparison between a random search working over a hyperparameter
    space and grid search. The figure demonstrates one possible pattern for walking
    through the grid, and at each cell, it evaluates the `learning_rate` and `middle_layer`
    variables. Grid search is an effective method for evaluating a range of possible
    combinations in a methodical and efficient manner.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9显示了在超参数空间中随机搜索和网格搜索之间的比较。该图演示了通过网格的可能模式之一，并且在每个单元格中，它评估`learning_rate`和`middle_layer`变量。网格搜索是一种以系统化和有效的方式评估一系列可能组合的有效方法。
- en: '![](../Images/CH05_F09_Lanham.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F09_Lanham.png)'
- en: Figure 5.9 Comparison of random search and grid search
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 随机搜索和网格搜索的比较
- en: 5.3.1 Using grid search for automatic HPO
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 使用网格搜索进行自动HPO
- en: In our next exercise, we upgrade our earlier random search attempt to use a
    more sophisticated grid search technique. While this technique is more robust
    and efficient, it is still bounded by the size of the grid. Using larger grid
    cells often limits results to local minimums or maximums. Finer and smaller cells
    can locate global minimums and maximums but at the cost of increased search space.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们接下来的练习中，我们将我们早期的随机搜索尝试升级为使用更复杂的网格搜索技术。虽然这种技术更加稳健和高效，但它仍然受限于网格的大小。使用较大的网格单元通常会将结果限制在局部最小值或最大值。更细小和密集的单元可以定位全局最小值和最大值，但代价是增加了搜索空间。
- en: The code in the next exercise notebook, EDL_5_3_GS_HPO.ipynb, is derived from
    our earlier random search example. As such, much of the code is the same, and
    as usual, we focus on just the parts that make this sample unique. Open the EDL_5_3_GS_HPO.ipynb
    in Colab and run all the cells via Runtime > Run All.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个练习笔记本中的代码，EDL_5_3_GS_HPO.ipynb，是从我们早期的随机搜索示例派生出来的。因此，大部分代码是相同的，并且，像往常一样，我们只关注使这个样本独特的部分。在
    Colab 中打开 EDL_5_3_GS_HPO.ipynb 并通过“运行”>“运行所有单元格”来运行所有单元格。
- en: The primary difference in the code for this example is the hyperparameter object
    now needs to track a parameter grid. We first look at the construction of a new
    `HyperparametersGrid` class and the `init` function. Inside this function, shown
    in listing 5.10, we extract the names of the input parameters into `self.hparms`
    and then test whether the first input points to a generator. If it does, then
    we generate a parameter grid with `self.create_grid`; otherwise, the instance
    will just be a child hyperparameter container.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的代码与主要区别在于超参数对象现在需要跟踪一个参数网格。我们首先看看新的 `HyperparametersGrid` 类和 `init` 函数的构建。在这个函数中，如列表
    5.10 所示，我们将输入参数的名称提取到 `self.hparms` 中，然后测试第一个输入是否指向一个生成器。如果是，那么我们使用 `self.create_grid`
    生成一个参数网格；否则，该实例将只是一个子超参数容器。
- en: 'Listing 5.10 EDL_5_3_GS_HPO.ipynb: The `HyperparametersGrid` init function'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10 EDL_5_3_GS_HPO.ipynb：`HyperparametersGrid` 初始化函数
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Extracts all the input arg names
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提取所有输入参数名
- en: ❷ Only creates a grid if it is a parent
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仅在它是父节点时创建网格
- en: ❸ Creates the parameter grid
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建参数网格
- en: ❹ Gets the size of the grid
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取网格的大小
- en: Next, we look at how the parameter grid is constructed in the `self.create_grid`
    function. The function, shown in listing 5.11, starts by creating an empty `grid`
    dictionary and then loops through the list of hyperparameters. It calls the hyperparameter
    generator, using next to return, in this case, a value and the total number of
    values. Then we loop again through the generator to extract each unique value
    and append it to a `row` list. After this, we append the row to the grid and then
    finish by injecting the `grid` into the `ParameterGrid` class. `ParameterGrid`
    is a helper class from scikit-learn that takes as input a dictionary of inputs
    and list of values and then constructs a grid in which each cell represents the
    various hyperparameter combinations. While we are only running this example with
    two hyperparameters over a two-dimensional grid, `ParameterGrid` can manage any
    number of dimensions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看看如何在 `self.create_grid` 函数中构建参数网格。该函数，如列表 5.11 所示，首先创建一个空的 `grid` 字典，然后遍历超参数列表。它调用超参数生成器，使用
    `next` 返回，在这种情况下，一个值及其总数量。然后我们再次遍历生成器以提取每个唯一值并将其追加到 `row` 列表中。之后，我们将行追加到网格中，然后通过将
    `grid` 注入到 `ParameterGrid` 类来完成。`ParameterGrid` 是 scikit-learn 中的一个辅助类，它接受一个输入字典和值列表作为输入，然后构建一个网格，其中每个单元格代表各种超参数组合。虽然我们在这个示例中只运行了两个超参数在二维网格上的示例，但
    `ParameterGrid` 可以管理任意数量的维度。
- en: 'Listing 5.11 EDL_5_3_GS_HPO.ipynb: `create_grid` function'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11 EDL_5_3_GS_HPO.ipynb：`create_grid` 函数
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Loops through all the hyperparameter generators
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历所有超参数生成器
- en: ❷ Extracts a value and the total number of values
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取一个值及其总数量
- en: ❸ Loops through the range of values
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历值的范围
- en: ❹ Appends the value to a row and then adds it to the grid
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将值追加到一行，然后将其添加到网格中
- en: ❺ Creates a ParameterGrid object from the dictionary grid
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从字典网格创建一个 `ParameterGrid` 对象
- en: With the internal parameter grid holding all the combinations of hyperparameters,
    we can now look at how the updated `next` function works, as shown in listing
    5.12\. At the top, we have a `reset` function that works to reset an index into
    the parameter grid. Every call to `next` increases the index and, thus, extracts
    the next value from the parameter grid (`self.grid`). The last line of code unpacks
    the grid value with `**` as an input into a new instance of `HyperparametersGrid`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部参数网格包含所有超参数组合后，我们现在可以看看更新的 `next` 函数是如何工作的，如列表 5.12 所示。在顶部，我们有一个 `reset`
    函数，它用于重置参数网格的索引。每次调用 `next` 都会增加索引，从而从参数网格 (`self.grid`) 中提取下一个值。代码的最后一行使用 `**`
    将网格值解包为 `HyperparametersGrid` 的新实例的输入。
- en: 'Listing 5.12 EDL_5_3_GS_HPO.ipynb: The `HyperparametersGrid` `next` function'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.12 EDL_5_3_GS_HPO.ipynb：`HyperparametersGrid` 的 `next` 函数
- en: '[PRE11]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Resets the function to reset the grid index
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重置函数以重置网格索引
- en: ❷ Increments the grid index
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 增加网格索引
- en: ❸ Checks the bounds of the index
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查索引的界限
- en: ❹ Returns the next parameter grid as a child hyperparameters object
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回下一个参数网格作为子超参数对象
- en: 'Using the new grid hyperparameters class also requires us to upgrade the generators
    we use to control the hyperparameter creation. For simplicity, we define two functions:
    one for floats and the other integers. Inside each function, we create an array
    of values called `grid` from a minimum to maximum at a step interval. We iterate
    through this list of values, yielding a new value and the total list length, as
    shown in the following listing. Having the total list length allows us to iterate
    through the generator to create the parameter grid, as seen previously.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新的网格超参数类也要求我们升级我们用来控制超参数创建的生成器。为了简化，我们定义了两个函数：一个用于浮点数，另一个用于整数。在每一个函数内部，我们从一个最小值到最大值以步长间隔创建一个名为
    `grid` 的值数组。我们遍历这个值列表，生成一个新的值和总列表长度，如下所示列表。拥有总列表长度允许我们遍历生成器以创建参数网格，正如之前所见。
- en: 'Listing 5.13 EDL_5_3_GS_HPO.ipynb: Generators'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.13 EDL_5_3_GS_HPO.ipynb：生成器
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The float function accepts the min/max and grid step.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 浮点函数接受最小/最大值和网格步长。
- en: ❷ Cycles through the list of values
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历值列表
- en: ❸ Yields a value the length of the grid cells
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成网格单元格长度的值
- en: ❹ The integer function accepts min/max and grid step.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 整数函数接受最小/最大值和网格步长。
- en: ❺ Yields a value the length of the grid cells
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 生成网格单元格长度的值
- en: Now, we can see how this new class and these generator functions are used to
    create the parent `hp` object, as shown in listing 5.14\. Setting up the variables
    is the same as we saw earlier; however, this time we are using the `grid` generator
    functions. Internally, after the class is initialized, an internal parameter grid
    is created. We can query information about the grid, like getting the total number
    of combinations or values. Then we can also call `next` on the parent `hp` object
    to generate a couple of child objects. We can calculate the number of grid combinations
    by taking the number of values for each hyperparameter and multiplying them all
    together. In our example, there are 9 values for `middle_layer`, 10 for `learning_rate`,
    1 for `epochs`, and 1 for `batch_size`, giving us a total of 90, or 10 x 9 x 1
    x 1 = 90\. Grid sizes can get large quickly, especially when working with multiple
    variables and smaller step sizes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到如何使用这个新类和这些生成器函数来创建父 `hp` 对象，如列表 5.14 所示。设置变量与之前所见相同；然而，这次我们使用的是 `grid`
    生成器函数。在类初始化后，内部创建了一个参数网格。我们可以查询有关网格的信息，例如获取组合的总数或值。然后我们也可以在父 `hp` 对象上调用 `next`
    来生成几个子对象。我们可以通过将每个超参数的值相乘来计算网格组合的数量。在我们的例子中，`middle_layer` 有 9 个值，`learning_rate`
    有 10 个值，`epochs` 有 1 个值，`batch_size` 也有 1 个值，总共是 90，即 10 x 9 x 1 x 1 = 90。网格大小可以迅速增大，尤其是在处理多个变量和较小的步长时。
- en: 'Listing 5.14 EDL_5_3_GS_HPO.ipynb: Creating a grid'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.14 EDL_5_3_GS_HPO.ipynb：创建网格
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Prints the number of grid combinations
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印网格组合的数量
- en: ❷ Prints the parameter grid inputs
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印参数网格输入
- en: ❸ Prints the next child hyperparameters object
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印下一个子超参数对象
- en: This example uses a GPU for training, but the code changes to implement are
    minor and won’t be shown. Instead, we focus on some subtle changes in the automation
    setup. `Runs` is now defined by `hp.grid_size`, and we create a new variable called
    `grid_size`, defined by the number of runs, as shown in the following listing.
    The second variable is used to define the size of grid cells we draw on the `hexbins`
    `fitness` evaluation plot.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例使用GPU进行训练，但实现代码的更改很小，不会展示。相反，我们关注自动化设置中的细微变化。`Runs`现在由`hp.grid_size`定义，我们创建了一个新变量`grid_size`，它由运行次数定义，如下所示。第二个变量用于定义我们在`hexbins`
    `fitness`评估图上绘制的网格单元格的大小。
- en: 'Listing 5.15 EDL_5_3_GS_HPO.ipynb: Creating a grid'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.15 EDL_5_3_GS_HPO.ipynb：创建网格
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ runs now equals grid_size
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 现在的`runs`等于`grid_size`
- en: ❷ Defines plot grid_size based on the total runs
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据总运行次数定义`plot_grid_size`
- en: ❸ Resets the parent hp object before starting
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在开始之前重置父`hp`对象
- en: Figure 5.10 shows the output of running this exercise to completion—all 90 runs
    in about 10 minutes. This is a far quicker result than the random search example
    but not as accurate. Notice that the final `fitness` in figure 5.3 (~17,000) is
    one-third that of the `fitness` shown in figure 5.5 (~55,000). Thus, our result
    with grid search is not as accurate, but it is certainly quicker and more efficient.
    We could always go back and narrow the search to a smaller one and reduce the
    step size to narrow in on accuracy.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10显示了运行此练习直到完成的结果——全部90次运行大约需要10分钟。这比随机搜索示例的结果快得多，但准确性不高。注意图5.3中的最终`fitness`（约17,000）是图5.5中显示的`fitness`（约55,000）的三分之一。因此，我们的网格搜索结果并不那么准确，但它确实更快、更高效。我们总是可以回过头来缩小搜索范围，并减小步长以提高准确性。
- en: The last thing we want to look at is modifying the output evaluation plot to
    set the `grid_size` based on our previously calculated variable. We use a hexbin
    plot to automatically map the `fitness` values by color. Then, we set the `grid_size`
    based on the number of combinations. In this simple example, shown in the following
    listing, we assume a square grid of parameters, but that may not always be accurate.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后要考虑的是修改输出评估图，根据我们之前计算出的变量设置`grid_size`。我们使用六边形图自动通过颜色映射`fitness`值。然后，我们根据组合数量设置`grid_size`。在以下列表中显示的简单示例中，我们假设参数是正方形网格，但这可能并不总是准确。
- en: 'Listing 5.16 EDL_5_3_GS_HPO.ipynb: Setting the `grid_size`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.16 EDL_5_3_GS_HPO.ipynb：设置`grid_size`
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Plots the hexbin with hyperparameter data
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 绘制带有超参数数据的六边形图
- en: ❷ Keeps the number of bins the same
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保持单元格数量不变
- en: ❸ Sets the size of each cell
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置每个单元格的大小
- en: ❹ Sets the color map to use
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置要使用的颜色图
- en: Grid search is an excellent technique when you want a methodical view of various
    hyperparameter combinations. However, pay special attention, again, to the output
    in figure 5.10, and note how the best `fitness` (the dark area) is within two
    cells of the worst `fitness` (the light area). Yet we can see plenty of areas
    that have good `fitness` around this light-colored cell, indicating we are likely
    missing a global minimum and/or maximum. The fix for this would be to go back
    and narrow the grid to just cover this two- to three-cell area—something that
    would require manual intervention on our part to better isolate the optimum hyperparameters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是一种在希望系统地查看各种超参数组合时非常出色的技术。然而，再次特别关注图5.10中的输出，并注意最佳`fitness`（深色区域）位于最差`fitness`（浅色区域）的两个单元格之内。然而，我们可以看到许多具有良好`fitness`的区域围绕这个浅色单元格，这表明我们可能遗漏了全局最小值和/或最大值。解决这个问题的方法是回到并缩小网格，仅覆盖这个两到三个单元格的区域——这需要我们手动干预以更好地隔离最佳超参数。
- en: '![](../Images/CH05_F10_Lanham.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F10_Lanham.png)'
- en: Figure 5.10 The final output of the grid search
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 网格搜索的最终输出
- en: Given that we have some experience now using EC methods for various problems,
    it follows that we take our next steps employing them for HPO. In the rest of
    this chapter, we look at using EC to provide better search mechanisms when performing
    HPO.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们现在使用EC方法解决各种问题已有一些经验，那么接下来我们采取的步骤就是使用它们进行HPO。在本章的其余部分，我们将探讨使用EC在执行HPO时提供更好的搜索机制。
- en: 5.4 Evolutionary computation for HPO
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 用于HPO的进化计算
- en: Now that we have established a good background on the problem of HPO, we can
    start to employ some EC methods to enhance both speed and accuracy. As we have
    seen in previous chapters, evolutionary methods provide an excellent toolset to
    optimize search on various problems. It only makes sense then that we evaluate
    the practical side of using EC to perform HPO.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对HPO问题有了良好的背景了解，我们可以开始使用一些EC方法来提高速度和准确性。正如我们在前面的章节中看到的，进化方法提供了一套优秀的工具集，可以用于优化各种问题的搜索。因此，评估使用EC进行HPO的实际应用是很有意义的。
- en: 5.4.1 Particle swarm optimization for HPO
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 粒子群优化用于HPO
- en: We start out introducing EC to DL by using PSO for HPO. PSO, as discussed in
    chapter 4, uses a `population` of swarming `particles` to find an optimum solution.
    Not only is PSO simple enough to implement with DEAP, but it also showcases the
    power of EC for solving problems like HPO.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用PSO进行HPO，将EC引入到DL中。PSO，如第4章所述，使用一群`particles`来寻找最优解。PSO不仅足够简单，可以用DEAP实现，而且展示了EC在解决像HPO这样的问题上的强大能力。
- en: 5.4.2 Adding EC and DEAP to automatic HPO
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 将EC和DEAP添加到自动HPO
- en: 'In the next exercise, we focus on two key aspects: adding EC/DEAP to perform
    automatic HPO and applying an EC method, like PSO, to the problem. We, again,
    continue to use the same base problem to compare results between methods. Not
    only does this make understanding the code easier, but it also provides us with
    a good baseline when comparing further methods.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们关注两个关键方面：将EC/DEAP添加到执行自动HPO，以及将EC方法（如PSO）应用于问题。我们再次使用相同的基本问题来比较不同方法的结果。这不仅使理解代码变得更容易，而且在比较其他方法时也提供了一个良好的基线。
- en: Open the EDL_5_4_PSO_HPO_PCA.ipynb notebook in Colab, and run all the cells
    via Runtime > Run All. Scroll down to the cell with the `HyperparametersEC` class
    definition, as shown in the following listing. Again, much of the heavy lifting
    in combining EC with DL occurs in the `HyperparametersEC` class. This time, we
    create a specialized version of this class called `HyperparametersEC`. We start
    our focus by looking at the base functions.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_5_4_PSO_HPO_PCA.ipynb笔记本，并通过“运行”>“运行所有单元格”来运行所有单元格。滚动到包含`HyperparametersEC`类定义的单元格，如下所示。再次，将EC与DL结合的重任大部分发生在`HyperparametersEC`类中。这次，我们创建了一个名为`HyperparametersEC`的专用版本。我们首先关注的是基本函数。
- en: 'Listing 5.17 EDL_5_4_PSO_HPO.ipynb: The `HyperparametersEC` base functions'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.17 EDL_5_4_PSO_HPO.ipynb：`HyperparametersEC`基本函数
- en: '[PRE16]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Initializes the class with input args
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用输入参数初始化类
- en: ❷ Overrides to a string function
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 重写字符串函数
- en: ❸ Exposes the current values
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提供当前值
- en: ❹ Returns the size of hyperparameters
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回超参数的大小
- en: After the base functions, look at the special `next` function used to call the
    parent `HyperparametersEC` object to derive a child. This is complex code that
    won’t entirely make sense until we look at the new generator methods. Notice how
    this function, shown in listing 5.18, takes as input an `individual` or vector
    of values. In this example, an `individual` represents a `particle`, but it could
    also represent any form of `individual` we describe using a vector. Another important
    detail to focus on is the use of the `send` function on the generator. `send`
    is like the `next` function in Python, but it allows generators to be initialized
    or input with values.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本函数之后，查看用于调用父`HyperparametersEC`对象的特殊`next`函数。这是一段复杂的代码，直到我们查看新的生成器方法之前，它可能不会完全有意义。注意这个函数（如列表5.18所示），它接受一个`individual`或值的向量作为输入。在这个例子中，`individual`代表一个`particle`，但它也可以代表我们使用向量描述的任何形式的`individual`。另一个需要关注的细节是生成器上`send`函数的使用。`send`类似于Python中的`next`函数，但它允许生成器初始化或输入值。
- en: 'Listing 5.18 EDL_5_4_PSO_HPO.ipynb: `HyperparametersEC` `next` function'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.18 EDL_5_4_PSO_HPO.ipynb：`HyperparametersEC` `next`函数
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Enumerates the hyperparameters
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列出超参数
- en: ❷ Initializes the generator for each hyperparameter
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为每个超参数初始化生成器
- en: ❸ Sends the index value to the generator and yields the value
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将索引值发送到生成器并产生值
- en: ❹ Returns the child object
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回子对象
- en: Since the `send` function allows values to be passed into a generator, we can
    now rewrite the generator functions to accommodate. The two functions of interest
    are the `linespace` and `linespace_int` generators, as shown in listing 5.19\.
    These generators allow inputs to be passed in using `i = yield`, where `yield`
    becomes the value input using the `send` function. The value `i` then becomes
    an index into the linear interpolated space between the values of `-1.0` and `1.0`,
    by applying the `clamp` function. As you may recall, the `send` function sent
    in the indexed value from the `individual`. Thus, each vector element in the `individual`
    becomes an index into the hyperparameter’s linear space defined by the min/max
    values used when setting up the parent `hp`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`send`函数允许将值传递到生成器中，我们现在可以重写生成器函数以适应。我们感兴趣的函数是`linespace`和`linespace_int`生成器，如列表5.19所示。这些生成器允许使用`i
    = yield`传递输入，其中`yield`成为使用`send`函数输入的值。值`i`然后成为`-1.0`和`1.0`之间的线性插值空间的索引，通过应用`clamp`函数。如您所回忆的，`send`函数发送了从`individual`中的索引值。因此，`individual`中的每个向量元素都成为由设置父`hp`时使用的最小/最大值定义的超参数线性空间中的索引。
- en: 'Listing 5.19 EDL_5_4_PSO_HPO.ipynb: Generator functions'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.19 EDL_5_4_PSO_HPO.ipynb：生成器函数
- en: '[PRE18]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Clamps the value between the min/max range
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将值夹在最小/最大范围内
- en: ❷ Sets i to input the value yield
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将i设置为输入值yield
- en: ❸ Linear interpolates the value
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 线性插值值
- en: ❹ Returns a static value
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回一个静态值
- en: Now, we can look at how this works in the following cell, where we instantiate
    the class and create a child object. Again, the creation of the parent hyperparameter
    object is done by passing in generators for each hyperparameter we want to track.
    After that, a simple `individual` is defined using a list of values in the range
    of `-1.0` to `1.0`, where each value represents the index in the linear space
    defined by setting the min/max values to the generator. This time, when we call
    `next` on the hyperparameter parent object, we get back a child object that is
    defined by the indexed values from the input `individual`, as shown in the following
    listing.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看以下单元格中如何实现这一功能，其中我们实例化类并创建一个子对象。再次强调，创建父超参数对象是通过传递我们想要跟踪的每个超参数的生成器来完成的。之后，使用范围在`-1.0`到`1.0`之间的值列表定义了一个简单的`individual`，其中每个值代表由设置最小/最大值到生成器定义的线性空间中的索引。这次，当我们对超参数父对象调用`next`时，我们得到一个由输入`individual`的索引值定义的子对象，如下所示。
- en: 'Listing 5.20 EDL_5_4_PSO_HPO.ipynb: Creating a parent hyperparameter object'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.20 EDL_5_4_PSO_HPO.ipynb：创建父超参数对象
- en: '[PRE19]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The linespace generator defined with min/max values
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用最小/最大值定义的线空间生成器
- en: ❷ The static generators configured
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 配置了静态生成器
- en: ❸ Creates a size 4 vector to represent the individual
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个大小为4的向量来表示个体
- en: ❹ Calls next to create a new hyperparameter child
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 调用next创建一个新的超参数子对象
- en: The bulk of the PSO setup and operations code is borrowed from the EDL_4_PSO.ipynb
    notebook covered in chapter 4\. Our focus here is on the configuration of the
    `toolbox` and `evaluate` function. In this code, we set the size of `particle`
    based on the `hp.size` or number of hyperparameters we want to track. Next, we
    reduce the `pmax`/`pmin` and `smin`/`smax` values to accommodate a smaller search
    space. Be sure to alter these values on your own to see what effect this has on
    HPO. At the end of the code in the following listing, we can see the registration
    of the `evaluate` function, where the `fitness` of each `particle` is evaluated.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: PSO设置和操作代码的大部分是从第4章中介绍的EDL_4_PSO.ipynb笔记本借用的。我们在这里的重点是`toolbox`和`evaluate`函数的配置。在这段代码中，我们根据`hp.size`或我们想要跟踪的超参数数量设置`particle`的大小。接下来，我们减小`pmax`/`pmin`和`smin`/`smax`的值以适应更小的搜索空间。务必根据您自己的情况修改这些值以查看这对HPO的影响。在以下列表中的代码末尾，我们可以看到`evaluate`函数的注册，其中评估了每个`particle`的`fitness`。
- en: 'Listing 5.21 EDL_5_4_PSO_HPO.ipynb: Creating a parent hyperparameters object'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.21 EDL_5_4_PSO_HPO.ipynb：创建父超参数对象
- en: '[PRE20]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ The size of particle, defined by number of hyperparameters
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 粒子的大小，由超参数数量定义
- en: ❷ Configures the particle search space
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 配置粒子搜索空间
- en: ❸ Registers the evaluate function
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 注册评估函数
- en: The `evaluate` function now needs to call the `train_function` by passing in
    a child hyperparameter object. Notice how this is slightly different from the
    way we previously called the network training function. This time, we generate
    a child hyperparameter object by calling `next` on the parent by passing in the
    `individual`. Then, the child hyperparameter is input into the `train_function`
    to generate the output. To get a full evaluation, we check the model loss on the
    entire dataset and then return this as the `fitness`, as shown in the following
    listing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的`evaluate`函数需要通过传递子超参数对象来调用`train_function`。注意这与我们之前调用网络训练函数的方式略有不同。这次，我们通过在父对象上调用`next`并传递`individual`来生成子超参数对象。然后，将子超参数输入到`train_function`中以生成输出。为了获得完整的评估，我们检查整个数据集上的模型损失，然后将此作为`fitness`返回，如以下列表所示。
- en: 'Listing 5.22 EDL_5_4_PSO_HPO.ipynb: Creating a parent hyperparameter object'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.22 EDL_5_4_PSO_HPO.ipynb：创建父超参数对象
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Generates the child hyperparameter by passing in an individual
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过传递个体生成子超参数
- en: ❷ Calls train by passing in the child hyperparameter
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过传递子超参数调用训练
- en: ❸ Predicts full model loss
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 预测完整模型损失
- en: ❹ Returns the fitness
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回`fitness`
- en: We can now move on to the last code block, shown in listing 5.23, and examine
    how the particle swarming works with our changes. The changes have been marked
    in bold and are added to better track the `particle` `fitness` and associated
    hyperparameters. After `evaluate` is called on the part `particle`, we call `hp.next(part)`
    to create a child copy. This isn’t required for PSO to function, but it helps
    us track `particle` history.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续到最后一个代码块，如图5.23所示，并检查粒子群集是如何与我们的更改一起工作的。更改已被加粗，并添加以更好地跟踪`particle` `fitness`和相关的超参数。在调用`evaluate`函数对部分`particle`进行评估后，我们调用`hp.next(part)`来创建一个子副本。这不是PSO功能所必需的，但它有助于我们跟踪`particle`历史。
- en: 'Listing 5.23 EDL_5_4_PSO_HPO.ipynb: Creating a parent hyperparameter object'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.23 EDL_5_4_PSO_HPO.ipynb：创建父超参数对象
- en: '[PRE22]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Captures the hyperparameter child object
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 捕获超参数子对象
- en: ❷ Appends values to the run history
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将值追加到运行历史记录中
- en: ❸ Captures the best fitness and hyperparameter objects
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 捕获最佳`fitness`和超参数对象
- en: Figure 5.6 is a capture of the final output from applying PSO to perform HPO
    over 10 iterations of swarming. You can clearly see in the far-left plot, the
    `fitness` evaluation plot, how the `particles` converged around a predicted best
    solution. Notice how the final best `fitness`, at around 34,000, was a better
    result than our grid search implementation. What’s more, PSO was able to achieve
    this in a fraction of the time of grid search.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6是应用PSO进行10次迭代群集优化后的最终输出截图。你可以在最左边的图中清楚地看到，`fitness`评估图显示了`particles`如何围绕一个预测的最佳解收敛。注意，最终的最佳`fitness`值，大约为34,000，比我们的网格搜索实现的结果要好。更重要的是，PSO能够在网格搜索的一小部分时间内完成这一点。
- en: The results in figure 5.11 look quite impressive compared to our previous random
    and grid search examples. However, PSO is not without its own problems, and while
    it appears to be performing better than grid search, it may not always do so.
    Furthermore, PSO is tightly defined by the parameters `smin`/`smax` and `pmin`/`pmax`,
    and adjusting these values correctly often requires careful thought or trial and
    error.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前的随机和网格搜索示例相比，图5.11的结果看起来相当令人印象深刻。然而，PSO并非没有自己的问题，尽管它似乎在性能上优于网格搜索，但它并不总是如此。此外，PSO的参数由`smin`/`smax`和`pmin`/`pmax`紧密定义，正确调整这些值通常需要仔细思考或试错。
- en: '![](../Images/CH05_F11_Lanham.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F11_Lanham.png)'
- en: Figure 5.11 Output from using PSO for HPO
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 使用PSO进行HPO的输出
- en: Reviewing the third subplot in figure 5.10, we can see how PSO converges to
    a region that it then swarms `particles` over to find better optimum solutions.
    The problem with this approach is that a swarm often gets stuck on a local minimum
    or maximum trying to find a global min/max value within the swarmed area. If no
    such global value is present in the area, then the swarm becomes stuck on a local
    min/max value.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图5.10的第三个子图，我们可以看到PSO如何收敛到一个区域，然后在该区域上群集`particles`以找到更好的最优解。这种方法的问题在于，群集往往会在寻找群集区域内的全局最小/最大值时陷入局部最小/最大值。如果该区域内没有这样的全局值，那么群集就会陷入局部最小/最大值。
- en: Considering this potential dilemma with PSO, we can look to other EC methods
    that can perform HPO better and avoid or minimize this problem. As we saw in chapter
    4, there are a few advanced EC methods that may help us overcome these concerns.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到PSO的这种潜在困境，我们可以考虑其他可以更好地执行HPO并避免或最小化这种问题的EC方法。正如我们在第四章中看到的，有一些高级EC方法可能有助于我们克服这些担忧。
- en: 5.5 Genetic algorithms and evolutionary strategies for HPO
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 针对HPO的遗传算法和进化策略
- en: We spent some time understanding how GA works in chapter 3 and then expanded
    on these concepts when we employed ES in chapter 4\. If you recall, ES is a specialized
    form of GA that applies a strategy to improve genetic operators, like mutation.
    We continue to use the same mutation strategy with GA and ES for the purposes
    of automatic HPO.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第三章花了一些时间来理解遗传算法（GA）的工作原理，然后在第四章中当我们采用进化策略（ES）时，对这些概念进行了扩展。如果你还记得，ES是一种特殊的GA，它应用策略来改进遗传操作，如突变。我们继续使用相同的突变策略与GA和ES进行自动HPO。
- en: 5.5.1 Applying evolutionary strategies to HPO
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 将进化策略应用于HPO
- en: In chapter 4, we looked at how to apply evolutionary strategies as an additional
    vector that controls the rate and application of mutation. By controlling mutation
    in this manner, we can better focus a whole `population` to arrive at a solution
    more quickly. In our next project, we employ ES and a means to perform automatic
    HPO.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四章中，我们探讨了如何将进化策略作为一个额外的向量来控制突变的速度和应用。通过这种方式控制突变，我们可以更好地集中整个`population`以更快地到达解决方案。在我们的下一个项目中，我们将采用ES和一种执行自动HPO的方法。
- en: Open notebook EDL_5_5_ES_HPO.ipynb in Colab, and run all the cells via Runtime
    > Run All. This notebook is based on EDL_4_4_ES.ipynb and borrows much of the
    code from that example. We also borrow several pieces from the last exercise to
    build this sample, which means this code likely looks familiar.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_5_5_ES_HPO.ipynb笔记本，并通过“运行”>“运行所有单元格”来运行所有单元格。这个笔记本基于EDL_4_4_ES.ipynb，并从该示例中借用了很多代码。我们还从上一个练习中借用了一些内容来构建这个示例，这意味着这段代码可能看起来很熟悉。
- en: We focus on the first difference by reviewing the ES hyperparameters. The first
    modification is setting the `IND_SIZE` variable to the number of hyperparameters.
    Then, we alter the `MAX_STRATEGY` to 5 to account for the larger search space,
    as shown in the following listing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过回顾ES超参数来关注第一个差异。第一个修改是将`IND_SIZE`变量设置为超参数的数量。然后，我们将`MAX_STRATEGY`更改为5，以适应更大的搜索空间，如下列表所示。
- en: 'Listing 5.24 EDL_5_5_ES_HPO.ipynb: Setting ES hyperparameters'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.24 EDL_5_5_ES_HPO.ipynb：设置ES超参数
- en: '[PRE23]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Sets the individual size to the number of hyperparameters
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将个体大小设置为超参数的数量
- en: ❷ Increases the max strategy to account for a wider search space
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 增加最大策略以适应更宽的搜索空间
- en: Next, we jump down to the block of code that sets up the `toolbox`, as shown
    in listing 5.25\. The only key changes we make here are modifying a couple of
    the hyperparameters, the alpha used in the `mate` operator, and the probability
    of mutation. Recall that alpha denotes the size of the blending that occurs between
    parents, rather than straight `crossover`.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们跳转到设置`toolbox`的代码块，如列表5.25所示。我们在这里做的唯一关键更改是修改了几个超参数，即`mate`操作符中使用的alpha和突变的概率。回想一下，alpha表示父母之间混合的大小，而不是直接的`crossover`。
- en: 'Listing 5.25 EDL_5_5_ES_HPO.ipynb: Creating the `toolbox`'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.25 EDL_5_5_ES_HPO.ipynb：创建`toolbox`
- en: '[PRE24]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Increases the alpha mask size
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 增加alpha掩码的大小
- en: ❷ Increases the probability of mutation occurring
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 增加突变发生的概率
- en: Lastly, we can look at the evolution code, shown in the following listing, to
    see how the `population` evolves to a solution.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以查看以下列表中的进化代码，以了解如何使`population`进化到解决方案。
- en: 'Listing 5.26 EDL_5_5_ES_HPO.ipynb: Creating the `toolbox`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.26 EDL_5_5_ES_HPO.ipynb：创建`toolbox`
- en: '[PRE25]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Uses the algorithms function to evolve for a single iteration
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用算法函数进行单次迭代进化
- en: ❷ Runs training again with best from evolution
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 再次使用进化中的最佳结果进行训练
- en: ❸ Predicts the output from model
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从模型预测输出
- en: ❹ Appends the fitness and hyperparameter child to the results
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将适应度和超参数子项添加到结果中
- en: Figure 5.12 shows the final output of running ES to perform HPO. Notice how
    the third plot on the end, the `fitness` evaluation, shows a much tighter concentration.
    This concentration is tighter than the PSO, and ES suffers some of the same problems.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12显示了运行ES进行HPO的最终输出。注意最后一个图表，即`fitness`评估，显示了更紧密的集中。这种集中比PSO更紧密，ES也遇到了一些相同的问题。
- en: '![](../Images/CH05_F12_Lanham.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F12_Lanham.png)'
- en: Figure 5.12 Example output of ES on HPO
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 ES在HPO上的示例输出
- en: With PSO, we have seen a problem with a swarm of `particles` getting stuck on
    a local minima or maxima. While this appears to be a similar problem with ES,
    it is important to note that the convergence is quicker and more focused. Adding
    a larger `population` can reduce or help ES identify global minimums more regularly.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PSO 中，我们看到了一群 `粒子` 卡在局部最小值或最大值上的问题。虽然这似乎与 ES 中的问题相似，但重要的是要注意，收敛速度更快，更集中。增加更大的
    `种群` 可以减少或帮助 ES 更频繁地识别全局最小值。
- en: 5.5.2 Expanding dimensions with principal component analysis
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 使用主成分分析扩展维度
- en: Up until now, we have been testing various methods across just two hyperparameters—the
    learning rate and batch size—to make visualizing results in two dimensions easier.
    If we want to visualize more hyperparameters in higher dimensions, we need to
    reduce the number of dimensions to two or three for visualization. Fortunately,
    there is a simple technique we can apply to visualize higher-dimensional output
    in two dimensions, called *principal component analysis* (PCA).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在测试仅跨越两个超参数（学习率和批量大小）的各种方法，以使在二维中可视化结果更容易。如果我们想在更高维中可视化更多超参数，我们需要将维度减少到二维或三维以进行可视化。幸运的是，有一个简单的技术我们可以应用，以在二维中可视化高维输出，称为
    *主成分分析* (PCA)。
- en: PCA is the process of reducing multi-dimensional vector data from higher to
    lower dimensions. In our example, we reduce four-dimensional outputs into two
    dimensions for visualization. You can think of the process as a projection from
    higher dimensions into lower dimensions. We show how this works and applies to
    visualizing HPO in the next exercise.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 是将多维向量数据从高维降低到低维的过程。在我们的例子中，我们将四维输出降低到二维以进行可视化。你可以将这个过程视为从高维到低维的投影。我们将在下一个练习中展示它是如何工作的以及如何应用于可视化
    HPO。
- en: Open notebook EDL_5_5_ES_HPO_PCA.ipynb in Colab, and then run all the cells
    via Runtime > Run All. The variation of EDL_5_5_ES_HPO.ipynb adds PCA, so we can
    automate additional hyperparameters and still visualize results in 2D.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Colab 中打开 Open notebook EDL_5_5_ES_HPO_PCA.ipynb，然后通过运行 > 运行所有单元格来运行所有单元格。EDL_5_5_ES_HPO.ipynb
    的变体添加了 PCA，因此我们可以自动化额外的超参数，并且仍然可以在二维中可视化结果。
- en: Most of the code is the same, but we focus on one cell that demonstrates setting
    up PCA and plotting some multidimensional output in 2D. scikit-learn provides
    a PCA class that can easily apply the transformation of data from higher dimensions
    into simpler component outputs. In the following code listing, we reduce example
    `individual` objects from four dimensions to two components.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码都是相同的，但我们专注于一个单元格，该单元格演示了设置 PCA 并在二维中绘制一些多维输出。scikit-learn 提供了一个 PCA 类，可以轻松地将数据从高维转换到更简单的组件输出。在下面的代码列表中，我们将示例
    `个体` 对象从四维降低到两个组件。
- en: 'Listing 5.27 EDL_5_5_ES_HPO_PCA.ipynb: Adding PCA'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.27 EDL_5_5_ES_HPO_PCA.ipynb：添加 PCA
- en: '[PRE26]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Creates a sample pop of individuals
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个样本个体种群
- en: ❷ Creates a PCA object with two dimensions
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个具有两个维度的 PCA 对象
- en: ❸ Fits the data
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 拟合数据
- en: ❹ Transposes the results into a new vector
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将结果转置到新的向量中
- en: ❺ Plots the output in two dimensions
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在二维中绘制输出
- en: Figure 5.13 shows the example output from listing 5.27 and the application of
    PCA to the contrived `population` data. It is important to understand that each
    axis is a component that represents a distance between elements in the vector
    space. PCA output is calculated by measuring the variance, or differences, between
    elements and generates components or an axis, along which each element falls.
    It’s important to understand that PCA plots are relative to the data being visualized.
    If you need or want to understand more about the PCA algorithm, be sure to check
    out the sklearn documentation or other online resources.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 显示了列表 5.27 的示例输出以及 PCA 在虚构的 `种群` 数据中的应用。重要的是要理解每个轴都是一个组件，它代表向量空间中元素之间的距离。PCA
    输出是通过测量元素之间的方差或差异来计算的，生成组件或轴，每个元素都落在这个轴上。重要的是要理解 PCA 图是相对于正在可视化的数据而言的。如果你需要或想要了解更多关于
    PCA 算法的知识，请务必查看 sklearn 文档或其他在线资源。
- en: '![](../Images/CH05_F13_Lanham.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F13_Lanham.png)'
- en: Figure 5.13 A PCA plot of four dimension points in component space
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 组件空间中四个维度点的 PCA 图
- en: With the ability to visualize data points with more than two dimensions, we
    can also extend our `hyperparameter` object to vary additional inputs. We now
    add the `batch_ size` and `epochs` as hyperparameters to vary, as shown in listing
    5.28\. Consider, for a moment, adding these two extra hyperparameters to a grid
    search problem. If we assume we want each hyperparameter to range across 10 cells
    or steps, then with 4 inputs, our search space would equal 10 x 10 x 10 x 10 =
    10,000, or 10,000 cells. Think back to our random search example that was set
    up for 10,000 runs and took over 12 hours to complete. This is the same amount
    of time it would take to perform a grid search over this same four-dimensional
    space.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们能够可视化超过两个维度的数据点，我们还可以将我们的`hyperparameter`对象扩展到变化额外的输入。我们现在将`batch size`和`epochs`添加为要变化的超参数，如列表5.28所示。暂时考虑将这两个额外超参数添加到网格搜索问题中。如果我们假设每个超参数的范围跨越10个单元格或步骤，那么有4个输入，我们的搜索空间将等于10
    x 10 x 10 x 10 = 10,000，或10,000个单元格。回想一下，我们的随机搜索例子是为10,000次运行设置的，并且花费了超过12小时来完成。这同样是执行相同四维空间网格搜索所需的时间。
- en: 'Listing 5.28 EDL_5_5_ES_HPO_PCA.ipynb: Adding hyperparameters'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.28 EDL_5_5_ES_HPO_PCA.ipynb：添加超参数
- en: '[PRE27]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Varies the batch size
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 改变批量大小
- en: ❷ Varies the number of epochs
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 改变epoch的数量
- en: The only other code change we need to apply is modifying the evaluation function
    output plot, as shown in listing 5.29\. We can borrow from the code in listing
    5.27 to apply the same process of reducing the hyperparameter outputs from the
    run history into two components. Then, we plot the transpose of these components
    into two-dimensional space using the `hexbins` function.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的唯一其他代码更改是修改评估函数输出图，如列表5.29所示。我们可以借鉴列表5.27中的代码来应用相同的过程，将运行历史中的超参数输出减少到两个组件。然后，我们使用`hexbins`函数将这些组件的转置绘制到二维空间中。
- en: 'Listing 5.29 EDL_5_5_ES_HPO_PCA.ipynb: Code to plot `fitness` evaluation'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.29 EDL_5_5_ES_HPO_PCA.ipynb：绘制`fitness`评估的代码
- en: '[PRE28]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Extracts the hyperparameters from the run history
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从运行历史中提取超参数
- en: ❷ Outputs the PCA components
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输出PCA成分
- en: Figure 5.14 shows the output of an ES being applied to HPO, with the third plot
    now composed of PCA components. This plot allows us to visualize the search for
    optimal hyperparameters across multiple dimensions. We can still see some clustering
    over the best solution, but it is also apparent that other points are now more
    spread out. Also notice how much the `fitness` has improved over our earlier examples,
    which can be attributed to the variation of the additional hyperparameters.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14显示了将ES应用于HPO的输出，其中第三个图现在由PCA成分组成。这个图使我们能够可视化跨多个维度的最优超参数的搜索。我们仍然可以看到一些最佳解决方案的聚类，但很明显，其他点现在分布得更广。同时，请注意`fitness`与我们的早期例子相比有了多大的提高，这可以归因于额外超参数的变化。
- en: '![](../Images/CH05_F14_Lanham.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F14_Lanham.png)'
- en: Figure 5.14 The output of ES HPO with PCA
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 带PCA的ES HPO输出
- en: The differences between our previous examples when only varying two hyperparameters
    are relatively minor. By adding additional hyperparameters, and thus dimensions,
    to the search space, we can see a clear increase in performance between ES and
    grid or random search. Remember, however, that ES is still susceptible to becoming
    stuck over local minima, suggesting we need to look at alternative methods.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前仅改变两个超参数时的例子之间的差异相对较小。通过向搜索空间添加额外的超参数，从而增加维度，我们可以看到ES与网格搜索或随机搜索之间的性能明显提高。然而，请记住，ES仍然容易陷入局部最小值，这表明我们需要考虑替代方法。
- en: 5.6 Differential evolution for HPO
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 用于HPO的差分进化
- en: We saw the power of DE at the end of chapter 4 when we employed this method
    to solve discontinuous solutions over ES. Given the unique method DE uses to evolve
    solutions, it follows that it would make a good candidate to automate HPO. DE
    could also likely overcome the sticking condition we observed in both PSO and
    ES.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章末尾，当我们使用这种方法在ES上解决不连续解时，我们看到了DE（差分进化）的力量。鉴于DE用于进化解决方案的独特方法，它自然成为自动化HPO的良好候选者。DE也可能克服我们在PSO和ES中观察到的粘滞条件。
- en: 5.6.1 Differential search for evolving HPO
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 用于演化的HPO的微分搜索
- en: 'Differential evolution employs a simple iterative algorithm that samples three
    random individuals from a `population`. It then takes the difference between two
    of those individuals and adds a scaled value to the third one. The result produces
    a fourth point: the next search area to target.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 差分进化使用一个简单的迭代算法，从种群中随机选择三个个体。然后，它计算两个个体之间的差异，并将一个缩放值加到第三个个体上。结果产生第四个点：下一个搜索目标区域。
- en: Figure 5.15 depicts a single evaluation of the DE algorithm in two dimensions.
    In the figure, three points (A, B, C) have been randomly sampled from a `population`.
    A difference vector from A to B (A – B) is calculated and then passed through
    a scaling function F. In most cases, we keep things simple by multiplying by a
    scale value of 1.0\. Then, we add the scaled difference vector to the third point
    to create a new target search point.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15展示了差分进化算法在二维空间中的单个评估。在图中，三个点（A、B、C）是从种群中随机抽取的。计算从A到B的差异向量（A – B），然后通过缩放函数F传递。在大多数情况下，我们通过乘以1.0的缩放值来保持简单。然后，我们将缩放后的差异向量加到第三个点上，以创建一个新的目标搜索点。
- en: '![](../Images/CH05_F15_Lanham.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F15_Lanham.png)'
- en: Figure 5.15 Differential evolution search
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 差分进化搜索
- en: The search mechanism employed by DE is flexible enough to break free of the
    swarm or clustering problems we saw earlier with PSO and ES. Of course, we want
    to see DE in action to confirm for ourselves this is a better method for HPO.
    In the next exercise, we continue to solve the same problem using DE over optimization
    of four hyperparameters.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 差分进化使用的搜索机制足够灵活，可以摆脱我们之前在PSO和ES中看到的群或聚类问题。当然，我们希望看到DE的实际应用来确认这是我们进行超参数优化（HPO）的更好方法。在下一个练习中，我们继续使用DE在优化四个超参数的同一问题上进行求解。
- en: Open notebook EDL_5_ES_HPO_PCA.ipynb in Colab, and then run all the cells via
    Runtime > Run All. If you like, you can also review a non-PCA version of this
    notebook running on just two dimensions by exploring EDL_5_ES_HPO.ipynb. We have
    seen all the code in this exercise previously, so we just revisit the parts that
    make it unique here, starting with the hyperparameters in the following listing.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab中打开EDL_5_ES_HPO_PCA.ipynb笔记本，然后通过运行 > 运行所有单元格来运行所有单元格。如果您愿意，您还可以通过探索EDL_5_ES_HPO.ipynb来查看仅运行在两个维度上的此笔记本的非PCA版本。我们之前已经看到了这个练习中的所有代码，所以我们只需回顾使其独特的部分，从以下列表中的超参数开始。
- en: 'Listing 5.30 EDL_5_6_DE_HPO_PCA.ipynb: Set up the `creator` and `toolbox`'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.30 EDL_5_6_DE_HPO_PCA.ipynb：设置`creator`和`toolbox`
- en: '[PRE29]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ The number of hyperparameter dimensions
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 超参数维度的数量
- en: ❷ The crossover rate
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 交叉率
- en: ❸ The scale factor/function
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 缩放因子/函数
- en: ❹ The total population
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 总种群
- en: Next, we revisit the DEAP code that sets up the `creator` and `toolbox`—we have
    covered everything in this code previously. Notice the use of the `NDIM` value
    in the `individual` registration to set the size in the following listing. On
    the final line, we can select the registration to be set to a random `selection`
    operator that outputs three elements, `k=3`.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们再次回顾设置`creator`和`toolbox`的DEAP代码——我们之前已经覆盖了这段代码中的所有内容。注意在`individual`注册中使用`NDIM`值来设置以下列表中的大小。在最后一行，我们可以选择将注册设置为输出三个元素的随机`选择`操作符，`k=3`。
- en: 'Listing 5.31 EDL_5_6_DE_HPO_PCA.ipynb: Setting up the `creator` and `toolbox`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.31 EDL_5_6_DE_HPO_PCA.ipynb：设置`creator`和`toolbox`
- en: '[PRE30]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Creates an individual with an equal size of hyperparameter dimensions
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个具有与超参数维度相等大小的个体
- en: ❷ Registers a random select function of size 3
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 注册一个大小为3的随机选择函数
- en: The only other code of interest is in the evolution section. This is code we
    already reviewed in chapter 4, but it is worth reviewing again. We call `individual`
    objects in DE, agents, since they have long lives like `particles` but evolve
    like agents. Notice the highlighted line, where the difference scaled vector is
    calculated and applied to single components of vector `y`. This calculation is
    guaranteed to only occur once for each randomly sampled index that matches the
    current vector component. However, the `crossover` rate does provide the opportunity
    to alter other component values to create a new `y`, as shown in the following
    listing.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 另外一个值得关注的代码是在进化部分。这是我们在第4章中已经审查过的代码，但值得再次审查。在差分进化中，我们称`individual`对象为代理，因为它们像`粒子`一样有很长的生命周期，但像代理一样进化。注意高亮显示的行，其中计算并应用了缩放差异向量到向量`y`的单个分量上。这个计算保证对于每个随机抽取的与当前向量分量匹配的索引只会发生一次。然而，`交叉率`确实提供了改变其他分量值以创建新的`y`的机会，如下面的列表所示。
- en: 'Listing 5.32 EDL_5_6_DE_HPO_PCA.ipynb: Evolving differential evolution'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.32 EDL_5_6_DE_HPO_PCA.ipynb：进化差分进化
- en: '[PRE31]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Enumerates over the population
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历种群
- en: ❷ Selects three agents
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择三个代理
- en: ❸ Finds a random index
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 找到一个随机索引
- en: ❹ Checks if there is crossover
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 检查是否存在交叉
- en: ❺ Applies the scaled vector function
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 应用缩放向量函数
- en: Figure 5.16 shows the final output of 10 `generations`, using DE to solve HPO
    for our target problem. Specifically, note the third evaluation plot and how the
    points are not clustered at all. Also notice that the best `fitness` generated
    from this method is around 81, a number that clearly exceeds any of our other
    previous attempts.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 显示了使用 DE 解决目标问题的 HPO 的最终输出，使用了 10 代。具体来说，注意第三个评估图，以及点并没有聚集在一起。此外，注意从这个方法生成的最佳“适应度”大约为
    81，这个数字明显超过了我们之前的所有其他尝试。
- en: '![](../Images/CH05_F16_Lanham.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F16_Lanham.png)'
- en: Figure 5.16 An example output of DE for HPO
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 DE 用于 HPO 的一个示例输出
- en: As we can see, the application of DE to HPO appears to provide an excellent
    mechanism that avoids the local minima sticking problem observed with PSO and
    ES. We can make a comparison of the three techniques by upgrading the PSO example
    to use PCA, as demonstrated in the EDL_5_4_PSO_HPO_PCA.ipynb notebook. Feel free
    to run that notebook on your own to observe the differences between PSO, ES, and
    DE.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，将 DE 应用于 HPO 似乎提供了一个避免 PSO 和 ES 中观察到的局部最小值粘滞问题的优秀机制。我们可以通过将 PSO 示例升级为使用
    PCA 来比较三种技术，如 EDL_5_4_PSO_HPO_PCA.ipynb 笔记本中所示。请随意运行该笔记本，以观察 PSO、ES 和 DE 之间的差异。
- en: Figure 5.17 shows a comparison of the evaluation plots from PSO, ES, and DE.
    Notice how the PSO produces a wide swarm of `particles`, roughly centered on what
    it expects to be the best solution. Likewise, ES produces a much tighter cluster
    of attempts, but the distribution is more of a narrow band across the output.
    We can clearly see with the DE plot that this algorithm is well suited to explore
    boundaries and avoids getting stuck over local minima.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 显示了 PSO、ES 和 DE 的评估图比较。注意 PSO 产生了一个广泛的“粒子”群，大致集中在它期望的最佳解附近。同样，ES 产生了一个更紧密的尝试集群，但分布更像是输出上的一个窄带。我们可以清楚地看到，DE
    图表明该算法非常适合探索边界，并避免陷入局部最小值。
- en: '![](../Images/CH05_F17_Lanham.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH05_F17_Lanham.png)'
- en: Figure 5.17 Comparison of EC methods for HPO
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 HPO 的 EC 方法比较
- en: This has been our first chapter applying the principles of EDL through the integration
    of EC with DL for HPO. We started off exploring basic random search and then moved
    up to grid search to set a baseline for the other methods. From there, we expanded
    to applying EC with PSO, ES, and, finally, DE.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一次通过将 EC 与 DL 集成来应用 EDL 原则的章节。我们一开始探索了基本的随机搜索，然后提升到网格搜索，为其他方法设定基准。从那里，我们扩展到应用
    PSO、ES，最终是 DE。
- en: Through our exploration of techniques in this chapter, it should be obvious
    now that EC methods have definitive applications to DL. As we explore in the rest
    of this book, these and other techniques can be applied for the improvement of
    DL. For now, though, let’s finish out the chapter with a summary of what we learned.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章对技术的探索，现在应该很明显，EC 方法在 DL 中有明确的应用。正如我们在本书的其余部分所探讨的，这些以及其他技术可以应用于 DL 的改进。然而，现在，让我们通过总结我们所学的内容来结束本章。
- en: Summary
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This was the first chapter in which we combined EC methods with DL, in our first
    exposure to EDL. Along the way, we learned several new techniques we could apply
    to PyTorch and, likely, other frameworks.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是第一个我们将 EC 方法与 DL 结合的章节，也是我们第一次接触 EDL。在这个过程中，我们学习了几个新技巧，我们可以将其应用于 PyTorch，以及其他框架。
- en: 'DL hyperparameter search (hyperparameter optimization HPO) requires extensive
    knowledge and experience to perform correctly:'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL 超参数搜索（超参数优化 HPO）要正确执行需要广泛的知识和经验：
- en: Knowledge of strategies to perform manual hyperparameter searches for various
    problems can be developed using basic rules and templates.
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基本规则和模板，可以开发出用于各种问题进行手动超参数搜索的策略。
- en: Writing a basic hyperparameter search tool can be demonstrated quickly with
    Python.
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 快速演示编写基本的超参数搜索工具。
- en: Random hyperparameter search is a search method that uses random sampling to
    generate results on a plot. By observing these random observations, a tuner can
    narrow the search down to specific areas of interest.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机超参数搜索是一种使用随机抽样在图表上生成结果的搜索方法。通过观察这些随机观察结果，调优器可以将搜索范围缩小到特定感兴趣的区域。
- en: Grid search is a method that maps hyperparameters to a grid of discrete values
    that are then evaluated in sequence. Visualizing the grid of results can assist
    the tuner in fine-tuning and selecting areas of specific interest for further
    tuning.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索是一种将超参数映射到离散值网格的方法，然后按顺序评估这些值。可视化结果网格可以帮助调优器进行微调和选择特定区域进行进一步调优。
- en: 'DEAP can quickly provide a variety of evolutionary methods to employ for HPO:'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DEAP可以快速提供多种进化方法用于超参数优化（HPO）：
- en: From GAs to DE, evolutionary hyperparameter search is often more efficient than
    grid or random search.
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从遗传算法（GAs）到差分进化（DE），进化超参数搜索通常比网格搜索或随机搜索更有效。
- en: For complex multidimensional hyperparameter optimizations, we can visualize
    the differences between various search forms by using dimensionality reduction
    techniques to produce 2D plots.
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于复杂的多维超参数优化，我们可以通过使用降维技术来生成二维图，以可视化各种搜索形式之间的差异。
- en: PCA is a good dimensionality reduction technique for visualization HPO.
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是可视化超参数优化（HPO）的良好降维技术。
- en: PSO is a great method to use on problems with relatively few hyperparameters.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 粒子群优化（PSO）是处理相对较少超参数问题的优秀方法。
- en: Differential evolution is ideal for more methodical and efficient searching
    of hyperparameters to avoid local minima clustering. Always evaluate the key differences
    between various search forms and understand when to employ which and when.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差分进化非常适合更系统化和高效地搜索超参数，以避免局部最小值聚类。始终评估各种搜索形式之间的关键差异，并理解何时使用哪种方法以及何时使用。
