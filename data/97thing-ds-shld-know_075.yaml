- en: Chapter 70\. Toward Algorithmic Humility
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第70章 向算法谦卑迈进
- en: Marc Faddoul
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Marc Faddoul
- en: '![](Images/marc_faddoul.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/marc_faddoul.png)'
- en: Research Associate, School of Information, UC Berkeley
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学伯克利分校信息学院研究助理
- en: 'Defendant #3172 is an unmarried 22-year-old female. She previously served two
    months in prison for marijuana trafficking and has just been arrested for engaging
    in a violent public altercation with her partner. Is the defendant going to commit
    a violent crime in the three-month period before the trial? To answer such a question,
    many American jurisdictions use algorithmic systems known as pretrial risk assessment
    tools. Let’s consider one of the most common of these tools, the Public Safety
    Assessment (PSA).^([1](ch70.xhtml#ch01fn53))'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '被告 #3172 是一名未婚的22岁女性。她曾因大麻贩卖入狱两个月，并刚刚因与伴侣在公共场合发生激烈冲突而被捕。在审判前三个月内，被告会不会再次犯下暴力犯罪？要回答这样的问题，许多美国司法管辖区使用称为预审风险评估工具的算法系统。让我们考虑其中最常见的工具之一，公共安全评估（PSA）。'
- en: When the PSA sees a high risk, it raises a red flag, and this automatically
    sends the defendant into detention, without further consideration from the judge
    to challenge the machine’s prediction. The stakes are high, as pretrial detention
    often comes with devastating consequences for the job and housing security of
    defendants, including those who are later proven innocent at trial. *Tragically,
    97% of these life-wrecking algorithmic red flags are in fact false alarms.*^([2](ch70.xhtml#ch01fn54))
    In other words, 3% of flagged defendants would have actually committed a violent
    crime had they been released, while the other 97% were detained unnecessarily.
    This is a strikingly poor performance, but it is somewhat unsurprising.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当 PSA 看到高风险时，它会引发警示，这会自动将被告送入拘留，而不再考虑法官对机器预测的挑战。风险很高，因为预审拘留通常会对被告的工作和住房安全造成毁灭性后果，包括那些在审判中后来被证明无罪的人。*悲剧的是，这些毁掉生活的算法警示中，97%
    实际上都是误报。*^([2](ch70.xhtml#ch01fn54)) 换句话说，被标记的被告中，有3% 实际上会在释放后犯下暴力犯罪，而其他97% 则是不必要的拘留。这是一个惊人的糟糕表现，但也不足为奇。
- en: Foreseeing a crime in the near future is hard, and machines are not oracles.
    They could, however, contribute valuable predictive clues, but only with the intellectual
    humility required for this task. Deplorably, the PSA was designed as an arrogant
    algorithm.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 预见未来犯罪是困难的，机器不是神谕。然而，它们可以提供有价值的预测线索，但前提是要有必要的智慧谦卑。遗憾的是，PSA 被设计成一种傲慢的算法。
- en: 'If defendants were systematically released before their trial, about 1% of
    them would commit a violent crime. By comparing the profiles of those offenders
    and that of a given defendant, the PSA does three times better than a random guess:
    it puts *only* 33 defendants in jail for every offender it detects.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '如果被告在审判前被系统性地释放，大约有1% 的被告会犯下暴力犯罪。通过比较这些罪犯的档案和特定被告的档案，PSA 的表现比随机猜测好三倍：它仅仅对每个检测到的罪犯关押了33名被告。 '
- en: Indeed, the limited demographic and judicial data available is vastly insufficient
    to predict a crime. Information about mental health or housing stability would
    be more predictive, but also hard to collect in a fair and systematic way. Even
    then, some randomness would remain. Two people who look identical to the algorithm
    can end up in different situations and react with different decisions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，有限的人口统计和司法数据远远不足以预测犯罪。有关心理健康或住房稳定性的信息可能更具预测性，但也很难以公正和系统的方式收集。即便如此，仍会存在一些随机性。两个对算法看起来完全相同的人可能最终处于不同的情况，并做出不同的决定。
- en: Moreover, the algorithm cannot learn from its mistakes. When a particular defendant
    is detained, there is no way to know whether they actually would have committed
    a crime had they been released. Inherently, the predictive signal is weak. Therefore,
    the algorithm could never predict crimes with high accuracy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，算法无法从其错误中学习。当特定被告被拘留时，无法知道如果他们被释放是否真的会犯罪。从根本上说，预测信号很弱。因此，该算法永远无法高准确度地预测犯罪。
- en: Errors are inevitable, but that is not a problem per se. Like other empirical
    sciences, data science can deal with uncertainty by making probabilistic predictions
    and including confidence intervals. The PSA, however, makes predictions on an
    arbitrary scale, with no indication of confidence or error rates. Some justify
    this design by claiming that it is simpler for judges to read, that they can’t
    make sense of probabilities. This is arrogant thinking. The current system is
    not simple; it is simplistic, which makes it deceiving. The algorithm misleads
    judges in its raising of prescriptive red flags. If it gave actual probabilities
    instead, judges would know that a “high risk flag” actually means “a chance of
    about 3%.”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 错误是不可避免的，但这本身并不是问题。像其他经验科学一样，数据科学可以通过进行概率预测和包括置信区间来处理不确定性。然而，PSA（Public Safety
    Assessment，公共安全评估）则在任意刻度上进行预测，没有显示置信度或错误率的迹象。一些人为此设计辩护，声称这样更容易让法官理解，因为他们无法理解概率。这是一种傲慢的想法。当前系统并不简单；它是简单化的，这使它具有误导性。算法通过提升规范性红旗误导法官。如果它实际上给出了概率，法官就会知道“高风险标志”实际上意味着“大约有3%的机会”。
- en: This design also understates a judge’s ability to gain additional contextual
    information. For instance, a humble algorithm might point out that domestic violence
    is a likely risk scenario for a particular defendant, encouraging the judge to
    investigate that defendant’s current partnership. For certain profiles, the algorithm’s
    training records may be limited or inconsistent. If the statistical signal is
    not strong enough to make a decent prediction, a humble algorithm would withdraw
    to completely defer the decision to the judge.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计也低估了法官获取额外背景信息的能力。例如，一个谦逊的算法可能指出，对于特定被告来说，家庭暴力是一个可能的风险情景，鼓励法官调查该被告当前的伴侣关系。对于某些档案，算法的训练记录可能有限或不一致。如果统计信号不足以进行良好的预测，一个谦逊的算法会完全依赖法官做出决定。
- en: Algorithms and humans are the same in that their expertise is never complete
    or uniform. As such, they should acknowledge their limits. The point of using
    algorithms in court is to remove human biases. This is a legitimate concern, as
    judges can be laden with bias regarding race, gender, social classes, and even
    what they had for lunch.^([3](ch70.xhtml#ch01fn55))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 算法和人类一样，并非完全或均衡地具备专业知识。因此，它们应该承认自己的局限性。在法庭上使用算法的目的是消除人类的偏见。这是一个合理的担忧，因为法官可能存在对种族、性别、社会阶层甚至午餐选择的偏见^[3](ch70.xhtml#ch01fn55)。
- en: Algorithms are sometimes thought of as being purely objective, but aside from
    the lunch menu, they are not immune to bias. As with humans, their biases stem
    from inaccurate generalizations based on their limited and nonrepresentative experience
    of the world. When an algorithm is presented with American judicial records, it
    instantly induces that African Americans are more likely to commit crimes. America’s
    lingering legacy of racial discrimination, and the fact that the judicial system
    is structurally biased against African Americans, is irrelevant to the machine.
    Even if a defendant’s ethnicity is not provided, it can still be inferred from
    the defendant’s zip code or from the number of juvenile misdemeanors in the defendant’s
    record.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有时人们认为算法是纯客观的，但除了午餐菜单之外，它们并不免于偏见。与人类一样，它们的偏见源于对世界有限和非典型经验的不准确概括。当算法面对美国的司法记录时，它会立即得出结论，认为非裔美国人更有可能犯罪。美国残存的种族歧视遗产以及司法系统对非裔美国人的结构性偏见对这种机器来说并不重要。即使没有提供被告的种族信息，也可以从被告的邮政编码或被告犯罪记录中推断出来。
- en: 'To balance for this effect, algorithms can be designed to enforce a certain
    fairness proposition. For instance, error rates can be constrained to be equal
    across different ethnic groups. Concretely, this constraint defines an affirmative
    action policy: *How much more tolerant should the algorithm be when considering
    black defendants to offset for racial discriminations?* Algorithms are scattered
    with such parameters, which can tweak crucial trade-offs behind the curtains.
    Arrogant algorithms can be intentionally opaque to conceal policy decisions. The
    most fundamental parameter is this one: *How many innocents are we willing to
    put in jail in order to prevent one person from committing a violent crime?* The
    answer—“33 innocents in jail for every offender”—is hidden inside the code. Indeed,
    who could legitimately argue out loud that a 1:33 ratio is fair? That number was
    picked so that the algorithm would reproduce the incarceration rates of the current
    system. But this is the wrong approach: instead of discreetly automating preposterous
    judicial standards, algorithms should expose and challenge their assumptions.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了平衡这种影响，可以设计算法以实施某种公平主张。例如，可以将错误率限制在不同族裔群体之间相等。具体来说，这种限制定义了一项平权行动政策：*当考虑黑人被告以抵消种族歧视时，算法应该更加宽容多少？*
    算法散布有这样的参数，可以调整幕后的关键权衡。傲慢的算法可能有意地不透明，以掩盖政策决策。最基本的参数就是这个：*我们愿意为了防止一个人犯下暴力犯罪而将多少无辜人关进监狱？*
    答案是“为了每个罪犯，愿意让33个无辜人坐牢”，这个数字隐藏在代码内部。确实，谁能合理地大声辩论说，1:33的比率是公平的？这个数字被选择，以便算法能够复制当前系统的监禁率。但这种做法是错误的：算法不应悄悄地自动化荒谬的司法标准，而应暴露和挑战它们的假设。
- en: In the digital age, computer programs have become preeminent regulators of our
    liberties—hence the dictum “Code is Law.”^([4](ch70.xhtml#ch01fn56)) Algorithmic
    designs, training sets, error rates, and fairness propositions therefore should
    all be transparent, for opacity can be tyranny. Data science can provide precious
    insights to guide complex and consequential decisions. However, data science can
    be detrimental to decision making when it conceals the complexity of the underlying
    problem.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字时代，计算机程序已成为我们自由的主要调控者——因此有了“代码即法则”的说法。^([4](ch70.xhtml#ch01fn56)) 算法设计、训练集、错误率和公平主张都应该是透明的，因为不透明可能是暴政。数据科学可以提供宝贵的洞察力，指导复杂和重大的决策。然而，当数据科学掩盖底层问题的复杂性时，它可能对决策造成不利影响。
- en: 'The popular mythology around artificial intelligence overstates the power of
    prediction tools. If algorithms are to replace or support human expertise, they
    should behave not like mysterious referees with almighty veto power but more like
    wise advisors. The same diligence should be expected of both machines and human
    experts: justifying decisions, acknowledging blind spots, and being intellectually
    humble. In turn, humans should engage with algorithms critically, by employing
    that essential part of their cognition that artificial intelligence will forever
    lack: common sense.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 围绕人工智能的流行神话夸大了预测工具的能力。如果算法要取代或支持人类专业知识，它们不应该表现得像拥有无上否决权的神秘裁判，而应该更像明智的顾问。无论是机器还是人类专家，都应该具备同样的勤奋精神：证明决策的合理性，承认盲点，并保持谦逊。相应地，人类应该以批判的态度与算法交互，利用人工智能永远缺乏的基本认知部分：常识。
- en: ^([1](ch70.xhtml#ch01fn53-marker)) The PSA is used as an illustrative example,
    but the shortcomings depicted in this essay also apply to the other pretrial risk
    assessment tools used in American jurisdictions. The PSA is arguably one of the
    more ethically designed tools.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch70.xhtml#ch01fn53-marker)) PSA被用作举例，但在本文所描绘的缺点也适用于美国司法管辖区使用的其他预审风险评估工具。PSA可以说是设计较为符合伦理的工具之一。
- en: '^([2](ch70.xhtml#ch01fn54-marker)) Marc Faddoul, Henriette Ruhrmann, and Joyce
    Lee, “A Risk Assessment of a Pretrial Risk Assessment Tool: Tussles, Mitigation
    Strategies, and Inherent Limits,” Computers and Society, Cornell University, submitted
    May 14, 2020, [*https://arxiv.org/abs/2005.07299*](https://arxiv.org/abs/2005.07299).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch70.xhtml#ch01fn54-marker)) Marc Faddoul, Henriette Ruhrmann, and Joyce
    Lee，《一种预先风险评估工具的风险评估：斗争、缓解策略和固有限制》，《计算机与社会》，康奈尔大学，2020年5月14日提交，[*https://arxiv.org/abs/2005.07299*](https://arxiv.org/abs/2005.07299)。
- en: '^([3](ch70.xhtml#ch01fn55-marker)) Shai Danziger, Jonathan Levav, and Liora
    Avnaim-Pesso, “Extraneous Factors in Judicial Decisions,” *Proceedings of the
    National Academy of Sciences* 108, no. 17 (2011): 6889–92, [*https://doi.org/10.1073/pnas.1018033108*](https://doi.org/10.1073/pnas.1018033108).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch70.xhtml#ch01fn55-marker)) Shai Danziger, Jonathan Levav, 和 Liora Avnaim-Pesso，《司法决策中的外部因素》，《国家科学院院刊》108卷，第17期（2011年）：6889–92，[*https://doi.org/10.1073/pnas.1018033108*](https://doi.org/10.1073/pnas.1018033108)。
- en: '^([4](ch70.xhtml#ch01fn56-marker)) Lawrence Lessig, *Code and Other Laws of
    Cyberspace* (New York: Basic Books, 1999).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch70.xhtml#ch01fn56-marker)) Lawrence Lessig，《网络空间的法则与其他法律》（纽约：基础书籍，1999年）。
