- en: 5 Advanced CNN architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5个高级CNN架构
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Working with CNN design patterns
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与CNN设计模式合作
- en: Understanding the LeNet, AlexNet, VGGNet, Inception, and ResNet network architectures
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LeNet、AlexNet、VGGNet、Inception和ResNet网络架构
- en: Welcome to part 2 of this book. Part 1 presented the foundation of neural networks
    architectures and covered multilayer perceptrons (MLPs) and convolutional neural
    networks (CNNs). We wrapped up part 1 with strategies to structure your deep neural
    network projects and tune their hyperparameters to improve network performance.
    In part 2, we will build on this foundation to develop computer vision (CV) systems
    that solve complex image classification and object detection problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到本书的第二部分。第一部分介绍了神经网络架构的基础，涵盖了多层感知器（MLPs）和卷积神经网络（CNNs）。我们在第一部分结束时总结了构建你的深度神经网络项目并调整其超参数以改进网络性能的策略。在第二部分，我们将在此基础上开发计算机视觉（CV）系统，以解决复杂图像分类和目标检测问题。
- en: In chapters 3 and 4, we talked about the main components of CNNs and setting
    up hyperparameters such as the number of hidden layers, learning rate, optimizer,
    and so on. We also talked about other techniques to improve network performance,
    like regularization, augmentation, and dropout. In this chapter, you will see
    how these elements come together to build a convolutional network. I will walk
    you through five of the most popular CNNs that were cutting edge in their time,
    and you will see how their designers thought about building, training, and improving
    networks. We will start with LeNet, developed in 1998, which performed fairly
    well at recognizing handwritten characters. You will see how CNN architectures
    have evolved since then to deeper CNNs like AlexNet and VGGNet, and beyond to
    more advanced and super-deep networks like Inception and ResNet, developed in
    2014 and 2015, respectively.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章和第四章中，我们讨论了CNN的主要组件以及设置超参数，如隐藏层数量、学习率、优化器等。我们还讨论了其他提高网络性能的技术，如正则化、增强和dropout。在本章中，你将看到这些元素是如何结合在一起构建卷积网络的。我将带你了解当时处于前沿的五种最流行的CNN，你将看到它们的开发者是如何考虑构建、训练和改进网络的。我们将从1998年开发的LeNet开始，它在识别手写字符方面表现相当不错。你将看到自那时起CNN架构是如何发展到更深的CNN，如AlexNet和VGGNet，以及更高级的超级深度网络，如2014年和2015年开发的Inception和ResNet。
- en: 'For each CNN architecture, you will learn the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个CNN架构，你将学习以下内容：
- en: Novel features --We will explore the novel features that distinguish these networks
    from others and what specific problems their creators were trying to solve.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新特性--我们将探索这些网络区别于其他网络的新特性以及它们的创造者试图解决的具体问题。
- en: Network architecture --We will cover the architecture and components of each
    network and see how they come together to form the end-to-end network.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络架构--我们将涵盖每个网络的架构和组件，并查看它们是如何结合在一起形成端到端网络的。
- en: Network code implementation --We will walk step-by-step through the network
    implementations using the Keras deep learning (DL) library. The goal of this section
    is for you to learn how to read research papers and implement new architectures
    as the need arises.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络代码实现--我们将逐步通过使用Keras深度学习（DL）库的网络实现进行讲解。本节的目标是让你学会如何阅读研究论文，并在需要时实现新的架构。
- en: Setting up learning hyperparameters --After you implement a network architecture,
    you need to set up the hyperparameters of the learning algorithms that you learned
    in chapter 4 (optimizer, learning rate, weight decay, and so on). We will implement
    the learning hyperparameters as presented in the original research paper of each
    network. In this section, you will see how performance evolved from one network
    to another over the years.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置学习超参数--在实现网络架构之后，你需要设置你在第四章中学到的学习算法的超参数（优化器、学习率、权重衰减等）。我们将按照每个网络的原研究论文中呈现的方式实现学习超参数。在本节中，你将看到性能是如何从一种网络演变到另一种网络，以及这些演变是如何随时间发展的。
- en: Network performance --Finally, you will see how each network performed on benchmark
    datasets like MNIST and ImageNet, as represented in their research papers.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络性能--最后，你将看到每个网络在基准数据集（如MNIST和ImageNet）上的表现，正如它们的研究论文中所展示的那样。
- en: 'The three main objectives of this chapter follow:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的三个主要目标是：
- en: Understanding the architecture and learning hyperparameters of advanced CNNs.
    You will be implementing simpler CNNs like AlexNet and VGGNet for simple- to medium-complexity
    problems. For very complex problems, you might want to use deeper networks like
    Inception and ResNet.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解高级CNN的架构和超参数的学习。你将实现像AlexNet和VGGNet这样的简单到中等复杂性的CNN。对于非常复杂的问题，你可能想使用更深层的网络，如Inception和ResNet。
- en: Understanding the novel features of each network and the reasons they were developed.
    Each succeeding CNN architecture solves a specific limitation in the previous
    one. After reading about the five networks in this chapter (and their research
    papers), you will build a strong foundation for reading and understanding new
    networks as they emerge.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解每个网络的独特特性和它们被开发的原因。每个后续的CNN架构都解决了前一个架构中的特定限制。在阅读本章中关于五个网络（及其研究论文）的内容后，你将为阅读和理解新出现的网络打下坚实的基础。
- en: Learning how CNNs have evolved and their designers’ thought processes. This
    will help you build an instinct for what works well and what problems may arise
    when building your own network.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习CNN如何演变以及其设计者的思维过程。这将帮助你培养对什么有效以及在你构建自己的网络时可能出现的什么问题的直觉。
- en: In chapter 3, you learned about the basic building blocks of convolutional layers,
    pooling layers, and fully connected layers of CNNs. As you will see in this chapter,
    in recent years a lot of CV research has focused on how to put together these
    basic building blocks to form effective CNNs. One of the best ways for you to
    develop your intuition is to examine and learn from these architectures (similar
    to how most of us may have learned to write code by reading other people’s code).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，你学习了CNN的基本构建块，包括卷积层、池化层和全连接层。正如你将在本章中看到的，近年来，许多计算机视觉研究都集中在如何将这些基本构建块组合起来形成有效的CNN。让你培养直觉的最好方法之一是检查并学习这些架构（类似于我们大多数人可能通过阅读他人的代码来学习编写代码）。
- en: To get the most out of this chapter, you are encouraged to read the research
    papers linked in each section before you read my explanation. What you have learned
    in part 1 of this book fully equips you to start reading research papers written
    by pioneers in the AI field. Reading and implementing research papers is by far
    one of the most valuable skills that you will build from reading this book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本章，我们鼓励你在阅读我的解释之前先阅读每个部分中链接的研究论文。本书第一部分的内容已经完全装备你开始阅读AI领域的先驱者所写的研究论文。阅读和实施研究论文是迄今为止你将从这个书中建立的最有价值的技能之一。
- en: TIP Personally, I feel the task of going through a research paper, interpreting
    the crux behind it, and implementing the code is a very important skill every
    DL enthusiast and practitioner should possess. Practically implementing research
    ideas brings out the thought process of the author and also helps transform those
    ideas into real-world industry applications. I hope that, by reading this chapter,
    you will get comfortable reading research papers and implementing their findings
    in your own work. The fast-paced evolution in this field requires us to always
    stay up-to-date with the latest research. What you will learn in this book (or
    in other publications) now will not be the latest and greatest in three or four
    years--maybe even sooner. The most valuable asset that I want you to take away
    from this book is a strong DL foundation that empowers you to get out in the real
    world and be able to read the latest research and implement it yourself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：我个人认为，阅读研究论文、解释其背后的核心，并实现代码是一项非常重要的技能，每个深度学习爱好者和实践者都应该掌握。实际实施研究想法可以揭示作者的思维过程，并帮助将这些想法转化为现实世界的工业应用。我希望通过阅读本章，你将能够轻松阅读研究论文并在自己的工作中实施其发现。这个领域的快速发展要求我们始终跟上最新的研究。现在你在本书（或在其他出版物）中学到的知识在三年或四年内可能不再是最新和最好的——甚至可能更早。我希望你从本书中带走的最有价值的资产是一个强大的深度学习基础，它能够让你走出现实世界，能够阅读最新的研究并自己实现它。
- en: Are you ready? Let’s get started!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好了吗？让我们开始吧！
- en: 5.1 CNN design patterns
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 CNN设计模式
- en: 'Before we jump into the details of the common CNN architectures, we are going
    to look at some common design choices when it comes to CNNs. It might seem at
    first that there are way too many choices to make. Every time we learn about something
    new in deep learning, it gives us more hyperparameters to design. So it is good
    to be able to narrow down our choices by looking at some common patterns that
    were created by pioneer researchers in the field so we can understand their motivation
    and start from where they ended rather than doing things completely randomly:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨常见的CNN架构的细节之前，我们将探讨一些关于CNN的常见设计选择。一开始可能会觉得有太多的选择要做。每次我们学习到深度学习中的新内容，都会给我们带来更多的超参数来设计。因此，通过查看由该领域的先驱研究者创建的一些常见模式，我们能够理解他们的动机，并从他们结束的地方开始，而不是完全随机地做事，这很好：
- en: 'Pattern 1: Feature extraction and classification --Convolutional nets are typically
    composed of two parts: the feature extraction part, which consists of a series
    of convolutional layers; and the classification part, which consists of a series
    of fully connected layers (figure 5.1). This is pretty much always the case with
    ConvNets, starting from LeNet and AlexNet to the very recent CNNs that have come
    out in the past few years, like Inception and ResNet.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式 1：特征提取和分类 -- 卷积网络通常由两部分组成：特征提取部分，由一系列卷积层组成；以及分类部分，由一系列全连接层组成（图5.1）。这种情况在ConvNets中几乎是始终如一的，从LeNet和AlexNet到最近几年出现的Inception和ResNet等最新的CNN。
- en: '![](../Images/5-1.png)'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/5-1.png)'
- en: Figure 5.1 Convolutional nets generally include feature extraction and classification.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.1 卷积网络通常包括特征提取和分类。
- en: 'Pattern 2: Image depth increases, and dimensions decrease --The input data
    at each layer is an image. With each layer, we apply a new convolutional layer
    over a new image. This pushes us to think of an image in a more generic way. First,
    you see that each image is a 3D object that has a height, width, and depth. Depth
    is referred to as the color channel, where depth is 1 for grayscale images and
    3 for color images. In the later layers, the images still have depth, but they
    are not colors per se: they are feature maps that represent the features extracted
    from the previous layers. That’s why the depth increases as we go deeper through
    the network layers. In figure 5.2, the depth of an image is equal to 96; this
    represents the number of feature maps in the layer. So, that’s one pattern you
    will always see: the image depth increases, and the dimensions decrease.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式 2：图像深度增加，维度减少 -- 每一层的输入数据是一个图像。随着每一层的应用，我们会在新的图像上应用一个新的卷积层。这促使我们以更通用的方式思考图像。首先，您会看到每个图像都是一个具有高度、宽度和深度的3D对象。深度指的是颜色通道，对于灰度图像，深度为1，对于彩色图像，深度为3。在后续的层中，图像仍然具有深度，但它们本身不是颜色：它们是特征图，代表从上一层提取的特征。这就是为什么随着我们深入网络层，深度会增加。在图5.2中，图像的深度等于96；这表示该层中的特征图数量。所以，这是一个您总会看到的模式：图像深度增加，维度减少。
- en: '![](../Images/5-2.png)'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](../Images/5-2.png)'
- en: Figure 5.2 Image depth increases, and the dimensions decrease.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.2 图像深度增加，维度减少。
- en: 'Pattern 3: Fully connected layers --This generally isn’t as strict a pattern
    as the previous two, but it’s very helpful to know. Typically, all fully connected
    layers in a network either have the same number of hidden units or decrease at
    each layer. It is rare to find a network where the number of units in the fully
    connected layers increases at each layer. Research has found that keeping the
    number of units constant doesn’t hurt the neural network, so it may be a good
    approach if you want to limit the number of choices you have to make when designing
    your network. This way, all you have to do is to pick a number of units per layer
    and apply that to all your fully connected layers.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式 3：全连接层 -- 这通常不像前两种模式那么严格，但了解这一点非常有帮助。通常，网络中的所有全连接层要么具有相同数量的隐藏单元，要么在每一层中递减。很难找到一个网络，其全连接层中的单元数量在每一层中都在增加。研究发现，保持单元数量不变并不会损害神经网络，因此，如果您想限制在设计网络时必须做出的选择数量，这可能是一个好方法。这样，您只需选择每一层的单元数量，并将其应用于所有全连接层。
- en: Now that you understand the basic CNN patterns, let’s look at some architectures
    that have implemented them. Most of these architectures are famous because they
    performed well in the ImageNet competition. ImageNet is a famous benchmark that
    contains millions of images; DL and CV researchers use the ImageNet dataset to
    compare algorithms. More on that later.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了基本的 CNN 模式，让我们来看看一些实现了这些模式的架构。其中大多数架构之所以闻名，是因为它们在 ImageNet 竞赛中表现出色。ImageNet
    是一个包含数百万图像的著名基准，DL 和 CV 研究人员使用 ImageNet 数据集来比较算法。关于这一点，我们稍后再谈。
- en: NOTE The snippets in this chapter are not meant to be runnable. The goal is
    to show you how to implement the specifications that are defined in a research
    paper. Visit the book’s website ([www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems))
    or Github repo ([https://github.com/moelgendy/deep_learning _for_vision_systems](https://github.com/moelgendy/deep_learning_for_vision_systems))
    for the full executable code.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：本章中的代码片段并非旨在可运行。目的是向你展示如何实现研究论文中定义的规范。请访问本书的网站 ([www.manning.com/books/deep-learning-for-vision-systems](http://www.manning.com/books/deep-learning-for-vision-systems))
    或 Github 仓库 ([https://github.com/moelgendy/deep_learning_for_vision_systems](https://github.com/moelgendy/deep_learning_for_vision_systems))
    以获取完整的可执行代码。
- en: 'Now, let’s get started with the first network we are going to discuss in this
    chapter: LeNet.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始本章将要讨论的第一个网络：LeNet。
- en: 5.2 LeNet-5
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 LeNet-5
- en: 'In 1998, Lecun et al. introduced a pioneering CNN called LeNet-5.[1](#pgfId-1155041)
    The LeNet-5 architecture is straightforward, and the components are not new to
    you (they were new back in 1998); you learned about convolutional, pooling, and
    fully connected layers in chapter 3\. The architecture is composed of five weight
    layers, and hence the name LeNet-5: three convolutional layers and two fully connected
    layers.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 1998 年，Lecun 等人引入了一种开创性的 CNN，称为 LeNet-5。[1](#pgfId-1155041) LeNet-5 架构简单明了，其中的组件对你来说并不陌生（它们在
    1998 年时是新的）；你在第 3 章中学习了卷积、池化和全连接层。该架构由五个权重层组成，因此得名 LeNet-5：三个卷积层和两个全连接层。
- en: DEFINITION We refer to the convolutional and fully connected layers as weight
    layers because they contain trainable weights as opposed to pooling layers that
    don’t contain any weights. The common convention is to use the number of weight
    layers to describe the depth of the network. For example, AlexNet (explained next)
    is said to be eight layers deep because it contains five convolutional and three
    fully connected layers. The reason we care more about weight layers is mainly
    because they reflect the model’s computational complexity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：我们称卷积层和全连接层为权重层，因为它们包含可训练的权重，而池化层则不包含任何权重。常见的做法是使用权重层的数量来描述网络的深度。例如，AlexNet（将在下一节中解释）被称为八层深度，因为它包含五个卷积层和三个全连接层。我们更关注权重层的原因主要是它们反映了模型的计算复杂度。
- en: 5.2.1 LeNet architecture
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 LeNet 架构
- en: 'The architecture of LeNet-5 is shown in figure 5.3:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet-5 的架构如图 5.3 所示：
- en: INPUT IMAGE ⇒ C1 ⇒ TANH ⇒ S2 ⇒ C3 ⇒ TANH ⇒ S4 ⇒ C5 ⇒ TANH ⇒ FC6 ⇒ SOFTMAX7
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像 ⇒ C1 ⇒ TANH ⇒ S2 ⇒ C3 ⇒ TANH ⇒ S4 ⇒ C5 ⇒ TANH ⇒ FC6 ⇒ SOFTMAX7
- en: where C is a convolutional layer, S is a subsampling or pooling layer, and FC
    is a fully connected layer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 C 代表卷积层，S 代表下采样或池化层，FC 代表全连接层。
- en: Notice that Yann LeCun and his team used tanh as an activation function instead
    of the currently state-of-the-art ReLU. This is because in 1998, ReLU had not
    yet been used in the context of DL, and it was more common to use tanh or sigmoid
    as an activation function in the hidden layers. Without further ado, let’s implement
    LeNet-5 in Keras.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 Yann LeCun 和他的团队使用了 tanh 作为激活函数，而不是目前最先进的 ReLU。这是因为 1998 年，ReLU 还没有在 DL
    的背景下使用，而在隐藏层中更常见的是使用 tanh 或 sigmoid 作为激活函数。不再赘述，让我们在 Keras 中实现 LeNet-5。
- en: '![](../Images/5-3.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-3.png)'
- en: Figure 5.3 LeNet architecture
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 LeNet 架构
- en: 5.2.2 LeNet-5 implementation in Keras
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 Keras 中的 LeNet-5 实现
- en: 'To implement LeNet-5 in Keras, read the original paper and follow the architecture
    information from pages 6-8\. Here are the main takeaways for building the LeNet-5
    network:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Keras 中实现 LeNet-5，请阅读原始论文并遵循第 6-8 页的架构信息。以下是构建 LeNet-5 网络的主要要点：
- en: 'Number of filters in each convolutional layer --As you can see in figure 5.3
    (and as defined in the paper), the depth (number of filters) of each convolutional
    layer is as follows: C1 has 6, C3 has 16, C5 has 120 layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层的滤波器数量--正如你在图 5.3 中所见（以及论文中定义的），每个卷积层的深度（滤波器数量）如下：C1 有 6 个，C3 有 16 个，C5
    有 120 个。
- en: Kernel size of each convolutional layer --The paper specifies that the `kernel_size`
    is 5 × 5.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层的核大小--论文中指定`kernel_size`为5 × 5。
- en: Subsampling (pooling) layers --A subsampling (pooling) layer is added after
    each convolutional layer. The receptive field of each unit is a 2 × 2 area (for
    example, `pool_size` is 2). Note that the LeNet-5 creators used average pooling,
    which computes the average value of its inputs, instead of the max pooling layer
    that we used in our earlier projects, which passes the maximum value of its inputs.
    You can try both if you are interested, to see the difference. For this experiment,
    we are going to follow the paper’s architecture.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子采样（池化）层--在每个卷积层之后添加一个子采样（池化）层。每个单元的感受野是一个2 × 2的区域（例如，`pool_size`为2）。请注意，LeNet-5的创造者使用了平均池化，它计算其输入的平均值，而不是我们在早期项目中使用的最大池化层，后者传递其输入的最大值。如果您感兴趣，可以尝试两者，看看区别。对于这个实验，我们将遵循论文的架构。
- en: Activation function --As mentioned before, the creators of LeNet-5 used the
    tanh activation function for the hidden layers because symmetric functions are
    believed to yield faster convergence compared to sigmoid functions (figure 5.4).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数--如前所述，LeNet-5的创造者为了隐藏层使用了tanh激活函数，因为对称函数被认为比sigmoid函数收敛更快（图5.4）。
- en: '![](../Images/5-4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-4.png)'
- en: Figure 5.4 The LeNet architecture consists of convolutional kernels of size
    5 × 5; pooling layers; an activation function (tanh); and three fully connected
    layers with 120, 84, and 10 neurons, respectively.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 LeNet架构由大小为5 × 5的卷积核、池化层、激活函数（tanh）以及分别有120、84和10个神经元的三个全连接层组成。
- en: 'Now let’s put that in code to build the LeNet-5 architecture:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将其放入代码中，以构建LeNet-5架构：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports the Keras model and layers
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入Keras模型和层
- en: ❷ Instantiates an empty sequential model
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实例化一个空的序列模型
- en: ❸ Flattens the CNN output to feed it fully connected layers
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将CNN输出展平以供全连接层完全连接
- en: ❹ Prints the model summary (figure 5.5)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印模型摘要（图5.5）
- en: LeNet-5 is a small neural network by today’s standards. It has 61,706 parameters,
    compared to millions of parameters in more modern networks, as you will see later
    in this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet-5按照今天的标准来说是一个小型的神经网络。它有61,706个参数，与本章后面将要看到的更现代网络的数百万个参数相比。
- en: A note when reading the papers discussed in this chapter
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章讨论的论文时的注意事项
- en: When you read the LeNet-5 paper, just know that it is harder to read than the
    others we will cover in this chapter. Most of the ideas that I mention in this
    section are in sections 2 and 3 of the paper. The later sections of the paper
    talk about something called the graph transformer network, which isn’t widely
    used today. So if you do try to read the paper, I recommend focusing on section
    2, which talks about the LeNet architecture and the learning details; then maybe
    take a quick look at section 3, which includes a bunch of experiments and results
    that are pretty interesting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读LeNet-5论文时，只需知道它比本章我们将要讨论的其他论文更难读。本节中提到的大多数想法都在论文的第2和第3部分。论文的后期部分讨论了被称为图变换网络的东西，这在今天并不广泛使用。所以如果你真的尝试去读这篇论文，我建议你专注于第2部分，它讨论了LeNet架构和学习细节；然后可以快速浏览第3部分，其中包含一些相当有趣的实验和结果。
- en: I recommend starting with the AlexNet paper (discussed in section 5.3), followed
    by the VGGNet paper (section 5.4), and then the LeNet paper. It is a good classic
    to look at once you go over the other ones.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议先从AlexNet论文（在第5.3节中讨论）开始，然后是VGGNet论文（第5.4节），最后是LeNet论文。一旦你阅读了其他论文，它是一个很好的经典论文去研究。
- en: '![](../Images/5-5.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-5.png)'
- en: Figure 5.5 LeNet-5 model summary
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 LeNet-5模型摘要
- en: 5.2.3 Setting up the learning hyperparameters
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 设置学习超参数
- en: 'LeCun and his team used scheduled decay learning where the value of the learning
    rate was decreased using the following schedule: 0.0005 for the first two epochs,
    0.0002 for the next three epochs, 0.00005 for the next four, and then 0.00001
    thereafter. In the paper, the authors trained their network for 20 epochs.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: LeCun及其团队使用了计划衰减学习，其中学习率的值按照以下计划降低：前两个epoch为0.0005，接下来的三个epoch为0.0002，接下来的四个epoch为0.00005，然后之后为0.00001。在论文中，作者训练了他们的网络20个epoch。
- en: 'Let’s build a `lr_schedule` function with this schedule. The method takes an
    integer epoch number as an argument and returns the learning rate (`lr`):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据这个计划构建一个`lr_schedule`函数。该方法接受一个整数epoch号作为参数，并返回学习率（`lr`）：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ lr is 0.0005 for the first two epochs, 0.0002 for the next three epochs (3
    to 5), 0.00005 for the next four (6 to 9), then 0.00001 thereafter (more than
    9).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶前两个epoch的学习率lr为0.0005，接下来的三个epoch（3到5）为0.0002，接下来的四个epoch（6到9）为0.00005，之后为0.00001（超过9）。
- en: 'We use the `lr_schedule` function in the following code snippet to compile
    the model:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下代码片段中使用`lr_schedule`函数来编译模型：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now start the network training for 20 epochs, as mentioned in the paper:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始网络训练20个epoch，正如论文中提到的：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: See the downloadable notebook included with the book’s code for the full code
    implementation, if you want to see this in action.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想看到完整的代码实现，请参阅书中代码附带的可下载笔记本。
- en: 5.2.4 LeNet performance on the MNIST dataset
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 LeNet在MNIST数据集上的性能
- en: When you train LeNet-5 on the MNIST dataset, you will get above 99% accuracy
    (see the code notebook with the book’s code). Try to re-run this experiment with
    the ReLU activation function in the hidden layers, and observe the difference
    in the network performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在MNIST数据集上训练LeNet-5时，你将获得超过99%的准确率（参见书中代码的代码笔记本）。尝试使用隐藏层中的ReLU激活函数重新运行此实验，并观察网络性能的差异。
- en: 5.3 AlexNet
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 AlexNet
- en: LeNet performs very well on the MNIST dataset. But it turns out that the MNIST
    dataset is very simple because it contains grayscale images (1 channel) and classifies
    into only 10 classes, which makes it an easier challenge. The main motivation
    behind AlexNet was to build a deeper network that can learn more complex functions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet在MNIST数据集上表现非常好。但结果证明，MNIST数据集非常简单，因为它包含灰度图像（1个通道）并且只分为10个类别，这使得它是一个更容易的挑战。AlexNet背后的主要动机是构建一个更深层的网络，能够学习更复杂的函数。
- en: AlexNet (figure 5.6) was the winner of the ILSVRC image classification competition
    in 2012\. Krizhevsky et al. created the neural network architecture and trained
    it on 1.2 million high-resolution images into 1,000 different classes of the ImageNet
    dataset.[2](#pgfId-1155200) AlexNet was state of the art at its time because it
    was the first real “deep” network that opened the door for the CV community to
    seriously consider convolutional networks in their applications. We will explain
    deeper networks later in this chapter, like VGGNet and ResNet, but it is good
    to see how ConvNets evolved and the main drawbacks of AlexNet that were the main
    motivation for the later networks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet（图5.6）是2012年ILSVRC图像分类竞赛的获胜者。Krizhevsky等人创建了神经网络架构，并在1.2百万张高分辨率图像上训练，将其分为ImageNet数据集的1000个不同的类别。[2](#pgfId-1155200)
    AlexNet在其时代是顶尖的，因为它是最早的真正“深度”网络，为CV社区打开了认真考虑卷积网络在应用中的大门。我们将在本章后面解释更深的网络，如VGGNet和ResNet，但了解卷积网络是如何演变的以及AlexNet的主要缺点是很好的，这些缺点是后来网络的主要动机。
- en: '![](../Images/5-6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6](../Images/5-6.png)'
- en: Figure 5.6 AlexNet architecture
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 AlexNet架构
- en: 'As you can see in figure 5.6, AlexNet has a lot of similarities to LeNet but
    is much deeper (more hidden layers) and bigger (more filters per layer). They
    have similar building blocks: a series of convolutional and pooling layers stacked
    on top of each other followed by fully connected layers and a softmax. We’ve seen
    that LeNet has around 61,000 parameters, whereas AlexNet has about 60 million
    parameters and 650,000 neurons, which gives it a larger learning capacity to understand
    more complex features. This allowed AlexNet to achieve remarkable performance
    in the ILSVRC image classification competition in 2012.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.6所示，AlexNet与LeNet有很多相似之处，但更深（更多隐藏层）且更大（每层更多过滤器）。它们有相似的构建模块：一系列堆叠在一起的卷积和池化层，随后是全连接层和softmax。我们了解到LeNet大约有61,000个参数，而AlexNet有大约6000万个参数和650万个神经元，这使得它具有更大的学习容量来理解更复杂的功能。这允许AlexNet在2012年的ILSVRC图像分类竞赛中取得显著的成绩。
- en: ImageNet and ILSVRC
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet和ILSVRC
- en: ImageNet ([http://image-net.org/index](http://image-net.org/index)) is a large
    visual database designed for use in visual object recognition software research.
    It is aimed at labeling and categorizing images into almost 22,000 categories
    based on a defined set of words and phrases. The images were collected from the
    web and labeled by humans using Amazon’s Mechanical Turk crowdsourcing tool. At
    the time of this writing, there are over 14 million images in the ImageNet project.
    To organize such a massive amount of data, the creators of ImageNet followed the
    WordNet hierarchy where each meaningful word/ phrase in WordNet is called a synonym
    set (synset for short). Within the ImageNet project, images are organized according
    to these synsets, with the goal being to have 1,000+ images per synset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet ([http://image-net.org/index](http://image-net.org/index))是一个大型视觉数据库，旨在用于视觉物体识别软件研究。它旨在根据一组定义的单词和短语将图像标记和分类到近22,000个类别。这些图像来自网络，并由人类使用亚马逊的Mechanical
    Turk众包工具进行标记。在撰写本文时，ImageNet项目中已有超过1400万张图像。为了组织如此大量的数据，ImageNet的创建者遵循了WordNet层次结构，其中WordNet中的每个有意义的单词/短语被称为同义词集（简称synset）。在ImageNet项目中，图像根据这些synset组织，目标是每个synset有1,000+张图像。
- en: The ImageNet project runs an annual software contest called the ImageNet Large
    Scale Visual Recognition Challenge (ILSVRC, [www.image-net.org/challenges/LSVRC](http://www.image-net.org/challenges/LSVRC)),
    where software programs compete to correctly classify and detect objects and scenes.
    We will use the ILSVRC challenge as a benchmark to compare different networks’
    performance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet项目每年举办一次名为ImageNet大规模视觉识别挑战（ILSVRC，[www.image-net.org/challenges/LSVRC](http://www.image-net.org/challenges/LSVRC)）的软件竞赛，软件程序在此竞赛中竞争正确分类和检测对象和场景。我们将使用ILSVRC挑战作为基准来比较不同网络的性能。
- en: 5.3.1 AlexNet architecture
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 AlexNet架构
- en: 'You saw a version of the AlexNet architecture in the project at the end of
    chapter 3\. The architecture is pretty straightforward. It consists of:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您在第3章末的项目中看到了AlexNet架构的一个版本。该架构相当直接。它包括：
- en: 'Convolutional layers with the following kernel sizes: 11 × 11, 5 × 5, and 3
    × 3'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有以下核大小的卷积层：11 × 11, 5 × 5, 和 3 × 3
- en: Max pooling layers for images downsampling
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像下采样使用的最大池化层
- en: Dropout layers to avoid overfitting
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Dropout层来避免过拟合
- en: Unlike LeNet, ReLU activation functions in the hidden layers and a softmax activation
    in the output layer
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与LeNet不同，隐藏层使用ReLU激活函数，输出层使用softmax激活函数
- en: 'AlexNet consists of five convolutional layers, some of which are followed by
    max-pooling layers, and three fully connected layers with a final 1000-way softmax.
    The architecture can be represented in text as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet由五个卷积层组成，其中一些后面跟着最大池化层，以及三个全连接层，最终有1000个类别的softmax。该架构可以用以下文本表示：
- en: INPUT IMAGE ⇒ CONV1 ⇒ POOL2 ⇒ CONV3 ⇒ POOL4 ⇒ CONV5 ⇒ CONV6 ⇒ CONV7 ⇒ POOL8
    ⇒ FC9 ⇒ FC10 ⇒ SOFTMAX7
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像 ⇒ CONV1 ⇒ POOL2 ⇒ CONV3 ⇒ POOL4 ⇒ CONV5 ⇒ CONV6 ⇒ CONV7 ⇒ POOL8 ⇒ FC9 ⇒
    FC10 ⇒ SOFTMAX7
- en: 5.3.2 Novel features of AlexNet
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 AlexNet的新特性
- en: Before AlexNet, DL was starting to gain traction in speech recognition and a
    few other areas. But AlexNet was the milestone that convinced a lot of people
    in the CV community to take a serious look at DL and demonstrate that it really
    works in CV. AlexNet presented some novel features that were not used in previous
    CNNs (like LeNet). You are already familiar with all of them from the previous
    chapters, so we’ll go through them quickly here.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在AlexNet之前，深度学习（DL）开始在语音识别和其他一些领域开始受到关注。但AlexNet是里程碑式的，它说服了CV社区中的许多人认真看待深度学习，并证明它确实在CV中有效。AlexNet提出了一些在以前的CNN（如LeNet）中没有使用的新特性。您已经从前面的章节中熟悉了它们，所以我们将在这里快速浏览它们。
- en: ReLU activation function
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReLU激活函数
- en: 'AlexNet uses ReLu for the nonlinear part instead of the tanh and sigmoid functions
    that were the earlier standard for traditional neural networks (like LeNet). ReLu
    was used in the hidden layers of the AlexNet architecture because it trains much
    faster. This is because the derivative of the sigmoid function becomes very small
    in the saturating region, and therefore the updates applied to the weights almost
    vanish. This phenomenon is called the vanishing gradient problem. ReLU is represented
    by this equation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet在非线性部分使用ReLU而不是tanh和sigmoid函数，这些函数是传统神经网络（如LeNet）早期标准。ReLU在AlexNet架构的隐藏层中使用，因为它训练得更快。这是因为sigmoid函数的导数在饱和区域变得非常小，因此应用于权重的更新几乎消失。这种现象称为梯度消失问题。ReLU由以下方程表示：
- en: '*f* (*x*) = max(0,*x*)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* (*x*) = max(0,*x*)'
- en: It’s discussed in detail in chapter 2.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章中对此进行了详细讨论。
- en: The vanishing gradient problem
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 消失梯度问题
- en: 'Certain activation functions, like the sigmoid function, squish a large input
    space into a small input space between 0 and 1 (-1 to 1 for tanh activations).
    Therefore, a large change in the input of the sigmoid function causes a small
    change in the output. As a result, the derivative becomes very small:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 某些激活函数，如sigmoid函数，将大的输入空间压缩到0到1（对于tanh激活为-1到1）之间的小输入空间。因此，sigmoid函数输入的大变化导致输出的小变化。结果，导数变得非常小：
- en: '![](../Images/5-unnumb-1K.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-unnumb-1K.png)'
- en: 'The vanishing gradient problem: a large change in the input of the sigmoid
    function causes a negligible change in the output.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 消失梯度问题：sigmoid函数输入的大变化导致输出变化微乎其微。
- en: We will talk more about the vanishing gradient phenomenon later in this chapter
    when we look at the ResNet architecture.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看ResNet架构时，在本章的后面我们将更详细地讨论消失梯度现象。
- en: Dropout layer
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dropout层
- en: As explained in chapter 3, dropout layers are used to prevent the neural network
    from overfitting. The neurons that are “dropped out” do not contribute to the
    forward pass and do not participate in backpropagation. This means every time
    an input is presented, the neural network samples a different architecture, but
    all of these architectures share the same weights. This technique reduces complex
    co-adaptations of neurons, since a neuron cannot rely on the presence of particular
    other neurons. Therefore, the neuron is forced to learn more robust features that
    are useful in conjunction with many different random subsets of the other neurons.
    Krizhevsky et al. used dropout with a probability of 0.5 in the two fully connected
    layers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如第3章所述，dropout层被用来防止神经网络过拟合。被“丢弃”的神经元不会对前向传递做出贡献，也不参与反向传播。这意味着每次输入时，神经网络都会采样一个不同的架构，但所有这些架构都共享相同的权重。这种技术减少了神经元之间的复杂共适应，因为一个神经元不能依赖于特定其他神经元的出现。因此，神经元被迫学习更多在与其他许多随机子集的神经元结合时有用的鲁棒特征。Krizhevsky等人在这两个全连接层中使用了概率为0.5的dropout。
- en: Data augmentation
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据增强
- en: One popular and very effective approach to avoid overfitting is to artificially
    enlarge the dataset using label-preserving transformations. This happens by generating
    new instances of the training images with transformations like image rotation,
    flipping, scaling, and many more. Data augmentation is explained in detail in
    chapter 4.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 避免过拟合的一种流行且非常有效的方法是通过使用标签保持变换来人工扩大数据集。这通过使用图像旋转、翻转、缩放等变换生成训练图像的新实例来实现。数据增强在第4章中有详细解释。
- en: Local response normalization
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 局部响应归一化
- en: AlexNet uses local response normalization. It is different from the batch normalization
    technique (explained in chapter 4). Normalization helps to speed up convergence.
    Nowadays, batch normalization is used instead of local response normalization;
    we will use BN in our implementation in this chapter.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet使用了局部响应归一化。这与第4章中解释的批量归一化技术不同。归一化有助于加快收敛速度。如今，批量归一化已取代局部响应归一化；在本章的实现中，我们将使用BN。
- en: Weight regularization
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重正则化
- en: 'Krizhevsky et al. used a weight decay of 0.0005\. Weight decay is another term
    for the L2 regularization technique explained in chapter 4\. This approach reduces
    the overfitting of the DL neural network model on training data to allow the network
    to generalize better on new data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Krizhevsky等人使用了0.0005的权重衰减。权重衰减是第4章中解释的L2正则化技术的另一个术语。这种方法减少了深度学习神经网络模型在训练数据上的过拟合，从而使网络在新的数据上更好地泛化：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The lambda (ƛ) value is a weight decay hyperparameter that you can tune. If
    you still see overfitting, you can reduce it by increasing the lambda value. In
    this case, Krizhevsky and his team found that a small decay value of 0.0005 was
    good enough for the model to learn.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda (λ) 值是一个权重衰减超参数，你可以调整它。如果你仍然看到过拟合，你可以通过增加λ值来减少它。在这种情况下，Krizhevsky和他的团队发现，一个小的衰减值0.0005对于模型学习来说已经足够好了。
- en: Training on multiple GPUs
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在多个GPU上训练
- en: 'Krizhevsky et al. used a GTX 580 GPU with only 3 GB of memory. It was state-of-the-art
    at the time but not large enough to train the 1.2 million training examples in
    the dataset. Therefore, the team developed a complicated way to spread the network
    across two GPUs. The basic idea was that a lot of the layers were split across
    two different GPUs that communicated with each other. You don’t need to worry
    about these details today: there are far more advanced ways to train deep networks
    on distributed GPUs, as we will discuss later in this book.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Krizhevsky等人使用了一个配备3 GB内存的GTX 580 GPU。在当时，它是最先进的，但不足以训练数据集中的1.2百万个训练示例。因此，该团队开发了一种复杂的方法，将网络分散到两个GPU上。基本思想是许多层被分割到两个不同的GPU上，这些GPU之间相互通信。你今天不需要担心这些细节：在本书后面的部分，我们将讨论在分布式GPU上训练深度网络的更先进的方法。
- en: 5.3.3 AlexNet implementation in Keras
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 Keras中的AlexNet实现
- en: Now that you’ve learned the basic components of AlexNet and its novel features,
    let’s apply them to build the AlexNet neural network. I suggest that you read
    the architecture description on page 4 of the original paper and follow along.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了AlexNet及其新颖特性的基本组成部分，让我们应用它们来构建AlexNet神经网络。我建议你阅读原始论文的第4页上的架构描述并跟随。
- en: 'As depicted in figure 5.7, the network contains eight weight layers: the first
    five are convolutional, and the remaining three are fully connected. The output
    of the last fully connected layer is fed to a 1000-way softmax that produces a
    distribution over the 1,000 class labels.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.7所示，该网络包含八个权重层：前五个是卷积层，剩下的三个是全连接层。最后一个全连接层的输出被送入一个1000路softmax，产生对1000个类别标签的分布。
- en: NOTE AlexNet input starts with 227 × 227 × 3 images. If you read the paper,
    you will notice that it refers to a dimensions volume of 224 × 224 × 3 for the
    input images. But the numbers make sense only for 227 × 227 × 3 images (figure
    5.7). I suggest that this could be a typing mistake in the paper.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：AlexNet输入从227 × 227 × 3图像开始。如果你阅读了论文，你会注意到它提到了输入图像的维度体积为224 × 224 × 3。但数字只对227
    × 227 × 3图像（图5.7）有意义。我建议这可能是在论文中的打字错误。
- en: '![](../Images/5-7.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-7.png)'
- en: 'Figure 5.7 AlexNet contains eight weight layers: five convolutional and three
    fully connected. Two contain 4,096 neurons, and the output is fed to a 1,000-neuron
    softmax.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 AlexNet包含八个权重层：五个卷积层和三个全连接层。其中两个包含4,096个神经元，输出被送入一个1,000个神经元的softmax。
- en: 'The layers are stacked together as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 层叠方式如下：
- en: 'CONV1--The authors used a large kernel size (11). They also used a large stride
    (4), which makes the input dimensions shrink by roughly a factor 4 (from 227 ×
    227 to 55 × 55). We calculate the dimensions of the output as follows:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CONV1--作者使用了一个大的内核大小（11）。他们还使用了一个大的步长（4），这使得输入维度大约缩小了4倍（从227 × 227到55 × 55）。我们如下计算输出维度：
- en: (227 - 11)/4 + 1 = 55
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (227 - 11)/4 + 1 = 55
- en: and the depth is the number of filters in the convolutional layer (96). The
    output dimensions are 55 × 55 × 96.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度是卷积层中的滤波器数量（96）。输出维度是55 × 55 × 96。
- en: 'POOL with a filter size of 3 × 3--This reduces the dimensions from 55 × 55
    to 27 × 27:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: POOL尺寸为3 × 3--这降低了维度从55 × 55到27 × 27：
- en: (55 - 3)/2 + 1 = 27
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (55 - 3)/2 + 1 = 27
- en: The pooling layer doesn’t change the depth of the volume. The output dimensions
    are 27 × 27 × 96.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 池化层不会改变体积的深度。输出维度是27 × 27 × 96。
- en: 'Similarly, we can calculate the output dimensions of the remaining layers:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算剩余层的输出维度：
- en: CONV2--Kernel size = 5, depth = 256, and stride = 1
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CONV2--内核大小= 5，深度= 256，步长= 1
- en: POOL--Size = 3 × 3, which downsamples its input dimensions from 27 × 27 to 13
    × 13
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: POOL--大小= 3 × 3，它将输入维度从27 × 27下采样到13 × 13
- en: CONV3--Kernel size = 3, depth = 384, and stride = 1
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CONV3--内核大小= 3，深度= 384，步长= 1
- en: CONV4--Kernel size = 3, depth = 384, and stride = 1
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CONV4--内核大小= 3，深度= 384，步长= 1
- en: CONV5--Kernel size = 3, depth = 256, and stride = 1
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CONV5--内核大小= 3，深度= 256，步长= 1
- en: POOL--Size = 3 × 3, which downsamples its input from 13 × 13 to 6 × 6
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: POOL--大小= 3 × 3，它将输入从13 × 13下采样到6 × 6
- en: Flatten layer--Flattens the dimension volume 6 × 6 × 256 to 1 × 9,216
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flatten层--将维度体积6 × 6 × 256展平为1 × 9,216
- en: FC with 4,096 neurons
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC层包含4,096个神经元
- en: FC with 4,096 neurons
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC层包含4,096个神经元
- en: Softmax layer with 1,000 neurons
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax层包含1,000个神经元
- en: 'NOTE You might be wondering how Krizhevsky and his team decided to implement
    this configuration. Setting up the right values of network hyperparameters like
    kernel size, depths, stride, pooling size, etc., is tedious and requires a lot
    of trial and error. The idea remains the same: we want to apply many weight layers
    to increase the model’s capacity to learn more complex functions. We also need
    to add pooling layers in between to downsample the input dimensions, as discussed
    in chapter 2\. With that said, setting up the exact hyperparameters comes as one
    of the challenges of CNNs. VGGNet (explained next) solves this problem by implementing
    a uniform layer configuration to reduce the amount of trial and error when designing
    your network.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能想知道Krizhevsky和他的团队是如何决定实施这种配置的。设置网络超参数的正确值，如核大小、深度、步长、池化大小等，是繁琐的，需要大量的试错。想法保持不变：我们希望应用许多权重层来增加模型学习更复杂函数的能力。我们还需要在之间添加池化层以降采样输入维度，正如第2章所讨论的。因此，设置确切的超参数是CNN的一个挑战。VGGNet（下文将解释）通过实现统一的层配置来解决设计网络时试错量的问题。
- en: 'Note that all of the convolutional layers are followed by a batch normalization
    layer, and all of the hidden layers are followed by ReLU activations. Now, let’s
    put that in code to build the AlexNet architecture:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所有卷积层之后都跟着一个批量归一化层，所有隐藏层之后都跟着ReLU激活。现在，让我们将其放入代码中，以构建AlexNet架构：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Imports the Keras model, layers, and regularizers
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入Keras模型、层和正则化器
- en: ❷ Instantiates an empty sequential model
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 实例化一个空的序列模型
- en: ❸ The activation function can be added on its own layer or within the Conv2D
    function as we did in previous implementations.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 激活函数可以添加到自己的层中，或者像我们在之前的实现中那样在Conv2D函数内添加。
- en: ❹ Note that the AlexNet authors did not add a pooling layer here.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 注意，AlexNet的作者在这里没有添加池化层。
- en: ❺ Similar to layer 3
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 与第3层类似
- en: ❻ Flattens the CNN output to feed it fully connected layers
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将CNN输出展平以供全连接层使用
- en: ❼ Prints the model summary
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 打印模型摘要
- en: 'When you print the model summary, you will see that the number of total parameters
    is 62 million:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打印模型摘要时，你会看到总参数数是6200万：
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: NOTE Both LeNet and AlexNet have many hyperparameters to tune. The authors of
    those networks had to go through many experiments to set the kernel size, strides,
    and padding for each layer, which makes the networks harder to understand and
    manage. VGGNet (explained next) solves this problem with a very simple, uniform
    architecture.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：LeNet和AlexNet都有许多超参数需要调整。那些网络的作者不得不进行许多实验来设置每层的核大小、步长和填充，这使得网络更难以理解和管理。VGGNet（下文将解释）通过一个非常简单、统一的架构解决了这个问题。
- en: 5.3.4 Setting up the learning hyperparameters
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.4 设置学习超参数
- en: 'AlexNet was trained for 90 epochs, which took 6 days on two Nvidia Geforce
    GTX 580 GPUs simultaneously. This is why you will see that the network is split
    into two pipelines in the original paper. Krizhevsky et al. started with an initial
    learning rate of 0.01 with a momentum of 0.9\. The `lr` is then divided by 10
    when the validation error stops improving:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet训练了90个epoch，在两个Nvidia Geforce GTX 580 GPU上同时进行，耗时6天。这就是为什么你会在原始论文中看到网络被分成两个管道的原因。Krizhevsky等人以0.01的初始学习率和0.9的动量开始。当验证误差停止改进时，`lr`被除以10：
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Reduces the learning rate by 0.1 when the validation error plateaus
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当验证误差停滞时，将学习率降低0.1
- en: ❷ Sets the SGD optimizer with lr of 0.01 and momentum of 0.9
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置SGD优化器，学习率为0.01，动量为0.9
- en: ❸ Compiles the model
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编译模型
- en: ❹ Trains the model and calls the reduce_lr value using callbacks in the training
    method
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在训练方法中使用回调函数训练模型并调用reduce_lr值
- en: 5.3.5 AlexNet performance
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.5 AlexNet性能
- en: AlexNet significantly outperformed all the prior competitors in the 2012 ILSVRC
    challenges. It achieved a winning top-5 test error rate of 15.3%, compared to
    26.2% achieved by the second-best entry that year, which used other traditional
    classifiers. This huge improvement in performance attracted the CV community’s
    attention to the potential that convolutional networks have to solve complex vision
    problems and led to more advanced CNN architectures, as you will see in the following
    sections of this chapter.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet在2012年ILSVRC挑战中显著优于所有之前的竞争对手。它实现了15.3%的获胜top-5测试错误率，而当年第二好的参赛者使用了其他传统分类器，其错误率为26.2%。这种巨大的性能提升吸引了CV社区对卷积网络解决复杂视觉问题潜力的关注，并导致了更先进的CNN架构，你将在本章的后续部分看到。
- en: Top-1 and top-5 error rates?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Top-1和top-5错误率是多少？
- en: Top-1 and top-5 are terms used mostly in research papers to describe the accuracy
    of an algorithm on a given classification task. The top-1 error rate is the percentage
    of the time that the classifier did not give the correct class the highest score,
    and the top-5 error rate is the percentage of the time that the classifier did
    not include the correct class among its top five guesses.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Top-1和top-5是主要在研究论文中使用的术语，用来描述算法在特定分类任务上的准确性。Top-1错误率是指分类器没有给出正确类别最高分数的百分比，而top-5错误率是指分类器在其前五次猜测中没有包含正确类别的百分比。
- en: 'Let’s apply this in an example. Suppose there are 100 classes, and we show
    the network an image of a cat. The classifier outputs a score or confidence value
    for each class as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来应用这个概念。假设有100个类别，我们向网络展示一张猫的图片。分类器为每个类别输出一个分数或置信度值，如下所示：
- en: 'Cat: 70%'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 猫：70%
- en: 'Dog: 20%'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 狗：20%
- en: 'Horse: 5%'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 马：5%
- en: 'Motorcycle: 4%'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摩托车：4%
- en: 'Car: 0.6%'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 汽车：0.6%
- en: 'Plane: 0.4%'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 飞机：0.4%
- en: This means the classifier was able to correctly predict the true class of the
    image in the top-1\. Try the same experiment for 100 images and observe how many
    times the classifier missed the true label, and that’s your top-1 error rate.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着分类器能够在top-1中正确预测图像的真实类别。尝试对100张图像进行相同的实验，并观察分类器错过真实标签的次数，这就是你的top-1错误率。
- en: The same idea holds for the top-5 error rate. In the example, if the true label
    is Horse, then the classifier missed the true label in the top-1 but caught it
    in the first five predicted classes (for example, top-5). Calculate how many times
    the classifier missed the true label in the top five predictions, and that’s your
    top-5.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的概念也适用于top-5错误率。在例子中，如果真实标签是马，那么分类器在top-1中错过了真实标签，但在前五个预测类别中捕捉到了它（例如，top-5）。计算分类器在top五预测中错过真实标签的次数，这就是你的top-5。
- en: Ideally, we want the model to always predict the correct class in the top-1\.
    But top-5 gives a more holistic evaluation of the model’s performance by defining
    how close the model is to the correct prediction for the missed classes.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望模型始终在top-1中预测正确的类别。但top-5通过定义模型对错过类别的正确预测有多接近，提供了对模型性能的更全面评估。
- en: 5.4 VGGNet
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 VGGNet
- en: VGGNet was developed in 2014 by the Visual Geometry Group at Oxford University
    (hence the name VGG).[3](#pgfId-1155487) The building components are exactly the
    same as those in LeNet and AlexNet, except that VGGNet is an even deeper network
    with more convolutional, pooling, and dense layers. Other than that, no new components
    are introduced here.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet于2014年由牛津大学视觉几何组开发（因此得名VGG）。[3](#pgfId-1155487)其构建组件与LeNet和AlexNet中的完全相同，只是VGGNet是一个更深层的网络，具有更多的卷积、池化和密集层。除此之外，这里没有引入新的组件。
- en: 'VGGNet, also known as VGG16, consists of 16 weight layers: 13 convolutional
    layers and 3 fully connected layers. Its uniform architecture makes it appealing
    in the DL community because it is very easy to understand.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet，也称为VGG16，由16个权重层组成：13个卷积层和3个全连接层。其均匀架构使其在深度学习社区中受到欢迎，因为它非常容易理解。
- en: 5.4.1 Novel features of VGGNet
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 VGGNet的新特性
- en: We’ve seen how challenging it can be to set up CNN hyperparameters like kernel
    size, padding, strides, and so on. VGGNet’s novel concept is that it has a simple
    architecture containing uniform components (convolutional and pooling layers).
    It improves on AlexNet by replacing large kernel-sized filters (11 and 5 in the
    first and second convolutional layers, respectively) with multiple 3 × 3 pool-size
    filters one after another.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，设置CNN超参数（如核大小、填充、步长等）可能具有挑战性。VGGNet的创新概念是它有一个简单的架构，包含均匀的组件（卷积和池化层）。它通过用多个3
    × 3池大小滤波器依次替换AlexNet中的大核大小滤波器（第一和第二卷积层分别为11和5）来改进AlexNet。
- en: 'The architecture is composed of a series of uniform convolutional building
    blocks followed by a unified pooling layer, where:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 架构由一系列均匀的卷积构建块组成，随后是一个统一的池化层，其中：
- en: All convolutional layers are 3 × 3 kernel-sized filters with a `strides` value
    of `1` and a `padding` value of `same`.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有卷积层都使用3 × 3核大小的滤波器，`strides`值为`1`，`padding`值为`same`。
- en: All pooling layers have a 2 × 2 pool size and a `strides` value of `2`.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有池化层都有2 × 2的池大小和`strides`值为`2`。
- en: Simonyan and Zisserman decided to use a smaller 3 × 3 kernel to allow the network
    to extract finer-level features of the image compared to AlexNet’s large kernels
    (11 × 11 and 5 × 5). The idea is that with a given convolutional receptive field,
    multiple stacked smaller kernels is better than a larger kernel because having
    multiple nonlinear layers increases the depth of the network; this enables it
    to learn more complex features at a lower cost because it has fewer learning parameters.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Simonyan和Zisserman决定使用一个更小的3 × 3核，以便网络能够提取比AlexNet的大核（11 × 11和5 × 5）更细粒度的图像特征。其理念是，在给定的卷积感受野中，多个堆叠的小核比一个大核更好，因为多个非线性层增加了网络的深度；这使得它能够在较低的成本下学习更复杂的特征，因为它具有更少的学习参数。
- en: For example, in their experiments, the authors noticed that a stack of two 3
    × 3 convolutional layers (without spatial pooling in between) has an effective
    receptive field of 5 × 5, and three 3 × 3 convolutional layers have the effect
    of a 7 × 7 receptive field. So by using 3 × 3 convolutions with higher depth,
    you get the benefits of using more nonlinear rectification layers (ReLU), which
    makes the decision function more discriminative. Second, this decreases the number
    of training parameters because when you use a three-layer 3 × 3 convolutional
    with C channels, the stack is parameterised by 32C2 = 27C2 weights compared to
    a single 7 × 7 convolutional layer that requires 72C2 = 49C2 weights, which is
    81% more parameters.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在他们的实验中，作者注意到两个3 × 3卷积层的堆叠（中间没有空间池化）具有5 × 5的有效感受野，而三个3 × 3卷积层的效果相当于7 × 7的感受野。因此，通过使用具有更高深度的3
    × 3卷积，你可以获得使用更多非线性整流层（ReLU）的好处，这使得决策函数更具判别性。其次，这减少了训练参数的数量，因为当你使用具有C通道的三层3 × 3卷积时，堆叠由32C2
    = 27C2权重参数化，而单个7 × 7卷积层需要72C2 = 49C2权重，这比81%更多的参数。
- en: Receptive field
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 感受野
- en: 'As explained in chapter 3, the receptive field is the effective area of the
    input image on which the output depends:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如第3章所述，感受野是输出所依赖的有效输入图像区域：
- en: '![](../Images/5-unnumb-3K.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-unnumb-3K.png)'
- en: This unified configuration of the convolutional and pooling components simplifies
    the neural network architecture, which makes it very easy to understand and implement.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这种卷积和池化组件的统一配置简化了神经网络架构，这使得它非常容易理解和实现。
- en: The VGGNet architecture is developed by stacking 3 × 3 convolutional layers
    with 2 × 2 pooling layers inserted after several convolutional layers. This is
    followed by the traditional classifier, which is composed of fully connected layers
    and a softmax, as depicted in figure 5.8.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet架构是通过堆叠3 × 3卷积层并在几个卷积层之后插入2 × 2池化层来开发的。这之后是传统的分类器，它由全连接层和softmax组成，如图5.8所示。
- en: '![](../Images/5-8.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-8.png)'
- en: Figure 5.8 VGGNet-16 architecture
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 VGGNet-16架构
- en: 5.4.2 VGGNet configurations
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 VGGNet配置
- en: Simonyan and Zisserman created several configurations for the VGGNet architecture,
    as shown in figure 5.9\. All of the configurations follow the same generic design.
    Configurations D and E are the most commonly used and are called VGG16 and VGG19,
    referring to the number of weight layers. Each block contains a series of 3 ×
    3 convolutional layers with similar hyperparameter configuration, followed by
    a 2 × 2 pooling layer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Simonyan和Zisserman为VGGNet架构创建了几个配置，如图5.9所示。所有配置都遵循相同的设计。配置D和E是最常用的，被称为VGG16和VGG19，指的是权重层的数量。每个块包含一系列具有类似超参数配置的3
    × 3卷积层，之后跟一个2 × 2池化层。
- en: '![](../Images/5-9.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-9.png)'
- en: Figure 5.9 VGGNet architecture configurations
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 VGGNet架构配置
- en: Table 5.1 lists the number of learning parameters (in millions) for each configuration.
    VGG16 yields ~138 million parameters; VGG19, which is a deeper version of VGGNet,
    has more than 144 million parameters. VGG16 is more commonly used because it performs
    almost as well as VGG19 but with fewer parameters.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1列出了每个配置的学习参数数量（以百万为单位）。VGG16产生约138百万个参数；VGG19，这是VGGNet的更深层版本，有超过144百万个参数。VGG16更常用，因为它几乎与VGG19的表现一样好，但参数更少。
- en: Table 5.1 VGGNet architecture parameters (in millions)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 VGGNet架构参数（以百万为单位）
- en: '| Network | A, A-LRN | B | C | D | E |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | A, A-LRN | B | C | D | E |'
- en: '| No. of parameters | 133 | 133 | 134 | 138 | 144 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 参数数量 | 133 | 133 | 134 | 138 | 144 |'
- en: VGG16 in Keras
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Keras中的VGG16
- en: Configurations D (VGG16) and E (VGG19) are the most commonly used configurations
    because they are deeper networks that can learn more complex functions. So, in
    this chapter, we will implement configuration D, which has 16 weight layers. VGG19
    (configuration E) can be similarly implemented by adding a fourth convolutional
    layer to the third, fourth, and fifth blocks as you can see in figure 5.9\. This
    chapter’s downloaded code includes a full implementation of both VGG16 and VGG19.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 配置D（VGG16）和E（VGG19）是最常用的配置，因为它们是更深层的网络，可以学习更复杂的函数。因此，在本章中，我们将实现配置D，它有16个权重层。VGG19（配置E）可以通过在第三、第四和第五块中添加一个第四卷积层来实现，如图5.9所示。本章下载的代码包括了VGG16和VGG19的完整实现。
- en: 'Note that Simonyan and Zisserman used the following regularization techniques
    to avoid overfitting:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Simonyan和Zisserman使用了以下正则化技术来避免过拟合：
- en: L2 regularization with weight decay of 5 × 10-4\. For simplicity, this is not
    added to the implementation that follows.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化，权重衰减为5 × 10^-4。为了简化，这个没有添加到下面的实现中。
- en: Dropout regularization for the first two fully connected layers, with a dropout
    ratio set to 0.5.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对前两个全连接层使用Dropout正则化，Dropout比率为0.5。
- en: 'The Keras code is as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Keras代码如下：
- en: '[PRE8]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Instantiates an empty sequential model
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化一个空的序列模型
- en: ❷ Prints the model summary
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印模型摘要
- en: 'When you print the model summary, you will see that the number of total parameters
    is ~138 million:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当你打印模型摘要时，你会看到总参数数量约为138百万：
- en: '[PRE9]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 5.4.3 Learning hyperparameters
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 学习超参数
- en: 'Simonyan and Zisserman followed a training procedure similar to that of AlexNet:
    the training is carried out using mini-batch gradient descent with momentum of
    0.9\. The learning rate is initially set to 0.01 and then decreased by a factor
    of 10 when the validation set accuracy stops improving.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Simonyan和Zisserman遵循了与AlexNet类似的训练程序：使用带有动量0.9的mini-batch梯度下降进行训练。初始学习率设置为0.01，当验证集准确率停止提高时，学习率减少10倍。
- en: 5.4.4 VGGNet performance
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 VGGNet性能
- en: 'VGG16 achieved a top-5 error rate of 8.1% on the ImageNet dataset compared
    to 15.3% achieved by AlexNet. VGG19 did even better: it was able to achieve a
    top-5 error rate of ~7.4%. It is worth noting that in spite of the larger number
    of parameters and the greater depth of VGGNet compared to AlexNet, VGGNet required
    fewer epochs to converge due to the implicit regularization imposed by greater
    depth and smaller convolutional filter sizes.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16在ImageNet数据集上实现了8.1%的top-5错误率，而AlexNet实现了15.3%。VGG19表现得更好：它能够实现大约7.4%的top-5错误率。值得注意的是，尽管与AlexNet相比，VGGNet的参数数量更多，深度更大，但由于深度更大和卷积滤波器尺寸更小，VGGNet需要更少的epoch来收敛。
- en: 5.5 Inception and GoogLeNet
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 Inception和GoogLeNet
- en: The Inception network came to the world in 2014 when a group of researchers
    at Google published their paper, “Going Deeper with Convolutions.”[4](#pgfId-1155663)
    The main hallmark of this architecture is building a deeper neural network while
    improving the utilization of the computing resources inside the network. One particular
    incarnation of the Inception network is called GoogLeNet and was used in the team’s
    submission for ILSVRC 2014\. It uses a network 22 layers deep (deeper than VGGNet)
    while reducing the number of parameters by 12 times (from ~138 million to ~13
    million) and achieving significantly more accurate results. The network used a
    CNN inspired by the classical networks (AlexNet and VGGNet) but implemented a
    novel element dubbed as the inception module.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Inception网络在2014年问世，当时谷歌的一组研究人员发表了他们的论文，“通过卷积加深网络。”[4](#pgfId-1155663) 这种架构的主要特点是构建一个更深的神经网络，同时提高网络内部计算资源的利用率。Inception网络的一个特定实现被称为GoogLeNet，并被用于团队在2014年ILSVRC的提交中。它使用了一个22层的网络（比VGGNet更深），通过将参数数量减少12倍（从约138百万减少到约1300万）并实现了显著更准确的结果。该网络使用了一个受经典网络（AlexNet和VGGNet）启发的CNN，但实现了一个被称为inception模块的新元素。
- en: 5.5.1 Novel features of Inception
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 Inception的新特性
- en: 'Szegedy et al. took a different approach when designing their network architecture.
    As we’ve seen in the previous networks, there are some architectural decisions
    that you need to make for each layer when you are designing a network, such as
    these:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy等人设计网络架构时采取了不同的方法。正如我们在前面的网络中看到的，在设计网络时，你需要为每一层做出一些架构决策，例如这些：
- en: 'The kernel size of the convolutional layer --We’ve seen in previous architectures
    that the kernel size varies: 1 × 1, 3 × 3, 5 × 5, and, in some cases, 11 × 11
    (as in AlexNet). When designing the convolutional layer, we find ourselves trying
    to pick and tune the kernel size of each layer that fits our dataset. Recall from
    chapter 3 that smaller kernels capture finer details of the image, whereas bigger
    filters will leave out minute details.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层的核大小——我们在之前的架构中看到，核大小是变化的：1 × 1、3 × 3、5 × 5，在某些情况下，11 × 11（如 AlexNet）。在设计卷积层时，我们发现自己试图挑选和调整适合我们数据集的每一层的核大小。回想第
    3 章，较小的核可以捕捉图像的更细微的细节，而较大的滤波器会忽略这些细节。
- en: When to use the pooling layer --AlexNet uses pooling layers every one or two
    convolutional layers to downsize spatial features. VGGNet applies pooling after
    every two, three, or four convolutional layers as the network gets deeper.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用池化层——AlexNet 在每个或每两个卷积层后使用池化层来缩小空间特征。VGGNet 在网络更深时，在每个两个、三个或四个卷积层后应用池化。
- en: Configuring the kernel size and positioning the pool layers are decisions we
    make mostly by trial and error and experiment with to get the optimal results.
    Inception says, “Instead of choosing a desired filter size in a convolutional
    layer and deciding where to place the pooling layers, let’s apply all of them
    all together in one block and call it the inception module.”
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 配置核大小和定位池化层是我们主要通过试错和实验来做出决定，以获得最佳结果。Inception 说：“与其在卷积层中选择一个期望的滤波器大小并决定池化层的位置，不如将它们全部应用在一个块中，并称之为
    Inception 模块。”
- en: That is, rather than stacking layers on top of each other as in classical architectures,
    Szegedy and his team suggest that we create an inception module consisting of
    several convolutional layers with different kernel sizes. The architecture is
    then developed by stacking the inception modules on top of each other. Figure
    5.10 shows how classical convolutional networks are architected versus the Inception
    network.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，与经典架构中层层堆叠的方式不同，Szegedy 和他的团队建议我们创建一个由多个不同核大小的卷积层组成的 Inception 模块。然后，通过堆叠
    Inception 模块来发展架构。图 5.10 展示了经典卷积网络与 Inception 网络的架构对比。
- en: '![](../Images/5-10.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-10.png)'
- en: Figure 5.10 Classical convolutional networks vs. the Inception network
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 经典卷积网络与 Inception 网络对比
- en: 'From the diagram, you can observe the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中，你可以观察到以下内容：
- en: In classical architectures like LeNet, AlexNet, and VGGNet, we stack convolutional
    and pooling layers on top of each other to build the feature extractors. At the
    end, we add the dense fully connected layers to build the classifier.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 LeNet、AlexNet 和 VGGNet 等经典架构中，我们堆叠卷积层和池化层来构建特征提取器。最后，我们添加密集的全连接层来构建分类器。
- en: In the Inception architecture, we start with a convolutional layer and a pooling
    layer, stack the inception modules and pooling layers to build the feature extractors,
    and then add the regular dense classifier layers.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Inception 架构中，我们从一个卷积层和一个池化层开始，堆叠 Inception 模块和池化层来构建特征提取器，然后添加常规的密集分类层。
- en: We’ve been treating the inception modules as black boxes to understand the bigger
    picture of the Inception architecture. Now, we will unpack the inception module
    to understand how it works.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直将 Inception 模块视为黑盒，以理解 Inception 架构的全貌。现在，我们将拆解 Inception 模块，以了解其工作原理。
- en: '5.5.2 Inception module: Naive version'
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 Inception 模块：朴素版本
- en: 'The inception module is a combination of four layers:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 模块由四个层组成：
- en: 1 × 1 convolutional layer
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层
- en: 3 × 3 convolutional layer
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 × 3 卷积层
- en: 5 × 5 convolutional layer
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5 × 5 卷积层
- en: 3 × 3 max-pooling layer
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 × 3 最大池化层
- en: The outputs of these layers are concatenated into a single output volume forming
    the input of the next stage. The naive representation of the inception module
    is shown in figure 5.11.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层的输出被连接成一个单一的输出体积，形成下一阶段的输入。朴素版本的 Inception 模块在图 5.11 中展示。
- en: 'The diagram may look a little overwhelming, but the idea is simple to understand.
    Let’s follow along with this example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图表可能看起来有点令人眼花缭乱，但理念简单易懂。让我们通过以下示例来理解：
- en: Suppose we have an input dimensional volume from the previous layer of size
    32 × 32 × 200.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们有一个来自前一层的输入维度体积，大小为 32 × 32 × 200。
- en: 'We feed this input to four convolutions simultaneously:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们同时将这个输入馈送到四个卷积中：
- en: 1 × 1 convolutional layer with `depth` = `64` and `padding` = `same`. The output
    of this kernel = 32 × 32 × 64.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth` = `64` 和 `padding` = `same` 的 1 × 1 卷积层。这个核的输出 = 32 × 32 × 64。'
- en: 3 × 3 convolutional layer with `depth` = `128` and `padding` = `same`. Output
    = 32 × 32 × 128.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth` = `128` 和 `padding` = `same` 的 3 × 3 卷积层。输出 = 32 × 32 × 128。'
- en: 5 × 5 convolutional layer with `depth` = `32` and `padding` = `same`. Output
    = 32 × 32 × 32.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`depth` = `32` 和 `padding` = `same` 的 5 × 5 卷积层。输出 = 32 × 32 × 32。'
- en: 3 × 3 max-pooling layer with `padding` = `same` and `strides` = `1`. Output
    = 32 × 32 × 32.
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding` = `same` 和 `strides` = `1` 的 3 × 3 最大池化层。输出 = 32 × 32 × 32。'
- en: We concatenate the depth of the four outputs to create one output volume of
    dimensions 32 × 32 × 256.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将四个输出的深度连接起来，创建一个维度为 32 × 32 × 256 的一维输出体积。
- en: '![](../Images/5-11.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-11.png)'
- en: Figure 5.11 Naive representation of an inception module
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 Naive representation of an inception module
- en: Now we have an inception module that takes an input volume of 32 × 32 × 200
    and outputs a volume of 32 × 32 × 256.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个 inception 模块，它接受一个 32 × 32 × 200 的输入体积，并输出一个 32 × 32 × 256 的体积。
- en: NOTE In the previous example, we use a `padding` value of `same`. In Keras,
    `padding` can be set to `same` or `valid`, as we saw in chapter 3\. The `same`
    value results in padding the input such that the output has the same length as
    the original input. We do that because we want the output to have width and height
    dimensions similar to the input. And we want to output similar dimensions in the
    inception module to simplify the depth concatenation process. Now we can just
    add up the depths of all the outputs to concatenate them into one output volume
    to be fed to the next layer in our network.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在前一个例子中，我们使用了一个 `padding` 值为 `same`。在 Keras 中，`padding` 可以设置为 `same` 或 `valid`，正如我们在第
    3 章中看到的。`same` 值会导致填充输入，使得输出长度与原始输入相同。我们这样做是因为我们希望输出具有与输入相似的宽度和高度维度。我们希望在 inception
    模块中输出相似的维度以简化深度连接过程。现在我们只需将所有输出的深度相加，将它们连接成一个输出体积，然后将其送入我们网络中的下一层。
- en: 5.5.3 Inception module with dimensionality reduction
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.3 具有维度缩减的 Inception 模块
- en: The naive representation of the inception module that we just saw has a big
    computational cost problem that comes with processing larger filters like the
    5 × 5 convolutional layer. To get a better sense of the compute problem with the
    naive representation, let’s calculate the number of operations that will be performed
    for the 5 × 5 convolutional layer in the previous example.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的 inception 模块的朴素表示法在处理像 5 × 5 卷积层这样的大尺寸滤波器时存在一个大的计算成本问题。为了更好地理解朴素表示法中的计算问题，让我们计算一下前一个例子中
    5 × 5 卷积层将要执行的操作数量。
- en: The input volume with dimensions of 32 × 32 × 200 will be fed to the 5 × 5 convolutional
    layer of 32 filters with dimensions = 5 × 5 × 32\. This means the total number
    of multiplications that the computer needs to compute is 32 × 32 × 200 multiplied
    by 5 × 5 × 32, which is more than 163 million operations. While we can perform
    this many operations with modern computers, this is still pretty expensive. This
    is when the dimensionality reduction layers can be very useful.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 维度为 32 × 32 × 200 的输入体积将被送入 32 个滤波器的 5 × 5 卷积层，滤波器维度 = 5 × 5 × 32。这意味着计算机需要计算的总乘法次数是
    32 × 32 × 200 乘以 5 × 5 × 32，这超过了 1.63 亿次操作。虽然我们可以用现代计算机执行这么多操作，但这仍然相当昂贵。这就是维度缩减层可以非常有用的时候。
- en: Dimensionality reduction layer (1 × 1 convolutional layer)
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 维度缩减层（1 × 1 卷积层）
- en: The 1 × 1 convolutional layer can reduce the operational cost of 163 million
    operations to about a tenth of that. That is why it is called a reduce layer.
    The idea here is to add a 1 × 1 convolutional layer before the bigger kernels
    like the 3 × 3 and 5 × 5 convolutional layers, to reduce their depth, which in
    turn will reduce the number of operations.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层可以将 1.63 亿次操作的操作成本降低到十分之一左右。这就是为什么它被称为缩减层。这里的想法是在 3 × 3 和 5 × 5 卷积层等更大的核之前添加一个
    1 × 1 卷积层，以减少它们的深度，从而减少操作的数量。
- en: Let’s look at an example. Suppose we have an input dimension volume of 32 ×
    32 × 200\. We then add a 1 × 1 convolutional layer with a depth of 16\. This reduces
    the dimension volume from 200 to 16 channels. We can then apply the 5 × 5 convolutional
    layer on the output, which has much less depth (figure 5.12).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。假设我们有一个输入维度体积为 32 × 32 × 200。然后我们添加一个深度为 16 的 1 × 1 卷积层。这将从 200 个通道减少维度体积到
    16 个通道。然后我们可以在输出上应用 5 × 5 卷积层，其深度要小得多（图 5.12）。
- en: '![](../Images/5-12.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-12.png)'
- en: Figure 5.12 Dimensionality reduction is used to reduce the computational cost
    by reducing the depth of the layer.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 使用降维通过减少层的深度来降低计算成本。
- en: Notice that the 32 × 32 × 200 input is processed through the two convolutional
    layers and outputs a volume of dimensions 32 × 32 × 32, which is the same as produced
    without applying the dimensionality reduction layer. But here, instead of processing
    the 5 × 5 convolutional layer on the entire 200 channels of the input volume,
    we take this huge volume and shrink its representation to a much smaller intermediate
    volume that has only 16 channels.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，32 × 32 × 200 的输入通过两个卷积层处理后，输出一个维度为 32 × 32 × 32 的体积，这与未应用降维层时产生的体积相同。但在这里，我们不是在整个输入体积的
    200 个通道上处理 5 × 5 卷积层，而是将这个巨大的体积缩小到只有 16 个通道的更小的中间体积。
- en: 'Now, let’s look at the computational cost involved in this operation and compare
    it to the 163 million multiplications that we got before applying the reduce layer:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个操作涉及的计算成本，并将其与我们之前应用降维层时得到的 1.63 亿次乘法进行比较：
- en: Computation
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 计算量
- en: = operations in the 1 × 1 convolutional layer + operations in the 5 × 5 convolutional
    layer
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: = 1 × 1 卷积层中的操作 + 5 × 5 卷积层中的操作
- en: = (32 × 32 × 200) multiplied by (1 × 1 × 16 + 32 × 32 × 16) multiplied by (5
    × 5 × 32)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: = (32 × 32 × 200) 乘以 (1 × 1 × 16 + 32 × 32 × 16) 乘以 (5 × 5 × 32)
- en: = 3.2 million + 13.1 million
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: = 320 万 + 1310 万
- en: The total number of multiplications in this operation is 16.3 million, which
    is a tenth of the 163 million multiplications that we calculated without the reduce
    layers.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作中的总乘法次数为 1630 万次，是未使用降维层时计算的 1.63 亿次的十分之一。
- en: The 1 × 1 convolutional layer
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层
- en: 'The idea of the 1 × 1 convolutional layer is that it preserves the spatial
    dimensions (height and width) of the input volume but changes the number of channels
    of the volume (depth):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层的思想是它保留了输入体积的空间维度（高度和宽度），但改变了体积的通道数（深度）：
- en: '![](../Images/5-unnumb-5Key.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-unnumb-5Key.png)'
- en: 1 × 1 conv layers preserve the spatial dimensions but change the depth.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层保留了空间维度，但改变了深度。
- en: 'The 1 × 1 convolutional layers are also known as bottleneck layers because
    the bottleneck is the smallest part of the bottle and reduce layers reduce the
    dimensionality of the network, making it look like a bottleneck:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层也被称为瓶颈层，因为瓶颈是瓶子中最小的一部分，降维层减少了网络的维度，使其看起来像瓶颈：
- en: '![](../Images/5-unnumb-6K.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-unnumb-6K.png)'
- en: 1 × 1 convolutional layers are called bottleneck layers.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层被称为瓶颈层。
- en: Impact of dimensionality reduction on network performance
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 降维对网络性能的影响
- en: You might be wondering whether shrinking the representation size so dramatically
    hurts the performance of the neural network. Szegedy et al. ran experiments and
    found that as long as you implement the reduce layer in moderation, you can shrink
    the representation size significantly without hurting performance--and save a
    lot of computations.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，如此大幅度地缩小表示大小是否会损害神经网络的性能。Szegedy 等人进行了实验，发现只要适度地实现降维层，就可以显著缩小表示大小而不会损害性能——并且节省大量计算。
- en: Now, let’s put the reduce layers into action and build a new inception module
    with dimensionality reduction. To do that, we will keep the same concept of concatenating
    the four layers from the naive representation. We will add a 1 × 1 convolutional
    reduce layer before the 3 × 3 and 5 × 5 convolutional layers to reduce their computational
    cost. We will also add a 1 × 1 convolutional layer after the 3 × 3 max-pooling
    layer because pooling layers don’t reduce the depth for their inputs. So, we will
    need to apply the reduce layer to their output before we do the concatenation
    (figure 5.13).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将降维层付诸实践，并构建一个新的具有降维功能的 inception 模块。为此，我们将保持从原始表示中连接四个层的相同概念。我们将在 3 ×
    3 和 5 × 5 卷积层之前添加一个 1 × 1 卷积降维层，以降低它们的计算成本。我们还将添加一个 1 × 1 卷积层在 3 × 3 最大池化层之后，因为池化层不会减少其输入的深度。因此，在我们进行连接之前，我们需要将降维层应用于它们的输出（图
    5.13）。
- en: '![](../Images/5-13.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-13.png)'
- en: Figure 5.13 Building an inception module with dimensionality reduction
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 使用降维构建 inception 模块
- en: We add dimensionality reduction prior to bigger convolutional layers to allow
    for increasing the number of units at each stage significantly without an uncontrolled
    blowup in computational complexity at later stages. Furthermore, the design follows
    the practical intuition that visual information should be processed at various
    scales and then aggregated so that the next stage can abstract features from the
    different scales simultaneously.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在更大的卷积层之前添加了降维，以便在后续阶段计算复杂度不受控制地激增之前，显著增加每个阶段的单元数量。此外，设计遵循了实用的直觉，即视觉信息应该在各种尺度上处理，然后汇总，以便下一阶段可以同时从不同尺度抽象特征。
- en: Recap of inception modules
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Inception模块的回顾
- en: To summarize, if you are building a layer of a neural network and you don’t
    want to have to decide what filter size to use in the convolutional layers or
    when to add pooling layers, the inception module lets you use them all and concatenate
    the depth of all the outputs. This is called the naive representation of the inception
    module.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果您正在构建神经网络的一层，并且不想决定在卷积层中使用什么滤波器大小或何时添加池化层，Inception模块允许您使用所有这些，并将所有输出的深度连接起来。这被称为Inception模块的朴素表示。
- en: We then run into the problem of computational cost that comes with using large
    filters. Here, we use a 1 × 1 convolutional layer called the reduce layer that
    reduces the computational cost significantly. We add reduce layers before the
    3 × 3 and 5 × 5 convolutional layers and after the max-pooling layer to create
    an inception module with dimensionality reduction.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遇到了使用大滤波器带来的计算成本问题。在这里，我们使用了一个称为reduce层的1 × 1卷积层，它显著降低了计算成本。我们在3 × 3和5
    × 5卷积层之前以及最大池化层之后添加reduce层，以创建一个具有降维的Inception模块。
- en: 5.5.4 Inception architecture
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.4 Inception架构
- en: Now that we understand the components of the inception module, we are ready
    to build the Inception network architecture. We use the dimension reduction representation
    of the inception module, stack inception modules on top of each other, and add
    a 3 × 3 pooling layer in between for downsampling, as shown in figure 5.14.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Inception模块的组成部分，我们准备构建Inception网络架构。我们使用Inception模块的降维表示，将Inception模块堆叠在一起，并在它们之间添加一个3
    × 3池化层以进行下采样，如图5.14所示。
- en: '![](../Images/5-14.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-14.png)'
- en: Figure 5.14 We build the Inception network by adding a stack of inception modules
    on top of each other.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14我们通过将Inception模块堆叠在一起来构建Inception网络。
- en: We can stack as many inception modules as we want to build a very deep convolutional
    network. In the original paper, the team built a specific incarnation of the inception
    module and called it GoogLeNet. They used this network in their submission for
    the ILSVRC 2014 competition. The GoogLeNet architecture is shown in figure 5.15.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以堆叠任意数量的Inception模块来构建一个非常深的卷积网络。在原始论文中，该团队构建了一个特定的Inception模块实例，并将其称为GoogLeNet。他们在2014年ILSVRC竞赛的提交中使用了这个网络。GoogLeNet架构如图5.15所示。
- en: '![](../Images/5-15.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-15.png)'
- en: 'Figure 5.15 The full GoogLeNet model consists of three parts: the first part
    has the classical CNN architecture like AlexNet and LeNet, the second part is
    a stack of inceptions modules and pooling layers, and the third part is the traditional
    fully connected classifiers.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15完整的GoogLeNet模型由三部分组成：第一部分具有类似于AlexNet和LeNet的经典CNN架构，第二部分是一堆Inception模块和池化层，第三部分是传统的全连接分类器。
- en: 'As you can see, GoogLeNet uses a stack of a total of nine inception modules
    and a max pooling layer every several blocks to reduce dimensionality. To simplify
    this implementation, we are going to break down the GoogLeNet architecture into
    three parts:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，GoogLeNet使用了一堆总共九个Inception模块，并在每隔几个块中使用最大池化层来降低维度。为了简化这个实现，我们将GoogLeNet架构分解为三个部分：
- en: Part A--Identical to the AlexNet and LeNet architectures; contains a series
    of convolutional and pooling layers.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A部分--与AlexNet和LeNet架构相同；包含一系列卷积和池化层。
- en: 'Part B --Contains nine inception modules stacked as follows: two inception
    modules + pooling layer + five inception modules + pooling layer + five inception
    modules.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B部分 --包含九个Inception模块堆叠如下：两个Inception模块 + 池化层 + 五个Inception模块 + 池化层 + 五个Inception模块。
- en: Part C --The classifier part of the network, consisting of the fully connected
    and softmax layers.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C部分 --网络的分类器部分，包括全连接和softmax层。
- en: 5.5.5 GoogLeNet in Keras
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.5 Keras中的GoogLeNet
- en: 'Now, let’s implement the GoogLeNet architecture in Keras (figure 5.16). Notice
    that the inception module takes the features from the previous module as input,
    passes them through four routes, concatenates the depth of the output of all four
    routes, and then passes the concatenated output to the next module. The four routes
    are as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Keras中实现GoogLeNet架构（图5.16）。注意，inception模块将前一个模块的特征作为输入，通过四个路径传递，将所有四个路径的输出深度连接起来，然后将连接后的输出传递到下一个模块。这四个路径如下：
- en: 1 × 1 convolutional layer
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层
- en: 1 × 1 convolutional layer + 3 × 3 convolutional layer
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层 + 3 × 3 卷积层
- en: 1 × 1 convolutional layer + 5 × 5 convolutional layer
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 × 1 卷积层 + 5 × 5 卷积层
- en: 3 × 3 pooling layer + 1 × 1 convolutional layer
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 × 3 池化层 + 1 × 1 卷积层
- en: '![](../Images/5-16.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-16.png)'
- en: Figure 5.16 The inception module of GoogLeNet
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 GoogLeNet的inception模块
- en: 'First we’ll build the `inception_module` function. It takes the number of filters
    of each convolutional layer as an argument and returns the concatenated output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将构建`inception_module`函数。它接受每个卷积层的滤波器数量作为参数，并返回连接后的输出：
- en: '[PRE10]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Creates the 1 × 1 convolutional layer that takes its input directly from the
    previous layer
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个1 × 1卷积层，其输入直接来自前一个层
- en: ❷ Concatenates together the depth of the three filters
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将三个滤波器的深度连接在一起
- en: GoogLeNet architecture
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GoogLeNet架构
- en: Now that the `inception_module` function is ready, let’s build the GoogLeNet
    architecture from figure 5.16\. To get the values of the `inception_module` function’s
    arguments, we will go through figure 5.17, which represents the hyperparameters
    set up as implemented by Szegedy et al. in the original paper. (Note that “#3
    × 3 reduce” and “#5 × 5 reduce” in the figure represent the 1 × 1 filters in the
    reduction layers that are used before the 3 × 3 and 5 × 5 convolutional layers.)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`inception_module`函数已经准备好了，让我们根据图5.16构建GoogLeNet架构。为了获取`inception_module`函数参数的值，我们将查看图5.17，它代表了Szegedy等人在原始论文中实现的超参数设置。（注意，图中的“#3
    × 3 reduce”和“#5 × 5 reduce”代表在3 × 3和5 × 5卷积层之前使用的1 × 1滤波器层。）
- en: '![](../Images/5-17.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-17.png)'
- en: Figure 5.17 Hyperparameters implemented by Szegedy et al. in the original Inception
    paper
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 Szegedy等人原Inception论文中实现的超参数
- en: Now, let’s go through the implementations of parts A, B, and C.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐一查看部分A、B和C的实现。
- en: 'Part A: Building the bottom part of the network'
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Part A: 构建网络的底部部分'
- en: Let’s build the bottom part of the network. This part consists of a 7 × 7 convolutional
    layer ⇒ 3 × 3 pooling layer ⇒ 1 × 1 convolutional layer ⇒ 3 × 3 convolutional
    layer ⇒ 3 × 3 pooling layer, as you can see in figure 5.18.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建网络的底部部分。这部分包括一个7 × 7卷积层⇒ 3 × 3池化层⇒ 1 × 1卷积层⇒ 3 × 3卷积层⇒ 3 × 3池化层，如图5.18所示。
- en: '![](../Images/5-18.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-18.png)'
- en: Figure 5.18 The bottom part of the network
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18网络的底部部分
- en: In the `LocalResponseNorm` layer, similar to in AlexNet, local response normalization
    is used to help speed up convergence. Nowadays, batch normalization is used instead.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在`LocalResponseNorm`层中，类似于AlexNet，使用局部响应归一化来帮助加速收敛。如今，批量归一化被用来代替。
- en: 'Here is the Keras code for part A:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是部分A的Keras代码：
- en: '[PRE11]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Part B: Building the inception modules and max-pooling layers'
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Part B: 构建inception模块和最大池化层'
- en: 'To build inception modules 3a and 3b and the first max-pooling layer, we use
    table 5.2 to start. The code is as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建inception模块3a和3b以及第一个最大池化层，我们使用表5.2开始。代码如下：
- en: Table 5.2 Inception modules 3a and 3b
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 Inception模块3a和3b
- en: '| Type | #1 × 1 | #3 × 3 reduce | #3 × 3 | #5 × 5 reduce | #5 × 5 | Pool proj
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | #1 × 1 | #3 × 3 reduce | #3 × 3 | #5 × 5 reduce | #5 × 5 | Pool proj
    |'
- en: '| Inception (3a) | 064 | 096 | 128 | 16 | 32 | 32 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| Inception (3a) | 064 | 096 | 128 | 16 | 32 | 32 |'
- en: '| Inception (3b) | 128 | 128 | 192 | 32 | 96 | 64 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Inception (3b) | 128 | 128 | 192 | 32 | 96 | 64 |'
- en: '[PRE12]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similarly, let’s create inception modules 4a, 4b, 4c, 4d, and 4e and the max
    pooling layer:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，让我们创建inception模块4a、4b、4c、4d和4e以及最大池化层：
- en: '[PRE13]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, let’s create modules 5a and 5b:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建模块5a和5b：
- en: '[PRE14]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Part C: Building the classifier part'
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Part C: 构建分类器部分'
- en: 'In their experiments, Szegedy et al. found that adding an 7 × 7 average pooling
    layer improved the top-1 accuracy by about 0.6%. They then added a dropout layer
    with 40% probability to reduce overfitting:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的实验中，Szegedy等人发现添加一个7 × 7平均池化层将top-1准确率提高了约0.6%。然后他们添加了一个40%概率的dropout层来减少过拟合：
- en: '[PRE15]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 5.5.6 Learning hyperparameters
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.6 学习超参数
- en: 'The team used a SGD gradient descent optimizer with 0.9 momentum. They also
    implemented a fixed learning rate decay schedule of 4% every 8 epochs. An example
    of how to implement the training specifications similar to the paper is as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 该团队使用了一个动量为 0.9 的 SGD 梯度下降优化器。他们还实施了一个每 8 个周期固定学习率衰减计划，衰减率为 4%。以下是如何实现与论文中类似的训练规范的示例：
- en: '[PRE16]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Implements the learning rate decay function
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实现学习率衰减函数
- en: 5.5.7 Inception performance on the CIFAR dataset
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.7 Inception 在 CIFAR 数据集上的性能
- en: GoogLeNet was the winner of the ILSVRC 2014 competition. It achieved a top-5
    error rate of 6.67%, which was very close to human-level performance and much
    better than previous CNNs like AlexNet and VGGNet.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet 在 2014 年的 ILSVRC 竞赛中获胜。它实现了 6.67% 的顶级错误率，这非常接近人类水平的表现，并且比之前的 CNN，如
    AlexNet 和 VGGNet，要好得多。
- en: 5.6 ResNet
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 ResNet
- en: The Residual Neural Network (ResNet) was developed in 2015 by a group from the
    Microsoft Research team.[5](#pgfId-1156076) They introduced a novel residual module
    architecture with skip connections. The network also features heavy batch normalization
    for the hidden layers. This technique allowed the team to train very deep neural
    networks with 50, 101, and 152 weight layers while still having lower complexity
    than smaller networks like VGGNet (19 layers). ResNet was able to achieve a top-5
    error rate of 3.57% in the ILSVRC 2015 competition, which beat the performance
    of all prior ConvNets.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Residual Neural Network (ResNet) 是由微软研究团队在 2015 年开发的。[5](#pgfId-1156076) 他们引入了一种具有跳过连接的新颖残差模块架构。该网络还针对隐藏层进行了大量的批量归一化。这项技术使得团队能够训练具有
    50、101 和 152 个权重层的非常深的神经网络，同时其复杂度仍低于 VGGNet（19 层）等较小的网络。ResNet 在 2015 年的 ILSVRC
    竞赛中实现了 3.57% 的顶级错误率，超过了所有先前卷积神经网络的表现。
- en: 5.6.1 Novel features of ResNet
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 ResNet 的新特性
- en: Looking at how neural network architectures evolved from LeNet, AlexNet, VGGNet,
    and Inception, you might have noticed that the deeper the network, the larger
    its learning capacity, and the better it extracts features from images. This mainly
    happens because very deep networks are able to represent very complex functions,
    which allows the network to learn features at many different levels of abstraction,
    from edges (at the lower layers) to very complex features (at the deeper layers).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 观察神经网络架构从 LeNet、AlexNet、VGGNet 和 Inception 的发展，你可能会注意到，网络越深，其学习容量越大，并且从图像中提取特征的能力越强。这主要是因为非常深的网络能够表示非常复杂的函数，这使得网络能够在许多不同层次的抽象中学习特征，从边缘（在底层）到非常复杂的特征（在深层）。
- en: 'Earlier in this chapter, we saw deep neural networks like VGGNet-19 (19 layers)
    and GoogLeNet (22 layers). Both performed very well in the ImageNet challenge.
    But can we build even deeper networks? We learned from chapter 4 that one downside
    of adding too many layers is that doing so makes the network more prone to overfit
    the training data. This is not a major problem because we can use regularization
    techniques like dropout, L2 regularization, and batch normalization to avoid overfitting.
    So, if we can take care of the overfitting problem, wouldn’t we want to build
    networks that are 50, 100, or even 150 layers deep? The answer is yes. We definitely
    should try to build very deep neural networks. We need to fix just one other problem,
    to unblock the capability of building super-deep networks: a phenomenon called
    vanishing gradients.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们看到了像 VGGNet-19（19 层）和 GoogLeNet（22 层）这样的深度神经网络。它们在 ImageNet 挑战中都表现出色。但我们能否构建更深层的网络呢？我们从第
    4 章中了解到，添加太多层的缺点是这样做会使网络更容易过拟合训练数据。这不是一个主要问题，因为我们可以使用正则化技术，如 dropout、L2 正则化和批量归一化来避免过拟合。因此，如果我们能够解决过拟合问题，我们难道不想构建深度为
    50、100 或甚至 150 层的网络吗？答案是肯定的。我们确实应该尝试构建非常深的神经网络。我们只需要解决另一个问题，即解开构建超深层网络的能力：一种称为梯度消失的现象。
- en: Vanishing and exploding gradients
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失和梯度爆炸
- en: The problem with very deep networks is that the signal required to change the
    weights becomes very small at earlier layers. To understand why, let’s consider
    the gradient descent process explained in chapter 2\. As the network backpropagates
    the gradient of the error from the final layer back to the first layer, it is
    multiplied by the weight matrix at each step; thus the gradient can decrease exponentially
    quickly to zero, leading to a vanishing gradient phenomenon that prevents the
    earlier layers from learning. As a result, the network’s performance gets saturated
    or even starts to degrade rapidly.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 非常深网络的难题在于，用于改变权重的信号在早期层变得非常小。为了理解原因，让我们考虑第2章中解释的梯度下降过程。当网络反向传播从最终层到第一层的误差梯度时，它在每一步都会乘以权重矩阵；因此梯度可以迅速指数级下降到零，导致梯度消失现象，阻止早期层学习。结果，网络的性能变得饱和，甚至开始迅速退化。
- en: In other cases, the gradient grows exponentially quickly and “explodes” to take
    very large values. This phenomenon is called exploding gradients.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，梯度会迅速指数级增长并“爆炸”到非常大的值。这种现象称为梯度爆炸。
- en: 'To solve the vanishing gradient problem, He et al. created a shortcut that
    allows the gradient to be directly backpropagated to earlier layers. These shortcuts
    are called skip connections : they are used to flow information from earlier layers
    in the network to later layers, creating an alternate shortcut path for the gradient
    to flow through. Another important benefit of the skip connections is that they
    allow the model to learn an identity function, which ensures that the layer will
    perform at least as well as the previous layer (figure 5.19).'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决梯度消失问题，He等人创建了一个捷径，允许梯度直接反向传播到早期层。这些捷径被称为跳跃连接：它们用于将网络早期层的信息流向后期层，为梯度流动创建一个替代的捷径路径。跳跃连接的另一个重要好处是，它们允许模型学习一个恒等函数，这确保该层至少与前一层的性能一样好（图5.19）。
- en: '![](../Images/5-19.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-19.png)'
- en: Figure 5.19 Traditional network without skip connections (left); network with
    a skip connection (right).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 无跳跃连接的传统网络（左侧）；有跳跃连接的网络（右侧）。
- en: 'At left in figure 5.19 is the traditional stacking of convolutional layers
    one after the other. On the right, we still stack convolutional layers as before,
    but we also add the original input to the output of the convolutional block. This
    is a skip connection. We then add both signals: skip connection + main path.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.19的左侧是传统的逐层堆叠的卷积层。在右侧，我们仍然像以前一样堆叠卷积层，但同时也将原始输入添加到卷积块的输出中。这是一个跳跃连接。然后我们添加两个信号：跳跃连接
    + 主路径。
- en: 'Note that the shortcut arrow points to the end of the second convolutional
    layer--not after it. The reason is that we add both paths before we apply the
    ReLU activation function of this layer. As you can see in figure 5.20, the *x*
    signal is passed along the shortcut path and then added to the main path, *f*(*x*).
    Then, we apply the ReLU activation to *f*(*x*) + *x* to produce the output signal:
    relu( *f*(*x*) + *x* ).'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，跳跃箭头指向第二个卷积层的末端——而不是之后。原因是我们在应用该层的ReLU激活函数之前添加了两个路径。如图5.20所示，*x*信号沿着捷径路径传递，然后添加到主路径，*f*(*x*)。然后，我们对*f*(*x*)
    + *x*应用ReLU激活，以产生输出信号：relu( *f*(*x*) + *x* )。
- en: '![](../Images/5-20.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-20.png)'
- en: Figure 5.20 Adding the paths and applying the ReLU activation function to solve
    the vanishing gradient problem that usually comes with very deep networks
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 添加路径并应用ReLU激活函数以解决通常伴随非常深网络的梯度消失问题
- en: 'The code implementation of the skip connection is straightforward:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃连接的代码实现很简单：
- en: '[PRE17]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Stores the value of the shortcut to be equal to the input x
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将捷径的值存储为等于输入x
- en: '❷ Performs the main path operations: CONV + ReLU + CONV'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 执行主路径操作：CONV + ReLU + CONV
- en: ❸ Adds both paths together
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将两个路径合并
- en: ❹ Applies the ReLU activation function
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 应用ReLU激活函数
- en: This combination of the skip connection and convolutional layers is called a
    residual block. Similar to the Inception network, ResNet is composed of a series
    of these residual block building blocks that are stacked on top of each other
    (figure 5.21).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这种跳跃连接和卷积层的组合被称为残差块。类似于Inception网络，ResNet由一系列这些残差块构建块组成，它们堆叠在一起（图5.21）。
- en: '![](../Images/5-21.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-21.png)'
- en: Figure 5.21 Classical CNN architecture (left). The Inception network consists
    of a set of inception modules (middle). The residual network consists of a set
    of residual blocks (right).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 经典CNN架构（左）。Inception网络由一系列Inception模块组成（中间）。残差网络由一系列残差块组成（右）。
- en: 'From the figure, you can observe the following:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中，你可以观察到以下内容：
- en: '*Feature extractors* --To build the feature extractor part of ResNet, we start
    with a convolutional layer and a pooling layer and then stack residual blocks
    on top of each other to build the network. When we are designing our ResNet network,
    we can add as many residual blocks as we want to build even deeper networks.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征提取器* --为了构建ResNet的特征提取器部分，我们从一个卷积层和一个池化层开始，然后在每个残差块上堆叠以构建网络。当我们设计我们的ResNet网络时，我们可以添加尽可能多的残差块来构建甚至更深的网络。'
- en: '*Classifiers* --The classification part is still the same as we learned for
    other networks: fully connected layers followed by a softmax.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类器* --分类部分与其他网络中我们学习的内容相同：全连接层后跟softmax。'
- en: Now that you know what a skip connection is and you are familiar with the high-level
    architecture of ResNet, let’s unpack residual blocks to understand how they work.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了什么是跳跃连接，并且熟悉了ResNet的高级架构，让我们来拆解残差块以了解它们是如何工作的。
- en: 5.6.2 Residual blocks
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 残差块
- en: 'A residual module consists of two branches:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 一个残差模块由两个分支组成：
- en: '*Shortcut pat*h (figure 5.22)--Connects the input to an addition of the second
    branch.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*捷径路径*（图5.22）--将输入连接到第二分支的加法。'
- en: '*Main path* --A series of convolutions and activations. The main path consists
    of three convolutional layers with ReLU activations. We also add batch normalization
    to each convolutional layer to reduce overfitting and speed up training. The main
    path architecture looks like this: [CONV ⇒ BN ⇒ ReLU] × 3.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主路径* --一系列卷积和激活。主路径由三个具有ReLU激活的卷积层组成。我们还在每个卷积层中添加批量归一化以减少过拟合并加快训练速度。主路径架构如下：[CONV
    ⇒ BN ⇒ ReLU] × 3。'
- en: '![](../Images/5-22.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-22.png)'
- en: Figure 5.22 The output of the main path is added to the input value through
    the shortcut before they are fed to the ReLU function.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22 主路径的输出通过捷径在输入值被馈送到ReLU函数之前被添加。
- en: Similar to what we explained earlier, the shortcut path is added to the main
    path right before the activation function of the last convolutional layer. Then
    we apply the ReLU function after adding the two paths.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前解释的类似，捷径路径在最后一个卷积层的激活函数之前被添加到主路径上。然后我们在添加两条路径后应用ReLU函数。
- en: Notice that there are no pooling layers in the residual block. Instead, He et
    al. decided to do dimension downsampling using bottleneck 1 × 1 convolutional
    layers, similar to the Inception network. So, each residual block starts with
    a 1 × 1 convolutional layer to downsample the input dimension volume, and a 3
    × 3 convolutional layer and another 1 × 1 convolutional layer to downsample the
    output. This is a good technique to keep control of the volume dimensions across
    many layers. This configuration is called a bottleneck residual block.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，残差块中没有池化层。相反，He等人决定使用瓶颈1 × 1卷积层进行维度下采样，类似于Inception网络。因此，每个残差块从1 × 1卷积层开始，以下采样输入维度体积，然后是3
    × 3卷积层和另一个1 × 1卷积层以下采样输出。这是一种在多层中保持体积维度的良好技术。这种配置被称为瓶颈残差块。
- en: When we are stacking residual blocks on top of each other, the volume dimensions
    change from one block to another. And as you might recall from the matrices introduction
    in chapter 2, to be able to perform matrix addition operations, the matrices should
    have similar dimensions. To fix this problem, we need to downsample the shortcut
    path as well, before merging both paths. We do that by adding a bottleneck layer
    (1 × 1 convolutional layer + batch normalization) to the shortcut path, as shown
    in figure 5.23\. This is called the reduce shortcut.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将残差块堆叠在一起时，体积维度从一个块变化到另一个块。并且你可能还记得第2章中矩阵介绍的内容，为了能够执行矩阵加法操作，矩阵应该具有相似的维度。为了解决这个问题，我们需要在下采样两条路径之前对捷径路径进行下采样。我们通过在捷径路径中添加一个瓶颈层（1
    × 1卷积层 + 批量归一化）来实现这一点，如图5.23所示。这被称为减少捷径。
- en: '![](../Images/5-23.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-23.png)'
- en: Figure 5.23 To reduce the input dimensionality, we add a bottleneck layer (1
    × 1 convolutional layer + batch normalization) to the shortcut path. This is called
    the reduce shortcut.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23 为了减少输入维度，我们在捷径路径中添加了一个瓶颈层（1 × 1卷积层 + 批量归一化）。这被称为减少捷径。
- en: 'Before we jump into the code implementation, let’s recap the discussion of
    residual blocks:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码实现之前，让我们回顾一下关于残差块的讨论：
- en: 'Residual blocks contain two paths: the shortcut path and the main path.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差块包含两个路径：捷径路径和主路径。
- en: 'The main path consists of three convolutional layers, and we add a batch normalization
    layer to them:'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主路径由三个卷积层组成，我们向它们添加批归一化层：
- en: 1 × 1 convolutional layer
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 × 1卷积层
- en: 3 × 3 convolutional layer
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 × 3卷积层
- en: 1 × 1 convolutional layer
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 × 1卷积层
- en: 'There are two ways to implement the shortcut path:'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现捷径路径有两种方式：
- en: '*Regular shortcut* --Add the input dimensions to the main path.'
  id: totrans-377
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*常规捷径* --将输入维度添加到主路径。'
- en: '*Reduce shortcu*t --Add a convolutional layer in the shortcut path before merging
    with the main path.'
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*减少捷径* --在合并到主路径之前，在捷径路径中添加一个卷积层。'
- en: 'When we are implementing the ResNet network, we will use both regular and reduce
    shortcuts. This will be clearer when you see the full implementation. But for
    now, we will implement `bottleneck_residual_block` function that takes a `reduce`
    Boolean argument. When `reduce` is `True`, this means we want to use the reduce
    shortcut; otherwise, it will implement the regular shortcut. The `bottleneck_residual_block`
    function takes the following arguments:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们实现ResNet网络时，我们将使用常规和缩减捷径。当你看到完整的实现时，这会变得更加清晰。但就目前而言，我们将实现`bottleneck_residual_block`函数，它接受一个`reduce`布尔参数。当`reduce`为`True`时，这意味着我们想要使用缩减捷径；否则，它将实现常规捷径。`bottleneck_residual_block`函数接受以下参数：
- en: '`X`--Input tensor of shape (number of samples, height, width, channel)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X`--形状为（样本数，高度，宽度，通道）的输入张量'
- en: '`f`--Integer specifying the shape of the middle convolutional layer’s window
    for the main path'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f`--整数，指定主路径中间卷积层窗口的形状'
- en: '`filters`--Python list of integers defining the number of filters in the convolutional
    layers of the main path'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filters`--定义主路径卷积层中滤波器数量的Python整数列表'
- en: '`reduce`--Boolean: `True` identifies the reduction layer'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce`--布尔值：`True`标识缩减层'
- en: '`s`--Integer (strides)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s`--整数（步长）'
- en: 'The function returns `X`: the output of the residual block, which is a tensor
    of shape (height, width, channel).'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 函数返回`X`：残差块的输出，它是一个形状为（高度，宽度，通道）的张量。
- en: 'The function is as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 函数如下：
- en: '[PRE18]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Unpacks the tuple to retrieve the filters of each convolutional layer
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 解包元组以检索每个卷积层的滤波器
- en: ❷ Saves the input value to use it later to add back to the main path
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将输入值保存起来，稍后用于将其添加到主路径
- en: ❸ Condition if reduce is True
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 当reduce为True时的条件
- en: ❹ To reduce the spatial size, applies a 1 × 1 convolutional layer to the shortcut
    path. To do that, we need both convolutional layers to have similar strides.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为了减少空间尺寸，将一个1 × 1卷积层应用于捷径路径。为此，我们需要两个卷积层具有相似的步长。
- en: ❺ If reduce, sets the strides of the first convolutional layer to be similar
    to the shortcut strides.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果reduce，将第一个卷积层的步长设置为与捷径步长相似。
- en: ❻ Adds the shortcut value to main path and passes it through a ReLU activation
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将捷径值添加到主路径，并通过ReLU激活传递
- en: 5.6.3 ResNet implementation in Keras
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.3 Keras中的ResNet实现
- en: 'You’ve learned a lot about residual blocks so far. Let’s add these blocks on
    top of each other to build the full ResNet architecture. Here, we will implement
    ResNet50: a version of the ResNet architecture that contains 50 weight layers
    (hence the name). You can use the same approach to develop ResNet with 18, 34,
    101, and 152 layers by following the architecture in figure 5.24 from the original
    paper.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学到了很多关于残差块的知识。让我们将这些块叠加在一起来构建完整的ResNet架构。在这里，我们将实现ResNet50：ResNet架构的一个版本，包含50个权重层（因此得名）。你可以通过遵循原始论文中的图5.24的架构来开发具有18、34、101和152层的ResNet。
- en: '![](../Images/5-24.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/5-24.png)'
- en: Figure 5.24 Architecture of several ResNet variations from the original paper
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.24 原始论文中几个ResNet变体的架构
- en: 'We know from the previous section that each residual module contains 3 × 3
    convolutional layers, and we now can compute the total number of weight layers
    inside the ResNet50 network as follows:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一节我们知道每个残差模块包含3 × 3卷积层，现在我们可以计算ResNet50网络内部的权重层总数如下：
- en: 'Stage 1: 7 × 7 convolutional layer'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阶段1：7 × 7卷积层
- en: 'Stage 2: 3 residual blocks, each containing [1 × 1 convolutional layer + 3
    × 3 convolutional layer + 1 × 1 convolutional layer] = 9 convolutional layers'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阶段2：3个残差块，每个块包含[1 × 1卷积层 + 3 × 3卷积层 + 1 × 1卷积层] = 9个卷积层
- en: 'Stage 3: 4 residual blocks = total of 12 convolutional layers'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三阶段：4个残差块 = 总共12个卷积层
- en: 'Stage 4: 6 residual blocks = total of 18 convolutional layers'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四阶段：6个残差块 = 总共18个卷积层
- en: 'Stage 5: 3 residual blocks = total of 9 convolutional layers'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第五阶段：3个残差块 = 总共9个卷积层
- en: Fully connected softmax layer
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接的softmax层
- en: When we sum all these layers together, we get a total of 50 weight layers that
    describe the architecture of ResNet50\. Similarly, you can compute the number
    of weight layers in the other ResNet versions.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有这些层加在一起时，我们得到总共50个权重层，这些层描述了ResNet50的架构。同样，你也可以计算其他ResNet版本中的权重层数量。
- en: NOTE In the following implementation, we use the residual block with reduce
    shortcut at the beginning of each stage to reduce the spatial size of the output
    from the previous layer. Then we use the regular shortcut for the remaining layers
    of that stage. Recall from our implementation of the `bottleneck_` `residual_block`
    function that we will set the argument `reduce` to `True` to apply the reduce
    shortcut.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在以下实现中，我们使用在每个阶段的开始处具有减少快捷支路的残差块来减少前一层输出的空间尺寸。然后我们使用该阶段的其余层的常规快捷支路。回想一下我们在`bottleneck_`
    `residual_block`函数中的实现，我们将设置`reduce`参数为`True`以应用减少快捷支路。
- en: 'Now let’s follow the 50-layer architecture from figure 5.24 to build the ResNet50
    network. We build a ResNet50 function that takes `input_shape` and `classes` as
    arguments and outputs the model:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们根据图5.24中的50层架构来构建ResNet50网络。我们构建一个ResNet50函数，该函数接受`input_shape`和`classes`作为参数，并输出模型：
- en: '[PRE19]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Defines the input as a tensor with shape input_shape
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将输入定义为具有input_shape形状的张量
- en: ❷ Creates the model
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建模型
- en: 5.6.4 Learning hyperparameters
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.4 学习超参数
- en: 'He et al. followed a training procedure similar to that of AlexNet: the training
    is carried out using mini-batch GD with momentum of 0.9\. The team set the learning
    rate to start with a value of 0.1 and then decreased it by a factor of 10 when
    the validation error stopped improving. They also used L2 regularization with
    a weight decay of 0.0001 (not implemented in this chapter for simplicity). As
    you saw in the earlier implementation, they used batch normalization right after
    each convolutional and before activation to speed up training:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: He等人遵循了类似于AlexNet的训练程序：使用带有动量0.9的mini-batch GD进行训练。团队将学习率初始值设为0.1，当验证误差停止改善时，将其减少10倍。他们还使用了L2正则化，权重衰减为0.0001（为了简化，本章未实现）。正如你在前面的实现中看到的，他们在每个卷积操作之后和激活之前使用了批量归一化来加速训练：
- en: '[PRE20]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Sets the training parameters
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置训练参数
- en: ❷ min_lr is the lower bound on the learning rate, and factor is the factor by
    which the learning rate will be reduced.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ min_lr是学习率的下限，factor是学习率减少的倍数。
- en: ❸ Compiles the model
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 编译模型
- en: ❹ Trains the model, calling the reduce_lr value using callbacks in the training
    method
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用训练方法中的回调函数调用reduce_lr值来训练模型
- en: 5.6.5 ResNet performance on the CIFAR dataset
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.5 ResNet在CIFAR数据集上的性能
- en: Similar to the other networks explained in this chapter, the performance of
    ResNet models is benchmarked based on their results in the ILSVRC competition.
    ResNet-152 won first place in the 2015 classification competition with a top-5
    error rate of 4.49% with a single model and 3.57% using an ensemble of models.
    This was much better than all the other networks, such as GoogLeNet (Inception),
    which achieved a top-5 error rate of 6.67%. ResNet also won first place in many
    object detection and image localization challenges, as we will see in chapter
    7\. More importantly, the residual blocks concept in ResNet opened the door to
    new possibilities for efficiently training super-deep neural networks with hundreds
    of layers.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章中解释的其他网络类似，ResNet模型的性能是基于他们在ILSVRC比赛中的结果进行基准测试的。ResNet-152在2015年分类比赛中以4.49%的top-5错误率获得第一名，使用单个模型时为3.57%，使用模型集成时为3.57%。这比其他所有网络都要好，例如GoogLeNet（Inception），其top-5错误率为6.67%。ResNet还在许多目标检测和图像定位挑战中获得了第一名，我们将在第7章中看到。更重要的是，ResNet中的残差块概念为高效训练具有数百层的超深层神经网络打开了新的可能性。
- en: Using open source implementations
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源实现
- en: Now that you have learned some of the most popular CNN architectures, I want
    to share some practical advice on how to use them. It turns out that a lot of
    these neural networks are difficult or finicky to replicate due to details of
    tuning hyperparameters such as learning decay and other things that make a difference
    for performance. DL researchers can even have a hard time replicating someone
    else’s polished work based on reading their paper.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学习了最流行的CNN架构中的一些，我想分享一些关于如何使用它们的实用建议。事实证明，由于学习衰减等超参数的调整细节以及其他影响性能的因素，许多这些神经网络难以复制或难以调整。深度学习研究人员甚至可能很难根据阅读他们的论文来复制他人的精炼工作。
- en: Fortunately, many DL researchers routinely open source their work on the internet.
    A simple search for the network implementation on GitHub will point you toward
    implementations in several DL libraries that you can clone and train. If you can
    locate the author’s implementation, you can usually get going much faster than
    by trying to re-implement a network from scratch--although sometimes, re-implementing
    from scratch can be a good exercise, like what we did earlier.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，许多深度学习研究人员通常会将其工作开源。在GitHub上简单搜索网络实现，就会指向几个深度学习库中的实现，您可以克隆并训练。如果您能找到作者的实现，通常会比尝试从头开始重新实现网络要快得多——尽管有时从头开始重新实现可能是一项很好的练习，就像我们之前所做的那样。
- en: Summary
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Classical CNN architectures have the same classical architecture of stacking
    convolutional and pooling layers on top of each other with different configurations
    for their layers.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典的CNN架构具有相同的经典架构，即通过不同的配置堆叠卷积和池化层。
- en: 'LeNet consists of five weight layers: three convolutional and two fully connected
    layers, with a pooling layer after the first and second convolutional layers.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeNet由五个权重层组成：三个卷积层和两个全连接层，在第一和第二个卷积层之后有一个池化层。
- en: 'AlexNet is deeper than LeNet and contains eight weight layers: five convolutional
    and three fully connected layers.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlexNet比LeNet更深，包含八个权重层：五个卷积层和三个全连接层。
- en: VGGNet solved the problem of setting up the hyperparameters of the convolutional
    and pooling layers by creating a uniform configuration for them to be used across
    the entire network.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGGNet通过为整个网络创建统一的配置来解决设置卷积和池化层超参数的问题。
- en: 'Inception tried to solve the same problem as VGGNet: instead of having to decide
    which filter size to use and where to add the pooling layer, Inception says, “Let’s
    use them all.”'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception试图解决与VGGNet相同的问题：不需要决定使用哪种滤波器大小以及在哪里添加池化层，Inception说，“让我们都用上。”
- en: ResNet followed the same approach as Inception and created residual blocks that,
    when stacked on top of each other, form the network architecture. ResNet attempted
    to solve the vanishing gradient problem that made learning plateau or degrade
    when training very deep neural networks. The ResNet team introduced skip connections
    that allow information to flow from earlier layers in the network to later layers,
    creating an alternate shortcut path for the gradient to flow through. The fundamental
    breakthrough with ResNet was that it allowed us to train extremely deep neural
    networks with hundreds of layers.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet遵循与Inception相同的方法，创建了残差块，当它们堆叠在一起时，形成了网络架构。ResNet试图解决在训练非常深的神经网络时出现的梯度消失问题，这会导致学习停滞或退化。ResNet团队引入了跳跃连接，允许信息从网络中的早期层流向后期层，为梯度流动创建了一条替代的快捷路径。ResNet的基本突破在于它使我们能够训练具有数百层的极其深的神经网络。
- en: '* * *'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '1.Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-Based Learning
    Applied to Document Recognition,” Proceedings of the IEEE 86 (11): 2278-2324,
    [http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '1.Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-Based Learning
    Applied to Document Recognition,” Proceedings of the IEEE 86 (11): 2278-2324,
    [http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf).'
- en: '2.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks,” Communications of the ACM 60 (6): 84-90,
    [https://dl.acm.org/doi/10.1145/3065386](https://dl.acm.org/doi/10.1145/3065386).'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '2.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks,” Communications of the ACM 60 (6): 84-90,
    [https://dl.acm.org/doi/10.1145/3065386](https://dl.acm.org/doi/10.1145/3065386).'
- en: 3.Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for
    Large-Scale Image Recognition,” 2014, [https://arxiv.org/pdf/1409.1556v6.pdf](https://arxiv.org/pdf/1409.1556v6.pdf).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 卡伦·西莫尼亚（Karen Simonyan）和安德鲁·齐塞拉曼（Andrew Zisserman），“用于大规模图像识别的超深卷积神经网络”，2014年，[https://arxiv.org/pdf/1409.1556v6.pdf](https://arxiv.org/pdf/1409.1556v6.pdf).
- en: 4. Christian Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott
    Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich,
    “Going Deeper with Convolutions,” in Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 1-9, 2015, [http://mng.bz/YryB](http://mng.bz/YryB).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 克里斯蒂安·塞格迪（Christian Szegedy）、克里斯蒂安·刘（Wei Liu）、杨庆佳（Yangqing Jia）、皮埃尔·塞尔曼内特（Pierre
    Sermanet）、斯科特·里德（Scott Reed）、德拉戈米尔·安古洛夫（Dragomir Anguelov）、杜米特鲁·埃尔汉（Dumitru Erhan）、文森特·范霍克（Vincent
    Vanhoucke）和安德鲁·拉宾诺维奇（Andrew Rabinovich），“通过卷积加深学习”，载于IEEE计算机视觉与模式识别会议论文集，第1-9页，2015年，[http://mng.bz/YryB](http://mng.bz/YryB).
- en: 5.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep Residual Learning
    for Image Recognition,” 2015, [http://arxiv.org/abs/1512.03385](http://arxiv.org/abs/1512.03385).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 凯明·赫（Kaiming He）、张祥宇（Xiangyu Zhang）、邵庆庆（Shaoqing Ren）和孙剑（Jian Sun），“用于图像识别的深度残差学习”，2015年，[http://arxiv.org/abs/1512.03385](http://arxiv.org/abs/1512.03385).
