- en: 6 Improving agents’ behaviors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 改善智能体的行为
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: You will learn about improving policies when learning from feedback that is
    simultaneously sequential and evaluative.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解在同时从顺序和评估反馈中学习时如何改进策略。
- en: You will develop algorithms for finding optimal policies in reinforcement learning
    environments when the transition and reward functions are unknown.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当状态转移和奖励函数未知时，你将开发在强化学习环境中寻找最优策略的算法。
- en: You will write code for agents that can go from random to optimal behavior using
    only their experiences and decision making, and train the agents in a variety
    of environments.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将编写代码，为那些仅通过经验和决策就能从随机行为转变为最优行为的智能体，并在各种环境中训练这些智能体。
- en: When it is obvious that the goals cannot be reached, don’t adjust the goals,
    adjust the action steps.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当显然无法达到目标时，不要调整目标，调整行动步骤。
- en: — Confucius Chinese teacher, editor, politician, and philosopher of the Spring
    and Autumn period of Chinese history
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: —— 孔子 中国春秋时期的教育家、编辑、政治家、哲学家
- en: 'Up until this chapter, you’ve studied in isolation and interplay learning from
    two of the three different types of feedback a reinforcement learning agent must
    deal with: sequential, evaluative, and sampled. In chapter 2, you learned to represent
    sequential decision-making problems using a mathematical framework known as the
    Markov decision processes. In chapter 3, you learned how to solve these problems
    with algorithms that extract policies from MDPs. In chapter 4, you learned to
    solve simple control problems that are multi-option, single-choice, decision-making
    problems, called Multi-Armed Bandits, when the MDP representation isn’t available
    to the agent. Finally, in chapter 5, we mixed these two types of control problems,
    that is, we dealt with control problems that are sequential and uncertain, but
    we only learned to estimate value functions. We solved what’s called the prediction
    problem, which is learning to evaluate policies, learning to predict returns.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之前，你已独立地学习了来自三种不同类型反馈的强化学习智能体的学习：顺序的、评估的和样本的。在第2章，你学习了如何使用被称为马尔可夫决策过程（Markov
    decision processes）的数学框架来表示顺序决策问题。在第3章，你学习了如何使用从MDP中提取策略的算法来解决这些问题。在第4章，你学习了在MDP表示不可用的情况下解决简单控制问题，这些问题是多选项、单选决策问题，称为多臂老虎机。最后，在第5章，我们将这两种类型的控制问题结合起来，即我们处理了顺序且不确定的控制问题，但我们只学会了估计价值函数。我们解决了所谓的预测问题，即学习评估策略，学习预测回报。
- en: 'In this chapter, we’ll introduce agents that solve the control problem, which
    we get simply by changing two things. First, instead of estimating state-value
    functions, *V*(*s*), we estimate action-value functions, *Q*(*s, a*). The main
    reason for this is that Q-functions, unlike V-functions, let us see the value
    of actions without having to use an MDP. Second, after we obtain these Q-value
    estimates, we use them to improve the policies. This is similar to what we did
    in the policy-iteration algorithm: we evaluate, we improve, then evaluate the
    improved policy, then improve on this improved policy, and so on. As I mentioned
    in chapter 3, this pattern is called *generalized policy iteration (GPI)*, and
    it can help us create an architecture that virtually any reinforcement learning
    algorithm fits under, including state-of-the-art deep reinforcement learning agents.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍解决控制问题的智能体，我们通过简单地改变两个东西来实现这一点。首先，我们不是估计状态价值函数V(s)，而是估计动作价值函数Q(s,
    a)。这样做的主要原因是因为Q函数，与V函数不同，它允许我们在不使用MDP的情况下看到动作的价值。其次，在我们获得这些Q值估计之后，我们使用它们来改进策略。这与我们在策略迭代算法中所做的是类似的：我们评估，我们改进，然后评估改进后的策略，然后在这个改进的策略上进一步改进，依此类推。正如我在第3章中提到的，这种模式被称为*广义策略迭代（GPI）*，它可以帮助我们创建一个架构，几乎任何强化学习算法都可以适应，包括最先进的深度强化学习智能体。
- en: 'The outline for this chapter is as follows: First, I’ll expand on the generalized
    policy-iteration architecture, and then you’ll learn about many different types
    of agents that solve the control problem. You’ll learn about the control version
    of the Monte Carlo prediction and temporal-difference learning agents. You’ll
    also learn about slightly different kinds of agents that decouple learning from
    behavior. What this all means in practical terms is that in this chapter, you
    develop agents that learn to solve tasks by trial-and-error learning. These agents
    learn optimal policies solely through their interaction with the environment.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的概述如下：首先，我将扩展广义策略迭代架构，然后您将了解许多不同类型的代理，它们解决了控制问题。您将了解蒙特卡洛预测和时序差分学习代理的控制版本。您还将了解稍微不同类型的代理，它们将学习与行为解耦。在实践意义上，这意味着在本章中，您将开发通过试错学习来解决问题的代理。这些代理通过与环境交互来学习最优策略。
- en: The anatomy of reinforcement learning agents
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习代理的解剖结构
- en: In this section, I’d like to give you a mental model that most, if not all,
    reinforcement learning agents fit under. First, every reinforcement learning agent
    gathers experience samples, either from interacting with the environment or from
    querying a learned model of an environment. Still, data is generated as the agents
    learn. Second, every reinforcement learning agent learns to estimate something,
    perhaps a model of the environment, or possibly a policy, a value function, or
    just the returns. Third, every reinforcement learning agent attempts to improve
    a policy; that’s the whole point of RL, after all.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我想向您提供一个心理模型，大多数，如果不是所有，强化学习代理都符合这个模型。首先，每个强化学习代理都会收集经验样本，无论是通过与环境的交互还是查询环境的学习模型。然而，数据是在代理学习的过程中产生的。其次，每个强化学习代理都会学习估计某些东西，可能是一个环境模型，或者可能是一个策略、一个价值函数，或者仅仅是回报。第三，每个强化学习代理都会尝试改进一个策略；毕竟，这就是强化学习（RL）的全部意义。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryRewards, returns, and
    value functions |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| ![刷新我的记忆](../Images/icons_Memory.png) | 奖励、回报和价值函数'
- en: '|  | Now is a good time to refresh your memory. You need to remember the difference
    between rewards, returns, and value functions, so that this chapter makes sense
    to you and you can develop agents that learn optimal policies through trial-and-error
    learning. Allow me to repeat myself.A *reward* is a numeric signal indicating
    the goodness of a transition. Your agent observes state *S*[*t*], takes action
    *A*[*t*]; then the environment changes and gives a reward *R*[*t*+1], and emits
    a new state *S*[*t*+1]. Rewards are that single numeric signal indicating the
    goodness of the transition occurring on every time step of an episode.![](../Images/06_00_Sidebar01a.png)A
    *return* is the summation of all the rewards received during an episode. Your
    agent receives reward *R*[*t*+1], then *R*[*t*+2], and so on until it gets the
    final reward RT right before landing in the terminal state *S*[*T*]. Returns are
    the sum of all those rewards during an episode. Returns are often defined as the
    discounted sum, instead of just a sum. A discounted sum puts a priority on rewards
    found early in an episode (depending on the discount factor, of course.) Technically
    speaking, a discounted sum is a more general definition of the return, since a
    discount factor of one makes it a plain sum.![](../Images/06_00_Sidebar01b.png)A
    *value* function is the expected return. Expectations are calculated as the sum
    of all possible values, each multiplied by the probability of its occurrence.
    Think of expectations as the average of an infinite number of samples; the expectation
    of returns is like sampling an infinite number of returns and averaging them.
    When you calculate a return starting after selecting an action, the expectation
    is the action-value function of that state-action pair, *Q*(*s, a*). If you disregard
    the action taken and count from the state s, that becomes the state-value function
    *V*(*s*).![](../Images/06_00_Sidebar01c.png) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|  | 现在是刷新记忆的好时机。你需要记住奖励、回报和价值函数之间的区别，这样你才能理解本章内容，并开发出通过试错学习来学习最优策略的代理。让我再重复一遍。*奖励*是一个表示状态转换好坏的数值信号。你的代理观察到状态
    *S*[*t*]，采取行动 *A*[*t*]；然后环境发生变化，给出奖励 *R*[*t*+1]，并发出新的状态 *S*[*t*+1]。奖励是表示在每个时间步长发生的转换好坏的单个数值信号！![奖励](../Images/06_00_Sidebar01a.png)
    *回报*是整个过程中收到的所有奖励的总和。你的代理收到奖励 *R*[*t*+1]，然后 *R*[*t*+2]，等等，直到它获得在终端状态 *S*[*T*]
    之前最后的奖励 RT。回报是整个过程中所有奖励的总和。回报通常定义为折扣总和，而不是简单的总和。折扣总和优先考虑在过程中早期找到的奖励（当然，这取决于折扣因子。）从技术上讲，折扣总和是对回报的更一般定义，因为折扣因子为1时，它就是一个简单的总和！![回报](../Images/06_00_Sidebar01b.png)
    *价值函数*是期望回报。期望是通过将所有可能值相加并乘以其发生的概率来计算的。将期望视为无限多个样本的平均值；回报的期望就像是从无限多个回报中进行采样并取平均值。当你从选择行动之后开始计算回报时，期望是该状态-行动对的行动价值函数，*Q*(*s,
    a*)。如果你忽略采取的行动并从状态 s 开始计数，那么它就变成了状态价值函数 *V*(*s*)。![价值函数](../Images/06_00_Sidebar01c.png)
    |'
- en: Most agents gather experience samples
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大多数代理收集经验样本
- en: One of the unique characteristics of RL is that agents learn by trial and error.
    The agent interacts with an environment, and as it does so, it gathers data. The
    unusual aspect here is that gathering data is a separate challenge from learning
    from data. And as you’ll see shortly, learning from data is also a different thing
    from improving from data. In RL, there is gathering, learning, and improving.
    For instance, an agent that’s pretty good at collecting data may not be as good
    at learning from data; or, conversely, an agent that isn’t good at collecting
    data may be good at learning from data, and so on. We all have that friend who
    didn’t take good notes in school, yet they did well on tests, while others had
    everything written down, but didn’t do as well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的一个独特特征是代理通过试错来学习。代理与环境交互，在这个过程中收集数据。这里不寻常的方面是，收集数据与从数据中学习是两个不同的挑战。正如你很快就会看到的，从数据中学习也与从数据中改进不同。在RL中，有收集、学习和改进。例如，一个在收集数据方面相当出色的代理可能并不擅长从数据中学习；或者相反，一个不擅长收集数据的代理可能在从数据中学习方面很出色，等等。我们都有那种在学校笔记记不好的朋友，但他们考试却做得很好，而其他人把所有东西都记下来，但成绩却不那么好。
- en: In chapter 3, when we learned about dynamic programming methods, I mentioned
    value and policy iteration shouldn’t be referred to as RL, but planning methods
    instead, the reason being they *interact* with the environment because a model
    of the environment, the MDP, is provided beforehand.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，当我们学习动态规划方法时，我提到价值迭代和政策迭代不应该被称为强化学习，而应该称为规划方法，原因在于它们与环境的*交互*，因为环境模型，即马尔可夫决策过程（MDP），是事先提供的。
- en: '| ŘŁ | With An RL AccentPlanning vs. learning problems |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 规划问题与学习问题 |'
- en: '|  | **Planning problems**: Refers to problems in which a model of the environment
    is available and thus, there’s no learning required. These types of problems can
    be solved with planning methods such as value iteration and policy iteration.
    The goal in these types of problems is to find, as opposed to learn, optimal policies.
    Suppose I give you a map and ask you to find the best route from point A to point
    B; there’s no learning required there, just planning.**Learning problems**: Refers
    to problems in which learning from samples is required, usually because there
    isn’t a model of the environment available or perhaps because it’s impossible
    to create one. The main challenge of learning problems is that we estimate using
    samples, and samples can have high variance, which means they’ll be of poor quality
    and difficult to learn from. Samples can also be biased, either because of being
    from a different distribution than the one estimating or because of using estimates
    to estimate, which can make our estimates incorrect altogether. Suppose I don’t
    give you a map of the area this time. How would you find “the best route”? By
    trial-and-error learning, likely. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | **规划问题**：指的是有环境模型可用的问题，因此不需要学习。这类问题可以使用价值迭代和政策迭代等规划方法来解决。这类问题的目标是找到最优策略，而不是学习。假设我给你一张地图，让你从点A找到到点B的最佳路线；这里不需要学习，只需要规划。**学习问题**：指的是需要从样本中进行学习的问题，通常是因为没有环境模型可用，或者可能是因为无法创建一个模型。学习问题的主要挑战是我们使用样本进行估计，而样本可能具有高方差，这意味着它们的质量较差，难以从中学习。样本也可能存在偏差，要么是因为来自与估计不同的分布，要么是因为使用估计来估计，这可能导致我们的估计完全错误。假设这次我没有给你该地区的地图。你将如何找到“最佳路线”？很可能是通过试错学习。|'
- en: For an algorithm to be considered a standard RL method, the aspect of interacting
    with the environment, with the problem we’re trying to solve, should be present.
    Most RL agents gather experience samples by themselves, unlike supervised learning
    methods, for instance, which are given a dataset. RL agents have the additional
    challenge of selecting their datasets. Most RL agents gather experience samples
    because RL is often about solving interactive learning problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要被视为标准的强化学习方法，算法与环境的交互，即我们试图解决的问题的方面，应该是存在的。大多数强化学习代理通过自己收集经验样本，与例如被给予数据集的监督学习方法不同，强化学习代理有选择其数据集的额外挑战。大多数强化学习代理收集经验样本，因为强化学习通常涉及解决交互式学习问题。
- en: '| ŘŁ | With An RL AccentNon-interactive vs. interactive learning problems |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的非交互式与交互式学习问题 |'
- en: '|  | **Non-interactive learning problems**: Refers to a type of learning problem
    in which there’s no need for or possibility of interacting with an environment.
    In these types of problems, there’s no interaction with an environment while learning,
    but there is learning from data previously generated. The objective is to find
    something given the samples, usually a policy, but not necessarily. For instance,
    in inverse RL, the objective is to recover the reward function given expert-behavior
    samples. In apprenticeship learning, the objective is to go from this recovered
    reward function to a policy. In behavioral cloning, which is a form of imitation
    learning, the goal is to go from expert-behavior samples directly to policies
    using supervised learning.**Interactive learning problems**: Refers to a type
    of learning problem in which learning and interaction are interleaved. The interesting
    aspect of these problems is that the learner also controls the data-gathering
    process. Optimal learning from samples is one challenge, and finding samples for
    optimal learning is another. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | **非交互式学习问题**：指的是一种学习问题，其中不需要或不可能与环境交互。在这些类型的问题中，学习过程中没有与环境的交互，但可以从之前生成的数据中进行学习。目标是根据样本找到某些东西，通常是策略，但不一定是。例如，在逆强化学习中，目标是根据专家行为样本恢复奖励函数。在学徒学习中，目标是根据恢复的奖励函数到策略。在行为克隆中，这是一种模仿学习的形式，目标是直接使用监督学习从专家行为样本到策略。**交互式学习问题**：指的是一种学习问题，其中学习和交互是交织在一起的。这些问题的有趣之处在于，学习者也控制着数据收集过程。从样本中进行最优学习是一个挑战，找到用于最优学习的样本是另一个挑战。|'
- en: Most agents estimate something
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大多数智能体都会估计某些东西
- en: After gathering data, there are multiple things an agent can do with this data.
    Certain agents, for instance, learn to predict expected re*turns* or value functions.
    In the previous chapter, you learned about many different ways of doing so, from
    using Monte Carlo to *TD* targets, from every-visit to first-visit MC targets,
    from *n*-step to *λ*-return targets. There are many different ways of calculating
    targets that can be used for estimating value functions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据之后，智能体可以使用这些数据做很多事情。例如，某些智能体学会预测预期的回报或价值函数。在上一章中，你学习了多种实现这一目标的方法，从使用蒙特卡洛到*TD*目标，从每次访问到首次访问MC目标，从*n*-步到*λ*-回报目标。有许多不同的方法可以用来计算目标，这些方法可以用于估计价值函数。
- en: But value functions aren’t the only thing agents can learn with experience samples.
    Agents may be designed to learn models of the environment, too. As you’ll see
    in the next chapter, model-based RL agents use the data collected for learning
    transition and reward functions. By learning a model of the environment, agents
    can predict the next state and reward. Further, with these, agents can either
    plan a sequence of actions similar to the way DP methods work or maybe use synthetic
    data generated from interacting with these learned models to learn something else.
    The point is that agents may be designed to learn models of the environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但价值函数并不是智能体可以用经验样本学习的唯一东西。智能体也可能被设计成学习环境模型。正如你将在下一章中看到的，基于模型的强化学习智能体使用收集的数据来学习状态转移和奖励函数。通过学习环境模型，智能体可以预测下一个状态和奖励。进一步来说，有了这些，智能体可以像DP方法一样规划一系列动作，或者使用与这些学习模型交互生成的合成数据来学习其他东西。关键是智能体可能被设计成学习环境模型。
- en: Moreover, agents can be designed to improve on policies directly using estimated
    returns. In later chapters, we’ll see how policy gradient methods consist of approximating
    functions that take in a state and output a probability distribution over actions.
    To improve these policy functions, we can use actual returns, in the simplest
    case, but also estimated value functions. Finally, agents can be designed to estimate
    multiple things at once, and this is the typical case. The important thing is
    that most agents estimate something.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，智能体可以被设计成直接使用估计的回报来改进策略。在后面的章节中，我们将看到策略梯度方法包括近似函数，这些函数接受状态作为输入并输出动作的概率分布。为了改进这些策略函数，我们可以使用实际回报，在最简单的情况下，也可以使用估计的价值函数。最后，智能体可以被设计成同时估计多个东西，这是典型的情况。重要的是大多数智能体都会估计某些东西。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryMonte Carlo vs. temporal-difference
    targets |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ![刷新我的记忆](../Images/icons_Memory.png) | 蒙特卡洛与时间差分目标 |'
- en: '|  | Other important concepts worth repeating are the different ways value
    functions can be estimated. In general, all methods that learn value functions
    progressively move estimates a fraction of the error towards the targets. The
    general equation that most learning methods follow is *estimate = estimate + step
    * error*. The error is simply the difference between a sampled target and the
    current estimate: (*target – estimate*). The two main and opposite ways for calculating
    these targets are Monte Carlo and temporal-difference learning.![](../Images/06_04_Sidebar04a.png)The
    Monte Carlo target consists of the actual return: really, nothing else. Monte
    Carlo estimation consists of adjusting the estimates of the value functions using
    the empirical (observed) mean return in place of the expected (as if you could
    average infinite samples) return.![](../Images/06_04_Sidebar04b.png)The temporal-difference
    target consists of an estimated return. Remember “bootstrapping”? It basically
    means using the estimated expected return from later states, for estimating the
    expected return from the current state. TD does that: learning a guess from a
    guess. The TD target is formed by using a single reward and the estimated expected
    return from the next state using the running value function estimates. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | 值得重复的其他重要概念是价值函数的不同估计方法。一般来说，所有学习价值函数的方法都是逐步将误差的一部分移动到目标。大多数学习方法遵循的一般方程是
    *估计 = 估计 + 步长 * 误差*。误差简单地是采样目标与当前估计之间的差异：（*目标 – 估计*）。计算这些目标的主要且相反的方法是蒙特卡洛和时序差分学习！![图片](../Images/06_04_Sidebar04a.png)蒙特卡洛目标由实际回报组成：实际上，没有其他东西。蒙特卡洛估计包括使用经验（观察）平均回报代替期望（如果你能平均无限样本）回报来调整价值函数的估计！![图片](../Images/06_04_Sidebar04b.png)时序差分目标由一个估计回报组成。还记得“自举”吗？它基本上意味着使用后续状态的估计期望回报来估计当前状态的期望回报。TD就是这样做的：从一个猜测中学习一个猜测。TD目标是通过使用单个奖励和下一个状态的估计期望回报（使用运行价值函数估计）来形成的。|'
- en: Most agents improve a policy
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大多数代理改进策略
- en: Lastly, most agents improve a policy. This final step heavily depends on the
    type of agent being trained and what the agent estimates. For instance, if the
    agent is estimating value functions, a common thing to improve is the target policy
    implicitly encoded in the value function, which is the policy being learned about.
    The benefit of improving the target policy is that the behavior policy, which
    is the data-generating policy, will consequently improve, therefore improving
    the quality of data the agent will subsequently gather. If the target and behavior
    policies are the same, then the improvement of the underlying value function explicitly
    increases the quality of the data generated afterward.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，大多数代理改进策略。这一最终步骤高度依赖于正在训练的代理的类型以及代理估计的内容。例如，如果代理正在估计价值函数，一个常见的事情是改进隐式编码在价值函数中的目标策略，即正在学习的策略。改进目标策略的好处是，行为策略（即数据生成策略）将相应地改进，从而提高代理随后收集的数据质量。如果目标和行为策略相同，那么底层价值函数的改进将明确提高随后生成数据的质量。
- en: Now, if a policy is being represented explicitly instead of through value functions,
    such as in policy gradient and actor-critic methods, agents can use actual returns
    to improve these policies. Agents can also use value functions to estimate returns
    for improving policies. Finally, in model-based RL, there are multiple options
    for improving policies. One can use a learned model of the environment to plan
    a sequence of actions. In this case, there’s an implicit policy being improved
    in the planning phase. One can use the model to learn a value function, instead,
    which implicitly encodes a policy. One can use the model to improve the policy
    directly, too. The bottom line is that all agents attempt to improve a policy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果策略是通过价值函数而不是显式表示，例如在策略梯度法和演员-评论家方法中，代理可以使用实际回报来改进这些策略。代理还可以使用价值函数来估计回报以改进策略。最后，在基于模型强化学习中，有多种改进策略的选项。可以使用学习到的环境模型来规划一系列动作。在这种情况下，在规划阶段隐式地改进了策略。可以使用模型来学习价值函数，这隐式地编码了一个策略。也可以使用模型直接改进策略。底线是，所有代理都试图改进策略。
- en: '| ŘŁ | With An RL AccentGreedy vs. epsilon-greedy vs. optimal policy |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的贪婪策略 vs. epsilon-贪婪策略 vs. 最优策略 |'
- en: '|  | **Greedy policy**: Refers to a policy that always selects the actions
    believed to yield the highest expected return from each and every state. It’s
    essential to know that a “greedy policy” is greedy with respect to a value function.
    The “believed” part comes from the value function. The insight here is that when
    someone says, “the greedy policy,” you must ask, greedy with respect to what?
    A greedy policy with respect to a random value function is a pretty bad policy.**Epsilon-greedy
    policy**: Refers to a policy that often selects the actions believed to yield
    the highest expected return from each and every state. Same as before applies;
    an epsilon-greedy policy is epsilon-greedy with respect to a specific value function.
    Always make sure you understand which value function is being referenced.**Optimal
    policy**: Refers to a policy that always selects the actions actually yielding
    the highest expected return from each and every state. While a greedy policy may
    or may not be an optimal policy, an optimal policy must undoubtedly be a greedy
    policy. You ask, “greedy with respect to what?” Well done! An optimal policy is
    a greedy policy with respect to a unique value function, the optimal value function.
    |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | **贪婪策略**：指的是一种策略，它总是选择每个状态下被认为能带来最高期望回报的动作。重要的是要知道，“贪婪策略”在价值函数上是贪婪的。“被认为”的部分来自价值函数。这里的洞察是，当有人说“贪婪策略”时，你必须问，相对于什么贪婪？相对于随机价值函数的贪婪策略是一个相当糟糕的策略。**ε-贪婪策略**：指的是一种策略，它通常选择被认为能从每个状态下带来最高期望回报的动作。与之前相同；ε-贪婪策略在特定价值函数上是ε-贪婪的。始终确保你理解正在引用哪个价值函数。**最优策略**：指的是一种策略，它总是选择从每个状态下实际上能带来最高期望回报的动作。虽然贪婪策略可能或可能不是最优策略，但最优策略无疑必须是一种贪婪策略。你问，“相对于什么贪婪？”做得好！最优策略是相对于唯一价值函数的贪婪策略，即最优价值函数。|'
- en: Generalized policy iteration
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 广义策略迭代
- en: Another simple pattern that’s more commonly used to understand the architecture
    of reinforcement learning algorithms is called *generalized policy iteration*
    (GPI). GPI is a general idea that the continuous interaction of policy evaluation
    and policy improvement drives policies towards optimality.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个更常用来理解强化学习算法架构的简单模式被称为**广义策略迭代**（GPI）。GPI是一个基本理念，即策略评估和策略改进的持续交互推动策略向最优性发展。
- en: 'As you probably remember, in the policy iteration algorithm, we had two processes:
    policy evaluation and policy improvement. The policy-evaluation phase takes in
    any policy, and it evaluates it; it estimates the policy’s value function. In
    policy improvement, these estimates, the value function, are used to obtain a
    better policy. Once policy evaluation and improvement stabilize, that is, once
    their interaction no longer produces any changes, then the policy and the value
    function are optimal.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如你很可能记得，在策略迭代算法中，我们有两个过程：策略评估和策略改进。策略评估阶段接受任何策略，并对其进行评估；它估计策略的价值函数。在策略改进中，这些估计，即价值函数，被用来获得更好的策略。一旦策略评估和改进稳定下来，也就是说，一旦它们的交互不再产生任何变化，那么策略和价值函数就是最优的。
- en: Now, if you remember, after studying policy iteration, we learned about another
    algorithm, called value iteration. This one was similar to policy iteration; it
    had a policy-evaluation and a policy-improvement phase. The main difference, however,
    was that the policy-evaluation phase consisted of a single iteration. In other
    words, the evaluation of the policy didn’t produce the actual value function.
    In the policy-evaluation phase of value iteration, the value function estimates
    move towards the actual value function, but not all the way there. Yet, even with
    this policy-evaluation phase, the generalized policy-iteration pattern for value
    iteration also produces the optimal value function and policy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你还记得，在学习策略迭代之后，我们学习了另一种算法，称为值迭代。这个算法与策略迭代类似；它包含策略评估和策略改进两个阶段。然而，主要区别在于策略评估阶段只包含一个迭代。换句话说，策略评估并没有产生实际的价值函数。在值迭代的策略评估阶段，价值函数的估计值会逐渐接近实际价值函数，但并不完全到达那里。尽管如此，即使有这个策略评估阶段，值迭代的广义策略迭代模式仍然会产生最优价值函数和策略。
- en: The critical insight here is that policy evaluation, in general, consists of
    gathering and estimating value functions, similar to the algorithms you learned
    about in the previous chapter. And as you know, there are multiple ways of evaluating
    a policy, numerous methods of estimating the value function of a policy, various
    approaches to *choose from* for checking off the policy-evaluation requirement
    of the generalized policy-iteration pattern.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键洞察是，策略评估通常包括收集和估计价值函数，类似于你在上一章中学到的算法。正如你所知，评估策略有多种方式，估计策略价值函数的方法众多，有各种选择方法来满足广义策略迭代模式中的策略评估要求。
- en: Furthermore, policy improvement consists of changing a policy to make it greedier
    with respect to a value function. In the policy improvement method of the policy-iteration
    algorithm, we make the policy entirely greedy with respect to the value function
    of the evaluated policy. But, we can completely greedify the policy only because
    we had the MDP of the environment. However, the policy-evaluation methods that
    we learned about in the previous chapter don’t require an MDP of the environment,
    and this comes at a cost. We can no longer completely greedify policies; we need
    to have our agents explore. Going forward, instead of completely greedifying the
    policy, we make the policy greedier, leaving room for exploration. This kind of
    partial policy improvement was used in chapter 4 when we used different explorations
    strategies for working with estimates.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，策略改进包括将策略改变为使其对价值函数更贪婪。在策略迭代算法的策略改进方法中，我们将策略完全变为对评估策略的价值函数贪婪。但是，我们之所以能够完全贪婪化策略，仅仅是因为我们有了环境的MDP。然而，我们在上一章中学到的策略评估方法不需要环境的MDP，这付出了一定的代价。我们不能再完全贪婪化策略；我们需要让我们的智能体进行探索。从现在开始，我们不再完全贪婪化策略，而是使策略更贪婪，留出探索的空间。这种部分策略改进在第4章中使用不同的探索策略时被采用。
- en: 'There you have it. Most RL algorithms follow this GPI pattern: they have distinct
    policy-evaluation and improvement phases, and all we must do is pick and choose
    the methods.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容。大多数强化学习算法遵循这种GPI模式：它们有独特的策略评估和改进阶段，我们只需挑选和选择方法即可。
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyGeneralized policy iteration
    and why you should listen to criticism |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| ![Miguel的类比](../Images/icons_Miguel.png) | Miguel的类比：广义策略迭代以及为什么你应该听取批评 |'
- en: '|  | Generalized policy iteration (GPI) is similar to the eternal dance of
    critics and performers. Policy evaluation gives the much-needed feedback that
    policy improvement uses to make policies better. In the same way, critics provide
    the much-needed feedback performers can use to do better.As Benjamin Franklin
    said, *“Critics are our friends, they show us our faults.”* He was a smart guy;
    he allowed GPI to help him improve. You let critics tell you what they think,
    you use that feedback to get better. It’s simple! Some of the best companies out
    there follow this process, too. What do you think the saying *“data-driven decisions”*
    means? It’s saying they make sure to use an excellent policy-evaluation process
    so that their policy-improvement process yields solid results; that’s the same
    pattern as GPI! Norman Vincent Peale said, *“The trouble with most of us is that
    we’d rather be ruined by praise than saved by criticism.”* Go, let critics help
    you.Just beware! That they can indeed help you doesn’t mean critics are always
    right or that you should take their advice blindly, especially if it’s feedback
    that you hear for the first time. Critics are usually biased, and so is policy
    evaluation! It’s your job as a great performer to listen to this feedback carefully,
    to get smart about gathering the best possible feedback, and to act upon it only
    when sure. But, in the end, the world belongs to those who do the work.Theodore
    Roosevelt said it best:*“It is not the critic who counts; not the man who points
    out how the strong man stumbles, or where the doer of deeds could have done them
    better. The credit belongs to the man who is actually in the arena, whose face
    is marred by dust and sweat and blood; who strives valiantly; who errs, who comes
    short again and again, because there is no effort without error and shortcoming;
    but who does actually strive to do the deeds; who knows great enthusiasms, the
    great devotions; who spends himself in a worthy cause; who at the best knows in
    the end the triumph of high achievement, and who at the worst, if he fails, at
    least fails while daring greatly, so that his place shall never be with those
    cold and timid souls who neither know victory nor defeat.”*In later chapters,
    we’ll study actor-critic methods, and you’ll see how this whole analogy extends,
    believe it or not! Actors and critics help each other. Stay tuned for more.It’s
    awe-inspiring that patterns in optimal decision making are valid across the board.
    What you learn studying DRL can help you become a better decision maker, and what
    you learn in your own life can help you create better agents.Cool, right? |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | 广义策略迭代（GPI）类似于批评家和表演者永恒的舞蹈。策略评估提供了政策改进所需的重要反馈，以使政策变得更好。同样，批评家提供了表演者可以用来做得更好的必要反馈。正如本杰明·富兰克林所说，*“批评家是我们的朋友，他们指出我们的错误。”*
    他是个聪明人；他允许GPI帮助他改进。你让批评家告诉你他们的想法，你利用这些反馈来变得更好。很简单！一些最好的公司也遵循这个流程。你认为*“数据驱动决策”*这句话是什么意思？它意味着他们确保使用一个出色的策略评估过程，以便他们的政策改进过程产生可靠的结果；这与GPI的模式相同！诺曼·文森特·皮尔说，*“我们大多数人的麻烦是，我们宁愿被赞誉所毁，也不愿被批评所救。”*
    去吧，让批评家帮助你。但要注意！他们确实可以帮助你，并不意味着批评家总是正确的，或者你应该盲目地接受他们的建议，尤其是如果你第一次听到这样的反馈。批评家通常是有偏见的，策略评估也是如此！作为一位伟大的表演者，你的任务是仔细倾听这些反馈，聪明地收集尽可能好的反馈，并在确定的情况下采取行动。但最终，世界属于那些做工作的人。西奥多·罗斯福说得最好：*“重要的不是批评家；不是指出强者跌倒或做事者本可以做得更好的那个人；荣誉属于真正进入竞技场的人，他的脸被尘土、汗水和鲜血弄脏；他英勇奋斗；他犯错误，一次又一次地失败，因为不努力就没有错误和不足；但他确实努力去做事；他了解巨大的热情、伟大的奉献；他为了一个崇高的目标而倾其所有；他最好地知道，最终，他要么取得了高成就的胜利，要么在最坏的情况下，如果他失败了，至少是勇敢地失败了，这样他的位置永远不会与那些既不知道胜利也不知道失败冷漠而胆怯的灵魂为伍。”*在后面的章节中，我们将研究演员-批评家方法，你会看到这个类比如何扩展，信不信由你！演员和批评家互相帮助。敬请期待更多内容。令人敬畏的是，最优决策模式在各个领域都是有效的。你在学习深度强化学习（DRL）中学到的知识可以帮助你成为一个更好的决策者，而你从自己的生活中学到的知识可以帮助你创造更好的智能体。酷，不是吗？
    |'
- en: Learning to improve policies of behavior
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习改进行为策略
- en: 'In the previous chapter, you learned how to solve the prediction problem: how
    to make agents most accurately estimate the value function of a given policy.
    However, while this is a useful ability for our agents to have, it doesn’t directly
    make them better at any task. In this section, you’ll learn how to solve the control
    problem: how to make agents optimize policies. This new ability allows agents
    to learn optimal behavior by trial-and-error learning, starting from arbitrary
    policies and ending in optimal ones. After this chapter you can develop agents
    that can solve any task represented by an MDP. The task has to be a discrete state-
    and action-space MDP, but other than that, it’s plug-and-play.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何解决预测问题：如何让智能体最准确地估计给定策略的价值函数。然而，虽然这对我们的智能体来说是一个有用的能力，但它并不能直接使它们在任何任务上表现得更好。在本节中，你将学习如何解决控制问题：如何让智能体优化策略。这种新的能力使智能体能够通过试错学习来学习最佳行为，从任意策略开始，最终达到最佳策略。在本章之后，你可以开发出能够解决任何由MDP表示的任务的智能体。任务必须是离散状态和动作空间的MDP，除此之外，它就是即插即用的。
- en: To show you a few agents, we’re going to leverage the GPI pattern you learned.
    That is, we’re going to select algorithms for the policy-evaluation phase from
    the ones you learned about in the last chapter, and strategies for the policy-improvement
    phase from the ones you learned about in the chapter before. Hopefully, this sets
    your imagination free on the possibilities. Just pick and choose algorithms for
    policy evaluation and improvement, and things will work, that’s because of the
    interaction of these two processes.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示几个智能体，我们将利用你所学到的GPI模式。也就是说，我们将从你在上一章中学到的算法中选择策略评估阶段的算法，以及从你在前一章中学到的算法中选择策略改进阶段的策略。希望这能激发你的想象力。只需挑选策略评估和改进的算法，一切就会顺利，这是因为这两个过程的相互作用。
- en: '| ŘŁ | With An RL AccentPrediction vs. control problem vs. policy evaluation
    vs. improvement |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的预测问题、控制问题、策略评估与改进问题 |'
- en: '|  | **Prediction problem**: Refers to the problem of evaluating policies,
    of estimating value functions given a policy. Estimating value functions is nothing
    but learning to predict returns. State-value functions estimate expected returns
    from states, and action-value functions estimate expected returns from state-action
    pairs.**Control problem**: Refers to the problem of finding optimal policies.
    The control problem is usually solved by following the pattern of generalized
    policy iteration (GPI), where the competing processes of policy evaluation and
    policy improvement progressively move policies towards optimality. RL methods
    often pair an action-value prediction method with policy improvement and action-selection
    strategies.**Policy evaluation**: Refers to algorithms that solve the prediction
    problem. Note that there’s a dynamic programming method called policy evaluation,
    but this term is also used to refer to all algorithms that solve the prediction
    problem.**Policy improvement**: Refers to algorithms that make new policies that
    improve on an original policy by making it greedier than the original with respect
    to the value function of that original policy. Note that policy improvement by
    itself doesn’t solve the control problem. Often a policy evaluation must be paired
    with a policy improvement to solve the control problem. Policy improvement only
    refers to the computation for improving a policy given its evaluation results.
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测问题**：指的是评估策略的问题，即在给定策略的情况下估计价值函数的问题。估计价值函数实际上就是学习预测回报。状态价值函数估计从状态中期望的回报，动作价值函数估计从状态-动作对中期望的回报。**控制问题**：指的是寻找最优策略的问题。控制问题通常通过遵循广义策略迭代（GPI）的模式来解决，其中策略评估和策略改进的竞争过程逐渐将策略推向最优。强化学习方法通常将动作价值预测方法与策略改进和动作选择策略配对。**策略评估**：指的是解决预测问题的算法。请注意，有一个名为策略评估的动态规划方法，但这个术语也用来指代所有解决预测问题的算法。**策略改进**：指的是通过使新策略相对于原始策略的价值函数更贪婪来改进原始策略的算法。请注意，策略改进本身并不能解决控制问题。通常必须将策略评估与策略改进配对来解决控制问题。策略改进仅指根据评估结果改进策略的计算。'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe slippery walk seven
    environment |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 一个具体的例子：滑稽的走步七环境 |'
- en: '|  | For this chapter, we use an environment called *slippery walk seven* (SWS).
    This environment is a walk, a single-row grid-world environment, with seven non-terminal
    states. The particular thing of this environment is that it’s a slippery walk;
    action effects are stochastic. If the agent chooses to go left, there is a chance
    it does, but there is also a chance that it goes right, or that it stays in place.Let
    me show you the MDP for this environment. Remember, though, that *unknown* to
    the agent. I’m only giving you this information for didactic reasons.Also, have
    in mind that to the agent, there are no relationships between the states in advance.
    The agent doesn’t know that state 3 is in the middle of the entire walk, or that
    it’s in between states 2 and 4; it doesn’t even know what a “walk” is! The agent
    doesn’t know that action zero goes left, or one goes right. Honestly, I encourage
    you to go to the Notebook and play with the environment yourself to gain a deeper
    understanding. The fact is that the agent only sees the state ids, say, 0, 1,
    2, and so on, and chooses either action 0 or 1.![](../Images/06_06_Sidebar08.png)Slippery
    walk seven environment MDPThe SWS environment is similar to the random walk (RW)
    environment that we learned about in the previous chapter, but with the ability
    to do control. Remember that the random walk is an environment in which the probability
    of going left, when taking the Left action, is equal to the probability of going
    right. And the probability of going right, when taking the Right action, is equal
    to the probability of going left, so there’s no control. This environment is noisy,
    but the actions the agent selects make a difference in its performance. And also,
    this environment has seven non-terminal states, as opposed to the five of the
    RW. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 对于本章，我们使用一个名为 *slippery walk seven* (SWS) 的环境。这个环境是一个行走环境，一个单行网格世界环境，有七个非终止状态。这个环境的特别之处在于它是一个滑溜的行走环境；动作效果是随机的。如果智能体选择向左走，它有可能这么做，但也有可能向右走，或者保持在原地。让我给你展示这个环境的MDP。记住，对于智能体来说，这些是未知的。我之所以提供这些信息，只是为了教学目的。此外，请记住，对于智能体来说，在事先没有任何状态之间的关系。智能体不知道状态3位于整个行走路径的中间，或者它位于状态2和4之间；它甚至不知道什么是“行走”！智能体不知道动作0表示向左走，或者动作1表示向右走。老实说，我鼓励你亲自去笔记本中玩这个环境，以获得更深入的理解。事实上，智能体只能看到状态ID，比如0，1，2等等，并选择动作0或1。![](../Images/06_06_Sidebar08.png)滑溜行走七环境MDPSWS环境与我们之前章节中学到的随机行走（RW）环境类似，但具有控制能力。记住，随机行走是一个环境，在执行左行动作时向左走的概率等于向右走的概率。而在执行右行动作时向右走的概率等于向左走的概率，因此没有控制。这个环境是有噪声的，但智能体选择的动作会影响其性能。此外，这个环境有七个非终止状态，而RW环境有五个。
    |'
- en: 'Monte Carlo control: Improving policies after each episode'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蒙特卡洛控制：在每个回合后改进策略
- en: Let’s try to create a control method using Monte Carlo prediction for our policy-evaluation
    needs. Let’s initially assume we’re using the same policy-improvement step we
    use for the policy-iteration algorithm. That is, the policy-improvement step gets
    the greedy policy with respect to the value function of the policy evaluated.
    Would this make an algorithm that helps us find optimal policies solely through
    interaction? Actually, no. There are two changes we need to make before we can
    make this approach work.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试创建一个控制方法，使用蒙特卡洛预测来满足我们的策略评估需求。让我们最初假设我们使用的是与策略迭代算法相同的策略改进步骤。也就是说，策略改进步骤获取与策略评估的价值函数相关的贪婪策略。这会形成一个帮助我们仅通过交互找到最优策略的算法吗？实际上，不会。在我们使这种方法生效之前，我们需要做出两个改变。
- en: First, we need to make sure our agent estimates the action-value function *Q*(*s,
    a*), instead of the *V(s, a)* that we estimated in the previous chapter. The problem
    with the V-function is that, without the MDP, it isn’t possible to know what the
    best action is to take from a state. In other words, the policy-improvement step
    wouldn’t work.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确保我们的智能体估计的是动作值函数 *Q*(*s, a*)，而不是我们在上一章中估计的 *V(s, a)*。V函数的问题在于，没有MDP，我们无法知道从某个状态采取的最佳动作是什么。换句话说，策略改进步骤将不会起作用。
- en: '![](../Images/06_07.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06_07.png)'
- en: We need to estimate action-value functions
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要估计动作值函数
- en: Second, we need to make sure our agent explores. The problem is that we’re no
    longer using the MDP for our policy-evaluation needs. When we estimate from samples,
    we get values for all of the state-action pairs we visited, but what if part of
    the best states weren’t visited?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们需要确保我们的智能体进行探索。问题是，我们不再使用MDP来满足我们的策略评估需求。当我们从样本中进行估计时，我们得到了我们访问的所有状态-动作对的值，但如果我们没有访问到最佳状态的一部分怎么办？
- en: '![](../Images/06_08.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06_08.png)'
- en: We need to explore
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要探索
- en: Therefore, let’s use first-visit Monte Carlo prediction for the policy-evaluation
    phase and a decaying epsilon-greedy action-selection strategy for the policy-improvement
    phase. And that’s it—you have a complete, model-free RL algorithm in which we
    evaluate policies with Monte Carlo prediction and improve them with decaying epsilon-greedy
    action-selection strategy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们在策略评估阶段使用首次访问蒙特卡洛预测，并在策略改进阶段使用衰减epsilon-greedy动作选择策略。就这样——你有一个完整的、无模型的RL算法，其中我们使用蒙特卡洛预测评估策略，并使用衰减epsilon-greedy动作选择策略来改进它们。
- en: As with value iteration, which has a truncated policy-evaluation step, we can
    truncate the Monte Carlo prediction method. Instead of rolling out several episodes
    for estimating the value function of a single policy using Monte Carlo prediction,
    as we did in the previous chapter, we truncate the prediction step after a *single
    full rollout* and trajectory sample estimation, and improve the policy right after
    that single estimation step. We alternate a single MC-prediction step and a single
    decaying epsilon-greedy action-selection improvement step.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与价值迭代一样，它有一个截断的政策评估步骤，我们可以截断蒙特卡洛预测方法。与我们在上一章中用蒙特卡洛预测估计单个策略的价值函数相比，我们截断预测步骤，在单个完整回合和轨迹样本估计之后，立即改进策略。我们交替进行单个MC预测步骤和单个衰减epsilon-greedy动作选择改进步骤。
- en: 'Let’s look at our first RL method MC control. You’ll see three functions:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的第一个RL方法MC控制。你会看到三个函数：
- en: 'decay_schedule: Compute decaying values as specified in the function arguments.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: decay_schedule：根据函数参数计算衰减值。
- en: 'generate_trajectory: Roll out the policy in the environment for a full episode.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: generate_trajectory：在环境中执行策略的全局回合。
- en: 'mc_control: Complete implementation of the MC control method.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mc_control：MC控制方法的完整实现。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonExponentially decaying schedule
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我说Python指数衰减计划 |'
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① The decay schedule we’ll use for both alpha and epsilon is the same we used
    in the previous chapter for alpha. Let’s go into more detail this time.② What
    I personally like about this function is that you give it an initial value, a
    minimum value, and the percentage of the max_steps to decay the values from initial
    to minimum.③ This decay_steps is the index where the decaying of values terminates
    and the min_value continues until max_steps.④ rem_steps is therefore the difference.⑤
    I’m calculating the values using the logspace starting from log_start, which I
    set by default to -2, and ending on 0\. The number of values in that space that
    I ask for is decay_steps and the base is log_base, which I default to 10\. Notice,
    I reverse those values!⑥ Because the values may not end exactly at 0, given it’s
    the log, I change them to be between 0 and 1 so that the curve looks smooth and
    nice.⑦ Then, we can do a linear transformation and get points between init_value
    and min_value.⑧ This pad function just repeats the rightmost value rem_step number
    of times. |
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们将用于alpha和epsilon的衰减计划与上一章中用于alpha的相同。这次让我们更详细地探讨一下。② 我个人喜欢这个函数的地方在于，你可以给它一个初始值、一个最小值，以及从初始值衰减到最小值的max_steps百分比。③
    这个decay_steps是值衰减终止的索引，min_value继续直到max_steps。④ 因此，rem_steps是这个差值。⑤ 我正在使用以log_start为起始点，默认设置为-2，并以0为结束点的logspace来计算值。我请求的这个空间中的值的数量是decay_steps，基数是log_base，默认设置为10。注意，我反转了这些值！⑥
    因为值可能不会正好结束在0，考虑到它是对数，我将它们改为介于0和1之间，以便曲线看起来平滑且美观。⑦ 然后，我们可以进行线性变换，得到init_value和min_value之间的点。⑧
    这个pad函数只是重复最右侧的值rem_step次数。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonGenerate exploratory policy
    trajectories |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我说Python生成探索性策略轨迹 |'
- en: '|  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① This version of the generate_trajectory function is slightly different. We
    now need to take in an action-selecting strategy, instead of a greedy policy.②
    We begin by initializing the done flag and a list of experiences named trajectory.③
    We then start looping through until the done flag is set to true.④ We reset the
    environment to interact in a new episode.⑤ Then start counting steps t.⑥ Then,
    use the passed ‘select_action’ function to pick an action.⑦ We step the environment
    using that action and obtain the full experience tuple.⑧ We append the experience
    to the trajectory list.⑨ If we hit a terminal state and the ‘done’ flag is raised,
    then break and return.⑩ And if the count of steps ‘t’ in the current trajectory
    hits the maximum allowed, we clear the trajectory, break, and try to obtain another
    trajectory.⑪ Remember to update the state.⑫ Finally, we return a NumPy version
    of the trajectory for easy data manipulation. |
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成轨迹函数的这个版本略有不同。我们现在需要接受一个动作选择策略，而不是贪婪策略。② 我们首先初始化 done 标志和一个名为 trajectory
    的经验列表。③ 然后开始循环，直到 done 标志设置为 true。④ 将环境重置以进行新循环的交互。⑤ 然后开始计数步骤 t。⑥ 然后，使用传递的 ‘select_action’
    函数选择一个动作。⑦ 使用该动作步进环境，并获得完整经验元组。⑧ 将经验追加到轨迹列表中。⑨ 如果我们达到终端状态并且 ‘done’ 标志被提升，则中断并返回。⑩
    如果当前轨迹中的步骤数 ‘t’ 达到最大允许值，则清除轨迹，中断，并尝试获取另一个轨迹。⑪ 记得更新状态。⑫ 最后，我们返回轨迹的 NumPy 版本，以便于数据操作。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonMonte Carlo control 1/2 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| ![Python 图标](../Images/icons_Python.png) | 我会说 PythonMonte Carlo 控制 1/2 |'
- en: '|  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① mc_control is similar to mc_prediction. The two main differences is that we
    now estimate the action-value function Q, and that we need to explore.② Notice
    in the function definition we are using values for epsilon to configure a decaying
    schedule for random exploration.③ We calculate values for the discount factors
    in advance. Notice we use max_steps because that’s the maximum length of a trajectory.④
    We also calculate alphas in advance using the passed values.⑤ Finally, we repeat
    for epsilon, and obtain an array that will work for the full training session.⑥
    Here we’re just setting up variables, including the Q-function.⑦ This is an epsilon-greedy
    strategy, though we decay epsilon on each episode, not step.⑧ Continues ... |
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ① mc_control 与 mc_prediction 类似。两个主要区别是，我们现在现在估计动作值函数 Q，并且我们需要进行探索。② 注意在函数定义中，我们使用
    epsilon 的值来配置随机探索的衰减计划。③ 我们提前计算折扣因子的值。注意我们使用 max_steps，因为那是轨迹的最大长度。④ 我们还提前使用传递的值计算
    alphas。⑤ 最后，我们重复 epsilon，并获得一个将在整个训练会话中工作的数组。⑥ 这里我们只是在设置变量，包括 Q 函数。⑦ 这是一个 epsilon-贪婪策略，尽管我们在每个循环而不是每一步衰减
    epsilon。⑧ 继续…… |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonMonte Carlo control 2/2 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ![Python 图标](../Images/icons_Python.png) | 我会说 PythonMonte Carlo 控制 2/2 |'
- en: '|  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ⑨ Repeating the previous line so that you can keep up with the indentation⑩
    Here we’re entering the episode loop. We’ll run for n_episodes. Remember that
    tqdm shows a nice progress bar, nothing out of this world.⑪ Every new episode
    ‘e’ we generate a new trajectory with the exploratory policy defined by the select_action
    function. We limit the trajectory length to max_steps.⑫ We now keep track of the
    visits to state-action pairs; this is another important change from the mc_prediction
    method.⑬ Notice here we’re processing trajectories *offline*, that is, after the
    interactions with the environment have stopped.⑭ Here we check for state-action-pair
    visits and act accordingly.⑮ We proceed to calculating the return the same way
    we did with the prediction method, except that we’re using a Q-function this time.⑯
    Notice how we’re using the alphas.⑰ After that, it’s a matter of saving values
    for post analysis.⑱ At the end, we extract the state-value function and the greedy
    policy. |
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 重复上一行以便保持缩进⑩ 这里我们进入循环播放。我们将运行 n_episodes。记住，tqdm 会显示一个漂亮的进度条，没有什么超乎寻常的。⑪ 每生成一个新的循环‘e’，我们都会使用由
    select_action 函数定义的探索策略生成一个新的轨迹。我们限制轨迹长度为 max_steps。⑫ 我们现在跟踪状态-动作对的访问次数；这是与 mc_prediction
    方法相比的另一个重要变化。⑬ 注意这里我们是在离线处理轨迹，即在与环境的交互停止后。⑭ 这里我们检查状态-动作对访问，并相应地采取行动。⑮ 我们以与预测方法相同的方式计算回报，只是这次我们使用了一个
    Q 函数。⑯ 注意我们如何使用 alphas。⑰ 之后，就是保存值以供后续分析的问题了。⑱ 最后，我们提取状态值函数和贪婪策略。|
- en: 'SARSA: Improving policies after each step'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SARSA：在每一步后改进策略
- en: As we discussed in the previous chapter, one of the disadvantages of Monte Carlo
    methods is that they’re offline methods in an episode-to-episode sense. What that
    means is that we must wait until we reach a terminal state before we can make
    any improvements to our value function estimates. However, it’s straightforward
    to use temporal-difference prediction for the policy-evaluation phase, instead
    of Monte Carlo prediction. By replacing MC with *TD* prediction, we now have a
    different algorithm, the well-known SARSA agent.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一章所讨论的，蒙特卡洛方法的一个缺点是它们在场景到场景的层面上是离线方法。这意味着我们必须等待达到终端状态，才能对我们的价值函数估计进行任何改进。然而，对于策略评估阶段，使用时间差分预测代替蒙特卡洛预测是直接的。通过用*TD*预测替换MC，我们现在有一个不同的算法，那就是众所周知的SARSA智能体。
- en: '![](../Images/06_09.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06_09.png)'
- en: Comparison between planning and control methods
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 计划与控制方法比较
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe SARSA agent 1/2 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonSARSA智能体 1/2 |'
- en: '|  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '① The SARSA agent is the direct conversion of *TD* for control problems. That
    is, at its core, SARSA is *TD* with two main changes. First, it evaluates the
    action-value function Q. Second, it uses an exploratory policy-improvement step.②
    We’re doing the same thing we did with mc_control using epsilon here.③ First,
    create several handy variables. Remember, pi_track will hold a greedy policy per
    episode.④ Then, we create the Q-function. I’m using ‘np.float64’ precision ...
    perhaps overkill.⑤ ‘Q_track’ will hold the estimated Q-function per episode.⑥
    The select_action function is the same as before: an epsilon-greedy strategy.⑦
    In SARSA, we don’t need to calculate all discount factors in advance, because
    we won’t use full returns. Instead, we use estimated returns, so we can calculate
    discounts online.⑧ Notice we are, however, calculating all alphas in advance.
    This function call returns a vector with corresponding alphas to use.⑨ The select_action
    function isn’t a decaying strategy on its own. We’re calculating decaying epsilons
    in advance, so our agent will be using a decaying epsilon-greedy strategy.⑩ Let’s
    continue on the next page. |'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ① SARSA智能体是控制问题的*TD*的直接转换。也就是说，在其核心，SARSA是*TD*，但有两个主要变化。首先，它评估动作值函数Q。其次，它使用一个探索性的策略改进步骤。②
    我们在这里使用epsilon与mc_control做同样的事情。③ 首先，创建几个有用的变量。记住，pi_track将保存每个场景的贪婪策略。④ 然后，我们创建Q函数。我使用‘np.float64’精度...可能有点过度。⑤
    ‘Q_track’将保存每个场景的估计Q函数。⑥ select_action函数与之前相同：一个epsilon贪婪策略。⑦ 在SARSA中，我们不需要预先计算所有折扣因子，因为我们不会使用完整回报。相反，我们使用估计回报，因此我们可以在线计算折扣。⑧
    注意，我们确实预先计算了所有alpha。这个函数调用返回一个包含相应alpha的向量以供使用。⑨ select_action函数本身不是一个衰减策略。我们预先计算衰减的epsilon，因此我们的智能体将使用衰减的epsilon贪婪策略。⑩
    让我们继续到下一页。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe SARSA agent 2/2 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说PythonSARSA智能体 2/2 |'
- en: '|  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ⑪ Same line. You know the drill.⑫ We’re now inside the episode loop.⑬ We start
    each episode by resetting the environment and the done flag.⑭ We select the action
    (perhaps exploratory) for the initial state.⑮ We repeat until we hit a terminal
    state.⑯ First, step the environment and get the experience.⑰ Notice that before
    we make any calculations, we need to obtain the action for the next step.⑱ We
    calculate the td_target using that next state-action pair. And we do the little
    trick for terminal states of multiplying by the expression (not done), which zeros
    out the future on terminal.⑲ Then calculate the td_error as the difference between
    the target and current estimate.⑳ Finally, update the Q-function by moving the
    estimates a bit toward the error.㉑ We update the state and action for the next
    step.㉒ Save the Q-function and greedy policy for analysis.㉓ At the end, calculate
    the estimated optimal V-function and its greedy policy, and return all this. |
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 同一行。你知道该怎么做。⑫ 我们现在处于场景循环中。⑬ 我们通过重置环境和done标志来开始每个场景。⑭ 我们为初始状态选择动作（可能是探索性的）。⑮
    我们重复操作，直到达到终端状态。⑯ 首先，执行环境步骤并获取经验。⑰ 注意，在我们进行任何计算之前，我们需要获取下一步的动作。⑱ 我们使用那个下一个状态-动作对来计算td_target。并且我们对终端状态进行一个小技巧，即乘以表达式（not
    done），这将零化终端的未来。⑲ 然后计算td_error作为目标和当前估计之间的差异。⑳ 最后，通过将估计稍微移动到误差方向来更新Q函数。㉑ 我们更新下一个步骤的状态和动作。㉒
    保存Q函数和贪婪策略以供分析。㉓ 最后，计算估计的最优V函数及其贪婪策略，并返回所有这些。
- en: '| ŘŁ | With An RL AccentBatch vs. offline vs. online learning problems and
    methods |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的批量、离线、在线学习问题和方法 |'
- en: '|  | **Batch learning problems and methods**: When you hear the term “batch
    learning,” people are referring to one of two things: they mean a type of learning
    problem in which experience samples are fixed and given in advance, or they mean
    a type of learning method which is optimized for learning synchronously from a
    batch of experiences, also called fitting methods. Batch learning methods are
    typically studied with non-interactive learning problems, more specifically, batch
    learning problems. But batch learning methods can also be applied to interactive
    learning problems. For instance, growing batch methods are batch learning methods
    that also collect data: they “grow” the batch. Also, batch learning problems don’t
    have to be solved with batch learning methods, the same way that batch learning
    methods aren’t designed exclusively to solve batch learning problems.**Offline
    learning problems and methods**: When you hear the term “offline learning,” people
    are usually referring to one of two things: they’re either talking about a problem
    setting in which there’s a simulation available for collecting data (as opposed
    to a real-world, online environment), or they could also be talking about learning
    methods that learn offline, meaning between episodes, for instance. Note that,
    in offline learning methods, learning and interaction can still be interleaved,
    but performance is only optimized after samples have been collected, similar to
    the growing batch described previouisly, but with the difference that, unlike
    growing batch methods, offline methods commonly discard old samples; they don’t
    grow a batch. MC methods, for instance, are often considered offline because learning
    and interaction are interleaved on an episode-to-episode basis. There are two
    distinct phases, interacting and learning; MC is interactive, but also an offline
    learning method.**Online learning problems and methods**: When you hear the term
    “online learning,” people are referring to one of two things: either to learning
    while interacting with a live system, such a robot, or to methods that learn from
    an experience as soon as it’s collected, on each and every time step.Note that
    offline and online learning are often used in different contexts. I’ve seen offline
    versus online to mean non-interactive versus interactive, but I’ve also seen them,
    as I mentioned, for distinguishing between learning from a simulator versus a
    live system.My definitions here are consistent with common uses of many RL researchers:
    Richard Sutton (2018 book), David Silver (2015 lectures), Hado van Hasselt (2018
    lectures), Michael Littman (2015 paper), and Csaba Szepesvari (2009 book).Be aware
    of the lingo, though. That’s what’s important. |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | **批量学习问题和方法**：当你听到“批量学习”这个术语时，人们通常指的是以下两种情况之一：他们指的是一种学习问题，其中经验样本是固定的，并且提前给出，或者他们指的是一种学习方法，该方法针对从一批经验中同步学习进行优化，也称为拟合方法。批量学习方法通常与非交互式学习问题一起研究，更具体地说，是与批量学习问题。但是，批量学习方法也可以应用于交互式学习问题。例如，增长批量方法是同时收集数据的批量学习方法：它们“增长”批量。此外，批量学习问题不必用批量学习方法来解决，就像批量学习方法并不是专门设计来仅解决批量学习问题一样。**离线学习问题和方法**：当你听到“离线学习”这个术语时，人们通常指的是以下两种情况之一：他们可能正在讨论一个可以用于收集数据的模拟环境（与真实世界的在线环境相对），或者他们可能正在讨论离线学习的方法，这意味着在剧集之间学习，例如。请注意，在离线学习方法中，学习和交互仍然可以交织在一起，但性能仅在收集样本之后才会得到优化，类似于之前描述的增长批量，但不同之处在于，与增长批量方法不同，离线方法通常丢弃旧样本；它们不会增长批量。例如，MC方法通常被认为是离线的，因为学习和交互是在剧集之间交织的。有两个不同的阶段，交互和学习；MC是交互式的，但也是一个离线学习方法。**在线学习问题和方法**：当你听到“在线学习”这个术语时，人们通常指的是以下两种情况之一：要么是在与实时系统（如机器人）交互时学习，要么是在收集经验后立即从经验中学习的方法，在每个时间步长上。请注意，离线和在线学习通常在不同的上下文中使用。我见过离线与在线被用来表示非交互式与交互式，但我也见过它们，正如我提到的，用来区分从模拟器学习与从实时系统学习。我这里的定义与许多强化学习研究者的常用定义一致：理查德·萨顿（2018年书籍）、大卫·西尔弗（2015年讲座）、哈德·范·哈塞尔特（2018年讲座）、迈克尔·利特曼（2015年论文）和卡萨·塞佩什瓦里（2009年书籍）。但是要注意术语，那才是重要的。
    |'
- en: Decoupling behavior from learning
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将行为与学习解耦
- en: I want you to think about the *TD* update equation for state-value functions
    for a second; remember, it uses *r*[*t*+1] *+* *γ**v(S*[*t*+1]) as the *TD* target.
    However, if you stare at the *TD* update equation for action-value functions instead,
    which is *r*[*t*+1] *+* *γ**Q(S*[*t*+1]*, A*[*t*+1]), you may notice there are
    a few more possibilities there. Look at the action being used and what that means.
    Think about what else you can put in there. One of the most critical advancements
    in reinforcement learning was the development of the *Q-learning* algorithm, a
    model-free off-policy bootstrapping method that directly approximates the optimal
    policy despite the policy generating experiences. Yes, this means the agent, in
    theory, can act randomly and still find the optimal value function and policies.
    How is this possible?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你花点时间思考一下状态值函数的**TD**更新方程；记住，它使用*r*[*t*+1] *+* *γ**v(S*[*t*+1])作为**TD**目标。然而，如果你盯着动作值函数的**TD**更新方程看，它是*r*[*t*+1]
    *+* *γ**Q(S*[*t*+1]*, A*[*t*+1])，你可能注意到这里有几个更多的可能性。看看所使用的动作及其含义。想想你还能放进去什么。强化学习中最关键的进步之一是**Q-learning**算法的发展，这是一种无模型、离策略的自举方法，它直接近似最优策略，尽管策略生成经验。是的，这意味着代理，从理论上讲，可以随机行动，仍然可以找到最优值函数和政策。这是怎么可能的？
- en: 'Q-learning: Learning to act optimally, even if we choose not to'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q-learning：即使我们选择不这样做，也要学会最优行动
- en: 'The SARSA algorithm is a sort of “learning on the job.” The agent learns about
    the same policy it uses for generating experience. This type of learning is called
    on-policy. On-policy learning is excellent—we learn from our own mistakes. But,
    let me make it clear, in on-policy learning, we learn from our own current mistakes
    only. What if we want to learn from our own previous mistakes? What if we want
    to learn from the mistakes of others? In on-policy learning, you can’t. Off-policy
    learning, on the other hand, is sort of “learning from others.” The agent learns
    about a policy that’s different from the policy-generating experiences. In off-policy
    learning, there are two policies: a behavior policy, used to generate experiences,
    to interact with the environment, and a target policy, which is the policy we’re
    learning about. SARSA is an on-policy method; Q-learning is an off-policy one.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA算法是一种“在工作中学习”的方法。代理学习的是它用于生成经验的政策。这种学习被称为在策略学习。在策略学习非常出色——我们从自己的错误中学习。但是，让我明确一点，在在策略学习中，我们只从自己的当前错误中学习。如果我们想从自己的过去错误中学习呢？如果我们想从别人的错误中学习呢？在在策略学习中，你做不到。另一方面，离策略学习是一种“从别人那里学习”的方法。代理学习的是与生成经验的政策不同的政策。在离策略学习中，有两种策略：一种行为策略，用于生成经验，与环境交互，另一种是目标策略，这是我们正在学习的策略。SARSA是一种在策略方法；Q-learning是一种离策略方法。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathSARSA vs. Q-learning update
    equations |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 给我看看数学SARSA与Q-learning更新方程 |'
- en: '|  | ![](../Images/06_10_Sidebar16.png) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/06_10_Sidebar16.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Q-learning agent 1/2
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonQ-learning代理 1/2 |'
- en: '|  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Notice that the beginning of the Q-learning agent is identical to the beginning
    of the SARSA agent.② In fact, I’m even using the same exact hyperparameters for
    both algorithms.③ Here are several handy variables.④ The Q-function and the tracking
    variable for offline analysis⑤ The same epsilon-greedy action-selection strategy⑥
    The vector with all alphas to be used during learning⑦ The vector with all epsilons
    to decay as desired⑧ Let’s continue on the next page. |
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ① 注意到Q-learning代理的开始与SARSA代理的开始是相同的。② 事实上，我甚至为这两个算法使用了完全相同的超参数。③ 这里有几个实用的变量。④
    Q函数和离线分析的跟踪变量⑤ 同样的epsilon-greedy动作选择策略⑥ 学习期间要使用的所有alpha的向量⑦ 要按需衰减的所有epsilon的向量⑧
    让我们继续到下一页。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe Q-learning agent 2/2
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonQ-learning代理 2/2 |'
- en: '|  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ⑨ Same line as before⑩ We’re iterating over episodes.⑪ We reset the environment
    and get the initial state, set the done flag to false.⑫ Now enter the interaction
    loop for online learning (steps).⑬ We repeat the loop until we hit a terminal
    state and a done flag is raised.⑭ First thing we do is select an action for the
    current state. Notice the use of epsilons.⑮ We step the environment and get a
    full experience tuple (*s*, *a*, *s’*, *r*, *d*).⑯ Next, we calculate the *TD*
    target. Q-learning is a special algorithm because it tries to learn the optimal
    action-value function q* even if it uses an exploratory policy such as the decaying
    epsilon-greedy we’re running. This is called off-policy learning.⑰ Again, the
    “not done” ensures the max value of the next state is set to zero on terminal
    states. It’s important that the agent doesn’t expect any reward after death!!!⑱
    Next, we calculate the *TD* error as the difference between the estimate and the
    target.⑲ We then move the Q-function for the state-action pair to be a bit closer
    to the error.⑳ Next, we update the state.㉑ Save the Q-function and the policy.㉒
    And obtain the V-function and final policy on exit. |
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 与之前同一行⑩ 我们正在迭代经过的情节。⑪ 我们重置环境并获取初始状态，将完成标志设置为false。⑫ 现在进入在线学习的交互循环（步骤）。⑬ 我们重复循环，直到达到终端状态并提升完成标志。⑭
    我们首先为当前状态选择一个动作。注意epsilon的使用。⑮ 我们对环境进行一步操作，并获取一个完整的经验元组（*s*, *a*, *s’*, *r*, *d*）。⑯
    接下来，我们计算*TD*目标。Q-learning是一个特殊的算法，因为它试图学习最优动作值函数q*，即使它使用的是探索性策略，例如我们正在运行的衰减epsilon-greedy。这被称为离策略学习。⑰
    再次，"未完成"确保在终端状态下下一个状态的最大值设置为零。确保代理在死亡后不会期望任何奖励非常重要！！！⑱ 接下来，我们计算*TD*误差，即估计值与目标之间的差异。⑲
    然后将状态动作对的Q函数移动到更接近误差的位置。⑳ 接下来，我们更新状态。㉑ 保存Q函数和政策。㉒ 并在退出时获得V函数和最终政策。|
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyHumans also learn on-policy
    and off-policy |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Miguel.png) | 米格尔的类比人类也进行策略学习和离策略学习 |'
- en: '|  | On-policy is learning about a policy that’s being used to make decisions;
    you can think about it as “learning on the job.” Off-policy learning is learning
    about a policy different from the policy used for making decisions. You can think
    about it as “learning from others’ experiences,” or “learning to be great, without
    trying to be great.” Both are important ways of learning and perhaps vital for
    a solid decision maker. Interestingly, you can see whether a person prefers to
    learn on-policy or off-policy pretty quickly.My son, for instance, tends to prefer
    on-policy learning. Sometimes I see him struggle playing with a toy, so I come
    over and try to show him how to use it, but then he complains until I leave him
    alone. He keeps trying and trying, and he eventually learns, but he prefers his
    own experience instead of others’. On-policy learning is a straightforward and
    stable way of learning.My daughter, on the other hand, seems to be OK with learning
    off-policy. She can learn from my demonstrations before she even attempts a task.
    I show her how to draw a house, then she tries.Now, **beware**; this is a stretch
    analogy. *different*, then you can refer to that as off-policy learning.Also,
    before you make conclusions about which one is “best,” know that in RL, both have
    pros and cons. On one hand, on-policy learning is intuitive and stable. If you
    want to get good at playing the piano, why not practice the piano?On the other
    hand, it seems useful to learn from sources other than your own hands-on experience;
    after all, there’s only so much time in a day. Maybe meditation can teach you
    something about playing the piano, and help you get better at it. But, while off-policy
    learning helps you learn from multiple sources (and/or multiple skills), methods
    using off-policy learning are often of higher variance and, therefore, slower
    to converge.Additionally, know that off-policy learning is one of the elements
    that, when combined, have been proven to lead to divergence: off-policy learning,
    bootstrapping, and function approximation. These don’t play nice together. You’ve
    learned about the first two, and the third one is soon to come. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 在策略学习中，是关于正在使用的策略进行决策的学习；你可以将其视为“在工作中学习”。离策略学习是关于与用于决策的策略不同的策略的学习。你可以将其视为“从他人的经验中学习”，或者“学习如何做得更好，而不试图做得更好”。这两种都是重要的学习方法，也许对于成为一个优秀的决策者至关重要。有趣的是，你可以很快地看出一个人更喜欢在策略学习还是离策略学习中。例如，我的儿子倾向于偏好在策略学习中。有时我看到他在玩玩具时遇到困难，所以我走过去试图教他如何使用它，但他直到我离开才会抱怨。他一直尝试，最终学会了，但他更喜欢自己的经验而不是别人的。在策略学习是一种直接且稳定的学习方式。另一方面，我的女儿似乎对离策略学习没有问题。她在尝试任务之前就可以从我的演示中学习。我教她如何画房子，然后她尝试。现在，**请注意**；这是一个牵强的类比。*不同*，那么你可以将其称为离策略学习。此外，在得出关于哪一种“最好”的结论之前，要知道在强化学习中，两者都有优点和缺点。一方面，在策略学习直观且稳定。如果你想擅长弹钢琴，为什么不练习钢琴呢？另一方面，似乎从除了自己的实际经验之外的其他来源学习是有用的；毕竟，一天中只有那么多时间。也许冥想可以教你一些关于弹钢琴的知识，并帮助你提高弹钢琴的技巧。但是，虽然离策略学习可以帮助你从多个来源（和/或多个技能）中学习，但使用离策略学习的方法通常具有更高的方差，因此收敛速度较慢。此外，要知道离策略学习是导致发散的元素之一：离策略学习、自助学习和函数逼近。这些并不总是相处得很好。你已经了解了前两个，第三个很快就会到来。
    |'
- en: '| ŘŁ | With An RL AccentGreedy in the limit with infinite exploration and stochastic
    approx. theory |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的贪婪策略在无限探索和随机逼近理论中 |'
- en: '|  | Greedy in the limit with infinite exploration (GLIE) is a set of requirements
    that on-policy RL algorithms, such as Monte Carlo control and SARSA, must meet
    to guarantee convergence to the optimal policy. The requirements are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 贪婪策略在无限探索（GLIE）是一组要求，这些要求是策略强化学习算法（如蒙特卡洛控制和SARSA）必须满足的，以确保收敛到最优策略。这些要求如下：'
- en: All state-action pairs must be explored infinitely often.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有状态-动作对都必须无限次地被探索。
- en: The policy must converge on a greedy policy.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略必须收敛到贪婪策略。
- en: 'What this means in practice is that an epsilon-greedy exploration strategy,
    for instance, must slowly decay epsilon towards zero. If it goes down too quickly,
    the first condition may not be met; if it decays too slowly, well, it takes longer
    to converge.Notice that for off-policy RL algorithms, such as Q-learning, the
    only requirement of these two that holds is the first one. The second one is no
    longer a requirement because in off-policy learning, the policy learned about
    is different than the policy we’re sampling actions from. Q-learning, for instance,
    only requires all state-action pairs to be updated sufficiently, and that’s covered
    by the first condition in this section.Now, whether you can check off with certainty
    that requirement using simple exploration strategies such as epsilon-greedy, that’s
    another question. In simple grid worlds and discrete action and state spaces,
    epsilon-greedy most likely works. But, it’s easy to imagine intricate environments
    that would require more than random behavior.There is another set of requirements
    for general convergence based on stochastic approximation theory that applies
    to all of these methods. Because we’re learning from samples, and samples have
    some variance, the estimates won’t converge unless we also push the learning rate,
    alpha, towards zero:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上这意味着，例如，ε-贪婪探索策略必须缓慢地将ε衰减到零。如果它下降得太快，第一个条件可能无法满足；如果衰减得太慢，那么，收敛需要更长的时间。注意，对于离策略强化学习算法，如Q学习，这两个条件中只有第一个是成立的。第二个条件不再是一个要求，因为在离策略学习中，所学习到的策略与我们采样动作的策略不同。例如，Q学习只需要所有状态-动作对都得到充分更新，而这在本节的第一条条件中已经涵盖。现在，你是否可以确信地使用简单的探索策略，如ε-贪婪，来满足那个要求，这是另一个问题。在简单的网格世界和离散的动作和状态空间中，ε-贪婪很可能是有效的。但是，很容易想象出需要比随机行为更复杂的复杂环境。基于随机逼近理论，对于所有这些方法，还有另一套基于一般收敛的要求。因为我们是从样本中学习的，样本存在一些变异性，除非我们也推动学习率α向零，否则估计不会收敛：
- en: The sum of learning rates must be infinite.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率的总和必须是无限的。
- en: The sum of squares of learning rates must be finite.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率平方的总和必须是有限的。
- en: That means you must pick a learning rate that decays but never reaches zero.
    For instance, if you use 1/*t* or 1/*e*, the learning rate is initially large
    enough to ensure the algorithm doesn’t follow only a single sample too tightly,
    but becomes small enough to ensure it finds the signal behind the noise.Also,
    even though these convergence properties are useful to know for developing the
    theory of RL algorithms, in practice, learning rates are commonly set to a small-enough
    constant, depending on the problem. Also, know that a small constant is better
    for non-stationary environments, which are common in the real world. |
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你必须选择一个衰减但永远不会达到零的学习率。例如，如果你使用 1/*t* 或 1/*e*，学习率最初足够大，以确保算法不会过于紧密地跟随单个样本，但最终变得足够小，以确保它能找到噪声背后的信号。此外，尽管这些收敛性质对于开发强化学习算法的理论是有用的，但在实践中，学习率通常根据问题设置为足够小的常数。另外，要知道对于非平稳环境，小常数更好，这在现实世界中很常见。|
- en: '| ŘŁ | With An RL AccentOn-policy vs. off-policy learning |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ŘŁ | 带有强化学习口音的在线策略与离线策略学习 |'
- en: '|  | **On-policy learning**: Refers to methods that attempt to evaluate or
    improve the policy used to make decisions. It is straightforward; think about
    a single policy. This policy generates behavior. Your agent evaluates that behavior
    and select areas of improvement based on those estimates. Your agent learns to
    assess and improve the same policy it uses for generating the data.**Off-policy
    learning**: Refers to methods that attempt to evaluate or improve a policy different
    from the one used to generate the data. This one is more complex. Think about
    two policies. One produces the data, the experiences, the behavior; but your agent
    uses that data to evaluate, improve, and overall learn about a different policy,
    a different behavior. Your agent learns to assess and improve a policy different
    than the one used for generating the data. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | **在线策略学习**：指的是尝试评估或改进用于决策的策略的方法。这是直截了当的；想想一个单一的政策。这个政策产生行为。你的智能体评估这种行为，并根据这些估计选择改进的区域。你的智能体学习评估和改进它用于生成数据的同一策略。**离线策略学习**：指的是尝试评估或改进与用于生成数据的策略不同的策略的方法。这更复杂。想想两个策略。一个产生数据，经验，行为；但你的智能体使用这些数据来评估、改进，并总体上学习关于不同的策略，不同的行为。你的智能体学习评估和改进一个不同于用于生成数据的策略。|'
- en: 'Double Q-learning: A max of estimates for an estimate of a max'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双Q学习：对最大估计值的估计的最大值
- en: Q-learning often overestimates the value function. Think about this. On every
    step, we take the maximum over the estimates of the action-value function of the
    next state. But what we need is the actual value of the maximum action-value function
    of the next state. In other words, we’re using the maximum over merely estimates
    as an estimate of the maximum.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习经常高估价值函数。想想看。在每一步，我们都在下一个状态的动作值函数估计中取最大值。但我们需要的是下一个状态的最大动作值函数的实际值。换句话说，我们正在使用仅仅估计的最大值作为最大值的估计。
- en: Doing this isn’t only an inaccurate way of estimating the maximum value but
    also a more significant problem, given that these bootstrapping estimates, which
    are used to form *TD* targets, are often biased. The use of a maximum of biased
    estimates as the estimate of the maximum value is a problem known as *maximization
    bias*.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做不仅是一种不准确估计最大值的方法，而且是一个更严重的问题，因为这些用于形成*TD*目标的自我引导估计往往是有偏差的。将最大偏差估计用作最大值估计的问题被称为*最大化偏差*。
- en: 'It’s simple. Imagine an action-value function whose actual values are all zeros,
    but the estimates have bias: some positive, some negative: for example, *always*
    taking a max, we always tend to take high values even if they have the largest
    bias, the biggest error. Doing this over and over again compounds the errors in
    a negative way.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。想象一个动作值函数，其实际值都是零，但估计有偏差：有的正，有的负：例如，*总是*取最大值，我们总是倾向于取高值，即使它们有最大的偏差，最大的错误。这样做一次又一次地以负面方式累积错误。
- en: 'We all know someone with a positive-bias personality who has let something
    go wrong in their lives: someone who’s blinded by shiny things that aren’t as
    shiny. To me, this is one of the reasons why many people advise against feeding
    the AI hype; because overestimation is often your enemy, and certainly something
    to mitigate for an improved performance.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道有些人性格积极，但在生活中却让事情出了差错：有些人被那些并不那么闪亮的东西所迷惑。对我来说，这就是许多人反对过分吹捧人工智能的原因之一；因为高估往往是你的敌人，而且确实需要采取措施来减轻，以改善性能。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe double Q-learning agent
    1/3 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python双Q学习代理 1/3 |'
- en: '|  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '① As you’d expect, double Q-learning takes the same exact arguments as Q-Learning.②
    We start with the same old handy variables.③ But immediately you should see a
    big difference here. We’re using two state-value functions Q1 and Q2\. You can
    think of this similar to cross-validation: one Q-function estimates will help
    us validate the other Q-function estimates. The issue, though, is now we’re splitting
    the experience between two separate functions. This somewhat slows down training.④
    The rest on this page is pretty straightforward, and you should already know what’s
    happening. The select_action, alphas, and epsilons are calculated the same way
    as before.⑤ Continues ... |'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ① 如你所料，双Q学习与Q学习具有相同的精确论点。② 我们从相同的旧变量开始。③ 但你应该立即看到这里有一个很大的不同。我们正在使用两个状态值函数Q1和Q2。你可以将其视为交叉验证：一个Q函数的估计将帮助我们验证另一个Q函数的估计。然而，问题现在是我们正在将经验分割成两个不同的函数。这多少会减慢训练速度。④
    本页的其余部分相当直接，你应该已经知道发生了什么。select_action、alphas和epsilons的计算方式与之前相同。⑤ 继续阅读... |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe double Q-learning agent
    2/3 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python双Q学习代理 2/3 |'
- en: '|  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '⑥ From the previous page⑦ We’re back inside the episode loop.⑧ For every new
    episode, we start by resetting the environment and getting an initial state.⑨
    Then we repeat until we hit a terminal state (and the done flag is set to True).⑩
    For every step we select an action using our select_action function.⑪ But notice
    something interesting: we’re using the mean of our two Q-functions!! We could
    also use the sum of our Q-functions here. They’ll give similar results.⑫ We then
    send the action to the environment and get the experience tuple.⑬ Things start
    changing now. Notice we flip a coin to determine an update to Q1 or Q2.⑭ We use
    the action Q1 thinks is best ...⑮ ... but get the value from Q2 to calculate the
    *TD* target.⑯ Notice here, we get the value from Q2 and prescribed by Q1.⑰ Then
    calculate the *TD* error from the Q1 estimate.⑱ Finally, move our estimate closer
    to that target by using the error.⑲ This line repeats on the next page ... |'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 从上一页⑦ 我们回到了剧集循环中。⑧ 对于每个新的剧集，我们首先重置环境并获取一个初始状态。⑨ 然后我们重复，直到我们达到终端状态（并且 done
    标志被设置为 True）。⑩ 对于每一步，我们使用 select_action 函数选择一个动作。⑪ 但请注意一个有趣的事情：我们正在使用两个 Q 函数的均值！！我们也可以在这里使用
    Q 函数的总和。它们会给出相似的结果。⑫ 然后我们将动作发送到环境中并获取经验元组。⑬ 现在事情开始发生变化。注意我们抛硬币来决定更新 Q1 或 Q2。⑭
    我们使用 Q1 认为最好的动作 ...⑮ ... 但使用 Q2 的值来计算 *TD* 目标。⑯ 注意在这里，我们使用 Q2 的值并由 Q1 指定。⑰ 然后计算
    Q1 估计的 *TD* 错误。⑱ 最后，通过使用错误将我们的估计值移动到目标附近。⑲ 这一行将在下一页重复 ... |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe double Q-learning agent
    3/3 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说 Python 双 Q-learning 代理 3/3 |'
- en: '|  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ⑳ Okay. From the previous page, we were calculating Q1.㉑ Now if the random int
    was 0 (50% of the times), we update the other Q-function, Q2.㉒ But, it’s basically
    the mirror of the other update. We get the argmax of Q2.㉓ Then use that action,
    but get the estimate from the other Q-function Q1.㉔ Again, pay attention to the
    roles of Q1 and Q2 here reversed.㉕ We calculate the *TD* error from the Q2 this
    time.㉖ And use it to update the Q2 estimate of the state-action pair.㉗ Notice
    how we use the ‘alphas’ vector.㉘ We change the value of the state variable and
    keep looping, again until we land on a terminal state and the ‘done’ variable
    is set to True.㉙ Here we store Q1 and Q2 for offline analysis.㉚ Notice the policy
    is the argmax of the mean of Q1 and Q2.㉛ The final Q is the mean.㉜ The final V
    is the max of Q.㉝ The final policy is the argmax of the mean of Qs.㉞ We end up
    returning all this. |
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ⑳ 好的。从上一页，我们正在计算 Q1。㉑ 如果随机整数是 0（50% 的时间），我们更新另一个 Q 函数，Q2。㉒ 但是，这基本上是另一个更新的镜像。我们得到
    Q2 的 argmax。㉓ 然后使用那个动作，但使用另一个 Q 函数 Q1 的估计值。㉔ 再次注意 Q1 和 Q2 在这里的角色是相反的。㉕ 这次我们计算
    Q2 的 *TD* 错误。㉖ 并用它来更新状态-动作对的 Q2 估计值。㉗ 注意我们如何使用‘alphas’向量。㉘ 我们改变状态变量的值并继续循环，直到我们到达一个终端状态并且‘done’变量被设置为
    True。㉙ 在这里我们存储 Q1 和 Q2 以供离线分析。㉚ 注意策略是 Q1 和 Q2 均值的 argmax。㉛ 最终的 Q 是均值。㉜ 最终的 V 是
    Q 的最大值。㉝ 最终的策略是 Qs 均值的 argmax。㉞ 我们最终返回所有这些。|
- en: One way of dealing with maximization bias is to track estimates in two Q-functions.
    At each time step, we choose one of them to determine the action, to determine
    which estimate is the highest according to that Q-function. But, then we use the
    other Q-function to obtain that action’s estimate. By doing this, there’s a lower
    chance of always having a positive bias error. Then, to select an action for interacting
    with the environment, we use the average, or the sum, across the two Q-functions
    for that state. That is, the maximum over *Q*[1](*S*[*t+1*]*)+Q*[2]*(S*[*t+1*]),
    for instance. The technique of using these two Q-functions is called *double learning*,
    and the algorithm that implements this technique is called *double Q-learning*.
    In a few chapters, you’ll learn about a deep reinforcement learning algorithm
    called *double deep Q-networks* (DDQN), which uses a variant of this double learning
    technique.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 处理最大化偏差的一种方法是在两个 Q 函数中跟踪估计值。在每一步，我们选择其中一个来确定动作，以确定根据该 Q 函数哪个估计值是最高的。但是，然后我们使用另一个
    Q 函数来获取该动作的估计值。通过这样做，有更低的概率总是有一个正偏差误差。然后，为了选择与环境交互的动作，我们使用该状态的两个 Q 函数的平均值或总和。也就是说，例如，*Q*[1](*S*[*t+1*]*)+Q*[2]*(S*[*t+1*])
    的最大值。使用这两个 Q 函数的技术称为 *双学习*，实现此技术的算法称为 *双 Q-learning*。在接下来的几章中，你将了解一种名为 *双深度 Q
    网络*（DDQN）的深度强化学习算法，它使用这种双学习技术的变体。
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsFVMC, SARSA, Q-learning,
    and double Q-learning on the SWS environment |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Details.png) | 这就是细节 FVMC、SARSA、Q-learning 和双 Q-learning
    在 SWS 环境上的应用 |'
- en: '|  | Let’s put it all together and test all the algorithms we just learned
    about in the Slippery Walk Seven environment.So you’re aware, I used the same
    hyperparameters in *all algorithms*, the same gamma, alpha, epsilon, and respective
    decaying schedules. Remember, if you don’t decay alpha toward 0, the algorithm
    doesn’t fully converge. I’m decaying it to 0.01, which is good enough for this
    simple environment. Epsilon should also be decayed to zero for full convergence,
    but in practice this is rarely done. In fact, often state-of-the-art implementations
    don’t even decay epsilon and use a constant value instead. Here, we’re decaying
    to 0.1.![](../Images/06_10_Sidebar25_alpha-epsilon.png)Another thing: note that
    in these runs, I set the same number of episodes for all algorithms; they all
    run 3,000 episodes in the SWS environment. You’ll notice some algorithms don’t
    converge in this many steps, but that doesn’t mean they wouldn’t converge at all.
    Also, some of the other environments in the chapter’s Notebook, such as Frozen
    Lake, terminate on a set number of steps, that is, your agent has 100 steps to
    complete each episode, else it’s given a done flag. This is somewhat of an issue
    that we’ll address in later chapters. But, please, go to the Notebook and have
    fun! I think you’ll enjoy playing around in there. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | 让我们把这些都放在一起，并在滑溜的七环境（Slippery Walk Seven）中测试我们刚刚学习到的所有算法。所以，你要知道，我在所有算法中使用了相同的超参数，相同的伽马（gamma）、alpha、epsilon以及相应的衰减计划。记住，如果你不将alpha衰减到0，算法就不会完全收敛。我将它衰减到0.01，这对于这个简单环境来说已经足够好了。epsilon也应该衰减到零以实现完全收敛，但在实践中这很少发生。事实上，最先进的实现通常甚至不会衰减epsilon，而是使用一个常数值。在这里，我们将其衰减到0.1。[![alpha-epsilon](../Images/06_10_Sidebar25_alpha-epsilon.png)](../Images/06_10_Sidebar25_alpha-epsilon.png)另一件事：请注意，在这些运行中，我为所有算法设置了相同数量的剧集；它们都在SWS环境中运行了3,000个剧集。你会发现一些算法在这么多步骤中不会收敛，但这并不意味着它们根本不会收敛。此外，本章笔记本中的其他一些环境，如冰湖（Frozen
    Lake），在达到一定数量的步骤后终止，也就是说，你的智能体有100步来完成每个剧集，否则它会被赋予完成标志。这是一个我们将在后续章节中解决的问题。但请，去笔记本里玩玩！我想你会喜欢在那里玩来玩去。'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpSimilar trends among bootstrapping
    and on-policy methods |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ![计数](../Images/icons_Tally.png) | 计算自举和在线策略方法之间的相似趋势'
- en: '|  | ![](../Images/06_10_Sidebar26.png) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏26](../Images/06_10_Sidebar26.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpExamining the policies learned
    in the SWS environment |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| ![计数](../Images/icons_Tally.png) | 计算在SWS环境中学习到的策略'
- en: '|  | ![](../Images/06_10_Sidebar27.png) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏27](../Images/06_10_Sidebar27.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpExamining the value functions
    learned in the SWS environment |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ![计数](../Images/icons_Tally.png) | 计算在SWS环境中学习到的值函数'
- en: '|  | ![](../Images/06_10_Sidebar28.png) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏28](../Images/06_10_Sidebar28.png) |'
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you put everything you’ve learned so far into practice. We
    learned about algorithms that optimize policies through trial-and-error learning.
    These algorithms learn from feedback that’s simultaneously sequential and evaluative;
    that is, these agents learn to simultaneously balance immediate and long-term
    goals and the gathering and utilization of information. But unlike in the previous
    chapter, in which we restricted our agents to solving the prediction problem,
    in this chapter, our agents learned to solve the control problem.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将迄今为止所学的一切付诸实践。我们学习了通过试错学习优化策略的算法。这些算法从同时具有顺序性和评估性的反馈中学习；也就是说，这些智能体学习同时平衡短期和长期目标以及信息的收集和利用。但与上一章不同，在上一章中，我们将智能体限制在解决预测问题，而在本章中，我们的智能体学会了解决控制问题。
- en: There are many essential concepts you learned about in this chapter. You learned
    that the prediction problem consists of evaluation policies, while the control
    problem consists of optimizing policies. You learned that the solutions to the
    prediction problem are in policy evaluation methods, such as those learned about
    in the previous chapter. But unexpectedly, the control problem isn’t solved alone
    by policy-improvement methods you’ve learned in the past. Instead, to solve the
    control problem, we need to use policy-evaluation methods that can learn to estimate
    action-value functions merely from samples and policy-improvement methods that
    take into account the need for exploration.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中你学习了许多重要的概念。你了解到预测问题由评估策略组成，而控制问题由优化策略组成。你了解到预测问题的解决方案在策略评估方法中，例如前一章中提到的那些。但出乎意料的是，控制问题并不是仅通过你过去学过的策略改进方法来解决的。相反，为了解决控制问题，我们需要使用策略评估方法来学习仅从样本中估计动作价值函数，以及考虑探索需求的策略改进方法。
- en: The key takeaway from this chapter is the generalized policy-iteration pattern
    (GPI) which consists of the interaction between policy-evaluation and policy-improvement
    methods. While policy evaluation makes the value function consistent with the
    policy evaluated, policy improvement reverses this consistency but produces a
    better policy. GPI tells us that by having these two processes interact, we iteratively
    produce better and better policies until convergence to optimal policies and value
    functions. The theory of reinforcement learning supports this pattern and tells
    us that, indeed, we can find optimal policies and value functions in the discrete
    state and action spaces with only a few requirements. You learned that GLIE and
    stochastic approximation theories apply at different levels to RL algorithms.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键要点是广义策略迭代模式（GPI），它由策略评估和策略改进方法之间的交互组成。虽然策略评估使价值函数与评估的策略一致，但策略改进则反转这种一致性，但产生更好的策略。GPI告诉我们，通过这两个过程的交互，我们可以迭代地产生更好和更好的策略，直到收敛到最优策略和价值函数。强化学习理论支持这种模式，并告诉我们，确实，我们可以在离散的状态和动作空间中找到最优策略和价值函数，只需满足几个要求。你了解到GLIE和随机逼近理论在不同层面上应用于RL算法。
- en: You learned about many other things, from on-policy to off-policy methods, from
    online to offline, and more. Double Q-learning and double learning, in general,
    are essential techniques that we build on later. In the next chapter, we examine
    advanced methods for solving the control problem. As environments get challenging,
    we use other techniques to learn optimal policies. Next, we look at methods that
    are more effective in solving environments, and they do so more efficiently, too.
    That is, they solve these environments, and do so using fewer experience samples
    than methods we learned about in this chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解了许多其他事物，从在线策略到离线策略，以及更多。双Q学习和双学习通常是我们后来构建的关键技术。在下一章中，我们将探讨解决控制问题的先进方法。随着环境变得具有挑战性，我们使用其他技术来学习最优策略。接下来，我们将查看更有效地解决环境的更有效的方法。也就是说，它们解决了这些环境，并且使用比本章中我们学习的方法更少的经验样本。
- en: By now, you
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Know that most RL agents follow a pattern known as generalized policy iteration
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解大多数RL代理遵循一种称为广义策略迭代的模式
- en: Know that GPI solves the control problem with policy evaluation and improvement
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解GPI通过策略评估和改进来解决控制问题
- en: Learned about several agents that follow the GPI pattern to solve the control
    problem
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解了几种遵循GPI模式来解决控制问题的代理
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ![推文图标](../Images/icons_Tweet.png) | 推文功能：独自工作并分享你的发现'
- en: '|  | Here are several ideas on how to take what you have learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you’ll take advantage of it.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，如何将你所学的知识提升到更高层次。如果你愿意，可以与全世界分享你的成果，并确保查看其他人所做的工作。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch06_tf01:** All the algorithms presented in this chapter use two crucial
    variables: the learning rate, alpha, and the discount factor, gamma. I want you
    to do an analysis on these two variables. For instance, how do these variables
    interact? How do they affect the total reward obtained by the agent, and the policy
    success rate?'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch06_tf01:** 本章中提出的所有算法都使用两个关键变量：学习率，alpha，和折扣因子，gamma。我希望你对这两个变量进行分析。例如，这些变量是如何相互作用的？它们如何影响智能体获得的总奖励和策略成功率？'
- en: '**#gdrl_ch06_tf02:** Another thing to think about after this chapter is that
    we used the same exploration strategy for all of the methods: it was an exponentially
    decaying epsilon-greedy strategy. But, is this the best strategy? How would you
    use other strategies from chapter 4? How about creating an exploration strategy
    of your own and testing that out? How about changing the hyperparameters related
    to the exploration strategy and see how results change? That shouldn’t be hard
    at all to try. Head to the book’s Notebooks and start by changing severalof the
    hyperparameters: then change the exploration strategy altogether, and tell us
    what you find.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch06_tf02:** 在本章之后，还有一件事需要考虑的是，我们为所有方法使用了相同的探索策略：这是一个指数衰减的ε-greedy策略。但是，这是否是最好的策略？你将如何使用第4章中的其他策略？自己创建一个探索策略并测试它如何？改变与探索策略相关的超参数，看看结果如何变化？尝试这些应该不会太难。前往本书的Notebooks，首先改变几个超参数：然后完全改变探索策略，告诉我们你发现了什么。'
- en: '**#gdrl_ch06_tf03:** You’re probably guessing this right. The algorithms in
    this chapter are also not using the time step limit correctly. Make sure to investigate
    what I’m alluding to, and once you find out, change the algorithms to do this
    right. Do the results change at all? Do agents now do better than before? Are
    they better in terms of estimating the optimal value function, the optimal policy,
    or both? How much better? Make sure to investigate, or come back after chapter
    8\. Share your findings.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch06_tf03:** 你可能已经猜到了。本章中的算法也没有正确使用时间步限制。确保调查我所暗示的内容，一旦你发现了，就将算法更正为正确使用。结果是否有所改变？智能体现在是否比以前做得更好？它们在估计最优值函数、最优策略或两者方面是否更好？好多少？确保进行调查，或者在第8章之后回来。分享你的发现。'
- en: '**#gdrl_ch06_tf04:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch06_tf04:** 在每一章中，我都使用最后的标签作为通用的标签。请随意使用这个标签来讨论与本章相关的任何其他工作。没有什么比你自己创造的任务更令人兴奋的了。确保分享你设定要调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '用你的发现写一条推文，@mimoralea（我会转发），并使用列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一个包含学习深度强化学习资源的博客文章列表。查看它吧：<link>
    #gdrl_ch01_tf01”我会确保转发并帮助他人找到你的工作。|'
