- en: Chapter 7\. Interacting with External Data Sources
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章\. 与外部数据源交互
- en: In Spark, in order to run any algorithm you need to read input data from a data
    source, then apply your algorithm in the form of a set of PySpark transformations
    and actions (expressed as a DAG), and finally write your desired output to a target
    data source. So, to write algorithms that perform well, it’s important to understand
    reading and writing from and to external data sources.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，为了运行任何算法，您需要从数据源读取输入数据，然后将您的算法应用为一组 PySpark 转换和操作（表达为 DAG），最后将所需的输出写入目标数据源。因此，为了编写性能良好的算法，了解从外部数据源读取和写入至关重要。
- en: In the previous chapters, we have explored interacting with the built-in data
    sources (RDDs and DataFrames) in Spark. In this chapter, we will focus on how
    Spark interfaces with external data sources.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经探讨了如何与 Spark 中的内置数据源（RDD 和 DataFrame）进行交互。在本章中，我们将专注于 Spark 如何与外部数据源进行接口交互。
- en: As [Figure 7-1](#spark_external_data_sources) shows, Spark can read data from
    a huge range of external storage systems like the Linux filesystem, Amazon S3,
    HDFS, Hive tables, and relational databases (such as Oracle, MySQL, or PostgreSQL)
    through its data source interface. This chapter will show you how to read data
    in and then convert it into RDDs or DataFrames for further processing. I’ll also
    show you how Spark’s data can be written back to external storage systems like
    files, Amazon S3, and JDBC-compliant databases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 7-1](#spark_external_data_sources) 所示，Spark 可以从广泛的外部存储系统（如 Linux 文件系统、Amazon
    S3、HDFS、Hive 表以及关系数据库（如 Oracle、MySQL 或 PostgreSQL））中读取数据，通过其数据源接口。本章将向您展示如何读取数据，并将其转换为
    RDD 或 DataFrame 以进行进一步处理。我还将向您展示如何将 Spark 的数据写回到文件、Amazon S3 和 JDBC 兼容的数据库中。
- en: '![daws 0701](Images/daws_0701.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0701](Images/daws_0701.png)'
- en: Figure 7-1\. Spark external data sources
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. Spark 外部数据源
- en: Relational Databases
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关系数据库
- en: Let’s start with relational databases. A relational database is a collection
    of data items organized as a set of formally described tables (created using the
    SQL `CREATE` `TABLE` statement) from which data can be accessed or reassembled
    in many different ways without the tables themselves needing to be reorganized.
    Open source relational databases (such as MySQL and PostgreSQL) are currently
    the predominant choice for storing data like social media network records, financial
    records, medical records, personal information, and manufacturing data. There
    are also many well-known and widely used licensed proprietary relational databases,
    such as MS SQL Server and Oracle.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从关系数据库开始。关系数据库是一组数据项，以一组形式描述的表格组织起来（使用 SQL `CREATE` `TABLE` 语句创建），可以在不需要重新组织表格本身的情况下，以许多不同的方式访问或重新组装数据。开源关系数据库（如
    MySQL 和 PostgreSQL）目前是存储社交媒体网络记录、财务记录、医疗记录、个人信息和制造数据的主要选择。还有许多著名且广泛使用的许可专有关系数据库，如
    MS SQL Server 和 Oracle。
- en: Informally, a relational database table has a set of rows and named columns,
    as shown in [Figure 7-2](#relational_databases). Each row in a table can have
    its own unique key (called a primary key). Rows in a table can be linked to rows
    in other tables by adding a column for the unique key of the linked row (such
    columns are known as foreign keys).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关系数据库表格非正式地具有一组行和命名列，如 [图 7-2](#relational_databases) 所示。表格中的每一行可以有其自己的唯一键（称为主键）。表格中的行可以通过添加指向其他表格行的唯一键的列（这些列称为外键）来链接到其他表格的行。
- en: '![daws 0702](Images/daws_0702.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0702](Images/daws_0702.png)'
- en: Figure 7-2\. A relational database table example
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 关系数据库表格示例
- en: 'PySpark provides two classes for reading data from and writing data to relational
    databases, as well as to other external data sources. These two classes are defined
    as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 提供了两个类，用于从关系数据库读取数据和将数据写入关系数据库，以及其他外部数据源。这两个类定义如下：
- en: '`class` `pyspark.sql.DataFrameReader(spark)`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`class` `pyspark.sql.DataFrameReader(spark)`'
- en: This is the interface used to read data into a DataFrame from an external storage
    system (filesystem, key/value store, etc.). Use `spark.read()` to access this.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从外部存储系统（文件系统、键值存储等）读取数据到 DataFrame 的接口。使用 `spark.read()` 来访问此功能。
- en: '`class` `pyspark.sql.DataFrameWriter(df)`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`class` `pyspark.sql.DataFrameWriter(df)`'
- en: This is the interface used to write a DataFrame to an external storage system.
    Use `DataFrame.write()` to access this.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将 DataFrame 写入外部存储系统的接口。使用 `DataFrame.write()` 来访问此功能。
- en: Reading from a Database
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据库中读取数据
- en: 'PySpark enables us to read in data from a relational database table and create
    a new DataFrame from it. You can read a table from any JDBC-compliant database,
    using the `pyspark.sql.DataFrameReader.load()` Python method. The `load()` method
    is defined as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark使我们能够从关系数据库表中读取数据，并创建一个新的DataFrame。您可以使用`pyspark.sql.DataFrameReader.load()`
    Python方法从任何符合JDBC的数据库中读取表。`load()`方法定义如下：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In order to read in data from a JDBC-compliant database table, you need to specify
    `format("jdbc")`. You can then pass in table attributes and connection parameters
    (such as the JDBC URL and your database credentials) as `options(*<key>*, *<value>*)`
    pairs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要从符合JDBC的数据库表中读取数据，需要指定`format("jdbc")`。然后可以将表属性和连接参数（例如JDBC URL和数据库凭据）作为`options(*<key>*,
    *<value>*)`对传入。
- en: To read data from and write it to a JDBC-compliant relational database, you
    will need access to the database server and sufficient privileges.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取并写入符合JDBC的关系数据库中的数据，您需要访问数据库服务器并具有足够的权限。
- en: Step 1\. Create a database table
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤1\. 创建数据库表
- en: 'In this step, we’ll connect to a MySQL database server and create a table called
    `dept` with seven rows. We execute the `mysql` client program to enter into the
    MySQL client shell (if you have installed a MySQL database on your MacBook, for
    example, the MySQL client will be available at */usr/local/bin/mysql*):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们将连接到MySQL数据库服务器，并创建一个名为`dept`的表，包含七行数据。我们执行`mysql`客户端程序来进入MySQL客户端shell（例如，如果你在MacBook上安装了MySQL数据库，则MySQL客户端将位于*/usr/local/bin/mysql*）：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO1-1)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO1-1)'
- en: Invoke the MySQL shell client.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 调用MySQL shell客户端。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO1-2)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO1-2)'
- en: Enter a valid password for the `root` user.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输入`root`用户的有效密码。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO1-3)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO1-3)'
- en: List the databases available in the MySQL database server.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列出MySQL数据库服务器中可用的数据库。
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO1-4)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO1-4)'
- en: These three databases are created by the MySQL database server.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个数据库是由MySQL数据库服务器创建的。
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO1-5)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO1-5)'
- en: The `mysql` database manages users, groups, and privileges.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`mysql`数据库管理用户、组和权限。'
- en: 'Next, we’ll create and select a database:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建并选择一个数据库：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO2-1)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO2-1)'
- en: Create a new database called `metadb`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`metadb`的新数据库。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO2-2)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO2-2)'
- en: Make `metadb` your current default database.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将`metadb`设置为当前默认数据库。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO2-3)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO2-3)'
- en: Show the tables in the `metadb` database (since it is a new database, there
    will be no tables in it).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 显示`metadb`数据库中的表（因为它是一个新数据库，所以其中将没有任何表）。
- en: 'Then we’ll create a new table called `dept` inside the `metadb` database:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将在`metadb`数据库内创建一个名为`dept`的新表：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO3-1)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO3-1)'
- en: This is the table definition for `dept`, which has four columns.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`dept`表的表定义，它包含四列。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO3-2)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO3-2)'
- en: List the tables in the `metadb` database.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列出`metadb`数据库中的表。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO3-3)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO3-3)'
- en: Describe the schema for `dept` table.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 描述`dept`表的模式。
- en: 'Finally, we insert the following seven rows into the `dept` table, using the
    `INSERT` statement:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`INSERT`语句将以下七行数据插入到`dept`表中：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can examine the contents of the `dept` table to make sure that it has these
    seven rows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查`dept`表的内容，确保它包含这七行数据：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: At this point, we are sure that there is a `metadb` database on the database
    server, which has a `dept` table with seven records.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们确信数据库服务器上存在一个名为`metadb`的数据库，其中包含一个具有七条记录的`dept`表。
- en: 'Step 2: Read the database table into a DataFrame'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤2：将数据库表读入DataFrame中
- en: 'Once you have a JDBC-compliant table (such as `dept`), then you can use the
    `pyspark.sql.DataFrameReader` class’s methods (a combination of `option()` and
    `load()`) to read the contents of the table and create a new DataFrame. To perform
    this read, you need a JAR file, which is a MySQL JDBC driver (you may download
    this JAR file from [the MySQL website](https://dev.mysql.com/downloads)). You
    can put the JAR file containing the MySQL driver class wherever you like; I’ll
    place it in:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有一个 JDBC 兼容的表（如 `dept`），那么您可以使用 `pyspark.sql.DataFrameReader` 类的方法（`option()`
    和 `load()` 的结合）来读取表的内容并创建一个新的 DataFrame。要执行此读取操作，您需要一个 JAR 文件，这是一个 MySQL JDBC
    驱动程序（您可以从 [MySQL 网站](https://dev.mysql.com/downloads) 下载此 JAR 文件）。您可以将包含 MySQL
    驱动程序类的 JAR 文件放在任何您喜欢的位置；我会把它放在：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: MySQL offers standard database driver connectivity (see [Connector/J](https://oreil.ly/WwK6k)
    for details) for using MySQL with applications and tools that are compatible with
    industry standards ODBC and JDBC. Any system that works with ODBC or JDBC can
    use MySQL.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL 提供标准的数据库驱动程序连接（参见 [Connector/J](https://oreil.ly/WwK6k) 了解详情），用于与符合行业标准
    ODBC 和 JDBC 的应用程序和工具一起使用 MySQL。任何支持 ODBC 或 JDBC 的系统都可以使用 MySQL。
- en: 'Next, we enter the PySpark shell by passing the JAR file to the `$SPARK_HOME/bin/pyspark`
    program:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过将 JAR 文件传递给 `$SPARK_HOME/bin/pyspark` 程序来进入 PySpark shell：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO4-1)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO4-1)'
- en: This is the driver class JAR for MySQL.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 MySQL 的驱动程序类 JAR。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO4-2)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO4-2)'
- en: Start the PySpark shell, loading the MySQL driver class JAR.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 PySpark shell，加载 MySQL 驱动程序类 JAR。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO4-3)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO4-3)'
- en: Make sure the `SparkSession` is available.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 确保 `SparkSession` 可用。
- en: 'Now we can use the `SparkSession` to read a relational table and create a new
    DataFrame:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 `SparkSession` 读取一个关系表并创建一个新的 DataFrame：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO5-1)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO5-1)'
- en: '`spark` is an instance of `SparkSession`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark` 是 `SparkSession` 的一个实例。'
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO5-2)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO5-2)'
- en: Returns a `DataFrameReader` that can be used to read data in as a DataFrame
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个可以用来读取数据作为 DataFrame 的 `DataFrameReader`
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO5-3)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO5-3)'
- en: Indicates that you are reading JDBC-compliant data
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表明您正在读取 JDBC 兼容数据
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO5-4)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO5-4)'
- en: The database URL
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库 URL
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO5-5)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO5-5)'
- en: The JDBC driver (loaded from the JAR file)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC 驱动程序（从 JAR 文件加载）
- en: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO5-6)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO5-6)'
- en: The database table name
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库表名
- en: '[![7](Images/7.png)](#co_interacting_with_external_data_sources_CO5-7)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_interacting_with_external_data_sources_CO5-7)'
- en: The database username
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库用户名
- en: '[![8](Images/8.png)](#co_interacting_with_external_data_sources_CO5-8)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_interacting_with_external_data_sources_CO5-8)'
- en: The database password
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库密码
- en: '[![9](Images/9.png)](#co_interacting_with_external_data_sources_CO5-9)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](Images/9.png)](#co_interacting_with_external_data_sources_CO5-9)'
- en: Loads data from a JDBC data source and returns it as a DataFrame
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从 JDBC 数据源加载数据并将其作为 DataFrame 返回
- en: 'Let’s take a look at the newly created DataFrame:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看新创建的 DataFrame：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO6-1)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO6-1)'
- en: Count the number of rows in the DataFrame.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 DataFrame 中行数。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO6-2)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO6-2)'
- en: Print the first 20 rows to the console.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 打印前 20 行到控制台。
- en: 'We can also examine its schema:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看其模式：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO7-1)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO7-1)'
- en: Print out the schema in tree format.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以树状格式打印模式。
- en: 'Step 3: Query the DataFrame'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 3：查询 DataFrame
- en: PySpark offers many ways to access a DataFrame. In addition to various SQL-like
    methods (such as `select(*<columns>*)`, `groupBy(*<columns>*)`, `min()`, `max()`,
    etc.), it allows you to execute fully fledged SQL queries on your DataFrame by
    first registering it as a “table” and then issuing queries against that registered
    table. We will discuss DataFrame table registration shortly. First, we will execute
    some SQL-like queries using DataFrame methods.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 提供了许多访问 DataFrame 的方式。除了各种类似 SQL 的方法（如 `select(*<columns>*)`、`groupBy(*<columns>*)`、`min()`、`max()`
    等），它还允许您通过首先将其注册为“表”，然后针对该注册表执行完全成熟的 SQL 查询来执行对 DataFrame 的查询。我们将很快讨论 DataFrame
    表注册。首先，我们将使用 DataFrame 方法执行一些类似 SQL 的查询。
- en: 'Here, we select all rows for two columns, `dept_number` and `manager`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择所有行的两列，`dept_number` 和 `manager`：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO8-1)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO8-1)'
- en: Select the `dept_number` and `manager` columns from the DataFrame.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DataFrame 中选择 `dept_number` 和 `manager` 列。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO8-2)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO8-2)'
- en: Display the selection result.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 显示选择结果。
- en: 'Next, we group all rows by `manager` and then find the minimum `dept_number`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们按 `manager` 列对所有行进行分组，然后找到最小的 `dept_number`：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here we group all rows by `manager` and then find the frequencies of the grouped
    data:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们按 `manager` 列对所有行进行分组，然后找到分组数据的频率：
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And here we do the same but additionally order the output by the `manager`
    column:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们做同样的事情，但额外按 `manager` 列排序输出：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To execute fully fledged SQL queries against a DataFrame, first you have to
    register your DataFrame as a table:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要对 DataFrame 执行完全成熟的 SQL 查询，首先必须将 DataFrame 注册为表：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can then execute regular SQL queries on it, as if it were a relational
    database table:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您随后可以对其执行常规的 SQL 查询，就像它是一个关系数据库表一样：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO9-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO9-1)'
- en: Register this DataFrame as a temporary table using the given name.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定的名称将此 DataFrame 注册为临时表。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO9-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO9-2)'
- en: You can now issue a SQL query against your registered table.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以针对您注册的表执行 SQL 查询了。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO9-3)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO9-3)'
- en: This prints the first 20 rows to the console.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将前 20 行打印到控制台。
- en: 'This query uses the “like” pattern matching for the `dept_location` column:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询使用 `dept_location` 列的“like”模式匹配：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And here we use `GROUP` `BY`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `GROUP` `BY`：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Writing a DataFrame to a Database
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 DataFrame 写入数据库
- en: We can write or save a Spark DataFrame to an external data source, such as a
    relational database table, using the `DataFrameWriter.save()` method. Let’s walk
    through an example.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `DataFrameWriter.save()` 方法将 Spark DataFrame 写入或保存到外部数据源，比如关系数据库表。让我们通过一个示例来详细说明。
- en: 'First, we’ll create a list of triplets `(<name>, <age>, <salary>)` as a local
    Python collection:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个三元组列表 `(<name>, <age>, <salary>)` 作为本地 Python 集合：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then we’ll convert this into a Spark DataFrame with the `SparkSession.createDataFrame()`
    method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用 `SparkSession.createDataFrame()` 方法将其转换为 Spark DataFrame：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO10-1)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO10-1)'
- en: Create a new DataFrame.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的 DataFrame。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO10-2)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO10-2)'
- en: Convert triplets into a DataFrame.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 将三元组转换为 DataFrame。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO10-3)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO10-3)'
- en: Impose a schema on the created DataFrame.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对创建的 DataFrame 强制执行模式。
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO10-4)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO10-4)'
- en: Display the contents of the newly created DataFrame.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新创建的 DataFrame 的内容。
- en: 'Now, we can convert the DataFrame into a relational table called `triplets`:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将 DataFrame 转换为名为 `triplets` 的关系表：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO11-1)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO11-1)'
- en: Returns a `DataFrameWriter` that can be used to write to an external device
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个可以用于向外部设备写入的 `DataFrameWriter`
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO11-2)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO11-2)'
- en: Indicates that you are writing to a JDBC-compliant database
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表明您正在向兼容 JDBC 的数据库写入
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO11-3)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO11-3)'
- en: The JDBC driver (loaded from the JAR file)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC 驱动程序（从 JAR 文件加载）
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO11-4)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO11-4)'
- en: Overwrites the table if it already exists
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果表已经存在，则覆盖该表
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO11-5)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO11-5)'
- en: The database URL
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库 URL
- en: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO11-6)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO11-6)'
- en: The target database table name
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 目标数据库表名称
- en: '[![7](Images/7.png)](#co_interacting_with_external_data_sources_CO11-7)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_interacting_with_external_data_sources_CO11-7)'
- en: The database username
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库用户名
- en: '[![8](Images/8.png)](#co_interacting_with_external_data_sources_CO11-8)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_interacting_with_external_data_sources_CO11-8)'
- en: The database password
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库密码
- en: '[![9](Images/9.png)](#co_interacting_with_external_data_sources_CO11-9)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](Images/9.png)](#co_interacting_with_external_data_sources_CO11-9)'
- en: Saves the DataFrame data as a database table
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DataFrame 数据保存为数据库表
- en: 'When writing the contents of a DataFrame to an external device, you can choose
    desired mode. The Spark JDBC writer supports the following modes:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当将 DataFrame 的内容写入外部设备时，可以选择所需的模式。Spark JDBC 写入器支持以下模式：
- en: '`append`'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`append`'
- en: Append the contents of this DataFrame to any existing data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将 DataFrame 的内容附加到任何现有数据。
- en: '`overwrite`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`overwrite`'
- en: Overwrite any existing data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖任何现有数据。
- en: '`ignore`'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`ignore`'
- en: Silently ignore this operation if data already exists.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据已经存在，则静默地忽略此操作。
- en: '`error` (default case)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`error`（默认情况）'
- en: Throw an exception if data already exists.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据已经存在，则抛出异常。
- en: 'Here, we verify that the `triplets` table was created in the `metadb` database
    on the MySQL database server under the `''metadb''` database:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们验证 `triplets` 表已在 MySQL 数据库服务器上的 `'metadb'` 数据库中创建：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](Images/1.png)](ch02.xhtml#comarker1)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](ch02.xhtml#comarker1)'
- en: Start the MySQL client shell.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 MySQL 客户端 shell。
- en: '[![2](Images/2.png)](ch02.xhtml#comarker2)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](ch02.xhtml#comarker2)'
- en: Enter the password for the `root` user.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为 `root` 用户输入密码。
- en: '[![3](Images/3.png)](ch02.xhtml#comarker3)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](ch02.xhtml#comarker3)'
- en: Select the desired database.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 选择所需的数据库。
- en: '[![4](Images/4.png)](ch02.xhtml#comarker4)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](ch02.xhtml#comarker4)'
- en: Make sure that the `triplets` table was created.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 确保 `triplets` 表已创建。
- en: '[![5](Images/5.png)](ch02.xhtml#comarker5)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](ch02.xhtml#comarker5)'
- en: Display the content of the `triplets` tables.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 `triplets` 表的内容。
- en: 'Next, we read the `triplets` table back from the MySQL relational database
    to make sure that the table is readable:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从 MySQL 关系数据库中重新读取 `triplets` 表，以确保该表可读：
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO12-1)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO12-1)'
- en: An instance of `SparkSession`
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` 的一个实例'
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO12-2)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO12-2)'
- en: Returns a `DataFrameReader` that can be used to read data in as a DataFrame
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个 `DataFrameReader`，可用于将数据读取为 DataFrame
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO12-3)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO12-3)'
- en: Indicates that you are reading JDBC-compliant data
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表示正在读取符合 JDBC 的数据
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO12-4)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO12-4)'
- en: The database URL
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库 URL
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO12-5)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO12-5)'
- en: The JDBC driver (loaded from the JAR file)
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC 驱动程序（从 JAR 文件加载）
- en: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO12-6)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO12-6)'
- en: The database table to be read
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取的数据库表
- en: '[![7](Images/7.png)](#co_interacting_with_external_data_sources_CO12-7)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_interacting_with_external_data_sources_CO12-7)'
- en: The database username
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库用户名
- en: '[![8](Images/8.png)](#co_interacting_with_external_data_sources_CO12-8)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[![8](Images/8.png)](#co_interacting_with_external_data_sources_CO12-8)'
- en: The database password
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库密码
- en: '[![9](Images/9.png)](#co_interacting_with_external_data_sources_CO12-9)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[![9](Images/9.png)](#co_interacting_with_external_data_sources_CO12-9)'
- en: Loads data from a JDBC data source and returns it as a DataFrame
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从 JDBC 数据源加载数据并将其返回为 DataFrame
- en: '[![10](Images/10.png)](#co_interacting_with_external_data_sources_CO12-10)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![10](Images/10.png)](#co_interacting_with_external_data_sources_CO12-10)'
- en: Shows the contents of the newly created DataFrame
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新创建 DataFrame 的内容
- en: Finally, we’ll execute some SQL queries on the newly created DataFrame.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在新创建的 DataFrame 上执行一些 SQL 查询。
- en: 'The following query finds the minimum and maximum of the `salary` column:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的查询找到 `salary` 列的最小值和最大值：
- en: '[PRE27]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO13-1)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO13-1)'
- en: Register this DataFrame as a temporary table using the name `mytriplets`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用名为 `mytriplets` 的临时表名注册此 DataFrame。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO13-2)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO13-2)'
- en: Execute the SQL statement and create a new DataFrame.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 SQL 语句并创建新的 DataFrame。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO13-3)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO13-3)'
- en: Display the result of the SQL statement.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 SQL 语句的结果。
- en: 'Here, we aggregate the `age` column by using SQL’s `GROUP` `BY`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过使用 SQL 的 `GROUP BY` 对 `age` 列进行聚合：
- en: '[PRE28]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we sort the result of the previous SQL query:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对前面 SQL 查询的结果进行排序：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Reading Text Files
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取文本文件
- en: 'Spark allows us to read text files and create DataFrames from them. Consider
    the following text file:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 允许我们读取文本文件并从中创建 DataFrame。考虑以下文本文件：
- en: '[PRE30]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s first create an `RDD[Row]` (where each element is a `Row` object):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个 `RDD[Row]`（其中每个元素是一个 `Row` 对象）：
- en: '[PRE31]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO14-1)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO14-1)'
- en: '`records` is an `RDD[String]`.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`records` 是一个 `RDD[String]`。'
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO14-2)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO14-2)'
- en: '`people` is an `RDD[Row]`.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`people` 是一个 `RDD[Row]`。'
- en: 'Now that we have `people` as an `RDD[Row]`, it is straightforward to create
    a DataFrame:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 `people` 作为 `RDD[Row]`，可以轻松地创建一个 DataFrame：
- en: '[PRE32]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO15-1)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO15-1)'
- en: '`people_df` is a `DataFrame[Row]`.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`people_df` 是一个 `DataFrame[Row]`。'
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO15-2)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO15-2)'
- en: Display the schema of the created DataFrame.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 显示创建的 DataFrame 的模式。
- en: 'Next, we’ll use a SQL query to manipulate the created DataFrame:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 SQL 查询来操作创建的 DataFrame：
- en: '[PRE33]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO16-1)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO16-1)'
- en: Register the `people_df` DataFrame as a temporary table using the name `people_table`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `people_df` DataFrame 注册为临时表，表名为 `people_table`。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO16-2)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO16-2)'
- en: '`spark.sql(*sql-query*)` creates a new DataFrame.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.sql(*sql-query*)` 创建一个新的 DataFrame。'
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO16-3)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO16-3)'
- en: '`spark.sql(*sql-query*)` creates a new DataFrame.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.sql(*sql-query*)` 创建一个新的 DataFrame。'
- en: We can save our DataFrame as a text file with `DataFrame.write()`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `DataFrame.write()` 将 DataFrame 保存为文本文件。
- en: Reading and Writing CSV Files
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和写入 CSV 文件
- en: 'A comma-separated values file is a text file that allows data to be saved in
    a table-structured format. The following is a simple example of a CSV file with
    a header row (metadata containing the names of the columns, separated by commas),
    called *cats.with.header.csv*:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 逗号分隔值文件是一种文本文件，允许以表格结构的格式保存数据。以下是一个带有标题行（包含列名的元数据，用逗号分隔）的简单 CSV 文件示例，名为 *cats.with.header.csv*：
- en: '[PRE34]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO17-1)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO17-1)'
- en: Header record starts with `#`, describes columns
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 头记录以 `#` 开头，描述列
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO17-2)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO17-2)'
- en: First record
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条记录
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO17-3)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO17-3)'
- en: Second record
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 第二条记录
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO17-4)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO17-4)'
- en: Third and final record
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第三和最后一条记录
- en: 'The following is a simple example of a CSV file without a header, called *cats.no.header.csv*:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个不带标题的简单 CSV 文件示例，名为 *cats.no.header.csv*：
- en: '[PRE35]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the next section, we’ll use these two files to demonstrate how Spark reads
    CSV files.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用这两个文件演示 Spark 如何读取 CSV 文件。
- en: Reading CSV Files
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取 CSV 文件
- en: Spark offers many methods to load CSV files into a DataFrame. I’ll show you
    a few of them here.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了许多方法将 CSV 文件加载到 DataFrame 中。这里我会展示其中一些方法。
- en: 'In this example, using the PySpark shell, we read a CSV file with a header
    and load it as a DataFrame:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，使用 PySpark shell，我们读取了一个带有标题的 CSV 文件，并将其加载为 DataFrame：
- en: '[PRE36]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO18-1)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO18-1)'
- en: Create a new `DataFrame` as `cats` using a `SparkSession`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `SparkSession` 创建一个名为 `cats` 的新 `DataFrame`。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO18-2)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO18-2)'
- en: Return a `DataFrameReader` that can be used to read data in as a DataFrame.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个可以用于将数据读取为DataFrame的`DataFrameReader`。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO18-3)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO18-3)'
- en: Specify that the input data source format is CSV.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 指定输入数据源格式为CSV。
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO18-4)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO18-4)'
- en: Indicate that the input CSV file has a header (note that the header is not part
    of the actual data).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 指示输入的CSV文件包含标题（注意标题不是实际数据的一部分）。
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO18-5)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO18-5)'
- en: Infer the DataFrame schema from the input file.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入文件推断DataFrame的模式。
- en: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO18-6)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO18-6)'
- en: Provide the path for the CSV file.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 提供CSV文件的路径。
- en: 'We can now display the contents of the newly created DataFrame and its inferred
    schema:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以显示新创建的DataFrame的内容和推断出的模式：
- en: '[PRE37]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO19-1)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO19-1)'
- en: Display the contents of the DataFrame.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 显示DataFrame的内容。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO19-2)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO19-2)'
- en: Display the schema of the DataFrame.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 显示DataFrame的模式。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO19-3)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO19-3)'
- en: Display the size of the DataFrame.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 显示DataFrame的大小。
- en: 'Now I’ll show you how to read a CSV file without a header and create a new
    DataFrame from it:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将向您展示如何读取一个没有标题的CSV文件，并从中创建一个新的DataFrame：
- en: '[PRE38]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO20-1)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO20-1)'
- en: Create a new DataFrame as `cats` using a `SparkSession`.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`SparkSession`创建一个名为`cats`的新DataFrame。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO20-2)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO20-2)'
- en: Return a `DataFrameReader` that can be used to read data in as a DataFrame.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个可以用于将数据读取为DataFrame的`DataFrameReader`。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO20-3)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO20-3)'
- en: Specify that the input data source format is CSV.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 指定输入数据源格式为CSV。
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO20-4)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO20-4)'
- en: Indicate that the input CSV file has no header.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 指示输入的CSV文件没有标题。
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO20-5)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO20-5)'
- en: Infer the DataFrame schema from the input file.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入文件推断DataFrame的模式。
- en: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO20-6)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO20-6)'
- en: Provide the path for the CSV file.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 提供CSV文件的路径。
- en: 'Let’s inspect the contents of the newly created DataFrame and its inferred
    schema:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查新创建的DataFrame及其推断出的模式的内容：
- en: '[PRE39]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO21-1)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO21-1)'
- en: Default column names
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 默认列名
- en: 'Next, we’ll define a schema with four columns:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个包含四列的模式：
- en: '[PRE40]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Here’s what happens if we use the defined schema to read in the same CSV file
    and create a DataFrame from it:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用定义的模式读取相同的CSV文件并从中创建一个DataFrame，会发生以下情况：
- en: '[PRE41]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO22-1)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO22-1)'
- en: Explicit column names
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 使用显式列名
- en: 'We can apply this predefined schema to any headerless CSV file:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个预定义的模式应用于任何没有标题的CSV文件：
- en: '[PRE42]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO23-1)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO23-1)'
- en: Return a `DataFrameReader` that can be used to read data in as a DataFrame.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个可以用于将数据读取为DataFrame的`DataFrameReader`。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO23-2)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO23-2)'
- en: Read a CSV file.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 读取一个CSV文件。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO23-3)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO23-3)'
- en: Use the given schema for the CSV file.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 使用给定的模式来读取CSV文件。
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO23-4)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO23-4)'
- en: Indicate that the CSV file has no header.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 指示CSV文件没有标题。
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO23-5)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO23-5)'
- en: Explicit column names are used.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用显式列名。
- en: Writing CSV Files
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入CSV文件
- en: 'There are several ways that you can create CSV files from DataFrames in Spark.
    The easiest option is to use the `.csv()` method of the `DataFrameWriter` class,
    accessed through `DataFrame.write()`. This method is defined as follows (note
    that this is just a small subset of the available parameters):'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以在Spark中从DataFrames创建CSV文件。最简单的选项是使用`DataFrameWriter`类的`.csv()`方法，通过`DataFrame.write()`访问。该方法定义如下（请注意，这只是可用参数的一个小子集）：
- en: '[PRE43]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s use this method to save our `cats4` DataFrame as a CSV file:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这种方法将我们的`cats4` DataFrame保存为CSV文件：
- en: '[PRE45]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then examine the saved file(s):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然后检查保存的文件：
- en: '[PRE46]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Note that in the output of the `ls` (list) command, we see two types of files:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`ls`（列出）命令的输出中，我们看到两种类型的文件：
- en: A zero-sized *SUCCESS* file, which indicates that the `write` operation was
    successful.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小为零的*SUCCESS*文件，表示`write`操作成功。
- en: One or more files whose names begin with *part-*, which represent the output
    from a single partition.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个以*part-*开头的文件，这些文件代表单个分区的输出。
- en: Note also that there’s no header data in the saved file.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意保存的文件中没有标题数据。
- en: 'Let’s try that again, this time specifying that we want a header row:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次，这次指定我们想要一个标题行：
- en: '[PRE47]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO24-1)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO24-1)'
- en: The header from our DataFrame
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们DataFrame的标题
- en: Reading and Writing JSON Files
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和写入JSON文件
- en: 'JavaScript Object Notation is a lightweight, text-based data interchange format
    that is easy for humans to read and write. A JSON object is composed of a set
    of (key, value) pairs enclosed in curly braces, like this:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript对象表示法是一种轻量级的基于文本的数据交换格式，易于人类阅读和编写。JSON对象由一组（键，值）对组成，用花括号括起来，像这样：
- en: '[PRE49]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO25-1)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO25-1)'
- en: A simple (key, value) pair
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的（键，值）对
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO25-2)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO25-2)'
- en: An array value
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数组值
- en: Reading JSON Files
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取JSON文件
- en: 'JSON data can read with the `DataFrameReader.json()` method, which can take
    a set of parameters such as the path and schema. Consider the following JSON file:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: JSON数据可以使用`DataFrameReader.json()`方法读取，该方法可以接受一组参数，如路径和模式。考虑以下JSON文件：
- en: '[PRE50]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can read this file and convert it to a DataFrame as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式读取此文件并将其转换为DataFrame：
- en: '[PRE51]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You can also use the `load()` method and pass it one or more JSON files:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`load()`方法，并将其传递给一个或多个JSON文件：
- en: '[PRE52]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO26-1)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO26-1)'
- en: Note that `data_path` is loaded twice.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`data_path`加载了两次。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO26-2)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO26-2)'
- en: The file’s contents are therefore included twice in the resulting DataFrame.
    You can also use this method to create a DataFrame from several input files.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 结果DataFrame中因此包含文件内容两次。您还可以使用此方法从多个输入文件创建DataFrame。
- en: Writing JSON Files
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写JSON文件
- en: 'To write a DataFrame as a `json` object, we can use `DataFrameWriter.json()`
    method. The method accepts a set of parameters and saves the contents of the DataFrame
    in JSON format:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要将DataFrame写入`json`对象，我们可以使用`DataFrameWriter.json()`方法。该方法接受一组参数，并将DataFrame的内容保存为JSON格式：
- en: '[PRE53]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Let’s first create a DataFrame:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个DataFrame：
- en: '[PRE54]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, write it to an output path as JSON:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将其作为JSON写入输出路径：
- en: '[PRE55]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Note that we have eight filenames that begin with *part-*: this means that
    our DataFrame was represented by eight partitions.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有八个以*part-*开头的文件名：这意味着我们的DataFrame由八个分区表示。
- en: 'Let’s take a look at these files:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些文件：
- en: '[PRE57]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'If you want to create a single file output, then you may put your `DataFrame`
    into a single partition before writing it out:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要创建单个文件输出，则可以在将其写出之前将DataFrame放入单个分区：
- en: '[PRE58]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO27-1)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO27-1)'
- en: '`repartition(numPartitions)` returns a new DataFrame partitioned by the given
    partitioning expressions; see Chapter 5 for details.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '`repartition(numPartitions)`返回一个由给定分区表达式分区的新DataFrame；有关详细信息，请参见第5章。'
- en: Reading from and Writing to Amazon S3
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Amazon S3读取和写入
- en: 'Amazon Simple Storage Service (S3) is a service offered by Amazon Web Services
    (AWS) that provides object storage through a web services interface. S3 objects
    are treated as web objects—that is, they are accessed via internet protocols using
    a URL identifier. Every S3 object has a unique URL, in this format:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Simple Storage Service（S3）是由Amazon Web Services（AWS）提供的一项服务，通过Web服务接口提供对象存储。S3对象被视为Web对象，即通过Internet协议使用URL标识符访问。每个S3对象都有一个唯一的URL，格式如下：
- en: '[PRE59]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'For example:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE60]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'where: `project-dev` is the bucket name and `dna/sample123.vcf` is a key.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：`project-dev`是存储桶名称，`dna/sample123.vcf`是一个键。
- en: 'S3 objects can also be accessed through the following URI schemas:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: S3对象也可以通过以下URI模式访问：
- en: '`s3n`'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '`s3n`'
- en: Uses the S3 Native FileSystem, a native filesystem for reading and writing regular
    files on S3.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 使用S3本地文件系统，这是用于在S3上读写常规文件的本地文件系统。
- en: '`s3a`'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '`s3a`'
- en: Uses the S3A Filesystem, a successor to the native filesystem. Designed to be
    a switch-in replacement for `s3n`, this filesystem binding supports larger files
    and promises higher performance.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 使用S3A文件系统，这是原生文件系统的继任者。设计为`s3n`的替代品，此文件系统绑定支持更大的文件并承诺更高的性能。
- en: '`s3`'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '`s3`'
- en: Uses the S3 Block FileSystem, a block-based filesystem backed by S3. Files are
    stored as blocks, just like they are in HDFS.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 使用S3块文件系统，这是一个基于块的文件系统，由S3支持。文件以块的形式存储，就像在HDFS中一样。
- en: The difference between `s3` and `s3n`/`s3a` is that `s3` is a block-based overlay
    on top of Amazon S3, while `s3n`/`s3a` are not (they are object-based). The difference
    between `s3n` and `s3a` is that `s3n` supports objects up to 5 GB in size, while
    `s3a` supports objects up to 5 TB in size and has better performance (both features
    are because it uses multi-part upload).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`s3`和`s3n`/`s3a`的区别在于，`s3`是Amazon S3上的块级覆盖层，而`s3n`/`s3a`不是（它们是基于对象的）。`s3n`和`s3a`的区别在于，`s3n`支持最大5GB的对象大小，而`s3a`支持最大5TB的对象大小并且具有更好的性能（这两个特性是因为它使用了多部分上传）。'
- en: 'For example, using the `s3` URI schema, we can access the *sample72.vcf* file
    as:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用`s3` URI模式，我们可以访问*sample72.vcf*文件如下：
- en: '[PRE61]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In general, to access any services from AWS, you have to be authenticated.
    There are many ways to do this. One method is to export your access key and secret
    key from the command line:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，要访问AWS的任何服务，您必须经过身份验证。有很多方法可以做到这一点。一种方法是从命令行导出您的访问密钥和秘密密钥：
- en: '[PRE62]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Another option is to set your credentials using the `SparkContext` object:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是使用`SparkContext`对象来设置您的凭据：
- en: '[PRE63]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Reading from Amazon S3
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Amazon S3读取
- en: You’ll need to use the `s3`, `s3n`, or `s3a` (for bigger S3 objects) URI schema
    for reading objects from S3.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要使用`s3`、`s3n`或`s3a`（用于更大的S3对象）URI模式来从S3中读取对象。
- en: 'If `spark` is an instance of `SparkSession`, then you may use the following
    to load a text file (an Amazon S3 object) and return a DataFrame (denoted as the
    variable `df`) with a single string column named `value`:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`spark`是`SparkSession`的实例，则可以使用以下内容加载文本文件（Amazon S3对象），并返回一个带有名为`value`的单个字符串列的DataFrame（变量名为`df`）：
- en: '[PRE64]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The following example shows how to read an S3 object. First we use the `boto3`
    library (`boto3` is the AWS SDK for Python, which allows Python developers to
    write software that makes use of Amazon services like S3 and EC2) to verify that
    the object exists, and then we read it using PySpark.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何读取S3对象。首先我们使用`boto3`库（`boto3`是AWS的Python SDK，允许Python开发者编写利用Amazon服务如S3和EC2的软件）来验证对象是否存在，然后使用PySpark来读取它。
- en: 'In the following code, we check for the existence of the `s3://caselog-dev/tmp/csv_file_10_rows.csv`:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们检查是否存在`s3://caselog-dev/tmp/csv_file_10_rows.csv`：
- en: '[PRE65]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Then, we load the object and create a new `DataFrame[String]`:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们加载该对象并创建一个新的`DataFrame[String]`：
- en: '[PRE66]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO28-1)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO28-1)'
- en: Define your S3 object path.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 定义您的S3对象路径。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO28-2)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO28-2)'
- en: Use `SparkSession` (as `spark`) to load the S3 object and create a DataFrame.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`SparkSession`（作为`spark`）加载S3对象并创建一个DataFrame。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO28-3)'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO28-3)'
- en: Show the contents of the newly created DataFrame.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 展示新创建的DataFrame的内容。
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO28-4)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO28-4)'
- en: Display the schema for the newly created DataFrame.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 显示新创建的DataFrame的模式。
- en: Writing to Amazon S3
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入到Amazon S3
- en: 'Once you’ve created your DataFrame:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您创建了DataFrame：
- en: '[PRE67]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'you may examine the contents and its associated schema:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查其内容及其关联的架构：
- en: '[PRE68]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Next, save the data to the Amazon S3 filesystem:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将数据保存到 Amazon S3 文件系统：
- en: '[PRE69]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You will see that the following files are created:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下文件被创建：
- en: '[PRE70]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now let’s read it back:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将其读取回来：
- en: '[PRE71]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We can also read the S3 object as CSV:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将 S3 对象读取为 CSV 格式：
- en: '[PRE72]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO29-1)'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO29-1)'
- en: Default column names
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 默认列名
- en: Reading and Writing Hadoop Files
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和写入 Hadoop 文件
- en: '[Hadoop](http://hadoop.apache.org) is an open source MapReduce programming
    framework that supports the processing and storage of extremely large datasets
    in a distributed computing environment. It’s designed to scale up from single
    servers to thousands of machines. The Hadoop project, sponsored by the [Apache
    Software Foundation](http://apache.org) (ASF), includes these modules:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hadoop](http://hadoop.apache.org) 是一个开源的 MapReduce 编程框架，支持在分布式计算环境中处理和存储极大数据集。它设计用于从单个服务器扩展到数千台机器。由
    [Apache Software Foundation](http://apache.org)（ASF）赞助的 Hadoop 项目包括以下模块：'
- en: Hadoop Common
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 公共模块
- en: The common utilities that support the other Hadoop modules.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 支持其他 Hadoop 模块的常见实用工具。
- en: Hadoop Distributed File System (HDFS)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 分布式文件系统（HDFS）
- en: A distributed filesystem that provides high-throughput access to application
    data. HDFS allows for the distributed processing of large datasets across clusters
    of computers using MapReduce programming models.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分布式文件系统，提供高吞吐量访问应用程序数据。HDFS 允许在使用 MapReduce 编程模型的计算机集群上进行大数据集的分布式处理。
- en: Hadoop YARN
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop YARN
- en: A framework for job scheduling and cluster resource management.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 用于作业调度和集群资源管理的框架。
- en: Hadoop MapReduce
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop MapReduce
- en: A YARN-based system for parallel processing of large datasets.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 YARN 的系统，用于大数据集的并行处理。
- en: In this section, I’ll show you how to read files from HDFS and create RDDs and
    DataFrames and how to write RDDs and DataFrames into HDFS. To follow along, you’ll
    need access to a Hadoop cluster.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将展示如何从 HDFS 中读取文件并创建 RDD 和 DataFrame，以及如何将 RDD 和 DataFrame 写入 HDFS。要跟随示例，您需要访问一个
    Hadoop 集群。
- en: Reading Hadoop Text Files
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取 Hadoop 文本文件
- en: To illustrate the complete process of reading a file from HDFS, first we’ll
    create a text file in HDFS, then we’ll use PySpark to read it in as a DataFrame
    as well as an RDD.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示从 HDFS 中读取文件的完整过程，首先我们将在 HDFS 中创建一个文本文件，然后使用 PySpark 将其读取为 DataFrame 和 RDD。
- en: 'Let *name_age_salary.csv* be a text file in a Linux filesystem (this file can
    be created with any text editor—note that `$` is the Linux operating system prompt):'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 让 *name_age_salary.csv* 是 Linux 文件系统中的一个文本文件（可以使用任何文本编辑器创建此文件——注意 `$` 是 Linux
    操作系统提示符）：
- en: '[PRE73]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Create a */test* directory in HDFS using the **`$HADOOP_HOME/bin/hdfs`** command:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **`$HADOOP_HOME/bin/hdfs`** 命令在 HDFS 中创建一个 */test* 目录：
- en: '[PRE74]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Then I copy *name_age_salary.csv* to the *hdfs:///test/* directory:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我将 *name_age_salary.csv* 复制到 *hdfs:///test/* 目录：
- en: '[PRE75]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'And I examine the contents of the file:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我检查文件的内容：
- en: '[PRE76]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Now that we have created a file in HDFS, we will read it and create a DataFrame
    and an RDD from its contents.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在 HDFS 中创建了一个文件，我们将读取它，并从其内容创建一个 DataFrame 和一个 RDD。
- en: 'First, we read the HDFS file and create a DataFrame with default column names
    (`_c0, _c1, _c2`). The general format for an HDFS URI is:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们读取 HDFS 文件，并创建一个带有默认列名 (`_c0, _c1, _c2`) 的 DataFrame。HDFS URI 的一般格式如下：
- en: '[PRE77]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: where *`<server>`* is the hostname of the NameNode and *`<port>`* is the NameNode’s
    port number.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *`<server>`* 是 NameNode 的主机名，*`<port>`* 是 NameNode 的端口号。
- en: 'In this example I use a Hadoop instance installed on my MacBook; the NameNode
    is `localhost` and the port number is `9000`:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我使用了安装在我的 MacBook 上的 Hadoop 实例；NameNode 是 `localhost`，端口号是 `9000`：
- en: '[PRE78]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO30-1)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO30-1)'
- en: Default column names
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 默认列名
- en: 'Let’s examine the schema for the newly created DataFrame:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查新创建的 DataFrame 的架构：
- en: '[PRE79]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'If you want to impose your own explicit schema (column names and data types)
    on a DataFrame, then you may do so as follows:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在 DataFrame 上强制使用自己的显式模式（列名和数据类型），则可以这样做：
- en: '[PRE80]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO31-1)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO31-1)'
- en: Explicit schema definition
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 显式模式定义
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO31-2)'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO31-2)'
- en: Enforce an explicit schema
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 强制使用显式模式
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO31-3)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO31-3)'
- en: Explicit column names
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 显式列名
- en: 'You can also read in an HDFS file and create an `RDD[String]` from it:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以从 HDFS 文件中读取，并创建一个`RDD[String]`：
- en: '[PRE81]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Writing Hadoop Text Files
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入 Hadoop 文本文件
- en: 'PySpark’s API enables us to save our RDDs and DataFrames into HDFS as files.
    First let’s look at how to save an RDD into an HDFS file:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的 API 允许我们将 RDD 和 DataFrame 保存为 HDFS 文件。首先让我们看看如何将 RDD 保存到 HDFS 文件中：
- en: '[PRE82]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The `RDD.saveAsTextFile(path)` method writes the elements of the dataset as
    a text file (or set of text files) into a given directory in the local filesystem,
    HDFS, or any other Hadoop-supported filesystem. Spark will call the `toString()`
    method on each element to convert it to a line of text in the file.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD.saveAsTextFile(path)`方法将数据集的元素写入文本文件（或一组文本文件）到指定的目录中，可以是本地文件系统、HDFS 或其他任何
    Hadoop 支持的文件系统。Spark 会调用每个元素的`toString()`方法将其转换为文件中的文本行。'
- en: 'Next, let’s examine what is created in HDFS (the output here is formatted to
    fit the page):'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们查看在 HDFS 中创建了什么（这里的输出格式已经适合页面）：
- en: '[PRE83]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The reason we got eight *part-** files is because the source RDD had eight
    partitions:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了八个*part-*文件的原因是因为源 RDD 有八个分区：
- en: '[PRE84]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'If you want to create a single *part-** file, then you should create a single
    RDD partition:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建一个单一的*part-*文件，那么你应该创建一个单一的 RDD 分区：
- en: '[PRE85]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let’s examine what is created in the HDFS row:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看在 HDFS 行创建了什么：
- en: '[PRE86]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We can save a DataFrame into HDFS by using a `DataFrameWriter`:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`DataFrameWriter`将 DataFrame 保存到 HDFS 中：
- en: '[PRE87]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Here’s what’s created in the HDFS:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是在 HDFS 中创建的内容：
- en: '[PRE88]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'You may save your DataFrames into HDFS in different data formats. For example,
    to save a DataFrame in Parquet format, you can use the following template:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将 DataFrame 以不同的数据格式保存到 HDFS 中。例如，要以 Parquet 格式保存 DataFrame，可以使用以下模板：
- en: '[PRE89]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Reading and Writing HDFS SequenceFiles
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取和写入 HDFS SequenceFiles
- en: Hadoop offers to persist any file types, including `SequenceFile`s in HDFS.
    `SequenceFile`s are flat files consisting of binary (key, value) pairs. Hadoop
    defines a `SequenceFile` class as `org.apache.hadoop.io.SequenceFile`. `SequenceFile`
    provides `SequenceFile.Writer`, `SequenceFile.Reader`, and `SequenceFile.Sorter`
    classes for writing, reading, and sorting, respectively. `SequenceFile` is the
    standard binary serialization format for Hadoop. It stores records of `Writable`
    (key, value) pairs, and supports splitting and compression. `SequenceFile`s are
    commonly used for intermediate data storage in MapReduce pipelines, since they
    are more efficient than text files.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 支持持久化任何文件类型，包括在 HDFS 中的`SequenceFile`。`SequenceFile` 是由二进制（键，值）对组成的平面文件。Hadoop
    将`SequenceFile`类定义为`org.apache.hadoop.io.SequenceFile`。`SequenceFile` 提供了用于写入、读取和排序的`SequenceFile.Writer`、`SequenceFile.Reader`和`SequenceFile.Sorter`类。`SequenceFile`
    是 Hadoop 的标准二进制序列化格式。它存储`Writable`（键，值）对的记录，并支持分割和压缩。由于它们比文本文件更高效，因此`SequenceFile`
    在 MapReduce 管道中常用于中间数据存储。
- en: Reading HDFS SequenceFiles
  id: totrans-480
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 读取 HDFS SequenceFiles
- en: 'Spark supports reading `SequenceFile`s using the `SparkContext.sequenceFile()`
    method. For example, to read a `SequenceFile` with `Text` keys and `DoubleWritable`
    values in Python, we would do the following:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持使用`SparkContext.sequenceFile()`方法读取`SequenceFile`。例如，在 Python 中读取具有`Text`键和`DoubleWritable`值的`SequenceFile`，我们将执行以下操作：
- en: '[PRE90]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Note that unlike with Java or Scala, we do not pass the data types of (key,
    value) pairs to the Spark API; Spark automatically converts Hadoop’s `Text` to
    `String` and `DoubleWritable` to `Double`.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与 Java 或 Scala 不同，我们不会将（键，值）对的数据类型传递给 Spark API；Spark 会自动将 Hadoop 的`Text`转换为`String`，将`DoubleWritable`转换为`Double`。
- en: Writing HDFS SequenceFiles
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 写入 HDFS SequenceFiles
- en: 'PySpark’s `RDD.saveAsSequenceFile()` method allows users to save an RDD of
    (key, value) pairs as a `SequenceFile`. For example, we can create an RDD from
    a Python collection and save it as a `SequenceFile` as follows:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的`RDD.saveAsSequenceFile()`方法允许用户将（键，值）对的 RDD 保存为`SequenceFile`。例如，我们可以从
    Python 集合创建一个 RDD，并将其保存为`SequenceFile`，如下所示：
- en: '[PRE91]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We can then read the newly created `SequenceFile` and convert it to an RDD
    of (key, value) pairs:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以读取新创建的`SequenceFile`，并将其转换为（键，值）对的 RDD：
- en: '[PRE92]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Reading and Writing Parquet Files
  id: totrans-489
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读写 Parquet 文件
- en: '[Parquet](https://parquet.apache.org) is a columnar data format supported by
    many data processing systems. It’s self-describing (metadata is included), language-independent,
    and ideal for fast analytics.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '[Parquet](https://parquet.apache.org) 是许多数据处理系统支持的列式数据格式。它是自描述的（包含元数据）、与语言无关的，并且非常适合快速分析。'
- en: Spark SQL provides support for both reading and writing Parquet files while
    automatically preserving the schema of the original data. When writing Parquet
    files, all columns are automatically converted to be nullable for compatibility
    reasons.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 在读取和写入 Parquet 文件时提供支持，同时自动保留原始数据的模式。在写入 Parquet 文件时，所有列都会自动转换为可为空，以确保兼容性。
- en: '[Figure 7-3](#logical_table_row_layout_column_layout) illustrates a logical
    table and its associated row and column layouts.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-3](#logical_table_row_layout_column_layout) 描述了一个逻辑表及其相关的行和列布局。'
- en: '![daws 0703](Images/daws_0703.png)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0703](Images/daws_0703.png)'
- en: Figure 7-3\. Logical table with row layout and column
  id: totrans-494
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 带有行布局和列布局的逻辑表
- en: Writing Parquet Files
  id: totrans-495
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入 Parquet 文件
- en: 'In this section, I’ll show you how to use the PySpark API to read a JSON file
    into a DataFrame and then save it as a Parquet file. Suppose we have the following
    JSON file:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用 PySpark API 将 JSON 文件读取为 DataFrame，然后将其保存为 Parquet 文件。假设我们有以下
    JSON 文件：
- en: '[PRE93]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Using `DataFrameReader`, we read the JSON file into a DataFrame object as `peopleDF`:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `DataFrameReader`，我们将 JSON 文件读取为 DataFrame 对象，命名为 `peopleDF`：
- en: '[PRE94]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'We can then save this as a Parquet file, maintaining the schema information:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将其保存为 Parquet 文件，保留模式信息：
- en: '[PRE95]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'You can inspect the contents of the directory to view the generated Parquet
    file:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查目录内容以查看生成的 Parquet 文件：
- en: '[PRE96]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'For testing and debugging purposes, you may create Parquet files from Python
    collections:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试和调试目的，您可以从 Python 集合创建 Parquet 文件：
- en: '[PRE97]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO32-1)'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO32-1)'
- en: Convert your Python collection into a DataFrame.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的 Python 集合转换为 DataFrame。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO32-2)'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO32-2)'
- en: Save your DataFrame as a set of Parquet files.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的 DataFrame 保存为一组 Parquet 文件。
- en: 'Again, you can inspect the directory to view the created Parquet files:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以检查目录以查看创建的 Parquet 文件：
- en: '[PRE98]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Reading Parquet Files
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取 Parquet 文件
- en: 'In this section, using PySpark, we’ll read in the Parquet file we just created.
    Note that Parquet files are self-describing, so the schema is preserved. The result
    of loading a Parquet file is a DataFrame:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，使用 PySpark，我们将读取刚刚创建的 Parquet 文件。注意，Parquet 文件是自描述的，因此模式得以保留。加载 Parquet
    文件的结果是一个 DataFrame：
- en: '[PRE99]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Parquet files can also be used to create a temporary view and then used in
    SQL statements:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件也可以用来创建临时视图，然后在 SQL 语句中使用：
- en: '[PRE100]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO33-1)'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO33-1)'
- en: '`parquet_table` acts as a relational table.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '`parquet_table` 充当关系表。'
- en: 'Parquet supports collection data types, including an array type. The following
    example reads a Parquet file that uses arrays:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 支持集合数据类型，包括数组类型。以下示例读取使用数组的 Parquet 文件：
- en: '[PRE101]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Reading and Writing Avro Files
  id: totrans-521
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取和写入 Avro 文件
- en: '[Apache Avro](https://avro.apache.org) is a language-neutral data serialization
    system. It stores the data definition in JSON format, making it easy to read and
    interpret, while the data itself is stored in a compact, efficient binary format.
    Avro files include markers that can be used to split large datasets into subsets
    suitable for MapReduce processing. Avro is a very fast serialization format.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '[Apache Avro](https://avro.apache.org) 是一种语言中立的数据序列化系统。它将数据定义存储为 JSON 格式，易于读取和解释，而数据本身则以紧凑高效的二进制格式存储。Avro
    文件包含标记，可用于将大型数据集分割成适合 MapReduce 处理的子集。Avro 是一种非常快速的序列化格式。'
- en: Reading Avro Files
  id: totrans-523
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取 Avro 文件
- en: 'Using PySpark, we can read an Avro file and create an associated DataFrame
    as follows:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PySpark，我们可以读取 Avro 文件并按如下方式创建一个关联的 DataFrame：
- en: '[PRE102]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO34-1)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO34-1)'
- en: To read/write Avro files, you have to import the required Avro libraries; the
    `spark-avro` module is external and not included in `spark-submit` or `pyspark`
    by default.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取/写入 Avro 文件，您必须导入所需的 Avro 库；`spark-avro` 模块是外部的，不包含在 `spark-submit` 或 `pyspark`
    的默认设置中。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO34-2)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO34-2)'
- en: Read an Avro file and create a DataFrame.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 Avro 文件并创建一个 DataFrame。
- en: Writing Avro Files
  id: totrans-531
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入 Avro 文件
- en: 'It’s just as easy to create an Avro file from a DataFrame. Here, we’ll use
    the DataFrame created in the previous section), saving it as an Avro file and
    then reading it back in as a DataFrame:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DataFrame 创建 Avro 文件同样简单。这里，我们将使用上一节创建的 DataFrame，将其保存为 Avro 文件，然后再将其读回作为
    DataFrame：
- en: '[PRE104]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO35-1)'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO35-1)'
- en: Import the required Avro libraries.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的 Avro 库。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO35-2)'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO35-2)'
- en: Create an Avro file.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 Avro 文件。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO35-3)'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO35-3)'
- en: Create a DataFrame from the new Avro file.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 从新的 Avro 文件创建一个 DataFrame。
- en: Reading from and Writing to MS SQL Server
  id: totrans-541
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 MS SQL Server 读取和写入
- en: '[MS SQL Server](https://oreil.ly/2JckS) is a relational database management
    system from Microsoft, designed and built to manage and store information as records
    in relational tables.'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '[微软 SQL Server](https://oreil.ly/2JckS) 是微软的关系型数据库管理系统，设计和构建用于将信息以记录形式存储在关系表中。'
- en: Writing to MS SQL Server
  id: totrans-543
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入 MS SQL Server
- en: 'The following example shows how to write a DataFrame (`df`) into a new SQL
    Server table:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何将 DataFrame (`df`) 写入新的 SQL Server 表中：
- en: '[PRE106]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO36-1)'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO36-1)'
- en: The JAR file containing this class must be in your `CLASSPATH`.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 包含此类的 JAR 文件必须在你的 `CLASSPATH` 中。
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO36-2)'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO36-2)'
- en: The `overwrite` mode first drops the table if it already exists in the database
    by default.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`overwrite` 模式会先删除数据库中已存在的表。
- en: To append your DataFrame rows to an existing table, you just need to replace
    `mode("overwrite")` with `mode("append")`.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 DataFrame 的行追加到现有表中，只需用 `mode("append")` 替换 `mode("overwrite")`。
- en: 'Note that Spark’s MS SQL connector by default uses the `READ_COMMITTED` isolation
    level when performing a bulk insert into the database. If you wish to override
    the isolation level, use this `mssqlIsolationLevel` option as shown here:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Spark 的 MS SQL 连接器在执行数据库的批量插入时，默认使用 `READ_COMMITTED` 隔离级别。如果希望覆盖隔离级别，请使用此
    `mssqlIsolationLevel` 选项，如下所示：
- en: '[PRE107]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Reading from MS SQL Server
  id: totrans-553
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 MS SQL Server 读取
- en: 'To read from an existing SQL Server table, you can use the following code snippet
    as a template:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 要从现有的 SQL Server 表中读取，可以使用以下代码片段作为模板：
- en: '[PRE108]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Reading Image Files
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取图像文件
- en: Spark 2.4.0.+ enables us to read binary data which can be useful in many machine
    learning applications (such as face recognition and logistic regression). Spark
    can load image files from a directory into a DataFrame, transforming compressed
    images (*.jpg*, *.png*, etc.) into raw image representations via `ImageIO` in
    the Java library. The loaded *DataFrame* has one `StructType` column, `"image"`,
    containing image data stored as an image schema.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.4.0.+ 能够读取二进制数据，在许多机器学习应用中非常有用（如人脸识别和逻辑回归）。Spark 可以从目录中加载图像文件到 DataFrame，通过
    Java 库中的 `ImageIO` 将压缩图像（*.jpg*, *.png* 等）转换为原始图像表示。加载的 *DataFrame* 有一个 `StructType`
    列 `"image"`，包含存储为图像模式的图像数据。
- en: Creating a DataFrame from Images
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从图像创建一个 DataFrame。
- en: 'Suppose we have the following images in a directory:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在一个目录中有以下图像：
- en: '[PRE109]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO37-1)'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO37-1)'
- en: Not an image
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 不是图像
- en: 'We can load all the images into a DataFrame and ignore any files that are not
    images as follows:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将所有图像加载到 DataFrame 中，并忽略任何不是图像的文件，如下所示：
- en: '[PRE110]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO38-1)'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO38-1)'
- en: The `format` has to be `"image"`.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '`format` 必须是 `"image"`。'
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO38-2)'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO38-2)'
- en: Drop/ignore non-image files.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 删除/忽略非图像文件。
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO38-3)'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO38-3)'
- en: Load images and create a DataFrame.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 加载图像并创建 DataFrame。
- en: 'Let’s examine the DataFrame’s schema:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查 DataFrame 的模式：
- en: '[PRE111]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO39-1)'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_interacting_with_external_data_sources_CO39-1)'
- en: File path of the image
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的文件路径
- en: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO39-2)'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_interacting_with_external_data_sources_CO39-2)'
- en: Height of the image
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的高度
- en: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO39-3)'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_interacting_with_external_data_sources_CO39-3)'
- en: Width of the image
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的宽度
- en: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO39-4)'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_interacting_with_external_data_sources_CO39-4)'
- en: Number of image channels
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通道数
- en: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO39-5)'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_interacting_with_external_data_sources_CO39-5)'
- en: OpenCV-compatible type
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 兼容 OpenCV 的类型
- en: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO39-6)'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_interacting_with_external_data_sources_CO39-6)'
- en: Image bytes
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 图像字节
- en: 'Now, let’s examine some of the columns in the created image DataFrame:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看创建的图像 DataFrame 中的一些列：
- en: '[PRE112]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Summary
  id: totrans-587
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'To recap:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：
- en: Reading and writing data is an integral part of data algorithms. Data sources
    should be carefully selected based on project and data requirements.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读写数据是数据算法的一个重要部分。根据项目和数据需求，应谨慎选择数据源。
- en: The Spark DataSource API provides a pluggable mechanism for accessing structured
    data though Spark SQL. Data sources can be more than just simple pipes that convert
    data and pull it into Spark.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark DataSource API 提供了一个可插拔的机制，用于通过 Spark SQL 访问结构化数据。数据源不仅可以是简单的管道，用于转换数据并将其拉入
    Spark。
- en: Spark SQL supports reading data from existing relational database tables, Apache
    Hive tables, columnar storage formats like Parquet and ORC, and row-based storage
    formats like Avro. Spark provides a simple API to integrate with all JDBC-compliant
    relational databases, Amazon S3, HDFS, and more. You can also easily read data
    from and save it to external data sources such as text, CSV, and JSON files.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL 支持从现有的关系型数据库表、Apache Hive 表、列式存储格式如 Parquet 和 ORC，以及行式存储格式如 Avro 中读取数据。Spark
    提供了一个简单的 API 用于与所有 JDBC 兼容的关系型数据库、Amazon S3、HDFS 等集成。您还可以轻松地从文本、CSV 和 JSON 文件等外部数据源中读取和保存数据。
