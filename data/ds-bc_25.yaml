- en: 20 Network-driven supervised machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 20 网络驱动的监督机器学习
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Using classifiers in supervised machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在监督机器学习中使用分类器
- en: Making simple predictions based on similarity
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于相似性进行简单预测
- en: Metrics for evaluating the quality of predictions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估预测质量的标准
- en: Common supervised learning methods in scikit-learn
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn中的常见监督学习方法
- en: 'People can learn from real-world observations. In some respects, machines can
    do the same. Teaching computers to metaphorically understand the world through
    curated experience is referred to as *supervised machine learning*. In recent
    years, supervised machine learning has been all over the news: computers have
    been taught to predict stock prices, diagnose diseases, and even drive cars. These
    advancements have been rightly heralded as cutting-edge innovations. Yet, in some
    ways, the algorithms behind these innovations are not that novel. Variants of
    existing machine learning techniques have been around for many decades; but due
    to limited computing power, these techniques could not be adequately applied.
    Only now has our computing strength caught up. Hence, ideas planted many years
    ago are finally bearing the fruits of significant technological advancements.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可以从现实世界的观察中学习。在某种程度上，机器也可以做到同样的事情。通过精心挑选的经验来隐喻性地让计算机理解世界，这被称为*监督机器学习*。近年来，监督机器学习一直备受关注：计算机被训练来预测股价、诊断疾病，甚至驾驶汽车。这些进步被正确地誉为尖端创新。然而，在某些方面，这些创新背后的算法并不那么新颖。现有的机器学习技术的变体已经存在了几十年；但由于计算能力的限制，这些技术无法得到充分的应用。只有现在，我们的计算能力才赶上了。因此，多年前种下的想法终于结出了显著技术进步的果实。
- en: In this section, we explore one of the oldest and simplest supervised learning
    techniques. This algorithm, called K-nearest neighbors, was first developed by
    the US Air Force in 1951\. It is rooted in network theory and can be traced back
    to discoveries made by the medieval scholar Alhazen. Despite its age, the algorithm’s
    usage has much in common with more modern techniques. Thus, the insights we’ll
    gain will be transferable to the broader field of supervised machine learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了一种最古老且最简单的监督学习技术。这个算法被称为K近邻算法，最早由美国空军在1951年开发。它根植于网络理论，可以追溯到中世纪学者阿尔哈森的发现。尽管这个算法历史悠久，但其应用与更现代的技术有很多共同之处。因此，我们将获得的见解可以转移到更广泛的监督机器学习领域。
- en: 20.1 The basics of supervised machine learning
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.1 监督机器学习的基本原理
- en: Supervised machine learning is used to automate certain tasks that would otherwise
    be done by human beings. The machine observes a human carrying out a task and
    then learns to replicate the observed behavior. We’ll illustrate this learning
    using the flower dataset introduced in section 14\. As a reminder, the dataset
    represents three different species of the iris flower, which are displayed in
    figure 20.1\. Visually, the three species look alike; botanists use subtle variations
    along the lengths and widths of the leaves to distinguish the species. That type
    of expert knowledge must be learned—no human or machine can distinguish between
    the species without training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习用于自动化某些任务，这些任务原本由人类完成。机器观察人类执行任务，然后学习复制观察到的行为。我们将使用第14节中引入的花卉数据集来说明这种学习。作为提醒，该数据集代表三种不同的鸢尾花物种，如图20.1所示。从视觉上看，这三种物种看起来很相似；植物学家使用叶片长度和宽度上的细微差异来区分物种。那种专业知识必须学习——没有经过训练，任何人类或机器都无法区分这些物种。
- en: '![](../Images/20-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-01.png)'
- en: 'Figure 20.1 Three species of iris flowers: *Setosa*, *Versicolor*, and *Virginica*.
    The species all look alike. Subtle differences in their leaves can be utilized
    to tell the species apart, but training is required to appropriately identify
    the different species.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.1 三种鸢尾花物种：*Setosa*、*Versicolor*和*Virginica*。这些物种看起来都很相似。它们叶片上的细微差异可以用来区分物种，但需要训练才能适当地识别不同的物种。
- en: Suppose a botany professor conducts an ecological analysis of a local pasture.
    Hundreds of iris plants are growing in the pasture, and the professor wishes to
    know the distribution of iris species among these plants. However, the professor
    is busy writing grants and doesn’t have time to examine all the flowers personally.
    The professor thus hires an assistant to examine the flowers in the field. Unfortunately,
    the assistant is not a botanist and lacks the skills to tell the species apart.
    Instead, the assistant chooses to meticulously measure the lengths and widths
    of the leaves for every flower. Can these measurements be used to automatically
    identify all species? This question lies at the heart of supervised learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一位植物学教授对当地牧场的生态进行了分析。牧场上生长着数百种鸢尾植物，教授希望了解这些植物中鸢尾物种的分布情况。然而，教授正忙于撰写拨款申请，没有时间亲自检查所有花朵。因此，教授雇佣了一位助手来检查牧场上的花朵。不幸的是，这位助手不是植物学家，缺乏区分物种的技能。相反，助手选择仔细测量每朵花的叶片长度和宽度。这些测量能否用于自动识别所有物种？这个问题正是监督学习的核心。
- en: Essentially, we want to construct a model that maps inputted measurements to
    one of three species categories. In machine learning, these inputted measurements
    are called *features*, and the outputted categories are called *classes*. The
    goal of supervised learning is to construct a model that can identify classes
    based on features. Such a model is called a *classifier*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们希望构建一个模型，将输入的测量值映射到三种物种类别之一。在机器学习中，这些输入的测量值被称为*特征*，输出的类别被称为*类别*。监督学习的目标是构建一个模型，可以根据特征识别类别。这样的模型被称为*分类器*。
- en: Note By definition, classes are discrete, categorical variables such as species
    of flower or type of car. Alternatively, there are models called *regressors*
    that predict numeric variables, such as the price of a house or the speed of a
    car.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：根据定义，类别是离散的、分类变量，如花的物种或汽车类型。另一种模型称为*回归器*，它预测数值变量，如房屋价格或汽车速度。
- en: 'There are many different types of machine learning classifiers. Whole books
    are dedicated to demarcating the various classifier categories. But despite their
    diversity, most classifiers require the same common steps for construction and
    implementation. To implement a classifier, we need to do the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中有许多不同类型的分类器。整本书都是专门用来区分各种分类器类别的。尽管它们种类繁多，但大多数分类器在构建和实施时都需要相同的共同步骤。要实现一个分类器，我们需要做以下事情：
- en: Compute the features for each data point. In our botany example, all data points
    are flowers, so we need to measure the leaf lengths for each flower.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个数据点的特征。在我们的植物学示例中，所有数据点都是花朵，因此我们需要测量每朵花的叶片长度。
- en: A domain expert must assign labels to a subset of the data points. Our botanist
    has no choice but to manually identify the species in a subset of the flowers.
    Without the professor’s supervision, the classifier cannot be constructed properly.
    The term *supervised learning* is derived from this supervised labeling phase.
    Labeling the subset of flowers takes time, but that effort will pay off once the
    classifier can make automated predictions.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 领域专家必须对数据点的一个子集分配标签。我们的植物学家别无选择，只能手动识别一部分花朵中的物种。没有教授的监督，分类器无法正确构建。*监督学习*这个术语就是从这个监督标签阶段来的。标记花朵的子集需要花费时间，但一旦分类器能够进行自动预测，这些努力就会得到回报。
- en: Show the classifier the combination of features and manually labeled classes.
    It then attempts to learn the association between the features and the classes.
    This learning phase varies from classifier to classifier.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向分类器展示特征组合和手动标记的类别。然后它尝试学习特征与类别之间的关联。这个学习阶段因分类器而异。
- en: Show the classifier a set of features that it has not previously encountered.
    It then attempts to predict the associated classes based on its exposure to the
    labeled data.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向分类器展示一组它之前未曾遇到的特征。然后它尝试根据其接触到的标记数据预测相关的类别。
- en: 'To construct a classifier, our botanist needs a set of features for a collection
    of identified flowers. Each flower is assigned the following four features (previously
    discussed in section 14):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个分类器，我们的植物学家需要一组已识别花朵的特征。每朵花被分配以下四个特征（在14节中已讨论过）：
- en: The length of a colorful petal
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持花瓣的有色花瓣的长度
- en: The width of the colorful petal
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持花瓣的有色花瓣的宽度
- en: The length of a green leaf supporting the petal
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持花瓣的绿色叶片的长度
- en: The width of the green leaf supporting the petal
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持花瓣的绿色叶片的宽度
- en: We can store these features in a feature matrix. Each matrix column corresponds
    to one of the four features, and each matrix row corresponds to a labeled flower.
    The class labels are stored in a NumPy array. Such arrays are intended to hold
    numbers and not text; so, in machine learning, class labels are represented as
    integers that range from 0 to *N* – 1 (where *N* is the total number of classes).
    In our iris example, we are dealing with three species, so class labels range
    from 0 to 2.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些特征存储在一个特征矩阵中。每个矩阵列对应于四个特征之一，每个矩阵行对应于一个标记的花朵。类别标签存储在一个NumPy数组中。这样的数组旨在存储数字而不是文本；因此，在机器学习中，类别标签表示为从0到*N*
    - 1（其中*N*是类别总数）的整数。在我们的鸢尾花示例中，我们处理三种物种，因此类别标签的范围是从0到2。
- en: As seen in section 14, we can load known iris features and class labels using
    scikit-learn’s `load_iris` function. Let’s do that now. Per existing scikit-learn
    convention, a feature matrix is usually assigned to a variable called `X`, and
    the class-label array is assigned to a variable called `y`. Following this convention,
    the following code loads the iris `X` and `y` by passing `return_X_y=True` into
    `load_iris`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如第14节所示，我们可以使用scikit-learn的`load_iris`函数加载已知的鸢尾花特征和类别标签。现在让我们这样做。按照现有的scikit-learn约定，特征矩阵通常分配给一个名为`X`的变量，类别标签数组分配给一个名为`y`的变量。遵循这一约定，以下代码通过将`return_X_y=True`传递给`load_iris`来加载鸢尾花的`X`和`y`。
- en: Listing 20.1 Loading iris features and class labels
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.1 加载鸢尾花特征和类别标签
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All 150 flower measurements have been labeled as belonging to one of three flower
    species. Such labeling is hard work. Imagine that our botanist only has time to
    label one-fourth of the flowers. The professor then constructs a classifier to
    predict the classes of the remaining flowers. Let’s simulate this scenario. We
    start by choosing the first one-fourth of data points in `X` and `y`. This data
    slice is referred to as `X_train` and `y_train` since it is used for training
    purposes; such datasets are called *training sets*. After sampling our training
    set, we investigate the contents of `y_train`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有150朵花的测量值都已标记为属于三种花卉物种之一。这种标记工作很辛苦。想象一下，我们的植物学家只有时间标记四分之一的花朵。然后教授构建了一个分类器来预测剩余花朵的类别。让我们模拟这个场景。我们首先选择`X`和`y`中的前四分之一数据点。这个数据切片被称为`X_train`和`y_train`，因为它用于训练目的；这样的数据集被称为*训练集*。在采样我们的训练集之后，我们调查`y_train`的内容。
- en: Listing 20.2 Creating a training set
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.2 创建训练集
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our training set contains just the labeled examples with Species 0; the remaining
    two flower species are not represented. To increase representation, we should
    sample at random from `X` and `y`. Random sampling can be achieved using scikit-learn’s
    `train_test_split` function, which takes as input `X` and `y` and returns four
    randomly generated outputs. The first two outputs are `X_train` and `y_train`,
    corresponding to our training set. The next two outputs cover the features and
    classes outside of our training set. These outputs can be utilized to test the
    classifier after training, so that data is commonly called the *test set*. We’ll
    refer to the test features and classes as `X_test` and `y_test`, respectively.
    Later in this section, we use the test set to evaluate our trained model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练集仅包含标记为物种0的示例；其他两种花卉物种没有代表。为了增加代表性，我们应该从`X`和`y`中进行随机采样。随机采样可以通过使用scikit-learn的`train_test_split`函数实现，该函数接受`X`和`y`作为输入，并返回四个随机生成的输出。前两个输出是`X_train`和`y_train`，对应我们的训练集。接下来的两个输出涵盖了训练集之外的特征和类别。这些输出可以在训练后用于测试分类器，因此这些数据通常被称为*测试集*。我们将测试特征和类别分别称为`X_test`和`y_test`。在本节的后面部分，我们使用测试集来评估我们的训练模型。
- en: Listing 20.3 calls the `train_test_split` function and passes it an optional
    `train_size=0.25` parameter. The `train_size` parameter ensures that 25% of our
    total data winds up in the training set. Finally, we print `y_train` to ensure
    that all three labels are properly represented.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.3调用了`train_test_split`函数，并传递了一个可选的`train_size=0.25`参数。`train_size`参数确保我们总数据的25%最终进入训练集。最后，我们打印`y_train`以确保所有三个标签都得到了适当的表示。
- en: Listing 20.3 Creating a training set through random sampling
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.3 通过随机采样创建训练集
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'All three label classes are present in the training data. How can we utilize
    both `X_train` and `y_train` to predict the classes of the remaining flowers in
    the test set? One simple strategy involves geometric proximity. As we saw in section
    14, the features in the iris dataset can be plotted in multidimensional space.
    This plotted data forms spatial clusters: elements in `X_test` are more likely
    to share their class with the `X_train` points found in the adjacent cluster.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中包含所有三个标签类别。我们如何利用 `X_train` 和 `y_train` 来预测测试集中剩余花朵的类别？一种简单的策略涉及几何邻近性。正如我们在第
    14 节中看到的，鸢尾花数据集中的特征可以在多维空间中绘制。这些绘制的数据形成空间簇：`X_test` 中的元素更有可能与相邻簇中找到的 `X_train`
    点共享它们的类别。
- en: Let’s illustrate this intuition by plotting both `X_train` and `X_test` in 2D
    space (figure 20.2). We use principal component analysis to shrink our data to
    two dimensions, and then we plot the reduced features in our training set while
    coloring each plotted point based on its labeled class. We also plot the elements
    of our test set using a triangular marker to indicate the lack of a label. We
    then guess the identity of the unlabeled points based on their proximity to labeled
    data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在 2D 空间中绘制 `X_train` 和 `X_test` 来说明这种直觉（图 20.2）。我们使用主成分分析将我们的数据缩小到两个维度，然后绘制我们的训练集中的降维特征，并根据每个绘制点的标记类别着色。我们还使用三角形标记绘制我们的测试集元素，以表示没有标签。然后我们根据未标记点与标记数据的接近程度猜测未标记点的身份。
- en: '![](../Images/20-02.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20-02.png)'
- en: Figure 20.2 Flower data points plotted in 2D. Each labeled flower is colored
    based on its species class. Unlabeled flowers are also present in the plot. Visually,
    we can guess the identity of the unlabeled flowers based on their proximity to
    labeled points.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2 在 2D 空间中绘制的花朵数据点。每个标记的花朵根据其物种类别着色。图中也存在未标记的花朵。从视觉上，我们可以根据它们与标记点的接近程度来猜测未标记花朵的身份。
- en: Listing 20.4 Plotting the training and test sets
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.4 绘制训练集和测试集
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the left-hand section of our plot, many unlabeled points cluster around
    Species 0\. There is no ambiguity here: these unlabeled flowers clearly belong
    to the same species. Elsewhere in the plot, certain unlabeled flowers are proximate
    to both Species 1 and Species 2\. For each such point, we need to quantify which
    labeled species are closer. Doing so requires us to track the Euclidean distance
    between each feature in `X_test` and each feature in `X_train`. Essentially, we
    need a distance matrix `M` where `M[i][j]` equals the Euclidean distance between
    `X_test[i]` and `X_test[j]`. Such a matrix can easily be generated using scikit-learn’s
    `euclidean_distances` function. We simply need to execute `euclidean_distances(X_test,
    X_train)` to return the distance matrix.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们图表的左侧部分，许多未标记的点围绕着物种 0 聚集。这里没有歧义：这些未标记的花朵显然属于同一物种。在图表的其他地方，某些未标记的花朵与物种 1
    和物种 2 都很接近。对于这样的每个点，我们需要量化哪些标记的物种更接近。这样做需要我们跟踪 `X_test` 中每个特征与 `X_train` 中每个特征之间的欧几里得距离。本质上，我们需要一个距离矩阵
    `M`，其中 `M[i][j]` 等于 `X_test[i]` 和 `X_test[j]` 之间的欧几里得距离。这样的矩阵可以很容易地使用 scikit-learn
    的 `euclidean_distances` 函数生成。我们只需执行 `euclidean_distances(X_test, X_train)` 来返回距离矩阵。
- en: Listing 20.5 Computing Euclidean distances between points
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.5 计算点之间的欧几里得距离
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Given any unlabeled point in `X_test`, we can assign a class using the following
    strategy:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `X_test` 中的任何未标记点，我们可以使用以下策略分配一个类别：
- en: Sort all data points in the training set based on their distance to the unlabeled
    points.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据它们到未标记点的距离对训练集中的所有数据点进行排序。
- en: Select the top *K*-nearest neighbors of the point. For now, we’ll arbitrarily
    set *K* to equal 3.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择点的 *K* 个最近邻。目前，我们将 *K* 随意设置为 3。
- en: Pick the most frequently occurring class across the *K* neighboring points.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择在 *K* 个邻近点中最常出现的类别。
- en: Essentially, we’re assuming that each unlabeled point shares a class that is
    common to its neighbors. This strategy forms the basis for the *K-nearest* *neighbors*
    (KNN) algorithm. Let’s try this strategy on a randomly chosen point.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们假设每个未标记点与其邻居共享一个类别，这个类别是它们的共同类别。这种策略是 *K-最近邻*（KNN）算法的基础。让我们在一个随机选择的点上尝试这种策略。
- en: Listing 20.6 Labeling a point based on its nearest neighbors
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.6 基于最近邻标记一个点
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The most common class label among the neighbors of Point 10 is Label 2\. How
    does this compare to the actual class of the flower?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 点 10 的邻居中最常见的类别标签是标签 2。这与花朵的实际类别相比如何？
- en: Listing 20.7 Checking the true class of a predicted label
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.7 检查预测标签的真实类别
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: KNN successfully identified the flower class of Point 10\. All we needed to
    do was to check the labeled neighbors and count the most common label among them.
    Interestingly, this process can be reformulated as a graph theory problem. We
    can treat each point as a node and its label as a node attribute and then choose
    an unlabeled point and extend edges to its *K* closest labeled neighbors. Visualizing
    the neighbor graph allows us to identify the point.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: KNN成功识别了点10的花卉类别。我们所需做的只是检查标签邻居并计算其中最常见的标签。有趣的是，这个过程可以被重新表述为一个图论问题。我们可以将每个点视为一个节点，其标签作为节点属性，然后选择一个未标记的点并扩展边到其最近的K个标记邻居。可视化邻居图可以帮助我们识别该点。
- en: Note This type of graph structure is called a *K-nearest neighbor graph* (k-NNG).
    Such graphs are used in a variety of fields, including transportation planning,
    image compression, and robotics. Additionally, these graphs can be used to improve
    the DBSCAN clustering algorithm.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这种类型的图结构被称为*K-最近邻图*（k-NNG）。这类图在许多领域都有应用，包括交通规划、图像压缩和机器人技术。此外，这些图还可以用来改进DBSCAN聚类算法。
- en: Let’s demonstrate the network formulation of the problem by plotting the neighbor
    graph of Point 10 (figure 20.3). We utilize NetworkX for the purpose of this visualization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过绘制点10的邻居图（图20.3）来展示问题的网络公式。我们利用NetworkX进行可视化。
- en: '![](../Images/20-03.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20-03.png)'
- en: Figure 20.3 A NetworkX graph representing an unlabeled point and its three nearest
    labeled neighbors. Two of three neighbors are labeled Class 2\. Thus, we can hypothesize
    that the unlabeled point also belongs to that majority class.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.3 一个表示未标记点和其三个最近标记邻居的NetworkX图。其中三个邻居中有两个被标记为类别2。因此，我们可以假设未标记的点也属于那个多数类别。
- en: Listing 20.8 Visualizing nearest neighbors with NetworkX
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.8 使用NetworkX可视化最近邻
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Plots and returns a NetworkX graph containing connections between an unlabeled
    data point and the labeled nearest neighbors of that point
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 绘制并返回一个包含未标记数据点和该点的标记最近邻之间连接的NetworkX图
- en: ❷ Obtains the labels of the nearest neighbors
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取最近邻的标签
- en: ❸ Colors the labeled neighbors based on their labels
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据标签为标记邻居着色
- en: KNN works when there are just three neighbors. What happens if we increase the
    neighbor count to four? Let’s find out (figure 20.4)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当只有三个邻居时，KNN可以工作。如果我们增加邻居数量到四个会发生什么？让我们找出答案（图20.4）
- en: '![](../Images/20-04.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20-04.png)'
- en: Figure 20.4 A NetworkX graph representing an unlabeled point and its four nearest
    labeled neighbors. Two of four neighbors are labeled Class 2, and the remaining
    two neighbors belong to Class 1\. No majority class is present. Thus, we are unable
    to identify the unlabeled point
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.4 一个表示未标记点和其四个最近标记邻居的NetworkX图。其中两个邻居被标记为类别2，其余两个邻居属于类别1。没有多数类别存在。因此，我们无法识别未标记的点
- en: Listing 20.9 Increasing the number of nearest neighbors
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.9 增加最近邻的数量
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There is a tie! No label dominates the majority. We can’t make a decision. What
    should we do? One option is to break the tie at random. A better approach is to
    factor in the distances to the labeled points. Labeled points that are closer
    to Point 10 are more likely to share the correct class. Hence, we should give
    more weight to more proximate points, but how?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 出现平局！没有标签主导多数。我们无法做出决定。我们该怎么办？一个选择是随机打破平局。一个更好的方法是考虑到标签点的距离。距离点10更近的标签点更有可能共享正确的类别。因此，我们应该给予更近的点更多的权重，但如何操作呢？
- en: 'Well, in our initial KNN implementation, each labeled point received an equal
    vote, like in a fair democracy. Now we want to weigh each vote based on distance.
    One simple weighing scheme is to give each labeled point `1 / distance` votes:
    a point that’s one unit away will receive one vote, a point that’s 0.5 units away
    will receive two votes, and a point that’s two units away will receive just half
    a vote. This doesn’t make for fair politics, but it could improve the output of
    our algorithm.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在我们最初的KNN实现中，每个标记点都得到了平等的投票，就像在一个公平的民主中一样。现在我们想要根据距离来权衡每个投票。一个简单的权衡方案是给每个标记点`1
    / distance`票：距离一个单位的点将得到一票，距离0.5单位的点将得到两票，距离两个单位的点将得到半票。这并不构成公平的政治，但它可能会改善我们算法的输出。
- en: The following code assigns each labeled point a vote amount equal to its inverse
    distance from Point 10\. Then we let the labeled points vote based on their class.
    We utilize the tallied votes to choose an elected class for our Point 10.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将每个标记点的投票量分配为其与点10的倒数距离相等。然后我们让标记点根据它们的类别进行投票。我们利用累计的投票来为我们的点10选择一个当选的类别。
- en: Listing 20.10 Weighing votes of neighbors based on distance
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.10 根据距离权衡邻居的投票
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once again, we’ve correctly chosen Class 2 as the true class of Point 10\. The
    optional weighted voting can potentially improve our final prediction. Of course,
    this improvement is by no means guaranteed; occasionally, weighted voting can
    worsen the outputted results. Depending on the preset value of our *K*, weighted
    voting can either improve or worsen our predictions. We won’t know for sure until
    we test prediction performance across a range of parameters. Such testing will
    require us to develop a robust metric for measuring performance accuracy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次正确地将类别2选为点10的真实类别。可选的加权投票可能会提高我们的最终预测。当然，这种改进并不保证；有时加权投票可能会使输出结果变得更糟。根据我们预设的*K*值，加权投票可能会改善或恶化我们的预测。我们只有在测试一系列参数的预测性能后才能确定。这种测试需要我们开发一个健壮的指标来衡量性能准确性。
- en: 20.2 Measuring predicted label accuracy
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.2 测量预测标签的准确性
- en: Thus far, we’ve examined class prediction for a single, randomly chosen point.
    Now we want to analyze predictions across all the points in `X_test`. We define
    a `predict` function for this purpose, which takes as input the index of an unlabeled
    point and a value of *K*, which we preset to 1.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经检查了一个随机选择的点的类别预测。现在我们想要分析`X_test`中所有点的预测。为此，我们定义了一个`predict`函数，它接受一个未标记点的索引和一个预设的*K*值，我们预设为1。
- en: Note We’re purposefully inputting a low value of *K* to generate a multitude
    of errors worth improving. Later, we measure the error across multiple *K*-values
    to optimize performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 我们故意输入一个低值的*K*来生成许多值得改进的错误。稍后，我们将测量多个*K*值的错误以优化性能。
- en: The final parameter is a `weighted_voting` Boolean, which we set to `False`.
    That Boolean determines whether votes should be distributed according to distance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个参数是一个`weighted_voting`布尔值，我们将其设置为`False`。这个布尔值确定投票是否应根据距离分布。
- en: Listing 20.11 Parameterizing KNN predictions
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.11 参数化KNN预测
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Predicts the label of a point using its row index in the distance matrix based
    on its K-nearest neighbors. The weighted_voting Boolean determines whether voting
    is weighted by neighbor distance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用点在距离矩阵中的行索引根据其K最近邻预测点的标签。`weighted_voting`布尔值确定投票是否根据邻居距离加权。
- en: ❷ Obtains the K-nearest neighbors
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取K最近邻
- en: ❸ Weighs votes equally if weighted_voting is False and by inverse distance otherwise.
    We take precautions when computing the inverse to avoid dividing by zero.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果`weighted_voting`为False，则平等地权衡投票，否则根据距离的倒数进行权衡。我们在计算倒数时采取预防措施，以避免除以零。
- en: ❹ Returns the class label with the most votes
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回获得最多投票的类别标签
- en: Let’s execute `predict` across all unlabeled indices. Following a common naming
    convention, we store the predicted classes in an array named `y_pred`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在所有未标记的索引上执行`predict`。遵循常见的命名约定，我们将预测的类别存储在一个名为`y_pred`的数组中。
- en: Listing 20.12 Predicting all unlabeled flower classes
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.12 预测所有未标记的花类
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We want to compare the predicted classes with the actual classes in `y_test`.
    Let’s start by printing out both the `y_pred` and the `y_test` arrays.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要将预测的类别与实际类别在`y_test`中进行比较。让我们先打印出`y_pred`和`y_test`数组。
- en: Listing 20.13 Comparing the predicted and actual classes
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.13 比较预测和实际类别
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It’s difficult to compare the two printed arrays. We can run an easier comparison
    if we aggregate the arrays into a single matrix `M` that contains three rows and
    three columns, corresponding to the number of classes. The rows track predicted
    classes, and the columns track the true class identities. Each element `M[i][j]`
    counts the co-occurrences between predicted Class *i* and actual Class *j*, as
    is illustrated in figure 20.5.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 比较打印出的两个数组很困难。如果我们将这些数组聚合到一个包含三行三列的单个矩阵`M`中，比较会更容易，这个矩阵对应于类别的数量。行跟踪预测类别，列跟踪真实类别的标识。每个元素`M[i][j]`计算预测类别*i*和实际类别*j*之间的共现次数，如图20.5所示。
- en: '![](../Images/20-05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20-05.png)'
- en: Figure 20.5 A hypothetical matrix representation of the predicted and actual
    classes. The rows correspond to predicted classes, and the columns correspond
    to the actual classes. Each element `M[i][j]` counts the co-occurrences between
    predicted Class *i* and actual Class *j*. Hence, the matrix diagonal is counting
    all the accurate predictions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.5 预测类别和实际类别的假设矩阵表示。行对应于预测类别，列对应于实际类别。每个元素 `M[i][j]` 计算预测类别 *i* 和实际类别 *j*
    之间的共现次数。因此，矩阵的对角线计算所有准确预测。
- en: This type of matrix representation is known as a *confusion matrix* or an *error
    matrix*. As we shall see shortly, the confusion matrix can help quantify prediction
    errors. We now compute the confusion matrix using `y_pred` and `y_test` and visualize
    the matrix as a heatmap using Seaborn (figure 20.6).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种矩阵表示法被称为 *混淆矩阵* 或 *误差矩阵*。正如我们很快将看到的，混淆矩阵可以帮助量化预测误差。我们现在使用 `y_pred` 和 `y_test`
    计算混淆矩阵，并使用 Seaborn 将矩阵可视化为热图（图20.6）。
- en: '![](../Images/20-06.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/20-06.png)'
- en: Figure 20.6 A confusion matrix comparing the predicted results to the actual
    results. The rows correspond to predicted classes, and the columns correspond
    to the actual classes. The matrix elements count all corresponding instances between
    the predicted and actual classes. The diagonal of the matrix counts all accurate
    predictions. Most of our counts lie along the matrix diagonal, indicating that
    our model is highly accurate.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.6 比较预测结果和实际结果的混淆矩阵。行对应于预测类别，列对应于实际类别。矩阵元素计算预测类别和实际类别之间的所有对应实例。矩阵的对角线计算所有准确预测。我们的大多数计数都沿着矩阵的对角线，这表明我们的模型非常准确。
- en: Listing 20.14 Computing the confusion matrix
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.14 计算混淆矩阵
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Computes the confusion matrix between y_pred and y_test
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算y_pred和y_test之间的混淆矩阵
- en: ❷ Checks for the total number of classes. This value defines the number of matrix
    rows and columns.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查类别的总数。此值定义了矩阵的行数和列数。
- en: ❸ Each predicted class Prediction corresponds to an actual class Actual. For
    every such pair, we add a 1 to the row Prediction and column Actual of our matrix.
    Note that if Prediction == Actual, then the added value appears on the diagonal
    of the matrix.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个预测类别预测与实际类别实际对应。对于每一对这样的类别，我们在矩阵的预测行和实际列中添加一个1。注意，如果预测 == 实际，则添加的值出现在矩阵的对角线上。
- en: Most of the values in the matrix lie along its diagonal. Each diagonal element
    `M[i][i]` tracks the number of accurately predicted instances of Class *i*. Such
    accurate predictions are commonly called *true positives*. Based on our displayed
    diagonal values, we know that our true positive count is very high. Let’s print
    the total true positive count by summing across `M.diagonal()`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的大多数值都沿着其对角线排列。每个对角元素 `M[i][i]` 跟踪类别 *i* 的准确预测实例数。这种准确的预测通常被称为 *真阳性*。根据我们显示的对角线值，我们知道我们的真阳性计数非常高。让我们通过求
    `M.diagonal()` 的和来打印总真阳性计数。
- en: Listing 20.15 Counting the number of accurate predictions
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.15 计算准确预测的数量
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The results include 104 accurate predictions: our accuracy is high. Of course,
    not all the predictions are accurate. Occasionally, our classifier gets confused
    and predicts the wrong class label: out of 113 total predictions, 9 predictions
    in the matrix lie outside the diagonal. The fraction of total accurate predictions
    is referred to as the *accuracy* score. Accuracy can be computed by dividing the
    diagonal sum across the total sum of matrix elements: in our case, dividing 104
    by 113 produces a high accuracy value.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 结果包括104个准确预测：我们的准确度很高。当然，并非所有预测都是准确的。有时，我们的分类器会混淆并预测错误的类别标签：在113个总预测中，矩阵中有9个预测位于对角线之外。总准确预测的分数被称为
    *准确度* 分数。准确度可以通过将矩阵元素的总和除以对角线总和来计算：在我们的情况下，将104除以113产生一个高准确度值。
- en: Listing 20.16 Measuring the accuracy score
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.16 测量准确度得分
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our predictions are quite accurate, but they are not perfect. Errors are present
    in the output. These errors are not equally distributed: for instance, by examining
    the matrix, we can see that our Class 0 predictions are always right. The model
    never confuses Class 0 with any other class or vice versa; all 38 predictions
    for that class lie along the diagonal. This is not the case for the other two
    classes: the model periodically confuses instances of Classes 1 and 2.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测相当准确，但并不完美。输出中存在错误。这些错误并不均匀分布：例如，通过检查矩阵，我们可以看到我们的第 0 类预测总是正确的。模型从未将第 0
    类与其他任何类别混淆，反之亦然；该类别的所有 38 个预测都位于对角线上。其他两个类别并非如此：模型会定期混淆第 1 类和第 2 类的实例。
- en: Let’s try to quantify the observed confusion. Consider the elements in matrix
    Row 1, which tracks our predictions of Class 1\. Summing across this row yields
    the total count of elements that we’ve predicted as belonging to Class 1.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试量化观察到的混淆。考虑矩阵的第 1 行元素，它跟踪我们对第 1 类的预测。对该行求和得到我们预测属于第 1 类的元素总数。
- en: Listing 20.17 Counting the predicted Class 1 elements
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.17 计算预测的第 1 类元素数量
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We predicted that Class 1 has 38 elements. How many of these predictions are
    correct? Well, 33 predictions lie along the `M[1][1]` diagonal. Thus, we’ve correctly
    identified 33 true positives of Class 1\. Meanwhile, the remaining 5 predictions
    lie in Column 2\. These 5 *false positives* represent Class 2 elements that we’ve
    misidentified as belong to Class 1; they make our Class 1 predictions less reliable.
    Just because our model returns a Class 1 label does not mean the prediction is
    correct. In fact, our Class 1 label is correct in just 33 of 38 total instances.
    The ratio 33 / 38 produces a metric called *precision*: the true positive count
    divided by the sum of true positives and false positives. The precision of Class
    *i* is also equal to `M[i][i]` divided by the sum across Row *i*. A low precision
    indicates that a predicted class label is not very reliable. Let’s output the
    precision of Class 1.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测第 1 类有 38 个元素。这些预测中有多少是正确的？嗯，有 33 个预测位于 `M[1][1]` 对角线上。因此，我们正确识别了 33 个第
    1 类的真实阳性。同时，剩余的 5 个预测位于第 2 列。这 5 个 *假阳性* 代表我们将属于第 2 类的元素错误地识别为属于第 1 类；这使我们的第 1
    类预测变得不那么可靠。仅仅因为我们的模型返回了第 1 类标签，并不意味着预测是正确的。事实上，在我们的 38 个总实例中，只有 33 个第 1 类标签是正确的。比率
    33 / 38 产生了一个称为 *精确度* 的指标：真实阳性数除以真实阳性数和假阳性数的总和。第 *i* 类的精确度也等于 `M[i][i]` 除以第 *i*
    行的总和。低精确度表明预测的类别标签不太可靠。让我们输出第 1 类的精确度。
- en: Listing 20.18 Computing the precision of Class 1
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.18 计算第 1 类的精确度
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The precision of Class 1 is 0.87, so a Class 1 label is reliable only 87% percent
    of the time. In the remaining 13% of instances, the prediction is a false positive.
    These false positives are a cause of error, but they’re not the only one: additional
    errors can be detected across the confusion matrix columns. Consider, for example,
    Column 1, which tracks all elements in `y_test`, whose true label is equal to
    Class 1\. Summing over Column 1 yields the total count of Class 1 elements.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 类的精确度为 0.87，因此第 1 类标签只有 87% 的时间是可靠的。在剩余的 13% 的实例中，预测结果是假阳性。这些假阳性是错误的原因之一，但并非唯一：混淆矩阵的列中还可以检测到其他错误。例如，考虑第
    1 列，它跟踪所有在 `y_test` 中的元素，其真实标签等于第 1 类。对第 1 列求和得到第 1 类元素的总数。
- en: Listing 20.19 Counting the total Class 1 elements
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.19 计算总第 1 类元素数量
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '37 elements in our test set belong to Class 1\. 33 of these elements lie along
    the `M[1][1]` diagonal: these true positive elements have been identified correctly.
    The remaining four elements lie in Row 2; these *false negatives* represent Class
    1 elements that we’ve misidentified as belonging to Class 2\. Hence, our identification
    of Class 1 elements is incomplete. Of the 37 possible class instances, only 33
    have been identified correctly. The ratio 33 / 37 produces a metric called *recall*
    : the true positive count divided by the sum of true positives and false negatives.
    The recall of Class *i* is also equal to `M[i][i]` divided by the sum across Column
    *i*. A low recall indicates that our predictor commonly misses valid instances
    of a class. Let’s output the recall of Class 1.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试集中，有 37 个元素属于类别 1。其中 33 个元素位于 `M[1][1]` 对角线上：这些真正的阳性元素已被正确识别。其余四个元素位于第
    2 行；这些 *假阴性* 代表我们错误地将属于类别 2 的类别 1 元素识别出来的情况。因此，我们对类别 1 元素的识别是不完整的。在 37 个可能的类别实例中，只有
    33 个被正确识别。比例 33 / 37 产生了一个称为 *召回率* 的指标：真正的阳性计数除以真正的阳性计数和假阴性计数之和。类别 *i* 的召回率也等于
    `M[i][i]` 除以第 *i* 列的总和。低召回率表明我们的预测器通常会错过某个类别的有效实例。让我们输出类别 1 的召回率。
- en: Listing 20.20 Computing the recall of Class 1
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.20 计算类别 1 的召回率
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The recall of Class 1 is 0.89, so we’re able to detect 89% of valid Class 1
    instances. The remaining 11% of instances are misidentified. The recall measures
    the fraction of identified Class 1 flowers in the pasture. By contrast, the precision
    measures the likelihood that a Class 1 prediction is correct.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 1 的召回率为 0.89，这意味着我们能够检测到 89% 的有效类别 1 实例。剩余的 11% 实例被错误识别。召回率衡量的是在牧场中被识别的类别
    1 花朵的比例。相比之下，精度衡量的是类别 1 预测正确的可能性。
- en: 'It’s worth noting that a maximum recall of 1.0 is trivial to achieve: we simply
    need to label each incoming data point as belonging to Class 1\. We will detect
    all valid instances of Class 1, but this high recall will come at a cost. Precision
    will drop drastically, because all instances of Class 0 and Class 2 will be misidentified
    as belonging to Class 1\. This low precision score equals `M[1][1] / M.sum()`.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，达到最大召回率 1.0 是非常简单的：我们只需将每个 incoming 数据点标记为属于类别 1。我们将检测到所有有效的类别 1 实例，但这样的高召回率将付出代价。精度将大幅下降，因为所有类别
    0 和类别 2 的实例都将被错误地识别为属于类别 1。这种低精度分数等于 `M[1][1] / M.sum()`。
- en: Listing 20.21 Checking precision at a recall of 1.0
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.21 在召回率为 1.0 时检查精度
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this same manner, a maximized precision is worthless if the recall is low.
    Imagine if the Class 1 precision equaled 1.0\. We’d thus have 100% confidence
    that all Class 1 predictions are correct. However, if the corresponding recall
    is too low, most Class 1 instances will be misidentified as belonging to another
    class. Hence, high-level confidence is of little use if the classifier ignores
    most true instances.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，如果召回率低，最大化精度也是没有价值的。想象一下，如果类别 1 的精度等于 1.0。那么，我们将有 100% 的信心认为所有类别 1 的预测都是正确的。然而，如果相应的召回率太低，大多数类别
    1 的实例将被错误地识别为属于另一个类别。因此，如果分类器忽略了大多数真实实例，那么高级别的信心就几乎没有用处。
- en: 'A good predictive model should yield both high precision and high recall. We
    therefore should combine precision and recall into a single score. How can we
    combine the two distinct measures? One obvious solution is to take their average
    by running `(precision + recall) / 2`. Unfortunately, this solution has an unexpected
    drawback. Both precision and recall are fractions: `M[1][1] / M[1].sum()` and
    `M[1][1] / M[:,1].sum()`, respectively. They share the same numerator but have
    different denominators. This is problematic; fractions should only be added if
    their denominators are equal. Thus, the summation required to take the average
    is ill advised. What should we do? Well, we can take the inverse of both precision
    and recall. The inversions will swap each numerator with the denominators, so
    `1 / precision` and `1 / recall` will share an equal denominator of `M[1][1]`.
    These inverted fractions can then be summed. Let’s see what happens when we take
    the average of the inverted metrics.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的预测模型应该产生高精确度和高召回率。因此，我们应该将精确度和召回率合并成一个单一的分数。我们如何将这两个不同的度量合并起来？一个明显的解决方案是运行`(precision
    + recall) / 2`来取它们的平均值。不幸的是，这个解决方案有一个意想不到的缺点。精确度和召回率都是分数：`M[1][1] / M[1].sum()`和`M[1][1]
    / M[:,1].sum()`，分别。它们有相同的分子，但有不同的分母。这是问题所在；分数只有在它们的分母相等时才能相加。因此，取平均所需的求和是不明智的。我们应该怎么办？嗯，我们可以取精确度和召回率的倒数。倒数会交换每个分子和分母，所以`1
    / precision`和`1 / recall`将共享一个相等的分母`M[1][1]`。然后，这些倒数的分数可以被相加。让我们看看当我们取倒数的平均度量时的结果。
- en: Listing 20.22 Taking the mean of the inverted metrics
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.22 取倒数的平均度量
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The average of the inverses is greater than 1.0, but both precision and recall
    have a maximum ceiling of 1.0\. Thus, their aggregation should fall below 1.0\.
    We can guarantee this by inverting the computed average.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 倒数的平均值大于1.0，但精确度和召回率都有一个最大上限为1.0。因此，它们的聚合应该低于1.0。我们可以通过取计算出的平均值的倒数来保证这一点。
- en: Listing 20.23 Taking the inverse of the inverted mean
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.23 取倒数的倒数
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our final aggregated score is 0.88, which lies between the precision of 0.87
    and the recall of 0.89\. Hence, this aggregation is a perfect balance of precision
    and recall. This aggregated metric is called the *f1-measure*, *f1-score*, or,
    commonly, simply the *f-measure*. The f-measure can be computed more directly
    by running `2 * precision * recall / (precision + recall)`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终的聚合分数是0.88，位于精确度0.87和召回率0.89之间。因此，这种聚合是精确度和召回率的完美平衡。这个聚合度量被称为*f1度量*、*f1分数*，或者通常简单地称为*f度量*。f度量可以通过运行`2
    * precision * recall / (precision + recall)`来更直接地计算。
- en: 'Note This inversion of the arithmetic mean of inverse values is called the
    *harmonic mean*. The harmonic mean is intended to measure the central tendency
    of rates, such as velocities. Suppose, for instance, that an athlete runs three
    laps around a one-mile lake. The first lap takes 10 minutes, the next lap takes
    16 minutes, and the final lap takes 20 minutes, so the athlete’s velocities in
    miles per minute are 1 / 10 (0.1), 1 / 16 (0.0625), and 1 / 20 (0.05). The arithmetic
    mean is (0.1 + 0.0625 + 0.05) / 3: approximately 0.071\. However, this value is
    erroneous since diverging denominators are summed. Instead, we should compute
    the harmonic mean, 3 / (10 + 16 + 20), which is approximately 0.065 miles per
    minute. By definition, the f-measure equals the harmonic mean of precision and
    recall.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种逆运算的倒数平均值被称为*调和平均数*。调和平均数旨在衡量比率（如速度）的中心趋势。例如，假设一名运动员在一英里湖边跑了三圈。第一圈用了10分钟，下一圈用了16分钟，最后一圈用了20分钟，因此运动员的每分钟英里数分别是1/10（0.1）、1/16（0.0625）和1/20（0.05）。算术平均数是（0.1
    + 0.0625 + 0.05）/ 3：大约是0.071。然而，这个值是错误的，因为分母在相加时发生了发散。相反，我们应该计算调和平均数，即3 / (10
    + 16 + 20)，这大约是0.065英里/分钟。根据定义，f度量等于精确度和召回率的调和平均数。
- en: Listing 20.24 Computing the f-measure of Class 1
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.24 计算第1类的f度量
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We should note that although in this instance, the f-measure is equal to the
    average of the precision and recall, this is not always the case. Consider a prediction
    that has one true positive, one false positive, and zero false negatives. What
    are the precision and the recall? How does their average compare to the f-measure?
    Let’s check.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，虽然在这个例子中，f度量等于精确度和召回率的平均值，但这并不总是如此。考虑一个预测结果，其中有一个真实正例，一个假正例，以及零个假负例。精确度和召回率是多少？它们的平均值与f度量相比如何？让我们来检查一下。
- en: Listing 20.25 Comparing the f-measure to the average
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.25 比较f度量与平均值
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this theoretical example, the precision is low: 50%. Meanwhile, the recall
    is a perfect 100%. The average value between these two measures is a tolerable
    75%. However, the f-measure is much lower than the average because the high recall
    cannot be justified by the exceptionally low precision value.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个理论示例中，精确度低：50%。同时，召回率是完美的100%。这两个度量之间的平均值是可接受的75%。然而，f度量比平均值低得多，因为高召回率不能由异常低的精确度值来证明。
- en: The f-measure provides us with a robust evaluation for an individual class.
    With this in mind, we’ll now compute the f-measure for each class in our dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: f度量为我们提供了一个对单个类别的稳健评估。考虑到这一点，我们现在将计算数据集中每个类别的f度量。
- en: Listing 20.26 Computing the f-measure for each class
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.26：计算每个类别的f度量
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The f-measure for Class 0 is 1.0: that distinct class can be identified with
    perfect precision and perfect recall. Meanwhile, Class 1 and Class 2 share an
    f-measure of 0.88\. The distinction between these classes is not perfect, and
    one is commonly mistaken for the other. These mistakes degrade the precision and
    recall of each class. Nonetheless, the final f-measure of 0.88 is wholly acceptable.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 类别0的f度量是1.0：这个独特的类别可以以完美的精确度和完美的召回率被识别。同时，类别1和类别2共享一个f度量0.88。这些类别之间的区别并不完美，一个常常被误认为是另一个。这些错误降低了每个类别的精确度和召回率。尽管如此，最终的f度量0.88是完全可接受的。
- en: 'Note There’s no official standard for an acceptable f-measure. Appropriate
    values can vary from problem to problem. But it’s common to treat f-measures like
    exam grades: an f-measure of 0.9 to 1.0 is treated like an A; the model performs
    exceptionally well. An f-measure of 0.8 to 0.89 is treated like a B; there’s room
    for improvement even though the model is acceptable. An f-measure of 0.7 to 0.79
    is treated like a C; the model performs adequately but is not very impressive.
    An f-measure of 0.6 to 0.69 is treated like a *D* ; unacceptable but still better
    than random. F-measure values below 0.6 are usually treated as totally unreliable.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对于可接受的f度量没有官方标准。合适的值可能因问题而异。但通常将f度量视为考试成绩：f度量在0.9到1.0之间被视为A；模型表现非常出色。f度量在0.8到0.89之间被视为B；尽管模型可以接受，但仍有改进的空间。f度量在0.7到0.79之间被视为C；模型表现尚可，但不太令人印象深刻。f度量在0.6到0.69之间被视为*D*；不可接受，但仍然比随机要好。f度量值低于0.6通常被视为完全不可靠。
- en: We computed three f-measures across three different classes. These f-measures
    can be combined into a single score by taking their mean. Listing 20.27 outputs
    that unified f-measure score.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个不同的类别上计算了三个f度量。这些f度量可以通过取平均值合并成一个单一的分数。列表20.27输出了这个统一的f度量分数。
- en: Note Our three f-measures are fractions with potentially different denominators.
    As we’ve discussed, it’s best to combine fractions only when the denominators
    are equal. Unfortunately, unlike with precision and recall, there’s no existing
    method for achieving denominator equality among f-measure outputs. Hence, we have
    no choice but to compute their average if we wish to obtain a unified score.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们的三个f度量是具有可能不同分母的分数。正如我们讨论的，只有在分母相等时才最好合并分数。不幸的是，与精确度和召回率不同，没有现有方法可以在f度量输出之间实现分母相等。因此，如果我们希望获得一个统一的分数，我们别无选择，只能计算它们的平均值。
- en: Listing 20.27 Computing a unified f-measure for all classes
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.27：计算所有类别的统一f度量
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The f-measure of 0.92 is identical to our accuracy. This is not surprising since
    both f-measure and accuracy are intended to measure model performance. However,
    we must emphasize that f-measure and accuracy are not guaranteed to be the same.
    The difference between the metrics is especially noticeable when the classes are
    *imbalanced*. In an imbalanced dataset, there are far more instances of some Class
    A than of some Class B. Let’s consider an example where we have 100 instances
    of Class A and just 1 instance of Class B. Furthermore, let’s suppose our Class
    B predictions have a recall of 100% and a precision of 50%. We can represent this
    scenario with a two-by-two confusion matrix of the form `[[99, 0], [1, 1]]`. Let’s
    compare the accuracy with the unified f-measure for this imbalanced result.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: f-measure 的 0.92 与我们的准确度相同。这并不令人惊讶，因为 f-measure 和准确度都是为了衡量模型性能而设计的。然而，我们必须强调，f-measure
    和准确度并不保证相同。当类别不平衡时，这两个指标之间的差异尤为明显。在一个不平衡的数据集中，某些类别 A 的实例远多于某些类别 B 的实例。让我们考虑一个我们有
    100 个类别 A 的实例和仅 1 个类别 B 的实例的例子。此外，假设我们的类别 B 预测的召回率为 100%，精确度为 50%。我们可以用形式为 `[[99,
    0], [1, 1]]` 的二乘二混淆矩阵来表示这种场景。让我们比较这个不平衡结果的准确度与统一 f-measure。
- en: Listing 20.28 Comparing performance metrics across imbalanced data
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.28 在不平衡数据上比较性能指标
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Our accuracy is nearly 100%. That accuracy is misleading—it doesn’t truly represent
    the terrible precision with which the model predicts the second class. Meanwhile,
    the lower f-measure better reflects the balance between the different class predictions.
    Generally, the f-measure is considered a superior prediction metric due to its
    sensitivity to imbalance. Going forward, we rely on the f-measure to evaluate
    our classifiers.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确度接近 100%。这种准确度具有误导性——它并不能真正代表模型预测第二类时的糟糕精确度。同时，较低的 f-measure 更好地反映了不同类别预测之间的平衡。一般来说，f-measure
    由于其对不平衡的敏感性，被认为是一个更优越的预测指标。因此，我们将继续依靠 f-measure 来评估我们的分类器。
- en: 20.2.1 Scikit-learn’s prediction measurement functions
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 20.2.1 Scikit-learn 的预测测量函数
- en: All the prediction metrics that we’ve discussed thus far are available in scikit-learn.
    They can be imported from the `sklearn.metrics` module. Each metric function takes
    as input `y_pred` and `y_test` and returns the metric criteria of our choice.
    For instance, we can compute the confusion matrix by importing and running `confusion_matrix`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止讨论的所有预测指标都可在 scikit-learn 中找到。它们可以从 `sklearn.metrics` 模块导入。每个指标函数都接受 `y_pred`
    和 `y_test` 作为输入，并返回我们选择的指标标准。例如，我们可以通过导入并运行 `confusion_matrix` 来计算混淆矩阵。
- en: Listing 20.29 Computing the confusion matrix using scikit-learn
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.29 使用 scikit-learn 计算混淆矩阵
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In that same manner, we can compute the accuracy by importing and running `accuracy_score`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，我们可以通过导入并运行 `accuracy_score` 来计算准确度。
- en: Listing 20.30 Computing the accuracy using scikit-learn
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.30 使用 scikit-learn 计算准确度
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Also, the f-measure can be computed with the `f1_score` function. Using this
    function is a bit more nuanced since the f-measure can be returned as a vector
    or unified mean. Passing `average=None` into the function returns a vector of
    individual f-measures for each class.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，f-measure 可以通过 `f1_score` 函数来计算。使用此函数稍微复杂一些，因为 f-measure 可以作为向量或统一平均值返回。将
    `average=None` 传递给函数会返回每个类别的单个 f-measure 向量。
- en: Listing 20.31 Computing all f-measures using scikit-learn
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.31 使用 scikit-learn 计算所有 f-measure
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Meanwhile, passing `average='macro'` returns a single average score.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，传递 `average='macro'` 会返回一个单一的平均分数。
- en: Note Passing `average='micro'` computes the mean precision and mean recall across
    all classes. Then, these mean values are used to compute a single f-measure score.
    Generally, this approach does not significantly impact the final unified f-measure
    result.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：传递 `average='micro'` 会计算所有类别的平均精确度和平均召回率。然后，这些平均值被用来计算一个单一的 f-measure 分数。通常，这种方法不会对最终的统一
    f-measure 结果产生重大影响。
- en: Listing 20.32 Computing a unified f-measure using scikit-learn
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.32 使用 scikit-learn 计算统一 f-measure
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using the `f1_score` function, we can readily optimize our KNN classifier across
    its input parameters.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `f1_score` 函数，我们可以轻松地优化我们的 KNN 分类器在其输入参数上的性能。
- en: Common scikit-learn classifier evaluation functions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 scikit-learn 分类器评估函数
- en: '`M = confusion_matrix(y_pred, y_test)`—Returns the confusion matrix `M` based
    on predicted classes in `y_pred` and the actual classes in `y_test`. Each matrix
    element `M[i][j]` counts the number of times that `y_pred[index] == i` while `y_test[index]
    == j` across every possible `index`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`M = confusion_matrix(y_pred, y_test)`—基于`y_pred`中的预测类别和`y_test`中的实际类别返回混淆矩阵`M`。每个矩阵元素`M[i][j]`计算在所有可能的`index`中`y_pred[index]
    == i`且`y_test[index] == j`的次数。'
- en: '`accuracy_score(y_pred, y_test)`—Returns the accuracy score based on predicted
    classes in `y_pred` and the actual classes in `y_test`. Given the confusion matrix
    `M`, the accuracy score is equal to `M.diagonal().sum() / M.sum()`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`accuracy_score(y_pred, y_test)`—基于`y_pred`中的预测类别和`y_test`中的实际类别返回准确度分数。给定混淆矩阵`M`，准确度分数等于`M.diagonal().sum()
    / M.sum()`。'
- en: '`f_measure_vector = f1_score(y_pred, y_test, average=None)`—Returns a vector
    of f-measures for all possible `f_measure_vector.size` classes. The f-measure
    of Class *i* is equal to `f_measure_vector[i]`. This equals the harmonic mean
    of the precision and recall of Class *i*. Both precision and recall can be computed
    from the confusion matrix `M`. The precision of Class *i* equals `M[i][i] / M[i].sum()`,
    and the recall of Class *i* equals `M[i][i] / M[:,i] .sum()`. The final f-measure
    value `f_measure_vector[i]` equals `2 * precision * recall / (precision + recall)`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f_measure_vector = f1_score(y_pred, y_test, average=None)`—返回所有可能的`f_measure_vector.size`类别的f度量向量。类别*i*的f度量等于`f_measure_vector[i]`。这等于类别*i*的精确度和召回率的调和平均。精确度和召回率都可以从混淆矩阵`M`中计算得出。类别*i*的精确度等于`M[i][i]
    / M[i].sum()`，类别*i*的召回率等于`M[i][i] / M[:,i] .sum()`。最终的f度量值`f_measure_vector[i]`等于`2
    * precision * recall / (precision + recall)`。'
- en: '`f1_score(y_pred, y_test, average=''macro'')`—Returns the average f-measure,
    equal to `f_measure_vector.mean()`.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f1_score(y_pred, y_test, average=''macro'')`—返回平均f度量，等于`f_measure_vector.mean()`。'
- en: 20.3 Optimizing KNN performance
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.3优化KNN性能
- en: 'Currently, our `predict` function takes two input parameters: *K* and `weighted_voting`.
    These parameters must be set before training and influence the classifier’s performance.
    Data scientists refer to such parameters as *hyperparameters*. All machine learning
    models have some hyperparameters that can be tweaked to enhance predictive power.
    Let’s try to optimize our classifier’s hyperparameters by iterating over all possible
    combinations of *K* and `weighted_voting`. Our *K* values range from 1 to `y_train
    .size`, and our Boolean `weighted_voting` parameter is set to `True` or `False`.
    For each hyperparameter combination, we train on `y_train` and compute `y_pred`.
    We then obtain the f-measure based on our predictions. All f-measures are plotted
    relative to the input *K*. We plot two separate curves: one for `weighted_voting
    = True` and another for `weighted_voting = False` (figure 20.7). Finally, we find
    the maximum f-measure in the plot and return its optimized parameters.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的`predict`函数接受两个输入参数：*K*和`weighted_voting`。这些参数必须在训练之前设置，并影响分类器的性能。数据科学家将此类参数称为*超参数*。所有机器学习模型都有一些可以调整以增强预测能力的超参数。让我们通过遍历所有可能的*K*和`weighted_voting`组合来优化我们的分类器超参数。我们的*K*值从1到`y_train
    .size`，我们的布尔`weighted_voting`参数设置为`True`或`False`。对于每个超参数组合，我们在`y_train`上训练并计算`y_pred`。然后根据我们的预测获得f度量。所有f度量都与输入*K*相关。我们绘制两条独立的曲线：一条用于`weighted_voting
    = True`，另一条用于`weighted_voting = False`（图20.7）。最后，我们在图中找到最大的f度量并返回其优化参数。
- en: '![](../Images/20-07.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20-07.png)'
- en: Figure 20.7 A plot of KNN weighted and unweighted voting performance measures
    across a range of input *K* values. F-measure is maximized when *K* is set to
    8\. There’s no significant difference for weighted and unweighted voting for low
    values of *K*. However, unweighted performance starts to degrade when *K* is larger
    than 10.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.7在一系列输入*K*值上KNN加权和无加权投票性能度量的图表。当*K*设置为8时，f度量最大化。对于低值的*K*，加权和无加权的投票没有显著差异。然而，当*K*大于10时，无加权的性能开始下降。
- en: Listing 20.33 Optimizing KNN hyperparameters
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.33优化KNN超参数
- en: '[PRE32]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Tracks the mapping between each parameter combination and the f-measure
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 跟踪每个参数组合与f度量之间的映射
- en: ❷ Computes a KNN prediction for each parameter combination
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对每个参数组合计算KNN预测
- en: ❸ Computes the f-measure for each parameter combination
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算每个参数组合的f度量
- en: ❹ Finds the parameters that maximize the f-measure
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 找到最大化f度量的参数
- en: Performance is maximized when *K* is set to 8 and weighted voting is activated.
    However, there’s no significant difference between the weighted and unweighted
    voting output for that value of *K*. Interestingly, as *K* continues to increase,
    the unweighted f-measure drops rapidly. Meanwhile, the weighted f-measure continues
    to hover at above 90%. Thus, weighted KNN appears to be more stable than the unweighted
    variant.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当*K*设置为8并且激活加权投票时，性能最大化。然而，对于这个值，加权投票和无加权投票的输出之间没有显著差异。有趣的是，随着*K*的继续增加，无加权f-measure迅速下降。同时，加权f-measure继续保持在90%以上。因此，加权KNN似乎比无加权变体更稳定。
- en: 'We gained these insights by exhaustively iterating over all the possible input
    parameters. This exhaustive approach is called a *parameter sweep* or *grid search*.
    A grid search is a simple but effective way to optimize hyperparameters. Though
    it suffers from computational complexity when the parameter count is high, a grid
    search is very easy to parallelize. With enough computing power, a grid search
    can effectively optimize many common machine learning algorithms. Generally, a
    grid search is conducted like this:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过穷举所有可能的输入参数来获得这些见解。这种穷举方法被称为*参数扫描*或*网格搜索*。网格搜索是一种简单但有效的方法来优化超参数。尽管当参数数量高时，它受到计算复杂性的影响，但网格搜索非常容易并行化。有了足够的计算能力，网格搜索可以有效地优化许多常见的机器学习算法。通常，网格搜索是这样进行的：
- en: Select our hyperparameters of interest.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们感兴趣的超参数。
- en: Assign a range of values to each hyperparameter.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个超参数分配一个值范围。
- en: Split our input data into a training set and a validation set. The validation
    set is used to measure the prediction quality. This approach is called *cross-validation*.
    Note that it is possible to split the data further into multiple training and
    validation sets; that way, multiple prediction metrics can be averaged out into
    a single score.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的输入数据分为训练集和验证集。验证集用于衡量预测质量。这种方法被称为*交叉验证*。注意，可以将数据进一步分为多个训练集和验证集；这样，多个预测指标可以平均到一个单一的分数上。
- en: Iterate over all possible hyperparameter combinations.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历所有可能的超参数组合。
- en: At each iteration, train a classifier on the training data using the specified
    hyperparameters.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，使用指定的超参数在训练数据上训练一个分类器。
- en: Measure the classifier’s performance using the validation set.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用验证集来衡量分类器的性能。
- en: Once all iterations are completed, return the hyperparameter combination with
    the highest metric output.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有迭代都完成，返回具有最高指标输出的超参数组合。
- en: Scikit-learn allows us to execute a grid search on all its built-in machine
    learning algorithms. Let’s utilize scikit-learn to run a grid search on KNN.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn允许我们在其所有内置机器学习算法上执行网格搜索。让我们利用scikit-learn在KNN上运行一个网格搜索。
- en: 20.4 Running a grid search using scikit-learn
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.4 使用scikit-learn运行网格搜索
- en: Scikit-learn has built-in logic for running KNN classification. We utilize this
    logic by importing the `KNeighborsClassifier` class.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn具有运行KNN分类的内置逻辑。我们通过导入`KNeighborsClassifier`类来利用这个逻辑。
- en: Listing 20.34 Importing scikit-learn’s KNN class
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.34 导入scikit-learn的KNN类
- en: '[PRE33]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Initializing the class creates a KNN classifier object. Per common convention,
    we store this object in a `clf` variable.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化类创建了一个KNN分类器对象。按照常规惯例，我们将此对象存储在`clf`变量中。
- en: 'Note The KNN algorithm can be extended beyond mere classification: it can be
    modified to predict continuous values. Imagine that we wish to predict the sale
    price of a house. We can do this by averaging the known sales prices for similar
    houses in the neighborhood. In that same way, we construct a KNN regressor that
    predicts a data point’s continuous value by averaging known values of its neighbors.
    Scikit-learn includes a `KNeighborsRegressor` class that is designed for this
    specific purpose.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：KNN算法可以扩展到不仅仅是分类：它可以修改为预测连续值。想象一下，我们希望预测房屋的销售价格。我们可以通过平均相似房屋的已知销售价格来实现这一点。同样，我们构建一个KNN回归器，通过平均其邻居的已知值来预测数据点的连续值。Scikit-learn包括一个`KNeighborsRegressor`类，专为这个特定目的设计。
- en: Listing 20.35 Initializing scikit-learn’s KNN classifier
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.35 初始化scikit-learn的KNN分类器
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The initialized `clf` object has preset specifications for *K* and weighted
    voting. The *K* value is stored in the `clf.n_neighbors` attribute, and the weighted
    voting specifications are stored in the `clf.weights` attribute. Let’s print and
    examine both these attributes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化的`clf`对象为*K*和加权投票预设了规格。*K*值存储在`clf.n_neighbors`属性中，加权投票规格存储在`clf.weights`属性中。让我们打印并检查这两个属性。
- en: Listing 20.36 Printing the preset KNN parameters
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.36 打印预设的KNN参数
- en: '[PRE35]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Our *K* is set to 5, and weighted voting is set to `uniform`, indicating that
    all votes are weighted equally. Passing `weights='distance'` into the initialization
    function ensures that votes are weighted by distance. Additionally, passing `n_neighbors=4`
    sets *K* to 4\. Let’s reinitialize `clf` with these parameters.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的*K*设置为5，加权投票设置为`uniform`，表示所有投票权重相等。将`weights='distance'`传递给初始化函数确保投票按距离加权。此外，传递`n_neighbors=4`将*K*设置为4。让我们用这些参数重新初始化`clf`。
- en: Listing 20.37 Setting scikit-learn’s KNN parameters
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.37 设置scikit-learn的KNN参数
- en: '[PRE36]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now we want to train our KNN model. Any scikit-learn `clf` classifier can be
    trained using the `fit` method. We simply need to execute `clf.fit(X, y)`, where
    `X` is a feature matrix and `y` is a class-label array. Let’s train the classifier
    using the training set defined by `X_train` and `y_train`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要训练我们的KNN模型。任何scikit-learn `clf`分类器都可以使用`fit`方法进行训练。我们只需执行`clf.fit(X, y)`，其中`X`是一个特征矩阵，`y`是一个类别标签数组。让我们使用由`X_train`和`y_train`定义的训练集来训练分类器。
- en: Listing 20.38 Training scikit-learn’s KNN classifier
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.38 训练scikit-learn的KNN分类器
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: After training, `clf` can predict the classes of any input `X_test` matrix (whose
    dimensions match `X_train`). Predictions are carried out with the `clf.predict`
    method. Running `clf.predict(X_test)` returns a `y_pred` prediction array. Subsequently,
    `y_pred` together with `y_test` can be used to calculate the f-measure.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，`clf`可以预测任何输入`X_test`矩阵（其维度与`X_train`匹配）的类别。预测是通过`clf.predict`方法进行的。运行`clf.predict(X_test)`返回一个`y_pred`预测数组。随后，`y_pred`与`y_test`一起可用于计算f-measure。
- en: Listing 20.39 Predicting classes with a trained KNN classifier
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.39 使用训练好的KNN分类器进行类别预测
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`clf` also allows us to extract more nuanced prediction outputs. For instance,
    we can generate the fraction of the votes received by each class for an inputted
    sample in `X_test`. To obtain this voting distribution, we need to run `clf.predict_proba(X_test)`.
    The `predict_proba` method returns a matrix whose columns correspond to vote ratios.
    Here we print the first four rows of this matrix, which correspond to `X_test[:5]`.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`clf`还允许我们提取更细致的预测输出。例如，我们可以生成输入样本在`X_test`中每个类别收到的投票比例。要获得这个投票分布，我们需要运行`clf.predict_proba(X_test)`。`predict_proba`方法返回一个矩阵，其列对应于投票比率。这里我们打印这个矩阵的前四行，它们对应于`X_test[:5]`。'
- en: Listing 20.40 Outputting vote ratios for each class
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.40 输出每个类别的投票比例
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As we can see, the data point at `X_test[0]` received 78.5% of votes for Class
    2\. The rest of the votes were given to Class 1\. Meanwhile, `X_test[4]` received
    a full 100% of votes for Class 2\. Even though both data points are assigned a
    class label of 2, the second point is assigned that label with a higher degree
    of confidence.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，数据点`X_test[0]`获得了类别2的78.5%的投票。其余的投票给了类别1。同时，`X_test[4]`获得了类别2的100%的投票。尽管这两个数据点都被分配了类别标签2，但第二个点分配该标签的置信度更高。
- en: It’s worth noting that all scikit-learn classifiers include their own version
    of `predict_proba`. The method returns an estimated probability distribution of
    data points belonging to some class. The column index with the highest probability
    is equal to the class label in `y_pred`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，所有scikit-learn分类器都包含它们自己的`predict_proba`版本。该方法返回数据点属于某个类别的估计概率分布。概率最高的列索引等于`y_pred`中的类别标签。
- en: Relevant scikit-learn classifier methods
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的scikit-learn分类器方法
- en: '`clf = KNeighborsClassifier()`—Initializes a KNN classifier where *K* = 5 and
    voting is uniform across the five nearest neighbors.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = KNeighborsClassifier()`—初始化一个*K* = 5且在五个最近邻中投票均匀的KNN分类器。'
- en: '`clf = KNeighborsClassifier(n_neighbors=x)`—Initializes a KNN classifier where
    *K* = `x` and voting is uniform across the `x` neighbors.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = KNeighborsClassifier(n_neighbors=x)`—初始化一个*K* = `x`且在`x`个邻居中投票均匀的KNN分类器。'
- en: '`clf = KNeighborsClassifier(n_neighbors=x, weights=''distance'')`—Initializes
    a KNN classifier where *K* = `x` and voting is weighted by distance to each of
    the `x` neighbors.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = KNeighborsClassifier(n_neighbors=x, weights=''distance'')`—初始化一个 KNN
    分类器，其中 *K* = `x`，投票按距离每个 `x` 个邻居进行加权。'
- en: '`clf.fit(X_train, y_train)`—Fits any classifier `clf` to predict classes `y`
    from features `X` based on training features `X_train` and training labeled classes
    `y_train`.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.fit(X_train, y_train)`—将任何分类器 `clf` 调整以根据训练特征 `X_train` 和训练标签类 `y_train`
    预测类别 `y`。'
- en: '`y = clf.predict(X)`—Predicts an array of classes associated with the feature
    matrix `X`. Each predicted class `y[i]` maps the matrix feature row `X[i]`.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y = clf.predict(X)`—预测与特征矩阵 `X` 相关的类别数组。每个预测类别 `y[i]` 将矩阵特征行 `X[i]` 映射到。'
- en: '`M = clf.predict_proba(X)`—Returns a matrix `M` of probability distributions.
    Each row `M[i]` represents the probability distribution of data point `i` belonging
    to some class. The class prediction of that data point equals the distribution’s
    maximum value. More concisely, `M[i].argmax() == clf.predict(X)[i]`.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`M = clf.predict_proba(X)`—返回一个概率分布矩阵 `M`。矩阵的每一行 `M[i]` 代表数据点 `i` 属于某个类的概率分布。该数据点的类别预测等于分布的最大值。更简洁地说，`M[i].argmax()
    == clf.predict(X)[i]`。'
- en: Now, let’s turn our attention to running a grid search across `KNeighborsClassifier`.
    First we need to specify a dictionary mapping between our hyperparameters and
    their value ranges. The dictionary keys equal our input parameters `n_neighbors`
    and `weights`. The dictionary values equal the respective iterables, `range(1,
    40)`, and `['uniform', 'distance']`. Let’s create this `hyperparams` dictionary.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向在 `KNeighborsClassifier` 上运行网格搜索。首先，我们需要指定一个将超参数及其值范围映射的字典。字典键等于我们的输入参数
    `n_neighbors` 和 `weights`。字典值等于相应的可迭代对象，`range(1, 40)` 和 `['uniform', 'distance']`。让我们创建这个
    `hyperparams` 字典。
- en: Listing 20.41 Defining a hyperparameter dictionary
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.41 定义超参数字典
- en: '[PRE40]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ In our manual grid search, the neighbor count ranged from 1 to y_train.size,
    where y_train.size equaled 37\. However, that parameter range can be set to any
    arbitrary value. Here, we set the range cutoff to 40, which is a nice round number.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在我们的手动网格搜索中，邻居数量从 1 到 y_train.size，其中 y_train.size 等于 37。然而，该参数范围可以设置为任何任意值。在这里，我们将范围截止值设置为
    40，这是一个很好的整数值。
- en: Next, we need to import scikit-learn’s `GridSearchCV` class, which we’ll use
    to execute the grid search.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要导入 scikit-learn 的 `GridSearchCV` 类，我们将使用它来执行网格搜索。
- en: Listing 20.42 Importing scikit-learn’s grid search class
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.42 导入 scikit-learn 的网格搜索类
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'It’s time to initialize the `GridSearchCV` class. We input three parameters
    into the initializing method. The first parameter is `KNeighborsClassifier()`:
    an initialized scikit-learn object whose hyperparameters we wish to optimize.
    Our second input is the `hyperparams` dictionary. Our final input is `scoring=''f1_macro''`,
    which sets the evaluation metric to the averaged f-measure value.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候初始化 `GridSearchCV` 类了。我们将三个参数输入到初始化方法中。第一个参数是 `KNeighborsClassifier()`：一个初始化的
    scikit-learn 对象，我们希望优化其超参数。我们的第二个输入是 `hyperparams` 字典。我们的最后一个输入是 `scoring='f1_macro'`，这会将评估指标设置为平均
    f-measure 值。
- en: The following code executes `GridSearchCV(KNeighborsClassifier(), hyperparams,
    scoring='f1_macro')`. The initialized object can perform classification, so we
    assign it to the variable `clf_grid`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码执行 `GridSearchCV(KNeighborsClassifier(), hyperparams, scoring='f1_macro')`。初始化的对象可以执行分类，因此我们将其赋值给变量
    `clf_grid`。
- en: Listing 20.43 Initializing scikit-learn’s grid search class
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.43 初始化 scikit-learn 的网格搜索类
- en: '[PRE42]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We’re ready to run grid search on our fully labeled dataset `X, y`. Running
    `clf_grid.fit(X, y)` executes this parameter sweep. Scikit-learn’s internal methods
    automatically split `X` and `y` during the validation process.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好在我们的完全标记的数据集 `X, y` 上运行网格搜索。执行 `clf_grid.fit(X, y)` 执行此参数扫描。Scikit-learn
    的内部方法在验证过程中自动拆分 `X` 和 `y`。
- en: Listing 20.44 Running a grid search using scikit-learn
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.44 使用 scikit-learn 运行网格搜索
- en: '[PRE43]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We’ve executed the grid search. The optimized hyperparameters are stored in
    the `clf_grid.best_params_` attribute, and the f-measure associated with these
    parameters is stored in `clf_grid.best_score_`. Let’s output these results.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经执行了网格搜索。优化的超参数存储在 `clf_grid.best_params_` 属性中，与这些参数相关的 f-measure 存储在 `clf_grid.best_score_`
    中。让我们输出这些结果。
- en: Listing 20.45 Checking the optimized grid search results
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.45 检查优化的网格搜索结果
- en: '[PRE44]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Scikit-learn’s grid search achieved an f-measure of 0.99\. This value is higher
    than our custom grid search output of 0.96\. Why is it higher? Well, scikit-learn
    has carried out a more sophisticated version of cross-validation. Rather than
    splitting the dataset into two parts, it split the data into five equal parts.
    Each individual data partition served as a training set, and the data outside
    each partition was used for testing. The five f-scores across the five training
    sets were computed and averaged. The final mean value of 0.99 represents a more
    accurate estimation of classifier performance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的网格搜索实现了0.99的f-measure。这个值高于我们自定义网格搜索的0.96输出。为什么更高呢？因为scikit-learn执行了一个更复杂的交叉验证版本。它不是将数据集分成两部分，而是将数据分成五个相等的部分。每个数据分区作为训练集，每个分区外的数据用于测试。计算了五个训练集的五个f-score并取平均值。最终的0.99平均值代表了对分类器性能的更准确估计。
- en: Note Splitting the data into five parts for evaluation purposes is called *5-fold
    cross-validation*. Generally, we can split the data into *K* equal parts. In `GridSearchCV`,
    the splitting is controlled by the `cv` parameter. Passing `cv = 2` splits the
    data into two parts, and the final f-measure resembles our original value of 0.96.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：将数据分成五部分用于评估称为*5折交叉验证*。通常，我们可以将数据分成*K*个相等的部分。在`GridSearchCV`中，分割由`cv`参数控制。传递`cv
    = 2`将数据分成两部分，最终的f-measure类似于我们原始的0.96值。
- en: Maximized performance is achieved when `n_neighbors` is set to 10 and weighted
    voting is activated. The actual KNN classifier containing these parameters is
    stored in the `clf_grid.best_estimator_` attribute.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 当`n_neighbors`设置为10并且激活加权投票时，达到最大性能。包含这些参数的实际KNN分类器存储在`clf_grid.best_estimator_`属性中。
- en: Note Multiple hyperparameter combinations lead to an f-measure of 0.99\. The
    chosen combination may vary across different machines. Thus, your parameter outputs
    may be slightly different even though the optimized f-measure will remain the
    same.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：多个超参数组合可能导致f-measure达到0.99。所选组合可能因不同机器而异。因此，即使优化的f-measure保持不变，你的参数输出也可能略有不同。
- en: Listing 20.46 Accessing the optimized classifier
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.46 访问优化后的分类器
- en: '[PRE45]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: By using `clf_best`, we can carry out predictions on new data. Alternatively,
    we can carry out predictions directly with our optimized `clf_grid` object by
    running `clf_grid.predict`. Both objects return identical results.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`clf_best`，我们可以在新数据上执行预测。或者，我们可以通过运行`clf_grid.predict`直接使用我们的优化`clf_grid`对象进行预测。这两个对象返回相同的结果。
- en: Listing 20.47 Generating predictions with `clf_grid`
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列表20.47 使用`clf_grid`生成预测
- en: '[PRE46]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Relevant scikit-learn grid search methods and attributes
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的scikit-learn网格搜索方法和属性
- en: '`clf_grid = GridSearchCV(ClassifierClass(), hyperparams, scoring = scoring_metric)`—Creates
    a grid search object intended to optimize classifier prediction across all possible
    hyperparameters based on a scoring metric specified by `scoring`. If `ClassifierClass()`
    is equal to `KNeighborsClassifier()`, then `clf_grid` serves to optimize `KNN`.
    If the `scoring_metric` is equal to `f1_macro`, the average f-measure is utilized
    for optimization.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf_grid = GridSearchCV(ClassifierClass(), hyperparams, scoring = scoring_metric)`—创建一个网格搜索对象，旨在根据由`scoring`指定的评分指标优化分类器在所有可能的超参数上的预测。如果`ClassifierClass()`等于`KNeighborsClassifier()`，则`clf_grid`用于优化`KNN`。如果`scoring_metric`等于`f1_macro`，则用于优化的平均f-measure。'
- en: '`clf_grid.fit(X, y)`—Executes a grid search to optimize classifier performance
    across all possible combinations of hyperparameter values.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf_grid.fit(X, y)`—执行网格搜索以优化所有可能的超参数值组合的分类器性能。'
- en: '`clf_grid.best_score_`—Returns the optimal measure of classifier performance
    after a grid search has been executed.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf_grid.best_score_`—执行网格搜索后，返回分类器性能的最佳度量。'
- en: '`clf_grid.best_params_`—Returns the combination of hyperparameters that leads
    to optimal performance based on the grid search.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf_grid.best_params_`—返回基于网格搜索导致最佳性能的超参数组合。'
- en: '`clf_best = clf_grid.best_estimator_`—Returns a scikit-learn classifier object
    that shows optimal performance based on a grid search.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf_best = clf_grid.best_estimator_`—返回一个基于网格搜索显示最佳性能的scikit-learn分类器对象。'
- en: '`clf_grid.predict(X)`—A shortcut to execute `clf_grid.best_estimator_ .predict(X)`.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf_grid.predict(X)`—执行`clf_grid.best_estimator_ .predict(X)`的快捷方式。'
- en: 20.5 Limitations of the KNN algorithm
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 20.5 KNN算法的局限性
- en: 'KNN is the simplest of all supervised learning algorithms. That simplicity
    leads to certain flaws. Unlike other algorithms, KNN is not interpretable: we
    can predict the class and inputted data point, but we cannot comprehend why that
    data point belongs to that class. Suppose we train a KNN model that predicts whether
    a high school student belongs to 1 of 10 possible social cliques. Even if the
    model is accurate, we still can’t understand why the student is classified as
    a jock and not as a member of the glee club. Later, we’ll encounter other algorithms
    that can be used to better understand how data features relate to class identity.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是所有监督学习算法中最简单的。这种简单性导致了一些缺陷。与其他算法不同，KNN 是不可解释的：我们可以预测类别和输入的数据点，但我们无法理解为什么这个数据点属于那个类别。假设我们训练一个
    KNN 模型来预测一个高中生是否属于 10 个可能的社会小团体中的 1 个。即使模型是准确的，我们仍然无法理解为什么这个学生被归类为运动员而不是校合唱团成员。稍后，我们将遇到其他算法，这些算法可以更好地理解数据特征与类别身份之间的关系。
- en: Additionally, KNN only works well when the feature count is low. As the number
    of features increases, potentially redundant information begins to creep into
    the data. Hence, the distance measures become less reliable, and the prediction
    quality suffers. Fortunately, feature redundancy can partially be alleviated by
    the dimension-reduction techniques introduced in section 14\. But even with the
    proper application of these techniques, large feature sets can still lead to less
    accurate predictions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，KNN 只在特征数量较少时表现良好。随着特征数量的增加，可能冗余的信息开始进入数据。因此，距离度量变得不那么可靠，预测质量受到影响。幸运的是，可以通过第
    14 节中介绍的降维技术部分缓解特征冗余。但是，即使正确应用这些技术，大型特征集仍然可能导致预测不够准确。
- en: Finally, the biggest problem with KNN is its speed. The algorithm can be very
    slow to run when the training set is large. Suppose we build a training set with
    a million labeled flowers. Naively, finding the nearest neighbors of an unlabeled
    flower would require us to scan its distance to each of the million flowers. This
    will take a lot of time. Of course, we can optimize for speed by organizing the
    training data more efficiently. The process is analogous to organizing words in
    a dictionary. Imagine we want to look up the word *data* in an unalphabetized
    dictionary. Since words are stored haphazardly, we need to scan each page. In
    the 6,000-page Oxford dictionary, this would take a very long time. Fortunately,
    all dictionaries are alphabetized, so we quickly look up the word by flipping
    open the dictionary at approximately its middle point. Here, at page 3,000, we
    encounter the letters *M* and *N*. Then we can flip the pages to the halfway point
    between page 3,000 and the inside cover; this takes us to page 1,500, which should
    contain words with the letter *D*. We’re thus much closer to our goal. Repeating
    this process several more times will take us to the word *data*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，KNN 的最大问题是其速度。当训练集很大时，算法运行可能会非常慢。假设我们构建了一个包含一百万个标记花朵的训练集。直观地，找到未标记花朵的最近邻需要我们扫描它与一百万朵花之间的距离。这将花费很多时间。当然，我们可以通过更有效地组织训练数据来优化速度。这个过程类似于组织字典中的单词。想象一下，我们想在未按字母顺序排列的字典中查找单词
    *data*。由于单词是随意存储的，我们需要扫描每一页。在 6,000 页的牛津字典中，这将花费很长时间。幸运的是，所有字典都是按字母顺序排列的，所以我们可以通过翻到大约中间的位置快速查找单词。在这里，在第
    3,000 页，我们遇到了字母 *M* 和 *N*。然后我们可以翻到第 3,000 页和内封之间的中间点；这带我们到第 1,500 页，应该包含以字母 *D*
    开头的单词。因此，我们离目标更近了。重复这个过程几次后，我们将到达单词 *data*。
- en: In a similar manner, we can quickly scan nearest neighbors if we first organize
    the training set by spatial distance. Scikit-learn employs a special data structure
    called a *K-D tree* to ensure that proximate training points are stored more closely
    to each other. This leads to faster scanning and quicker neighbor lookup. The
    details of K-D tree construction are beyond the scope of this book, but you’re
    encouraged to read Manning’s *Advanced Algorithms and Data Structures* by Marcello
    La Rocca to learn more about this very useful technique (2021, [www.manning.com/books/algorithms-and-data-structures-in-action](http://www.manning.com/books/algorithms-and-data-structures-in-action)).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，如果我们首先按空间距离组织训练集，我们可以快速扫描最近邻。Scikit-learn 使用一种特殊的数据结构，称为 *K-D 树*，以确保邻近的训练点存储得更近。这导致扫描更快，查找邻居更快。K-D
    树构建的细节超出了本书的范围，但鼓励您阅读 Marcello La Rocca 的 *Advanced Algorithms and Data Structures*（2021，[www.manning.com/books/algorithms-and-data-structures-in-action](http://www.manning.com/books/algorithms-and-data-structures-in-action)）以了解更多关于这种非常有用的技术。
- en: Despite the built-in lookup optimization, as we mentioned, KNN can still be
    slow to run when the training set is large. The reduction is especially cumbersome
    during hyperparameter optimization. We’ll illustrate this slowdown by increasing
    the elements in our training set `(X, y)` 2,000-fold. Then we’ll time the grid
    search for the expanded data.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管内置的查找优化，正如我们提到的，当训练集很大时，KNN 仍然可能运行缓慢。在超参数优化期间，这种减少尤其繁琐。我们将通过将训练集 `(X, y)`
    中的元素增加 2,000 倍来展示这种减速。然后我们将计时扩展数据的网格搜索。
- en: Warning The following code will take a long time to run.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 以下代码运行时间将很长。
- en: Listing 20.48 Optimizing KNN on a large training set
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 20.48 在大型训练集上优化 KNN
- en: '[PRE47]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Our grid search took over 16 minutes to run! This is not an acceptable running
    time. We need an alternative solution. In the following section, we explore new
    classifiers whose prediction running time is not dependent on the training set
    size. We develop these classifiers from commonsense first principles and then
    we utilize their scikit-learn implementations.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网格搜索运行了超过 16 分钟！这不是可接受的运行时间。我们需要一个替代方案。在下一节中，我们将探讨新的分类器，其预测运行时间不依赖于训练集的大小。我们首先从常识性原理出发开发这些分类器，然后利用它们的
    scikit-learn 实现。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In *supervised machine learning*, our goal is to find a mapping between inputted
    measurements called *features* and outputted categories called *classes*. A model
    that identifies classes based on features is called a *classifier*.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 *监督式机器学习* 中，我们的目标是找到输入测量值（称为 *特征*）和输出类别（称为 *类别*）之间的映射。基于特征识别类别的模型称为 *分类器*。
- en: To construct a classifier, we first require a dataset with both features and
    labeled classes. This dataset is called a *training set*.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要构建一个分类器，我们首先需要一个包含特征和标记类别的数据集。这个数据集称为 *训练集*。
- en: One very simple classifier is *K-nearest neighbors* (KNN). KNN can classify
    an unlabeled point based on the plurality class among the *K*-nearest labeled
    points in the training set. Essentially, these neighbors vote to decide the unknown
    classes. Optionally, the voting can be weighted based on the distance of the neighbors
    to the unlabeled point.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个非常简单的分类器是 *K 近邻*（KNN）。KNN 可以根据训练集中 *K* 个最近标记点的多数类别对未标记点进行分类。本质上，这些邻居投票来决定未知类别。可选地，投票可以根据邻居到未标记点的距离进行加权。
- en: We can evaluate the performance of a classifier by computing a *confusion matrix*,
    `M`. Each diagonal element `M[i][i]` tracks the number of accurately predicted
    instances of class `i`. Such accurate predictions are called the *true positive*
    instances of a class. The fraction of total elements along the diagonal of `M`
    is the *accuracy score*.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过计算 *混淆矩阵* `M` 来评估分类器的性能。矩阵 `M` 的每个对角元素 `M[i][i]` 跟踪类别 `i` 的准确预测实例数量。这种准确的预测被称为该类别的
    *真阳性* 实例。`M` 对角线上元素的总数与 `M` 对角线上元素总数的比例是 *准确率*。
- en: Predicted Class A elements that actually belong to Class B are called the *false
    positives* of Class A. Dividing the true positive count by the sum of true positives
    and false positives produces a metric called *precision*. A low precision indicates
    that the predicted class label is not very reliable.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际属于类别 B 但被预测为类别 A 的元素被称为类别 A 的 *假阳性*。将真阳性数量除以真阳性数量和假阳性数量的总和，得到一个称为 *精度* 的指标。低精度表明预测的类别标签不太可靠。
- en: Actual Class A elements that are predicted to belong to Class B are called the
    *false negatives* of Class A. Dividing the true positive count by the sum of true
    positives and false negatives produces a metric known as *recall*. A low recall
    indicates that our predictor commonly misses valid instances of a class.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际属于A类但预测属于B类的元素被称为A类的*假阴性*。将真正例的数量除以真正例和假阴性的总和产生一个称为*召回率*的指标。低召回率表明我们的预测器通常会错过某个类别的有效实例。
- en: A good classifier should yield both high precision and high recall. We can combine
    precision and recall into a single metric called the *f-measure*. Given precision
    `p` and recall `r`, we can compute the f-measure by running `2 * p * r / (p +
    r)`. Multiple f-measures across multiple classes can be averaged into a single
    score.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个好的分类器应该同时具有高精确度和高召回率。我们可以将精确度和召回率结合成一个单一的指标，称为*f度量*。给定精确度`p`和召回率`r`，我们可以通过计算`2
    * p * r / (p + r)`来得到f度量。可以将多个类别的多个f度量平均到一个单一的得分上。
- en: The f-measure can sometimes be superior to the accuracy, especially when data
    is imbalanced, so it’s the preferable evaluation metric.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: f度量有时可能优于准确度，尤其是在数据不平衡的情况下，因此它是首选的评估指标。
- en: To optimize KNN performance, we need to choose an optimal value for *K*. We
    also need to decide whether to utilize weighted voting. These two parameterized
    inputs are called *hyperparameters*. Such hyperparameters must be set before training.
    All machine learning models have hyperparameters that can be tweaked to enhance
    predictive power.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了优化KNN的性能，我们需要选择一个最佳的*K*值。我们还需要决定是否使用加权投票。这两个参数化的输入被称为*超参数*。这些超参数必须在训练之前设置。所有机器学习模型都有可以调整以增强预测能力的超参数。
- en: The simplest hyperparameter optimization technique is called a *grid search*,
    which is conducted by iterating over every possible hyperparameter combination.
    Before the iterations, the original dataset is split into a training set and a
    validation set. This splitting is referred to as *cross-validation*. Then we iterate
    over the parameters. At each iteration, the classifier is trained and evaluated.
    Finally, we choose the hyperparameter values that lead to the highest metric output.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最简单的超参数优化技术称为*网格搜索*，它通过遍历每个可能的超参数组合来实现。在迭代之前，原始数据集被分成训练集和验证集。这种分割被称为*交叉验证*。然后我们遍历参数。在每次迭代中，对分类器进行训练和评估。最后，我们选择导致最高指标输出的超参数值。
