- en: Chapter 7\. Geospatial and Temporal Data Analysis on Taxi Trip Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。出租车行程数据的地理空间和时间数据分析
- en: Geospatial data refers to data that has location information embedded in it
    in some form. Such data is being generated currently at a massive scale by billions
    of sources, such as mobile phones and sensors, every day. Data about movement
    of humans and machines, and from remote sensing, is significant for our economy
    and general well-being. Geospatial analytics can provide us with the tools and
    methods we need to make sense of all that data and put it to use in solving problems
    we face.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 地理空间数据指的是数据中嵌入有某种形式的位置信息。这类数据每天由数十亿来源（如手机和传感器）大规模生成。关于人类和机器移动的数据以及来自遥感的数据对我们的经济和整体福祉至关重要。地理空间分析可以为我们提供处理所有这些数据并将其用于解决面临问题的工具和方法。
- en: The PySpark and PyData ecosystems have evolved considerably over the last few
    years when it comes to geospatial analysis. They are being used across industries
    for handling location-rich data and, in turn, impacting our daily lives. One daily
    activity where geospatial data manifests itself in a visible way is local transport.
    The phenomenon of digital cab hailing services becoming popular over the last
    few years has led to us being more aware of geospatial technology. In this chapter,
    we’ll use our PySpark and data analysis skills in this domain as we work with
    a dataset containing information about trips taken by cabs in New York City.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在地理空间分析方面，PySpark 和 PyData 生态系统在过去几年中发生了显著发展。它们被各行各业用来处理富有位置信息的数据，并对我们的日常生活产生影响。地方交通是一个可以明显看到地理空间数据应用的日常活动领域。过去几年中数字打车服务的流行使我们更加关注地理空间技术。在本章中，我们将利用我们在该领域中的
    PySpark 和数据分析技能来处理一个数据集，该数据集包含有关纽约市出租车行程的信息。
- en: 'One statistic that is important to understanding the economics of taxis is
    *utilization*: the fraction of time that a cab is on the road and is occupied
    by one or more passengers. One factor that impacts utilization is the passenger’s
    destination: a cab that drops off passengers near Union Square at midday is much
    more likely to find its next fare in just a minute or two, whereas a cab that
    drops someone off at 2 A.M. on Staten Island may have to drive all the way back
    to Manhattan before it finds its next fare. We’d like to quantify these effects
    and find out the average time it takes for a cab to find its next fare as a function
    of the borough in which it dropped its passengers off—Manhattan, Brooklyn, Queens,
    the Bronx, Staten Island, or none of the above (e.g., if it dropped the passenger
    off somewhere outside of the city, like Newark Liberty International Airport).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 了解出租车经济的一个重要统计数据是*利用率*：出租车在道路上被一名或多名乘客占用的时间比例。影响利用率的一个因素是乘客的目的地：在正午将乘客送至联合广场附近的出租车很可能在一两分钟内找到下一个乘客，而在凌晨2点将乘客送至史泰登岛的出租车可能需要驾驶回曼哈顿才能找到下一个乘客。我们希望量化这些影响，并找出出租车在其将乘客卸下的区域（曼哈顿、布鲁克林、皇后区、布朗克斯、史泰登岛或其他地方，如纽瓦克自由国际机场）找到下一个乘客的平均时间。
- en: We’ll start by setting up our dataset, and then we’ll dive into geospatial analysis.
    We will learn about the GeoJSON format and use tools from the PyData ecosystem
    in combination with PySpark. We’ll use GeoPandas for working with *geospatial
    information*, like points of longitude and latitude and spatial boundaries. To
    wrap things up, we will work with temporal features of our data, such as date
    and time, by performing a type of analysis called sessionization. This will help
    us understand utilization of New York City cabs. PySpark’s DataFrame API provides
    out-of-the-box data types and methods to handle temporal data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从设置数据集开始，然后深入进行地理空间分析。我们将学习关于 GeoJSON 格式的知识，并结合 PyData 生态系统中的工具与 PySpark
    使用。我们将使用 GeoPandas 来处理*地理空间信息*，如经度和纬度点以及空间边界。最后，我们将通过执行会话化类型的分析来处理数据的时间特征，比如日期和时间。这将帮助我们了解纽约市出租车的利用情况。PySpark
    的 DataFrame API 提供了处理时间数据的内置数据类型和方法。
- en: Let’s get going by downloading our dataset and exploring it using PySpark.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过下载数据集并使用 PySpark 进行探索来开始吧。
- en: Preparing the Data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'For this analysis, we’re only going to consider the fare data from January
    2013, which will be about 2.5 GB of data after we uncompress it. You can access
    the [data for each month of 2013](https://oreil.ly/7m7Ki), and if you have a sufficiently
    large PySpark cluster at your disposal, you can re-create the following analysis
    against all of the data for the year. For now, let’s create a working directory
    on our client machine and take a look at the structure of the fare data:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此分析，我们只考虑2013年1月的票价数据，解压后大约为2.5 GB数据。您可以访问[2013年每个月的数据](https://oreil.ly/7m7Ki)，如果您有一个足够大的PySpark集群可供使用，可以对整年的数据重新进行以下分析。现在，让我们在客户机上创建一个工作目录，并查看票价数据的结构：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each row of the file after the header represents a single taxi ride in CSV format.
    For each ride, we have some attributes of the cab (a hashed version of the medallion
    number) as well as the driver (a hashed version of the *hack license*, which is
    what a license to drive a taxi is called), some temporal information about when
    the trip started and ended, and the longitude/latitude coordinates for where the
    passengers were picked up and dropped off.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文件头后的每一行表示CSV格式中的单个出租车行程。对于每次行程，我们有一些有关出租车的属性（中介牌号的散列版本）以及驾驶员的信息（出租车驾驶执照的散列版本，这就是出租车驾驶许可证的称呼），有关行程何时开始和结束的一些时间信息，以及乘客上下车的经度/纬度坐标。
- en: 'Let’s create a *taxidata* directory and copy the trip data into the storage:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个*taxidata*目录，并将行程数据复制到存储中：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have used a local filesystem here, but this may not be the case for you.
    It’s more likely nowadays to use a cloud native filesystem such as AWS S3 or GCS.
    In such a scenario, you will upload the data to S3 or GCS, respectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们使用了本地文件系统，但您可能不是这种情况。现在更常见的是使用云原生文件系统，如AWS S3或GCS。在这种情况下，您需要分别将数据上传到S3或GCS。
- en: 'Now start the PySpark shell:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始PySpark shell：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the PySpark shell has loaded, we can create a dataset from the taxi data
    and examine the first few lines, just as we have in other chapters:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦PySpark shell加载完成，我们就可以从出租车数据创建一个数据集，并检查前几行，就像我们在其他章节中所做的那样：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This looks like a well-formatted dataset at first glance. Let’s have a look
    at the DataFrame’s schema:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 乍看之下，这看起来是一个格式良好的数据集。让我们再次查看DataFrame的架构：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We are representing the `pickup_datetime` and `dropoff_datetime` fields as `Strings`
    and storing the individual `(x,y)` coordinates of the pickup and drop-off locations
    in their own fields as `Doubles`. We want the datetime fields as timestamps since
    that will allow us to manipulate and analyze them conveniently.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`pickup_datetime`和`dropoff_datetime`字段表示为`Strings`，并将接送地点的个体`(x,y)`坐标存储在其自己的`Doubles`字段中。我们希望将日期时间字段转换为时间戳，因为这样可以方便地进行操作和分析。
- en: Converting Datetime Strings to Timestamps
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将日期时间字符串转换为时间戳
- en: As mentioned previously, PySpark provides out-of-the-box methods for handling
    temporal data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PySpark提供了处理时间数据的开箱即用方法。
- en: 'Specifically, we will use the `to_timestamp` function to parse the datetime
    strings and convert them into timestamps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用`to_timestamp`函数解析日期时间字符串并将其转换为时间戳：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s have a look at the schema again:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次查看架构：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `pickup_datetime` and `dropoff_datetime` fields are timestamps now. Well
    done!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`pickup_datetime`和`dropoff_datetime`字段都是时间戳了。干得好！
- en: 'We’d mentioned that this dataset contains trips from January 2013\. Don’t just
    take our word for this, though. We can confirm this by sorting the `pickup_datetime`
    field to get the latest datetime in the data. For this, we use DataFrame’s `sort`
    method combined with PySpark column’s `desc` method:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到这个数据集包含2013年1月的行程。不过，不要只听我们的话。我们可以通过对`pickup_datetime`字段进行排序来确认数据中的最新日期时间。为此，我们使用DataFrame的`sort`方法结合PySpark列的`desc`方法：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With our data types in place, let’s check if there are any inconsistencies in
    our data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保数据类型正确后，让我们检查数据中是否存在任何不一致之处。
- en: Handling Invalid Records
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理无效记录
- en: Anyone who has been working with large-scale, real-world datasets knows that
    they invariably contain at least a few records that do not conform to the expectations
    of the person who wrote the code to handle them. Many PySpark pipelines have failed
    because of invalid records that caused the parsing logic to throw an exception.
    When performing interactive analysis, we can get a sense of potential anomalies
    in the data by focusing on key variables.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 任何在大规模、真实世界数据集上工作过的人都知道，这些数据集中必然包含至少一些不符合编写处理代码人员期望的记录。许多 PySpark 管道由于无效记录导致解析逻辑抛出异常而失败。在进行交互式分析时，我们可以通过关注关键变量来感知数据中潜在的异常。
- en: In our case, variables containing geospatial and temporal information are worth
    looking at for inconsistencies. Presence of null values in these columns will
    definitely throw off our analysis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，包含地理空间和时间信息的变量存在不一致性是值得注意的。这些列中的空值肯定会影响我们的分析结果。
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s remove the null values from our data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据中删除空值：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another commonsense check that we can do is for latitude and longitude records
    where the values are zero. We know that for the region we’re concerned with, those
    would be invalid values:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常识检查是检查纬度和经度记录中值为零的情况。我们知道对于我们关注的地区，这些值是无效的：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_geospatial_and_temporal_data_analysis___span_class__keep_together__on_taxi_trip_data__span__CO1-1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_geospatial_and_temporal_data_analysis___span_class__keep_together__on_taxi_trip_data__span__CO1-1)'
- en: Multiple `OR` conditions will be true if either of them evaluates to `True`
    for any record.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何记录的任一条件为 `True`，则多个`OR`条件将为真。
- en: We have quite a few of these. If it looks as if a taxi took a passenger to the
    South Pole, we can be reasonably confident that the record is invalid and should
    be excluded from our analysis. We will not remove them but get back to them toward
    the end of the next section to see how they can affect our analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有很多这样的情况。如果看起来一辆出租车带乘客去了南极，我们可以相当有信心地认为该记录是无效的，并应从分析中排除。我们不会立即删除它们，而是在下一节结束时回顾它们，看看它们如何影响我们的分析。
- en: 'In production settings, we handle these exceptions one at a time by checking
    the logs for the individual tasks, figuring out which line of code threw the exception,
    and then figuring out how to tweak the code to ignore or correct the invalid records.
    This is a tedious process, and it often feels like we’re playing whack-a-mole:
    just as we get one exception fixed, we discover another one on a record that came
    later within the partition.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们逐个处理这些异常，通过检查各个任务的日志，找出抛出异常的代码行，然后调整代码以忽略或修正无效记录。这是一个繁琐的过程，常常感觉像是在玩打地鼠游戏：就在我们修复一个异常时，我们发现分区内稍后出现的记录中又有另一个异常。
- en: One strategy that experienced data scientists deploy when working with a new
    dataset is to add a `try-except` block to their parsing code so that any invalid
    records can be written out to the logs without causing the entire job to fail.
    If there are only a handful of invalid records in the entire dataset, we might
    be okay with ignoring them and continuing with our analysis.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家处理新数据集时，一个常用的策略是在他们的解析代码中添加 `try-except` 块，以便任何无效记录都可以被写入日志，而不会导致整个作业失败。如果整个数据集中只有少数无效记录，我们可能可以忽略它们并继续分析。
- en: Now that we have prepared our dataset, let’s get started with geospatial analysis.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好我们的数据集，让我们开始地理空间分析。
- en: Geospatial Analysis
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 地理空间分析
- en: There are two major kinds of geospatial data—vector and raster—and there are
    different tools for working with each type. In our case, we have latitude and
    longitude for our taxi trip records, and vector data stored in the GeoJSON format
    that represents the boundaries of the different boroughs of New York. We’ve looked
    at the latitude and longitude points. Let’s start by having a look at the GeoJSON
    data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要类型的地理空间数据——矢量和栅格——以及用于处理每种类型的不同工具。在我们的案例中，我们有出租车行程记录的纬度和经度，以及以 GeoJSON
    格式存储的矢量数据，表示纽约不同区域的边界。我们已经查看了纬度和经度点。让我们先看看 GeoJSON 数据。
- en: Intro to GeoJSON
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GeoJSON 简介
- en: The data we’ll use for the boundaries of boroughs in New York City comes written
    in a format called *GeoJSON*. The core object in GeoJSON is called a *feature*,
    which is made up of a *geometry* instance and a set of key-value pairs called
    *properties*. A geometry is a shape like a point, line, or polygon. A set of features
    is called a `FeatureCollection`. Let’s pull down the GeoJSON data for the NYC
    borough maps and take a look at its structure.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于纽约市各区域边界的数据以*GeoJSON*格式呈现。GeoJSON中的核心对象称为*要素*，由一个*几何*实例和一组称为*属性*的键值对组成。几何是如点、线或多边形等形状。一组要素称为`FeatureCollection`。让我们下载纽约市区地图的GeoJSON数据，并查看其结构。
- en: 'In the *taxidata* directory on your client machine, download the data and rename
    the file to something a bit shorter:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端机器的*taxidata*目录中，下载数据并将文件重命名为稍短的名称：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Open the file and look at a feature record. Note the properties and the geometry
    objects—in this case, a polygon representing the boundaries of the borough and
    the properties containing the name of the borough and other related information.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 打开文件并查看要素记录。注意属性和几何对象，例如表示区域边界的多边形和包含区域名称及其他相关信息的属性。
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: GeoPandas
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GeoPandas
- en: The first thing you should consider when choosing a library to perform geospatial
    analysis is determine what kind of data you will need to work with. We need a
    library that can parse GeoJSON data and can handle spatial relationships, like
    detecting whether a given longitude/latitude pair is contained inside a polygon
    that represents the boundaries of a particular borough. We will use the [GeoPandas
    library](https://geopandas.org) for this task. GeoPandas is an open source project
    to make working with geospatial data in Python easier. It extends the data types
    used by the pandas library, which we used in previous chapters, to allow spatial
    operations on geometric data types.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择用于执行地理空间分析的库时，首先要考虑的是确定你需要处理的数据类型。我们需要一个可以解析GeoJSON数据并处理空间关系的库，比如检测给定的经度/纬度对是否包含在表示特定区域边界的多边形内。我们将使用[GeoPandas库](https://geopandas.org)来完成这项任务。GeoPandas是一个开源项目，旨在使Python中的地理空间数据处理更加简单。它扩展了pandas库中使用的数据类型，允许对几何数据类型进行空间操作，我们在之前的章节中已经使用过pandas库。
- en: 'Install the GeoPandas package using pip:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pip安装GeoPandas包：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let us now start examining the geospatial aspects of the taxi data. For each
    trip, we have longitude/latitude pairs representing where the passenger was picked
    up and dropped off. We would like to be able to determine which borough each of
    these longitude/latitude pairs belongs to, and identify any trips that did not
    start or end in any of the five boroughs. For example, if a taxi took passengers
    from Manhattan to Newark Liberty International Airport, that would be a valid
    ride that would be interesting to analyze, even though it would not end within
    one of the five boroughs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始研究出租车数据的地理空间方面。对于每次行程，我们有经度/纬度对，表示乘客上车和下车的位置。我们希望能够确定每个经度/纬度对属于哪个区域，并识别没有在五个区域之一开始或结束的行程。例如，如果一辆出租车将乘客从曼哈顿送往纽瓦克自由国际机场，那将是一个有效的行程，也值得分析，尽管它不会在五个区域中的任何一个结束。
- en: 'To perform our borough analysis, we need to load the GeoJSON data we downloaded
    earlier and stored in the *nyc-boroughs.geojson* file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行我们的区域分析，我们需要加载我们之前下载并存储在*nyc-boroughs.geojson*文件中的GeoJSON数据：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Before we use the GeoJSON features on the taxi trip data, we should take a moment
    to think about how to organize this geospatial data for maximum efficiency. One
    option would be to research data structures that are optimized for geospatial
    lookups, such as quad trees, and then find or write our own implementation. Instead,
    we will try to come up with a quick heuristic that will allow us to bypass that
    bit of work.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用GeoJSON特性处理出租车行程数据之前，我们应该花点时间考虑如何组织这些地理空间数据以实现最大效率。一种选择是研究针对地理空间查找进行优化的数据结构，比如四叉树，然后找到或编写我们自己的实现。不过，我们将尝试提出一个快速的启发式方法，以便我们可以跳过那部分工作。
- en: We will iterate through the `gdf` until we find a feature whose geometry contains
    a given `point` of longitude/latitude. Most taxi rides in NYC begin and end in
    Manhattan, so if the geospatial features that represent Manhattan are earlier
    in the sequence, our searches will end relatively quickly. We can use the fact
    that the `boroughCode` property of each feature can be used as a sorting key,
    with the code for Manhattan equal to 1 and the code for Staten Island equal to
    5\. Within the features for each borough, we want the features associated with
    the largest polygons to come before those associated with the smaller polygons,
    because most trips will be to and from the “major” region of each borough.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过`gdf`迭代，直到找到一个几何图形包含给定经度/纬度的`point`。大多数纽约市的出租车行程始发和结束在曼哈顿，因此如果代表曼哈顿的地理空间特征在序列中较早出现，我们的搜索会相对快速结束。我们可以利用每个特征的`boroughCode`属性作为排序键，曼哈顿的代码等于1，斯塔滕岛的代码等于5。在每个行政区的特征中，我们希望与较大多边形相关联的特征优先于与较小多边形相关联的特征，因为大多数行程将会发生在每个行政区的“主要”区域之间。
- en: 'We will calculate area associated with each feature’s geometry and store it
    as a new column:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算与每个特征几何相关联的区域，并将其存储为一个新列：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Sorting the features by the combination of the borough code and the area of
    each feature’s geometry should do the trick:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将特征按照行政区代码和每个特征几何区域的组合排序应该能解决问题：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that we’re sorting based on area value in descending order because we want
    the largest polygons to come first, and `sort_values` sorts in ascending order
    by default.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们基于面积值按降序排序，因为我们希望最大的多边形首先出现，而`sort_values`默认按升序排序。
- en: 'Now we can broadcast the sorted features in the `gdf` GeoPandas DataFrame to
    the cluster and write a function that uses these features to find out in which
    of the five boroughs (if any) a particular trip ended:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将`gdf` GeoPandas DataFrame中排序后的特征广播到集群，并编写一个函数，利用这些特征来找出特定行程结束在五个行政区中的哪一个（如果有的话）：
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can apply `find_borough` to the trips in the `taxi_raw` DataFrame to create
    a histogram of trips by borough:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`find_borough`应用于`taxi_raw` DataFrame中的行程，以创建一个按行政区划分的行程直方图：
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As we expected, the vast majority of trips end in the borough of Manhattan,
    while relatively few trips end in Staten Island. One surprising observation is
    the number of trips that end outside of any borough; the number of `null` records
    is substantially larger than the number of taxi rides that end in the Bronx.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预料到，绝大多数行程的终点在曼哈顿区，而只有相对较少的行程终点在斯塔滕岛。一个令人惊讶的观察是，有多少行程的终点在任何一个行政区外；`null`记录的数量远远大于在布朗克斯结束的出租车行程的数量。
- en: We had talked about handling such invalid records earlier but did not remove
    them. It is left as an exercise for you to remove such records and create a histogram
    from the cleaned-up data. Once done, you will notice a reduction in the number
    of `null` entries, leaving a much more reasonable number of observations that
    had drop-offs outside the city.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过如何处理这些无效记录，但并未将其删除。现在由你来移除这些记录并从清理后的数据中创建直方图。完成后，你会注意到`null`条目的减少，留下了更合理的在城市外进行下车的观察数据。
- en: Having worked with the geospatial aspects of our data, let us now dig deeper
    into the temporal nature of our data by performing sessionization using PySpark.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了数据的地理空间方面，让我们现在通过使用PySpark对数据的时间特性进行更深入的挖掘来执行会话化。
- en: Sessionization in PySpark
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark中的会话化
- en: The kind of analysis, in which we want to analyze a single entity as it executes
    a series of events over time, is called *sessionization*, and is commonly performed
    over web logs to analyze the behavior of the users of a website. PySpark provides
    `Window` and aggregation functions out of the box that can be used to perform
    such analysis. These allow us to focus on business logic instead of trying to
    implement complex data manipulation and calculation. We will use these in the
    next section to better understand utilization of taxi cabs in our dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种分析中，我们希望分析单个实体随时间执行一系列事件的类型被称为*会话化*，通常在Web日志中执行以分析网站用户的行为。 PySpark提供了`Window`和聚合函数，可以用来执行这种分析。这些允许我们专注于业务逻辑，而不是试图实现复杂的数据操作和计算。我们将在下一节中使用这些功能来更好地理解数据集中出租车的利用率。
- en: Sessionization can be a very powerful technique for uncovering insights in data
    and building new data products that can be used to help people make better decisions.
    For example, Google’s spell-correction engine is built on top of the sessions
    of user activity that Google builds each day from the logged records of every
    event (searches, clicks, maps visits, etc.) occurring on its web properties. To
    identify likely spell-correction candidates, Google processes those sessions looking
    for situations where a user typed a query, didn’t click anything, typed a slightly
    different query a few seconds later, and then clicked a result and didn’t come
    back to Google. Then it counts how often this pattern occurs for any pair of queries.
    If it occurs frequently enough (e.g., if every time we see the query “untied stats,”
    it’s followed a few seconds later by the query “united states”), then we assume
    that the second query is a spell correction of the first.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 会话化可以是揭示数据洞察力并构建可帮助人们做出更好决策的新数据产品的强大技术。例如，谷歌的拼写纠正引擎是建立在每天从其网站属性上发生的每个事件（搜索、点击、地图访问等）的用户活动会话之上的。为了识别可能的拼写纠正候选项，谷歌处理这些会话，寻找用户输入一个查询后没有点击任何内容、几秒钟后再输入稍有不同的查询，然后点击一个结果且不返回谷歌的情况。然后，计算这种模式对于任何一对查询发生的频率。如果发生频率足够高（例如，每次看到查询“untied
    stats”后几秒钟后都跟随查询“united states”），则我们认为第二个查询是对第一个查询的拼写纠正。
- en: This analysis takes advantage of the patterns of human behavior that are represented
    in the event logs to build a spell-correction engine from data that is more powerful
    than any engine that could be created from a dictionary. The engine can be used
    to perform spell correction in any language and can correct words that might not
    be included in any dictionary (e.g., the name of a new startup) or queries like
    “untied stats” where none of the words are misspelled! Google uses similar techniques
    to show recommended and related searches, as well as to decide which queries should
    return a OneBox result that gives the answer to a query on the search page itself,
    without requiring that the user click through to a different page. There are OneBoxes
    for weather, scores from sporting events, addresses, and lots of other kinds of
    queries.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这项分析利用事件日志中体现的人类行为模式来构建一个拼写纠正引擎，该引擎使用的数据比任何从字典创建的引擎更为强大。该引擎可用于任何语言的拼写纠正，并能纠正可能不包含在任何字典中的单词（例如新创企业的名称）或像“untied
    stats”这样的查询，其中没有任何单词拼写错误！谷歌使用类似的技术来显示推荐和相关搜索，以及决定哪些查询应返回一个OneBox结果，即在搜索页面本身给出查询答案，而无需用户点击转到不同页面。OneBox可用于天气、体育比赛得分、地址以及许多其他类型的查询。
- en: So far, information about the set of events that occurs to each entity is spread
    out across the DataFrame’s partitions, so, for analysis, we need to place these
    relevant events next to each other and in chronological order. In the next section,
    we will show how to efficiently construct and analyze sessions using advanced
    PySpark functionality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，每个实体发生的事件集合的信息分散在DataFrame的分区中，因此，为了分析，我们需要将这些相关事件放在一起并按时间顺序排列。在接下来的部分中，我们将展示如何使用高级PySpark功能有效地构建和分析会话。
- en: 'Building Sessions: Secondary Sorts in PySpark'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建会话：PySpark中的二次排序
- en: The naive way to create sessions in PySpark is to perform a `groupBy` on the
    identifier we want to create sessions for and then sort the events post-shuffle
    by a timestamp identifier. If we only have a small number of events for each entity,
    this approach will work reasonably well. However, because this approach requires
    all the events for any particular entity to be in memory at the same time, it
    will not scale as the number of events for each entity gets larger and larger.
    We need a way of building sessions that does not require all of the events for
    a particular entity to be held in memory at the same time for sorting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在PySpark中创建会话的简单方法是对要创建会话的标识符执行`groupBy`，然后按时间戳标识符进行后续事件排序。如果每个实体只有少量事件，这种方法将表现得相当不错。但是，由于这种方法要求任何特定实体的所有事件同时在内存中以进行排序，所以随着每个实体的事件数量越来越大，它将无法扩展。我们需要一种构建会话的方法，不需要将特定实体的所有事件同时保留在内存中以进行排序。
- en: 'In MapReduce, we can build sessions by performing a *secondary sort*, where
    we create a composite key made up of an identifier and a timestamp value, sort
    all of the records on the composite key, and then use a custom partitioner and
    grouping function to ensure that all of the records for the same identifier appear
    in the same output partition. Fortunately, PySpark can also support a similar
    pattern by using `Window` functions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在MapReduce中，我们可以通过执行*二次排序*来构建会话，其中我们创建一个由标识符和时间戳值组成的复合键，对所有记录按复合键排序，然后使用自定义分区器和分组函数确保同一标识符的所有记录出现在同一输出分区中。幸运的是，PySpark也可以通过使用`Window`函数支持类似的模式：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'First, we use the `partitionBy` method to ensure that all of the records that
    have the same value for the `license` column end up in the same partition. Then,
    within each of these partitions, we sort the records by their `license` value
    (so all trips by the same driver appear together) and then by their `pickupTime`
    so that the sequence of trips appears in sorted order within the partition. Now
    when we aggregate the trip records, we can be sure that the trips are ordered
    in a way that is optimal for sessions analysis. Because this operation triggers
    a shuffle and a fair bit of computation and we’ll need to use the results more
    than once, we cache them:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`partitionBy`方法确保所有具有相同`license`列值的记录最终位于同一个分区中。然后，在每个分区内，我们按其`license`值对记录进行排序（使得同一驾驶员的所有行程出现在一起），然后再按其`pickupTime`排序，以使行程序列按排序顺序出现在分区内。现在，当我们聚合行程记录时，我们可以确保行程按照适合会话分析的方式进行排序。由于此操作触发了洗牌和相当数量的计算，并且我们需要多次使用结果，因此我们将它们缓存：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Executing a sessionization pipeline is an expensive operation, and the sessionized
    data is often useful for many different analysis tasks that we might want to perform.
    In settings where one might want to pick up on the analysis later or collaborate
    with other data scientists, it is a good idea to amortize the cost of sessionizing
    a large dataset by only performing the sessionization once and then writing the
    sessionized data to a filesystem such as S3 or HDFS so that it can be used to
    answer lots of different questions. Performing sessionization once is also a good
    way to enforce standard rules for session definitions across the entire data science
    team, which has the same benefits for ensuring apples-to-apples comparisons of
    results.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 执行会话化管道是一项昂贵的操作，而且会话化数据通常对我们可能要执行的许多不同分析任务都很有用。在可能希望稍后继续分析或与其他数据科学家合作的环境中，通过仅执行一次会话化大型数据集并将会话化数据写入诸如S3或HDFS之类的文件系统，可以分摊会话化成本，使其可用于回答许多不同的问题是一个好主意。仅执行一次会话化还是一种强制执行会话定义标准规则于整个数据科学团队的好方法，这对确保结果的苹果对苹果比较具有相同的好处。
- en: 'At this point, we are ready to analyze our sessions data to see how long it
    takes for a driver to find his next fare after a drop-off in a particular borough.
    We will use the `lag` function along with the `window_spec` object created earlier
    to take two trips and compute the duration in seconds between the drop-off time
    of the first trip and the pickup time of the second:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们准备分析我们的会话数据，以查看司机在特定区域卸客后多长时间找到下一个乘客。我们将使用之前创建的`lag`函数以及`window_spec`对象来获取两次行程，并计算第一次行程的卸客时间和第二次行程的接客时间之间的持续时间（以秒为单位）：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we should do a validation check to ensure that most of the durations are
    nonnegative:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该进行验证检查，确保大部分持续时间是非负的：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Only a few of the records have a negative duration, and when we examine them
    more closely, there don’t seem to be any common patterns to them that we could
    use to understand the source of the erroneous data. If we exclude these negative
    duration records from our input dataset and look at the average and standard deviation
    of the pickup times by borough, we see this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 只有少数记录具有负持续时间，当我们更仔细地检查它们时，似乎没有任何我们可以用来理解错误数据来源的共同模式。如果我们从输入数据集中排除这些负持续时间记录，并查看按区域的接车时间的平均值和标准差，我们可以看到这个：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we would expect, the data shows that drop-offs in Manhattan have the shortest
    amount of downtime for drivers, at around 10 minutes. Taxi rides that end in Brooklyn
    have a downtime of more than twice that, and the relatively few rides that end
    in Staten Island take drivers an average of almost 45 minutes to get to their
    next fare.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，数据显示，曼哈顿的下车时间最短，约为10分钟。在布鲁克林结束的出租车行程的空闲时间是这个的两倍以上，而在史泰登岛结束的相对较少的行程平均需要司机近45分钟才能到达下一个乘客。
- en: As the data demonstrates, taxi drivers have a major financial incentive to discriminate
    among passengers based on their final destination; drop-offs in Staten Island,
    in particular, involve an extensive amount of downtime for a driver. The NYC Taxi
    and Limousine Commission has made a major effort over the years to identify this
    discrimination and has fined drivers who have been caught rejecting passengers
    because of where they wanted to go. It would be interesting to attempt to examine
    the data for unusually short taxi rides that could be indicative of a dispute
    between the driver and the passenger about where the passenger wanted to be dropped
    off.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如数据所显示的那样，出租车司机有很大的经济激励来根据乘客的最终目的地进行歧视；特别是在史泰登岛下车，司机需要大量的空闲时间。多年来，纽约市出租车和豪华轿车委员会一直在努力识别这种歧视行为，并对因乘客目的地而拒载的司机处以罚款。有趣的是尝试分析数据，找出可能表明司机与乘客对于乘客要下车的地方存在分歧的异常短程出租车行程。
- en: Where to Go from Here
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来的发展方向
- en: In this chapter, we worked with both temporal and spatial features of a real-world
    dataset. The familiarity with geospatial analysis that you have gained so far
    can be used to dive into frameworks such as Apache Sedona or GeoMesa. They will
    have a steeper learning curve compared to working with GeoPandas and UDFs but
    will be more efficient. There’s also a lot of scope for using data visualization
    with geospatial and temporal data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们处理了真实数据集的时间和空间特征。到目前为止，您所获得的地理空间分析的熟悉度可以用于深入研究诸如Apache Sedona或GeoMesa等框架。与使用GeoPandas和UDF相比，它们将具有更陡峭的学习曲线，但会更高效。在地理空间和时间数据的数据可视化方面也有很大的应用空间。
- en: 'Further, imagine using this same technique on the taxi data to build an application
    that could recommend the best place for a cab to go after a drop-off based on
    current traffic patterns and the historical record of next-best locations contained
    within this data. You could also look at the information from the perspective
    of someone trying to catch a cab: given the current time, place, and weather data,
    what is the probability that I will be able to hail a cab from the street within
    the next five minutes? This sort of information could be incorporated into applications
    like Google Maps to help travelers decide when to leave and which travel option
    they should take.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，想象一下使用相同的技术在出租车数据上构建一个应用程序，根据当前交通模式和历史记录的最佳下一个位置，推荐出租车在下车后最好去的地方。您还可以从需要打车的人的角度查看信息：根据当前时间、地点和天气数据，我能在接下来的五分钟内从街上拦到出租车的概率是多少？这种信息可以整合到应用程序中，例如谷歌地图，帮助旅行者决定何时出发以及选择哪种出行方式。
