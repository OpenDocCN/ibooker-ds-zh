- en: appendix  Machine learning refresher
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 机器学习复习
- en: This appendix covers the basics of machine learning that are most relevant to
    human-in-the-loop machine learning, including interpreting the output from a machine
    learning model; understanding softmax and its limitations; calculating accuracy
    through recall, precision, F-score area under the ROC curve (AUC), and chance-adjusted
    accuracy; and measuring the performance of machine learning from a human perspective.
    This book assumes that you have basic machine learning knowledge. Even if you
    are experienced, you may want to review this appendix. In particular, the parts
    related to softmax and accuracy are especially important for this book and are
    sometimes overlooked by people who are looking only at algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录涵盖了与人类在循环机器学习最相关的机器学习基础知识，包括解释机器学习模型的输出；理解softmax及其局限性；通过召回率、精确率、F分数、ROC曲线下面积（AUC）和调整后的准确率来计算准确度；以及从人类的角度衡量机器学习性能。本书假设你具备基本的机器学习知识。即使你有经验，你可能也想回顾这个附录。特别是与softmax和准确度相关的部分对于本书尤为重要，有时会被只关注算法的人忽视。
- en: A.1 Interpreting predictions from a model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.1 从模型中解释预测
- en: 'Almost all supervised machine learning models give you two things:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的监督式机器学习模型都会给你两样东西：
- en: A predicted label (or set of predictions)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测标签（或一组预测）
- en: A number (or set of numbers) associated with each predicted label
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与每个预测标签相关联的数字（或一组数字）
- en: 'Suppose that we have a simple object detection model that tries to distinguish
    among four types of objects: “Cyclist,” “Pedestrian,” “Sign,” and “Animal.” The
    model might give us a prediction like the following listing.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个简单的目标检测模型，它试图区分四种类型的对象：“骑自行车的人”、“行人”、“标志”和“动物”。该模型可能会给出以下预测列表。
- en: Listing A.1 Example of a JSON-encoded prediction from a model
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.1 模型JSON编码预测示例
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ In this prediction, the object is predicted to be “Cyclist” with 91.9% accuracy.
    The scores will add to 100%, giving us the probability distribution for this item.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在这个预测中，对象被预测为“骑自行车的人”，准确度为91.9%。这些得分加起来为100%，给出了这个项目的概率分布。
- en: You can see in the example that “Cyclist” is predicted with a 0.919 score. The
    scores that might have been “Pedestrian,” “Sign,” or “Animal” are 0.014, 0.050,
    and 0.0168, respectively. The four scores total to 1.0, which makes the score
    like a probability or confidence. You could interpret 0.919 as 91.9% confidence
    that the object is a “Cyclist,” for example. Together, the scores are known as
    a *probability distribution.*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从示例中看到，“骑自行车的人”被预测为得分为0.919。可能被预测为“行人”、“标志”或“动物”的得分分别是0.014、0.050和0.0168。这四个得分总和为1.0，这使得得分类似于概率或置信度。例如，你可以将0.919解释为有91.9%的置信度认为该对象是“骑自行车的人”。这些得分合在一起被称为*概率分布*。
- en: A.1.1 Probability distributions
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.1 概率分布
- en: In the machine learning literature, the term *probability distribution* means
    only that the numbers across the predicted labels add up to 100%; it does not
    necessarily mean that each number reflects the actual model confidence that the
    prediction is correct. For neural networks, logistic regression, and other types
    of related discriminative supervised learning algorithms, it is not the job of
    the algorithm to know how confident its predictions are. The job of the algorithm
    is trying to discriminate among the labels based on the features—hence, the name
    *discriminative supervised learning*. The raw scores from the last layer of a
    neural network are the network’s trying to discriminate among the predictions
    it is making. Depending on the parameters of the model, those raw scores in the
    final layer can be any real number. Although it is outside the scope of this book
    to go into why neural models don’t produce good probability distributions, as
    a general rule, most models tend to be overconfident, predicting the most likely
    label with a higher score than its actual probability, but when there is rare
    data, the models can be underconfident. So the scores that come out of these algorithms
    often need to be converted to something that more closely approximates the true
    confidence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习文献中，术语*概率分布*仅意味着预测标签之间的数字总和为100%，这并不一定意味着每个数字都反映了模型对预测正确的实际置信度。对于神经网络、逻辑回归和其他类型的相关判别监督学习算法，算法的任务并不是知道其预测的置信度。算法的任务是尝试根据特征在标签之间进行区分——因此，称为*判别监督学习*。神经网络最后一层的原始分数是网络尝试在其做出的预测中进行区分的分数。根据模型的参数，这些最终层的原始分数可以是任何实数。尽管本书的范围不涉及为什么神经网络不能产生好的概率分布，但一般来说，大多数模型往往过于自信，预测最可能的标签时给出的分数高于其实际概率，但当有罕见数据时，模型可能会缺乏自信。因此，这些算法输出的分数通常需要转换为更接近真实置信度的东西。
- en: The probability distribution might be called something different in your favorite
    library. See the following sidebar for more about the differences.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在你喜欢的库中，概率分布可能被称作不同的名称。请参阅以下侧边栏了解更多关于差异的信息。
- en: 'Score, confidence, and probability: Do not trust the name!'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分数、置信度和概率：不要相信名称！
- en: Machine learning libraries—open libraries and commercial ones—often use the
    terms score, confidence, and probability interchangeably. You may not even find
    consistency within the same library.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习库——无论是开源的还是商业的——通常将分数、置信度和概率互换使用。你甚至可能发现在同一个库中也没有一致性。
- en: I’ve encountered this situation. When I was running product for Amazon Comprehend,
    AWS’s natural language processing (NLP) service, we had to decide what we should
    call the numbers associated with each prediction. After long discussion, we decided
    that confidence was misleading, as the outputs from the system were not confidences
    according to the strict statistical definition of a probability, so we went with
    score instead. An existing computer vision service at AWS, Amazon Rekognition,
    already used confidence for this same score when predicting the labels of images
    (and still does to this day).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到过这种情况。当我负责亚马逊Comprehend的产品时，这是AWS的自然语言处理（NLP）服务，我们必须决定应该称每个预测相关的数字为什么。经过长时间的讨论，我们决定“置信度”这个词具有误导性，因为系统的输出并不是根据概率的严格统计定义的置信度，所以我们选择了“分数”这个词。AWS现有的计算机视觉服务Amazon
    Rekognition在预测图像标签时（至今仍然如此）已经使用了“置信度”来表示这个相同的分数。
- en: Most machine learning libraries are built with less consideration to naming
    conventions than large cloud companies give them, so you shouldn’t trust the numbers
    associated with your predictions based on their names alone. Read the documentation
    for your machine learning library or service to find out what the numbers associated
    with each prediction mean.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习库在命名约定方面的考虑不如大型云公司给予的那么多，因此你不应该仅根据它们的名称来信任与预测相关的数字。阅读你的机器学习库或服务的文档，以了解与每个预测相关的数字的含义。
- en: For generative supervised learning algorithms, like most Bayesian algorithms,
    the algorithm *is* trying to explicitly model each label, so the confidences can
    be read directly from your model. These confidences, however, rely on assumptions
    about the underlying distribution of the data (such as a normal distribution)
    and the prior probability of each label.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成式监督学习算法，如大多数贝叶斯算法，算法*确实*试图显式地建模每个标签，因此置信度可以直接从你的模型中读取。然而，这些置信度依赖于对数据潜在分布（如正态分布）和每个标签的先验概率的假设。
- en: To complicate things further, you can extend a discriminative supervised learning
    algorithm with generative supervised learning methods to get a truer statistical
    “probability” straight from the model. Today, generative methods for getting accurate
    probabilities from discriminative models are not available in the most widely
    used machine learning libraries. You are overwhelmingly more likely to get a probability
    distribution generated by the softmax algorithm, so we will start there.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步复杂化，你可以通过生成式监督学习方法扩展判别式监督学习算法，以从模型中获得更真实的统计“概率”。今天，从判别模型中获得准确概率的生成方法在大多数广泛使用的机器学习库中是不可用的。你更有可能得到由softmax算法生成的概率分布，因此我们将从这里开始。
- en: A.2 Softmax deep dive
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.2 softmax深入探讨
- en: The most common models are neural networks, and neural network predictions are
    almost always converted to a 0–1 range of scores by means of softmax. Softmax
    is defined as
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的模型是神经网络，神经网络预测几乎总是通过softmax转换到0-1的分数范围。softmax被定义为
- en: '![](../Images/APPA_F00_Munro_E01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F00_Munro_E01.png)'
- en: The outputs of a neural network will look something like figure A.1.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输出将类似于图A.1。
- en: '![](../Images/APPA_F01_Munro.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F01_Munro.png)'
- en: Figure A.1 How softmax creates probability distributions in two types of architectures.
    In the top example, softmax is the activation function of the output (final) layer,
    directly outputting a probability distribution. In the bottom example, a linear
    activation function is used on the output layer, creating model scores (logits)
    that are converted to probability distributions via softmax. The bottom architecture
    is only slightly more complicated but is preferred for active learning, as it
    is more informative.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.1：softmax在两种类型架构中如何创建概率分布。在上面的例子中，softmax是输出（最终）层的激活函数，直接输出一个概率分布。在下面的例子中，输出层使用了线性激活函数，创建了模型分数（logits），这些分数通过softmax转换成概率分布。底部的架构稍微复杂一些，但更适合主动学习，因为它提供了更多信息。
- en: As figure A.1 shows, softmax is often used as the activation function on the
    final layer of the model to produce a probability distribution as the set of scores
    associated with the predicted labels. Softmax can also be used to create a probability
    distribution from the outputs of a linear activation function (the *logits*).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如图A.1所示，softmax通常用作模型最终层的激活函数，以产生一个概率分布，作为与预测标签相关联的分数集合。softmax也可以从线性激活函数（*logits*）的输出中创建概率分布。
- en: It is common to use softmax in the final layer or to look only at the result
    of softmax applied to the logits. Softmax is lossy and loses the distinction between
    uncertainty due to strongly competing information and uncertainty due to lack
    of information. We assume that we are using the second type of architecture in
    figure A.1, but the effects would apply whether softmax is an activation function
    or is applied to the model scores.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在最终层使用softmax，或者只查看对logits应用softmax的结果。softmax是有损的，它丢失了由于强烈竞争信息导致的不确定性和由于信息不足导致的不确定性之间的区别。我们假设我们正在使用图A.1中的第二种架构，但效果会适用于softmax是激活函数还是应用于模型分数的情况。
- en: If you are using the second kind of architecture in figure A.1, an activation
    function in the final layer with negative values such as Leaky ReLU is often better
    for human-in-the-loop architectures than functions that have a zero lower bound,
    such as ReLU. For some of the active learning strategies in this book, it can
    help to quantify the amount of negative information for one output. If you know
    that some other activation function is more accurate for predicting labels, you
    might consider retraining your final layer for active learning. This strategy—retraining
    part of a model specifically for human-in-the-loop tasks—is covered throughout
    this book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用图A.1中的第二种架构，在最后一层使用具有负值的激活函数，例如Leaky ReLU，对于包含人类操作员的架构通常比具有零下限的函数（如ReLU）更好。对于本书中的一些主动学习策略，这有助于量化一个输出中负信息的数量。如果你知道某些其他激活函数在预测标签方面更准确，你可能会考虑重新训练你的最后一层以进行主动学习。这种策略——为包含人类操作员的任务专门重新训练模型的一部分——在本书中有所涉及。
- en: Regardless of the architecture you are using and the range of inputs to softmax,
    understanding the softmax equation is important because it is lossy (which is
    widely known) and makes arbitrary input assumptions that can change the rank order
    of the confidence of predictions (which is not widely known).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用什么架构以及softmax的输入范围如何，理解softmax方程式都很重要，因为它是有损的（这是众所周知的）并且对任意输入假设，这可能会改变预测置信度的排名顺序（这并不广为人知）。
- en: A.2.1 Converting the model output to confidences with softmax
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.1 将模型输出转换为置信度使用softmax
- en: Here’s an example implementation of softmax in Python, using the PyTorch library:[¹](#pgfId-1002397)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用PyTorch库实现的softmax示例：[¹](#pgfId-1002397)
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Strictly speaking, this function should be called *softargmax* but in machine
    learning circles it is almost always shortened to *softmax.* You might also see
    it called a *Boltzmann distribution* or *Gibbs distribution*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，这个函数应该被称为*softargmax*，但在机器学习领域几乎总是简称为*softmax*。你也可能看到它被称为*玻尔兹曼分布*或*吉布斯分布*。
- en: To get an idea of what the softmax transformation in the preceding equation
    is doing, let’s break down the pieces. Suppose that you predicted the object in
    an image, and the model gave you raw scores of 1, 4, 2, and 3\. The highest number,
    4, will become the most confident prediction (table A.1).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解前一个方程中的softmax变换在做什么，让我们分解这些部分。假设你预测了图像中的对象，并且模型给你提供了原始分数1，4，2和3。最大的数字4将成为最自信的预测（表A.1）。
- en: Table A.1 An example prediction with the scores (z, logits); each score to the
    power of the natural exponent (e); and the normalized exponents, which are the
    softmax values. The normalized vector is called the probability distribution because
    the numbers in a 0–1 range and add up to 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.1 使用分数（z，logits）的示例预测；每个分数乘以自然指数（e）；以及归一化指数，即softmax值。归一化向量被称为概率分布，因为数字在0-1范围内，且总和为1。
- en: '| Predicted label | Cyclist | Pedestrian | Sign | Animal |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | 骑行者 | 行人 | 标志 | 动物 |'
- en: '| scores (z[1],..z[4]) | 1.0 | 4.0 | 2.0 | 3.0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 分数 (z[1],..z[4]) | 1.0 | 4.0 | 2.0 | 3.0 |'
- en: '| e^z | 2.72 | 54.60 | 7.39 | 20.09 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| e^z | 2.72 | 54.60 | 7.39 | 20.09 |'
- en: '| softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| softmax | 0.0321 | 0.6439 | 0.0871 | 0.2369 |'
- en: The final row, softmax, is each e^z divided by the sum of all numbers in the
    e^z row. These raw scores,—1, 4, 2, and 3—will be used throughout this section
    to keep the examples consistent and because they add to 10, which makes intuition
    easier. The exact range of numbers you get will depend on your activation function.
    If you are using softmax as the final activation function, the exact numbers will
    be a combination of activation function and weights on the output of the previous
    layer. Exact integers are unlikely, but the range of 1–4 will be common in a lot
    of architectures.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行，softmax，是e^z除以e^z行中所有数字的总和。这些原始分数——1，4，2和3——将在本节中用于保持示例的一致性，并且因为它们加起来是10，这使得直觉更容易理解。你得到的数字的确切范围将取决于你的激活函数。如果你使用softmax作为最终的激活函数，确切的数字将是激活函数和前一层输出的权重的组合。确切的整数不太可能，但在许多架构中，1-4的范围将是常见的。
- en: As table A.1 shows, “Pedestrian” is the most confident prediction for our example,
    and the confidence numbers are stretched out from raw numbers; 4.0 out of 10.0
    in the raw scores becomes 64% in the softmax. The “Pedestrian” prediction became
    much bigger in the e^z step, where it is 54.60, e^(4.0) = 54.60, so the most probable
    label comes to dominate the denominator equation as the largest number.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 A.1 所示，“行人”是我们示例中最自信的预测，置信度数字是从原始数字拉伸出来的；原始分数中的 4.0/10.0 变成了 softmax 中的 64%。在
    e^z 步骤中，“行人”预测变得很大，其中它是 54.60，e^(4.0) = 54.60，因此最可能的标签成为分母方程中的最大数，从而主导了方程。
- en: 'The benefits of interpretability should be clear: by converting the numbers
    to exponentials and normalizing them, we are able to convert an unbounded range
    of positive and negative numbers into probability estimates that are in a 0–1
    range and add up to 1\. Also, the exponentials might map more closely to real
    probabilities than if we normalized the raw scores. If your model is training
    by using maximum likelihood estimation (MLE), the most popular way to train a
    neural model, it is optimizing the log likelihood. So using an exponential on
    log likelihood takes us to an actual likelihood.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性的好处应该是显而易见的：通过将数字转换为指数并归一化，我们能够将正负数的无界范围转换为 0-1 范围内的概率估计，并且总和为 1。此外，指数可能比如果我们归一化原始分数时更接近真实概率。如果你的模型是通过使用最大似然估计（MLE）进行训练的，这是训练神经模型最流行的方式，那么它是在优化对数似然。因此，在对数似然上使用指数将我们带到了实际的似然。
- en: A.2.2 The choice of base/temperature for softmax
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.2 softmax 的基数/温度选择
- en: 'As an alternative to changing the base from *e*, you can divide the numerator
    and denominator by a constant. This technique is called changing the *temperature*
    of softmax, so it is typically represented by *T*, which is typically 1 in the
    literature when no number for temperature is reported:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为将基数从 *e* 改变的替代方案，你可以将分子和分母除以一个常数。这种技术被称为改变 softmax 的 *温度*，因此通常用 *T* 表示，在文献中通常没有报告温度数值时，*T*
    通常为 1：
- en: '![](../Images/APPA_F01_Munro_E01.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F01_Munro_E01.png)'
- en: Mathematically, there’s no difference between changing the softmax base and
    changing the temperature; you get the same sets of probability distributions (although
    not at the same rates). We use softmax base in this book because it makes some
    of the explanations in chapter 3 easier to understand. If you’re using a softmax
    function that doesn’t let you change the base, you might find it easier to experiment
    with temperature.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，改变 softmax 的基数和改变温度之间没有区别；你得到相同的概率分布集（尽管不是以相同的速率）。我们在这本书中使用 softmax
    基数，因为它使得第 3 章的一些解释更容易理解。如果你使用的是一个不允许你改变基数的 softmax 函数，你可能发现实验温度更容易一些。
- en: Why use base = *e*(or temperature = 1)? Honestly, the reason why *e* is the
    number we use for normalizing our data is a little shaky. In many areas of machine
    learning, *e* has special properties, *but this area isn’t one of theme*. Euler’s
    number (*e*) is approximately 2.71828\. As you’ll recall from your high school
    mathematics classes, *e*^x is its own derivative, and as a result, it has a lot
    of interesting properties. In machine learning, we particularly like the fact
    that *e*^x is the derivative of itself (figure A.2).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用基数 = *e*（或温度 = 1）？老实说，为什么 *e* 是我们用于归一化数据的数字有点不稳固。在机器学习的许多领域，*e* 具有特殊的性质，*但这个领域不是其中之一*。欧拉数
    (*e*) 大约是 2.71828。正如你从高中数学课程中回忆的那样，*e*^x 是其自身的导数，因此它具有许多有趣的性质。在机器学习中，我们特别喜欢 *e*^x
    是其自身的导数这一事实（图 A.2）。
- en: '![](../Images/APPA_F02_Munro.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F02_Munro.png)'
- en: Figure A.2 Graph showing e as its own integral. The slope at f(1) = 1 is 1,
    the slope at f(2) = 2 is 2, and so on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.2 显示 e 作为其自身的积分。在 f(1) = 1 处的斜率为 1，在 f(2) = 2 处的斜率为 2，依此类推。
- en: The slope at f(x) is f(x) for any given x, the slope of the *e*^x curve at f
    ́(1) is 1, the slope of the curve at f ́(2) is 2, and so on. You may remember
    this slope written as f ́(1) = 1 and f ́(2) = 2 in your high school mathematics
    books; the apostrophe indicates the derivative and is called *f prime*. Or you
    may have seen the slope written as dy/dx or ẏ. These three notations—f ́, dy/dx,
    and ẏ—come from different mathematicians (Lagrange, Leibniz, and Newton) respectively
    but mean the same thing. You probably used Lagrange’s notation in high school,
    Leibniz’s in a machine learning course, and Newton’s if you come from physics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在f(x)处的斜率是f(x)，对于任何给定的x，*e*^x曲线在f ́(1)处的斜率是1，曲线在f ́(2)处的斜率是2，依此类推。你可能记得在高中数学书中将这个斜率写成f
    ́(1) = 1和f ́(2) = 2；撇号表示导数，称为*导数*。或者你可能看到斜率写成dy/dx或ẏ。这三个符号—f ́, dy/dx, 和 ẏ—分别来自不同的数学家（Lagrange，Leibniz，和Newton），但意味着相同的事情。你可能在高中学过Lagrange的符号，在机器学习课程中学过Leibniz的符号，如果你来自物理学，你可能学过Newton的符号。
- en: The property of f ́(x) = f(x) is what we mean when we say that *e*^x is its
    own derivative. If you used any base other than *e* for the exponential curve,
    you would not get this property. In machine learning, we need to take derivatives
    of functions to converge them. The *learning* in *machine learning* is mainly
    converging functions, so when we know the derivative of a function is itself,
    we save a lot of compute power.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说*e*^x是其自身的导数时，我们指的是f(x) = f(x)的性质。如果你使用除了*e*以外的任何基数作为指数曲线，你就不会得到这个性质。在机器学习中，我们需要对函数求导以使其收敛。机器学习中的*学习*主要是函数的收敛，因此当我们知道函数的导数是其自身时，我们可以节省大量的计算能力。
- en: That doesn’t necessarily mean that *e* is the best number for your particular
    dataset when you are trying to find the best confidence measure, however. From
    the same input, compare the two graphs in figure A.3, which use *e* (2.71828)
    as the exponential base on the left and 10 as the exponential base on the right.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你试图找到最佳置信度度量时，这并不意味着*e*是适合你特定数据集的最佳数字。从相同的输入中，比较图A.3中的两个图表，左边的图表使用*e*（2.71828）作为指数基数，右边的图表使用10作为指数基数。
- en: '![](../Images/APPA_F03_Munro.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F03_Munro.png)'
- en: Figure A.3 Comparing two bases for exponentials (e and 10) for softmax on the
    same raw output data from a model. The graphs show that the higher the base, the
    higher the estimated probability of the highest score, with the highest score
    dominating the softmax equation to a greater extent at higher bases.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.3 比较softmax在相同原始输出数据上使用指数基数e和10的情况。图表显示，基数越高，最高分的估计概率越高，在更高的基数下，最高分在softmax方程中的主导作用越强。
- en: As you can see, the choice of exponent can matter a lot. If we use 10, confidence
    of “Pedestrian” in our data is now 90%, and the next-most-confident label is less
    than 10%. Table A.2 shows the scores from using 10 as the exponential base for
    softmax on our example data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，指数的选择可能非常重要。如果我们使用10，我们数据中“行人”的置信度为90%，而下一个最置信的标签不到10%。表A.2显示了使用10作为softmax的指数基数在我们示例数据上的分数。
- en: Table A.2 Repeating the softmax algorithm from the same scores (z, logits) but
    using 10 instead of e as the power
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.2 使用10而不是e作为幂的softmax算法从相同的分数（z，logits）重复
- en: '| Predicted label | Cyclist | Pedestrian | Sign | Animal |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | 骑行者 | 行人 | 标志 | 动物 |'
- en: '| scores (z[1], . . . z[4]) | 1.0 | 4.0 | 2.0 | 3.0 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 分数（z[1]，...z[4]） | 1.0 | 4.0 | 2.0 | 3.0 |'
- en: '| 10^z | 10.00 | 10000.00 | 100.00 | 1000.00 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 10^z | 10.00 | 10000.00 | 100.00 | 1000.00 |'
- en: '| softmax (10) | 0.09% | 90.01% | 0.90% | 9.00% |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| softmax (10) | 0.09% | 90.01% | 0.90% | 9.00% |'
- en: 'This table gives us a clearer idea of how important the largest number is.
    With 10 as the exponential base, we get 1 plus 4 zeros (10,000), which is clearly
    much bigger than any of the other numbers that get pushed down in the final softmax
    equation as a result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格让我们更清楚地了解最大数值的重要性。以10为指数基数，我们得到1加上4个零（10,000），这显然比在最终的softmax方程中被推下的其他任何数值都要大得多：
- en: '*The higher the exponential base for softmax, the more polarized the probabilities*.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*softmax的指数基数越高，概率的极化程度就越高*。'
- en: The choice of base won’t change which prediction is the most confident for a
    single item, so it is often overlooked in machine learning tasks when people care
    only about the predictive accuracy over the labels. The choice of base can change
    the rank order of confidence, however. That is, Item A might be more confident
    than Item B under base *e* but less confident under base 10.Table A.3 shows an
    example.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基础的选择不会改变单个项目哪个预测最自信，所以在人们只关心预测准确性而忽略标签的情况下，基础的选择在机器学习任务中常常被忽视。然而，基础的选择可以改变置信度的排名顺序。也就是说，项目A在基础*e*下可能比项目B更自信，但在基础10下则可能不那么自信。表A.3展示了这样一个例子。
- en: Table A.3 Two sets of possible inputs to softmax, which will be ranked differently
    depending on the base/temperature used
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.3：softmax可能的两组输入，根据使用的基数/温度不同，它们的排名会有所不同
- en: '| Predicted label | Cyclist | Pedestrian | Sign | Animal |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 预测标签 | 骑行者 | 行人 | 标志 | 动物 |'
- en: '| Inputs A | 3.22 | 2.88 | 3.03 | 3.09 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 输入A | 3.22 | 2.88 | 3.03 | 3.09 |'
- en: '| Inputs B | 3.25 | 3.24 | 3.23 | 1.45 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 输入B | 3.25 | 3.24 | 3.23 | 1.45 |'
- en: In both cases, A and B predict that “Cyclist” is the most likely label. But
    which one is more confident in the correct label? As figure A.4 shows, the answer
    depends on the base and temperature.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，A和B都预测“Cyclist”是最可能的标签。但哪个在正确标签上更自信？如图A.4所示，答案取决于基础和温度。
- en: 'Many people find it surprising that the graph in figure A.4 is possible, including
    a reviewer of this book, a reviewer at the prominent ICML conference, and a Turing
    Award winner, which is why I added this figure late in writing this book. Given
    a random set of inputs (in my experiments), you get the effect in figure A.4 for
    only about 1% of input pairs. For the least confident predictions when sampling
    for active learning, however, the samples can vary by up to 50%! Sampling the
    least confident items is the most common strategy for active learning and is discussed
    in chapter 3\. So this widespread misconception has been widely missed in human-in-the-loop
    machine learning: changing the base or temperature of softmax has the potential
    to create more accurate systems by manipulating variables that people previously
    assumed to be invariant.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人发现图A.4中的图形令人惊讶，包括本书的一位审稿人、一位在著名ICML会议上的审稿人，以及一位图灵奖获得者，这就是为什么我在本书写作后期添加了这个图。给定一个随机的输入集（在我的实验中），只有大约1%的输入对会产生图A.4中的效果。然而，在为主动学习采样最不自信的预测时，样本可以变化多达50%。在主动学习中，采样最不自信的项目是最常见的策略，这在第3章中有讨论。因此，这种广泛存在的误解在人类在环机器学习中已被广泛忽视：通过操纵人们以前认为是不变的变量，改变softmax的基础或温度有可能创建更准确的系统。
- en: '![](../Images/APPA_F04_Munro.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F04_Munro.png)'
- en: Figure A.4 Comparing the inputs in table A.3 (A = [3.22, 2.88, 3.03, 3.09] and
    B = [3.25, 3.24, 3.23, 1.45]) with different softmax bases and different temperature,
    showing either set of inputs could have the most confident result depending on
    the base or temperature. Strictly, the x-axis in the bottom graph is the inverse
    temperature, which is an valid equally scaling metric, although not as common.
    We use the inverse here to show both graphs going up and to the right in more
    or less the same way.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.4：比较表A.3中的输入（A = [3.22, 2.88, 3.03, 3.09] 和 B = [3.25, 3.24, 3.23, 1.45]）与不同的softmax基数和不同的温度，显示根据基数或温度，任一组输入都可能有最自信的结果。严格来说，底部图形的x轴是逆温度，这是一个有效的等比例度量标准，尽管不如常见。我们在这里使用逆温度是为了显示两个图形以更多或更少相同的方式向上和向右移动。
- en: Assume that in this text, softmax uses base = *e* and temperature = *e* in unless
    explicitly stated otherwise. For now, it is important to get an idea of softmax
    transforms your inputs into a probability distribution.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在本文中，除非明确说明，softmax使用基础 = *e* 和温度 = *e*。现在，重要的是要了解softmax如何将输入转换成概率分布。
- en: A.2.3 The result from dividing exponentials
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.3 指数除法的结果
- en: 'Remember that softmax normalizes the exponentials of the inputs, and recall
    from your high school mathematics the equation: c^((a-b)) = c^a / c^b. Therefore,
    when softmax normalizes the exponentials by dividing by all of them, the division
    of exponentials is essentially subtracting the absolute value of the scores. In
    other words, only the relative difference among the scores from your model counts
    with softmax, not their actual values.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，softmax函数会规范化输入的指数，并回想一下你高中数学中的公式：c^((a-b)) = c^a / c^b。因此，当softmax通过除以所有指数来规范化指数时，指数的除法本质上就是减去得分的绝对值。换句话说，只有模型得分之间的相对差异在softmax中起作用，而不是它们的实际值。
- en: Let’s plug in our scores of (1.0, 4.0, 2.0, 3.0) to create scenarios in which
    we add 10, 100, and –3 to each of them, so we are changing the sum of scores but
    we are keeping the differences between scores the same. As figure A.5 shows, the
    probability distributions are identical even though the raw scores differ considerably
    in each of the four sets of predictions because the differences between the four
    raw scores were identical. The difference between 4 and 3 is the same as the difference
    between 104 and 103\. This limitation is an important one to understand.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将(1.0, 4.0, 2.0, 3.0)的分数代入，以创建添加10、100和-3到每个分数的场景，这样我们改变了分数的总和，但保持了分数之间的差异。如图A.5所示，概率分布是相同的，尽管在四个预测集中的原始分数差异很大，因为四个原始分数之间的差异是相同的。4和3之间的差异与104和103之间的差异相同。这种限制是一个重要的理解点。
- en: '![](../Images/APPA_F05_Munro.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPA_F05_Munro.png)'
- en: 'Figure A.5 Softmax equivalencies: four model scores that give identical probability
    distributions under softmax. The four softmax probability distributions are identical
    despite coming from different model scores, showing that only the differences
    between the scores matter. The scores of (1, 4, 2, 3), for example, give the same
    probability distribution under softmax as (101, 104, 102, 103).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.5：Softmax等价性：四个模型分数在softmax下给出相同的概率分布。尽管来自不同的模型分数，但四个softmax概率分布是相同的，这表明只有分数之间的差异才是重要的。例如，(1,
    4, 2, 3)的分数在softmax下给出的概率分布与(101, 104, 102, 103)相同。
- en: To see this concept from another point of view, try multiplying each of (1.0,
    4.0, 2.0, 3.0) by a constant instead of adding a constant, as in figure A.5\.
    Figure A.6 shows the result from multiplying.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从另一个角度理解这个概念，尝试将(1.0, 4.0, 2.0, 3.0)中的每个数乘以一个常数，而不是像图A.5中那样加一个常数。图A.6显示了乘法的结果。
- en: '![](../Images/APPA_F06_Munro.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPA_F06_Munro.png)'
- en: Figure A.6 Two score distributions are identical except for scale. The right
    scores are 10 times the left scores. Under softmax, these scores result in different
    probability distributions. This figure also shows the result of changing the temperature
    rather than the base. If we start with the values on the left but lower the temperature
    to 0.1 (effectively multiplying the logits by 10), we get more weight going to
    the most confident predictions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.6：两个分数分布除了比例不同外是相同的。右侧的分数是左侧分数的10倍。在softmax下，这些分数导致不同的概率分布。此图还显示了改变温度而不是基数的效应。如果我们从左侧的值开始，但将温度降低到0.1（实际上是将logits乘以10），我们将得到更多权重分配给最自信的预测。
- en: In figure A.6, you can see that although the scores from the last layer differ
    only by the scale of the y-axis, they produce different probability distributions
    under softmax. For the distributions with lower scores, softmax produced a probability
    distribution that is a tighter set of numbers than the logits, but with the higher
    scores, it produced a wider distribution.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在图A.6中，你可以看到，尽管来自最后一层的分数仅在y轴的比例上有所不同，但在softmax下它们产生了不同的概率分布。对于分数较低的分布，softmax产生了一个比logits更紧密的数字集的概率分布，但分数较高时，它产生了一个更宽的分布。
- en: Be careful of large inputs into softmax
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 小心softmax中的大输入
- en: You run the risk of hardware overflow errors when using softmax with large input
    values, because the exponent step will produce large values. If you calculate
    e to the power of 1,000 on your computer, you may see a system error or an infinite
    value (inf), and this result can affect downstream processes. You have two ways
    to avoid this overflow, and I recommend using one of them if you decide to start
    experimenting with softmax.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用softmax处理大输入值时，你可能会遇到硬件溢出错误，因为指数步骤会产生大数值。如果你在电脑上计算1,000的e次方，你可能会看到系统错误或无限值（inf），这种结果可能会影响下游过程。你有两种方法可以避免这种溢出，如果你决定开始尝试使用softmax，我建议你使用其中一种方法。
- en: 'The first method is to subtract a constant from your inputs so that the maximum
    among your inputs is 0\. This method uses the phenomena in figure A.5 to your
    advantage: subtracting a constant gives you the same probability distribution
    without creating overflow during the exponential step. The second method is to
    use the log of softmax (PyTorch’s default behavior), which keeps the range of
    numbers contained.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是从你的输入中减去一个常数，使得你的输入中的最大值为0。这种方法利用了图A.5中的现象：减去一个常数给你相同的概率分布，而不会在指数步骤中产生溢出。第二种方法是使用softmax的对数（PyTorch的默认行为），它保持了数字的范围。
- en: In our examples so far, we have treated softmax as a normalization on the scores
    from an output layer. You can also use softmax as the activation function of the
    output layer itself. All the observations about the choice of base/temperature
    and how that spreads out the data in different ways still applies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中，我们将softmax视为输出层得分的归一化。你也可以将softmax用作输出层的激活函数。关于选择基础/温度以及它如何以不同方式分散数据的所有观察结果仍然适用。
- en: This section and the graphs associated with it are probably the longest description
    of softmax that you will read anywhere, but this information is important for
    human-in-the-loop machine learning. Softmax is the most common algorithm used
    to generate probability distributions from machine learning predictions, yet many
    people think that *e* as the choice of base has special properties for generating
    confidences (it does not), or that the choice of base won’t change the rank order
    of uncertainty. So, the ability to truly understand what softmax is doing will
    help you select the right uncertainty sampling strategy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节及其相关的图表可能是你将在任何地方读到的关于softmax的最长描述，但这个信息对于人机交互机器学习非常重要。Softmax是从机器学习预测中生成概率分布的最常用算法，但许多人认为选择基础为e具有生成置信度的特殊属性（它没有），或者选择基础不会改变不确定性的排名顺序。因此，真正理解softmax的作用将有助于你选择正确的不确定性采样策略。
- en: A.3 Measuring human-in-the-loop machine learning systems
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.3 测量人机交互的机器学习系统
- en: You have many ways to measure the success of a human-in-the-loop machine learning
    system, and the metrics you use will depend on your task. This section covers
    some of the most important metrics.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你有多种方式来衡量人机交互机器学习系统的成功，你使用的指标将取决于你的任务。本节涵盖了其中一些最重要的指标。
- en: A.3.1 Precision, recall, and F-score
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3.1 精确度、召回率和F分数
- en: For the machine learning algorithm, it is common to use the well-known metrics
    precision, recall, and F-score. *F-score* is the harmonic mean of precision and
    recall for a label, where true positives are the correct predictions for that
    label; *false positive* are items incorrectly predicted for that label; and *false
    negative* are items that have that label but were predicted to be something else.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习算法，通常使用众所周知的指标精确度、召回率和F分数。*F分数*是精确度和召回率的调和平均数，其中真实正例是该标签的正确预测；*假阳性*是该标签的错误预测项；*假阴性*是具有该标签但被预测为其他事物的项。
- en: '![](../Images/APPA_F06_Munro_E01.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPA_F06_Munro_E01.png)'
- en: If you use plain accuracy and your label is rare, then most of the accuracy
    will be determined by the large number of true negatives. One method of adjusting
    for this imbalance is known as *chance-adjusted agreement* which we’ll cover in
    the next section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用纯准确度并且你的标签是罕见的，那么大部分的准确度将由大量真实负例决定。调整这种不平衡的一种方法被称为*机会调整一致性*，我们将在下一节中介绍。
- en: A.3.2 Micro and macro precision, recall, and F-score
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3.2 微观和宏观精确度、召回率和F分数
- en: The calculations for precision, recall, and F-score are typically for one of
    the labels in the data. There are two common ways to combine the accuracies for
    each label into a single accuracy score. *Micro scores* aggregate accuracy at
    the per-item level, calculating for each item. *Macro scores* calculate accuracy
    for each label independently.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度、召回率和F分数的计算通常是针对数据中的一个标签。有两种常见的方法可以将每个标签的准确度组合成一个单一的准确度分数。*微观分数*在每项级别上汇总准确度，为每个项目进行计算。*宏观分数*独立地为每个标签计算准确度。
- en: If you have one label that is much more frequent than the other, that frequency
    will contribute most to the micro precision, micro recall, and micro F-scores.
    This result may be what you want in some cases, as it gives you an accuracy number
    that is weighted by the labels in your test data. But if you know that your test
    data is not balanced across the labels that your model will encounter when deployed,
    or if you want your model to be equally accurate in predicting all labels regardless
    of how frequent they are, macro accuracy scores are more appropriate.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个标签比其他标签出现频率高得多，那么这个频率将对微观精确度、微观召回率和微观F分数贡献最大。在某些情况下，这个结果可能正是你想要的，因为它给出了一个加权准确度数字，这个数字是根据你的测试数据中的标签来加权的。但是，如果你知道你的测试数据在模型部署时遇到的标签之间不平衡，或者你希望你的模型在预测所有标签时都能保持相同的准确度，无论它们的频率如何，那么宏观准确度分数更为合适。
- en: 'A.3.3 Taking random chance into account: Chance-adjusted accuracy'
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3.3 考虑随机机会：机会调整准确度
- en: 'Suppose that you have two labels, and they are equally frequent. If your model
    randomly predicts labels, it will still be 50% accurate. Obviously, that result
    is unfairly positive, making it hard to compare the accuracy with a different
    model in which more labels may not be balanced. Chance-adjusted accuracy makes
    the random-chance number 0 and scales the score accordingly:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有两个标签，并且它们出现的频率相等。如果你的模型随机预测标签，它仍然会是50%的准确性。显然，这个结果是不公平的积极结果，使得很难与不同模型进行比较，该模型中可能不是所有标签都平衡。调整随机概率的准确性将随机概率数设为0，并相应地调整分数：
- en: '![](../Images/APPA_F06_Munro_E02.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPA_F06_Munro_E02.png)'
- en: So if you were 60% accurate on the task with two labels of equal frequency,
    chance-adjusted accuracy is (60% – 50%) / (1 – 50%) = 20%. Although chance-adjusted
    accuracy is not commonly used for evaluating the accuracy of model predictions,
    it *is* widely used for evaluating the accuracy of human labeling. Chance-adjusted
    accuracy is more useful when you have big differences in the frequency of different
    labels. You have multiple ways to calculate random chance; we cover these techniques
    in chapter 8 when we focus on annotation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你在两个频率相等的标签的任务上准确率为60%，则调整随机概率的准确性为（60% - 50%）/（1 - 50%）= 20%。尽管调整随机概率的准确性不常用于评估模型预测的准确性，但它*确实*广泛用于评估人工标注的准确性。当不同标签的频率有较大差异时，调整随机概率的准确性更有用。你有多种计算随机概率的方法；我们在第8章中关注注释时将介绍这些技术。
- en: 'A.3.4 Taking confidence into account: Area under the ROC curve (AUC)'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3.4 考虑置信度：ROC曲线下的面积（AUC）
- en: In addition to accuracy over the predicted labels from a model, we care about
    whether confidence correlates with accuracy, so we can calculate area under the
    ROC curve (AUC). A ROC (receiver operating characteristic) curve rank-orders a
    dataset by confidence and calculates the rate of true positives versus false positives.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型对预测标签的准确性之外，我们还关心置信度是否与准确性相关，因此我们可以计算ROC曲线下的面积（AUC）。ROC（接收者操作特征）曲线按置信度对数据集进行排序，并计算真阳性与假阳性的比率。
- en: An example is shown in figure A.7\. The ROC curve is created by plotting the
    true positive rate (TPR) against the false positive rate (FPR) in an order determined
    by model confidence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 示例如图A.7所示。ROC曲线是通过按模型置信度确定的顺序绘制真阳性率（TPR）与假阳性率（FPR）来创建的。
- en: ROC curves can help us decide where we could trust the model’s decision and
    where we want to back off to human judgments. AUC is the calculation of the space
    under the curve relative to the overall space. You can eyeball AUC to be about
    0.80 in figure A.7.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线可以帮助我们决定在何处可以信任模型的决策，在何处我们希望退回到人工判断。AUC是曲线下空间相对于整体空间的计算。你可以从图A.7中估计AUC大约为0.80。
- en: '![](../Images/APPA_F07_Munro.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPA_F07_Munro.png)'
- en: Figure A.7 An example ROC curve, plotting TPR against FPR in an order determined
    by model confidence. In this example, we see that the line of the ROC curve is
    near vertical for the first 20%. This line tells us that for the 20% most confident
    predictions, we are almost 100% accurate. The ROC curve is almost horizontal at
    1.0 for the final 30%. This line tells us that by the time we get to the 30% least
    confident predictions for a label, few items with that label remain.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.7 一个ROC曲线示例，按模型置信度确定的顺序绘制TPR与FPR。在这个例子中，我们看到ROC曲线的前20%几乎垂直。这条线告诉我们，对于最自信的20%预测，我们几乎有100%的准确性。ROC曲线在1.0处几乎水平，对于最后的30%。这条线告诉我们，当我们到达标签30%最不自信的预测时，带有该标签的项目很少。
- en: AUC is the area under the curve generated by ROC as a percentage of the entire
    possible area. AUC is also the probability that of any two randomly selected items
    with different labels, the correct label was predicted with higher confidence.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: AUC是ROC曲线下面积占整个可能面积的比例。AUC也是随机选择两个具有不同标签的项目中，正确标签被更高置信度预测的概率。
- en: 'So we can calculate AUC by comparing the confidence of every item with label
    *e* with every item without the label (*u*):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过比较带有标签*e*的每个项目的置信度与没有标签的每个项目的置信度来计算AUC：
- en: '![](../Images/APPA_F07_Munro_E01.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/APPA_F07_Munro_E01.png)'
- en: This algorithm compares every item in each set with each other, so it has O(N²)
    complexity. You can order the items first and recursively find the ordering position
    for O(N Log(N)) complexity if you need to speed this calculation because of a
    large number of evaluation items.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法将每个集合中的每个项目与其他每个项目进行比较，因此它具有O(N²)的复杂度。如果你需要加快计算速度，因为评估项目数量很大，你可以首先对项目进行排序，并以O(N
    Log(N))的复杂度递归地找到排序位置。
- en: 'As we can calculate micro and macro precision, recall, and F-score, we can
    calculate micro and macro AUC:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以计算微观和宏观的精确度、召回率和F分数，我们还可以计算微观和宏观的AUC：
- en: '*Micro AUC*—Calculate the AUC, but instead of calculating it for items in one
    label, calculate it for all items across all labels.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*微观AUC*—计算AUC，但不是针对一个标签中的项目计算，而是针对所有标签中的所有项目计算。'
- en: '*Macro AUC*—Calculate the AUC for each label separately, and take the average
    AUC of all labels.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*宏观AUC*—分别计算每个标签的AUC，并取所有标签的平均AUC。'
- en: A.3.5 Number of model errors spotted
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3.5 发现的模型错误数量
- en: 'If you have a system in which a machine learning model backs off to a human
    when there might be errors, you can count the number of errors found. You might
    decide that anything below 50% confidence might be an error, for example, and
    put all those model predictions in front of a person to accept or correct:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个机器学习模型在可能出错时退回到人工的系统，你可以计算发现的错误数量。你可能决定低于50%的置信度可能是错误，例如，并将所有这些模型预测放在一个人面前接受或修正：
- en: '![](../Images/APPA_F07_Munro_E02.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F07_Munro_E02.png)'
- en: This equation tells you the percentage of items flagged for human review that
    needed correcting. One variation is to calculate the percentage of all errors,
    which gives you the total accuracy for humans plus model predictions. Another
    variation is to calculate the number of errors surfaced per hour or minute, which
    may make more sense if you have a fixed amount of time for the human component.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式告诉你需要人工修正的项目标记百分比。一种变化是计算所有错误的百分比，这给出了人类预测加上模型预测的总准确率。另一种变化是计算每小时或每分钟出现的错误数量，如果你有固定的时间用于人工组件，这可能更有意义。
- en: A.3.6 Human labor cost saved
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3.6 节省的人工劳动成本
- en: Another way to calculate the human cost is to measure how much time and effort
    were saved. Whether you’re using active learning to be smarter about which items
    to label (chapters 3–6) or improving the quality controls and interface for annotation
    (chapters 8–11), improving the efficiency, accuracy, and user experience of the
    human component of a human-in-the-loop system may be more important than making
    small changes in the accuracy of the models. Figure A.8 shows an example.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种计算人力成本的方法是衡量节省了多少时间和精力。无论你是使用主动学习来更明智地选择哪些项目进行标注（第3-6章）还是改进标注的质量控制和界面（第8-11章），提高人工组件在人工反馈系统中的效率、准确性和用户体验可能比在模型准确度上做出的小幅改变更为重要。图A.8展示了示例。
- en: '![](../Images/APPA_F08_Munro.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/APPA_F08_Munro.png)'
- en: Figure A.8 The reduction in labels is needed. In this example, the strategy
    that uses active learning (chapters 3–6) reaches the same accuracy as random sampling
    with fewer than half as many labels. B / (a + b) = 53% reduction in labels needed.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图A.8 标签数量的减少是必要的。在这个例子中，使用主动学习（第3-6章）的策略达到与随机抽样相同的准确度，但所需的标签数量不到一半。B / (a +
    b) = 标签需要减少53%。 '
- en: As figure A.8 shows, active learning can reduce the number of labels required
    by 53% when we look at the x-axis, but if we look at the y-axis, the accuracy
    difference at that point is about 20%. If you come from an algorithms background,
    you are probably more accustomed to looking at the y-axis because you typically
    compare two algorithms on the same data. So if you are comparing the same algorithm
    on two different datasets, the x-axis has the more important numbers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如图A.8所示，当我们查看x轴时，主动学习可以将所需的标签数量减少53%，但如果查看y轴，该点的准确度差异大约为20%。如果你来自算法背景，你可能更习惯于查看y轴，因为你通常在相同的数据上比较两个算法。所以如果你在比较两个不同数据集上的相同算法，x轴有更重要的数字。
- en: A.3.7 Other methods for calculating accuracy in this book
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3.7 本书计算准确度的其他方法
- en: 'This appendix covers the most standard methods for calculating accuracy, but
    some accuracy metrics that are specific to machine learning are not covered here:
    bilingual evaluation understudy (BLEU) for language generation; intersection over
    union (IoU) for object detection; per-demographic accuracies; and chance-adjusted
    agreement for human annotations. These metrics are introduced at the appropriate
    places in the book, and so you don’t need to understand them yet in this refresher.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录涵盖了计算准确度的最标准方法，但一些特定于机器学习的准确度指标在此未涵盖：用于语言生成的双语评估助手（BLEU）；用于目标检测的交集与并集（IoU）；按人口统计的准确度；以及人类标注的调整后一致性。这些指标在书中的适当位置介绍，因此你在此复习中不需要理解它们。
- en: '* * *'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)An earlier version of this chapter used the NumPy library instead of the
    PyTorch library. You can see those examples at [http://mng.bz/Xd4p](http://mng.bz/Xd4p).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (1.)本章的早期版本使用了NumPy库而不是PyTorch库。您可以在[http://mng.bz/Xd4p](http://mng.bz/Xd4p)查看那些示例。
