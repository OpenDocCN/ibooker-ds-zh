- en: Chapter 5\. Anomaly Detection with K-means Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。使用K均值聚类进行异常检测
- en: 'Classification and regression are powerful, well-studied techniques in machine
    learning. [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)
    demonstrated using a classifier as a predictor of unknown values. But there was
    a catch: to predict unknown values for new data, we had to know the target values
    for many previously seen examples. Classifiers can help only if we, the data scientists,
    know what we are looking for and can provide plenty of examples where input produced
    a known output. These were collectively known as *supervised learning* techniques,
    because their learning process receives the correct output value for each example
    in the input.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和回归是机器学习中强大且深入研究的技术。[第4章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)展示了如何使用分类器作为未知值的预测器。但是有一个问题：为了预测新数据的未知值，我们必须知道许多先前见过的示例的目标值。只有当数据科学家知道自己在寻找什么并且可以提供大量示例，输入才能产生已知输出时，分类器才能提供帮助。这些被统称为*监督学习*技术，因为它们的学习过程为输入中的每个示例接收正确的输出值。
- en: 'However, sometimes the correct output is unknown for some or all examples.
    Consider the problem of dividing up an ecommerce site’s customers by their shopping
    habits and tastes. The input features are their purchases, clicks, demographic
    information, and more. The output should be groupings of customers: perhaps one
    group will represent fashion-conscious buyers, another will turn out to correspond
    to price-sensitive bargain hunters, and so on.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时对于某些或所有示例，正确的输出是未知的。考虑将电子商务网站的客户根据其购物习惯和喜好分组的问题。输入特征包括他们的购买、点击、人口统计信息等。输出应该是客户的分组：也许一个组将代表注重时尚的购买者，另一个组可能对应于价格敏感的猎奇者，等等。
- en: 'If you were asked to determine this target label for each new customer, you
    would quickly run into a problem in applying a supervised learning technique like
    a classifier: you don’t know a priori who should be considered fashion-conscious,
    for example. In fact, you’re not even sure if “fashion-conscious” is a meaningful
    grouping of the site’s customers to begin with!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要为每个新客户确定这个目标标签，您将很快在应用监督学习技术（如分类器）时遇到问题：您事先不知道谁应被认为是时尚意识强的人。实际上，您甚至不确定“时尚意识强”的定义是否能够有效地将网站的客户分组起来！
- en: Fortunately, *unsupervised learning* techniques can help. These techniques do
    not learn to predict a target value, because none is available. They can, however,
    learn structure in data and find groupings of similar inputs, or learn what types
    of input are likely to occur and what types are not. This chapter will introduce
    unsupervised learning using clustering implementations in MLlib. Specifically,
    we will use the K-means clustering algorithm for identifying anomalies in network
    traffic data. Anomaly detection is often used to find fraud, detect network attacks,
    or discover problems in servers or other sensor-equipped machinery. In these cases,
    it’s important to be able to find new types of anomalies that have never been
    seen before—new forms of fraud, intrusions, and failure modes for servers. Unsupervised
    learning techniques are useful in these cases because they can learn what input
    data normally looks like and therefore detect when new data is unlike past data.
    Such new data is not necessarily attacks or fraud; it is simply unusual and therefore
    worth further investigation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，*无监督学习*技术可以提供帮助。这些技术不会学习预测目标值，因为没有目标值可用。然而，它们可以学习数据中的结构，并找到相似输入的分组，或者学习哪些类型的输入可能发生，哪些不可能。本章将介绍使用MLlib中的聚类实现的无监督学习。具体来说，我们将使用K均值聚类算法来识别网络流量数据中的异常。异常检测通常用于发现欺诈、检测网络攻击或发现服务器或其他传感器设备中的问题。在这些情况下，能够发现以前从未见过的新类型异常是非常重要的——新形式的欺诈、入侵和服务器故障模式。无监督学习技术在这些情况下很有用，因为它们可以学习输入数据通常的外观，并因此在新数据与过去数据不同之时进行检测。这样的新数据不一定是攻击或欺诈；它只是不寻常，因此值得进一步调查。
- en: We will start with the basics of the K-means clustering algorithm. This will
    be followed by an introduction to the KDD Cup 1999 dataset. We’ll then create
    our first K-means model using PySpark. Then we’ll go over methods for determining
    a good value of *k*—number of clusters—when implementing the K-means algorithm.
    Next, we’ll improve our model by normalizing the input features and using previously
    discarded categorical features by implementing the one-hot encoding method. We
    will wrap up by going over the entropy metric and exploring some results from
    our model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从K均值聚类算法的基础开始。接着介绍KDD Cup 1999数据集。然后使用PySpark创建我们的第一个K均值模型。然后我们将讨论在实施K均值算法时确定好的*k*值（簇的数量）的方法。接下来，我们通过实现一位热编码方法来改进我们的模型，通过归一化输入特征和使用先前被丢弃的分类特征。最后，我们将回顾熵指标并探索一些我们模型的结果。
- en: K-means Clustering
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值聚类
- en: The inherent problem of anomaly detection is, as its name implies, that of finding
    unusual things. If we already knew what “anomalous” meant for a dataset, we could
    easily detect anomalies in the data with supervised learning. An algorithm would
    receive inputs labeled “normal” and “anomaly” and learn to distinguish the two.
    However, the nature of anomalies is that they are unknown unknowns. Put another
    way, an anomaly that has been observed and understood is no longer an anomaly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测的固有问题，顾名思义，是寻找不寻常的事物。如果我们已经知道数据集中“异常”的含义，我们可以很容易地通过监督学习检测数据中的异常。算法会接收标记为“正常”和“异常”的输入，并学会区分两者。然而，异常的本质在于它们是未知的未知。换句话说，一旦观察并理解了的异常就不再是异常了。
- en: Clustering is the best-known type of unsupervised learning. Clustering algorithms
    try to find natural groupings in data. Data points that are like one another but
    unlike others are likely to represent a meaningful grouping, so clustering algorithms
    try to put such data into the same cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是最知名的无监督学习类型。聚类算法试图在数据中找到自然的分组。那些相似但不同于其他数据点的数据点可能代表一个有意义的分组，因此聚类算法试图将这样的数据放入同一簇中。
- en: K-means clustering may be the most widely used clustering algorithm. It attempts
    to detect *k* clusters in a dataset, where *k* is given by the data scientist.
    *k* is a hyperparameter of the model, and the right value will depend on the dataset.
    In fact, choosing a good value for *k* will be a central plot point in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: K均值聚类可能是最广泛使用的聚类算法。它试图在数据集中检测*k*个簇，其中*k*由数据科学家给定。*k*是模型的超参数，合适的值将取决于数据集。事实上，在本章中选择一个合适的*k*值将是一个核心情节点。
- en: What does “like” mean when the dataset contains information such as customer
    activity? Or transactions? K-means requires a notion of distance between data
    points. It is common to use simple Euclidean distance to measure distance between
    data points with K-means, and as it happens, this is one of two distance functions
    supported by Spark MLlib as of this writing, the other one being Cosine. The Euclidean
    distance is defined for data points whose features are all numeric. “Like” points
    are those whose intervening distance is small.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集包含客户活动或交易等信息时，“相似”是什么意思？K均值需要一个数据点之间距离的概念。通常使用简单的欧氏距离来测量K均值中数据点之间的距离，正如现在这篇文章中所做的一样，这也是Spark
    MLlib支持的两种距离函数之一，另一种是余弦距离。欧氏距离适用于所有特征都是数值的数据点。“相似”的点是那些中间距离较小的点。
- en: 'To K-means, a cluster is simply a point: the center of all the points that
    make up the cluster. These are, in fact, just feature vectors containing all numeric
    features and can be called vectors. However, it may be more intuitive to think
    of them as points here, because they are treated as points in a Euclidean space.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于K均值来说，一个簇就是一个点：所有构成该簇的点的中心。事实上，这些仅仅是包含所有数值特征的特征向量，并且可以称为向量。然而，在这里把它们看作点可能更直观，因为它们在欧几里得空间中被视为点。
- en: This center is called the cluster *centroid* and is the arithmetic mean of the
    points—hence the name K-*means*. To start, the algorithm picks some data points
    as the initial cluster centroids. Then each data point is assigned to the nearest
    centroid. Then for each cluster, a new cluster centroid is computed as the mean
    of the data points just assigned to that cluster. This process is repeated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此中心称为聚类的*质心*，是点的算术平均值，因此得名K-*means*。算法首先选择一些数据点作为初始的聚类质心。然后将每个数据点分配给最近的质心。然后对于每个簇，计算新的聚类质心作为刚分配到该簇的数据点的平均值。这个过程重复进行。
- en: We will now look at a use case that depicts how K-means clustering can help
    us identify potentially anomalous activity in a network.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将介绍一个用例，描述K均值聚类如何帮助我们识别网络中潜在的异常活动。
- en: Identifying Anomalous Network Traffic
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现异常网络流量
- en: Cyberattacks are increasingly visible in the news. Some attacks attempt to flood
    a computer with network traffic to crowd out legitimate traffic. But in other
    cases, attacks attempt to exploit flaws in networking software to gain unauthorized
    access to a computer. While it’s quite obvious when a computer is being bombarded
    with traffic, detecting an exploit can be like searching for a needle in an incredibly
    large haystack of network requests.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 网络攻击越来越频繁地出现在新闻中。一些攻击试图用网络流量淹没计算机，以排挤合法流量。但在其他情况下，攻击试图利用网络软件中的漏洞来未经授权地访问计算机。当计算机被大量流量轰炸时很明显，但检测利用漏洞可以像在大量网络请求的巨大干草堆中寻找针一样困难。
- en: Some exploit behaviors follow known patterns. For example, accessing every port
    on a machine in rapid succession is not something any normal software program
    should ever need to do. However, it is a typical first step for an attacker looking
    for services running on the computer that may be exploitable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击行为遵循已知模式。例如，快速连续访问机器上的每个端口并非任何正常软件程序所需。然而，这是攻击者寻找可能易受攻击的计算机服务的典型第一步。
- en: If you were to count the number of distinct ports accessed by a remote host
    in a short time, you would have a feature that probably predicts a port-scanning
    attack quite well. A handful is probably normal; hundreds indicate an attack.
    The same goes for detecting other types of attacks from other features of network
    connections—number of bytes sent and received, TCP errors, and so forth.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算远程主机在短时间内访问的不同端口数量，你可能会得到一个相当好的预测端口扫描攻击的特征。几个端口可能是正常的；数百个表示攻击。检测网络连接其他特征的其他类型攻击也是如此——发送和接收的字节数、TCP错误等。
- en: But what about those unknown unknowns? The biggest threat may be the one that
    has never yet been detected and classified. Part of detecting potential network
    intrusions is detecting anomalies. These are connections that aren’t known to
    be attacks but do not resemble connections that have been observed in the past.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么未知的未知情况呢？最大的威胁可能是从未被检测和分类的威胁。检测潜在网络入侵的一部分是检测异常情况。这些连接不被认为是攻击，但与过去观察到的连接不相似。
- en: Here, unsupervised learning techniques like K-means can be used to detect anomalous
    network connections. K-means can cluster connections based on statistics about
    each of them. The resulting clusters themselves aren’t interesting per se, but
    they collectively define types of connections that are like past connections.
    Anything not close to a cluster could be anomalous. Clusters are interesting insofar
    as they define regions of normal connections; everything else is unusual and potentially
    anomalous.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，像K均值这样的无监督学习技术可以用来检测异常网络连接。K均值可以根据每个连接的统计信息进行聚类。结果的聚类本身并不有趣，但它们集体定义了与过去连接类似的连接类型。与聚类不接近的任何连接可能是异常的。聚类之所以有趣，是因为它们定义了正常连接的区域；其他一切都是不寻常的，可能是异常的。
- en: KDD Cup 1999 Dataset
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KDD Cup 1999数据集
- en: The [KDD Cup](https://oreil.ly/UtYd9) was an annual data mining competition
    organized by a special interest group of the Association for Computing Machinery
    (ACM). Each year, a machine learning problem was posed, along with a dataset,
    and researchers were invited to submit a paper detailing their best solution to
    the problem. In 1999, the topic was network intrusion, and the dataset is [still
    available](https://oreil.ly/ezBDa) at the KDD website. We will need to download
    the *kddcupdata.data.gz* and *kddcup.info* files from the website. The remainder
    of this chapter will walk through building a system to detect anomalous network
    traffic using Spark, by learning from this data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[KDD Cup](https://oreil.ly/UtYd9) 是由计算机协会的一个特别兴趣小组组织的年度数据挖掘竞赛。每年，他们会提出一个机器学习问题，并提供一个数据集，邀请研究人员提交详细描述他们对问题的最佳解决方案的论文。1999年的主题是网络入侵，数据集仍然可以在[KDD网站](https://oreil.ly/ezBDa)上找到。我们需要从该网站下载*kddcupdata.data.gz*和*kddcup.info*文件。本章的其余部分将演示如何使用Spark构建系统来检测异常网络流量。'
- en: Don’t use this dataset to build a real network intrusion system! The data did
    not necessarily reflect real network traffic at the time—even if it did, it reflects
    traffic patterns from more than 20 years ago.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不要使用此数据集构建真实的网络入侵系统！该数据并不一定反映当时的真实网络流量——即使它反映了，它也反映了20多年前的流量模式。
- en: Fortunately, the organizers had already processed raw network packet data into
    summary information about individual network connections. The dataset is about
    708 MB in size and contains about 4.9 million connections. This is large, if not
    massive, and is certainly sufficient for our purposes here. For each connection,
    the dataset contains information such as the number of bytes sent, login attempts,
    TCP errors, and so on. Each connection is one line of CSV-formatted data, containing
    38 features. Feature information and ordering can be found in the *kddcup.info*
    file.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，组织者已经将原始网络数据处理成关于各个网络连接的摘要信息。数据集大小约为708 MB，包含约490万个连接。对于我们的目的来说，这是一个很大甚至是庞大的数据集，绝对足够了。对于每个连接，数据集包含诸如发送的字节数、登录尝试、TCP错误等信息。每个连接是一个CSV格式的数据行，包含38个特征。特征信息和顺序可以在*kddcup.info*文件中找到。
- en: 'Unzip the *kddcup.data.gz* data file and copy it into your storage. This example,
    like others, will assume the file is available at *data/kddcup.data*. Let’s see
    the data in its raw form:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 解压*kddcup.data.gz*数据文件并将其复制到您的存储中。例如，假设文件位于*data/kddcup.data*。让我们看看数据的原始形式：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This connection, for example, was a TCP connection to an HTTP service—215 bytes
    were sent, and 45,706 bytes were received. The user was logged in, and so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这个连接是一个TCP连接到HTTP服务——发送了215字节，接收了45,706字节。用户已登录等等。
- en: Many features are counts, like `num_file_creations` in the 17th column, as listed
    in the *kddcup.info* file. Many features take on the value 0 or 1, indicating
    the presence or absence of a behavior, like `su_attempted` in the 15th column.
    They look like the one-hot encoded categorical features from [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests),
    but are not grouped and related in the same way. Each is like a yes/no feature,
    and is therefore arguably a categorical feature. It is not always valid to translate
    categorical features as numbers and treat them as if they had an ordering. However,
    in the special case of a binary categorical feature, in most machine learning
    algorithms, mapping these to a numeric feature taking on values 0 and 1 will work
    well.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 许多特征是计数，例如在第17列中列出的`num_file_creations`，如*kddcup.info*文件中所示。许多特征取值为0或1，表示行为的存在或不存在，例如在第15列中的`su_attempted`。它们看起来像是来自[第四章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)的独热编码分类特征，但并非以同样的方式分组和相关。每个特征都像是一个是/否特征，因此可以说是一个分类特征。通常情况下，将分类特征转换为数字并视其具有顺序是不总是有效的。但是，在二元分类特征的特殊情况下，在大多数机器学习算法中，将其映射为取值为0和1的数值特征将效果很好。
- en: The rest are ratios like `dst_host_srv_rerror_rate` in the next-to-last column
    and take on values from 0.0 to 1.0, inclusive.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的是像`dst_host_srv_rerror_rate`这样的比率，位于倒数第二列，并且取值范围从0.0到1.0，包括0.0和1.0。
- en: Interestingly, a label is given in the last field. Most connections are labeled
    `normal.`, but some have been identified as examples of various types of network
    attacks. These would be useful in learning to distinguish a known attack from
    a normal connection, but the problem here is anomaly detection and finding potentially
    new and unknown attacks. This label will be mostly set aside for our purposes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，标签位于最后一个字段中。大多数连接被标记为`normal.`，但有些被识别为各种类型的网络攻击的示例。这些将有助于学习区分已知攻击和正常连接，但问题在于异常检测和发现潜在的新攻击和未知攻击。对于我们的目的，这个标签将大部分被忽略。
- en: A First Take on Clustering
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对聚类的初步尝试
- en: Open the `pyspark-shell`, and load the CSV data as a dataframe. It’s a CSV file
    again, but without header information. It’s necessary to supply column names as
    given in the accompanying *kddcup.info* file.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`pyspark-shell`，并将CSV数据加载为数据帧。这又是一个没有头部信息的CSV文件。需要按照附带的*kddcup.info*文件中给出的列名提供列名。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Begin by exploring the dataset. What labels are present in the data, and how
    many are there of each? The following code simply counts by label and prints the
    results in descending order by count:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从探索数据集开始。数据中有哪些标签，每个标签有多少个？以下代码简单地按标签计数并按计数降序打印结果：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are 23 distinct labels, and the most frequent are `smurf.` and `neptune.`
    attacks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有23个不同的标签，最频繁的是`smurf.`和`neptune.`攻击。
- en: Note that the data contains nonnumeric features. For example, the second column
    may be `tcp`, `udp`, or `icmp`, but K-means clustering requires numeric features.
    The final label column is also nonnumeric. To begin, these will simply be ignored.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，数据中包含非数值特征。例如，第二列可能是`tcp`、`udp`或`icmp`，但是K均值聚类需要数值特征。最终的标签列也是非数值的。因此，在开始时，这些特征将被简单地忽略。
- en: Aside from this, creating a K-means clustering of the data follows the same
    pattern as was seen in [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests).
    A `VectorAssembler` creates a feature vector, a `KMeans` implementation creates
    a model from the feature vectors, and a `Pipeline` stitches it all together. From
    the resulting model, it’s possible to extract and examine the cluster centers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，创建数据的K均值聚类与[第四章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)中看到的模式相同。`VectorAssembler`创建特征向量，`KMeans`实现从特征向量创建模型，而`Pipeline`则将它们全部串联起来。从生成的模型中，可以提取并检查聚类中心。
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It’s not easy to interpret the numbers intuitively, but each of these represents
    the center (also known as centroid) of one of the clusters that the model produced.
    The values are the coordinates of the centroid in terms of each of the numeric
    input features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不容易直观地解释这些数字，但每个数字代表模型生成的一个聚类中心（也称为质心）。这些值是每个数值输入特征的质心坐标。
- en: Two vectors are printed, meaning K-means was fitting *k*=2 clusters to the data.
    For a complex dataset that is known to exhibit at least 23 distinct types of connections,
    this is almost certainly not enough to accurately model the distinct groupings
    within the data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 打印了两个向量，这意味着K均值将*k*=2个群集适合于数据。对于已知至少具有23种不同连接类型的复杂数据集来说，这几乎肯定不足以准确建模数据中的不同分组。
- en: This is a good opportunity to use the given labels to get an intuitive sense
    of what went into these two clusters by counting the labels within each cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用给定标签获取直观感觉的好机会，以了解这两个聚类中的内容，通过计算每个聚类内的标签数量。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result shows that the clustering was not at all helpful. Only one data point
    ended up in cluster 1!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示聚类并没有提供任何帮助。只有一个数据点最终进入了群集1！
- en: Choosing k
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择k值
- en: Two clusters are plainly insufficient. How many clusters are appropriate for
    this dataset? It’s clear that there are 23 distinct patterns in the data, so it
    seems that *k* could be at least 23, or likely even more. Typically, many values
    of *k* are tried to find the best one. But what is “best”?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，两个聚类是不够的。对于这个数据集来说，适当的聚类数量是多少？显然，数据中有23种不同的模式，因此*k*至少应该是23，甚至可能更多。通常，会尝试多个*k*值来找到最佳值。但是，“最佳”是什么？
- en: A clustering could be considered good if each data point were near its closest
    centroid, where “near” is defined by the Euclidean distance. This is a simple,
    common way to evaluate the quality of a clustering, by the mean of these distances
    over all points, or sometimes, the mean of the distances squared. In fact, `KMeansModel`
    offers a `ClusteringEvaluator` method that computes the sum of squared distances
    and can easily be used to compute the mean squared distance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个数据点都接近其最近的质心，则可以认为聚类是好的，其中“接近”由欧氏距离定义。这是评估聚类质量的简单常见方法，通过所有点上这些距离的均值，或者有时是距离平方的均值。实际上，`KMeansModel`提供了一个`ClusteringEvaluator`方法，可以计算平方距离的和，并且可以轻松地用于计算平均平方距离。
- en: 'It’s simple enough to manually evaluate the clustering cost for several values
    of *k*. Note that this code could take 10 minutes or more to run:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对几个*k*值手动评估聚类成本是相当简单的。请注意，此代码可能需要运行10分钟或更长时间：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1)'
- en: Scores will be shown here using scientific notation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分数将使用科学计数法显示。
- en: The printed result shows that the score decreases as *k* increases. Note that
    scores are shown in scientific notation; the first value is over 10⁷, not just
    a bit over 6.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的结果显示随着*k*的增加，分数下降。请注意，分数使用科学计数法显示；第一个值超过了10⁷，而不仅仅是略高于6。
- en: Again, your values will be somewhat different. The clustering depends on a randomly
    chosen initial set of centroids.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您的数值将会有所不同。聚类依赖于随机选择的初始质心集。
- en: However, this much is obvious. As more clusters are added, it should always
    be possible to put data points closer to the nearest centroid. In fact, if *k*
    is chosen to equal the number of data points, the average distance will be 0 because
    every point will be its own cluster of one!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一点是显而易见的。随着添加更多的簇，总是可以将数据点放置在最近的质心附近。事实上，如果*k*被选为数据点的数量，那么平均距离将为0，因为每个点将成为自己的一个包含一个点的簇！
- en: Worse, in the preceding results, the distance for *k*=80 is higher than for
    *k*=60\. This shouldn’t happen because a higher *k* always permits at least as
    good a clustering as a lower *k*. The problem is that K-means is not necessarily
    able to find the optimal clustering for a given *k*. Its iterative process can
    converge from a random starting point to a local minimum, which may be good but
    is not optimal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，在先前的结果中，*k*=80的距离比*k*=60的距离要大。这不应该发生，因为更高的*k*至少应该允许与更低的*k*一样好的聚类。问题在于，对于给定的*k*，K-means不一定能找到最优的聚类。它的迭代过程可以从一个随机起始点收敛到一个局部最小值，这可能很好但不是最优的。
- en: This is still true even when more intelligent methods are used to choose initial
    centroids. [K-means++ and K-means||](https://oreil.ly/zes8d) are variants of selection
    algorithms that are more likely to choose diverse, separated centroids and lead
    more reliably to good clustering. Spark MLlib, in fact, implements K-means||.
    However, all still have an element of randomness in selection and can’t guarantee
    an optimal clustering.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在使用更智能的方法选择初始质心时，这一点仍然是真实的。[K-means++和K-means||](https://oreil.ly/zes8d)是选择算法的变体，更有可能选择多样化、分离的质心，并更可靠地导致良好的聚类。事实上，Spark
    MLlib实现了K-means||。然而，所有这些算法在选择时仍然具有随机性，并不能保证最优的聚类。
- en: The random starting set of clusters chosen for *k*=80 perhaps led to a particularly
    suboptimal clustering, or it may have stopped early before it reached its local
    optimum.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的随机起始簇集合*k*=80可能导致特别不理想的聚类，或者在达到局部最优之前可能会提前停止。
- en: We can improve it by running the iteration longer. The algorithm has a threshold
    via `setTol` that controls the minimum amount of cluster centroid movement considered
    significant; lower values mean the K-means algorithm will let the centroids continue
    to move longer. Increasing the maximum number of iterations with `setMaxIter`
    also prevents it from potentially stopping too early at the cost of possibly more
    computation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过延长迭代时间来改善它。算法通过`setTol`设定了一个阈值，用于控制被认为是显著的簇质心移动的最小量；较低的值意味着K-means算法将允许质心继续移动更长时间。通过`setMaxIter`增加最大迭代次数也可以防止算法在可能的情况下过早停止，但可能会增加计算量。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1)'
- en: Increase from default 20.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从默认的20增加。
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2)'
- en: Decrease from default 1.0e-4.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从默认的1.0e-4减少。
- en: 'This time, at least the scores decrease consistently:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，至少得分是一致递减的：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We want to find a point past which increasing *k* stops reducing the score much—or
    an “elbow” in a graph of *k* versus score, which is generally decreasing but eventually
    flattens out. Here, it seems to be decreasing notably past 100\. The right value
    of *k* may be past 100.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到一个点，在这个点之后增加*k*不会显著减少得分，或者在*k*与得分之间的图形中找到一个“拐点”，该图形通常是递减的，但最终会趋于平缓。在这里，看起来过了100之后递减明显。*k*的合适值可能超过100。
- en: Visualization with SparkR
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR进行可视化
- en: At this point, it could be useful to step back and understand more about the
    data before clustering again. In particular, looking at a plot of the data points
    could be helpful.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，重新进行聚类之前了解更多关于数据的信息可能会有帮助。特别是查看数据点的图表可能会有所帮助。
- en: Spark itself has no tools for visualization, but the popular open source statistical
    environment [R](https://www.r-project.org) has libraries for both data exploration
    and data visualization. Furthermore, Spark also provides some basic integration
    with R via [SparkR](https://oreil.ly/XX0Q9). This brief section will demonstrate
    using R and SparkR to cluster the data and explore the clustering.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Spark本身没有用于可视化的工具，但流行的开源统计环境[R](https://www.r-project.org)具有用于数据探索和数据可视化的库。此外，Spark还通过[SparkR](https://oreil.ly/XX0Q9)提供与R的基本集成。本简短部分将演示使用R和SparkR对数据进行聚类和探索聚类。
- en: SparkR is a variant of the `spark-shell` used throughout this book and is invoked
    with the command `sparkR`. It runs a local R interpreter, like `spark-shell` runs
    a variant of the Scala shell as a local process. The machine that runs `sparkR`
    needs a local installation of R, which is not included with Spark. This can be
    installed, for example, with `sudo apt-get install r-base` on Linux distributions
    like Ubuntu, or `brew install R` with [Homebrew](http://brew.sh) on macOS.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 是本书中贯穿始终的 `spark-shell` 的一个变体，可以通过命令 `sparkR` 调用。它运行一个本地 R 解释器，就像 `spark-shell`
    运行 Scala shell 的变体作为本地进程一样。运行 `sparkR` 的机器需要一个本地安装的 R，Spark 不包含在内。例如，在 Ubuntu
    等 Linux 发行版上可以通过 `sudo apt-get install r-base` 安装它，或者在 macOS 上可以通过 [Homebrew](http://brew.sh)
    使用 `brew install R` 安装。
- en: SparkR is a command-line shell environment, like R. To view visualizations,
    it’s necessary to run these commands within an IDE-like environment that can display
    images. [RStudio](https://www.rstudio.com) is an IDE for R (and works with SparkR);
    it runs on a desktop operating system so it will be usable here only if you are
    experimenting with Spark locally rather than on a cluster.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 是一个类似于 R 的命令行 shell 环境。要查看可视化效果，需要在能够显示图片的类 IDE 环境中运行这些命令。[RStudio](https://www.rstudio.com)
    是 R 的 IDE（也适用于 SparkR）；它运行在桌面操作系统上，因此只有在本地实验 Spark 而不是在集群上时才能使用它。
- en: If you are running Spark locally, [download](https://oreil.ly/JZGQm) the free
    version of RStudio and install it. If not, then most of the rest of this example
    can still be run with `sparkR` on a command line—for example, on a cluster—though
    it won’t be possible to display visualizations this way.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本地运行 Spark，请[下载](https://oreil.ly/JZGQm)免费版的 RStudio 并安装它。如果不是，那么本示例的大部分仍可在命令行上使用
    `sparkR` 运行，例如在集群上，尽管无法以此方式显示可视化结果。
- en: 'If you’re running via RStudio, launch the IDE and configure `SPARK_HOME` and
    `JAVA_HOME`, if your local environment does not already set them, to point to
    the Spark and JDK installation directories, respectively:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过 RStudio 运行，请启动 IDE 并配置 `SPARK_HOME` 和 `JAVA_HOME`，如果本地环境尚未设置它们，则设置为指向 Spark
    和 JDK 安装目录：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#comarker1a)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#comarker1a)'
- en: Replace with actual paths, of course.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，需要用实际路径替换。
- en: Note that these steps aren’t needed if you are running `sparkR` on the command
    line. Instead, it accepts command-line configuration parameters such as `--driver-memory`,
    just like `spark-shell`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果在命令行上运行 `sparkR`，则不需要这些步骤。相反，它接受命令行配置参数，如 `--driver-memory`，就像 `spark-shell`
    一样。
- en: 'SparkR is an R-language wrapper around the same DataFrame and MLlib APIs that
    have been demonstrated in this chapter. It’s therefore possible to re-create a
    K-means simple clustering of the data:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 是围绕相同的 DataFrame 和 MLlib API 的 R 语言包装器，这些 API 已在本章中展示过。因此，可以重新创建数据的 K-means
    简单聚类：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#comarker1b)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#comarker1b)'
- en: Replace with path to *kddcup.data*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 替换为 *kddcup.data* 的路径。
- en: '[![2](assets/2.png)](#comarker2)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#comarker2)'
- en: Name columns.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 命名列。
- en: '[![3](assets/3.png)](#comarker3)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#comarker3)'
- en: Drop nonnumeric columns again.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次删除非数值列。
- en: '[![4](assets/4.png)](#comarker4)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#comarker4)'
- en: '`~ .` means all columns.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`~ .` 表示所有列。'
- en: From here, it’s straightforward to assign a cluster to each data point. The
    operations above show usage of the SparkR APIs which naturally correspond to core
    Spark APIs, but are expressed as R libraries in R-like syntax. The actual clustering
    is executed using the same JVM-based, Scala language implementation in MLlib.
    These operations are effectively a *handle*, or remote control, to distributed
    operations that are not executing in R.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，为每个数据点分配一个集群非常简单。上述操作展示了使用 SparkR API 的用法，这些 API 自然对应于核心 Spark API，但表现为类似
    R 语法的 R 库。实际的聚类是使用同一基于 JVM 的 Scala 语言实现的 MLlib 执行的。这些操作实际上是对不在 R 中执行的分布式操作的一种
    *句柄* 或远程控制。
- en: R has its own rich set of libraries for analysis and its own similar concept
    of a dataframe. It is sometimes useful, therefore, to pull some data down into
    the R interpreter to be able to use these native R libraries, which are unrelated
    to Spark.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: R 有其自己丰富的分析库集合，以及其自己类似的 dataframe 概念。因此，有时将一些数据拉入 R 解释器以使用这些与 Spark 无关的本地 R
    库是很有用的。
- en: 'Of course, R and its libraries are not distributed, and so it’s not feasible
    to pull the whole dataset of 4,898,431 data points into R. However, it’s easy
    to pull only a sample:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，R 及其库不是分布式的，因此不可能将 4,898,431 个数据点的整个数据集导入 R。不过，只导入一个样本非常容易：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1)'
- en: 1% sample without replacement
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 无重复抽样的1%样本
- en: '`clustering_sample` is actually a local R dataframe, not a Spark DataFrame,
    so it can be manipulated like any other data in R. Above, `str` shows the structure
    of the dataframe.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`clustering_sample`实际上是一个本地的R数据框，而不是Spark DataFrame，因此可以像R中的任何其他数据一样进行操作。上面的`str`显示了数据框的结构。'
- en: 'For example, it’s possible to extract the cluster assignment and then show
    statistics about the distribution of assignments:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以提取聚类分配，然后显示关于分配分布的统计信息：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1)'
- en: Only the clustering assignment column
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 只有聚类分配列
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2)'
- en: Everything but the clustering assignment
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了聚类分配以外的所有内容。
- en: For example, this shows that most points fell into cluster 0\. Although much
    more could be done with this data in R, further coverage of this is beyond the
    scope of this book.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这显示大多数点都属于聚类0。虽然在R中可以对这些数据进行更多操作，但是这超出了本书的范围。
- en: 'To visualize the data, a library called `rgl` is required. It will be functional
    only if running this example in RStudio. First, install (once) and load the library:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化数据，需要一个名为`rgl`的库。只有在RStudio中运行这个示例时，它才能正常工作。首先，安装（仅需一次）并加载这个库：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that R may prompt you to download other packages or compiler tools to complete
    installation, because installing the package means compiling its source code.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，R可能会提示您下载其他软件包或编译器工具来完成安装，因为安装该软件包意味着编译其源代码。
- en: 'This dataset is 38-dimensional. It will have to be projected down into at most
    three dimensions to visualize it with a *random projection*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是38维的。为了在随机投影中可视化它，最多必须将其投影到三维空间中：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1)'
- en: Make a random 3-D projection and normalize.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 进行一个随机的3维投影并进行归一化。
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2)'
- en: Project and make a new dataframe.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 投影并创建一个新的数据框。
- en: This creates a 3-D dataset out of a 38-D dataset by choosing three random unit
    vectors and projecting the data onto them. This is a simplistic, rough-and-ready
    form of dimension reduction. Of course, there are more sophisticated dimension
    reduction algorithms, like principal component analysis or the singular value
    decomposition. These are available in R but take much longer to run. For purposes
    of visualization in this example, a random projection achieves much the same result,
    faster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择三个随机单位向量并将数据投影到它们上面，这将从一个38维的数据集创建一个3维的数据集。这是一种简单粗糙的降维方法。当然，还有更复杂的降维算法，如主成分分析或奇异值分解。这些算法在R中也有实现，但运行时间更长。在这个例子中，为了可视化的目的，随机投影可以更快地达到类似的结果。
- en: 'Finally, the clustered points can be plotted in an interactive 3-D visualization:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以在交互式3D可视化中绘制聚类点：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that this will require running RStudio in an environment that supports
    the `rgl` library and graphics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这将需要在支持`rgl`库和图形的环境中运行RStudio。
- en: The resulting visualization in [Figure 5-1](#AnomalyDetection_Projection1) shows
    data points in 3-D space. Many points fall on top of one another, and the result
    is sparse and hard to interpret. However, the dominant feature of the visualization
    is its L shape. The points seem to vary along two distinct dimensions, and little
    in other dimensions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图5-1](#AnomalyDetection_Projection1)中的结果可视化显示了三维空间中的数据点。许多点重叠在一起，结果稀疏且难以解释。然而，可视化的主要特征是其L形状。点似乎沿着两个不同的维度变化，而其他维度变化较小。
- en: This makes sense because the dataset has two features that are on a much larger
    scale than the others. Whereas most features have values between 0 and 1, the
    bytes-sent and bytes-received features vary from 0 to tens of thousands. The Euclidean
    distance between points is therefore almost completely determined by these two
    features. It’s almost as if the other features don’t exist! So it’s important
    to normalize away these differences in scale to put features on near-equal footing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有道理的，因为数据集有两个特征的量级比其他特征大得多。而大多数特征的值在0到1之间，而字节发送和字节接收特征的值则在0到数万之间变化。因此，点之间的欧氏距离几乎完全由这两个特征决定。其他特征几乎不存在！因此，通过标准化消除这些规模差异非常重要，以便让特征处于近乎相等的地位。
- en: '![aaps 0501](assets/aaps_0501.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0501](assets/aaps_0501.png)'
- en: Figure 5-1\. Random 3-D projection
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 随机3-D投影
- en: Feature Normalization
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征标准化
- en: 'We can normalize each feature by converting it to a standard score. This means
    subtracting the mean of the feature’s values from each value and dividing by the
    standard deviation, as shown in the standard score equation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将每个特征转换为标准分来标准化每个特征。这意味着从每个值中减去特征的值的平均值，并除以标准差，如标准分公式所示：
- en: <math alttext="n o r m a l i z e d Subscript i Baseline equals StartFraction
    f e a t u r e Subscript i Baseline minus mu Subscript i Baseline Over sigma Subscript
    i Baseline EndFraction" display="block"><mrow><mi>n</mi> <mi>o</mi> <mi>r</mi>
    <mi>m</mi> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <msub><mi>d</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><msub><mi>e</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>i</mi></msub></mrow> <msub><mi>σ</mi>
    <mi>i</mi></msub></mfrac></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="n o r m a l i z e d Subscript i Baseline equals StartFraction
    f e a t u r e Subscript i Baseline minus mu Subscript i Baseline Over sigma Subscript
    i Baseline EndFraction" display="block"><mrow><mi>n</mi> <mi>o</mi> <mi>r</mi>
    <mi>m</mi> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <msub><mi>d</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><msub><mi>e</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>i</mi></msub></mrow> <msub><mi>σ</mi>
    <mi>i</mi></msub></mfrac></mrow></math>
- en: In fact, subtracting means has no effect on the clustering because the subtraction
    effectively shifts all the data points by the same amount in the same direction.
    This does not affect interpoint Euclidean distances.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，减去均值对聚类没有影响，因为这种减法实际上是以相同方向和相同数量移动所有数据点。这并不影响点与点之间的欧氏距离。
- en: MLlib provides `StandardScaler`, a component that can perform this kind of standardization
    and be easily added to the clustering pipeline.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib提供了`StandardScaler`，这是一种可以执行此类标准化并轻松添加到聚类管道中的组件。
- en: 'We can run the same test with normalized data on a higher range of *k*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在更高范围的*k*上使用标准化数据运行相同的测试：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This has helped put dimensions on more equal footing, and the absolute distances
    between points (and thus the cost) is much smaller in absolute terms. However,
    the above output doesn’t yet provide an obvious value of *k* beyond which increasing
    it does little to improve the cost.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于使维度更平等，并且点之间的绝对距离（因此成本）在绝对数值上要小得多。然而，上述输出还没有提供一个明显的*k*值，超过这个值增加对成本的改进很少。
- en: Another 3-D visualization of the normalized data points reveals a richer structure,
    as expected. Some points are spaced in regular, discrete intervals in one direction;
    these are likely projections of discrete dimensions in the data, like counts.
    With 100 clusters, it’s hard to make out which points come from which clusters.
    One large cluster seems to dominate, and many clusters correspond to small, compact
    subregions (some of which are omitted from this zoomed detail of the entire 3-D
    visualization). The result, shown in [Figure 5-2](#AnomalyDetection_Projection2),
    does not necessarily advance the analysis but is an interesting sanity check.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个对标准化数据点进行的3-D可视化揭示了更丰富的结构，正如预期的那样。一些点在一个方向上以规则的离散间隔分布；这些很可能是数据中离散维度的投影，比如计数。有100个聚类，很难辨别哪些点来自哪些聚类。一个大聚类似乎占主导地位，许多聚类对应于小而紧凑的子区域（其中一些在整个3-D可视化的放大细节中被省略）。结果显示在[图5-2](#AnomalyDetection_Projection2)中，虽然并未必然推进分析，但是作为一个有趣的合理性检查。
- en: '![aaps 0502](assets/aaps_0502.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0502](assets/aaps_0502.png)'
- en: Figure 5-2\. Random 3-D projection, normalized
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2\. 随机3-D投影，标准化
- en: Categorical Variables
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别变量
- en: Normalization was a valuable step forward, but more can be done to improve the
    clustering. In particular, several features have been left out entirely because
    they aren’t numeric. This is throwing away valuable information. Adding them back,
    in some form, should produce a better-informed clustering.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化是向前迈出的一步，但可以做更多工作来改进聚类。特别是，一些特征完全被忽略，因为它们不是数字型的。这样做等于丢失了宝贵的信息。以某种形式将它们加回来，应该会产生更为明智的聚类。
- en: Earlier, three categorical features were excluded because nonnumeric features
    can’t be used with the Euclidean distance function that K-means uses in MLlib.
    This is the reverse of the issue noted in [“Random Forests”](ch04.xhtml#RandomDecisionForests),
    where numeric features were used to represent categorical values but a categorical
    feature was desired.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，由于非数值特征无法与MLlib中K-means使用的欧氏距离函数一起使用，因此排除了三个分类特征。这与[“随机森林”](ch04.xhtml#RandomDecisionForests)中所述的问题相反，其中数值特征用于表示分类值，但是希望使用分类特征。
- en: 'The categorical features can be translated into several binary indicator features
    using one-hot encoding, which can be viewed as numeric dimensions. For example,
    the second column contains the protocol type: `tcp`, `udp`, or `icmp`. This feature
    could be thought of as *three* features, as if features “is TCP,” “is UDP,” and
    “is ICMP” were in the dataset. The single feature value `tcp` might become `1,0,0`;
    `udp` might be `0,1,0`; and so on.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征可以通过使用一位有效编码转换为几个二进制指示特征，这可以视为数值维度。例如，第二列包含协议类型：`tcp`、`udp`或`icmp`。这个特征可以被看作是*三*个特征，就好像数据集中有“是TCP”、“是UDP”和“是ICMP”一样。单个特征值`tcp`可能会变成`1,0,0`；`udp`可能会变成`0,1,0`；依此类推。
- en: Here again, MLlib provides components that implement this transformation. In
    fact, one-hot encoding string-valued features like `protocol_type` are actually
    a two-step process. First, the string values are converted to integer indices
    like 0, 1, 2, and so on using `StringIndexer`. Then, these integer indices are
    encoded into a vector with `OneHotEncoder`. These two steps can be thought of
    as a small `Pipeline` in themselves.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，MLlib提供了实现此转换的组件。事实上，像`protocol_type`这样的字符串值特征的一位有效编码实际上是一个两步过程。首先，使用`StringIndexer`将字符串值转换为整数索引（如0、1、2等）。然后，使用`OneHotEncoder`将这些整数索引编码成一个向量。这两个步骤可以看作是一个小的`Pipeline`。
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1)'
- en: Return pipeline and name of output vector column
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 返回管道和输出向量列的名称
- en: This method produces a `Pipeline` that can be added as a component in the overall
    clustering pipeline; pipelines can be composed. All that is left is to make sure
    to add the new vector output columns into `VectorAssembler`’s output and proceed
    as before with scaling, clustering, and evaluation. The source code is omitted
    for brevity here, but can be found in the repository accompanying this chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法生成一个`Pipeline`，可以作为整体聚类流水线的一个组件添加；流水线可以组合。现在要做的就是确保将新的向量输出列添加到`VectorAssembler`的输出中，并像以前一样进行缩放、聚类和评估。此处省略了源代码以保持简洁，但可以在本章节附带的存储库中找到。
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: These sample results suggest, possibly, *k*=180 as a value where the score flattens
    out a bit. At least the clustering is now using all input features.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例结果表明，可能是*k*=180，这个值使得分数略微趋于平稳。至少现在聚类已经使用了所有的输入特征。
- en: Using Labels with Entropy
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用带熵的标签
- en: Earlier, we used the given label for each data point to create a quick sanity
    check of the quality of the clustering. This notion can be formalized further
    and used as an alternative means of evaluating clustering quality and, therefore,
    of choosing *k*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们使用每个数据点的给定标签来快速检查聚类质量的合理性。这个概念可以进一步形式化，并用作评估聚类质量和因此选择*k*的替代手段。
- en: The labels tell us something about the true nature of each data point. A good
    clustering, it seems, should agree with these human-applied labels. It should
    put together points that share a label frequently and not lump together points
    of many different labels. It should produce clusters with relatively homogeneous
    labels.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 标签告诉我们关于每个数据点真实特性的一些信息。一个好的聚类似乎应该与这些人工标记的标签一致。它应该将共享标签的点聚在一起，并且不应该将许多不同标签的点混在一起。它应该生成具有相对均匀标签的聚类。
- en: 'You may recall from [“Random Forests”](ch04.xhtml#RandomDecisionForests) that
    we have metrics for homogeneity: Gini impurity and entropy. These are functions
    of the proportions of labels in each cluster and produce a number that is low
    when the proportions are skewed toward few, or one, label. Entropy will be used
    here for illustration:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得[“随机森林”](ch04.xhtml#RandomDecisionForests)中关于同质性的指标：基尼不纯度和熵。这些都是每个聚类中标签比例的函数，并产生一个在标签倾向于少数或一个标签时较低的数字。这里将使用熵进行说明：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A good clustering would have clusters whose collections of labels are homogeneous
    and so have low entropy. A weighted average of entropy can therefore be used as
    a cluster score:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的聚类应该具有标签集合是同质的聚类，因此具有低熵。因此，可以使用熵的加权平均作为聚类得分：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1)'
- en: Predict cluster for each datum.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 预测每个数据点的聚类。
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2)'
- en: Count labels, per cluster
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 统计每个聚类的标签
- en: '[![3](assets/3.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3)'
- en: Average entropy weighted by cluster size.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由聚类大小加权的平均熵。
- en: 'As before, this analysis can be used to obtain some idea of a suitable value
    of *k*. Entropy will not necessarily decrease as *k* increases, so it is possible
    to look for a local minimum value. Here again, results suggest *k*=180 is a reasonable
    choice because its score is actually lower than 150 and 210:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，此分析可用于获取*k*的合适值的某些想法。熵不一定会随*k*的增加而减少，因此可以寻找局部最小值。在这里，结果再次表明*k*=180是一个合理的选择，因为其得分实际上比150和210低：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Clustering in Action
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类活动
- en: 'Finally, with confidence, we can cluster the full, normalized dataset with
    *k*=180\. Again, we can print the labels for each cluster to get some sense of
    the resulting clustering. Clusters do seem to be dominated by one type of attack
    each and contain only a few types:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以有信心地将完整的归一化数据集聚类到*k*=180。同样，我们可以打印每个集群的标签，以便对得到的聚类结果有所了解。每个集群似乎只由一种类型的攻击主导，并且只包含少量类型：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1)'
- en: See accompanying source code for `fit_pipeline_4` definition.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 参见`fit_pipeline_4`定义的相关源代码。
- en: 'Now we can make an actual anomaly detector. Anomaly detection amounts to measuring
    a new data point’s distance to its nearest centroid. If this distance exceeds
    some threshold, it is anomalous. This threshold might be chosen to be the distance
    of, say, the 100th-farthest data point from among known data:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以制作一个真正的异常检测器。异常检测相当于测量新数据点到其最近质心的距离。如果此距离超过某个阈值，则为异常。此阈值可能选择为已知数据中第100个最远数据点的距离：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The final step can be to apply this threshold to all new data points as they
    arrive. For example, Spark Streaming can be used to apply this function to small
    batches of input data arriving from sources like Kafka or files in cloud storage.
    Data points exceeding the threshold might trigger an alert that sends an email
    or updates a database.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步可以是在所有新数据点到达时应用此阈值。例如，Spark Streaming可以用于将此函数应用于从Kafka或云存储文件等来源接收的小批量输入数据。超过阈值的数据点可能会触发发送电子邮件或更新数据库的警报。
- en: Where to Go from Here
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步该去哪里
- en: The `KMeansModel` is, by itself, the essence of an anomaly detection system.
    The preceding code demonstrated how to apply it to data to detect anomalies. This
    same code could be used within [Spark Streaming](https://oreil.ly/UHHBR) to score
    new data as it arrives in near real time, and perhaps trigger an alert or review.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeansModel`本身就是异常检测系统的核心。前面的代码演示了如何将其应用于数据以检测异常。这段代码也可以在[Spark Streaming](https://oreil.ly/UHHBR)中使用，以几乎实时地对新数据进行评分，并可能触发警报或审核。'
- en: MLlib also includes a variation called `StreamingKMeans`, which can update a
    clustering incrementally as new data arrives in a `StreamingKMeansModel`. We could
    use this to continue to learn, approximately, how new data affects the clustering,
    and not just to assess new data against existing clusters. It can be integrated
    with Spark Streaming as well. However, it has not been updated for the new DataFrame-based
    APIs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib还包括一种称为`StreamingKMeans`的变体，它可以在`StreamingKMeansModel`中增量地更新聚类。我们可以使用它来持续学习，大致了解新数据如何影响聚类，而不仅仅是评估新数据与现有聚类的关系。它也可以与Spark
    Streaming集成。但是，它尚未针对新的基于DataFrame的API进行更新。
- en: This model is only a simplistic one. For example, Euclidean distance is used
    in this example because it is the only distance function supported by Spark MLlib
    at this time. In the future, it may be possible to use distance functions that
    can better account for the distributions of and correlations between features,
    such as the [Mahalanobis distance](https://oreil.ly/PKG7A).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型只是一个简单的模型。例如，在此示例中使用欧几里得距离，因为它是Spark MLlib当前支持的唯一距离函数。未来可能会使用能更好地考虑特征分布和相关性的距离函数，例如[马氏距离](https://oreil.ly/PKG7A)。
- en: There are also more sophisticated [cluster-quality evaluation metrics](https://oreil.ly/9yE9P)
    that could be applied (even without labels) to pick *k*, such as the [Silhouette
    coefficient](https://oreil.ly/LMN1h). These tend to evaluate not just closeness
    of points within one cluster, but closeness of points to other clusters. Finally,
    different models could be applied instead of simple K-means clustering; for example,
    a [Gaussian mixture model](https://oreil.ly/KTgD6) or [DBSCAN](https://oreil.ly/xlshs)
    could capture more subtle relationships between data points and the cluster centers.
    Spark MLlib already implements [Gaussian mixture models](https://oreil.ly/LG84u);
    implementations of others may become available in Spark MLlib or other Spark-based
    libraries in the future.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更复杂的[集群质量评估指标](https://oreil.ly/9yE9P)，即使没有标签，也可以应用于选择*k*，例如[轮廓系数](https://oreil.ly/LMN1h)。这些指标通常评估的不仅是一个集群内点的接近度，还包括点到其他集群的接近度。最后，可以应用不同的模型来替代简单的K均值聚类；例如，[高斯混合模型](https://oreil.ly/KTgD6)或[DBSCAN](https://oreil.ly/xlshs)可以捕捉数据点与集群中心之间更微妙的关系。Spark
    MLlib已经实现了[高斯混合模型](https://oreil.ly/LG84u)，其他模型的实现可能会在未来出现在Spark MLlib或其他基于Spark的库中。
- en: Of course, clustering isn’t just for anomaly detection. In fact, it’s more often
    associated with use cases where the actual clusters matter! For example, clustering
    can also be used to group customers according to their behaviors, preferences,
    and attributes. Each cluster, by itself, might represent a usefully distinguishable
    type of customer. This is a more data-driven way to segment customers rather than
    leaning on arbitrary, generic divisions like “age 20–34” and “female.”
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，聚类不仅仅用于异常检测。事实上，它更常与实际集群关系重要的用例相关联！例如，聚类还可以用于根据客户的行为、偏好和属性进行分组。每个集群本身可能代表一种有用的可区分客户类型。这是一种更加数据驱动的客户分段方式，而不是依赖于任意的通用分割，如“年龄20-34岁”和“女性”。
