- en: 'Chapter 46\. In Depth: Manifold Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第46章 深入探讨：流形学习
- en: In the previous chapter we saw how PCA can be used for dimensionality reduction,
    reducing the number of features of a dataset while maintaining the essential relationships
    between the points. While PCA is flexible, fast, and easily interpretable, it
    does not perform so well when there are *nonlinear* relationships within the data,
    some examples of which we will see shortly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到PCA可以用于降维，减少数据集的特征数，同时保持点之间的基本关系。虽然PCA灵活、快速且易于解释，但当数据中存在非线性关系时，它的表现并不理想，我们很快将看到一些例子。
- en: 'To address this deficiency, we can turn to *manifold learning algorithms*—a
    class of unsupervised estimators that seek to describe datasets as low-dimensional
    manifolds embedded in high-dimensional spaces. When you think of a manifold, I’d
    suggest imagining a sheet of paper: this is a two-dimensional object that lives
    in our familiar three-dimensional world.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一不足，我们可以转向*流形学习算法*——一类无监督估计器，旨在将数据集描述为嵌入在高维空间中的低维流形。当你想到流形时，我建议想象一张纸：这是一个二维物体，存在于我们熟悉的三维世界中。
- en: 'In the parlance of manifold learning, you can think of this sheet as a two-dimensional
    manifold embedded in three-dimensional space. Rotating, reorienting, or stretching
    the piece of paper in three-dimensional space doesn’t change its flat geometry:
    such operations are akin to linear embeddings. If you bend, curl, or crumple the
    paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional
    space is no longer linear. Manifold learning algorithms seek to learn about the
    fundamental two-dimensional nature of the paper, even as it is contorted to fill
    the three-dimensional space.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在流形学的术语中，你可以将这张纸片看作是嵌入在三维空间中的二维流形。在三维空间中旋转、重新定向或拉伸这张纸片并不会改变其平面几何特性：这些操作类似于线性嵌入。如果你将纸张弯曲、卷曲或揉皱，它仍然是一个二维流形，但是嵌入到三维空间的方式不再是线性的。流形学算法旨在学习纸张的基本二维特性，即使它被扭曲以填充三维空间。
- en: 'Here we will examine a number of manifold methods, going most deeply into a
    subset of these techniques: multidimensional scaling (MDS), locally linear embedding
    (LLE), and isometric mapping (Isomap).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将研究多种流形方法，深入探讨其中的一些技术：多维尺度法（MDS）、局部线性嵌入（LLE）和等距映射（Isomap）。
- en: 'We begin with the standard imports:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Manifold Learning: “HELLO”'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形学习：“HELLO”
- en: 'To make these concepts more clear, let’s start by generating some two-dimensional
    data that we can use to define a manifold. Here is a function that will create
    data in the shape of the word “HELLO”:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地阐明这些概念，让我们首先生成一些二维数据，以便用来定义一个流形。这里是一个能创建“HELLO”形状数据的函数：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s call the function and visualize the resulting data ([Figure 46-1](#fig_0510-manifold-learning_files_in_output_6_0)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用这个函数，并可视化生成的数据（参见[图 46-1](#fig_0510-manifold-learning_files_in_output_6_0)）。
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output is two dimensional, and consists of points drawn in the shape of
    the word “HELLO”. This data form will help us to see visually what these algorithms
    are doing.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是二维的，由“HELLO”形状的点组成。这种数据形式将帮助我们直观地了解这些算法的作用。
- en: '![output 6 0](assets/output_6_0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![output 6 0](assets/output_6_0.png)'
- en: Figure 46-1\. Data for use with manifold learning
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-1\. 用于流形学习的数据
- en: Multidimensional Scaling
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多维尺度法
- en: 'Looking at data like this, we can see that the particular choices of *x* and
    *y* values of the dataset are not the most fundamental description of the data:
    we can scale, shrink, or rotate the data, and the “HELLO” will still be apparent.
    For example, if we use a rotation matrix to rotate the data, the *x* and *y* values
    change, but the data is still fundamentally the same (see [Figure 46-2](#fig_0510-manifold-learning_files_in_output_9_0)).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这样的数据，我们可以看到数据集中特定的*x*和*y*值并不是数据最基本的描述：我们可以缩放、收缩或旋转数据，而“HELLO”仍然是显而易见的。例如，如果我们使用旋转矩阵旋转数据，*x*和*y*值会改变，但数据基本上仍然是相同的（参见[图
    46-2](#fig_0510-manifold-learning_files_in_output_9_0)）。
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![output 9 0](assets/output_9_0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![output 9 0](assets/output_9_0.png)'
- en: Figure 46-2\. Rotated dataset
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-2\. 旋转的数据集
- en: 'This confirms that the *x* and *y* values are not necessarily fundamental to
    the relationships in the data. What *is* fundamental, in this case, is the *distance*
    between each point within the dataset. A common way to represent this is to use
    a distance matrix: for <math alttext="upper N"><mi>N</mi></math> points, we construct
    an <math alttext="upper N times upper N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math>
    array such that entry <math alttext="left-parenthesis i comma j right-parenthesis"><mrow><mo>(</mo>
    <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></math> contains the distance
    between point <math alttext="i"><mi>i</mi></math> and point <math alttext="j"><mi>j</mi></math>
    . Let’s use Scikit-Learn’s efficient `pairwise_distances` function to do this
    for our original data:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了*x*和*y*值不一定是数据关系中的基本要素。在这种情况下，基本的是数据集中每个点之间的距离。表示这一点的常见方法是使用距离矩阵：对于<math
    alttext="upper N"><mi>N</mi></math>个点，我们构造一个<math alttext="upper N times upper
    N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math>数组，使得条目<math alttext="left-parenthesis
    i comma j right-parenthesis"><mrow><mo>(</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi>
    <mo>)</mo></mrow></math>包含点<math alttext="i"><mi>i</mi></math>和点<math alttext="j"><mi>j</mi></math>之间的距离。让我们使用Scikit-Learn的高效`pairwise_distances`函数来为我们的原始数据做到这一点：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As promised, for our *N*=1,000 points, we obtain a 1,000 × 1,000 matrix, which
    can be visualized as shown here (see [Figure 46-3](#fig_0510-manifold-learning_files_in_output_13_0)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如承诺的那样，对于我们的*N*=1,000个点，我们获得了一个1,000 × 1,000的矩阵，可以像这样进行可视化（参见[图 46-3](#fig_0510-manifold-learning_files_in_output_13_0)）。
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![output 13 0](assets/output_13_0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![output 13 0](assets/output_13_0.png)'
- en: Figure 46-3\. Visualization of the pairwise distances between points
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-3\. 点之间的成对距离可视化
- en: 'If we similarly construct a distance matrix for our rotated and translated
    data, we see that it is the same:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们类似地为我们旋转和平移的数据构建一个距离矩阵，我们会看到它是相同的：
- en: '[PRE6]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This distance matrix gives us a representation of our data that is invariant
    to rotations and translations, but the visualization of the matrix in [Figure 46-3](#fig_0510-manifold-learning_files_in_output_13_0)
    is not entirely intuitive. In the representation shown there, we have lost any
    visible sign of the interesting structure in the data: the “HELLO” that we saw
    before.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个距离矩阵给出了一个与旋转和平移无关的数据表示，但在[图 46-3](#fig_0510-manifold-learning_files_in_output_13_0)中的矩阵可视化并不完全直观。在那里显示的表示中，我们失去了数据中有趣结构的任何可见迹象：“HELLO”。
- en: 'Further, while computing this distance matrix from the (*x*, *y*) coordinates
    is straightforward, transforming the distances back into *x* and *y* coordinates
    is rather difficult. This is exactly what the multidimensional scaling algorithm
    aims to do: given a distance matrix between points, it recovers a <math alttext="upper
    D"><mi>D</mi></math> -dimensional coordinate representation of the data. Let’s
    see how it works for our distance matrix, using the `precomputed` dissimilarity
    to specify that we are passing a distance matrix ([Figure 46-4](#fig_0510-manifold-learning_files_in_output_17_0)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从(*x*, *y*)坐标计算距离矩阵很简单，但将距离转换回*x*和*y*坐标却相当困难。这正是多维缩放算法的目标：给定点之间的距离矩阵，它恢复数据的<math
    alttext="upper D"><mi>D</mi></math>维坐标表示。让我们看看它如何处理我们的距离矩阵，使用`precomputed`表示我们正在传递一个距离矩阵（见[图 46-4](#fig_0510-manifold-learning_files_in_output_17_0)）。
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![output 17 0](assets/output_17_0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![output 17 0](assets/output_17_0.png)'
- en: Figure 46-4\. An MDS embedding computed from the pairwise distances
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-4\. 从成对距离计算得到的MDS嵌入
- en: The MDS algorithm recovers one of the possible two-dimensional coordinate representations
    of our data, using *only* the <math alttext="upper N times upper N"><mrow><mi>N</mi>
    <mo>×</mo> <mi>N</mi></mrow></math> distance matrix describing the relationship
    between the data points.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MDS算法使用描述数据点之间关系的<math alttext="upper N times upper N"><mrow><mi>N</mi> <mo>×</mo>
    <mi>N</mi></mrow></math>距离矩阵之一，仅恢复了我们数据的可能的二维坐标表示。
- en: MDS as Manifold Learning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MDS作为流形学习
- en: 'The usefulness of this becomes more apparent when we consider the fact that
    distance matrices can be computed from data in *any* dimension. So, for example,
    instead of simply rotating the data in the two-dimensional plane, we can project
    it into three dimensions using the following function (essentially a three-dimensional
    generalization of the rotation matrix used earlier):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到距离矩阵可以从*任何*维度的数据中计算时，这种用处变得更加明显。例如，我们不仅可以简单地将数据在二维平面上旋转，还可以使用以下函数将其投影到三维（本质上是早期使用的旋转矩阵的三维推广）。
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s visualize these points to see what we’re working with ([Figure 46-5](#fig_0510-manifold-learning_files_in_output_22_0)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这些点，看看我们在处理什么（见[图 46-5](#fig_0510-manifold-learning_files_in_output_22_0)）。
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![output 22 0](assets/output_22_0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![output 22 0](assets/output_22_0.png)'
- en: Figure 46-5\. Data embedded linearly into three dimensions
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-5\. 数据线性嵌入到三维空间中
- en: We can now ask the `MDS` estimator to input this three-dimensional data, compute
    the distance matrix, and then determine the optimal two-dimensional embedding
    for this distance matrix. The result recovers a representation of the original
    data, as shown in [Figure 46-6](#fig_0510-manifold-learning_files_in_output_24_0).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以要求`MDS`估计器输入这三维数据，计算距离矩阵，然后确定这个距离矩阵的最优二维嵌入。结果恢复了原始数据的表示，如[图 46-6](#fig_0510-manifold-learning_files_in_output_24_0)所示。
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is essentially the goal of a manifold learning estimator: given high-dimensional
    embedded data, it seeks a low-dimensional representation of the data that preserves
    certain relationships within the data. In the case of MDS, the quantity preserved
    is the distance between every pair of points.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是流形学习估计器的目标：在给定高维嵌入数据的情况下，寻找保持数据内部某些关系的低维表示。在MDS的情况下，保持的量是每对点之间的距离。
- en: '![output 24 0](assets/output_24_0.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![output 24 0](assets/output_24_0.png)'
- en: Figure 46-6\. The MDS embedding of the three-dimensional data recovers the input
    up to a rotation and reflection
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-6\. 三维数据的MDS嵌入恢复了输入，经过了旋转和反射。
- en: 'Nonlinear Embeddings: Where MDS Fails'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性嵌入：MDS失败的地方
- en: 'Our discussion thus far has considered *linear* embeddings, which essentially
    consist of rotations, translations, and scalings of data into higher-dimensional
    spaces. Where MDS breaks down is when the embedding is nonlinear—that is, when
    it goes beyond this simple set of operations. Consider the following embedding,
    which takes the input and contorts it into an “S” shape in three dimensions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的是*线性*嵌入，这本质上是将数据旋转、平移和缩放到更高维空间中。当嵌入是非线性的时候，即超出这简单操作集合时，MDS失效了。考虑下面的嵌入，它将输入扭曲成了三维空间中的“S”形状：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is again three-dimensional data, but as we can see in [Figure 46-7](#fig_0510-manifold-learning_files_in_output_29_0)
    the embedding is much more complicated.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样是三维数据，但正如我们在[图 46-7](#fig_0510-manifold-learning_files_in_output_29_0)中所见，嵌入更加复杂。
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 29 0](assets/output_29_0.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![output 29 0](assets/output_29_0.png)'
- en: Figure 46-7\. Data embedded nonlinearly into three dimensions
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-7\. 数据非线性嵌入到三维空间中
- en: 'The fundamental relationships between the data points are still there, but
    this time the data has been transformed in a nonlinear way: it has been wrapped
    up into the shape of an “S.”'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点之间的基本关系仍然存在，但这次数据以非线性的方式进行了转换：它被包裹成了一个“S”形状。
- en: If we try a simple MDS algorithm on this data, it is not able to “unwrap” this
    nonlinear embedding, and we lose track of the fundamental relationships in the
    embedded manifold (see [Figure 46-8](#fig_0510-manifold-learning_files_in_output_31_0)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试在这些数据上使用简单的MDS算法，它无法“展开”这个非线性嵌入，我们会丢失嵌入流形中基本的关系（见[图 46-8](#fig_0510-manifold-learning_files_in_output_31_0)）。
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![output 31 0](assets/output_31_0.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![output 31 0](assets/output_31_0.png)'
- en: Figure 46-8\. The MDS algorithm applied to the nonlinear data; it fails to recover
    the underlying structure
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-8\. 应用于非线性数据的MDS算法；它未能恢复底层结构
- en: The best two-dimensional *linear* embedding does not unwrap the S-curve, but
    instead discards the original y-axis.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的二维*线性*嵌入并没有展开S形曲线，而是丢弃了原始的y轴。
- en: 'Nonlinear Manifolds: Locally Linear Embedding'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性流形：局部线性嵌入
- en: How can we move forward here? Stepping back, we can see that the source of the
    problem is that MDS tries to preserve distances between faraway points when constructing
    the embedding. But what if we instead modified the algorithm such that it only
    preserves distances between nearby points? The resulting embedding would be closer
    to what we want.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们该怎么继续？退一步来看，我们可以看到问题的根源在于，MDS试图在构建嵌入时保持远距离点之间的距离。但如果我们改变算法，使其仅保持附近点之间的距离会怎样？结果的嵌入会更接近我们想要的。
- en: Visually, we can think of it as illustrated [Figure 46-9](#fig_images_in_0510-lle-vs-mds).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，我们可以将其想象成如[图 46-9](#fig_images_in_0510-lle-vs-mds)所示。
- en: 'Here each faint line represents a distance that should be preserved in the
    embedding. On the left is a representation of the model used by MDS: it tries
    to preserve the distances between each pair of points in the dataset. On the right
    is a representation of the model used by a manifold learning algorithm called
    *locally linear embedding*: rather than preserving *all* distances, it instead
    tries to preserve only the distances between *neighboring points* (in this case,
    the nearest 100 neighbors of each point).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里每条淡淡的线表示一个应该在嵌入中保留的距离。左侧是MDS使用的模型的表示：它试图保持数据集中每对点之间的距离。右侧是一种名为*局部线性嵌入*的流形学习算法使用的模型的表示：它不是保留*所有*距离，而是试图仅保留*相邻点*之间的距离（在这种情况下，每个点的最近100个邻居）。
- en: 'Thinking about the left panel, we can see why MDS fails: there is no way to
    unroll this data while adequately preserving the length of every line drawn between
    the two points. For the right panel, on the other hand, things look a bit more
    optimistic. We could imagine unrolling the data in a way that keeps the lengths
    of the lines approximately the same. This is precisely what LLE does, through
    a global optimization of a cost function reflecting this logic.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑左侧面板，我们可以看到为什么MDS失败了：没有办法展开这些数据同时充分保持两点之间绘制的每条线的长度。另一方面，对于右侧面板，情况看起来更加乐观。我们可以想象以一种方式展开数据，以保持线的长度大致相同。这正是LLE通过全局优化反映这种逻辑的成本函数所做的。
- en: '![05.10 LLE vs MDS](assets/05.10-LLE-vs-MDS.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![05.10 LLE vs MDS](assets/05.10-LLE-vs-MDS.png)'
- en: Figure 46-9\. Representation of linkages between points within MDS and LLE^([1](ch46.xhtml#idm45858725565312))
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图46-9\. MDS和LLE之间点之间联系的表示^([1](ch46.xhtml#idm45858725565312))
- en: LLE comes in a number of flavors; here we will use the *modified LLE* algorithm
    to recover the embedded two-dimensional manifold. In general, modified LLE does
    better than other flavors of the algorithm at recovering well-defined manifolds
    with very little distortion (see [Figure 46-10](#fig_0510-manifold-learning_files_in_output_36_0)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LLE有许多变体；在这里，我们将使用*改进的LLE*算法来恢复嵌入的二维流形。总的来说，改进的LLE比算法的其他变体更能够恢复具有很少扭曲的明确定义流形（参见[图46-10](#fig_0510-manifold-learning_files_in_output_36_0)）。
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result remains somewhat distorted compared to our original manifold, but
    captures the essential relationships in the data!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们原始流形相比，结果仍然有些扭曲，但捕捉到了数据中的基本关系！
- en: '![output 36 0](assets/output_36_0.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![output 36 0](assets/output_36_0.png)'
- en: Figure 46-10\. Locally linear embedding can recover the underlying data from
    a nonli‐ nearly embedded input
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图46-10\. 局部线性嵌入可以从非线性嵌入的输入中恢复底层数据
- en: Some Thoughts on Manifold Methods
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对流形方法的一些思考
- en: Compelling as these examples may be, in practice manifold learning techniques
    tend to be finicky enough that they are rarely used for anything more than simple
    qualitative visualization of high-dimensional data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些示例可能很引人注目，但在实践中，流形学习技术往往很难处理，因此很少被用于除了高维数据的简单定性可视化之外的任何其他用途。
- en: 'The following are some of the particular challenges of manifold learning, which
    all contrast poorly with PCA:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是流形学习的一些特定挑战，这些挑战都与PCA相比非常不利：
- en: In manifold learning, there is no good framework for handling missing data.
    In contrast, there are straightforward iterative approaches for dealing with missing
    data in PCA.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流形学习中，没有处理缺失数据的良好框架。相比之下，在PCA中有直接的迭代方法来处理缺失数据。
- en: In manifold learning, the presence of noise in the data can “short-circuit”
    the manifold and drastically change the embedding. In contrast, PCA naturally
    filters noise from the most important components.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流形学习中，数据中的噪声存在可以“短路”流形并显著改变嵌入的情况。相比之下，PCA自然地从最重要的组件中过滤噪声。
- en: The manifold embedding result is generally highly dependent on the number of
    neighbors chosen, and there is generally no solid quantitative way to choose an
    optimal number of neighbors. In contrast, PCA does not involve such a choice.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流形嵌入结果通常高度依赖于选择的邻居数，并且通常没有一种可靠的定量方法来选择最佳邻居数。相比之下，PCA不涉及这样的选择。
- en: In manifold learning, the globally optimal number of output dimensions is difficult
    to determine. In contrast, PCA lets you find the number of output dimensions based
    on the explained variance.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流形学习中，确定全局最佳输出维度的难度很大。相比之下，PCA可以根据解释的方差来确定输出维度的数量。
- en: In manifold learning, the meaning of the embedded dimensions is not always clear.
    In PCA, the principal components have a very clear meaning.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流形学习中，嵌入维度的含义并不总是清晰的。在PCA中，主成分有一个非常明确的含义。
- en: In manifold learning, the computational expense of manifold methods scales as
    <math alttext="upper O left-bracket upper N squared right-bracket"><mrow><mi>O</mi>
    <mo>[</mo> <msup><mi>N</mi> <mn>2</mn></msup> <mo>]</mo></mrow></math> or <math
    alttext="upper O left-bracket upper N cubed right-bracket"><mrow><mi>O</mi> <mo>[</mo>
    <msup><mi>N</mi> <mn>3</mn></msup> <mo>]</mo></mrow></math> . For PCA, there exist
    randomized approaches that are generally much faster (though see the [*megaman*
    package](https://oreil.ly/VLBly) for some more scalable implementations of manifold
    learning).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流形学习中，流形方法的计算开销按<math alttext="upper O left-bracket upper N squared right-bracket"><mrow><mi>O</mi>
    <mo>[</mo> <msup><mi>N</mi> <mn>2</mn></msup> <mo>]</mo></mrow></math> 或 <math
    alttext="upper O left-bracket upper N cubed right-bracket"><mrow><mi>O</mi> <mo>[</mo>
    <msup><mi>N</mi> <mn>3</mn></msup> <mo>]</mo></mrow></math> 的方式扩展。对于PCA，存在一些随机化方法通常要快得多（尽管请参考[*megaman*
    package](https://oreil.ly/VLBly)以获取更多可扩展的流形学习实现）。
- en: With all that on the table, the only clear advantage of manifold learning methods
    over PCA is their ability to preserve nonlinear relationships in the data; for
    that reason I tend to explore data with manifold methods only after first exploring
    it with PCA.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，流形学习方法相对于PCA唯一明显的优势是它们能够保留数据中的非线性关系；因此，我倾向于仅在首先使用PCA探索数据后才使用流形方法探索数据。
- en: 'Scikit-Learn implements several common variants of manifold learning beyond
    LLE and Isomap (which we’ve used in a few of the previous chapters and will look
    at in the next section): the Scikit-Learn documentation has a [nice discussion
    and comparison of them](https://oreil.ly/tFzS5). Based on my own experience, I
    would give the following recommendations:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 实现了除了LLE和Isomap之外的几种常见流形学习变体（我们在前几章中使用过Isomap，将在下一节中查看）：Scikit-Learn
    文档对它们进行了[很好的讨论和比较](https://oreil.ly/tFzS5)。根据我的经验，我会提出以下建议：
- en: For toy problems such as the S-curve we saw before, LLE and its variants (especially
    modified LLE) perform very well. This is implemented in `sklearn.manifold.LocallyLinearEmbedding`.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于像我们之前看到的S曲线这样的玩具问题，LLE及其变体（特别是修改的LLE）表现非常好。这在`sklearn.manifold.LocallyLinearEmbedding`中实现。
- en: For high-dimensional data from real-world sources, LLE often produces poor results,
    and Isomap seems to generally lead to more meaningful embeddings. This is implemented
    in `sklearn.manifold.Isomap`.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于来自现实世界来源的高维数据，LLE通常会产生较差的结果，而Isomap似乎通常会导致更有意义的嵌入。这在`sklearn.manifold.Isomap`中实现。
- en: For data that is highly clustered, *t-distributed stochastic neighbor embedding*
    (t-SNE) seems to work very well, though it can be very slow compared to other
    methods. This is implemented in `sklearn.manifold.TSNE`.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高度聚类的数据，*t-分布随机近邻嵌入*（t-SNE）似乎效果非常好，尽管与其他方法相比速度较慢。这在`sklearn.manifold.TSNE`中实现。
- en: If you’re interested in getting a feel for how these work, I’d suggest running
    each of the methods on the data in this section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些方法的运行感兴趣，我建议你在这一节的数据上分别运行每种方法。
- en: 'Example: Isomap on Faces'
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 例如：Faces 上的 Isomap
- en: 'One place manifold learning is often used is in understanding the relationship
    between high-dimensional data points. A common case of high-dimensional data is
    images: for example, a set of images with 1,000 pixels each can be thought of
    as a collection of points in 1,000 dimensions, with the brightness of each pixel
    in each image defining the coordinate in that dimension.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 流形学习经常用于理解高维数据点之间的关系。高维数据的一个常见案例是图像：例如，每个包含 1,000 个像素的图像集可以被视为一个 1,000 维空间中的点集，其中每个像素的亮度定义了该维度上的坐标。
- en: 'To illustrate, let’s apply Isomap on some data from the Labeled Faces in the
    Wild dataset, which we previously saw in Chapters [43](ch43.xhtml#section-0507-support-vector-machines)
    and [45](ch45.xhtml#section-0509-principal-component-analysis). Running this command
    will download the dataset and cache it in your home directory for later use:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，让我们在一些来自Labeled Faces in the Wild数据集的数据上应用Isomap，我们之前在第[43](ch43.xhtml#section-0507-support-vector-machines)章和第[45](ch45.xhtml#section-0509-principal-component-analysis)章中看到过。运行此命令将下载数据集并将其缓存到您的主目录供以后使用：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have 2,370 images, each with 2,914 pixels. In other words, the images can
    be thought of as data points in a 2,914-dimensional space!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有2,370张图像，每张图像有2,914个像素。换句话说，这些图像可以被视为2,914维空间中的数据点！
- en: Let’s display several of these images to remind us what we’re working with (see
    [Figure 46-11](#fig_0510-manifold-learning_files_in_output_43_0)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示几张这些图像，以便提醒我们正在处理的内容（参见[图46-11](#fig_0510-manifold-learning_files_in_output_43_0)）。
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![output 43 0](assets/output_43_0.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![output 43 0](assets/output_43_0.png)'
- en: Figure 46-11\. Examples of the input faces
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图46-11\. 输入人脸示例
- en: 'When we encountered this data in [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis),
    our goal was essentially compression: to use the components to reconstruct the
    inputs from the lower-dimensional representation.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第45章](ch45.xhtml#section-0509-principal-component-analysis)遇到这些数据时，我们的目标基本上是压缩：使用组件从较低维度表示重建输入。
- en: PCA is versatile enough that we can also use it in this context, where we would
    like to plot a low-dimensional embedding of the 2,914-dimensional data to learn
    the fundamental relationships between the images. Let’s again look at the explained
    variance ratio, which will give us an idea of how many linear features are required
    to describe the data (see [Figure 46-12](#fig_0510-manifold-learning_files_in_output_45_0)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: PCA足够灵活，我们也可以在此上下文中使用它，我们想要绘制2914维数据的低维嵌入，以学习图像之间的基本关系。让我们再次查看解释的方差比率，这将为我们提供所需的线性特征数量的概念（参见[图46-12](#fig_0510-manifold-learning_files_in_output_45_0)）。
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![output 45 0](assets/output_45_0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![output 45 0](assets/output_45_0.png)'
- en: Figure 46-12\. Cumulative variance from the PCA projection
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图46-12\. 来自PCA投影的累积方差
- en: We see that for this data, nearly 100 components are required to preserve 90%
    of the variance. This tells us that the data is intrinsically very high-dimensional—it
    can’t be described linearly with just a few components.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些数据，我们看到需要近100个组件才能保留90%的方差。这告诉我们，数据在本质上是非常高维的——不能仅用少量组件线性描述。
- en: 'When this is the case, nonlinear manifold embeddings like LLE and Isomap may
    be helpful. We can compute an Isomap embedding on these faces using the same pattern
    shown before:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当情况如此时，非线性流形嵌入如LLE和Isomap可能会有所帮助。我们可以使用与之前相同的模式在这些人脸上计算Isomap嵌入：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is a two-dimensional projection of all the input images. To get
    a better idea of what the projection tells us, let’s define a function that will
    output image thumbnails at the locations of the projections:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是所有输入图像的二维投影。为了更好地了解投影告诉我们的内容，让我们定义一个函数，该函数将在投影位置输出图像缩略图：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Calling this function now, we see the result in [Figure 46-13](#fig_0510-manifold-learning_files_in_output_51_0).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在调用此函数，我们将在[图46-13](#fig_0510-manifold-learning_files_in_output_51_0)中看到结果。
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![output 51 0](assets/output_51_0.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![output 51 0](assets/output_51_0.png)'
- en: Figure 46-13\. Isomap embedding of the LFW data
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图46-13\. LFW数据的Isomap嵌入
- en: 'The result is interesting. The first two Isomap dimensions seem to describe
    global image features: the overall brightness of the image from left to right,
    and the general orientation of the face from bottom to top. This gives us a nice
    visual indication of some of the fundamental features in our data.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 结果很有趣。前两个Isomap维度似乎描述了全局图像特征：图像的整体亮度从左到右，以及脸部的一般方向从底部到顶部。这为我们提供了一些数据中一些基本特征的良好视觉指示。
- en: From here, we could then go on to classify this data (perhaps using manifold
    features as inputs to the classification algorithm) as we did in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以继续对这些数据进行分类（也许使用流形特征作为分类算法的输入），就像我们在[第43章](ch43.xhtml#section-0507-support-vector-machines)中所做的那样。
- en: 'Example: Visualizing Structure in Digits'
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：可视化数字中的结构
- en: 'As another example of using manifold learning for visualization, let’s take
    a look at the MNIST handwritten digits dataset. This is similar to the digits
    dataset we saw in [Chapter 44](ch44.xhtml#section-0508-random-forests), but with
    many more pixels per image. It can be downloaded from [*http://openml.org*](http://openml.org)
    with the Scikit-Learn utility:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '作为使用流形学习进行可视化的另一个示例，让我们看一下MNIST手写数字数据集。这类似于我们在[第44章](ch44.xhtml#section-0508-random-forests)中看到的数字数据集，但每个图像的像素更多。可以使用Scikit-Learn工具从[*http://openml.org*](http://openml.org)下载它： '
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The dataset consists of 70,000 images, each with 784 pixels (i.e., the images
    are 28 × 28). As before, we can take a look at the first few images (see [Figure 46-14](#fig_0510-manifold-learning_files_in_output_56_0)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由70,000个图像组成，每个图像有784个像素（即，图像为28 × 28）。和以前一样，我们可以查看前几个图像（参见[图46-14](#fig_0510-manifold-learning_files_in_output_56_0)）。
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![output 56 0](assets/output_56_0.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![output 56 0](assets/output_56_0.png)'
- en: Figure 46-14\. Examples of MNIST digits
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图46-14\. MNIST数字示例
- en: This gives us an idea of the variety of handwriting styles in the dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们了解到数据集中手写风格的多样性。
- en: Let’s compute a manifold learning projection across the data. For speed here,
    we’ll only use 1/30 of the data, which is about ~2,000 points (because of the
    relatively poor scaling of manifold learning, I find that a few thousand samples
    is a good number to start with for relatively quick exploration before moving
    to a full calculation). [Figure 46-15](#fig_0510-manifold-learning_files_in_output_58_0)
    shows the result.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在数据集中进行流形学习投影。为了加快速度，我们只使用了 1/30 的数据，大约是 ~2,000 个点（由于流形学习的比例尺度相对较差，我发现几千个样本是一个相对快速探索的良好起点，然后再转向完整的计算）。[图
    46-15](#fig_0510-manifold-learning_files_in_output_58_0) 展示了结果。
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![output 58 0](assets/output_58_0.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![output 58 0](assets/output_58_0.png)'
- en: Figure 46-15\. Isomap embedding of the MNIST digit data
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-15\. MNIST 数字数据的Isomap嵌入
- en: The resulting scatter plot shows some of the relationships between the data
    points, but is a bit crowded. We can gain more insight by looking at just a single
    number at a time (see [Figure 46-16](#fig_0510-manifold-learning_files_in_output_60_0)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 结果散点图展示了数据点之间的一些关系，但有点拥挤。我们可以通过逐个查看单个数字来获得更多见解（参见[图 46-16](#fig_0510-manifold-learning_files_in_output_60_0)）。
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![output 60 0](assets/output_60_0.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![output 60 0](assets/output_60_0.png)'
- en: Figure 46-16\. Isomap embedding of only the 1s within the MNIST dataset
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 46-16\. 仅包含MNIST数据集中数字1的Isomap嵌入
- en: 'The result gives you an idea of the variety of forms that the number 1 can
    take within the dataset. The data lies along a broad curve in the projected space,
    which appears to trace the orientation of the digit. As you move up the plot,
    you find 1s that have hats and/or bases, though these are very sparse within the
    dataset. The projection lets us identify outliers that have data issues: for example,
    pieces of the neighboring digits that snuck into the extracted images.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果让你了解到数据集中数字 1 可以呈现的各种形式。数据沿着投影空间中的一条宽曲线分布，这条曲线似乎跟数字的方向有关。通过观察图上方的部分，你会发现一些带帽子和/或底座的数字
    1，尽管它们在数据集中非常稀疏。投影让我们能够识别出存在数据问题的离群值：例如，一些邻近数字的部分可能已经混入提取的图像中。
- en: Now, this in itself may not be useful for the task of classifying digits, but
    it does help us get an understanding of the data, and may give us ideas about
    how to move forward—such as how we might want to preprocess the data before building
    a classification pipeline.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这本身对于分类数字可能并不有用，但它确实帮助我们了解数据，并可能给我们关于如何继续进行的想法——比如我们可能希望在构建分类管道之前对数据进行预处理。
- en: ^([1](ch46.xhtml#idm45858725565312-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/gu4iE).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch46.xhtml#idm45858725565312-marker)) 在[在线附录](https://oreil.ly/gu4iE)中可以找到产生此图的代码。
