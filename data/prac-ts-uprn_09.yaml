- en: Chapter 9\. Machine Learning for Time Series
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。时间序列的机器学习
- en: In this chapter, we will look at a few examples of applying machine learning
    methods to time series analysis. This is a relatively young area of time series
    analysis but one that has shown promise. The machine learning methods we will
    study were not originally developed for time series–specific data—unlike the statistical
    models we studied in the past two chapters—but they have proven useful for it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看几个例子，将机器学习方法应用于时间序列分析。这是时间序列分析的一个相对年轻的领域，但已显示出潜力。我们将研究的机器学习方法并不是原本为时间序列特定数据开发的——与我们在过去两章中研究的统计模型不同——但它们已被证明对其非常有用。
- en: This turn to machine learning is a shift from our previous work in forecasting
    in earlier chapters of this book. Up to this point we have focused on statistical
    models for time series forecasts. In developing such models, we formulated an
    underlying theory about the dynamics of a time series and the statistics describing
    the noise and uncertainty in its behavior. We then used the hypothesized dynamics
    of the process to make predictions and also to estimate our degree of uncertainty
    about the predictions. With such methods, both model identification and parameter
    estimation required that we think carefully about the best way to describe the
    dynamics of our data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这一转向机器学习是从本书早期章节的预测工作中转变而来的。到目前为止，我们一直专注于时间序列预测的统计模型。在开发这些模型时，我们制定了关于时间序列动态和描述其行为中噪声和不确定性的统计的基本理论。然后，我们利用假设的过程动态进行预测，还估计了关于预测的不确定度。对于这些方法，模型识别和参数估计都要求我们认真思考如何最好地描述我们数据的动态。
- en: We now turn to methodologies in which we do not posit an underlying process
    or any rules about that underlying process. We instead focus on identifying patterns
    that describe the process’s behavior in ways relevant to predicting the outcome
    of interest, such as the appropriate classification label for a time series. We
    will also consider unsupervised learning for time series, in the form of time
    series clustering.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向那些不假设底层过程或关于底层过程的任何规则的方法。我们的注意力转向识别描述过程行为的模式，这些模式对于预测感兴趣的结果（如时间序列的适当分类标签）具有相关性。我们还将考虑无监督学习，以时间序列聚类的形式。
- en: We cover prediction and classification with tree-based methodologies as well
    as clustering as a form of classification. In the case of tree-based methodologies,
    formulating features of our time series is a necessary step along the way of using
    the methodology, as trees are not a “time-aware” methodology, unlike, say, an
    ARIMA model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们涵盖了基于树的方法进行预测和分类，以及聚类作为分类的一种形式。在基于树的方法中，形成时间序列的特征是使用方法的必要步骤，因为树不是一个“时间感知”的方法，不像ARIMA模型。
- en: In the case of clustering and distance-based classification, we will see that
    we have the option to use features or to use the original time series as an input.
    To use the time series itself as an input, we study a distance metric known as
    *dynamic time warping*, which can be applied directly to time series, preserving
    the full chronological set of information in our data rather than collapsing it
    into a necessarily limited set of features.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类和基于距离的分类情况下，我们将看到我们可以选择使用特征或使用原始时间序列作为输入的选项。要将时间序列本身用作输入，我们研究了一种称为*动态时间规整*的距离度量标准，它可以直接应用于时间序列，保留了数据中完整的时间顺序信息，而不是将其折叠成一组必然有限的特征。
- en: Time Series Classification
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列分类
- en: In this section we work through an example of converting raw electroencephalogram
    (EEG) time series data into features, which can in turn be used for machine learning
    algorithms. We then use decision tree methods to classify EEG data after we have
    extracted features from the EEG time series.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过一个例子，演示如何将原始脑电图（EEG）时间序列数据转换为特征，然后可以将这些特征用于机器学习算法。之后，我们使用决策树方法对从EEG时间序列中提取的特征进行分类。
- en: Selecting and Generating Features
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择和生成特征
- en: 'In the previous chapter, we went through a general discussion of the purposes
    of time series feature generation. We also worked through a brief example of generating
    features for a time series data set via `tsfresh`. Now we will generate features
    with another time series feature package that was discussed: `cesium`.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们对时间序列特征生成的目的进行了一般讨论。我们还通过`tsfresh`对时间序列数据集生成特征进行了简要示例。现在我们将使用另一个讨论过的时间序列特征包`cesium`来生成特征。
- en: 'One of the very handy attributes of the `cesium` package is that it comes with
    a variety of helpful time series data sets, including an EEG data set originally
    derived from a [2001 research paper](https://perma.cc/YZD5-CTJF). In this paper,
    you can read more on the data preparation details. For our purposes it suffices
    to know that the five categories of EEG time series present in the data set all
    represent equal-length segments cut out of continuous time readings of EEG samples
    from the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`cesium`包的一个非常方便的属性之一是它提供了各种有用的时间序列数据集，包括最初来源于[2001年研究论文](https://perma.cc/YZD5-CTJF)的EEG数据集。在这篇论文中，您可以阅读更多有关数据准备的详细信息。对于我们的目的，知道EEG时间序列数据集中的五个类别均代表从以下连续时间EEG样本中剪切出的等长片段即可。'
- en: EEG recordings of healthy people with eyes both opened and closed (two separate
    categories)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康人群闭眼和睁眼时的EEG记录（两个独立类别）
- en: EEG recordings of epilepsy patients during seizure-free times from two non-seizure-related
    areas of the brain (two separate categories)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 癫痫患者在发作期间无发作时的脑部EEG记录，来自两个非癫痫相关区域（两个独立类别）
- en: An intracranial recording of EEG during a seizure (one category).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在癫痫发作期间进行的颅内EEG记录（一个类别）。
- en: 'We download that data set via a convenience function supplied by `cesium`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`cesium`提供的便捷函数下载了该数据集：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It can be helpful to first view a few examples of the data we are analyzing
    to get an idea of how we would like to classify these time series:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先查看我们正在分析的数据的几个示例可能是有帮助的，以了解我们希望如何对这些时间序列进行分类：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'These plots show some differences between the classes of EEG measurement (see
    [Figure 9-1](#fig-0901)). It is not surprising that the EEG plots are markedly
    different: they are measuring activity in different parts of the brain during
    different activities in both healthy subjects and epilepsy patients.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表显示了EEG测量类别之间的一些差异（见[图9-1](#fig-0901)）。EEG图表明显不同并不奇怪：它们正在测量不同大脑部分在健康受试者和癫痫患者中不同活动期间的活动。
- en: '![](assets/ptsa_0901.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0901.png)'
- en: Figure 9-1\. We plot three randomly selected samples from the EEG data set.
    These are independent samples and not contemporaneous measurements from different
    parts of the same brain. Each is an independent time series measurement taken
    of a different patient at a different time.
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 我们绘制了来自EEG数据集的三个随机选择样本。这些是独立的样本，而不是来自同一大脑不同部分的同时测量。每个是在不同时间对不同患者进行的独立时间序列测量。
- en: These visualizations provide guidance for feature generation. For example, class
    Z and G seem to have less skewed data than class S. Additionally, each class has
    quite a different range of values, as we can see by inspecting the y-axis. This
    suggests that an amplitude feature could be useful. Also, it is not just the overall
    amplitude but the overall distribution of the points that seems characteristically
    different in the three classes. We will use these features as well as a few others
    in our analysis, and next we show the code to generate such features.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可视化为特征生成提供了指导。例如，Z类和G类似乎比S类具有更少的偏斜数据。此外，每个类别的值范围差异相当大，通过检查y轴可以看出。这表明幅度特征可能是有用的。此外，不仅仅是总振幅，而是点的整体分布在三个类别中似乎具有特征性差异。我们将在分析中使用这些特征以及其他几个，并且接下来我们展示生成这些特征的代码。
- en: 'Here, we generate the features with `cesium`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`cesium`生成特征：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This produces our features as depicted in [Figure 9-2](#fig-0902), a screenshot
    from a Jupyter notebook.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了我们的特征，如从Jupyter笔记本中截图的[图9-2](#fig-0902)所示。
- en: '![](assets/ptsa_0902.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0902.png)'
- en: Figure 9-2\. Numerical values of features we generated for the first few samples
    in our data set.
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. 我们在数据集中生成的前几个样本的特征数值。
- en: Note that many of these values are not normalized, so that would be something
    we’d want to keep in mind were we using a technique that assumed normalized inputs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些值中许多没有经过标准化，所以如果我们使用假定已标准化输入的技术，这将是我们需要牢记的事情。
- en: 'We should also confirm that we understand what our features are indicating
    and that our understanding matches what `cesium` computes. As an illustration
    of error checking and common sense affirmation, we can verify the `percent_beyond_1_std`
    for one time series sample:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应确认我们理解我们的特征指示的内容，并且我们的理解是否与`cesium`计算的内容相符。作为错误检查和常识确认的例证，我们可以验证一个时间序列样本的`percent_beyond_1_std`：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Features Should Be Ergodic
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征应该是遍历的
- en: In choosing features to generate for a time series, be sure to select features
    that are *ergodic*, meaning that the values measured will each converge to a stable
    value as more data from the same process is collected. An example where this is
    not the case is a random walk, for which the measurement of the mean of the process
    is meaningless and will not be ergodic. The mean of a random walk will not converge
    to a specific value.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择为时间序列生成特征时，请确保选择*遍历性*特征，意味着随着来自同一过程的更多数据的收集，测量的值将收敛到稳定值。不满足这一条件的例子是随机漫步，对于随机漫步来说，过程的平均值测量是无意义的，也不是遍历性的。随机漫步的平均值不会收敛到一个特定值。
- en: In the EEG data we’ve plotted, different subsamples from any given time series
    are clearly comparable and the series itself is weakly stationary, so the features
    we generated make sense.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们绘制的 EEG 数据中，从任何给定时间序列的不同子样本显然是可比较的，并且序列本身是弱稳态的，因此我们生成的特征是有意义的。
- en: You should be able to verify any of the features you are using. This is a simple
    matter of responsible analysis. You should not be providing information to your
    algorithm that you cannot understand, explain, and verify.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够验证你正在使用的任何特征。这只是一个负责任分析的简单问题。你不应该向你的算法提供你不能理解、解释和验证的信息。
- en: Warning
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Don’t overuse feature generation libraries. It’s not difficult to write your
    own code to generate features. If you work in a domain where a particular set
    of features is generated often and in the same combinations repeatedly, you should
    write your own code even if you have initially used a package.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过度使用特征生成库。编写自己的代码生成特征并不难。如果你在一个领域工作，在这个领域中特定组合的特征经常生成，你应该编写自己的代码，即使最初使用了一个包。
- en: You will be able to optimize your code in ways that authors of a general exploratory
    package cannot. For example, if you have several features that rely on the calculation
    of the mean of a time series, you can create code to calculate that mean only
    once rather than once per separate feature calculated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过各种方法优化你的代码，而普通探索性包的作者无法做到这一点。例如，如果你有几个特征依赖于时间序列的平均值计算，你可以创建代码只计算一次平均值，而不是分别计算每个特征一次。
- en: Decision Tree Methods
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树方法
- en: 'Tree-based methods mirror the way humans make decisions: one step at a time,
    and in a highly nonlinear fashion. They mirror the way we make complicated decisions:
    one step at a time, thinking about how one variable should affect our decision,
    and then another, much like a flow chart.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的方法反映了人类做决策的方式：一步一步地，以非常非线性的方式。它们反映了我们做复杂决策的方式：一步一步地思考一个变量如何影响我们的决策，然后是另一个变量，非常类似于流程图。
- en: I assume you have already worked with decision trees or can quickly intuit what
    decision trees are. If you could use more support, pause here and check out some
    [background reading](https://perma.cc/G9AA-ANEN).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设你已经使用过决策树或可以快速直觉地理解决策树是什么。如果你需要更多支持，请暂停在这里并查看一些[背景阅读](https://perma.cc/G9AA-ANEN)。
- en: '[Figure 9-3](#fig-0903) shows a simple example of a decision tree that might
    be used to make an estimate of someone’s body weight.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-3](#fig-0903) 展示了一个简单的决策树示例，用于估计某人的体重。'
- en: '![](assets/ptsa_0903.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0903.png)'
- en: Figure 9-3\. In this simple regression tree, a series of logical branches are
    used to arrive at a predicted weight for a human. This is a crude model, but it
    illustrates the nonlinear and variable approach that even a simple tree model
    can apply to a regression problem.
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3。在这个简单的回归树中，一系列逻辑分支用于得出人体预测体重。这是一个粗糙的模型，但它说明了即使是简单的树模型也可以应用于回归问题的非线性和多变方法。
- en: There are abundant examples of humans behaving like a decision tree when analyzing
    time series data. For example, a discretionary stock market trader may use technical
    indicators, but they will likely use them in a serial hierarchical fashion, just
    like a tree—first asking, for example, which direction the momentum is trending
    according to one technical indicator before then asking how volatility is evolving
    over time, with the answer to this second question interacting with the first
    in a tree-like nonlinear fashion. Likely they have something like a decision tree
    structure in their brain that they use to make forecasts about how the market
    will move.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析时间序列数据时，有大量人类行为表现得像一个决策树。例如，一名自主的股票市场交易员可能会使用技术指标，但他们可能会以串行的层次化方式使用它们，就像一棵树一样
    —— 首先询问，例如，根据一个技术指标趋势动量的方向是什么，然后询问随时间变化波动率的演变情况，这第二个问题的答案与第一个问题以非线性的树状方式相互作用。很可能他们的大脑中有类似决策树结构，用来预测市场的走向。
- en: Similarly, when a medical professional reads an EEG or ECG, it is not uncommon
    to first look for the presence of one feature before considering another, sequentially
    working through a series of factors. If one feature is present and another isn’t,
    it will lead to a different diagnosis than in the converse case and hence a different
    forecast regarding a patient’s prognosis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当医疗专业人员阅读脑电图（EEG）或心电图（ECG）时，通常会先寻找一个特征的存在，然后再考虑其他特征，依次通过一系列因素进行工作。如果一个特征存在而另一个不存在，将会导致与相反情况下不同的诊断，从而对患者预后产生不同的预测。
- en: We will use the features we generated from the EEG data as inputs to two different
    decision tree methods, random forests and gradient boosted trees, each of which
    can be used for classification tasks. Our task will be classifying the EEG data
    we’ve discussed solely on the basis of the features we generated from the raw
    data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用从脑电图数据生成的特征作为输入，应用于两种不同的决策树方法，随机森林和梯度增强树，每种方法都可以用于分类任务。我们的任务将仅基于我们从原始数据生成的特征对讨论过的脑电图数据进行分类。
- en: Random forest
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: A random forest is a model in which we use not one decision tree but many. Our
    classification or regression is the result of averaging the outputs of these trees.
    Random forests look to the “wisdom of the crowd” where the crowd is composed of
    many simple models, none of which may itself be particularly good, but all of
    which together often outperform a highly refined but single decision tree.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种模型，其中我们不是使用单一的决策树，而是使用许多决策树。我们的分类或回归结果是这些树输出的平均值。随机森林寻求“众人的智慧”，其中众人由许多简单模型组成，每个模型本身可能并不特别好，但是所有这些模型在一起通常能够胜过一个经过精细调整的单一决策树。
- en: The idea of assembling a collection of models to produce a forecast rather than
    merely striving to find the single “best” model was articulated as early as 1969
    in *The Combination of Forecasts*, a research paper by two venerable statisticians,
    J.M. Bates and C.W.J. Granger. That paper showed that combining two separate forecasts
    of airline passenger data could lead to a model that had lower mean squared error,
    a surprising and, at the time, unintuitive result. The younger generation of analysts,
    who often have worked their way into data analysis via machine learning rather
    than statistics, finds such an idea intuitive rather than unsettling, and the
    random forest has become a workhorse for all sorts of forecasting problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 早在1969年的研究论文《预测的组合》中，两位备受尊敬的统计学家J.M.贝茨和C.W.J.格兰杰提出了将一组模型组合以产生预测的想法，而不仅仅是努力寻找单一的“最佳”模型。该论文表明，组合两个单独的航空公司乘客数据预测可以导致具有较低均方误差的模型，这是一个令人惊讶且当时并不直观的结果。年轻一代分析师，他们通常是通过机器学习而不是统计学进入数据分析领域，对这样的想法感到直观而不是令人不安，而随机森林已经成为各种预测问题的得力工具。
- en: A random forest is constructed according to parameters specifying the number
    of trees to train, as well as the maximum allowed depth of those trees. Then,
    for each individual tree, a random sample of the data and of its features is used
    to train that tree. The trees are generally parameterized to be quite simple so
    that overfitting can be avoided and the model can average over many general models,
    none of which is especially good but all of which are sufficiently general to
    avoid “traps” in the data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是根据指定的树木数量和这些树木的最大允许深度来构建的。然后，对于每棵树，使用数据和其特征的随机样本来训练该树。这些树通常被参数化为相当简单，以避免过拟合，并且模型可以对许多一般模型求平均，其中没有一个特别好，但所有这些模型都足够一般化以避免数据中的“陷阱”。
- en: 'As mentioned earlier, we will input the features computed for each time series
    sample into the model as our training outputs. In theory, we could think of ways
    to input our raw time series data, rather than the computed features, but there
    are a number of problems with this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们将计算出的每个时间序列样本的特征输入模型作为我们的训练输出。理论上，我们可以考虑如何输入原始的时间序列数据，而不是计算出的特征，但是这种方法存在一些问题：
- en: It would be complicated to deal with time series of unequal length.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不等长时间序列会很复杂。
- en: Such a large number of inputs (equal to or close to the number of time steps)
    would result in computationally expensive models and training.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这么多输入（等于或接近时间步长的数量）将导致计算成本高昂的模型和训练。
- en: Assuming no particular time step is very important (since any given time step
    would correlate to one feature), there would be a great deal of noise and very
    little signal to train on from the perspective of the tree, which would be seeing
    each time step as an input.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设没有特别重要的时间步长（因为任何给定时间步长都会与一个特征相关联），从树的角度来看，训练时会有很多噪音和非常少的信号。
- en: 'So random forests are not a good tool for working with time series data in
    its raw form, but they can be useful for working with it once it has been compressed
    into summary features. Here are a few specific reasons:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机森林不适合处理原始形式的时间序列数据，但在其被压缩为摘要特征后，它们可以是有用的。以下是一些具体原因：
- en: From an efficiency/computational resources perspective, it is wonderful to think
    that we can distill extremely long time series into a handful of features and
    find a model with reasonable accuracy.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从效率/计算资源的角度来看，我们可以将极长的时间序列压缩成少数特征，并找到具有合理准确度的模型，这是非常棒的。
- en: It is helpful that a random forest reduces the risk of overfitting. As we have
    discussed previously, overfitting is particularly a problem for time series analysis
    because of unfortunate synergies between overfitting and lookahead. Having a deliberately
    dumb/simple methodology combats some of this concern.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林减少了过拟合的风险是有帮助的。正如我们之前讨论过的，过拟合对于时间序列分析是一个问题，因为过拟合和前瞻之间存在不利的协同效应。有意愿采用愚蠢/简单的方法可以部分解决这些问题。
- en: Random forests may be particularly apt for time series data for which we do
    not have a working model or hypothesis regarding the underlying mechanics of the
    process.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们没有工作模型或关于过程基础机制的假设的时间序列数据，随机森林可能特别合适。
- en: As a general rule, analysts have had more success deploying random forests for
    cases of time series classification rather than for cases of time series forecasting.
    In contrast, the next method we discuss, gradient boosted trees, has been largely
    successful at both tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一般规则是，分析师在时间序列分类案例中更容易成功地部署随机森林，而不是时间序列预测案例。相比之下，我们讨论的下一个方法，梯度提升树，在这两个任务中都取得了很大成功。
- en: Gradient boosted trees
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: '*Boosting* is another way of building an ensemble of predictors. Boosting creates
    models sequentially with the idea that later models should correct the mistakes
    of earlier models and that data misfit by earlier models should be more heavily
    weighted by later models.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*提升* 是另一种构建预测器集合的方法。提升按顺序创建模型，后续模型应纠正先前模型的错误，并且后续模型应更重视先前模型的数据拟合不良。'
- en: Gradient boosted trees have become the go-to boosting methodology, and a particularly
    successful one for time series as evidenced by their success in some data science
    competitions in recent years.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树已成为首选的提升方法论，尤其在时间序列中表现卓越，这一点在最近几年的一些数据科学竞赛中得到了证明。
- en: '`XGBoost` works by building trees sequentially, with each tree seeking to predict
    the residuals of the combination of prior trees. So, for example, the first tree
    built by `XGBoost` will attempt to match the data directly (a category or a numeric
    value). The second tree will attempt to predict the true value minus the predicted
    value. The third tree will attempt to predict the true value minus the first tree’s
    predicted value minus the second tree’s prediction of the first tree’s residuals.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`XGBoost` 的工作方式是顺序构建树，每棵树都试图预测之前树组合的残差。例如，`XGBoost` 构建的第一棵树将尝试直接匹配数据（一个类别或数值）。第二棵树将尝试预测真实值减去预测值。第三棵树将尝试预测真实值减去第一棵树预测值再减去第二棵树对第一棵树残差的预测值。'
- en: However, `XGBoost` does not simply build models infinitely, attempting to minimize
    the residuals of the predicted residuals of the predicted residuals ad infinitum.
    The `XGBoost` algorithm minimizes a loss function that also includes a penalty
    term for model complexity, and this penalty term limits the number of trees that
    will be produced. It is also possible to directly limit the number of trees produced.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`XGBoost` 并不是无限地构建模型，试图无限制地最小化预测的残差的预测的残差。`XGBoost` 算法最小化的损失函数还包括一个惩罚项，用于限制将要生成的树的数量。也可以直接限制生成的树的数量。
- en: Over the last few years many have reported much greater success with `XGBoost`
    than with machine learning methods for time series, such as in Kaggle competitions
    or at industry machine learning conferences.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，许多人报告使用 `XGBoost` 比起传统的时间序列机器学习方法（例如在 Kaggle 竞赛或工业机器学习会议中）取得了更大的成功。
- en: Tip
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Tip
- en: '*Bagging* (or, more formally, *bootstrap aggregating*) refers to a technique
    for training models wherein randomly generated training sets are created for each
    different model in an ensemble. Random forests generally use a bagging methodology
    in model training.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bagging*（或更正式地称为 *bootstrap aggregating*）是一种训练模型的技术，其中为集成中的每个不同模型随机生成训练集。随机森林通常在模型训练中使用
    bagging 方法。'
- en: Boosting, as noted, refers to a technique for training models in which an ensemble
    is composed of sequentially trained models, each of which focuses on correcting
    the error made by its predecessor. Boosting is at the core of how gradient boosted
    tree models are trained.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting，正如已经注意到的，是一种训练模型的技术，其中一个集成由顺序训练的模型组成，每个模型专注于纠正其前任所犯的错误。Boosting 是梯度提升树模型训练的核心所在。
- en: Code example
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Code example
- en: In the case of random forests and `XGBoost`, it can be easier to code a machine
    learning model than it is to understand how that model works. In this example,
    we will train both a random forest and a gradient boosted tree model to classify
    our EEG data based on the features we generated.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机森林和 `XGBoost`，编写一个机器学习模型可能比理解该模型的工作原理更容易。在这个例子中，我们将训练一个随机森林和一个梯度提升树模型来基于我们生成的特征对我们的
    EEG 数据进行分类。
- en: 'We use `sklearn` to divide our data into training and testing data sets:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `sklearn` 将我们的数据分成训练和测试数据集：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We begin with a random forest classifier. Here we see the ease with which we
    can create a model to classify our EEG data:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用随机森林分类器。在这里，我们可以看到如何轻松创建一个用于分类我们的 EEG 数据的模型：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are then able to determine the out-of-sample accuracy of our data with a
    method call on the `Classifier` object:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后可以通过对 `Classifier` 对象的方法调用来确定我们数据的外样准确性：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In just a few lines of code, we have a model that can do better than I would
    be able to do as a human classifier (one without a medical education). Remember,
    too, that thanks to feature selection this model sees only summary statistics
    rather than an entire EEG.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅仅几行代码中，我们有一个模型，它比我作为一个没有医学教育的人类分类器做得更好。还要记住，由于特征选择，这个模型只看到摘要统计数据，而不是整个 EEG。
- en: 'The code for the `XGBoost` classifier is similarly straightforward and succinct:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`XGBoost` 分类器的代码同样简单而简洁：'
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see that the `XGBoost` classifier model does slightly better than the
    random forest model. It also trains slightly faster, as we can see with this quick
    experiment to calculate the training time for each model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `XGBoost` 分类器模型比随机森林模型稍微好一些。它还训练速度稍快，正如我们可以通过这个快速实验计算每个模型的训练时间所看到的：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This execution speed is a substantial improvement, with the random forest taking
    50% more time than `XGBoost`. While this is not a definitive test, it does point
    to an advantage of `XGBoost`, particularly if you are dealing with large data
    sets. You’d want to make sure that this advantage scaled up when you used larger
    data sets with more examples and more features.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种执行速度显著提高，随机森林比`XGBoost`花费的时间多50%。虽然这不是一个明确的测试，但它确实指出了`XGBoost`的优势，特别是在处理大数据集时。您需要确保这种优势在使用更多示例和更多特征的更大数据集时扩展。
- en: We could fairly ask whether there is something about our particular set of hyperparameters
    that gave the advantage to `XGBoost` over a random forest. For example, what if
    we use less complex trees by setting a lower depth? Or what if we allow fewer
    total decision trees to exist in the model? It’s quite easy to test these possibilities,
    and again we see that `XGBoost` tends to maintain its edge.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以公平地问，是否有关于我们特定的超参数集合，使`XGBoost`比随机森林更有优势的内容。例如，如果我们通过设置较低的深度使用更简单的树，或者如果我们允许模型中存在更少的决策树总数，这些可能性都很容易测试，再次显示出`XGBoost`倾向于保持其优势。
- en: 'For example, if we allow the same number of decision trees in the ensemble
    but lessen the complexity by decreasing the depth of the trees, we see that the
    gradient boosted model maintains a higher accuracy than does the random forest
    model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们允许集成中有相同数量的决策树，但通过减少树的深度降低了复杂性，我们会发现梯度提升模型的准确性高于随机森林模型：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is true even when we reduce the tree complexity further:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们进一步减少树的复杂度，这也是真实的：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There are several possible reasons to explain the performance and the putative
    advantage of gradient boosted trees over random forests. One important consideration
    is that we do not know for certain that all the features we selected for our classification
    are particularly useful. This highlights an example of when boosting (gradient
    boosted trees) could be preferable to bagging (random forest). Boosting will be
    more likely to ignore useless features because it will always make use of the
    full set of features and privilege the relevant ones, whereas some trees that
    result from bagging will be forced to use less meaningful features.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 解释梯度提升树优于随机森林的性能和潜在优势的几个可能原因。一个重要的考虑因素是，我们不能确定我们为分类选择的所有特征是否特别有用。这突显了梯度提升（梯度提升树）可能优于装袋（随机森林）的一个例子。提升会更有可能忽略无用的特征，因为它总是利用完整的特征集并优先选择相关的特征，而某些由装袋产生的树则被迫使用意义较小的特征。
- en: This also suggests the helpfulness of boosting when paired with the supercharged
    feature generation libraries we discussed in the last chapter. If you do take
    the approach of generating hundreds of time series features—far more than you
    can reasonably inspect—boosting may be a safeguard from truly disastrous results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这也表明了在与我们在上一章中讨论过的超级特征生成库配对时，提升的有用性。如果您采取生成数百个时间序列特征的方法——远远超出您能合理检查的数量——提升可能会避免真正灾难性的结果。
- en: Tip
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Gradient boosted trees are particularly useful for large data sets, including
    large time series data sets. The implementation you choose will likely affect
    your accuracy slightly and your training speed. In addition to `XGBoost` you should
    also consider `LightGBM` and `CatBoost`. These latter packages have sometimes
    been reported to perform quite a bit faster than `XGBoost`, though sometimes at
    a slight decrease in out-of-sample test accuracy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树对于大数据集特别有用，包括大型时间序列数据集。您选择的实现可能会稍微影响您的准确性和训练速度。除了`XGBoost`，您还应考虑`LightGBM`和`CatBoost`。据报道，后两个软件包的性能有时比`XGBoost`快得多，尽管有时在样本外测试准确性略有下降。
- en: Classification versus regression
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类与回归
- en: In the previous examples we considered random forests and gradient boosted tree
    methodologies for time series classification. These same methodologies can also
    be used for time series predictions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们考虑了用于时间序列分类的随机森林和梯度提升树方法。这些方法也可以用于时间序列预测。
- en: Many statisticians argue that machine learning has been less successful—or no
    more successful—than traditional time series statistical analysis in the domain
    of forecasting. However, in the last several years, gradient boosted trees for
    prediction have taken off and are indeed often outperforming traditional statistical
    models when given sufficiently large data sets, in both forecasting competitions
    and in industry applications. In such cases, however, extensive time is taken
    to tune the models’ parameters as well as to prepare time series features.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 许多统计学家认为，在预测领域，机器学习并没有比传统的时间序列统计分析更成功，或者说并没有更成功。然而，在过去几年中，梯度提升树用于预测已经起飞，并且在给定足够大的数据集时，通常在预测竞赛和工业应用中表现优异。然而，在这种情况下，需要花费大量时间来调整模型的参数以及准备时间序列特征。
- en: One of the strengths of gradient boosted tree models is that they approach “autopilot”
    in their ability to weed out irrelevant or noisy features and focus on the most
    important ones. This tendency alone, however, will not be enough to get state-of-the-art
    performance from a model. Even for a seemingly automatic method, such as gradient
    boosted trees, the outputs can only be as good as the inputs. The most important
    way to improve your model will still be to provide high-quality and well-tested
    input features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树模型的一个优势在于它们在筛选掉无关或嘈杂特征并专注于最重要特征方面接近“自动驾驶”。然而，仅凭这种倾向还不足以使模型达到最先进的性能。即使对于看似自动的方法，如梯度提升树，输出的好坏也取决于输入的质量。提高模型性能的最重要方式仍然是提供高质量和经过充分测试的输入特征。
- en: There are many options for improving upon the current model. We could learn
    from it by using `XGBoost`’s option to produce feature importance metrics. This
    could help us identify traits of useful features and nonuseful features, and we
    could then expand the data set by adding similar features to those that are already
    judged useful. We could also do a hyperparameter grid search to tune our model
    parameterization. Finally, we could look at our mislabeled data’s raw time series
    to see whether there are traits of the mislabeled data that may not be represented
    by our current set of features. We could consider adding features that might better
    describe the mislabeled data, augmenting our inputs further.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以改进当前的模型。我们可以通过使用`XGBoost`的选项来生成特征重要性指标来学习。这可以帮助我们识别有用特征和无用特征的特性，然后我们可以通过添加类似于已判断为有用的特征的新特征来扩展数据集。我们还可以进行超参数网格搜索来调整模型参数化。最后，我们可以查看误标记数据的原始时间序列，看看是否有误标记数据的特性未被当前特征集所表示。我们可以考虑添加能更好地描述误标记数据的特征，进一步增强我们的输入。
- en: Clustering
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: The general idea of clustering is that data points that are similar to one another
    constitute meaningful groups for purposes of analysis. This idea holds just as
    true for time series data as for other kinds of data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的一般理念是，彼此相似的数据点在分析目的上构成有意义的群组。这一理念对于时间序列数据和其他类型的数据同样适用。
- en: As with our earlier discussion, I assume that you have some familiarity with
    the relevant machine learning concepts in a non-time-series context. If you are
    not familiar with clustering techniques, I recommend that you pursue some short
    [background reading](https://perma.cc/36EX-3QJU) on clustering before continuing
    this section.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的那样，我假设您对非时间序列上下文中的相关机器学习概念有一定的了解。如果您对聚类技术不熟悉，我建议您在继续本节之前进行一些简短的[背景阅读](https://perma.cc/36EX-3QJU)。
- en: Clustering for time series can be used both for classification and for forecasting.
    In the case of classification, we can use clustering algorithms to identify the
    desired number of clusters during the training phase. We can then use these clusters
    to establish types of time series and recognize when new samples belong to a particular
    group.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的聚类可以用于分类和预测两种情况。在分类的情况下，我们可以使用聚类算法在训练阶段识别出所需的聚类数量。然后，我们可以使用这些聚类来建立时间序列的类型，并识别新样本是否属于特定群组。
- en: In the case of forecasting, the application can be pure clustering or can be
    inspired by clustering in the form of using relevant distance metrics (more on
    distance metrics soon). There are a few options for generating forecasts at a
    horizon, *h*, in the future based on clustering and related techniques. Remember
    that in this case, we will not have fully observed a time series but only its
    first *N* steps, from which we want to forecast its value at time step *N* + *h*.
    In such a case there are a few options.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测的情况下，应用可以是纯粹的聚类，也可以受到聚类的启发，以使用相关的距离度量（关于距离度量的更多内容即将详述）。有几种选项可以根据聚类和相关技术在未来的时间*H*内生成预测。请记住，在这种情况下，我们将不会完全观察到时间序列，而只能看到其前*N*步，我们希望预测其在时间步*N*
    + *h*的值。在这种情况下有几种选项。
- en: One option is to use class membership to generate a forecast based on typical
    behavior in that class. To do this, first determine which cluster a time series
    sample belongs to based on its first *N* time steps and then infer likely future
    behavior based on cluster membership. Specifically, look at how values of time
    series in this cluster tend to change their values between time step *N* and time
    step *N* + *h*. Note that you would want to perform the original clustering for
    all the time series based on their first *N* steps rather than all parts of the
    time series to avoid lookahead.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是使用类成员资格来生成基于该类典型行为的预测。为此，首先确定时间序列样本根据其前*N*个时间步属于哪个聚类，然后根据聚类成员资格推断未来行为的可能性。具体来说，查看这个聚类中时间序列值在时间步*N*和时间步*N*
    + *h*之间如何变化。请注意，您需要基于其前*N*步执行所有时间序列的原始聚类，而不是基于时间序列的所有部分，以避免预先看到。
- en: Another option is to predict future behavior of a sample time series based on
    the behavior of its nearest neighbor (or neighbors) in the sample space. In this
    scenario, based on metrics from the first *N* time steps, find a time series sample’s
    nearest neighbor(s), for which the full trajectory is known. Then average the
    *N* + *h* behavior of these nearest neighbors, and this is your forecast for the
    current sample.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是基于样本时间序列在样本空间中最近邻（或最近邻）的行为预测未来行为。在这种情况下，根据前*N*个时间步的指标，找到时间序列样本的最近邻（们），其完整轨迹已知。然后对这些最近邻的*N*
    + *h*行为取平均，这就是当前样本的预测。
- en: In the case of both classification and forecasting, the most important consideration
    is how to assess similarity between time series. Clustering can be done with a
    variety of distance metrics, and much research has been devoted to thinking about
    how to measure distance in high-dimensional problems. For example, what is the
    “distance” between two job applicants? What is the “distance” between two blood
    samples? These challenges are already present in cross-sectional data, and they
    persist in time series data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类和预测的情况下，最重要的考虑因素是如何评估时间序列之间的相似性。可以使用各种距离度量进行聚类，并且已经有大量研究致力于思考如何在高维问题中测量距离。例如，两个求职者之间的“距离”是什么？两个血样本之间的“距离”是什么？这些挑战在横截面数据中已经存在，并且在时间序列数据中仍然存在。
- en: 'We have two broad classes of distance-metric options when applying clustering
    techniques to time series data:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在将聚类技术应用于时间序列数据时，我们有两类距离度量选项：
- en: Distance based on features
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征的距离
- en: Generate features for the time series and treat these as the coordinates for
    which to calculate data. This does not fully solve the problem of choosing a distance
    metric, but it reduces the problem to the same distance metric problem posed by
    any cross-sectional data set.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为时间序列生成特征，并将其视为计算数据的坐标。这并不能完全解决选择距离度量的问题，但它将问题缩小到与任何横截面数据集提出的距离度量问题相同的范围内。
- en: Distance based on the raw time series data
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基于原始时间序列数据的距离
- en: Find a way to determine how “close” different time series are, preferably in
    a way that can handle different temporal scales, a different number of measurements,
    and other likely disparities between time series samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一种方法来确定不同时间序列的“接近程度”，最好能够处理不同的时间尺度、不同的测量次数以及其他可能存在的时间序列样本之间的差异。
- en: We will apply both of these distance metrics to a time series data set in which
    each sample represents the projection of a handwritten word from a 2D image to
    a 1D time series.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这两种距离度量应用于一个时间序列数据集，其中每个样本代表从2D图像到1D时间序列的手写单词的投影。
- en: Generating Features from the Data
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据生成特征
- en: We have already discussed ways of generating and selecting features. Here we
    consider how to assess the distance between time series data sets based on the
    similarity of their features.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了生成和选择特征的方法。在这里，我们考虑如何根据它们特征的相似性来评估时间序列数据集之间的距离。
- en: In an ideal scenario, we would already have culled unimportant or uninteresting
    time series, perhaps by using a tree to assess feature importance. We do not want
    to include such features in a distance calculation since they may falsely indicate
    dissimilarities between two time series, when in fact they simply do not indicate
    a relevant similarity relative to the classes in our classification task or the
    outcome in our forecasting task.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，我们可能已经通过使用树来评估特征的重要性来删除不重要或无趣的时间序列。我们不希望将这些特征包含在距离计算中，因为它们可能会错误地指示两个时间序列之间的不同，而实际上它们只是不与我们分类任务中的类别或预测任务中的结果相关。
- en: As we did with the EEG data set, we start our analysis by taking a look at some
    class examples and noticing what obvious differences there are over time and in
    the structure of the time series.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们对EEG数据集所做的那样，我们从查看一些类别示例开始我们的分析，并注意随时间变化及时间序列结构中的明显差异。
- en: Our data is a subset of the [FiftyWords](https://oreil.ly/yadNp) data set available
    from the UEA and UCR Time Series Classification Repository. This data set was
    released by the authors of a 2003 [paper](https://oreil.ly/01UJ8) on clustering
    handwritten words in historical documents. In that paper, the authors developed
    “word profiles” as a way of mapping the 2D image of a handwritten word into a
    1D curve, consisting of the same number of measurements regardless of word length.
    The repository data set is not identical to that in the paper, but the same principles
    apply. The purpose of the original paper was to develop a method of tagging all
    similar or identical words in a document with a single label so that humans could
    go back and label these words digitally (a feat that nowadays might very well
    be directly accomplished by a neural network, with 20 years of technological improvement
    to help with the task).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据是UEA和UCR时间序列分类存储库提供的[FiftyWords](https://oreil.ly/yadNp)数据集的子集。这个数据集是根据一篇2003年的[论文](https://oreil.ly/01UJ8)中的方法进行释放的，该论文研究了在历史文档中聚类手写单词。在那篇论文中，作者开发了“单词轮廓”作为将手写单词的二维图像映射到一维曲线的一种方法，该曲线由相同数量的测量组成，无论单词长度如何。存储库数据集与论文中的数据集不完全相同，但原理相同。原始论文的目的是开发一种方法，通过单一标签标记文档中所有相似或相同的单词，以便人类可以返回并数字化地标记这些单词（这是一个现在可能会直接由神经网络完成的壮举，借助20年的技术进步来帮助完成任务）。
- en: So in this example we see sample *projection profiles* of the words, where “projection”
    refers to the fact that they converted an image from a 2D space to a 1D space,
    and this latter space, since ordering mattered, is amenable to time series analysis.
    Note that the “time” axis is not actually time but rather left-to-right progressions
    of written words. Nonetheless the concept is the same—ordered and equally spaced
    data—so for simplicity I will use the words *time* and *temporal* in the analysis
    even though this is not strictly true. For our use case, there isn’t a distinction.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们看到了*投影文件*中的样本投影，其中“投影”指的是它们将图像从二维空间转换为一维空间，后者由于顺序的重要性，适合进行时间序列分析。请注意，“时间”轴实际上不是时间，而是书面文字从左到右的进展。尽管如此，概念是相同的——有序且等间距的数据——因此为了简单起见，在分析中我将使用*时间*和*时间上的*这两个词，尽管这并非严格正确。对于我们的用例，这没有区别。
- en: In [Figure 9-4](#fig-0904) we see examples of a few distinct words.^([1](ch09.html#idm45576031287336))
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图9-4](#fig-0904)中，我们看到了几个不同单词的示例。^([1](ch09.html#idm45576031287336))
- en: When we examine this data, and particularly when we consider the patterns obvious
    to the human eye in the plots, just as with the EEG we can formulate a starting
    point of some features to consider for our analysis—for example, the heights and
    locations of the peaks as well as their characteristics, such as how sharply they
    rise and what shape there is at the top of the peak.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查这些数据时，特别是考虑到绘图中对人眼明显的模式，就像对EEG一样，我们可以制定一些特征作为我们分析的起点——例如，峰值的高度和位置以及它们上升的锐度和峰顶的形状。
- en: Many of the features I am describing begin to sound more like image recognition
    features than like a time series, and this is a helpful perspective to have for
    feature generation. After all, visual data is often data we can easily process
    and have more intuition about than time series data. It could be helpful to think
    in terms of images when thinking about features. It is also a perspective that
    illustrates why generating features can be surprisingly difficult. In some cases
    it may be obvious from examining a time series how to distinguish two classes,
    but we may find that how to write the code is not so obvious. Or we may find that
    we can write the code, but it is extremely taxing.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述的许多特征开始听起来更像是图像识别特征，而不像时间序列，这对于特征生成是一个有用的视角。毕竟，视觉数据通常是我们可以轻松处理并且更直观的数据。在考虑特征时，将其视为图像可能会有所帮助。这个视角也说明了为什么生成特征可能会令人惊讶地困难。在某些情况下，通过检查时间序列可能很明显如何区分两个类别，但我们可能会发现编写代码并不那么明显。或者我们可能会发现，我们可以编写代码，但这非常费力。
- en: '![](assets/ptsa_0904.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0904.png)'
- en: 'Figure 9-4\. The projection profiles of three different words (12, 23, and
    9) are quite different from one another. We also already see some features that
    could distinguish these words from one another: the temporal location (x-axis)
    of the largest peak or even of the second-largest peak, how many local peaks there
    are, the overall range of values, and the average convexity of the curves.'
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 三个不同单词（12、23 和 9）的投影轮廓彼此非常不同。我们已经看到一些可以区分这些单词的特征：最大峰值或第二大峰值的时间位置（x 轴），有多少局部峰值，值的总范围以及曲线的平均凸度。
- en: In the last decade, deep learning has emerged as the strongest performer for
    image classification, and with sufficient data, we could train a deep learning
    classifier on the images of these plots (more on this in [Chapter 10](ch10.html#dl_for_time_series_chapter)).
    For now, it will be helpful for us to think of ways around the difficulty of programming.
    For example, it would be difficult to generate features locating each peak, as
    peak finding is programmatically demanding and something of an art form.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，深度学习已经成为图像分类的最强表现者，有了足够的数据，我们可以在这些图像的基础上训练深度学习分类器（更多内容请参见[第10章](ch10.html#dl_for_time_series_chapter)）。目前，对于我们来说，想出解决编程困难的方法将是有帮助的。例如，生成定位每个峰值的特征将会很困难，因为找峰值在程序上要求很高，而且有点艺术性。
- en: 'We can also use a 1D histogram, either of all the class examples or of an individual
    example. This may suggest computationally less taxing ways either of identifying
    peaks or finding other proxy values that will map onto the overall shapes we see
    in the time series. Here we plot the same individual class members plotted before,
    now accompanied by their 1D histograms (see [Figure 9-5](#fig-0905)):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 1D 直方图，无论是所有类别示例的还是单个示例的。这可能建议出一些计算上较少要求的方法，用于识别峰值或找到其他代理值，这些代理值将映射到我们在时间序列中看到的整体形状。在这里，我们绘制了以前绘制过的相同的单个类成员，现在伴随它们的
    1D 直方图（参见[图 9-5](#fig-0905)）：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](assets/ptsa_0905.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0905.png)'
- en: Figure 9-5\. Another way of measuring classes to brainstorm useful features.
    In particular, the histogram of individual class examples indicates that attributes
    of the histogram, such as the number of local peaks, the skewness, and the kurtosis
    would be helpful and possibly good proxies for some of the attributes of the time
    series curves themselves that are obvious to the human eye but not easy to identify
    with code.
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. 衡量类别的另一种方法是，头脑风暴有用的特征。特别是，单个类别示例的直方图表明直方图的属性，如局部峰值的数量，偏斜度和峰度可能会有所帮助，并且可能是一些显而易见的时间序列曲线属性的良好代理，这些属性对人眼来说很明显，但用代码识别起来并不容易。
- en: 'We also want to make sure that the examples we are looking at are not outliers
    compared to the other examples for these words. For this reason, we construct
    the 2D histogram for two words to get an idea of the individual variation (see
    [Figure 9-6](#fig-0906)):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望确保我们正在查看的示例与这些单词的其他示例不是离群值。因此，我们构造了两个单词的 2D 直方图，以了解个体变异（参见[图 9-6](#fig-0906)）：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](assets/ptsa_0906.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0906.png)'
- en: Figure 9-6\. A 2D histogram of the 1D word projections for word = 12\. The y-axis
    is the value at a given time step, and the x-axis represents the 270 time steps
    for each time series sample/word projection.
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-6\. 单词 = 12 的 1D 单词投影的 2D 直方图。y 轴是给定时间步长的值，x 轴代表每个时间序列样本/单词投影的 270 个时间步长。
- en: '[Figure 9-6](#fig-0906) shows the 2D histogram for all the word = 12 members
    of the data set. While the individual curve in [Figure 9-5](#fig-0905) suggested
    that we focus on finding the two large peaks that seemed to dominate the time
    series, here we see that what most of the members of this class have in common
    is likely the flat span between these peaks, which appears to go from around time
    steps 120 through 200, based on the intensity of the points in that region.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-6](#fig-0906)展示了数据集中所有word = 12成员的2D直方图。虽然[图9-5](#fig-0905)中的单个曲线表明我们应该专注于找到似乎主导时间序列的两个大峰值，但在这里我们看到，这一类大多数成员共同点可能是这些峰值之间的平坦区间，根据该区域内点的强度，这个区间似乎从大约时间步骤120到200。'
- en: We can also use this 2D histogram to establish a cutoff point for the maximum
    peak for this class, which seems to vary in location between time steps 50 and
    150\. We may even want to code up a feature that is as specific as “is the maximum
    value reached between points 50 and 150?”
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用这个2D直方图来建立这一类的最大峰值的截止点，这一点似乎在时间步骤50和150之间的位置变化。我们甚至可能希望编写一个像“在点50到150之间是否达到最大值”这样具体的功能。
- en: We plot another 2D histogram for the same reason, this time choosing word class
    23, which has many small bumps in the example we plotted in [Figure 9-5](#fig-0905),
    a difficult-to-quantize feature (see [Figure 9-7](#fig-0907)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们出于同样的原因绘制了另一个2D直方图，这次选择word类别23，该类别在我们在[图9-5](#fig-0905)中绘制的例子中有许多小颠簸，这是一个难以量化的特征（参见[图9-7](#fig-0907)）。
- en: '![](assets/ptsa_0907.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0907.png)'
- en: Figure 9-7\. A 2D histogram of the 1D word projections for word = 23\. The y-axis
    is the value at a given time step, and the x-axis represents the 270 time steps
    for each time series sample/word projection.
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. word = 23的1D单词投影的2D直方图。y轴表示给定时间步骤的值，x轴代表每个时间序列样本/单词投影的270个时间步骤。
- en: It is not surprising to see a particularly “smeared” histogram for word class
    23 in [Figure 9-7](#fig-0907), given that even the example we plotted in [Figure 9-5](#fig-0905)
    showed so many features that we would expect to see a lot of smearing in the 2D
    histogram if the features did not match up exactly between samples. However, we
    also see that the maximum point value in this class comes at a nonoverlapping
    range of time steps here, as compared to word class 12\. For this class, the maximum
    value comes after 150, which makes sense given that the two largest peaks we saw
    in the word class 23 example were in this range. The 2D histogram tends to substantiate
    that the earlier peak is not as tall as the later peak, suggesting other ways
    to quantize the shape of this time series to distinguish it from other classes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 看到word类别23在[图9-7](#fig-0907)中特别“扩展”的直方图并不奇怪，因为即使我们在[图9-5](#fig-0905)中绘制的例子中显示了许多特征，如果这些特征在样本之间没有完全匹配，我们也会预期在2D直方图中看到许多扩展。然而，我们还看到这一类的最大点值在这里的时间步骤范围内没有重叠，与word类别12相比。对于这一类，最大值出现在150之后，这是合理的，因为我们在word类别23示例中看到的两个最大峰值正好在这个范围内。2D直方图倾向于证实，较早的峰值不如较晚的峰值高，表明有其他方法来量化这个时间序列的形状，以区别于其他类别。
- en: The 2D histograms are helpful in letting us know the variability of a feature
    within an individual class, so that we do not unduly rely on a single class example
    when thinking about how to form our features.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些2D直方图有助于让我们了解单个类别内特征的变异性，这样我们在考虑如何形成我们的特征时，就不会过分依赖一个单一的类别示例。
- en: In this case we choose to generate a set of features derived from the shape
    of the word projection, and an additional set of features derived from the shape
    of the word projection’s histogram (projecting a 1D summary into a different 1D
    summary, from which we generate features). This is in response to the large “smears”
    we see in the 2D histograms, indicating that there are peaks but that their location
    is not particularly stable. Using a histogram to generate a second characteristic
    shape for each word projection may prove more reliable and characteristic than
    the word projection itself. Histograms characterize what kinds of values appear
    in the series without characterizing their location in the series, which is what
    is important to us given that the peaks in the projections don’t have especially
    stable temporal locations within the time series.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们选择生成一组从词投影形状衍生出的特征，以及从词投影直方图形状衍生出的另一组特征（将一维摘要投射到不同的一维摘要，再从中生成特征）。这是对我们在二维直方图中看到的大“模糊区域”的响应，表明存在高峰，但它们的位置并不特别稳定。使用直方图生成每个词投影的第二个特征形状可能比词投影本身更可靠和特征化。直方图表征系列中出现的值的类型，而不表征它们在系列中的位置，而对于我们来说，重要的是考虑到投影中的高峰并没有特别稳定的时间位置。
- en: 'First, we generate features for the times series, which are 270 time steps
    in length apiece. In this case, we shorten the name of the function used to generate
    features for code readability:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为每个时间序列生成270个时间步长的特征。在这种情况下，我们缩短了用于生成代码可读性特征的函数名称：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next we generate histograms and use these as another time series for which to
    generate features:^([2](ch09.html#idm45576031005608))
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成直方图，并将其用作另一个要生成特征的时间序列：^([2](ch09.html#idm45576031005608))
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We made sure that all histograms use the same number of bins and the same range
    of values as the basis for the bins, via the parameters we pass to `np.histogram()`.
    This ensures that all the histograms are directly comparable, having the same
    range of bin values, which will be the “temporal” axis when these histograms are
    run through the time series feature generation. If we did not enforce this consistency,
    the features generated would not necessarily be meaningful in comparing one histogram
    to another.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过传递给`np.histogram()`的参数，确保所有的直方图使用相同数量和值范围的箱子作为基础。这样做确保所有的直方图是直接可比较的，具有相同范围的箱值，这将在这些直方图通过时间序列特征生成时成为“时间”轴。如果我们不强制保持这种一致性，生成的特征可能不一定有意义，无法比较一个直方图与另一个。
- en: 'Finally, we combine these two sources of features:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们结合这两个特征来源：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Temporally Aware Distance Metrics
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间感知距离度量
- en: When running clustering analysis we have to choose a distance metric. With time
    series features, as we just did, we can apply a variety of standard distance metrics
    to them, as is done in standard clustering analysis on cross-sectional data. If
    you are not familiar with the process of choosing a distance metric in such cases,
    I recommend a short detour to do some [background reading](https://perma.cc/MHL9-2Y8A).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 运行聚类分析时，我们必须选择一个距离度量。对于时间序列特征，正如我们刚才所做的那样，我们可以将各种标准距离度量应用于它们，就像在横截面数据的标准聚类分析中所做的那样。如果您对在这些情况下选择距离度量的过程不熟悉，我建议您先做一些[背景阅读](https://perma.cc/MHL9-2Y8A)。
- en: In this section we will focus on the problem of measuring similarity between
    time series by defining a distance metric between them. One of the most well-known
    examples of such metrics is dynamic time warping (DTW). DTW is apt for clustering
    a time series whose most salient feature is its overall shape, as is the case
    with our word projection data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论通过定义它们之间的距离度量来测量时间序列之间相似性的问题。其中一个最著名的度量指标例子是动态时间规整（DTW）。DTW适用于聚类时间序列，其最显著的特征是其整体形状，正如我们的词投影数据所示。
- en: The technique’s name is inspired by the methodology, which relies on temporal
    “warping” to align time series along their temporal axis so as to compare their
    shapes. A picture is far more valuable than words for conveying the concept of
    dynamic time warping, so take a look at [Figure 9-8](#fig-0908). The temporal
    (x) axis is warped—that is, expanded or contracted as convenient—to find the best
    alignment of points between the two curves depicted (i.e., two time series), in
    order to compare their shape.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术的名字源于其方法论，依赖于时间上的“扭曲”，以使时间序列沿其时间轴对齐，从而比较它们的形状。用图片来传达动态时间扭曲的概念远比用文字更有价值，所以请看一下[图 9-8](#fig-0908)。时间（x
    轴）被扭曲——即根据需要被扩展或收缩——以便在两条描绘的曲线（即两个时间序列）之间找到最佳的点对齐，以比较它们的形状。
- en: '![](assets/ptsa_0908.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/ptsa_0908.png)'
- en: 'Figure 9-8\. How dynamic time warping works. Each point on one time series
    is mapped to a point on the opposite time series, but there is no requirement
    that there must be a one-to-one mapping of points. This has a few implications:
    (1) Time series need not be of the same length or even of the same timescale.
    What matters is the shape. (2) Time does not always move forward during the fitting
    process, and may not move at the same pace for each time series. By time moving,
    I mean progressing along the curve in the x-axis direction. Source: [Wikipedia](https://perma.cc/F9ER-RTDS).'
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-8\. 动态时间扭曲的工作原理。一个时间序列上的每个点都映射到另一个时间序列上的一个点，但并不要求必须有一对一的点映射。这有一些影响：(1) 时间序列不需要具有相同的长度或相同的时间尺度。重要的是形状。(2)
    在拟合过程中，时间并不总是沿着相同的速度前进，并且可能不同时间序列的进展速度也不相同。通过时间的进展，我指的是沿着x轴方向的曲线。来源：[维基百科](https://perma.cc/F9ER-RTDS)。
- en: Note that the actual time values on the time axis of one curve as compared to
    another are not relevant in the standard form of this algorithm. We could be comparing
    one time series that is measured in nanoseconds to another that is measured in
    millennia (although this would likely not be a sensible exercise). The purpose
    of the algorithm is akin to comparing the visual “shape” of that algorithm rather
    than thinking about how much time is passing. Indeed, “time” is really only meant
    in the more general sense of an ordered evenly spaced set of points along the
    x-axis rather than time proper.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种算法的标准形式中，与另一条曲线相比的一个曲线上的实际时间值与另一个曲线上的时间轴上的时间值并不相关。我们可以比较一个以纳秒为单位测量的时间序列与另一个以千年为单位测量的时间序列（虽然这可能不是一个明智的练习）。算法的目的类似于比较该算法的视觉“形状”，而不是考虑时间流逝的多少。实际上，“时间”在这里只是指一个有序的均匀间隔的点集，而不是时间本身的概念。
- en: 'The rules of DTW are as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: DTW的规则如下：
- en: Every point in one time series must be matched with at least one point of the
    other time series.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一个时间序列中的点都必须至少与另一个时间序列中的一个点匹配。
- en: The first and last indices of each time series must be matched with their counterparts
    in the other time series.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时间序列的第一个和最后一个索引必须与另一个时间序列中相应的索引匹配。
- en: The mapping of points must be such that time moves forward and not backward.
    There is no going back in time by matching a point in one time series with a point
    in the other time series that has already been passed on the time axis. However,
    time does not need to move forward constantly. For example, two consecutive time
    steps in the original series could be warped by being condensed to the same place
    on the x-axis during the fit, as is illustrated in [Figure 9-8](#fig-0908) at
    the first “kink” in the upper curve/solid line.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点的映射必须使时间向前移动，而不是向后。通过匹配一个时间序列中的一个点与另一个时间序列中的一个已经在时间轴上过去的点，不能回到过去。然而，时间不必始终向前移动。例如，原始系列中的两个连续时间步骤可以通过在拟合期间被压缩到x轴上的相同位置来被扭曲，就像在[图 9-8](#fig-0908)中在上曲线/实线的第一个“弯曲”处所示。
- en: There are many ways that the temporal alignment can be adjusted to follow these
    rules, but the selected match is the one that minimizes the distance between the
    curves. This distance, or cost function, is often measured as the sum of absolute
    differences between matched points, where the absolute difference is the difference
    between the points’ values.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以调整时间对齐以遵循这些规则，但选择的匹配是使曲线之间距离最小化的匹配。这个距离，或成本函数，通常被测量为匹配点之间的绝对差的和，其中绝对差是指点之间的值的差异。
- en: 'Now that we have an idea of how DTW works intuitively, we can take a look at
    the code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对DTW的直观工作方式有了一定的了解，我们可以看看代码：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As indicated in the comments, the solution to this problem is an example of
    dynamic programming, and DTW distance is a classic dynamic programming problem.
    We can take one step at a time on the path from the beginning of each time series
    to the end, and we know that we can build on the solution one step at a time and
    refer back to our earlier knowledge to make later decisions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如评论所指出的，解决这个问题的方法是动态规划的一个例子，DTW距离是一个经典的动态规划问题。我们可以从每个时间序列的开头逐步迈出一步，知道我们可以一步一步地建立解决方案，并参考我们先前的知识来做出后续的决策。
- en: There are a number of different DTW implementations, with a variety of ideas
    for making the search for the optimum solution, or a near-optimum solution, more
    efficient. These should be explored, particularly if you are working with a larger
    data set.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的DTW实现，有各种各样的想法来使寻找最优解或接近最优解的过程更加高效。如果您正在处理更大的数据集，应该特别注意这些。
- en: 'There are also other ways of measuring distances between time series. Here
    are a few:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他衡量时间序列之间距离的方法。以下是几种：
- en: Fréchet distance
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 弗雷歇距离
- en: This is the maximum distance between two curves during a time-warping-like traversal
    of the curves that always seeks to minimize the distance between two curves. This
    distance metric is often explained by the analogy of a dog and its human companion
    walking on the two curves with a leash between them. They each need to traverse
    a separate curve from beginning to end, and they can go at different speeds and
    vary their speeds along the curve so long as they always move in the same direction.
    The Fréchet distance is the shortest length of leash necessary for them to complete
    the task following the optimal trajectory (assuming they can find it!).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两条曲线在时间扭曲遍历期间的最大距离，始终寻求最小化两条曲线之间的距离的一个示例。这种距离度量通常通过一个狗和它的主人伴随着两条曲线之间的系带进行解释。它们需要分别从开始到结束遍历每条曲线，它们可以以不同的速度前进，并且可以在曲线上改变速度，只要它们始终朝着同一个方向移动。弗雷歇距离是它们完成任务所需的系带的最短长度，遵循最佳轨迹（假设它们能找到！）。
- en: Pearson correlation
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: The correlation between two time series can be a way of measuring the distance
    between them. Unlike with other distance metrics, you will minimize the distance
    between the time series by maximizing the metric of the correlation. Correlation
    is relatively easy to compute. However, this method requires that the time series
    have the same number of data points or that one be downsampled to match the fewer
    data points of the other. The time complexity of computing correlation is *O*(*n*),
    which makes it a particularly efficient metric from a computational resources
    perspective.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 两个时间序列之间的相关性可以作为衡量它们之间距离的一种方式。与其他距离度量不同，通过最大化相关性指标来最小化时间序列之间的距离。相关性相对容易计算。然而，这种方法要求时间序列具有相同数量的数据点，或者将一个时间序列进行降采样以匹配另一个时间序列的较少数据点。计算相关性的时间复杂度为*O*(*n*)，这使得它在计算资源方面特别高效。
- en: Longest common subsequence
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最长公共子序列
- en: This distance measure is appropriate for time series that represent a sequence
    of categorical or integral values. In such cases, to consider the similarity of
    two time series, we can determine the length of the longest common subsequence,
    meaning the longest length of consecutive values that are exactly identical, although
    their exact location in the time series is not required to match. As with DTW,
    this means that we are more concerned with finding a shape of commonality rather
    than where in time the common shape is occurring. Also note that like DTW, but
    unlike Pearson correlation, this does not require that the time series have the
    same length. A related measure is *edit distance*, whereby we find the number
    of changes we would need to make to one time series to make it identical to another
    and use this value to define a distance metric.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种距离测量适用于表示分类或整数值序列的时间序列。在这种情况下，为了考虑两个时间序列的相似性，我们可以确定最长公共子序列的长度，即连续值完全相同的最长长度，尽管它们在时间序列中的确切位置不必匹配。与DTW类似，这意味着我们更关心找到一个共同性的形状，而不是共同形状出现的时间。还要注意，像DTW一样，但不像皮尔逊相关性，这不要求时间序列具有相同的长度。一个相关的测量是编辑距离，通过该距离我们找到我们需要对一个时间序列进行的更改数量，使其与另一个时间序列完全相同，并使用此值定义距离度量。
- en: Distance Versus Similarity
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离与相似性
- en: The literature on measuring distances between time series also uses the term
    similarity to describe these metrics. In most cases, you can treat these terms
    interchangeably, namely as a way to establish which time series are more or less
    like one another. That said, some metrics will be proper distances, such as the
    Fréchet distance, which can be computed with proper units (such as “feet” or “kg/dollar”
    or whatever metric a time series is measuring). Other measures are unit-free,
    such as a correlation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有关测量时间序列之间距离的文献也使用相似性一词来描述这些度量。在大多数情况下，你可以将这些术语互换使用，即用来确定哪些时间序列更像或更不像彼此的一种方式。尽管如此，有些度量将是适当的距离，例如Fréchet距离，它可以用适当的单位（例如“英尺”或“kg/美元”或时间序列正在测量的任何度量单位）进行计算。其他度量则是无单位的，例如相关性。
- en: 'Sometimes a little creativity can go a long way toward finding a simple but
    apt solution, so it is always a good idea to consider exactly what your needs
    are and define them as specifically as possible. Consider a [Stack Overflow post](https://perma.cc/389W-68AH)
    seeking a distance metric for a specific application, namely classifying time
    series to match them to one of three centroids from a prior clustering analysis.
    The three classes were:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，一点创意可以为寻找简单但恰当的解决方案提供很大帮助，因此，考虑清楚你的需求并尽可能具体地定义它们总是个好主意。考虑一个[Stack Overflow帖子](https://perma.cc/389W-68AH)，寻找一种距离度量用于特定应用，即将时间序列分类以匹配之前聚类分析中的三个质心之一。这三个类别分别是：
- en: A flat line.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一条平直的线。
- en: A peak at the beginning of the time series, and otherwise a flat line.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列开始处有峰值，其他时间为平直线。
- en: A peak at the end of the time series, and otherwise a flat line. The user found
    that several standard distance metrics, including Euclidean distance and DTW,
    failed to do the trick. In this case, DTW was too generous and rated any time
    series with a peak as equally close to the time series with a peak at the end
    and a peak at the beginning (so DTW is not a panacea, despite being computationally
    taxing!).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列结束处有峰值，其他时间为平直线。用户发现包括欧几里得距离和DTW在内的几种标准距离度量方法未能达到预期效果。在这种情况下，DTW过于宽松，认为任何具有峰值的时间序列与开始处和结束处均有峰值的时间序列距离相等（因此，尽管计算复杂，DTW并非万能药！）。
- en: In this case, a clever commenter suggested a transform that would make distance
    metrics work better, namely to compare the cumulative summed time series rather
    than the original time series. After this transform, both the Euclidean and DTW
    distances gave the correct ordering such that a time series with a peak at the
    beginning showed the least distance to the prototype of that class, rather than
    an equal distance to the prototype with a peak at the beginning of the time series
    and the prototype with a peak at the end of the time series. This should remind
    us of earlier analyses we have studied, in which transforming a time series can
    make ARIMA appropriate even if the raw data does not meet the necessary conditions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一位聪明的评论者建议了一种转换，使距离度量工作得更好，即比较累积总和的时间序列而不是原始时间序列。在这种转换之后，无论是欧几里得距离还是DTW距离都能正确排序，使得时间序列在开始处有峰值的最短距离与该类别原型的距离最小，而不是与开始和结束处均有峰值的原型距离相等。这应该让我们想起我们之前学习过的分析，即通过转换时间序列可以使ARIMA模型适用，即使原始数据不满足必要条件。
- en: 'Unfortunately, there is no “autopilot” for choosing a distance metric. You
    will need to use your best judgment to find a balance of:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，选择距离度量的“自动驾驶”功能并不存在。你需要凭借自己的判断力来找到以下平衡：
- en: Minimizing the use of computational resources.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化计算资源的使用。
- en: Choosing a metric that emphasizes the features of a time series most relevant
    to your ultimate goal.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择强调时间序列特征与你最终目标最相关的度量。
- en: Making sure that your distance metric reflects the assumptions and strengths/weaknesses
    of the analytical methods you are pairing it with. For example, *k*-means clustering
    does not use pairwise distances but rather minimizes variances, such that only
    Euclidean-like distances make sense for this technique.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你的距离度量反映了你正在配对的分析方法的假设和优缺点。例如，*k*-均值聚类不使用成对距离，而是最小化方差，因此只有类似于欧几里得距离的技术才有意义。
- en: Clustering Code
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类代码
- en: Now that we have discussed how to generate features for clustering analysis
    and how to measure distance directly between time series as a distance metric
    for clustering, we will perform the clustering with our selected features and
    with our pairwise DTW distance matrix to compare the results.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何为聚类分析生成特征，以及如何直接在时间序列之间测量距离作为聚类的距离度量，我们将使用我们选择的特征和我们的配对DTW距离矩阵执行聚类，以比较结果。
- en: Hierarchical clustering of normalized features
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规范化特征的层次聚类
- en: 'We computed features for our words-as-time-series for both the time series
    of the original recording and the histogram of the time series. These features
    may occur on quite different scales, so if we want to apply a single distance
    metric to them, we normalize them as is standard operating procedure for feature-based
    clustering generally:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为我们的单词作为时间序列计算了特征，分别为原始记录的时间序列和时间序列的直方图。这些特征可能在完全不同的尺度上出现，因此如果我们想对它们应用单一的距离度量，我们会像特征聚类的标准操作程序一样对它们进行标准化：
- en: '[PRE18]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We select a hierarchical clustering algorithm and perform a fit for 50 clusters,
    since we are seeking to match these clusters to the 50 words in our data set:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了一种层次聚类算法，并对50个聚类进行了拟合，因为我们试图将这些聚类与我们数据集中的50个单词匹配：
- en: '[PRE19]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then we want to see whether the clusters (whose labels are arbitrary with respect
    to the original word labels) show useful correspondences to the word labels:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们想看看这些聚类（其标签与原始单词标签无关）是否显示出与单词标签有用的对应关系：
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We are lucky that we are working with labeled data; we might otherwise draw
    incorrect conclusions based on the clusters we formed. In this case fewer than
    half of the clusters relate strongly to a single word. If we go back and think
    about how to improve this result, we have a number of options:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很幸运地在处理标记数据，否则我们可能会根据我们形成的聚类得出错误的结论。在这种情况下，少于一半的聚类与单个词密切相关。如果我们回过头考虑如何改进这个结果，我们有几个选择：
- en: We only used six features. This is not a large number of features, so we could
    add more.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只使用了六个特征。这并不是很多特征，所以我们可以添加更多。
- en: We could look for features that would be relatively uncorrelated, which we did
    not do here.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以寻找相对不相关的特征，这一点我们在这里没有做。
- en: We are still missing obviously useful features. There were some features we
    noticed from the visual exploration of the data that we did not include, such
    as the number and location of distinctive peaks. We should probably rework this
    analysis to include that information or some better proxy for it.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仍然缺少显然有用的特征。从数据的视觉探索中我们注意到一些特征，但没有包括在内，比如显著峰值的数量和位置。我们可能需要重新设计这个分析，以包括该信息或一些更好的代理。
- en: We should explore using other distance metrics, perhaps some that will weight
    certain features more strongly than others, privileging the features the human
    eye finds useful.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该探索使用其他距离度量，也许是那些更加强调某些特征而不是其他特征的度量，优先考虑人眼认为有用的特征。
- en: Hierarchical clustering with the DTW distance matrix
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用DTW距离矩阵的层次聚类
- en: 'We have already completed the difficult portion of direct clustering based
    on time series clustering by calculating the pairwise distance matrix via DTW.
    This is computationally taxing, which is why we are careful to save the results
    in case we want to revisit the analysis:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了基于时间序列聚类的直接聚类的困难部分，通过计算通过DTW的配对距离矩阵。这是计算上非常耗费资源的，这就是为什么我们小心保存结果以便在需要时重新审视分析的原因：
- en: '[PRE21]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that we have them, we can use a hierarchical clustering algorithm:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了它们，我们可以使用层次聚类算法：
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And finally, as before, we compare the correspondence between the fitted clusters
    and the known labels:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与以往一样，我们比较拟合的聚类与已知标签之间的对应关系：
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We see that this DTW-based clustering does substantially better than our feature-based
    clustering. However, if you run the DTW distance computation code on your own
    computer—particularly if it’s a standard-issue laptop—you will see just how much
    longer the DTW takes to compute compared to the features we chose. It’s likely
    that we can improve our feature-based clustering, whereas the DTW distance clustering,
    now that it’s computed, does not leave any straightforward means for improvement.
    Our alternatives to improving this would be:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到基于DTW的这种聚类比我们基于特征的聚类要好得多。然而，如果你在自己的计算机上运行DTW距离计算代码——特别是如果它是一台标准笔记本电脑——你将看到DTW需要比我们选择的特征计算更长的时间。我们很可能可以改进我们基于特征的聚类，而现在计算出的DTW距离聚类则没有明显的改进途径。我们改进这一点的替代方案可能是：
- en: Include features as well as the DTW distance. This is tricky both from a coding
    perspective and also from the conceptual perspective of deciding how to combine
    the features with the DTW distance.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括特征以及DTW距离。从编码的角度来看，这很棘手，从概念的角度来看，也很难决定如何将特征与DTW距离结合起来。
- en: Try other distance metrics. As discussed earlier, the appropriate distance metric
    will depend on your data, your goals, and your downstream analysis. We would need
    to define our goal for this word analysis more narrowly and geometrically, and
    then we could think about whether DTW is really the best metric for what we want
    to accomplish.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试其他距离度量。正如前面讨论的那样，适当的距离度量将取决于您的数据、您的目标和您的下游分析。我们需要更明确地定义我们对这个词分析的目标，并几何地思考一下DTW是否真的是我们想要实现的最佳度量。
- en: More Resources
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多资源
- en: 'On time series distance and similarity measures:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于时间序列距离和相似度测量：
- en: 'Meinard Müller, [“Dynamic Time Warping,”](https://perma.cc/R24Q-UR84) in *Information
    Retrieval* for Music and Motion (Berlin: Springer, 2007), 69–84, https://perma.cc/R24Q-UR84.'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Meinard Müller，[“动态时间规整”，](https://perma.cc/R24Q-UR84) 收录于《信息检索》中，用于音乐和动作（柏林：斯普林格，2007年），69–84，https://perma.cc/R24Q-UR84。
- en: This chapter of Müller’s book offers an extensive overview of dynamic time warping,
    including a discussion of common approximations made to reduce the computational
    complexity of computing DTW.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Müller书中的这一章节提供了动态时间规整的广泛概述，包括讨论为减少计算DTW的计算复杂度所做的常见近似。
- en: Stéphane Pelletier, [“Computing the Fréchet Distance Between Two Polygonal Curves,”](https://perma.cc/5QER-Z89V)
    (lecture notes, Computational Geometry, McGill University, 2002), https://perma.cc/5QER-Z89V.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Stéphane Pelletier，[“计算两个多边形曲线之间的弗雷歇距离”，](https://perma.cc/5QER-Z89V)（讲座笔记，计算几何，麦吉尔大学，2002年），https://perma.cc/5QER-Z89V。
- en: This set of lecture notes from McGill University offers an intuitive visual
    and algorithmic explanation of what the Fréchet distance is and how it can be
    calculated.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 麦吉尔大学的这套讲座笔记提供了弗雷歇距离是什么以及如何计算的直观视觉和算法解释。
- en: Pjotr Roelofsen, [“Time Series Clustering,”](https://perma.cc/K8HJ-7FFE) master’s
    thesis, Business Analytics, Vrije Universiteit Amsterdam, 2018, https://perma.cc/K8HJ-7FFE.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pjotr Roelofsen，[“时间序列聚类”，](https://perma.cc/K8HJ-7FFE) 硕士论文，商业分析，阿姆斯特丹自由大学，2018年，https://perma.cc/K8HJ-7FFE。
- en: This master’s thesis on time series clustering begins with a helpful and very
    thorough discussion of the mainstream techniques for calculating distances between
    time series, including information about the computational complexity of the distance
    calculation and helpful illustrations that assist with building intuition.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这篇关于时间序列聚类的硕士论文，从详尽且有用的讨论开始，介绍了计算时间序列之间距离的主流技术，包括距离计算的计算复杂度信息，以及帮助建立直觉的有用示例。
- en: 'Joan Serrà and Josep Ll. Arcos, [“An Empirical Evaluation of Similarity Measures
    for Time Series Classification,”](https://perma.cc/G2J4-TNMX) *Knowledge-Based
    Systems* 67 (2014): 305–14, https://perma.cc/G2J4-TNMX.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Joan Serrà 和 Josep Ll. Arcos，[“时间序列分类相似度测量的经验评估”，](https://perma.cc/G2J4-TNMX)
    《基于知识的系统》67卷（2014年）：305–14，https://perma.cc/G2J4-TNMX。
- en: 'This article offers empirical analysis of out-of-sample testing accuracy for
    classification models built with seven different measures of time series similarity:
    Euclidean distance, Fourier coefficients, AR models, DTW, edit distance, time-warped
    edit distance, and minimum jump costs dissimilarity. The authors tested these
    measures on 45 publicly available data sets from the UCR time series repository.'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文对使用七种不同的时间序列相似度测量方法构建分类模型的外样本测试精度进行了经验分析：欧氏距离、傅里叶系数、AR模型、DTW、编辑距离、时间扭曲编辑距离和最小跳跃成本不相似度。作者在UCR时间序列库的45个公开数据集上测试了这些测量方法。
- en: 'On machine learning for time series:'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于时间序列的机器学习：
- en: Keogh Eamonn, [“Introduction to Time Series Data Mining,”](https://perma.cc/ZM9L-NW7J)
    slideshow tutorial, n.d., https://perma.cc/ZM9L-NW7J.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Keogh Eamonn，[“时间序列数据挖掘简介”，](https://perma.cc/ZM9L-NW7J) 幻灯片教程，无日期，https://perma.cc/ZM9L-NW7J。
- en: This series of slides gives an overview to preprocessing time series data for
    machine learning purposes, measuring distance between time series, and identifying
    “motifs” that can be used for analysis and comparison.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这组幻灯片概述了预处理时间序列数据以进行机器学习，测量时间序列之间的距离，并识别可用于分析和比较的“主题”。
- en: 'Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos, [“The
    M4 Competition: Results, Findings, Conclusion and Way Forward,”](https://perma.cc/42HZ-YVUU)
    *International Journal of Forecasting* 34, no. 4 (2018): 802–8, https://perma.cc/42HZ-YVUU.'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Spyros Makridakis, Evangelos Spiliotis, 和 Vassilios Assimakopoulos, [“M4竞赛：结果、发现、结论与未来方向,”](https://perma.cc/42HZ-YVUU)
    *国际预测学杂志* 34卷, 第4期 (2018): 802–8, https://perma.cc/42HZ-YVUU.'
- en: This article summarizes results of the M4 competition in 2018, which compared
    a variety of time series forecasting techniques, including many ensemble techniques,
    on a randomly selected set of 100,000 time series, including data collected at
    various frequencies (yearly, hourly, etc.). In this overview of the competition
    results, the authors indicate that a few “hybrid” approaches, relying heavily
    on statistics but also with some machine learning components, took both first
    and second place in the competition. Results such as these point to the importance
    of understanding and deploying both statistical and machine learning approaches
    to forecasting.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文总结了2018年M4竞赛的结果，该竞赛比较了多种时间序列预测技术，包括许多集成技术，应用于随机选择的10万个时间序列数据，包括以不同频率收集的数据（年度、每小时等）。在竞赛结果的概述中，作者指出少数“混合”方法，在统计学上严重依赖但也包含一些机器学习组件，夺得了竞赛的头两名。这些结果表明了理解和应用统计学和机器学习方法在预测中的重要性。
- en: ^([1](ch09.html#idm45576031287336-marker)) Note that information about the actual
    content of each “word” is not available and was not especially relevant when the
    original data set was put together. The idea was to recognize that all words of
    one label were the same so as to cut down on the human effort needed to label
    documents.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#idm45576031287336-marker)) 注意，关于每个“单词”实际内容的信息是不可用的，并且当原始数据集编制时并不特别重要。其想法是认识到同一标签的所有单词都是相同的，以减少标记文档所需的人力工作。
- en: ^([2](ch09.html#idm45576031005608-marker)) Just like the word projections themselves,
    for which the x-axis is not actually time but another ordered, evenly spaced axis
    that may as well be time, the same is true for the histograms. We can think of
    their x-axis as time for purposes of our analysis, such as generating features.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.html#idm45576031005608-marker)) 就像单词预测本身一样，其中x轴实际上不是时间，而是另一个有序、均匀间隔的轴，可以看作是时间，直方图也是如此。在我们的分析中，可以将它们的x轴视为时间，以生成特征等目的。
