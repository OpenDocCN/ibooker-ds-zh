- en: Part 3\. Greater than the sum of its parts
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三部分. 集体大于部分之和
- en: At this point, you’ve learned a number of AI techniques that draw from classical
    tree search, machine learning, and reinforcement learning. Each is powerful on
    its own, but each also has limitations. To make a truly strong Go AI, you’ll need
    to combine everything you’ve learned so far. Integrating all these pieces is a
    serious engineering feat. This part covers the architecture of AlphaGo, the AI
    that rocked the Go world—and the AI world! To conclude the book, you’ll learn
    about the elegantly simple design of AlphaGo Zero, the strongest version of AlphaGo
    to date.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你已经学习了许多从经典树搜索、机器学习和强化学习中汲取的AI技术。每一种技术本身都很强大，但每一种也都有局限性。要打造一个真正强大的围棋AI，你需要将迄今为止所学的一切结合起来。整合所有这些元素是一项重大的工程壮举。本部分将介绍AlphaGo的架构，这是震撼围棋世界——以及AI世界的AI！为了结束本书，你将了解AlphaGo
    Zero的优雅简单设计，这是迄今为止AlphaGo的最强版本。
- en: 'Chapter 13\. AlphaGo: Bringing it all together'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第13章. AlphaGo：集大成之作
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Diving into the guiding principles that led Go bots to play at superhuman strength
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入研究引导围棋机器人以超人类水平进行游戏的指导原则
- en: Using tree search, supervised deep learning, and reinforcement learning to build
    such a bot
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用树搜索、监督深度学习和强化学习来构建这样的机器人
- en: Implementing your own version of DeepMind’s AlphaGo engine
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现DeepMind的AlphaGo引擎的自己的版本
- en: When DeepMind’s Go bot AlphaGo played move 37 of game 2 against Lee Sedol in
    2016, it took the Go world by storm. Commentator Michael Redmond, a professional
    player with nearly a thousand top-level games under his belt, did a double-take
    on air; he even briefly removed the stone from the demo board while looking around
    as if to confirm that AlphaGo made the right move. (“I still don’t really understand
    the mechanics of it,” Redmond told the American Go E-Journal the next day.) Lee,
    the world-wide dominant player of the past decade, spent 12 minutes studying the
    board before responding. [Figure 13.1](#ch13fig01) displays the legendary move.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当DeepMind的围棋机器人AlphaGo在2016年与李世石进行第二局比赛的第37步时，它引起了围棋世界的轰动。解说员迈克尔·雷德蒙德，一个拥有近千场顶级比赛的职业选手，在直播中惊讶地停顿了一下；他甚至短暂地从演示板上取下了棋子，四处张望，好像要确认AlphaGo走的是正确的棋。雷德蒙德第二天告诉美国的围棋电子杂志：“我仍然不太理解它的机制。”过去十年间全球主导的选手李世石在回应之前花费了12分钟研究棋盘。[图13.1](#ch13fig01)展示了这一传奇般的走法。
- en: Figure 13.1\. The legendary shoulder hit that AlphaGo played against Lee Sedol
    in the second game of their series. This move stunned many professional players.
  id: totrans-8
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.1\. AlphaGo在与李世石系列赛的第二局中对李世石进行的传奇肩击。这一走法让许多职业选手感到震惊。
- en: '![](Images/13fig01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/13fig01.jpg)'
- en: 'The move defied conventional Go theory. The diagonal approach, or *shoulder
    hit*, is an invitation for the white stone to extend along the side and make a
    solid wall. If the white stone is on the third line and the black stone is on
    the fourth line, this is considered a roughly even exchange: white gets points
    on the side, while black gets influence toward the center. But when the white
    stone is on the fourth line, the wall locks up too much territory. (To any strong
    Go players who are reading, we apologize for drastically oversimplifying this.)
    A fifth-line shoulder hit looks a little amateurish—or at least it did until “Professor
    Alpha” took four out of five games against a legend. The shoulder hit was the
    first of many surprises from AlphaGo. Fast-forward a year, and everyone from top
    pros to casual club players is experimenting with AlphaGo moves.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步违反了传统的围棋理论。对角线方法，或称*肩击*，是邀请白子沿着边延伸并形成坚固的墙壁。如果白子位于第三线而黑子位于第四线，这被认为是一种大致均等的交换：白子获得边上的分数，而黑子获得向中心的势力。但是当白子位于第四线时，墙壁锁定了过多的领地。（对于正在阅读的任何强大的围棋选手，我们为过于简化这一过程表示歉意。）第五线的肩击看起来有点业余——至少在“Alpha教授”在与传奇人物的四局比赛中赢得四局之前是这样的。肩击是AlphaGo带来的许多惊喜中的第一个。快进一年，从顶级职业选手到业余俱乐部选手，每个人都开始尝试AlphaGo的走法。
- en: 'In this chapter, you’re going to learn how AlphaGo works by implementing all
    of its building blocks. AlphaGo is a clever combination of supervised deep learning
    from professional Go records (which you learned about in [chapters 5](kindle_split_017.xhtml#ch05)–[8](kindle_split_020.xhtml#ch08)),
    deep reinforcement learning with self-play (covered in [chapters 9](kindle_split_021.xhtml#ch09)–[12](kindle_split_024.xhtml#ch12)),
    and using these deep networks to improve tree search in a novel way. You might
    be surprised by how much you already know about the ingredients of AlphaGo. To
    be more precise, the AlphaGo system we’ll be describing in detail works as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将通过实现所有构建块来学习AlphaGo是如何工作的。AlphaGo是监督深度学习（你已经在 [第5章](kindle_split_017.xhtml#ch05)–[第8章](kindle_split_020.xhtml#ch08)
    中了解过）与自我对弈的深度强化学习（在第 [第9章](kindle_split_021.xhtml#ch09)–[第12章](kindle_split_024.xhtml#ch12)
    中介绍）的巧妙结合，以及以新颖的方式使用这些深度网络来改进树搜索。你可能会惊讶于你对AlphaGo成分的了解程度。更准确地说，我们将详细描述的AlphaGo系统工作如下：
- en: You start off by training *two* deep convolutional neural networks (*policy
    networks*) for move prediction. One of these network architectures is a bit deeper
    and *produces more-accurate results*, whereas the other one is smaller and *faster
    to evaluate*. We’ll call them the *strong* and *fast* policy networks, respectively.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你首先训练 *两个* 深度卷积神经网络（*策略网络*）来进行移动预测。其中一个网络架构稍微深一些，*产生更准确的结果*，而另一个则更小，*评估更快*。我们将分别称它们为
    *强大* 和 *快速* 策略网络。
- en: The strong and fast policy networks use a slightly more sophisticated board
    encoder with 48 feature planes. They also use a deeper architecture than what
    you’ve seen in [chapters 6](kindle_split_018.xhtml#ch06) and [7](kindle_split_019.xhtml#ch07),
    but other than that, they should look familiar. [Section 13.1](#ch13lev1sec1)
    covers AlphaGo’s policy network architectures.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强大且快速的策略网络使用一个稍微复杂一些的棋盘编码器，具有48个特征平面。它们还使用比你在 [第6章](kindle_split_018.xhtml#ch06)
    和 [第7章](kindle_split_019.xhtml#ch07) 中看到的更深的架构，但除此之外，它们应该看起来很熟悉。[第13.1节](#ch13lev1sec1)
    介绍了AlphaGo的策略网络架构。
- en: After the first training step of policy networks is complete, you take the strong
    policy network as a starting point for self-play in [section 13.2](#ch13lev1sec2).
    If you do this with a lot of compute power, this will result in a massive improvement
    of your bots.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在完成策略网络的第一次训练步骤后，你将强大的策略网络作为自我对弈的起点，这在 [第13.2节](#ch13lev1sec2) 中有说明。如果你使用大量的计算能力来做这件事，这将导致你的机器人有巨大的改进。
- en: As a next step, you take the strong self-play network to derive a *value network*
    from it in [section 13.3](#ch13lev1sec3). This completes the network training
    stage, and you don’t do any deep learning after this point.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为下一步，你从强大的自我对弈网络中导出一个 *价值网络*，这在 [第13.3节](#ch13lev1sec3) 中有详细说明。这完成了网络训练阶段，之后你不再进行任何深度学习。
- en: To play a game of Go, you use tree search as a basis for play, but instead of
    plain Monte Carlo rollouts as in [chapter 4](kindle_split_016.xhtml#ch04), you
    use the fast policy network to guide the next steps. Also, you balance the output
    of this tree-search algorithm with what your value function tells you. We’ll tell
    you all about this innovation in [section 13.4](#ch13lev1sec4).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要玩围棋，你使用树搜索作为游戏的基础，但与 [第4章](kindle_split_016.xhtml#ch04) 中的纯蒙特卡洛滚动不同，你使用快速策略网络来指导下一步。同时，你平衡这个树搜索算法的输出与你的价值函数告诉你的信息。我们将在
    [第13.4节](#ch13lev1sec4) 中详细介绍这一创新。
- en: Performing this whole process from training policies, to self-play, to running
    games with search on a superhuman level requires massive compute resources and
    time. [Section 13.5](#ch13lev1sec5) gives you some ideas on what it took to make
    AlphaGo as strong as it is and what to expect from your own experiments.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练策略到自我对弈，再到在超人类水平上运行带有搜索的游戏，整个过程需要大量的计算资源和时间。[第13.5节](#ch13lev1sec5) 会给你一些关于AlphaGo如何变得如此强大以及你可以从自己的实验中期待什么的想法。
- en: '[Figure 13.2](#ch13fig02) gives an overview of the whole process we just sketched.
    Throughout the chapter, we’ll zoom into parts of this diagram and provide you
    with more details in the respective sections.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图13.2](#ch13fig02) 给出了我们刚刚概述的整个过程的概览。在整个章节中，我们将深入探讨这个图的部分，并在相应的章节中提供更多细节。'
- en: 'Figure 13.2\. How to train the three neural networks that power the AlphaGo
    AI. Starting with a collection of human game records, you can train two neural
    networks to predict the next move: a small, fast network and a large, strong network.
    You can then further improve the playing strength of the large network through
    reinforcement learning. The self-play games also provide data to train a value
    network. AlphaGo then uses all three networks in a tree-search algorithm that
    can produce incredibly strong game play.'
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 13.2\. 如何训练驱动 AlphaGo 人工智能的三个神经网络。从一组人类游戏记录开始，你可以训练两个神经网络来预测下一步棋：一个小巧快速的神经网络和一个大而强大的神经网络。然后，你可以通过强化学习进一步改进大网络的竞技强度。自我对弈游戏也提供了数据来训练一个价值网络。AlphaGo
    然后使用一个树搜索算法，该算法可以产生极其强大的游戏表现。
- en: '![](Images/13fig02_alt.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 的替代文本](Images/13fig02_alt.jpg)'
- en: 13.1\. Training deep neural networks for AlphaGo
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1\. 为 AlphaGo 训练深度神经网络
- en: 'In the introduction, you learned that AlphaGo uses three neural networks: two
    policy networks and one value network. Although this may seem like a lot at first,
    in this section, you’ll see that these networks and the input features that feed
    into them are conceptually close to each other. Perhaps the most surprising part
    about deep learning as used in AlphaGo is how much you already know about it after
    completing [chapters 5](kindle_split_017.xhtml#ch05) to [12](kindle_split_024.xhtml#ch12).
    Before we go into details of how these neural networks are built and trained,
    let’s quickly discuss their role in the AlphaGo system:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍中，你了解到 AlphaGo 使用三个神经网络：两个策略网络和一个价值网络。虽然一开始这可能看起来很多，但在本节中，你会发现这些网络以及输入到它们中的特征在概念上彼此非常接近。AlphaGo
    中使用的深度学习最令人惊讶的部分可能就是，在完成第 5 章 [chapters 5](kindle_split_017.xhtml#ch05) 到第 12
    章 [chapters 12](kindle_split_024.xhtml#ch12) 后，你已经对其了解了很多。在我们深入探讨这些神经网络是如何构建和训练的细节之前，让我们快速讨论它们在
    AlphaGo 系统中的作用：
- en: '***Fast policy network*—** This Go move-prediction network is comparable in
    size to the networks you trained in [chapters 7](kindle_split_019.xhtml#ch07)
    and [8](kindle_split_020.xhtml#ch08). Its purpose isn’t to be the most accurate
    move predictor, but rather a good predictor that’s really fast at predicting moves.
    This network is used in [section 13.4](#ch13lev1sec4) in tree-search rollouts—and
    you’ve seen in [chapter 4](kindle_split_016.xhtml#ch04) that you need to create
    a lot of them quickly for tree search to become an option. We’ll put a little
    less emphasis on this network and focus on the following two.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***快速策略网络*—** 这个围棋走棋预测网络的大小与你在第 7 章 [chapters 7](kindle_split_019.xhtml#ch07)
    和第 8 章 [chapters 8](kindle_split_020.xhtml#ch08) 中训练的网络相当。它的目的不是成为最准确的走棋预测器，而是一个真正快速预测走棋的好预测器。这个网络在
    [第 13.4 节](#ch13lev1sec4) 的树搜索展开中使用——你在第 4 章 [chapters 4](kindle_split_016.xhtml#ch04)
    中看到，你需要快速创建很多这样的网络，以便树搜索成为可能。我们将对这个网络稍微减少关注，并专注于以下两个。'
- en: '***Strong policy network*—** This move-prediction network is optimized for
    accuracy, not speed. It’s a convolutional network that’s deeper than its fast
    version and can be more than twice as good at predicting Go moves. As the fast
    version, this network is trained on human game-play data, as you did in [chapter
    7](kindle_split_019.xhtml#ch07). After this training step is completed, the strong
    policy network is used as a starting point for self-play by using reinforcement-learning
    techniques from [chapters 9](kindle_split_021.xhtml#ch09) and [10](kindle_split_022.xhtml#ch10).
    This step will make this policy network even stronger.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***强大策略网络*—** 这个走棋预测网络优化的是准确性，而不是速度。它是一个比快速版本更深层的卷积网络，在预测围棋走棋方面可以比快速版本好两倍以上。与快速版本一样，这个网络是在人类游戏数据上训练的，就像你在第
    7 章 [chapters 7](kindle_split_019.xhtml#ch07) 中做的那样。完成这个训练步骤后，强大策略网络将作为自我对弈的起点，使用第
    9 章 [chapters 9](kindle_split_021.xhtml#ch09) 和第 10 章 [chapters 10](kindle_split_022.xhtml#ch10)
    中的强化学习技术。这一步将使这个策略网络变得更强大。'
- en: '***Value network*—** The self-play games played by the strong policy network
    generate a new data set that you can use to train a value network. Specifically,
    you use the outcome of these games and the techniques from [chapters 11](kindle_split_023.xhtml#ch11)
    and [12](kindle_split_024.xhtml#ch12) to learn a value function. This value network
    will then play an integral role in [section 13.4](#ch13lev1sec4).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***价值网络*—** 强大策略网络进行的自我对弈游戏产生了一个新的数据集，你可以用它来训练一个价值网络。具体来说，你使用这些游戏的结局和第 11 章
    [chapters 11](kindle_split_023.xhtml#ch11) 和第 12 章 [chapters 12](kindle_split_024.xhtml#ch12)
    中的技术来学习一个价值函数。这个价值网络将在 [第 13.4 节](#ch13lev1sec4) 中扮演一个核心角色。'
- en: 13.1.1\. Network architectures in AlphaGo
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.1\. AlphaGo 中的网络架构
- en: Now that you roughly know what each of the three deep neural networks is used
    for in AlphaGo, we can show you how to build these networks in Python, using Keras.
    Here’s a quick description of the network architectures, before we show you the
    code. If you need a refresher on terminology for convolutional networks, have
    a look at [chapter 7](kindle_split_019.xhtml#ch07) again.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您大致了解了 AlphaGo 中三个深度神经网络各自的作用，我们可以向您展示如何使用 Keras 在 Python 中构建这些网络。在我们展示代码之前，这里是对网络架构的简要描述。如果您需要关于卷积网络的术语复习，请再次查看[第
    7 章](kindle_split_019.xhtml#ch07)。
- en: The strong policy network is a 13-layer convolutional network. All of these
    layers produce 19 × 19 filters; you consistently keep the original board size
    across the whole network. For this to work, you need to *pad* the inputs accordingly,
    as you did in [chapter 7](kindle_split_019.xhtml#ch07). The first convolutional
    layer has a kernel size of 5, and all following layers work with a kernel size
    of 3\. The last layer uses softmax activations and has one output filter, and
    the first 12 layers use ReLU activations and have 192 output filters each.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强策略网络是一个 13 层的卷积网络。所有这些层都产生 19 × 19 的滤波器；您在整个网络中一致地保持原始棋盘大小。为了实现这一点，您需要相应地填充输入，就像在[第
    7 章](kindle_split_019.xhtml#ch07)中所做的那样。第一层卷积的核大小为 5，所有后续层都使用 3 核大小。最后一层使用 softmax
    激活，有一个输出滤波器，前 12 层使用 ReLU 激活，每个有 192 个输出滤波器。
- en: The value network is a 16-layer convolutional network, the first 12 of which
    are *exactly the same as the strong policy network*. Layer 13 is an additional
    convolutional layer, structurally identical to layers 2–12\. Layer 14 is a convolutional
    layer with kernel size 1 and one output filter. The network is topped off with
    two dense layers, one with 256 outputs and ReLU activations, and a final one with
    one output and tanh activation.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值网络是一个 16 层的卷积网络，其中前 12 层与强策略网络完全相同。第 13 层是一个额外的卷积层，结构与第 2-12 层相同。第 14 层是一个核大小为
    1 的卷积层，有一个输出滤波器。网络顶部有两个密集层，一个有 256 个输出和 ReLU 激活，另一个有一个输出和 tanh 激活。
- en: As you can see, both policy and value networks in AlphaGo are the same kind
    of deep convolutional neural network that you already encountered in [chapter
    6](kindle_split_018.xhtml#ch06). The fact that these two networks are so similar
    allows you to define them in a single Python function. Before doing so, we introduce
    a little shortcut in Keras that shortens the network definition quite a bit. Recall
    from [chapter 7](kindle_split_019.xhtml#ch07) that you can pad input images in
    Keras with the `ZeroPadding2D` utility layer. It’s perfectly fine to do so, but
    you can save some ink in your model definition by moving the padding into the
    `Conv2D` layer. What you want to do in both value and policy networks is to pad
    the input to each convolutional layer so that the output filters have the *same*
    size as the input (19 × 19). For instance, instead of explicitly padding the 19
    × 19 input of the first layer to 23 × 23 images so that the following convolutional
    layer with kernel size 5 produces 19 × 19 output filters, you tell the convolutional
    layer to retain the input size. You do this by providing the argument `padding='same'`
    to your convolutional layer, which will take care of the padding for you. With
    this neat shortcut in mind, let’s define the first 11 layers that AlphaGo’s policy
    and value networks have in common. You find this definition in our GitHub repository
    in alphago.py in the dlgo.networks module.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，AlphaGo 中的策略网络和价值网络与您在[第 6 章](kindle_split_018.xhtml#ch06)中遇到的相同类型的深度卷积神经网络。这两个网络如此相似的事实使得您可以用单个
    Python 函数来定义它们。在这样做之前，我们介绍一个 Keras 中的小技巧，它可以大大缩短网络定义的长度。回想一下[第 7 章](kindle_split_019.xhtml#ch07)，您可以使用
    `ZeroPadding2D` 实用层在 Keras 中填充输入图像。这样做是可以的，但您可以通过将填充移动到 `Conv2D` 层来节省一些模型定义中的墨水。在价值和策略网络中，您想要对每个卷积层的输入进行填充，以便输出滤波器的大小与输入相同（19
    × 19）。例如，您不需要明确地将第一层的 19 × 19 输入填充到 23 × 23 图像，以便后续的 5 核大小的卷积层产生 19 × 19 的输出滤波器，而是告诉卷积层保留输入大小。您通过向卷积层提供
    `padding='same'` 参数来实现这一点，这将为您处理填充。有了这个巧妙的快捷方式，让我们定义 AlphaGo 的策略网络和价值网络共有的前 11
    层。您可以在我们的 GitHub 仓库中找到这个定义，在 alphago.py 的 dlgo.networks 模块中。
- en: Listing 13.1\. Initializing a neural network for both policy and value networks
    in AlphaGo
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.1\. 在 AlphaGo 中初始化策略网络和价值网络的神经网络
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* With this Boolean flag, you specify whether you want a policy or value
    network.**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用这个布尔标志，您指定您想要策略网络还是价值网络。**'
- en: '***2* All but the last convolutional layers have the same number of filters.**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 除了最后的卷积层外，所有层的过滤器数量相同。**'
- en: '***3* The first layer has kernel size 5, all others only 3.**'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 第一层的核大小为5，所有其他层只有3。**'
- en: '***4* The first 12 layers of AlphaGo’s policy and value network are identical.**'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* AlphaGo的策略网络和价值网络的前12层是相同的。**'
- en: Note that you didn’t yet specify the input shape of the first layer. That’s
    because that shape differs slightly for policy and value networks. You’ll see
    the difference when we introduce the AlphaGo board encoder in the next section.
    To continue the definition of `model`, you’re just one final convolutional layer
    away from defining the strong policy network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你还没有指定第一层的输入形状。这是因为策略网络和价值网络的形状略有不同。当你我们在下一节介绍AlphaGo棋盘编码器时，你会看到这种差异。为了继续定义`model`，你只需要一个最终的卷积层就可以定义强大的策略网络。
- en: Listing 13.2\. Creating AlphaGo’s strong policy network in Keras
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.2\. 在Keras中创建AlphaGo的强大策略网络
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, you add a final `Flatten` layer to flatten the predictions and
    ensure consistency with your previous model definitions from [chapters 5](kindle_split_017.xhtml#ch05)
    to [8](kindle_split_020.xhtml#ch08).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，你添加了一个最终的`Flatten`层来展平预测并确保与第5章到第8章中之前的模型定义的一致性。
- en: If you want to return AlphaGo’s value network instead, adding two more `Conv2D`
    layers, two `Dense` layers, and one `Flatten` layer to connect them will do the
    job.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要返回AlphaGo的价值网络，添加两个额外的`Conv2D`层、两个`Dense`层和一个`Flatten`层来连接它们即可。
- en: Listing 13.3\. Building AlphaGo’s value network in Keras
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.3\. 在Keras中构建AlphaGo的价值网络
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We don’t explicitly discuss the architecture of the fast policy network here;
    the definition of input features and network architecture of the fast policy is
    technically involved and doesn’t contribute to a deeper understanding of the AlphaGo
    system. For your own experiments, it’s perfectly fine to use one of the networks
    from our dlgo.networks module, such as `small`, `medium`, or `large`. The main
    idea for the fast policy is to have a smaller network than the strong policy that’s
    quick to evaluate. We’ll guide you through the training process in more detail
    throughout the next sections.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里没有明确讨论快速策略网络的架构；快速策略的输入特征和网络架构的定义在技术上比较复杂，并不有助于更深入地理解AlphaGo系统。对于你自己的实验，使用我们dlgo.networks模块中的一个网络，如`small`、`medium`或`large`，是完全可行的。快速策略的主要思想是拥有一个比强大策略更小的网络，可以快速评估。我们将在接下来的章节中更详细地指导你进行训练过程。
- en: 13.1.2\. The AlphaGo board encoder
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.2\. AlphaGo棋盘编码器
- en: Now that you know all about the network architectures used in AlphaGo, let’s
    discuss how to encode Go board data the AlphaGo way. You’ve implemented quite
    a few board encoders in [chapters 6](kindle_split_018.xhtml#ch06) and [7](kindle_split_019.xhtml#ch07)
    already, including `oneplane`, `sevenplane`, or `simple`, all of which you stored
    in the dlgo.encoders module. The feature planes used in AlphaGo are just a little
    more sophisticated than what you’ve encountered before, but represent a natural
    continuation of the encoders shown so far.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了AlphaGo使用的网络架构，让我们来讨论如何以AlphaGo的方式编码围棋棋盘数据。你已经在第6章和第7章中实现了相当多的棋盘编码器，包括`oneplane`、`sevenplane`或`simple`，所有这些你都已经存储在dlgo.encoders模块中。AlphaGo使用的特征平面比之前遇到的情况稍微复杂一些，但代表了迄今为止显示的编码器的自然延续。
- en: The AlphaGo board encoder for policy networks has 48 feature planes; for value
    networks, you augment these features with one additional plane. These 48 planes
    are made up of 11 concepts, some of which you’ve used before and others that are
    new. We’ll discuss each of them in more detail. In general, AlphaGo makes a bit
    more use of Go-specific tactical situations than the board encoder examples we’ve
    discussed so far. A prime example of this is making the concept of *ladder captures
    and escapes* (see [figure 13.3](#ch13fig03)) part of the feature set.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo用于策略网络的棋盘编码器有48个特征平面；对于价值网络，你增加一个额外的平面来增强这些特征。这48个平面由11个概念组成，其中一些你之前已经使用过，而其他的是新的。我们将更详细地讨论每一个。总的来说，AlphaGo比我们之前讨论的棋盘编码器例子更多地使用了围棋特定的战术情况。一个典型的例子是将*梯形捕捉和逃脱*的概念（见[图13.3](#ch13fig03)）纳入特征集。
- en: Figure 13.3\. AlphaGo encoded many Go tactical concepts directly into its feature
    planes, including *ladders*. In the first example, a white stone has just one
    liberty—meaning black could capture on the next turn. The white player extends
    the white stone to gain an extra liberty. But black can again reduce the white
    stones to one liberty. This sequence continues until it hits the edge of the board,
    where white is captured. On the other hand, if there’s a white stone in the path
    of the ladder, white may be able to escape capture. AlphaGo included feature planes
    that indicated whether a ladder would be successful.
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.3\. AlphaGo直接将许多围棋战术概念编码到其特征平面中，包括*梯子*。在第一个例子中，一个白子只有一个自由度——这意味着黑子在下一次回合可以吃掉它。白子玩家延伸白子以获得额外的自由度。但黑子可以再次将白子减少到只有一个自由度。这个序列一直持续到触及棋盘边缘，此时白子被吃掉。另一方面，如果梯子路径上有白子，白子可能能够逃脱被吃掉。AlphaGo包含了表示梯子是否成功的特征平面。
- en: '![](Images/13fig03_alt.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/13fig03_alt.jpg)'
- en: A technique you consistently used in all of your Go board encoders that’s also
    present in AlphaGo is the use of *binary features*. For instance, when capturing
    liberties (empty adjacent points on the board), you didn’t just use one feature
    plane with liberty counts for each stone on the board, but chose a binary representation
    with planes indicating whether a stone had 1, 2, 3, or more liberties. In AlphaGo,
    you see the exact same idea, but with eight feature planes to binarize counts.
    In the example of liberties, that means eight planes to indicate 1, 2, 3, 4, 5,
    6, 7, or at least 8 liberties for a stone.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你在所有围棋棋盘编码器中一直使用的技术，AlphaGo也有，那就是使用*二进制特征*。例如，在吃自由度（棋盘上的空相邻点）时，你不仅仅使用一个特征平面来为棋盘上的每个石头计算自由度，而是选择了一个二进制表示，其中平面表示石头是否有1、2、3个或更多自由度。在AlphaGo中，你看到的是完全相同的概念，但使用了八个特征平面来二进制化计数。在自由度的例子中，这意味着有八个平面来表示1、2、3、4、5、6、7个或至少8个自由度。
- en: The only fundamental difference from what you’ve seen in [chapters 6](kindle_split_018.xhtml#ch06)
    to [8](kindle_split_020.xhtml#ch08) is that AlphaGo encodes stone color explicitly
    in *separate* feature planes. Recall that in the `sevenplane` encoder from [chapter
    7](kindle_split_019.xhtml#ch07), you had liberty planes for both black and white
    stones. In AlphaGo, you have only one set of features counting liberties. Additionally,
    all features are expressed in terms of the player to play next. For instance,
    the feature set Capture Size, counting the number of stones that would be captured
    by a move, counts the stones the *current* player would capture, whatever stone
    color this might be.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与你在第6章到第8章中看到的内容相比，唯一的根本区别是AlphaGo在*单独*的特征平面中明确编码了石头颜色。回想一下，在第7章的`sevenplane`编码器中，你既有黑子的自由度平面，也有白子的自由度平面。在AlphaGo中，你只有一套计算自由度的特征。此外，所有特征都是以下一个玩家的角度来表达的。例如，计算吃子大小的特征集，即计算一个移动会吃掉多少子，它计算的是*当前*玩家会吃掉的子，无论是什么颜色。
- en: '[Table 13.1](#ch13table01) summarizes all the features used in AlphaGo. The
    first 48 planes are used for policy networks, and the last one only for value
    networks.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[表13.1](#ch13table01) 总结了AlphaGo中使用的所有特征。前48个平面用于策略网络，最后一个仅用于价值网络。'
- en: Table 13.1\. Feature planes used in AlphaGo
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表13.1\. AlphaGo使用的特征平面
- en: '| Feature name | Number of planes | Description |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 特征名称 | 平面数量 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Stone color | 3 | Three feature planes indicating stone color—one each for
    the current player, the opponent, and the empty points on the board. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 石头颜色 | 3 | 三个表示石头颜色的特征平面——分别用于当前玩家、对手和棋盘上的空点。 |'
- en: '| Ones | 1 | A feature plane entirely filled with the value 1. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 1s | 1 | 一个完全填充值为1的特征平面。 |'
- en: '| Zeros | 1 | A feature plane entirely filled with the value 0. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 0s | 1 | 一个完全填充值为0的特征平面。 |'
- en: '| Sensibleness | 1 | A move on this plane is 1 if the move is legal and doesn’t
    fill the current player’s eyes, and 0 otherwise. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 敏感性 | 1 | 在这个平面上，如果移动是合法的并且不会填满当前玩家的眼位，则移动得分为1，否则为0。 |'
- en: '| Turns since | 8 | This set of eight binary planes indicates how many moves
    ago a move was played. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 自由度自上次移动以来 | 8 | 这组八个二进制平面表示移动发生前的移动次数。 |'
- en: '| Liberties | 8 | Number of liberties of the string of stones this move belongs
    to, split into eight binary planes. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 自由度 | 8 | 该移动所属的石头串的自由度数量，分为八个二进制平面。 |'
- en: '| Liberties after move | 8 | If this move was played, how many liberties would
    this result in? |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 移动后的自由度 | 8 | 如果这个移动被实施，会产生多少自由度？ |'
- en: '| Capture size | 8 | How many opponent stones would this move capture? |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 捕获大小 | 8 | 这个移动将捕获多少对手的棋子？ |'
- en: '| Self-atari size | 8 | If this move was played, how many of your own stones
    would be put into atari and could be captured by the opponent in the next move?
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 自我阿塔里大小 | 8 | 如果执行这个移动，你的多少自己的棋子会被放入阿塔里并可能在下一轮被对手捕获？ |'
- en: '| Ladder capture | 1 | Can this stone be captured in a ladder? |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 梯子捕获 | 1 | 这颗棋子能否在梯子中被捕获？ |'
- en: '| Ladder escape | 1 | Can this stone escape all possible ladders? |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 梯子逃脱 | 1 | 这颗棋子能否逃脱所有可能的梯子？ |'
- en: '| Current player color | 1 | A plane filled with 1s if current player is black,
    or 0s if the player is white. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 当前玩家颜色 | 1 | 如果当前玩家是黑子，则用1填充的平面，如果玩家是白子，则用0填充。 |'
- en: The implementation of these features can be found in our GitHub repository under
    alphago.py in the dlgo.encoders module. Although implementing each of the feature
    sets from [table 13.1](#ch13table01) isn’t difficult, it’s also not particularly
    interesting when compared to all the exciting parts making up AlphaGo that still
    lie ahead of us. Implementing ladder captures is somewhat tricky, and encoding
    the number of turns since a move was played requires modifications to your Go
    board definition. So if you’re interested in how this can be done, check out our
    implementation on GitHub.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能的实现可以在我们的GitHub仓库中找到，位于dlgo.encoders模块下的alphago.py。虽然从[表13.1](#ch13table01)实现每个特征集并不困难，但与AlphaGo中仍在我们面前的所有令人兴奋的部分相比，这也不算特别有趣。实现梯子捕获有些棘手，而编码自移动以来经过的回合数需要对你的围棋棋盘定义进行修改。所以如果你对如何实现这一点感兴趣，请查看我们在GitHub上的实现。
- en: Let’s quickly look at how an `AlphaGoEncoder` can be initialized, so you can
    use it to train deep neural networks. You provide a Go board size and a Boolean
    called `use_player_plane` that indicates whether to use the 49th feature plane.
    This is shown in the following listing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看如何初始化`AlphaGoEncoder`，这样你就可以用它来训练深度神经网络。你提供一个围棋棋盘大小和一个名为`use_player_plane`的布尔值，表示是否使用第49个特征平面。这如下面的列表所示。
- en: Listing 13.4\. Signature and initialization of your AlphaGo board encoder
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.4\. AlphaGo棋盘编码器的签名和初始化
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 13.1.3\. Training AlphaGo-style policy networks
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.1.3\. 训练AlphaGo风格的策略网络
- en: 'Having network architectures and input features ready, the first step of training
    policy networks for AlphaGo follows the exact procedure we introduced in [chapter
    7](kindle_split_019.xhtml#ch07): specifying a board encoder and an agent, loading
    Go data, and training the agents with this data. [Figure 13.4](#ch13fig04) illustrates
    the process. The fact that you use slightly more elaborate features and networks
    doesn’t change this one bit.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好网络架构和输入特征后，训练AlphaGo策略网络的第一个步骤遵循我们在[第7章](kindle_split_019.xhtml#ch07)中介绍的确切程序：指定棋盘编码器和代理，加载围棋数据，并使用这些数据训练代理。[图13.4](#ch13fig04)说明了这个过程。你使用稍微更详细的特征和网络并不改变这一点。
- en: Figure 13.4\. The supervised training process for AlphaGo’s policy networks
    is exactly the same as the flow covered in [chapters 6](kindle_split_018.xhtml#ch06)
    and [7](kindle_split_019.xhtml#ch07). You replay human game records and reproduce
    the game states. Each game state is encoded as a tensor (this diagram shows a
    tensor with only two planes; AlphaGo used 48 planes). The training target is a
    vector the same size as the board, with a 1 where the human actually played.
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图13.4\. AlphaGo策略网络的监督训练过程与[第6章](kindle_split_018.xhtml#ch06)和[第7章](kindle_split_019.xhtml#ch07)中覆盖的流程完全相同。你回放人类游戏记录并重现游戏状态。每个游戏状态都被编码为一个张量（此图显示了一个只有两个平面的张量；AlphaGo使用了48个平面）。训练目标是与棋盘大小相同的向量，在人类实际落子处为1。
- en: '![](Images/13fig04_alt.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/13fig04_alt.jpg)'
- en: To initialize and train AlphaGo’s strong policy network, you first need to instantiate
    an `AlphaGoEncoder`, and create two Go data generators for training and testing,
    just as you did in [chapter 7](kindle_split_019.xhtml#ch07). You find this step
    on GitHub under examples/alphago/alphago_policy_sl.py.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化和训练AlphaGo的强大策略网络，你首先需要实例化一个`AlphaGoEncoder`，并为训练和测试创建两个围棋数据生成器，就像你在[第7章](kindle_split_019.xhtml#ch07)中所做的那样。你可以在GitHub上的examples/alphago/alphago_policy_sl.py找到这一步。
- en: Listing 13.5\. Loading data for the first step of training AlphaGo’s policy
    network
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.5\. 加载AlphaGo策略网络训练的第一步数据
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, you can load AlphaGo’s policy network by using the `alphago_model` function
    defined earlier in this section and compile this Keras model with categorical
    cross-entropy and stochastic gradient descent. We call this model alphago_sl_policy
    to signify that it’s a policy network trained by supervised learning (*sl*).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以使用本节之前定义的 `alphago_model` 函数加载 AlphaGo 的策略网络，并使用分类交叉熵和随机梯度下降编译这个 Keras
    模型。我们称这个模型为 alphago_sl_policy，以表明它是一个通过监督学习（*sl*）训练的策略网络。
- en: Listing 13.6\. Creating an AlphaGo policy network with Keras
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.6\. 使用 Keras 创建 AlphaGo 策略网络
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now all that’s left for this first stage of training is to call `fit_generator`
    on this policy network, using both training and test generators as you did in
    [chapter 7](kindle_split_019.xhtml#ch07). Apart from using a larger network and
    a more sophisticated encoder, this is precisely what you did in [chapters 6](kindle_split_018.xhtml#ch06)
    to [8](kindle_split_020.xhtml#ch08).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在第一阶段训练剩下的就是调用这个策略网络的 `fit_generator` 方法，使用训练和测试生成器，就像你在第 7 章（kindle_split_019.xhtml#ch07）中做的那样。除了使用更大的网络和更复杂的编码器外，这正是你在第
    6 章（kindle_split_018.xhtml#ch06）到第 8 章（kindle_split_020.xhtml#ch08）中所做的。
- en: After training has finished, you can create a `DeepLearningAgent` from `model`
    and `encoder` and store it for the next two training phases that we discuss next.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你可以从 `model` 和 `encoder` 创建一个 `DeepLearningAgent` 并将其存储起来，以备我们接下来讨论的下一个训练阶段使用。
- en: Listing 13.7\. Training and persisting a policy network
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.7\. 训练和持久化策略网络
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For the sake of simplicity, in this chapter you don’t need to train fast and
    strong policy networks separately, as in the original AlphaGo paper. Instead of
    training a smaller and faster second policy network, you can use alphago_sl_agent
    as the fast policy. In the next section, you’ll see how to use this agent as a
    starting point for reinforcement learning, which will lead to a stronger policy
    network.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，在本章中，你不需要像原始 AlphaGo 论文中那样分别训练快速且强大的策略网络。你不需要训练一个更小、更快的第二个策略网络，而是可以使用
    alphago_sl_agent 作为快速策略。在下一节中，你将看到如何使用这个智能体作为强化学习的起点，这将导致一个更强大的策略网络。
- en: 13.2\. Bootstrapping self-play from policy networks
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2\. 从策略网络启动自我对弈
- en: Having trained a relatively strong policy agent with alphago_sl_agent, you can
    now use this agent to let it play against itself, using the policy gradient algorithm
    covered in [chapter 10](kindle_split_022.xhtml#ch10). As you’ll see in [section
    13.5](#ch13lev1sec5), in DeepMind’s AlphaGo you let *different versions of the
    strong policy network* play against the currently strongest version. This prevents
    overfitting and results in overall better performance, but our simple approach
    of letting alphago_sl_agent play against itself conveys the general idea to use
    self-play to make a policy agent stronger.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 alphago_sl_agent 训练了一个相对强大的策略智能体之后，你现在可以使用这个智能体让它与自己对弈，使用第 10 章（kindle_split_022.xhtml#ch10）中介绍的政策梯度算法。正如你将在第
    13.5 节（#ch13lev1sec5）中看到的，在 DeepMind 的 AlphaGo 中，你让 *不同版本的强大策略网络* 与当前最强的版本对弈。这可以防止过拟合，并导致整体性能更好，但我们的简单方法让
    alphago_sl_agent 与自己对弈传达了使用自我对弈来使策略智能体变得更强大的基本思想。
- en: 'For the next training phase, you first load the supervised-learning policy
    network alphago_sl_agent twice: one version serves as your new reinforcement-learning
    agent called alphago_rl_agent, and the other as its opponent. This step can be
    found under examples/alphago/alphago_policy_sl.py on GitHub.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个训练阶段，你首先加载监督学习策略网络 alphago_sl_agent 两次：一个版本作为你的新强化学习智能体，称为 alphago_rl_agent，另一个作为其对手。这一步骤可以在
    GitHub 上的 examples/alphago/alphago_policy_sl.py 中找到。
- en: Listing 13.8\. Loading the trained policy network twice to create two self-play
    opponents
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.8\. 加载训练好的策略网络两次以创建两个自我对弈对手
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, you can use these two agents to engage in self-play and store the resulting
    experience data, for training purposes. This experience data is used to train
    alphago_rl_agent with it. You then store the trained reinforcement-learning policy
    agent and the experience data acquired through self-play, because you need this
    data to train AlphaGo’s value network with it as well.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以使用这两个智能体进行自我对弈并存储由此产生的经验数据，用于训练目的。这些经验数据用于训练 alphago_rl_agent。然后你将训练好的强化学习策略智能体和通过自我对弈获得的经验数据存储起来，因为你还需要这些数据来训练
    AlphaGo 的价值网络。
- en: Listing 13.9\. Generating self-play data for your `PolicyAgent` to learn from
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.9\. 为你的 `PolicyAgent` 生成自我对弈数据
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that this example uses a utility function called `experience_simulation`
    from dlgo.rl.simulate. The implementation can be found on GitHub, but all this
    function does is set up two agents to engage in self-play for a specified number
    of games (`num_games`) and return the experience data as `ExperienceCollector`,
    a concept introduced in [chapter 9](kindle_split_021.xhtml#ch09).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个例子使用了一个名为`experience_simulation`的效用函数，来自dlgo.rl.simulate。实现可以在GitHub上找到，但这个函数所做的只是设置两个代理进行指定数量的游戏（`num_games`）的自我对弈，并将经验数据作为`ExperienceCollector`返回，这是一个在第9章中引入的概念。
- en: When AlphaGo entered the stage in 2016, the strongest open source Go bot was
    *Pachi* (which you can learn more about in [appendix C](kindle_split_030.xhtml#app03)),
    ranked around 2 dan amateur level. Simply letting the reinforcement learning agent
    alphago_rl_agent pick the next move led to an impressive 85% win rate of AlphaGo
    against Pachi. Convolutional neural networks were used for Go move prediction
    before, but never fared better than around 10% against Pachi. This shows you the
    relative strength gain of self-play over purely supervised learning with deep
    neural networks. If you run your own experiments, don’t expect your bots to start
    out on such a high ranking—it’s unlikely that you have (or can afford) the compute
    power necessary.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当AlphaGo在2016年进入舞台时，最强的开源围棋机器人是 *Pachi*（你可以在[附录C](kindle_split_030.xhtml#app03)中了解更多信息），大约是2段业余水平。仅仅让强化学习代理alphago_rl_agent选择下一步就导致了AlphaGo对Pachi的85%的胜率，这是一个令人印象深刻的胜率。在之前，卷积神经网络被用于围棋走法预测，但从未在Pachi上表现出超过10%的胜率。这显示了自我对弈相对于纯监督学习使用深度神经网络的相对强度增益。如果你运行自己的实验，不要期望你的机器人一开始就有如此高的排名——你很可能没有（或负担不起）必要的计算能力。
- en: 13.3\. Deriving a value network from self-play data
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3\. 从自我对弈数据中推导价值网络
- en: The third and last step in AlphaGo’s network-training process is to train a
    value network *from the same self-play experience data* that you just used for
    alphago_rl_agent. This step looks structurally similar to the last. You first
    initialize an AlphaGo value network and create a `ValueAgent` with an AlphaGo
    board encoder. This training step can also be found in examples/alphago/alphago_value.py
    on GitHub.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo网络训练过程的第三步和最后一步是从你刚刚用于alphago_rl_agent的相同自我对弈经验数据中训练一个价值网络。这一步骤在结构上与上一步相似。你首先初始化一个AlphaGo价值网络，并使用一个AlphaGo棋盘编码器创建一个`ValueAgent`。这个训练步骤也可以在GitHub上的examples/alphago/alphago_value.py中找到。
- en: Listing 13.10\. Initializing an AlphaGo value network
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.10\. 初始化AlphaGo价值网络
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can now pick up the experience data from self-play once again and train
    your value agent with it, after which you persist the agent just as the other
    two before.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以再次从自我对弈中获取经验数据，并用它来训练你的价值代理，之后就像之前的两个一样持久化代理。
- en: Listing 13.11\. Training a value network from experience data
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.11\. 从经验数据中训练价值网络
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: At this point, if you were to break into the premises of DeepMind’s AlphaGo
    team (you shouldn’t) and assume team members used Keras in the same way you did
    to train AlphaGo (they didn’t), then getting your hands on the network parameters
    for fast policy, strong policy, and value network, you’d have a Go bot that plays
    at superhuman level. That is, provided you know how to use these three deep networks
    appropriately in a tree search algorithm. The next section is all about that.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，如果你要打破DeepMind的AlphaGo团队的前提（你不应该）并假设团队成员使用Keras的方式与你训练AlphaGo的方式相同（他们没有），那么你将能够接触到快速策略、强策略和价值网络的网络参数，你将拥有一个在超人类水平上玩围棋的围棋机器人。也就是说，前提是你知道如何在树搜索算法中适当地使用这三个深度网络。下一节将全部关于这一点。
- en: 13.4\. Better search with policy and value networks
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4\. 使用策略和价值网络进行更好的搜索
- en: 'Recall from [chapter 4](kindle_split_016.xhtml#ch04) that in pure Monte Carlo
    tree search applied to the game of Go, you build a tree of game states by using
    these four steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第4章](kindle_split_016.xhtml#ch04)，在纯蒙特卡洛树搜索应用于围棋游戏中，你通过使用这四个步骤来构建游戏状态树：
- en: '***Select*—** You traverse the game tree by randomly selecting among *children*.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***选择***—** 你通过在 *子节点* 中随机选择来遍历游戏树。'
- en: '***Expand*—** You add a new *node* to the tree (a new game state).'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***扩展***—** 你向树中添加一个新的 *节点*（一个新的游戏状态）。'
- en: '***Evaluate*—** From this state, which is sometimes referred to as a *leaf*,
    simulate a game completely randomly.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***评估***—** 从这个状态开始，有时也被称为 *叶节点*，完全随机地模拟一场游戏。'
- en: '***Update*—** After the simulation is completed, update your tree statistics
    accordingly.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***更新***—** 模拟完成后，相应地更新你的树统计信息。'
- en: Simulating many games will lead to more and more accurate statistics, which
    you can then use to pick the next move.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟许多游戏将导致越来越准确的统计数据，然后你可以使用这些数据来选择下一步。
- en: The AlphaGo system uses a more sophisticated tree-search algorithm, but you’ll
    still recognize many of its parts. The preceding four steps are still integral
    to AlphaGo’s MCTS algorithm, but you’ll use deep neural networks in a smart way
    to evaluate positions, expand nodes, and track statistics. For the rest of the
    chapter, we’ll show you exactly how and develop a version of AlphaGo’s tree search
    along the way.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo系统使用更复杂的树搜索算法，但你仍然会认出其中许多部分。前四个步骤仍然是AlphaGo的蒙特卡洛树搜索算法的核心，但你将巧妙地使用深度神经网络来评估位置、扩展节点和跟踪统计数据。在本章的其余部分，我们将向你展示具体如何操作，并在过程中开发AlphaGo的树搜索版本。
- en: 13.4.1\. Using neural networks to improve Monte Carlo rollouts
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.1. 使用神经网络改进蒙特卡洛树搜索
- en: '[Sections 13.1](#ch13lev1sec1), [13.2](#ch13lev1sec2), and [13.3](#ch13lev1sec3)
    described in detail how to train three neural networks for AlphaGo: fast and strong
    policies and a value network. How can you use these networks to improve Monte
    Carlo tree search? The first thing that comes to mind is to stop playing games
    at random and instead use a policy network to guide rollouts. That’s exactly what
    the fast policy network is for and it explains the name—rollouts need to be *fast*
    to carry out many of them.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[第13.1节](#ch13lev1sec1)、[13.2节](#ch13lev1sec2)和[13.3节](#ch13lev1sec3)详细描述了如何训练三个神经网络用于AlphaGo：快速且强大的策略网络和值网络。你如何使用这些网络来改进蒙特卡洛树搜索？首先想到的是停止随机玩游戏，而是使用策略网络来引导模拟。这正是快速策略网络的作用，这也解释了其名称——模拟需要**快速**以执行大量操作。'
- en: The following listing shows how to greedily select moves from a policy network
    for a given Go game state. You choose the best possible move until the game is
    over and return 1 if the current player wins, and –1 otherwise.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了如何从给定的围棋游戏状态中贪婪地选择策略网络中的移动。你选择最佳可能的移动，直到游戏结束，如果当前玩家获胜则返回1，否则返回-1。
- en: Listing 13.12\. Doing rollouts with the fast policy network
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.12. 使用快速策略网络进行模拟
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using this rollout strategy is already beneficial in itself, because policy
    networks are naturally much better at choosing moves than tossing a coin. But
    you still have a lot of room for improvement.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种模拟策略本身就是有益的，因为策略网络在选择移动方面比抛硬币要自然得多。但你的改进空间仍然很大。
- en: For instance, when you arrive at a leaf node in your tree and need to expand
    it, instead of randomly selecting a new node for expansion, you can *ask the strong
    policy network for good moves*. A policy network gives you a probability distribution
    over all next moves, and each node can track this probability so that strong moves
    (according to the policy) are chosen more likely than others. We call these node
    probabilities *prior probabilities*, because they give us prior knowledge of the
    strength of a move before doing any tree search.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你到达树中的叶节点并需要扩展它时，你不必随机选择一个新节点进行扩展，而是可以**向强大的策略网络请求好的移动**。策略网络为你提供所有下一步的概率分布，每个节点可以跟踪这个概率，这样根据策略的强移动比其他移动更有可能被选择。我们称这些节点概率为**先验概率**，因为它们在执行任何树搜索之前就为我们提供了关于移动强度的先验知识。
- en: 'Finally, here’s how the value network comes into play. You already improved
    your rollout mechanism by replacing random guesses with a policy network. Nevertheless,
    at each leaf you compute only the outcome of a single game to estimate how valuable
    the leaf is. But estimating the value of a position is precisely what you trained
    the value network to be good at, so you already have a sophisticated guess for
    that. What AlphaGo does is *weigh* the outcome of rollouts against the output
    of the value network. If you think about it, that’s similar to the way you make
    decisions when playing games as a human: you try to look ahead as many moves as
    realistically possible, but you also take your experience of the game into account.
    If you can read out a sequence of moves that might be good for you, that can supersede
    your hunch that the position isn’t that great, and vice versa.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是价值网络如何发挥作用。你已经通过用策略网络替换随机猜测来改进了你的 rollout 机制。尽管如此，在每一片叶子你只计算一个游戏的结局来估计叶子的价值。但估计位置的价值正是你训练价值网络擅长的事情，所以你已经有了一个复杂的猜测。AlphaGo
    做的是 *权衡* rollout 的结果和价值网络的输出。如果你这么想，那和你作为一个人类玩游戏时做决定的方式很相似：你试图尽可能长远地展望，但你也考虑游戏的经验。如果你可以读出一个可能对你有利的移动序列，那么它可以超越你认为位置不是那么好的直觉，反之亦然。
- en: Now that you roughly know what each of the three deep neural networks used in
    AlphaGo are for and how tree search can be improved with them, let’s have a closer
    look at the details.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你大致知道了 AlphaGo 中使用的三个深度神经网络的作用以及如何通过它们改进树搜索，让我们更详细地看看细节。
- en: 13.4.2\. Tree search with a combined value function
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.2\. 带有组合价值函数的树搜索
- en: In [chapter 11](kindle_split_023.xhtml#ch11), you saw action values, also called
    *Q-values*, applied to the game of Go. To recap, for a current board state *s*
    and a potential next move *a*, an action value *Q*(*s*,*a*) estimates how good
    a move *a* would be in the situation *s*. You’ll see in a bit how to define *Q*(*s*,*a*);
    for now, just note that each node in an AlphaGo search tree stores Q-values. Additionally,
    each node tracks *visit counts*, meaning how often this node has been traversed
    by search, as well as *prior probabilities P*(*s*,*a*), or how valuable the strong
    policy network thinks action *a* would be from *s*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 11 章](kindle_split_023.xhtml#ch11) 中，你看到了动作值，也称为 *Q 值*，应用于围棋游戏。为了回顾，对于一个当前棋盘状态
    *s* 和一个潜在的下一步棋 *a*，动作值 *Q*(*s*,*a*) 估计在情况 *s* 下棋 *a* 会是好是坏。你很快就会看到如何定义 *Q*(*s*,*a*)；现在，只需注意
    AlphaGo 搜索树中的每个节点都存储 Q 值。此外，每个节点还跟踪 *访问次数*，即这个节点被搜索遍历的频率，以及 *先验概率 P*(*s*,*a*)，或者强策略网络认为从
    *s* 出发动作 *a* 的价值如何。
- en: Each node in a tree has precisely one parent, but potentially multiple *children*,
    which you can encode as Python dictionary-mapping moves to other nodes. With this
    convention, you can define an `AlphaGoNode` as follows.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 树中的每个节点恰好有一个父节点，但可能有多个 *子节点*，你可以将它们编码为 Python 字典，将移动映射到其他节点。按照这个约定，你可以定义一个 `AlphaGoNode`
    如下。
- en: Listing 13.13\. A simple view on a node in an AlphaGo tree
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.13\. AlphaGo 树中一个节点的简单视图
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s say you’re being thrown into an ongoing game, have already built a large
    tree, and collected visit counts and a good estimate of action values. What you
    want is to simulate a number of games and track game statistics so that at the
    end of the simulation, you can pick the best move you found. How do you traverse
    the tree to simulate a game? If you’re in game state *s* and denote the respective
    visit count as *N*(*s*), you can choose an action as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被扔进一个正在进行的游戏，已经构建了一个大树，收集了访问次数和动作值的好估计。你想要模拟几场比赛并跟踪游戏统计数据，以便在模拟结束时，你可以选择你找到的最佳移动。你如何遍历树来模拟游戏？如果你处于游戏状态
    *s*，并且分别表示访问次数为 *N*(*s*)，你可以选择一个动作如下：
- en: '![](Images/p0279_01.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0279_01.jpg)'
- en: 'This might look a little complicated at first, but you can break down this
    equation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能一开始看起来有点复杂，但你可以将这个方程分解：
- en: The argmax notation means that you take the argument *a* for which the formula
    *Q*(*s*,*a*) + *P*(*s*,*a*) / (1 + *N*(s,*a*)) is maximized.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: argmax 符号表示你取公式 *Q*(*s*,*a*) + *P*(*s*,*a*) / (1 + *N*(s,*a*)) 最大的 *a* 参数。
- en: The term you maximize is composed of two parts, the Q-value and prior probabilities
    *normalized* by visit counts.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你要最大化的术语由两部分组成，Q 值和通过访问次数 *标准化* 的先验概率。
- en: In the beginning, visit counts are zeros, which means you give equal weight
    to Q-value and prior probability by maximizing over *Q*(*s*,*a*) + *P*(*s*,*a*).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始时，访问次数为零，这意味着你通过最大化 *Q*(*s*,*a*) + *P*(*s*,*a*) 来对 Q 值和先验概率给予相同的权重。
- en: If visit counts become very large, the term *P*(*s*,*a*) / (1 + *N*(*s*,*a*))
    becomes negligible, which effectively leaves you with *Q*(*s*,*a*).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果访问次数变得非常大，项 *P*(*s*,*a*) / (1 + *N*(*s*,*a*)) 变得可以忽略不计，这实际上让你只剩下 *Q*(*s*,*a*)。
- en: We call this utility function *u*(*s*,*a*) = *P*(*s*,*a*) / (1 + *N*(*s*,*a*)).
    In the next section, you’ll slightly modify *u*(*s*,*a*), but this version has
    all the components you need to reason about it. With this notation, you can also
    write a’ = argmax[a]*Q*(s,*a*) + *u*(*s*,*a*) for move selection.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们称这个效用函数为 *u*(*s*,*a*) = *P*(*s*,*a*) / (1 + *N*(*s*,*a*))。在下一节中，你将稍微修改 *u*(*s*,*a*)，但这个版本包含了你需要来推理它的所有组件。有了这个符号，你也可以为移动选择写出
    a’ = argmax[a]*Q*(s,*a*) + *u*(*s*,*a*)。
- en: To summarize, you select actions by weighing prior probabilities against Q-values.
    As you traverse the tree, accumulate visit counts, and get better estimates of
    *Q*, you slowly forget about your *prior estimation* and put more and more trust
    in Q-values. You could also say that you rely less on prior knowledge and explore
    more. This might be analogous to your own game-play experience. Let’s say you
    play your favorite strategy board game all night. At the beginning of the night,
    you bring all your prior experience to the table, but as the night progresses,
    you (hopefully) try new things and update your beliefs about what works and what
    doesn’t.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，你通过权衡先验概率和 Q 值来选择动作。当你遍历树时，累积访问次数，并得到更好的 Q 值估计，你逐渐忘记你的 *先验估计*，并且越来越信任 Q
    值。你也可以说，你依赖先验知识较少，探索更多。这可能与你的游戏体验相似。比如说，你整夜都在玩你最喜欢的策略棋盘游戏。在夜幕降临之初，你把所有先前的经验都带到桌面上，但随着夜晚的进行，你（希望）尝试新事物，并更新你对什么有效、什么无效的信念。
- en: 'So this is how AlphaGo *selects* moves from an existing tree, but how about
    *expanding* the tree if you’ve reached a leaf *l*? See [figure 13.5](#ch13fig05).
    First, you compute the predictions of the strong policy network *P*(*l*) and store
    them as prior probabilities for each child of *l*. Then you *evaluate* the leaf
    node by *combining policy rollouts and value network* as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是 AlphaGo 从现有树中选择动作的方式，但如果你到达了叶节点 *l*，如何 *扩展* 树呢？参见[图 13.5](#ch13fig05)。首先，你计算强策略网络
    *P*(*l*) 的预测，并将它们作为 *l* 的每个子节点的先验概率存储。然后你通过以下方式 *评估* 叶节点：将策略回滚和价值网络 *结合*。
- en: '| *V*(*l*) = λ · value(*l*) + (1 – λ) · rollout(*l*) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| *V*(*l*) = λ · value(*l*) + (1 – λ) · rollout(*l*) |'
- en: In this equation, value(*l*) is the result of your value network for *l*, rollout(*l*)
    denotes the game result of a fast policy rollout from *l*, and l is a value between
    0 and 1, which you will set to 0.5 by default.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，value(*l*) 是你的价值网络对 *l* 的结果，rollout(*l*) 表示从 *l* 出发的一个快速策略回滚的游戏结果，l 是介于
    0 和 1 之间的一个值，你默认将其设置为 0.5。
- en: Figure 13.5\. To evaluate possible board positions, AlphaGo combines two independent
    evaluations. First, it feeds the board position to its value network, which directly
    returns an estimated chance of winning. Second, it uses the fast policy network
    to complete the game from that position and observes who won. The evaluation used
    in the tree search is a weighted sum of those two parts.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 13.5。为了评估可能的棋盘位置，AlphaGo 结合了两个独立的评估。首先，它将棋盘位置输入到其价值网络中，该网络直接返回一个估计的获胜概率。其次，它使用快速策略网络从这个位置完成游戏，并观察谁获胜。在树搜索中使用的评估是这两个部分的加权总和。
- en: '![](Images/13fig05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/13fig05.jpg)'
- en: 'Taking a step back, keep in mind that you want to simulate a total of *n* games
    with tree search to pick a move at the end. To make this work, you need to update
    visit counts and Q-values at the end of the simulations. Visit counts are easy;
    you increment the count of a node by 1 if it has been traversed by search. To
    update Q-values, you sum up *V*(*l*) for all visited leaf nodes *l* and divide
    by visit counts:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 退一步来说，记住你希望通过树搜索模拟总共 *n* 场游戏来在最后选择一步棋。为了使这成为可能，你需要在模拟结束时更新访问次数和 Q 值。访问次数很容易；如果一个节点被搜索遍历，你只需将其计数增加
    1。要更新 Q 值，你需要对所有访问过的叶节点 *l* 的 *V*(*l*) 求和，然后除以访问次数：
- en: '![](Images/p0281_01.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0281_01.jpg)'
- en: 'Here you sum up over all *n* simulations and add the leaf node value of the
    *i*th simulation, if that simulation traversed the node corresponding to (*s*,*a*).
    To summarize this whole process, let’s look at how you modified the four-step
    tree-search process from [chapter 4](kindle_split_016.xhtml#ch04):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你将所有*n*次模拟的总和加起来，如果模拟遍历了对应于(*s*,*a*)的节点，则添加第*i*次模拟的叶节点值。为了总结整个过程，让我们看看你是如何修改了第4章中的四个步骤树搜索过程的：
- en: '***Select*—** You traverse the game tree by choosing actions that maximize
    *Q*(*s*,*a*) + *u*(*s*,*a*).'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***选择*—** 你通过选择最大化*Q*(*s*,*a*) + *u*(*s*,*a*)的动作来遍历游戏树。'
- en: '***Expand*—** When expanding a new leaf, you ask the strong policy network
    once for prior probabilities to be stored for each child.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***扩展*—** 当扩展一个新的叶节点时，你将一次询问强策略网络以存储每个子节点的先验概率。'
- en: '***Evaluate*—** At the end of a simulation, a leaf node is evaluated by averaging
    the output of the value network with the outcome of a rollout that uses the fast
    policy.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***评估*—** 在模拟结束时，通过将值网络的输出与使用快速策略的rollout结果平均来评估一个叶节点。'
- en: '***Update*—** After all simulations are completed, you update visit counts
    and Q-values traversed in simulations.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***更新*—** 在完成所有模拟后，你更新模拟中访问的次数和Q值。'
- en: 'The one thing we haven’t discussed yet is how to pick a move to *play* after
    the simulations have finished. That’s simple: you pick the most visited node!
    This might even seem a little too simplistic, but keep in mind that nodes get
    more and more visits over time as their Q-values improve. After you go through
    enough simulations, the node visit count will give you a great indication of a
    move’s relative worth.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有讨论的一件事是在模拟完成后如何选择一个动作来*玩*。这很简单：你选择访问次数最多的节点！这甚至可能看起来有点过于简单，但请记住，随着Q值的提高，节点会越来越多地被访问。在你完成足够的模拟后，节点访问次数将为你提供关于动作相对价值的良好指示。
- en: 13.4.3\. Implementing AlphaGo’s search algorithm
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.3\. 实现AlphaGo的搜索算法
- en: Having discussed how AlphaGo uses neural networks in combination with tree search,
    let’s go ahead and implement this algorithm in Python. Your goal is to create
    an `Agent` that has a `select_move` method as specified by the AlphaGo methodology.
    The code for this section can be found under dlgo/agent/alphago.py on GitHub.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了AlphaGo如何结合树搜索使用神经网络之后，让我们继续在Python中实现这个算法。你的目标是创建一个具有`select_move`方法的`Agent`，该方法由AlphaGo方法论指定。本节的代码可以在GitHub上的dlgo/agent/alphago.py下找到。
- en: You start with the full definition of an AlphaGo tree node, which you already
    sketched in the preceding section. An `AlphaGoNode` has a parent and children
    represented as a dictionary of moves to other nodes. A node also comes with a
    `visit_count`, a `q_value`, and a `prior_value`. Additionally, you store the *utility
    function* `u_value` of this node.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你从AlphaGo树节点的完整定义开始，这在前面一节中已经草拟了。`AlphaGoNode`有一个父节点和子节点，它们以移动到其他节点的字典形式表示。节点还包含一个`visit_count`、一个`q_value`和一个`prior_value`。此外，你存储这个节点的*效用函数*
    `u_value`。
- en: Listing 13.14\. Defining an AlphaGo tree node in Python
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.14\. 在Python中定义AlphaGo树节点
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1* Tree nodes have one parent and potentially many children.**'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 树节点有一个父节点和可能有很多子节点。**'
- en: '***2* A node is initialized with a prior probability.**'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 节点以一个先验概率初始化。**'
- en: '***3* The utility function will be updated during search.**'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 效用函数将在搜索过程中更新。**'
- en: 'Such a node will be used by the tree-search algorithm in three places:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的节点将在树搜索算法的三个地方使用：
- en: '**`select_child`—** Traversing the tree in a simulation, you select children
    of a node according to argmax[a]*Q*(*s*,*a*) + *u*(*s*,*a*); you pick the action
    maximizing the sum of Q-value and utility function.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`select_child`—** 在模拟中遍历树时，你根据argmax[a]*Q*(*s*,*a*) + *u*(*s*,*a*)选择节点的子节点；你选择最大化Q值和效用函数总和的动作。'
- en: '**`expand_children`—** At a leaf node, you’ll ask the strong policy to evaluate
    all legal moves from this position and add new `AlphaGoNode` instances for each
    one of them.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`expand_children`—** 在一个叶节点，你会要求强策略评估从这个位置出发的所有合法移动，并为每个移动添加一个新的`AlphaGoNode`实例。'
- en: '**`update_values`—** Finally, after completing all simulations, you update
    `visit_count`, `q_value`, and `u_value` accordingly.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**`update_values`—** 最后，在完成所有模拟后，你相应地更新`visit_count`、`q_value`和`u_value`。'
- en: The first two of these methods are straightforward, as shown in the following
    listing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，前两种方法很简单。
- en: Listing 13.15\. Selecting an AlphaGo child by maximizing Q-value
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表13.15\. 通过最大化Q值选择AlphaGo子节点
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The third method to update the summary statistics of the AlphaGo node is a
    little more intricate. First of all, you use a slightly more sophisticated version
    of the utility function:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 AlphaGo 节点汇总统计的第三种方法稍微复杂一些。首先，您使用一个稍微复杂一点的效用函数：
- en: '![](Images/p0282_01.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0282_01.jpg)'
- en: Compared to the version introduced in the preceding section, this utility has
    two extra terms. The first term *c[u],* which we’ll call `c_u` in code, scales
    utility by a fixed *constant* for all nodes, which we set to 5 by default. The
    second term further scales utility by the square root of the parent’s visit count
    (you denote the parent of the node in question by *Np*). This leads to higher
    utility of nodes whose parents have been visited more frequently.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一节中引入的版本相比，这个效用有两个额外的项。第一个项 *c[u]*，我们在代码中将其称为 `c_u`，通过一个固定的 *constant* 缩放所有节点的效用，我们默认设置为
    5。第二个项通过父节点访问次数的平方根进一步缩放效用（您用 *Np* 表示所讨论节点的父节点）。这导致父节点被访问得更频繁的节点具有更高的效用。
- en: Listing 13.16\. Updating visit counts, Q-value, and utility of an AlphaGo node
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.16\. 更新 AlphaGo 节点的访问次数、Q值和效用
- en: '[PRE15]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* You update parents first to ensure that you traverse the tree from top
    to bottom.**'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 首先更新父节点以确保从上到下遍历树。**'
- en: '***2* Increment the visit count for this node.**'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 增加此节点的访问次数。**'
- en: '***3* Add the specified leaf value to the Q-value, normalized by visit count.**'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将指定的叶值添加到 Q值中，并按访问次数进行归一化。**'
- en: '***4* Update utility with current visit counts.**'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用当前访问次数更新效用。**'
- en: 'This completes the definition of `AlphaGoNode`. You can now use this tree structure
    in the search algorithm used in AlphaGo. The class `AlphaGoMCTS` that you’ll be
    implementing is an `Agent` and is initialized with multiple arguments. First,
    you provide this agent with a fast and strong policy and a value network. Second,
    you need to specify AlphaGo-specific parameters for rollouts and evaluation:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了 `AlphaGoNode` 的定义。现在您可以使用这个树结构在 AlphaGo 中使用的搜索算法。您将要实现的 `AlphaGoMCTS` 类是一个
    `Agent`，它通过多个参数进行初始化。首先，您为这个代理提供一个快速且强大的策略和一个值网络。其次，您需要指定 AlphaGo 特定的回滚和评估参数：
- en: '**`lambda_value`—** This is the l value you use to weigh rollouts and value
    function against each other: *V*(*l*) = l·value(*l*) + (1 – l) · rollout(*l*).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`lambda_value`—** 这是您用于权衡回滚和值函数的l值：*V*(*l*) = l·value(*l*) + (1 – l) · rollout(*l*).'
- en: '**`num_simulations`—** This value specifies how many simulations will be run
    in the selection process of a move.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`num_simulations`—** 此值指定在移动选择过程中将运行多少次模拟。'
- en: '**`depth`—** With this parameter, you tell the algorithm how many moves per
    simulation to look ahead (you specify the search depth).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`depth`—** 使用此参数，您告诉算法每个模拟需要前瞻多少步（您指定搜索深度）。'
- en: '**`rollout_limit`—** When determining a leaf value, you run a policy rollout
    rollout(*l*). You use the parameter `rollout_limit` to tell AlphaGo how many moves
    to roll out before judging the outcome.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`rollout_limit`—** 在确定叶值时，您运行一个策略回滚 rollout(*l*)。您使用参数 `rollout_limit` 来告诉
    AlphaGo 在判断结果之前需要回滚多少步。'
- en: Listing 13.17\. Initializing an `AlphaGoMCTS` Go playing agent
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.17\. 初始化 `AlphaGoMCTS` 围棋玩代理
- en: '[PRE16]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It’s now time to implement the `select_move` method of this new `Agent`, which
    does pretty much all the heavy lifting in this algorithm. We sketched AlphaGo’s
    tree-search procedure in the preceding section, but let’s go through the steps
    one more time:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候实现这个新 `Agent` 的 `select_move` 方法了，这个方法在这个算法中做了大部分繁重的工作。我们在上一节中概述了 AlphaGo
    的树搜索过程，但让我们再次过一遍步骤：
- en: When you want to play a move, the first thing you do is to run `num_simulations`
    simulations on your game tree.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您想要移动时，您首先在您的游戏树上运行 `num_simulations` 次模拟。
- en: In each simulation, you carry out look-ahead search until the specified `depth`
    is reached.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次模拟中，你执行前瞻搜索，直到达到指定的 `depth`。
- en: If a node doesn’t have any children, *expand* the tree by adding new `AlphaGoNode`
    for each legal move, using the strong policy network for prior probabilities.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个节点没有任何子节点，则通过为每个合法移动添加新的 `AlphaGoNode` 来 *扩展* 树，使用强大的策略网络作为先验概率。
- en: If a node does have children, *select* one by choosing the move that maximizes
    Q-value plus utility.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个节点有子节点，则通过选择最大化Q值加效用的移动来 *选择* 一个。
- en: Play the move used in this simulation on the Go board.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在围棋盘上播放这个模拟中使用的移动。
- en: When the depth is reached, *evaluate* this leaf node by computing the combined
    value function from the value network and a policy rollout.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当达到深度时，通过计算值网络和政策回滚的联合值函数来评估此叶节点。
- en: Update all AlphaGo nodes with the leaf values from simulations.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模拟的叶值更新所有 AlphaGo 节点。
- en: 'This process is precisely what you’ll implement in `select_move`. Note that
    this method uses two other utility methods that we’ll discuss later: `policy_probabilities`
    and `policy_rollout`.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程正是你将在 `select_move` 中实现的内容。请注意，这个方法使用了两个我们稍后将要讨论的其他实用方法：`policy_probabilities`
    和 `policy_rollout`。
- en: Listing 13.18\. The main method in AlphaGo’s tree-search process
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.18\. AlphaGo 树搜索过程中的主方法
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '***1* From the current state, play out a number of simulations.**'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从当前状态开始，执行一系列模拟。**'
- en: '***2* Play moves until the specified depth is reached.**'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 执行移动直到达到指定的深度。**'
- en: '***3* If the current node doesn’t have any children...**'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 如果当前节点没有任何子节点...**'
- en: '***4* ...expand them with probabilities from the strong policy.**'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* ...使用强策略的概率来扩展它们。**'
- en: '***5* If there are children, you can select one and play the corresponding
    move.**'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 如果有子节点，你可以选择一个并执行相应的移动。**'
- en: '***6* Compute the output of the value network and a rollout by the fast policy.**'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 计算价值网络输出和快速策略的模拟。**'
- en: '***7* Determine the combined value function.**'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 确定组合价值函数。**'
- en: '***8* Update values for this node in the backup phase.**'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 在备份阶段更新此节点的值。**'
- en: You might have noticed at this point that although you ran all simulations,
    you still haven’t played any move. You do so by playing the most visited node,
    after which the only thing left to do is set a new `root` node accordingly and
    return the suggested move.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，尽管你运行了所有模拟，但你还没有执行任何移动。你通过执行最常访问的节点来完成这一操作，之后唯一剩下的事情就是相应地设置一个新的 `root`
    节点并返回建议的移动。
- en: Listing 13.19\. Selecting the most visited node and updating the tree’s root
    node
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.19\. 选择最常访问的节点并更新树的根节点
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1* Pick the most visited child of the root as the next move.**'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 选择根节点的最常访问子节点作为下一个移动。**'
- en: '***2* If the picked move is a child, set the new root to this child node.**'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果选择的移动是一个子节点，将新的根设置为这个子节点。**'
- en: This already completes the main process of AlphaGo’s tree search, so let’s have
    a look at the two utility methods we left out earlier. `policy_probabilities`,
    used in node expansion, computes predictions of the strong policy network, restricts
    these predictions to legal moves, and then normalizes the remaining predictions.
    The method returns both legal moves and their normalized policy network predictions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经完成了 AlphaGo 树搜索的主要过程，所以让我们看看我们之前忽略的两个实用方法。`policy_probabilities`，在节点扩展中使用，计算强策略网络的预测，将预测限制在合法移动上，然后对剩余的预测进行归一化。该方法返回合法移动及其归一化的策略网络预测。
- en: Listing 13.20\. Computing normalized strong policy values for legal moves on
    the board
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.20\. 计算棋盘上合法移动的正常化强策略值
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The last helper method you need is `policy_rollout` to compute the game result
    of a rollout using the fast policy. All this method does is *greedily* select
    the strongest move according to the fast policy until the rollout limit is reached,
    and then see who won. You return 1 if the player to move next won, –1 if the other
    player won, and 0 if no result has been reached.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的最后一个辅助方法是 `policy_rollout`，用于使用快速策略计算模拟的游戏结果。这个方法所做的只是根据快速策略 *贪婪地* 选择最强的移动，直到达到模拟限制，然后查看谁赢了。如果你移动的玩家赢了，则返回
    1，如果另一个玩家赢了，则返回 –1，如果没有达到结果，则返回 0。
- en: Listing 13.21\. Playing until the `rollout_limit` is reached
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.21\. 继续游戏直到达到 `rollout_limit`
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: With all the work you put into developing the `Agent` framework and implementing
    an AlphaGo agent, you can now use an `AlphaGoMCTS` instance to play games easily.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在你投入大量工作开发 `Agent` 框架并实现 AlphaGo 代理之后，你现在可以使用 `AlphaGoMCTS` 实例轻松地玩游戏。
- en: Listing 13.22\. Initializing an AlphaGo agent with three deep neural networks
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 13.22\. 使用三个深度神经网络初始化 AlphaGo 代理
- en: '[PRE21]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This agent can be used in the exact same way as all the other agents you’ve
    developed in [chapters 7](kindle_split_019.xhtml#ch07) to [12](kindle_split_024.xhtml#ch12).
    In particular, you can register HTTP and GTP frontends for this agent, as you
    did in [chapter 8](kindle_split_020.xhtml#ch08). This makes it possible to play
    against your AlphaGo bot, let other bots play against it, or even register and
    run it on an online Go server (such as OGS, as shown in [appendix E](kindle_split_032.xhtml#app05)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代理可以像你在第 7 章（kindle_split_019.xhtml#ch07）到第 12 章（kindle_split_024.xhtml#ch12）中开发的其它所有代理一样使用。特别是，你可以为这个代理注册
    HTTP 和 GTP 前端，就像你在第 8 章（kindle_split_020.xhtml#ch08）中所做的那样。这使得你可以与你的 AlphaGo 机器人玩游戏，让其他机器人与之对抗，甚至可以在在线围棋服务器（如附录
    E 中所示的 OGS）上注册和运行它。
- en: 13.5\. Practical considerations for training your own AlphaGo
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5. 训练自己的AlphaGo的实际考虑因素
- en: 'In the preceding section, you developed a rudimentary version of the tree-search
    algorithm used in AlphaGo. This algorithm *can* lead to a superhuman level of
    Go game play, but you need to read the fine print to get there. You need to not
    only ensure that you do a good job at training all three deep neural networks
    used in AlphaGo, but also ensure that the simulations in tree search are run fast
    enough, so you don’t have to wait hours on end for AlphaGo to suggest the next
    move. Here are a few pointers for you to make the most of it:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你开发了一个AlphaGo使用的树搜索算法的初步版本。这个算法*可以*达到超凡的围棋游戏水平，但你需要仔细阅读说明才能达到那个水平。你需要不仅确保你训练好AlphaGo中使用的所有三个深度神经网络，还要确保树搜索中的模拟运行得足够快，这样你就不必连续等待数小时等待AlphaGo提出下一步棋。以下是一些帮助你充分利用它的提示：
- en: The first step of training, supervised learning of policy networks, was run
    on a corpus of 160,000 games from KGS, translating into about 30 million game
    states. In total, DeepMind’s AlphaGo team computed 340 million training steps.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的第一步，策略网络的监督学习，是在160,000场KGS比赛语料库上进行的，这相当于大约3000万个游戏状态。DeepMind的AlphaGo团队总共计算了3.4亿个训练步骤。
- en: The good news is that you have access to the exact same data set; DeepMind used
    the KGS training set we introduced in [chapter 7](kindle_split_019.xhtml#ch07)
    as well. In principle, nothing stops you from running the same number of training
    steps. The bad news is that even if you have a state-of-the-art GPU, the training
    process may take many months, if not years.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好消息是，你可以访问完全相同的数据集；DeepMind使用了我们在[第7章](kindle_split_019.xhtml#ch07)中介绍过的KGS训练集。原则上，没有任何东西阻止你运行相同数量的训练步骤。坏消息是，即使你拥有最先进的GPU，训练过程可能需要数月甚至数年。
- en: The AlphaGo team addressed this issue by *distributing* training across 50 GPUs,
    reducing training time to three weeks. This is unlikely an option for you, particularly
    because we haven’t discussed how to train deep networks in a distributed way.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo团队通过在50个GPU上*分配*训练任务，将训练时间缩短至三周来解决这一问题。这对你来说可能不是一个可行的选项，尤其是因为我们还没有讨论如何以分布式方式训练深度网络。
- en: What you can do to come up with satisfying results is scale down each part of
    the equation. Use one of the board encoders from [chapter 7](kindle_split_019.xhtml#ch07)
    or 8 and use much smaller networks than the AlphaGo policy and value networks
    introduced in this chapter. Also, start with a small training set first, so you
    get a feeling for the training process.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了得到令人满意的结果，你可以缩小方程的每一部分。使用[第7章](kindle_split_019.xhtml#ch07)或第8章中的某个棋盘编码器，并使用比本章中介绍的AlphaGo策略网络和价值网络小得多的网络。此外，首先从一个小的训练集开始，这样你就能对训练过程有一个感觉。
- en: In self-play, DeepMind generated 30 million distinct positions. This is vastly
    more than you can realistically hope to create. As a rule of thumb, try to generate
    as many self-play positions as human game positions from supervised learning.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自我对弈中，DeepMind生成了3000万个不同的位置。这远远超过你实际希望创造的。一般来说，尝试生成尽可能多的自我对弈位置，就像从监督学习中生成的人类游戏位置一样。
- en: If you simply take the large networks laid out in this chapter and train them
    on very little data, you’ll likely end up worse than running a smaller network
    on more data.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你只是简单地使用本章中介绍的大型网络，并在非常少的数据上进行训练，你很可能会比在更多数据上运行较小的网络效果更差。
- en: The fast policy network is used frequently in rollouts, so to speed up tree
    search, make sure your fast policy is really small in the beginning, such as the
    networks you used in [chapter 6](kindle_split_018.xhtml#ch06).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模拟中，快速策略网络被频繁使用，所以为了加快树搜索速度，确保你的快速策略一开始就非常小，例如[第6章](kindle_split_018.xhtml#ch06)中使用的网络。
- en: The tree-search algorithm you implemented in the preceding section computes
    simulations *sequentially*. To speed up the process, DeepMind *parallelized* search
    and used a total of 40 search threads. In this parallel version, multiple GPUs
    were used to evaluate deep networks in parallel, and multiple CPUs were used to
    carry out the other parts of the tree search.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在上一节中实现的树搜索算法是*顺序*计算模拟的。为了加快这个过程，DeepMind对搜索进行了并行化，并使用了总共40个搜索线程。在这个并行版本中，多个GPU被用来并行评估深度网络，而多个CPU被用来执行树搜索的其他部分。
- en: Running tree search on multiple CPUs is feasible in principle (recall that you
    used multithreading for data preparation in [chapter 7](kindle_split_019.xhtml#ch07)
    as well), but is a little too involved to cover here.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个CPU上运行树搜索在原则上是可以实现的（回想一下，你在[第7章](kindle_split_019.xhtml#ch07)中也是使用了多线程进行数据准备），但内容过于复杂，这里不便详细展开。
- en: What you can do to improve the game-play experience, trading speed for strength,
    is to reduce the number of simulations run and the search depth used in search.
    This won’t lead to superhuman performance, but at least the system becomes playable.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过减少模拟次数和搜索深度来提高游戏体验，以速度换取强度。这不会导致超凡的表现，但至少系统变得可玩。
- en: As you can see from these points, although the method of combining supervised
    and reinforcement learning with tree search in this novel way was an impressive
    feat, the engineering effort that went into scaling network training, evaluation,
    and tree search deserves its fair share of the credit in building the first Go
    bot to play better than top professionals.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，虽然将监督学习和强化学习与树搜索以这种新颖的方式相结合是一种令人印象深刻的成就，但将网络训练、评估和树搜索扩展到规模所需的工程努力，在构建第一个能比顶尖职业选手玩得更好的围棋机器人方面，也应得到应有的认可。
- en: In the last chapter, you’ll see the next development stage of the AlphaGo system.
    It not only skips the supervised learning from human game records, but *plays
    even stronger* than the original AlphaGo system implemented in this chapter.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你将看到AlphaGo系统的下一个发展阶段。它不仅跳过了从人类游戏记录中进行的监督学习，而且比本章实现的原始AlphaGo系统表现得更加出色。
- en: 13.6\. Summary
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.6. 概述
- en: 'To power an AlphaGo system, you have to train three deep neural networks: two
    policy networks and a value network.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了驱动AlphaGo系统，你必须训练三个深度神经网络：两个策略网络和一个价值网络。
- en: The fast policy network is trained from human game-play data and has to be quick
    enough to run many rollouts in AlphaGo’s tree-search algorithm. The outcome of
    rollouts is used to evaluate leaf positions.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速策略网络从人类游戏数据中训练，并且必须足够快，以便在AlphaGo的树搜索算法中运行多次滚动。滚动结果用于评估叶位置。
- en: The strong policy network is first trained on human data and then improved with
    self-play, using the policy gradient algorithm. You use this network in AlphaGo
    to compute prior probabilities for node selection.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强策略网络首先在人类数据上训练，然后通过自我对弈和策略梯度算法进行改进。你使用这个网络在AlphaGo中计算节点选择的先验概率。
- en: The value network is trained on experience data generated from self-play and
    used for position evaluation for leaf nodes, together with policy rollouts.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值网络在自我对弈生成的经验数据上训练，用于评估叶节点的位置，并与策略滚动一起使用。
- en: Selecting a move with AlphaGo means generating numerous simulations, traversing
    the game tree. After the simulation step is completed, the most visited node is
    selected.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AlphaGo选择走法意味着生成大量模拟，遍历游戏树。在模拟步骤完成后，选择访问次数最多的节点。
- en: In the simulation, nodes are *selected* by maximizing Q-values plus utility.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模拟中，节点是通过最大化Q值加上效用值来选择的。
- en: When a leaf is reached, a node gets *expanded* by using the strong policy for
    prior probabilities.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当达到叶节点时，节点通过使用强策略来扩展先验概率。
- en: A leaf node is evaluated by a combined value function, mixing the output of
    the value network with the outcome of a fast policy rollout.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶节点通过一个组合价值函数进行评估，该函数将价值网络的输出与快速策略滚动的结果混合。
- en: In the backup phase of the algorithm, visit counts, Q-values, and utility values
    receive updates according to the chosen actions.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在算法的备份阶段，访问次数、Q值和效用值根据所选动作进行更新。
- en: 'Chapter 14\. AlphaGo Zero: Integrating tree search with reinforcement learning'
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第14章. AlphaGo Zero：将树搜索与强化学习集成
- en: '*This chapter covers*'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Playing games with a variation on Monte Carlo tree search
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛树搜索的变体玩游戏
- en: Integrating tree search into self-play for reinforcement learning
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将树搜索集成到自我对弈的强化学习中
- en: Training a neural network to enhance a tree-search algorithm
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络以增强树搜索算法
- en: After DeepMind revealed the second edition of AlphaGo, code-named *Master*,
    Go fans all over the world scrutinized its shocking style of play. Master’s games
    were full of surprising new moves. Although Master was bootstrapped from human
    games, it was continuously enhanced with reinforcement learning, and that enabled
    it to discover new moves that humans didn’t play.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DeepMind 公布了 AlphaGo 的第二个版本，代号为 *Master* 之后，全世界的围棋爱好者都仔细审视了其令人震惊的棋风。Master
    的比赛充满了令人惊讶的新走法。尽管 Master 是从人类游戏中启动的，但它通过强化学习不断得到增强，这使得它能够发现人类未曾玩过的走法。
- en: 'This led to an obvious question: what if AlphaGo didn’t rely on human games
    at all, but instead learned entirely using reinforcement learning? Could it still
    reach a superhuman level, or would it get stuck playing with beginners? Would
    it rediscover patterns played by human masters, or would it play in an incomprehensible
    new alien style? All these questions were answered when AlphaGo Zero (AGZ) was
    announced in 2017.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个明显的问题：如果 AlphaGo 完全不依赖人类游戏，而是完全使用强化学习来学习，它是否还能达到超人类水平，或者它会陷入与初学者的对局？它会重新发现人类大师所玩过的模式，还是会以一种难以理解的新外星风格进行游戏？当
    AlphaGo Zero（AGZ）在 2017 年宣布时，所有这些问题都得到了解答。
- en: AlphaGo Zero was built on an improved reinforcement-learning system, and it
    trained itself from scratch without any input from human games. Although its first
    games were worse than any human beginner’s, AGZ improved steadily and quickly
    surpassed every previous edition of AlphaGo.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo Zero 是基于一个改进的强化学习系统构建的，并且它从零开始自我训练，没有任何来自人类游戏的输入。尽管它的第一场比赛比任何人类初学者的比赛都要差，但
    AGZ 不断稳步提升，并迅速超越了 AlphaGo 的所有先前版本。
- en: To us, the most astonishing thing about AlphaGo Zero is how it does more with
    less. In many ways, AGZ is much simpler than the original AlphaGo. No more handcrafted
    feature planes. No more human game records. No more Monte Carlo rollouts. Instead
    of two neural networks and three training processes, AlphaGo Zero used one neural
    network and one training process.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，AlphaGo Zero 最令人惊讶的事情是它如何以更少的资源做更多的事情。在许多方面，AGZ 比原始的 AlphaGo 简单得多。不再有手工制作的特征平面。不再有人类游戏记录。不再有蒙特卡洛模拟。AlphaGo
    Zero 使用一个神经网络和一个训练过程，而不是两个神经网络和三个训练过程。
- en: And yet AlphaGo Zero was stronger than the original AlphaGo! How is that possible?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，AlphaGo Zero 比原始的 AlphaGo 更强大！这是如何实现的？
- en: First, AGZ used a truly massive neural network. The strongest version ran on
    a network with capacity roughly equivalent to 80 convolution layers—over four
    times the size of the original AlphaGo network.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，AGZ 使用了一个真正庞大的神经网络。最强版本运行在一个容量大约相当于 80 个卷积层的网络上——是原始 AlphaGo 网络的四倍以上。
- en: Second, AGZ used an innovative new reinforcement-learning technique. The original
    AlphaGo trained its policy network alone, in a manner similar to what we described
    in [chapter 10](kindle_split_022.xhtml#ch10); later that policy network was used
    to improve tree search. In contrast, AGZ integrated tree search with reinforcement
    learning from the beginning. This algorithm is the focus of this chapter.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，AGZ 使用了一种创新的新的强化学习技术。原始的 AlphaGo 单独训练其策略网络，方式与我们第 10 章（kindle_split_022.xhtml#ch10）中描述的类似；后来该策略网络被用来改进树搜索。相比之下，AGZ
    从一开始就将树搜索与强化学习相结合。这个算法是本章的重点。
- en: To start, we go over the structure of the neural network that AGZ trains. Next,
    we describe the tree-search algorithm in depth. AGZ uses the same tree search
    in both self-play and competitive games. After that, we cover how AGZ trains its
    network from its experience data. To wrap up, we briefly cover a few practical
    tricks that AGZ uses to make the training process stable and efficient.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们概述 AGZ 训练的神经网络结构。接下来，我们深入描述树搜索算法。AGZ 在自我对局和竞技比赛中都使用相同的树搜索。然后，我们介绍 AGZ 如何从其经验数据中训练其网络。最后，我们简要介绍一些
    AGZ 用来使训练过程稳定和高效的实用技巧。
- en: 14.1\. Building a neural network for tree search
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1. 构建用于树搜索的神经网络
- en: 'AlphaGo Zero uses a single neural network with one input and two outputs: one
    output produces a probability distribution over moves, and the other output produces
    a single value representing whether the game favors white or black. This is the
    same structure we used for actor-critic learning in [chapter 12](kindle_split_024.xhtml#ch12).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo Zero 使用了一个具有一个输入和两个输出的单一神经网络：一个输出产生对移动的概率分布，另一个输出产生一个代表游戏是否有利于白方或黑方的单一值。这正是我们在第
    12 章（kindle_split_024.xhtml#ch12）中使用的演员-评论家学习结构。
- en: There’s one small difference between the output of the AGZ network and the network
    we used in [chapter 12](kindle_split_024.xhtml#ch12), and the difference is around
    passing in the game. In previous cases where we implemented self-play, we hardcoded
    logic around passing to end the game. For example, the `PolicyAgent` self-play
    bot from [chapter 9](kindle_split_021.xhtml#ch09) included custom logic so that
    it wouldn’t fill in its own eyes, thereby killing its own stones. If the only
    legal moves were self-destructive, the `PolicyAgent` would pass. This ensured
    that self-play games ended in a sensible place.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: AGZ 网络的输出与我们第 12 章中使用的网络输出之间有一个小的差异，差异在于游戏中的“过”。在之前我们实现自我对弈的案例中，我们硬编码了关于“过”的逻辑以结束游戏。例如，第
    9 章中的 `PolicyAgent` 自我对弈机器人包含了自定义逻辑，这样它就不会填满自己的眼位，从而杀死自己的棋子。如果唯一的合法移动是自我毁灭性的，`PolicyAgent`
    将会“过”。这确保了自我对弈游戏以一个合理的位置结束。
- en: Because AGZ uses a tree search during self-play, you don’t need that custom
    logic. You can treat a pass the same as any other move, and expect the bot to
    learn when passing is appropriate. If the tree search reveals that playing a stone
    will lose the game, it’ll pick a pass instead. This means your action output needs
    to return a probability for passing along with every point on the board. Instead
    of returning a vector of size 19 × 19 = 361 to represent each point on the board,
    your network will produce a vector of size 19 × 19 + 1 = 362 to represent each
    point on the board *and* the pass move. [Figure 14.1](#ch14fig01) illustrates
    this new move encoding.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 AGZ 在自我对弈时使用树搜索，所以你不需要那种自定义逻辑。你可以将“过”视为与其他任何移动一样，并期望机器人学习何时应该“过”。如果树搜索显示下棋会输掉游戏，它将选择“过”。这意味着你的动作输出需要为每个棋盘上的点返回一个“过”的概率。而不是返回一个表示棋盘上每个点的
    19 × 19 = 361 大小的向量，你的网络将生成一个 19 × 19 + 1 = 362 大小的向量来表示棋盘上的每个点以及“过”的移动。[图 14.1](#ch14fig01)
    展示了这种新的移动编码。
- en: 'Figure 14.1\. Encoding possible moves as a vector. As in previous chapters,
    AGZ uses a vector where each element maps to a point on the game board. AGZ adds
    one last element that maps to the pass move. This example is on a 5 × 5 board,
    so the vector has dimension 26: 25 for the points on the board and 1 for passing.'
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.1\. 将可能的移动编码为向量。与之前的章节一样，AGZ 使用一个向量，其中每个元素映射到游戏板上的一个点。AGZ 添加了一个映射到“过”移动的最后一个元素。这个例子是在一个
    5 × 5 的棋盘上，所以向量维度为 26：25 个表示棋盘上的点，1 个表示“过”。
- en: '![](Images/14fig01.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/14fig01.jpg)'
- en: This means you have to modify the board encoder slightly. In previous board
    encoders, you implemented `encode_point` and `decode_point_index`, which translated
    between the elements of the vector and points on the board. For an AGZ-style bot,
    you’ll replace these with new functions, `encode_move` and `decode_move_index`.
    The encoding for playing a stone remains the same; you use the next index to represent
    a pass.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你必须稍微修改棋盘编码器。在之前的棋盘编码器中，你实现了 `encode_point` 和 `decode_point_index`，它们在向量的元素和棋盘上的点之间进行转换。对于
    AGZ 风格的机器人，你将用新的函数 `encode_move` 和 `decode_move_index` 替换这些函数。下棋的编码保持不变；你使用下一个索引来表示“过”。
- en: Listing 14.1\. Modifying the board encoder to include passing
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.1\. 修改棋盘编码器以包含“过”
- en: '[PRE22]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1* Same point encoding as used in previous encoders**'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 与之前编码器使用的相同点编码**'
- en: '***2* Uses the next index to represent a pass**'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用下一个索引来表示“过”**'
- en: '***3* The neural network doesn’t learn resignation.**'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 神经网络不会学习认输。**'
- en: Apart from the treatment of passing, the input and output of the AGZ network
    are identical to what we covered in [chapter 12](kindle_split_024.xhtml#ch12).
    For the inner layers of the network, AGZ used an extremely deep stack of convolution
    layers, with a few modern enhancements to make training smoother (we cover those
    briefly at the end of this chapter). A large network is powerful, but it also
    requires more computation, both for training and self-play. If you don’t have
    access to the same kind of hardware as DeepMind, you may have better luck with
    a smaller network. Feel free to experiment to find the right balance of power
    and speed for your needs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对“过”的处理外，AGZ 网络的输入和输出与我们第 12 章中介绍的内容相同。对于网络的内部层，AGZ 使用了极其深的卷积层堆叠，并添加了一些现代改进以使训练更加平滑（我们将在本章末尾简要介绍这些改进）。大型网络功能强大，但也需要更多的计算，无论是训练还是自我对弈。如果你没有像
    DeepMind 那样的硬件，你可能用较小的网络会有更好的运气。请随意实验，以找到满足你需求的功率和速度的最佳平衡。
- en: 'As for board encoding, you could use any encoding scheme we covered in this
    book, from the basic encoder in [chapter 6](kindle_split_018.xhtml#ch06) up to
    the 48-plane encoder in [chapter 13](kindle_split_026.xhtml#ch13). AlphaGo Zero
    used the simplest possible encoder: just the location of the black and white stones
    on the board, plus a plane indicating whose turn it is. (To handle ko, AGZ also
    included planes for the previous seven board positions.) But there’s no technical
    reason why you can’t use game-specific feature planes, and it’s possible they’d
    make learning faster. In part, the researchers wanted to remove as much human
    knowledge as they could just to prove it was possible. In your own experiments,
    you should feel free to try different combinations of feature planes while using
    the AGZ reinforcement-learning algorithm.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 至于棋盘编码，您可以使用本书中提到的任何编码方案，从第6章中的基本编码器到第13章中的48平面编码器。AlphaGo Zero使用了最简单的编码器：只是棋盘上黑白棋子的位置，加上一个表示轮到谁的平面。（为了处理劫争，AGZ还包括了前七个棋盘位置的平面。）但是，没有技术理由说明您不能使用特定于游戏的特征平面，而且它们可能会使学习更快。在某种程度上，研究人员想要尽可能多地移除人类知识，只是为了证明这是可能的。在您自己的实验中，您应该可以自由尝试使用AGZ强化学习算法的不同特征平面组合。
- en: 14.2\. Guiding tree search with a neural network
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2. 使用神经网络引导树搜索
- en: 'In reinforcement learning, a *policy* is an algorithm that tells an agent how
    to make decisions. In previous examples of reinforcement learning, the policy
    was relatively simple. In policy gradient learning ([chapter 10](kindle_split_022.xhtml#ch10))
    and actor-critic learning ([chapter 12](kindle_split_024.xhtml#ch12)), a neural
    network directly told you which move to pick: that’s the policy. In Q-learning
    ([chapter 11](kindle_split_023.xhtml#ch11)), the policy involved computing the
    Q-value for each possible move; then you picked the move with the highest Q.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一个*策略*是一个算法，它告诉智能体如何做出决策。在之前的强化学习示例中，策略相对简单。在策略梯度学习（[第10章](kindle_split_022.xhtml#ch10)）和演员-评论家学习（[第12章](kindle_split_024.xhtml#ch12)）中，神经网络直接告诉您选择哪个走法：这就是策略。在Q学习（[第11章](kindle_split_023.xhtml#ch11)）中，策略涉及到计算每个可能走法的Q值；然后您选择Q值最高的走法。
- en: AGZ’s policy includes a form of tree search. You’ll still use a neural network,
    but the purpose of the neural network is to guide the tree search, rather than
    to choose or evaluate moves directly. Including tree search during self-play means
    the self-play games are more realistic. In turn, that means the training process
    is more stable.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: AGZ的策略包括一种树搜索形式。您仍然会使用神经网络，但神经网络的目的在于引导树搜索，而不是直接选择或评估走法。在自我对弈中包含树搜索意味着自我对弈的游戏更加真实。反过来，这意味着训练过程更加稳定。
- en: The tree-search algorithm builds on ideas you’ve already studied. If you’ve
    studied the Monte Carlo tree-search algorithm ([chapter 4](kindle_split_016.xhtml#ch04))
    and the original AlphaGo ([chapter 13](kindle_split_026.xhtml#ch13)), the AlphaGo
    Zero tree-search algorithm will seem familiar; [table 14.1](#ch14table01) compares
    the three algorithms. First, we’ll describe the data structure that AGZ uses to
    represent a game tree. Next, we’ll walk through the algorithm that AGZ uses to
    add a new position to the game tree.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 树搜索算法建立在您已经学习过的想法之上。如果您已经学习了蒙特卡洛树搜索算法（[第4章](kindle_split_016.xhtml#ch04)）和原始AlphaGo（[第13章](kindle_split_026.xhtml#ch13)），那么AlphaGo
    Zero的树搜索算法将看起来很熟悉；[表14.1](#ch14table01)比较了这三种算法。首先，我们将描述AGZ用来表示游戏树的数据结构。接下来，我们将介绍AGZ用来向游戏树添加新位置的计算方法。
- en: Table 14.1\. Comparing tree-search algorithms
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表14.1. 比较树搜索算法
- en: '|   | MCTS | AlphaGo | AlphaGo Zero |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|   | MCTS | AlphaGo | AlphaGo Zero |'
- en: '| --- | --- | --- | --- |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Branch selection** | UCT score | UCT score + prior from policy network
    | UCT score + prior from combined network |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| **分支选择** | UCT分数 | UCT分数 + 策略网络的先验 | UCT分数 + 结合网络的先验 |'
- en: '| **Branch evaluation** | Randomized playouts | Value network + randomized
    playouts | Value from combined network |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| **分支评估** | 随机模拟 | 值网络 + 随机模拟 | 结合网络的值 |'
- en: The general idea of tree-search algorithms, as they apply to board games, is
    to find the move that leads to the best outcome. You determine that by examining
    possible sequences of moves that might follow. But the number of possible sequences
    is beyond enormous, so you need to make a decision while examining only a tiny
    fraction of the possible sequences. The art and science of tree-search algorithms
    is in how to pick the branches to explore in order to get the best result in the
    least time.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 树搜索算法的一般思想，如应用于棋类游戏，是找到导致最佳结果的一步棋。你通过检查可能跟随的移动序列来确定这一点。但是，可能的序列数量是巨大的，因此你需要在检查可能的序列的一小部分时做出决定。树搜索算法的艺术和科学在于如何选择要探索的分支，以便在尽可能短的时间内获得最佳结果。
- en: 'Just as in MCTS, the AGZ tree-search algorithm runs for a certain number of
    rounds, and in each round it adds another board position to the tree. As you execute
    more and more rounds, the tree continues to grow larger, and the algorithm’s estimates
    get more accurate. For the purposes of illustration, imagine that you’re already
    in the middle of the algorithm: you’ve already built up a partial tree, and you
    want to expand the tree with a new board position. [Figure 14.2](#ch14fig02) shows
    such an example game tree.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在MCTS中，AGZ树搜索算法运行一定数量的回合，并且在每个回合中，它将另一个棋盘位置添加到树中。随着你执行越来越多的回合，树继续变大，算法的估计变得更加准确。为了说明目的，想象你已经处于算法的中间：你已经建立了一个部分树，并且你想用一个新的棋盘位置扩展树。[图14.2](#ch14fig02)显示了这样一个示例游戏树。
- en: Figure 14.2\. A partial AGZ-style search tree. In this case, it’s black’s turn
    to move, and the search has explored three possible game states so far. The tree
    also contains branches that represent moves the search hasn’t yet visited; most
    of those have been omitted for space.
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.2\. 一种部分AGZ风格的搜索树。在这种情况下，轮到黑方移动，搜索已经探索了三种可能的游戏状态。树还包含代表搜索尚未访问的移动的分支；为了节省空间，大多数这些分支都被省略了。
- en: '![](Images/14fig02_alt.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/14fig02_alt.jpg)'
- en: 'Each node in the game tree represents a possible board position. From that
    position, you also know which follow-up moves are legal. The algorithm has already
    visited some of those follow-up moves, but not all of them. You create a *branch*
    for each follow-up move, whether you’ve visited or not. Each branch tracks the
    following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏树中的每个节点代表一个可能的棋盘位置。从该位置，你也知道哪些后续移动是合法的。算法已经访问了一些后续移动，但并非全部。对于每个后续移动，无论你是否访问过，你都会创建一个*分支*。每个分支跟踪以下内容：
- en: A *prior* probability of the move, indicating how good you expect this move
    to be, before you try visiting it.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动的*先验概率*，表示在尝试访问之前你期望这个移动有多好。
- en: The number of times you’ve visited the branch during the tree search. This may
    be zero.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在树搜索期间你访问该分支的次数。这可能为零。
- en: The *expected value* of all visits that passed through this branch. This is
    an average over all visits through the tree. To make updating this average easier,
    you store the sum of the values; you can then divide by the visit count to get
    the average.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有通过此分支的访问的*期望值*。这是对所有通过树的访问的平均值。为了使更新这个平均值更容易，你存储值的总和；然后你可以除以访问次数来得到平均值。
- en: For each branch that you *have* visited, the node also contains a pointer to
    a *child node*. In the following listing, you define a minimal `Branch` structure
    to contain branch statistics.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你*已经访问过的每个分支*，节点还包含一个指向*子节点*的指针。在以下列表中，你定义了一个最小的`Branch`结构来包含分支统计信息。
- en: Listing 14.2\. A structure to track branch statistics
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.2\. 跟踪分支统计的结构
- en: '[PRE23]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now you’re ready to build the structure that represents the search tree. The
    following listing defines a `ZeroTreeNode` class.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好构建表示搜索树的结构的结构了。以下列表定义了一个`ZeroTreeNode`类。
- en: Listing 14.3\. A node in an AGZ-style search tree
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.3\. AGZ风格搜索树中的一个节点
- en: '[PRE24]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* In the root of the tree, parent and last_move will be None.**'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在树的根节点中，parent和last_move将是None。**'
- en: '***2* Later, children will map from a Move to another ZeroTreeNode.**'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 之后，子节点将从一个移动映射到另一个ZeroTreeNode。**'
- en: '***3* Returns a list of all possible moves from this node**'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 返回从该节点出发的所有可能的移动**'
- en: '***4* Allows adding new nodes into the tree**'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 允许将新节点添加到树中**'
- en: '***5* Checks whether there’s a child node for a particular move**'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 检查是否存在特定移动的子节点**'
- en: '***6* Returns a particular child node**'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 返回特定的子节点**'
- en: The `ZeroTreeNode` class also includes some helpers for reading the statistics
    off its children.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`ZeroTreeNode`类还包括一些辅助函数，用于从其子节点读取统计数据。'
- en: Listing 14.4\. Helpers to read branch information from a tree node
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.4. 从树节点读取分支信息的辅助函数
- en: '[PRE25]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 14.2.1\. Walking down the tree
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.1. 向下走树
- en: In each round of the search, you start by walking down the tree. The point is
    to see what a possible future board position is, so you can evaluate whether it’s
    good. To get an accurate evaluation, you should assume that your opponent will
    respond to your moves in the strongest possible way. Of course, you don’t know
    what the strongest response is yet; you must try out a variety of moves to find
    out which are good. This section describes an algorithm for selecting strong moves
    in the face of uncertainty.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索的每一轮中，你首先从树中向下走。目的是看看可能的未来棋盘位置是什么，以便评估它是否好。为了获得准确的评估，你应该假设你的对手将以最强烈的方式对你的移动做出回应。当然，你还不确定最强的回应是什么；你必须尝试各种移动来找出哪些是好的。本节描述了一个在不确定性面前选择强移动的算法。
- en: The expected value provides an estimate of how good each possible move is. But
    the estimates aren’t equally accurate. If you’ve spent more time reading out a
    particular branch, its estimate will be better.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 期望值提供了对每个可能移动的好坏的估计。但这些估计并不完全准确。如果你在某个特定分支上花费了更多时间阅读，其估计将更好。
- en: You can continue to read out one of the best variations in more detail, which
    will further improve its estimate. Alternately, you can read out a branch that
    you’ve explored less, in order to improve your estimate. Maybe that move is better
    than you initially thought; the only way to find out is expanding it further.
    Once again, you see the opposing goals of *exploitation* and *exploration*.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续更详细地阅读其中一种最佳变化，这将进一步提高其估计。或者，你可以阅读你探索较少的分支，以改善你的估计。也许这个移动比你最初想象的要好；唯一找到的方法是进一步扩展它。再次，你看到了*利用*和*探索*的对立目标。
- en: 'The original MCTS algorithm used the UCT (upper confidence bound for trees;
    see [chapter 4](kindle_split_016.xhtml#ch04)) formula to balance these goals.
    The UCT formula balanced two priorities:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的MCTS算法使用了UCT（树的置信上限；见第4章[chapter 4](kindle_split_016.xhtml#ch04)）公式来平衡这些目标。UCT公式平衡了两个优先级：
- en: If you’ve visited a branch many times, you trust its expected value. In that
    case, you prefer the branches with higher estimated values.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你多次访问过某个分支，你会信任它的期望值。在这种情况下，你更倾向于选择具有更高估计值的分支。
- en: If you’ve visited a branch only a few times, its expected value may be way off.
    Whether its expected value is good or bad, you want to visit it a few times to
    improve its estimate.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你只访问过某个分支几次，其期望值可能相差甚远。无论其期望值是好是坏，你都想访问它几次以提高其估计。
- en: 'AGZ adds a third factor:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: AGZ添加了第三个因素：
- en: Among branches with few visits, you prefer the ones with a high prior. Those
    are the moves that intuitively look good, before considering the exact details
    of this game.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在访问次数较少的分支中，你更喜欢那些具有高先验概率的分支。这些是在考虑游戏的详细细节之前直观上看起来很好的移动。
- en: 'Mathematically, AGZ’s scoring function looks like this:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，AGZ的评分函数看起来是这样的：
- en: '![](Images/p0304_01.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0304_01.jpg)'
- en: 'The parts of the equation are as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式的各个部分如下：
- en: '*Q* is the expected value averaged over all visits through a branch. (It’s
    zero if you haven’t visited the branch yet.)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q* 是通过分支访问的所有访问的平均期望值。（如果你还没有访问过该分支，则为零。）'
- en: '*P* is the prior for the move under consideration.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P* 是考虑中的移动的先验概率。'
- en: '*N* is the number of visits to the *parent* node.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 是访问父节点的次数。'
- en: '*n* is the number of visits to the *child* branch.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是访问子分支的次数。'
- en: '*c* is a factor that balances exploration against exploitation (generally,
    you have to set this by trial and error).'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* 是平衡探索和利用的一个因素（通常，你必须通过试错来设置这个值）。'
- en: 'Look at the example in [figure 14.3](#ch14fig03). Branch A has been visited
    twice and has a slightly good evaluation of Q = 0.1\. Branch B has been visited
    once and has a bad evaluation: Q = –0.5\. Branch C has no visits yet but has a
    prior probability of P = 0.038.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 看看[图14.3](#ch14fig03)的例子。分支A已被访问两次，Q = 0.1的评估略好。分支B已被访问一次，评估不好：Q = –0.5。分支C尚未访问，但先验概率为P
    = 0.038。
- en: Figure 14.3\. Choosing which branch to follow in the AGZ tree search. In this
    example, you’re considering three branches from the starting position. (In reality,
    there would be many more possible moves, which we omitted for space.) To choose
    a branch, you consider the number of times you’ve already visited the branch,
    your estimated value for the branch, and the prior probability of the move.
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.3\. 在 AGZ 树搜索中选择要跟随的分支。在这个例子中，你正在考虑从起始位置出发的三条分支。（实际上，会有更多的可能移动，但我们为了节省空间而省略了。）为了选择一个分支，你考虑你已经访问该分支的次数、对该分支的估计值以及移动的先验概率。
- en: '![](Images/14fig03_alt.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/14fig03_alt.jpg)'
- en: '[Table 14.2](#ch14table02) shows how to calculate the uncertainty component.
    Branch A has the highest Q component, indicating that you’ve seen some good board
    positions underneath it. Branch C has the highest UCT component: we’ve never visited,
    so we have the highest uncertainty around that branch. Branch B has a lower evaluation
    than A, and more visits than C, so it’s not likely to be a good choice at this
    point.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 14.2](#ch14table02) 展示了如何计算不确定性成分。分支 A 具有最高的 Q 成分，表明你已经在它下面看到了一些好的棋盘位置。分支
    C 具有最高的 UCT 成分：我们从未访问过，所以我们对该分支的不确定性最高。分支 B 的评估低于 A，访问次数多于 C，所以在这个时候它不太可能是一个好的选择。'
- en: Table 14.2\. Choosing a branch to follow
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 14.2\. 选择要跟随的分支
- en: '|   | Q | n | N | P | P√N / (n + 1) |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|   | Q | n | N | P√N / (n + 1) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **Branch A** | 0.1 | 2 | 3 | 0.068 | 0.039 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| **分支 A** | 0.1 | 2 | 3 | 0.068 | 0.039 |'
- en: '| **Branch B** | –0.5 | 1 | 3 | 0.042 | 0.036 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| **分支 B** | –0.5 | 1 | 3 | 0.042 | 0.036 |'
- en: '| **Branch C** | 0 | 0 | 3 | 0.038 | 0.065 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| **分支 C** | 0 | 0 | 3 | 0.038 | 0.065 |'
- en: Assuming you’ve eliminated branch B, how do you choose between branch A and
    branch C? It depends on the value of the parameter *c*. A small value of *c* favors
    the high-value branch (in this case, A). A large value of *c* favors the branch
    with most uncertainty (in this case, C). For example, at *c* = 1.0, you’d choose
    A (at a score of 0.139 to 0.065). At *c* = 4.0, you’d choose C (0.260 to 0.256).
    Neither is objectively right; it’s just a trade-off. The following listing shows
    how to calculate this score in Python.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经排除了分支 B，你如何在分支 A 和分支 C 之间进行选择？这取决于参数 *c* 的值。*c* 的较小值倾向于高值分支（在这种情况下，A）。*c*
    的较大值倾向于具有最多不确定性的分支（在这种情况下，C）。例如，在 *c* = 1.0 时，你会选择 A（得分为 0.139 到 0.065）。在 *c*
    = 4.0 时，你会选择 C（0.260 到 0.256）。这都不是客观正确的；这只是权衡。下面的列表展示了如何在 Python 中计算这个分数。
- en: Listing 14.5\. Choosing a child branch
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.5\. 选择子分支
- en: '[PRE26]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '***1* node.moves() is a list of moves. When you pass in key=score_branch, then
    max will return the move with the highest value of the score_branch function.**'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** node.moves() 是一个移动列表。当你传入 key=score_branch 时，max 将返回具有最高 score_branch
    函数值的移动。'
- en: After you’ve chosen a branch, you repeat the same calculation on its children
    to choose the next branch. You continue the same process until you reach a branch
    with no children.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在你选择了一个分支之后，你会在其子节点上重复相同的计算来选择下一个分支。你继续这个过程，直到你到达一个没有子节点的分支。
- en: Listing 14.6\. Walking down the search tree
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.6\. 沿着搜索树下行
- en: '[PRE27]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '***1* The next section shows the implementation of create_node.**'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1*** 下一个部分展示了 create_node 的实现。'
- en: '***2* This is the first step in a process that repeats many times per move.
    self.num_moves controls the number of times you repeat the search process.**'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2*** 这是每次移动中重复多次的过程的第一步。self.num_moves 控制你重复搜索过程的次数。'
- en: '***3* When has_child returns False, you’ve reached the bottom of the tree.**'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3*** 当 has_child 返回 False 时，你已经到达了树的底部。'
- en: 14.2.2\. Expanding the tree
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.2\. 扩展树
- en: At this point, you’ve reached an unexpanded branch of the tree. You can’t search
    any further, because there’s no node in the tree corresponding to the current
    move. The next step is to create a new node and add it to the tree.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你已经到达了树的未扩展分支。你不能进一步搜索，因为没有节点在树中对应当前的移动。下一步是创建一个新的节点并将其添加到树中。
- en: To create a new node, you take the previous game state and apply the current
    move to get a new game state. You can then feed the new game state to your neural
    network, which gives you two valuable things. First, you get the prior estimates
    for all possible follow-up moves from the new game state. Second, you get the
    estimated value of the new game state. You use this information to initialize
    the statistics for the branches from this new node.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新节点，你将前一个游戏状态应用于当前走法以获得一个新的游戏状态。然后，你可以将新的游戏状态输入到你的神经网络中，这会给你两件有价值的事情。首先，你得到从新游戏状态出发的所有可能后续走法的先验估计。其次，你得到新游戏状态的价值估计。你使用这些信息来初始化从这个新节点出发的分支的统计信息。
- en: Listing 14.7\. Creating a new node in the search tree
  id: totrans-337
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.7\. 在搜索树中创建一个新节点
- en: '[PRE28]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '***1* The Keras predict function is a batch function that takes an array of
    examples. You must wrap your board_tensor in an array.**'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* Keras 的 predict 函数是一个批量函数，它接受一个示例数组。你必须将你的 board_tensor 包裹在一个数组中。**'
- en: '***2* Likewise, predict returns arrays with multiple results, so you must pull
    out the first item.**'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 同样，预测具有多个结果的回报数组，因此你必须提取第一个项目。**'
- en: '***3* Unpack the priors vector into a dictionary mapping from Move objects
    to their corresponding prior probabilities.**'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将先验向量拆分为一个字典，将走法对象映射到它们对应的先验概率。**'
- en: Finally, you walk back up the tree and update the statistics for each parent
    that lead to this node, as shown in [figure 14.4](#ch14fig04). For each node in
    this path, you increment the visit count and update the total expected value.
    At each node, the perspective switches between the black player and the white
    player, so you need to flip the sign of the value at each step.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你沿着树向上走，并更新导致此节点的每个父节点的统计信息，如图 [图 14.4](#ch14fig04) 所示。对于路径中的每个节点，你增加访问次数并更新总期望价值。在每个节点，视角会在黑方和白方之间切换，因此你需要在每个步骤中翻转价值的符号。
- en: Listing 14.8\. Expanding the search tree and updating all node statistics
  id: totrans-343
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.8\. 扩展搜索树并更新所有节点统计信息
- en: '[PRE29]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '***1* At each level in the tree, you switch perspective between the two players.
    Therefore, you must multiply the value by –1: what’s good for black is bad for
    white, and vice versa.**'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在树的每个级别上，你会在两个玩家之间切换视角。因此，你必须将值乘以 –1：对黑方有利的就是对白方不利，反之亦然。**'
- en: Figure 14.4\. Expanding an AGZ-style search tree. First, you calculate a new
    game state. You create a new node from that game state and add it to the tree.
    The neural network then gives you a value estimate for that game state. Finally,
    you update all parents of the new node. You increment the visit count *N* by one
    and update the average value *V*. Here, *T* represents the total value across
    all visits through a node; that’s just bookkeeping to make it easy to recalculate
    the average.
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.4\. 扩展 AGZ 风格的搜索树。首先，你计算一个新的游戏状态。从这个游戏状态创建一个新的节点并将其添加到树中。然后，神经网络为你提供该游戏状态的价值估计。最后，你更新新节点的所有父节点的统计信息。你将访问次数
    *N* 增加 1 并更新平均价值 *V*。在这里，*T* 代表通过节点的所有访问的总价值；这只是为了便于重新计算平均值而进行的簿记。
- en: '![](Images/14fig04_alt.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/14fig04_alt.jpg)'
- en: This whole process repeats over and over, and the tree expands each time. AGZ
    used 1,600 rounds per move during the self-play process. In competitive games,
    you should run the algorithm for as many rounds as you have time for. The bot
    will choose better and better moves as it performs more rounds.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程会反复进行，每次都会扩展树。AGZ 在自我对弈过程中每个走法使用了 1,600 轮。在竞技游戏中，你应该运行尽可能多的轮数。随着进行更多的轮数，机器人会选择越来越好的走法。
- en: 14.2.3\. Selecting a move
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.2.3\. 选择走法
- en: After you’ve built up the search tree as deeply as you can, it’s time to select
    a move. The simplest rule for move selection is to pick the move with the highest
    visit count.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在你尽可能深地构建搜索树之后，是时候选择一个走法了。选择走法的最简单规则是选择访问次数最高的走法。
- en: Why use the visit counts and not the expected value? You can assume the branch
    with the most visits has a high expected value. Here’s why. Refer to the preceding
    branch-selection formula. As the number of visits to a branch grows, the factor
    of 1 / (*n* + 1) gets smaller and smaller. Therefore, the branch selection function
    will just pick based on Q. The branches with higher Q values get the most visits.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用访问次数而不是期望值？你可以假设访问次数最多的分支具有高期望值。原因如下。参考前面的分支选择公式。随着对分支的访问次数增加，1 / (*n*
    + 1) 这个因子会越来越小。因此，分支选择函数将仅基于 Q 值进行选择。具有更高 Q 值的分支会获得更多的访问次数。
- en: Now, if a branch has only a few visits, anything’s possible. It may have a small
    Q or a huge Q. But you also can’t trust its estimate based on a small number of
    visits. If you just picked the branch with the highest Q, you might get a branch
    with just one visit, and its true value may be much smaller. That’s why you select
    based on visit counts instead. It guarantees that you pick a branch with a high
    estimated value *and* a reliable estimate.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果一个分支只有少数访问次数，任何情况都是可能的。它可能有一个小的 Q 值或一个巨大的 Q 值。但你也不能基于少量访问次数来信任它的估计。如果你只是选择了具有最高
    Q 值的分支，你可能会得到一个只有一次访问的分支，而它的真实值可能要小得多。这就是为什么你基于访问次数进行选择的原因。这保证了你选择一个具有高估计值并且可靠的估计的分支。
- en: Listing 14.9\. Selecting the move with the highest visit count
  id: totrans-353
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.9\. 选择具有最高访问次数的移动
- en: '[PRE30]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In contrast to other self-play agents in this book, the `ZeroAgent` has no
    special logic around when to pass. That’s because you include passing in the search
    tree: you can treat it just like any other move.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中其他自我对弈代理不同，`ZeroAgent` 在何时传递方面没有特殊的逻辑。这是因为你在搜索树中包含了传递：你可以将其视为任何其他移动。
- en: With our `ZeroAgent` implementation complete, you can now implement your `simulate_game`
    function for self-play.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 `ZeroAgent` 实现完成后，你现在可以为你自己的 `simulate_game` 函数实现自我对弈。
- en: Listing 14.10\. Simulating a self-play game
  id: totrans-357
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.10\. 模拟自我对弈游戏
- en: '[PRE31]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 14.3\. Training
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3\. 训练
- en: The training target for the value output is a 1 if the agent won the game, and
    a –1 if it lost. By averaging over many games, you learn a value between those
    extremes that indicates your bot’s chance of winning. It’s the exact same setup
    you used for Q-learning (in [chapter 11](kindle_split_023.xhtml#ch11)) and for
    actor-critic learning ([chapter 12](kindle_split_024.xhtml#ch12)).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 对于值输出的训练目标是，如果代理赢得了游戏，则为 1，如果它输了，则为 -1。通过平均许多游戏，你可以学习到一个介于这两个极端之间的值，这表明你的机器人获胜的机会。这与你在第
    11 章（[章节 11](kindle_split_023.xhtml#ch11)）中使用的 Q-learning 以及在第 12 章（[章节 12](kindle_split_024.xhtml#ch12)）中使用的演员-评论家学习设置完全相同。
- en: The action output is a little different. Just as in policy learning ([chapter
    10](kindle_split_022.xhtml#ch10)) and actor-critic learning ([chapter 12](kindle_split_024.xhtml#ch12)),
    the neural network has an output that produces a probability distribution over
    legal moves. In policy learning, you trained a network to match the exact move
    that the agent chose (in cases where the agent won the game). AGZ works differently
    in a subtle way. It trains its network to match the number of times it visited
    each move during the tree search.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 动作输出略有不同。正如在策略学习（[章节 10](kindle_split_022.xhtml#ch10)）和演员-评论家学习（[章节 12](kindle_split_024.xhtml#ch12)）中一样，神经网络有一个输出，它产生一个覆盖合法移动的概率分布。在策略学习中，你训练了一个网络来匹配代理选择的精确移动（在代理赢得游戏的情况下）。AGZ
    以一种微妙的方式有所不同。它训练其网络来匹配在树搜索期间每个移动被访问的次数。
- en: 'To illustrate how that can improve its playing strength, think about how the
    MCTS-style search algorithms work. Assume, for the moment, that you have a value
    function that’s at least vaguely correct; it doesn’t have to be super precise
    as long as it roughly separates winning positions from losing positions. Then
    imagine that you throw out the prior probabilities entirely and run the search
    algorithm. By design, the search will spend more time in the most promising branches.
    The branch selection logic makes this happen: the Q component in the UCT formula
    means high-value branches are selected more often. If you had unlimited time to
    run the search, it’d eventually converge on the best moves.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这如何提高其游戏强度，考虑一下 MCTS 风格的搜索算法是如何工作的。暂时假设你有一个至少大致正确的值函数；只要它大致区分了获胜位置和失败位置，它就不需要非常精确。然后想象一下，你完全抛弃了先验概率并运行搜索算法。按照设计，搜索将在最有希望的分支上花费更多时间。分支选择逻辑使得这一点发生：UCT
    公式中的 Q 部分意味着高价值分支被选择得更频繁。如果你有无限的时间来运行搜索，它最终会收敛到最佳移动。
- en: After a sufficient number of rounds in the tree search, you can think of the
    visit counts as the source of truth. You know whether these moves are good or
    bad because you checked what happens if you play them. So the search counts become
    your target values for training the prior function.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在树搜索中进行足够多的回合后，你可以将访问次数视为真理的来源。你知道这些移动是好是坏，因为你检查了如果你玩这些移动会发生什么。因此，搜索计数成为你训练先验函数的目标值。
- en: The prior function tries to predict where the tree search would spend its time,
    if you gave it plenty of time to run. Armed with a function trained on previous
    runs, your tree search can save time and go straight into searching out the more
    important branches. With an accurate prior function, your search algorithm can
    spend just a small number of rollouts, but get similar results to a slower search
    that required a much larger number of rollouts. In a sense, you can think that
    the network is “remembering” what happened in previous searches and using that
    knowledge to skip ahead.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 先验函数试图预测如果给它足够的时间运行，树搜索会在哪里花费时间。如果你使用在先前运行上训练的函数，你的树搜索可以节省时间并直接搜索更重要的分支。具有准确的先验函数，你的搜索算法只需少量模拟，但可以得到与需要大量模拟的较慢搜索相似的结果。从某种意义上说，你可以认为网络“记住”了先前搜索中发生的事情，并使用这些知识来跳过。
- en: To set up training in this way, you need to store the search counts after each
    move. In previous chapters, you used a generic `ExperienceCollector` that could
    apply to many RL implementations. In this case, however, the search counts are
    specific to AGZ, so you’ll make a custom collector. The structure is mostly the
    same.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以这种方式设置训练，你需要存储每次移动后的搜索计数。在之前的章节中，你使用了一个通用的 `ExperienceCollector`，它可以应用于许多强化学习实现。然而，在这种情况下，搜索计数是特定于
    AGZ 的，所以你需要创建一个自定义收集器。结构大致相同。
- en: Listing 14.11\. A specialized experience collector for AGZ-style learning
  id: totrans-366
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.11\. 适用于 AGZ 风格学习的专用经验收集器
- en: '[PRE32]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Listing 14.12\. Passing along the decision to the experience collector
  id: totrans-368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.12\. 将决策传递给经验收集器
- en: '[PRE33]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The action output of your neural network uses a softmax activation. Recall that
    the softmax activation ensures that its values sum to 1\. For training, you should
    also make sure that the training target sums to 1\. To do this, divide the total
    visit counts by its sum; this operation is called *normalizing*. [Figure 14.5](#ch14fig05)
    shows an example.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 你的神经网络的动作输出使用 softmax 激活。回想一下，softmax 激活确保其值求和为 1。对于训练，你也应该确保训练目标求和为 1。为此，将总访问计数除以其总和；这个操作称为
    *正则化*。[图 14.5](#ch14fig05) 展示了一个示例。
- en: Figure 14.5\. Normalizing a vector. During self-play, you track the number of
    times you visit each move. For training, you must normalize the vector so it sums
    to 1.
  id: totrans-371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 14.5\. 向量正则化。在自我对弈过程中，你跟踪访问每个移动的次数。对于训练，你必须使向量求和为 1。
- en: '![](Images/14fig05.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/14fig05.jpg)'
- en: Other than that, the training process looks similar to training an actor-critic
    network in [chapter 12](kindle_split_024.xhtml#ch12). The following listing shows
    the implementation.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这一点，训练过程看起来与第 12 章中训练演员-评论家网络的过程相似。以下列表显示了实现方式。
- en: Listing 14.13\. Training the combined network
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.13\. 训练组合网络
- en: '[PRE34]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '***1* See [chapter 10](kindle_split_022.xhtml#ch10) for a discussion of learning_rate
    and batch_size.**'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 关于学习率（learning_rate）和批量大小（batch_size）的讨论，请参阅第 10 章（kindle_split_022.xhtml#ch10）。**'
- en: '***2* Normalizes the visit counts. When you call np.sum with axis=1, it sums
    along each row of the matrix. The reshape call reorganizes those sums into matching
    rows. Then you can divide the original counts by their sums.**'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 正则化访问次数。当你使用 np.sum 并设置 axis=1 时，它会沿着矩阵的每一行进行求和。reshape 调用重新组织这些求和结果到匹配的行。然后你可以将原始计数除以它们的总和。**'
- en: 'The overall reinforcement-learning cycle is the same as what you studied in
    [chapters 9](kindle_split_021.xhtml#ch09) through [12](kindle_split_024.xhtml#ch12):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 整个强化学习循环与你在第 9 章到第 12 章中学习的内容相同：
- en: Generate a huge batch of self-play games.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成大量自我对弈游戏。
- en: Train the model on the experience data.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在经验数据上训练模型。
- en: Test the updated model against the previous version.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对更新的模型与上一个版本进行测试。
- en: If the new version is measurably stronger, switch to the new version.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果新版本明显更强，则切换到新版本。
- en: If not, generate more self-play games and try again.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不是，生成更多自我对弈游戏并再次尝试。
- en: Repeat as many times as needed.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要多少次就重复多少次。
- en: '[Listing 14.14](#ch14ex14) shows an example that runs a single cycle of this
    process. Fair warning: you’ll need a *lot* of self-play games to build a strong
    Go AI from nothing. AlphaGo Zero reached a superhuman level of play, but it took
    nearly 5 million self-play games to get there.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.14](#ch14ex14) 展示了运行此过程单次循环的示例。警告：你需要大量的自我对弈游戏才能从零开始构建一个强大的围棋 AI。AlphaGo
    Zero 达到了超人类的水平，但为此需要近 500 万次自我对弈游戏。'
- en: Listing 14.14\. A single cycle of the reinforcement-learning process
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 14.14\. 强化学习过程的单次循环
- en: '[PRE35]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '***1* Build a network with four convolutional layers. To build a strong bot,
    you can add many more layers.**'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 构建一个包含四个卷积层的网络。为了构建一个强大的机器人，你可以添加更多的层。**'
- en: '***2* Add the action output to the network.**'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将动作输出添加到网络中。**'
- en: '***3* Add the value output to the network.**'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将值输出添加到网络中。**'
- en: '***4* We use 10 rounds per move here just so the demo runs quickly. For real
    training, you’ll need much more; AGZ used 1,600.**'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在这里，我们每一步使用 10 轮只是为了使演示运行得更快。对于真正的训练，你需要更多；AGZ 使用了 1,600。**'
- en: '***5* Simulate five games before training. For real training, you’ll want to
    train on much larger batches (thousands of games).**'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 在训练之前模拟五场比赛。对于真正的训练，你将需要在更大的批次（数千场比赛）上进行训练。**'
- en: 14.4\. Improving exploration with Dirichlet noise
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4\. 使用狄利克雷噪声改进探索
- en: Self-play reinforcement learning is an inherently random process. Your bot can
    easily drift in a weird direction, especially early in the training process. To
    prevent the bot from getting stuck, it’s important to provide a little randomness.
    That way, if the bot gets fixated on a really terrible move, it’ll have a small
    chance to learn a better move. In this section, we describe one of the tricks
    AGZ used to ensure good exploration.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 自我博弈强化学习是一个本质上随机的进程。你的机器人很容易漂移到一个奇怪的方向，尤其是在训练的早期。为了防止机器人陷入困境，提供一点随机性是很重要的。这样，如果机器人专注于一个非常糟糕的动作，它将有机会学习更好的动作。在本节中，我们描述了
    AGZ 用来确保良好探索的一个技巧。
- en: 'In previous chapters, you used a few different techniques to add variety to
    a bot’s selections. For example, in [chapter 9](kindle_split_021.xhtml#ch09),
    you randomly sampled from your bot’s policy output; in [chapter 11](kindle_split_023.xhtml#ch11),
    you used the ϵ-greedy algorithm: some fraction ϵ of the time, the bot would ignore
    its model entirely and choose a totally random move instead. In both cases, you
    added randomness at the time the bot made a decision. AGZ uses a different method
    to introduce randomness earlier, during the search process.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你使用了几种不同的技术来为机器人的选择增加多样性。例如，在第 [9 章](kindle_split_021.xhtml#ch09) 中，你从机器人的策略输出中随机采样；在第
    [11 章](kindle_split_023.xhtml#ch11) 中，你使用了 ϵ-greedy 算法：一部分时间 ϵ，机器人会完全忽略其模型并选择一个完全随机的动作。在这两种情况下，你都是在机器人做出决策时添加随机性。AGZ
    使用不同的方法在搜索过程中早期引入随机性。
- en: Imagine that on each turn, you artificially boosted the prior of one or two
    randomly chosen moves. Early in the search process, the prior controls which branches
    get explored, so those moves will get extra visits. If they turn out to be bad
    moves, the search will quickly move on to other branches, so no harm done. But
    this would ensure that every move gets a few visits occasionally, so the search
    doesn’t develop blind spots.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，在每一轮中，你人工增强了一到两个随机选择的动作的先验概率。在搜索过程的早期，先验概率控制着哪些分支被探索，因此这些动作将获得额外的访问。如果它们最终证明是糟糕的动作，搜索将迅速转移到其他分支，所以没有造成伤害。但这样会确保偶尔每个动作都会得到几次访问，这样搜索就不会发展出盲点。
- en: 'AGZ achieves a similar effect by adding noise—small random numbers—to the priors
    at the root of each search tree. By drawing the noise from a *Dirichlet distribution*,
    you get the exact effect described previously: a few moves get an artificial boost,
    while the others stay untouched. In this section, we explain the properties of
    the Dirichlet distribution and show how to generate Dirichlet noise by using NumPy.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: AGZ 通过向每个搜索树的根节点添加噪声（小的随机数）来达到类似的效果。通过从 *狄利克雷分布* 中抽取噪声，你得到之前描述的精确效果：一些动作得到人工的增强，而其他动作保持不变。在本节中，我们解释了狄利克雷分布的特性，并展示了如何使用
    NumPy 生成狄利克雷噪声。
- en: 'Throughout this book, you’ve used probability distributions over game moves.
    When you sample from such a distribution, you get a particular move. The Dirichlet
    distribution is a probability distribution over probability distributions: when
    you sample from the Dirichlet distribution, you get another probability distribution.
    The NumPy function `np.random.dirichlet` generates samples from a Dirichlet distribution.
    It takes a vector argument and returns a vector of the same dimension. The following
    listing shows a few example draws: the result is a vector, and it always sums
    to 1—meaning the result is itself a valid probability distribution.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，你使用了游戏动作的概率分布。当你从这样的分布中采样时，你会得到一个特定的动作。狄利克雷分布是概率分布的概率分布：当你从狄利克雷分布中采样时，你会得到另一个概率分布。NumPy
    函数 `np.random.dirichlet` 生成狄利克雷分布的样本。它接受一个向量参数并返回一个相同维度的向量。以下列表显示了一些示例抽取：结果是向量，并且总是加起来为
    1——这意味着结果是有效的概率分布。
- en: Listing 14.15\. Using `np.random.dirichlet` to sample from a Dirichlet distribution
  id: totrans-399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.15\. 使用 `np.random.dirichlet` 从Dirichlet分布中采样
- en: '[PRE36]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can control the output of the Dirichlet distribution with a *concentration*
    parameter, usually denoted as α. When α is close to 0, the Dirichlet distribution
    will generate “lumpy” vectors: most of the values with be close to 0, and just
    a few values will be larger. When α is large, the samples will be “smooth”: the
    values will be closer to each other. The following listing shows the effect of
    changing the concentration parameter.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用一个*浓度*参数来控制Dirichlet分布的输出，通常表示为α。当α接近0时，Dirichlet分布将生成“块状”向量：大多数值将接近0，只有少数值会较大。当α较大时，样本将“平滑”：值将更接近彼此。以下列表显示了改变浓度参数的效果。
- en: Listing 14.16\. Drawing from a Dirichlet distribution when α is close to zero
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.16\. 当α接近零时从Dirichlet分布中抽取
- en: '[PRE37]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '***1* Drawing from a Dirichlet distribution with a small concentration parameter.
    The results are “lumpy”: most of the mass is concentrated in one or two elements.**'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用小浓度参数从Dirichlet分布中抽取。结果是“块状”的：大部分质量集中在向量的一或两个元素上。**'
- en: '***2* Drawing from a Dirichlet distribution with a large concentration parameter.
    In each result, the mass is spread evenly over all elements of the vector.**'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用大浓度参数从Dirichlet分布中抽取。在每个结果中，质量均匀分布在向量的所有元素上。**'
- en: This shows a recipe for modifying your priors. By choosing a small α, you get
    a distribution in which a few moves have high probabilities, and the rest are
    close to zero. Then you can take a weighted average of the true priors with the
    Dirichlet noise. AGZ used a concentration parameter of 0.03.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了修改你的先验概率的配方。通过选择一个小的α，你得到一个分布，其中少数移动具有较高的概率，其余的接近零。然后你可以对真实的先验概率与Dirichlet噪声进行加权平均。AGZ使用了0.03的浓度参数。
- en: 14.5\. Modern techniques for deeper neural networks
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5\. 深度神经网络的现代技术
- en: Neural network design is a hot research topic. One never-ending problem is how
    to make training stable on deeper and deeper networks. AlphaGo Zero applied a
    couple of cutting-edge techniques that are quickly becoming standards. The details
    are beyond the scope of this book, but we introduce them at a high level here.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络设计是一个热门的研究课题。一个永无止境的问题是如何在更深层次的网络中使训练稳定。AlphaGo Zero应用了几项迅速成为标准的尖端技术。这些细节超出了本书的范围，但我们在这里以高层次介绍它们。
- en: 14.5.1\. Batch normalization
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.5.1\. 批标准化
- en: 'The idea of deep neural networks is that each layer can learn an increasingly
    high-level representation of the original data. But what exactly are these representations?
    What we mean is that some meaningful property of the original data will show up
    as a particular numeric value in the activation of a particular neuron in the
    layer. But the mapping between actual numbers is completely arbitrary. If you
    multiply every activation in a layer by 2, for example, you haven’t lost any information:
    you’ve just changed the scale. In principle, such a transformation doesn’t affect
    the network’s capacity to learn.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的想法是每一层都可以学习原始数据的一个越来越高级的表示。但究竟是什么表示呢？我们的意思是，原始数据的一些有意义属性将作为一个特定神经元的激活中的特定数值出现。但实际数值之间的映射是完全任意的。例如，如果你将一个层中每个激活乘以2，你并没有丢失任何信息：你只是改变了尺度。原则上，这种变换不会影响网络的学习能力。
- en: But the absolute value of the activations can affect practical training performance.
    The idea of *batch normalization* is to shift each layer’s activations so they’re
    centered around 0, and scale them so the variance is 1\. At the start of training,
    you don’t know what the activations will look like. Batch normalization provides
    a scheme for learning the correct shift and scale on the fly during the training
    process; the normalizing transform will adjust as its inputs change throughout
    training.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 但激活的绝对值可能会影响实际的训练性能。*批标准化*的想法是将每一层的激活移动到以0为中心，并缩放它们，使方差为1。在训练开始时，你不知道激活会是什么样子。批标准化提供了一种在训练过程中动态学习正确偏移和缩放的方案；归一化变换会根据其输入在训练过程中的变化进行调整。
- en: How does batch normalization improve training? That’s still an open area of
    research. The original researchers developed batch normalization in order to reduce
    *covariate shift*. The activations in any layer tend to drift during training.
    Batch normalization corrects that drift, reducing the learning burden on the later
    layers. But the latest research suggests that the covariate shift may not be as
    important as initially thought. Instead, the value may be in the way batch normalization
    makes the loss function smoother.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化如何提高训练效果？这仍然是一个开放的研究领域。原始研究人员开发批归一化是为了减少**协变量偏移**。任何层的激活在训练过程中往往会漂移。批归一化纠正了这种漂移，减轻了后续层的学习负担。但最新的研究表明，协变量偏移可能不像最初认为的那么重要。相反，其价值可能在于批归一化使损失函数变得更平滑的方式。
- en: Although researchers are still studying *why* batch normalization works, it’s
    well established that it *does* work. Keras provides a `BatchNormalization` layer
    that you can add to your networks. The following listing shows an example of adding
    batch normalization to a convolution layer in Keras.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究人员仍在研究**为什么**批归一化有效，但已经确定它**确实**有效。Keras提供了一个`BatchNormalization`层，你可以将其添加到你的网络中。以下列表显示了在Keras中向卷积层添加批归一化的示例。
- en: Listing 14.17\. Adding batch normalization to a Keras network
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表14.17\. 将批归一化添加到Keras网络中
- en: '[PRE38]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1* The axis should match the convolution data_format. For channels_first,
    use axis=1 (the first axis). For channels_last, use axis=-/1 (the last axis).**'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1** 轴应该与卷积数据格式匹配。对于channels_first，使用axis=1（第一个轴）。对于channels_last，使用axis=-/1（最后一个轴）。'
- en: '***2* The normalization happens between the convolution and the relu activation.**'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2** 正规化发生在卷积和relu激活之间。'
- en: 14.5.2\. Residual networks
  id: totrans-418
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.5.2\. 残差网络
- en: Imagine you’ve successfully trained a neural network that has three hidden layers
    in the middle. What happens if you add a fourth layer? In theory, this should
    strictly increase the capacity of your network. In the worst case, when you train
    the four-layer network, the first three layers could learn the same things they
    did in the three-layer network, while the fourth layer just passes its input untouched.
    You’d hope it could also learn *more*, but you wouldn’t expect it to learn *less*.
    At least, you’d expect the deeper network should be able to overfit (memorize
    the training set in a way that doesn’t necessarily translate to new examples).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经成功训练了一个具有中间三层隐藏层的神经网络。如果你添加一个第四层会发生什么？从理论上讲，这应该严格增加你网络的容量。在最坏的情况下，当你训练四层网络时，前三层可能会学习与三层网络中相同的内容，而第四层只是未改变地传递其输入。你希望它也能学习**更多**，但你不会期望它学习**更少**。至少，你期望更深的网络应该能够过拟合（以某种方式记住训练集，但这并不一定适用于新示例）。
- en: In reality, this doesn’t always happen. When you try to train a four-layer network,
    there are more possible ways to organize the data than on a three-layer network.
    Sometimes, because of the quirks of stochastic gradient descent on complicated
    loss surfaces, you can add more layers and find you can’t even overfit. The idea
    of a *residual network* is to simplify what the extra layer is trying to learn.
    If three layers can do an OK job learning a problem, you can force the fourth
    layer to focus on learning the gap between whatever the first three layers learned
    and the objective. (That gap is the *residual*, hence the name.)
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，这种情况并不总是发生。当你尝试训练一个四层网络时，组织数据的方式比三层网络要多得多。有时，由于在复杂的损失表面上随机梯度下降的怪异之处，你可能会增加更多层，却发现甚至无法过拟合。**残差网络**的想法是简化额外层试图学习的内容。如果三层可以很好地学习一个问题，你可以迫使第四层专注于学习前三层学习的内容与目标之间的差距。（这个差距就是**残差**，因此得名。）
- en: To implement this, you sum the *input* to your extra layers with the *output*
    from your extra layers, as illustrated in [figure 14.6](#ch14fig06). The connection
    from the previous layer to the summation layer is called a *skip connection*.
    Normally, residual networks are organized into small blocks; there are around
    two or three layers per block with a skip connection beside them. Then you can
    stack as many blocks together as needed.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，你将额外层的**输入**与额外层的**输出**相加，如图14.6所示。从前一层的连接到求和层的连接称为**跳跃连接**。通常，残差网络被组织成小的块；每个块大约有两到三层，旁边有一个跳跃连接。然后你可以根据需要堆叠尽可能多的块。
- en: Figure 14.6\. A residual block. The output of the two inner layers is added
    to the output of the previous layer. The effect is that the inner layers learn
    the difference, or residual, between the objective and what the previous layer
    learned.
  id: totrans-422
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图14.6。残差块。两个内部层的输出被添加到前一层的输出中。其效果是内部层学习目标与前一层学习内容之间的差异或残差。
- en: '![](Images/14fig06.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/14fig06.jpg)'
- en: 14.6\. Exploring additional resources
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.6。探索额外资源
- en: If you’re interested in experimenting with AlphaGo Zero-style bots, there are
    many open source projects inspired by the original AGZ paper. If you want a superhuman
    Go AI, either to play against or to study the source code, you have an embarrassment
    of riches.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣于尝试AlphaGo Zero风格的机器人，有许多开源项目受到原始AGZ论文的启发。如果你想拥有一个超越人类的围棋AI，无论是用来对弈还是研究源代码，你将拥有丰富的资源。
- en: 'Leela Zero is an open source implementation of an AGZ-style bot. The self-play
    process is distributed: if you have CPU cycles to spare, you can generate self-play
    games and upload them for training. At this writing, the community has contributed
    over 8 million games, and Leela Zero is already strong enough to beat professional
    Go players. [http://zero.sjeng.org/](http://zero.sjeng.org/)'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leela Zero是一个AGZ风格机器人的开源实现。自我对弈过程是分布式的：如果你有多余的CPU周期，你可以生成自我对弈游戏并将它们上传进行训练。截至本文写作时，社区已经贡献了超过800万场比赛，Leela
    Zero已经足够强大，可以击败职业围棋选手。[http://zero.sjeng.org/](http://zero.sjeng.org/)
- en: Minigo is another open source implementation, written in Python with TensorFlow.
    It’s fully integrated with Google Cloud Platform so you can use Google’s public
    cloud to run experiments. [https://github.com/tensorflow/minigo](https://github.com/tensorflow/minigo)
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minigo是另一个开源实现，用Python和TensorFlow编写。它与Google Cloud Platform完全集成，因此你可以使用Google的公共云来运行实验。[https://github.com/tensorflow/minigo](https://github.com/tensorflow/minigo)
- en: Facebook AI Research implemented the AGZ algorithm on top of its ELF reinforcement
    learning platform. The result, ELF OpenGo, is now freely available, and it’s among
    the strongest Go AIs today. [https://facebook.ai/developers/tools/elf](https://facebook.ai/developers/tools/elf)
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook AI Research在其ELF强化学习平台上实现了AGZ算法。结果是ELF OpenGo，现在它是免费可用的，并且是目前最强的围棋AI之一。[https://facebook.ai/developers/tools/elf](https://facebook.ai/developers/tools/elf)
- en: Tencent has also implemented and trained an AGZ-style bot, which they have released
    as PhoenixGo. The bot is also known as BensonDarr on the Fox Go server, where
    it has beaten many of the world’s top players. [https://github.com/Tencent/PhoenixGo](https://github.com/Tencent/PhoenixGo)
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 腾讯也实施并训练了一个AGZ风格的机器人，他们将其作为PhoenixGo发布。这个机器人在Fox围棋服务器上也被称作BensonDarr，在那里它击败了许多世界顶级选手。[https://github.com/Tencent/PhoenixGo](https://github.com/Tencent/PhoenixGo)
- en: If Go isn’t your thing, Leela Chess Zero is a fork of Leela Zero that has been
    adapted to learn chess instead. It’s already at least as strong as human grandmasters,
    and chess fans have praised its exciting and creative play. [https://github.com/LeelaChessZero/lczero](https://github.com/LeelaChessZero/lczero)
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你不喜欢围棋，Leela Chess Zero是Leela Zero的一个分支，它已经被调整为学习国际象棋。它已经至少与人类围棋大师一样强大，并且国际象棋爱好者对其激动人心和富有创造性的玩法给予了赞扬。[https://github.com/LeelaChessZero/lczero](https://github.com/LeelaChessZero/lczero)
- en: 14.7\. Wrapping up
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7。总结
- en: 'That wraps up our introduction to the cutting-edge AI techniques that power
    modern Go AIs. We encourage you to take matters into your own hands from here:
    either experiment with your own Go bot, or try applying these modern techniques
    to other games.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对现代围棋AI所使用的尖端AI技术的介绍。我们鼓励你从现在开始自己动手：要么尝试自己的围棋机器人，要么尝试将这些现代技术应用到其他游戏中。
- en: 'But also think beyond games. When you read about the latest application of
    machine learning, you now have a framework for understanding what’s happening.
    Think about the following:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 但也要超越游戏。当你阅读有关机器学习最新应用的内容时，你现在有一个框架来理解正在发生的事情。考虑以下问题：
- en: What is the model or neural network structure?
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型或神经网络结构是什么？
- en: What is the loss function or training objective?
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数或训练目标是什么？
- en: What is the training process?
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程是什么？
- en: How are the inputs and outputs encoded?
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出是如何编码的？
- en: How can the model fit in with traditional algorithms or practical software applications?
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型如何与传统算法或实际软件应用相结合？
- en: We hope that we’ve inspired you to try your own experiments with deep learning,
    whether they’re in games or another field.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们已经激发了你尝试自己进行深度学习实验的灵感，无论是游戏还是其他领域。
- en: 14.8\. Summary
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.8。总结
- en: AlphaGo Zero uses a single neural network with two outputs. One output indicates
    which moves are important, and the other output indicates which player is ahead.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo Zero 使用一个具有两个输出的单一神经网络。一个输出指示哪些移动是重要的，另一个输出指示哪个玩家领先。
- en: The AlphaGo Zero tree-search algorithm is similar to Monte Carlo tree search,
    with two major differences. Instead of using random games to evaluate a position,
    it relies solely on a neural network. In addition, it uses a neural network to
    guide the search toward new branches.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo Zero 的树搜索算法类似于蒙特卡洛树搜索，但有两个主要区别。它不是使用随机游戏来评估位置，而是完全依赖于神经网络。此外，它还使用神经网络来引导搜索走向新的分支。
- en: AlphaGo Zero’s neural network is trained against the number of times it visited
    particular moves in the search process. In that way, it’s specifically trained
    to enhance tree search, rather than to select moves directly.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo Zero 的神经网络是通过在搜索过程中访问特定移动的次数进行训练的。这样，它专门训练来增强树搜索，而不是直接选择移动。
- en: A *Dirichlet distribution* is a probability distribution over probability distributions.
    The concentration parameter controls how clumpy the resulting probability distributions
    are. AlphaGo Zero uses Dirichlet noise to add controlled randomness to its search
    process, to make sure all moves get explored occasionally.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*狄利克雷分布* 是一种概率分布，它是在概率分布上的概率分布。集中参数控制着结果概率分布的密集程度。AlphaGo Zero 使用狄利克雷噪声为其搜索过程添加受控的随机性，以确保偶尔探索所有移动。'
- en: Batch normalization and residual networks are two modern techniques that help
    you train very deep neural networks.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化和残差网络是两种现代技术，可以帮助你训练非常深的神经网络。
