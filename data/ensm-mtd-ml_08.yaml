- en: '6 Sequential ensembles: Newton boosting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 序列集成：牛顿提升法
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using Newton’s descent to optimize loss functions for training models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用牛顿下降法优化训练模型的损失函数
- en: Implementing and understanding how Newton boosting works
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现并理解牛顿提升法的工作原理
- en: Learning with regularized loss functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则化损失函数进行学习
- en: Introducing XGBoost as a powerful framework for Newton boosting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍XGBoost作为牛顿提升法强大框架的引入
- en: Avoiding overfitting with XGBoost
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBoost避免过拟合
- en: 'In the previous two chapters, we saw two approaches to constructing sequential
    ensembles: In chapter 4, we introduced a new ensemble method called adaptive boosting
    (AdaBoost), which uses weights to identify the most misclassified examples. In
    chapter 5, we introduced another ensemble method called gradient boosting, which
    uses gradients (residuals) to identify the most misclassified examples. The fundamental
    intuition behind both of these boosting methods is to target the most misclassified
    (essentially, the worst behaving) examples at every iteration to improve classification.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '在前两章中，我们看到了构建序列集成的两种方法：在第4章中，我们介绍了一种新的集成方法，称为自适应提升（AdaBoost），它使用权重来识别最被错误分类的示例。在第5章中，我们介绍了一种另一种集成方法，称为梯度提升，它使用梯度（残差）来识别最被错误分类的示例。这两种提升方法背后的基本直觉是在每次迭代中针对最被错误分类的（本质上，表现最差的）示例来提高分类。 '
- en: In this chapter, we introduce a third boosting approach—Newton boosting—which
    combines the advantages of both AdaBoost and gradient boosting and uses *weighted
    gradients* (or weighted residuals) to identify the most misclassified examples.
    As with gradient boosting, the framework of Newton boosting can be applied to
    any loss function, which means that any classification, regression, or ranking
    problem can be boosted using weak learners. In addition to this flexibility, packages
    such as XGBoost are now available that can scale Newton boosting to big data through
    parallelization. Unsurprisingly, Newton boosting is currently considered by many
    practitioners to be a state-of-the-art ensemble approach.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种第三种提升方法——牛顿提升法，它结合了AdaBoost和梯度提升的优点，并使用*加权梯度*（或加权残差）来识别最被错误分类的示例。与梯度提升一样，牛顿提升法的框架可以应用于任何损失函数，这意味着任何分类、回归或排序问题都可以使用弱学习器进行提升。除了这种灵活性之外，如XGBoost之类的包现在可以并行化扩展牛顿提升法以处理大数据。不出所料，牛顿提升法目前被许多从业者认为是领先的集成方法。
- en: Because Newton boosting builds on Newton’s descent, we kick off the chapter
    with examples of Newton’s descent and how it can be used to train a machine-learning
    model (section 6.1). Section 6.2 aims to provide intuition for learning with weighted
    residuals, the key intuition behind Newton boosting. As always, we implement our
    own version of Newton boosting to understand how it combines gradient descent
    and boosting to train a sequential ensemble.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于牛顿提升法建立在牛顿下降法的基础上，我们本章首先通过牛顿下降法的示例及其如何用于训练机器学习模型（第6.1节）来开启本章。第6.2节旨在为使用加权残差进行学习提供直观理解，这是牛顿提升法背后的关键直觉。一如既往，我们实现了自己版本的牛顿提升法，以理解它是如何结合梯度下降和提升法来训练序列集成。
- en: Section 6.3 introduces XGBoost, a free and open source gradient-boosting and
    Newton-boosting package, which is widely used for building and deploying real-world
    machine-learning applications. In section 6.4, we see how we can avoid overfitting
    with strategies such as early stopping and adapting the learning rate with XGBoost.
    Finally, in section 6.5, we’ll reuse the real-world study of document retrieval
    from chapter 5 to compare the performance of XGBoost to LightGBM, its variants,
    and random forests.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第6.3节介绍了XGBoost，这是一个免费且开源的梯度提升和牛顿提升包，它被广泛用于构建和部署现实世界的机器学习应用。在第6.4节中，我们看到如何通过早期停止和调整学习率等策略使用XGBoost来避免过拟合。最后，在第6.5节中，我们将重新使用第5章中关于文档检索的实例研究，以比较XGBoost与LightGBM、其变体和随机森林的性能。
- en: 'The origins and motivation for devising Newton boosting are analogous to those
    of the gradient-boosting algorithm: the optimization of loss functions. Gradient
    descent, which gradient boosting is based on, is a first-order optimization method
    in that it uses first derivatives during optimization.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 设计牛顿提升法的起源和动机与梯度提升算法类似：损失函数的优化。梯度提升基于的梯度下降是一种一阶优化方法，它在优化过程中使用一阶导数。
- en: Newton’s method, or Newton’s descent, is a second-order optimization method,
    in that it uses both first and second-derivative information together to compute
    a Newton step. When combined with boosting, we obtain the ensemble method of Newton
    boosting. We begin this chapter by discussing how Newton’s method inspires a powerful
    and widely used ensemble method.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿法，或称牛顿下降法，是一种二阶优化方法，因为它同时使用一阶和二阶导数信息来计算牛顿步。当与提升法结合时，我们得到牛顿提升的集成方法。我们本章首先讨论牛顿法如何激发一种强大且广泛使用的集成方法。
- en: 6.1 Newton’s method for minimization
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 牛顿法求最小值
- en: 'Iterative optimization methods such as gradient descent and Newton’s method
    perform an update within each iteration: next = current + (step × direction).
    In gradient descent (figure 6.1, left), first-derivative information only allows
    us to construct a local linear approximation at best. While this gives us a descent
    direction, different step lengths can give us vastly different estimates and may
    ultimately slow down convergence.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代优化方法，如梯度下降法和牛顿法，在每个迭代步骤中都会进行更新：next = current + (step × direction)。在梯度下降法（图6.1左侧）中，仅使用一阶导数信息最多只能构建一个局部线性近似。虽然这给出了下降方向，但不同的步长可以给出截然不同的估计，并可能最终减慢收敛速度。
- en: Incorporating second-derivative information, as Newton’s descent does, allows
    us to construct a local quadratic approximation! This extra information leads
    to a better local approximation, resulting in better steps and faster convergence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入二阶导数信息，正如牛顿下降法所做的那样，我们可以构建一个局部二次近似！这个额外信息导致更好的局部近似，从而产生更好的步骤和更快的收敛。
- en: '![CH06_F01_Kunapuli](../Images/CH06_F01_Kunapuli.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli](../Images/CH06_F01_Kunapuli.png)'
- en: Figure 6.1 Comparing gradient descent (left) and Newton’s method (right). Gradient
    descent only uses local first-order information near the current solution, which
    leads to a linear approximation of the function being optimized. Different step
    lengths will then lead to different next steps. Newton’s method uses both local
    first- and second-order information near the current solution, leading to a quadratic
    (parabolic) approximation of the function being optimized. This provides a better
    estimate of the next step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 比较梯度下降法（左）和牛顿法（右）。梯度下降法仅使用当前解附近的局部一阶信息，这导致对被优化函数的线性近似。不同的步长将导致不同的下一步。牛顿法使用当前解附近的局部一阶和二阶信息，导致对被优化函数的二次（抛物线）近似。这提供了对下一步的更好估计。
- en: NOTE The approach described in this chapter, Newton’s method for optimization,
    is derived from a more general root-finding method, also called Newton’s method.
    We’ll often use the term Newton’s descent to refer to Newton’s method for minimization.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章中描述的方法，即牛顿优化法，是从一个更一般的根查找方法推导出来的，也称为牛顿法。我们经常使用牛顿下降来指牛顿法求最小值。
- en: More formally, gradient descent computes the next update as
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，梯度下降法计算下一个更新为
- en: '![CH06_F01_Kunapuli-eqs-0x](../Images/CH06_F01_Kunapuli-eqs-0x.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli-eqs-0x](../Images/CH06_F01_Kunapuli-eqs-0x.png)'
- en: where *α*[t] is the step length, and (-*f'*(*w*[t])) is the negative gradient,
    or the negative of the first derivative. Newton’s method computes the next update
    as
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *α*[t] 是步长，(-*f'*(*w*[t])) 是负梯度，即一阶导数的相反数。牛顿法计算下一个更新为
- en: '![CH06_F01_Kunapuli-eqs-2x](../Images/CH06_F01_Kunapuli-eqs-2x.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli-eqs-2x](../Images/CH06_F01_Kunapuli-eqs-2x.png)'
- en: where *f''*(*w*[t]) is the second derivative, and the step length *α*[t] is
    1.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f''*(*w*[t]) 是二阶导数，步长 *α*[t] 为 1。
- en: 'NOTE Unlike gradient descent, Newton’s descent computes exact steps and doesn’t
    require a step-length computation. However, we’ll explicitly include the step
    length for two reasons: (1) to enable us to immediately compare and understand
    the differences between gradient descent and Newton’s descent; and (2) more importantly,
    unlike Newton’s descent, Newton boosting can only approximate the step and will
    require us to specify a step length similar to gradient descent and gradient boosting.
    As we’ll see, this step length in Newton boosting is nothing more than the learning
    rate.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与梯度下降法不同，牛顿下降法计算精确的步骤，不需要步长计算。然而，我们将明确包括步长，原因有两个：（1）使我们能够立即比较和理解梯度下降法和牛顿下降法之间的差异；（2）更重要的是，与牛顿下降法不同，牛顿提升法只能近似步骤，并将需要我们指定一个类似于梯度下降法和梯度提升法的步长。正如我们将看到的，牛顿提升法中的这个步长不过是学习率。
- en: The second derivative and the Hessian matrix
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数和海森矩阵
- en: 'For univariate functions (i.e., functions in one variable), the second derivative
    is easy to compute: we simply differentiate the function twice. For instance,
    for the function *f*(*w*) = *x*⁵, the first derivative is *f*’(*x*) = *∂f*/*∂x*
    = 5*x*⁴, and the second derivative is *f*’’(*x*) = *∂f*/*∂x∂y* = 20*x*³.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一元函数（即，一个变量的函数），二阶导数的计算很简单：我们只需对函数求导两次。例如，对于函数 *f*(*w*) = *x*⁵，其第一导数是 *f*’(*x*)
    = *∂f*/*∂x* = 5*x*⁴，而第二导数是 *f*’’(*x*) = *∂f*/*∂x∂y* = 20*x*³。
- en: For multivariate functions, or functions in many variables, the calculation
    of the second derivative is a little more involved. This is because we now have
    to consider differentiating the multivariate function with respect to pairs of
    variables.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多元函数，或者多个变量的函数，二阶导数的计算稍微复杂一些。这是因为我们现在必须考虑相对于变量对的多元函数的求导。
- en: 'To see this, consider a function in three variables: *f*(*x*,*y*,*z*). The
    gradient of this function is straightforward to compute: we differentiate the
    function with respect to each of the variables *x*, *y*, and *z* (where w.r.t.
    is “with respect to”):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这一点，考虑一个三变量的函数：*f*(*x*,*y*,*z*)。这个函数的梯度很容易计算：我们对每个变量 *x*，*y* 和 *z*（其中 w.r.t.
    是“相对于”的意思）求导：
- en: '![CH06_F01_Kunapuli-eqs-5x](../Images/CH06_F01_Kunapuli-eqs-5x.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli-eqs-5x](../Images/CH06_F01_Kunapuli-eqs-5x.png)'
- en: 'To compute the second derivative, we have to further differentiate each entry
    of the gradient with respect to *x*, *y*, and *z* again. This produces a matrix
    known as the Hessian:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算二阶导数，我们必须再次对梯度的每个元素相对于 *x*，*y* 和 *z* 求导。这产生了一个称为Hessian的矩阵：
- en: '![CH06_F01_Kunapuli-eqs-6xa](../Images/CH06_F01_Kunapuli-eqs-6xa.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli-eqs-6xa](../Images/CH06_F01_Kunapuli-eqs-6xa.png)'
- en: The Hessian is a symmetric matrix because the order of differentiation doesn’t
    change the result, meaning that
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian是一个对称矩阵，因为求导的顺序不会改变结果，这意味着
- en: '![CH06_F01_Kunapuli-eqs-8x](../Images/CH06_F01_Kunapuli-eqs-8x.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli-eqs-8x](../Images/CH06_F01_Kunapuli-eqs-8x.png)'
- en: and so on, for all pairs of variables in *f*. In the multivariate case, the
    extension of Newton’s method is given by
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以此类推，对于 *f* 中的所有变量对。在多元情况下，牛顿法的扩展由
- en: '![CH06_F01_Kunapuli-eqs-9x](../Images/CH06_F01_Kunapuli-eqs-9x.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli-eqs-9x](../Images/CH06_F01_Kunapuli-eqs-9x.png)'
- en: where ▽*f*(*w*[t]) is the gradient vector of the multivariate function f, and
    -▽²(*w*[t])^(-1) is the inverse of the Hessian matrix. Inverting the second-derivative
    Hessian matrix is the multivariate equivalent of dividing by the term *f''*(*w*[t]).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ▽*f*(*w*[t]) 是多元函数 f 的梯度向量，而 -▽²(*w*[t])^(-1) 是Hessian矩阵的逆。求逆二阶导数Hessian矩阵是相对于除以项
    *f''*(*w*[t]) 的多元等价。
- en: For large problems with many variables, inverting the Hessian matrix can become
    quite computationally expensive, slowing down overall optimization. As we’ll see
    in section 6.2, Newton boosting circumvents this problem by computing second derivatives
    for individual examples to avoid inverting the Hessian.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有许多变量的大型问题，求逆Hessian矩阵可能变得相当计算密集，从而减慢整体优化速度。正如我们将在第6.2节中看到的，牛顿加速通过为单个示例计算二阶导数来避免求逆Hessian，从而绕过这个问题。
- en: 'For now, let’s continue to explore the differences between gradient descent
    and Newton’s method. We return to the two examples we used in section 5.1: the
    simple illustrative Branin function and the squared-loss function. We’ll use these
    examples to illustrate the differences between gradient descent and Newton’s descent.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续探讨梯度下降法和牛顿法之间的差异。我们回到第5.1节中使用的两个例子：简单的Branin函数和平方损失函数。我们将使用这些例子来说明梯度下降法和牛顿下降法之间的差异。
- en: 6.1.1 Newton’s method with an illustrative example
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 牛顿法的一个示例
- en: Recall from chapter 5 that the Branin function comprises two variables (*w*[1]
    and *w*[2]), defined as
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第5章，Branin函数包含两个变量 (*w*[1] 和 *w*[2])，定义为
- en: '![CH06_F01_Kunapuli-eqs-13x](../Images/CH06_F01_Kunapuli-eqs-13x.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Kunapuli-eqs-13x](../Images/CH06_F01_Kunapuli-eqs-13x.png)'
- en: where *α* = 1, *b* = 5.1/4*π*², *c* = 5/*π*, *r* = 6, *s* = 10, and *t* = 1/8*π*
    are fixed constants. This function is shown in figure 6.2 and has four minimizers
    at the centers of the elliptical regions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *α* = 1，*b* = 5.1/4*π*²，*c* = 5/*π*，*r* = 6，*s* = 10，*t* = 1/8*π* 是固定的常数。这个函数在图6.2中显示，并在椭圆区域的中心有四个最小值点。
- en: '![CH06_F02_Kunapuli](../Images/CH06_F02_Kunapuli.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F02_Kunapuli](../Images/CH06_F02_Kunapuli.png)'
- en: Figure 6.2 The surface plot (left) and contour plot (right) of the Branin function.
    We can visually verify that this function has four minima, which are the centers
    of the elliptical regions in the contour plot.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 Branin函数的表面图（左）和等高线图（右）。我们可以直观地验证该函数有四个最小值，这些最小值是等高线图中的椭圆区域的中心。
- en: 'We’ll take our gradient descent implementation from the previous section and
    modify it to implement Newton’s method. There are two key differences: (1) we
    compute the descent direction using the gradient and the Hessian, that is, using
    both the first- and second-derivative information; and (2) we drop the computation
    of the step length, that is, we assume that the step length is 1\. The modified
    pseudocode is shown here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从上一节中的梯度下降实现开始，并对其进行修改以实现牛顿法。有两个关键区别：(1) 我们使用梯度和Hessian来计算下降方向，即使用一阶和二阶导数信息；(2)
    我们省略了步长的计算，即我们假设步长为1。修改后的伪代码如下所示：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The key steps in this pseudocode are steps 1 and 2, where the descent direction
    is computed using the inverse Hessian matrix (second derivatives) and the gradient
    (first derivatives). Note that, as with gradient descent, the Newton’s descent
    direction is negated.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该伪代码中的关键步骤是步骤1和2，其中下降方向是使用逆Hessian矩阵（二阶导数）和梯度（一阶导数）计算的。请注意，与梯度下降一样，牛顿的下降方向是取反的。
- en: 'Step 3 is included to explicitly illustrate that, unlike gradient descent,
    Newton’s method doesn’t require the computation of a step length. Instead, the
    step length can be set ahead of time, much like a learning rate. Once the descent
    direction is identified, step 4 implements the Newton update: *w*[*t*+1] = *w*[*t*]
    + (-▽²*f*(*w*[t])^(-1 )▽*f*(*w*[t])).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤3包括以明确说明，与梯度下降不同，牛顿法不需要计算步长。相反，步长可以预先设置，就像学习率一样。一旦确定了下降方向，步骤4就实现了牛顿更新：*w*[*t*+1]
    = *w*[*t*] + (-▽²*f*(*w*[t])^(-1)▽*f*(*w*[t]))。
- en: After we compute each update, similar to gradient descent, we check for convergence;
    here, our convergence test is to see how close *w*[*t*+1] and *w*[t] are to each
    other. If they are close enough, we terminate; if not, we continue on to the next
    iteration. The following listing implements Newton’s method.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们计算每个更新后，类似于梯度下降，我们检查收敛性；在这里，我们的收敛性测试是查看 *w*[*t*+1] 和 *w*[t] 之间的距离有多近。如果它们足够接近，我们就终止；如果不，我们继续进行下一次迭代。以下列表实现了牛顿法。
- en: Listing 6.1 Newton’s descent
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1 牛顿下降
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Newton’s descent requires a function f, its gradient g, and its Hessian h.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 牛顿下降需要一个函数f，其梯度g和其Hessian h。
- en: ❷ Initializes Newton’s descent to not converged
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化牛顿下降为未收敛
- en: ❸ Computes the gradient and the Hessian
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算梯度和Hessian
- en: ❹ Computes the Newton direction
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算牛顿方向
- en: ❺ Sets step length to 1, for simplicity
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将步长设置为1，以简化
- en: ❻ Computes the update
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算更新
- en: ❼ Computes change from previous iteration
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算与前一次迭代的差异
- en: ❽ Converges if change is small or maximum iterations are reached
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 如果变化很小或达到最大迭代次数则收敛
- en: ❾ Gets ready for the next iteration
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 准备下一次迭代
- en: Note that the step length is set to 1, although for Newton boosting, as we’ll
    see, the step length will become the learning rate.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，步长被设置为1，尽管对于牛顿加速，正如我们将看到的，步长将变为学习率。
- en: 'Let’s take our implementation of Newton’s descent for a spin. We’ve already
    implemented the Branin function and its gradient in the previous section. This
    implementation is shown here again:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一下我们的牛顿下降实现。我们已经在上一节中实现了Branin函数及其梯度。此实现再次显示如下：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also need the Hessian (second derivative) matrix for Newton’s descent. We
    can compute it by analytically differentiating the gradient (first derivative)
    vector:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要牛顿下降的Hessian（二阶导数）矩阵。我们可以通过解析微分梯度（一阶导数）向量来计算它：
- en: '![CH06_F02_Kunapuli-eqs-19xa](../Images/CH06_F02_Kunapuli-eqs-19xa.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F02_Kunapuli-eqs-19xa](../Images/CH06_F02_Kunapuli-eqs-19xa.png)'
- en: 'This can also be implemented as shown here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以像下面这样实现：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As with gradient descent, Newton’s descent (refer to listing 6.1) also requires
    an initial guess x_init. Here, we’ll initialize gradient descent with *w*[init]
    = [2,-5]''. Now, we can call the Newton’s descent procedure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与梯度下降一样，牛顿下降（参见表格6.1）也需要一个初始猜测x_init。在这里，我们将梯度下降初始化为 *w*[init] = [2,-5]'. 现在，我们可以调用牛顿下降过程：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Newton’s descent returns an optimal solution w_optimal (which is [3.142, 2.275]')
    and the solution path w_path. So how does Newton’s descent compare to gradient
    descent? In figure 6.3, we plot the solution paths of both optimization algorithms
    together.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿下降返回一个最优解w_optimal（即[3.142, 2.275]')和解决方案路径w_path。那么牛顿下降与梯度下降相比如何呢？在图6.3中，我们同时绘制了两种优化算法的解决方案路径。
- en: 'The result of this comparison is pretty striking: Newton’s descent is able
    to exploit the additional local information about the curvature of the function
    provided by the Hessian matrix to take a more direct path to the solution. In
    contrast, gradient descent only has the first-order gradient information to work
    with and takes a roundabout path to the same solution.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种比较的结果非常引人注目：牛顿下降能够利用Hessian矩阵提供的关于函数曲率的额外局部信息，以更直接的方式到达解。相比之下，梯度下降只有一阶梯度信息可以工作，并采取迂回的路径到达相同的解。
- en: '![CH06_F03_Kunapuli](../Images/CH06_F03_Kunapuli.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F03_Kunapuli](../Images/CH06_F03_Kunapuli.png)'
- en: Figure 6.3 We compare the solution paths of Newton’s descent and gradient descent
    starting from [2,-5] (square) and both converging to one of the local minima (circle).
    Newton’s descent (solid line) progresses toward the local minimum in a more direct
    way compared to gradient descent (dotted line). This is because Newton’s descent
    uses a more informative second-order local approximation with each update, while
    gradient descent only uses a first-order local approximation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 我们比较了从[2,-5]（正方形）开始的牛顿下降和梯度下降的解决方案路径，它们都收敛到局部最小值之一（圆形）。与梯度下降（虚线）相比，牛顿下降（实线）以更直接的方式向局部最小值前进。这是因为牛顿下降在每个更新中使用更具有信息量的二阶局部近似，而梯度下降只使用一阶局部近似。
- en: Properties of Newton’s descent
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿下降的性质
- en: We note a couple of important things about Newton’s descent and its similarities
    to gradient descent. First, unlike gradient descent, Newton’s method computes
    the descent step exactly and doesn’t require a step length. Keep in mind that
    our purpose is to extend Newton’s descent to Newton boosting. From this perspective,
    the step length can be interpreted as a learning rate.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到关于牛顿下降及其与梯度下降相似性的几个重要事项。首先，与梯度下降不同，牛顿法精确地计算下降步长，不需要步长。记住，我们的目的是将牛顿下降扩展到牛顿提升。从这个角度来看，步长可以解释为学习率。
- en: Choosing an effective learning rate (say, using cross validation like we did
    for AdaBoost or gradient boosting) is very much akin to choosing a good step length.
    Instead of selecting a learning rate to accelerate convergence, in boosting algorithms,
    we select the learning rate to help us avoid overfitting and to generalize better
    to the test set and future data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个有效的学习率（例如，像我们在AdaBoost或梯度提升中那样使用交叉验证）与选择一个好的步长非常相似。在提升算法中，我们选择学习率来帮助我们避免过拟合，并更好地泛化到测试集和未来的数据，而不是选择学习率来加速收敛。
- en: A second important point to keep in mind is that, like gradient descent, Newton’s
    descent is also sensitive to our choice of initial point. Different initializations
    will lead Newton’s descent to different local minimizers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个需要记住的重要点是，像梯度下降一样，牛顿下降也对我们的初始点选择很敏感。不同的初始化会导致牛顿下降收敛到不同的局部最小值。
- en: In addition to local minimizers, a bigger problem is that our choice of initial
    point can also lead Newton’s descent to converge to *saddle points*. This is a
    problem faced by all descent algorithms and is illustrated in figure 6.4.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了局部最小值之外，更大的问题是我们的初始点选择也可能导致牛顿下降收敛到*鞍点*。这是所有下降算法面临的问题，如图6.4所示。
- en: 'A saddle point mimics a local minimizer: at both locations, the gradient of
    the function becomes zero. However, saddle points aren’t true local minimizers:
    the saddle-like shape means that it’s curving upwards in one direction and curving
    downwards in another. This is in contrast to local minimizers, which are bowl-shaped.
    However, both local minimizers and saddle points have zero gradients. This means
    that descent algorithms can’t distinguish between the two and sometimes converge
    to saddle points instead of minimizers.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 鞍点模仿局部最小值：在两个位置，函数的梯度都变为零。然而，鞍点并不是真正的局部最小值：鞍形状意味着它在某个方向上向上弯曲，在另一个方向上向下弯曲。这与局部最小值形成对比，局部最小值是碗状的。然而，局部最小值和鞍点都具有零梯度。这意味着下降算法无法区分两者，有时会收敛到鞍点而不是最小值。
- en: '![CH06_F04_Kunapuli](../Images/CH06_F04_Kunapuli.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F04_Kunapuli](../Images/CH06_F04_Kunapuli.png)'
- en: Figure 6.4 A saddle point of the Branin function lies between two minimizers
    and, like the minimizers, it has a zero gradient at its location. This causes
    all descent methods to converge to saddle points.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 Branin 函数的鞍点位于两个最小化点之间，就像最小化点一样，它在位置处具有零梯度。这导致所有下降方法都收敛到鞍点。
- en: The existence of saddle points and local minimizers depends on the functions
    being optimized, of course. For our purposes, most common loss functions are convex
    and “well shaped,” meaning that we can safely use Newton’s descent and Newton
    boosting. Care should be taken, however, to ensure convexity when creating and
    using custom loss functions. Handling such non-convex loss functions is an active
    and ongoing research area.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 鞍点和局部最小值的存在取决于被优化的函数，当然。对于我们的目的，大多数常见的损失函数都是凸的且“形状良好”，这意味着我们可以安全地使用牛顿下降和牛顿提升。然而，在创建和使用自定义损失函数时，应小心确保凸性。处理这种非凸损失函数是一个活跃且持续的研究领域。
- en: 6.1.2 Newton’s descent over loss functions for training
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 牛顿下降在训练损失函数中的应用
- en: 'So how does Newton’s descent fare on a machine-learning task? To see this,
    we can revisit the simple 2D classification problem from chapter 5, section 5.1.2,
    on which we’ve previously trained a model using gradient descent. The task is
    a binary classification problem, with data generated as shown here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，牛顿下降在机器学习任务中的表现如何？为了看到这一点，我们可以回顾第 5 章第 5.1.2 节中的简单 2D 分类问题，我们之前已经使用梯度下降训练了一个模型。这个任务是一个二元分类问题，数据生成方式如下所示：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We visualize this synthetic data set in figure 6.5.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 6.5 中可视化这个合成数据集。
- en: '![CH06_F05_Kunapuli](../Images/CH06_F05_Kunapuli.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Kunapuli](../Images/CH06_F05_Kunapuli.png)'
- en: Figure 6.5 A (nearly) linearly separable two-class data set over which we’ll
    train a classifier. The positive examples have labels *y* = 1, and the negative
    examples have labels *y* = 0.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 一个（几乎）线性可分的两类数据集，我们将在此数据集上训练分类器。正例的标签 *y* = 1，而负例的标签 *y* = 0。
- en: Recall that we want to train a linear classifier *h*[w](*x*) = *w*[1]*x*[1]
    + *w*[2]*x*[2]. This classifier takes 2D data points *x* = [*x*[1],*x*[2]]' and
    returns a prediction. As in chapter 5, section 5.1.2, we’ll use the squared loss
    function for this task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们想要训练一个线性分类器 *h*[w](*x*) = *w*[1]*x*[1] + *w*[2]*x*[2]。这个分类器接受 2D 数据点
    *x* = [*x*[1],*x*[2]]' 并返回一个预测。正如第 5 章第 5.1.2 节中所述，我们将使用平方损失函数来完成这个任务。
- en: The linear classifier is parameterized by weights *w* = [*w*[1],*w*[2]]'. The
    weights, of course, have to be learned such that they minimize some loss over
    the data to achieve the best training fit.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器由权重 *w* = [*w*[1],*w*[2]]' 参数化。当然，这些权重必须通过学习来最小化数据上的某些损失，以实现最佳的训练拟合。
- en: 'The squared loss measures the error between true labels *y*[i] and their corresponding
    predictions *h*[w](*x*[i]) as shown here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失衡量了真实标签 *y*[i] 与其对应的预测 *h*[w](*x*[i]) 之间的误差，如下所示：
- en: '![CH06_F05_Kunapuli-eqs-20x](../Images/CH06_F05_Kunapuli-eqs-20x.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Kunapuli-eqs-20x](../Images/CH06_F05_Kunapuli-eqs-20x.png)'
- en: Here, *X* is an *n* × *d* data matrix of *n* training examples with *d* features
    each, and y is a *d* × 1 vector of true labels. The expression on the far right
    is a compact way of representing the loss over the entire data set using vector
    and matrix notation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X* 是一个 *n* × *d* 的数据矩阵，包含 *n* 个训练示例，每个示例有 *d* 个特征，而 y 是一个 *d* × 1 的真实标签向量。右侧的表达式是使用向量矩阵符号表示整个数据集损失的紧凑方式。
- en: For Newton’s descent, we’ll need the gradient and Hessian of this loss function.
    These can be obtained by differentiating the loss function analytically, just
    as with the Branin function. In vector-matrix notation, these can also be compactly
    written as
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于牛顿下降，我们需要这个损失函数的梯度和 Hessian。这些可以通过对损失函数进行解析微分获得，就像对 Branin 函数做的那样。在向量矩阵符号中，这些也可以紧凑地写成
- en: '![CH06_F05_Kunapuli-eqs-22x](../Images/CH06_F05_Kunapuli-eqs-22x.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Kunapuli-eqs-22x](../Images/CH06_F05_Kunapuli-eqs-22x.png)'
- en: '![CH06_F05_Kunapuli-eqs-23x](../Images/CH06_F05_Kunapuli-eqs-23x.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Kunapuli-eqs-23x](../Images/CH06_F05_Kunapuli-eqs-23x.png)'
- en: 'Note that the Hessian is a 2 × 2 matrix. The implementations of the loss function,
    its gradient, and Hessian are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Hessian 是一个 2 × 2 矩阵。损失函数、其梯度以及 Hessian 的实现如下：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have all the components of the loss function, we can use Newton’s
    descent to compute an optimal solution, that is, “learn a model.” We can compare
    the model learned by Newton’s descent to the one learned by gradient descent (which
    we implemented in chapter 5). We initialize both gradient descent and Newton’s
    descent with *w* = [0,0,0.99]'':'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了损失函数的所有组成部分，我们可以使用牛顿下降法来计算一个最优解，即“学习一个模型”。我们可以将牛顿下降法学习的模型与梯度下降法（我们在第五章中实现的方法）学习的模型进行比较。我们用*w*
    = [0,0,0.99]初始化梯度下降法和牛顿下降法：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The squared loss function we’re optimizing is convex and has only one minimizer.
    Both gradient descent and Newton’s descent essentially learn the same model, though
    they terminate as soon as they reach the threshold 10-3, roughly the third decimal
    place. We can easily verify that this learned model achieves a training accuracy
    of 99.5%:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在优化的平方损失函数是凸的，并且只有一个最小值。梯度下降法和牛顿下降法本质上学习的是同一个模型，尽管它们在达到阈值10-3时就会停止，大约是第三位小数。我们可以很容易地验证这个学习到的模型达到了99.5%的训练准确率：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: While both gradient descent and Newton’s descent learn the same model, they
    arrive there in decidedly different ways, as figure 6.6 illustrates.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然梯度下降法和牛顿下降法学习的是同一个模型，但它们到达那里的方式截然不同，如图6.6所示。
- en: '![CH06_F06_Kunapuli](../Images/CH06_F06_Kunapuli.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Kunapuli](../Images/CH06_F06_Kunapuli.png)'
- en: Figure 6.6 The solution paths of Newton’s descent (solid line) versus gradient
    descent (dotted line) as well as the models produced by Newton’s and gradient
    descent. Gradient descent takes 20 iterations to learn this model, while Newton’s
    descent takes 12 iterations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 牛顿下降法（实线）与梯度下降法（虚线）的解路径，以及牛顿下降法和梯度下降法产生的模型。梯度下降法需要20次迭代来学习这个模型，而牛顿下降法只需要12次迭代。
- en: The key takeaway is that Newton’s descent is a powerful optimization method
    in the family of descent methods. It converges to solutions much faster because
    it takes local second-order derivative information (essentially curvature) into
    account in constructing descent directions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要点是牛顿下降法是下降法家族中的一种强大优化方法。因为它在构建下降方向时考虑了局部二阶导数信息（本质上就是曲率），所以它比其他方法更快地收敛到解。
- en: 'This additional information about the shape of the objective (or loss) function
    being optimized greatly aids convergence. This comes at a computational cost,
    however: with more variables, the second derivative, or the Hessian, which holds
    the second-order information, becomes increasingly difficult to manage, especially
    as it has to be inverted.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化（或损失）函数形状的额外信息极大地促进了收敛。然而，这也带来了计算成本：随着变量的增加，二阶导数或Hessian（包含二阶信息）变得越来越难以管理，尤其是当它需要求逆时。
- en: As we’ll see in the next section, Newton boosting avoids computing or inverting
    the entire Hessian matrix by using an approximation with *pointwise second derivatives*,
    essentially second derivatives computed and inverted per training example, which
    keeps training efficient.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在下一节中将要看到的，牛顿提升法通过使用带有*点wise二阶导数*的近似来避免计算或求逆整个Hessian矩阵，这实际上是在每个训练示例上计算和求逆的二阶导数，从而保持训练效率。
- en: '6.2 Newton boosting: Newton’s method + boosting'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 牛顿提升法：牛顿法 + 提升法
- en: We begin our deep dive into Newton boosting by gaining an intuitive understanding
    of how Newton boosting differs from gradient boosting. We’ll compare the two methods
    side by side to see exactly what Newton boosting adds to each iteration.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过获得牛顿提升法与梯度提升法差异的直观理解开始深入研究牛顿提升法。我们将对比这两种方法，以确切了解牛顿提升法为每次迭代添加了什么。
- en: '6.2.1 Intuition: Learning with weighted residuals'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 直觉：使用加权残差进行学习
- en: 'As with other boosting methods, Newton boosting learns a new weak estimator
    every iteration such that it fixes the misclassifications or errors made by the
    previous iteration. AdaBoost identifies and characterizes misclassified examples
    that need attention by assigning *weights* to them: badly misclassified examples
    are assigned higher weights. A weak classifier trained on such weighted examples
    will focus on them more during learning.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他提升法一样，牛顿提升法在每次迭代中都学习一个新的弱估计器，以便纠正前一次迭代中犯的错误或错误。AdaBoost通过给它们分配*权重*来识别和描述需要关注的误分类示例：错误分类严重的示例被分配更高的权重。在这样加权的示例上训练的弱分类器会在学习过程中更加关注它们。
- en: Gradient boosting characterizes misclassified examples that need attention through
    *residuals*. A residual is simply another means to measure the extent of misclassification
    and is computed as the gradient of the loss function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升通过残差来表征需要关注的误分类示例。残差简单地是衡量误分类程度的一种方法，其计算方式为损失函数的梯度。
- en: 'Newton boosting does both and uses *weighted residuals*! The residuals in Newton
    boosting are computed in exactly the same way as in gradient boosting: using the
    gradient of the loss function (the first derivative). The weights, on the other
    hand, are computed using the Hessian of the loss function (the second derivative).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿提升既使用加权残差！牛顿提升中的残差计算方式与梯度提升完全相同：使用损失函数的梯度（一阶导数）。另一方面，权重是通过损失函数的Hessian矩阵（二阶导数）来计算的。
- en: Newton boosting is Newton’s descent + boosting
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿提升是牛顿下降 + 提升法
- en: 'As we saw in chapter 5, each gradient-boosting iteration mimics gradient descent.
    At iteration *t*, gradient descent updates the model f[t] using the gradient of
    the loss function (▽*L*(*f*[*t*]) = *g*[*t*]):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在第5章中看到的，每次梯度提升迭代都模拟梯度下降。在迭代 *t* 时，梯度下降使用损失函数的梯度（▽*L*(*f*[*t*]) = *g*[*t*])来更新模型
    f[t]:'
- en: '![CH06_F06_Kunapuli-eqs-25x](../Images/CH06_F06_Kunapuli-eqs-25x.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Kunapuli-eqs-25x](../Images/CH06_F06_Kunapuli-eqs-25x.png)'
- en: 'Rather than compute the overall gradient directly, g[t], gradient boosting
    learns a weak estimator (*h*[*t*]^(*GB*)) over the individual gradients, which
    are also residuals. That is, a weak estimator is trained over the data and corresponding
    residuals (*x*[*i*] – *g*[*i*](*x*[*i*]))^(*n*)[*i*=1]. The model is then updated
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接计算整体梯度 g[t] 不同，梯度提升学习在单个梯度（也是残差）上学习一个弱估计器 (*h*[*t*]^(*GB*))。也就是说，弱估计器在数据及其对应的残差
    (*x*[*i*] – *g*[*i*](*x*[*i*]))^(*n*)[*i*=1] 上进行训练。然后，模型按照以下方式更新：
- en: '![CH06_F06_Kunapuli-eqs-28x](../Images/CH06_F06_Kunapuli-eqs-28x.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Kunapuli-eqs-28x](../Images/CH06_F06_Kunapuli-eqs-28x.png)'
- en: 'Similarly, Newton boosting mimics Newton’s descent. At iteration *t*, Newton’s
    descent updates the model f[t] using the gradient of the loss function (▽*L*(*f*[*t*])
    = *g*[*t*]) (exactly as with the earlier gradient descent) and the Hessian of
    the loss function (▽²*L*(*f*[*t*]) = *He*[*t*]):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '同样地，牛顿提升模拟了牛顿下降。在迭代 *t* 时，牛顿下降使用损失函数的梯度（▽*L*(*f*[*t*]) = *g*[*t*])（与早期的梯度下降完全一样）和损失函数的Hessian矩阵（▽²*L*(*f*[*t*])
    = *He*[*t*])来更新模型 f[t]:'
- en: '![CH06_F06_Kunapuli-eqs-31x](../Images/CH06_F06_Kunapuli-eqs-31x.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Kunapuli-eqs-31x](../Images/CH06_F06_Kunapuli-eqs-31x.png)'
- en: Computing the Hessian can often be very computationally expensive. Newton boosting
    avoids the expense of computing the gradient or the Hessian by learning a weak
    estimator over the individual gradients and Hessians.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Hessian矩阵通常可能非常计算量大。牛顿提升通过在单个梯度上学习一个弱估计器来避免计算梯度或Hessian的开销。
- en: For each training example, in addition to the gradient residual, we have to
    incorporate the Hessian information as well, all the while ensuring that the overall
    weak estimator that we want to train approximates Newton’s descent. How do we
    do this?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个训练示例，除了梯度残差外，我们还要结合Hessian信息，同时确保我们想要训练的整体弱估计器近似牛顿下降。我们如何做到这一点？
- en: Observe that the Hessian matrix is inverted in the Newton update (*He*[*t*]^(-1)).
    For a single training example, the second (functional) derivative will be a scalar
    (a single number instead of a matrix). This means that the term *He*[*t*]^(-1)*g*[*t*]
    becomes (*g*[*t*](*x*[*i*]))/(*He*[*t*](*x*[*i*])); these are simply the residuals
    *g*[t](*x*[i]) weighted by the Hessians (*g*[*t*](*x*[*i*]))/(*He*[*t*](*x*[*i*])).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到牛顿更新中Hessian矩阵被求逆（*He*[*t*]^(-1)）。对于单个训练示例，二阶（函数）导数将是一个标量（一个数字而不是矩阵）。这意味着项
    *He*[*t*]^(-1)*g*[*t*] 变成了 (*g*[*t*](*x*[*i*]))/(*He*[*t*](*x*[*i*]))；这些是简单的残差
    *g*[t](*x*[i])，由Hessian矩阵加权（*g*[*t*](*x*[*i*]))/(*He*[*t*](*x*[*i*])）。
- en: 'Thus, for Newton boosting, we train a weak estimator (*h*[*t*]^(*NB*)) using
    Hessian-weighted gradient residuals, that is, (*x*[*i*], -(*g*[*i*](*x*[*i*]))/(*He*[*i*](*x*[*i*])))^(*n*)[*i*=1],
    and, voilà, we can update our ensemble in exactly the same way as gradient boosting:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于牛顿提升，我们使用Hessian加权的梯度残差（即 (*x*[*i*], -(*g*[*i*](*x*[*i*]))/(*He*[*i*](*x*[*i*])))^(*n*)[*i*=1]
    来训练一个弱估计器 (*h*[*t*]^(*NB*))，然后，我们可以像梯度提升一样更新我们的集成：
- en: '![CH06_F06_Kunapuli-eqs-37x](../Images/CH06_F06_Kunapuli-eqs-37x.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Kunapuli-eqs-37x](../Images/CH06_F06_Kunapuli-eqs-37x.png)'
- en: In summary, Newton boosting uses Hessian-weighted residuals, while gradient
    boosting uses unweighted residuals.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，牛顿提升使用Hessian加权的残差，而梯度提升使用未加权的残差。
- en: What do the Hessians add?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 汉萨军有什么作用？
- en: 'So, what kind of additional information do these Hessian-based weights add
    to boosting? Mathematically, Hessians, or second derivatives, correspond to the
    curvature or how “curvy” a function is. In Newton boosting, we weight gradients
    by second-derivative information for each training example *x*[i]:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些基于海森矩阵的权重为提升法增加了什么样的额外信息？从数学上讲，海森矩阵或二阶导数对应于曲率或函数的“弯曲程度”。在牛顿提升法中，我们通过每个训练例子*x*[i]的二阶导数信息来加权梯度：
- en: '![CH06_F06_Kunapuli-eqs-38x](../Images/CH06_F06_Kunapuli-eqs-38x.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Kunapuli-eqs-38x](../Images/CH06_F06_Kunapuli-eqs-38x.png)'
- en: A large value of the second derivative *He*[t](*x*[i]) implies that the curvature
    of the function is large at *x*[i]. At these curvy regions, the Hessian weight
    decreases the gradient, which, in turn, leads Newton boosting to take smaller,
    more conservative steps.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数*He*[t](*x*[i])的大值意味着在*x*[i]处函数的曲率很大。在这些弯曲区域，海森矩阵的权重会降低梯度，这反过来又导致牛顿提升法采取更小、更保守的步骤。
- en: Conversely, if the second derivative *He*[t](*x*[i]) is small, then the curvature
    at *x*[i] is small, meaning that the function is rather flat. In such situations,
    the Hessian weight allows Newton’s descent to take large, bolder steps so it can
    traverse the flat area faster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果二阶导数*He*[t](*x*[i])很小，那么在*x*[i]处的曲率就很小，这意味着函数相当平坦。在这种情况下，海森矩阵的权重允许牛顿下降法采取更大、更大胆的步骤，以便更快地穿越平坦区域。
- en: 'Thus, second derivatives combined with first-derivative residuals can capture
    the notion of “misclassification” very effectively. Let’s see this in action over
    a commonly used loss function—the logistic loss—which measures the extent of the
    mis- classification:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，二阶导数与一阶导数残差相结合可以非常有效地捕捉“误分类”的概念。让我们通过一个常用的损失函数——逻辑损失函数来观察这一过程，逻辑损失函数衡量了误分类的程度：
- en: '![CH06_F06_Kunapuli-eqs-39x](../Images/CH06_F06_Kunapuli-eqs-39x.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Kunapuli-eqs-39x](../Images/CH06_F06_Kunapuli-eqs-39x.png)'
- en: The logistic loss is compared to the squared loss function in figure 6.7 (left).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑损失与平方损失函数在图6.7（左侧）中进行了比较。
- en: '![CH06_F07_Kunapuli](../Images/CH06_F07_Kunapuli.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F07_Kunapuli](../Images/CH06_F07_Kunapuli.png)'
- en: 'Figure 6.7 Left: Logistic loss versus squared loss function; Center: Negative
    gradient and Hessian of the logistic loss; Right: Hessian-scaled negative gradient
    of the logistic loss'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 左：逻辑损失与平方损失函数；中：逻辑损失的负梯度及海森矩阵；右：逻辑损失的海森矩阵缩放负梯度
- en: 'In figure 6.7 (center), we look at the logistic loss function and its corresponding
    gradient (first derivative) and Hessian (second derivative). All of these are
    functions of the misclassification margin: the product of the true label (*y*)
    and the prediction (*f*(*x*)). If *y* and *f*(*x*) have opposite signs, then we
    have *y* ⋅ *f*(*x*) < 0\. In this case, the true label doesn’t match the predicted
    label, and we have a misclassification. Thus, the left part of the logistic loss
    curve (with negative margins) corresponds to misclassified examples and measures
    the extent of the misclassification. Similarly, the right part of the logistic
    loss (with positive margins) corresponds to correctly classified examples, whose
    loss is nearly 0, as we expect.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.7（中央），我们观察逻辑损失函数及其相应的梯度（一阶导数）和海森矩阵（二阶导数）。所有这些都是误分类边界的函数：真实标签（*y*）和预测（*f*(*x*)）的乘积。如果*y*和*f*(*x*)具有相反的符号，那么我们就有*y*
    ⋅ *f*(*x*) < 0。在这种情况下，真实标签与预测标签不匹配，我们有一个误分类。因此，逻辑损失曲线的左侧部分（具有负边界）对应于误分类的例子，并衡量了误分类的程度。同样，逻辑损失曲线的右侧部分（具有正边界）对应于正确分类的例子，其损失接近0，正如我们所期望的。
- en: The second derivative achieves its highest values around 0, which corresponds
    to the elbow of the logistic loss function. This isn’t surprising because we can
    see that the logistic loss function is curviest around the elbow and flat to the
    left and right of the elbow.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数在其最高值大约在0处，这对应于逻辑损失函数的拐点。这并不令人惊讶，因为我们可以看到逻辑损失函数在拐点附近是最弯曲的，而在拐点的左右两侧则是平坦的。
- en: In figure 6.7 (right), we can see the effect of weighting the gradients. For
    well-classified examples (*y* ⋅ *f*(*x*) > 0), the overall gradient as well as
    the weighted gradient are 0\. This means that these examples won’t participate
    in the boosting iteration.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.7（右侧），我们可以看到加权梯度的效果。对于正确分类的例子（*y* ⋅ *f*(*x*) > 0），整体梯度和加权梯度都是0。这意味着这些例子不会参与提升迭代。
- en: On the other hand, for misclassified examples (*y* ⋅ *f*(*x*) < 0), the overall
    weighed gradient (*g*[*i*](*x*[*i*]))/(*He*[*i*](*x*[*i*])) increases steeply
    with misclassification. In general, it increases far more steeply than the unweighted
    gradient.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于被错误分类的例子（*y* ⋅ *f*(*x*) < 0），整体加权梯度 (*g*[*i*](*x*[*i*]))/(*He*[*i*](*x*[*i*]))
    会随着错误分类而急剧增加。一般来说，它比未加权的梯度增加得更加陡峭。
- en: 'Now, we can answer the question of what the Hessians do: they incorporate local
    curvature information to ensure that badly misclassified training examples get
    higher weights. This is illustrated in figure 6.8.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以回答Hessian的作用问题：它们通过引入局部曲率信息，确保错误分类严重的训练示例获得更高的权重。这如图6.8所示。
- en: '![CH06_F08_Kunapuli](../Images/CH06_F08_Kunapuli.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Kunapuli](../Images/CH06_F08_Kunapuli.png)'
- en: Figure 6.8 Unweighted residuals (left) used by gradient boosting compared to
    Hessian-weighted residuals (right) used by Newton boosting. Positive values of
    misclassification margin (*y* ⋅ *f*(*x*) > 0) indicate correct classification.
    For misclassifications, we have *y* ⋅ *f*(*x*) < 0\. For badly misclassified examples,
    the Hessian-weighted gradients capture that notion more effectively than unweighted
    gradients.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 梯度提升法使用的未加权残差（左）与牛顿提升法使用的Hessian加权残差（右）的比较。错误分类边缘的正值（*y* ⋅ *f*(*x*) > 0）表示正确分类。对于错误分类，我们有
    *y* ⋅ *f*(*x*) < 0。对于错误分类严重的例子，Hessian加权的梯度比未加权的梯度更有效地捕捉这一概念。
- en: The more badly misclassified a training example is, the further to the left
    it will be in figure 6.8\. Hessian weighting of residuals ensures that training
    examples further to the left will get higher weights. This is in sharp contrast
    to gradient boosting, which is unable to differentiate training examples as effectively
    because it only uses unweighted residuals.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 训练示例的错误分类越严重，它在图6.8中的位置就越靠左。残差的Hessian加权确保了更靠左的训练示例将获得更高的权重。这与梯度提升法形成鲜明对比，因为梯度提升法无法像使用未加权残差那样有效地区分训练示例。
- en: To summarize, Newton boosting aims to use both first-derivative (gradient) information
    and second-derivative (Hessian) information to ensure that misclassified training
    examples receive focus based on the extent of the misclassification.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，牛顿提升旨在使用一阶导数（梯度）信息和二阶导数（Hessian）信息，以确保根据错误分类的程度，错误分类的训练示例得到关注。
- en: '6.2.2 Intuition: Learning with regularized loss functions'
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 直觉：使用正则化损失函数进行学习
- en: Before proceeding, let’s introduce the notion of *regularized loss functions*.
    A regularized loss function contains an additional smoothing term along with the
    loss function, making it more convex, or bowl-like.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们引入正则化损失函数的概念。正则化损失函数包含一个额外的平滑项和损失函数，使其更加凸形，或像碗一样。
- en: Regularizing a loss function introduces additional structure to the learning
    problem, which often stabilizes and accelerates the resulting learning algorithms.
    Regularization also allows us to control the complexity of the model being learned
    and improves the overall robustness and generalization capabilities of the model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化损失函数为学习问题引入了额外的结构，这通常可以稳定并加速由此产生的学习算法。正则化还允许我们控制正在学习的模型复杂度，并提高模型的总体鲁棒性和泛化能力。
- en: Essentially, a regularized loss function explicitly captures the fit versus
    complexity tradeoff inherent in most machine-learning models (see chapter 1, section
    1.3).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，正则化损失函数明确地捕捉了大多数机器学习模型中固有的拟合与复杂度之间的权衡（参见第1章，第1.3节）。
- en: 'A regularized loss function is of the following form:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化损失函数具有以下形式：
- en: '![CH06_F08_Kunapuli-eqs-41x](../Images/CH06_F08_Kunapuli-eqs-41x.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Kunapuli-eqs-41x](../Images/CH06_F08_Kunapuli-eqs-41x.png)'
- en: 'The regularization term measures the flatness (the opposite of “curviness”)
    of the model: the more it’s minimized, the less complex the learned model is.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项衡量模型的平坦度（“曲率”的相反面）：它越被最小化，学习到的模型就越简单。
- en: 'The loss term measures the fit to the training data through a loss function:
    the more it’s minimized, the better the fit to the training data. The regularization
    parameter α trades off between these two competing objectives (in section 1.3,
    this tradeoff was achieved by the parameter *C*, which is essentially the inverse
    of *α*):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 损失项通过损失函数衡量对训练数据的拟合程度：它越被最小化，对训练数据的拟合就越好。正则化参数 α 在这两个相互竞争的目标之间进行权衡（在第1.3节中，这种权衡是通过参数
    *C* 实现的，它实际上是 *α* 的倒数）：
- en: A large value of α means the model will focus more on regularization and simplicity
    and less on training error, which causes the model to have higher training error
    and underfit.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α的较大值意味着模型将更多地关注正则化和简洁性，而较少关注训练误差，这导致模型具有更高的训练误差和欠拟合。
- en: A small value of α means the model will focus more on training errors and learn
    more complex models, which causes the model to have lower training errors and
    possibly overfit.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α的较小值意味着模型将更多地关注训练误差，并学习更复杂的模型，这导致模型具有更低的训练误差，并可能过拟合。
- en: Thus, a regularized loss function allows us to trade off between fit and complexity
    during learning, ultimately leading to models that generalize well in practice.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正则化损失函数使我们能够在学习过程中在拟合和复杂度之间进行权衡，最终导致在实践中具有良好泛化能力的模型。
- en: As we saw in chapter 1, section 1.3, there are several ways to introduce regularization
    and control model complexity during learning. For example, limiting the maximum
    depth of trees or the number of nodes prevents trees from overfitting.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第1章第1.3节中看到的，在学习过程中有几种方法可以引入正则化并控制模型复杂度。例如，限制树的最大深度或节点数可以防止树过拟合。
- en: 'Another common approach is through L2 regularization, which amounts to introducing
    a penalty over the model directly. That is, if we have a model *f*(*x*), L2 regularization
    introduces a penalty over model by *f*(*x*)²:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的方法是通过L2正则化，这相当于直接对模型引入惩罚。也就是说，如果我们有一个模型*f*(*x*)，L2正则化通过*f*(*x*)²对模型引入惩罚：
- en: '![CH06_F08_Kunapuli-eqs-42x](../Images/CH06_F08_Kunapuli-eqs-42x.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Kunapuli-eqs-42x](../Images/CH06_F08_Kunapuli-eqs-42x.png)'
- en: The loss functions of many common machine-learning approaches can be expressed
    in this form. In chapter 5, we implemented the gradient-boosting algorithm for
    the *unregularized* squared loss function as
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 许多常见机器学习方法的损失函数都可以用这种形式表示。在第5章中，我们实现了对**未正则化**的平方损失函数的梯度提升算法，如下所示：
- en: '![CH06_F08_Kunapuli-eqs-43x](../Images/CH06_F08_Kunapuli-eqs-43x.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Kunapuli-eqs-43x](../Images/CH06_F08_Kunapuli-eqs-43x.png)'
- en: between the true label *y* and the predicted label *f*(*x*). In this setting,
    unregularized loss functions simply have the regularization parameter α = 0.1.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实标签*y*和预测标签*f*(*x*)之间。在这个设置中，未正则化的损失函数简单地具有正则化参数α = 0.1。
- en: 'We’ve already seen an example of a regularized loss function (chapter 1, section
    1.3.2) with support vector machines (SVMs), which use the regularized hinge loss
    function:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一个正则化损失函数的例子（第1章第1.3.2节），即支持向量机（SVMs），它们使用正则化截断损失函数：
- en: '![CH06_F08_Kunapuli-eqs-44x](../Images/CH06_F08_Kunapuli-eqs-44x.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Kunapuli-eqs-44x](../Images/CH06_F08_Kunapuli-eqs-44x.png)'
- en: In this chapter, we consider the *regularized logistic loss* function, which
    is commonly used in logistic regression as
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考虑**正则化逻辑损失**函数，它在逻辑回归中常用，如下所示：
- en: '![CH06_F08_Kunapuli-eqs-45x](../Images/CH06_F08_Kunapuli-eqs-45x.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Kunapuli-eqs-45x](../Images/CH06_F08_Kunapuli-eqs-45x.png)'
- en: which augments the standard logistic loss log(1 + *e*^(-*y*⋅*f*(*x*))), with
    a regularization term α ⋅ *f*(*x*)². Figure 6.9 illustrates the regularized logistic
    loss for α = 0.1\. Observe how the regularization term makes the overall loss
    function’s profile curvier and more bowl-like.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这增加了标准逻辑损失log(1 + *e*^(-*y*⋅*f*(*x*)))，并引入了一个正则化项α ⋅ *f*(*x*)²。图6.9展示了α = 0.1时的正则化逻辑损失。观察正则化项如何使整体损失函数的轮廓更加弯曲，更像碗形。
- en: 'Regularization parameter α trades off between fit and complexity: as α is increased,
    the regularization effect will increase, making the overall surface more convex
    and ignoring the contributions of the loss function. Because the loss function
    affects the fit, over-regularizing the model (by setting high values of alpha)
    will lead to underfitting.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化参数α在拟合和复杂度之间进行权衡：随着α的增加，正则化效果会增加，使整体表面更加凸起，并忽略损失函数的贡献。因为损失函数影响拟合，过度正则化模型（通过设置高α值）会导致欠拟合。
- en: '![CH06_F09_Kunapuli](../Images/CH06_F09_Kunapuli.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F09_Kunapuli](../Images/CH06_F09_Kunapuli.png)'
- en: Figure 6.9 The standard logistic loss function (left) versus the regularized
    logistic loss function (right), which is curvier and has a better-defined minimum
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 标准逻辑损失函数（左）与正则化逻辑损失函数（右）的比较，后者更弯曲，具有更明确的极小值
- en: 'The gradient and Hessian of the regularized logistic loss function can be computed
    as the first and second derivatives with respect to the model’s prediction (*f*(x)):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化逻辑损失函数的梯度和对Hessian的计算可以表示为对模型预测(*f*(x))的一阶和二阶导数：
- en: '![CH06_F09_Kunapuli-eqs-47x](../Images/CH06_F09_Kunapuli-eqs-47x.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F09_Kunapuli-eqs-47x](../Images/CH06_F09_Kunapuli-eqs-47x.png)'
- en: '![CH06_F09_Kunapuli-eqs-48x](../Images/CH06_F09_Kunapuli-eqs-48x.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F09_Kunapuli-eqs-48x](../Images/CH06_F09_Kunapuli-eqs-48x.png)'
- en: The following listing implements functions to compute the regularized logistic
    loss, with the value of the parameter α = 0.1.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表实现了计算正则化逻辑损失的函数，参数α的值为0.1。
- en: Listing 6.2 Regularized logistic loss, gradient, and Hessian with λ = 0.1
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2 正则化逻辑损失、梯度和Hessian，λ = 0.1
- en: '[PRE9]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These functions can now be used to compute the residuals and corresponding Hessian
    weights that we’ll need for Newton boosting.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数现在可以用来计算残差和相应的Hessian权重，这些是我们将用于牛顿提升所需的。
- en: 6.2.3 Implementing Newton boosting
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 实现牛顿提升
- en: 'In this section, we’ll develop our own implementation of Newton boosting. The
    basic algorithm can be outlined with the following pseudocode:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发我们自己的牛顿提升实现。基本算法可以用以下伪代码概述：
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![CH06_F09_Kunapuli-eqs-49x](../Images/CH06_F09_Kunapuli-eqs-49x.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F09_Kunapuli-eqs-49x](../Images/CH06_F09_Kunapuli-eqs-49x.png)'
- en: '[PRE11]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Unsurprisingly, this training procedure is the same as gradient boosting, with
    the only change being the computation of Hessian-weighted residuals in steps 1
    and 2\. Because the general algorithmic framework for gradient and Newton boosting
    is the same, we can combine and implement them together. The following listing
    extends listing 5.2 to incorporate Newton boosting, which it uses for training
    only with the following flag: use_Newton=True.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，这个训练过程与梯度提升相同，唯一的区别在于步骤1和2中计算Hessian加权的残差。因为梯度和牛顿提升的通用算法框架是相同的，我们可以将它们合并并一起实现。以下列表扩展了列表5.2以包含牛顿提升，它使用以下标志仅用于训练：use_Newton=True。
- en: Listing 6.3 Newton boosting for the regularized logistic loss
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3 正则化逻辑损失的牛顿提升
- en: '[PRE12]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Gets dimensions of the data set
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取数据集的维度
- en: ❷ Initializes an empty ensemble
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化一个空的集成
- en: ❸ Predictions of the ensemble on the training set
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练集上的集成预测
- en: ❹ If Newton boosting, computes Hessian-weighted residuals
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果是牛顿提升，则计算Hessian加权的残差
- en: ❺ Else computes unweighted residuals for gradient boosting
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 否则计算梯度提升的无权残差
- en: ❻ Fits weak regression tree (*h*[t]) to the examples and residuals
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将弱回归树(*h*[t])拟合到示例和残差
- en: ❼ Gets predictions of the weak learner, *h*[t]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 获取弱学习者的预测，*h*[t]
- en: ❽ Sets up the loss function as a line search problem
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将损失函数设置为线搜索问题
- en: ❾ Finds the best step length using the golden section search
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 使用黄金分割搜索找到最佳步长
- en: ❿ Updates the ensemble predictions
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 更新集成预测
- en: ⓫ Updates the ensemble
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 更新集成
- en: Once the model is learned, we can make predictions exactly as with AdaBoost
    or gradient boosting because the ensemble learned is a sequential ensemble. The
    following listing is the same prediction function used by these previously introduced
    methods, repeated here for convenience.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被学习，我们就可以像AdaBoost或梯度提升一样进行预测，因为学到的集成是一个顺序集成。以下列表是这些先前介绍的方法使用的相同预测函数，在此重复以方便起见。
- en: Listing 6.4 Predictions of Newton boosting
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 牛顿提升的预测
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Initializes all the predictions to 0
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将所有预测初始化为0
- en: ❷ Aggregates individual predictions from each regressor
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 聚合每个回归器的单个预测
- en: ❸ Converts weighted predictions to -1/1 labels
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将加权预测转换为-1/1标签
- en: 'Let’s compare the performance of our implementations of gradient boosting (from
    the previous chapter) and Newton boosting:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较我们实现的梯度提升（来自上一章）和牛顿提升的性能：
- en: '[PRE14]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Converts training labels to -1/1
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将训练标签转换为-1/1
- en: ❷ Splits into train and test sets
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分割为训练集和测试集
- en: ❸ Newton boosting
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 牛顿提升
- en: ❹ Gradient boosting
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 梯度提升
- en: 'We see that Newton boosting produces a test error of around 8% compared to
    gradient boosting, which achieves 12%:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与梯度提升相比，牛顿提升的测试误差约为8%，而梯度提升达到12%：
- en: '[PRE15]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Visualizing gradient-boosting iterations
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化梯度提升迭代
- en: 'Now that we have our joint gradient-boosting and Newton-boosting implementation
    (listing 6.3), we can compare the behaviors of both of these algorithms. First,
    note that they both train and grow their ensembles in roughly the same way. The
    key difference between them is in the residuals they use for ensemble training:
    gradient boosting uses the negative gradients directly as residuals, whereas Newton
    boosting uses the negative Hessian-weighted gradients.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的联合梯度提升和牛顿提升实现（列表6.3），我们可以比较这两个算法的行为。首先，注意它们都以大致相同的方式训练和增长它们的集成。它们之间的关键区别在于它们用于集成训练的残差：梯度提升直接使用负梯度作为残差，而牛顿提升使用负Hessian加权的梯度。
- en: Let’s step through the first few iterations to see what the effect of Hessian
    weighting is. In the first iteration, both gradient and Newton boosting are initialized
    with *F*(*x*[i]) = 0.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步查看前几次迭代，看看Hessian加权的效应。在第一次迭代中，梯度提升和牛顿提升都初始化为*F*(*x*[i]) = 0。
- en: Both gradient and Newton boosting use residuals as a means to measure the extent
    of misclassification so that the most misclassified training examples can get
    more attention in the current iteration. In figure 6.10, the very first iteration,
    the effect of Hessian weighting is immediately observable. Using second-derivative
    information to weight the residuals increases the separation between the two classes,
    making them easier to classify.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升和牛顿提升都使用残差作为衡量错误分类程度的手段，以便在当前迭代中，最被错误分类的训练示例能够获得更多关注。在图6.10的第一个迭代中，Hessian加权的效应立即可见。使用二阶导数信息对残差进行加权增加了两个类之间的分离，使得它们更容易被分类。
- en: '![CH06_F10_Kunapuli](../Images/CH06_F10_Kunapuli.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F10_Kunapuli](../Images/CH06_F10_Kunapuli.png)'
- en: 'Figure 6.10 Iteration 1: Negative gradients (left) as residuals in gradient
    boosting versus Hessian-weighted negative gradients (right) as residuals in Newton
    boosting'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 迭代1：梯度提升中的负梯度（左）作为残差与牛顿提升中的Hessian加权的负梯度（右）作为残差
- en: This behavior can also be seen in the second (figure 6.11) and third (figure
    6.12) iterations, where Hessian weighting enables greater stratification of misclassifications,
    enabling the weak learning algorithm to construct more effective weak learners.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为也可以在第二次（图6.11）和第三次（图6.12）迭代中看到，其中Hessian加权使得错误分类的分层更加明显，使得弱学习算法能够构建更有效的弱学习器。
- en: '![CH06_F11_Kunapuli](../Images/CH06_F11_Kunapuli.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F11_Kunapuli](../Images/CH06_F11_Kunapuli.png)'
- en: 'Figure 6.11 Iteration 2: Negative gradients (left) as residuals in gradient
    boosting versus Hessian-weighted negative gradients (right) as residuals in Newton
    boosting'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 迭代2：梯度提升中的负梯度（左）作为残差与牛顿提升中的Hessian加权的负梯度（右）作为残差
- en: '![CH06_F12_Kunapuli](../Images/CH06_F12_Kunapuli.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F12_Kunapuli](../Images/CH06_F12_Kunapuli.png)'
- en: 'Figure 6.12 Iteration 3: Negative gradients (left) as residuals in gradient
    boosting versus Hessian-weighted negative gradients (right) as residuals in Newton
    boosting'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 迭代3：梯度提升中的负梯度（左）作为残差与牛顿提升中的Hessian加权的负梯度（右）作为残差
- en: In summary, Newton boosting aims to use both first-derivative (gradient) information
    and second-derivative (Hessian) information to ensure that misclassified training
    examples receive increased attention dependent on the extent of the misclassification.
    Figure 6.12 illustrates how Newton boosting grows the ensemble and decreases the
    error steadily over successive iterations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，牛顿提升旨在同时使用一阶导数（梯度）信息和二阶导数（Hessian）信息，以确保被错误分类的训练示例根据错误分类的程度获得更多关注。图6.12展示了牛顿提升如何在连续迭代中逐步增长集成并降低误差。
- en: We can observe the progression of the Newton-boosting classifier in figure 6.13
    across many iterations, as more and more base estimators are added to the ensemble.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图6.13中观察到牛顿提升分类器在许多迭代中的进展，随着越来越多的基估计器被添加到集成中。
- en: '![CH06_F13_Kunapuli](../Images/CH06_F13_Kunapuli.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F13_Kunapuli](../Images/CH06_F13_Kunapuli.png)'
- en: Figure 6.13 Newton boosting across 20 iterations
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 Newton boosting经过20次迭代
- en: '6.3 XGBoost: A framework for Newton boosting'
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 XGBoost：牛顿提升框架
- en: XGBoost, or eXtreme Gradient Boosting, is an open source gradient-boosting framework
    (originated from a research project by Tianqi Chen). It gained widespread recognition
    and adoption, especially in the data science competition community, after its
    success in the Higgs Boson Machine Learning Challenge.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，或极端梯度提升，是一个开源的梯度提升框架（起源于陈天奇的研究项目）。它在希格斯玻色子机器学习挑战赛中的成功之后，在数据科学竞赛社区中获得了广泛的认可和采用。
- en: XGBoost has since evolved into a powerful boosting framework that provides parallelization
    and distributed processing capabilities that allow it to scale to very large data
    sets. Today, XGBoost is available in many languages, including Python, R, and
    C/C++, and it’s deployed on several data science platforms such as Apache Spark
    and H2O.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost已经发展成为一个功能强大的提升框架，它提供了并行化和分布式处理能力，使其能够扩展到非常大的数据集。今天，XGBoost可用在多种语言中，包括Python、R和C/C++，并且部署在多个数据科学平台，如Apache
    Spark和H2O。
- en: 'XGBoost has several key features that make it applicable in a variety of domains
    as well as for large-scale data:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost具有几个关键特性，使其适用于各种领域以及大规模数据：
- en: Newton boosting on regularized loss functions to directly control the complexity
    of the regression tree functions (weak learners) that constitute the ensemble
    (section 6.3.1)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在正则化损失函数上使用牛顿提升来直接控制构成集成（6.3.1节）的回归树函数（弱学习器）的复杂性
- en: Algorithmic speedups such as weighted quantile sketch, a variant of the histogram-based
    split-finding algorithm (that LightGBM uses) for faster training (section 6.3.1)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法加速，如加权分位数草图，这是基于直方图分割查找算法（LightGBM使用的）的一种变体，用于更快地训练（6.3.1节）
- en: Support for a large number of loss functions for classification, regression,
    and ranking, as well as application-specific custom loss functions, similar to
    LightGBM
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持大量用于分类、回归和排序的损失函数，以及类似LightGBM的应用特定自定义损失函数
- en: Block-based system design that stores data in memory in smaller units called
    blocks; this allows for parallel learning, better caching, and efficient multithreading
    (these details are out of scope for this book)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于块的系统设计，将数据存储在内存中称为块的小单元中；这允许并行学习、更好的缓存和有效的多线程（这些细节超出了本书的范围）
- en: Because it’s impossible to detail all the features available in XGBoost in this
    limited space, this section and the next introduce XGBoost, its usage, and applications
    in practical settings. This will enable you to springboard further into advanced
    use cases of XGBoost for your applications through its documentation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在这个有限的空间中无法详细说明XGBoost的所有功能，本节和下一节将介绍XGBoost、其实际应用中的使用和应用程序。这将使您能够通过其文档进一步深入到XGBoost的高级用例。
- en: 6.3.1 What makes XGBoost “extreme”?
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 XGBoost“极端”的原因是什么？
- en: In a nutshell, XGBoost is extreme due to Newton boosting with regularized loss
    functions, efficient tree learning, and a parallelizable implementation. In particular,
    the success of XGBoost lies in the fact that its boosting implementation feature
    conceptual and algorithmic improvements designed *specifically* for tree-based
    learning. In this section, we’ll focus on how XGBoost improves the robustness
    and generalizability of tree-based ensembles so efficiently.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，XGBoost之所以极端，是因为它使用了正则化损失函数的牛顿提升、高效的树学习以及可并行化的实现。特别是，XGBoost的成功在于其提升实现中的概念和算法改进，这些改进是专门针对基于树的学习的。在本节中，我们将重点关注XGBoost如何高效地提高基于树的集成模型的鲁棒性和泛化能力。
- en: Regularized loss functions for learning
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 用于学习的正则化损失函数
- en: 'In section 6.2.2, we saw several examples of L2-regularized loss functions
    of the following form:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在6.2.2节中，我们看到了几个以下形式的L2正则化损失函数的例子：
- en: '![CH06_F13_Kunapuli-eqs-52x](../Images/CH06_F13_Kunapuli-eqs-52x.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F13_Kunapuli-eqs-52x](../Images/CH06_F13_Kunapuli-eqs-52x.png)'
- en: 'If we only consider tree-based learners for weak models in our ensemble, there
    are other ways to *directly control* the complexity of the trees during learning.
    XGBoost does this by introducing another regularization term to limit the number
    of leaf nodes:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只考虑我们的集成中弱模型的基于树的学习者，还有其他方法可以直接在训练期间控制树的复杂性。XGBoost通过引入另一个正则化项来限制叶子节点的数量来实现这一点：
- en: '![CH06_F13_Kunapuli-eqs-53x](../Images/CH06_F13_Kunapuli-eqs-53x.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F13_Kunapuli-eqs-53x](../Images/CH06_F13_Kunapuli-eqs-53x.png)'
- en: How does this control the complexity of a tree? By limiting the number of leaf
    nodes, this additional term will force tree learning to train shallower trees,
    which in turn makes the trees weaker and less complex.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何控制树的复杂性的？通过限制叶子节点的数量，这个附加项将迫使树学习训练更浅的树，这反过来又使得树变得更弱和更简单。
- en: XGBoost uses this regularized objective function in many ways. For instance,
    during tree learning, instead of using a scoring function such as the Gini criterion
    or entropy for split finding, XGBoost uses the regularized learning objective
    described previously. Thus, this criterion is used to determine the *structure*
    of the individual trees, which are the weak learners in the ensemble.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost以多种方式使用这个正则化目标函数。例如，在树学习期间，XGBoost不是使用Gini标准或熵等评分函数来寻找分割，而是使用之前描述的正则化学习目标。因此，这个标准用于确定单个树的结构，这些树是集成中的弱学习器。
- en: XGBoost also uses this objective to compute the leaf values themselves, which
    are essentially the regression values that gradient boosting aggregates. Thus,
    this criterion is used to determine the *parameters* of the individual trees as
    well.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost也使用这个目标函数来计算叶子节点的值，这些值本质上就是梯度提升法聚合的回归值。因此，这个标准也被用来确定单个树的*参数*。
- en: 'An important caveat before we move on: the additional regularization term allows
    direct control over model complexity and downstream generalization. This comes
    at a price, however, in that we now have an extra parameter γ to worry about.
    Because γ is a user-defined parameter, we have to set this value, along with α
    and many others. These will often have to be selected by CV and can add to the
    overall model development time and effort.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前的一个重要注意事项：额外的正则化项允许直接控制模型复杂度和下游泛化。然而，这也有代价，我们现在有一个额外的参数 γ 需要关注。由于 γ 是用户定义的参数，我们必须设置此值，以及
    α 和许多其他参数。这些通常需要通过交叉验证来选择，可能会增加整体模型开发的时间和努力。
- en: Weighted quantile-based Newton boosting
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 基于加权分位数的牛顿提升
- en: Even with a regularized learning objective, the biggest computational bottleneck
    is in scaling learning to large data sets, specifically, in identifying optimal
    splits for use during learning of the regression tree base estimators.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有正则化的学习目标，最大的计算瓶颈在于将学习扩展到大型数据集，特别是识别用于回归树基估计器学习过程中的最佳分割。
- en: The standard approach to tree learning exhaustively enumerates all possible
    splits in the data. As we’ve seen in chapter 5, section 5.2.4, this isn’t a good
    idea for large data sets. Efficient modifications, such as histogram-based splitting,
    bin the data instead so that we evaluate far fewer splits.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 树学习的标准方法会穷举数据中所有可能的分割。正如我们在第 5 章第 5.2.4 节中看到的，对于大型数据集来说这不是一个好主意。有效的改进，如基于直方图的分割，将数据分箱，从而评估远少于分割的数量。
- en: Implementations such as LightGBM incorporate further improvements, such as sampling
    and feature bundling, to speed up tree learning. XGBoost also aims to bring these
    notions into its implementation. However, there is one key consideration unique
    to XGBoost. Packages such as LightGBM implement gradient boosting, while XGBoost
    implements Newton boosting. This means that XGBoost’s tree learning has to consider
    Hessian-weighted training examples, unlike LightGBM, where all the examples are
    weighted equally!
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如 LightGBM 这样的实现进一步集成了改进，例如采样和特征捆绑，以加快树学习。XGBoost 也旨在将其概念引入其实现中。然而，XGBoost 有一个独特的考虑因素。例如，LightGBM
    实现了梯度提升，而 XGBoost 实现了牛顿提升。这意味着 XGBoost 的树学习必须考虑 Hessian 权重的训练示例，而 LightGBM 中所有示例都是等权重的！
- en: XGBoost’s approximate split-finding algorithm, *weighted quantile sketch*, aims
    to find ideal split points using quantiles in the features. This is analogous
    to histogram-based splitting, which uses bins employed by gradient-boosting algorithms.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 的近似分割查找算法，*加权分位数草图*，旨在使用特征中的分位数来找到理想的分割点。这与基于直方图的分割类似，后者使用梯度提升算法使用的桶。
- en: 'The details of weighted quantile sketch and its implementation are considerable
    and can’t be covered here owing to limited space. However, here are our key takeaways:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 加权分位数草图及其实现的细节相当多，由于空间有限，这里无法全部涵盖。然而，以下是我们的一些关键要点：
- en: Conceptually, XGBoost also uses approximate split-finding algorithms; these
    algorithms consider additional information unique to Newton boosting (e.g., Hessian
    weights). Ultimately, they are similar to histogram-based algorithms and aim to
    bin the data. Unlike other histogram-based algorithms that bucket data into evenly
    sized bins, XGBoost bins data into feature-dependent buckets. At the end of the
    day, XGBoost trades off exactness for efficiency by adapting clever strategies
    for split finding.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从概念上讲，XGBoost 也使用近似分割查找算法；这些算法考虑了牛顿提升特有的额外信息（例如，Hessian 权重）。最终，它们与基于直方图的算法类似，旨在对数据进行分箱。与将数据分入均匀大小分箱的其他基于直方图的算法不同，XGBoost
    将数据分入特征相关的桶中。最终，XGBoost 通过采用巧妙的分割查找策略，以效率为代价来权衡精确度。
- en: From an implementation standpoint, XGBoost presorts and organizes data into
    blocks both in memory and on disk. Once this is done, XGBoost further exploits
    this organization by caching access patterns, using block compression, and chunking
    the data into easily accessible shards. These steps significantly improve the
    efficiency of Newton boosting, allowing it to scale to very large data sets.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从实现的角度来看，XGBoost 在内存和磁盘上对数据进行预排序和组织。一旦完成，XGBoost 通过缓存访问模式、使用块压缩和数据分块为易于访问的碎片来进一步利用这种组织。这些步骤显著提高了牛顿提升的效率，使其能够扩展到非常大的数据集。
- en: 6.3.2 Newton boosting with XGBoost
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 XGBoost 的牛顿提升
- en: 'We kick off our explorations of XGBoost with the breast cancer data set, which
    we’ve used several times in the past as a pedagogical data set:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以乳腺癌数据集开始对XGBoost的探索，这是我们过去多次用作教学数据集的数据集：
- en: '[PRE16]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: NOTE XGBoost is available for Python, R, and many platforms. See the XGBoost
    installation guide for detailed instructions on installation at [http://mng.bz/61eZ](http://mng.bz/61eZ).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：XGBoost适用于Python、R和许多平台。有关安装的详细说明，请参阅XGBoost安装指南，网址为[http://mng.bz/61eZ](http://mng.bz/61eZ)。
- en: 'For Python users, especially those who are familiar with scikit-learn, XGBoost
    provides a familiar interface that is designed to look and feel like scikit-learn.
    Using this interface, it’s very easy to set up and train an XGBoost model:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python用户，尤其是那些熟悉scikit-learn的用户，XGBoost提供了一个熟悉的接口，该接口设计得看起来和感觉就像scikit-learn。使用此接口，设置和训练XGBoost模型非常容易：
- en: '[PRE17]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We set the loss function to be the logistic loss, set the number of iterations
    (with 1 estimator trained per iteration) to 20, and set the maximum tree depth
    to 1. This results in an ensemble of 20 decision stumps (trees of depth 1).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将损失函数设置为逻辑损失，将迭代次数（每次迭代训练1个估计器）设置为20，并将最大树深度设置为1。这导致了一个由20个决策树（深度为1的树）组成的集成。
- en: 'It’s also similarly easy to predict labels on test data and evaluate model
    performance:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上预测标签和评估模型性能同样简单：
- en: '[PRE18]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Alternatively, we can use XGBoost’s native interface, which was originally designed
    to read data in the LIBSVM format, which is well-suited for storing sparse data
    with lots of zeros efficiently.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用XGBoost的本地接口，该接口最初是为了读取LIBSVM格式的数据而设计的，这种格式非常适合高效地存储包含大量零的稀疏数据。
- en: 'In the LIBSVM format (which was introduced in the case study in chapter 5,
    section 5.5.1), each line of the data file contains a single training example
    represented as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在LIBSVM格式（在第5章第5.5.1节的案例研究中介绍）中，数据文件的每一行都包含一个单独的训练示例，表示如下：
- en: '[PRE19]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'XGBoost uses a data object called DMatrix to group data and corresponding labels
    together. DMatrix objects can be created by reading data directly from files or
    from other array-like objects. Here, we create two DMatrix objects called trn
    and tst to represent the train and test data matrices:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost使用一个名为DMatrix的数据对象来将数据和相应的标签组合在一起。DMatrix对象可以通过直接从文件或其他类似数组的对象中读取数据来创建。在这里，我们创建了两个名为trn和tst的DMatrix对象来表示训练和测试数据矩阵：
- en: '[PRE20]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We also set up the training parameters using a dictionary and train an XGBoost
    model using trn and the parameters:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用字典设置训练参数，并使用trn和参数训练XGBoost模型：
- en: '[PRE21]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Care must be taken while using this model for prediction, however. Models trained
    with certain loss functions will return prediction probabilities rather than the
    predictions directly. The logistic loss function is one such case.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在使用此模型进行预测时必须小心。使用某些损失函数训练的模型将返回预测概率而不是直接预测。逻辑损失函数就是这样一种情况。
- en: 'These prediction probabilities can be converted to binary classification labels
    0/1 by thresholding at 0.5\. That is, all test examples with prediction probability
    ≥ 0.5 are classified into Class 1 and the rest into Class 0:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测概率可以通过在0.5处阈值化转换为二进制分类标签0/1。也就是说，所有预测概率≥0.5的测试示例都被分类为类别1，其余的都被分类为类别0：
- en: '[PRE22]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, XGBoost supports three different types of boosting approaches, which
    can be set through the booster parameter:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，XGBoost支持三种不同的提升方法，可以通过booster参数进行设置：
- en: booster='gbtree' is the default setting and implements Newton boosting using
    trees as weak learners trained using tree-based regression.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: booster='gbtree'是默认设置，它通过使用基于树的回归训练的树作为弱学习器实现牛顿提升。
- en: booster='gblinear' implements Newton boosting using linear functions as weak
    learners trained using linear regression.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: booster='gblinear'通过使用线性回归训练的线性函数作为弱学习器实现牛顿提升。
- en: booster='dart' trains an ensemble using Dropouts meet Multiple Additive Regression
    Trees (DART), as previously described in chapter 5, section 5.4.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: booster='dart'使用Dropouts meet Multiple Additive Regression Trees (DART)训练集成，如第5章第5.4节中所述。
- en: We can also train (parallel) random forest ensembles using XGBoost by carefully
    setting the training parameters to ensure training examples and feature subsampling.
    This is generally only useful when you want to use XGBoost’s parallel and distributed
    training architecture to explicitly train parallel ensembles.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过仔细设置训练参数来使用XGBoost训练（并行）随机森林集成，以确保训练示例和特征子采样。这通常只在你想使用XGBoost的并行和分布式训练架构来显式训练并行集成时才有用。
- en: 6.4 XGBoost in practice
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 XGBoost的实际应用
- en: 'In this section, we describe how to train models in practice using XGBoost.
    As with AdaBoost and gradient boosting, we look to set the learning rate (section
    6.4.1) or employ early stopping (section 6.4.2) as a means to control overfitting,
    as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了如何使用XGBoost在实际中训练模型。与AdaBoost和梯度提升一样，我们寻求设置学习率（第6.4.1节）或采用提前停止（第6.4.2节）作为控制过度拟合的手段，如下所示：
- en: By selecting an effective learning rate, we try to control the rate at which
    the model learns so that it doesn’t rapidly fit and then overfit the training
    data. We can think of this as a proactive modeling approach, where we try to identify
    a good training strategy so that it leads to a good model.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过选择一个有效的学习率，我们试图控制模型学习的速率，使其不会快速拟合并过度拟合训练数据。我们可以将其视为一种主动建模方法，其中我们试图确定一个好的训练策略，以便它能够导致一个好的模型。
- en: By enforcing early stopping, we try to stop training as soon as we observe that
    the model is starting to overfit. We can think of this as a reactive modeling
    approach, where we contemplate terminating training as soon as we think we have
    a good model.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实施提前停止，我们试图在观察到模型开始过度拟合时立即停止训练。我们可以将其视为一种反应式建模方法，其中我们考虑在认为我们有一个好模型时立即终止训练。
- en: 6.4.1 Learning rate
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 学习率
- en: Recall from section 6.1 that the step length is analogous to the learning rate
    and is a measure of each weak learner’s contribution to the entire ensemble. The
    learning rate allows greater control over how quickly the complexity of the ensemble
    grows. Therefore, it’s essential that we identify the best learning rate for our
    data set in practice so that we can avoid overfitting and generalize well after
    training.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第6.1节，步长与学习率类似，是每个弱学习器对整个集成贡献的度量。学习率允许我们更好地控制集成复杂性的增长速度。因此，在实践中确定我们数据集的最佳学习率至关重要，这样我们就可以避免过度拟合，并在训练后很好地泛化。
- en: Learning rate via cross validation
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通过交叉验证确定学习率
- en: As we’ve seen in the preceding section, XGBoost provides an interface that plays
    nicely with scikit-learn. This subsection shows how we can combine the functionalities
    of both packages to effectively perform parameter selection using CV. While we
    use CV to set the learning rate here, CV can be used to select other learning
    parameters such as maximum tree depth, number of leaf nodes, and even loss-function-specific
    parameters.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，XGBoost提供了一个与scikit-learn兼容的接口。本小节展示了我们如何结合这两个包的功能，有效地使用交叉验证进行参数选择。虽然我们在这里使用交叉验证来设置学习率，但交叉验证可以用于选择其他学习参数，如最大树深度、叶子节点数，甚至损失函数特定的参数。
- en: We combine scikit-learn’s StratifiedKFold class to split the training data into
    10 folds of training and validation sets. StratifiedKFold ensures that we preserve
    class distributions, that is, the fractions of different classes across the folds.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们结合scikit-learn的StratifiedKFold类将训练数据分成10个训练和验证数据集。StratifiedKFold确保我们保留类别分布，即不同类别在各个数据集中的比例。
- en: 'First, we initialize the learning rates we’re interested in exploring:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化我们感兴趣探索的学习率：
- en: '[PRE23]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we set up StratifiedKFold to split the training data into 10 folds:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置StratifiedKFold将训练数据分成10个数据集：
- en: '[PRE24]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the following listing, we perform CV by training and evaluating models on
    each of the 10 folds with XGBoost.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们通过在每个10个数据集中使用XGBoost训练和评估模型来进行交叉验证。
- en: Listing 6.5 Cross validation with XGBoost and scikit-learn
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 使用XGBoost和scikit-learn进行交叉验证
- en: '[PRE25]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Saves training and validation errors
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 保存训练和验证误差
- en: ❷ Trains an XGBoost classifier for each fold with different learning rates
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用不同的学习率在每个数据集上训练XGBoost分类器
- en: ❸ Saves training and validation errors
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 保存训练和验证误差
- en: ❹ Averages training and validation errors across folds
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在数据集的各个数据集间平均训练和验证误差
- en: When applied to the breast cancer data set (see section 6.3.2), we obtain the
    averaged training and validation errors for this data set. We visualize these
    errors for different learning rates in figure 6.14.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于乳腺癌数据集（见第6.3.2节），我们获得该数据集的平均训练和验证误差。我们在图6.14中可视化不同学习率下的这些误差。
- en: '![CH06_F14_Kunapuli](../Images/CH06_F14_Kunapuli.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F14_Kunapuli](../Images/CH06_F14_Kunapuli.png)'
- en: Figure 6.14 Averaged training and validation errors of XGBoost across 10 folds
    of the breast cancer data set
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 XGBoost在乳腺癌数据集的10个数据集上的平均训练和验证误差
- en: As the learning rate decreases, XGBoost’s performance degrades as the boosting
    process becomes increasingly more conservative and exhibits underfitting behavior.
    As the learning rate increases, XGBoost’s performance, once again, degrades as
    the boosting process becomes increasingly more aggressive and exhibits overfitting
    behavior. The best value among our parameter choices appears to be learning_rate=1.2,
    which is generally in the region between 1.0 and 1.5.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 随着学习率的降低，XGBoost的性能随着提升过程变得越来越保守并表现出欠拟合行为而下降。当学习率增加时，XGBoost的性能再次下降，因为提升过程变得越来越激进并表现出过拟合行为。在我们的参数选择中，学习率=1.2似乎是最好的值，这通常位于1.0和1.5之间。
- en: Cross validation with XGBoost
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost的交叉验证
- en: Beyond parameter selection, CV can also be useful to characterize model performance.
    In listing 6.6, we use XGBoost’s built-in CV functionality to characterize how
    XGBoost’s performance changes as we increase the number of estimators in the ensemble.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，交叉验证（CV）还可以用来描述模型性能。在列表6.6中，我们使用XGBoost内置的CV功能来描述随着我们在集成中增加估计器的数量，XGBoost的性能如何变化。
- en: We use the XGBoost.cv function to perform 10-fold CV, as shown in the following
    listing. Observe that xgb.cv is called in nearly the same way as xgb.fit from
    the previous section.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用XGBoost.cv函数执行10折交叉验证，如下所示。观察发现，xgb.cv的调用方式几乎与上一节中的xgb.fit相同。
- en: Listing 6.6 Cross validation with XGBoost
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 XGBoost的交叉验证
- en: '[PRE26]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this listing, the model performance is characterized by error, which is passed
    to XGBoost.cv using the argument metrics={'error'}. The training and test cross-validation
    errors are shown in figure 6.15.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表中，模型性能通过误差来描述，该误差通过metrics={'error'}参数传递给XGBoost.cv，如图6.15所示。
- en: '![CH06_F15_Kunapuli](../Images/CH06_F15_Kunapuli.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F15_Kunapuli](../Images/CH06_F15_Kunapuli.png)'
- en: Figure 6.15 The average error across the folds decreases with increasing iterations,
    as we add more and more base estimators into the ensemble.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 随着迭代次数的增加，折平均误差逐渐降低，因为我们不断地将更多的基估计器添加到集成中。
- en: Another interesting observation from figure 6.15 is that training and validation
    performance stop improving meaningfully at around 35 iterations. This suggests
    that there’s no significant performance improvement to be gained by prolonging
    training beyond this point. This brings us, rather neatly, to the notion of early
    stopping, which we’ve encountered before with both AdaBoost and gradient boosting.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 从图6.15中还可以观察到另一个有趣的观察结果：在约35次迭代时，训练和验证性能不再有显著提升。这表明，在此之后延长训练时间不会带来显著的性能提升。这很自然地引出了早期停止的概念，我们之前在AdaBoost和梯度提升中已经遇到过。
- en: 6.4.2 Early stopping
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 早期停止
- en: As the number of base estimators in the ensemble increases, the complexity of
    the ensemble also increases, which eventually leads to overfitting. To avoid this,
    instead of training the model, we can stop before we reach the limit of ensemble
    size.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 随着集成中基估计器数量的增加，集成的复杂性也增加，这最终会导致过拟合。为了避免这种情况，我们可以在达到集成大小极限之前停止训练模型。
- en: Early stopping with XGBoost works pretty similarly to LightGBM, where we specify
    a value for the parameter early_stopping_rounds. The performance of the ensemble
    is scored after each iteration on a validation set, which is split from the training
    set for the purpose of identifying a good early stopping point.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost的早期停止与LightGBM非常相似，其中我们为参数early_stopping_rounds指定一个值。在每次迭代后，集成性能在验证集上评分，该验证集是从训练集中分割出来的，目的是确定一个好的早期停止点。
- en: As long as the overall score (say, accuracy) improves over the last early_ stopping_rounds,
    XGBoost will continue to train. However, when the score doesn’t improve after
    early_stopping_rounds, XGBoost terminates training.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 只要整体分数（例如，准确率）在最后的早期停止轮次（early_ stopping_rounds）之后有所提高，XGBoost将继续训练。然而，当分数在早期停止轮次之后没有提高时，XGBoost将终止训练。
- en: The following listing illustrates early stopping using XGBoost. Note that train_test_split
    is used to create an independent validation set that is used by XGBoost to identify
    an early stopping point.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了使用XGBoost进行早期停止的示例。请注意，train_test_split用于创建一个独立的验证集，该验证集被XGBoost用来确定早期停止点。
- en: Listing 6.7 Early stopping with XGBoost
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 XGBoost的早期停止
- en: '[PRE27]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The three key parameters for early stopping in the preceding listing are the
    number of early stopping rounds and the evaluation set: early_ stopping_rounds=5
    and eval_set=[(Xval, yval)], and the evaluation metric eval_metric=''auc''. With
    these parameters, training terminates after 13 rounds even though n_estimators
    was initialized to 50 in XGBClassifier:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提前停止的三个关键参数是提前停止轮数和评估集：early_stopping_rounds=5和eval_set=[(Xval, yval)]，以及评估指标eval_metric='auc'。有了这些参数，训练在13轮后终止，尽管XGBClassifier中的n_estimators被初始化为50：
- en: '[PRE28]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Thus, early stopping can greatly improve training times, while ensuring that
    model performance doesn’t degrade excessively.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提前停止可以大大提高训练时间，同时确保模型性能不会过度下降。
- en: '6.5 Case study redux: Document retrieval'
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 案例研究重述：文档检索
- en: To conclude this chapter, we revisit the case study from chapter 5 that addressed
    the task of document retrieval, which identifies and retrieves documents from
    a database to match a user’s query. In chapter 5, we compared several gradient-boosting
    approaches available in LightGBM.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束本章，我们回顾了第5章中的案例研究，该研究解决了文档检索任务，即从数据库中识别和检索文档以匹配用户的查询。在第5章中，我们比较了LightGBM中可用的几个梯度提升方法。
- en: In this chapter, we’ll train Newton boosting models using XGBoost on the document
    retrieval task and compare the performance of XGBoost and LightGBM. In addition
    to this comparison, this case study also illustrates how to set up randomized
    CV for effective parameter selection in XGBoost over large data sets.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用XGBoost在文档检索任务上训练牛顿提升模型，并比较XGBoost和LightGBM的性能。除了这个比较之外，这个案例研究还说明了如何在XGBoost中设置随机交叉验证以在大数据集上进行有效的参数选择。
- en: 6.5.1 The LETOR data set
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 LETOR数据集
- en: 'We use the LEarning TO Rank (LETOR) v4.0 data set, which is made freely available
    by Microsoft Research. Each training example corresponds to a query-document pair,
    with features describing the query, the document, and the matches between them.
    Each training label is a relevance rank: least relevant, moderately relevant,
    or highly relevant.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用由微软研究院免费提供的LEarning TO Rank (LETOR) v4.0数据集。每个训练示例对应一个查询-文档对，其中包含描述查询、文档以及它们之间匹配的特征。每个训练标签是一个相关性排名：最少相关、适度相关或高度相关。
- en: 'This problem is set up as a three-class classification problem of identifying
    the relevance class (least, moderately, or highly relevant) given a training example:
    a query-document pair. For purposes of convenience and consistency, we’ll use
    the functionalities provided by XGBoost’s scikit-learn wrapper along with modules
    from scikit-learn itself. First, let’s load the LETOR data set:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被设定为一个三分类问题，即根据一个训练示例（查询-文档对）识别相关性类别（最少、适度或高度相关）。为了方便和一致性，我们将使用XGBoost的scikit-learn包装器以及scikit-learn本身的模块。首先，让我们加载LETOR数据集：
- en: '[PRE29]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, let’s split this into train and test sets:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将数据集分为训练集和测试集：
- en: '[PRE30]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 6.5.2 Document retrieval with XGBoost
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 使用XGBoost进行文档检索
- en: As we have a three-class (multiclass) classification problem, we train a tree-based
    XGBoost classifier using the softmax loss function. Softmax loss is a generalization
    of the logistic loss function to multiclass classification and is commonly used
    in many multiclass learning algorithms, including multinomial logistic regression
    and deep neural networks.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个三分类（多分类）问题，我们使用softmax损失函数训练了一个基于树的XGBoost分类器。Softmax损失函数是logistic损失函数在多分类中的推广，并且在许多多分类学习算法中常用，包括多项式逻辑回归和深度神经网络。
- en: 'We set the loss function for training with objective=''multi:softmax'', and
    the evaluation function for testing with eval_metric=''merror''. The evaluation
    function is a multiclass error, that is, a generalization of a 0-1 misclassification
    error from the binary to multiclass case. We don’t use merror as the training
    objective because it’s not differentiable and isn’t amenable to computing gradients
    and Hessians:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练的损失函数设置为objective='multi:softmax'，测试的评估函数设置为eval_metric='merror'。评估函数是一个多分类错误，即从二分类到多分类的0-1误分类错误的推广。我们不使用merror作为训练目标，因为它不可微，不便于计算梯度和Hessian：
- en: '[PRE31]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We also set n_jobs=-1 to enable XGBoost to use all available CPU cores to accelerate
    training with parallelization.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置n_jobs=-1以启用XGBoost使用所有可用的CPU核心，通过并行化加速训练。
- en: 'As with LightGBM, XGBoost also requires that we set several training hyperparameters,
    such as learning rate (to control the rate of learning) or the number of leaf
    nodes (to control the complexity of the base-estimator trees). These hyperparameters
    are selected using scikit-learn’s randomized CV module: RandomizedSearchCV. Specifically,
    we perform 5-fold CV over a grid of various parameter choices; however, instead
    of exhaustively evaluating all possible learning parameter combinations the way
    GridSearchCV does, RandomizedSearchCV samples a smaller number of model combinations
    for faster parameter selection:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LightGBM 一样，XGBoost 也需要我们设置几个训练超参数，例如学习率（用于控制学习速率）或叶节点数（用于控制基估计树复杂度）。这些超参数是通过
    scikit-learn 的随机 CV 模块 RandomizedSearchCV 来选择的：具体来说，我们在各种参数选择的网格上执行 5 折 CV；然而，与
    GridSearchCV 一样，RandomizedSearchCV 并不彻底评估所有可能的参数组合，而是随机采样更少的模型组合以加快参数选择：
- en: '[PRE32]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can explore several different values of some key parameters as described
    here:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以探索一些关键参数的不同值，如下所述：
- en: learning_rate—Controls overall contribution of each tree to the ensemble
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: learning_rate — 控制每个树对集成整体贡献的大小
- en: max_depth—Limits tree depth to accelerate training and decrease complexity
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: max_depth — 限制树深度以加速训练并降低复杂度
- en: min_child_weight—Limits each leaf node by the sum of the Hessian values to control
    overfitting
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: min_child_weight — 通过 Hessian 值之和限制每个叶节点，以控制过拟合
- en: colsample_bytree—Specifies the fraction of features to sample from the training
    data, respectively, to accelerate training (similar to feature subsampling performed
    by random forests or random subspaces)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: colsample_bytree — 指定从训练数据中采样的特征比例，分别用于加速训练（类似于随机森林或随机子空间中执行的特征子采样）
- en: reg_alpha and reg_lambda—Specifies the amount of regularization of the leaf
    node values to control overfitting as well
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: reg_alpha 和 reg_lambda — 指定叶节点值的正则化量，以控制过拟合
- en: 'The following code specifies the ranges of values for the parameters we’re
    interested in searching over to identify an effective training parameter combination:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码指定了我们感兴趣搜索的参数值的范围，以确定有效的训练参数组合：
- en: '[PRE33]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As mentioned earlier, the grid over these parameters produces too many combinations
    to evaluate efficiently. Thus, we adopt randomized search with CV and randomly
    sample a much smaller number of parameter combinations:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这些参数的网格产生了太多的组合，无法有效地评估。因此，我们采用带有 CV 的随机搜索，并随机采样大量更小的参数组合：
- en: '[PRE34]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Observe that we’ve set refit=True in RandomizedSearchCV, which enables the training
    of one final model using the optimal parameter combination identified by RandomizedSearchCV.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在 RandomizedSearchCV 中设置了 refit=True，这允许使用 RandomizedSearchCV 确定的最佳参数组合训练一个最终模型。
- en: 'After training, we compare the performance of XGBoost with four models trained
    by LightGBM in chapter 5, section 5.5:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们比较了 XGBoost 与第 5 章第 5.5 节中由 LightGBM 训练的四个模型的表现：
- en: '*Random forest*—Parallel homogeneous ensemble of randomized decision trees.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机森林* — 随机决策树的并行同质集成。'
- en: '*Gradient boosted decision trees* *(GBDT)*—This is the standard approach to
    gradient boosting that represents a balance between models with good generalization
    performance and training speed.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度提升决策树* (GBDT) — 这是一种标准的梯度提升方法，代表了具有良好泛化性能和训练速度的模型之间的平衡。'
- en: '*Gradient-based with One-Side Sampling (GOSS)*—This variant of gradient boosting
    downsamples the training data and is ideally suited for large data sets. Due to
    downsampling, it may lose out on generalization, but it’s typically very fast
    to train.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于梯度的单侧采样 (GOSS)* — 这种梯度提升的变体对训练数据进行下采样，非常适合大型数据集。由于下采样，它可能会在泛化方面有所损失，但通常训练速度非常快。'
- en: '*Dropouts meet Multiple Additive Regression Trees (DART)*—This variant incorporates
    the notion of dropout from deep learning, where neural units are randomly and
    temporarily dropped during backpropagation iterations to mitigate overfitting.
    DART is often the slowest of all the gradient-boosting options available in LightGBM.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropouts 与多个加性回归树 (DART)* — 这种变体结合了深度学习中的 dropout 概念，其中神经单元在反向传播迭代中随机且临时地被丢弃，以减轻过拟合。DART
    通常是在 LightGBM 中所有梯度提升选项中最慢的。'
- en: XGBoost uses regularized loss functions and Newton boosting. In contrast, the
    random forest ensemble doesn’t use any gradient information, while GBDT, GOSS,
    and DART use gradient boosting.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 使用正则化损失函数和牛顿提升。相比之下，随机森林集成不使用任何梯度信息，而 GBDT、GOSS 和 DART 使用梯度提升。
- en: As before, we compare the performance of all algorithms using test set accuracy
    (figure 6.16, left) and overall model development time (figure 6.16, right), which
    includes CV-based parameter selection as well as training time.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用测试集准确率（图6.16，左）和整体模型开发时间（图6.16，右）来比较所有算法的性能，这包括基于交叉验证的参数选择以及训练时间。
- en: '![CH06_F16_Kunapuli](../Images/CH06_F16_Kunapuli.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F16_Kunapuli](../Images/CH06_F16_Kunapuli.png)'
- en: 'Figure 6.16 Left: Comparing test set accuracy of random forest, GBDT, GOSS,
    and DART; Right: Comparing the overall training times of random forest, GBDT,
    GOSS, and DART (all trained using LightGBM)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 左：比较随机森林、GBDT、GOSS 和 DART 的测试集准确率；右：比较随机森林、GBDT、GOSS 和 DART 的整体训练时间（所有使用
    LightGBM 训练）
- en: 'Following are the key takeaways from this experiment (see figure 6.16):'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从这次实验中得出的关键结论（见图6.16）：
- en: On training performance, XGBoost performs comparably to DART, GOSS, and GBDT
    and outperforms random forest. On test set performance, XGBoost is second only
    to DART.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练性能上，XGBoost 的表现与 DART、GOSS 和 GBDT 相当，优于随机森林。在测试集性能上，XGBoost 仅次于 DART。
- en: On training time, the overall model development time of XGBoost is significantly
    shorter than DART. This suggests that there is an application-dependent tradeoff
    to be made here between the need for the additional performance improvement and
    the accompanying computational overhead incurred.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练时间上，XGBoost 的整体模型开发时间明显短于 DART。这表明，在这里需要在额外的性能提升需求和伴随的计算开销之间做出应用相关的权衡。
- en: Finally, these results are dependent on various choices made during modeling,
    such as learning parameter ranges and randomization. Further performance gains
    are possible with careful feature engineering, loss function selection, and using
    distributed processing for efficiency.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，这些结果取决于建模过程中做出的各种选择，例如学习参数范围和随机化。通过仔细的特征工程、损失函数选择和使用分布式处理以提高效率，可以获得进一步的性能提升。
- en: Summary
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Newton’s descent is another optimization algorithm, similar to gradient descent.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牛顿下降法是另一种优化算法，类似于梯度下降法。
- en: Newton’s descent uses second-order (Hessian) information to accelerate optimization
    as compared to gradient descent, which only uses first-order (gradient) information.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牛顿下降法使用二阶（海森）信息来加速优化，与仅使用一阶（梯度）信息的梯度下降法相比。
- en: Newton boosting combines Newton’s descent and boosting to train a sequential
    ensemble of weak learners.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牛顿提升法结合了牛顿下降法和提升法来训练一个弱学习者的序列集成。
- en: Newton boosting uses weighted residuals to characterize correctly classified
    and poorly classified training examples. This is analogous to both how AdaBoost
    uses weights and how gradient boosting uses residuals.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牛顿提升法使用加权残差来表征正确分类和错误分类的训练样本。这与 AdaBoost 使用权重和梯度提升法使用残差的方式类似。
- en: Weak learners in Newton boosting are regression trees that are trained over
    the weighted residuals of the training examples and approximate the Newton step.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 牛顿提升法中的弱学习者是回归树，它们在训练样本的加权残差上训练，并近似牛顿步。
- en: Like gradient boosting, Newton boosting can be applied to a wide variety of
    loss functions arising from classification, regression, or ranking tasks.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与梯度提升法类似，牛顿提升法可以应用于来自分类、回归或排序任务的广泛损失函数。
- en: Optimizing a regularized loss function helps control the complexity of the weak
    learners in the learned ensemble, prevent overfitting, and improve generalization.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化正则化损失函数有助于控制学习集中弱学习者的复杂性，防止过拟合，并提高泛化能力。
- en: XGBoost is a powerful, publicly available framework for tree-based Newton boosting
    that incorporates Newton boosting, efficient split finding, and distributed learning.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 是一个强大的、公开可用的基于树的牛顿提升框架，它集成了牛顿提升、高效的分割查找和分布式学习。
- en: 'XGBoost optimizes a regularized learning objective consisting of the loss function
    (to fit the data) and two regularization functions: L2 regularization and number
    of leaf nodes.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 优化了一个正则化学习目标，该目标由损失函数（用于拟合数据）和两个正则化函数组成：L2 正则化和叶子节点数量。
- en: As with AdaBoost and gradient boosting, we can avoid overfitting in Newton boosting
    by choosing an effective learning rate or via early stopping. XGBoost supports
    both.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 AdaBoost 和梯度提升法一样，我们可以在牛顿提升法中通过选择有效的学习率或通过提前停止来避免过拟合。XGBoost 支持这两种方法。
- en: XGBoost implements an approximate split-finding algorithm called weighted quantile
    sketch, which is similar to histogram-based split finding but adapted and optimized
    for efficient Newton boosting.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 实现了一个称为加权分位数草图的大致分割查找算法，该算法类似于基于直方图的分割查找，但经过调整和优化，以适应高效的牛顿提升。
- en: In addition to a wide variety of loss functions for classification, regression,
    and ranking, XGBoost also provides support for incorporation of our own custom,
    problem-specific loss functions for training.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了提供用于分类、回归和排序的广泛损失函数外，XGBoost 还支持在训练过程中集成我们自己的定制、特定问题的损失函数。
