- en: Chapter 4\. Making Predictions with Decision Trees and Decision Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。使用决策树和决策森林进行预测
- en: Classification and regression are the oldest and most well-studied types of
    predictive analytics. Most algorithms you will likely encounter in analytics packages
    and libraries are classification or regression techniques, like support vector
    machines, logistic regression, neural networks, and deep learning. The common
    thread linking regression and classification is that both involve predicting one
    (or more) values given one (or more) other values. To do so, both require a body
    of inputs and outputs to learn from. They need to be fed both questions and known
    answers. For this reason, they are known as types of supervised learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和回归是最古老和最深入研究的预测分析类型。您在分析软件包和库中可能遇到的大多数算法都是分类或回归技术，比如支持向量机、逻辑回归、神经网络和深度学习。将回归和分类联系在一起的共同线索是，它们都涉及根据一个或多个其他值来预测一个（或多个）值。为了做到这一点，它们需要一组输入和输出来学习。它们需要同时提供问题和已知答案。因此，它们被称为监督学习的类型。
- en: PySpark MLlib offers implementations of a number of classification and regression
    algorithms. These include decision trees, naïve Bayes, logistic regression, and
    linear regression. The exciting thing about these algorithms is that they can
    help predict the future—or at least, predict the things we don’t yet know for
    sure, like the likelihood you will buy a car based on your online behavior, whether
    an email is spam given the words it contains, or which acres of land are likely
    to grow the most crops given their location and soil chemistry.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark MLlib提供了多种分类和回归算法的实现。其中包括决策树、朴素贝叶斯、逻辑回归和线性回归。这些算法的令人兴奋之处在于，它们可以帮助预测未来——或者至少预测我们尚不确定的事物，比如基于你的在线行为预测你购买汽车的可能性，根据电子邮件中的词汇预测邮件是否为垃圾，或者哪些土地可能根据其位置和土壤化学成分种植出最多的庄稼。
- en: In this chapter, we will focus on a popular and flexible type of algorithm for
    both classification and regression (decision trees) and the algorithm’s extension
    (random decision forests). First, we will understand the basics of decision trees
    and forests and introduce the former’s PySpark implementation. The PySpark implementation
    of decision trees supports binary and multiclass classification, and regression.
    The implementation partitions data by rows, allowing distributed training with
    millions or even billions of instances. This will be followed by preparation of
    our dataset and creating our first decision tree. Then we’ll tune our decision
    tree model. We’ll finish up by training a random forest model on our processed
    dataset and making predictions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于一种流行且灵活的分类和回归算法（决策树），以及该算法的扩展（随机决策森林）。首先，我们将理解决策树和森林的基础知识，并介绍前者的PySpark实现。决策树的PySpark实现支持二元和多类分类，以及回归。该实现通过行进行数据分区，允许使用数百万甚至数十亿个实例进行分布式训练。接下来我们将准备我们的数据集并创建我们的第一棵决策树。然后我们将调整我们的决策树模型。最后，我们将在处理过的数据集上训练一个随机森林模型并进行预测。
- en: Although PySpark’s decision tree implementation is easy to get started with,
    it is helpful to understand the fundamentals of decision tree and random forest
    algorithms. That is what we’ll cover in the next section.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PySpark的决策树实现易于入门，理解决策树和随机森林算法的基础知识是有帮助的。这是我们将在下一节中讨论的内容。
- en: Decision Trees and Forests
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和森林
- en: '*Decision trees* are a family of algorithms that can naturally handle both
    categorical and numeric features. Building a single tree can be done using parallel
    computing, and many trees can be built in parallel at once. They are robust to
    outliers in the data, meaning that a few extreme and possibly erroneous data points
    might not affect predictions at all. They can consume data of different types
    and on different scales without the need for preprocessing or normalization.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树（*Decision trees*）是一类算法家族，可以自然处理类别和数值特征。使用并行计算可以构建单棵树，并且可以同时并行构建多棵树。它们对数据中的异常值具有鲁棒性，这意味着少数极端甚至可能错误的数据点可能完全不会影响预测结果。它们可以处理不同类型和不同规模的数据，无需预处理或标准化。
- en: 'Decision tree–based algorithms have the advantage of being comparatively intuitive
    to understand and reason about. In fact, we all probably use the same reasoning
    embodied in decision trees, implicitly, in everyday life. For example, I sit down
    to have morning coffee with milk. Before I commit to that milk and add it to my
    brew, I want to predict: is the milk spoiled? I don’t know for sure. I might check
    if the use-by date has passed. If not, I predict no, it’s not spoiled. If the
    date has passed, but it was three or fewer days ago, I take my chances and predict
    no, it’s not spoiled. Otherwise, I sniff the milk. If it smells funny, I predict
    yes, and otherwise no.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于决策树的算法具有相对直观和理解的优势。事实上，我们在日常生活中可能都在隐含地使用决策树体现的同样推理方式。例如，我早晨坐下来喝带牛奶的咖啡。在我决定使用牛奶加入我的咖啡之前，我想预测一下：牛奶是不是变质了？我不确定。我可能会检查一下使用日期是否已过期。如果没有，我预测它没变质。如果日期已经过期，但是距离过期日不到三天，我会冒险预测它没变质。否则，我会闻一下牛奶。如果闻起来有点怪，我预测是变质的；否则，预测不是。
- en: This series of yes/no decisions that leads to a prediction are what decision
    trees embody. Each decision leads to one of two results, which is either a prediction
    or another decision, as shown in [Figure 4-1](#MilkDecisionTree). In this sense,
    it is natural to think of the process as a tree of decisions, where each internal
    node in the tree is a decision, and each leaf node is a final answer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列的是/否决策导致了一个预测结果，这就是决策树所体现的。每个决策都导致两种结果中的一种，即预测结果或另一个决策，如[图4-1](#MilkDecisionTree)所示。从这个意义上说，把这个过程想象成一个决策树是很自然的，其中树的每个内部节点都是一个决策，每个叶节点都是最终答案。
- en: That is a simplistic decision tree and was not built with any rigor. To elaborate,
    consider another example. A robot has taken a job in an exotic pet store. It wants
    to learn, before the shop opens, which animals in the shop would make a good pet
    for a child. The owner lists nine pets that would and wouldn’t be suitable before
    hurrying off. The robot compiles the information found in [Table 4-1](#Pet_Stats)
    from examining the animals.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个简单的决策树，没有经过严谨的构建。为了阐述，再考虑另一个例子。一个机器人在一家异国情调的宠物商店找了一份工作。它想在商店开门前学习，哪些动物适合孩子作为宠物。店主匆匆忙忙列出了九种适合和不适合的宠物。机器人从检查动物中收集到的信息编制了[表4-1](#Pet_Stats)。
- en: '![aaps 0401](assets/aaps_0401.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0401](assets/aaps_0401.png)'
- en: 'Figure 4-1\. Decision tree: is milk spoiled?'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1\. 决策树：牛奶是否变质？
- en: Table 4-1\. Exotic pet store “feature vectors”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. 异国情调宠物商店“特征向量”
- en: '| Name | Weight (kg) | # Legs | Color | Good pet? |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| Name | Weight (kg) | # Legs | Color | Good pet? |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Fido | 20.5 | 4 | Brown | Yes |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| Fido | 20.5 | 4 | Brown | Yes |'
- en: '| Mr. Slither | 3.1 | 0 | Green | No |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| Mr. Slither | 3.1 | 0 | Green | No |'
- en: '| Nemo | 0.2 | 0 | Tan | Yes |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| Nemo | 0.2 | 0 | Tan | Yes |'
- en: '| Dumbo | 1390.8 | 4 | Gray | No |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Dumbo | 1390.8 | 4 | Gray | No |'
- en: '| Kitty | 12.1 | 4 | Gray | Yes |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Kitty | 12.1 | 4 | Gray | Yes |'
- en: '| Jim | 150.9 | 2 | Tan | No |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Jim | 150.9 | 2 | Tan | No |'
- en: '| Millie | 0.1 | 100 | Brown | No |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Millie | 0.1 | 100 | Brown | No |'
- en: '| McPigeon | 1.0 | 2 | Gray | No |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| McPigeon | 1.0 | 2 | Gray | No |'
- en: '| Spot | 10.0 | 4 | Brown | Yes |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Spot | 10.0 | 4 | Brown | Yes |'
- en: The robot can make a decision for the nine listed pets. There are many more
    pets available in the store. It still needs a methodology for deciding which animals
    among the rest will be suitable as pets for kids. We can assume that the characteristics
    of all animals are available. Using the decision data provided by the store owner
    and a decision tree, we can help the robot learn what a good pet for a kid looks
    like.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人可以为九种列出的宠物做出决策。商店里还有更多的宠物可供选择。它仍需要一种方法来决定其余动物中哪些适合孩子作为宠物。我们可以假设所有动物的特征都是已知的。利用商店主提供的决策数据和决策树，我们可以帮助机器人学习什么样的动物是适合孩子作为宠物的。
- en: Although a name is given, it will not be included as a feature in our decision
    tree model. There is little reason to believe the name alone is predictive; “Felix”
    could name a cat or a poisonous tarantula, for all the robot knows. So, there
    are two numeric features (weight, number of legs) and one categorical feature
    (color) predicting a categorical target (is/is not a good pet for a child.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有名字，但名字不会包含在我们的决策树模型的特征中。没有理由相信仅仅是名字就具有预测性；对于机器人来说，“费利克斯”可能是猫，也可能是有毒的大蜘蛛。因此，我们有两个数值特征（体重、腿的数量）和一个分类特征（颜色），预测一个分类目标（是否是孩子的好宠物）。
- en: The way a decision tree works is by making one or more decisions in sequence
    based on provided features. To start off, the robot might try to fit a simple
    decision tree to this training data, consisting of a single decision based on
    weight, as shown in [Figure 4-2](#PetStoreDecisionTree1).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的工作原理是基于提供的特征进行一个或多个顺序决策。起初，机器人可能会尝试将一个简单的决策树适配到这些训练数据上，该决策树仅基于体重做出一个决策，如[图 4-2](#PetStoreDecisionTree1)所示。
- en: '![aaps 0402](assets/aaps_0402.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0402](assets/aaps_0402.png)'
- en: Figure 4-2\. Robot’s first decision tree
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 机器人的第一棵决策树
- en: 'The logic of the decision tree is easy to read and make sense of: 500kg animals
    certainly sound unsuitable as pets. This rule predicts the correct value in five
    of nine cases. A quick glance suggests that we could improve the rule by lowering
    the weight threshold to 100kg. This gets six of nine examples correct. The heavy
    animals are now predicted correctly; the lighter animals are only partly correct.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的逻辑易于阅读和理解：500公斤的动物显然不适合作为宠物。这条规则在九个案例中预测了五个正确的值。快速浏览表明，我们可以通过将体重阈值降低到100公斤来改进规则。这样做可以在九个例子中正确预测六个。重的动物现在被正确预测了；轻的动物只部分正确。
- en: So, a second decision can be constructed to further refine the prediction for
    examples with weights less than 100kg. It would be good to pick a feature that
    changes some of the incorrect Yes predictions to No. For example, there is one
    small green animal, sounding suspiciously like a snake, that will be classified
    by our current model as a suitable pet candidate. The robot could predict correctly
    by adding a decision based on color, as shown in [Figure 4-3](#PetStoreDecisionTree2).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以构建第二个决策来进一步调整对于体重小于100公斤的例子的预测。最好选择一个能够将某些不正确的“是”预测变成“否”的特征。例如，有一种听起来像蛇的小型绿色动物，我们当前的模型将其分类为合适的宠物候选者。通过基于颜色的决策，如[图 4-3](#PetStoreDecisionTree2)所示，机器人可以进行正确预测。
- en: '![aaps 0403](assets/aaps_0403.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0403](assets/aaps_0403.png)'
- en: Figure 4-3\. Robot’s next decision tree
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 机器人的下一棵决策树
- en: 'Now, seven of nine examples are correct. Of course, decision rules could be
    added until all nine were correctly predicted. The logic embodied in the resulting
    decision tree would probably sound implausible when translated into common speech:
    “If the animal’s weight is less than 100kg, its color is brown instead of green,
    and it has fewer than 10 legs, then yes, it is a suitable pet.” While perfectly
    fitting the given examples, a decision tree like this would fail to predict that
    a small, brown, four-legged wolverine is not a suitable pet. Some balance is needed
    to avoid this phenomenon, known as *overfitting*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，九个例子中有七个是正确的。当然，可以添加决策规则，直到所有九个都正确预测。但是，当翻译成通俗语言时，这样的决策树所包含的逻辑可能会显得不太可信：“如果动物的体重小于100公斤，它的颜色是棕色而不是绿色，并且它的腿少于10条，那么是，它适合作为宠物。”虽然完全符合给定的例子，但这样的决策树无法预测到小型、棕色、四条腿的狼獾不适合作为宠物。需要一些平衡来避免这种现象，称为*过拟合*。
- en: Decision trees generalize into a more powerful algorithm, called *random forests*.
    Random forests combine many decision trees to reduce the risk of overfitting and
    train the decision trees separately. The algorithm injects randomness into the
    training process so that each decision tree is a bit different. Combining the
    predictions reduces the variance of the predictions, makes the resulting model
    more generalizable, and improves performance on test data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树推广为一种更强大的算法，称为*随机森林*。随机森林结合了许多决策树，以减少过拟合的风险，并单独训练决策树。该算法在训练过程中注入随机性，使得每棵决策树都有所不同。组合预测减少了预测的方差，使得生成的模型更具普适性，并提高了在测试数据上的性能。
- en: This is enough of an introduction to decision trees and random forests for us
    to begin using them with PySpark. In the next section, we will introduce the dataset
    that we’ll work with and prepare it for use in PySpark.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就足够我们开始在PySpark中使用决策树和随机森林了。在接下来的部分，我们将介绍我们将使用的数据集，并为在PySpark中使用做好准备。
- en: Preparing the Data
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: The dataset used in this chapter is the well-known Covtype dataset, available
    [online](https://oreil.ly/spUWl) as a compressed CSV-format data file, *covtype.data.gz*,
    and accompanying info file, *covtype.info*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集是著名的Covtype数据集，可以在[线上](https://oreil.ly/spUWl)获取，格式为压缩的CSV文件*covtype.data.gz*及其附带的信息文件*covtype.info*。
- en: The dataset records the types of forest-covered parcels of land in Colorado,
    USA. It’s only a coincidence that the dataset concerns real-world forests! Each
    data record contains several features describing each parcel of land—like its
    elevation, slope, distance to water, shade, and soil type—along with the known
    forest type covering the land. The forest cover type is to be predicted from the
    rest of the features, of which there are 54 in total.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集记录了美国科罗拉多州森林覆盖地块的类型。这个数据集和现实世界的森林有些巧合！每条数据记录包含描述每块地的多个特征，如海拔、坡度、距离水源的距离、阴影和土壤类型，以及覆盖该地块的已知森林类型。要从其他特征中预测森林覆盖类型，总共有
    54 个特征。
- en: This dataset has been used in research and even a [Kaggle competition](https://oreil.ly/LpjgW).
    It is an interesting dataset to explore in this chapter because it contains both
    categorical and numeric features. There are 581,012 examples in the dataset, which
    does not exactly qualify as big data but is large enough to be manageable as an
    example and still highlight some issues of scale.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集已经在研究中使用过，甚至在[Kaggle 比赛](https://oreil.ly/LpjgW)中也有应用。这是本章中探索的一个有趣的数据集，因为它既包含分类特征又包含数值特征。数据集中有
    581,012 个示例，虽然不完全符合大数据的定义，但足够作为一个示例来管理，并且仍然突出了一些规模问题。
- en: Thankfully, the data is already in a simple CSV format and does not require
    much cleansing or other preparation to be used with PySpark MLlib. The *covtype.data*
    file should be extracted and copied into your local or cloud storage (such as
    AWS S3).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这些数据已经是简单的 CSV 格式，不需要太多的清洗或其他准备工作即可用于 PySpark MLlib。*covtype.data* 文件应该被提取并复制到您的本地或云存储（如
    AWS S3）中。
- en: Start `pyspark-shell`. You may find it helpful to give the shell a healthy amount
    of memory to work with, as building decision forests can be resource intensive.
    If you have the memory, specify `--driver-memory 8g` or similar.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 开始`pyspark-shell`。如果要构建决策森林，给 shell 分配足够的内存是很有帮助的。如果有足够的内存，可以指定`--driver-memory
    8g`或类似的参数。
- en: CSV files contain fundamentally tabular data, organized into rows of columns.
    Sometimes these columns are given names in a header line, although that’s not
    the case here. The column names are given in the companion file, *covtype.info*.
    Conceptually, each column of a CSV file has a type as well—a number, a string—but
    a CSV file doesn’t specify this.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件基本上包含表格数据，组织成行和列。有时这些列在标题行中有命名，虽然这里并非如此。列名在伴随文件*covtype.info*中给出。从概念上讲，CSV
    文件的每列也有一个类型，可以是数字或字符串，但 CSV 文件本身并未指定这一点。
- en: 'It’s natural to parse this data as a dataframe because this is PySpark’s abstraction
    for tabular data, with a defined column schema, including column names and types.
    PySpark has built-in support for reading CSV data. Let’s read our dataset as a
    DataFrame using the built-in CSV reader:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然而然的解析数据的方法，因为这是 PySpark 处理表格数据的抽象，具有定义的列模式，包括列名和类型。PySpark 内置支持读取 CSV 数据。让我们使用内置的
    CSV 读取器将数据集读取为 DataFrame：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code reads the input as CSV and does not attempt to parse the first line
    as a header of column names. It also requests that the type of each column be
    inferred by examining the data. It correctly infers that all of the columns are
    numbers, and, more specifically, integers. Unfortunately, it can name the columns
    only `_c0` and so on.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将输入作为 CSV 文件读取，并且不尝试解析第一行作为列名的标题。它还请求通过检查数据来推断每列的类型。它正确推断出所有列都是数字，具体来说是整数。不幸的是，它只能将列命名为
    `_c0` 等。
- en: We can look at the *covtype.info* file for the column names.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看*covtype.info* 文件获取列名。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looking at the column information, it’s clear that some features are indeed
    numeric. `Elevation` is an elevation in meters; `Slope` is measured in degrees.
    However, `Wilderness_Area` is something different, because it is said to span
    four columns, each of which is a 0 or 1\. In reality, `Wilderness_Area` is a categorical
    value, not a numeric one.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 查看列信息后，显然一些特征确实是数值型的。`Elevation` 是以米为单位的海拔；`Slope` 以度数表示。然而，`Wilderness_Area`
    是不同的，因为据说它跨越四列，每列都是 0 或 1。事实上，`Wilderness_Area` 是一个分类值，而不是数值型的。
- en: These four columns are actually a one-hot or 1-of-N encoding. When this form
    of encoding is performed on a categorical feature, one categorical feature that
    takes on *N* distinct values becomes *N* numeric features, each taking on the
    value 0 or 1\. Exactly one of the *N* values has value 1, and the others are 0\.
    For example, a categorical feature for weather that can be `cloudy`, `rainy`,
    or `clear` would become three numeric features, where `cloudy` is represented
    by `1,0,0`; `rainy` by `0,1,0`; and so on. These three numeric features might
    be thought of as `is_cloudy`, `is_rainy`, and `is_clear` features. Likewise, 40
    of the columns are really one `Soil_Type` categorical feature.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这四列实际上是一种独热编码或者1-of-N编码。当对分类特征执行这种编码时，一个具有*N*个不同值的分类特征变成了*N*个数值特征，每个特征取值为0或1。精确地说，*N*个值中有一个值为1，其余值为0。例如，一个关于天气的分类特征可以是`cloudy`、`rainy`或者`clear`，将变成三个数值特征，其中`cloudy`表示为`1,0,0`，`rainy`表示为`0,1,0`，等等。这三个数值特征可以被看作是`is_cloudy`、`is_rainy`和`is_clear`特征。同样地，有40列其实只是一个`Soil_Type`分类特征。
- en: This isn’t the only possible way to encode a categorical feature as a number.
    Another possible encoding simply assigns a distinct numeric value to each possible
    value of the categorical feature. For example, `cloudy` may become 1.0, `rainy`
    2.0, and so on. The target itself, `Cover_Type`, is a categorical value encoded
    as a value 1 to 7.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是将分类特征编码为数字的唯一可能方法。另一种可能的编码方法是简单地为分类特征的每个可能值分配一个唯一的数值。例如，`cloudy`可能变成1.0，`rainy`变成2.0，依此类推。目标本身`Cover_Type`是一个编码为1到7的分类值。
- en: Be careful when encoding a categorical feature as a single numeric feature.
    The original categorical values have no ordering, but when encoded as a number,
    they appear to. Treating the encoded feature as numeric leads to meaningless results
    because the algorithm is effectively pretending that `rainy` is somehow greater
    than, and two times larger than, `cloudy`. It’s OK as long as the encoding’s numeric
    value is not used as a number.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当将分类特征编码为单个数值特征时要小心。原始的分类值没有排序，但编码为数字后似乎有了排序。将编码特征视为数值会导致无意义的结果，因为算法实际上是假装`rainy`比`cloudy`更大，且大两倍。只要不将编码的数值作为数字使用，这种做法是可以接受的。
- en: We have seen both types of encodings of categorical features. It would have,
    perhaps, been simpler and more straightforward to not encode such features (and
    in two ways, no less) and to instead simply include their values directly, like
    “Rawah Wilderness Area.” This may be an artifact of history; the dataset was released
    in 1998\. For performance reasons or to match the format expected by libraries
    of the day, which were built more for regression problems, datasets often contain
    data encoded in these ways.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了分类特征的两种编码方式。也许，将这些特征直接包含其值，如“Rawah Wilderness Area”，而不编码（并且以两种方式编码）可能更简单和直接。这可能是历史的产物；数据集发布于1998年。出于性能原因或者为了匹配当时更多用于回归问题的库的格式，数据集通常以这些方式编码数据。
- en: 'In any event, before proceeding, it is useful to add column names to this DataFrame
    to make it easier to work with:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，在继续之前，给这个DataFrame添加列名是非常有用的，以便更轻松地处理：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1)'
- en: + concatenates collections.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: + 连接集合。
- en: The wilderness- and soil-related columns are named `Wilderness_Area_0`, `Soil_Type_0`,
    etc., and a bit of Python can generate these 44 names without having to type them
    all out. Finally, the target `Cover_Type` column is cast to a `double` value up
    front, because it will actually be necessary to consume it as a `double` rather
    than `int` in all PySpark MLlib APIs. This will become apparent later.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与荒野和土壤相关的列名为`Wilderness_Area_0`、`Soil_Type_0`等，一小段Python代码即可生成这44个名称，无需逐个输入。最后，目标`Cover_Type`列被最初转换为`double`值，因为在所有PySpark
    MLlib API中，需要将其作为`double`而不是`int`使用。这一点稍后会变得明显。
- en: You can call `data.show` to see some rows of the dataset, but the display is
    so wide that it will be difficult to read it all. `data.head` displays it as a
    raw `Row` object, which will be more readable in this case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以调用`data.show`来查看数据集的部分行，但显示的宽度太大，很难一眼看完。`data.head`以原始`Row`对象显示，这种情况下更易读。
- en: Now that we’re familiar with our dataset and have processed it, we can train
    a decision tree model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了数据集并对其进行了处理，可以训练一个决策树模型。
- en: Our First Decision Tree
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个决策树
- en: 'In [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    we built a recommender model right away on all of the available data. This created
    a recommender that could be sense-checked by anyone with some knowledge of music:
    looking at a user’s listening habits and recommendations, we got some sense that
    it was producing good results. Here, that is not possible. We would have no idea
    how to make up a 54-feature description of a new parcel of land in Colorado, or
    what kind of forest cover to expect from such a parcel.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 3 章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中，我们立即在所有可用数据上建立了一个推荐模型。这创造了一个可以被具有一些音乐知识的任何人检查的推荐系统：观察用户的听音乐习惯和推荐，我们有些感觉它产生了良好的结果。在这里，这是不可能的。我们不知道如何对科罗拉多州的一个新地块进行
    54 特征描述，或者可以从这样一个地块期望什么样的森林覆盖。
- en: 'Instead, we must jump straight to holding out some data for purposes of evaluating
    the resulting model. Before, the AUC metric was used to assess the agreement between
    held-out listening data and predictions from recommendations. AUC may be viewed
    as the probability that a randomly chosen good recommendation ranks above a randomly
    chosen bad recommendation. The principle is the same here, although the evaluation
    metric will be different: *accuracy*. The majority—90%—of the data will again
    be used for training, and, later, we’ll see that a subset of this training set
    will be held out for cross-validation (the CV set). The other 10% held out here
    is actually a third subset, a proper test set.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们必须直接开始保留一些数据以评估生成的模型。以前，AUC 指标用于评估保留的听数据和推荐预测之间的一致性。AUC 可视为随机选择一个好的推荐高于随机选择一个坏的推荐的概率。这里的原则是相同的，尽管评估指标将会不同：*准确性*。大多数——90%——的数据将再次用于训练，稍后，我们会看到这个训练集的一个子集将被保留用于交叉验证（CV
    集）。这里保留的另外 10% 实际上是第三个子集，一个真正的测试集。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The data needs a little more preparation to be used with a classifier in MLlib.
    The input DataFrame contains many columns, each holding one feature that could
    be used to predict the target column. MLlib requires all of the inputs to be collected
    into *one* column, whose value is a vector. PySpark’s `VectorAssembler` class
    is an abstraction for vectors in the linear algebra sense and contains only numbers.
    For most intents and purposes, they work like a simple array of `double` values
    (floating-point numbers). Of course, some of the input features are conceptually
    categorical, even if they’re all represented with numbers in the input.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要做更多的准备工作，以便在 MLlib 中与分类器一起使用。输入的 DataFrame 包含许多列，每列都持有一个可以用来预测目标列的特征。MLlib
    要求所有的输入被收集到 *一个* 列中，其值是一个向量。PySpark 的 `VectorAssembler` 类是线性代数意义上向量的一个抽象，只包含数字。在大多数意图和目的上，它们就像一个简单的
    `double` 值数组（浮点数）。当然，一些输入特征在概念上是分类的，即使它们在输入中都用数字表示。
- en: 'Fortunately, the `VectorAssembler` class can do this work:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`VectorAssembler` 类可以完成这项工作：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1)'
- en: Excludes the label, Cover_Type
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 排除标签，Cover_Type
- en: The key parameters of `VectorAssembler` are the columns to combine into the
    feature vector, and the name of the new column containing the feature vector.
    Here, all columns—*except* the target, of course—are included as input features.
    The resulting DataFrame has a new `featureVector` column, as shown.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorAssembler` 的关键参数是要合并成特征向量的列，以及包含特征向量的新列的名称。在这里，所有列——当然除了目标列——都作为输入特征包含在内。生成的
    DataFrame 有一个新的 `featureVector` 列，如图所示。'
- en: The output doesn’t look exactly like a sequence of numbers, but that’s because
    this shows a raw representation of the vector, represented as a `SparseVector`
    instance to save storage. Because most of the 54 values are 0, it stores only
    nonzero values and their indices. This detail won’t matter in classification.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来并不像一系列数字，但这是因为它显示的是一个原始的向量表示，表示为一个 `SparseVector` 实例以节省存储空间。因为 54 个值中大多数是
    0，它仅存储非零值及其索引。这个细节在分类中并不重要。
- en: '`VectorAssembler` is an example of `Transformer` within the current MLlib Pipelines
    API. It transforms the input DataFrame into another DataFrame based on some logic,
    and is composable with other transformations into a pipeline. Later in this chapter,
    these transformations will be connected into an actual `Pipeline`. Here, the transformation
    is just invoked directly, which is sufficient to build a first decision tree classifier
    model:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorAssembler`是当前MLlib Pipelines API中的`Transformer`示例。它根据一些逻辑将输入的DataFrame转换为另一个DataFrame，并且可以与其他转换组合成管道。在本章的后面，这些转换将连接成一个实际的`Pipeline`。在这里，转换只是直接调用，已足以构建第一个决策树分类器模型：'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, the essential configuration for the classifier consists of column names:
    the column containing the input feature vectors and the column containing the
    target value to predict. Because the model will later be used to predict new values
    of the target, it is given the name of a column to store predictions.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，分类器的基本配置包括列名：包含输入特征向量的列和包含目标值以预测的列。因为模型将来会用于预测目标的新值，所以需要给出一个列名来存储预测结果。
- en: Printing a representation of the model shows some of its tree structure. It
    consists of a series of nested decisions about features, comparing feature values
    to thresholds. (Here, for historical reasons, the features are referred to only
    by number, not name, unfortunately.)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 打印模型的表示形式显示了它的树结构的一部分。它由一系列关于特征的嵌套决策组成，比较特征值与阈值。（由于历史原因，这些特征只用数字而不是名称来表示，这一点很不幸。）
- en: 'Decision trees are able to assess the importance of input features as part
    of their building process. That is, they can estimate how much each input feature
    contributes to making correct predictions. This information is simple to access
    from the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树能够在构建过程中评估输入特征的重要性。也就是说，它们可以估计每个输入特征对于做出正确预测的贡献程度。从模型中获取这些信息很简单：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This pairs importance values (higher is better) with column names and prints
    them in order from most to least important. Elevation seems to dominate as the
    most important feature; most features are estimated to have virtually no importance
    when predicting the cover type!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将重要性值（数值越高越好）与列名配对，并按重要性从高到低顺序打印它们。海拔似乎是最重要的特征；大多数特征在预测覆盖类型时估计几乎没有任何重要性！
- en: The resulting `DecisionTreeClassificationModel` is itself a transformer because
    it can transform a dataframe containing feature vectors into a dataframe also
    containing predictions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的`DecisionTreeClassificationModel`本身也是一个转换器，因为它可以将包含特征向量的数据框转换为同样包含预测的数据框。
- en: 'For example, it might be interesting to see what the model predicts on the
    training data and compare its prediction with the known correct cover type:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看模型对训练数据的预测，并将其与已知的正确覆盖类型进行比较可能会很有趣：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Interestingly, the output also contains a `probability` column that gives the
    model’s estimate of how likely it is that each possible outcome is correct. This
    shows that in these instances, it’s fairly sure the answer is 3 in several cases
    and quite sure the answer isn’t 1.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，输出还包含一个`probability`列，显示了模型对每种可能结果正确的估计概率。这表明在这些情况下，它相当确信答案是3，并且几乎可以肯定答案不是1。
- en: Eagle-eyed readers might note that the probability vectors actually have eight
    values even though there are only seven possible outcomes. The vector’s values
    at indices 1 to 7 do contain the probability of outcomes 1 to 7\. However, there
    is also a value at index 0, which always shows as probability 0.0\. This can be
    ignored, as 0 isn’t even a valid outcome, as this says. It’s a quirk of representing
    this information as a vector that’s worth being aware of.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者可能会注意到，概率向量实际上有八个值，即使只有七种可能的结果。向量中索引为1到7的值包含了结果1到7的概率。然而，索引为0的值总是显示为0.0的概率。这可以忽略，因为0不是有效的结果，正如所述的那样。这是表示这一信息的向量的一种特殊方式，值得注意。
- en: Based on the above snippet, it looks like the model could use some work. Its
    predictions look like they are often wrong. As with the ALS implementation in
    [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set), the
    `DecisionTreeClassifier` implementation has several hyperparameters for which
    a value must be chosen, and they’ve all been left to defaults here. Here, the
    test set can be used to produce an unbiased evaluation of the expected accuracy
    of a model built with these default hyperparameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基于以上片段，看起来模型可能需要进一步改进。它的预测看起来经常是错误的。与[第3章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中的ALS实现一样，`DecisionTreeClassifier`实现有几个需要选择值的超参数，这些参数都被默认设置。在这里，测试集可以用来产生使用这些默认超参数构建的模型的预期准确性的无偏评估。
- en: 'We will now use `MulticlassClassificationEvaluator` to compute accuracy and
    other metrics that evaluate the quality of the model’s predictions. It’s an example
    of an evaluator in MLlib, which is responsible for assessing the quality of an
    output DataFrame in some way:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用`MulticlassClassificationEvaluator`来计算准确率和其他评估模型预测质量的指标。这是MLlib中评估器的一个示例，负责以某种方式评估输出DataFrame的质量：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After being given the column containing the “label” (target, or known correct
    output value) and the name of the column containing the prediction, it finds that
    the two match about 70% of the time. This is the accuracy of this classifier.
    It can compute other related measures, like the F1 score. For our purposes here,
    accuracy will be used to evaluate classifiers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在给出包含“标签”（目标或已知正确输出值）的列和包含预测的列名之后，它发现这两者大约有70%的匹配率。这就是分类器的准确率。它还可以计算其他相关的度量值，如F1分数。在这里，我们将使用准确率来评估分类器。
- en: This single number gives a good summary of the quality of the classifier’s output.
    Sometimes, however, it can be useful to look at the *confusion matrix*. This is
    a table with a row and a column for every possible value of the target. Because
    there are seven target category values, this is a 7×7 matrix, where each row corresponds
    to an actual correct value, and each column to a predicted value, in order. The
    entry at row *i* and column *j* counts the number of times an example with true
    category *i* was predicted as category *j*. So, the correct predictions are the
    counts along the diagonal, and the predictions are everything else.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单一数字很好地总结了分类器输出的质量。然而，有时候查看*混淆矩阵*也很有用。这是一个表格，其中为每个可能的目标值（target）都有一行和一列。因为有七个目标类别值，所以这是一个7×7的矩阵，其中每行对应一个实际正确值，每列对应一个按顺序预测的值。在第*i*行和第*j*列的条目计数了真实类别*i*被预测为类别*j*的次数。因此，正确的预测在对角线上计数，而其他则是预测。
- en: It’s possible to calculate a confusion matrix directly with the DataFrame API,
    using its more general operators.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接使用DataFrame API计算混淆矩阵，利用其更一般的操作符。
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1)'
- en: Replace null with 0.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 用0替换空值。
- en: Spreadsheet users may have recognized the problem as just like that of computing
    a pivot table. A pivot table groups values by two dimensions, whose values become
    rows and columns of the output, and computes some aggregation within those groupings,
    like a count here. This is also available as a PIVOT function in several databases
    and is supported by Spark SQL. It’s arguably more elegant and powerful to compute
    it this way.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 电子表格用户可能已经意识到这个问题就像计算数据透视表一样。数据透视表通过两个维度对值进行分组，这些值成为输出的行和列，并在这些分组内计算一些聚合，例如在这里计数。这也可以作为几个数据库中的PIVOT函数提供，并受到Spark
    SQL的支持。通过这种方式计算可能更加优雅和强大。
- en: Although 70% accuracy sounds decent, it’s not immediately clear whether it is
    outstanding or poor. How well would a simplistic approach do to establish a baseline?
    Just as a broken clock is correct twice a day, randomly guessing a classification
    for each example would also occasionally produce the correct answer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管70%的准确率听起来不错，但是否出色或糟糕并不立即清楚。一个简单的方法能建立基线吗？就像一个停止的时钟每天会准确两次一样，随机猜测每个例子的分类也偶尔会得出正确答案。
- en: 'We could construct such a random “classifier” by picking a class at random
    in proportion to its prevalence in the training set. For example, if 30% of the
    training set were cover type 1, then the random classifier would guess “1” 30%
    of the time. Each classification would be correct in proportion to its prevalence
    in the test set. If 40% of the test set were cover type 1, then guessing “1” would
    be correct 40% of the time. Cover type 1 would then be guessed correctly 30% x
    40% = 12% of the time and contribute 12% to overall accuracy. Therefore, we can
    evaluate the accuracy by summing these products of probabilities:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以构建这样一个随机的“分类器”，方法是按照其在训练集中的普遍性随机选择一个类别。例如，如果训练集中有30%的覆盖类型1，则随机分类器会有30%的概率猜测“1”。每个分类在测试集中的正确性与其在测试集中的普遍性成比例。如果测试集中有40%的覆盖类型1，则猜测“1”将有40%的准确率。因此，覆盖类型1将在30%
    x 40% = 12%的时间内被正确猜测，并对总体准确率贡献12%。因此，我们可以通过对这些概率乘积求和来评估准确性：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1)'
- en: Count by category
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 按类别计数
- en: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2)'
- en: Order counts by category
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 按类别排序计数
- en: '[![3](assets/3.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3)'
- en: Sum products of pairs in training and test sets
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集和测试集中求对数乘积
- en: Random guessing achieves 37% accuracy then, which makes 70% seem like a good
    result after all. But the latter result was achieved with default hyperparameters.
    We can do even better by exploring what the hyperparameters actually mean for
    the tree-building process. That is what we will do in the next section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随机猜测当时的准确率为37%，这使得70%看起来像是一个很好的结果。但后者是使用默认超参数实现的。通过探索超参数对树构建过程的实际影响，我们可以取得更好的效果。这正是我们将在下一节中做的。
- en: Decision Tree Hyperparameters
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树超参数
- en: 'In [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    the ALS algorithm exposed several hyperparameters whose values we had to choose
    by building models with various combinations of values and then assessing the
    quality of each result using some metric. The process is the same here, although
    the metric is now multiclass accuracy instead of AUC. The hyperparameters controlling
    how the tree’s decisions are chosen will be quite different as well: maximum depth,
    maximum bins, impurity measure, and minimum information gain.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第3章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)，ALS 算法揭示了几个超参数，我们必须通过使用不同数值组合构建模型，然后使用某些度量评估每个结果的质量来选择这些数值。尽管度量标准现在是多类准确率而不是AUC，但过程是相同的。控制决策树决策方式的超参数也将大不相同：最大深度、最大
    bin 数、纯度度量和最小信息增益。
- en: '*Maximum depth* simply limits the number of levels in the decision tree. It
    is the maximum number of chained decisions that the classifier will make to classify
    an example. It is useful to limit this to avoid overfitting the training data,
    as illustrated previously in the pet store example.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*最大深度* 简单地限制了决策树中的层级数。它是分类器用于分类示例时将做出的最大连锁决策数量。为了避免过度拟合训练数据，限制这一点非常有用，正如前面在宠物店示例中所示。'
- en: 'The decision tree algorithm is responsible for coming up with potential decision
    rules to try at each level, like the `weight >= 100` or `weight >= 500` decisions
    in the pet store example. Decisions are always of the same form: for numeric features,
    decisions are of the form `feature >= value`; and for categorical features, they
    are of the form `feature in (value1, value2, …)`. So, the set of decision rules
    to try is really a set of values to plug in to the decision rule. These are referred
    to as *bins* in the PySpark MLlib implementation. A larger number of bins requires
    more processing time but might lead to finding a more optimal decision rule.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法负责提出在每个级别尝试的潜在决策规则，例如宠物店示例中的 `weight >= 100` 或 `weight >= 500` 决策。决策始终具有相同的形式：对于数值特征，决策形式为
    `feature >= value`；对于分类特征，形式为 `feature in (value1, value2, …)`。因此，要尝试的决策规则集合实际上是要插入到决策规则中的一组值。在
    PySpark MLlib 实现中称为 *bins*。更多的 bin 数量需要更多的处理时间，但可能会导致找到更优的决策规则。
- en: What makes a decision rule good? Intuitively, a good rule would meaningfully
    distinguish examples by target category value. For example, a rule that divides
    the Covtype dataset into examples with only categories 1–3 on the one hand and
    4–7 on the other would be excellent because it clearly separates some categories
    from others. A rule that resulted in about the same mix of all categories as are
    found in the whole dataset doesn’t seem helpful. Following either branch of such
    a decision leads to about the same distribution of possible target values and
    so doesn’t really make progress toward a confident classification.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使得决策规则变得好？直觉上，一个好的规则能够有意义地通过目标类别值来区分示例。例如，将Covtype数据集分成只有类别1-3和4-7的例子的规则会非常好，因为它清晰地将一些类别与其他类别分开。一个导致与整个数据集中所有类别几乎相同混合的规则似乎不会有帮助。遵循这样一个决策的任一分支会导致可能目标值的大致相同分布，因此并不会真正取得向有信心的分类的进展。
- en: 'Put another way, good rules divide the training data’s target values into relatively
    homogeneous, or “pure,” subsets. Picking a best rule means minimizing the impurity
    of the two subsets it induces. There are two commonly used measures of impurity:
    Gini impurity and entropy.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，好的规则将训练数据的目标值分成相对同质或“纯净”的子集。选择最佳规则意味着最小化它引起的两个子集的不纯度。有两种常用的不纯度度量：基尼不纯度和熵。
- en: '*Gini impurity* is directly related to the accuracy of the random guess classifier.
    Within a subset, it is the probability that a randomly chosen classification of
    a randomly chosen example (both according to the distribution of classes in the
    subset) is *incorrect*. To calculate this value, we first multiply each class
    with its respective proportion among all classes. Then we subtract the sum of
    all the values from 1\. If a subset has *N* classes and *p*[*i*] is the proportion
    of examples of class *i*, then its Gini impurity is given in the Gini impurity
    equation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*基尼不纯度*与随机猜测分类器的准确性直接相关。在一个子集内，它是随机选择的分类中一个被选择的例子（根据子集中类别的分布）*不正确*的概率。为了计算这个值，我们首先将每个类别乘以其在所有类别中的比例。然后从1中减去所有值的总和。如果一个子集有*N*个类别，*p*[*i*]是类别*i*的例子比例，则其基尼不纯度在基尼不纯度方程中给出：'
- en: <math alttext="upper I Subscript upper G Baseline left-parenthesis p right-parenthesis
    equals 1 minus sigma-summation Underscript i equals 1 Overscript upper N Endscripts
    p Subscript i Superscript 2" display="block"><mrow><msub><mi>I</mi> <mi>G</mi></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msubsup><mi>p</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper I Subscript upper G Baseline left-parenthesis p right-parenthesis
    equals 1 minus sigma-summation Underscript i equals 1 Overscript upper N Endscripts
    p Subscript i Superscript 2" display="block"><mrow><msub><mi>I</mi> <mi>G</mi></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msubsup><mi>p</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></math>
- en: If the subset contains only one class, this value is 0 because it is completely
    “pure.” When there are *N* classes in the subset, this value is larger than 0
    and is largest when the classes occur the same number of times—maximally impure.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果子集只包含一个类别，那么这个值为0，因为它完全是“纯净的”。当子集中有*N*个类别时，这个值大于0，当且仅当各类别出现次数相同时值最大——即最大不纯度。
- en: '*Entropy* is another measure of impurity, borrowed from information theory.
    Its nature is more difficult to explain, but it captures how much uncertainty
    the collection of target values in the subset implies about predictions for data
    that falls in that subset. A subset containing one class suggests that the outcome
    for the subset is completely certain and has 0 entropy—no uncertainty. A subset
    containing one of each possible class, on the other hand, suggests a lot of uncertainty
    about predictions for that subset because data has been observed with all kinds
    of target values. This has high entropy. Hence, low entropy, like low Gini impurity,
    is a good thing. Entropy is defined by the entropy equation:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*熵*是另一种来自信息论的不纯度度量。其本质更难以解释，但它捕捉了子集中目标值集合关于数据预测的不确定性程度。包含一个类别的子集表明该子集的结果是完全确定的，并且熵为0——没有不确定性。另一方面，包含每种可能类别的子集则表明对于该子集的预测有很大的不确定性，因为数据已经观察到各种目标值。这就有很高的熵。因此，低熵和低基尼不纯度一样，是一个好事。熵由熵方程定义：'
- en: <math alttext="upper I Subscript upper E Baseline left-parenthesis p right-parenthesis
    equals sigma-summation Underscript i equals 1 Overscript upper N Endscripts p
    Subscript i Baseline log left-parenthesis StartFraction 1 Over p Subscript i Baseline
    EndFraction right-parenthesis equals minus sigma-summation Underscript i equals
    1 Overscript upper N Endscripts p Subscript i Baseline log left-parenthesis p
    Subscript i Baseline right-parenthesis" display="block"><mrow><msub><mi>I</mi>
    <mi>E</mi></msub> <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn>
    <msub><mi>p</mi> <mi>i</mi></msub></mfrac> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper I Subscript upper E Baseline left-parenthesis p right-parenthesis
    equals sigma-summation Underscript i equals 1 Overscript upper N Endscripts p
    Subscript i Baseline log left-parenthesis StartFraction 1 Over p Subscript i Baseline
    EndFraction right-parenthesis equals minus sigma-summation Underscript i equals
    1 Overscript upper N Endscripts p Subscript i Baseline log left-parenthesis p
    Subscript i Baseline right-parenthesis" display="block"><mrow><msub><mi>I</mi>
    <mi>E</mi></msub> <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn>
    <msub><mi>p</mi> <mi>i</mi></msub></mfrac> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: Interestingly, uncertainty has units. Because the logarithm is the natural log
    (base *e*), the units are *nats*, the base *e* counterpart to more familiar *bits*
    (which we can obtain by using log base 2 instead). It really is measuring information,
    so it’s also common to talk about the *information gain* of a decision rule when
    using entropy with decision trees.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，不确定性是有单位的。因为对数是自然对数（以*e*为底），单位是*nats*，是更熟悉的*bits*的对应物（通过使用以2为底的对数可以得到）。它确实是在测量信息，因此在使用熵来进行决策树时，谈论决策规则的*信息增益*也很常见。
- en: 'One or the other measure may be a better metric for picking decision rules
    in a given dataset. They are, in a way, similar. Both involve a weighted average:
    a sum over values weighted by *p*[*i*]. The default in PySpark’s implementation
    is Gini impurity.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定数据集中，其中一个或另一个度量可能更适合选择决策规则。在某种程度上，它们是相似的。两者都涉及加权平均：按*p*[i]加权值的总和。PySpark实现的默认值是基尼不纯度。
- en: Finally, *minimum information gain* is a hyperparameter that imposes a minimum
    information gain, or decrease in impurity, for candidate decision rules. Rules
    that do not improve the subsets’ impurity enough are rejected. Like a lower maximum
    depth, this can help the model resist overfitting because decisions that barely
    help divide the training input may in fact not helpfully divide future data at
    all.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*最小信息增益*是一个超参数，它对候选决策规则施加最小信息增益或不纯度减少要求。那些不能足够改善子集不纯度的规则将被拒绝。与较低的最大深度类似，这可以帮助模型抵抗过拟合，因为仅仅能帮助将训练输入划分的决策，可能实际上在将来的数据中并不帮助划分。
- en: Now that we understand the relevant hyperparameters of a decision tree algorithm,
    we will tune our model in the next section to improve its performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了决策树算法的相关超参数，接下来我们将在下一节调整我们的模型以提高其性能。
- en: Tuning Decision Trees
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整决策树
- en: It’s not obvious from looking at the data which impurity measure leads to better
    accuracy or what maximum depth or number of bins is enough without being excessive.
    Fortunately, as in [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    it’s simple to let PySpark try a number of combinations of these values and report
    the results.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据看，不明显哪种不纯度度量可以带来更好的准确性，或者最大深度或者箱数目是否足够而不过度。幸运的是，就像在[第3章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中一样，让PySpark尝试多个这些值的组合并报告结果是很简单的。
- en: 'First, it’s necessary to set up a pipeline encapsulating the two steps we performed
    in previous sections—creating a feature vector and using it to create a decision
    tree model. Creating the `VectorAssembler` and `DecisionTreeClassifier` and chaining
    these two `Transformer`s together results in a single `Pipeline` object that represents
    these two operations together as one operation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要设置一个流水线，封装我们在前几节中执行的两个步骤——创建特征向量和使用它创建决策树模型。创建`VectorAssembler`和`DecisionTreeClassifier`，并将这两个`Transformer`链接在一起，结果是一个单一的`Pipeline`对象，它将这两个操作作为一个操作整合在一起：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Naturally, pipelines can be much longer and more complex. This is about as
    simple as it gets. Now we can also define the combinations of hyperparameters
    that should be tested using the PySpark ML API’s built-in support, `ParamGridBuilder`.
    It’s also time to define the evaluation metric that will be used to pick the “best”
    hyperparameters, and that is again `MulticlassClassificationEvaluator`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，流水线可以更长，更复杂。这是最简单的情况。现在我们还可以使用PySpark ML API内置的支持，定义应该使用的超参数组合的`ParamGridBuilder`。现在是定义评估指标的时候了，将用于选择“最佳”超参数，这是`MulticlassClassificationEvaluator`：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This means that a model will be built and evaluated for two values of four
    hyperparameters. That’s 16 models. They’ll be evaluated by multiclass accuracy.
    Finally, `TrainValidationSplit` brings these components together—the pipeline
    that makes models, model evaluation metrics, and hyperparameters to try—and can
    run the evaluation on the training data. It’s worth noting that `CrossValidator`
    could be used here as well to perform full k-fold cross-validation, but it is
    *k* times more expensive and doesn’t add as much value in the presence of big
    data. So, `TrainValidationSplit` is used here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着将建立一个模型，并对四个超参数的两个值进行评估。这是16个模型。它们将通过多类精度进行评估。最后，`TrainValidationSplit`将这些组件结合在一起——创建模型的流水线、模型评估指标和要尝试的超参数——并且可以在训练数据上运行评估。值得注意的是，这里也可以使用`CrossValidator`来执行完整的k折交叉验证，但它的成本是*k*倍，并且在处理大数据时添加的价值不如`TrainValidationSplit`大。因此，这里使用了`TrainValidationSplit`：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will take several minutes or more, depending on your hardware, because
    it’s building and evaluating many models. Note the train ratio parameter is set
    to 0.9\. This means that the training data is actually further subdivided by `TrainValidationSplit`
    into 90%/10% subsets. The former is used for training each model. The remaining
    10% of the input is held out as a cross-validation set to evaluate the model.
    If it’s already holding out some data for evaluation, then why did we hold out
    10% of the original data as a test set?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将花费几分钟甚至更长时间，具体取决于您的硬件，因为它正在构建和评估许多模型。请注意，训练比例参数设置为0.9。这意味着训练数据实际上由`TrainValidationSplit`进一步分为90%/10%子集。前者用于训练每个模型。剩余的10%输入数据保留为交叉验证集以评估模型。如果已经保留了一些数据用于评估，那么为什么我们还要保留原始数据的10%作为测试集？
- en: If the purpose of the CV set was to evaluate *parameters* that fit to the *training*
    set, then the purpose of the test set is to evaluate *hyperparameters* that were
    “fit” to the CV set. That is, the test set ensures an unbiased estimate of the
    accuracy of the final, chosen model and its hyperparameters.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果CV集的目的是评估适合*训练*集的*参数*，那么测试集的目的是评估适合CV集的*超参数*。换句话说，测试集确保了对最终选择的模型及其超参数准确度的无偏估计。
- en: Say that the best model chosen by this process exhibits 90% accuracy on the
    CV set. It seems reasonable to expect it will exhibit 90% accuracy on future data.
    However, there’s an element of randomness in how these models are built. By chance,
    this model and evaluation could have turned out unusually well. The top model
    and evaluation result could have benefited from a bit of luck, so its accuracy
    estimate is likely to be slightly optimistic. Put another way, hyperparameters
    can overfit too.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个过程选择的最佳模型在CV集上表现出90%的准确率。预计它将在未来的数据上表现出90%的准确率似乎是合理的。然而，构建这些模型的过程中存在随机性。由于偶然因素，这个模型和评估结果可能会特别好。顶级模型和评估结果可能会受益于一点运气，因此其准确度估计可能稍微乐观一些。换句话说，超参数也可能会出现过拟合的情况。
- en: To really assess how well this best model is likely to perform on future examples,
    we need to evaluate it on examples that were not used to train it. But we also
    need to avoid examples in the CV set that were used to evaluate it. That is why
    a third subset, the test set, was held out.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正评估这个最佳模型在未来示例中的表现如何，我们需要对未用于训练的示例进行评估。但我们也需要避免使用CV集中用于评估它的示例。这就是为什么第三个子集，即测试集，被保留的原因。
- en: 'The result of the validator contains the best model it found. This itself is
    a representation of the best overall *pipeline* it found, because we provided
    an instance of a pipeline to run. To query the parameters chosen by `DecisionTreeClassifier`,
    it’s necessary to manually extract `DecisionTreeClassificationModel` from the
    resulting `PipelineModel`, which is the final stage in the pipeline:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 验证器的结果包含它找到的最佳模型。这本身就是它找到的最佳整体*管道*的表示，因为我们提供了一个要运行的管道实例。要查询由`DecisionTreeClassifier`选择的参数，必须手动从最终的`PipelineModel`中提取`DecisionTreeClassificationModel`，这是管道中的最后一个阶段：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This output contains a lot of information about the fitted model, but it also
    tells us that entropy apparently worked best as the impurity measure and that
    a max depth of 20 was not surprisingly better than 1\. It might be surprising
    that the best model was fit with just 40 bins, but this is probably a sign that
    40 was “plenty” rather than “better” than 300\. Lastly, no minimum information
    gain was better than a small minimum, which could imply that the model is more
    prone to underfit than overfit.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出包含关于拟合模型的大量信息，但它还告诉我们，熵显然作为杂质度量效果最好，并且最大深度为20并不出奇。最佳模型仅使用40个箱似乎有些令人惊讶，但这可能表明40个“足够”而不是“比300更好”。最后，没有最小信息增益比小最小信息增益更好，这可能意味着该模型更容易欠拟合而不是过拟合。
- en: 'You may wonder if it is possible to see the accuracy that each of the models
    achieved for each combination of hyperparameters. The hyperparameters and the
    evaluations are exposed by `getEstimatorParamMaps` and `validationMetrics`, respectively.
    They can be combined to display all of the parameter combinations sorted by metric
    value:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道是否可以查看每个模型在每种超参数组合下实现的准确率。超参数和评估结果通过`getEstimatorParamMaps`和`validationMetrics`公开。它们可以组合在一起，按度量值排序显示所有参数组合：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: What was the accuracy that this model achieved on the CV set? And, finally,
    what accuracy does the model achieve on the test set?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在CV集上达到了什么准确率？最后，这个模型在测试集上达到了什么准确率？
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1)'
- en: '`best_Model` is a complete pipeline.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`best_Model` 是一个完整的流水线。'
- en: The results are both about 91%. It happens that the estimate from the CV set
    was pretty fine to begin with. In fact, it is not usual for the test set to show
    a very different result.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结果都约为 91%。偶然情况下，来自 CV 集的估计一开始就相当不错。事实上，测试集显示非常不同的结果并不常见。
- en: This is an interesting point at which to revisit the issue of overfitting. As
    discussed previously, it’s possible to build a decision tree so deep and elaborate
    that it fits the given training examples very well or perfectly but fails to generalize
    to other examples because it has fit the idiosyncrasies and noise of the training
    data too closely. This is a problem common to most machine learning algorithms,
    not just decision trees.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是重新审视过拟合问题的一个有趣时刻。如前所述，可能会构建一个非常深奥的决策树，使其非常好或完全适合给定的训练示例，但由于过于密切地拟合了训练数据的特异性和噪声，无法推广到其他示例。这是大多数机器学习算法（不仅仅是决策树）常见的问题。
- en: When a decision tree has overfit, it will exhibit high accuracy when run on
    the same training data that it fit the model to, but low accuracy on other examples.
    Here, the final model’s accuracy was about 91% on other, new examples. Accuracy
    can just as easily be evaluated over the same data that the model was trained
    on, `trainData`. This gives an accuracy of about 95%. The difference is not large
    but suggests that the decision tree has overfit the training data to some extent.
    A lower maximum depth might be a better choice.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当决策树过度拟合时，在用于拟合模型的相同训练数据上，其准确率会很高，但在其他示例上准确率会很低。在这里，最终模型对其他新示例的准确率约为 91%。准确率也可以很容易地在模型训练时的相同数据上评估，`trainData`。这样的准确率约为
    95%。差异不大，但表明决策树在某种程度上过度拟合了训练数据。更低的最大深度可能是更好的选择。
- en: So far, we’ve implicitly treated all input features, including categoricals,
    as if they’re numeric. Can we improve our model’s performance further by treating
    categorical features as exactly that? We will explore this next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将所有输入特征（包括分类特征）隐式地视为数值。通过将分类特征作为确切的分类特征处理，我们可以进一步提高模型的性能。我们将在接下来探讨这一点。
- en: Categorical Features Revisited
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类特征再探讨
- en: The categorical features in our dataset are one-hot encoded as several binary
    0/1 values. Treating these individual features as numeric turns out to be fine,
    because any decision rule on the “numeric” features will choose thresholds between
    0 and 1, and all are equivalent since all values are 0 or 1.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的分类特征被单热编码为多个二进制 0/1 值。将这些单独特征视为数值处理是可行的，因为对于“数值”特征的任何决策规则都会选择 0 到 1 之间的阈值，而且所有阈值都是等价的，因为所有值都是
    0 或 1。
- en: Of course, this encoding forces the decision tree algorithm to consider the
    values of the underlying categorical features individually. Because features like
    soil type are broken down into many features and because decision trees treat
    features individually, it is harder to relate information about related soil types.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种编码方式迫使决策树算法单独考虑底层分类特征的值。由于特征如土壤类型被分解成许多特征，并且决策树单独处理特征，因此更难以关联相关土壤类型的信息。
- en: For example, nine different soil types are actually part of the Leighton family,
    and they may be related in ways that the decision tree can exploit. If soil type
    were encoded as a single categorical feature with 40 soil values, then the tree
    could express rules like “if the soil type is one of the nine Leighton family
    types” directly. However, when encoded as 40 features, the tree would have to
    learn a sequence of nine decisions on soil type to do the same, this expressiveness
    may lead to better decisions and more efficient trees.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，九种不同的土壤类型实际上是莱顿家族的一部分，它们可能以决策树可以利用的方式相关联。如果将土壤类型编码为一个具有 40 个土壤值的单一分类特征，则树可以直接表达诸如“如果土壤类型是九种莱顿家族类型之一”的规则。然而，当编码为
    40 个特征时，树必须学习关于土壤类型的九个决策序列才能达到相同的表达能力，这种表达能力可能会导致更好的决策和更高效的树。
- en: However, having 40 numeric features represent one 40-valued categorical feature
    increases memory usage and slows things down.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将 40 个数值特征代表一个 40 值分类特征增加了内存使用量并减慢了速度。
- en: 'What about undoing the one-hot encoding? This would replace, for example, the
    four columns encoding wilderness type with one column that encodes the wilderness
    type as a number between 0 and 3, like `Cover_Type`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如何撤销独热编码？例如，将四列编码的荒野类型替换为一个列，该列将荒野类型编码为 0 到 3 之间的数字，如`Cover_Type`：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1)'
- en: Note UDF definition
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 UDF 定义
- en: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2)'
- en: Drop one-hot columns; no longer needed
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 删除一热编码列；不再需要
- en: Here `VectorAssembler` is deployed to combine the 4 and 40 wilderness and soil
    type columns into two `Vector` columns. The values in these `Vector`s are all
    0, except for one location that has a 1\. There’s no simple DataFrame function
    for this, so we have to define our own UDF that can be used to operate on columns.
    This turns these two new columns into numbers of just the type we need.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用`VectorAssembler`来将 4 个荒野和土壤类型列合并成两个`Vector`列。这些`Vector`中的值都是 0，只有一个位置是
    1。没有简单的 DataFrame 函数可以做到这一点，因此我们必须定义自己的 UDF 来操作列。这将把这两个新列转换为我们需要的类型的数字。
- en: 'We can now transform our dataset by removing one-hot encoding using our function
    defined above:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过上面定义的函数转换我们的数据集，删除独热编码：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: From here, nearly the same process as above can be used to tune the hyperparameters
    of a decision tree model built on this data and to choose and evaluate a best
    model. There’s one important difference, however. The two new numeric columns
    have nothing about them that indicates they’re actually an encoding of categorical
    values. To treat them as numbers is not correct, as their ordering is meaningless.
    The model will still get built but because of some information in these features
    not being available, the accuracy may suffer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，几乎可以使用与上述相同的过程来调整建立在这些数据上的决策树模型的超参数，并选择和评估最佳模型。但是有一个重要的区别。这两个新的数值列并没有任何信息表明它们实际上是分类值的编码。将它们视为数字是不正确的，因为它们的排序是没有意义的。模型仍然会构建，但由于这些特征中的一些信息不可用，精度可能会受到影响。
- en: Internally MLlib can store additional metadata about each column. The details
    of this data are generally hidden from the caller but include information such
    as whether the column encodes a categorical value and how many distinct values
    it takes on. To add this metadata, it’s necessary to put the data through `VectorIndexer`.
    Its job is to turn input into properly labeled categorical feature columns. Although
    we did much of the work already to turn the categorical features into 0-indexed
    values, `VectorIndexer` will take care of the metadata.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，MLlib 可以存储关于每列的额外元数据。这些数据的细节通常对调用方隐藏，但包括列是否编码为分类值以及它取多少个不同的值等信息。要添加这些元数据，需要通过`VectorIndexer`处理数据。它的工作是将输入转换为正确标记的分类特征列。虽然我们已经做了大部分工作，将分类特征转换为
    0 索引的值，但`VectorIndexer`将负责元数据。
- en: 'We need to add this stage to the `Pipeline`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将此阶段添加到`Pipeline`中：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1)'
- en: '>= 40 because soil has 40 values'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 大于等于 40，因为土壤有 40 个值
- en: The approach assumes that the training set contains all possible values of each
    of the categorical features at least once. That is, it works correctly only if
    all 4 soil values and all 40 wilderness values appear in the training set so that
    all possible values get a mapping. Here, that happens to be true, but may not
    be for small training sets of data in which some labels appear very infrequently.
    In those cases, it could be necessary to manually create and add a `VectorIndexerModel`
    with the complete value mapping supplied manually.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法假设训练集中至少包含每个分类特征的所有可能值一次。也就是说，只有当所有 4 个土壤值和所有 40 个荒野值出现在训练集中时，它才能正确工作，以便所有可能的值都能得到映射。在这里，情况确实如此，但对于一些标签非常少出现的小训练数据集来说，情况可能并非如此。在这些情况下，可能需要手动创建并添加一个`VectorIndexerModel`，并手动提供完整的值映射。
- en: Aside from that, the process is the same as before. You should find that it
    chose a similar best model but that accuracy on the test set is about 93%. By
    treating categorical features as actual categorical features in the previous sections,
    the classifier improved its accuracy by almost 2%.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，这个过程和之前是一样的。你应该发现它选择了一个类似的最佳模型，但是在测试集上的准确率大约是93%。通过在前几节中将分类特征视为实际的分类特征，分类器的准确率提高了将近2%。
- en: We have trained and tuned a decision tree. Now, we will move on to random forests,
    a more powerful algorithm. As we will see in the next section, implementing them
    using PySpark will be surprisingly straightforward at this point.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练并调整了一个决策树。现在，我们将转向随机森林，这是一个更强大的算法。正如我们将在下一节看到的，使用PySpark实现它们在这一点上将会非常简单。
- en: Random Forests
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: If you have been following along with the code examples, you may have noticed
    that your results differ slightly from those presented in the code listings in
    the book. That is because there is an element of randomness in building decision
    trees, and the randomness comes into play when you’re deciding what data to use
    and what decision rules to explore.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经跟着代码示例走过来，你可能会注意到你的结果与书中代码清单中呈现的结果略有不同。这是因为在构建决策树时存在随机因素，而当你决定使用什么数据和探索什么决策规则时，随机性就会发挥作用。
- en: The algorithm does not consider every possible decision rule at every level.
    To do so would take an incredible amount of time. For a categorical feature over
    *N* values, there are 2^(*N*)–2 possible decision rules (every subset except the
    empty set and entire set). For an even moderately large *N*, this would create
    billions of candidate decision rules.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在每个级别并不考虑每一个可能的决策规则。这样做将需要大量的时间。对于一个包含*N*个值的分类特征，存在2^(*N*)–2个可能的决策规则（每个子集除了空集和整个集合）。对于一个即使是适度大的*N*，这将产生数十亿个候选决策规则。
- en: Instead, decision trees use several heuristics to determine which few rules
    to actually consider. The process of picking rules also involves some randomness;
    only a few features picked at random are looked at each time, and only values
    from a random subset of the training data. This trades a bit of accuracy for a
    lot of speed, but it also means that the decision tree algorithm won’t build the
    same tree every time. This is a good thing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，决策树使用几个启发式方法来确定实际考虑哪些规则。挑选规则的过程也涉及一些随机性；每次只查看随机选择的少数几个特征，并且仅来自训练数据的随机子集的值。这种做法在速度上稍微牺牲了一点准确性，但也意味着决策树算法不会每次都构建相同的树。这是件好事。
- en: 'It’s good for the same reason that the “wisdom of the crowds” usually beats
    individual predictions. To illustrate, take this quick quiz: how many black taxis
    operate in London?'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 出于同样的原因，通常“众人的智慧”会胜过个体的预测。为了说明这一点，来做个快速测验：伦敦有多少辆黑色出租车？
- en: Don’t peek at the answer; guess first.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 不要先看答案；先猜测。
- en: 'I guessed 10,000, which is well off the correct answer of about 19,000\. Because
    I guessed low, you’re a bit more likely to have guessed higher than I did, and
    so the average of our answers will tend to be more accurate. There’s that regression
    to the mean again. The average guess from an informal poll of 13 people in the
    office was indeed closer: 11,170.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜大约10,000辆，这远低于正确答案约为19,000辆。因为我猜得比较少，你更有可能比我猜得多，所以我们的平均答案将更接近正确。这里又出现了均值回归。办公室里13个人的非正式投票平均猜测确实更接近：11,170。
- en: A key to this effect is that the guesses were independent and didn’t influence
    one another. (You didn’t peek, did you?) The exercise would be useless if we had
    all agreed on and used the same methodology to make a guess, because the guesses
    would have been the same answer—the same potentially quite wrong answer. It would
    even have been different and worse if I’d merely influenced you by stating my
    guess up front.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种效应的关键在于猜测是独立的，并且不会相互影响。（你没有偷看，对吧？）如果我们都同意并使用相同的方法进行猜测，这个练习将毫无用处，因为猜测将会是相同的答案——可能是完全错误的答案。如果我仅仅通过提前说明我的猜测来影响你，结果甚至会更糟。
- en: It would be great to have not one tree but many trees, each producing reasonable
    but different and independent estimations of the right target value. Their collective
    average prediction should fall close to the true answer, more than any individual
    tree’s does. It’s the *randomness* in the process of building that helps create
    this independence. This is the key to *random forests*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 不只一棵树，而是许多树将会很棒，每棵树都会产生合理但不同和独立的目标值估计。它们的集体平均预测应该接近真实答案，比任何单棵树的预测更接近。这个过程中的*随机性*有助于创建这种独立性。这是*随机森林*的关键。
- en: Randomness is injected by building many trees, each of which sees a different
    random subset of data—and even of features. This makes the forest as a whole less
    prone to overfitting. If a particular feature contains noisy data or is deceptively
    predictive only in the *training* set, then most trees will not consider this
    problem feature most of the time. Most trees will not fit the noise and will tend
    to “outvote” the trees that have fit the noise in the forest.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建许多树注入随机性，每棵树都看到不同的随机数据子集——甚至是特征子集。这使得整个森林对过度拟合的倾向较小。如果特定特征包含嘈杂数据或只在*训练*集中具有迷惑性预测能力，那么大多数树大部分时间都不会考虑这个问题特征。大多数树不会适应噪声，并倾向于“否决”在森林中适应噪声的树。
- en: The prediction of a random forest is simply a weighted average of the trees’
    predictions. For a categorical target, this can be a majority vote or the most
    probable value based on the average of probabilities produced by the trees. Random
    forests, like decision trees, also support regression, and the forest’s prediction
    in this case is the average of the number predicted by each tree.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的预测只是树预测的加权平均值。对于分类目标，这可以是多数投票或基于树产生的概率平均值的最可能值。与决策树一样，随机森林也支持回归，而在这种情况下，森林的预测是每棵树预测数的平均值。
- en: 'While random forests are a more powerful and complex classification technique,
    the good news is that it’s virtually no different to use it in the pipeline that
    has been developed in this chapter. Simply drop in a `RandomForestClassifier`
    in place of `DecisionTreeClassifier` and proceed as before. There’s really no
    more code or API to understand in order to use it:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然随机森林是一种更强大且复杂的分类技术，好消息是在本章开发的流水线中使用它实际上几乎没有区别。只需将`RandomForestClassifier`放入`DecisionTreeClassifier`的位置，并像以前一样继续即可。实际上，没有更多的代码或API需要理解以便使用它：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that this classifier has another hyperparameter: the number of trees to
    build. Like the max bins hyperparameter, higher values should give better results
    up to a point. The cost, however, is that building many trees of course takes
    many times longer than building one.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个分类器还有另一个超参数：要构建的树的数量。像最大箱子超参数一样，更高的值应该在一定程度上产生更好的结果。然而，代价是，构建许多树当然比构建一棵树需要花费的时间长得多。
- en: The accuracy of the best random forest model produced from a similar tuning
    process is 95% off the bat—about 2% better already, although viewed another way,
    that’s a 28% reduction in the error rate over the best decision tree built previously,
    from 7% down to 5%. You may do better with further tuning.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从类似调整过程中产生的最佳随机森林模型的准确率一开始就达到了95%——比以前建立的最佳决策树高出约2%，尽管从另一个角度来看，这是从7%降到5%的误差率减少了28%。通过进一步调整，你可能会获得更好的结果。
- en: 'Incidentally, at this point we have a more reliable picture of feature importance:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，在这一点上，我们对特征重要性有了更可靠的了解：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Random forests are appealing in the context of big data because trees are supposed
    to be built independently, and big data technologies like Spark and MapReduce
    inherently need *data-parallel* problems, where parts of the overall solution
    can be computed independently on parts of the data. The fact that trees can, and
    should, train on only a subset of features or input data makes it trivial to parallelize
    building the trees.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林在大数据背景下非常吸引人，因为树被认为是独立构建的，而大数据技术如Spark和MapReduce固有地需要*数据并行*问题，即整体解决方案的各部分可以独立计算在数据的各部分上。树可以且应该仅在特征或输入数据的子集上训练，使得并行构建树变得微不足道。
- en: Making Predictions
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行预测
- en: Building a classifier, while an interesting and nuanced process, is not the
    end goal. The goal is to make predictions. This is the payoff, and it is comparatively
    quite easy.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 构建分类器，虽然是一个有趣且微妙的过程，但并非最终目标。目标是进行预测。这才是回报，而且相对来说相当容易。
- en: The resulting “best model” is actually a whole pipeline of operations. It encapsulates
    how input is transformed for use with the model and includes the model itself,
    which can make predictions. It can operate on a dataframe of new input. The only
    difference from the `data` DataFrame we started with is that it lacks the `Cover_Type`
    column. When we’re making predictions—especially about the future, says Mr. Bohr—the
    output is of course not known.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的“最佳模型”实际上是一整套操作流程。它包含了如何将输入转换为模型使用的方式，并包括模型本身，可以进行预测。它可以在新输入的数据框架上操作。与我们开始的`data`数据框架唯一的区别是缺少`Cover_Type`列。当我们进行预测时——尤其是关于未来的预测，如Bohr先生所说——输出当然是未知的。
- en: 'To prove it, try dropping the `Cover_Type` from the test data input and obtaining
    a prediction:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一点，请尝试从测试数据输入中删除`Cover_Type`并获得预测：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The result should be 6.0, which corresponds to class 7 (the original feature
    was 1-indexed) in the original Covtype dataset. The predicted cover type for the
    land described in this example is Krummholz.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是6.0，这对应于原始Covtype数据集中的第7类（原始特征为1索引）。本示例描述的土地的预测覆盖类型是Krummholz。
- en: Where to Go from Here
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步该去哪里
- en: 'This chapter introduced two related and important types of machine learning,
    classification and regression, along with some foundational concepts in building
    and tuning models: features, vectors, training, and cross-validation. It demonstrated
    how to predict a type of forest cover from things like location and soil type
    using the Covtype dataset, with decision trees and forests implemented in PySpark.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了机器学习中的两种相关且重要的类型，分类和回归，以及一些构建和调整模型的基础概念：特征、向量、训练和交叉验证。它演示了如何使用Covtype数据集，通过位置和土壤类型等因素预测森林覆盖类型，使用PySpark实现了决策树和随机森林。
- en: 'As with recommenders in [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    it could be useful to continue exploring the effect of hyperparameters on accuracy.
    Most decision tree hyperparameters trade time for accuracy: more bins and trees
    generally produce better accuracy but hit a point of diminishing returns.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 就像[第3章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中的推荐系统一样，继续探索超参数对准确性的影响可能会很有用。大多数决策树超参数在时间和准确性之间进行权衡：更多的箱子和树通常可以提高准确性，但会遇到收益递减的点。
- en: 'The classifier here turned out to be very accurate. It’s unusual to achieve
    more than 95% accuracy. In general, you will achieve further improvements in accuracy
    by including more features or transforming existing features into a more predictive
    form. This is a common, repeated step in iteratively improving a classifier model.
    For example, for this dataset, the two features encoding horizontal and vertical
    distance-to-surface-water features could produce a third feature: straight-line
    distance-to-surface-water features. This might turn out to be more useful than
    either original feature. Or, if it were possible to collect more data, we might
    try adding new information like soil moisture to improve classification.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的分类器结果非常准确。通常情况下，超过95%的准确率是不寻常的。一般来说，通过包含更多特征或将现有特征转换为更具预测性的形式，可以进一步提高准确性。这是在迭代改进分类器模型中常见且重复的步骤。例如，对于这个数据集，编码水平和垂直距离到水面特征的两个特征可以产生第三个特征：直线距离到水面特征。这可能比任何原始特征都更有用。或者，如果可能收集更多数据，我们可以尝试添加像土壤湿度这样的新信息来改善分类。
- en: Of course, not all prediction problems in the real world are exactly like the
    Covtype dataset. For example, some problems require predicting a continuous numeric
    value, not a categorical value. Much of the same analysis and code applies to
    this type of *regression* problem; the `RandomForestRegressor` class will be of
    use in this case.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并非所有真实世界中的预测问题都与Covtype数据集完全相同。例如，有些问题需要预测连续的数值，而不是分类值。对于这种类型的*回归*问题，相同的分析和代码适用；在这种情况下，`RandomForestRegressor`类将非常有用。
- en: 'Furthermore, decision trees and forests are not the only classification or
    regression algorithms, and not the only ones implemented in PySpark. Each algorithm
    operates quite differently from decision trees and forests. However, many elements
    are the same: they plug into a `Pipeline` and operate on columns in a dataframe,
    and have hyperparameters that you must select using training, cross-validation,
    and test subsets of the input data. The same general principles, with these other
    algorithms, can also be deployed to model classification and regression problems.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，决策树和随机森林并非唯一的分类或回归算法，也不是仅有的 PySpark 实现算法。每种算法的操作方式都与决策树和随机森林有很大不同。然而，许多元素是相同的：它们都可以插入到
    `Pipeline` 中，在数据帧的列上操作，并且有超参数需要使用训练、交叉验证和测试数据子集来选择。同样的一般原则，也可以用于这些其他算法来建模分类和回归问题。
- en: These have been examples of supervised learning. What happens when some, or
    all, of the target values are unknown? The following chapter will explore what
    can be done in this situation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是监督学习的例子。当一些或所有目标值未知时会发生什么？接下来的章节将探讨在这种情况下可以采取的措施。
