- en: Chapter 11\. Understanding Kubernetes internals
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章\. 理解Kubernetes内部机制
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: What components make up a Kubernetes cluster
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构成Kubernetes集群的组件
- en: What each component does and how it does it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个组件的功能以及它是如何实现的
- en: How creating a Deployment object results in a running pod
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建Deployment对象如何导致运行中的Pod
- en: What a running pod is
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行中的Pod是什么
- en: How the network between pods works
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod之间的网络是如何工作的
- en: How Kubernetes Services work
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes服务是如何工作的
- en: How high-availability is achieved
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何实现高可用性
- en: By reading this book up to this point, you’ve become familiar with what Kubernetes
    has to offer and what it does. But so far, I’ve intentionally not spent much time
    explaining exactly how it does all this because, in my opinion, it makes no sense
    to go into details of how a system works until you have a good understanding of
    what the system does. That’s why we haven’t talked about exactly how a pod is
    scheduled or how the various controllers running inside the Controller Manager
    make deployed resources come to life. Because you now know most resources that
    can be deployed in Kubernetes, it’s time to dive into how they’re implemented.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读这本书到这一点，你已经熟悉了Kubernetes提供的内容以及它所做的工作。但到目前为止，我故意没有花太多时间解释它到底是如何做到这一切的，因为在我看来，在你对系统的工作有良好理解之前，深入探讨系统的工作原理是没有意义的。这就是为什么我们没有详细讨论Pod是如何被调度的，以及运行在Controller
    Manager内部的各个控制器是如何使部署的资源变得活跃的。因为你现在已经了解了Kubernetes中可以部署的大多数资源，现在是时候深入了解它们的实现方式了。
- en: 11.1\. Understanding the architecture
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1\. 理解架构
- en: 'Before you look at how Kubernetes does what it does, let’s take a closer look
    at the components that make up a Kubernetes cluster. In [chapter 1](index_split_017.html#filepos122588),
    you saw that a Kubernetes cluster is split into two parts:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在你查看Kubernetes是如何工作的之前，让我们更仔细地看看构成Kubernetes集群的组件。在[第1章](index_split_017.html#filepos122588)中，你看到Kubernetes集群被分为两部分：
- en: The Kubernetes Control Plane
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes控制平面
- en: The (worker) nodes
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （工作）节点
- en: Let’s look more closely at what these two parts do and what’s running inside
    them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这两部分的功能以及它们内部运行的内容。
- en: Components of the Control Plane
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面的组件
- en: The Control Plane is what controls and makes the whole cluster function. To
    refresh your memory, the components that make up the Control Plane are
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面是控制和使整个集群运行的部分。为了刷新你的记忆，构成控制平面的组件包括
- en: The etcd distributed persistent storage
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 分布式持久化存储
- en: The API server
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API服务器
- en: The Scheduler
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器
- en: The Controller Manager
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器管理器
- en: These components store and manage the state of the cluster, but they aren’t
    what runs the application containers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件存储和管理集群的状态，但它们不是运行应用程序容器的部分。
- en: Components running on the worker nodes
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点上运行的组件
- en: 'The task of running your containers is up to the components running on each
    worker node:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 运行你的容器的任务由每个工作节点上的组件来完成：
- en: The Kubelet
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubelet
- en: The Kubernetes Service Proxy (kube-proxy)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes服务代理（kube-proxy）
- en: The Container Runtime (Docker, rkt, or others)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时（Docker、rkt或其他）
- en: Add-on components
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 附加组件
- en: Beside the Control Plane components and the components running on the nodes,
    a few add-on components are required for the cluster to provide everything discussed
    so far. This includes
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 除了控制平面组件和运行在节点上的组件之外，集群还需要一些附加组件来提供到目前为止所讨论的所有内容。这包括
- en: The Kubernetes DNS server
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes DNS服务器
- en: The Dashboard
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仪表板
- en: An Ingress controller
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口控制器
- en: Heapster, which we’ll talk about in [chapter 14](index_split_105.html#filepos1325290)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heapster，我们将在[第14章](index_split_105.html#filepos1325290)中讨论
- en: The Container Network Interface network plugin (we’ll explain it later in this
    chapter)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器网络接口网络插件（我们将在本章后面解释）
- en: 11.1.1\. The distributed nature of Kubernetes components
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.1\. Kubernetes组件的分布式特性
- en: The previously mentioned components all run as individual processes. The components
    and their inter-dependencies are shown in [figure 11.1](#filepos1040687).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的组件都作为独立进程运行。组件及其相互依赖关系在[图11.1](#filepos1040687)中显示。
- en: Figure 11.1\. Kubernetes components of the Control Plane and the worker nodes
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1\. 控制平面和工作节点上的Kubernetes组件
- en: '![](images/00188.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00188.jpg)'
- en: To get all the features Kubernetes provides, all these components need to be
    running. But several can also perform useful work individually without the other
    components. You’ll see how as we examine each of them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得Kubernetes提供的所有功能，所有这些组件都需要运行。但其中一些也可以在没有其他组件的情况下单独执行有用的任务。你将在我们检查每个组件时看到这一点。
- en: '|  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Checking the status of the Control Plane components
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 检查控制平面组件的状态
- en: 'The API server exposes an API resource called ComponentStatus, which shows
    the health status of each Control Plane component. You can list the components
    and their statuses with `kubectl`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器公开了一个名为ComponentStatus的API资源，它显示了每个控制平面组件的健康状态。您可以使用`kubectl`列出组件及其状态：
- en: '`$ kubectl get componentstatuses` `NAME                 STATUS    MESSAGE             
    ERROR scheduler            Healthy   ok controller-manager   Healthy   ok etcd-0              
    Healthy   {"health": "true"}`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get componentstatuses` `NAME                 STATUS    MESSAGE             
    ERROR scheduler            Healthy   ok controller-manager   Healthy   ok etcd-0              
    Healthy   {"health": "true"}`'
- en: '|  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: How these components communicate
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件的通信方式
- en: Kubernetes system components communicate only with the API server. They don’t
    talk to each other directly. The API server is the only component that communicates
    with etcd. None of the other components communicate with etcd directly, but instead
    modify the cluster state by talking to the API server.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes系统组件仅与API服务器通信。它们不会直接相互交谈。API服务器是唯一与etcd通信的组件。其他组件不会直接与etcd通信，而是通过与API服务器交谈来修改集群状态。
- en: Connections between the API server and the other components are almost always
    initiated by the components, as shown in [figure 11.1](#filepos1040687). But the
    API server does connect to the Kubelet when you use `kubectl` to fetch logs, use
    `kubectl attach` to connect to a running container, or use the `kubectl port-forward`
    command.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器与其他组件之间的连接几乎总是由组件发起的，如图11.1所示。[figure 11.1](#filepos1040687)。但当你使用`kubectl`获取日志、使用`kubectl
    attach`连接到正在运行的容器或使用`kubectl port-forward`命令时，API服务器会连接到Kubelet。
- en: '|  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `kubectl attach` command is similar to `kubectl exec`, but it attaches to
    the main process running in the container instead of running an additional one.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl attach`命令与`kubectl exec`类似，但它连接到容器中运行的主进程，而不是运行额外的进程。'
- en: '|  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Running multiple instances of individual components
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单个组件的多个实例
- en: Although the components on the worker nodes all need to run on the same node,
    the components of the Control Plane can easily be split across multiple servers.
    There can be more than one instance of each Control Plane component running to
    ensure high availability. While multiple instances of etcd and API server can
    be active at the same time and do perform their jobs in parallel, only a single
    instance of the Scheduler and the Controller Manager may be active at a given
    time—with the others in standby mode.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然工作节点上的组件都需要在同一个节点上运行，但控制平面的组件可以轻松地跨多个服务器分割。可以有多个控制平面组件的实例运行，以确保高可用性。虽然etcd和API服务器的多个实例可以同时激活并并行执行其工作，但调度器和控制器管理器在任何给定时间可能只有一个实例处于活动状态，其他则处于待机模式。
- en: How components are run
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 组件的运行方式
- en: The Control Plane components, as well as kube-proxy, can either be deployed
    on the system directly or they can run as pods (as shown in [listing 11.1](#filepos1044794)).
    You may be surprised to hear this, but it will all make sense later when we talk
    about the Kubelet.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面组件以及kube-proxy可以直接部署在系统上，或者作为pods运行（如图11.1所示。[listing 11.1](#filepos1044794)）。您可能会对此感到惊讶，但当我们谈到Kubelet时，这一切都会变得有意义。
- en: The Kubelet is the only component that always runs as a regular system component,
    and it’s the Kubelet that then runs all the other components as pods. To run the
    Control Plane components as pods, the Kubelet is also deployed on the master.
    The next listing shows pods in the `kube-system` namespace in a cluster created
    with `kubeadm`, which is explained in [appendix B](index_split_138.html#filepos1737471).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet是唯一始终以常规系统组件形式运行的组件，它通过运行其他组件作为pods来运行。要将控制平面组件作为pods运行，Kubelet也部署在主节点上。下面的列表显示了使用`kubeadm`创建的集群中`kube-system`命名空间下的pods，这将在附录B中解释。[appendix
    B](index_split_138.html#filepos1737471)。
- en: Listing 11.1\. Kubernetes components running as pods
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.1\. Kubernetes以pods形式运行的组件
- en: '`$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName`![](images/00006.jpg)`--sort-by
    spec.nodeName -n kube-system` `POD                              NODE kube-controller-manager-master  
    master` `1` `kube-dns-2334855451-37d9k        master` `1` `etcd-master                     
    master` `1` `kube-apiserver-master            master` `1` `kube-scheduler-master           
    master` `1` `kube-flannel-ds-tgj9k            node1` `2` `kube-proxy-ny3xm                
    node1` `2` `kube-flannel-ds-0eek8            node2` `2` `kube-proxy-sp362                
    node2` `2` `kube-flannel-ds-r5yf4            node3` `2` `kube-proxy-og9ac                
    node3` `2`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName`![](images/00006.jpg)`--sort-by
    spec.nodeName -n kube-system` `POD                              NODE kube-controller-manager-master  
    master` `1` `kube-dns-2334855451-37d9k        master` `1` `etcd-master                     
    master` `1` `kube-apiserver-master            master` `1` `kube-scheduler-master           
    master` `1` `kube-flannel-ds-tgj9k            node1` `2` `kube-proxy-ny3xm                
    node1` `2` `kube-flannel-ds-0eek8            node2` `2` `kube-proxy-sp362                
    node2` `2` `kube-flannel-ds-r5yf4            node3` `2` `kube-proxy-og9ac                
    node3` `2`'
- en: 1 etcd, API server, Scheduler, Controller Manager, and the DNS server are running
    on the master.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 etcd、API服务器、调度器、控制器管理器和DNS服务器都在主节点上运行。
- en: 2 The three nodes each run a Kube Proxy pod and a Flannel networking pod.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 这三个节点各自运行一个Kube Proxy pod和一个Flannel网络pod。
- en: As you can see in the listing, all the Control Plane components are running
    as pods on the master node. There are three worker nodes, and each one runs the
    kube-proxy and a Flannel pod, which provides the overlay network for the pods
    (we’ll talk about Flannel later).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表所示，所有控制平面组件都在主节点上作为pod运行。有三个工作节点，每个节点运行一个kube-proxy和一个Flannel网络pod，为pod提供覆盖网络（我们稍后会讨论Flannel）。
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: As shown in the listing, you can tell `kubectl` to display custom columns with
    the `-o custom-columns` option and sort the resource list with `--sort-by`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表所示，您可以使用`-o custom-columns`选项告诉`kubectl`显示自定义列，并使用`--sort-by`对资源列表进行排序。
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now, let’s look at each of the components up close, starting with the lowest
    level component of the Control Plane—the persistent storage.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐一近距离观察每个组件，从控制平面的最低级组件——持久存储开始。
- en: 11.1.2\. How Kubernetes uses etcd
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.2\. Kubernetes如何使用etcd
- en: All the objects you’ve created throughout this book—Pods, ReplicationControllers,
    Services, Secrets, and so on—need to be stored somewhere in a persistent manner
    so their manifests survive API server restarts and failures. For this, Kubernetes
    uses etcd, which is a fast, distributed, and consistent key-value store. Because
    it’s distributed, you can run more than one etcd instance to provide both high
    availability and better performance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书的整个过程中创建的所有对象——Pods、ReplicationControllers、Services、Secrets等等——都需要以持久的方式存储在某处，以便它们的manifest在API服务器重启和故障中幸存。为此，Kubernetes使用etcd，它是一个快速、分布式且一致性的键值存储。由于它是分布式的，您可以运行多个etcd实例，以提供高可用性和更好的性能。
- en: The only component that talks to etcd directly is the Kubernetes API server.
    All other components read and write data to etcd indirectly through the API server.
    This brings a few benefits, among them a more robust optimistic locking system
    as well as validation; and, by abstracting away the actual storage mechanism from
    all the other components, it’s much simpler to replace it in the future. It’s
    worth emphasizing that etcd is the only place Kubernetes stores cluster state
    and metadata.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与etcd直接通信的唯一组件是Kubernetes API服务器。所有其他组件都通过API服务器间接读取和写入数据到etcd。这带来了一些好处，其中包括更健壮的乐观锁系统以及验证；并且，通过将实际的存储机制从所有其他组件中抽象出来，未来替换它要简单得多。值得强调的是，etcd是Kubernetes存储集群状态和元数据的唯一位置。
- en: '|  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: About optimistic concurrency control
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于乐观并发控制
- en: Optimistic concurrency control (sometimes referred to as optimistic locking)
    is a method where instead of locking a piece of data and preventing it from being
    read or updated while the lock is in place, the piece of data includes a version
    number. Every time the data is updated, the version number increases. When updating
    the data, the version number is checked to see if it has increased between the
    time the client read the data and the time it submits the update. If this happens,
    the update is rejected and the client must re-read the new data and try to update
    it again.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 乐观并发控制（有时称为乐观锁定）是一种方法，在这种方法中，不是锁定数据块并防止在锁定期间读取或更新数据，而是在数据块中包含一个版本号。每次更新数据时，版本号都会增加。在更新数据时，会检查版本号是否在客户端读取数据和提交更新之间增加。如果发生这种情况，更新将被拒绝，客户端必须重新读取新数据并再次尝试更新。
- en: The result is that when two clients try to update the same data entry, only
    the first one succeeds.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，当两个客户端尝试更新相同的数据条目时，只有第一个客户端会成功。
- en: All Kubernetes resources include a `metadata.resourceVersion` field, which clients
    need to pass back to the API server when updating an object. If the version doesn’t
    match the one stored in etcd, the API server rejects the update.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Kubernetes 资源都包含一个 `metadata.resourceVersion` 字段，客户端在更新对象时需要将其传递回 API 服务器。如果版本与
    etcd 中存储的版本不匹配，API 服务器将拒绝更新。
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: How resources are stored in etcd
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 资源在 etcd 中的存储方式
- en: As I’m writing this, Kubernetes can use either etcd version 2 or version 3,
    but version 3 is now recommended because of improved performance. etcd v2 stores
    keys in a hierarchical key space, which makes key-value pairs similar to files
    in a file system. Each key in etcd is either a directory, which contains other
    keys, or is a regular key with a corresponding value. etcd v3 doesn’t support
    directories, but because the key format remains the same (keys can include slashes),
    you can still think of them as being grouped into directories. Kubernetes stores
    all its data in etcd under /registry. The following listing shows a list of keys
    stored under /registry.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我写这篇文章时，Kubernetes 可以使用 etcd 的版本 2 或版本 3，但现在推荐使用版本 3，因为其性能得到了提升。etcd v2 在分层键空间中存储键，这使得键值对类似于文件系统中的文件。etcd
    中的每个键要么是一个目录，包含其他键，要么是一个具有相应值的常规键。etcd v3 不支持目录，但由于键格式保持不变（键可以包含斜杠），你仍然可以将它们视为分组到目录中。Kubernetes
    将其所有数据存储在 `/registry` 下的 etcd 中。以下列表显示了 `/registry` 下存储的键列表。
- en: Listing 11.2\. Top-level entries stored in etcd by Kubernetes
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2\. Kubernetes 在 etcd 中存储的顶级条目
- en: '`$ etcdctl ls /registry` `/registry/configmaps /registry/daemonsets /registry/deployments
    /registry/events /registry/namespaces /registry/pods ...`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ etcdctl ls /registry` `/registry/configmaps /registry/daemonsets /registry/deployments
    /registry/events /registry/namespaces /registry/pods ...`'
- en: You’ll recognize that these keys correspond to the resource types you learned
    about in the previous chapters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你会认出这些键对应于你在前几章中学到的资源类型。
- en: '|  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re using v3 of the etcd API, you can’t use the `ls` command to see the
    contents of a directory. Instead, you can list all keys that start with a given
    prefix with `etcdctl get /registry --prefix=true`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 etcd API 的 v3 版本，你不能使用 `ls` 命令来查看目录的内容。相反，你可以使用 `etcdctl get /registry
    --prefix=true` 来列出所有以给定前缀开始的键。
- en: '|  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The following listing shows the contents of the /registry/pods directory.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了 `/registry/pods` 目录的内容。
- en: Listing 11.3\. Keys in the /registry/pods directory
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.3\. `/registry/pods` 目录中的键
- en: '`$ etcdctl ls /registry/pods` `/registry/pods/default /registry/pods/kube-system`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ etcdctl ls /registry/pods` `/registry/pods/default /registry/pods/kube-system`'
- en: As you can infer from the names, these two entries correspond to the `default`
    and the `kube-system` namespaces, which means pods are stored per namespace. The
    following listing shows the entries in the /registry/pods/default directory.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从名称可以推断，这两个条目对应于 `default` 和 `kube-system` 命名空间，这意味着每个命名空间存储了不同的 pod。以下列表显示了
    `/registry/pods/default` 目录中的条目。
- en: Listing 11.4\. etcd entries for pods in the `default` namespace
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.4\. `default` 命名空间中 pod 的 etcd 条目
- en: '`$ etcdctl ls /registry/pods/default` `/registry/pods/default/kubia-159041347-xk0vc
    /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ etcdctl ls /registry/pods/default` `/registry/pods/default/kubia-159041347-xk0vc
    /registry/pods/default/kubia-159041347-wt6ga /registry/pods/default/kubia-159041347-hp2o5`'
- en: Each entry corresponds to an individual pod. These aren’t directories, but key-value
    entries. The following listing shows what’s stored in one of them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个条目对应一个单独的 pod。这些不是目录，而是键值条目。以下列表显示了其中一个条目中存储的内容。
- en: Listing 11.5\. An etcd entry representing a pod
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.5\. 代表Pod的etcd条目
- en: '`$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga` `{"kind":"Pod","apiVersion":"v1","metadata":{"name":"kubia-159041347-wt6ga",
    "generateName":"kubia-159041347-","namespace":"default","selfLink":...`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga` `{"kind":"Pod","apiVersion":"v1","metadata":{"name":"kubia-159041347-wt6ga",
    "generateName":"kubia-159041347-","namespace":"default","selfLink":...`'
- en: You’ll recognize that this is nothing other than a pod definition in JSON format.
    The API server stores the complete JSON representation of a resource in etcd.
    Because of etcd’s hierarchical key space, you can think of all the stored resources
    as JSON files in a filesystem. Simple, right?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你会认出这不过是一个JSON格式的Pod定义。API服务器将资源的完整JSON表示存储在etcd中。由于etcd的分层键空间，你可以将所有存储的资源视为文件系统中的JSON文件。简单，对吧？
- en: '|  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: Prior to Kubernetes version 1.7, the JSON manifest of a `Secret` resource was
    also stored like this (it wasn’t encrypted). If someone got direct access to etcd,
    they knew all your Secrets. From version 1.7, Secrets are encrypted and thus stored
    much more securely.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes版本1.7之前，`Secret`资源的JSON清单也是这样存储的（它没有被加密）。如果有人直接访问了etcd，他们就能知道你所有的机密信息。从版本1.7开始，Secrets被加密，因此存储得更加安全。
- en: '|  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Ensuring the consistency and validity of stored objects
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 确保存储对象的致性和有效性
- en: Remember Google’s Borg and Omega systems mentioned in [chapter 1](index_split_017.html#filepos122588),
    which are what Kubernetes is based on? Like Kubernetes, Omega also uses a centralized
    store to hold the state of the cluster, but in contrast, multiple Control Plane
    components access the store directly. All these components need to make sure they
    all adhere to the same optimistic locking mechanism to handle conflicts properly.
    A single component not adhering fully to the mechanism may lead to inconsistent
    data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在[第1章](index_split_017.html#filepos122588)中提到的Google的Borg和Omega系统吗？它们是Kubernetes的基础？就像Kubernetes一样，Omega也使用一个集中存储来保存集群的状态，但与之不同的是，多个控制平面组件直接访问存储。所有这些组件都需要确保它们都遵循相同的乐观锁机制来正确处理冲突。如果一个组件没有完全遵循该机制，可能会导致数据不一致。
- en: Kubernetes improves this by requiring all other Control Plane components to
    go through the API server. This way updates to the cluster state are always consistent,
    because the optimistic locking mechanism is implemented in a single place, so
    less chance exists, if any, of error. The API server also makes sure that the
    data written to the store is always valid and that changes to the data are only
    performed by authorized clients.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes通过要求所有其他控制平面组件通过API服务器来改进这一点。这样，集群状态更新总是保持一致，因为乐观锁机制只在单一位置实现，因此错误的可能性更小。API服务器还确保写入存储的数据始终有效，并且数据的变化只能由授权客户端执行。
- en: Ensuring consistency when etcd is clustered
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当etcd集群化时确保一致性
- en: For ensuring high availability, you’ll usually run more than a single instance
    of etcd. Multiple etcd instances will need to remain consistent. Such a distributed
    system needs to reach a consensus on what the actual state is. etcd uses the RAFT
    consensus algorithm to achieve this, which ensures that at any given moment, each
    node’s state is either what the majority of the nodes agrees is the current state
    or is one of the previously agreed upon states.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保高可用性，你通常会运行多个etcd实例。多个etcd实例需要保持一致性。这样的分布式系统需要就实际状态达成共识。etcd使用RAFT共识算法来实现这一点，这确保了在任何给定时刻，每个节点的状态要么是大多数节点同意的当前状态，要么是之前达成共识的状态之一。
- en: Clients connecting to different nodes of an etcd cluster will either see the
    actual current state or one of the states from the past (in Kubernetes, the only
    etcd client is the API server, but there may be multiple instances).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到etcd集群不同节点的客户端将看到实际当前状态或过去的状态之一（在Kubernetes中，唯一的etcd客户端是API服务器，但可能有多个实例）。
- en: The consensus algorithm requires a majority (or quorum) for the cluster to progress
    to the next state. As a result, if the cluster splits into two disconnected groups
    of nodes, the state in the two groups can never diverge, because to transition
    from the previous state to the new one, there needs to be more than half of the
    nodes taking part in the state change. If one group contains the majority of all
    nodes, the other one obviously doesn’t. The first group can modify the cluster
    state, whereas the other one can’t. When the two groups reconnect, the second
    group can catch up with the state in the first group (see [figure 11.2](#filepos1057067)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性算法要求集群拥有多数（或法定多数）才能进步到下一个状态。因此，如果集群分裂成两个不连接的节点组，两组的状态永远不会分歧，因为要从上一个状态过渡到新状态，需要超过一半的节点参与状态变化。如果一个组包含所有节点的多数，另一个组显然不包含。第一个组可以修改集群状态，而另一个组不能。当两个组重新连接时，第二个组可以赶上第一个组的状态（参见[图11.2](#filepos1057067))。
- en: Figure 11.2\. In a split-brain scenario, only the side which still has the majority
    (quorum) accepts state changes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2。在脑裂场景中，只有仍然拥有多数（法定多数）的一侧才接受状态变化。
- en: '![](images/00009.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00009.jpg)'
- en: Why the number of etcd instances should be an odd numbers
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么etcd实例的数量应该是奇数
- en: etcd is usually deployed with an odd number of instances. I’m sure you’d like
    to know why. Let’s compare having two vs. having one instance. Having two instances
    requires both instances to be present to have a majority. If either of them fails,
    the etcd cluster can’t transition to a new state because no majority exists. Having
    two instances is worse than having only a single instance. By having two, the
    chance of the whole cluster failing has increased by 100%, compared to that of
    a single-node cluster failing.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: etcd通常以奇数个实例部署。我相信你一定想知道为什么。让我们比较有两个实例和有一个实例的情况。有两个实例需要两个实例都存在才能拥有多数。如果其中任何一个失败，由于没有多数存在，etcd集群无法过渡到新状态。有两个实例比只有一个实例更糟糕。通过有两个实例，整个集群失败的可能性比单节点集群失败的可能性增加了100%。
- en: The same applies when comparing three vs. four etcd instances. With three instances,
    one instance can fail and a majority (of two) still exists. With four instances,
    you need three nodes for a majority (two aren’t enough). In both three- and four-instance
    clusters, only a single instance may fail. But when running four instances, if
    one fails, a higher possibility exists of an additional instance of the three
    remaining instances failing (compared to a three-node cluster with one failed
    node and two remaining nodes).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较三个实例和四个实例时，情况也相同。有三个实例时，一个实例可以失败，而多数（两个）仍然存在。有四个实例时，需要三个节点才能拥有多数（两个不够）。在三个和四个实例的集群中，只有一个实例可能失败。但运行四个实例时，如果其中一个实例失败，剩余三个实例中另一个实例失败的可能性更高（与一个失败节点和两个剩余节点的三个节点集群相比）。
- en: Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient.
    It can handle a two- or a three-node failure, respectively, which suffices in
    almost all situations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，对于大型集群，五个或七个节点的etcd集群就足够了。它可以分别处理两个或三个节点的故障，这在几乎所有情况下都足够了。
- en: 11.1.3\. What the API server does
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.3. API服务器的作用
- en: The Kubernetes API server is the central component used by all other components
    and by clients, such as `kubectl`. It provides a CRUD (Create, Read, Update, Delete)
    interface for querying and modifying the cluster state over a RESTful API. It
    stores that state in etcd.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API服务器是所有其他组件以及客户端（如`kubectl`）使用的中心组件。它通过RESTful API提供了一个CRUD（创建、读取、更新、删除）接口，用于查询和修改集群状态。它将这种状态存储在etcd中。
- en: In addition to providing a consistent way of storing objects in etcd, it also
    performs validation of those objects, so clients can’t store improperly configured
    objects (which they could if they were writing to the store directly). Along with
    validation, it also handles optimistic locking, so changes to an object are never
    overridden by other clients in the event of concurrent updates.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在etcd中提供一致的方式存储对象外，它还执行这些对象的验证，因此客户端不能存储配置不当的对象（如果它们直接写入存储，它们可以这样做）。除了验证外，它还处理乐观锁，因此对象的变化在并发更新的情况下永远不会被其他客户端覆盖。
- en: One of the API server’s clients is the command-line tool `kubectl` you’ve been
    using from the beginning of the book. When creating a resource from a JSON file,
    for example, `kubectl` posts the file’s contents to the API server through an
    HTTP POST request. [Figure 11.3](#filepos1060090) shows what happens inside the
    API server when it receives the request. This is explained in more detail in the
    next few paragraphs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器的一个客户端是从本书开始就一直在使用的命令行工具`kubectl`。例如，从JSON文件创建资源时，`kubectl`通过HTTP POST请求将文件内容发送到API服务器。[图11.3](#filepos1060090)显示了API服务器在收到请求时内部发生的事情。这将在接下来的几段中详细解释。
- en: Figure 11.3\. The operation of the API server
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3\. API服务器的操作
- en: '![](images/00027.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00027.jpg)'
- en: Authenticating the client with authentication plugins
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用认证插件认证客户端
- en: First, the API server needs to authenticate the client sending the request.
    This is performed by one or more authentication plugins configured in the API
    server. The API server calls these plugins in turn, until one of them determines
    who is sending the request. It does this by inspecting the HTTP request.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，API服务器需要验证发送请求的客户端。这是通过API服务器中配置的一个或多个认证插件完成的。API服务器依次调用这些插件，直到其中一个确定谁在发送请求。它是通过检查HTTP请求来做到这一点的。
- en: Depending on the authentication method, the user can be extracted from the client’s
    certificate or an HTTP header, such as `Authorization`, which you used in [chapter
    8](index_split_070.html#filepos790863). The plugin extracts the client’s username,
    user ID, and groups the user belongs to. This data is then used in the next stage,
    which is authorization.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据认证方法，用户可以从客户端证书或HTTP头（例如`Authorization`，您在[第8章](index_split_070.html#filepos790863)中使用过）中提取出来。插件提取客户端的用户名、用户ID以及用户所属的组。然后，这些数据在下一阶段使用，即授权阶段。
- en: Authorizing the client with authorization plugins
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用授权插件授权客户端
- en: Besides authentication plugins, the API server is also configured to use one
    or more authorization plugins. Their job is to determine whether the authenticated
    user can perform the requested action on the requested resource. For example,
    when creating pods, the API server consults all authorization plugins in turn,
    to determine whether the user can create pods in the requested namespace. As soon
    as a plugin says the user can perform the action, the API server progresses to
    the next stage.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 除了认证插件外，API服务器还配置了使用一个或多个授权插件。它们的工作是确定经过认证的用户是否可以在请求的资源上执行请求的操作。例如，在创建Pod时，API服务器依次咨询所有授权插件，以确定用户是否可以在请求的命名空间中创建Pod。一旦某个插件表示用户可以执行该操作，API服务器就进入下一阶段。
- en: Validating and/or Modifying the resource in the request with admi- ission control
    plugins
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用准入控制插件验证和/或修改请求中的资源
- en: If the request is trying to create, modify, or delete a resource, the request
    is sent through Admission Control. Again, the server is configured with multiple
    Admission Control plugins. These plugins can modify the resource for different
    reasons. They may initialize fields missing from the resource specification to
    the configured default values or even override them. They may even modify other
    related resources, which aren’t in the request, and can also reject a request
    for whatever reason. The resource passes through all Admission Control plugins.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果请求尝试创建、修改或删除资源，请求将通过准入控制。同样，服务器配置了多个准入控制插件。这些插件可以出于不同原因修改资源。它们可能将资源规范中缺失的字段初始化为配置的默认值，甚至覆盖它们。它们甚至可以修改其他相关资源，这些资源不在请求中，并且也可以根据任何原因拒绝请求。资源将通过所有准入控制插件。
- en: '|  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When the request is only trying to read data, the request doesn’t go through
    the Admission Control.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当请求仅尝试读取数据时，请求不会通过准入控制。
- en: '|  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Examples of Admission Control plugins include
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 准入控制插件的例子包括
- en: '`AlwaysPullImages`—Overrides the pod’s `imagePullPolicy` to `Always`, forcing
    the image to be pulled every time the pod is deployed.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AlwaysPullImages`—覆盖Pod的`imagePullPolicy`为`Always`，强制每次部署Pod时都拉取镜像。'
- en: '`ServiceAccount`—Applies the default service account to pods that don’t specify
    it explicitly.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServiceAccount`—将默认服务账户应用于未明确指定的Pod。'
- en: '`NamespaceLifecycle`—Prevents creation of pods in namespaces that are in the
    process of being deleted, as well as in non-existing namespaces.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NamespaceLifecycle`—防止在正在被删除的命名空间以及不存在命名空间中创建Pod。'
- en: '`ResourceQuota`—Ensures pods in a certain namespace only use as much CPU and
    memory as has been allotted to the namespace. We’ll learn more about this in [chapter
    14](index_split_105.html#filepos1325290).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ResourceQuota`—确保特定命名空间中的Pod只使用分配给该命名空间的CPU和内存。我们将在第14章中了解更多关于这个内容。[第14章](index_split_105.html#filepos1325290)。'
- en: You’ll find a list of additional Admission Control plugins in the Kubernetes
    documentation at [https://kubernetes.io/docs/admin/admission-controllers/](https://kubernetes.io/docs/admin/admission-controllers/).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Kubernetes文档中找到额外的Admission Control插件的列表，网址为[https://kubernetes.io/docs/admin/admission-controllers/](https://kubernetes.io/docs/admin/admission-controllers/)。
- en: Validating the resource and storing it persistently
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 验证资源并持久存储
- en: After letting the request pass through all the Admission Control plugins, the
    API server then validates the object, stores it in etcd, and returns a response
    to the client.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在让请求通过所有Admission Control插件之后，API服务器随后验证对象，将其存储在etcd中，并向客户端返回响应。
- en: 11.1.4\. Understanding how the API server notifies clients of resource changes
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.4\. 理解API服务器如何通知客户端资源变化
- en: The API server doesn’t do anything else except what we’ve discussed. For example,
    it doesn’t create pods when you create a ReplicaSet resource and it doesn’t manage
    the endpoints of a service. That’s what controllers in the Controller Manager
    do.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器除了我们讨论的内容外，不做任何事情。例如，当你创建ReplicaSet资源时，它不会创建Pod，它也不会管理服务的端点。这是Controller
    Manager中的控制器所做的事情。
- en: But the API server doesn’t even tell these controllers what to do. All it does
    is enable those controllers and other components to observe changes to deployed
    resources. A Control Plane component can request to be notified when a resource
    is created, modified, or deleted. This enables the component to perform whatever
    task it needs in response to a change of the cluster metadata.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但API服务器甚至不会告诉这些控制器要做什么。它所做的只是使这些控制器和其他组件能够观察已部署资源的更改。控制平面组件可以请求在资源被创建、修改或删除时被通知。这使得组件能够在集群元数据发生变化时执行所需的任何任务。
- en: Clients watch for changes by opening an HTTP connection to the API server. Through
    this connection, the client will then receive a stream of modifications to the
    watched objects. Every time an object is updated, the server sends the new version
    of the object to all connected clients watching the object. [Figure 11.4](#filepos1065747)
    shows how clients can watch for changes to pods and how a change to one of the
    pods is stored into etcd and then relayed to all clients watching pods at that
    moment.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端通过打开到API服务器的HTTP连接来监视变化。通过这个连接，客户端将接收对监视对象的修改流。每当一个对象被更新时，服务器会将对象的最新版本发送给所有连接的、正在监视该对象的客户端。[图11.4](#filepos1065747)展示了客户端如何监视Pod的变化，以及当一个Pod发生变化时，它是如何存储到etcd中，然后传递给当时所有监视该Pod的客户端的。
- en: Figure 11.4\. When an object is updated, the API server sends the updated object
    to all interested watchers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4\. 当一个对象被更新时，API服务器将更新后的对象发送给所有感兴趣的监视者。
- en: '![](images/00048.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00048.jpg)'
- en: One of the API server’s clients is the `kubectl` tool, which also supports watching
    resources. For example, when deploying a pod, you don’t need to constantly poll
    the list of pods by repeatedly executing `kubectl get pods`. Instead, you can
    use the `--watch` flag and be notified of each creation, modification, or deletion
    of a pod, as shown in the following listing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: API服务器的一个客户端是`kubectl`工具，它也支持监视资源。例如，在部署Pod时，你不需要通过反复执行`kubectl get pods`来不断轮询Pod列表。相反，你可以使用`--watch`标志，并在Pod的创建、修改或删除时收到通知，如下面的列表所示。
- en: Listing 11.6\. Watching a pod being created and then deleted
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.6\. 监视Pod的创建和删除
- en: '`$ kubectl get pods --watch` `NAME                    READY     STATUS             
    RESTARTS   AGE kubia-159041347-14j3i   0/1       Pending             0         
    0s kubia-159041347-14j3i   0/1       Pending             0          0s kubia-159041347-14j3i  
    0/1       ContainerCreating   0          1s kubia-159041347-14j3i   0/1      
    Running             0          3s kubia-159041347-14j3i   1/1       Running            
    0          5s kubia-159041347-14j3i   1/1       Terminating         0         
    9s kubia-159041347-14j3i   0/1       Terminating         0          17s kubia-159041347-14j3i  
    0/1       Terminating         0          17s kubia-159041347-14j3i   0/1      
    Terminating         0          17s`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pods --watch` `名称                    就绪     状态             
    重启次数   年龄 kubia-159041347-14j3i   0/1       待定             0          0s kubia-159041347-14j3i  
    0/1       待定             0          0s kubia-159041347-14j3i   0/1       容器创建中  
    0          1s kubia-159041347-14j3i   0/1       运行中             0          3s
    kubia-159041347-14j3i   1/1       运行中             0          5s kubia-159041347-14j3i  
    1/1       终止中         0          9s kubia-159041347-14j3i   0/1       终止中        
    0          17s kubia-159041347-14j3i   0/1       终止中         0          17s kubia-159041347-14j3i  
    0/1       终止中         0          17s`'
- en: 'You can even have `kubectl` print out the whole YAML on each watch event like
    this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以让 `kubectl` 在每个监视事件上打印出整个 YAML，如下所示：
- en: '`$ kubectl get pods -o yaml --watch`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pods -o yaml --watch`'
- en: The watch mechanism is also used by the Scheduler, which is the next Control
    Plane component you’re going to learn more about.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 监视机制也被调度器使用，这是你接下来将要了解更多信息的下一个控制平面组件。
- en: 11.1.5\. Understanding the Scheduler
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.5\. 理解调度器
- en: You’ve already learned that you don’t usually specify which cluster node a pod
    should run on. This is left to the Scheduler. From afar, the operation of the
    Scheduler looks simple. All it does is wait for newly created pods through the
    API server’s watch mechanism and assign a node to each new pod that doesn’t already
    have the node set.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到，通常不指定 pod 应该运行在哪个集群节点上。这由调度器来决定。从远处看，调度器的操作看起来很简单。它所做的只是通过 API 服务器的监视机制等待新创建的
    pod，并为每个尚未设置节点的新的 pod 分配一个节点。
- en: The Scheduler doesn’t instruct the selected node (or the Kubelet running on
    that node) to run the pod. All the Scheduler does is update the pod definition
    through the API server. The API server then notifies the Kubelet (again, through
    the watch mechanism described previously) that the pod has been scheduled. As
    soon as the Kubelet on the target node sees the pod has been scheduled to its
    node, it creates and runs the pod’s containers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器不会指示所选节点（或在该节点上运行的 Kubelet）运行 pod。调度器所做的所有事情只是通过 API 服务器更新 pod 定义。然后 API
    服务器通知 Kubelet（再次，通过之前描述的监视机制）pod 已被调度。一旦目标节点上的 Kubelet 看到pod已被调度到其节点，它就会创建并运行
    pod 的容器。
- en: Although a coarse-grained view of the scheduling process seems trivial, the
    actual task of selecting the best node for the pod isn’t that simple. Sure, the
    simplest Scheduler could pick a random node and not care about the pods already
    running on that node. On the other side of the spectrum, the Scheduler could use
    advanced techniques such as machine learning to anticipate what kind of pods are
    about to be scheduled in the next minutes or hours and schedule pods to maximize
    future hardware utilization without requiring any rescheduling of existing pods.
    Kubernetes’ default Scheduler falls somewhere in between.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对调度过程的粗略视图看起来很简单，但实际选择最佳节点以供 pod 使用的工作并不简单。当然，最简单的调度器可能会随机选择一个节点，而不关心该节点上已经运行的
    pod。在光谱的另一端，调度器可以使用诸如机器学习等高级技术来预测在接下来的几分钟或几小时内将要调度的 pod 类型，并将 pod 调度到最大化未来硬件利用率，而无需重新调度现有的
    pod。Kubernetes 的默认调度器介于两者之间。
- en: Understanding the default scheduling algorithm
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 理解默认调度算法
- en: 'The selection of a node can be broken down into two parts, as shown in [figure
    11.5](#filepos1070314):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的选择可以分为两个部分，如图 11.5 所示：
- en: Filtering the list of all nodes to obtain a list of acceptable nodes the pod
    can be scheduled to.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤所有节点列表以获得一个可接受的节点列表，该节点列表可以调度 pod。
- en: Prioritizing the acceptable nodes and choosing the best one. If multiple nodes
    have the highest score, round-robin is used to ensure pods are deployed across
    all of them evenly.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先选择可接受的节点并选择最佳节点。如果有多个节点具有最高分数，则使用轮询确保 pod 均匀地部署在所有这些节点上。
- en: Figure 11.5\. The Scheduler finds acceptable nodes for a pod and then selects
    the best node for the pod.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5\. 调度器为 pod 找到可接受的节点，然后选择最佳的节点。
- en: '![](images/00067.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00067.jpg)'
- en: Finding acceptable nodes
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找可接受的节点
- en: To determine which nodes are acceptable for the pod, the Scheduler passes each
    node through a list of configured predicate functions. These check various things
    such as
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定哪些节点适合Pod，调度器将每个节点通过一系列配置的谓词函数。这些检查各种事情，例如
- en: Can the node fulfill the pod’s requests for hardware resources? You’ll learn
    how to specify them in [chapter 14](index_split_105.html#filepos1325290).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点能否满足Pod对硬件资源的需求？你将在[第14章](index_split_105.html#filepos1325290)中学习如何指定它们。
- en: Is the node running out of resources (is it reporting a memory or a disk pressure
    condition)?
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点是否资源耗尽（是否报告内存或磁盘压力条件）？
- en: If the pod requests to be scheduled to a specific node (by name), is this the
    node?
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Pod请求被调度到特定的节点（按名称），这是否是这个节点？
- en: Does the node have a label that matches the node selector in the pod specification
    (if one is defined)?
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点是否有标签与Pod规范中的节点选择器匹配（如果已定义）？
- en: If the pod requests to be bound to a specific host port (discussed in [chapter
    13](index_split_099.html#filepos1232167)), is that port already taken on this
    node or not?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Pod请求绑定到特定的主机端口（在第13章中讨论，index_split_099.html#filepos1232167），这个端口是否已经被这个节点占用？
- en: If the pod requests a certain type of volume, can this volume be mounted for
    this pod on this node, or is another pod on the node already using the same volume?
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Pod请求某种类型的卷，这个卷是否可以在这个节点上为这个Pod挂载，或者节点上是否已经有其他Pod正在使用相同的卷？
- en: Does the pod tolerate the taints of the node? Taints and tolerations are explained
    in [chapter 16](index_split_118.html#filepos1486732).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod是否容忍节点的污点？污点和容忍在第16章中解释（index_split_118.html#filepos1486732）。
- en: Does the pod specify node and/or pod affinity or anti-affinity rules? If yes,
    would scheduling the pod to this node break those rules? This is also explained
    in [chapter 16](index_split_118.html#filepos1486732).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod是否指定了节点和/或亲和性或反亲和性规则？如果是，将Pod调度到这个节点是否会违反这些规则？这也在第16章中解释（index_split_118.html#filepos1486732）。
- en: All these checks must pass for the node to be eligible to host the pod. After
    performing these checks on every node, the Scheduler ends up with a subset of
    the nodes. Any of these nodes could run the pod, because they have enough available
    resources for the pod and conform to all requirements you’ve specified in the
    pod definition.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些检查都必须通过，节点才有资格托管Pod。在每个节点上执行这些检查后，调度器最终会得到一个节点子集。这些节点中的任何一个都可以运行Pod，因为它们有足够的可用资源来运行Pod，并且符合你在Pod定义中指定的所有要求。
- en: Selecting the best node for the pod
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的Pod节点
- en: Even though all these nodes are acceptable and can run the pod, several may
    be a better choice than others. Suppose you have a two-node cluster. Both nodes
    are eligible, but one is already running 10 pods, while the other, for whatever
    reason, isn’t running any pods right now. It’s obvious the Scheduler should favor
    the second node in this case.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有这些节点都是可接受的并且可以运行Pod，但其中一些可能比其他节点更好。假设你有一个双节点集群。两个节点都是合格的，但其中一个节点已经运行了10个Pod，而另一个节点，由于某种原因，目前没有运行任何Pod。在这种情况下，显然调度器应该优先考虑第二个节点。
- en: Or is it? If these two nodes are provided by the cloud infrastructure, it may
    be better to schedule the pod to the first node and relinquish the second node
    back to the cloud provider to save money.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 或者是吗？如果这两个节点由云基础设施提供，那么将Pod调度到第一个节点并将第二个节点归还给云服务提供商以节省资金可能更好。
- en: Advanced scheduling of pods
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Pod的高级调度
- en: Consider another example. Imagine having multiple replicas of a pod. Ideally,
    you’d want them spread across as many nodes as possible instead of having them
    all scheduled to a single one. Failure of that node would cause the service backed
    by those pods to become unavailable. But if the pods were spread across different
    nodes, a single node failure would barely leave a dent in the service’s capacity.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑另一个例子。想象一下有多个Pod副本。理想情况下，你希望它们尽可能分散在多个节点上，而不是全部调度到单个节点上。该节点的故障会导致由这些Pod支持的服务不可用。但如果Pod分散在不同的节点上，单个节点的故障几乎不会对服务的容量造成影响。
- en: Pods belonging to the same Service or ReplicaSet are spread across multiple
    nodes by default. It’s not guaranteed that this is always the case. But you can
    force pods to be spread around the cluster or kept close together by defining
    pod affinity and anti-affinity rules, which are explained in [chapter 16](index_split_118.html#filepos1486732).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，属于同一Service或ReplicaSet的Pod会分散在多个节点上。但这并不保证这种情况总是成立。但你可以通过定义Pod亲和性和反亲和性规则来强制Pod在集群中分散或保持紧密，这些规则在第16章中解释（index_split_118.html#filepos1486732）。
- en: Even these two simple cases show how complex scheduling can be, because it depends
    on a multitude of factors. Because of this, the Scheduler can either be configured
    to suit your specific needs or infrastructure specifics, or it can even be replaced
    with a custom implementation altogether. You could also run a Kubernetes cluster
    without a Scheduler, but then you’d have to perform the scheduling manually.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是这两个简单的案例也展示了调度可能有多么复杂，因为它取决于众多因素。因此，调度器可以被配置以适应您的特定需求或基础设施细节，或者甚至可以完全用自定义实现替换。您也可以在没有调度器的情况下运行
    Kubernetes 集群，但那样您就必须手动执行调度。
- en: Using multiple Schedulers
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个调度器
- en: Instead of running a single Scheduler in the cluster, you can run multiple Schedulers.
    Then, for each pod, you specify the Scheduler that should schedule this particular
    pod by setting the `schedulerName` property in the pod spec.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在集群中运行多个调度器，而不是运行单个调度器。然后，对于每个 Pod，您通过在 Pod 规范中设置 `schedulerName` 属性来指定应该调度此特定
    Pod 的调度器。
- en: Pods without this property set are scheduled using the default Scheduler, and
    so are pods with `schedulerName` set to `default-scheduler`. All other pods are
    ignored by the default Scheduler, so they need to be scheduled either manually
    or by another Scheduler watching for such pods.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 没有设置此属性的 Pod 将使用默认调度器进行调度，`schedulerName` 设置为 `default-scheduler` 的 Pod 也同样。所有其他
    Pod 都会被默认调度器忽略，因此它们需要手动调度或由另一个监视此类 Pod 的调度器进行调度。
- en: You can implement your own Schedulers and deploy them in the cluster, or you
    can deploy an additional instance of Kubernetes’ Scheduler with different configuration
    options.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在集群中实现自己的调度器并将它们部署进去，或者您可以部署 Kubernetes 调度器的不同配置选项的额外实例。
- en: 11.1.6\. Introducing the controllers running in the Controller Manager
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.6. 控制器管理器中运行的控制器介绍
- en: As previously mentioned, the API server doesn’t do anything except store resources
    in etcd and notify clients about the change. The Scheduler only assigns a node
    to the pod, so you need other active components to make sure the actual state
    of the system converges toward the desired state, as specified in the resources
    deployed through the API server. This work is done by controllers running inside
    the Controller Manager.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，API 服务器除了在 etcd 中存储资源并通知客户端关于更改之外，不做任何事情。调度器只为 Pod 分配节点，因此您需要其他活跃组件来确保系统的实际状态收敛到
    API 服务器通过部署的资源指定的所需状态。这项工作由控制器管理器内部运行的控制器完成。
- en: The single Controller Manager process currently combines a multitude of controllers
    performing various reconciliation tasks. Eventually those controllers will be
    split up into separate processes, enabling you to replace each one with a custom
    implementation if necessary. The list of these controllers includes the
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 目前单个控制器管理器进程结合了执行各种协调任务的众多控制器。最终，这些控制器将被拆分为单独的进程，使您能够在必要时用自定义实现替换每个控制器。这些控制器包括
- en: Replication Manager (a controller for ReplicationController resources)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制管理器（ReplicationController 资源的控制器）
- en: ReplicaSet, DaemonSet, and Job controllers
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaSet、DaemonSet 和作业控制器
- en: Deployment controller
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署控制器
- en: StatefulSet controller
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有状态集控制器
- en: Node controller
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点控制器
- en: Service controller
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务控制器
- en: Endpoints controller
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端点控制器
- en: Namespace controller
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名空间控制器
- en: PersistentVolume controller
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持久卷控制器
- en: Others
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他
- en: What each of these controllers does should be evident from its name. From the
    list, you can tell there’s a controller for almost every resource you can create.
    Resources are descriptions of what should be running in the cluster, whereas the
    controllers are the active Kubernetes components that perform actual work as a
    result of the deployed resources.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这些控制器各自的功能应该从其名称中显而易见。从列表中，您可以知道几乎为每个可以创建的资源都有一个控制器。资源是对集群中应该运行什么的描述，而控制器是执行实际工作的活跃
    Kubernetes 组件，这些工作是由部署的资源触发的。
- en: Understanding what controllers do and how they do it
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 理解控制器做什么以及它们是如何做的
- en: Controllers do many different things, but they all watch the API server for
    changes to resources (Deployments, Services, and so on) and perform operations
    for each change, whether it’s a creation of a new object or an update or deletion
    of an existing object. Most of the time, these operations include creating other
    resources or updating the watched resources themselves (to update the object’s
    `status`, for example).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器执行许多不同的事情，但它们都监视API服务器以获取资源（部署、服务等）的更改，并对每个更改执行操作，无论是新对象的创建、更新还是现有对象的删除。大多数时候，这些操作包括创建其他资源或更新被监视的资源本身（例如更新对象的`status`）。
- en: In general, controllers run a reconciliation loop, which reconciles the actual
    state with the desired state (specified in the resource’s `spec` section) and
    writes the new actual state to the resource’s `status` section. Controllers use
    the watch mechanism to be notified of changes, but because using watches doesn’t
    guarantee the controller won’t miss an event, they also perform a re-list operation
    periodically to make sure they haven’t missed anything.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，控制器运行一个协调循环，该循环将实际状态与期望状态（在资源的`spec`部分指定）进行协调，并将新的实际状态写入资源的`status`部分。控制器使用监视机制来通知更改，但由于使用监视并不能保证控制器不会错过事件，它们还定期执行重新列表操作，以确保没有错过任何内容。
- en: Controllers never talk to each other directly. They don’t even know any other
    controllers exist. Each controller connects to the API server and, through the
    watch mechanism described in [section 11.1.3](#filepos1058800), asks to be notified
    when a change occurs in the list of resources of any type the controller is responsible
    for.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器之间从不直接交流。它们甚至不知道其他控制器的存在。每个控制器都连接到API服务器，并通过[第11.1.3节](#filepos1058800)中描述的监视机制，请求在控制器负责的任何类型资源的列表发生变化时通知它。
- en: We’ll briefly look at what each of the controllers does, but if you’d like an
    in-depth view of what they do, I suggest you look at their source code directly.
    The sidebar explains how to get started.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要地看看每个控制器都做了什么，但如果你想深入了解它们的功能，我建议你直接查看它们的源代码。侧边栏解释了如何开始。
- en: '|  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: A few pointers on exploring the controllers’ source code
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 探索控制器源代码的一些提示
- en: 'If you’re interested in seeing exactly how these controllers operate, I strongly
    encourage you to browse through their source code. To make it easier, here are
    a few tips:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，想确切地看到这些控制器是如何操作的，我强烈建议你浏览它们的源代码。为了使其更容易，这里有一些提示：
- en: The source code for the controllers is available at [https://github.com/kubernetes/kubernetes/blob/master/pkg/controller](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器的源代码可在[https://github.com/kubernetes/kubernetes/blob/master/pkg/controller](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller)找到。
- en: Each controller usually has a constructor in which it creates an `Informer`,
    which is basically a listener that gets called every time an API object gets updated.
    Usually, an Informer listens for changes to a specific type of resource. Looking
    at the constructor will show you which resources the controller is watching.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 每个控制器通常都有一个构造函数，在其中它创建一个`Informer`，这基本上是一个在API对象更新时被调用的监听器。通常，Informer监听特定类型资源的更改。查看构造函数将显示控制器正在监视哪些资源。
- en: Next, go look for the `worker()` method. In it, you’ll find the method that
    gets invoked each time the controller needs to do something. The actual function
    is often stored in a field called `syncHandler` or something similar. This field
    is also initialized in the constructor, so that’s where you’ll find the name of
    the function that gets called. That function is the place where all the magic
    happens.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，去找`worker()`方法。在其中，你会发现每次控制器需要做某事时都会被调用的方法。实际函数通常存储在一个名为`syncHandler`或类似字段的字段中。这个字段也在构造函数中初始化，所以你会在那里找到被调用的函数的名称。那个函数就是所有魔法发生的地方。
- en: '|  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The Replication Manager
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 复制管理器
- en: The controller that makes ReplicationController resources come to life is called
    the Replication Manager. We talked about how ReplicationControllers work in [chapter
    4](index_split_038.html#filepos358794). It’s not the ReplicationControllers that
    do the actual work, but the Replication Manager. Let’s quickly review what the
    controller does, because this will help you understand the rest of the controllers.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使ReplicationController资源活跃起来的控制器被称为副本管理器。我们在[第4章](index_split_038.html#filepos358794)中讨论了ReplicationController的工作方式。真正执行工作的是副本管理器，而不是ReplicationController。让我们快速回顾一下控制器做了什么，因为这将帮助你理解其他控制器。
- en: In [chapter 4](index_split_038.html#filepos358794), we said that the operation
    of a ReplicationController could be thought of as an infinite loop, where in each
    iteration, the controller finds the number of pods matching its pod selector and
    compares the number to the desired replica count.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](index_split_038.html#filepos358794)中，我们说ReplicationController的操作可以被视为一个无限循环，在每次迭代中，控制器找到匹配其pod选择器的pods数量，并将这个数量与所需的副本数量进行比较。
- en: Now that you know how the API server can notify clients through the watch mechanism,
    it’s clear that the controller doesn’t poll the pods in every iteration, but is
    instead notified by the watch mechanism of each change that may affect the desired
    replica count or the number of matched pods (see [figure 11.6](#filepos1082264)).
    Any such changes trigger the controller to recheck the desired vs. actual replica
    count and act accordingly.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了API服务器如何通过watch机制通知客户端，那么很明显，控制器并不是在每次迭代中都轮询pods，而是由watch机制通知每个可能影响所需副本数量或匹配pods数量的变化（见[图11.6](#filepos1082264)）。任何此类变化都会触发控制器重新检查所需的副本数量与实际副本数量，并相应地采取行动。
- en: Figure 11.6\. The Replication Manager watches for changes to API objects.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6。副本管理器监视API对象的变化。
- en: '![](images/00086.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00086.jpg)'
- en: You already know that when too few pod instances are running, the Replication-Controller
    runs additional instances. But it doesn’t actually run them itself. It creates
    new Pod manifests, posts them to the API server, and lets the Scheduler and the
    Kubelet do their job of scheduling and running the pod.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道，当运行的pod实例太少时，Replication-Controller会运行额外的实例。但实际上它并不运行这些实例。它创建新的Pod清单，将它们发布到API服务器，并让调度器和Kubelet完成它们的任务，即调度和运行pod。
- en: The Replication Manager performs its work by manipulating Pod API objects through
    the API server. This is how all controllers operate.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 副本管理器通过API服务器操作Pod API对象来完成其工作。所有控制器都是这样操作的。
- en: The ReplicaSet, the DaemonSet, and the Job controllers
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet、DaemonSet和Job控制器
- en: The ReplicaSet controller does almost the same thing as the Replication Manager
    described previously, so we don’t have much to add here. The DaemonSet and Job
    controllers are similar. They create Pod resources from the pod template defined
    in their respective resources. Like the Replication Manager, these controllers
    don’t run the pods, but post Pod definitions to the API server, letting the Kubelet
    create their containers and run them.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSet控制器几乎与之前描述的副本管理器做相同的事情，所以我们在这里没有太多要补充的。DaemonSet和Job控制器也是类似的。它们从各自资源中定义的pod模板创建Pod资源。像副本管理器一样，这些控制器不运行pods，而是将Pod定义发布到API服务器，让Kubelet创建它们的容器并运行它们。
- en: The Deployment controller
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment控制器
- en: The Deployment controller takes care of keeping the actual state of a deployment
    in sync with the desired state specified in the corresponding Deployment API object.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Deployment控制器负责确保部署的实际状态与在相应的Deployment API对象中指定的所需状态保持同步。
- en: The Deployment controller performs a rollout of a new version each time a Deployment
    object is modified (if the modification should affect the deployed pods). It does
    this by creating a ReplicaSet and then appropriately scaling both the old and
    the new ReplicaSet based on the strategy specified in the Deployment, until all
    the old pods have been replaced with new ones. It doesn’t create any pods directly.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 每次修改Deployment对象时（如果修改应该影响已部署的pods），Deployment控制器都会执行新版本的发布。它是通过创建ReplicaSet，然后根据Deployment中指定的策略适当地扩展旧的和新的ReplicaSet来实现的，直到所有旧pods都被新pods替换。它不会直接创建任何pods。
- en: The StatefulSet Controller
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 状态集控制器
- en: The StatefulSet controller, similarly to the ReplicaSet controller and other
    related controllers, creates, manages, and deletes Pods according to the spec
    of a StatefulSet resource. But while those other controllers only manage Pods,
    the StatefulSet controller also instantiates and manages PersistentVolumeClaims
    for each Pod instance.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 状态集控制器，类似于副本集控制器和其他相关控制器，根据状态集资源的规范创建、管理和删除Pod。但与其他控制器只管理Pod不同，状态集控制器还为每个Pod实例实例化和管理持久卷声明。
- en: The Node controller
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 节点控制器
- en: The Node controller manages the Node resources, which describe the cluster’s
    worker nodes. Among other things, a Node controller keeps the list of Node objects
    in sync with the actual list of machines running in the cluster. It also monitors
    each node’s health and evicts pods from unreachable nodes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 节点控制器管理节点资源，这些资源描述了集群的工作节点。在众多事情中，节点控制器保持节点对象列表与集群中实际运行的机器列表同步。它还监控每个节点的健康状况，并将Pod从不可达的节点驱逐出去。
- en: The Node controller isn’t the only component making changes to Node objects.
    They’re also changed by the Kubelet, and can obviously also be modified by users
    through REST API calls.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 节点控制器并不是唯一对节点对象进行更改的组件。Kubelet也会更改它们，并且显然也可以通过REST API调用由用户修改。
- en: The Service controller
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 服务控制器
- en: In [chapter 5](index_split_046.html#filepos469093), when we talked about Services,
    you learned that a few different types exist. One of them was the `LoadBalancer`
    service, which requests a load balancer from the infrastructure to make the service
    available externally. The Service controller is the one requesting and releasing
    a load balancer from the infrastructure, when a `LoadBalancer`-type Service is
    created or deleted.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 5 章](index_split_046.html#filepos469093)中，当我们讨论服务时，你了解到存在几种不同类型。其中之一是`LoadBalancer`服务，它从基础设施请求负载均衡器以使服务外部可用。当创建或删除`LoadBalancer`类型的服务时，服务控制器是请求和释放基础设施中的负载均衡器的控制器。
- en: The Endpoints controller
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 端点控制器
- en: You’ll remember that Services aren’t linked directly to pods, but instead contain
    a list of endpoints (IPs and ports), which is created and updated either manually
    or automatically according to the pod selector defined on the Service. The Endpoints
    controller is the active component that keeps the endpoint list constantly updated
    with the IPs and ports of pods matching the label selector.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得服务并不是直接链接到Pod，而是包含一个端点列表（IP和端口），该列表是手动或根据服务上定义的Pod选择器自动创建和更新的。端点控制器是保持端点列表不断更新为匹配标签选择器的Pod的IP和端口的主动组件。
- en: As [figure 11.7](#filepos1086979) shows, the controller watches both Services
    and Pods. When Services are added or updated or Pods are added, updated, or deleted,
    it selects Pods matching the Service’s pod selector and adds their IPs and ports
    to the Endpoints resource. Remember, the Endpoints object is a standalone object,
    so the controller creates it if necessary. Likewise, it also deletes the Endpoints
    object when the Service is deleted.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 11.7](#filepos1086979)所示，控制器监视着服务和Pod。当服务被添加或更新，或者Pod被添加、更新或删除时，它会选择与服务的Pod选择器匹配的Pod，并将它们的IP和端口添加到端点资源中。记住，端点对象是一个独立对象，因此控制器在必要时会创建它。同样，当服务被删除时，它也会删除端点对象。
- en: Figure 11.7\. The Endpoints controller watches Service and Pod resources, and
    manages Endpoints.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7\. 端点控制器监视服务和Pod资源，并管理端点。
- en: '![](images/00104.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00104.jpg)'
- en: The Namespace controller
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间控制器
- en: Remember namespaces (we talked about them in [chapter 3](index_split_028.html#filepos271328))?
    Most resources belong to a specific namespace. When a Namespace resource is deleted,
    all the resources in that namespace must also be deleted. This is what the Namespace
    controller does. When it’s notified of the deletion of a Namespace object, it
    deletes all the resources belonging to the namespace through the API server.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 记得命名空间（我们在[第 3 章](index_split_028.html#filepos271328)中讨论过）？大多数资源都属于特定的命名空间。当一个命名空间资源被删除时，该命名空间中的所有资源也必须被删除。这就是命名空间控制器所做的事情。当它被通知命名空间对象的删除时，它会通过API服务器删除属于该命名空间的所有资源。
- en: The PersistentVolume controller
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 持久卷控制器
- en: In [chapter 6](index_split_055.html#filepos588298) you learned about PersistentVolumes
    and PersistentVolumeClaims. Once a user creates a PersistentVolumeClaim, Kubernetes
    must find an appropriate PersistentVolume and bind it to the claim. This is performed
    by the PersistentVolume controller.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](index_split_055.html#filepos588298)中，你学习了关于PersistentVolumes和PersistentVolumeClaims的内容。一旦用户创建PersistentVolumeClaim，Kubernetes必须找到一个合适的PersistentVolume并将其绑定到声明。这是由PersistentVolume控制器执行的。
- en: When a PersistentVolumeClaim pops up, the controller finds the best match for
    the claim by selecting the smallest PersistentVolume with the access mode matching
    the one requested in the claim and the declared capacity above the capacity requested
    in the claim. It does this by keeping an ordered list of PersistentVolumes for
    each access mode by ascending capacity and returning the first volume from the
    list.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当PersistentVolumeClaim出现时，控制器通过选择与请求中声明匹配的访问模式且声明容量高于请求容量的最小PersistentVolume来找到最佳匹配。它是通过按容量升序保持每个访问模式的PersistentVolume有序列表，并从列表中返回第一个卷来实现的。
- en: Then, when the user deletes the PersistentVolumeClaim, the volume is unbound
    and reclaimed according to the volume’s reclaim policy (left as is, deleted, or
    emptied).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当用户删除PersistentVolumeClaim时，卷将解除绑定并根据卷的回收策略（保持原样、删除或清空）回收。
- en: Controller wrap-up
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器总结
- en: You should now have a good feel for what each controller does and how controllers
    work in general. Again, all these controllers operate on the API objects through
    the API server. They don’t communicate with the Kubelets directly or issue any
    kind of instructions to them. In fact, they don’t even know Kubelets exist. After
    a controller updates a resource in the API server, the Kubelets and Kubernetes
    Service Proxies, also oblivious of the controllers’ existence, perform their work,
    such as spinning up a pod’s containers and attaching network storage to them,
    or in the case of services, setting up the actual load balancing across pods.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该对每个控制器做什么以及控制器通常如何工作有一个很好的感觉。再次强调，所有这些控制器都通过API服务器操作API对象。它们不会直接与Kubelets通信或向它们发出任何类型的指令。实际上，它们甚至不知道Kubelets的存在。在控制器更新API服务器中的资源后，Kubelets和Kubernetes服务代理（同样对控制器的存在一无所知）执行它们的工作，例如启动Pod的容器并将网络存储附加到它们，或者在服务的情况下，在Pod之间设置实际的负载均衡。
- en: The Control Plane handles one part of the operation of the whole system, so
    to fully understand how things unfold in a Kubernetes cluster, you also need to
    understand what the Kubelet and the Kubernetes Service Proxy do. We’ll learn that
    next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 控制面处理整个系统操作的一部分，因此要完全理解Kubernetes集群中事情的发展，还需要了解Kubelet和Kubernetes服务代理的作用。我们将在下一节学习这一点。
- en: 11.1.7\. What the Kubelet does
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.7\. Kubelet执行的操作
- en: In contrast to all the controllers, which are part of the Kubernetes Control
    Plane and run on the master node(s), the Kubelet and the Service Proxy both run
    on the worker nodes, where the actual pods containers run. What does the Kubelet
    do exactly?
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有作为Kubernetes控制平面一部分并在主节点（s）上运行的控制器不同，Kubelet和服务代理都在运行实际Pod容器的worker节点上运行。Kubelet究竟做什么？
- en: Understanding the Kubelet’s job
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Kubelet的工作
- en: In a nutshell, the Kubelet is the component responsible for everything running
    on a worker node. Its initial job is to register the node it’s running on by creating
    a Node resource in the API server. Then it needs to continuously monitor the API
    server for Pods that have been scheduled to the node, and start the pod’s containers.
    It does this by telling the configured container runtime (which is Docker, CoreOS’
    rkt, or something else) to run a container from a specific container image. The
    Kubelet then constantly monitors running containers and reports their status,
    events, and resource consumption to the API server.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Kubelet是负责在worker节点上运行一切组件。它的初始任务是创建API服务器中的Node资源来注册其运行的节点。然后它需要持续监控API服务器以查找已调度到节点的Pod，并启动Pod的容器。它是通过告诉配置的容器运行时（Docker、CoreOS的rkt或其他）从特定的容器镜像运行一个容器来实现的。然后Kubelet持续监控正在运行的容器并向API服务器报告它们的状态、事件和资源消耗。
- en: The Kubelet is also the component that runs the container liveness probes, restarting
    containers when the probes fail. Lastly, it terminates containers when their Pod
    is deleted from the API server and notifies the server that the pod has terminated.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet也是运行容器存活探测的组件，当探测失败时重启容器。最后，当它们的Pod从API服务器删除时，它会终止容器并通知服务器Pod已终止。
- en: Running static pods without the API server
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有 API 服务器的情况下运行静态 pods
- en: Although the Kubelet talks to the Kubernetes API server and gets the pod manifests
    from there, it can also run pods based on pod manifest files in a specific local
    directory as shown in [figure 11.8](#filepos1091770). This feature is used to
    run the containerized versions of the Control Plane components as pods, as you
    saw in the beginning of the chapter.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Kubelet 与 Kubernetes API 服务器通信并从那里获取 pod 清单，但它也可以根据特定本地目录中的 pod 清单文件运行 pods，如图
    11.8 所示。此功能用于运行控制平面组件的容器化版本作为 pods，正如你在本章开头所见。
- en: Figure 11.8\. The Kubelet runs pods based on pod specs from the API server and
    a local file directory.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8. Kubelet 根据来自 API 服务器的 pod 规范和本地文件目录运行 pods。
- en: '![](images/00123.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00123.jpg)'
- en: Instead of running Kubernetes system components natively, you can put their
    pod manifests into the Kubelet’s manifest directory and have the Kubelet run and
    manage them. You can also use the same method to run your custom system containers,
    but doing it through a DaemonSet is the recommended method.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 除了以原生方式运行 Kubernetes 系统组件外，您可以将它们的 pod 清单放入 Kubelet 的清单目录中，并让 Kubelet 运行和管理它们。您还可以使用相同的方法运行您自定义的系统容器，但通过
    DaemonSet 执行是推荐的方法。
- en: 11.1.8\. The role of the Kubernetes Service Proxy
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.8. Kubernetes 服务代理的作用
- en: Beside the Kubelet, every worker node also runs the kube-proxy, whose purpose
    is to make sure clients can connect to the services you define through the Kubernetes
    API. The kube-proxy makes sure connections to the service IP and port end up at
    one of the pods backing that service (or other, non-pod service endpoints). When
    a service is backed by more than one pod, the proxy performs load balancing across
    those pods.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Kubelet 之外，每个工作节点还运行 kube-proxy，其目的是确保客户端可以通过 Kubernetes API 连接到你定义的服务。kube-proxy
    确保连接到服务 IP 和端口的连接最终到达支持该服务的某个 pod（或其他非 pod 服务端点）。当一个服务由多个 pod 支持时，代理在这些 pod 之间执行负载均衡。
- en: Why it’s called a proxy
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么叫代理
- en: The initial implementation of the kube-proxy was the `userspace` proxy. It used
    an actual server process to accept connections and proxy them to the pods. To
    intercept connections destined to the service IPs, the proxy configured `iptables`
    rules (`iptables` is the tool for managing the Linux kernel’s packet filtering
    features) to redirect the connections to the proxy server. A rough diagram of
    the `userspace` proxy mode is shown in [figure 11.9](#filepos1093556).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy 的初始实现是 `userspace` 代理。它使用实际的服务器进程来接受连接并将它们代理到 pods。为了拦截目的地为服务 IP
    的连接，代理配置了 `iptables` 规则（`iptables` 是管理 Linux 内核数据包过滤功能的工具）以将连接重定向到代理服务器。`userspace`
    代理模式的粗略图示见 [图 11.9](#filepos1093556)。
- en: Figure 11.9\. The `userspace` proxy mode
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9. `userspace` 代理模式
- en: '![](images/00140.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00140.jpg)'
- en: The kube-proxy got its name because it was an actual proxy, but the current,
    much better performing implementation only uses `iptables` rules to redirect packets
    to a randomly selected backend pod without passing them through an actual proxy
    server. This mode is called the `iptables` proxy mode and is shown in [figure
    11.10](#filepos1094229).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy 得名于它实际上是一个代理，但当前的、性能更好的实现仅使用 `iptables` 规则将数据包重定向到随机选择的后端 pod，而不通过实际的代理服务器。这种模式称为
    `iptables` 代理模式，如图 11.10 所示。
- en: Figure 11.10\. The `iptables` proxy mode
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10. `iptables` 代理模式
- en: '![](images/00158.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00158.jpg)'
- en: The major difference between these two modes is whether packets pass through
    the kube-proxy and must be handled in user space, or whether they’re handled only
    by the Kernel (in kernel space). This has a major impact on performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模式之间的主要区别在于数据包是否通过 kube-proxy 并必须在用户空间中处理，或者是否仅由内核（在内核空间）处理。这对性能有重大影响。
- en: Another smaller difference is that the `userspace` proxy mode balanced connections
    across pods in a true round-robin fashion, while the `iptables` proxy mode doesn’t—it
    selects pods randomly. When only a few clients use a service, they may not be
    spread evenly across pods. For example, if a service has two backing pods but
    only five or so clients, don’t be surprised if you see four clients connect to
    pod A and only one client connect to pod B. With a higher number of clients or
    pods, this problem isn’t so apparent.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个较小的区别是，`userspace` 代理模式以真正的轮询方式平衡 pod 之间的连接，而 `iptables` 代理模式则不这样做——它随机选择
    pod。当只有少数客户端使用服务时，它们可能不会均匀地分布在 pod 上。例如，如果一个服务有两个后端 pod 但只有大约五个客户端，如果你看到四个客户端连接到
    pod A 而只有一个客户端连接到 pod B，请不要感到惊讶。随着客户端或 pod 数量的增加，这个问题就不那么明显了。
- en: You’ll learn exactly how `iptables` proxy mode works in [section 11.5](index_split_092.html#filepos1126048).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在[第 11.5 节](index_split_092.html#filepos1126048)中详细了解 `iptables` 代理模式的工作原理。
- en: 11.1.9\. Introducing Kubernetes add-ons
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.9\. 介绍 Kubernetes 附加组件
- en: We’ve now discussed the core components that make a Kubernetes cluster work.
    But in the beginning of the chapter, we also listed a few add-ons, which although
    not always required, enable features such as DNS lookup of Kubernetes services,
    exposing multiple HTTP services through a single external IP address, the Kubernetes
    web dashboard, and so on.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经讨论了使 Kubernetes 集群工作的核心组件。但在本章的开头，我们也列出了一些附加组件，尽管它们并非总是必需的，但它们启用了诸如 Kubernetes
    服务的 DNS 查找、通过单个外部 IP 地址公开多个 HTTP 服务、Kubernetes 网络仪表板等功能。
- en: How add-ons are deployed
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 附加组件的部署方式
- en: These components are available as add-ons and are deployed as pods by submitting
    YAML manifests to the API server, the way you’ve been doing throughout the book.
    Some of these components are deployed through a Deployment resource or a ReplicationController
    resource, and some through a DaemonSet.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件作为附加组件提供，并通过向 API 服务器提交 YAML 清单的方式部署为 pod，正如你在本书中一直所做的那样。其中一些组件通过 Deployment
    资源或 ReplicationController 资源部署，而另一些则通过 DaemonSet 部署。
- en: For example, as I’m writing this, in Minikube, the Ingress controller and the
    dashboard add-ons are deployed as ReplicationControllers, as shown in the following
    listing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我撰写本文时，在 Minikube 中，Ingress 控制器和仪表板附加组件作为 ReplicationControllers 部署，如下面的列表所示。
- en: Listing 11.7\. Add-ons deployed with ReplicationControllers in Minikube
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.7\. 在 Minikube 中与 ReplicationControllers 部署的附加组件
- en: '`$ kubectl get rc -n kube-system` `NAME                       DESIRED   CURRENT  
    READY     AGE default-http-backend       1         1         1         6d kubernetes-dashboard      
    1         1         1         6d nginx-ingress-controller   1         1        
    1         6d`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get rc -n kube-system` `NAME                       DESIRED   CURRENT  
    READY     AGE default-http-backend       1         1         1         6d kubernetes-dashboard      
    1         1         1         6d nginx-ingress-controller   1         1        
    1         6d`'
- en: The DNS add-on is deployed as a Deployment, as shown in the following listing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 附加组件作为 Deployment 部署，如下面的列表所示。
- en: Listing 11.8\. The `kube-dns` Deployment
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.8\. `kube-dns` Deployment
- en: '`$ kubectl get deploy -n kube-system` `NAME       DESIRED   CURRENT   UP-TO-DATE  
    AVAILABLE   AGE kube-dns   1         1         1            1           6d`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get deploy -n kube-system` `NAME       DESIRED   CURRENT   UP-TO-DATE  
    AVAILABLE   AGE kube-dns   1         1         1            1           6d`'
- en: Let’s see how DNS and the Ingress controllers work.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 DNS 和 Ingress 控制器是如何工作的。
- en: How the DNS server works
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 服务器的工作原理
- en: All the pods in the cluster are configured to use the cluster’s internal DNS
    server by default. This allows pods to easily look up services by name or even
    the pod’s IP addresses in the case of headless services.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，集群中的所有 pod 都配置为使用集群的内部 DNS 服务器。这允许 pod 通过名称轻松查找服务，甚至在无头服务的情况下查找 pod 的
    IP 地址。
- en: The DNS server pod is exposed through the `kube-dns` service, allowing the pod
    to be moved around the cluster, like any other pod. The service’s IP address is
    specified as the `nameserver` in the /etc/resolv.conf file inside every container
    deployed in the cluster. The `kube-dns` pod uses the API server’s watch mechanism
    to observe changes to Services and Endpoints and updates its DNS records with
    every change, allowing its clients to always get (fairly) up-to-date DNS information.
    I say fairly because during the time between the update of the Service or Endpoints
    resource and the time the DNS pod receives the watch notification, the DNS records
    may be invalid.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: DNS服务器Pod通过`kube-dns`服务暴露，允许Pod在集群中移动，就像任何其他Pod一样。该服务的IP地址被指定为每个容器内部的`/etc/resolv.conf`文件中的`nameserver`。`kube-dns`
    Pod使用API服务器的监视机制来观察服务和服务端点的变化，并在每次变化时更新其DNS记录，使其客户端始终获得（相对）最新的DNS信息。我说相对是因为在服务或端点资源更新和DNS
    Pod收到监视通知之间的时间，DNS记录可能无效。
- en: How (most) Ingress controllers work
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: （大多数）Ingress控制器是如何工作的
- en: Unlike the DNS add-on, you’ll find a few different implementations of Ingress
    controllers, but most of them work in the same way. An Ingress controller runs
    a reverse proxy server (like Nginx, for example), and keeps it configured according
    to the Ingress, Service, and Endpoints resources defined in the cluster. The controller
    thus needs to observe those resources (again, through the watch mechanism) and
    change the proxy server’s config every time one of them changes.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 与DNS附加组件不同，你会发现有几个不同的Ingress控制器实现，但它们大多数以相同的方式工作。Ingress控制器运行一个反向代理服务器（例如Nginx），并根据集群中定义的Ingress、服务和端点资源进行配置。因此，控制器需要观察这些资源（再次，通过监视机制），并在它们中的任何一个发生变化时更改代理服务器的配置。
- en: Although the Ingress resource’s definition points to a Service, Ingress controllers
    forward traffic to the service’s pod directly instead of going through the service
    IP. This affects the preservation of client IPs when external clients connect
    through the Ingress controller, which makes them preferred over Services in certain
    use cases.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Ingress资源的定义指向服务，但Ingress控制器直接将流量转发到服务的Pod，而不是通过服务IP。这影响了外部客户端通过Ingress控制器连接时客户端IP的保留，这使得它们在某些用例中比服务更受欢迎。
- en: Using other add-ons
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他附加组件
- en: You’ve seen how both the DNS server and the Ingress controller add-ons are similar
    to the controllers running in the Controller Manager, except that they also accept
    client connections instead of only observing and modifying resources through the
    API server.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到DNS服务器和Ingress控制器附加组件与在Controller Manager中运行的控制器类似，只是它们也接受客户端连接，而不是仅通过API服务器观察和修改资源。
- en: Other add-ons are similar. They all need to observe the cluster state and perform
    the necessary actions when that changes. We’ll introduce a few other add-ons in
    this and the remaining chapters.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其他附加组件类似。它们都需要观察集群状态，并在状态发生变化时执行必要的操作。我们将在本章和剩余章节中介绍几个其他附加组件。
- en: 11.1.10\. Bringing it all together
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 11.1.10. 将一切整合
- en: You’ve now learned that the whole Kubernetes system is composed of relatively
    small, loosely coupled components with good separation of concerns. The API server,
    the Scheduler, the individual controllers running inside the Controller Manager,
    the Kubelet, and the kube-proxy all work together to keep the actual state of
    the system synchronized with what you specify as the desired state.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经了解到整个Kubernetes系统由相对较小、松散耦合的组件组成，具有良好的关注点分离。API服务器、调度器、在Controller Manager中运行的各个控制器、Kubelet和kube-proxy共同工作，以保持系统的实际状态与您指定的期望状态同步。
- en: For example, submitting a pod manifest to the API server triggers a coordinated
    dance of various Kubernetes components, which eventually results in the pod’s
    containers running. You’ll learn how this dance unfolds in the next section.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将Pod规范提交给API服务器会触发各种Kubernetes组件的协调舞蹈，最终导致Pod的容器运行。你将在下一节中了解这个舞蹈是如何展开的。
- en: 11.2\. How controllers cooperate
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 11.2. 控制器之间的协作
- en: You now know about all the components that a Kubernetes cluster is comprised
    of. Now, to solidify your understanding of how Kubernetes works, let’s go over
    what happens when a Pod resource is created. Because you normally don’t create
    Pods directly, you’re going to create a Deployment resource instead and see everything
    that must happen for the pod’s containers to be started.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经了解了 Kubernetes 集群由哪些组件组成。现在，为了巩固你对 Kubernetes 工作原理的理解，让我们回顾一下当创建 pod 资源时会发生什么。因为你通常不会直接创建
    pod，所以你将创建一个部署资源，并查看启动 pod 容器必须发生的所有操作。
- en: 11.2.1\. Understanding which components are involved
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 11.2.1\. 理解涉及哪些组件
- en: Even before you start the whole process, the controllers, the Scheduler, and
    the Kubelet are watching the API server for changes to their respective resource
    types. This is shown in [figure 11.11](#filepos1102412). The components depicted
    in the figure will each play a part in the process you’re about to trigger. The
    diagram doesn’t include etcd, because it’s hidden behind the API server, and you
    can think of the API server as the place where objects are stored.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始整个过程之前，控制器、调度器和 Kubelet 都在监视 API 服务器，以查看它们各自资源类型的更改。这如图 11.11 所示。图中所示的所有组件都将参与你即将触发的过程。该图不包括
    etcd，因为它隐藏在 API 服务器后面，你可以将 API 服务器视为对象存储的地方。
- en: Figure 11.11\. Kubernetes components watching API objects through the API server
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11\. Kubernetes 组件通过 API 服务器监视 API 对象
- en: '![](images/00075.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00075.jpg)'
- en: 11.2.2\. The chain of events
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 11.2.2\. 事件链
- en: Imagine you prepared the YAML file containing the Deployment manifest and you’re
    about to submit it to Kubernetes through `kubectl`. `kubectl` sends the manifest
    to the Kubernetes API server in an HTTP POST request. The API server validates
    the Deployment specification, stores it in etcd, and returns a response to `kubectl`.
    Now a chain of events starts to unfold, as shown in [figure 11.12](#filepos1103210).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你已经准备好了包含部署清单的 YAML 文件，并且你即将通过 `kubectl` 将其提交给 Kubernetes。`kubectl` 会以 HTTP
    POST 请求的形式将清单发送到 Kubernetes API 服务器。API 服务器验证部署规范，将其存储在 etcd 中，并返回响应给 `kubectl`。现在，一系列事件开始展开，如图
    11.12 所示。
- en: Figure 11.12\. The chain of events that unfolds when a Deployment resource is
    posted to the API server
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12\. 当部署资源提交到 API 服务器时展开的事件链
- en: '![](images/00036.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00036.jpg)'
- en: The Deployment controller creates the ReplicaSet
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 部署控制器创建副本集
- en: All API server clients watching the list of Deployments through the API server’s
    watch mechanism are notified of the newly created Deployment resource immediately
    after it’s created. One of those clients is the Deployment controller, which,
    as we discussed earlier, is the active component responsible for handling Deployments.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 所有通过 API 服务器监视部署列表的 API 服务器客户端在部署资源创建后立即收到通知。其中之一是部署控制器，正如我们之前讨论的，它是负责处理部署的活跃组件。
- en: As you may remember from [chapter 9](index_split_074.html#filepos865425), a
    Deployment is backed by one or more ReplicaSets, which then create the actual
    pods. As a new Deployment object is detected by the Deployment controller, it
    creates a ReplicaSet for the current specification of the Deployment. This involves
    creating a new ReplicaSet resource through the Kubernetes API. The Deployment
    controller doesn’t deal with individual pods at all.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从 [第 9 章](index_split_074.html#filepos865425) 记忆的那样，部署背后由一个或多个副本集支持，然后创建实际的
    pod。当部署控制器检测到新的部署对象时，它会为部署的当前规范创建一个副本集。这涉及到通过 Kubernetes API 创建一个新的副本集资源。部署控制器根本不处理单个
    pod。
- en: The ReplicaSet controller creates the Pod resources
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 副本集控制器创建 pod 资源
- en: The newly created ReplicaSet is then picked up by the ReplicaSet controller,
    which watches for creations, modifications, and deletions of ReplicaSet resources
    in the API server. The controller takes into consideration the replica count and
    pod selector defined in the ReplicaSet and verifies whether enough existing Pods
    match the selector.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的副本集随后被副本集控制器获取，该控制器监视 API 服务器中副本集资源的创建、修改和删除。控制器考虑副本集定义的副本数量和 pod 选择器，并验证是否有足够的现有
    pod 与选择器匹配。
- en: The controller then creates the Pod resources based on the pod template in the
    ReplicaSet (the pod template was copied over from the Deployment when the Deployment
    controller created the ReplicaSet).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器随后根据副本集中的 pod 模板（当部署控制器创建副本集时，pod 模板从部署复制过来）创建 pod 资源。
- en: The Scheduler assigns a node to the newly created Pods
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器为新创建的Pod分配节点
- en: These newly created Pods are now stored in etcd, but they each still lack one
    important thing—they don’t have an associated node yet. Their `nodeName` attribute
    isn’t set. The Scheduler watches for Pods like this, and when it encounters one,
    chooses the best node for the Pod and assigns the Pod to the node. The Pod’s definition
    now includes the name of the node it should be running on.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新创建的Pod现在存储在etcd中，但它们各自仍然缺少一个重要的事情——它们还没有关联的节点。它们的`nodeName`属性尚未设置。调度器监视这样的Pod，当它遇到这样的Pod时，会选择最适合该Pod的节点并将Pod分配给该节点。Pod的定义现在包括它应该在哪个节点上运行的名字。
- en: Everything so far has been happening in the Kubernetes Control Plane. None of
    the controllers that have taken part in this whole process have done anything
    tangible except update the resources through the API server.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有的事情都在Kubernetes控制平面中发生。参与整个过程的控制器除了通过API服务器更新资源外，没有做任何有形的工作。
- en: The Kubelet runs the pod’s containers
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet运行Pod的容器
- en: The worker nodes haven’t done anything up to this point. The pod’s containers
    haven’t been started yet. The images for the pod’s containers haven’t even been
    downloaded yet.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，工作节点还没有做任何事情。Pod的容器还没有启动。Pod容器的镜像甚至还没有下载。
- en: But with the Pod now scheduled to a specific node, the Kubelet on that node
    can finally get to work. The Kubelet, watching for changes to Pods on the API
    server, sees a new Pod scheduled to its node, so it inspects the Pod definition
    and instructs Docker, or whatever container runtime it’s using, to start the pod’s
    containers. The container runtime then runs the containers.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着Pod现在被调度到特定的节点，该节点上的Kubelet终于可以开始工作了。Kubelet正在监视API服务器上Pod的变化，看到一个新的Pod被调度到它的节点，因此它检查Pod定义并指示Docker或它所使用的任何容器运行时启动Pod的容器。容器运行时随后运行这些容器。
- en: 11.2.3\. Observing cluster events
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 11.2.3\. 观察集群事件
- en: Both the Control Plane components and the Kubelet emit events to the API server
    as they perform these actions. They do this by creating Event resources, which
    are like any other Kubernetes resource. You’ve already seen events pertaining
    to specific resources every time you used `kubectl describe` to inspect those
    resources, but you can also retrieve events directly with `kubectl get events`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面组件和Kubelet在执行这些操作时都会向API服务器发送事件。它们通过创建事件资源来完成，这些资源类似于其他任何Kubernetes资源。每次你使用`kubectl
    describe`来检查这些资源时，你都已经看到了与特定资源相关的事件，但你也可以直接使用`kubectl get events`来检索事件。
- en: Maybe it’s me, but using `kubectl get` to inspect events is painful, because
    they’re not shown in proper temporal order. Instead, if an event occurs multiple
    times, the event is displayed only once, showing when it was first seen, when
    it was last seen, and the number of times it occurred. Luckily, watching events
    with the `--watch` option is much easier on the eyes and useful for seeing what’s
    happening in the cluster.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是我个人的感受，但使用`kubectl get`来检查事件很痛苦，因为它们没有按照正确的时间顺序显示。相反，如果一个事件发生多次，事件只会显示一次，显示它首次出现的时间、最后一次出现的时间和发生次数。幸运的是，使用`--watch`选项来监视事件对眼睛来说更容易接受，并且有助于了解集群中正在发生的事情。
- en: The following listing shows the events emitted in the process described previously
    (some columns have been removed and the output is edited heavily to make it legible
    in the limited space on the page).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了之前描述过程中发出的事件（一些列已被删除，输出经过大量编辑以在页面有限的空白空间中可读）。
- en: Listing 11.9\. Watching events emitted by the controllers
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.9\. 监视控制器发出的事件
- en: '`$ kubectl get events --watch` `NAME             KIND         REASON             
    SOURCE ... kubia            Deployment   ScalingReplicaSet   deployment-controller`
    ![](images/00006.jpg) `Scaled up replica set kubia-193 to 3 ... kubia-193       
    ReplicaSet   SuccessfulCreate    replicaset-controller` ![](images/00006.jpg)
    `Created pod: kubia-193-w7ll2 ... kubia-193-tpg6j  Pod          Scheduled          
    default-scheduler` ![](images/00006.jpg) `Successfully assigned kubia-193-tpg6j
    to node1 ... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller`
    ![](images/00006.jpg) `Created pod: kubia-193-39590 ... kubia-193        ReplicaSet  
    SuccessfulCreate    replicaset-controller` ![](images/00006.jpg) `Created pod:
    kubia-193-tpg6j ... kubia-193-39590  Pod          Scheduled           default-scheduler`
    ![](images/00006.jpg) `Successfully assigned kubia-193-39590 to node2 ... kubia-193-w7ll2 
    Pod          Scheduled           default-scheduler` ![](images/00006.jpg) `Successfully
    assigned kubia-193-w7ll2 to node2 ... kubia-193-tpg6j  Pod          Pulled             
    kubelet, node1` ![](images/00006.jpg) `Container image already present on machine
    ... kubia-193-tpg6j  Pod          Created             kubelet, node1` ![](images/00006.jpg)
    `Created container with id 13da752 ... kubia-193-39590  Pod          Pulled             
    kubelet, node2` ![](images/00006.jpg) `Container image already present on machine
    ... kubia-193-tpg6j  Pod          Started             kubelet, node1` ![](images/00006.jpg)
    `Started container with id 13da752 ... kubia-193-w7ll2  Pod          Pulled             
    kubelet, node2` ![](images/00006.jpg) `Container image already present on machine
    ... kubia-193-39590  Pod          Created             kubelet, node2` ![](images/00006.jpg)
    `Created container with id 8850184 ...`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get events --watch` `NAME             KIND         REASON             
    SOURCE ... kubia            Deployment   ScalingReplicaSet   deployment-controller`
    ![](images/00006.jpg) `Scaled up replica set kubia-193 to 3 ... kubia-193       
    ReplicaSet   SuccessfulCreate    replicaset-controller` ![](images/00006.jpg)
    `Created pod: kubia-193-w7ll2 ... kubia-193-tpg6j  Pod          Scheduled          
    default-scheduler` ![](images/00006.jpg) `Successfully assigned kubia-193-tpg6j
    to node1 ... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller`
    ![](images/00006.jpg) `Created pod: kubia-193-39590 ... kubia-193        ReplicaSet  
    SuccessfulCreate    replicaset-controller` ![](images/00006.jpg) `Created pod:
    kubia-193-tpg6j ... kubia-193-39590  Pod          Scheduled           default-scheduler`
    ![](images/00006.jpg) `Successfully assigned kubia-193-39590 to node2 ... kubia-193-w7ll2 
    Pod          Scheduled           default-scheduler` ![](images/00006.jpg) `Successfully
    assigned kubia-193-w7ll2 to node2 ... kubia-193-tpg6j  Pod          Pulled             
    kubelet, node1` ![](images/00006.jpg) `Container image already present on machine
    ... kubia-193-tpg6j  Pod          Created             kubelet, node1` ![](images/00006.jpg)
    `Created container with id 13da752 ... kubia-193-39590  Pod          Pulled             
    kubelet, node2` ![](images/00006.jpg) `Container image already present on machine
    ... kubia-193-tpg6j  Pod          Started             kubelet, node1` ![](images/00006.jpg)
    `Started container with id 13da752 ... kubia-193-w7ll2  Pod          Pulled             
    kubelet, node2` ![](images/00006.jpg) `Container image already present on machine
    ... kubia-193-39590  Pod          Created             kubelet, node2` ![](images/00006.jpg)
    `Created container with id 8850184 ...`'
- en: As you can see, the `SOURCE` column shows the controller performing the action,
    and the `NAME` and `KIND` columns show the resource the controller is acting on.
    The `REASON` column and the `MESSAGE` column (shown in every second line) give
    more details about what the controller has done.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`SOURCE`列显示了执行操作的控制器，而`NAME`和`KIND`列显示了控制器正在对其采取行动的资源。`REASON`列和`MESSAGE`列（每行显示一次）提供了控制器所做操作的更多详细信息。
- en: 11.3\. Understanding what a running pod is
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 11.3. 理解运行中的Pod是什么
- en: With the pod now running, let’s look more closely at what a running pod even
    is. If a pod contains a single container, do you think that the Kubelet just runs
    this single container, or is there more to it?
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Pod已经开始运行，让我们更仔细地看看一个运行中的Pod究竟是什么。如果一个Pod只包含一个容器，你认为Kubelet只是运行这个单个容器，还是还有其他的事情要做？
- en: You’ve run several pods throughout this book. If you’re the investigative type,
    you may have already snuck a peek at what exactly Docker ran when you created
    a pod. If not, let me explain what you’d see.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 您在这本书中运行了几个Pod。如果您是调查型的人，您可能已经偷偷地看了一眼创建Pod时Docker到底运行了什么。如果不是，让我来解释您会看到什么。
- en: 'Imagine you run a single container pod. Let’s say you create an Nginx pod:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您运行了一个单个容器的Pod。比如说，您创建了一个Nginx Pod：
- en: '`$ kubectl run nginx --image=nginx` `deployment "nginx" created`'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run nginx --image=nginx` `deployment "nginx" created`'
- en: You can now `ssh` into the worker node running the pod and inspect the list
    of running Docker containers. I’m using Minikube to test this out, so to `ssh`
    into the single node, I use `minikube ssh`. If you’re using GKE, you can `ssh`
    into a node with `gcloud compute ssh <node name>`.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用`ssh`连接到运行Pod的工作节点，并检查正在运行的Docker容器列表。我在使用Minikube进行测试，所以为了连接到单个节点，我使用`minikube
    ssh`。如果你使用GKE，你可以使用`gcloud compute ssh <node name>`连接到节点。
- en: Once you’re inside the node, you can list all the running containers with `docker
    ps`, as shown in the following listing.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你进入了节点内部，你可以使用`docker ps`列出所有正在运行的容器，如下面的列表所示。
- en: Listing 11.10\. Listing running Docker containers
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.10\. 列出正在运行的Docker容器
- en: '`docker@minikubeVM:~$` `docker ps` `CONTAINER ID   IMAGE                  COMMAND                
    CREATED c917a6f3c3f7   nginx                  "nginx -g ''daemon off"  4 seconds
    ago 98b8bf797174   gcr.io/.../pause:3.0   "/pause"                7 seconds ago
    ...`'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker@minikubeVM:~$` `docker ps` `CONTAINER ID   IMAGE                  COMMAND                
    CREATED c917a6f3c3f7   nginx                  "nginx -g ''daemon off"  4 seconds
    ago 98b8bf797174   gcr.io/.../pause:3.0   "/pause"                7 seconds ago
    ...`'
- en: '|  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: I’ve removed irrelevant information from the previous listing—this includes
    both columns and rows. I’ve also removed all the other running containers. If
    you’re trying this out yourself, pay attention to the two containers that were
    created a few seconds ago.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经从之前的列表中移除了无关信息——这包括列和行。我还移除了所有其他正在运行的容器。如果你自己尝试这个操作，请注意几秒钟前创建的两个容器。
- en: '|  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As expected, you see the Nginx container, but also an additional container.
    Judging from the `COMMAND` column, this additional container isn’t doing anything
    (the container’s command is `"pause")`. If you look closely, you’ll see that this
    container was created a few seconds before the Nginx container. What’s its role?
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，你看到了Nginx容器，但还有一个额外的容器。从`COMMAND`列来看，这个额外的容器并没有做任何事情（容器的命令是`"pause"`)。如果你仔细观察，你会看到这个容器是在Nginx容器之前几秒钟创建的。它的作用是什么？
- en: This pause container is the container that holds all the containers of a pod
    together. Remember how all containers of a pod share the same network and other
    Linux namespaces? The pause container is an infrastructure container whose sole
    purpose is to hold all these namespaces. All other user-defined containers of
    the pod then use the namespaces of the pod infrastructure container (see [figure
    11.13](#filepos1115311)).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个暂停容器是包含一个Pod中所有容器的容器。还记得Pod中的所有容器共享相同的网络和其他Linux命名空间吗？暂停容器是一个基础设施容器，它的唯一目的是包含所有这些命名空间。然后，Pod中的所有其他用户定义的容器都使用Pod基础设施容器的命名空间（参见[图11.13](#filepos1115311)）。
- en: Figure 11.13\. A two-container pod results in three running containers sharing
    the same Linux namespaces.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13\. 一个包含两个容器的Pod导致三个运行中的容器共享相同的Linux命名空间。
- en: '![](images/00115.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00115.jpg)'
- en: Actual application containers may die and get restarted. When such a container
    starts up again, it needs to become part of the same Linux namespaces as before.
    The infrastructure container makes this possible since its lifecycle is tied to
    that of the pod—the container runs from the time the pod is scheduled until the
    pod is deleted. If the infrastructure pod is killed in the meantime, the Kubelet
    recreates it and all the pod’s containers.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的应用容器可能会死亡并重新启动。当这样的容器再次启动时，它需要成为之前相同的Linux命名空间的一部分。基础设施容器使得这一点成为可能，因为它的生命周期与Pod的生命周期绑定——容器从Pod被调度开始运行，直到Pod被删除。如果在此时基础设施Pod被杀死，Kubelet会重新创建它以及Pod的所有容器。
- en: 11.4\. Inter-pod networking
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 11.4\. Pod间网络
- en: By now, you know that each pod gets its own unique IP address and can communicate
    with all other pods through a flat, NAT-less network. How exactly does Kubernetes
    achieve this? In short, it doesn’t. The network is set up by the system administrator
    or by a Container Network Interface (CNI) plugin, not by Kubernetes itself.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你知道每个Pod都有自己的唯一IP地址，并且可以通过一个平坦的、无NAT的网络与其他所有Pod进行通信。Kubernetes究竟是如何实现这一点的呢？简而言之，它并没有实现。网络的设置是由系统管理员或容器网络接口（CNI）插件完成的，而不是由Kubernetes本身完成。
- en: 11.4.1\. What the network must be like
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 11.4.1\. 网络必须具备的特点
- en: Kubernetes doesn’t require you to use a specific networking technology, but
    it does mandate that the pods (or to be more precise, their containers) can communicate
    with each other, regardless if they’re running on the same worker node or not.
    The network the pods use to communicate must be such that the IP address a pod
    sees as its own is the exact same address that all other pods see as the IP address
    of the pod in question.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 不要求你使用特定的网络技术，但它确实要求 pods（或者更准确地说，它们的容器）能够相互通信，无论它们是否运行在同一个工作节点上。pods
    用于通信的网络必须是这样的，即 pod 看到的自己的 IP 地址与所有其他 pod 看到的该 pod 的 IP 地址完全相同。
- en: Look at [figure 11.14](#filepos1117480). When pod A connects to (sends a network
    packet to) pod B, the source IP pod B sees must be the same IP that pod A sees
    as its own. There should be no network address translation (NAT) performed in
    between—the packet sent by pod A must reach pod B with both the source and destination
    address unchanged.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 看看 [图 11.14](#filepos1117480)。当 pod A 连接到（向 pod B 发送网络数据包）pod B 时，pod B 看到的源
    IP 地址必须与 pod A 看到的自己的 IP 地址相同。在两者之间不应执行网络地址转换（NAT）——pod A 发送的数据包必须以源和目标地址不变的方式到达
    pod B。
- en: Figure 11.14\. Kubernetes mandates pods are connected through a NAT-less network.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14. Kubernetes 要求 pods 通过无 NAT 网络连接。
- en: '![](images/00176.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00176.jpg)'
- en: This is important, because it makes networking for applications running inside
    pods simple and exactly as if they were running on machines connected to the same
    network switch. The absence of NAT between pods enables applications running inside
    them to self-register in other pods.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为它使得在 pods 内运行的应用程序的网络变得简单，就像它们在连接到同一网络交换机的机器上运行一样。pod 之间没有 NAT 的存在使得它们内部运行的应用程序能够在其他
    pod 中进行自我注册。
- en: For example, say you have a client pod X and pod Y, which provides a kind of
    notification service to all pods that register with it. Pod X connects to pod
    Y and tells it, “Hey, I’m pod X, available at IP 1.2.3.4; please send updates
    to me at this IP address.” The pod providing the service can connect to the first
    pod by using the received IP address.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个客户端 pod X 和 pod Y，它为所有注册到它的 pods 提供一种通知服务。pod X 连接到 pod Y 并告诉它：“嘿，我是
    pod X，可在 IP 1.2.3.4 上找到；请将更新发送到这个 IP 地址。”提供服务的 pod 可以通过使用接收到的 IP 地址连接到第一个 pod。
- en: The requirement for NAT-less communication between pods also extends to pod-to-node
    and node-to-pod communication. But when a pod communicates with services out on
    the internet, the source IP of the packets the pod sends does need to be changed,
    because the pod’s IP is private. The source IP of outbound packets is changed
    to the host worker node’s IP address.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 pods 之间无 NAT 通信的要求也扩展到了 pod 到节点和节点到 pod 的通信。但是，当 pod 与互联网上的服务通信时，pod 发送的数据包的源
    IP 地址确实需要更改，因为 pod 的 IP 地址是私有的。出站数据包的源 IP 地址被更改为主机工作节点的 IP 地址。
- en: Building a proper Kubernetes cluster involves setting up the networking according
    to these requirements. There are various methods and technologies available to
    do this, each with its own benefits or drawbacks in a given scenario. Because
    of this, we’re not going to go into specific technologies. Instead, let’s explain
    how inter-pod networking works in general.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 构建合适的 Kubernetes 集群需要根据这些要求设置网络。有各种方法和技术可供选择，每种方法在特定场景下都有其自身的优点或缺点。因此，我们不会深入探讨具体技术。相反，让我们解释一下
    pod 之间的网络是如何工作的。
- en: 11.4.2\. Diving deeper into how networking works
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 11.4.2. 深入了解网络工作原理
- en: In [section 11.3](index_split_090.html#filepos1112051), we saw that a pod’s
    IP address and network namespace are set up and held by the infrastructure container
    (the pause container). The pod’s containers then use its network namespace. A
    pod’s network interface is thus whatever is set up in the infrastructure container.
    Let’s see how the interface is created and how it’s connected to the interfaces
    in all the other pods. Look at [figure 11.15](#filepos1119856). We’ll discuss
    it next.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 11.3 节](index_split_090.html#filepos1112051) 中，我们了解到 pod 的 IP 地址和网络命名空间是由基础设施容器（即
    pause 容器）设置和保留的。然后 pod 的容器使用其网络命名空间。因此，pod 的网络接口就是基础设施容器中设置的内容。让我们看看接口是如何创建的，以及它是如何连接到所有其他
    pod 的接口的。看看 [图 11.15](#filepos1119856)。我们将在下一节讨论它。
- en: Figure 11.15\. Pods on a node are connected to the same bridge through virtual
    Ethernet interface pairs.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15. 节点上的 pods 通过虚拟以太网接口对连接到同一个网桥。
- en: '![](images/00028.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00028.jpg)'
- en: Enabling communication between pods on the same node
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 启用同一节点上 pods 之间的通信
- en: Before the infrastructure container is started, a virtual Ethernet interface
    pair (a veth pair) is created for the container. One interface of the pair remains
    in the host’s namespace (you’ll see it listed as `vethXXX` when you run `ifconfig`
    on the node), whereas the other is moved into the container’s network namespace
    and renamed `eth0`. The two virtual interfaces are like two ends of a pipe (or
    like two network devices connected by an Ethernet cable)—what goes in on one side
    comes out on the other, and vice-versa.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动基础设施容器之前，为容器创建一个虚拟以太网接口对（veth对）。这对中的一个接口保留在主机命名空间中（当你在这个节点上运行`ifconfig`时，你会看到它被列为`vethXXX`），而另一个被移动到容器的网络命名空间并重命名为`eth0`。这两个虚拟接口就像管道的两端（或者就像通过以太网线连接的两个网络设备）——一侧进入的内容会在另一侧出来，反之亦然。
- en: The interface in the host’s network namespace is attached to a network bridge
    that the container runtime is configured to use. The `eth0` interface in the container
    is assigned an IP address from the bridge’s address range. Anything that an application
    running inside the container sends to the `eth0` network interface (the one in
    the container’s namespace), comes out at the other veth interface in the host’s
    namespace and is sent to the bridge. This means it can be received by any network
    interface that’s connected to the bridge.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 主机网络命名空间中的接口连接到容器运行时配置的网络桥接器。容器中的`eth0`接口分配了桥接器地址范围内的IP地址。容器内部运行的应用程序发送到`eth0`网络接口（容器命名空间中的那个）的内容，会从主机命名空间中的另一个veth接口出来，并发送到桥接器。这意味着它可以被连接到桥接器的任何网络接口接收。
- en: If pod A sends a network packet to pod B, the packet first goes through pod
    A’s veth pair to the bridge and then through pod B’s veth pair. All containers
    on a node are connected to the same bridge, which means they can all communicate
    with each other. But to enable communication between containers running on different
    nodes, the bridges on those nodes need to be connected somehow.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 如果pod A向pod B发送网络数据包，数据包首先通过pod A的veth对到达桥接器，然后通过pod B的veth对。一个节点上的所有容器都连接到同一个桥接器，这意味着它们可以相互通信。但为了使运行在不同节点上的容器之间的通信成为可能，那些节点上的桥接器需要以某种方式连接起来。
- en: Enabling communication between pods on different nodes
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 启用不同节点上pod之间的通信
- en: You have many ways to connect bridges on different nodes. This can be done with
    overlay or underlay networks or by regular layer 3 routing, which we’ll look at
    next.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你有多种方式连接不同节点上的桥接器。这可以通过覆盖网络或底层网络或通过常规的第三层路由来实现，我们将在下一节中探讨。
- en: You know pod IP addresses must be unique across the whole cluster, so the bridges
    across the nodes must use non-overlapping address ranges to prevent pods on different
    nodes from getting the same IP. In the example shown in [figure 11.16](#filepos1122601),
    the bridge on node A is using the 10.1.1.0/24 IP range and the bridge on node
    B is using 10.1.2.0/24, which ensures no IP address conflicts exist.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道pod IP地址在整个集群中必须是唯一的，因此节点之间的桥接器必须使用不重叠的地址范围，以防止不同节点上的pod获得相同的IP。在图11.16的示例中，节点A上的桥接器使用10.1.1.0/24
    IP范围，而节点B上的桥接器使用10.1.2.0/24，这确保了不存在IP地址冲突。
- en: Figure 11.16\. For pods on different nodes to communicate, the bridges need
    to be connected somehow.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16。为了使不同节点上的pod能够通信，桥接器需要以某种方式连接起来。
- en: '![](images/00124.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00124.jpg)'
- en: '[Figure 11.16](#filepos1122601) shows that to enable communication between
    pods across two nodes with plain layer 3 networking, the node’s physical network
    interface needs to be connected to the bridge as well. Routing tables on node
    A need to be configured so all packets destined for 10.1.2.0/24 are routed to
    node B, whereas node B’s routing tables need to be configured so packets sent
    to 10.1.1.0/24 are routed to node A.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.16](#filepos1122601)显示，为了通过普通的第三层网络在两个节点之间的pod之间启用通信，节点的物理网络接口需要连接到桥接器。节点A上的路由表需要配置，以便所有目标为10.1.2.0/24的数据包被路由到节点B，而节点B的路由表需要配置，以便发送到10.1.1.0/24的数据包被路由到节点A。'
- en: With this type of setup, when a packet is sent by a container on one of the
    nodes to a container on the other node, the packet first goes through the veth
    pair, then through the bridge to the node’s physical adapter, then over the wire
    to the other node’s physical adapter, through the other node’s bridge, and finally
    through the veth pair of the destination container.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种配置下，当一个节点上的容器向另一个节点上的容器发送数据包时，数据包首先通过veth对，然后通过桥接到节点的物理适配器，接着通过电线到达另一个节点的物理适配器，通过另一个节点的桥接器，最后通过目标容器的veth对。
- en: This works only when nodes are connected to the same network switch, without
    any routers in between; otherwise those routers would drop the packets because
    they refer to pod IPs, which are private. Sure, the routers in between could be
    configured to route packets between the nodes, but this becomes increasingly difficult
    and error-prone as the number of routers between the nodes increases. Because
    of this, it’s easier to use a Software Defined Network (SDN), which makes the
    nodes appear as though they’re connected to the same network switch, regardless
    of the actual underlying network topology, no matter how complex it is. Packets
    sent from the pod are encapsulated and sent over the network to the node running
    the other pod, where they are de-encapsulated and delivered to the pod in their
    original form.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这只在节点连接到相同的网络交换机时才有效，中间没有路由器；否则，那些路由器会丢弃数据包，因为它们引用的是Pod IP，这些IP是私有的。当然，中间的路由器可以被配置为在节点之间路由数据包，但随着节点之间路由器数量的增加，这变得越来越困难且容易出错。因此，使用软件定义网络（SDN）更容易，它使得节点看起来就像它们连接到相同的网络交换机一样，无论实际的底层网络拓扑多么复杂。从Pod发送的数据包被封装并通过网络发送到运行另一个Pod的节点，在那里它们被解封装并以原始形式交付给Pod。
- en: 11.4.3\. Introducing the Container Network Interface
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 11.4.3\. 介绍容器网络接口
- en: To make it easier to connect containers into a network, a project called Container
    Network Interface (CNI) was started. The CNI allows Kubernetes to be configured
    to use any CNI plugin that’s out there. These plugins include
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易地将容器连接到网络，启动了一个名为容器网络接口（CNI）的项目。CNI允许Kubernetes配置使用任何现有的CNI插件。这些插件包括
- en: Calico
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico
- en: Flannel
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel
- en: Romana
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Romana
- en: Weave Net
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave Net
- en: And others
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及其他
- en: We’re not going to go into the details of these plugins; if you want to learn
    more about them, refer to [https://kubernetes.io/docs/concepts/cluster-administration/addons/](https://kubernetes.io/docs/concepts/cluster-administration/addons/).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨这些插件的细节；如果你想要了解更多关于它们的信息，请参考[https://kubernetes.io/docs/concepts/cluster-administration/addons/](https://kubernetes.io/docs/concepts/cluster-administration/addons/)。
- en: Installing a network plugin isn’t difficult. You only need to deploy a YAML
    containing a DaemonSet and a few other supporting resources. This YAML is provided
    on each plugin’s project page. As you can imagine, the DaemonSet is used to deploy
    a network agent on all cluster nodes. It then ties into the CNI interface on the
    node, but be aware that the Kubelet needs to be started with `--network-plugin=cni`
    to use CNI.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 安装网络插件并不困难。你只需要部署一个包含DaemonSet和其他一些支持资源的YAML文件。这个YAML文件在每个插件的项目页面上提供。正如你可以想象的那样，DaemonSet用于在所有集群节点上部署网络代理。然后它连接到节点的CNI接口，但请注意，Kubelet需要以`--network-plugin=cni`的方式启动才能使用CNI。
- en: 11.5\. How services are implemented
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 11.5\. 服务的实现方式
- en: In [chapter 5](index_split_046.html#filepos469093) you learned about Services,
    which allow exposing a set of pods at a long-lived, stable IP address and port.
    In order to focus on what Services are meant for and how they can be used, we
    intentionally didn’t go into how they work. But to truly understand Services and
    have a better feel for where to look when things don’t behave the way you expect,
    you need to understand how they are implemented.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](index_split_046.html#filepos469093)中，你学习了关于服务的内容，服务允许在持久稳定IP地址和端口上暴露一组Pod。为了专注于服务的目的以及它们的使用方式，我们故意没有深入探讨它们的工作原理。但为了真正理解服务，并在事情没有按预期进行时更好地了解查找方向，你需要了解它们是如何实现的。
- en: 11.5.1\. Introducing the kube-proxy
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 11.5.1\. 介绍kube-proxy
- en: Everything related to Services is handled by the kube-proxy process running
    on each node. Initially, the kube-proxy was an actual proxy waiting for connections
    and for each incoming connection, opening a new connection to one of the pods.
    This was called the `userspace` proxy mode. Later, a better-performing `iptables`
    proxy mode replaced it. This is now the default, but you can still configure Kubernetes
    to use the old mode if you want.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 与服务相关的一切都由每个节点上运行的kube-proxy进程处理。最初，kube-proxy是一个实际的代理，等待连接，并为每个传入连接打开到Pod的一个新连接。这被称为`userspace`代理模式。后来，性能更好的`iptables`代理模式取代了它。现在这是默认模式，但如果你愿意，仍然可以配置Kubernetes使用旧模式。
- en: Before we continue, let’s quickly review a few things about Services, which
    are relevant for understanding the next few paragraphs.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们快速回顾一下关于服务的一些内容，这些内容对于理解接下来的几段很重要。
- en: We’ve learned that each Service gets its own stable IP address and port. Clients
    (usually pods) use the service by connecting to this IP address and port. The
    IP address is virtual—it’s not assigned to any network interfaces and is never
    listed as either the source or the destination IP address in a network packet
    when the packet leaves the node. A key detail of Services is that they consist
    of an IP and port pair (or multiple IP and port pairs in the case of multi-port
    Services), so the service IP by itself doesn’t represent anything. That’s why
    you can’t ping them.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到每个服务都拥有自己的稳定IP地址和端口。客户端（通常是Pod）通过连接到这个IP地址和端口来使用服务。这个IP地址是虚拟的——它没有分配给任何网络接口，并且在包离开节点时，该IP地址永远不会被列为网络包的源IP地址或目标IP地址。服务的一个关键细节是，它们由一个IP和端口号对（或多个IP和端口号对，在多端口服务的情况下）组成，因此服务IP本身并不代表任何东西。这就是为什么你不能ping它们的原因。
- en: 11.5.2\. How kube-proxy uses iptables
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 11.5.2\. kube-proxy如何使用iptables
- en: When a service is created in the API server, the virtual IP address is assigned
    to it immediately. Soon afterward, the API server notifies all kube-proxy agents
    running on the worker nodes that a new Service has been created. Then, each kube-proxy
    makes that service addressable on the node it’s running on. It does this by setting
    up a few `iptables` rules, which make sure each packet destined for the service
    IP/port pair is intercepted and its destination address modified, so the packet
    is redirected to one of the pods backing the service.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 当在API服务器中创建服务时，虚拟IP地址会立即分配给它。随后不久，API服务器通知所有在工作节点上运行的kube-proxy代理，已创建了一个新的服务。然后，每个kube-proxy在其运行的节点上使该服务地址可访问。它是通过设置一些`iptables`规则来做到这一点的，这些规则确保每个目标为服务IP/端口号对的包被拦截，并且其目标地址被修改，因此包被重定向到支持该服务的某个Pod。
- en: Besides watching the API server for changes to Services, kube-proxy also watches
    for changes to Endpoints objects. We talked about them in [chapter 5](index_split_046.html#filepos469093),
    but let me refresh your memory, as it’s easy to forget they even exist, because
    you rarely create them manually. An Endpoints object holds the IP/port pairs of
    all the pods that back the service (an IP/port pair can also point to something
    other than a pod). That’s why the kube-proxy must also watch all Endpoints objects.
    After all, an Endpoints object changes every time a new backing pod is created
    or deleted, and when the pod’s readiness status changes or the pod’s labels change
    and it falls in or out of scope of the service.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监视API服务器中服务的更改外，kube-proxy还监视端点对象的更改。我们在第5章（index_split_046.html#filepos469093）中讨论了它们，但让我来刷新一下你的记忆，因为很容易忘记它们的存在，因为你很少手动创建它们。端点对象包含所有支持服务的Pod的IP/端口号对（IP/端口号对也可以指向除Pod之外的东西）。这就是为什么kube-proxy也必须监视所有端点对象。毕竟，每当创建或删除一个新的后端Pod，或者Pod的就绪状态改变，或者Pod的标签改变，并且它进入或退出服务的范围时，端点对象都会发生变化。
- en: Now let’s see how kube-proxy enables clients to connect to those pods through
    the Service. This is shown in [figure 11.17](#filepos1129571).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看kube-proxy是如何使客户端能够通过服务连接到那些Pod的。这如图11.17所示。
- en: Figure 11.17\. Network packets sent to a Service’s virtual IP/port pair are
    modified and redirected to a randomly selected backend pod.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17\. 发送到服务虚拟IP/端口号对的网络包被修改并重定向到随机选择的后端Pod。
- en: '![](images/00141.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00141.jpg)'
- en: The figure shows what the `kube-proxy` does and how a packet sent by a client
    pod reaches one of the pods backing the Service. Let’s examine what happens to
    the packet when it’s sent by the client pod (pod A in the figure).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了`kube-proxy`做了什么以及客户端Pod发送的包如何到达支持服务的某个Pod。让我们检查当客户端Pod（图中的Pod A）发送该包时发生了什么。
- en: The packet’s destination is initially set to the IP and port of the Service
    (in the example, the Service is at 172.30.0.1:80). Before being sent to the network,
    the packet is first handled by node A’s kernel according to the `iptables` rules
    set up on the node.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 包的初始目标设置为服务的IP和端口（在示例中，服务位于172.30.0.1:80）。在发送到网络之前，该包首先由节点A的内核根据节点上设置的`iptables`规则进行处理。
- en: The kernel checks if the packet matches any of those `iptables` rules. One of
    them says that if any packet has the destination IP equal to 172.30.0.1 and destination
    port equal to 80, the packet’s destination IP and port should be replaced with
    the IP and port of a randomly selected pod.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 内核检查该包是否匹配那些`iptables`规则之一。其中之一规定，如果任何包的目标IP等于172.30.0.1且目标端口等于80，则该包的目标IP和端口应替换为随机选择的Pod的IP和端口。
- en: The packet in the example matches that rule and so its destination IP/port is
    changed. In the example, pod B2 was randomly selected, so the packet’s destination
    IP is changed to 10.1.2.1 (pod B2’s IP) and the port to 8080 (the target port
    specified in the Service spec). From here on, it’s exactly as if the client pod
    had sent the packet to pod B directly instead of through the service.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的数据包符合该规则，因此其目标IP/端口被更改。在示例中，Pod B2被随机选择，因此数据包的目标IP被更改为10.1.2.1（Pod B2的IP），端口更改为8080（在Service规范中指定的目标端口）。从现在开始，这就像客户端Pod直接将数据包发送到Pod而不是通过服务一样。
- en: It’s slightly more complicated than that, but that’s the most important part
    you need to understand.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这比那稍微复杂一点，但这是你需要理解的最重要部分。
- en: 11.6\. Running highly available clusters
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 11.6\. 运行高度可用的集群
- en: One of the reasons for running apps inside Kubernetes is to keep them running
    without interruption with no or limited manual intervention in case of infrastructure
    failures. For running services without interruption it’s not only the apps that
    need to be up all the time, but also the Kubernetes Control Plane components.
    We’ll look at what’s involved in achieving high availability next.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中运行应用程序的原因之一是确保它们在没有中断的情况下运行，在基础设施故障的情况下没有或有限的手动干预。为了无中断地运行服务，不仅应用程序需要始终处于运行状态，Kubernetes控制平面组件也需要始终处于运行状态。我们将探讨实现高可用性所涉及的内容。
- en: 11.6.1\. Making your apps highly available
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 11.6.1\. 使你的应用程序高度可用
- en: When running apps in Kubernetes, the various controllers make sure your app
    keeps running smoothly and at the specified scale even when nodes fail. To ensure
    your app is highly available, you only need to run them through a Deployment resource
    and configure an appropriate number of replicas; everything else is taken care
    of by Kubernetes.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中运行应用程序时，各种控制器确保即使在节点失败的情况下，应用程序也能平稳运行并达到指定的规模。为了确保应用程序高度可用，你只需要通过部署资源运行它们并配置适当数量的副本；其他所有事情都由Kubernetes处理。
- en: Running multiple instances to reduce the likelihood of downtime
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 运行多个实例以降低中断的可能性
- en: This requires your apps to be horizontally scalable, but even if that’s not
    the case in your app, you should still use a Deployment with its replica count
    set to one. If the replica becomes unavailable, it will be replaced with a new
    one quickly, although that doesn’t happen instantaneously. It takes time for all
    the involved controllers to notice that a node has failed, create the new pod
    replica, and start the pod’s containers. There will inevitably be a short period
    of downtime in between.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这要求你的应用程序具有水平扩展性，即使你的应用程序不具备这种特性，你也应该使用一个副本计数设置为1的部署。如果副本变得不可用，它将很快被一个新的副本替换，尽管这并不是瞬间发生的。所有涉及的控制器都需要时间来注意到一个节点已经失败，创建新的Pod副本，并启动Pod的容器。在这之间不可避免地会有一个短暂的中断期。
- en: Using leader-election for non-horizontally scalable apps
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 为非水平扩展应用程序使用领导者选举
- en: To avoid the downtime, you need to run additional inactive replicas along with
    the active one and use a fast-acting lease or leader-election mechanism to make
    sure only one is active. In case you’re unfamiliar with leader election, it’s
    a way for multiple app instances running in a distributed environment to come
    to an agreement on which is the leader. That leader is either the only one performing
    tasks, while all others are waiting for the leader to fail and then becoming leaders
    themselves, or they can all be active, with the leader being the only instance
    performing writes, while all the others are providing read-only access to their
    data, for example. This ensures two instances are never doing the same job, if
    that would lead to unpredictable system behavior due to race conditions.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免中断，你需要运行额外的非活动副本与活动副本一起，并使用快速响应的租约或领导者选举机制来确保只有一个处于活动状态。如果你不熟悉领导者选举，它是一种在分布式环境中运行多个应用程序实例的方式，以便就哪个是领导者达成一致。这个领导者要么是唯一执行任务的实例，而其他所有实例都在等待领导者失败然后自己成为领导者，或者它们都可以是活动的，其中领导者是唯一执行写入操作的实例，而其他所有实例则提供对数据的只读访问，例如。这确保了两个实例永远不会执行相同的工作，如果那样做会导致由于竞争条件而出现不可预测的系统行为。
- en: The mechanism doesn’t need to be incorporated into the app itself. You can use
    a sidecar container that performs all the leader-election operations and signals
    the main container when it should become active. You’ll find an example of leader
    election in Kubernetes at [https://github.com/kubernetes/contrib/tree/master/election](https://github.com/kubernetes/contrib/tree/master/election).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 该机制不需要集成到应用程序本身中。你可以使用一个侧边容器，该容器执行所有领导者选举操作，并在应该变为活动状态时向主容器发出信号。你可以在Kubernetes中找到领导者选举的示例，见[https://github.com/kubernetes/contrib/tree/master/election](https://github.com/kubernetes/contrib/tree/master/election)。
- en: Ensuring your apps are highly available is relatively simple, because Kubernetes
    takes care of almost everything. But what if Kubernetes itself fails? What if
    the servers running the Kubernetes Control Plane components go down? How are those
    components made highly available?
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的应用高度可用相对简单，因为Kubernetes几乎处理了所有事情。但如果是Kubernetes本身失败了怎么办？如果运行Kubernetes控制平面组件的服务器宕机了怎么办？这些组件是如何实现高度可用的？
- en: 11.6.2\. Making Kubernetes Control Plane components highly available
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 11.6.2\. 使Kubernetes控制平面组件高度可用
- en: 'In the beginning of this chapter, you learned about the few components that
    make up a Kubernetes Control Plane. To make Kubernetes highly available, you need
    to run multiple master nodes, which run multiple instances of the following components:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，你学习了组成Kubernetes控制平面的少数几个组件。为了使Kubernetes高度可用，你需要运行多个主节点，这些节点运行以下组件的多个实例：
- en: etcd, which is the distributed data store where all the API objects are kept
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd，这是所有API对象存储的分布式数据存储
- en: API server
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API服务器
- en: Controller Manager, which is the process in which all the controllers run
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制管理器，这是所有控制器运行的进程
- en: Scheduler
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器
- en: Without going into the actual details of how to install and run these components,
    let’s see what’s involved in making each of these components highly available.
    [Figure 11.18](#filepos1135786) shows an overview of a highly available cluster.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入探讨如何安装和运行这些组件的具体细节，让我们看看使每个组件高度可用所涉及的内容。[图11.18](#filepos1135786) 展示了一个高度可用的集群概览。
- en: Figure 11.18\. A highly-available cluster with three master nodes
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18\. 具有三个主节点的高度可用集群
- en: '![](images/00159.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00159.jpg)'
- en: Running an etcd cluster
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 运行一个etcd集群
- en: Because etcd was designed as a distributed system, one of its key features is
    the ability to run multiple etcd instances, so making it highly available is no
    big deal. All you need to do is run it on an appropriate number of machines (three,
    five, or seven, as explained earlier in the chapter) and make them aware of each
    other. You do this by including the list of all the other instances in every instance’s
    configuration. For example, when starting an instance, you specify the IPs and
    ports where the other etcd instances can be reached.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 由于etcd被设计为一个分布式系统，其关键特性之一就是能够运行多个etcd实例，因此使其高度可用并不是什么大问题。你所需要做的就是在一个适当数量的机器上运行它（如本章前面所述的三台、五台或七台）并使它们相互了解。你可以通过在每个实例的配置中包含所有其他实例的列表来实现这一点。例如，在启动一个实例时，你指定其他etcd实例可以访问的IP地址和端口号。
- en: etcd will replicate data across all its instances, so a failure of one of the
    nodes when running a three-machine cluster will still allow the cluster to accept
    both read and write operations. To increase the fault tolerance to more than a
    single node, you need to run five or seven etcd nodes, which would allow the cluster
    to handle two or three node failures, respectively. Having more than seven etcd
    instances is almost never necessary and begins impacting performance.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: etcd将在所有实例之间复制数据，因此当三机集群运行时，一个节点的故障仍然允许集群接受读写操作。为了提高超过单个节点的容错能力，你需要运行五个或七个etcd节点，这分别允许集群处理两个或三个节点故障。拥有超过七个etcd实例几乎是不必要的，并且开始影响性能。
- en: Running multiple instances of the API server
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 运行多个API服务器实例
- en: Making the API server highly available is even simpler. Because the API server
    is (almost completely) stateless (all the data is stored in etcd, but the API
    server does cache it), you can run as many API servers as you need, and they don’t
    need to be aware of each other at all. Usually, one API server is collocated with
    every etcd instance. By doing this, the etcd instances don’t need any kind of
    load balancer in front of them, because every API server instance only talks to
    the local etcd instance.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 使API服务器高可用性甚至更简单。因为API服务器（几乎完全）是无状态的（所有数据都存储在etcd中，但API服务器会缓存它），你可以运行你需要的任何数量的API服务器，它们根本不需要相互了解。通常，每个API服务器都与每个etcd实例一起运行。通过这样做，etcd实例不需要在它们前面有任何类型的负载均衡器，因为每个API服务器实例只与本地etcd实例通信。
- en: The API servers, on the other hand, do need to be fronted by a load balancer,
    so clients (`kubectl`, but also the Controller Manager, Scheduler, and all the
    Kubelets) always connect only to the healthy API server instances.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 与API服务器不同，API服务器确实需要由负载均衡器进行前端处理，因此客户端（`kubectl`，但也包括控制器管理器、调度器和所有Kubelets）始终只连接到健康的API服务器实例。
- en: Ensuring high availability of the controllers and the Scheduler
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 确保控制器和调度器的高可用性
- en: Compared to the API server, where multiple replicas can run simultaneously,
    running multiple instances of the Controller Manager or the Scheduler isn’t as
    simple. Because controllers and the Scheduler all actively watch the cluster state
    and act when it changes, possibly modifying the cluster state further (for example,
    when the desired replica count on a ReplicaSet is increased by one, the ReplicaSet
    controller creates an additional pod), running multiple instances of each of those
    components would result in all of them performing the same action. They’d be racing
    each other, which could cause undesired effects (creating two new pods instead
    of one, as mentioned in the previous example).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 与可以同时运行多个副本的API服务器相比，运行多个控制器管理器或调度器的实例并不简单。因为控制器和调度器都积极监视集群状态，并在状态发生变化时采取行动，可能会进一步修改集群状态（例如，当ReplicaSet期望的副本数增加一个时，ReplicaSet控制器会创建一个额外的Pod），运行这些组件的多个实例会导致它们都执行相同的操作。它们会相互竞争，这可能会导致不期望的效果（如前例中提到的，创建两个而不是一个新Pod）。
- en: For this reason, when running multiple instances of these components, only one
    instance may be active at any given time. Luckily, this is all taken care of by
    the components themselves (this is controlled with the `--leader-elect` option,
    which defaults to true). Each individual component will only be active when it’s
    the elected leader. Only the leader performs actual work, whereas all other instances
    are standing by and waiting for the current leader to fail. When it does, the
    remaining instances elect a new leader, which then takes over the work. This mechanism
    ensures that two components are never operating at the same time and doing the
    same work (see [figure 11.19](#filepos1139712)).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当运行这些组件的多个实例时，在任何给定时间只能有一个实例处于活动状态。幸运的是，所有这些都是由组件本身处理的（这是通过`--leader-elect`选项控制的，默认为true）。每个单独的组件只有在它是选定的领导者时才会处于活动状态。只有领导者执行实际工作，而所有其他实例都处于待命状态，等待当前领导者失败。当它失败时，剩余的实例将选举一个新的领导者，然后接管工作。这种机制确保两个组件永远不会同时操作并执行相同的工作（参见[图11.19](#filepos1139712)）。
- en: Figure 11.19\. Only a single Controller Manager and a single Scheduler are active;
    others are standing by.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.19。只有一个控制器管理器和单个调度器处于活动状态；其他都在待命。
- en: '![](images/00177.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00177.jpg)'
- en: The Controller Manager and Scheduler can run collocated with the API server
    and etcd, or they can run on separate machines. When collocated, they can talk
    to the local API server directly; otherwise they connect to the API servers through
    the load balancer.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器管理器和调度器可以与API服务器和etcd一起运行，或者它们可以运行在不同的机器上。当它们一起运行时，它们可以直接与本地API服务器通信；否则，它们通过负载均衡器连接到API服务器。
- en: Understanding the leader election mechanism used in Control Plane components
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 理解在控制平面组件中使用的领导者选举机制
- en: What I find most interesting here is that these components don’t need to talk
    to each other directly to elect a leader. The leader election mechanism works
    purely by creating a resource in the API server. And it’s not even a special kind
    of resource—the Endpoints resource is used to achieve this (abused is probably
    a more appropriate term).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这里最有趣的是，这些组件不需要直接相互通信来选举领导者。领导者选举机制完全通过在API服务器中创建资源来实现。这甚至不是一种特殊类型的资源——端点资源被用来实现这一点（滥用可能是一个更合适的词）。
- en: There’s nothing special about using an Endpoints object to do this. It’s used
    because it has no side effects as long as no Service with the same name exists.
    Any other resource could be used (in fact, the leader election mechanism will
    soon use ConfigMaps instead of Endpoints).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 使用端点对象（Endpoints object）来完成这个操作并没有什么特别之处。之所以使用它，是因为只要没有同名的服务存在，它就不会产生副作用。任何其他资源都可以使用（实际上，领导选举机制很快将使用ConfigMaps而不是端点）。
- en: I’m sure you’re interested in how a resource can be used for this purpose. Let’s
    take the Scheduler, for example. All instances of the Scheduler try to create
    (and later update) an Endpoints resource called `kube-scheduler`. You’ll find
    it in the `kube-system` namespace, as the following listing shows.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信你对如何使用资源来完成这个目的很感兴趣。以调度器（Scheduler）为例。所有调度器的实例都试图创建（稍后更新）一个名为`kube-scheduler`的端点资源。你可以在`kube-system`命名空间中找到它，如下所示。
- en: Listing 11.11\. The `kube-scheduler` Endpoints resource used for leader-election
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.11. 用于领导选举的`kube-scheduler`端点资源
- en: '`$ kubectl get endpoints kube-scheduler -n kube-system -o yaml` `apiVersion:
    v1 kind: Endpoints metadata:   annotations:     control-plane.alpha.kubernetes.io/leader:
    ''{"holderIdentity":` ![](images/00006.jpg) `"minikube","leaseDurationSeconds":15,"acquireTime":`
    ![](images/00006.jpg) `"2017-05-27T18:54:53Z","renewTime":"2017-05-28T13:07:49Z",`
    ![](images/00006.jpg) `"leaderTransitions":0}''   creationTimestamp: 2017-05-27T18:54:53Z
      name: kube-scheduler   namespace: kube-system   resourceVersion: "654059"  
    selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler   uid: f847bd14-430d-11e7-9720-080027f8fa4e
    subsets: []`'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get endpoints kube-scheduler -n kube-system -o yaml` `apiVersion:
    v1 kind: Endpoints metadata:   annotations:     control-plane.alpha.kubernetes.io/leader:
    ''{"holderIdentity":` ![](images/00006.jpg) `"minikube","leaseDurationSeconds":15,"acquireTime":`
    ![](images/00006.jpg) `"2017-05-27T18:54:53Z","renewTime":"2017-05-28T13:07:49Z",`
    ![](images/00006.jpg) `"leaderTransitions":0}''   creationTimestamp: 2017-05-27T18:54:53Z
      name: kube-scheduler   namespace: kube-system   resourceVersion: "654059"  
    selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler   uid: f847bd14-430d-11e7-9720-080027f8fa4e
    subsets: []`'
- en: The `control-plane.alpha.kubernetes.io/leader` annotation is the important part.
    As you can see, it contains a field called `holderIdentity`, which holds the name
    of the current leader. The first instance that succeeds in putting its name there
    becomes the leader. Instances race each other to do that, but there’s always only
    one winner.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '`control-plane.alpha.kubernetes.io/leader`注解是重要的部分。正如你所见，它包含一个名为`holderIdentity`的字段，该字段包含当前领导者的名称。第一个成功将其名称放入那里的实例将成为领导者。实例们相互竞争以完成这项任务，但总是只有一个赢家。'
- en: Remember the optimistic concurrency we explained earlier? That’s what ensures
    that if multiple instances try to write their name into the resource only one
    of them succeeds. Based on whether the write succeeded or not, each instance knows
    whether it is or it isn’t the leader.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们之前解释的乐观并发吗？那就是确保如果多个实例尝试将它们的名称写入资源，只有其中一个能够成功。根据写入是否成功，每个实例都知道自己是否是领导者。
- en: Once becoming the leader, it must periodically update the resource (every two
    seconds by default), so all other instances know that it’s still alive. When the
    leader fails, other instances see that the resource hasn’t been updated for a
    while, and try to become the leader by writing their own name to the resource.
    Simple, right?
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成为领导者，它必须定期更新资源（默认情况下每两秒更新一次），这样所有其他实例都知道它仍然活跃。当领导者失败时，其他实例会看到资源已经有一段时间没有更新了，并尝试通过将自己的名称写入资源来成为领导者。简单，对吧？
- en: 11.7\. Summary
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 11.7. 摘要
- en: Hopefully, this has been an interesting chapter that has improved your knowledge
    of the inner workings of Kubernetes. This chapter has shown you
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这章内容有趣，并且有助于提高你对Kubernetes内部运作的了解。本章展示了
- en: What components make up a Kubernetes cluster and what each component is responsible
    for
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes集群由哪些组件组成以及每个组件负责什么
- en: How the API server, Scheduler, various controllers running in the Controller
    Manager, and the Kubelet work together to bring a pod to life
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API服务器、调度器、在控制器管理器中运行的各个控制器以及Kubelet是如何协同工作，使Pod得以启动
- en: How the infrastructure container binds together all the containers of a pod
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施容器是如何将一个Pod中的所有容器绑定在一起的
- en: How pods communicate with other pods running on the same node through the network
    bridge, and how those bridges on different nodes are connected, so pods running
    on different nodes can talk to each other
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod是如何通过网络桥接与其他节点上运行的Pod进行通信的，以及这些不同节点上的桥接是如何连接的，这样不同节点上的Pod就可以互相通信
- en: How the kube-proxy performs load balancing across pods in the same service by
    configuring `iptables` rules on the node
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kube-proxy如何通过在节点上配置`iptables`规则，在同一个服务中的Pod之间执行负载均衡
- en: How multiple instances of each component of the Control Plane can be run to
    make the cluster highly available
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何运行每个控制平面组件的多个实例以使集群具有高可用性
- en: Next, we’ll look at how to secure the API server and, by extension, the cluster
    as a whole.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何保护API服务器，以及如何通过扩展保护整个集群。
