- en: 18 Sequence-to-sequence models for chatbots
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18 为聊天机器人构建序列到序列模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Examining sequence-to-sequence architecture
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查序列到序列架构
- en: Performing vector embedding of words
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行单词的向量嵌入
- en: Implementing a chatbot by using real-world data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实世界数据实现聊天机器人
- en: Talking to customer service over the phone is a burden for both the customer
    and the company. Service providers pay a good chunk of money to hire customer
    service representatives, but what if it’s possible to automate most of this effort?
    Can we develop software to interface with customers through natural language?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过电话与客户服务交谈对客户和公司来说都是一种负担。服务提供商支付了一大笔钱来雇佣客户服务代表，但如果我们能够自动化大部分这项工作怎么办？我们能否开发出通过自然语言与客户交互的软件？
- en: The idea isn’t as far-fetched as you might think. Chatbots are getting a lot
    of hype because of unprecedented developments in natural language processing using
    deep-learning techniques. Perhaps, given enough training data, a chatbot could
    learn to navigate the most commonly addressed customer problems through natural
    conversations. If the chatbot were truly efficient, it could not only save the
    company money by eliminating the need to hire representatives, but also accelerate
    the customer’s search for an answer.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法并不像你想象的那么遥远。由于深度学习技术在自然语言处理方面的前所未有的发展，聊天机器人受到了很多炒作。也许，给定足够的训练数据，聊天机器人可以学会通过自然对话来导航最常见的客户问题。如果聊天机器人真正高效，它不仅可以通过消除雇佣代表的需求来为公司节省资金，还可以加速客户寻找答案的过程。
- en: In this chapter, you’ll build a chatbot by feeding a neural network thousands
    of examples of input and output sentences. Your training dataset is a pair of
    English utterances. If you ask “How are you?” for example, the chatbot should
    respond, “Fine, thank you.”
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将通过向神经网络提供成千上万的输入和输出句子示例来构建一个聊天机器人。你的训练数据集是一对英语话语。例如，如果你问“你好吗？”，聊天机器人应该回答，“很好，谢谢。”
- en: Note In this chapter, we’re thinking of *sequences* and *sentences* as interchangeable
    concepts. In our implementation, a sentence will be a sequence of letters. Another
    common approach is to represent a sentence as a sequence of words.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，我们将*序列*和*句子*视为可互换的概念。在我们的实现中，一个句子将是一系列字母的序列。另一种常见的方法是将句子表示为一系列单词的序列。
- en: 'In effect, the algorithm will try to produce an intelligent natural language
    response to each natural language query. You’ll be implementing a neural network
    that uses two primary concepts taught in previous chapters: multiclass classification
    and recurrent neural networks (RNNs).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，该算法将尝试为每个自然语言查询生成一个智能的自然语言响应。你将实现一个神经网络，它使用了前几章教授的两个主要概念：多类分类和循环神经网络（RNNs）。
- en: 18.1 Building on classification and RNNs
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.1 建立在分类和RNNs之上
- en: '*Classification* is a machine-learning approach to predict the category of
    an input data item. Furthermore, multiclass classification allows for more than
    two classes. You saw in chapter 6 how to implement such an algorithm in TensorFlow.
    Specifically, the cost function between the model’s prediction (a sequence of
    numbers) and the ground truth (a one-hot vector) tries to find the distance between
    two sequences by using the cross-entropy loss.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类*是一种机器学习方法，用于预测输入数据项的类别。此外，多类分类允许超过两个类别。你在第6章中看到了如何在TensorFlow中实现这样的算法。具体来说，模型预测（一个数字序列）与真实值（一个one-hot向量）之间的成本函数试图通过交叉熵损失来找到两个序列之间的距离。'
- en: Note A one-hot vector is like an all-zero vector, except that one of the dimensions
    has a value of `1`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：one-hot向量就像一个全零向量，除了其中一个维度有一个值为`1`。
- en: 'In this case, implementing a chatbot, you’ll use a variant of the cross-entropy
    loss to measure the difference between two sequences: the model’s response (which
    is a sequence) against the ground truth (which is also a sequence).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，实现一个聊天机器人，你会使用交叉熵损失函数的变体来衡量两个序列之间的差异：模型的响应（这是一个序列）与真实值（也是一个序列）。
- en: Exercise 18.1
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 练习18.1
- en: In TensorFlow, you can use the cross-entropy loss function to measure the similarity
    between a one-hot vector, such as (1, 0, 0), and a neural network’s output, such
    as (2.34, 0.1, 0.3). On the other hand, English sentences aren’t numeric vectors.
    How can you use the cross-entropy loss to measure the similarity between English
    sentences?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，你可以使用交叉熵损失函数来衡量一个one-hot向量，例如（1，0，0），与神经网络输出，例如（2.34，0.1，0.3）之间的相似度。另一方面，英文句子不是数值向量。你如何使用交叉熵损失来衡量英文句子之间的相似度？
- en: '**Answer**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: A crude approach would be to represent each sentence as a vector by counting
    the frequency of each word within the sentence. Then compare the vectors to see
    how closely they match up.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一种粗略的方法是将每个句子表示为一个向量，通过计算句子中每个单词的频率来实现。然后比较这些向量以查看它们匹配得有多接近。
- en: You may recall that RNNs are a neural network design for incorporating not only
    input from the current time step, but also state information from previous inputs.
    Chapters 16 and 17 covered these networks in great detail, and they’ll be used
    again in this chapter. RNNs represent input and output as time-series data, which
    is exactly what you need to represent sequences.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，RNNs 是一种神经网络设计，它不仅能够结合当前时间步的输入，还能结合之前输入的状态信息。第 16 章和第 17 章详细介绍了这些网络，它们将在本章再次被使用。RNNs
    将输入和输出表示为时间序列数据，这正是你需要用来表示序列的。
- en: A naïve idea is to use an out-of-the-box RNN to implement a chatbot. Let’s see
    why this is a bad approach. The input and output of the RNN are natural language
    sentences, so the inputs (*x*[t], *x*[t-1], *x*[t-2], ...) and outputs (*y*[t],
    *y*[t-1], *y*[t-2], ...) can be sequences of words. The problem in using an RNN
    to model conversations is that the RNN produces an output result immediately.
    If your input is a sequence of words (*How*, *are*, *you*), the first output word
    will depend on only the first input word. The output sequence item *y*[t] of the
    RNN couldn’t look ahead to future parts of the input sentence to make a decision;
    it would be limited by knowledge of only previous input sequences (*x*[t], *x*[t-1],
    *x*[t-2], ... ). The naïve RNN model tries to come up with a response to the user’s
    query before they’ve finished asking it, which can lead to incorrect results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个天真想法是使用现成的 RNN 来实现聊天机器人。让我们看看为什么这是一个不好的方法。RNN 的输入和输出是自然语言句子，所以输入 (*x*[t]，*x*[t-1]，*x*[t-2]，...)
    和输出 (*y*[t]，*y*[t-1]，*y*[t-2]，...) 可以是单词序列。使用 RNN 来模拟对话的问题在于 RNN 会立即产生输出结果。如果你的输入是一个单词序列
    (*How*，*are*，*you*)，第一个输出单词将只取决于第一个输入单词。RNN 的输出序列项 *y*[t] 不能向前查看输入句子的未来部分来做出决定；它将仅限于了解之前的输入序列
    (*x*[t]，*x*[t-1]，*x*[t-2]，... )。天真的 RNN 模型试图在用户完成提问之前就给出一个响应，这可能导致错误的结果。
- en: 'Instead, you’ll use two RNNs: one for the input sentence and the other for
    the output sequence. When the first RNN finished processing the input sequence,
    it’ll send the hidden state to the second RNN to process the output sentence.
    You can see the two RNNs, labeled Encoder and Decoder, in figure 18.1.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你将使用两个 RNN：一个用于输入句子，另一个用于输出序列。当第一个 RNN 完成输入序列的处理后，它将隐藏状态发送到第二个 RNN 以处理输出句子。你可以在图
    18.1 中看到这两个 RNN，分别标记为编码器（Encoder）和解码器（Decoder）。
- en: '![CH18_F01_Mattmann2](../Images/CH18_F01_Mattmann2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F01_Mattmann2](../Images/CH18_F01_Mattmann2.png)'
- en: Figure 18.1 Here’s a high-level view of your neural network model. The input
    ayy is passed into the encoder RNN, and the decoder RNN is expected to respond
    with lmao. These examples are toy examples for your chatbot, but you could imagine
    more-complicated pairs of sentences for the input and output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.1 这是您神经网络模型的高级视图。输入 ayy 被传递到编码器 RNN，解码器 RNN 预期会响应 lmao。这些例子是您聊天机器人的玩具例子，但你可以想象更复杂的输入和输出句子对。
- en: We’re bringing concepts of multiclass classification and RNNs from previous
    chapters into designing a neural network that learns to map an input sequence
    to an output sequence. The RNNs provide a way of encoding the input sentence,
    passing a summarized state vector to the decoder, and then decoding it to a response
    sentence. To measure the cost between the model’s response and the ground truth,
    we look to the function used in multiclass classifica-tion—cross-entropy loss—for
    inspiration.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从前几章中引入多类分类和 RNN 的概念，来设计一个学习将输入序列映射到输出序列的神经网络。RNNs 提供了一种编码输入句子、将总结的状态向量传递到解码器，然后将其解码为响应句子的方法。为了衡量模型响应与真实值之间的成本，我们借鉴了多类分类中使用的函数——交叉熵损失。
- en: This architecture is called a *sequence-to-sequence (seq2seq)* neural network
    architecture. The training data you use will be thousands of pairs of sentences
    mined from movie scripts. The algorithm will observe these dialogue examples and
    eventually learn to form responses to arbitrary queries you might ask it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构被称为 *序列到序列（seq2seq）* 神经网络架构。你使用的训练数据将是成千上万对从电影剧本中挖掘出的句子。算法将观察这些对话示例，并最终学会对您可能提出的任意查询形成响应。
- en: Exercise 18.2
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 18.2
- en: What other industries could benefit from a chatbot?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些其他行业可以从聊天机器人中受益？
- en: '**Answer**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: One example is a conversation partner for young students as a tool to teach
    subjects such as English, math, and even computer science.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是作为教学工具的对话伙伴，用于教授英语、数学甚至计算机科学等科目。
- en: By the end of the chapter, you’ll have your own chatbot that can respond somewhat
    intelligently to your queries. It won’t be perfect, because this model always
    responds the same way for the same input query.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将拥有自己的聊天机器人，它可以对你的查询做出一定程度的智能响应。它不会完美，因为这个模型总是对相同的输入查询以相同的方式做出响应。
- en: 'Suppose that you’re traveling to a foreign country without any ability to speak
    the language. A clever salesman hands you a book, claiming that it’s all you need
    to respond to sentences in the foreign language. You’re supposed to use the book
    like a dictionary. When someone says a phrase in the foreign language, you can
    look it up, and the book will have the response written out for you to read aloud:
    “If someone says *Hello*, you say *Hi.*”'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在前往一个外国旅行，而你没有任何说这种语言的能力。一个聪明的销售人员给你一本书，声称这是你需要用来回应外语句子的所有东西。你被要求像字典一样使用这本书。当有人用外语说一个短语时，你可以查找它，书中将为你准备好要大声读出的回答：“如果有人说*你好*，你就说*嗨*。”
- en: Sure, this book might be a practical lookup table for small talk, but can a
    lookup table get you the correct response for arbitrary dialogue? Of course not!
    Consider looking up the question “Are you hungry?” The answer is stamped in the
    book and will never change.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这本书可能是一个实用的查找表，用于日常闲聊，但查找表能为你提供任意对话的正确响应吗？当然不能！考虑查找问题“你饿吗？”答案在书中，永远不会改变。
- en: The lookup table is missing state information, which is a key component in dialogue.
    In your seq2seq model, you’ll suffer from a similar issue, but the model is a
    good start. Believe it or not, at present, hierarchical state representation for
    intelligent dialogue still isn’t the norm; many chatbots start with these seq2seq
    models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 查找表中缺少状态信息，这是对话中的一个关键组件。在你的seq2seq模型中，你将面临类似的问题，但这个模型是一个良好的起点。信不信由你，目前，用于智能对话的分层状态表示还不是标准；许多聊天机器人都是从这些seq2seq模型开始的。
- en: 18.2 Understanding seq2seq architecture
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.2 理解seq2seq架构
- en: The seq2seq model attempts to learn a neural network that predicts an output
    sequence from an input sequence. Sequences are a little different from traditional
    vectors because a sequence implies an ordering of events.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq模型试图学习一个神经网络，从输入序列预测输出序列。序列与传统向量有一点不同，因为序列暗示了事件的顺序。
- en: 'Time is an intuitive way to order events: we usually end up alluding to words
    related to time, such as *temporal*, *time* *series*, *past*, and *future*. We
    like to say that RNNs propagate information to *future* *time* steps, for example,
    or that RNNs capture *temporal dependencies*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 时间是排序事件的直观方式：我们通常都会提到与时间相关的词汇，如*时间*、*时间序列*、*过去*和*未来*。我们喜欢说RNN将信息传播到*未来*时间步，例如，或者RNN捕获*时间依赖性*。
- en: note RNNs are covered in detail in chapter 16.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: note RNNs在第16章中有详细讲解。
- en: The seq2seq model is implemented with multiple RNNs. A single RNN cell is depicted
    in figure 18.2; it serves as the building block for the rest of the seq2seq model
    architecture.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq模型使用多个RNN实现。单个RNN单元如图18.2所示；它是seq2seq模型架构其余部分的构建块。
- en: '![CH18_F02_Mattmann2](../Images/CH18_F02_Mattmann2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F02_Mattmann2](../Images/CH18_F02_Mattmann2.png)'
- en: Figure 18.2 The input, output, and states of an RNN. You can ignore the intricacies
    of how an RNN is implemented. All that matters is the formatting of your input
    and output.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2 RNN的输入、输出和状态。你可以忽略RNN实现细节的复杂性。重要的是你的输入和输出的格式。
- en: First, you’ll learn how to stack RNNs to improve the model’s complexity. Then
    you’ll learn how to pipe the hidden state of one RNN to another RNN so that you
    can have an encoder and decoder network. As you’ll begin to see, starting to use
    RNNs is fairly easy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将学习如何堆叠RNN来提高模型复杂性。然后你将学习如何将一个RNN的隐藏状态传递给另一个RNN，以便你可以有一个编码器和解码器网络。正如你将开始看到的那样，开始使用RNN相对容易。
- en: After that, you’ll get an introduction to converting natural language sentences
    to a sequence of vectors. After all, RNNs understand only numeric data, so you’ll
    absolutely need this conversion process. Because a *sequence* is another way of
    saying “a list of tensors,” you need to make sure that you can convert your data
    accordingly. A sentence is a sequence of words, but words aren’t tensors. The
    process of converting words to tensors or, more commonly, vectors is called *embedding*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你将了解将自然语言句子转换为向量序列的介绍。毕竟，RNN 只能理解数值数据，所以你绝对需要这个转换过程。因为 *序列* 另一种说法是“张量列表”，你需要确保你可以相应地转换你的数据。句子是一系列单词，但单词不是张量。将单词转换为张量或更常见的是向量的过程称为
    *嵌入*。
- en: Last, you’ll put all these concepts together to implement the seq2seq model
    on real-world data. The data will come from thousands of conversations from movie
    scripts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将把这些概念结合起来，在真实世界数据上实现 seq2seq 模型。数据将来自数千个电影剧本的对话。
- en: You can hit the ground running with the code in listing 18.1\. Open a new Python
    file, and start copying the listing code to set up constants and placeholders.
    You’ll define the shape of the placeholder as `[None, seq_size, input_dim]`, where
    `None` means that the size is dynamic (the batch size may change), `seq_size`
    is the length of the sequence, and `input_dim` is the dimension of each sequence
    item.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接使用列表 18.1 中的代码开始运行。打开一个新的 Python 文件，并开始复制列表代码以设置常量和占位符。你将定义占位符的形状为 `[None,
    seq_size, input_dim]`，其中 `None` 表示大小是动态的（批处理大小可能会变化），`seq_size` 是序列的长度，`input_dim`
    是每个序列项的维度。
- en: Listing 18.11 Setting up constants and placeholders
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.11 设置常量和占位符
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ All you need is TensorFlow.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所需的一切就是 TensorFlow。
- en: ❷ Dimension of each sequence element
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每个序列元素的维度
- en: ❸ Maximum length of sequence
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 序列的最大长度
- en: To generate an RNN cell like the one in figure 18.2, TensorFlow provides a helpful
    `LSTMCell` class. Listing 18.2 shows how to use this class and extract the outputs
    and states from the cell. For convenience, the listing defines a helper function
    called `make_cell` to set up the LSTM RNN cell. Defining a cell isn’t enough,
    however; you also need to call `tf.nn.dynamic_rnn` on it to set up the network;
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成如图 18.2 所示的 RNN 单元，TensorFlow 提供了一个有用的 `LSTMCell` 类。列表 18.2 展示了如何使用这个类并从单元中提取输出和状态。为了方便，列表定义了一个名为
    `make_cell` 的辅助函数来设置 LSTM RNN 单元。然而，定义一个单元还不够；你还需要调用 `tf.nn.dynamic_rnn` 来设置网络；
- en: Listing 18.2 Making a simple RNN cell
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.2 创建一个简单的 RNN 单元
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Check out the tf.contrib.rnn documentation for other types of cells, such
    as GRU.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 查看tf.contrib.rnn文档了解其他类型的单元，例如 GRU。
- en: '❷ Two results will be generated: outputs and states.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将生成两个结果：输出和状态。
- en: ❸ Input sequence to the RNN
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输入到 RNN 的序列
- en: You may remember from previous chapters that you can improve a neural network’s
    complexity by adding hidden layers. More layers means more parameters, which likely
    means that the model can represent more functions because it’s more flexible.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，从前几章中你可以通过添加隐藏层来提高神经网络的复杂性。更多的层意味着更多的参数，这很可能意味着模型可以表示更多的函数，因为它更灵活。
- en: You know what? You can stack cells. Nothing is stopping you. Doing so makes
    the model more complex, so perhaps this two-layered RNN model will perform better
    because it’s more expressive. Figure 18.3 shows two cells stacked.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗？你可以堆叠单元。没有什么可以阻止你这样做。这样做会使模型更复杂，因此这个双层 RNN 模型可能会表现得更好，因为它更具表现力。图 18.3 显示了两个堆叠的单元。
- en: '![CH18_F03_Mattmann2](../Images/CH18_F03_Mattmann2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F03_Mattmann2](../Images/CH18_F03_Mattmann2.png)'
- en: Figure 18.3 You can stack RNN cells to form a more complicated architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3 你可以将 RNN 单元堆叠以形成一个更复杂的架构。
- en: Warning The more flexible the model is, the more likely it’ll be to overfit
    the training data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：模型越灵活，越有可能过拟合训练数据。
- en: In TensorFlow, you can intuitively implement this two-layered RNN network. First,
    you create a new variable scope for the second cell. To stack RNNs, you can pipe
    the output of the first cell to the input of the second cell, as shown in listing
    18.3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以直观地实现这个双层 RNN 网络。首先，为第二个单元创建一个新的变量作用域。为了堆叠 RNN，你可以将第一个单元的输出管道连接到第二个单元的输入，如列表
    18.3 所示。
- en: Listing 18.3 Stacking two RNN cells
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.3 堆叠两个 RNN 单元
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Defining a variable scope helps prevent runtime errors due to variable reuse.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义变量作用域有助于防止由于变量重用导致的运行时错误。
- en: ❷ Input to this cell will be the other cell’s output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入到这个单元将是另一个单元的输出。
- en: What if you want four layers of RNNs? Figure 18.4 shows four RNN cells stacked.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想有四层的RNN？图18.4显示了堆叠了四个RNN单元。
- en: '![CH18_F04_Mattmann2](../Images/CH18_F04_Mattmann2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F04_Mattmann2](../Images/CH18_F04_Mattmann2.png)'
- en: Figure 18.4 TensorFlow lets you stack as many RNN cells as you want.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4 TensorFlow允许你堆叠任意数量的RNN单元。
- en: The TensorFlow library provides a useful shortcut for stacking cells called
    `MultiRNNCell`. Listing 18.4 shows how to use this helper function to build arbitrarily
    large RNN cells.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow库提供了一个有用的快捷方式来堆叠单元，称为`MultiRNNCell`。列表18.4展示了如何使用这个辅助函数构建任意大的RNN单元。
- en: Listing 18.4 Using `MultiRNNCell` to stack multiple cells
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.4 使用`MultiRNNCell`堆叠多个单元
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The for-loop syntax is the preferred way to construct a list of RNN cells.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ for循环语法是构建RNN单元列表的首选方式。
- en: So far, you’ve grown RNNs vertically by piping outputs of one cell to the inputs
    of another. In the seq2seq model, you’ll want one RNN cell to process the input
    sentence and another RNN cell to process the output sentence. To communicate between
    the two cells, you can also connect RNNs horizontally by connecting states from
    cell to cell, as shown in figure 18.5.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经通过将一个单元的输出管道到另一个单元的输入来垂直增长RNN。在seq2seq模型中，你将想要一个RNN单元来处理输入句子，另一个RNN单元来处理输出句子。为了在两个单元之间进行通信，你还可以通过连接单元的状态来水平连接RNN，如图18.5所示。
- en: You’ve stacked RNN cells vertically and connected them horizontally, vastly
    increasing the number of parameters in the network. Is what you’ve done utter
    blasphemy? Yes. You’ve built a monolithic architecture by composing RNNs every
    which way. But there’s a method to this madness, because this insane neural network
    architecture is the backbone of the seq2seq model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经垂直堆叠了RNN单元并将它们水平连接，大大增加了网络中的参数数量。你所做的是不是亵渎神灵？是的。你通过以各种方式组合RNN来构建了一个单体架构。但这种方法并非毫无道理，因为这个疯狂的人工神经网络架构是seq2seq模型的核心。
- en: '![CH18_F05_Mattmann2](../Images/CH18_F05_Mattmann2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F05_Mattmann2](../Images/CH18_F05_Mattmann2.png)'
- en: Figure 18.5 You can use the last states of the first cell as the next cell’s
    initial state. This model can learn mapping from an input sequence to an output
    sequence.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.5 你可以使用第一个单元的最后状态作为下一个单元的初始状态。这个模型可以学习从输入序列到输出序列的映射。
- en: As you can see in figure 18.5, the seq2seq model appears to have two input sequences
    and two output sequences. But only input 1 will be used for the input sentence,
    and only output 2 will be used for the output sentence.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如图18.5所示，seq2seq模型似乎有两个输入序列和两个输出序列。但只有输入1将被用于输入句子，只有输出2将被用于输出句子。
- en: You may be wondering what to do with the other two sequences. Strangely enough,
    the output 1 sequence is entirely unused by the seq2seq model. And as you’ll see,
    the input 2 sequence is crafted with some output 2 data in a feedback loop.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道如何处理其他两个序列。奇怪的是，输出1序列在seq2seq模型中完全未被使用。而且正如你将看到的，输入2序列通过一些输出2数据在反馈循环中精心制作。
- en: Your training data for designing a chatbot will be pairs of input and output
    sentences, so you’ll need to better understand how to embed words in a tensor.
    Section 18.3 covers how to do so in TensorFlow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 设计聊天机器人的训练数据将是输入和输出句子的配对，因此你需要更好地理解如何在张量中嵌入单词。第18.3节介绍了在TensorFlow中如何这样做。
- en: Exercise 18.3
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 练习18.3
- en: Sentences may be represented by a sequence of characters or words, but can you
    think of other sequential representations of sentences?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 句子可以表示为字符或单词的序列，但你能否想到其他句子的顺序表示？
- en: '**Answer**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Both phrases and grammatical information (verbs, nouns, and so forth) could
    be used. More frequently, real applications use natural language processing (NLP)
    lookups to standardize word forms, spellings, and meanings. One example of a library
    that does this translation is fastText from Facebook ([https://github.com/facebookresearch/
    fastText](https://github.com/facebookresearch/fastText)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 两者短语和语法信息（动词、名词等）都可以使用。更常见的是，实际应用使用自然语言处理（NLP）查找来标准化单词形式、拼写和含义。一个执行此转换的库示例是Facebook的fastText
    ([https://github.com/facebookresearch/ fastText](https://github.com/facebookresearch/fastText))。
- en: 18.3 Vector representation of symbols
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.3符号的向量表示
- en: 'Words and letters are symbols, and converting symbols to numeric values is
    easy in TensorFlow. Suppose that you have four words in your vocabulary: word[0]:
    *the* ; word[1]: *fight*; word[2]: *wind*; and word[3]: *like*.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '单词和字母是符号，在TensorFlow中将符号转换为数值很容易。假设你的词汇表中有四个单词：word[0]: *the* ; word[1]: *fight*;
    word[2]: *wind*; 和 word[3]: *like*。'
- en: Now suppose that you want to find the embeddings for the sentence “Fight the
    wind.” The symbol *fight* is located at index 1 of the lookup table, *the* at
    index 0, and *wind* at index 2\. If you want to find the embedding of the word
    *fight*, you have to refer to its index, which is 1, and consult the lookup table
    at index 1 to identify the embedding value. In the first example, each word is
    associated with a number, as shown in figure 18.6.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你想找到句子“Fight the wind.”的嵌入表示。符号*fight*位于查找表中的索引1，*the*位于索引0，而*wind*位于索引2。如果你想找到单词*fight*的嵌入表示，你必须参考它的索引，即1，并咨询索引1处的查找表以识别嵌入值。在第一个例子中，每个单词都与一个数字相关联，如图18.6所示。
- en: '![CH18_F06_Mattmann2](../Images/CH18_F06_Mattmann2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F06_Mattmann2](../Images/CH18_F06_Mattmann2.png)'
- en: Figure 18.6 A mapping from symbols to scalars
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.6 从符号到标量的映射
- en: 'The following snippet shows how to define such a mapping between symbols and
    numeric values with TensorFlow code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段展示了如何使用TensorFlow代码定义符号与数值之间的这种映射：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Or maybe the words are associated with vectors, as shown in figure 18.7\. This
    method is often the preferred way of representing words. You can find a thorough
    tutorial on vector representation of words in the official TensorFlow docs: [http://mng.bz/35M8](http://mng.bz/35M8).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，单词可能与向量相关联，如图18.7所示。这种方法通常是表示单词的首选方式。你可以在官方TensorFlow文档中找到一个关于单词向量表示的详细教程：[http://mng.bz/35M8](http://mng.bz/35M8)。
- en: '![CH18_F07_Mattmann2](../Images/CH18_F07_Mattmann2.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F07_Mattmann2](../Images/CH18_F07_Mattmann2.png)'
- en: Figure 18.7 A mapping from symbols to vectors
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7 从符号到向量的映射
- en: You can implement the mapping between words and vectors in TensorFlow, as shown
    in listing 18.5.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在TensorFlow中实现单词与向量之间的映射，如图18.5所示。
- en: Listing 18.5 Defining a lookup table of 4D vectors
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.5 定义4D向量的查找表
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This may sound over the top, but you can represent a symbol by a tensor of any
    rank you want, not numbers (rank 0) or vectors (rank 1) alone. In figure 18.8,
    you’re mapping symbols to tensors of rank 2.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有些过分，但你可以用任何你想要的秩的张量来表示一个符号，而不仅仅是数字（秩0）或向量（秩1）。在图18.8中，你将符号映射到秩为2的张量。
- en: '![CH18_F08_Mattmann2](../Images/CH18_F08_Mattmann2.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F08_Mattmann2](../Images/CH18_F08_Mattmann2.png)'
- en: Figure 18.8 A mapping from symbols to tensors
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.8 从符号到张量的映射
- en: Listing 18.6 shows how to implement this mapping of words to tensors in TensorFlow.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.6展示了如何在TensorFlow中实现这种将单词映射到张量的方法。
- en: Listing 18.6 Defining a lookup table of tensors
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.6 定义张量查找表
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `embedding_lookup` function provided by TensorFlow is an optimized way to
    access embeddings by indices, as shown in listing 18.7.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供的`embedding_lookup`函数是一种通过索引访问嵌入的优化方式，如列表18.7所示。
- en: Listing 18.7 Looking up the embeddings
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.7 查找嵌入
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Embeddings lookup corresponding to the words fight, the, and wind
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对应于单词fight、the和wind的嵌入查找
- en: In reality, the embedding matrix isn’t something you ever have to hardcode.
    These listings are provided so that you can understand the ins and outs of the
    `embedding_ lookup` function in TensorFlow, because you’ll be using it heavily
    soon. The embedding lookup table will be learned automatically over time by training
    the neural network. You start by defining a random, normally distributed lookup
    table. Then TensorFlow’s optimizer will adjust the matrix values to minimize the
    cost.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，嵌入矩阵并不是你需要硬编码的东西。这些列表提供是为了让你理解TensorFlow中`embedding_lookup`函数的细节，因为你很快就会大量使用它。嵌入查找表将通过训练神经网络自动随时间学习。你首先定义一个随机、正态分布的查找表。然后TensorFlow的优化器将调整矩阵值以最小化成本。
- en: Exercise 18.4
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 练习18.4
- en: Follow the official TensorFlow word2vec tutorial at [www.tensorflow.org/tutorials/
    word2vec](http://www.tensorflow.org/tutorials/word2vec) to get more familiar with
    embeddings.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 按照官方TensorFlow word2vec教程[www.tensorflow.org/tutorials/word2vec](http://www.tensorflow.org/tutorials/word2vec)来熟悉嵌入表示。
- en: '**Answer**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: This tutorial will teach you to visualize the embeddings by using TensorBoard
    and TensorFlow.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将教会你如何使用TensorBoard和TensorFlow来可视化嵌入表示。
- en: 18.4 Putting it all together
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.4 整合所有内容
- en: The first step in using natural language input in a neural network is deciding
    on a mapping between symbols and integer indices. Two common ways to represent
    sentences are a sequence of *letters* and a sequence of *words*. Let’s say, for
    simplicity, that you’re dealing with sequences of letters, so you’ll need to build
    a mapping between characters and integer indices.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中使用自然语言输入的第一步是确定符号与整数索引之间的映射。表示句子的两种常见方式是字母序列和单词序列。为了简单起见，假设你处理的是字母序列，因此你需要构建字符与整数索引之间的映射。
- en: Note The official code repository is available on the book’s website ([http://
    mng.bz/emeQ](http://mng.bz/emeQ)) and GitHub ([http://mng.bz/pz8z](http://mng.bz/pz8z)).
    From there, you can get the code running without needing to copy and paste from
    the book.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：官方代码仓库可在本书网站（[http://mng.bz/emeQ](http://mng.bz/emeQ)）和 GitHub（[http://mng.bz/pz8z](http://mng.bz/pz8z)）上找到。从那里，你可以获取代码运行，无需从书中复制粘贴。
- en: Listing 18.8 shows how to build mappings between integers and characters. If
    you feed this function a list of strings, it’ll produce two dictionaries, representing
    the mappings.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.8 展示了如何建立整数和字符之间的映射。如果你向这个函数提供一个字符串列表，它将生成两个字典，表示映射。
- en: Listing 18.8 Extracting character vocab
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.8 提取字符词汇表
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ List of input sentences for training
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练输入句子列表
- en: ❷ List of corresponding output sentences for training
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对应的训练输出句子列表
- en: Next, you’ll define all your hyperparameters and constants in listing 18.9\.
    These elements usually are values that you can tune by hand through trial and
    error. Typically, greater values for the number of dimensions or layers result
    in a more complex model, which is rewarding if you have big data, fast processing
    power, and lots of time.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将在列表 18.9 中定义所有超参数和常量。这些元素通常是你可以通过试错手动调整的值。通常，维度或层数的更大值会导致更复杂的模型，如果你有大量数据、快速处理能力和大量时间，这将是有益的。
- en: Listing 18.9 Defining hyperparameters
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.9 定义超参数
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Number of epochs
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练轮数
- en: ❷ RNN’s hidden dimension size
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ RNN 的隐藏维度大小
- en: ❸ RNN’s number of stacked cells
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ RNN 的堆叠单元数量
- en: ❹ Embedding dimension of sequence elements for the encoder and decoder
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 编码器和解码器序列元素的嵌入维度
- en: ❺ Batch size
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 批处理大小
- en: ❻ It’s possible to have different vocabularies between the encoder and decoder.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 编码器和解码器之间可能具有不同的词汇表。
- en: Next, list all placeholders. As you can see in listing 18.10, the placeholders
    nicely organize the input and output sequences necessary to train the network.
    You’ll have to track both the sequences and their lengths. For the decoder part,
    you’ll also need to compute the maximum sequence length. The `None` value in the
    shape of these placeholders means that the tensor may take on an arbitrary size
    in that dimension. The batch size may vary in each run, for example. But for simplicity,
    you’ll keep the batch size the same at all times.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，列出所有占位符。如列表 18.10 所示，占位符很好地组织了训练网络所需的输入和输出序列。你需要跟踪序列及其长度。对于解码器部分，你还需要计算最大序列长度。这些占位符形状中的
    `None` 值表示张量可以在该维度上具有任意大小。例如，批处理大小可能在每次运行中变化。但为了简单起见，你将始终保持批处理大小不变。
- en: Listing 18.10 Listing placeholders
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.10 列表占位符
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Sequence of integers for the encoder’s input
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 编码器输入的整数序列
- en: ❷ Shape is batch-size × sequence length.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 形状是批处理大小 × 序列长度。
- en: ❸ Lengths of sequences in a batch
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 批处理中序列的长度
- en: ❹ Shape is dynamic because the length of a sequence can change.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 形状是动态的，因为序列的长度可以改变。
- en: ❺ Sequence of integers for the decoder’s output
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 解码器输出的整数序列
- en: ❻ Shape is batch-size × sequence length.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 形状是批处理大小 × 序列长度。
- en: ❼ Lengths of sequences in a batch
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 批处理中序列的长度
- en: ❽ Shape is dynamic because the length of a sequence can change.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 形状是动态的，因为序列的长度可以改变。
- en: ❾ Maximum length of a decoder sequence in a batch
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 批处理中解码器序列的最大长度
- en: Let’s define helper functions to construct RNN cells. These functions, shown
    in listing 18.11, should appear familiar to you from section 18.3.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义辅助函数来构建 RNN 单元。这些函数，如列表 18.11 所示，应该对你来说在 18.3 节中已经熟悉了。
- en: Listing 18.11 Helper functions to build RNN cells
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.11 构建 RNN 单元的辅助函数
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You’ll build the encoder and decoder RNN cells by using the helper functions
    you’ve defined. As a reminder, I’ve copied the seq2seq model for you in figure
    18.9 to visualize the encoder and decoder RNNs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你将通过使用你定义的辅助函数来构建编码器和解码器 RNN 单元。作为提醒，我已经为你复制了图 18.9 中的 seq2seq 模型，以可视化编码器和解码器
    RNN。
- en: '![CH18_F09_Mattmann2](../Images/CH18_F09_Mattmann2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F09_Mattmann2](../Images/CH18_F09_Mattmann2.png)'
- en: Figure 18.9 The seq2seq model learns a transformation between an input sequence
    to an output sequence by using an encoder RNN and a decoder RNN.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.9 seq2seq模型通过使用编码器RNN和解码器RNN来学习输入序列到输出序列之间的转换。
- en: Let’s talk about the encoder cell part first, because in listing 18.12, you’ll
    build the encoder cell. The produced states of the encoder RNN will be stored
    in a variable called `encoder_state`. RNNs also produce an output sequence, but
    you don’t need access to that in a standard seq2seq model, so you can ignore or
    delete it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先谈谈编码器单元部分，因为在列表18.12中，您将构建编码器单元。编码器RNN产生的状态将存储在一个名为`encoder_state`的变量中。RNN还会产生一个输出序列，但在标准的seq2seq模型中，您不需要访问它，因此您可以忽略或删除它。
- en: It’s also typical to convert letters or words in a vector representation, often
    called *embedding*. TensorFlow provides a handy function called `embed_sequence`
    that can help you embed the integer representation of symbols. Figure 18.10 shows
    how the encoder input accepts numeric values from a lookup table. You can see
    the encoder in action at the beginning of listing 18.13.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 将字母或单词转换为向量表示也是常见的，通常称为*嵌入*。TensorFlow提供了一个方便的函数`embed_sequence`，可以帮助您将符号的整数表示嵌入。图18.10显示了编码器如何从查找表中接受数值。您可以在列表18.13的开头看到编码器的实际操作。
- en: '![CH18_F10_Mattmann2](../Images/CH18_F10_Mattmann2.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F10_Mattmann2](../Images/CH18_F10_Mattmann2.png)'
- en: Figure 18.10 The RNNs accept only sequences of numeric values as input or output,
    so you’ll convert your symbols to vectors. In this case, the symbols are words,
    such as the, fight, wind, and like. Their corresponding vectors are associated
    in the embedding matrix.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.10 RNN只接受数值序列作为输入或输出，因此您需要将符号转换为向量。在这种情况下，符号是单词，如the、fight、wind和like。它们对应的向量与嵌入矩阵相关联。
- en: Listing 18.12 Encoder embedding and cell
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.12 编码器嵌入和单元
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Input seq of numbers (row indices)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入数字序列（行索引）
- en: ❷ Rows of embedding matrix
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 嵌入矩阵的行
- en: ❸ Columns of embedding matrix
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 嵌入矩阵的列
- en: ❹ You don’t need to hold on to that value.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 您不需要坚持这个值。
- en: The decoder RNN’s output is a sequence of numeric values representing a natural
    language sentence and a special symbol to represent that the sequence has ended.
    You’ll label this end-of-sequence symbol as `<EOS>`. Figure 18.11 illustrates
    this process.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器RNN的输出是一个表示自然语言句子和表示序列结束的特殊符号的数值序列。您将这个序列结束符号标记为`<EOS>`。图18.11说明了这个过程。
- en: The input sequence to the decoder RNN will look similar to the decoder’s output
    sequence, but instead of having the `<EOS>` (end of sequence) special symbol at
    the end of each sentence, it will have a `<GO>` special symbol at the front. That
    way, after the decoder reads its input from left to right, it starts with no extra
    information about the answer, making it a robust model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器RNN的输入序列将类似于解码器的输出序列，但每个句子的末尾没有`<EOS>`（序列结束）特殊符号，而是在开头有一个`<GO>`特殊符号。这样，解码器从左到右读取输入后，开始时没有关于答案的额外信息，这使得它成为一个健壮的模型。
- en: '![CH18_F11_Mattmann2](../Images/CH18_F11_Mattmann2.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![CH18_F11_Mattmann2](../Images/CH18_F11_Mattmann2.png)'
- en: Figure 18.11 The decoder’s input is prefixed with a special `<GO>` symbol, whereas
    the output is suffixed by a special `<EOS>` symbol.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.11 解码器的输入前缀了一个特殊的`<GO>`符号，而输出后缀了一个特殊的`<EOS>`符号。
- en: Listing 18.13 shows how to perform these slicing and concatenating operations.
    The new sequence for the decoder’s input will be called `decoder_input_seq`. You’ll
    use TensorFlow’s `tf.concat` operation to glue matrices together. In the listing,
    you define a `go_prefixes` matrix, which will be a column vector containing only
    the `<GO>` symbol.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.13展示了如何执行这些切片和连接操作。解码器输入的新序列将被称为`decoder_input_seq`。您将使用TensorFlow的`tf.concat`操作将矩阵粘合在一起。在列表中，您定义了一个`go_prefixes`矩阵，它将是一个只包含`<GO>`符号的列向量。
- en: Listing 18.13 Preparing input sequences to the decoder
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.13 准备解码器的输入序列
- en: '[PRE13]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Crops the matrix by ignoring the last column
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过忽略最后一列来裁剪矩阵
- en: ❷ Creates a column vector of <GO> symbols
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个包含<GO>符号的列向量
- en: ❸ Concatenates the <GO> vector to the beginning of the cropped matrix
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将<GO>向量连接到裁剪矩阵的开头
- en: Now let’s construct the decoder cell. As shown in listing 18.14, you’ll first
    embed the decoder sequence of integers in a sequence of vectors, called `decoder_input_embedded`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来构建解码器单元。如列表18.14所示，您首先将解码器整数序列嵌入到向量序列中，称为`decoder_input_embedded`。
- en: 'The embedded version of the input sequence will be fed to the decoder’s RNN,
    so create the decoder RNN cell. One more thing: you’ll need a layer to map the
    output of the decoder to a one-hot representation of the vocabulary, which you
    call `output_layer`. The process of setting up the decoder starts out being similar
    to the setup process for the encoder.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 输入序列的嵌入版本将被馈送到解码器的RNN中，因此创建解码器RNN单元。还有一件事：你需要一个层将解码器的输出映射到词汇表的一个热编码表示，你称之为`output_layer`。设置解码器的过程开始时与设置编码器的过程相似。
- en: Listing 18.14 Decoder embedding and cell
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.14 解码器嵌入和单元
- en: '[PRE14]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Okay, here’s where things get weird. You have two ways to retrieve the decoder’s
    output: during training and during inference. The training decoder will be used
    only during training, whereas the inference decoder will be used for testing on
    never-before-seen data.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这里事情变得奇怪了。你有两种方法来检索解码器的输出：在训练期间和推理期间。训练解码器仅在训练期间使用，而推理解码器用于对从未见过的数据进行测试。
- en: The reason for having two ways to obtain an output sequence is that during training,
    you have the ground-truth data available, so you can use information about the
    known output to help speed the learning process. But during inference, you have
    no ground-truth output labels, so you must resort to making inferences by using
    only the input sequence.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两种获取输出序列的方法的原因是，在训练过程中，你有可用的事实数据，因此你可以使用有关已知输出的信息来帮助加速学习过程。但在推理过程中，你没有事实输出标签，因此你必须求助于仅使用输入序列进行推理。
- en: Listing 18.15 implements the training decoder. You’ll feed `decoder_input_seq`
    into the decoder’s input, using `TrainingHelper`. This helper op manages the input
    to the decoder RNN for you.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.15实现了训练解码器。你将`decoder_input_seq`输入到解码器的输入中，使用`TrainingHelper`。这个辅助操作为你管理解码器RNN的输入。
- en: Listing 18.15 Decoder output (training)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.15 解码器输出（训练）
- en: '[PRE15]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you care to obtain output from the seq2seq model on test data, you no longer
    have access to `decoder_input_seq`. Why? Well, the decoder input sequence is derived
    from the decoder output sequence, which is available only with the training dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关心从测试数据中获取seq2seq模型的输出，你将不再能够访问`decoder_input_seq`。为什么？因为解码器输入序列是从解码器输出序列派生出来的，而输出序列仅在训练数据集中可用。
- en: Listing 18.16 implements the decoder output op for the inference case. Here
    again, you’ll use a helper op to feed the decoder an input sequence.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.16实现了推理情况的解码器输出操作。在这里，你将再次使用辅助操作向解码器提供一个输入序列。
- en: Listing 18.16 Decoder output (inference)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.16 解码器输出（推理）
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Helper for the inference process
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 辅助推理过程
- en: ❷ Basic decoder
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 基本解码器
- en: ❸ Performs dynamic decoding by using the decoder
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过使用解码器执行动态解码
- en: Compute the cost by using TensorFlow’s `sequence_loss` method. You’ll need access
    to the inferred decoder output sequence and the ground-truth output sequence.
    Listing 18.17 defines the `cost` function in code.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow的`sequence_loss`方法计算成本。你需要访问推断的解码器输出序列和事实输出序列。列表18.17在代码中定义了`cost`函数。
- en: Listing 18.17 The `cost` function
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.17 `cost`函数
- en: '[PRE17]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Renames the tensors for your convenience
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为你的方便重命名张量
- en: ❷ Creates the weights for sequence_loss
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建序列损失的权重
- en: ❸ Uses TensorFlow’s built-in sequence loss function
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用TensorFlow的内置序列损失函数
- en: Last, call an optimizer to minimize the cost. But you’ll do one trick you might
    have never seen before. In deep networks like this one, you need to limit extreme
    gradient change to ensure that the gradient doesn’t change too dramatically, using
    a technique called *gradient clipping*. Listing 18.18 shows you how.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，调用一个优化器来最小化成本。但你会做一个你可能从未见过的技巧。在像这样的深度网络中，你需要限制极端梯度变化，以确保梯度不会发生太大的变化，使用的技术称为*梯度裁剪*。列表18.18展示了如何操作。
- en: Exercise 18.5
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 练习18.5
- en: Try the seq2seq model without gradient clipping to experience the difference.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不使用梯度裁剪的seq2seq模型，体验一下差异。
- en: '**Answer**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: You’ll notice that without gradient clipping, the network sometimes adjusts
    the gradients too much, causing numerical instabilities.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，没有梯度裁剪时，网络有时会过度调整梯度，导致数值不稳定性。
- en: Listing 18.18 Calling an optimizer
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表18.18 调用一个优化器
- en: '[PRE18]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Gradient clipping
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 梯度裁剪
- en: That listing concludes the seq2seq model implementation. In general, the model
    is ready to be trained after you’ve set up the optimizer, as in listing 18.18\.
    You can create a session and run `train_op` with batches of training data to learn
    the parameters of the model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 该列表总结了 seq2seq 模型的实现。一般来说，在设置优化器后，模型就准备好训练了，如列表 18.18 所示。您可以通过运行 `train_op`
    并使用训练数据批次来创建会话并学习模型的参数。
- en: Oh, right—you need training data from someplace! How can you obtain thousands
    of pairs of input and output sentences? Fear not; section 18.5 covers that process.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，对了——你需要从某处获取训练数据！你怎么能获得成千上万对输入和输出句子？别担心；第 18.5 节涵盖了该过程。
- en: 18.5 Gathering dialogue data
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.5 收集对话数据
- en: The Cornell Movie-Dialogs Corpus ([http://mng.bz/W28O](http://mng.bz/W28O))
    is a dataset of more than 220,000 conversations from more than 600 movies. You
    can download the zip file from the official web page.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔电影对话语料库 ([http://mng.bz/W28O](http://mng.bz/W28O)) 是一个包含超过 60 部电影中 22 万多段对话的数据集。您可以从官方网站下载
    zip 文件。
- en: Warning Because there’s a huge amount of data, you can expect the training algorithm
    to take a long time. If your TensorFlow library is configured to use only the
    CPU, it might take an entire day to train. On a GPU, training this network may
    take 30 minutes to an hour.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 由于数据量巨大，您可能预计训练算法需要很长时间。如果您的 TensorFlow 库配置为仅使用 CPU，训练可能需要整整一天。在 GPU 上，训练此网络可能需要
    30 分钟到 1 小时。
- en: 'Here is an example of a small snippet of back-and-forth conversation between
    two people (A and B):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是两个人（A 和 B）之间来回对话的小片段示例：
- en: 'A: They do not!'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 他们不做！'
- en: 'B: They do too!'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'B: 他们也这样做！'
- en: 'A: Fine.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 好的。'
- en: 'Because the goal of the chatbot is to produce intelligent output for every
    possible input utterance, you’ll structure your training data based on contingent
    pairs of conversation. In the example, the dialogue generates the following pairs
    of input and output sentences:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因为聊天机器人的目标是针对每个可能的输入话语产生智能输出，所以您将根据对话的偶然对来构建您的训练数据。在示例中，对话生成了以下输入和输出句子的对：
- en: “They do not!” ® “They do too!”
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “他们不做！”®“他们也这样做！”
- en: “They do too!” ® “Fine.”
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “他们也这样做！”®“好的。”
- en: For your convenience, we’ve already processed the data and made it available
    for you online. You can find it at [http://mng.bz/OvlE](http://mng.bz/OvlE). After
    completing the download, you can run listing 18.19, which uses the `load_sentences`
    helper function from the GitHub repo under the `Listing 18-eoc-assign.ipynb` Jupyter
    notebook.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了您的方便，我们已处理数据并将其在线提供给您。您可以在 [http://mng.bz/OvlE](http://mng.bz/OvlE) 找到它。完成下载后，您可以运行列表
    18.19，它使用了 GitHub 仓库下 `Listing 18-eoc-assign.ipynb` Jupyter 笔记本中的 `load_sentences`
    辅助函数。
- en: Listing 18.19 Training the model
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.19 训练模型
- en: '[PRE19]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Loads the input sentences as a list of strings
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将输入句子作为字符串列表加载
- en: ❷ Loads the corresponding output sentences the same way
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 以相同的方式加载相应的输出句子
- en: ❸ Loops through the letters
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历字母
- en: ❹ Loops through the lines of text
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历文本行
- en: ❺ Appends the EOS symbol to the end of the output data
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将 EOS 符号追加到输出数据的末尾
- en: ❻ Loops through the lines
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 遍历行
- en: ❼ It’s a good idea to save the learned parameters.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 保存学习到的参数是个好主意。
- en: ❽ Loops through the epochs
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 遍历时代
- en: ❾ Loops by the number of batches
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 通过批次数进行遍历
- en: ❿ Gets input and output pairs for the current batch
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 获取当前批次的输入和输出对
- en: ⓫ Runs the optimizer on the current batch
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 在当前批次上运行优化器
- en: Because you saved the model parameters to a file, you can easily load the model
    into another program and query the network for responses to new input. Run the
    `inference_ logits` op to obtain the chatbot response.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您已将模型参数保存到文件中，您可以轻松地将模型加载到另一个程序中，并查询网络对新输入的响应。运行 `inference_logits` 操作以获取聊天机器人的响应。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow can build a seq2seq neural network, based on knowledge you’ve acquired
    from the book so far.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 可以根据您到目前为止从书中获得的知识构建 seq2seq 神经网络。
- en: You can embed natural language in TensorFlow.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在 TensorFlow 中嵌入自然语言。
- en: RNNs can be used as a building block for a more interesting model.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 可以用作构建更复杂模型的基石。
- en: After training the model on examples of dialogue from movie scripts, you can
    treat the algorithm like a chatbot, inferring natural language responses from
    natural input.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对电影剧本中的对话示例进行模型训练后，您可以像对待聊天机器人一样处理该算法，从自然输入中推断出自然语言响应。
