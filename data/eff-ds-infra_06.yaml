- en: 6 Going to production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 进入生产阶段
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Deploying workflows to a highly scalable and highly available production scheduler
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将工作流程部署到高度可扩展和高度可用的生产调度器
- en: Setting up a centralized metadata service to track experiments company-wide
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个集中式元数据服务以跟踪公司范围内的实验
- en: Defining stable execution environments with various software dependencies
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义具有各种软件依赖关系的稳定执行环境
- en: Leveraging versioning to allow multiple people to develop multiple versions
    of a project safely
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用版本控制来允许多人安全地开发多个版本的项目
- en: 'Thus far we have been starting all workflows on a personal workstation, maybe
    a laptop. However, it is not a good idea to run business-critical applications
    in a prototyping environment. The reasons are many: laptops get lost, they are
    hard to control and manage centrally, and, more fundamentally, the needs of rapid,
    human-in-the-loop prototyping are very different from the needs of *production
    deployments*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在个人工作站上启动所有工作流程，可能是一台笔记本电脑。然而，在原型环境中运行业务关键应用程序并不是一个好主意。原因有很多：笔记本电脑可能会丢失，它们难以集中控制和管理工作，更重要的是，快速、人工参与的原型设计需求与生产部署的需求非常不同。
- en: 'What does “deploying to production” mean exactly? The word *production* is
    used frequently but is seldom defined precisely. Although particular use cases
    may have their own definitions, we recognize the following two characteristics
    that are common in most production deployments:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “部署到生产”究竟意味着什么？这个词*生产*经常被使用，但很少被精确定义。尽管特定的用例可能有它们自己的定义，但我们认识到以下两个特性在大多数生产部署中是常见的：
- en: '*Automation*—Production workflows should run without any human involvement.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自动化*—生产工作流程应在没有任何人工参与的情况下运行。'
- en: '*High availability*—Production workflows should not fail.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高可用性*—生产工作流程不应失败。'
- en: 'The main characteristic of production workflows is that they should run without
    a human operator: they should start, execute, and output results *automatically*.
    Note that automation doesn’t imply that they work in isolation. They can start
    as a result of some external event, such as new data becoming available.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生产工作流程的主要特征是它们应在没有人工操作员的情况下运行：它们应该自动启动、执行并输出结果。请注意，自动化并不意味着它们是孤立的。它们可以由某些外部事件启动，例如新数据的可用性。
- en: They should not fail, at least not frequently, because failures make it harder
    for other systems to depend on the application, and fixing failures requires slow
    and tedious human intervention. In technical terms, not failing and being dependable
    mean that the application is *highly available*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它们不应频繁失败，至少不应频繁失败，因为失败会使其他系统更难依赖应用程序，并且修复失败需要缓慢而繁琐的人工干预。从技术角度来说，不失败和可信赖意味着应用程序是*高度可用的*。
- en: Automation and high availability are almost always necessary requirements for
    production deployments, but they are not always sufficient. Your particular use
    cases may have additional requirements, for instance, around low latency predictions,
    ability to handle massive datasets, or integration with specific production systems.
    This chapter discusses how to satisfy both the common and bespoke requirements
    of production deployments.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化和高可用性几乎是生产部署的必要要求，但它们并不总是足够的。您的特定用例可能还有额外的需求，例如，关于低延迟预测、处理大量数据集的能力或与特定生产系统的集成。本章讨论如何满足生产部署的常见和定制需求。
- en: As we discussed in chapter 2, prototyping and interacting with production deployments
    are separate but intertwined activities. The act of prototyping can’t be automated,
    prototypes are certainly not highly available, and their defining characteristic
    is human involvement. Production deployments are the opposite.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第2章中讨论的，原型设计和与生产部署的交互是分离但又相互关联的活动。原型设计无法自动化，原型当然不是高度可用的，它们的定义特征是人的参与。生产部署则相反。
- en: 'We should make it easy to move between the two modalities, because data science
    projects aren’t waterfalls that lead from a prototype to a final deployment in
    a linear process. Instead, data science applications should be iterated constantly.
    Deploying to production should be a simple, frequent, unremarkable event—a concept
    that software engineers call *continuous deployment*. Making all projects continuously
    deployable is a worthy goal, which we start exploring in this chapter and continue
    in chapter 8\. To counter common misconceptions, it is useful to recognize that
    “production” does *not* imply the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该使在这两种模式之间移动变得容易，因为数据科学项目不是从原型到最终部署的线性过程的水下瀑布。相反，数据科学应用程序应不断迭代。将应用程序部署到生产应该是一个简单、频繁、不引人注目的事件——软件工程师称之为*持续部署*的概念。使所有项目都能持续部署是一个值得追求的目标，我们将在本章开始探索，并在第8章继续探讨。为了反驳常见的误解，认识到“生产”*并不*意味着以下内容是有用的：
- en: Not all production applications handle large amounts of compute or data. You
    can certainly have small-scale but business-critical applications that must not
    fail. Also, not all production applications need to exhibit high performance.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有生产应用程序都处理大量的计算或数据。你当然可以有小规模但业务至关重要的应用程序，这些应用程序不能失败。此外，并非所有生产应用程序都需要表现出高性能。
- en: There isn’t necessarily only one production deployment of an application. Especially
    in the context of data science, it is common to have multiple production deployments
    side by side, such as for the purpose of A/B testing.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个应用程序不一定只有一个生产部署。特别是在数据科学的背景下，通常会有多个生产部署并排运行，例如为了进行A/B测试。
- en: 'Production doesn’t imply any particular technical approach: production applications
    can be workflows running nightly, microservices serving real-time requests, workflows
    updating Excel sheets, or any other approach required by the use case.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产并不意味着任何特定的技术方法：生产应用程序可以是每晚运行的流程，是服务于实时请求的微服务，是更新Excel表的流程，或任何其他满足用例的方法。
- en: Going to production doesn’t have to be a tedious and anxiety-inducing process.
    In fact, effective data science infrastructure should make it easy to deploy early
    versions to production so their behavior can be observed in a realistic setting.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入生产阶段不一定要是一个繁琐和令人焦虑的过程。事实上，有效的数据科学基础设施应该使得将早期版本部署到生产变得容易，以便在现实环境中观察其行为。
- en: However, production should always imply a level of *stability*. In this chapter,
    we introduce a number of time-tested defensive techniques to protect production
    deployments. These techniques build upon the ones introduced in earlier chapters.
    A robust, scalable compute layer is a critical element of production readiness.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，生产部署始终应意味着一种*稳定性*。在本章中，我们介绍了一系列经过时间考验的防御性技术来保护生产部署。这些技术建立在前面章节介绍的技术之上。一个健壮、可扩展的计算层是生产准备的关键要素。
- en: Starting with the fundamentals, section 6.1 covers executing workflows in a
    stable manner outside of a personal workstation, without any human intervention.
    This is the main concern of the *job scheduler* layer in our infrastructure stack,
    depicted in figure 6.1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础知识开始，第6.1节涵盖了在个人工作站之外以稳定方式执行工作流程，无需任何人为干预。这是我们基础设施堆栈中*作业调度器层*的主要关注点，如图6.1所示。
- en: '![CH06_F01_Tuulos](../../OEBPS/Images/CH06_F01_Tuulos.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F01_Tuulos](../../OEBPS/Images/CH06_F01_Tuulos.png)'
- en: Figure 6.1 The stack of effective data science infrastructure
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 有效的数据科学基础设施的堆栈
- en: After this, we focus on keeping the workflow’s execution environment as stable
    as possible, which is the role of the *versioning* layer in the stack. Many production
    failures happen not because the application itself fails but because something
    in its environment changes. Whereas techniques like @retry, introduced in chapter
    4, handle failures originating from the user code, section 6.2 shows how to guard
    against failures caused by changes in the software environment *surrounding* the
    user code, such as new releases of rapidly evolving libraries like TensorFlow
    or pandas.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们专注于尽可能保持工作流程执行环境的稳定性，这是堆栈中*版本控制层*的作用。许多生产故障并不是因为应用程序本身失败，而是因为其环境中的某些东西发生了变化。而像第4章中介绍的@retry这样的技术，处理来自用户代码的故障，第6.2节展示了如何防止由用户代码*周围*的软件环境变化引起的故障，例如快速发展的库（如TensorFlow或pandas）的新版本。
- en: Section 6.3 focuses on preventing human errors and accidents. We need to execute
    production workflows in isolated and versioned environments so that data scientists
    can keep experimenting with new versions, without having to fear they will interfere
    with production deployments. Optimally, you should be able to ask a fresh intern
    to create a version of a production application and deploy it in parallel to the
    main version, knowing that they can’t do anything to break the existing version,
    not even by accident.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第6.3节侧重于防止人为错误和事故。我们需要在隔离和版本化的环境中执行生产工作流程，以便数据科学家可以继续实验新版本，而无需担心它们会干扰生产部署。理想情况下，你应该能够要求一位新实习生创建一个生产应用程序的版本，并将其与主版本并行部署，知道他们无法对现有版本造成任何破坏，甚至不是意外造成的。
- en: You don’t need to apply all techniques presented in this chapter to every project.
    Defensive features that make an application more robust can slow down prototyping,
    and they can make it harder to deploy and test new production versions, hurting
    overall productivity. Fortunately, production-readiness is a spectrum, not a binary
    label. You can prototype a project quickly on a local workstation, deploy an early
    version to production as described in section 6.1, and apply techniques from sections
    6.2 and 6.3 later as the stakes become higher. In other words, you can *harden
    projects gradually* against failures as the project matures.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要将本章中介绍的所有技术应用到每个项目中。使应用程序更健壮的防御性功能可能会减慢原型设计，并可能使部署和测试新生产版本变得更加困难，从而影响整体生产力。幸运的是，生产就绪是一个连续体，而不是一个二元标签。你可以在本地工作站上快速原型化一个项目，如第6.1节所述，将早期版本部署到生产环境中，并在风险变得更高时，稍后应用第6.2节和第6.3节的技术。换句话说，随着项目的成熟，你可以逐渐*加固项目以抵御失败*。
- en: Inevitable tradeoffs exist between the needs of prototyping and production.
    A central goal of effective data science infrastructure is finding good compromises
    between the two modalities. Figure 6.2 illustrates the typical tradeoffs on the
    project’s maturation path.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在原型设计和生产需求之间不可避免地存在权衡。有效数据科学基础设施的一个主要目标是在这两种模式之间找到良好的折衷方案。图6.2展示了项目成熟路径上的典型权衡。
- en: '![CH06_F02_Tuulos](../../OEBPS/Images/CH06_F02_Tuulos.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F02_Tuulos](../../OEBPS/Images/CH06_F02_Tuulos.png)'
- en: Figure 6.2 The maturation path of a typical data science project
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 典型数据科学项目的成熟路径
- en: At each stage on the project’s maturation path, you need to address a new set
    of deficiencies. Over time, as the project becomes more robust, some flexibility
    is lost. New versions of the project can start the cycle from the beginning, incrementally
    improving the production version.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目成熟路径的每个阶段，你都需要解决一组新的缺陷。随着时间的推移，随着项目的日益稳健，一些灵活性会丧失。项目的新版本可以从开始处重新开始循环，逐步改进生产版本。
- en: The earlier chapters of this book have led you to the “basic workflow” stage.
    This chapter teaches you the next level of production-readiness. By the end of
    the chapter, you will reach a level that has been proven sufficient to power some
    of the most business-critical ML applications at Netflix and other large companies.
    You can find all code listings for this chapter at [http://mng.bz/06oW](http://mng.bz/06oW).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的前几章已将你引导到“基本工作流程”阶段。本章将教你生产就绪的下一级。到本章结束时，你将达到一个已被证明足以支持Netflix和其他大型公司一些最关键业务机器学习应用的水平。你可以在此处找到本章的所有代码列表：[http://mng.bz/06oW](http://mng.bz/06oW)。
- en: 6.1 Stable workflow scheduling
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 稳定的工作流程调度
- en: '*Harper takes Alex to tour the company’s new cupcake production facility. Encouraged
    by Alex’s promising prototypes, Harper wants to start using machine learning to
    optimize operations at the facility. Harper reminds Alex that any unplanned interruptions
    in the cupcake pipeline affect the company’s revenue directly, so, hopefully,
    Alex’s models will work flawlessly. Hearing this, Bowie suggests that they should
    start scheduling training and optimization workflows in a much more robust environment
    than Alex’s laptop.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*哈珀带亚历克斯参观了公司的新杯子蛋糕生产设施。受到亚历克斯有希望的样品的鼓舞，哈珀希望开始使用机器学习来优化该设施的操作。哈珀提醒亚历克斯，任何计划外的中断都会直接影响公司的收入，因此，希望亚历克斯的模型能够完美运行。听到这些，鲍伊建议他们应该在一个比亚历克斯的笔记本电脑更加健壮的环境中开始安排训练和优化工作流程。*'
- en: '![CH06_F02_UN01_Tuulos](../../OEBPS/Images/CH06_F02_UN01_Tuulos.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F02_UN01_Tuulos](../../OEBPS/Images/CH06_F02_UN01_Tuulos.png)'
- en: 'This section answers a simple question: How can I execute my workflows reliably
    without human intervention? Thus far, we have been executing workflows on the
    command line by executing a command like'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回答了一个简单的问题：我如何在没有人为干预的情况下可靠地执行我的工作流程？到目前为止，我们一直在命令行上通过执行类似以下命令来执行工作流程：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Typing the command requires human intervention, so this is not a good approach
    for production deployments that should run automatically. Moreover, if you close
    your terminal while the command is executing, the workflow crashes—the workflow
    is not highly available. Note that for prototyping use cases, the run command
    is perfect, because it affords very quick iterations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输入命令需要人为干预，因此这不是适用于应自动运行的部署生产部署的好方法。此外，如果您在命令执行时关闭了终端，工作流程会崩溃——工作流程不是高度可用的。请注意，对于原型设计用例，运行命令是完美的，因为它提供了非常快速的迭代。
- en: 'In chapter 2, we discussed production-grade workflow schedulers or orchestrators,
    which are the right tool for the job. Figure 6.3 reminds us of the role of the
    job scheduling layer: the job scheduler only needs to walk through the steps of
    the workflow, that is, decide *how* to orchestrate the DAG and send each task
    to the compute layer, which decides *where* to execute them. Crucially, neither
    the job scheduler nor the compute layer need to care *what* gets executed exactly—more
    about that later.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们讨论了适用于生产级别的流程调度器或编排器，这是完成工作的正确工具。图6.3提醒我们工作调度层的作用：工作调度器只需要遍历工作流程的步骤，即决定如何编排DAG并将每个任务发送到计算层，计算层决定在哪里执行它们。关键的是，工作调度器和计算层都不需要关心确切执行什么——关于这一点稍后还会详细说明。
- en: '![CH06_F03_Tuulos](../../OEBPS/Images/CH06_F03_Tuulos.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F03_Tuulos](../../OEBPS/Images/CH06_F03_Tuulos.png)'
- en: 'Figure 6.3 The role of the scheduling layer: How to orchestrate the DAG'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 调度层的作用：如何编排DAG
- en: 'Although walking through a DAG doesn’t sound hard, remember that we are talking
    about production use cases: the orchestrator may need handle large workflows with
    tens of thousands of tasks, there may be thousands of such workflows executing
    concurrently, and the orchestrator should be able to handle a plethora of failure
    scenarios gracefully, including the data center where the orchestrator itself
    executes. Moreover, the orchestrator must be able to trigger workflow executions
    on various conditions, such as when new data arrives, and it should provide ergonomic
    UIs for monitoring and alerting. All in all, building such a system is a highly
    nontrivial engineering challenge, so it is wise to rely on the best available
    off-the-shelf systems and services for the job. We listed a few suitable candidates
    in chapter 2.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然遍历DAG听起来不难，但请记住，我们正在讨论生产用例：编排器可能需要处理包含数万个任务的庞大工作流程，可能有数千个这样的工作流程同时执行，编排器应该能够优雅地处理各种故障场景，包括编排器本身执行的数据中心。此外，编排器必须能够根据各种条件触发工作流程执行，例如当新数据到达时，并且它应该提供便于监控和警报的用户界面。总的来说，构建这样的系统是一个高度复杂的工程挑战，因此明智的做法是依靠最好的现成系统和服务。我们在第二章中列出了一些合适的候选者。
- en: This is the approach taken by Metaflow as well. Metaflow includes a local scheduler
    that is good enough for prototyping—the one that powers the run command. For production
    use cases, you can deploy your Metaflow workflow to a few different production
    schedulers without changing anything in the code. We will use one such scheduler,
    AWS Step Functions, to demonstrate the idea in this section. Note that, in general,
    the discussion of this section is not specific to AWS Step Functions. You can
    apply the pattern to other job schedulers as well.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是Metaflow采取的方法。Metaflow包括一个本地调度器，这对于原型设计来说已经足够好了——它支持运行命令。对于生产用例，您可以将Metaflow工作流程部署到几个不同的生产调度器，而无需在代码中进行任何更改。在本节中，我们将使用其中一个调度器，AWS
    Step Functions，来演示这个想法。请注意，一般来说，本节的讨论并不特定于AWS Step Functions。您也可以将这种模式应用于其他工作调度器。
- en: 'However, before we get to production scheduling, we must take care of one other
    detail: we need a centralized service to track execution metadata across all runs,
    because we can’t rely on locally stored metadata when orchestrating workflows
    in the cloud.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们进入生产调度之前，我们必须注意另一个细节：我们需要一个集中式服务来跟踪所有运行的执行元数据，因为在云中编排工作流程时，我们不能依赖于本地存储的元数据。
- en: 6.1.1 Centralized metadata
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 集中式元数据
- en: In chapter 3, we talked about *experiment tracking*—the concept of keeping track
    of executions and their results. The term is a bit misleading. We are interested
    in keeping track of not only *experiments* but also production executions. Hence,
    we prefer to use a generic term *metadata* to refer to all bookkeeping activities.
    Figure 6.4 shows the role of metadata tracking in the context of task execution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们讨论了*实验跟踪*——跟踪执行及其结果的概念。这个术语有点误导。我们感兴趣的是跟踪的不仅仅是*实验*，还包括生产执行。因此，我们更喜欢使用一个通用术语*元数据*来指代所有账目活动。图
    6.4 展示了元数据跟踪在任务执行背景下的作用。
- en: '![CH06_F04_Tuulos](../../OEBPS/Images/CH06_F04_Tuulos.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F04_Tuulos](../../OEBPS/Images/CH06_F04_Tuulos.png)'
- en: Figure 6.4 The role of the Metaflow metadata service
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 Metaflow 元数据服务的作用
- en: At the top, we have a job scheduler that walks through the DAG. The figure shows
    Metaflow’s local scheduler as an example, but the job scheduler could be a production
    scheduler as well, such as AWS Step Functions. The scheduler sends tasks to the
    compute layer, which executes the user code. The user code produces results, or
    artifacts, such as self.model in the figure, which are stored in a datastore as
    discussed in chapter 3.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，我们有一个作业调度器，它会遍历 DAG。图中以 Metaflow 的本地调度器为例，但作业调度器也可以是生产调度器，例如 AWS Step Functions。调度器将任务发送到计算层，执行用户代码。用户代码产生结果或工件，如图中的
    self.model，这些工件存储在数据存储库中，正如第 3 章所述。
- en: On the side, the metadata service keeps track of all runs and tasks that are
    started. Also, when an artifact is written to the datastore, the metadata service
    records the location of the artifact in the datastore. Crucially, metadata doesn’t
    record the data itself because it is already in the datastore. This allows us
    to keep the metadata service relatively lightweight—it is responsible only for
    bookkeeping, not large-scale data or compute.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在旁边，元数据服务跟踪所有启动的运行和任务。此外，当工件写入数据存储库时，元数据服务记录工件在数据存储库中的位置。关键的是，元数据不记录数据本身，因为数据已经在数据存储库中。这使我们能够保持元数据服务的相对轻量级——它只负责账目，而不是大规模数据或计算。
- en: After data and metadata have been stored, they can be queried from outside systems.
    For instance, as we have seen in previous chapters, we can use the Metaflow Client
    API to query runs and their results in a notebook. The Client API talks to the
    metadata service to figure out what runs are available and to the datastore to
    access their results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据和元数据存储后，可以从外部系统查询。例如，正如我们在前面的章节中看到的，我们可以使用 Metaflow 客户端 API 在笔记本中查询运行及其结果。客户端
    API 与元数据服务通信以确定可用的运行，并与数据存储库通信以访问其结果。
- en: Most modern frameworks for data science and ML infrastructure provide a service
    for centralized metadata tracking. For instance, MLflow ([https://mlflow.org](https://mlflow.org))
    provides a Tracking Server, and Kubeflow ([https://kubeflow.org](https://kubeflow.org))
    comes with a built-in centralized dashboard that stores and shows all executions
    that have been executed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代数据科学和机器学习基础设施框架都提供集中式元数据跟踪服务。例如，MLflow ([https://mlflow.org](https://mlflow.org))
    提供了跟踪服务器，而 Kubeflow ([https://kubeflow.org](https://kubeflow.org)) 内置了一个集中式仪表板，用于存储和显示所有已执行的运行。
- en: 'By default, Metaflow tracks metadata in local files. This is sufficient for
    small-scale, personal prototyping, but for more serious use cases, you should
    set up a cloud-based centralized metadata service. Centralized metadata tracking
    provides the following benefits:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Metaflow 在本地文件中跟踪元数据。这对于小规模的个人原型设计来说是足够的，但对于更严肃的使用场景，你应该设置一个基于云的集中式元数据服务。集中式元数据跟踪提供了以下好处：
- en: You need a metadata service to be able to use a production scheduler because
    a cloud-based scheduler doesn’t have a concept of “local files.”
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要一个元数据服务才能使用生产调度器，因为基于云的调度器没有“本地文件”的概念。
- en: You can execute runs anywhere, both in prototyping and production, and rest
    assured that metadata is always tracked consistently in a single place.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在任何地方执行运行，无论是原型设计还是生产，并且可以放心，元数据始终在单一位置一致地跟踪。
- en: Centralized metadata enables collaboration because all users can discover and
    access the results of past runs, regardless of who initiated them. More about
    this in section 6.3.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中式元数据使协作成为可能，因为所有用户都可以发现和访问过去运行的成果，无论是由谁启动的。更多内容请参阅第 6.3 节。
- en: 'A cloud-based service is more stable: all metadata can be stored in a database
    that can be replicated and regularly backed up.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云端服务更稳定：所有元数据都可以存储在可以复制和定期备份的数据库中。
- en: Setting up a centralized metadata service for Metaflow
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为Metaflow设置集中式元数据服务
- en: As of the writing of this book, the metadata service provided by Metaflow is
    a typical containerized microservice that uses Amazon Relational Database Service
    (RDS) to store the metadata. It is possible to deploy the service on AWS Elastic
    Container Service (ECS) or on Elastic Kubernetes Service (EKS), for example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 就本书撰写时的情况而言，Metaflow提供的元数据服务是一个典型的容器化微服务，它使用Amazon关系数据库服务（RDS）来存储元数据。该服务可以部署在AWS弹性容器服务（ECS）或弹性Kubernetes服务（EKS）上，例如。
- en: The easiest way to deploy the service is to use the CloudFormation template
    provided by Metaflow, as instructed by Metaflow’s installation instructions ([https://docs.metaflow.org](https://docs.metaflow.org)).
    Another benefit of using the CloudFormation template is that it can also set up
    AWS Step Functions, which we will use in the next section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 部署服务的最简单方法是使用Metaflow安装说明中提供的CloudFormation模板（[https://docs.metaflow.org](https://docs.metaflow.org)）。使用CloudFormation模板的另一个好处是它还可以设置AWS
    Step Functions，我们将在下一节中使用它。
- en: After you have set up and configured the Metaflow metadata service, you can
    use the Client API to access results as before—nothing changes in the user-facing
    API. You can ensure that Metaflow is using a metadata service instead of local
    files by running
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在您设置并配置了Metaflow元数据服务之后，您可以使用客户端API访问结果，就像之前一样——用户界面API没有任何变化。您可以通过运行以下命令来确保Metaflow正在使用元数据服务而不是本地文件：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If the service is properly configured, you should see an output like the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务配置正确，您应该看到以下类似的输出：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can test that the service works correctly by executing any flow with the
    run command. You should notice that the run and task IDs are much shorter (e.g.,
    HelloFlow/2 versus HelloFlow/1624840556112887) when using the service. The local
    mode uses timestamps as IDs, whereas the service produces globally unique short
    IDs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用run命令执行任何流程来测试服务是否正常工作。您应该注意到，当使用服务时，运行和任务ID要短得多（例如，HelloFlow/2与HelloFlow/1624840556112887相比）。本地模式使用时间戳作为ID，而服务生成全局唯一的短ID。
- en: 'If you are unsure what metadata service the Client API uses, you can find it
    using the get_metadata function. You can execute a cell like this in a notebook:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不确定客户端API使用的是哪个元数据服务，您可以使用get_metadata函数来查找。您可以在笔记本中执行如下单元格：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If a service is used correctly, you should see an output like the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务使用正确，您应该看到以下类似的输出：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Metaflow stores its configuration at ~/.metaflowconfig/ in the user’s home directory.
    If you have many data scientists in your organization, it is beneficial to share
    the same set of configuration files with all of them, so they can benefit from
    a consistent infrastructure and collaborate through a shared metadata service
    and datastore. On the other hand, if you need to maintain boundaries between organizations,
    for example, for data governance reasons, you can set up multiple independent
    metadata services. You can also define hard security boundaries between different
    datastores by using separate S3 buckets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow将配置存储在用户主目录的~/.metaflowconfig/中。如果您组织中有许多数据科学家，与他们都共享同一组配置文件是有益的，这样他们可以从一致的基础设施中受益，并通过共享的元数据服务和数据存储进行协作。另一方面，如果您需要维护组织之间的边界，例如出于数据治理的原因，您可以设置多个独立的元数据服务。您还可以通过使用单独的S3存储桶来定义不同数据存储之间的严格安全边界。
- en: 'After you have configured Metaflow to use an S3-based datastore and centralized
    metadata, there may be times when you want to test something with a local datastore
    and metadata, for instance, for troubleshooting purposes. You can do this as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在您配置Metaflow使用基于S3的数据存储和集中式元数据之后，可能会有时候您想使用本地数据存储和元数据来测试某些内容，例如用于故障排除。您可以按照以下方式操作：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These options instruct Metaflow to fall back to a local datastore and metadata,
    regardless of the default configuration.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项指示Metaflow在默认配置的情况下回退到本地数据存储和元数据。
- en: 6.1.2 Using AWS Step Functions with Metaflow
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 使用AWS Step Functions与Metaflow
- en: 'AWS Step Functions (SFN) is a highly available and scalable workflow orchestrator
    (job scheduler) provided as a cloud service by AWS. Although many other off-the-shelf
    workflow orchestrators are available, SFN has a number of appealing features compared
    to alternatives, as described next:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Step Functions (SFN) 是由AWS提供的云服务，它是一个高度可用和可扩展的工作流编排器（作业调度器）。尽管有许多其他现成的流程编排器可用，但与替代方案相比，SFN具有许多吸引人的特性，如下所述：
- en: Similar to AWS Batch, it is a fully managed service. In the operator’s point
    of view, the service is practically maintenance-free.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 AWS Batch 类似，它是一个完全托管的服务。从操作者的角度来看，该服务实际上无需维护。
- en: AWS has an excellent track record of making services highly available and scalable.
    Though many alternatives claim to have these characteristics, not all of them
    work as advertised.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 在提供高度可用性和可扩展性的服务方面有着卓越的记录。尽管许多替代方案声称具有这些特性，但并非所有都能按预期工作。
- en: The total cost of operation can be very competitive compared to the full cost
    of operating a similar service in-house, particularly considering that keeping
    the system running requires no staff.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与内部运营类似服务的全部成本相比，运营总成本可以非常有竞争力，尤其是考虑到保持系统运行不需要任何人员。
- en: SFN integrates with other AWS services seamlessly.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SFN 可以无缝集成其他 AWS 服务。
- en: When it comes to the downsides of SFN, as of the writing this book, it is hard
    to define workflows for SFN manually without a library like Metaflow using their
    native JSON-based syntax, and the GUI is a bit clunky. Also, SFN has some limits
    that constrain the maximum size of a workflow, which may affect workflows with
    very wide foreaches.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到 SFN 的缺点时，截至本书编写时，如果没有像 Metaflow 这样的库使用它们本地的基于 JSON 的语法手动定义 SFN 工作流程，那么很难定义。此外，SFN
    的 GUI 有点笨拙。还有，SFN 的一些限制会约束工作流程的最大大小，这可能会影响具有非常宽泛 foreachs 的工作流程。
- en: Let’s see how it works in practice. First, make sure you have deployed the Step
    Functions integration to Metaflow as instructed in the Metaflow’s documentation.
    The easiest way is to use the provided CloudFormation template, which sets it
    up for you, together with a metadata service. Next, let’s define a simple flow,
    shown in the next listing, that we use to test SFN.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在实际中的工作方式。首先，确保你已经按照 Metaflow 文档中的说明部署了 Step Functions 集成。最简单的方法是使用提供的
    CloudFormation 模板，它为你设置好一切，包括元数据服务。接下来，让我们定义一个简单的流程，如下一列表所示，我们用它来测试 SFN。
- en: Listing 6.1 A simple flow to test Step Functions
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 测试 Step Functions 的简单流程
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Save the code to sfntest.py and make sure it works locally:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 sfntest.py 并确保它在本地工作：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let’s deploy the workflow to production! All you have to do is to execute
    the next command:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将工作流程部署到生产环境！你只需要执行下一个命令：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If all goes well, you should see output like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会看到如下输出：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We don’t have to worry about most of this output for now. We will dive deeper
    into *production tokens* in section 6.3\. It is worth paying attention to the
    last line, though: as it says, in its current form, the workflow doesn’t start
    automatically. We will need to start or trigger it manually.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们不必担心大部分这些输出。我们将在 6.3 节中深入探讨 *生产令牌*。不过，最后一行值得注意：正如它所说的，在其当前形式下，工作流程不会自动启动。我们需要手动启动或触发它。
- en: What exactly happened when you ran step-function create? Metaflow did a bunch
    of work behind the scenes, which is illustrated in figure 6.5.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行 step-function create 时，究竟发生了什么？Metaflow 在幕后做了一系列工作，如图 6.5 所示。
- en: '![CH06_F05_Tuulos](../../OEBPS/Images/CH06_F05_Tuulos.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F05_Tuulos](../../OEBPS/Images/CH06_F05_Tuulos.png)'
- en: Figure 6.5 How Metaflow deploys to AWS Step Functions
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 Metaflow 部署到 AWS Step Functions 的方式
- en: 'The following sequence of operations was executed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 执行了以下操作序列：
- en: Metaflow packaged all Python code in the current working directory and uploaded
    it to the datastore (S3) so it can be executed remotely. More about this in the
    next section.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow 将当前工作目录中的所有 Python 代码打包并上传到数据存储（S3），以便远程执行。更多内容将在下一节中介绍。
- en: Metaflow parsed the workflow DAG and translated it to a syntax that SFN understands.
    In other words, it converted your local Metaflow workflow to a bona fide SFN workflow.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow 解析了工作流程 DAG 并将其转换为 SFN 理解的语法。换句话说，它将你的本地 Metaflow 工作流程转换为真正的 SFN 工作流程。
- en: Metaflow made a number of calls to AWS APIs to deploy the translated workflow
    to the cloud.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow 调用了一系列 AWS API 来部署翻译后的工作流程到云端。
- en: 'Notably, the user didn’t have to change anything in their code to make it deployable
    to the cloud. This is an important feature of Metaflow: you can test the code
    locally and with a compute layer of your choice, like AWS Batch, before deploying
    it to production. It is exactly the same code running in production, so you can
    be confident that if the code worked during local testing, it will also work in
    production. Even more important, if the workflow fails in production, you can
    reproduce and fix the issue locally and deploy a fixed version to production just
    by running step-functions create again—more about this in section 6.3.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，用户不需要更改他们的代码就可以将其部署到云端。这是Metaflow的一个重要特性：你可以在将代码部署到生产之前，在本地以及使用你选择的计算层（如AWS
    Batch）测试代码。实际上，生产中运行的是相同的代码，所以如果你在本地测试中代码有效，你就可以有信心它在生产中也能工作。更重要的是，如果工作流程在生产中失败，你可以在本地重现并修复问题，然后只需再次运行step-functions
    create即可将修复版本部署到生产——更多关于这一点在第6.3节中介绍。
- en: Productivity tip When it comes to interaction with production deployments, which
    we discussed in chapter 2, it is crucial that the workflows you prototype locally
    can work in production with minimal changes, and vice versa. This makes it easy
    to test the code locally before deploying it production and, when tasks fail in
    production, reproduce those issues locally.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 生产效率提示：当涉及到与第2章中讨论的生产部署的交互时，至关重要的是，你在本地原型化的工作流程可以在生产中以最小的更改工作，反之亦然。这使得在部署到生产之前在本地测试代码变得容易，当生产中的任务失败时，可以在本地重现这些问题。
- en: Before we run the workflow, let’s log in to the AWS console to see how the workflow
    looks on the SFN side. Navigate to the Step Functions UI on the AWS Console. You
    should see a list of workflows, like what is shown in figure 6.6.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行工作流程之前，让我们登录到AWS控制台，看看工作流程在SFN侧看起来如何。导航到AWS控制台上的步骤函数UI。你应该会看到一个工作流程列表，如图6.6所示。
- en: '![CH06_F06_Tuulos](../../OEBPS/Images/CH06_F06_Tuulos.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F06_Tuulos](../../OEBPS/Images/CH06_F06_Tuulos.png)'
- en: Figure 6.6 List of workflows (state machines) on the AWS Step Functions console
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 AWS Step Functions控制台上的工作流程列表（状态机）
- en: The view shown in figure 6.6 gives you a quick overview of currently executing,
    succeeded, and failed runs. When you click the name of the run, you will get to
    the view shown in figure 6.7.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6中显示的视图为你提供了当前执行、成功和失败的运行的快速概述。当你点击运行的名称时，你会进入如图6.7所示的视图。
- en: 'There is not much to see here yet, because there are no executions. Although
    the “Start execution” button sounds enticing, there is actually a better way to
    start a run. If you click the button, SFN will ask you to specify a JSON that
    should include parameters for the flow, which is a bit tedious to do manually.
    Instead, we can trigger an execution on the command line like so:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里目前没有什么可看的，因为没有执行。尽管“开始执行”按钮听起来很有吸引力，但实际上有更好的启动运行的方式。如果你点击该按钮，SFN会要求你指定一个JSON文件，该文件应包含流程的参数，这有点麻烦手动完成。相反，我们可以在命令行上触发一个执行，如下所示：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should see an output like the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下类似的输出：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![CH06_F07_Tuulos](../../OEBPS/Images/CH06_F07_Tuulos.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F07_Tuulos](../../OEBPS/Images/CH06_F07_Tuulos.png)'
- en: Figure 6.7 A workflow on the AWS Step Functions console
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 AWS Step Functions控制台上的工作流程
- en: When executing flows on SFN, the Metaflow run IDs correspond to the SFN run
    IDs, other than the sfn prefix added by Metaflow, so it is easy to know what Metaflow
    runs map to which SFN executions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在SFN上执行流程时，Metaflow运行ID对应于SFN运行ID，除了Metaflow添加的sfn前缀之外，所以很容易知道哪些Metaflow运行映射到哪些SFN执行。
- en: The trigger command is similar to the run command in the sense that they both
    execute a workflow. However, instead of executing it locally, trigger makes the
    workflow execute on SFN. Crucially, you could shut down your laptop, and the run
    would keep running on SFN, in contrast to local runs. In fact, SFN supports workflows
    that execute for as long as a year! It would be quite inconvenient to keep your
    laptop running without interruptions for that long.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 触发命令与运行命令类似，因为它们都执行一个工作流程。然而，它们不是在本地执行，触发命令使工作流程在SFN上执行。关键的是，你可以关闭你的笔记本电脑，运行将在SFN上继续，这与本地运行相反。实际上，SFN支持执行长达一年的工作流程！长时间不间断地运行笔记本电脑将非常不方便。
- en: Now if you refresh the workflow listing, you should see a new execution. If
    you click it, you get to the run view shown in figure 6.8.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你刷新工作流程列表，你应该会看到一个新的执行。如果你点击它，你会进入如图6.8所示的运行视图。
- en: '![CH06_F08_Tuulos](../../OEBPS/Images/CH06_F08_Tuulos.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_Tuulos](../../OEBPS/Images/CH06_F08_Tuulos.png)'
- en: Figure 6.8 A run on the AWS Step Functions console
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 AWS Step Functions控制台上的运行
- en: 'Note how the ID in the upper-left corner matches the ID that was output by
    trigger. Conveniently, the run view visualizes the DAG of our workflow. It updates
    in real time, showing what steps are being executed. If you click a step in the
    DAG, on the right panel you see a Resource link that takes you to the AWS Batch
    console that shows an AWS Batch job corresponding to the executing task. On the
    AWS Batch console, you can click the Log Stream link to see the task’s output
    in real time. You can see logs even more easily by using the familiar logs command
    (replace the ID with your ID output by trigger):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，左上角的 ID 与触发器输出的 ID 匹配。方便的是，运行视图可视化了我们工作流程的 DAG。它实时更新，显示正在执行哪些步骤。如果您在 DAG
    中单击一个步骤，在右侧面板中您会看到一个资源链接，该链接会将您带到 AWS Batch 控制台，显示与正在执行的任务对应的 AWS Batch 作业。在 AWS
    Batch 控制台中，您可以单击日志流链接以实时查看任务的输出。您可以通过使用熟悉的日志命令（将 ID 替换为触发器输出的您的 ID）更轻松地查看日志：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: All Metaflow commands and the Client API work equally with local runs and runs
    execution on SFN—only the format of their IDs differ.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Metaflow 命令和客户端 API 都可以在本地运行和 SFN 上的运行执行中同等工作——只是它们的 ID 格式不同。
- en: As shown next, we can trigger another execution with a custom parameter value
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，我们可以使用自定义参数值触发另一个执行
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: which produces a new run ID like
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个新的运行 ID，例如
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can confirm that the parameter change took effect by inspecting artifacts
    of the triggered run with the command
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过检查触发运行的工件来确认参数更改是否生效
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'or by checking the logs of the start step. Note that you can list SFN runs
    on the command line by executing the following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 或者通过检查启动步骤的日志。注意，您可以通过执行以下命令在命令行中列出 SFN 运行：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Hence, it is possible to trigger and discover runs as well as examine logs and
    artifacts without logging in to the SFN console.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以在不登录到 SFN 控制台的情况下触发和发现运行，以及检查日志和工件。
- en: 'Let’s take a deep breath and summarize what we just learned:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深呼吸并总结一下我们刚刚学到的内容：
- en: We defined a normal Metaflow workflow that we were able to test locally as before.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个正常的 Metaflow 工作流程，我们能够像以前一样在本地进行测试。
- en: We used a command, step-functions create, to deploy the workflow to a highly
    available, scalable production scheduler in the cloud, AWS Step Functions.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 step-functions create 命令将工作流程部署到云中高度可用、可扩展的生产调度器 AWS Step Functions。
- en: We triggered a production run with step-functions trigger, optionally using
    custom parameters. The workflow would keep running even if you shut down your
    laptop.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 step-functions 触发器触发了生产运行，并可选择使用自定义参数。即使您关闭了笔记本电脑，工作流程也会继续运行。
- en: We monitored workflow executions in real time on a GUI provided by SFN.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 SFN 提供的 GUI 上实时监控工作流程执行。
- en: We inspected the logs and results of a production run using familiar CLI commands.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用熟悉的 CLI 命令检查了生产运行的日志和结果。
- en: The simplicity of the whole process may make it seem deceptively trivial. However,
    being able to deploy workflows to a production scheduler this easily is truly
    a superpower for data scientists! Using this approach results in highly available
    workflows, thanks to SFN. However, we launched the workflow manually with trigger,
    so the setup is not yet fully automated. In the next section, we will address
    this shortcoming.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程的简单性可能会让人误以为它非常简单。然而，能够如此轻松地将工作流程部署到生产调度器，对于数据科学家来说确实是一种超级能力！使用这种方法的结果是高度可用的工作流程，归功于
    SFN。然而，我们手动使用触发器启动了工作流程，因此设置尚未完全自动化。在下一节中，我们将解决这个缺点。
- en: 6.1.3 Scheduling runs with @schedule
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 使用 @schedule 安排运行
- en: 'Production flows should execute without any human intervention. If you have
    a complex environment with many interdependent workflows, it is advisable to trigger
    workflows programmatically based on events, for instance, retrain a model whenever
    the input data updates. This section focuses on a simpler, more common approach:
    deploying workflows to run on a predetermined schedule.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 生产流程应无需任何人工干预即可执行。如果您有一个复杂的环境，其中包含许多相互依赖的工作流程，建议根据事件以编程方式触发工作流程，例如，当输入数据更新时重新训练模型。本节重点介绍一个更简单、更常见的方法：将工作流程部署到预定的时间表上运行。
- en: Metaflow provides a flow-level decorator, @schedule, which allows you to define
    an execution schedule for the workflow. See an example in the following code listing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 提供了一个流程级别的装饰器 @schedule，允许您为工作流程定义执行时间表。请参见以下代码示例中的示例。
- en: Listing 6.2 A flow with @schedule
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 使用 @schedule 的流程
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Makes the workflow trigger automatically every midnight
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使工作流程在午夜自动触发
- en: 'Save the code in dailysfntest.py. Here, we define a simple daily schedule by
    annotating the flow with @schedule(daily=True). This will make the flow start
    every midnight in the UTC time zone. The @schedule doesn’t have any effect when
    running the flow locally. It will take effect when you execute the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存在dailysfntest.py中。在这里，我们通过使用@schedule(daily=True)注解流程来定义一个简单的每日计划。这将使流程在UTC时区的午夜开始。当本地运行流程时，@schedule没有任何效果。它将在您执行以下操作时生效：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: That’s it! The workflow will now run automatically once a day. You can observe
    run IDs of past executions on the SFN UI or with step-functions list-runs as described
    earlier.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！工作流程现在将每天自动运行一次。您可以在SFN UI上观察过去执行的运行ID，或者使用前面描述的步骤函数list-runs。
- en: Note that you can’t change parameter values for scheduled runs—all parameters
    are assigned their default values. If you need to parametrize the workflow dynamically,
    you can do so using arbitrary Python code, for example, in the start step.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您不能为计划中的运行更改参数值——所有参数都分配了它们的默认值。如果您需要动态参数化工作流程，可以使用任意Python代码，例如在启动步骤中。
- en: 'Besides running the workflow daily, the following shorthand decorators are
    available:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了每天运行工作流程之外，还提供了以下简写装饰器：
- en: '@schedule(weekly=True)—Runs the workflow on Sundays at midnight'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@schedule(weekly=True)—每周日午夜运行工作流程'
- en: '@schedule(hourly=True)—Runs the workflows hourly'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@schedule(hourly=True)—每小时运行工作流程'
- en: 'Alternatively, you can define a custom schedule with the cron attribute. For
    instance, this expression runs a workflow daily at 10 a.m.: @schedule(cron=''0
    10 * * ? *''). You can find more examples of cron schedules and a description
    of the syntax at [http://mng.bz/KxvE](http://mng.bz/KxvE).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用cron属性定义一个自定义的计划。例如，这个表达式每天上午10点运行工作流程：@schedule(cron='0 10 * * ? *')。您可以在[http://mng.bz/KxvE](http://mng.bz/KxvE)找到更多cron计划的示例和语法描述。
- en: Now we have learned how to schedule flows for execution without human supervision,
    which covers the *automation* requirement of production deployments. Next, we
    will shift our attention to *high availability*, that is, how to keep the deployments
    stable in the midst of a constantly changing environment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何在没有人监督的情况下安排流程执行，这涵盖了生产部署的*自动化*需求。接下来，我们将把注意力转向*高可用性*，即如何在不断变化的环境中保持部署的稳定性。
- en: 6.2 Stable execution environments
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 稳定的执行环境
- en: '*Alex is proud of the modern infrastructure stack that they set up with Bowie.
    It allows all data scientists at the company to develop workflows locally and
    deploy them easily to a robust production scheduler in the cloud. Alex heads to
    a well-deserved vacation. During the vacation, Alex receives a notification alerting
    that one of the production workflows that had been running flawlessly for weeks
    had mysteriously crashed last night. As Alex investigates the matter, it turns
    out that the workflow always installs the latest version of TensorFlow. Just yesterday,
    TensorFlow released a new version that is incompatible with the production workflow.
    Why do these things always seem to happen during vacation?*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*亚历克斯为他们与鲍伊一起搭建的现代基础设施堆栈感到自豪。这允许公司的所有数据科学家在本地开发工作流程，并轻松地将它们部署到云中的强大生产调度器。亚历克斯开始了应得的假期。在假期期间，亚历克斯收到一条通知，提醒他一个已经完美运行了几周的生产工作流程昨晚神秘地崩溃了。当亚历克斯调查此事时，发现工作流程总是安装TensorFlow的最新版本。就在昨天，TensorFlow发布了一个与生产工作流程不兼容的新版本。为什么这些事情总是在假期发生？*'
- en: '![CH06_F08_UN02_Tuulos](../../OEBPS/Images/CH06_F08_UN02_Tuulos.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F08_UN02_Tuulos](../../OEBPS/Images/CH06_F08_UN02_Tuulos.png)'
- en: Practically all data science and machine learning workflows use third-party
    libraries. In fact, it is almost certain that the vast majority of all lines of
    code in your workflows reside in these libraries. During development, you can
    test the behavior of the library with different datasets and parameterizations
    to gain confidence that a particular version of the library works correctly, but
    what happens when a new version is released? Most modern machine learning and
    data science libraries evolve rapidly.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，几乎所有数据科学和机器学习工作流程都使用第三方库。事实上，几乎可以肯定的是，您工作流程中的绝大多数代码行都位于这些库中。在开发过程中，您可以通过使用不同的数据集和参数化来测试库的行为，以获得对特定版本的库工作正确的信心，但当一个新版本发布时会发生什么呢？大多数现代机器学习和数据科学库都在快速发展。
- en: This problem is not specific to data science. Software engineers have been grappling
    with *dependency management* for decades. Many established best practices for
    developing and releasing software libraries exist. For instance, most well-behaving
    libraries are careful in changing their public APIs, which would break applications
    that use the library.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题并不仅限于数据科学。软件工程师们几十年来一直在努力解决*依赖管理*问题。已经存在许多关于开发和发布软件库的最佳实践。例如，大多数表现良好的库在更改其公共API时会非常谨慎，这可能会破坏使用该库的应用程序。
- en: 'The public API of a library is supposed to provide a clear contract. Imagine
    you have a library that provides a function, requests.get(url), which fetches
    the given URL over HTTP and returns the content as a byte string. Just reading
    a short description of the function makes it clear how the function is supposed
    to behave. Contrast this with a machine learning library that provides the following
    API for K-means clustering: KMeans(k).fit(data). The contract is much looser:
    the library may change how the clusters are initialized, which optimization algorithm
    is used, how the data is handled, and how the implementation is distributed over
    multiple CPU or GPU cores, without changing the API. All these changes may subtly
    change the behavior of the library and cause unintentional side effects.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 库的公共API应该提供一个清晰的契约。想象你有一个提供函数requests.get(url)的库，该函数通过HTTP获取给定的URL并返回字节字符串内容。只需阅读该函数的简短描述，就可以清楚地了解该函数应该如何表现。与此相对比的是，一个提供以下API的机器学习库用于K-means聚类：KMeans(k).fit(data)。契约要宽松得多：库可能会更改簇的初始化方式、使用的优化算法、数据处理方式以及如何在多个CPU或GPU核心上分配实现，而无需更改API。所有这些变化都可能微妙地改变库的行为并导致意外的副作用。
- en: Loose API contracts and the statistical nature of data science in general make
    dependency management for data science workflows a trickier problem than what
    software engineers have faced. For instance, imagine that a machine learning library
    includes a method called train(). The method may use a technique like *stochastic
    gradient descent* internally, which can be implemented in many different ways,
    each with its own pros and cons. If the implementation changes across library
    versions, it may have major effects on the resulting model, although, technically,
    the train() API stays intact. Failures in software engineering are often pretty
    clear—maybe the return type has changed, or the function raises a new exception,
    causing the program to crash with a verbose error message. In data science, however,
    you may simply get a slightly skewed distribution of results without any failures,
    making it hard to even notice that something has changed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 松散的API契约和数据科学的一般统计特性使得数据科学工作流程的依赖管理比软件工程师面临的问题更为复杂。例如，想象一个机器学习库包含一个名为train()的方法。该方法可能使用一种像*随机梯度下降*这样的技术，这可以以许多不同的方式实现，每种方式都有其自身的优缺点。如果实现方式在库的不同版本之间发生变化，它可能会对生成的模型产生重大影响，尽管技术上train()
    API保持不变。软件工程中的失败通常非常明显——可能是返回类型已更改，或者函数引发了一个新的异常，导致程序因冗长的错误消息而崩溃。然而，在数据科学中，你可能会简单地得到一个略有偏斜的结果分布，而没有任何失败，这使得甚至难以注意到有什么变化。
- en: When you are prototyping a new application, it is certainly convenient to have
    the flexibility to use the latest versions of any libraries. During prototyping,
    no one relies on the outputs of your workflow, so rapid changes are acceptable
    and expected. However, when you deploy the workflow in production, being more
    careful with dependencies starts to matter. You don’t want to be notified during
    vacation about unexpected failures in a production pipeline (like poor Alex) or
    face hard-to-debug questions about why the results seem subtly different than
    before. To avoid any surprises, you want production deployments to execute in
    an environment that is as stable as possible, so that nothing changes without
    your explicit action and approval.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在原型化一个新应用程序时，能够灵活地使用任何库的最新版本当然是非常方便的。在原型化过程中，没有人会依赖你的工作流程输出，因此快速变化是可以接受和预期的。然而，当你将工作流程部署到生产环境中时，对依赖关系的谨慎处理开始变得重要。你不想在度假期间接到关于生产管道（比如糟糕的亚历克斯）意外失败的通知，或者面对难以调试的问题，比如为什么结果看起来与之前略有不同。为了避免任何意外，你希望生产部署在尽可能稳定的环境中执行，这样除非你明确行动和批准，否则什么都不会改变。
- en: Execution environment
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 执行环境
- en: What do we mean by the *execution environment*? Consider the Metaflow workflows
    that we have executed earlier, such as KMeansFlow, which we developed in the previous
    chapter. When you run a workflow by executing, say, python kmeans_flow.py run,
    the entry point to execution is kmeans_flow.py, but a number of other modules
    and libraries need to be present for the flow to execute successfully, which together
    make the execution environment for kmeans_flow.py. Figure 6.9 illustrates the
    idea.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的*执行环境*是什么意思？考虑我们之前执行过的Metaflow工作流程，例如KMeansFlow，这是我们之前章节中开发的。当你通过执行，比如，python
    kmeans_flow.py run来运行工作流程时，执行的入口点是kmeans_flow.py，但为了使流程成功执行，还需要存在许多其他模块和库，这些模块和库共同构成了kmeans_flow.py的执行环境。图6.9说明了这个概念。
- en: '![CH06_F09_Tuulos](../../OEBPS/Images/CH06_F09_Tuulos.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F09_Tuulos](../../OEBPS/Images/CH06_F09_Tuulos.png)'
- en: Figure 6.9 Layers of the execution environment of a flow
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 流程的执行环境层
- en: At the center we have the flow itself, kmeans_ flow.py. The flow may use user-defined
    supporting modules, like scale_data.py, which contain functions for loading data.
    To execute the flow, you need Metaflow, which is a library in itself. On top of
    this, you have all other third-party libraries like Scikit-Learn, which we used
    in the K-means example. Finally, the whole package executes on top of an operating
    system.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在中心，我们有流程本身，kmeans_flow.py。流程可能使用用户定义的支持模块，如scale_data.py，其中包含加载数据的函数。要执行流程，你需要Metaflow，它本身就是一个库。在此基础上，你还有所有其他第三方库，如Scikit-Learn，我们在K-means示例中使用过。最后，整个包在操作系统上执行。
- en: To provide a stable execution environment for production deployments, it is
    beneficial to freeze an immutable snapshot of all of the layers shown in figure
    6.9\. Note that this includes all *transitive dependencies**,* that is, all libraries
    that other libraries themselves use. By doing this, you ensure that new library
    releases can’t have unplanned effects on the deployment. You are in control of
    how and when you want to upgrade libraries.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为生产部署提供一个稳定的执行环境，冻结图6.9中显示的所有层的不可变快照是有益的。请注意，这包括所有*传递依赖**，即其他库本身使用的所有库。通过这样做，你可以确保新的库发布不会对部署产生未计划的影响。你可以控制如何以及何时升级库。
- en: Recommendation To minimize surprises, it is beneficial to freeze all code in
    the production deployment, including the workflow itself and all of its dependencies.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：为了最小化意外，冻结生产部署中的所有代码是有益的，包括工作流程本身及其所有依赖项。
- en: 'Technically, we have many ways to implement snapshotting like this. A common
    way is to package all of the layers shown in figure 6.9 in a container image using
    a tool like Docker. We discuss this approach in section 6.2.3\. Alternatively,
    Metaflow provides built-in functionality that takes care of snapshotting the layers,
    which we discuss in the next sections. To make the discussions more fun and concrete,
    we frame them in the context of a realistic data science application: time-series
    forecasting.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，我们有多种方式来实现这种快照功能。一种常见的方法是使用Docker等工具将图6.9中显示的所有层打包到一个容器镜像中。我们将在6.2.3节中讨论这种方法。另一种方法是Metaflow提供了内置的功能来处理层的快照，我们将在下一节中讨论。为了使讨论更加有趣和具体，我们将它们放在一个真实的数据科学应用背景下：时间序列预测。
- en: 'Example: Time-series forecasting'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：时间序列预测
- en: Countless applications exist for time-series forecasting, that is, predicting
    future data points given historical data. Many techniques for forecasting and
    many off-the-shelf packages include efficient implementations of these techniques.
    Packages like these are a typical example of the software libraries we want to
    include in a data science workflow, so we use a forecasting application to demonstrate
    the concepts related to dependency management and stable execution environments.
    In the following examples, we will use a library called Sktime ([sktime.org](https://sktime.org)).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测有无数的应用，即根据历史数据预测未来的数据点。许多预测技术和现成的软件包都包括这些技术的有效实现。这类软件库是我们希望在数据科学工作流程中包含的典型例子，因此我们使用预测应用来展示与依赖管理及稳定执行环境相关的概念。在以下示例中，我们将使用一个名为Sktime的库([sktime.org](https://sktime.org))。
- en: 'Because we just learned how to schedule a workflow to run automatically, let’s
    pick an application that requires frequent updates: weather forecasting. We don’t
    dare to dive deep into meteorology—instead, we simply provide an hourly temperature
    forecast for the next few days given a series of past temperatures at a given
    location. Naturally, this is a silly way of forecasting weather, but temperature
    data is readily available and we can implement the application easily.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因为刚刚学习了如何自动调度工作流，让我们选择一个需要频繁更新的应用程序：天气预报。我们不敢深入探讨气象学——相反，我们只是根据给定位置的过去温度序列，提供未来几天的每小时温度预报。当然，这是一种愚蠢的天气预报方式，但温度数据很容易获得，我们可以轻松实现这个应用程序。
- en: 'We will use a service called OpenWeatherMap ([openweathermap.org](https://openweathermap.org/))
    to obtain weather data. To use the service, you need to sign up for a free account
    at the site. After you have signed up, you will receive a private application
    ID token, which you can input in the following examples. The required token is
    a string that looks like this: 6e5db45abe65e3110be635abfb9bdac5.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用名为OpenWeatherMap的服务（[openweathermap.org](https://openweathermap.org/））来获取天气数据。要使用此服务，您需要在网站上注册一个免费账户。注册后，您将收到一个私有的应用程序ID令牌，您可以在以下示例中输入。所需的令牌是一个看起来像这样的字符串：6e5db45abe65e3110be635abfb9bdac5。
- en: After you have completed the weather forecasting examples, you can replace the
    weather dataset with another real-time time series, such as stock prices, as an
    exercise.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 完成天气预报示例后，可以将天气数据集替换为另一个实时时间序列，如股价，作为练习。
- en: 6.2.1 How Metaflow packages flows
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 Metaflow如何打包流程
- en: In chapter 4, we learned how to execute workflows in the cloud simply by executing
    run—with batch. Somehow, Metaflow managed to take the code that you wrote on a
    local workstation, maybe a laptop, and execute it hundreds or thousands of miles
    away in a cloud data center. Remarkably, you didn’t have to do anything to save
    or package the code in any particular way to make this happen. This is thanks
    to the fact that Metaflow packages the user code and its supporting modules automatically
    in a *code package*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我们学习了如何通过执行run-with-batch在云中简单地执行工作流。不知何故，Metaflow能够将您在本地工作站（可能是一台笔记本电脑）上编写的代码，在数百或数千英里外的云数据中心执行。令人惊讶的是，您不需要做任何事情来保存或以任何特定方式打包代码，就可以实现这一点。这要归功于Metaflow自动将用户代码及其支持模块打包到*代码包*中。
- en: By default, the code package includes the flow module, any other Python (.py)
    files in the current working directory and its subdirectories, and the Metaflow
    library itself. This corresponds to the highlighted layers in figure 6.10.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，代码包包括流程模块，当前工作目录及其子目录中的任何其他Python (.py) 文件，以及Metaflow库本身。这对应于图6.10中突出显示的层。
- en: '![CH06_F10_Tuulos](../../OEBPS/Images/CH06_F10_Tuulos.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F10_Tuulos](../../OEBPS/Images/CH06_F10_Tuulos.png)'
- en: Figure 6.10 What is included in the Metaflow code package
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 Metaflow代码包包含的内容
- en: To see how this works in practice, let’s start by creating a supporting module
    for our weather forecast application. The module, shown in listing 6.3, fetches
    a time series of temperatures in a given location for the past five days from
    OpenWeatherMap.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解实际工作原理，让我们首先为我们的天气预报应用程序创建一个支持模块。该模块，如列表6.3所示，从OpenWeatherMap获取给定位置过去五天的温度时间序列。
- en: Listing 6.3 A utility module to get temperature time series
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3 获取温度时间序列的实用模块
- en: '[PRE19]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ An API endpoint that returns historical weather data
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回历史天气数据的API端点
- en: ❷ Returns a time series of temperatures over the past five days
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回过去五天的温度时间序列
- en: ❸ Does imports inside the function to avoid module-level dependencies
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在函数内部进行导入以避免模块级依赖
- en: ❹ Requests data for the past five days in chronological order
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 按时间顺序请求过去五天的数据
- en: ❺ Prepares and sends a request to OpenWeatherMap
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 准备并发送请求到OpenWeatherMap
- en: ❻ Constructs a time series of hourly temperatures
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 构建每小时温度的时间序列
- en: ❼ Converts a pandas time series to a list of tuples
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将pandas时间序列转换为元组列表
- en: 'Save the code to openweatherdata.py. The module contains two functions: get_
    historical_weather_data, which returns a time series of temperatures over the
    past five days, and a utility function, series_to_list, which converts a pandas
    time series to a list of tuples.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到openweatherdata.py。该模块包含两个函数：get_historical_weather_data，它返回过去五天的温度时间序列，以及一个实用函数，series_to_list，它将pandas时间序列转换为元组列表。
- en: 'The get_historical_weather_data function takes three arguments: your private
    appid, which you can obtain by signing up at OpenWeatherMap, and the latitude
    (lat) and longitude (lon) of the location for which you want the weather data.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: get_historical_weather_data 函数接受三个参数：你的私有 appid，你可以通过在 OpenWeatherMap 上注册来获取它，以及你想要获取天气数据的地点的纬度（lat）和经度（lon）。
- en: 'The function showcases an important convention: in contrast to the typical
    Python convention of doing all import statements at the top of the module, we
    import all third-party modules—that is, modules not in the Python standard library
    like pandas and Requests—inside the function body. This makes it possible for
    anyone to import the module, even if they don’t have these two libraries installed.
    They can use some functions of the module without having to install every single
    dependency.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数展示了一个重要的习惯用法：与在模块顶部执行所有导入语句的典型 Python 习惯相反，我们在函数体内导入所有第三方模块——即不在 Python 标准库中的模块，如
    pandas 和 Requests。这使得即使没有安装这两个库的人也能导入该模块。他们可以使用模块的一些功能，而无需安装每个依赖项。
- en: Convention If you think a supporting module may be used in many different contexts,
    it is a good idea to import any third-party libraries inside the function body
    that uses the libraries instead of importing them at the top of the file. This
    way, the module itself can be imported without having to install the union of
    all dependencies required by all functions of the module.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 习惯用法 如果你认为一个支持模块可能在许多不同的上下文中使用，那么在文件顶部而不是在函数体内导入任何第三方库是一个好主意。这样，模块本身就可以导入，而无需安装模块中所有函数所需的所有依赖项的并集。
- en: The OpenWeatherMap API returns hourly data for a single day in a request, so
    we need a loop to retrieve data for the past five days. For each day, the service
    returns a JSON object, which contains an array of hourly temperatures in degrees
    Fahrenheit. If you prefer Celsius, change units from imperial to metric. We convert
    the daily arrays to a single pandas time series, which is keyed by a datetime
    object for each hour. This format makes it easy to plot and use the data for forecasting.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: OpenWeatherMap API 在一个请求中返回单日的每小时数据，因此我们需要一个循环来检索过去五天的数据。对于每一天，该服务返回一个包含每小时温度（华氏度）数组的
    JSON 对象。如果你更喜欢摄氏度，请将单位从英制改为公制。我们将每日数组转换为单个 pandas 时间序列，每个小时都有一个 datetime 对象作为键。这种格式使得绘图和使用数据来进行预测变得容易。
- en: The series_to_list function simply takes a pandas time series like what is produced
    by get_historical_weather_data and converts it to a Python list of tuples. We
    will get back to the motivation of this function later.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: series_to_list 函数简单地将 get_historical_weather_data 生成的 pandas 时间序列转换为 Python
    元组列表。我们稍后会回到这个函数的动机。
- en: 'A benefit of having a separate module is that you can test it easily, independent
    of any flows. Open a notebook or a Python shell and try the following lines:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个独立的模块的好处是你可以轻松地对其进行测试，而无需依赖任何流程。打开一个笔记本或 Python shell，尝试以下行：
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You can replace LAT and LON with a location other than San Francisco. Replace
    APPID with your private token. If all goes well, you should see a list of temperatures.
    Note that you need to have pandas installed to be able to do this. If you don’t
    have it installed, don’t worry—you will be able to see results soon nonetheless!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用除旧金山以外的其他地点替换 LAT 和 LON。用你的私有令牌替换 APPID。如果一切顺利，你应该会看到一个温度列表。请注意，你需要安装 pandas
    才能执行此操作。如果你还没有安装，不要担心——你很快就能看到结果！
- en: Next, we can start developing the actual flow for forecasting. Following our
    spiral approach to flow development, we don’t worry about the forecasting model
    yet. We just plug in the inputs, which are provided by openweatherdata.py, and
    some outputs. We’ll use @conda to include external libraries as we did in the
    two previous chapters, and we’ll cover this in more detail in the next section.
    The following listing contains the first iteration of ForecastFlow.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始开发实际的预测流程。遵循我们的螺旋式流程开发方法，我们目前不担心预测模型。我们只需插入输入，这些输入由 openweatherdata.py
    提供，以及一些输出。我们将使用 @conda 来包含外部库，就像在前两个章节中做的那样，我们将在下一节中详细介绍这一点。以下列表包含 ForecastFlow
    的第一次迭代。
- en: Listing 6.4 The first version of ForecastFlow
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 ForecastFlow 的第一个版本
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Loads the input data in the start step using our openweatherdata module
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在开始步骤中使用我们的 openweatherdata 模块加载数据
- en: ❷ Saves the pandas data series in one artifact
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将 pandas 数据序列保存在一个工件中
- en: ❸ Saves a Python version of the data series in another artifact
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据序列的 Python 版本保存在另一个工件中
- en: ❹ This is our output step. It plots the time series.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这是我们输出步骤。它绘制时间序列。
- en: ❺ Plots and saves the time series in an in-memory buffer
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在内存缓冲区中绘制并保存时间序列
- en: ❻ Stores the plot in an artifact
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将图表存储在工件中
- en: Save the listing to forecast1.py. To run the listing, you need Conda installed
    as instructed in the appendix.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表保存为forecast1.py。要运行列表，您需要按照附录中的说明安装Conda。
- en: 'The start step is responsible for getting the input data, which it delegates
    to the supporting module, openweatherdata.py, which we created in listing 6.3\.
    Notably, the start step creates two artifacts: pd_past5days, which contains a
    pandas time series of temperatures for the past five days, and past5days, which
    contains the same data converted to a Python list. Note that we didn’t have to
    specify the pandas dependency explicitly because it is a transitive dependency
    of the Seaborn package.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 开始步骤负责获取输入数据，并将其委托给辅助模块openweatherdata.py，这是我们第6.3节中创建的。值得注意的是，开始步骤创建了两个工件：pd_past5days，它包含过去五天的温度的pandas时间序列，以及past5days，它包含相同的数据，但已转换为Python列表。请注意，我们不需要显式指定pandas依赖项，因为它是由Seaborn包的传递依赖项。
- en: 'You may wonder what the point is of storing the same data twice, just in two
    different formats. The motivation is, again, dependencies: to read pd_past5days
    using the Client API, for example, in a notebook, you need pandas—a particular
    version of pandas—to be installed. In contrast, you can read past5days without
    any dependencies besides Python. We could store only past5days, but other steps
    of the flow need a pandas version, and they are guaranteed to have the correct
    version of pandas available, thanks to the @conda decorator.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么需要将相同的数据存储两次，只是两种不同的格式。动机再次是依赖关系：例如，要在笔记本中使用Client API读取pd_past5days，你需要安装特定版本的pandas。相比之下，你可以不依赖任何其他库来读取past5days。我们本可以只存储past5days，但流程的其他步骤需要pandas版本，并且由于@conda装饰器，它们保证有正确的pandas版本。
- en: Recommendation You should prefer storing artifacts as built-in Python types
    instead of objects that rely on third-party libraries, because native Python types
    are universally readable in different contexts without external dependencies.
    If you need to use a complex object within a flow, consider storing both a shareable
    Python version as well as an object version as separate artifacts.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：您应该优先将工件存储为内置Python类型，而不是依赖于第三方库的对象，因为原生Python类型在不同的上下文中都是可读的，无需外部依赖。如果您需要在流程中使用复杂对象，请考虑同时存储一个可共享的Python版本以及一个对象版本作为单独的工件。
- en: 'Try executing the flow as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试以下方式执行流程：
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Replace my-private-token with your personal OpenWeatherMap token. Running the
    flow for the first time will take a few minutes because the Conda environments
    need to be initialized. Subsequent runs should be much faster.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 将my-private-token替换为您的个人OpenWeatherMap令牌。第一次运行流程需要几分钟，因为需要初始化Conda环境。后续运行应该会快得多。
- en: 'After the run has completed, you can open a notebook and instantiate a Run
    object that corresponds to the run that just finished. You can see a temperature
    plot by executing the following cell:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完成后，您可以在笔记本中打开一个实例化Run对象的笔记本，该对象对应于刚刚完成的运行。您可以通过执行以下单元格来查看温度图：
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Replace the run ID with an actual ID from your run. Figure 6.11 shows what the
    result looks like. It shows a plot of hourly temperatures in Las Vegas, unless
    you change the --location, over five days. In this case, you can see a clear pattern
    of temperature variation between day and night. Your time series will look different
    because the weather is not constant.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 将运行ID替换为您运行的实际ID。图6.11显示了结果的外观。它显示了五天内拉斯维加斯的每小时温度图，除非您更改了--location，否则它显示了昼夜之间温度变化的明显模式。由于天气不是恒定的，您的时间序列看起来会不同。
- en: '![CH06_F11_Tuulos](../../OEBPS/Images/CH06_F11_Tuulos.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F11_Tuulos](../../OEBPS/Images/CH06_F11_Tuulos.png)'
- en: Figure 6.11 A time series of hourly temperatures in Las Vegas
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 拉斯维加斯的每小时温度时间序列
- en: 'The fact that you can show the plot with a single statement highlights another
    important detail about dependency management: sometimes it is beneficial to produce
    images inside Metaflow instead of in a notebook. Although you could call plot_
    series with pd_past5days in a notebook as well, it would require that you have
    the pandas, Sktime, and Seaborn packages installed and available in your notebook
    kernel. Even if you have them installed, your colleague might not.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用单个语句显示图表的事实突出了依赖关系管理的一个重要细节：有时在Metaflow内部生成图像而不是在笔记本中是有益的。尽管您也可以在笔记本中调用plot_series与pd_past5days，但这要求您在笔记本内核中安装并可用pandas、Sktime和Seaborn包。即使您已安装，您的同事可能没有。
- en: How to produce and store images inside Metaflow is demonstrated by the plot
    step. Many visualization libraries like Matplotlib allow a plot to be rendered
    and saved in an in-memory buffer (buf). You can then save bytes from the buffer,
    that is, an image file, in a Metaflow artifact (here, self.plot), so the Client
    API can easily retrieve it.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow中如何生成和存储图像，通过绘图步骤进行演示。许多可视化库，如Matplotlib，允许将绘图渲染并保存到内存缓冲区（buf）中。然后，您可以保存缓冲区中的字节，即图像文件，到Metaflow工件（此处为self.plot），以便客户端API可以轻松检索它。
- en: Recommendation If your flow benefits from plots that should be easily accessible
    by multiple stakeholders, consider producing and saving them inside Metaflow steps
    instead of in a notebook. This way stakeholders don’t need to install any additional
    dependencies to be able to see the images.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：如果您的流程从易于多个利益相关者访问的图表中受益，请考虑在Metaflow步骤内生成和保存它们，而不是在笔记本中。这样，利益相关者无需安装任何额外的依赖项即可查看图像。
- en: This pattern of producing plots inside Metaflow is particularly useful for production
    deployments, after you have determined what plots are useful for monitoring the
    flow, and it is beneficial to be able to share them widely. In contrast, during
    prototyping, it is probably easier to design and iterate on visualizations rapidly
    in a notebook.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在Metaflow内部生成图表的这种模式对于生产部署特别有用，在您确定哪些图表对监控流程有用之后，能够广泛共享它们是有益的。相比之下，在原型设计期间，在笔记本中快速设计和迭代可视化可能更容易。
- en: Metaflow code package
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow代码包
- en: 'Now that we have a working flow, we can get back to the original question of
    this section: what is included in a Metaflow code package, and how is it constructed?'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个正在运行的流程，我们可以回到本节原始问题的原点：Metaflow代码包包含什么，以及它是如何构建的？
- en: 'You can see the contents of the code package by executing package list:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过执行package list来查看代码包的内容：
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that you need to specify --environment=conda for all commands, including
    package list, that apply to a flow using the @conda decorator. You can also set
    an environment variable, METAFLOW_ENVIRONMENT=conda, to avoid having to set the
    option explicitly.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您需要为所有命令指定--environment=conda，包括使用@conda装饰器应用于流程的包列表。您还可以设置环境变量，METAFLOW_ENVIRONMENT=conda，以避免必须显式设置选项。
- en: 'You should see a long list of files. Notice the following two things about
    the list:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个长文件列表。注意列表中的以下两点：
- en: By default, Metaflow includes all files ending with the .py suffix—that is,
    Python source files—in the current working directory and its subdirectories in
    the job package. This allows you to use custom modules and Python packages in
    your projects easily—just include them in the same working directory.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，Metaflow将当前工作目录及其子目录中所有以.py后缀结尾的文件（即Python源文件）包含在作业包中。这允许您在项目中轻松使用自定义模块和Python包——只需将它们包含在相同的当前工作目录中。
- en: Metaflow includes Metaflow itself in the job package, which allows you to use
    generic container images in the cloud, because they don’t need to have Metaflow
    preinstalled. Also, it guarantees that the results you see locally match the results
    you get in the cloud.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow将Metaflow自身包含在作业包中，这使得您可以在云中使用通用容器镜像，因为它们不需要预先安装Metaflow。这也保证了您在本地看到的与在云中获得的相同结果。
- en: 'Sometimes you may want to include files other than Python in the code package.
    For instance, your data processing step may execute SQL statements stored in separate
    .sql files or your code may call a custom binary. You can include any files in
    the job package by using the --package-suffixes option. Consider a hypothetical
    project with the following directory structure:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您可能希望在代码包中包含除Python之外的其他文件。例如，您的数据处理步骤可能执行存储在单独.sql文件中的SQL语句，或者您的代码可能调用自定义的二进制文件。您可以通过使用--package-suffixes选项将任何文件包含在作业包中。考虑一个具有以下目录结构的假设项目：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here, mylibrary is a *Python package* (if you are not familiar with Python
    packages, see [http://mng.bz/95g0](http://mng.bz/95g0)) that contains two modules,
    database and preprocess. Packages allow you to group multiple interrelated modules
    as a library. You can use the custom package in your step code simply by writing
    the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，mylibrary 是一个 *Python 包*（如果你不熟悉 Python 包，请参阅 [http://mng.bz/95g0](http://mng.bz/95g0)），它包含两个模块，database
    和 preprocess。包允许你将多个相互关联的模块作为一个库分组。你可以在步骤代码中使用自定义包，只需编写以下内容：
- en: '[PRE26]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This will work even when you execute a hypothetical flow in myflow.py
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这即使在执行 myflow.py 中的假设流程时也能正常工作
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: or deploy it to step functions, because Metaflow packages all Python files recursively
    in the job package. However, to include input_data.sql in the code package, you
    need to execute
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 或者将其部署到步骤函数中，因为 Metaflow 会递归地将所有 Python 文件打包到作业包中。然而，要包含 input_data.sql 到代码包中，你需要执行
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'which instructs Metaflow to include all .sql files in addition to .py files
    in the code package. To access the SQL file in your code, you can open the file
    as usual, like so:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这条指令指示 Metaflow 包含所有 .sql 文件以及 .py 文件到代码包中。要访问代码中的 SQL 文件，你可以像通常一样打开文件，如下所示：
- en: '[PRE29]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that you should always use relative paths instead of absolute paths (any
    path starting with a slash) like /Users/ville/arc/sql/input_data.sql in your Metaflow
    code because absolute paths won’t work outside your personal workstation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你应该始终在 Metaflow 代码中使用相对路径而不是绝对路径（任何以斜杠开头的路径），例如 /Users/ville/arc/sql/input_data.sql，因为绝对路径在你的个人工作站外将不起作用。
- en: Technically, you could include arbitrary data files in the code package as well.
    However, as the name implies, code packages should be used only for executable
    code. It is better to handle data as data artifacts that benefit from deduplication
    and lazy loading. You can use the IncludeFile construct covered in chapter 3 to
    bundle arbitrary data files in your runs, which is a good solution for small datasets.
    The next chapter provides more ideas for managing large datasets.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上，你可以在代码包中包含任意的数据文件。然而，正如其名所示，代码包应该仅用于可执行代码。更好地将数据作为数据工件处理，这些工件可以从去重和延迟加载中受益。你可以使用第
    3 章中介绍的 IncludeFile 构造来捆绑运行中的任意数据文件，这对于小数据集是一个很好的解决方案。下一章提供了更多管理大数据集的想法。
- en: 6.2.2 Why dependency managements matters
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 为什么依赖管理很重要
- en: In the previous section, we learned how Metaflow packages local Python files
    automatically in a code package that can be shipped to different compute layers,
    like AWS Batch, for execution. In terms of stable execution environments, we covered
    the three innermost layers of the onion, as depicted in figure 6.12, but code
    packages don’t address the question of third-party libraries.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了 Metaflow 如何自动将本地 Python 文件打包到代码包中，该代码包可以发送到不同的计算层，如 AWS Batch，以执行。在稳定执行环境方面，我们涵盖了洋葱的最内层三个层，如图
    6.12 所示，但代码包并没有解决第三方库的问题。
- en: '![CH06_F12_Tuulos](../../OEBPS/Images/CH06_F12_Tuulos.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F12_Tuulos](../../OEBPS/Images/CH06_F12_Tuulos.png)'
- en: Figure 6.12 Focusing on the libraries layer of the execution environment
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 关注执行环境的库层
- en: Why can’t we include libraries in the code package, too? Most important, modern
    ML libraries tend to be complex beasts, implemented in large part in compiled
    languages like C++. They have much more involved requirements than simple Python
    packages. In particular, practically all libraries depend on many other libraries,
    so the “libraries” layer includes not only the libraries you import directly,
    such as TensorFlow, but also all libraries—tens of them—that TensorFlow uses internally.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们不能在代码包中也包含库呢？最重要的是，现代机器学习库往往很复杂，大部分是用 C++ 等编译语言实现的。它们比简单的 Python 包有更多的要求。特别是，几乎所有库都依赖于许多其他库，所以“库”层不仅包括你直接导入的库，如
    TensorFlow，还包括 TensorFlow 内部使用的所有库——多达几十个。
- en: We call these libraries-used-by-libraries *transitive dependencies*. To determine
    the full set of libraries that need to be included in the execution environment
    of a task, we must identify all libraries and their transitive dependencies. Determining
    this graph of dependencies—the operation is commonly called *dependency resolution*—is
    a surprisingly nontrivial problem.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些由库使用的库称为 *传递依赖*。为了确定需要包含在任务执行环境中的完整库集，我们必须识别所有库及其传递依赖。确定这个依赖图——这个操作通常被称为
    *依赖解析*——是一个令人惊讶的非平凡问题。
- en: 'You might wonder, isn’t this a solved problem already? After all, you can pip
    install tensorflow, and often it just works. Consider the following two problems
    that you may have faced, too:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这个问题不是已经解决了吗？毕竟，你可以pip install tensorflow，通常它都能正常工作。考虑以下你可能也遇到过的两个问题：
- en: '*Conflicts*—The more libraries you install, the more likely it is that the
    dependency graph of a library you want to install conflicts with an existing library,
    and installation fails. For instance, many ML libraries such as Scikit-Learn and
    TensorFlow require specific versions of the NumPy library, so it is common to
    have conflicts related to a wrong version of NumPy.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*冲突*——你安装的库越多，你想要安装的库的依赖图与现有库发生冲突的可能性就越大，安装就会失败。例如，许多机器学习库，如Scikit-Learn和TensorFlow，需要特定版本的NumPy库，因此经常会有与NumPy版本错误相关的冲突。'
- en: Issues like this can be hard to debug and solve. A common solution is that someone
    at the company carefully maintains a mutually compatible set of packages, which
    is a tedious job. Worse, it limits the speed of iteration. Different projects
    can’t make choices independently because everybody must work with a common blessed
    set of libraries.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种问题可能很难调试和解决。一个常见的解决方案是公司里有人仔细维护一组相互兼容的包，这是一项繁琐的工作。更糟糕的是，它限制了迭代的速度。不同的项目不能独立做出选择，因为每个人都必须使用一组共同的库。
- en: Another common solution to limit the size of the dependency graph and, hence,
    minimize the likelihood of conflicts is to use *virtual environments* (see [http://mng.bz/j2aV](http://mng.bz/j2aV)).
    With virtual environments, you can create and manage isolated sets of libraries.
    This is a great concept, but managing many virtual environments manually can be
    tedious as well.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个常见的解决方案是限制依赖图的大小，从而最大限度地减少冲突的可能性，即使用*虚拟环境*（见[http://mng.bz/j2aV](http://mng.bz/j2aV)）。使用虚拟环境，你可以创建和管理隔离的库集。这是一个很好的概念，但手动管理多个虚拟环境也可能很繁琐。
- en: '*Reproducibility*—pip install (or conda install) performs dependency resolution
    from scratch by default. This means that you may get a different set of libraries
    every time you run, for example, pip install tensorflow. Even if you require a
    specific version of TensorFlow, it is possible that its transitive dependencies
    have evolved over time. If you want to reproduce the results of a past execution,
    say, a run that happened a month ago, it might be practically impossible to determine
    the exact set of libraries used to produce the results.'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可重复性*——pip install（或conda install）默认情况下从头开始执行依赖项解析。这意味着每次运行，例如pip install
    tensorflow，你可能会得到一组不同的库。即使你要求特定的TensorFlow版本，它的传递依赖也可能随着时间的推移而演变。如果你想重现过去执行的结果，比如一个月前的一次运行，可能实际上无法确定用于产生结果的精确库集。'
- en: Note that virtual environments don’t help with this reproducibility problem
    by default, because pip install tensorflow is equally unpredictable inside a virtual
    environment. To get a stable execution environment, you need to freeze the whole
    virtual environment in itself.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，默认情况下，虚拟环境并不能帮助解决这个可重复性问题，因为pip install tensorflow在虚拟环境中同样不可预测。为了获得一个稳定的执行环境，你需要将整个虚拟环境本身冻结。
- en: The first problem hurts prototyping, as you can’t experiment with the latest
    libraries easily. The second problem hurts production, because production deployments
    may fail due to surprising changes in their libraries. These problems are universal
    to every infrastructure—they are not specific to Metaflow or any other technical
    approach.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题损害了原型设计，因为你不能轻易地实验最新的库。第二个问题损害了生产，因为生产部署可能会因为库中的意外变化而失败。这些问题对每个基础设施都是通用的——它们并不特定于Metaflow或任何其他技术方法。
- en: Containers for dependency management
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项管理容器
- en: Today, the most common solution for dependency management is to use a container
    image. As we briefly discussed in chapter 4, container images can encapsulate
    all of the layers in figure 6.12, including the operating system, albeit often
    the operating system *kernel*—the core of the operating system that interacts
    with hardware—is shared among many containers.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，最常见的依赖项管理解决方案是使用容器镜像。正如我们在第4章中简要讨论的那样，容器镜像可以封装图6.12中的所有层，包括操作系统，尽管操作系统*内核*——与硬件交互的操作系统核心——通常在许多容器之间共享。
- en: 'From the dependency management point of view, container images work like virtual
    environments, with similar pros and cons: they can help partition dependency graphs
    to avoid conflicts. A downside is that you need a system, such as a *continuous
    integration and deployment* (CI/CD) setup with a container registry, to create
    and manage a zoo of images. Most companies manage only a handful of production-ready
    images to reduce the amount of complexity.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从依赖管理角度来看，容器镜像就像虚拟环境一样工作，具有类似的优缺点：它们可以帮助划分依赖图以避免冲突。一个缺点是，你需要一个系统，例如具有容器注册库的
    *持续集成和持续部署*（CI/CD）设置，来创建和管理一系列的镜像。大多数公司只管理少量已准备好的生产镜像，以减少复杂性。
- en: Furthermore, although executing the same code on the same image guarantees a
    high level of reproducibility, producing reproducible images requires some effort.
    If you simply use pip install tensorflow in your image specification (e.g., Dockerfile),
    you have simply pushed the reproducibility problem one layer deeper.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管在相同的图像上执行相同的代码可以保证高度的再现性，但要生成可再现的图像则需要一些努力。如果你仅仅在图像规范（例如 Dockerfile）中使用
    pip install tensorflow，那么你只是将可再现性问题推向了更深的一层。
- en: Metaflow works well for container-based dependency management. Assuming you
    have a mechanism to create images, you can create an image that contains all the
    libraries you need and let Metaflow’s code package overlay the user code on top
    of the base image on the fly. This is a solid solution, especially for production
    deployments.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow 对于基于容器的依赖管理效果良好。假设你有一个创建镜像的机制，你可以创建一个包含所有所需库的镜像，并让 Metaflow 的代码包在运行时将用户代码覆盖在基础镜像之上。这是一个可靠的解决方案，特别是对于生产部署。
- en: For prototyping, a challenge is that creating and using containers locally is
    not straightforward. To address this shortcoming, Metaflow comes with built-in
    support for the Conda package manager, which combines an easy prototyping experience
    with stable production environments.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于原型设计来说，一个挑战是本地创建和使用容器并不直接。为了解决这一不足，Metaflow 内置了对 Conda 包管理器的支持，它将简单的原型设计体验与稳定的生成环境相结合。
- en: A pragmatic recipe for dependency management
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖管理的实用方法
- en: 'You can adopt a layered approach to dependency management, to balance the needs
    of rapid prototyping and stable production. A pragmatic recipe follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以采用分层的方法来管理依赖项，以平衡快速原型设计和稳定生产的需求。以下是一个实用方法：
- en: Define the DAG and simple steps inside a flow module. For simple flows and prototyping,
    this may be all you need. You can rely on whatever libraries you have installed
    locally.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流程模块内部定义 DAG 和简单步骤。对于简单的流程和原型设计，这可能就是你所需要的全部。你可以依赖你本地已安装的任何库。
- en: Create separate *supporting modules* for logically related sets of functions.
    A separate module can be shared across flows and can be used outside Metaflow,
    such as in a notebook. Separate modules are also amenable to testing, for example,
    using standard unit-testing frameworks like PyTest ([pytest.org](https://pytest.org)).
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为逻辑上相关的函数集创建单独的 *支持模块*。一个单独的模块可以在多个流程之间共享，也可以在 Metaflow 之外使用，例如在笔记本中。单独的模块也适合进行测试，例如使用标准的单元测试框架如
    PyTest ([pytest.org](https://pytest.org))。
- en: Use Python *packages to create custom libraries* that consist of multiple modules.
    As long as the package is in the same directory hierarchy as the main flow module,
    it will be included in the code package automatically.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python *包创建包含多个模块的定制库*。只要包与主流程模块位于相同的目录层次结构中，它就会自动包含在代码包中。
- en: Manage *third-party libraries* using @conda.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 @conda 管理第三方库。
- en: If you have complex dependency management needs that @conda can’t handle and/or
    your company has a working setup for creating *container images*, use them as
    an alternative or complement to @conda.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要复杂的依赖管理，而 @conda 无法处理，或者你的公司有创建 *容器镜像* 的工作流程，那么你可以将它们作为 @conda 的替代品或补充。
- en: 'These layers work well together: a complex project can consist of many flows,
    and these flows may share many modules and packages. They can run on a company-specific
    base image with project-specific dependencies overplayed on top of them using
    @conda.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层可以很好地协同工作：一个复杂的项目可以由许多流程组成，这些流程可能共享许多模块和包。它们可以在公司特定的基础镜像上运行，使用 @conda 在其上覆盖项目特定的依赖项。
- en: 6.2.3 Using the @conda decorator
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 使用 @conda 装饰器
- en: 'Conda ([https://conda.io](https://conda.io)) is an open source package manager
    that is widely used in the Python data science and machine learning ecosystem.
    Although Conda doesn’t solve all dependency management problems by itself, it
    is a solid tool that you can use to solve the problems described earlier. Metaflow
    provides a built-in integration with Conda for the following reasons:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Conda([https://conda.io](https://conda.io))是一个开源的包管理器，在Python数据科学和机器学习生态系统中被广泛使用。尽管Conda本身不能解决所有依赖管理问题，但它是一个可靠的工具，您可以使用它来解决前面描述的问题。Metaflow提供与Conda的内置集成，原因如下：
- en: The Conda ecosystem contains a huge number of ML and data science libraries.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conda生态系统包含大量的机器学习和数据科学库。
- en: Conda helps solve the conflict problem by providing built-in virtual environments
    and a robust dependency resolver. It allows us to solve the reproducibility problem
    by freezing environments, as we will see soon.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conda通过提供内置的虚拟环境和强大的依赖解析器来帮助解决冲突问题。正如我们很快就会看到的，它通过冻结环境来允许我们解决可重复性问题。
- en: Conda handles not only Python dependencies but also system libraries. This is
    an important feature for data science libraries in particular, because they contain
    many compiled components and non-Python transitive dependencies. As a bonus, Conda
    handles the Python interpreter itself as a dependency, so you can use different
    versions of Python.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conda不仅处理Python依赖项，还处理系统库。这对于数据科学库尤其重要，因为它们包含许多编译组件和非Python的间接依赖项。作为额外的好处，Conda还将Python解释器本身作为依赖项处理，因此您可以使用不同的Python版本。
- en: To see how Metaflow uses Conda to solve the dependency management problems in
    practice, let’s continue our forecasting example. The following code listing contained
    a skeleton flow that fetched input data—the temperature over the past five days—and
    plotted it. We will expand the flow in this code by adding a step, forecast, that
    performs the actual forecasting.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解Metaflow如何在实际中利用Conda解决依赖管理问题，让我们继续我们的预测示例。以下代码列表包含了一个骨架流程，该流程获取输入数据——过去五天的温度，并将其绘制出来。我们将通过添加一个执行实际预测的步骤forecast来扩展此代码中的流程。
- en: Listing 6.5 ForecastFlow with a forecast step
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5：具有预测步骤的ForecastFlow
- en: '[PRE30]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Schedules the forecast to run daily
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计划每天运行预测
- en: ❷ Replaces the default with your actual OpenWeatherData API token
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将默认值替换为您的实际OpenWeatherData API令牌
- en: ❸ The start step is exactly the same as before.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 开始步骤与之前完全相同。
- en: ❹ Creates a predictor that looks at the past 48 hours to predict the next 48
    hours
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建一个预测器，它查看过去48小时来预测接下来的48小时
- en: ❺ Saves the predictions in a plain Python list for easy access
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将预测保存为纯Python列表，以便于访问
- en: ❻ Plots the historical data and the forecast
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 绘制历史数据和预测数据
- en: Save the code to forecast2.py. Note that we added the @schedule decorator so
    the flow can be run on a production scheduler, Step Functions, automatically.
    This requires that you include your personal OpenWeatherMap API token as the default
    value in the appid parameter, because no custom parameter values can be specified
    for scheduled runs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到forecast2.py。请注意，我们添加了@schedule装饰器，以便流程可以在生产调度程序Step Functions上自动运行。这要求您在appid参数中将您的个人OpenWeatherMap
    API令牌作为默认值包含在内，因为无法为计划运行指定自定义参数值。
- en: 'The forecast step is the new and exciting part in this flow. It uses a particular
    method for time-series prediction called *the Theta method**,* implemented in
    Sktime by the class ThetaForecaster. You can learn the method in detail at [https://sktime.org](https://sktime.org).
    The method is useful for our temperature forecasting application because it accounts
    for seasonality. Looking at figure 6.13, it is clear that temperatures follow
    a diurnal cyclical pattern, at least in the desert city of Las Vegas. We use the
    past 48 hours to forecast the next 48 hours. Note that we store the predictions
    in an easily accessible pure-Python artifact, predictions, in addition to a pandas
    time-series pd_predictions, as discussed earlier in this chapter. We plot the
    predictions together with historical data in the plot step. Now you can run the
    following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 预测步骤是此流程中新颖且令人兴奋的部分。它使用一种特定的时间序列预测方法，称为**Theta方法**，由Sktime中的ThetaForecaster类实现。您可以在[https://sktime.org](https://sktime.org)详细了解该方法。由于该方法考虑了季节性，它对我们温度预测应用很有用。查看图6.13，很明显，温度遵循日循环模式，至少在拉斯维加斯这座沙漠城市是这样的。我们使用过去48小时的数据来预测接下来的48小时。请注意，我们除了存储在前面章节中讨论的pandas时间序列pd_predictions之外，还将预测存储在一个易于访问的纯Python对象predictions中。我们在绘图步骤中将预测与历史数据一起绘制。现在您可以运行以下命令：
- en: '[PRE31]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using a notebook, like we did before, we can plot the results as shown in figure
    6.13.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 使用笔记本，就像我们之前做的那样，我们可以将结果绘制成图6.13所示。
- en: '![CH06_F13_Tuulos](../../OEBPS/Images/CH06_F13_Tuulos.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F13_Tuulos](../../OEBPS/Images/CH06_F13_Tuulos.png)'
- en: Figure 6.13 Forecasted hourly temperatures for Las Vegas
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 拉斯维加斯的预测每小时温度
- en: Just by looking at figure 6.13, the forecast seems believable for this particular
    time series. As an exercise, you can test different methods and parameterizations
    for different locations. To make the exercise even more realistic, you can use
    OpenWeatherMap APIs to obtain real forecasts that are based on real meteorology
    and compare your forecasts to theirs programmatically. Also, you can back-test
    forecasts by seeing how well you could have forecasted historical data based on
    older historical data.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 只需看图6.13，这个特定时间序列的预测看起来是可信的。作为一个练习，你可以测试不同地点的不同方法和参数化。为了使练习更加真实，你可以使用OpenWeatherMap
    API获取基于真实气象学的实际预测，并通过程序将你的预测与他们的预测进行比较。此外，你也可以通过查看基于更早的历史数据，你能够多好地预测历史数据来回测预测。
- en: Discovering Conda packages
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 发现Conda包
- en: In all examples so far, we have simply provided a predefined list of libraries
    for every @conda decorator. How do you find library versions that are available
    in the first place?
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有之前的示例中，我们只是为每个@conda装饰器提供了一个预定义的库列表。你如何找到最初可用的库版本？
- en: Conda has the concept of Channels, which correspond to different providers of
    packages. The original company behind Conda, Anaconda, maintains default channels
    for Conda. Another common channel is Conda Forge, which is a community-maintained
    repository of Conda packages. You can set up a custom, private Conda channel,
    too.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Conda有通道的概念，这对应于不同的包提供者。Conda背后的原始公司Anaconda维护了Conda的默认通道。另一个常见的通道是Conda Forge，这是一个社区维护的Conda包仓库。你还可以设置一个自定义的私有Conda通道。
- en: To find packages and available versions, you can either use the conda search
    command on the command line, or you can search packages at [https://anaconda.org.](https://anaconda.org)
    (Note that .org is the community site whereas .com refers to the company.)
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找包和可用版本，你可以在命令行上使用conda search命令，或者你可以在[https://anaconda.org.](https://anaconda.org)搜索包（注意，.org是社区网站，而.com指的是公司）。
- en: 'This book is not about time-series forecasting, but rather about infrastructure,
    so we will focus on unpacking how the @conda decorator works. After you start
    a run, but before any tasks execute, @conda performs the following sequence of
    actions:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书不是关于时间序列预测，而是关于基础设施，因此我们将专注于解释@conda装饰器的工作原理。在你开始运行之前，但在任何任务执行之前，@conda执行以下序列操作：
- en: It walks through every step of the flow and determines which virtual environments
    need to be created. Every unique combination of Python version and libraries requires
    an isolated environment.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它会遍历流程的每一步，并确定需要创建哪些虚拟环境。每个Python版本和库的唯一组合都需要一个隔离的环境。
- en: If an existing environment is found locally that has the right Python version
    and libraries, it can be used without changes. This is why subsequent executions
    are faster than the first one.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在本地找到一个具有正确Python版本和库的现有环境，则无需更改即可使用。这就是为什么后续执行比第一次更快的原因。
- en: If an existing environment is not found, Conda is used to perform dependency
    resolution to resolve the full list of libraries, including transitive dependencies,
    which need to be installed.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果找不到现有环境，Conda将用于执行依赖关系解析，以解析需要安装的完整库列表，包括传递依赖项。
- en: Installed libraries are uploaded in the datastore, such as S3, to make sure
    they can be accessed quickly and reliably by all tasks. A fast internet connection
    or using a cloud-based workstation helps make this step fast.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 已安装的库被上传到数据存储中，如S3，以确保所有任务都可以快速可靠地访问。快速的网络连接或使用基于云的工作站有助于使这一步骤更快。
- en: 'This sequence ensures that there’s a stable execution environment for each
    step, which includes all libraries requested. Reflecting on the problems of dependency
    management, note that we did the following here:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这个序列确保每个步骤都有一个稳定的执行环境，包括所有请求的库。反思依赖关系管理的问题，请注意我们在这里做了以下操作：
- en: We minimize the likelihood of dependency conflicts by having minimally small
    environments—a separate virtual environment for each step. Maintaining environments
    this granular would be quite infeasible to do manually, but Metaflow handles it
    for us automatically.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过为每个步骤创建一个最小化的独立虚拟环境来最小化依赖冲突的可能性。手动维护如此细粒度的环境几乎是不切实际的，但Metaflow会自动为我们处理。
- en: Dependency resolution is performed only once to reduce the likelihood of surprises
    during development.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅执行一次依赖项解析以减少开发过程中出现意外的情况的可能性。
- en: The list of dependencies is declared in the code itself, so the version information
    is stored in a version control system both by Metaflow and also by Git, if you
    use Git to store your workflows. This ensures a decent level of reproducibility,
    because you and your colleagues have an explicit declaration of the dependencies
    needed to reproduce results.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖项列表在代码本身中声明，因此版本信息由Metaflow和Git（如果您使用Git存储您的工作流程）存储。这确保了相当高的可重复性，因为您和您的同事有明确的依赖项声明，这些依赖项是重现结果所需的。
- en: Unsafe steps
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 不安全步骤
- en: Note that when you use —environment=conda, all steps are executed in an isolated
    Conda environment, even if they don’t specify an explicit @conda decorator. For
    instance, the end step in listing 6.5 is executed in a bare-bones environment
    with no extra libraries, because it didn’t specify any library requirements. You
    can’t import any libraries (outside Metaflow itself) that are not explicitly listed
    in @conda. This is a feature, not a bug—it ensures that steps don’t accidentally
    rely on libraries that are not declared in the code, which is a critical feature
    to ensure *reproducibility*.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当您使用—environment=conda时，所有步骤都在一个隔离的Conda环境中执行，即使它们没有指定显式的@conda装饰器。例如，列表6.5中的最后一步在一个没有任何额外库的裸机环境中执行，因为它没有指定任何库要求。您不能导入任何未在@conda中明确列出的库（除了Metaflow本身之外的库）。这是一个特性，而不是错误——它确保步骤不会意外地依赖于代码中未声明的库，这是确保*可重复性*的关键特性。
- en: However, in some special cases, you may need to mix isolated steps with “unsafe”
    steps. For instance, your workstation or the underlying container image may contain
    libraries that you can’t install with Conda. You can declare a step unsafe by
    adding the decorator @conda(disabled=True), which makes the step execute as if
    no Conda was used. Note that doing this negates many benefits of Conda, especially
    when it comes to production deployments discussed later.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些特殊情况下，您可能需要将隔离步骤与“不安全”步骤混合。例如，您的工作站或底层容器镜像可能包含您无法使用Conda安装的库。您可以通过添加装饰器@conda(disabled=True)来声明步骤不安全，这将使步骤执行时仿佛没有使用Conda。请注意，这样做会否定许多Conda的好处，尤其是在后面讨论的生产部署中。
- en: The @conda decorator in the cloud
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 云中的@conda装饰器
- en: 'Remarkably, you can run exactly the same code in the cloud as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，您可以在云中运行完全相同的代码，如下所示：
- en: '[PRE32]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When running with a cloud-based compute layer like AWS Batch, the execution
    begins with the same sequence of operations as listed earlier. However, Metaflow
    needs to perform extra work to recreate the virtual environments in containers
    on the fly. Here’s what happens in the cloud before a task executes:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用AWS Batch等基于云的计算层运行时，执行开始时与前面列出的操作序列相同。然而，Metaflow需要执行额外的工作来在容器中动态地重新创建虚拟环境。以下是任务执行之前云中发生的情况：
- en: The compute layer launches a preconfigured container image. Notably, the container
    image doesn’t need to include Metaflow, the user code, or its dependencies. Metaflow
    overlays the execution environment on top of the image.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算层启动一个预配置的容器镜像。值得注意的是，容器镜像不需要包含Metaflow、用户代码或其依赖项。Metaflow在镜像之上叠加执行环境。
- en: The code package includes the exact list of libraries that need to be installed.
    Notably, dependency resolution is not run again, so all tasks are guaranteed to
    have precisely the same environments. Note that this wouldn’t be the case if you
    ran pip install some_package inside your step code. It is possible that tasks
    would end up with slightly different execution environments, leading to hard-to-debug
    failures.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码包包括需要安装的确切库列表。值得注意的是，不会再次运行依赖项解析，因此所有任务都保证具有完全相同的执行环境。请注意，如果您在步骤代码中运行pip install
    some_package，情况可能并非如此。任务最终可能具有略微不同的执行环境，导致难以调试的故障。
- en: Metaflow pulls the required libraries from its own datastore where they have
    been cached. This is critical for two reasons. First, imagine running a wide foreach,
    for example, hundreds of instances in parallel. If all of them hit an upstream
    package repository in parallel, it would amount to a *distributed denial of service
    attack*—the package repository can refuse to serve the files to so many parallel
    clients. Second, occasionally the package repositories delete or change files,
    which could lead to a task failing—again in a hard-to-debug fashion.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Metaflow 从其自己的数据存储中拉取所需的库，这些库已经被缓存。这有两个关键原因。首先，想象一下运行一个宽泛的 foreach，例如，数百个实例并行运行。如果它们都并行地访问上游包仓库，这将相当于一种
    *分布式拒绝服务攻击*——包仓库可以拒绝向这么多并行客户端提供服务。其次，偶尔包仓库会删除或更改文件，这可能导致任务失败——再次以难以调试的方式。
- en: These steps guarantee that you can *prototype quickly* and locally with your
    favorite libraries that can be specific to each project, and you can execute the
    same code at scale in the cloud without having to worry about the execution environment.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤确保你可以快速本地原型化，使用你喜欢的库，这些库可能对每个项目都是特定的，并且你可以在云中按比例执行相同的代码，而无需担心执行环境。
- en: 'We can piggyback on the same mechanism to achieve robust production deployments.
    Let’s test the idea by deploying the forecasting flow to production, as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用相同的机制来实现健壮的生产部署。让我们通过以下方式将预测流程部署到生产环境中来测试这个想法：
- en: '[PRE33]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Similar to the run command, step-functions create performs the four steps of
    dependency resolutions prior to the production deployment. Consequently, the production
    deployment is guaranteed to be isolated from any changes in the flow code (thanks
    to the code package), as well as changes in its dependencies (thanks to @conda)
    and transient errors in package repositories (thanks to package caching in the
    datastore). All in all, you are guaranteed to have stable execution environments.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 与运行命令类似，step-functions create 在生产部署之前执行四个依赖解析步骤。因此，生产部署将保证与流程代码的任何更改（多亏了代码包）以及其依赖项的更改（多亏了
    @conda）以及数据存储中包的暂时性错误（多亏了包缓存）隔离。总的来说，你将保证拥有稳定的执行环境。
- en: Congratulations—you just deployed a real data science application to production!
    You can use the Step Functions UI to observe daily runs. As an exercise, you can
    create a notebook that plots daily forecasts in a single view.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你刚刚将一个真实的数据科学应用部署到生产环境中！你可以使用 Step Functions UI 来观察日常运行。作为一个练习，你可以创建一个笔记本，在单个视图中绘制每日预测。
- en: Flow-level dependencies with @conda_base
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 @conda_base 的流程级依赖
- en: 'Notice how listing 6.5 contains the following same set of dependencies for
    the start and forecast steps:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 注意列表 6.5 中开始和预测步骤包含以下相同的依赖集：
- en: '[PRE34]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The plot step has only one additional library. As the number of steps increases,
    it may start feeling redundant to add the same dependencies to every step. As
    a solution, Metaflow provides a flow-level @conda_base decorator that specifies
    attributes shared by all steps. Any step-specific additions can be specified with
    a step-level @conda. The following listing shows an alternative version of ForecastFlow
    that takes this approach. The function bodies are equal to listing 6.5, so they
    are omitted for brevity.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图步骤只有一个额外的库。随着步骤数量的增加，可能开始觉得在每一步都添加相同的依赖项显得冗余。作为一个解决方案，Metaflow 提供了一个流程级 @conda_base
    装饰器，它指定了所有步骤共享的属性。任何特定步骤的附加项都可以使用步骤级 @conda 指定。以下列表显示了 ForecastFlow 的一个采用此方法的替代版本。函数体与列表
    6.5 相同，因此为了简洁起见省略了它们。
- en: Listing 6.6 Demonstrating @conda_base
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.6 展示了 @conda_base 的使用
- en: '[PRE35]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Uses @conda_base to define a common Python version and libraries
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 @conda_base 定义一个公共 Python 版本和库
- en: ❷ Uses a step-level @conda to add step-level additions to the common base
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用步骤级的 @conda 向公共基础添加步骤级附加
- en: 'This concludes our exploration into dependency management for now. We will
    use and expand these lessons later in chapter 9, which presents a realistic machine
    learning application using pluggable dependencies. Next, we will address another
    important element of production deployments that is a common source of sporadic
    failures: human beings.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们暂时结束了对依赖管理的探索。我们将在第 9 章中使用和扩展这些经验，该章将展示一个使用可插拔依赖项的现实机器学习应用。接下来，我们将解决生产部署的另一个重要元素，这是间歇性失败的一个常见来源：人类。
- en: 6.3 Stable operations
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 稳定操作
- en: '*An intern, Finley, joins the company for the summer. Finley has a strong theoretical
    background in Bayesian statistics. Alex suggests that they could organize a fun
    internal competition to compare the performance of a Bayesian model created by
    Finley to a neural network model that Alex has wanted to build. Initially, Alex
    is overjoyed to see that the neural network model seems to be working better in
    the benchmark. However, as they validate the final results, they notice that Alex’s
    prediction workflow had accidentally used Finley’s model, so, in fact, Finley
    is the winner. If only Alex had been more careful in organizing the experiment,
    they could have spent more time perfecting the model instead of being misled by
    incorrect results.*'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '*一名实习生，芬利，加入了公司度过夏天。芬利在贝叶斯统计方面有很强的理论基础。亚历克斯建议他们可以组织一场有趣的内部竞赛，比较芬利创建的贝叶斯模型和亚历克斯一直想构建的神经网络模型的性能。起初，亚历克斯看到神经网络模型在基准测试中似乎表现更好，非常高兴。然而，当他们验证最终结果时，他们注意到亚历克斯的预测工作流程意外地使用了芬利的模型，所以实际上芬利是赢家。如果亚历克斯在组织实验时更加小心，他们本可以花更多的时间完善模型，而不是被错误的结果误导。*'
- en: '![CH06_F13_UN03_Tuulos](../../OEBPS/Images/CH06_F13_UN03_Tuulos.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F13_UN03_Tuulos](../../OEBPS/Images/CH06_F13_UN03_Tuulos.png)'
- en: 'Imagine you have deployed a workflow to a production scheduler for the first
    time, as discussed in the previous sections. For most projects, this is merely
    the beginning, not the end. Increasingly often, the data scientists who develop
    workflows are also responsible for operating them in production. Hence, the data
    scientist has two responsibilities: first, keeping the production workflows running
    without interruptions, and second, continuing development on the workflow to improve
    results.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你第一次将工作流程部署到生产调度器中，正如前几节所讨论的那样。对于大多数项目来说，这仅仅是开始，而不是结束。越来越频繁的是，开发工作流程的数据科学家也负责在生产中运行它们。因此，数据科学家有两个职责：首先，确保生产工作流程不间断地运行，其次，继续开发工作流程以改进结果。
- en: 'A challenge is that the two responsibilities have diametrically opposite goals:
    the production workflows need to be as stable as possible, whereas prototyping
    may require drastic changes in the project. The key to solving this dilemma is
    to keep production workflows clearly isolated from prototyping, so that no matter
    what happens in the prototyping environment, it can’t affect production, and vice
    versa.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一个挑战是，这两个职责有截然相反的目标：生产工作流程需要尽可能稳定，而原型可能需要在项目中做出剧烈的改变。解决这个困境的关键是将生产工作流程与原型明确隔离，这样无论原型环境中发生什么，都不会影响生产，反之亦然。
- en: In a larger project, you may not have only a single prototyping version and
    a single production version. Instead, a team of data scientists can work on various
    prototypes concurrently. To test the experimental versions, they may be deployed
    to run side by side with a production version in a production-like environment.
    All in all, you can have any number of versions of a project running concurrently
    at different levels of maturity. All versions of the project must stay neatly
    isolated from each other to make sure that the results are clean from any interference.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个更大的项目中，你可能不仅仅只有一个原型版本和一个生产版本。相反，一组数据科学家可以同时工作在多个原型上。为了测试实验版本，他们可能被部署到与生产版本并行运行的生产环境。总的来说，你可以有任意数量的项目版本在不同的成熟度级别上同时运行。所有项目版本都必须保持整洁的隔离，以确保结果不受任何干扰。
- en: To make this idea more concrete, imagine a data science application, say, a
    recommendation system, which is under continuous development by a team of data
    scientists. The team’s mandate is to improve the system by pushing experiments
    through an *experimentation funnel*, depicted in figure 6.14.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个想法更加具体，想象一个数据科学应用，比如一个推荐系统，它由一组数据科学家持续开发。团队的使命是通过一个*实验漏斗*推动实验，如图6.14所示。
- en: '![CH06_F14_Tuulos](../../OEBPS/Images/CH06_F14_Tuulos.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F14_Tuulos](../../OEBPS/Images/CH06_F14_Tuulos.png)'
- en: Figure 6.14 The experimentation funnel
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 实验漏斗
- en: At the top of the funnel, the team has tens or hundreds of ideas how the system
    could be improved. Members of the team can prototype and test a subset of prioritized
    ideas on their local workstations. Initial results from prototypes can help to
    determine which ideas warrant further development—it is expected that not all
    ideas will survive. Additional rounds of local development may follow.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在漏斗的顶部，团队有数十或数百种想法来改进系统。团队成员可以在他们的本地工作站上对优先级较高的想法进行原型设计和测试。原型设计的初步结果可以帮助确定哪些想法值得进一步开发——预期并非所有想法都能存活。可能还会进行额外的本地开发轮次。
- en: Once you have a fully functional experimental workflow (or a set of workflows),
    often you will want to run an A/B experiment that compares the new version to
    the current production version in a live production environment. This requires
    that you have two or more test deployments running side by side. After some time,
    you can analyze the results to determine whether the new version should be promoted
    to be the new production version.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有一个完全功能的实验工作流程（或一系列工作流程），通常你会在实际的生产环境中运行一个A/B测试，将新版本与当前生产版本进行比较。这要求你同时运行两个或更多测试部署。经过一段时间后，你可以分析结果以确定新版本是否应该升级为新的生产版本。
- en: The versioning layer
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制层
- en: The versioning layer helps to organize, isolate, and track all versions so it
    becomes feasible to manage even hundreds of concurrent versions and projects that
    are present at different levels of the experimentation funnel. In terms of our
    infrastructure stack, the versioning layer helps to determine *what versions of
    the code* are executed, as depicted in figure 6.15.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制层有助于组织、隔离和跟踪所有版本，使其成为管理数百个并发版本和项目的可行方法，这些版本和项目存在于实验漏斗的不同层级。就我们的基础设施堆栈而言，版本控制层有助于确定*执行什么版本的代码*，如图6.15所示。
- en: '![CH06_F15_Tuulos](../../OEBPS/Images/CH06_F15_Tuulos.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F15_Tuulos](../../OEBPS/Images/CH06_F15_Tuulos.png)'
- en: 'Figure 6.15 The role of the versioning layer: What code gets executed'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 版本控制层的作用：执行什么代码
- en: Naturally the data scientist needs to write the code, that is, the actual business
    logic of the workflow in the first place. This is a concern of the (software)
    *architecture layer*, which we will discuss in chapter 8\. It is hard to abstract
    away or generalize the business logic, which tends to be very specific to each
    project and use case, so we expect the data scientist to exercise a good deal
    of freedom and responsibility when it comes to developing it. Hence, it makes
    sense to place the architecture layer toward the top of the infrastructure stack
    where the infrastructure should be less constraining.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，数据科学家需要编写代码，即工作流程的实际业务逻辑。这是（软件）*架构层*的关心问题，我们将在第8章中讨论。抽象或泛化业务逻辑很难，它往往非常具体于每个项目和用例，因此我们期望数据科学家在开发时能够行使相当大的自由度和责任感。因此，将架构层放置在基础设施堆栈的顶部是有意义的，在那里基础设施应该施加较少的限制。
- en: In contrast, the infrastructure can be more opinionated about versioning—keeping
    things neatly organized and versions clearly separated isn’t a matter of personal
    preference but rather an organizational requirement. It is highly beneficial for
    everyone to use the same approach to versioning to facilitate collaboration and
    to avoid conflicts (like Alex and Finley’s scenario earlier), which is another
    motivation for the infrastructure to provide a built-in versioning layer.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基础设施在版本控制方面可能更有意见——保持事物整洁有序、版本清晰分离不是个人偏好的问题，而是一个组织要求。对每个人来说，使用相同的版本控制方法来促进协作并避免冲突（如之前提到的Alex和Finley的情况）是非常有益的，这也是基础设施提供内置版本控制层的一个动机。
- en: 'We can summarize the roles of the highlighted layers in figure 6.15, from the
    top down, as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结图6.15中突出层的作用，从上到下如下：
- en: The data scientist designs and develops the business logic, that is, the architecture
    of the workflow (the architecture layer).
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学家设计和开发业务逻辑，即工作流程的架构（架构层）。
- en: The infrastructure helps to manage multiple concurrently prototyped and deployed
    versions of the business logic (the versioning layer), to facilitate an experimentation
    funnel. Together, the architecture and the versioning layer determine *what* code
    is executed.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基础设施有助于管理多个同时原型设计和部署的业务逻辑版本（版本控制层），以促进实验漏斗。一起，架构层和版本控制层决定了*执行什么代码*。
- en: A robust production scheduler (the job scheduler layer), such as AWS Step Functions,
    determines *how* and when a particular workflow DAG is executed.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个健壮的生产调度器（作业调度层），如AWS Step Functions，确定特定工作流程DAG的执行方式和时间。
- en: Finally, the compute layer is responsible for finding a server instance *where*
    each task of the workflow can be executed.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算层负责找到服务器实例*在哪里*可以执行工作流程中的每个任务。
- en: 'As discussed earlier, a major benefit of separating the “what, how, and where”
    is that we can design each subsystem independently. This section shows a reference
    implementation of the versioning layer as provided by Metaflow, but you could
    use another framework or approach to achieve the same goal: building and operating
    an experimentation funnel that allows a team of data scientists to iterate and
    test various versions of applications frictionlessly.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，将“什么、如何和在哪里”分开的一个主要好处是我们可以独立设计每个子系统。本节展示了Metaflow提供的版本层的一个参考实现，但你也可以使用另一个框架或方法来实现相同的目标：构建和运行一个实验漏斗，允许数据科学团队无摩擦地迭代和测试应用程序的各种版本。
- en: There isn’t a single right way of building the funnel. We give you a set of
    tools and knowledge that help you design and customize an approach to versioning
    and deployments that work for your specific needs. For instance, many companies
    have developed their own custom wrapper scripts or CI/CD (continuous integration/continuous
    deployment) pipelines that leverage the mechanisms presented next. We begin the
    section by looking at versioning during prototyping and, after that, how to achieve
    safely isolated production deployments.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 构建漏斗没有唯一正确的方法。我们提供了一套工具和知识，帮助你设计并定制一个适合你特定需求的版本控制和部署方法。例如，许多公司已经开发了他们自己的自定义包装脚本或CI/CD（持续集成/持续部署）管道，利用了下面介绍的机制。我们开始本节，先看看原型化过程中的版本控制，然后是如何实现安全隔离的生产部署。
- en: 6.3.1 Namespaces during prototyping
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 原型化过程中的命名空间
- en: 'Consider the scenario that Alex and Finley went through in the beginning of
    this section: both of them were prototyping alternative workflows for the same
    project. Accidentally, Alex analyzed Finley’s results instead of his own. This
    highlights the need of staying organized during prototyping: each prototype should
    stay clearly isolated from others.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下本节开头Alex和Finley所经历的情景：他们两人都在为同一个项目原型化不同的工作流程。意外的是，Alex分析了Finley的结果而不是自己的。这突出了在原型化过程中保持组织的重要性：每个原型都应该与其他原型保持清晰的隔离。
- en: Metaflow has the concept of *namespaces* that help keep runs and artifacts organized.
    Let’s demonstrate it with a simple workflow shown next.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow有一个名为*命名空间*的概念，有助于保持运行和工件的组织。让我们通过下面的简单工作流程来演示它。
- en: Listing 6.7 A flow showing namespaces in action
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 显示命名空间作用的工作流程
- en: '[PRE36]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Prints the current namespace
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打印当前命名空间
- en: 'Save the code in namespaceflow.py, and execute it as usual:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存在namespaceflow.py中，并像往常一样执行：
- en: '[PRE37]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should see an output that mentions your username, for instance:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一个提到你用户名的输出，例如：
- en: '[PRE38]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now open a Python interpreter or a notebook, and execute the following lines:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开一个Python解释器或笔记本，并执行以下行：
- en: '[PRE39]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This will print the run ID of the latest run *in the user’s namespace*. In
    the case of the previous example, it will show the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印用户命名空间中最新运行的运行ID。在先前的例子中，它将显示以下内容：
- en: '[PRE40]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note how the run ID 1625945750782199 matches the latest executed run. You can
    also execute get_namespace() to confirm that the namespace is indeed the same
    as was used by the flow.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 注意运行ID 1625945750782199与最新执行的运行匹配。你还可以执行get_namespace()来确认命名空间确实与flow使用的相同。
- en: 'To show that latest-run works as expected, let’s run the flow again, as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明latest-run按预期工作，让我们再次运行flow，如下所示：
- en: '[PRE41]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now the Run ID is 1625946102336634. If you test latest_run again, you should
    see this ID.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的运行ID是1625946102336634。如果你再次测试latest_run，你应该看到这个ID。
- en: 'Next, let’s test how namespaces work when multiple users are working together.
    Execute the following command, which simulates another user, otheruser, running
    the flow concurrently:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们测试当多个用户一起工作时命名空间是如何工作的。执行以下命令，该命令模拟另一个用户otheruser同时运行flow：
- en: '[PRE42]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note that you are not supposed to set USER explicitly in real life. The variable
    is set automatically by your workstation. We set it here explicitly only to demonstrate
    how Metaflow behaves in the presence of multiple users.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在现实生活中，你不应该显式设置USER。该变量由你的工作站自动设置。我们在这里显式设置它只是为了演示Metaflow在存在多个用户时的行为。
- en: For this run, the ID is 1625947446325543, and the namespace is user:otheruser.
    Figure 6.16 summarizes the executions in each namespace.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个运行，ID是1625947446325543，命名空间是user:otheruser。图6.16总结了每个命名空间中的执行情况。
- en: '![CH06_F16_Tuulos](../../OEBPS/Images/CH06_F16_Tuulos.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F16_Tuulos](../../OEBPS/Images/CH06_F16_Tuulos.png)'
- en: Figure 6.16 Namespaces of two users, each with their own latest_run
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 两位用户的命名空间，每人都有自己的最新运行
- en: 'Now if you examine latest_run again, you will see that it still returns 1625946102336634,
    that is, Ville’s latest run, and not the absolute newest run ID that was executed
    by otheruser. The reason is that the Client API respects the current namespace
    by default: it doesn’t return the latest_run across all users but returns *your
    latest run*.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果您再次检查最新运行，您将看到它仍然返回1625946102336634，即维莱的最新运行，而不是其他用户执行的绝对最新的运行ID。原因是客户端API默认尊重当前命名空间：它不会返回所有用户的最新运行，而是返回*您的最新运行*。
- en: 'Built-in namespaces avoid situations like what Alex and Finley faced: It would
    be confusing if, say, your notebook that uses latest_run started showing different
    results because a colleague of yours executed the flow. By keeping metadata and
    artifacts namespaced, we can avoid such surprises.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 内置命名空间避免了像亚历克斯和芬利遇到的情况：如果你的笔记本使用的是最新运行，而你的同事执行了流程，那么它显示不同的结果将会很令人困惑。通过将元数据和工件名称空间化，我们可以避免这样的惊喜。
- en: 'Besides latest_run, you can refer to any specific run by their ID, like here:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 除了最新运行之外，您还可以通过它们的ID引用任何特定的运行，例如这里：
- en: '[PRE43]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This will work because the specific run is in the current namespace. In contrast,
    try this:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这将有效，因为特定的运行在当前命名空间中。相比之下，尝试以下操作：
- en: '[PRE44]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This will produce the following exception
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下异常
- en: '[PRE45]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: because the requested flow doesn’t exist in the current namespace—it belongs
    to otheruser. This behavior ensures that you don’t accidentally refer to someone
    else’s results, if, for example, you mistype a run ID.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 因为请求的流程当前命名空间中不存在——它属于其他用户。这种行为确保了您不会意外地引用其他人的结果，例如，如果您误输了运行ID。
- en: Note By default, the Client API allows you to inspect only runs and artifacts
    that you have produced. Actions taken by other users won’t affect results returned
    by your Client API, unless you switch namespaces explicitly. In particular, a
    relative reference like latest_run is safe to use because it refers to the latest
    run in the current namespace and, hence, its return value can’t change unexpectedly.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：默认情况下，客户端API允许您检查您生成的运行和工件。其他用户采取的操作不会影响您的客户端API返回的结果，除非您明确切换命名空间。特别是，相对引用如最新运行是安全的，因为它指的是当前命名空间中的最新运行，因此其返回值不会意外改变。
- en: Switching namespaces
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 切换命名空间
- en: 'Namespaces are not a security feature. They are not meant to hide information;
    they just help in keeping things organized. You can inspect the results of any
    other user simply by switching the namespace. For instance, try the following:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间不是一个安全特性。它们不是为了隐藏信息；它们只是帮助保持事物组织有序。您可以通过切换命名空间来检查任何其他用户的成果。例如，尝试以下操作：
- en: '[PRE46]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Use the namespace function to switch to another namespace. After a namespace
    call, the Client API accesses objects under the new namespace. Hence, it is possible
    to access a run by otheruser 1625947446325543. Correspondingly,
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命名空间函数切换到另一个命名空间。在命名空间调用之后，客户端API访问新命名空间下的对象。因此，可以访问其他用户1625947446325543的运行。相应地，
- en: '[PRE47]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: returns 1625947446325543. As you can expect, in this namespace you would get
    an error when accessing Ville’s runs.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 返回1625947446325543。正如您所预期的，在这个命名空间中，当访问维莱的运行时，您将得到一个错误。
- en: 'The namespace call is a convenient way to switch namespaces, for example, in
    a notebook. However, the Client API is also used inside flows to access data from
    other flows. For instance, remember ClassifierPredictFlow in chapter 3 that used
    the following lines to access the latest trained model:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间调用是一个方便的方法来切换命名空间，例如，在笔记本中。然而，客户端API也被用于流程内部以访问来自其他流程的数据。例如，记住第3章中提到的ClassifierPredictFlow，它使用了以下行来访问最新的训练模型：
- en: '[PRE48]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The Client API respects namespace also inside flows. The previous latest_run
    would return only models trained by your ClassifierTrainFlow. Now imagine that
    you wanted to use a model trained by your colleague, Alice. You could add a line,
    namespace(''user:alice''), in the flow code to switch the namespace. However,
    what if the next day you want to try a model by another colleague, Bob? You could
    keep changing the code back and forth, but there’s a better approach. Without
    changing anything in the code, you can switch namespaces on the command line with
    the --namespace option as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端API在流程内部也尊重命名空间。之前的latest_run只会返回由你的ClassifierTrainFlow训练的模型。现在假设你想要使用你的同事Alice训练的模型。你可以在流程代码中添加一行，namespace('user:alice')，以切换命名空间。然而，如果你第二天想尝试另一位同事Bob的模型呢？你可以不断更改代码，但有一个更好的方法。无需更改代码，你可以在命令行上使用--namespace选项切换命名空间，如下所示：
- en: '[PRE49]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This makes it easy to switch between different inputs without having to hardcode
    anything in the code itself. Figure 6.17 illustrates the idea.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得在不同输入之间切换变得容易，而无需在代码本身中硬编码任何内容。图6.17说明了这个想法。
- en: '![CH06_F17_Tuulos](../../OEBPS/Images/CH06_F17_Tuulos.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F17_Tuulos](../../OEBPS/Images/CH06_F17_Tuulos.png)'
- en: Figure 6.17 Switching between two namespaces
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 在两个命名空间之间切换
- en: Switching namespaces changes only how the Client API *reads* data. It doesn’t
    change how the results are stored—they are always attached to your username. Reading
    from any namespace is a safe operation in the sense that you can’t accidentally
    overwrite or corrupt existing data. By limiting writes to your own namespace by
    default, you can be sure that your actions won’t have unintended side effects
    for other users.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 切换命名空间只会改变客户端API*读取*数据的方式。它不会改变结果存储的方式——它们始终附加到你的用户名下。从任何命名空间读取是一个安全的操作，因为你不可能意外地覆盖或损坏现有数据。通过默认限制写入到自己的命名空间，你可以确保你的操作不会对其他用户产生不期望的副作用。
- en: Note Switching namespace with the namespace function or the --namespace option
    changes only the way the Client API reads results. It doesn’t change the way results
    are written. They still belong to the current user’s namespace by default.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：使用namespace函数或--namespace选项切换命名空间只会改变客户端API读取结果的方式。它不会改变结果写入的方式。默认情况下，它们仍然属于当前用户的命名空间。
- en: Global namespace
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 全局命名空间
- en: What if you see a run ID like NamespaceFlow/1625947446325543 in a log file,
    but you have no idea who started the run? You wouldn’t know which namespace to
    use. In situations like this, you can disable the namespace safeguards by calling
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在日志文件中看到一个运行ID，例如NamespaceFlow/1625947446325543，但你不知道是谁启动了这次运行？你不知道应该使用哪个命名空间。在这种情况下，你可以通过调用以下命令来禁用命名空间保护：
- en: '[PRE50]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: After this, you can access any object (runs, artifacts, etc.) without limitation.
    The latest _run will refer to the latest run executed by anyone, so its value
    may change at any time.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你可以无限制地访问任何对象（运行、工件等）。最新的_run将指向任何人执行的最新运行，因此其值可能会随时更改。
- en: Recommendation Don’t use namespace(None) inside flows, because it exposes the
    flow to unintended side effects caused by other people (or even yourself, inadvertently)
    running flows. It can be a handy tool for exploring data, such as in a notebook.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：不要在流程中使用namespace(None)，因为它会使流程暴露于其他人员（甚至可能是你自己，无意中）运行流程所引起的不期望的副作用。它可以是一个探索数据的便捷工具，例如在笔记本中。
- en: 6.3.2 Production namespaces
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 生产命名空间
- en: The previous section discussed namespaces during prototyping. In this context,
    it is natural to namespace runs by user, because, factually, there’s always a
    single unambiguous user who executes the command run. But what about production
    deployments? There’s no one executing a run, so whose namespace should we use?
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节讨论了原型设计中的命名空间。在这种情况下，按用户命名空间运行是自然而然的，因为实际上，总是有一个单一的、明确的用户执行命令运行。但关于生产部署呢？没有人在执行运行，那么我们应该使用谁的命名空间？
- en: 'Metaflow creates a new *production namespace* for each production deployment
    that is not attached to any user. Let’s see what this means in practice by deploying
    the namespaceflow.py from listing 6.7 to AWS Step functions as follows:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: Metaflow为每个未附加到任何用户的、新的生产部署创建一个新的*生产命名空间*。让我们通过以下方式将列表6.7中的namespaceflow.py部署到AWS
    Step functions，以了解这实际上意味着什么：
- en: '[PRE51]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You should see an output like this:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似以下的输出：
- en: '[PRE52]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: As the output indicates, a new unique namespace, production:namespaceflow-0-fyaw,
    was created for the deployment. As you can see, the namespace is not tied to a
    user, like user:ville, which we used during prototyping.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如输出所示，为部署创建了一个新的唯一命名空间，production:namespaceflow-0-fyaw。如您所见，该命名空间并不绑定到用户，例如我们在原型设计期间使用的user:ville。
- en: If you run step-functions create again, you will notice that the production
    namespace doesn’t change. The deployment’s namespace is tied to the flow name.
    It doesn’t change unless you explicitly request a new namespace by executing
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您再次运行步骤函数创建，您会注意到生产命名空间不会改变。部署的命名空间绑定到流程名称。除非您通过执行来显式请求新的命名空间，否则它不会改变。
- en: '[PRE53]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'To see the namespace in action, let’s trigger an execution on Step Functions
    like so:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看命名空间的实际操作，让我们在步骤函数上触发一个执行，如下所示：
- en: '[PRE54]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Wait for a minute or two for the execution to start. After this, open a notebook
    or a Python interpreter, and execute the following lines. Replace the namespace
    with the actual unique namespace that was output by step-functions create:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 等待一分钟左右，让执行开始。之后，打开一个笔记本或Python解释器，并执行以下行。将命名空间替换为step-functions create输出的实际唯一命名空间：
- en: '[PRE55]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: You should see a Run object with a long ID prefixed with sfn-, like
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个带有sfn-前缀的长ID的运行对象，例如
- en: '[PRE56]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: A key benefit of production namespaces is that your workflows can safely use
    the Client API, and relative references like .latest_run in particular, knowing
    the production deployments stay isolated from any prototyping that any users perform
    locally in their own namespace.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 生产命名空间的一个关键优势是，您的流程可以安全地使用客户端API，特别是像.latest_run这样的相对引用，知道生产部署始终与任何用户在其个人命名空间中本地执行的任何原型设计保持隔离。
- en: Authorizing deployments
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 授权部署
- en: 'Production namespaces include an important safeguard mechanism. Imagine a new
    employee, Charles, is getting used to Metaflow, and he explores various commands.
    As we discussed previously, local prototyping is always safe, because results
    are tied to Charles’s personal namespace. Charles might also test production deployments
    and execute the following:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 生产命名空间包括一个重要的安全机制。想象一下，一位新员工查尔斯正在熟悉Metaflow，并探索各种命令。正如我们之前讨论的，本地原型设计总是安全的，因为结果绑定到查尔斯的个人命名空间。查尔斯也可能测试生产部署并执行以下操作：
- en: '[PRE57]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Charles’s local version of namespaceflow.py might not be production ready, so
    by doing this, he might end up accidentally breaking the production deployment.
    We want to encourage experimentation, so we should make sure that new employees
    (or anyone else) don’t have to be afraid of accidentally breaking anything.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 查尔斯的本地namespaceflow.py版本可能不是生产就绪的，因此通过这种方式，他可能会意外地破坏生产部署。我们希望鼓励实验，因此我们应该确保新员工（或其他人）不必担心意外破坏任何东西。
- en: 'To prevent accidents from happening, Metaflow prevents Charles from running
    step-functions create by default. Charles needs to know a unique *production token*
    that defines the production namespace to be able to run the command. In this case,
    if Charles really needs to deploy the flow to production, he would reach out to
    people who have deployed the flow previously, obtain the token, and execute the
    following:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止发生事故，Metaflow阻止查尔斯运行默认创建的步骤函数。查尔斯需要知道一个唯一的*生产令牌*，才能运行该命令。在这种情况下，如果查尔斯真的需要将流程部署到生产环境，他需要联系之前部署过该流程的人，获取令牌，并执行以下操作：
- en: '[PRE58]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The --authorize flag is only needed for the first deployment. After this, Charles
    can keep deploying the flow like anyone else. Note that --authorize is not a security
    feature. Charles can discover the token by himself, too, as we will see soon.
    It is only meant to act as an explicit confirmation for the action, which makes
    it a bit harder to cause damage inadvertently.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: --authorize标志仅适用于第一次部署。在此之后，查尔斯可以像其他人一样继续部署流程。请注意，--authorize不是一个安全功能。查尔斯也可以自己发现令牌，正如我们很快就会看到的。它只是作为一个对动作的明确确认，这使得无意中造成损害的可能性稍微降低。
- en: 6.3.3 Parallel deployments with @project
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 使用@project的并行部署
- en: When you run step-functions create, a flow is deployed to the production scheduler.
    The deployment is named automatically after the name of the FlowSpec class. In
    other words, by default there’s exactly one production version attached to a flow
    name. You or your colleagues (after authorization) can update the deployment by
    running step-functions create again, but the newer version will overwrite the
    previous one.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行 step-functions create 时，一个流程被部署到生产调度器。部署的名称自动根据 FlowSpec 类的名称命名。换句话说，默认情况下，每个流程名称都附有一个精确的生产版本。你可以（在授权后）通过再次运行
    step-functions create 来更新部署，但新版本将覆盖旧版本。
- en: As we discussed in the beginning of this section, larger projects may need multiple
    parallel but isolated production deployments, for example, to facilitate A/B testing
    of new experimental versions of the flow. Furthermore, a complex application may
    consist of multiple flows (like ClassifierTrainFlow and ClassifierPredictFlow
    in chapter 3), which should exist in the same namespace, so they can share artifacts
    among themselves safely. By default, when you deploy two flows with distinct names,
    a unique namespace will be generated for each one of them.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节开头讨论的那样，较大的项目可能需要多个并行但隔离的生产部署，例如，为了便于测试流程的新实验版本。此外，一个复杂的应用程序可能由多个流程组成（例如，第
    3 章中的 ClassifierTrainFlow 和 ClassifierPredictFlow），它们应该存在于同一个命名空间中，这样它们可以在彼此之间安全地共享工件。默认情况下，当你部署具有不同名称的两个流程时，将为每个流程生成一个唯一的命名空间。
- en: To address these needs, we can use a flow-level decorator called @project. The
    @project decorator doesn’t do anything by itself, but it enables a flow, or multiple
    flows, to be deployed in production in a special way. The @project decorator is
    an optional feature that can help organize larger projects. You can start without
    it, have a single production version, and add it later as needs grow. Let’s use
    a simple example shown in listing 6.8 to demonstrate the concept.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，我们可以使用一个名为 @project 的流程级装饰器。@project 装饰器本身并不做任何事情，但它允许一个流程或多个流程以特殊方式在生产环境中部署。@project
    装饰器是一个可选功能，可以帮助组织较大的项目。你可以先不使用它，只保留一个生产版本，然后在需要增长时再添加它。让我们使用列表 6.8 中所示的简单示例来演示这个概念。
- en: Listing 6.8 A flow with the @project decorator
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.8 带有 @project 装饰器的流程
- en: '[PRE59]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: ❶ Annotates the flow with a project decorator with a unique name
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用具有唯一名称的项目装饰器注释流程
- en: Save the code to firstflow.py. We annotated the flow with @project, which needs
    to be given a unique name. All flows with the same project name will use a single
    shared namespace.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 firstflow.py。我们使用 @project 对流程进行了注释，这需要一个唯一的名称。具有相同项目名称的所有流程将使用单个共享命名空间。
- en: 'Let’s see what happens when you deploy it to Step Functions, as shown here:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当你将其部署到步骤函数时会发生什么，如下所示：
- en: '[PRE60]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Thanks to the @project decorator, the flow isn’t deployed with the name FirstFlow
    as usual. Instead, it is named demo_project.user.ville.FirstFlow. The @project
    is used to create parallel, uniquely named deployments. By default, the deployment
    is prefixed by the project name (demo_project) and the user who deployed the flow
    (user.ville). If another team member ran step-functions create with this flow,
    they would get a personal, unique deployment. This allows anyone to test their
    prototypes easily in a production environment without interfering with the main
    production version.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了 @project 装饰器，流程不是以通常的 FirstFlow 名称部署。相反，它被命名为 demo_project.user.ville.FirstFlow。@project
    装饰器用于创建并行、具有唯一名称的部署。默认情况下，部署以项目名称（demo_project）和部署流程的用户（user.ville）为前缀。如果另一位团队成员使用此流程运行
    step-functions create，他们将获得一个个人、唯一的部署。这允许任何人在生产环境中轻松测试他们的原型，而不会干扰主要的生产版本。
- en: 'Sometimes experiments are not clearly tied to a single user. Maybe multiple
    data scientists collaborate on a joint experiment. In this case, it is natural
    to deploy the flow as a *branch*. Try this:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 有时实验并没有明确地与单个用户相关联。也许多个数据科学家合作进行联合实验。在这种情况下，将流程作为 *分支* 部署是自然的。尝试以下操作：
- en: '[PRE61]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'It will produce a deployment named demo_project.test.testbranch.FirstFlow—note
    that no username is present in the name. You can create any number of independent
    branches. Note that triggering respects --branch too. Try the following:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 它将生成一个名为 demo_project.test.testbranch.FirstFlow 的部署——请注意，名称中不包含用户名。你可以创建任意数量的独立分支。请注意，触发也尊重
    --branch。尝试以下操作：
- en: '[PRE62]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: It will trigger an execution of demo_project.test.testbranch.FirstFlow.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 它将触发 demo_project.test.testbranch.FirstFlow 的执行。
- en: By convention, if your project has a single blessed production version, you
    can deploy it with
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，如果你的项目只有一个受推崇的生产版本，你可以使用以下方式部署：
- en: '[PRE63]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: which will produce a deployment named demo_project.prod.FirstFlow. The --production
    option deploys a branched deployment like any other—there’s no special semantics
    in --production. However, it can help to clearly distinguish the main production
    version from other experimental branches.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个名为 demo_project.prod.FirstFlow 的部署。--production 选项像部署任何其他分支一样部署分支部署——在
    --production 中没有特殊的语义。然而，它可以帮助清楚地区分主生产版本和其他实验分支。
- en: Besides allowing multiple parallel, isolated production deployments, the @project
    decorator is useful because it creates a single, unified namespace across multiple
    flows. To test the idea, let’s create another flow for the same @project, shown
    in the following code listing.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 除了允许多个并行、隔离的生产部署外，@project 装饰器很有用，因为它在多个流程中创建了一个单一、统一的命名空间。为了测试这个想法，让我们为同一个
    @project 创建另一个流程，如下面的代码列表所示。
- en: Listing 6.9 Another flow in the same @project
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.9 同一个 @project 中的另一个流程
- en: '[PRE64]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: ❶ Accesses an artifact in the same namespace
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 访问同一命名空间中的工件
- en: 'Save the code in secondflow.py. You can test the flows locally by running the
    following:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 secondflow.py。你可以通过运行以下命令在本地测试流程：
- en: '[PRE65]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Locally, the latest_run in SecondFlow refers to the latest run of FirstFlow
    in your personal namespace, in my case, user:ville. Let’s deploy SecondFlow to
    our test branch as shown here:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 本地，SecondFlow 中的 latest_run 指的是你个人命名空间中 FirstFlow 的最新运行，在我的情况下，是用户：ville。让我们按照以下方式将
    SecondFlow 部署到我们的测试分支：
- en: '[PRE66]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: This will deploy a flow named demo_project.test.testbranch.SecondFlow. Remarkably,
    both FirstFlow and SecondFlow share the same namespace, which in my case is named
    mfprj-pbnipyjz2ydyqlmi-0-zphk. The project namespaces are generated based on a
    hash of the branch name, the project name, and a unique token, so they will look
    a bit cryptic.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 这将部署一个名为 demo_project.test.testbranch.SecondFlow 的流程。值得注意的是，FirstFlow 和 SecondFlow
    使用相同的命名空间，在我的情况下，它被命名为 mfprj-pbnipyjz2ydyqlmi-0-zphk。项目命名空间是基于分支名称、项目名称和唯一令牌的哈希值生成的，因此它们看起来有点神秘。
- en: 'Now, trigger an execution on Step Functions as follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 Step Functions 上触发以下执行：
- en: '[PRE67]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'After a while, you can examine the results in a notebook or a Python interpreter
    like this:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，你可以像这样在笔记本或 Python 解释器中检查结果：
- en: '[PRE68]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We use the global namespace, so we don’t need to know the exact namespace used
    by our testbranch. However, this practice is a bit dangerous if other people are
    running SecondFlows simultaneously. Note that the flow name, SecondFlow, is still
    the same: the @project prefix is used only to name the flow for the production
    scheduler.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用全局命名空间，因此我们不需要知道我们的 testbranch 所使用的确切命名空间。然而，如果其他人同时运行 SecondFlows，这种做法有点危险。请注意，流程名称
    SecondFlow 仍然是相同的：@project 前缀仅用于为生产调度器命名流程。
- en: To see the power of @project, you can now make edits in FirstFlow, for instance,
    change the model string to something else. You can test changes locally as before,
    which won’t impact the production deployment. After you are happy with the changes,
    you can deploy the improved FirstFlow as well as SecondFlow to a new branch, say,
    newbranch. The setup is visualized in figure 6.18.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到 @project 的力量，你现在可以在 FirstFlow 中进行编辑，例如，将模型字符串更改为其他内容。你可以像以前一样在本地测试更改，这不会影响生产部署。在你对更改满意后，你可以将改进后的
    FirstFlow 以及 SecondFlow 部署到新的分支，比如 newbranch。设置如图 6.18 所示。
- en: '![CH06_F18_Tuulos](../../OEBPS/Images/CH06_F18_Tuulos.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F18_Tuulos](../../OEBPS/Images/CH06_F18_Tuulos.png)'
- en: Figure 6.18 Three @project branches
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 三种 @project 分支
- en: 'When you execute them, only the pair of workflows deployed to newbranch is
    affected by the changes. The old version at testbranch is not affected. As depicted
    in figure 6.18, in this scenario we have three independent namespaces: the default
    user namespace used during prototyping and the two branches in production.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行它们时，只有部署到 newbranch 的新工作流会受到更改的影响。testbranch 上的旧版本不受影响。如图 6.18 所示，在这种情况下，我们有三个独立的命名空间：用于原型设计的默认用户命名空间和两个生产分支。
- en: Let’s summarize what we learned in this section in the context of an actual
    data science project. The project is developed continuously by multiple data scientists.
    They can generate hundreds of ideas on how to improve the project. However, we
    don’t know which ideas work well without testing them in a realistic production
    environment. We visualized the process as an experiment funnel, depicted again
    in figure 6.19.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下在本节中学到的内容，结合一个实际的数据科学项目。该项目由多个数据科学家持续开发。他们可以提出数百种改进项目的想法。然而，如果没有在现实的生产环境中测试，我们不知道哪些想法是有效的。我们将这个过程可视化为一个实验漏斗，如图
    6.19 所示。
- en: '![CH06_F19_Tuulos](../../OEBPS/Images/CH06_F19_Tuulos.png)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![CH06_F19_Tuulos](../../OEBPS/Images/CH06_F19_Tuulos.png)'
- en: Figure 6.19 Facilitating the experimentation funnel using @project
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 使用 @project 促进实验漏斗
- en: Thanks to user namespaces, data scientists are able to prototype new versions
    and run them locally without fear of interfering with each other. Namespaces are
    enabled automatically to all Metaflow runs. Once they have identified the most
    promising ideas, they can deploy them to production as a custom --branch using
    @project. These custom branches can then be used to feed predictions to A/B experiments,
    for example. Finally, when an experiment has proven its value, it can be promoted
    to become the new main production version, deployed with --production.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了用户命名空间，数据科学家能够原型化新版本并在本地运行，无需担心它们会相互干扰。命名空间会自动启用所有 Metaflow 运行。一旦他们确定了最有希望的想法，他们可以使用
    @project 将其部署到生产环境作为一个自定义 --branch。然后，这些自定义分支可以用来向 A/B 实验提供预测。最后，当一个实验证明了其价值，它可以被提升为新的主要生产版本，使用
    --production 部署。
- en: Summary
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Using a centralized metadata server helps to track all executions and artifacts
    across all projects, users, and production deployments.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集中式元数据服务器有助于跟踪所有项目、用户和生产部署中的所有执行和工件。
- en: Leverage a highly available, scalable production scheduler like AWS Step Functions
    to execute workflows on a schedule without human supervision.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用高度可用、可扩展的生产调度器，如 AWS Step Functions，在无需人工监督的情况下按计划执行工作流。
- en: Use the @schedule decorator to make workflows run automatically on a predefined
    schedule.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 @schedule 装饰器使工作流能够自动在预定义的计划上运行。
- en: Metaflow’s code packages encapsulate the user-defined code and supporting modules
    for cloud-based execution.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaflow 的代码包封装了用户定义的代码和云执行所需的支持模块。
- en: Use containers and the @conda decorator to manage third-party dependencies in
    production deployments.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用容器和 @conda 装饰器来管理生产部署中的第三方依赖。
- en: User namespaces help isolate prototypes that users run on their local workstations,
    making sure that prototypes don’t interfere with each other.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户命名空间有助于隔离用户在其本地工作站上运行的原型，确保原型之间不会相互干扰。
- en: Production deployments get a namespace of their own, isolated from prototypes.
    New users must obtain a production token to deploy new versions to production,
    which prevents accidental overwrites.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产部署拥有自己的命名空间，与原型隔离。新用户必须获取生产令牌才能将新版本部署到生产环境中，这可以防止意外覆盖。
- en: The @project decorator allows multiple parallel, isolated workflows to be deployed
    to production concurrently.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '@project 装饰器允许多个并行、隔离的工作流同时部署到生产环境中。'
- en: Use @project to create a unified namespace across multiple workflows.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 @project 在多个工作流之间创建统一的命名空间。
