- en: '6 Multidimensional data frames: Using PySpark with JSON data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 多维数据帧：使用 PySpark 处理 JSON 数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Drawing parallels between JSON documents and Python data structures
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 JSON 文档和 Python 数据结构之间建立联系
- en: Ingesting JSON data within a data frame
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据帧中摄取 JSON 数据
- en: Representing hierarchical data in a data frame through complex column types
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过复杂列类型在数据帧中表示层次化数据
- en: Reducing duplication and reliance on auxiliary tables with a document/hierarchical
    data model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文档/层次化数据模型减少重复和对外部表的依赖
- en: Creating and unpacking data from complex data types
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从复杂数据类型创建和提取数据
- en: 'Thus far, we have used PySpark’s data frame to work with textual (chapters
    2 and 3) and tabular (chapters 4 and 5) data. Both data formats were pretty different,
    but they fit seamlessly into the data frame structure. I believe we’re ready to
    push the abstraction a little further by representing *hierarchical information*
    within a data frame. Imagine it for a moment: columns within columns, the ultimate
    flexibility.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用 PySpark 的数据帧来处理文本数据（第 2 章和第 3 章）和表格数据（第 4 章和第 5 章）。这两种数据格式相当不同，但它们无缝地融入了数据帧结构。我相信我们准备好将抽象进一步推进，通过在数据帧中表示层次化信息。想象一下：列中的列，这是终极灵活性。
- en: This chapter is about ingesting and working with hierarchical JSON data using
    the PySpark data frame. JSON (JavaScript Object Notation) data rapidly took over
    as the dominant data format for exchanging information between a client (such
    as your browser) and a server. In the context of big data, JSON allows you to
    store more rich data types than plain scalar values compared to tabular serialization
    formats such as CSV. I first introduce the JSON format and how we can draw parallels
    to Python data structures. I go over the three container structures available
    for the data frame, the array, the map, and the struct, and how they are used
    to represent more rich data layouts. I cover how we can use them to represent
    multidimensional data and how the struct can represent hierarchical information.
    Finally, I wrap that information into a schema, a very useful construct for documenting
    what’s in your data frame.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了如何使用 PySpark 数据帧来摄取和处理层次化 JSON 数据。JSON（JavaScript 对象表示法）数据迅速成为客户端（如您的浏览器）和服务器之间交换信息的占主导地位的数据格式。在大数据背景下，与表格序列化格式（如
    CSV）相比，JSON 允许您存储比纯标量值更丰富的数据类型。我首先介绍了 JSON 格式以及我们如何将其与 Python 数据结构进行比较。我介绍了数据帧中可用的三种容器结构：数组、映射和结构，以及它们如何用于表示更丰富的数据布局。我涵盖了如何使用它们来表示多维数据以及结构如何表示层次化信息。最后，我将这些信息封装到一个模式中，这是一个非常有用的结构，用于记录数据帧中的内容。
- en: '6.1 Reading JSON data: Getting ready for the schemapocalypse'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 读取 JSON 数据：为模式灾难做准备
- en: Every data-processing job in PySpark starts with data ingestion; JSON data is
    no exception. This section explains what JSON is, how to use the specialized JSON
    reader with PySpark, and how a JSON file is represented within a data frame. After
    this, you’ll be able to reason about your JSON data and map it to PySpark data
    types.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中，每个数据处理作业都以数据摄取开始；JSON 数据也不例外。本节解释了 JSON 是什么，如何使用 PySpark 的专用 JSON
    读取器，以及 JSON 文件如何在数据帧中表示。在此之后，您将能够对您的 JSON 数据进行推理并将其映射到 PySpark 数据类型。
- en: 'For this chapter, we use a JSON dump of information about the TV show *Silicon
    Valley* from TV Maze. I uploaded the data in the book’s repository (under `./data/
    shows`), but you can download it directly from the TV Maze API (available online:
    [http://mng.bz/g4oR](http://mng.bz/g4oR)). A simplified version of the JSON document
    is illustrated in the next listing; the main parts are numerated, and I go over
    each of them.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们使用 TV Maze 中关于电视剧《硅谷》的信息的 JSON 导出。我在本书的存储库中上传了数据（在 `./data/shows` 下），但您也可以直接从
    TV Maze API（在线可用：[http://mng.bz/g4oR](http://mng.bz/g4oR)）下载它。下一个列表展示了简化版的 JSON
    文档；主要部分已编号，我将逐一介绍它们。
- en: Listing 6.1 A simplified sample of the JSON object
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 JSON 对象的简化示例
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ At the top level, a JSON object looks like a Python dictionary. Both use the
    brackets to delimit object boundaries.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在顶层，JSON 对象看起来像 Python 字典。两者都使用括号来界定对象边界。
- en: ❷ JSON data is encoded into key-value pairs, just like in a dictionary. JSON
    keys must be strings.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ JSON 数据被编码成键值对，就像在字典中一样。JSON 键必须是字符串。
- en: ❸ JSON arrays can contain multiple values (here, we have a single string).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ JSON 数组可以包含多个值（这里我们有一个单个字符串）。
- en: ❹ Objects can be values too; you can nest objects within one another this way.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对象也可以是值；您可以通过这种方式在对象内部嵌套对象。
- en: ❺ Our episodes are each objects contained within an array.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们的内容是包含在数组中的对象。
- en: My first thought when I saw the JSON data format was that it looked a lot like
    a Python dictionary. I still think it’s a valid way to map a JSON document mentally,
    and the next section will explain how we can use our Python knowledge to quickly
    internalize JSON.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我看到 JSON 数据格式时，我的第一个想法是它看起来很像 Python 字典。我仍然认为这是一种在心理上映射 JSON 文档的有效方式，下一节将解释我们如何利用我们的
    Python 知识快速内化 JSON。
- en: '6.1.1 Starting small: JSON data as a limited Python dictionary'
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 从小做起：将 JSON 数据视为有限的 Python 字典
- en: In this section, we cover a brief introduction to the JSON format and how we
    can build a mental model of the data with Python data structures. Following this,
    we validate our intuition by parsing a small JSON message in Python. Just like
    with CSV, translating your original data into PySpark structures helps tremendously
    in knowing how to map your data transformations; you instinctively know where
    a field maps to and can get to coding faster.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍 JSON 格式以及我们如何使用 Python 数据结构构建数据的心智模型。在此之后，我们通过在 Python 中解析一个小 JSON
    消息来验证我们的直觉。就像 CSV 一样，将您的原始数据转换为 PySpark 结构有助于了解如何映射您的数据转换；您本能地知道字段映射的位置，并且可以更快地开始编码。
- en: JSON data is a long-standing data interchange format that became massively popular
    for its readability and its relatively small size. JSON stands for *JavaScript
    Object Notation*, a fitting name considering that each JSON file can be thought
    of as a JavaScript object. The official JSON website ([https://json.org](https://json.org))
    contains a more formal introduction to the JSON data format. Since we focus on
    the Python programming language, I will frame my exploration of the JSON spec
    through the lens of the Python family of data structures.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 数据是一种长期存在的数据交换格式，因其可读性和相对较小的尺寸而变得非常流行。JSON 代表 *JavaScript Object Notation*，考虑到每个
    JSON 文件都可以被视为一个 JavaScript 对象，这个名字非常合适。官方 JSON 网站 ([https://json.org](https://json.org))
    包含对 JSON 数据格式的更正式介绍。由于我们专注于 Python 编程语言，我将通过 Python 数据结构系列来探讨 JSON 规范。
- en: Looking at listing 6.1 and figure 6.1, we notice that our document starts with
    an opening curly bracket, `{`. Every valid JSON document is an *object*;[¹](#pgfId-1013503)
    JavaScript uses the bracket as an object delimiter. In Python, the direct equivalent
    of an object, as far as JSON goes, is the dictionary. Just like a dictionary,
    a JSON object has keys and values. The top-level object in a JSON document is
    called the *root* object or element.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 观察列表 6.1 和图 6.1，我们注意到我们的文档以一个开括号 `{` 开始。每个有效的 JSON 文档都是一个 *对象*；[¹](#pgfId-1013503)
    JavaScript 使用括号作为对象分隔符。在 Python 中，就 JSON 而言，对象的直接等价物是字典。就像字典一样，JSON 对象有键和值。JSON
    文档中的顶层对象被称为 *根对象* 或元素。
- en: '![](../Images/06-01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-01.png)'
- en: 'Figure 6.1 A simple JSON object, illustrating its main components: the root
    object, the keys, and the values. Objects use bracket delimiters and arrays/lists
    use square bracket delimiters. JSON uses quotes for string values but not for
    numerical values.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 一个简单的 JSON 对象，说明了其主要组件：根对象、键和值。对象使用括号分隔符，数组/列表使用方括号分隔符。JSON 使用引号表示字符串值，但不表示数值。
- en: A JSON object—or a Python dictionary—both have keys and values. According to
    the JSON specification, the keys of a JSON object must be a string. Python dictionaries
    don’t have that limitation, but we can adapt without any problems.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 对象——或 Python 字典——都有键和值。根据 JSON 规范，JSON 对象的键必须是一个字符串。Python 字典没有这个限制，但我们没有问题地进行了适配。
- en: 'Finally, the values of a JSON object can represent a few data types:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，JSON 对象的值可以表示几种数据类型：
- en: Strings (which use the double-quote character `"` as a quoting character).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串（使用双引号字符 `"` 作为引号字符）。
- en: Numbers (JavaScript does not differentiate between integers and floating-point
    numbers).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字（JavaScript 不区分整数和浮点数）。
- en: Booleans (`true` or `false`, which are not capitalized like in Python).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布尔值 (`true` 或 `false`，与 Python 中的大小写不同)。
- en: '`null`, which is akin to the Python `None`.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`null`，类似于 Python 的 `None`。'
- en: Arrays, which are delimited by the square bracket `[` . They are akin to the
    Python list.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数组，由方括号 `[` 分隔。它们类似于 Python 列表。
- en: Objects, which are delimited by the curly bracket `{`.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象，由花括号 `{` 分隔。
- en: 'If you make the switch between the JSON and Python terms (arrays to lists and
    objects to dictionaries), working with JSON will be a breeze in Python. To finish
    our analogy, I read in the next listing: my simple JSON object using the `json`
    module, available in the Python standard library.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你切换 JSON 和 Python 术语（数组到列表，对象到字典），在 Python 中处理 JSON 将变得轻而易举。为了完成我们的类比，我在下一个列表中读取：使用
    Python 标准库中可用的 `json` 模块读取简单的 JSON 对象。
- en: Listing 6.2 Reading a simple JSON document as a Python dictionary
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 将简单的 JSON 文档读取为 Python 字典
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ I import the json module, available in the Python standard library.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我导入了 Python 标准库中可用的 json 模块。
- en: ❷ Our loaded document looks like a Python dictionary with string keys. Python
    recognized that 143 was an integer and parsed the number as such.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们加载的文档看起来像 Python 字典，具有字符串键。Python 识别出 143 是一个整数，并将其解析为这样的数字。
- en: ❸ Our loaded document is of type dict.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们加载的文档类型为 dict。
- en: In this section, I introduced how the JSON object can be thought of as a limited
    Python dictionary. Keys are always strings, and values can take numerical, Boolean,
    string, or `null` values. You can also have arrays of elements or objects as values,
    which enables nesting and hierarchical organization of the data. Now that we understand
    how it works in Python, the next sections show how to read JSON data using PySpark
    and introduce the most complex data frame schema we’ve encountered so far. Before
    you know it, you’ll conquer the schemapocalypse!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我介绍了如何将 JSON 对象视为一个有限的 Python 字典。键始终是字符串，值可以是数值、布尔值、字符串或 `null` 值。你还可以有元素数组或对象作为值，这使数据可以嵌套和分层组织。现在我们了解了它在
    Python 中的工作方式，接下来的几节将展示如何使用 PySpark 读取 JSON 数据，并介绍我们迄今为止遇到的最复杂的数据框模式。很快，你将征服模式末日！
- en: '6.1.2 Going bigger: Reading JSON data in PySpark'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 扩展范围：在 PySpark 中读取 JSON 数据
- en: This section introduces reading JSON data using the specialized JSON `SparkReader`
    object. We discuss the most common and useful parameters of the reader. With this
    information handy, you will be equipped to read JSON files into a data frame.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了使用专门的 JSON `SparkReader` 对象读取 JSON 数据。我们讨论了读者最常见和最有用的参数。有了这些信息，你将能够将 JSON
    文件读取到数据框中。
- en: For this section, we will take the data introduced at the beginning of the chapter.
    We read the JSON document in one fell swoop, using the specialized `SparkReader`
    object. The result is available in the following listing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用本章开头介绍的数据。我们一次性读取 JSON 文档，使用专门的 `SparkReader` 对象。结果如下所示。
- en: Listing 6.3 Ingesting a JSON document using the JSON specialized `SparkReader`
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 使用 JSON 专门的 `SparkReader` 导入 JSON 文档
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The specialized SparkReader object is accessible by calling the json method
    on spark.read, just like with CSV or text data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过在 spark.read 上调用 json 方法，可以访问专门的 SparkReader 对象，就像 CSV 或文本数据一样。
- en: ❷ The document I ingested contains only a single record.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我导入的文档只包含一条记录。
- en: Two elements pop to mind when reviewing the code. First, we do not use any optional
    parameters. Unlike CSV data, JSON data doesn’t need to worry about record delimiters
    or inferring data types (JSON forces the usage of string delimiters, so the value
    `03843` is a number, where `"03843"` is a string), which reduces the need to doctor
    the reading process by a fair amount. Many options are available for relaxing
    the JSON specification (e.g., allowing single quotes for strings, comments, or
    unquoted keys). If your JSON document is “up-to-spec” and you have no special
    need for some values not covered within the data types that JSON provided, the
    stock reader will work fine. When the data is less than pristine, the options
    to bend the reader to your will are there, ready to assist. I will introduce method
    options as we need them, but if you can’t wait any longer, you can read the docstring
    for the `json` method of the `DataFrameReader` object.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾代码时，有两个元素浮现在脑海中。首先，我们没有使用任何可选参数。与 CSV 数据不同，JSON 数据不需要担心记录分隔符或推断数据类型（JSON
    强制使用字符串分隔符，因此值 `03843` 是一个数字，而 `"03843"` 是一个字符串），这大大减少了需要调整读取过程的需求。有许多选项可以放松 JSON
    规范（例如，允许字符串使用单引号，注释或不带引号的键）。如果你的 JSON 文档符合规范，并且你没有特殊需求来处理 JSON 提供的数据类型之外的一些值，那么默认的读取器将工作得很好。当数据不够完美时，有选项可以调整读取器以满足你的需求，随时准备提供帮助。我将在需要时介绍方法选项，但如果你迫不及待，可以阅读
    `DataFrameReader` 对象的 `json` 方法的文档字符串。
- en: 'The second odd thing about our data ingestion is that we only have a single
    record. If we take a moment to reflect on this, it makes sense: TVMaze provides
    the result of our query in a single document. In the PySpark world, reading JSON
    follows this rule: *one JSON document, one line, one record*. This means that
    if you want to have multiple JSON records in the same document, you need to have
    one document per line and no new line within your document. The JSON Lines document
    format ([http://jsonlines.org/](http://jsonlines.org/)) has a more formal definition
    if you are interested. By opening the JSON document we read in listing 6.3 (a
    regular text editor will do), you see that we only have a single line in the file.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据摄入的第二个奇怪之处在于我们只有一个记录。如果我们花点时间反思这一点，这是有道理的：TVMaze在我们的查询结果中提供了一个文档。在PySpark世界中，读取JSON遵循以下规则：*一个JSON文档，一行，一个记录*。这意味着如果你想在同一个文档中有多条JSON记录，你需要每行一个文档，文档内没有换行符。如果你对JSON
    Lines文档格式（[http://jsonlines.org/](http://jsonlines.org/)）感兴趣，它有一个更正式的定义。通过打开我们在列表6.3中读取的JSON文档（一个常规文本编辑器就可以做到），你会发现文件中只有一行。
- en: 'If you want to ingest multiple documents across multiple files, you need to
    set the `multiLine` (careful about the capital L!) parameter to true. This will
    change the JSON reading rule to the following: *one JSON document, one file, one
    record*. With this, you can use the glob pattern (using a `*` to refer to multiple
    files), as seen in chapter 3, or pass a directory containing only JSON files with
    the same schema as an argument to the reader. I made two more shows available
    in the `data/shows` directory (*Breaking Bad* and *The Golden Girls*, to cover
    a wide gamut). In the next listing, I read the three JSON documents in one fell
    swoop and show that I indeed have three records.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要跨多个文件摄入多个文档，你需要将`multiLine`（注意大写的L！）参数设置为true。这将改变JSON读取规则为以下形式：*一个JSON文档，一个文件，一个记录*。有了这个，你可以使用glob模式（使用`*`来指代多个文件），如第3章所示，或者将包含与读取器相同模式的JSON文件的目录作为参数传递。我在`data/shows`目录中提供了两个额外的示例（*绝命毒师*和*黄金女孩*，以覆盖广泛的范围）。在下一个列表中，我一次性读取了三个JSON文档，并展示了确实有三个记录。
- en: Listing 6.4 Reading multiple JSON documents using the `multiLine` option
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 使用`multiLine`选项读取多个JSON文档
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This section covered how to import a simple JSON document in PySpark and how
    we can tweak the specialized JSON reader to accommodate common use cases. In the
    next section, we focus on how complex data types help us navigate hierarchical
    data within a data frame.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何在PySpark中导入简单的JSON文档以及我们如何调整专门的JSON读取器以适应常见用例。在下一节中，我们将重点关注复杂数据类型如何帮助我们导航数据框架中的层次数据。
- en: 6.2 Breaking the second dimension with complex data types
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 使用复杂数据类型打破第二维度
- en: 'This section takes the JSON data model and applies it in the context of the
    PySpark data frame. I go a little deeper into PySpark’s complex data types: the
    array and the map. I take PySpark’s columnar model and translate it into hierarchical
    data models. At the end of this section, you’ll know how to represent, access,
    and process container types in a PySpark data frame. This will prove useful in
    processing hierarchical or object oriented data, like the `shows` data we are
    working with.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将JSON数据模型应用于PySpark数据框架的上下文中。我深入探讨了PySpark的复杂数据类型：数组和映射。我将PySpark的列式模型转换为层次数据模型。本节结束时，你将了解如何在PySpark数据框架中表示、访问和处理容器类型。这将在处理层次或面向对象的数据时非常有用，例如我们正在处理的`shows`数据。
- en: PySpark’s ability to use complex types inside the data frame is what allows
    its remarkable flexibility. While you still have the tabular abstraction to work
    with, your cells are supercharged since they can contain more than a single value.
    It’s just like going from 2D to 3D, and even beyond!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark在数据框架中使用复杂类型的能力使其具有非凡的灵活性。虽然你仍然有表格抽象来工作，但你的单元格被超级充电，因为它们可以包含多个值。这就像从2D到3D，甚至更高级！
- en: 'A *complex* type isn’t complex in the Python sense: where Python uses complex
    data in the sense of images, maps, video files and so on, Spark uses this term
    to refer to data types *that contain other types*. Because of this, I also use
    the term *container* or *compound* type as a synonym for complex types. I find
    them to be less ambiguous; a container-type column contains values of other types.
    In Python, the main complex types are the list, the tuple, and the dictionary.
    In PySpark, we have the array, the map, and the struct. With these, you will be
    able to express an infinite amount of data layout.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 的意义上，*复杂* 类型并不复杂：Python 使用复杂数据来表示图像、地图、视频文件等，而 Spark 使用这个术语来指代 *包含其他类型的数据类型*。因此，我也使用术语
    *容器* 或 *复合* 类型作为复杂类型的同义词。我发现它们更不容易产生歧义；容器类型列包含其他类型的值。在 Python 中，主要的复杂类型是列表、元组和字典。在
    PySpark 中，我们有数组、映射和结构体。有了这些，你将能够表达无限多的数据布局。
- en: 'No type left behind: If you want to dig deeper into scalar data types'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 不遗漏任何类型：如果你想深入了解标量数据类型
- en: In chapter 1 to 3, we mostly dealt with *scalar* data, which contains a single
    value. Those types map seamlessly to Python types; for instance, a `string` type
    PySpark column maps to a Python string. Because Spark borrows the Java/Scala type
    convention, there are some peculiarities that I introduce as we encounter them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 章到第 3 章中，我们主要处理 *标量* 数据，它包含单个值。这些类型无缝映射到 Python 类型；例如，PySpark 的 `string`
    类型列映射到 Python 字符串。因为 Spark 借用了 Java/Scala 类型约定，所以有一些特殊性，我会在遇到时介绍。
- en: 'I think I’ve held the punch for long enough: behold, the next listing reveals
    our data frame’s schema!'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我已经说得够多了：看吧，下一列表揭示了我们的数据框模式！
- en: Listing 6.5 Nested structures with a deeper level of indentation
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 嵌套结构，具有更深层次的缩进
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Like a JSON document, the top-level element of our data frame schema is called
    the root.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 就像 JSON 文档一样，我们数据框模式的最顶层元素被称为根。
- en: ❷ A complex column introduces a new level of nesting in the data frame schema.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 复杂列在数据框模式中引入了新的嵌套层级。
- en: 'I had to truncate the schema so that we can focus on the important point here:
    the *hierarchy* within the schema. PySpark took every top-level key—the keys from
    the root object—and parsed them as columns (see the next listing for the top-level
    columns). When a column had a scalar value, the type was inferred according to
    the JSON specification we saw in section 6.1.1.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我不得不截断模式，以便我们可以关注这里的重要点：模式中的 *层次结构*。PySpark 解析了每个顶层键——从根对象中来的键——并将它们解析为列（参见下一列表中的顶层列）。当一个列具有标量值时，类型是根据我们在
    6.1.1 节中看到的 JSON 规范推断的。
- en: Listing 6.6 Printing the columns of the `shows` data frame
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.6 打印 `shows` 数据框的列
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this section, I briefly introduced the schema of our ingested JSON document.
    The next sections will cover two complex column types that Spark provides, starting
    with the array, and then the map.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我简要介绍了我们摄取的 JSON 文档的模式。下一节将介绍 Spark 提供的两个复杂列类型，首先是数组，然后是映射。
- en: '6.2.1 When you have more than one value: The array'
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 当你有多个值时：数组
- en: 'In this section, I introduce the simplest container type in PySpark: the array.
    I explain where the array is most commonly used as well as the main methods to
    create, operate, and extract data from an array column.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我介绍了 PySpark 中最简单的容器类型：数组。我解释了数组最常用于何处，以及创建、操作和从数组列中提取数据的主要方法。
- en: 'In section 6.1.1, I loosely equated a JSON array to a Python list. In the PySpark
    world, the same follows, with an important distinction: PySpark arrays are containers
    for values *of the same type*. This precision has an important impact on how PySpark
    ingests both JSON documents and, more generally, nested structures, so I’ll explain
    this in more detail.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 6.1.1 节中，我将 JSON 数组松散地等同于 Python 列表。在 PySpark 世界中，情况也是如此，但有一个重要的区别：PySpark
    数组是 *相同类型值* 的容器。这种精确性对 PySpark 如何处理 JSON 文档以及更一般的嵌套结构有重要影响，所以我将更详细地解释这一点。
- en: 'In listing 6.5, the `genres` array points to an `element` item, which is of
    type `string` (I reproduced the relevant section). Like any other type within
    the data frame, we need to provide a complete type story for any complex type,
    including the array. With this loss of flexibility in what an array can contain,
    we gain a better grasp of the data contained within the column and can avoid hard-to-track
    bugs. We will refer to array columns using the `Array[element]` notation (e.g.,
    `Array[string]` represents a column containing an array of strings):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.5 中，`genres` 数组指向一个 `element` 项目，其类型为 `string`（我复制了相关部分）。像数据框中的任何其他类型一样，我们需要为任何复杂类型提供完整的类型描述，包括数组。由于数组可以包含的内容灵活性降低，我们更好地掌握了列中的数据，并可以避免难以追踪的错误。我们将使用
    `Array[element]` 符号来引用数组列（例如，`Array[string]` 表示包含字符串数组的列）：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Warning PySpark will not raise an error if you try to read an array-type column
    with multiple types. Instead, it will simply default to the lowest common denominator,
    usually the string. This way, you don’t lose any data, but you will get a surprise
    later if your code expects an array of another type.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果你尝试使用具有多个类型的数组类型列进行读取，PySpark 不会引发错误。相反，它将简单地默认为最低公共分母，通常是字符串。这样，你不会丢失任何数据，但如果你期望的代码是另一种类型的数组，你会在以后得到一个惊喜。
- en: To work a little with the array, I select a subset of the `shows` data frame
    so as to not lose focus in this huge schema. In the next listing, I select the
    `name` and `genres` columns and show the record. Unfortunately, *Silicon Valley*
    is a single-genre show, so our array is a little too basic for my taste. Let’s
    make it a little more interesting.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了稍微操作一下数组，我选择 `shows` 数据框的一个子集，这样就不会在这个庞大的模式中失去焦点。在下一个列表中，我选择了 `name` 和 `genres`
    列，并显示了记录。不幸的是，*硅谷* 是一个单类型节目，所以我们的数组对我来说有点过于基础了。让我们让它变得更有趣一些。
- en: Listing 6.7 Selecting the `name` and `genres` columns
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.7 选择 `name` 和 `genres` 列
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Conceptually, our `genres` column can be thought of as containing lists of
    elements within each record. In chapter 2, we had a similar situation with breaking
    our lines into words. Visually, it looks like figure 6.2: our `Comedy` value is
    within a list-type structure, inside the column.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们的 `genres` 列可以被视为包含每个记录中元素的列表。在第 2 章中，我们有过类似的情景，将我们的行拆分成单词。从视觉上看，它看起来像图
    6.2：我们的 `Comedy` 值位于列表类型的结构中，在列内。
- en: '![](../Images/06-02.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-02.png)'
- en: Figure 6.2 A visual depiction of the `array_subset` data frame. The `genres`
    column is of type `Array[string]`, meaning that it contains any number of string
    values in a list-type container.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 `array_subset` 数据框的视觉表示。`genres` 列的类型为 `Array[string]`，这意味着它包含任何数量的字符串值，在一个列表类型的容器中。
- en: To get to the value inside the array, we need to extract them. PySpark provides
    a very pythonic way to work with arrays as if they were lists. In listing 6.8,
    I show the main ways to access the (only) element in my array. Arrays are zero-indexed
    when retrieving elements inside, just like Python lists. Unlike Python lists,
    passing an index that would go beyond the content of the list returns `null`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数组内的值，我们需要提取它们。PySpark 提供了一种非常 Pythonic 的方式来处理数组，就像它们是列表一样。在列表 6.8 中，我展示了访问我数组中（唯一）元素的主要方法。数组在检索元素时是零索引的，就像
    Python 列表一样。与 Python 列表不同，传递一个超出列表内容的索引会返回 `null`。
- en: Listing 6.8 Extracting elements from an array
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.8 从数组中提取元素
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Use the dot notation and the usual square bracket with the index inside.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用点符号和通常的方括号，其中包含索引。
- en: ❷ Instead of the index in square bracket syntax, we can use the getItem() method
    on the Column object.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们可以使用 Column 对象上的 getItem() 方法，而不是方括号语法中的索引。
- en: Warning Although the square bracket approach looks very pythonic, you can’t
    use it as a slicing tool. PySpark will accept only one integer as an index, so
    `array_subset.genres[0:10]` will fail and return an `AnalysisException` with a
    cryptic error message. Echoing chapter 1, PySpark is a veneer on top of Spark
    (Java/Scala). This provides a consistent API across languages at the expense of
    not always feeling integrated in the host language; here, PySpark fails to be
    pythonic by not allowing the slicing of arrays.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：尽管方括号方法看起来非常 Pythonic，但你不能将其用作切片工具。PySpark 只接受一个整数作为索引，所以 `array_subset.genres[0:10]`
    会失败，并返回一个包含神秘错误信息的 `AnalysisException`。与第 1 章呼应，PySpark 是 Spark（Java/Scala）的包装层。这为跨语言提供了一致的
    API，但代价是主机语言中并不总是感觉集成；在这里，PySpark 由于不允许切片数组而未能实现 Pythonic。
- en: 'PySpark’s array functions—available in the `pyspark.sql.functions` module—are
    almost all prefixed with the `array_` keyword (some, like `size()` in listing
    6.9, can be applied to more than one complex type and therefore are not prefixed).
    It is therefore pretty easy to review them in one fell swoop in the API documentation
    (see [http://mng.bz/5Kj1](http://mng.bz/5Kj1)). Next, we use functions to create
    a beefier array and do a little exploration with it. In listing 6.9, I perform
    the following tasks:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的数组函数——位于 `pyspark.sql.functions` 模块中——几乎都以前缀 `array_` 开头（一些，如列表 6.9
    中的 `size()`，可以应用于多种复杂类型，因此没有前缀）。因此，在 API 文档中一次性查看它们非常容易（见 [http://mng.bz/5Kj1](http://mng.bz/5Kj1)）。接下来，我们使用函数创建一个更强大的数组，并对其进行一些探索。在列表
    6.9 中，我执行以下任务：
- en: I create three literal columns (using `lit()` to create scalar columns, then
    `make_array()`) to create an array of possible genres. PySpark won’t accept Python
    lists as an argument to `lit()`, so we have to go the long route by creating individual
    scalar columns before combining them into a single array. Chapter 8 covers UDFs
    that can return array columns.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我创建了三个字面量列（使用 `lit()` 创建标量列，然后 `make_array()`）来创建一个可能的流派数组。PySpark 不接受 Python
    列作为 `lit()` 的参数，因此我们必须通过创建单个标量列然后再将它们组合成一个数组来走一条长路。第 8 章涵盖了可以返回数组列的 UDF。
- en: I then use the function `array_repeat()` to create a column repeating the `Comedy`
    string we extracted in listing 6.8 five times. I finally compute the size of both
    columns, de-dupe both arrays, and intersect them, yielding our original `[Comedy]`
    array from listing 6.7.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我随后使用函数 `array_repeat()` 创建一列，重复我们在列表 6.8 中提取的 `Comedy` 字符串五次。最后，我计算了两列的大小，去重了两个数组，并将它们交集，得到了列表
    6.7 中的原始 `[Comedy]` 数组。
- en: Listing 6.9 Performing multiple operations on an array column
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.9 在数组列上执行多个操作
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Creating an array from three columns using the array() function
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 `array()` 函数从三个列创建一个数组
- en: ❷ Duplicating the values five times within an array using array_repeat()
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 `array_repeat()` 在数组内重复值五次
- en: ❸ Computing the number of elements into both arrays using the size() function
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 `size()` 函数计算两个数组中的元素数量
- en: ❹ Removing duplicates into both arrays with the array_distinct() method. Since
    Some_Genres doesn’t have any duplicates, the values within the array don’t change.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 `array_distinct()` 方法将重复项从两个数组中移除。由于 Some_Genres 没有任何重复项，数组内的值没有变化。
- en: ❺ By intersecting both arrays using array_intersect(), the only value common
    to both arrays is Comedy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过使用 `array_intersect()` 交集两个数组，只有两个数组共有的值是 Comedy。
- en: 'When you want to know the position of a value in an array, you can use `array_
    position()`. This function takes two arguments:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想知道数组中一个值的位时，你可以使用 `array_position()`。这个函数接受两个参数：
- en: An array column to perform the search
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于执行搜索的数组列
- en: A value to search for within the array
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数组中搜索的值
- en: 'It returns the cardinal position of the *value* within the *array column* (first
    value is `1`, second value is `2`, etc.). If the value does not exist, the function
    returns `0`. I illustrate this in listing 6.10\. This inconsistency between zero-based
    indexing (for `getItem()`) and one-based/cardinal indexing (for `array_position()`)
    can be confusing: I remember this difference by calling the position via `getItem()`
    or the square brackets *index* versus *position* for the return value of the `array_position()`
    function, just like in the PySpark API.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回数组列中 *值* 的基数位置（第一个值是 `1`，第二个值是 `2`，等等）。如果值不存在，函数返回 `0`。我在列表 6.10 中说明了这一点。`getItem()`
    的零基索引（对于 `getItem()`）和 `array_position()` 的基于一/基数索引（对于 `array_position()` 返回值）之间的这种不一致可能会令人困惑：我通过调用
    `getItem()` 或 `array_position()` 函数的返回值中的方括号 *index* 与 *position* 来记住这个差异，就像在
    PySpark API 中一样。
- en: Listing 6.10 Using `array_position()` to search for `Genres` string
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.10 使用 `array_position()` 搜索 `Genres` 字符串
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this section, we looked at the array using our `shows` data frame. We saw
    that a PySpark array contains elements of the same time, and an array column has
    access to some container functions (e.g., `size()`) as well as a handful of array-specific
    functions, prefixed by `array_`. The next section will introduce an equally useful
    but much less frequently used complex type, the map.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 `shows` 数据帧查看数组。我们看到了 PySpark 数组包含相同类型的元素，数组列可以访问一些容器函数（例如，`size()`）以及一些以
    `array_` 为前缀的数组特定函数。下一节将介绍一个同样有用但使用频率较低的复杂类型，即映射。
- en: '6.2.2 The map type: Keys and values within a column'
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 映射类型：列中的键和值
- en: This section covers the map column type and where it can be used successfully.
    Maps are less common as a column type; reading a JSON document won’t yield columns
    of type `map`, but they are nonetheless useful to represent simple key-value pairs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了映射列类型及其成功应用的地方。映射作为列类型并不常见；读取 JSON 文档不会产生类型为 `map` 的列，但它们对于表示简单的键值对仍然很有用。
- en: 'A map is conceptually very close to a Python *typed* dictionary: you have keys
    and values just like in a dictionary, but as with the array, the keys need to
    be of the same type, and the values need to be of the same type (the type for
    the keys can be different than the type for the values). Values can be `null`,
    but keys can’t, just like with Python dictionaries.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，映射与 Python 的 *类型化* 字典非常接近：你拥有键和值，就像在字典中一样，但与数组一样，键需要是同一类型，值也需要是同一类型（键的类型可以不同于值的类型）。值可以是
    `null`，但键不能，就像 Python 字典一样。
- en: One of the easiest ways to create a map is from two columns of type array. We
    will do so by collecting some information about the `name`, `language`, `type`,
    and `url` columns into an array and using the `map_from_arrays()` function, like
    in the next listing.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建映射的最简单方法之一是从两个数组类型的列。我们将通过收集有关 `name`、`language`、`type` 和 `url` 列的一些信息到一个数组，并使用
    `map_from_arrays()` 函数来实现，就像在下一条列表中一样。
- en: Listing 6.11 Creating a map from two arrays
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.11 从两个数组创建映射
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ We can access the value corresponding to a key using the dot notation within
    the col() function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们可以使用 col() 函数内的点符号来访问与键对应的值。
- en: ❷ We can also pass the key value within brackets, as we can in a Python dictionary.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们也可以在括号内传递键值，就像在 Python 字典中一样。
- en: ❸ Just like with the array, we can use dot notation to get the column and then
    use the bracket to select the right key.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 就像数组一样，我们可以使用点符号来获取列，然后使用括号来选择正确的键。
- en: Just like with the array, PySpark provides a few functions to work with maps
    under the `pyspark.sql.functions` module. Most of them are prefixed or suffixed
    with `map`, such as `map_values()` (which creates an array column out of the map
    values) or `create_map()` (which creates a map from the columns passed as a parameter,
    alternating between keys and values). The exercises at the end of this section
    and the end of the chapter provide more practice with the `map` column type.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 就像数组一样，PySpark 在 `pyspark.sql.functions` 模块下提供了一些函数来处理映射。其中大多数都以 `map` 为前缀或后缀，例如
    `map_values()`（它从映射值创建一个数组列）或 `create_map()`（它从作为参数传递的列创建映射，交替使用键和值）。本节末尾和本章末尾的练习提供了更多关于
    `map` 列类型的实践。
- en: If the `map` maps (pun intended) to a Python dictionary, why did our JSON document
    not have any maps? Because maps keys and values need to be the same type, respectively—something
    JSON objects are not forced to do—we need a more flexible container to accommodate
    objects. It’s also much more useful to have the top-level name/value pairs as
    columns, like PySpark did with our `shows` data frame in listing 6.3\. The next
    section will introduce the struct, which is the backbone of the data frame as
    we know it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果映射映射到 Python 字典，为什么我们的 JSON 文档没有映射？因为映射的键和值需要分别是同一类型，而 JSON 对象并不强制这样做——我们需要一个更灵活的容器来容纳对象。将顶层名称/值对作为列，就像
    PySpark 在列表 6.3 中的 `shows` 数据帧所做的那样，也更有用。下一节将介绍 struct，它是我们所知的数据帧的骨架。
- en: Null elements in arrays and maps
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数组和映射中的空元素
- en: 'When defining an array or a map, you can also pass an optional parameter (`containsNull`
    for the array, `valueContainsNull` for the map) that will indicate PySpark if
    it can accept `null` elements. This is different than the `nullable` flag at column
    level: here, we can mention if *any of the elements (or values)* can be `null`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义数组或映射时，你还可以传递一个可选参数（对于数组是 `containsNull`，对于映射是 `valueContainsNull`），这将指示
    PySpark 是否可以接受 `null` 元素。这与列级别的 `nullable` 标志不同：在这里，我们可以提到是否 *任何元素（或值）* 可以是 `null`。
- en: I don’t use non-nullable/no-`null`–element columns when working with data frames,
    but if your data model requires it, the option is available.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我在处理数据帧时不会使用不可为空的/无 `null` 元素的列，但如果你的数据模型需要它，这个选项是可用的。
- en: Exercise 6.1
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 6.1
- en: 'Assume the following JSON document:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 假设以下 JSON 文档：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: What is the schema once read by `spark.read.json`?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `spark.read.json` 读取后，模式是什么？
- en: Exercise 6.2
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 6.2
- en: 'Assume the following JSON document:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设以下 JSON 文档：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '6.3 The struct: Nesting columns within columns'
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 结构：列内的嵌套列
- en: This section covers the struct as a column type, and also as the foundation
    of the data frame. We look at how we can reason about our data frame in terms
    of structs and how to navigate a data frame with nested structs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍struct作为列类型，以及它是数据框的基础。我们探讨如何从struct的角度来推理我们的数据框，以及如何导航嵌套struct的数据框。
- en: 'The `struct` is akin to a JSON object, in the sense that the key or name of
    each pair is a string and that each record can be of a different type. If we take
    a small subset of the columns in our data frame, like in listing 6.12, we see
    that the `schedule` column contains two fields:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`struct`类似于JSON对象，因为在每个对的关键字或名称都是一个字符串，并且每条记录可以是不同类型。如果我们从我们的数据框的列中取一个小子集，就像列表6.12中那样，我们看到`schedule`列包含两个字段：'
- en: '`days`, an array of strings'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`days`，一个字符串数组'
- en: '`time`, a string'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time`，一个字符串'
- en: Listing 6.12 The `schedule` column with an array of strings and a string
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.12 带有字符串数组和字符串的`schedule`列
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '❶ The schedule column is a struct. When looking at the nesting that rises from
    the column, we notice that the struct contains two named fields: days (an Array[string])
    and time, a string.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调度列是一个struct。当我们查看从列中产生的嵌套时，我们注意到struct包含两个命名字段：days（一个Array[string]）和time，一个字符串。
- en: 'The struct is very different from the array and the map in that *the number
    of fields and their names are known ahead of time*. In our case, the `schedule`
    struct column is fixed: we know that each record of our data frame will contain
    that `schedule` struct (or a `null` value, if we want to be pedantic), and within
    that struct there will be an array of strings, `days`, and a string, `time`. The
    array and the map enforce the types of the values, but not their numbers or names.
    The struct allows for more versatility of types, as long as you name each field
    and provide the type ahead of time.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Struct与数组和map非常不同，因为它*在事先就知道字段的数量和名称*。在我们的例子中，`schedule` struct列是固定的：我们知道我们数据框的每条记录都将包含那个`schedule`
    struct（或者如果我们想严谨一点，是一个`null`值），并且在该struct中，将有一个字符串数组`days`和一个字符串`time`。数组和map强制执行值的类型，但不是它们的数量或名称。只要为每个字段命名并提供类型，struct就允许有更多类型的灵活性。
- en: Conceptually, I find that the easiest way to think about the struct column type
    is to imagine a small data frame within your column records. Using our example
    in listing 6.12, we can visualize that `schedule` is a data frame of two columns
    (`days` and `time`) trapped within the column. I illustrated the nested column
    analogy in figure 6.3.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我发现最容易思考struct列类型的方法是想象你的列记录中有一个小数据框。使用列表6.12中的例子，我们可以可视化`schedule`是一个包含两个列（`days`和`time`）的数据框，被困在列中。我在图6.3中说明了嵌套列的类比。
- en: '![](../Images/06-03.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3](../Images/06-03.png)'
- en: 'Figure 6.3 The `shows.select("schedule")` data frame. The column is a struct
    containing two named fields: `days` and `time`.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 `shows.select("schedule")` 数据框。该列是一个包含两个命名字段的struct：`days`和`time`。
- en: Structs are able to be nested within one another. As an example, in listing
    6.5 (or listing 6.13), the first field of our data frame, `_embedded`, is a struct
    that contains an array field, `episodes`. That array contains structs `_links`,
    which contains a struct `self`, which contains a string field, `href`. We are
    facing a pretty confusing nesting here! Don’t worry if this is still a little
    hard to envision; the next section will decipher the nesting dolls arrangement
    of structs by navigating our data frame.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Struct可以嵌套在彼此之中。例如，在列表6.5（或列表6.13）中，我们数据框的第一个字段`_embedded`是一个包含数组字段`episodes`的struct。该数组包含包含struct
    `_links`的struct，其中包含一个包含字符串字段`href`的struct。我们在这里面临一个相当复杂的嵌套结构！如果这仍然有点难以想象，请不要担心；下一节将通过导航我们的数据框来解释struct的嵌套娃娃排列。
- en: 6.3.1 Navigating structs as if they were nested columns
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 将struct视为嵌套列进行导航
- en: This section covers how to extract values from nested structs inside a data
    frame. PySpark provides the same convenience when working with nested columns
    as it would for regular columns. I cover the dot and bracket notations, and explain
    how PySpark treats nesting when using other complex structures. We work with the
    `_embedded` column by cleaning the useless nesting.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍如何从数据框内部的嵌套struct中提取值。PySpark在处理嵌套列时提供与处理常规列相同的便利性。我涵盖了点号和方括号表示法，并解释了PySpark在使用其他复杂结构时如何处理嵌套。我们通过清理无用的嵌套来处理`_embedded`列。
- en: Before going all hands on the keyboard, we’ll draft the structure of the `_embedded`
    column as a tree to get a sense of what we’re working with. In the following listing,
    I provide the output of the `printSchema()` command, which I drew in figure 6.4.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在动手操作键盘之前，我们将 `_embedded` 列的结构草拟为一个树形结构，以了解我们正在处理的内容。在下面的列表中，我提供了 `printSchema()`
    命令的输出，我在图 6.4 中绘制了这个输出。
- en: Listing 6.13 The `_embedded` column schema
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列 6.13 `_embedded` 列的模式
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '❶ _embedded contains a single field: episodes.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `_embedded` 包含一个字段：`episodes`。
- en: ❷ episodes is an Array[Struct]. Yes, it’s possible.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `episodes` 是一个 `Array[Struct]`。是的，这是可能的。
- en: ❸ Each episode is a record in the array, containing all the named fields in
    the struct. _links is a Struct[Struct[string]] field. PySpark will represent multiple
    levels of nesting without problems.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每个 `episodes` 都是数组中的一个记录，包含结构体中所有的命名字段。`_links` 是一个 `Struct[Struct[string]]`
    字段。PySpark 可以无问题地表示多级嵌套。
- en: 'For starters, we see in figure 6.4 that `_embedded` is a useless struct, as
    it contains only one field. In listing 6.14, I create a new top-level column called
    `episodes` that refers directly to the `episodes` field in the `_embedded` struct.
    For this, I use the `col` function and `_embedded.episodes`. This is consistent
    with the “struct as a mini data frame” mental model: you can refer to struct fields
    using the same notation as you would for a data frame.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在图 6.4 中看到 `_embedded` 是一个无用的结构体，因为它只包含一个字段。在列表 6.14 中，我创建了一个新的顶级列 `episodes`，它直接引用
    `_embedded` 结构体中的 `episodes` 字段。为此，我使用了 `col` 函数和 `_embedded.episodes`。这与“结构体作为迷你数据帧”的思维模型一致：你可以使用与数据帧相同的符号来引用结构体字段。
- en: '![](../Images/06-04.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4](../Images/06-04.png)'
- en: Figure 6.4 The schema for the `_embedded` field of our data frame
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 我们数据帧 `_embedded` 字段的模式
- en: Listing 6.14 Promoting the fields within a struct as columns
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列 6.14 将结构体内的字段提升为列
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ We lost the _embedded column and promoted the field of the struct (episodes)
    as a top-level column.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们丢失了 `_embedded` 列，并将结构体（`episodes`）的字段提升为顶级列。
- en: Finally, we look at drilling through structs nested in arrays. In section 6.2.1,
    I explained that we can refer to individual elements in the array using the index
    in brackets after the column reference. What about extracting the names of all
    the episodes, which are within the `episodes` array of structs?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看如何遍历嵌套在数组中的结构体。在第 6.2.1 节中，我解释了我们可以使用列引用后的括号中的索引来引用数组中的单个元素。那么，如何提取所有
    `episodes` 数组中嵌套的 `episodes` 的名称呢？
- en: 'Turns out PySpark will allow you to drill within an array and will return the
    subset of the struct *in array form*. This is best explained by an example: in
    the next listing, I extract the `episodes.name` field from the `shows_clean` data
    frame. Since `episodes` is an array of struct and `name` is one of the string
    fields, `episodes.name` is an array of strings.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，PySpark 允许你在数组内部进行遍历，并将结构体的子集以数组形式返回。这最好通过一个例子来说明：在下一个列表中，我从 `shows_clean`
    数据帧中提取了 `episodes.name` 字段。由于 `episodes` 是结构体数组，而 `name` 是字符串字段之一，因此 `episodes.name`
    是一个字符串数组。
- en: Listing 6.15 Selecting a field in an `Array[Struct]` to create a column
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列 6.15 在 `Array[Struct]` 中选择一个字段以创建列
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ episodes.name refers to the name field of the elements of the episodes array.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `episodes.name` 指的是 `episodes` 数组元素的名称字段。
- en: ❷ Since we have multiple records in the episodes array, episodes.name extracts
    the name field or each record in the array and packs it into an array of names.
    I explode (chapter 2 and section 6.5) the array to show the names clearly.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于 `episodes` 数组中有多个记录，`episodes.name` 提取数组中每个记录的名称字段，并将其打包成一个名称数组。我使用展开操作（第
    2 章和第 6.5 节）来清晰地显示这些名称。
- en: This section walked through the struct hierarchy using the same notation you
    would use for extracting columns from a data frame. We now can extract any field
    from our JSON document and know exactly what to expect. The next section will
    leverage our knowledge of complex data types and use that knowledge in crafting
    a schema. I also touch on the advantages and trade-offs of using hierarchical
    schemas and complex data types.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过使用与从数据帧中提取列相同的符号，遍历了结构体层次结构。现在我们可以从我们的 JSON 文档中提取任何字段，并确切地知道期望得到什么。下一节将利用我们对复杂数据类型的了解，并在构建模式时使用这些知识。我还简要提到了使用分层模式和复杂数据类型的优缺点。
- en: 6.4 Building and using the data frame schema
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 构建和使用数据帧模式
- en: In this section, I cover how to define and use a schema with a PySpark data
    frame. We build the schema for our JSON object programmatically and review the
    out-of-the-box types PySpark offers. Being able to use Python structures (serialized
    as JSON) means that we can manipulate our schemas just like any other data structure;
    we can reuse our data manipulation tool kit for manipulating our data frame’s
    metadata. By doing this, we also address the potential slowdown from `inferSchema`,
    as we don’t need Spark to read the data twice (once to infer the schema, once
    to perform the read).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍如何使用 PySpark 数据框定义和使用模式。我们以编程方式构建 JSON 对象的模式，并回顾 PySpark 提供的内置类型。能够使用
    Python 结构（序列化为 JSON）意味着我们可以像处理任何其他数据结构一样处理我们的模式；我们可以重用我们的数据操作工具包来操作数据框的元数据。通过这样做，我们还解决了
    `inferSchema` 的潜在减速问题，因为我们不需要 Spark 两次读取数据（一次推断模式，一次执行读取）。
- en: 'In section 6.3, I explained that we can think of a struct column as a mini
    data frame nested in said column. The opposite also works: you can think of a
    data frame as having a single-struct entity, with the columns the top-level fields
    of the “root” struct. In any output of `printSchema()` (I reproduced the relevant
    part of listing 6.5 in the next listing for convenience), all the top-level fields
    are connected to the `root`.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 6.3 节中，我解释了我们可以将结构列视为嵌套在该列中的微型数据框。反之亦然：您可以将数据框视为具有单个结构实体的结构，其中列是“根”结构的顶级字段。在任何
    `printSchema()` 的输出中（为了方便，我在下一列表中重现了列表 6.5 的相关部分），所有顶级字段都与 `root` 相连。
- en: Listing 6.16 A sample of the schema for the `shows` data frame
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.16 `shows` 数据框模式的示例
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ All the top-level fields (or columns) are children of a root implicit struct.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所有顶级字段（或列）都是根隐式结构的子结构。
- en: There are two syntaxes you can use to create a schema. In the next section,
    we review the explicit, programmatic one. PySpark also accepts a DDL-style schema,
    which is covered in chapter 7, where we discuss PySpark and SQL.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用两种语法来创建模式。在下一节中，我们将回顾显式、程序化的方法。PySpark 还接受 DDL 风格的模式，这在第 7 章中讨论，其中我们讨论了
    PySpark 和 SQL。
- en: 6.4.1 Using Spark types as the base blocks of a schema
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 使用 Spark 类型作为模式的基本块
- en: In this section, I cover the column types in the context of a schema definition.
    I build the schema for our `shows` data frame from scratch and include some programmatic
    niceties of the PySpark schema-building capabilities. I introduce PySpark data
    types and how to assemble them in a struct to build your data frame schema. Decoupling
    the data from the schema means that you can control how your data is represented
    in your data frame and improve the robustness of your data transformation programs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将从零开始介绍模式定义中的列类型。我构建了我们 `shows` 数据框的模式，并包括一些 PySpark 模式构建能力的编程技巧。我介绍了
    PySpark 数据类型以及如何在结构中组装它们来构建您的数据框模式。将数据与模式解耦意味着您可以控制数据在数据框中的表示方式，并提高您数据转换程序的鲁棒性。
- en: 'The data types we use to build a schema are located in the `pyspark.sql.types`
    module. They are such a frequent import when working with data frames that, just
    like `pyspark.sql.functions`, they are usually imported with the qualified prefix
    `T`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于构建模式的 数据类型 位于 `pyspark.sql.types` 模块中。当处理数据框时，这些数据类型被频繁导入，就像 `pyspark.sql.functions`
    一样，通常使用带限定前缀的 `T` 进行导入：
- en: '[PRE19]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Tip Just like with functions using a capital `F`, the common convention is to
    use a capital `T` when importing the types module. I strongly recommend doing
    the same.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：与使用大写字母`F`的函数一样，当导入类型模块时，常见的约定是使用大写字母`T`。我强烈建议这样做。
- en: 'Within the `pyspark.sql.types`, there are two main kinds of objects. First,
    you have the `types` object, which represents a column of a certain type. All
    of those objects follow the `ValueType()` CamelCase syntax: for instance, a long
    column would be represented by a `LongType()` object. Most scalar types do not
    take any parameters (except for `DecimalType(precision, scale)`, which is used
    for decimal numbers that have a precise amount of precision before and after the
    decimal point). Complex types, such as the array and the map, take the types of
    their values directly in the constructor. For example, an array of strings would
    be `ArrayType(StringType())`, and a map of strings mapping to longs would be `MapType(StringType(),
    LongType())`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pyspark.sql.types`中，有两种主要类型的对象。首先，你有`types`对象，它代表了一定类型的列。所有这些对象都遵循`ValueType()`驼峰式语法：例如，长列将由一个`LongType()`对象表示。大多数标量类型不接收任何参数（除了用于小数点前后有精确数量精度的`DecimalType(precision,
    scale)`，它用于小数）。复杂类型，如数组和映射，直接在构造函数中接收它们的值类型。例如，字符串数组将是`ArrayType(StringType())`，而将字符串映射到长整型的映射将是`MapType(StringType(),
    LongType())`。
- en: Second, you have the field object; in other words, the `StructField()`. PySpark
    provides a `StructType()` that can contain an arbitrary number of named fields;
    programmatically, this translates to a `StructType()` taking a list of `StructField()`.
    Easy as that!
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，你有字段对象；换句话说，是`StructField()`。PySpark提供了一个可以包含任意数量命名字段的`StructType()`；在程序上，这相当于一个接收`StructField()`列表的`StructType()`。就这么简单！
- en: 'A `StructField()` contains two mandatory as well as two optional parameters:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructField()`包含两个强制参数以及两个可选参数：'
- en: The `name` of the field, passed as a string
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段的`name`，以字符串形式传递
- en: The `dataType` of the field, passed as a type object
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段的`dataType`，以类型对象形式传递
- en: (Optional) A `nullable` flag, which determines if the field can be `null` or
    not (by default `True`)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）一个`nullable`标志，它确定字段是否可以是`null`（默认为`True`）。
- en: (Optional) A `metadata` dictionary that contains arbitrary information, which
    we will use for column metadata when working with ML pipelines (in chapter 13)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）一个`metadata`字典，它包含任意信息，我们将使用它作为与ML管道（在第13章中）一起工作的列元数据。
- en: Tip If you provide a reduced schema—meaning you only define a subset of the
    fields—PySpark will only read the defined fields. In the case where you only need
    a subset of columns/fields from a very wide data frame, you can save a significant
    amount of time!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果您提供了一个简化的模式——这意味着您只定义了字段的一个子集——PySpark将只读取定义的字段。在您只需要从非常宽的数据框中读取子集的列/字段的情况下，您可以节省大量时间！
- en: 'Putting all this together, the `summary` string field of the `shows` data frame
    would be encoded in a StructField like so:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，`shows`数据框的`summary`字符串字段将被编码在一个`StructField`中，如下所示：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In listing 6.17, I’ve done the `_embedded` schema of the `shows` data frame.
    While very verbose, we gain intimate knowledge of the data frame structure. Since
    the data frame schemas are regular Python classes, we can assign them to variables
    and build our schema from the bottom up. I usually split the structs containing
    more than three or so fields into their own variables, so my code doesn’t read
    like a whole block of structs interspersed with brackets.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表6.17中，我完成了`shows`数据框的`_embedded`模式。虽然非常冗长，但我们获得了对数据框结构的深入了解。由于数据框模式是常规的Python类，我们可以将它们分配给变量，并从底部向上构建我们的模式。我通常将包含三个或更多字段的`struct`拆分成自己的变量，这样我的代码就不会像是一个带有括号的整个`struct`块。
- en: Listing 6.17 The schema for the `_embedded` field
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.17 `_embedded`字段的模式
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '❶ The _links field contains a self struct that itself contains a single-string
    field: href.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `_links`字段包含一个包含单个字符串字段的self `struct`：href。
- en: '❷ The image field is a struct of two string fields: medium and original.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 图片字段是一个包含两个字符串字段的`struct`：medium和original。
- en: ❸ Since types are Python objects, we can pass them to variables and use them.
    Using episodes_links_schema and episode_image_schema makes our schema for an episode
    look much cleaner.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于类型是Python对象，我们可以将它们传递给变量并使用它们。使用episodes_links_schema和episode_image_schema可以使我们的剧集模式看起来更加整洁。
- en: ❹ It’s obvious that our _embedded column contains a single field, episodes,
    which contains an array of episodes. Using good variable names helps with documenting
    our intent without relying on comments.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 很明显，我们的`_embedded`列包含一个名为episodes的单个字段，它包含一个剧集数组。使用好的变量名有助于记录我们的意图，而无需依赖注释。
- en: 'This section covered how to build a schema from the bottom up: you can use
    the types and field from the `pyspark.sql.types` module and create one field for
    each column. When you have a struct column, you treat it the same way: create
    a `StructType()` and assign struct fields. With these simple rules, you should
    be able to construct any schema you need. The next section will leverage our schema
    to read the JSON in strict fashion.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何自下而上构建模式：您可以使用`pyspark.sql.types`模块中的类型和字段，并为每一列创建一个字段。当您有一个结构化列时，您以相同的方式处理：创建一个`StructType()`并分配结构化字段。遵循这些简单的规则，您应该能够构建您需要的任何模式。下一节将利用我们的模式以严格的方式读取JSON。
- en: 6.4.2 Reading a JSON document with a strict schema in place
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2：在现有严格模式中读取JSON文档
- en: This section covers how to read a JSON document while enforcing a precise schema.
    This proves extremely useful when you want to improve the robustness of your data
    pipeline; it’s better to know you’re missing a few columns at ingestion time than
    to get an error later in the program. I review some convenient practices when
    you expect the data to fit a certain mold and how you can rely on PySpark to keep
    you sane in the world of messy JSON documents. As a bonus, you can expect a better
    performance when reading data with a schema in place, because `inferSchema` requires
    a pre-read of the data just to infer the schema.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何在强制执行精确模式的情况下读取JSON文档。当您希望提高数据管道的鲁棒性时，这非常有用；在程序后期出现错误时，知道您在摄入时缺少几个列比知道您缺少列要好。我回顾了一些方便的实践，当您期望数据符合某种模式时，以及您如何依赖PySpark在混乱的JSON文档世界中保持清醒。作为额外的好处，当使用模式读取数据时，您可以期待更好的性能，因为`inferSchema`需要预先读取数据以推断模式。
- en: 'If you analyzed listing 6.17 field by field, you might have realized that I
    defined `airdate` as a date and `airstamp` as a timestamp. In section 6.1.2, I
    listed the types available within a JSON document; missing from the lot were dates
    and timestamps. PySpark has your back on this: we can, fortunately, leverage some
    options of the JSON reader to read certain strings as dates and timestamps. To
    do so, you need to provide a full schema for your document; good thing we have
    one ready. In listing 6.18, I read my JSON document once more, but this time I
    provide an explicit schema. Note the change in type for `airdate` and `airstamp`.
    I also provide a new parameter, `mode`, which, when set to `FAILFAST`, will error
    if it encounters a malformed record versus the schema provided.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您逐字段分析了6.17的列表，您可能会意识到我将`airdate`定义为日期，将`airstamp`定义为时间戳。在6.1.2节中，我列出了JSON文档中可用的类型；其中缺少的是日期和时间戳。幸运的是，PySpark在这方面支持您：我们可以利用JSON读取器的某些选项来读取某些字符串作为日期和时间戳。为此，您需要为您的文档提供一个完整的模式；幸运的是，我们已经有了一个现成的模式。在6.18列表中，我再次读取我的JSON文档，但这次我提供了一个显式的模式。注意`airdate`和`airstamp`类型的变化。我还提供了一个新的参数`mode`，当设置为`FAILFAST`时，如果遇到与提供的模式不匹配的格式错误的记录，则会报错。
- en: Because we only pass a partial schema (`embedded_schema`), PySpark will only
    read the defined columns. In this case, we only cover the `_embedded` struct,
    so that’s the only part of the data frame we read. This is a convenient way to
    avoid reading everything before dropping unused columns.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只传递了部分模式（`embedded_schema`），PySpark将只读取定义的列。在这种情况下，我们只覆盖了`_embedded`结构化，所以我们只读取数据框的这一部分。这是一种方便的方法，可以在删除未使用的列之前避免读取所有内容。
- en: Since our dates and timestamp in our JSON document are ISO-8601 compliant (`yyyy-MM-dd`
    for the date and `yyyy-MM-ddTHH:mm:ss.SSSXXX` for the timestamp), we do not have
    to customize the JSON `DataFrameReader` to automatically parse our values. If
    you are facing a nonstandard date or timestamp format, you’ll need to pass the
    right format to `dateFormat` or `timestampFormat`. The format grammar is available
    on the official Spark documentation website ([http://mng.bz/6ZgD](http://mng.bz/6ZgD)).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的JSON文档中的日期和时间戳符合ISO-8601标准（日期为`yyyy-MM-dd`，时间戳为`yyyy-MM-ddTHH:mm:ss.SSSXXX`），我们不需要自定义JSON
    `DataFrameReader`来自动解析我们的值。如果您遇到非标准日期或时间戳格式，您需要将正确的格式传递给`dateFormat`或`timestampFormat`。格式语法可在官方Spark文档网站上找到（[http://mng.bz/6ZgD](http://mng.bz/6ZgD)）。
- en: Warning If you are using any version of Spark 2, the format followed for `dateFormat`
    and `timestampFormat` is different. Look for `java.text.SimpleDateFormat` if this
    is the case.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果您正在使用Spark 2的任何版本，`dateFormat`和`timestampFormat`遵循的格式是不同的。如果情况如此，请查找`java.text.SimpleDateFormat`。
- en: Listing 6.18 Reading a JSON document using an explicit partial schema
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.18：使用显式部分模式读取JSON文档
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ We pass our schema to the schema parameter. Since our schema is a subset of
    the JSON document, we only read the defined fields.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们将我们的模式传递给模式参数。由于我们的模式是JSON文档的一个子集，我们只读取定义的字段。
- en: ❷ By selecting the FAILFAST mode, our DataFrameReader will crash if our schema
    is incompatible.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过选择FAILFAST模式，如果我们的模式不兼容，我们的DataFrameReader将会崩溃。
- en: A successful read is promising, but since I want to verify my new date and timestamp
    field, I drill, explode, and show the fields in the following listing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的读取是有希望的，但既然我想验证我的新日期和时间戳字段，我就深入挖掘，爆炸，并在以下列表中显示字段。
- en: Listing 6.19 Validating the `airdate` and `airstamp` field reading
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.19 验证`airdate`和`airstamp`字段读取
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Everything here looks fine. What happens if the schema does not match? PySpark,
    even in `FAILFAST`, will allow absent fields in the document if the schema allows
    for `null` values. In listing 6.20, I pollute my schema, changing two `StringType()`
    to `LongType()`. I did not include the whole stack trace, but the resulting error
    is a `Py4JJavaError` that hits it right on the head: our string value is not a
    `bigint` (or `long`). You won’t know which one, though: the stack trace only gives
    what it tried to parse and what is expected.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里看起来一切正常。如果模式不匹配会发生什么？即使在`FAILFAST`模式下，如果模式允许`null`值，PySpark也会允许文档中缺少字段。在列表6.20中，我污染了我的模式，将两个`StringType()`改为`LongType()`。我没有包括整个堆栈跟踪，但结果是直接命中要害的`Py4JJavaError`：我们的字符串值不是`bigint`（或`long`）。不过，你不会知道是哪一个：堆栈跟踪只给出了它尝试解析的内容和预期的内容。
- en: Note `Py4J` ([https://www.py4j.org/](https://www.py4j.org/)) is a library that
    enables Python programs to access Java objects in a JVM. In the case of PySpark,
    it helps bridge the gap between the pythonic veneer and the JVM-based Spark. In
    chapter 2, we saw—without naming it—`Py4J` in action, as most `pyspark.sql.functions`
    call a `_jvm` function. This makes the core Spark functions as fast in PySpark
    as they are in Spark, at the expense of some odd errors once in a while.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`Py4J`（[https://www.py4j.org/](https://www.py4j.org/））是一个库，它使Python程序能够访问JVM中的Java对象。在PySpark的情况下，它帮助弥合了Pythonic外观和基于JVM的Spark之间的差距。在第2章中，我们看到——虽然没有命名——`Py4J`在行动，因为大多数`pyspark.sql.functions`调用一个`_jvm`函数。这使得核心Spark函数在PySpark中的速度与在Spark中一样快，但有时会带来一些奇怪的错误。
- en: Listing 6.20 Witnessing a JSON document ingestion with incompatible schema
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.20 观察具有不兼容模式的JSON文档摄入
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ I import the relevant error (Py4JJavaError) to be able to catch and analyze
    it.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我导入相关的错误（Py4JJavaError）以便能够捕获和分析它。
- en: ❷ I change two fields from string to long in my schema.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我在我的模式中将两个字段从字符串改为长整型。
- en: ❸ PySpark will give the types of the two fields, but won’t give you which field
    is problematic. Time for some forensic analysis, I guess.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ PySpark将给出两个字段的类型，但不会告诉你哪个字段有问题。我想是时候进行一些法医分析了。
- en: This section was a short one but is still incredibly useful. We saw how to use
    the schema information to create a strict contract between the data provider and
    the data processor (us). In practice, this kind of strict schema assertion provides
    a better error message when the data is not what you expect, and allows you to
    avoid some errors (or a wrong result) down the line.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部分虽然简短，但仍然非常有用。我们看到了如何使用模式信息在数据提供者和数据处理者（我们）之间创建一个严格的合同。在实践中，这种严格的模式断言在数据不是你期望的那样时提供了更好的错误消息，并允许你避免一些错误（或错误的结果）。
- en: 'FAILFAST: When do you want to get in trouble?'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: FAILFAST：你什么时候想陷入麻烦？
- en: It seems a little paranoid to use `FAILFAST` while setting a verbose schema
    all by hand. Unfortunately, data is messy and people can be sloppy, and when you
    rely on data to make decisions, *garbage in, garbage out*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动设置详细模式时使用`FAILFAST`似乎有点偏执。不幸的是，数据很混乱，人们可能会粗心大意，当你依赖数据来做决策时，*垃圾输入，垃圾输出*。
- en: 'In my professional career, I’ve encountered data integrity problems so often
    when reading data that I now firmly believe that you need to diagnose them as
    early as possible. `FAILFAST` mode is one example: by default, PySpark will set
    malformed records to `null` (the `PERMISSIVE` approach). When exploring, I consider
    this perfectly legitimate. But I’ve had enough sleepless nights after a business
    stakeholder called me at the last minute because the “results are weird” and thus
    try to minimize data drama at every opportunity.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的职业生涯中，我在读取数据时遇到了很多数据完整性问题，以至于我现在坚信你需要尽早诊断这些问题。`FAILFAST`模式就是一个例子：默认情况下，PySpark会将格式错误的记录设置为`null`（`PERMISSIVE`方法）。在探索时，我认为这是完全合理的。但在我被一个业务利益相关者在最后一刻打电话给我，说“结果很奇怪”之后，我经历了很多不眠之夜，因此我尽可能地减少数据戏剧性。
- en: '6.4.3 Going full circle: Specifying your schemas in JSON'
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 完整循环：在 JSON 中指定你的架构
- en: This section covers a different approach to the schema definition. Instead of
    using the verbose constructors seen in section 6.4, I explain how you can define
    your schema in JSON. We’re going full circle using JSON for both the data and
    its schema!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一种不同的架构定义方法。与 6.4 节中看到的冗长构造函数不同，我解释了如何使用 JSON 定义你的架构。我们使用 JSON 来定义数据和其架构，实现了完整循环！
- en: The `StructType` object has a handy `fromJson()` method (note the `camelCase`
    used here, where the first letter of the first word is not capitalized, but the
    others are) that will read a JSON-formatted schema. As long as we know how to
    provide a proper JSON schema, we should be good to go.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructType` 对象有一个方便的 `fromJson()` 方法（注意这里使用的 `camelCase`，其中第一个单词的首字母不使用大写，其余则使用大写），它将读取一个
    JSON 格式的架构。只要我们知道如何提供适当的 JSON 架构，我们就应该可以顺利地进行。'
- en: 'To understand the layout and content of a typical PySpark data frame, we use
    our `shows_with_schema` data frame and the `schema` attribute. Unlike `printSchema()`,
    which prints our schema to a standard output, `schema` returns an internal representation
    of the schema in terms of `StructType`. Fortunately, `StructType` comes with two
    methods for exporting its content into a JSON-esque format:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解典型 PySpark 数据框的布局和内容，我们使用我们的 `shows_with_schema` 数据框和 `schema` 属性。与 `printSchema()`
    不同，后者将我们的架构打印到标准输出，`schema` 返回一个以 `StructType` 为术语的架构的内部表示。幸运的是，`StructType` 提供了两种方法将内容导出为类似
    JSON 的格式：
- en: '`json()` will output a string containing the JSON-formatted schema.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`json()` 将输出一个包含 JSON 格式架构的字符串。'
- en: '`jsonValue()` will return the schema as a dictionary.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jsonValue()` 将返回架构作为字典。'
- en: 'In listing 6.21, I pretty-print, with the help of the `pprint` module from
    the standard library, a subset of the schema of the `shows_with_schema` data frame.
    The result is very reasonable—each element is a JSON object containing four fields:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.21 中，我使用标准库中的 `pprint` 模块，将 `shows_with_schema` 数据框的架构的一部分进行了美化打印。结果是相当合理的——每个元素都是一个包含四个字段的
    JSON 对象：
- en: '`name`, a string representing the name of the field'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`，一个表示字段名称的字符串'
- en: '`type`, a string (for scalar values) containing the data type (e.g., `"string"`
    or `"long"`) or an object (for complex values) representing the type of the field'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type`，一个字符串（用于标量值）包含数据类型（例如，`"string"` 或 `"long"`）或一个对象（用于复杂数值），表示字段的类型'
- en: '`nullable`, a Boolean indicating if the field can contain `null` values'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nullable`，一个布尔值，表示字段是否可以包含 `null` 值'
- en: a `metadata` object containing the metadata of the field
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含字段元数据的 `metadata` 对象
- en: Listing 6.21 Pretty-printing the schema
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.21 美化打印架构
- en: '[PRE25]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ pprint pretty prints Python data structures into the shell. It makes reading
    nested dictionaries much easier.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `pprint` 将 Python 数据结构美化打印到壳中。它使得阅读嵌套字典变得容易得多。
- en: These are the same parameters we pass to a `StructField`, as seen in section
    6.4.1\. The array, map, and struct have a slightly more involved type representation
    to go with their slightly more involved data representation. Rather than enumerating
    them out long, remember that you can have a refresher straight from your REPL
    by creating a dummy object and calling `jsonValue()` on it. I do it in the following
    listing.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们传递给 `StructField` 的相同参数，如 6.4.1 节所示。数组、映射和结构体具有稍微复杂一些的类型表示，以匹配它们稍微复杂一些的数据表示。与其长篇累牍地列举它们，记住你可以通过创建一个虚拟对象并在其上调用
    `jsonValue()` 来直接从你的 REPL 中获得复习。我在以下列表中这样做。
- en: Listing 6.22 Pretty-printing dummy complex types
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.22 美化打印虚拟复杂数据类型
- en: '[PRE26]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '❶ The array types contains three elements: containsNull, elementType, and type
    (which is always array).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数组类型包含三个元素：containsNull、elementType 和 type（总是数组）。
- en: ❷ The map contains similar elements as the array, but with keyType and valueType
    instead of elementType and valueContainsNull (a null key does not make sense).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 映射包含与数组类似元素，但使用 keyType 和 valueType 代替 elementType 和 valueContainsNull（null
    键没有意义）。
- en: '❸ The struct contains the same elements as the constructors: we have a type
    of struct and a fields element containing an JSON array of objects. Each StructField
    contains the same four fields as the constructor seen in section 6.3.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 结构体包含与构造函数相同的元素：我们有一个结构体类型和一个包含 JSON 对象数组的字段元素。每个 `StructField` 包含与 6.3 节中看到的构造函数相同的四个字段。
- en: Finally, we can close the loop by making sure that our JSON-schema is consistent
    with the one currently being used. For this, we’ll export the schema of `shows_with_schema`
    in a JSON string, load it as a JSON object, and then use `StructType.fromJson()`
    method to re-create the schema. As we can see in the next listing, the two schemas
    are equivalent.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过确保我们的 JSON 模式与当前正在使用的模式一致来闭合循环。为此，我们将 `shows_with_schema` 的模式导出为 JSON
    字符串，将其加载为 JSON 对象，然后使用 `StructType.fromJson()` 方法重新创建模式。正如我们可以在下一个列表中看到的那样，这两个模式是等效的。
- en: Listing 6.23 Validating JSON schema is equal to data frame schema
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.23 验证 JSON 模式等于数据框模式
- en: '[PRE27]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: While this seems like a mere parlor trick, having the ability to serialize the
    schema of your data frame in a common format is a great help on your journey to
    consistent and predictable big data. You can version-control your schema and share
    your expectations with others. Furthermore, since JSON has a high affinity to
    Python dictionaries, you can use regular Python code to convert to and from any
    schema-definition language. (Chapter 7 contains information about DDL, a way to
    describe data schemas, which is what SQL databases use for defining schemas).
    PySpark gives you first-class access to define and access your data layout.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来像是一个简单的客厅把戏，但能够将你的数据框模式序列化为通用格式，对你的大数据一致性和可预测性之旅大有裨益。你可以对模式进行版本控制，并与他人分享你的期望。此外，由于
    JSON 与 Python 字典有很高的亲和力，你可以使用常规的 Python 代码在任意模式定义语言之间进行转换。（第 7 章包含有关 DDL 的信息，DDL
    是描述数据模式的一种方式，SQL 数据库用于定义模式）。PySpark 允许你以一等公民的身份定义和访问你的数据布局。
- en: This section covered how PySpark organizes data within a data frame and communicates
    this back to you through the schema. You learned how to create one programmatically,
    as well as how to import and export JSON-formatted schemas. The next section explains
    why complex data structures make sense when analyzing large data sets.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 PySpark 在数据框内组织数据以及如何通过模式将此信息传达给你。你学习了如何通过编程创建模式，以及如何导入和导出 JSON 格式的模式。下一节将解释为什么在分析大型数据集时，复杂的数据结构是有意义的。
- en: Exercise 6.3
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 6.3
- en: What is wrong with this schema?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式有什么问题？
- en: '[PRE28]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '6.5 Putting it all together: Reducing duplicate data with complex data types'
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 将一切整合：使用复杂数据类型减少重复数据
- en: This section takes the hierarchical data model and presents the advantages in
    a big data setting. We look at how it helps reduce data duplication without relying
    on auxiliary data frames and how we can expand and contract the complex types.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本节采用层次数据模型，并展示了在大数据环境下的优势。我们探讨了它如何帮助减少数据重复，而不依赖于辅助数据框，以及我们如何扩展和收缩复杂类型。
- en: 'When looking at a new table (or a data frame), I always ask myself, *what does
    each record contain*? Another way to approach this question is by completing the
    following sentence: each record contains a single ____________.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看一个新的表格（或数据框）时，我总是问自己，*每条记录包含什么内容*？另一种处理这个问题的方式是完成以下句子：每条记录包含一个 ____________。
- en: 'Tip Database folks sometimes call this a *primary key*. A primary key has specific
    implications in data base design. In my day-to-day life, I use the term *exposure
    record*: each record represents a single point of exposure, meaning that there
    is no overlap between records. This avoids domain-specific language (retail: customer
    or transaction; insurance: insured or policy year; banking: customer or balance
    at end of day). This is not an official term, but I find it very convenient, as
    it is portable across domains.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：数据库人员有时称这为 *主键*。主键在数据库设计中具有特定的含义。在我的日常生活中，我使用术语 *曝光记录*：每条记录代表一个单独的曝光点，这意味着记录之间没有重叠。这避免了特定领域的语言（零售：客户或交易；保险：被保险人或保单年度；银行：客户或日终余额）。这不是一个官方术语，但我发现它非常方便，因为它可以在各个领域之间迁移。
- en: In the case of the `shows` data frame, each record contains a single *show*.
    When looking at the fields, we can say “each show has a (insert name of the field).”
    For instance, each show has an ID, a name, a URL, and so on. What about episodes?
    A show definitely has more than one episode. By now, I am pretty sure you see
    how the hierarchical data model and the complex Spark column types solve this
    elegantly, but let’s review what the traditional “rows and columns” model has
    to say about this.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `shows` 数据框的情况下，每条记录包含一个 *show*。当查看字段时，我们可以这样说：“每个 show 有一个（插入字段名称）。”例如，每个
    show 有一个 ID、一个名称、一个 URL 等等。关于剧集呢？一个 show 一定有不止一个剧集。到现在为止，我非常确信你已经看到了层次数据模型和复杂的
    Spark 列类型是如何优雅地解决这个问题，但让我们回顾一下传统的“行和列”模型对此有何看法。
- en: In the two-dimensional world, if we wanted to have a table containing shows
    and episodes, we would proceed with one of two scenarios.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维世界中，如果我们想要有一个包含节目和剧集的表，我们会进行两种情况之一。
- en: '![](../Images/06-05.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-05.png)'
- en: Figure 6.5 A hierarchical relationship can be expressed via a link/relation
    between two tables. Here, our show is linked to its episodes through a `show_id`
    key.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5通过两个表之间的链接/关系可以表达层次关系。在这里，我们的节目通过`show_id`键与其剧集相链接。
- en: First, we could have a `shows` table linked to an `episodes` table, using a
    star schema like the one encountered in chapters 4 and 5\. Visually, figure 6.5
    explains how we would separate the `shows` and `episodes` hierarchical relationship
    using two tables. In this case, our data is *normalized*, and while we have no
    duplication, getting all the information we want means joining tables according
    to keys.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以有一个`shows`表与一个`episodes`表相链接，使用类似于第4章和第5章中遇到的星型模式。从视觉上看，图6.5解释了我们将如何使用两个表来分离`shows`和`episodes`的层次关系。在这种情况下，我们的数据是*规范化的*，而且我们没有重复，但获取所有我们想要的信息意味着根据键连接表。
- en: Second, we could have a joined table with scalar records (no nested structure).
    In our case, it becomes harder to make sense of our unit of exposure. If we look
    at the map and array types we’d need to “scalarize,” we have shows, episodes,
    genres, and days. An “each episode-show-genre-day-of-airing” unit of exposure
    table makes little sense. In figure 6.6, I show a table with only those four records
    as an example. We see duplication of the data for the `show_id` and the `genre`,
    which provides no additional information. Furthermore, having a joined table means
    that the relationship between the records is lost. Is the `genre` field the genre
    of the show or the episode?
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们可以有一个包含标量记录的连接表（没有嵌套结构）。在我们的情况下，这使我们的暴露单位难以理解。如果我们看看我们需要“标量化”的地图和数组类型，我们有节目、剧集、类型和天数。一个“每个剧集-节目-类型-播出日”的暴露单位表几乎没有意义。在图6.6中，我展示了一个只有这四个记录的表作为示例。我们看到`show_id`和`genre`的数据重复，这并没有提供额外的信息。此外，有一个连接表意味着记录之间的关系丢失。`genre`字段是节目的类型还是剧集的类型？
- en: '![](../Images/06-06.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-06.png)'
- en: Figure 6.6 A joined representation of our `shows` hierarchical model. We witness
    data duplication and a loss of relationship information.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6展示了我们的`shows`层次模型的连接表示。我们见证了数据重复和关系信息的丢失。
- en: Since the beginning of the book, all of our data processing has tried to converge
    with having a single table. If we want to avoid data duplication, keep the relationship
    information, and have a single table, then we can—and should!—use the data frame’s
    complex column types. In our `shows` data frame
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 自从本书开始，我们所有的数据处理都试图收敛到只有一个表。如果我们想避免数据重复，保留关系信息，并且有一个单一的表，那么我们可以——并且应该！——使用数据帧的复杂列类型。在我们的`shows`数据帧中
- en: Each record represents a show.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个记录代表一个节目。
- en: A show has multiple episodes (array of structs column).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一档节目包含多个剧集（结构体数组的列）。
- en: Each episode has many fields (struct column within the array).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个剧集有许多字段（数组内的结构体列）。
- en: Each show can have multiple genres (array of string column).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节目可以有多个类型（字符串数组的列）。
- en: Each show has a schedule (struct column).
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节目都有一个时间表（结构体列）。
- en: Each schedule belonging to a show can have multiple days (array) but a single
    time (string).
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个属于节目的时间表可以有多个日期（数组），但只有一个时间（字符串）。
- en: Visually, it looks like figure 6.7\. It’s clear that the episodes, the genre,
    and the schedule belong to the shows, yet we can have multiple episodes without
    duplicating any data.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，它看起来像图6.7。很明显，剧集、类型和时间表属于节目，但我们可以在不重复任何数据的情况下有多个剧集。
- en: '![](../Images/06-07.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-07.png)'
- en: Figure 6.7 A sample of the `shows` data frame showcasing the hierarchical (or
    object-oriented) model
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7展示了`shows`数据帧的一个示例，展示了层次结构（或面向对象）模型。
- en: An efficient, hierarchical data model is a thing of beauty, but sometimes we
    need to leave our ivory tower and work on the data. The next section will show
    how to expand and contract array columns to your liking to get your Goldilocks
    data frame at every stage.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 一个高效、层次化的数据模型是一件美丽的事物，但有时我们需要走出象牙塔，处理数据。下一节将展示如何根据您的喜好扩展和收缩数组列，以在每个阶段获得您的“金发女郎”数据帧。
- en: '6.5.1 Getting to the “just right” data frame: Explode and collect'
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1达到“恰到好处”的数据帧：分解和收集
- en: This section covers how to use `explode` and `collect` operations to go from
    hierarchical to tabular and back. We cover the methods to break an array or a
    map into discrete records and how to get the records back into the original structure.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了如何使用 `explode` 和 `collect` 操作从层次结构转换为表格结构，并返回原结构。我们涵盖了将数组或映射拆分为离散记录的方法，以及如何将这些记录重新组合到原始结构中。
- en: In chapter 2, we already saw how to break an array of values into discrete records
    using the `explode()` function. We will now revisit the exploding operation by
    generalizing it to the map, looking at the behavior when your data frame has multiple
    columns, and seeing the different options PySpark provided.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，我们已经看到了如何使用 `explode()` 函数将值数组拆分为离散记录。现在，我们将通过将其泛化到映射中重新审视爆炸操作，查看数据帧具有多个列时的行为，并查看
    PySpark 提供的不同选项。
- en: In listing 6.24, I take a small subset of columns and explode the `_embedded.episodes`
    one, producing a data frame containing one record per episode. This is the same
    use case that we saw in chapter 2, but with more columns present. PySpark duplicates
    the values in the columns that aren’t being exploded.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 6.24 中，我选取了一小部分列，将 `_embedded.episodes` 爆炸一次，生成一个包含每个剧集一条记录的数据帧。这与我们在第 2
    章中看到的用例相同，但列更多。PySpark 会复制那些未被爆炸的列中的值。
- en: Listing 6.24 Exploding the `_embedded.episodes` into 53 distinct records
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.24 将 `_embedded.episodes` 爆炸成 53 个不同的记录
- en: '[PRE29]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ We explode an array column creating one record per element contained in the
    array.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们爆炸一个数组列，为数组中的每个元素创建一个记录。
- en: 'Explode can also happen with maps: the keys and values will be exploded in
    two different fields. For completeness, I’ll introduce the second type of explosion:
    `posexplode()`. The “pos” stands for position: it explodes the column and returns
    an additional column before the data that contains the position as a long. In
    listing 6.25, I create a simple map from two fields in the array, then `posexplode()`
    each record. Since a map column has a key and a value field, `posexplode()` on
    a map column will generate three columns; when aliasing the result, we need to
    pass three parameters to `alias()`.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 爆炸也可以应用于映射：键和值将在两个不同的字段中爆炸。为了完整性，我将介绍第二种爆炸类型：`posexplode()`。其中“pos”代表位置：它爆炸列并返回一个包含位置的
    long 类型的额外列。在列表 6.25 中，我从数组中的两个字段创建了一个简单的映射，然后对每个记录进行 `posexplode()`。由于映射列有一个键和一个值字段，对映射列进行
    `posexplode()` 将生成三个列；在别名结果时，我们需要向 `alias()` 传递三个参数。
- en: Listing 6.25 Exploding a map using `posexplode()`
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.25 使用 `posexplode()` 爆炸映射
- en: '[PRE30]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '❶ We build a map from two arrays: first is the key; second is the values.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们从两个数组中构建一个映射：第一个是键；第二个是值。
- en: '❷ By position exploding, we create three columns: the position, the key, and
    the value of each element in our map have a record.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过位置爆炸，我们创建了三个列：位置、键和映射中每个元素的值。
- en: Both `explode()` and `posexplode()` will skip any `null` values in the array
    or the map. If you want to have `null` as records, you can use `explode_outer()`
    or `posexplode_outer()` the same way.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`explode()` 和 `posexplode()` 都会跳过数组或映射中的任何 `null` 值。如果您想将 `null` 作为记录，可以使用
    `explode_outer()` 或 `posexplode_outer()`，方法相同。'
- en: 'Now that we have exploded data frames, we’ll do the opposite by collecting
    our records into a complex column. For this, PySpark provides two aggregation
    functions: `collect_list()` and `collect_set()`. Both work the same way: they
    take a column as an argument and return an array column as a result. Where `collect_list()`
    returns one array element per column record, `collect_set()` will return one array
    element per *distinct* column record, just like a Python set.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经爆炸了数据帧，我们将通过将记录收集到一个复杂列中来做相反的操作。为此，PySpark 提供了两个聚合函数：`collect_list()`
    和 `collect_set()`。这两个函数的工作方式相同：它们将列作为参数，并返回一个数组列作为结果。`collect_list()` 每个列记录返回一个数组元素，而
    `collect_set()` 将每个 *distinct* 列记录返回为一个数组元素，就像 Python 集合一样。
- en: Listing 6.26 Collecting our results back into an array
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.26 将我们的结果收集回数组
- en: '[PRE31]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Collecting an exploded map is not supported out of the box, but it’s easy knowing
    that you can pass multiple `collect_list()` functions as an argument to `agg()`.
    You can then use `map_from_arrays()`. Look at listings 6.25 and 6.26 for the building
    blocks.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，不支持收集爆炸后的映射，但您可以通过将多个 `collect_list()` 函数作为参数传递给 `agg()` 函数来轻松实现。然后，您可以使用
    `map_from_arrays()`。请参阅列表 6.25 和 6.26 以了解构建块。
- en: This section covered the transformation from container columns to distinct records
    and back. With this, we can go from hierarchical to denormalized columns and back,
    without relying on auxiliary tables. In the final section of the chapter, I explain
    how to create your own structs by creating the last missing piece of our hierarchical
    data model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了从容器列到独立记录以及反向转换的过程。有了这个，我们可以从层次结构到非规范化列，然后再返回，而不依赖于辅助表。在章节的最后部分，我解释了如何通过创建我们层次数据模型的最后一块缺失部分来创建自己的结构体。
- en: '6.5.2 Building your own hierarchies: Struct as a function'
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 构建自己的层次结构：结构体作为函数
- en: This section concludes the chapter by showing how you can create structs within
    a data frame. With this last tool in your toolbox, the structure of a data frame
    will have no secrets for you.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过展示如何在数据框中创建结构体来结束本章。有了这个工具箱中的最后一个工具，数据框的结构将对你来说不再有任何秘密。
- en: To create a struct, we use the `struct()` function from the `pyspark.sql.functions`
    module. This function takes a number of columns as parameters (just like `select()`)
    and returns a struct column containing the columns passed as parameters as fields.
    Easy as pie!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建结构体，我们使用来自 `pyspark.sql.functions` 模块的 `struct()` 函数。这个函数接受多个列作为参数（就像 `select()`
    一样）并返回一个包含作为参数传递的列的字段的结构体列。简单得就像做饼一样！
- en: In the next listing, I create a new struct `info` containing a few columns from
    the `shows` data frame.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我创建了一个新的结构体 `info`，其中包含 `shows` 数据框中的几个列。
- en: Listing 6.27 Creating a `struct` column using the `struct` function
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.27 使用 `struct` 函数创建 `struct` 列
- en: '[PRE32]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ The struct function can take one or more column objects (or column names).
    I passed a literal column to indicate that I’ve watched the show.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 结构体函数可以接受一个或多个列对象（或列名）。我传递了一个字面量列来表示我已经观看了该节目。
- en: ❷ The info column is a struct and contains the three fields we specified.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `info` 列是一个结构体，包含我们指定的三个字段。
- en: ❸ The info column is a struct and contains the three fields we specified.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ `info` 列是一个结构体，包含我们指定的三个字段。
- en: Tip Just like with a top-level data frame, you can unpack (or select) all the
    columns from a struct using the star implicit column identifier, `column.*` .
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：就像顶层数据框一样，您可以使用星号隐式列标识符 `column.*` 来解包（或选择）结构体中的所有列。
- en: 'This chapter introduced the power and flexibility of the data frame using a
    data model that is starkly incompatible with most two-dimensional data representations:
    the hierarchical document data model. We ingested, processed, navigated, and molded
    a JSON document with the same data frame and set of functions that we used for
    textual and tabular data. This expands the power of the data frame beyond mere
    rows and columns and provides an alternative to the relational model, where we
    duplicate data to represent when a record has multiple values for a field.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过使用与大多数二维数据表示形式截然不同的数据模型——层次文档数据模型——介绍了数据框的强大功能和灵活性。我们使用与文本和表格数据相同的同一个数据框和函数集来摄取、处理、导航和塑造
    JSON 文档。这扩展了数据框的功能，使其超越了单纯的行和列，并为关系模型提供了一种替代方案，在关系模型中，我们通过复制数据来表示记录的某个字段具有多个值的情况。
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: PySpark has a specialized JSON `DataFrameReader` for ingesting JSON documents
    within a data frame. The default parameters will read a well-formed JSONLines
    document, while setting `multiLine=True` will read a series of JSON documents,
    each in their own files.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 为在数据框中摄取 JSON 文档提供了一个专门的 `DataFrameReader`。默认参数将读取格式良好的 JSONLines 文档，而将
    `multiLine=True` 设置为读取一系列 JSON 文档，每个文档位于自己的文件中。
- en: JSON data can be thought of as a Python dictionary. Nested (or hierarchical)
    elements are allowed through arrays (Python lists) and objects (Python dictionaries).
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将 JSON 数据视为 Python 字典。通过数组（Python 列表）和对象（Python 字典）允许嵌套（或层次）元素。
- en: In PySpark, hierarchical data models are represented through complex column
    types. The array represents lists of elements of the same type, the map represents
    multiple keys and values (akin to a Python dictionary), and the struct represents
    an object in the JSON sense.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PySpark 中，层次数据模型通过复杂列类型来表示。数组表示相同类型的元素列表，映射表示多个键和值（类似于 Python 字典），而结构体表示 JSON
    中的对象。
- en: PySpark provides a programatic API to build data frame schemas on top of a JSON
    representation. Having an explicit schema reduces the risk of having data in an
    incompatible type, leading to further analysis errors in the data-manipulation
    stage.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 提供了一个程序性 API，用于在 JSON 表示形式之上构建数据框模式。具有显式模式可以降低数据在数据操纵阶段出现不兼容类型的风险，从而导致进一步的分析错误。
- en: Complex types can be created and broken down via the data frame API with operations
    such as explosion, collection, and unpacking.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过数据帧 API 中的操作（如爆炸、集合和解包）创建和分解复杂类型。
- en: Additional exercises
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外练习
- en: Exercise 6.4
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6.4
- en: Why is it a bad idea to use the period or the square bracket in a column name,
    given that you also use it to reach hierarchical entities within a data frame?
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到你也在数据帧中用它来访问层次实体，为什么在列名中使用点或方括号是一个坏主意？
- en: Exercise 6.5
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6.5
- en: Although much less common, you can create a data frame from a dictionary. Since
    dictionaries are so close to JSON documents, build the schema for ingesting the
    following dictionary. (Both JSON or PySpark schemas are valid here.)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不太常见，但你也可以从字典创建一个数据帧。由于字典与 JSON 文档非常接近，为以下字典构建摄入模式。（这里 JSON 或 PySpark 模式都是有效的。）
- en: '[PRE33]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Exercise 6.6
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6.6
- en: Using `three_shows`, compute the time between the first and last episodes for
    each show. Which show had the longest tenure?
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `three_shows` 计算每个剧集的第一集和最后一集之间的时间。哪个剧集的任期最长？
- en: Exercise 6.7
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6.7
- en: Take the `shows` data frame and extract the air date and name of each episode
    in two array columns.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `shows` 数据帧中提取每个剧集的播出日期和名称，并将它们放入两个数组列中。
- en: Exercise 6.8
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6.8
- en: 'Given the following data frame, create a new data frame that contains a single
    map from `one` to `square`:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 给定以下数据帧，创建一个新的数据帧，其中包含一个从 `one` 到 `square` 的单个映射：
- en: '[PRE34]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '* * *'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ According to “The JavaScript Object Notation (JSON) Data Interchange Format”
    ([https://datatracker.ietf.org/doc/html/rfc8259](https://datatracker.ietf.org/doc/html/rfc8259))
    you can also have a valid JSON text with only a value (e.g., a number, a string,
    a Boolean, or `null`). For our purposes, those JSON texts are not useful as you
    can simply parse the value directly.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 根据“JavaScript 对象表示法（JSON）数据交换格式”（[https://datatracker.ietf.org/doc/html/rfc8259](https://datatracker.ietf.org/doc/html/rfc8259)）你还可以有一个只包含值（例如，数字、字符串、布尔值或
    `null`）的有效 JSON 文本。对于我们来说，这些 JSON 文本没有用，因为你可以直接解析值。
