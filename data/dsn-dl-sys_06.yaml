- en: 6 Model serving design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 模型服务设计
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining model serving
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型服务
- en: Common model serving challenges and approaches
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的模型服务挑战和解决方案
- en: Designing model serving systems for different user scenarios
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为不同的用户场景设计模型服务系统
- en: '*Model serving* is the process of executing a model with user input data. Among
    all the activities in a deep learning system, model serving is the closest to
    the end customers. After all the hard work of dataset preparation, training algorithm
    development, hyperparameter tuning, and testing results in models is completed,
    these models are presented to customers by model serving services.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型服务*是指使用用户输入数据执行模型的过程。在深度学习系统的所有活动中，模型服务是与最终客户最接近的。在完成数据集准备、训练算法开发、超参数调整和测试结果后，这些模型通过模型服务服务呈现给客户。'
- en: Take speech translation as an example. After training a sequence-to-sequence
    model for voice translation, the team is ready to present it to the world. For
    people to use this model remotely, the model is usually hosted in a web service
    and exposed by a web API. Then we (the customers) can send our voice audio file
    over the web API and get back a translated voice audio file. All the model loading
    and execution happens at the web service backend. Everything included in this
    user workflow—service, model files, and model execution—is called *model serving*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以语音翻译为例。在训练了一个用于语音翻译的序列到序列模型之后，团队准备将其呈现给世界。为了让人们能够远程使用这个模型，模型通常托管在云服务中并通过Web
    API进行暴露。然后我们（客户）可以通过Web API发送我们的语音音频文件，并获取回翻译后的语音音频文件。用户工作流程中包含的所有内容——服务、模型文件和模型执行——统称为*模型服务*。
- en: Building model serving applications is another special deep learning domain
    for which software engineers are particularly well suited. Model serving uses
    request latency, scalability, availability, and operability—all areas that engineers
    know inside and out. With some introduction to the concepts of deep learning model
    serving, developers who have some experience with distributed computing can play
    a significant role in building the model serving element.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型服务应用程序是软件工程师特别擅长的另一个特殊深度学习领域。模型服务使用请求延迟、可扩展性、可用性和可操作性——这些都是工程师了如指掌的领域。在了解了深度学习模型服务的一些概念之后，有分布式计算经验的开发者可以在构建模型服务元素中发挥重要作用。
- en: Serving models in production can be challenging because models are trained by
    various frameworks and algorithms, so the methods and libraries to execute the
    model vary. Also, the terminology used in the model serving field is confusing,
    with too many terms, like *model prediction* and *model inference*, that sound
    different but mean the same thing in the serving context. Furthermore, there are
    many model serving options from which to choose. On the one hand, we have black-box
    solutions like TensorFlow Serving, TorchServe, and NVIDIA Triton Inference Server.
    On the other, we have customized approaches like building your own predictor service
    or embedding models directly into your applications. These approaches all seem
    very similar and capable, so it is hard to select one over another. Therefore,
    if you are new to this domain, you can quickly become lost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中提供模型可能会具有挑战性，因为模型是由各种框架和算法训练的，所以执行模型的方法和库各不相同。此外，模型服务领域中使用的术语令人困惑，有太多术语，如*模型预测*和*模型推理*，在服务上下文中听起来不同但意思相同。此外，还有许多模型服务选项可供选择。一方面，我们有像TensorFlow
    Serving、TorchServe和NVIDIA Triton Inference Server这样的黑盒解决方案。另一方面，我们有定制方法，如构建自己的预测服务或将模型直接嵌入到您的应用程序中。这些方法看起来都非常相似且功能强大，因此很难选择一个而舍弃另一个。因此，如果您对这个领域是新手，您可能会很快感到迷茫。
- en: 'Our goal here is to help you find your way. We hope to empower you to design
    and build the model serving solution that best fits your situation. To achieve
    this goal, we have lots of content to cover, from the conceptual understanding
    of model serving and service-design considerations to concrete examples and model
    deployment workflow. To avoid exhausting you with a super-long chapter, we divide
    this content into two chapters: chapter 6 focuses on concepts, definitions, and
    design, and chapter 7 puts those concepts into practice, including building a
    sample prediction service and addressing open source tools as well as deploying
    and monitoring model production.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是帮助你找到自己的方向。我们希望赋予你设计和构建最适合你情况的模型服务解决方案的能力。为了实现这一目标，我们有很多内容需要涵盖，从模型服务的概念理解和服务设计考虑因素，到具体示例和模型部署工作流程。为了避免让你感到疲惫，我们将内容分为两章：第6章侧重于概念、定义和设计，第7章将这些概念付诸实践，包括构建示例预测服务、处理开源工具以及部署和监控模型生产。
- en: In this chapter, we start by clarifying the terminology and providing our own
    definitions of the elements used in model serving. We also describe the main challenges
    facing us in the model serving field. Then we will move to the design aspect,
    explaining the three common strategies of model serving and designing a model
    serving system that fits different use cases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先澄清术语，并为我们自己在模型服务中使用的元素提供定义。我们还描述了我们在模型服务领域面临的主要挑战。然后我们将转向设计方面，解释模型服务的三种常见策略，并设计适合不同用例的模型服务系统。
- en: By reading this chapter, you will not only gain a solid understanding of how
    model serving works, but you will also know the common design patterns that can
    address most of the model serving use cases. With the concepts and terminology
    clear in your mind, you should be comfortable joining any model serving–related
    discussion or reading articles and papers on the topic. And, of course, this chapter
    builds the foundation so you can follow the practical work addressed in the next
    chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读本章，你不仅将获得对模型服务如何工作的扎实理解，还将了解可以解决大多数模型服务用例的常见设计模式。在你心中明确概念和术语后，你应该能够舒适地参与任何与模型服务相关的讨论或阅读有关该主题的文章和论文。当然，本章还为你奠定了基础，以便你能够跟随下一章中提到的实际工作。
- en: 6.1 Explaining model serving
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 解释模型服务
- en: In the engineering of model serving, the terminology is a major problem. For
    example, *model*, *model architecture*, *inference graph*, *prediction*, and *inference*
    are all terms people use without clearly defining them, so they can have the same
    meaning or refer to different concepts depending on the context (model serving
    or model training). When we work with data scientists to build model serving solutions,
    the confusion around model serving terms causes a lot of miscommunication. In
    this section, we will explain the core concepts of model serving and interpret
    commonly used terminologies from an engineering perspective to help you avoid
    falling into the terminology trap.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型服务的工程实践中，术语是一个主要问题。例如，“模型”、“模型架构”、“推理图”、“预测”和“推理”都是人们在使用时没有明确定义的术语，因此它们可以具有相同的意义或根据上下文（模型服务或模型训练）指代不同的概念。当我们与数据科学家合作构建模型服务解决方案时，围绕模型服务术语的混淆会导致大量误解。在本节中，我们将解释模型服务的核心概念，并从工程角度解释常用术语，以帮助你避免陷入术语陷阱。
- en: 6.1.1 What is a machine learning model?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 什么是机器学习模型？
- en: There are multiple definitions of machine learning models in academia, from
    distilled representations of learnings of datasets to mathematical representations
    for recognizing certain patterns or making decisions based on previously unseen
    information. Nevertheless, as model serving developers, we can understand a model
    simply as a collection of files that are produced during training.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界对机器学习模型有多种定义，从数据集学习的蒸馏表示到识别某些模式或基于先前未见信息做出决策的数学表示。尽管如此，作为模型服务开发者，我们可以简单地将模型理解为在训练过程中产生的文件集合。
- en: The idea of a model is simple, but many people misunderstand that models are
    just static files. Although models are saved as files, they aren’t static; they’re
    essentially executable programs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的概念很简单，但许多人误解了模型只是静态文件。尽管模型以文件的形式保存，但它们并不是静态的；它们本质上是可以执行的程序。
- en: Let’s take apart that statement and determine what it means. A model consists
    of a machine learning algorithm, model data, and a model executor. A *model executor*
    is a wrapper code of the machine learning algorithm; it takes user input and runs
    the algorithm to compute and return prediction results. A *machine learning algorithm*
    refers to the algorithm used in model training, sometimes also called *model architecture*.
    Using speech translation as an example again, if the translation model is trained
    by a sequence-to-sequence network as its training algorithm, the machine learning
    algorithm in the model is the same sequence-to-sequence network. *Model data*
    is the data required to run the machine learning algorithm, such as the neural
    network’s learned parameters (weights and biases), embeddings, and label classes.
    Figure 6.1 illustrates a generic model structure.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个陈述，确定其含义。模型由机器学习算法、模型数据和模型执行器组成。*模型执行器*是机器学习算法的包装代码；它接受用户输入，运行算法以计算并返回预测结果。*机器学习算法*指的是模型训练中使用的算法，有时也称为*模型架构*。以语音翻译为例，如果翻译模型是由序列到序列网络作为其训练算法训练的，那么模型中的机器学习算法就是相同的序列到序列网络。*模型数据*是运行机器学习算法所需的数据，例如神经网络的学到的参数（权重和偏差）、嵌入和标签类别。图6.1说明了通用模型结构。
- en: '![](../Images/06-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-01.png)'
- en: Figure 6.1 A model is composed of a machine learning algorithm, model executor,
    and model data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 模型由机器学习算法、模型执行器和模型数据组成。
- en: Note We often refer to machine learning algorithms as *model algorithms* in
    this chapter for simplicity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了简便起见，在本章中我们通常将机器学习算法称为“模型算法”。
- en: The most important takeaway in this section is that the output of a model training
    execution—or simply, a model—isn’t just a set of static data. In contrast, deep
    learning models are executable programs that include a machine learning algorithm
    and its dependent data, so the models can make predictions based on input data
    at run time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节最重要的启示是，模型训练执行的输出——或者简单地说，模型——不仅仅是一组静态数据。相反，深度学习模型是可执行的程序，包括机器学习算法及其依赖的数据，因此模型可以在运行时根据输入数据进行预测。
- en: Note Models are not only weights and biases. Sometimes data scientists save
    a neural network’s trained parameters—weights and biases—to a file and name it
    “model file.” This confuses people into thinking a model is just a data file that
    contains only weights and biases. Weights and biases are the model *data*, but
    we also need the algorithm and the wrapper code to run the prediction.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：模型不仅仅是权重和偏差。有时数据科学家会将神经网络的训练参数——权重和偏差——保存到文件中，并将其命名为“模型文件”。这会让人们误以为模型只是一个包含只有权重和偏差的数据文件。权重和偏差是模型的数据，但我们还需要算法和包装代码来运行预测。
- en: 6.1.2 Model prediction and inference
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 模型预测和推理
- en: Academics may consider model inference and prediction to be two separate concepts.
    A model inference can refer to learning about how data is generated and understanding
    its causes and effects, whereas a model prediction might refer to predicting future
    events.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界可能将模型推理和预测视为两个不同的概念。模型推理可以指了解数据的生成方式，理解其因果关系，而模型预测可能指预测未来事件。
- en: A sample model prediction scenario might include using sales records to train
    a model to predict which individuals are likely to respond to the next marketing
    campaign. And a sample model inference scenario would include using sales records
    to train a model to understand the sales effect from the product price and customer
    income. The predictive accuracy on previously unseen data for model inference
    is not very important because the main focus is on learning the data generation
    process. Model training is designed to fit the full dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的模型预测场景可能包括使用销售记录来训练模型，预测哪些个人可能对下一次营销活动做出响应。而一个典型的模型推理场景可能包括使用销售记录来训练模型，理解产品价格和客户收入对销售的影响。对于模型推理在之前未见过的数据上的预测准确性并不很重要，因为主要关注的是学习数据生成过程。模型训练旨在拟合整个数据集。
- en: 'From an engineering perspective, model prediction and model inference mean
    the same. Although models can be built and used for different purposes, both model
    prediction and model inference in the context of model serving refer to the same
    action: executing the model with given data points to obtain a set of output values.
    Figure 6.2 illustrates the model serving workflow for the prediction model and
    the inference model; as you can see, there is no difference between them.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从工程角度来看，模型预测和模型推理意味着相同。尽管模型可以用于不同的目的，但在模型服务的背景下，模型预测和模型推理都指同一动作：使用给定的数据点执行模型以获得一组输出值。图6.2说明了预测模型和推理模型的模型服务工作流程；正如你所见，它们之间没有区别。
- en: '![](../Images/06-02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2](../Images/06-02.png)'
- en: Figure 6.2 Model prediction and model inference are the same in model serving
    engineering.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 模型预测和模型推理在模型服务工程中是相同的。
- en: To simplify the text in the illustrations of this chapter, starting from figure
    6.2, we use the word *model* to represent model data, model executor, and machine
    learning (model) algorithm. This is not only to keep the text short but also to
    emphasize that the machine learning model is an executable program.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化本章插图中的文本，从图6.2开始，我们使用单词“模型”来表示模型数据、模型执行器和机器学习（模型）算法。这不仅是为了使文本简短，而且是为了强调机器学习模型是一个可执行程序。
- en: 6.1.3 What is model serving?
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 什么是模型服务？
- en: '*Model* *serving* simply means executing a model with input data to make predictions,
    which includes fetching the expected model, setting up the model’s execution environment,
    executing the model to make a prediction with given data points, and returning
    the prediction result. The most used method for model serving is to host models
    in a web service and expose the model’s predict function through a web API.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: “模型服务”简单地说就是使用输入数据执行模型以进行预测，这包括检索预期的模型、设置模型的执行环境、使用给定的数据点执行模型进行预测，并返回预测结果。最常用的模型服务方法是托管模型在Web服务中，并通过Web
    API公开模型的预测函数。
- en: Suppose we build an object detection model to detect sharks in seacoast images;
    we can build a web service to host this model and expose a shark detection web
    API. This web API can then be used by beach hotels anywhere in the world to detect
    sharks with their own coast images. Conventionally, we call the model serving
    web service the prediction service.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们构建一个目标检测模型来检测海岸图像中的鲨鱼；我们可以构建一个网络服务来托管这个模型并公开一个鲨鱼检测网络API。这个网络API然后可以被世界各地的海滩酒店使用，以他们自己的海岸图像检测鲨鱼。传统上，我们称模型服务的网络服务为预测服务。
- en: 'A typical model prediction workflow in a prediction service has four steps:
    receiving a user request; loading the model from an artifact store to memory or
    GPU; executing the model’s algorithm; and, finally, returning the prediction results.
    Figure 6.3 shows this workflow.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测服务中，一个典型的模型预测工作流程有四个步骤：接收用户请求；从工件存储中加载模型到内存或GPU；执行模型的算法；最后，返回预测结果。图6.3展示了这个工作流程。
- en: '![](../Images/06-03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3](../Images/06-03.png)'
- en: Figure 6.3 A typical model prediction workflow in a prediction service
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 预测服务中的典型模型预测工作流程
- en: 'Besides the four-step prediction workflow, figure 6.3 also mentions three main
    components of the model serving: the prediction service (A), the model artifactory
    store (B), and the prediction web API (C). The model artifactory store (component
    B) holds all the models produced by the model training. The web API (component
    C) receives prediction requests. The prediction service (component A) responds
    to the prediction request, loads the model from the artifactory store, runs the
    model, and returns the prediction result.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了四个步骤的预测工作流程外，图6.3还提到了模型服务的三个主要组件：预测服务（A）、模型工件存储（B）和预测网络API（C）。模型工件存储（组件B）保存了模型训练产生的所有模型。网络API（组件C）接收预测请求。预测服务（组件A）响应预测请求，从工件存储中加载模型，运行模型，并返回预测结果。
- en: Although the four steps of the prediction workflow are generally applicable
    to all kinds of models, the actual implementation of the steps depends on the
    business needs, model training algorithm, and model training framework. We will
    discuss design options for prediction services in section 6.3, and we will present
    two sample prediction services in chapter 7.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然预测工作流程的四个步骤通常适用于所有类型的模型，但步骤的实际实现取决于业务需求、模型训练算法和模型训练框架。我们将在第6.3节中讨论预测服务的选项，并在第7章中展示两个示例预测服务。
- en: Model serving runs machine learning algorithms in a special mode
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务以特殊模式运行机器学习算法
- en: 'Model training and model serving execute the same machine learning algorithm
    but in two different models: learning mode and evaluation mode.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练和模型服务执行相同的机器学习算法，但在两个不同的模型中进行：学习模式和评估模式。
- en: In the learning mode, we run the algorithm in an *open loop*, meaning in each
    training iteration, we first run the neural network (algorithm) with an input
    data sample to calculate prediction results. Based on the difference between the
    prediction results and the expected results, the network’s parameters (weights
    and bias) are updated to fit the dataset closer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习模式下，我们以*开环*方式运行算法，这意味着在每次训练迭代中，我们首先使用输入数据样本运行神经网络（算法）以计算预测结果。根据预测结果与预期结果之间的差异，网络的参数（权重和偏差）被更新以更好地适应数据集。
- en: In the evaluation model, the neural network (algorithm) is run in a closed loop,
    which means that the network’s parameters will not be updated. The neural network
    is run solely to obtain the prediction results. So from a code implementation
    perspective, model serving is essentially running the machine learning algorithm
    (neural network) in the evaluation mode.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模式下，神经网络（算法）以*闭环*方式运行，这意味着网络的参数将不会更新。神经网络的运行仅为了获得预测结果。因此，从代码实现的角度来看，模型服务本质上是在评估模式下运行机器学习算法（神经网络）。
- en: 6.1.4 Model serving challenges
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 模型服务挑战
- en: Building a web service to serve models cost-effectively is a lot more complicated
    than running models locally on our laptops. Following are the six common challenges
    for serving models in web services.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 建立一个高效服务的Web服务来提供模型比在我们笔记本电脑上本地运行模型要复杂得多。以下是Web服务中提供模型时常见的六个挑战。
- en: The model prediction API differs per model algorithm. Different deep learning
    algorithms (such as recurrent neural networks and convolutional neural networks
    [CNN]) require different input data formats, and their output format can also
    vary. When designing the web prediction API, it’s quite challenging to design
    a unified web API that meets the input data requirements for every model algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测API因模型算法而异。不同的深度学习算法（如循环神经网络和卷积神经网络 [CNN]）需要不同的输入数据格式，它们的输出格式也可能不同。在设计Web预测API时，设计一个满足每个模型算法输入数据要求的统一Web
    API是一项相当具有挑战性的任务。
- en: Model executing environments are different per training framework. Models can
    be trained in different frameworks, such as TensorFlow and PyTorch. And each training
    framework has its special setup and configuration to execute its models. The prediction
    service should encapsulate the model execution environment setup at its backend,
    so customers can focus on using the model prediction API, not the framework with
    which this model is trained.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型执行环境因训练框架而异。模型可以在不同的框架中训练，例如TensorFlow和PyTorch。每个训练框架都有其特殊的设置和配置来执行其模型。预测服务应在后端封装模型执行环境设置，这样客户就可以专注于使用模型预测API，而不是训练此模型的框架。
- en: There are too many model serving tools, libraries, and systems from which to
    choose. If we decide to use existing open source approaches to model serving,
    the immediate question becomes which approach we should choose. There are 20+
    different options, such as TorchServe, TensorFlow Serving, NVIDIA Triton Inference
    Server, Seldon Core, and KFServing. How do we know which approach works best for
    our situation?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可供选择的服务器工具、库和系统太多。如果我们决定使用现有的开源模型服务方法，那么立即的问题就变成了我们应该选择哪种方法。有20多种不同的选择，例如TorchServe、TensorFlow
    Serving、NVIDIA Triton Inference Server、Seldon Core和KFServing。我们如何知道哪种方法最适合我们的情况？
- en: There is no universal, most cost-effective model serving design; we need to
    tailor a model serving approach that fits our own use case. Unlike model training
    and hyperparameter tuning service, which both have a one-fits-all approach—prediction
    service design heavily depends on concrete user scenarios. For example, designing
    a prediction service that supports just one model, such as a flower recognition
    model, is a lot different than designing a prediction service that supports 10
    different types of models, such as PDF scanning, text intent classification, and
    image classification.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种通用的、最具成本效益的模型服务设计；我们需要量身定制一个适合我们自己的用例的模型服务方法。与模型训练和超参数调整服务一样，它们都有一种一刀切的方法——预测服务设计严重依赖于具体用户场景。例如，设计一个仅支持一种模型（如花卉识别模型）的预测服务，与设计一个支持10种不同类型模型（如PDF扫描、文本意图分类和图像分类）的预测服务有很大的不同。
- en: Reduce model prediction latency while maintaining resource saturation. From
    a cost-efficiency perspective, we want our compute resources to be fully saturated
    with model prediction workloads. In addition, we would like to provide our customers
    with a real-time model prediction experience, so we don’t want the prediction
    latency to drop because of the rigid infrastructure budget. To accomplish this,
    we need to reduce the time cost at every step of the prediction workflow innovatively,
    such as loading the model faster or preheating the model before serving.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在保持资源饱和的同时减少模型预测延迟。从成本效益的角度来看，我们希望我们的计算资源完全饱和于模型预测工作负载。此外，我们希望为客户提供实时模型预测体验，因此我们不希望由于刚性的基础设施预算而导致预测延迟下降。为了实现这一点，我们需要创新性地在每个预测工作流程的步骤中减少时间成本，例如加快模型加载或预先加热模型以供服务。
- en: Model deployment and post-deployment model monitoring are things we should consider
    on day one. Model deployment—progressing a model from training to production—is
    critical for successful model development. We want to advance the model to production
    quickly, and we want to have multiple versions of the model in production, so
    we can evaluate different training algorithms quickly and choose the best model.
    Post-deployment model monitoring can help detect model performance regression;
    it’s a crucial protection mechanism for models in fraud detection and loan approval,
    for instance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部署和部署后模型监控是我们应该在第一天考虑的事情。模型部署——将模型从训练状态推进到生产状态——对于成功的模型开发至关重要。我们希望快速将模型推进到生产状态，并且希望在生产中有多个模型版本，这样我们可以快速评估不同的训练算法并选择最佳模型。部署后模型监控可以帮助检测模型性能退化；例如，在欺诈检测和贷款审批中，它是一个至关重要的保护机制。
- en: The good news is that these six challenges are all engineering problems, so
    you will be able to handle them! We will discuss how to address them here and
    in the next chapter.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，这六个挑战都是工程问题，所以你将能够处理它们！我们将在本章和下一章中讨论如何解决这些问题。
- en: 6.1.5 Model serving terminology
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 模型服务术语
- en: As we proceed through the chapter, we’d like to refresh your memory of the model
    serving terms. Many terms have various definitions in academia but are interchangeable
    in practice when talking about model serving. The following definitions should
    help you and your colleagues avoid confusion when they are mentioned.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入本章，我们希望刷新你对模型服务术语的记忆。许多术语在学术界有不同的定义，但在讨论模型服务时可以互换使用。以下定义应该有助于你和你的同事在提及这些术语时避免混淆。
- en: '*Model serving, model scoring**, model inference*, and *model prediction* are
    interchangeable terminologies in the deep learning context. They all refer to
    executing a model with given data points. In this book, we will use *model serving*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习环境中，*模型服务*、*模型评分*、*模型推理*和*模型预测*是可互换的术语。它们都指的是使用给定的数据点执行模型。在这本书中，我们将使用*模型服务*。
- en: '*Prediction service**, scoring service**, inference service*, and *model serving
    service* are interchangeable; they refer to the web service that allows remote
    model execution. In this book, we use the prediction service.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测服务*、*评分服务*、*推理服务*和*模型服务服务*是可互换的；它们指的是允许远程模型执行的Web服务。在这本书中，我们使用预测服务。'
- en: '*Predict* and *inference* are interchangeable in the model serving context;
    they are the entry function related to running the model algorithm. In this book,
    we use *predict*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型服务环境中，*预测*和*推理*是可互换的；它们是与运行模型算法相关的入口函数。在这本书中，我们使用*预测*。
- en: '*Prediction request*, *scoring request**,* and *inference request* are interchangeable;
    they refer to the web API request that executes a model to make a prediction.
    In this book, we use *prediction request*.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测请求*、*评分请求*和*推理请求*可以互换使用；它们指的是执行模型进行预测的Web API请求。在这本书中，我们使用*预测请求*。'
- en: '*Machine learning algorithm*, *training algorithm*, and *model algorithm* are
    interchangeable, as we state in section 6.1.3; the algorithm that runs in model
    training and serving is the same machine learning algorithm (same neural network)
    but in a different execution mode.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习算法*、*训练算法*和*模型算法*可以互换使用，正如我们在6.1.3节中所述；在模型训练和提供中运行的算法是相同的机器学习算法（相同的神经网络），但执行模式不同。'
- en: '*Model deployment* and *model release* are interchangeable; they indicate the
    process of deploying/copying a trained model (files) to the production environment
    where the business is running, so the customer can benefit from this new model.
    Typically, this refers to loading the model files into the prediction service.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型部署*和*模型发布*可以互换使用；它们表示将训练好的模型（文件）部署/复制到业务运行的生产环境中的过程，以便客户可以从这个新模型中受益。通常，这指的是将模型文件加载到预测服务中。'
- en: 6.2 Common model serving strategies
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 常见的模型服务策略
- en: 'Before we review the concrete model serving use cases and prediction service
    designs in section 6.3, let’s first check out the three common model serving strategies:
    direct model embedding, model service, and model server. No matter what you need
    to do for your specific use cases, you can usually take one of the following three
    approaches to build your prediction service.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们回顾6.3节中具体的模型服务用例和预测服务设计之前，让我们首先了解一下三种常见的模型服务策略：直接模型嵌入、模型服务和模型服务器。无论你需要为你的特定用例做什么，你通常可以采取以下三种方法之一来构建你的预测服务。
- en: 6.2.1 Direct model embedding
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 直接模型嵌入
- en: Direct model embedding means loading the model and running model prediction
    inside the user application’s process. For example, a flower identity–check mobile
    app can load an image classification model directly in its local process and predict
    plant identity from the given photos. The entire model loading and serving happen
    within the model app locally (on the phone), without talking to other processes
    or remote servers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型嵌入意味着在用户应用程序的进程中加载模型并运行模型预测。例如，一个花卉识别移动应用程序可以直接在其本地进程中加载图像分类模型，并从提供的照片中预测植物身份。整个模型加载和提供都在模型应用程序的本地（在手机上）进行，无需与其他进程或远程服务器通信。
- en: Most user applications, like mobile apps, are written in strongly typed languages,
    such as Go, Java, and C#, but most deep learning modeling code is written in Python.
    It is therefore difficult to embed model code into application code, and even
    if you do, the process can take a while. To facilitate model prediction across
    non-Python processes, deep learning frameworks such as PyTorch and TensorFlow
    provide C++ libraries. Additionally, TensorFlow offers Java ([https://github.com/tensorflow/java](https://github.com/tensorflow/java))
    and JavaScript ([https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs))
    libraries for loading and executing TensorFlow models directly from Java or JavaScript
    applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数用户应用程序，如移动应用程序，是用强类型语言编写的，例如Go、Java和C#，但大多数深度学习建模代码是用Python编写的。因此，将模型代码嵌入到应用程序代码中很困难，即使你做到了，这个过程也可能需要一段时间。为了方便在非Python进程中进行模型预测，深度学习框架如PyTorch和TensorFlow提供了C++库。此外，TensorFlow还提供了Java
    ([https://github.com/tensorflow/java](https://github.com/tensorflow/java)) 和JavaScript
    ([https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs)) 库，可以直接从Java或JavaScript应用程序加载和执行TensorFlow模型。
- en: Another disadvantage of direct embedding is resource consumption. If the model
    runs on client devices, users without high-end devices may not have a good experience.
    Running big deep learning models requires a lot of computation, and this can cause
    slower apps.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 直接嵌入的另一个缺点是资源消耗。如果模型在客户端设备上运行，没有高端设备的用户可能不会有一个好的体验。运行大型深度学习模型需要大量的计算，这可能会导致应用程序运行缓慢。
- en: Lastly, direct embedding involves mixing model serving code with application
    business logic, which poses a challenge for backward compatibility. Therefore,
    because it is rarely used, we only describe it briefly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，直接嵌入涉及将模型服务代码与应用程序业务逻辑混合，这给向后兼容性带来了挑战。因此，因为它很少使用，所以我们只简要描述了它。
- en: 6.2.2 Model service
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 模型服务
- en: '*Model* *service* refers to running model serving on the server side. For each
    model, each version of a model, or each type of model, we build a dedicated web
    service for it. This web service exposes the model prediction API over HTTP or
    gRPC interfaces.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型* *服务* 指的是在服务器端运行模型服务。对于每个模型，每个模型的版本或每种类型的模型，我们为它构建一个专用的Web服务。这个Web服务通过HTTP或gRPC接口公开模型预测API。'
- en: The model service manages the full life cycle of model serving, including fetching
    the model file from the model artifact store, loading the model, executing the
    model algorithm for a customer request, and unloading the model to reclaim the
    server resources. Using the documents classification use case as an example, to
    automatically sort documents in images and PDF by their content, we can train
    a CNN model for OCR (optical character recognition) to extract text from document
    images or PDF. To serve this model in a model service approach, we build a web
    service exclusively for this CNN model, and the web API is only designed for this
    CNN model’s prediction function. Sometimes we build a dedicated web service for
    each major model version update.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务管理模型服务的整个生命周期，包括从模型工件存储中检索模型文件、加载模型、为客户请求执行模型算法以及卸载模型以回收服务器资源。以文档分类用例为例，为了自动按内容对图像和PDF中的文档进行排序，我们可以训练一个用于OCR（光学字符识别）的CNN模型从文档图像或PDF中提取文本。在模型服务方法中提供此模型时，我们为这个CNN模型构建一个专用的Web服务，并且Web
    API仅设计用于这个CNN模型的预测功能。有时，我们为每个主要模型版本更新构建一个专用的Web服务。
- en: The common pattern of model service is to build the model execution logic into
    a Docker image and use gRPC or HTTP interface to expose the model’s predict function.
    For service setup, we can host multiple service instances and employ a load balancer
    to distribute customers’ prediction requests to these instances.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务的常见模式是将模型执行逻辑构建到Docker镜像中，并使用gRPC或HTTP接口来公开模型的预测功能。对于服务设置，我们可以托管多个服务实例，并使用负载均衡器将这些客户的预测请求分发到这些实例。
- en: The biggest advantage of the model service approach is simplicity. We can easily
    convert a model’s training container to a model serving container because, essentially,
    a model prediction execution entails running the trained model neural network.
    The model training code can turn into a prediction web service quickly by adding
    an HTTP or gRPC interface and setting the neural network to evaluation mode. We
    will see a model service’s design and use case in sections 6.3.1 and 6.3.2 and
    a concrete code example in chapter 7.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务方法的最大优势是简单性。我们可以轻松地将模型的训练容器转换为模型服务容器，因为本质上，模型预测执行涉及运行训练好的模型神经网络。通过添加HTTP或gRPC接口并将神经网络设置为评估模式，模型训练代码可以快速转变为预测Web服务。我们将在第6.3.1节和第6.3.2节中看到模型服务的架构和用例，并在第7章中看到一个具体的代码示例。
- en: Because model service is specific to the model algorithm, we need to build separate
    services for different model types or versions. If you have several different
    models to serve, this one service-per-model approach can spawn many services,
    and the maintenance work for these services—such as patching, deploying, and monitoring—can
    be exhausting. If you are facing this situation, the model server approach is
    the right choice.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型服务是针对特定模型算法的，因此我们需要为不同的模型类型或版本构建单独的服务。如果您有多个不同的模型需要提供，这种每个模型一个服务的方法可能会产生许多服务，对这些服务的维护工作（如修补、部署和监控）可能会非常繁琐。如果您面临这种情况，模型服务器方法就是正确的选择。
- en: 6.2.3 Model server
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 模型服务器
- en: The model server approach is designed to handle multiple types of models in
    a black-box manner. Regardless of the model algorithm and model version, the model
    server can operate these models with a unified web prediction API. The model server
    is the next stage; we no longer need to make code changes or deploy new services
    with a new type of model or new version of the model. This saves a lot of duplicate
    development and maintenance work from the model service approach.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务器方法旨在以黑盒方式处理多种类型的模型。无论模型算法和模型版本如何，模型服务器都可以通过统一的Web预测API来操作这些模型。模型服务器是下一阶段；我们不再需要为新类型的模型或模型的新版本进行代码更改或部署新服务。这节省了从模型服务方法中大量重复的开发和维护工作。
- en: Yet, the model server approach is a lot more complicated to implement and manage
    than the model service approach. Handling model serving for various types of models
    in one service and one unified API is complicated. The model algorithms and model
    data are different; their predict functions are also different. For example, an
    image classification model can be trained with a CNN network, whereas a text classification
    model can be trained with a long short-term memory (LSTM) network. Their input
    data is different (text vs. image), and their algorithms are different (CNN vs.
    LSTM). Their model data also varies; text classification models require embedding
    files to encode input text whereas CNN models don’t require embedding files. These
    differences present many challenges to finding a low-maintenance, low-cost, and
    unified serving approach.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型服务器方法在实现和管理上比模型服务方法复杂得多。在一个服务和统一的API中处理各种类型的模型服务是复杂的。模型算法和模型数据不同；它们的预测函数也不同。例如，图像分类模型可以用CNN网络训练，而文本分类模型可以用长短期记忆（LSTM）网络训练。它们的输入数据不同（文本与图像），它们的算法也不同（CNN与LSTM）。它们的模型数据也有所不同；文本分类模型需要嵌入文件来编码输入文本，而CNN模型则不需要嵌入文件。这些差异为寻找低维护、低成本和统一的服务方法带来了许多挑战。
- en: Although building a model server approach is difficult, it’s definitely possible.
    Many open source model serving libraries and services—such as TensorFlow Serving,
    TorchServe, and NVIDIA Triton Inference Server—offer model server solutions. We
    simply need to build customized integration logic to incorporate these tools into
    our existing systems to solve business needs—for example, integrating TorchServe
    into our model storage, monitoring, and alerting system.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然构建模型服务器方法很困难，但绝对是可能的。许多开源模型服务库和服务——例如TensorFlow Serving、TorchServe和NVIDIA
    Triton Inference Server——提供了模型服务器解决方案。我们只需要构建定制的集成逻辑，将这些工具整合到我们现有的系统中，以解决业务需求——例如，将TorchServe整合到我们的模型存储、监控和警报系统中。
- en: From a model deployment perspective, the model server is a black-box approach.
    As long as we save the model file following the model server standards, the model
    prediction should function when we upload the model-to-model server through its
    management API. The complexity of model serving implementation and maintenance
    can be greatly reduced. We will see a model server design and use case in section
    6.3.3 and a code example with TorchServe in chapter 7.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型部署的角度来看，模型服务器是一种黑盒方法。只要我们按照模型服务器标准保存模型文件，当我们通过其管理API上传模型到模型服务器时，模型预测应该能够正常工作。模型服务的实现和维护的复杂性可以大大降低。我们将在第6.3.3节中看到一个模型服务器的设计和用例，以及第7章中用TorchServe的代码示例。
- en: Note Should we always consider a model server approach? Not always. If we don’t
    think of service development cost and maintenance cost, the model server approach
    is the most powerful because it’s designed to cover all types of models. But if
    we care about model serving cost efficiency—and we should!—then the ideal approach
    depends on the use cases. In the next section, we will discuss the common model
    serving use cases and the applied design.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们是否总是应该考虑模型服务器方法？不一定。如果我们不考虑服务开发成本和维护成本，模型服务器方法是最强大的，因为它旨在涵盖所有类型的模型。但如果我们关心模型服务的成本效率——我们应该关心！——那么理想的方法取决于用例。在下一节中，我们将讨论常见的模型服务用例和适用的设计。
- en: 6.3 Designing a prediction service
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 设计预测服务
- en: A common mistake in software system design is aiming to build an omnipotent
    system without considering the concrete user scenario. Overdesign will redirect
    our focus from the immediate customer needs to the features that might be useful
    in the future. As a result, the system either takes an unnecessarily long time
    to build or is difficult to use. This is especially true for model serving.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 软件系统设计中常见的错误是试图构建一个全能的系统，而没有考虑具体的用户场景。过度设计会将我们的注意力从立即的客户需求转移到可能在未来有用的功能上。结果，系统要么构建时间过长，要么难以使用。这对于模型服务尤其如此。
- en: Deep learning is an expensive business, both in terms of human and computational
    resources. We should build only the necessities to move models into production
    as quickly as possible and minimize the operation costs. To do so, we need to
    begin with user scenarios.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一项昂贵的业务，无论是从人力还是计算资源来看。我们应该只构建必要的部分，以便尽可能快地将模型投入生产，并最小化运营成本。为此，我们需要从用户场景开始。
- en: In this section, we will present three typical model serving scenarios, from
    simple to complex. For each use case, we explain the scenario and illustrate a
    suitable high-level design. By reading the following three subsections sequentially,
    you will see how the prediction service’s design evolves when use cases become
    more and more complicated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍三种典型的模型服务场景，从简单到复杂。对于每个用例，我们将解释场景并展示一个合适的高级设计。通过依次阅读以下三个小节，您将看到当用例变得越来越复杂时，预测服务的设计是如何演变的。
- en: Note The goal of prediction service design is not to build a powerful system
    that works for various models but to build a system that suits the circumstances
    in a cost-efficient manner.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：预测服务设计的目的是不是构建一个适用于各种模型的强大系统，而是构建一个以成本效益的方式适应特定情况的服务系统。
- en: 6.3.1 Single model application
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 单模型应用
- en: Imagine building a mobile app that can swap people’s faces between two pictures.
    The consumer expects the app UI to upload photos, select sources and target pictures,
    and execute a deepfake model ([https://arxiv.org/abs/1909.11573](https://arxiv.org/abs/1909.11573))
    for swapping faces between the two selected images. For an application like this
    that only needs to work with one model, the serving approach can be either model
    service (6.2.2) or direct model embedding (6.2.1).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下构建一个可以在两张图片之间交换人脸的移动应用。消费者期望应用UI能够上传照片，选择源图片和目标图片，并执行深度伪造模型（[https://arxiv.org/abs/1909.11573](https://arxiv.org/abs/1909.11573)）以在选定的两张图片之间交换人脸。对于这种只需要与一个模型工作的应用，服务方法可以是模型服务（6.2.2）或直接模型嵌入（6.2.1）。
- en: Model service approach
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务方法
- en: 'From the discussion in section 6.2.2, the model service approach involves building
    a web service for each model. So we can build the face-swap model app with the
    following three components: a front UI app (component A) that runs on our phone;
    an application backend to handle user operation (component B); and a backend service,
    or *predictor* (component C), to host a deepfake model and expose a web API to
    execute the model for each face-swap request.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从第6.2.2节中的讨论来看，模型服务方法涉及为每个模型构建一个网络服务。因此，我们可以用以下三个组件来构建人脸交换模型应用：一个运行在手机上的前端UI应用（组件A）；一个处理用户操作的应用后端（组件B）；以及一个后端服务，或称为*预测器*（组件C），用于托管深度伪造模型并暴露一个Web
    API以执行每个人脸交换请求。
- en: When a user uploads a source image and a target image and clicks the face-swap
    button on the mobile app, the mobile backend application will receive the request
    and call the predictor’s web API for face-swapping. Then the predictor preprocesses
    the user request data (the images), executes the model algorithm, and postprocesses
    the model output (the images) to the application backend. Ultimately, the mobile
    app will display the source and target images with swapped faces. Figure 6.4 illustrates
    a general design that suits the face-swap use case.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户上传源图像和目标图像并在移动应用上点击人脸交换按钮时，移动后端应用将接收请求并调用预测器的Web API进行人脸交换。然后预测器预处理用户请求数据（图像），执行模型算法，并将模型输出（图像）后处理到应用后端。最终，移动应用将显示交换人脸的源图像和目标图像。图6.4展示了适用于人脸交换用例的一般设计。
- en: '![](../Images/06-04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-04.png)'
- en: Figure 6.4 A single model predictor design in a client/server setup
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 客户/服务器设置中的单模型预测器设计
- en: If we zoom into the predictor (component C), we see that the model serving logic
    works the same as the general model prediction workflow that we introduced in
    figure 6.3\. The predictor (model serving service) loads the model file from the
    model artifactory and runs the model to respond to the request received by the
    web interface.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们放大查看预测器（组件C），我们会看到模型服务逻辑与我们在图6.3中介绍的通用模型预测工作流程相同。预测器（模型服务服务）从模型工件库中加载模型文件并运行模型以响应通过Web界面接收到的请求。
- en: The design in figure 6.4 generally works for any application that has a web
    backend and only one model. The key component in this design is the predictor;
    it is a web service and often runs as a Docker container. We can implement this
    approach quickly because this predictor container can be easily converted from
    the model training container that builds the model. The two main work items that
    transform a training container to a predictor container are the web predict API
    and the evaluation mode in the training neural network. We will present a concrete
    predictor container example in section 7.1.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4中的设计通常适用于任何具有网络后端且只有一个模型的任何应用程序。这个设计中的关键组件是预测器；它是一个网络服务，通常作为一个Docker容器运行。我们可以快速实现这种方法，因为预测器容器可以很容易地从构建模型的模型训练容器转换而来。将训练容器转换为预测器容器的两个主要工作项是网络预测API和在训练神经网络中的评估模式。我们将在第7.1节中展示一个具体的预测器容器示例。
- en: Direct model embedding approach
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 直接模型嵌入方法
- en: Another design approach for building a single model application is combining
    the model execution code with the application’s user logic code. There is no server
    backend, so everything happens locally on the user’s computer or phone. Using
    the face swap app as an example, the deepfake model file is in the application’s
    deployment package, and when the application starts, the model is loaded into
    the application’s process space. Figure 6.5 illustrates this concept.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建单个模型应用程序的另一种设计方法是结合模型执行代码与应用程序的用户逻辑代码。没有服务器后端，所以所有事情都在用户的电脑或手机上本地发生。以人脸交换应用为例，深度伪造模型文件包含在应用程序的部署包中，当应用程序启动时，模型被加载到应用程序的进程空间中。图6.5说明了这个概念。
- en: '![](../Images/06-05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5](../Images/06-05.png)'
- en: Figure 6.5 In the direct model embedding design, the model is executed in the
    same process as the application logic.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 在直接模型嵌入设计中，模型与应用程序逻辑在同一个进程中执行。
- en: Model serving doesn’t have to run in a separate service. In figure 6.5, we see
    that the model serving code (the single model box) and the data transformation
    code can run with the user logic code in the same application. Nowadays, many
    deep learning frameworks provide libraries to run models in non-Python applications.
    For example, TensorFlow offers Java, C++, and JavaScript SDK to load and execute
    models. With SDK’s help, we can train and execute models directly in Java/C++/
    JavaScript applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务不一定要在单独的服务中运行。在图6.5中，我们看到模型服务代码（单个模型框）和数据转换代码可以与应用程序的用户逻辑代码在同一应用程序中运行。如今，许多深度学习框架提供库，可以在非Python应用程序中运行模型。例如，TensorFlow提供了Java、C++和JavaScript
    SDK，用于加载和执行模型。借助SDK的帮助，我们可以在Java/C++/JavaScript应用程序中直接训练和执行模型。
- en: Note Why should we consider direct model embedding? By using model embedding,
    we can directly integrate model serving logic with application logic and run them
    together in the same process space. This provides two advantages over the predictor
    service approach in figure 6.4\. First, it reduces one network hop; there is no
    web request to the predictor, and model execution happens locally. Second, it
    improves service debuggability because we can run the application as one piece
    locally.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 为什么我们应该考虑直接模型嵌入？通过使用模型嵌入，我们可以直接将模型服务逻辑与应用程序逻辑集成，并在同一个进程空间中一起运行。这比图6.4中的预测器服务方法提供了两个优势。首先，它减少了一个网络跳数；没有到预测器的网络请求，模型执行是本地发生的。其次，它提高了服务的可调试性，因为我们可以在本地作为一个整体运行应用程序。
- en: Why is the model service approach more popular?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么模型服务方法更受欢迎？
- en: 'Although the direct model embedding approach looks simple and saves one network
    hop, it is still not a popular choice for building model serving. Here are the
    four reasons:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管直接模型嵌入方法看起来简单且节省了一个网络跳数，但它仍然不是构建模型服务的流行选择。以下是四个原因：
- en: The model algorithm has to be reimplemented in a different language. A model’s
    algorithm and execution code is usually written in Python. If we choose a model
    service approach, implementing the model serving as a web service (predictor in
    figure 6.4), we can reuse most of the training code and build it quickly. But
    if we choose to embed model serving in a non-Python application, we must reimplement
    model loading, model execution, and data process logic in the application’s language
    (such as Java or C++). This work is nontrivial, and not many developers have the
    depth of knowledge to rewrite the training algorithms.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型算法必须用不同的语言重新实现。模型的算法和执行代码通常是用Python编写的。如果我们选择模型服务方法，将模型服务作为网络服务（如图6.4中的预测器）实现，我们可以重用大部分训练代码并快速构建。但如果我们选择将模型服务嵌入到非Python应用程序中，我们必须在应用程序的语言（如Java或C++）中重新实现模型加载、模型执行和数据处理逻辑。这项工作非同小可，并且不是很多开发者都有足够的知识深度来重写训练算法。
- en: The ownership boundary is blurred. When embedding a model into an application,
    the business logic code can mingle with the serving code. When the codebase becomes
    complicated, it’s difficult to draw a boundary between the serving code (owned
    by the data scientist) and other application code (owned by the developer). When
    data scientists and developers are from two different teams but work on the same
    code repo, the shipping velocity will drop significantly because the cross-team
    code review and deployment takes longer than usual.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有权边界变得模糊。当将模型嵌入到应用程序中时，业务逻辑代码可以与服务代码混合。当代码库变得复杂时，很难在服务代码（由数据科学家拥有）和其他应用程序代码（由开发者拥有）之间划清界限。当数据科学家和开发者来自不同的团队但共同工作在同一个代码库时，交付速度会显著下降，因为跨团队代码审查和部署的时间比平时长。
- en: Performance problems can occur on the client’s devices. Usually, apps are run
    on the customer’s mobiles, tablets, or lower-end laptops. On these devices, capturing
    features from raw user data and then preprocessing model input data and running
    model prediction can lead to performance problems such as CPU usage spikes, app
    slowdown, and high memory usage.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在客户端设备上可能会出现性能问题。通常，应用程序在客户的手机、平板电脑或低端笔记本电脑上运行。在这些设备上，从原始用户数据中提取特征，然后预处理模型输入数据并运行模型预测可能导致性能问题，如CPU使用率激增、应用程序减速和高内存使用。
- en: A memory leak can occur easily. For example, when executing a TensorFlow model
    in Java, the algorithm execution and input/output parameter objects are all created
    in the native space. These objects won’t be recycled by Java GC (Garbage Collection)
    automatically; we have to manually depose them. It’s very easy to overlook recycling
    the native resources claimed by the model, and because the native objects’ memory
    allocations are not tracked in Java heap, their memory usage is difficult to observe
    and measure. So the memory leak can happen and is hard to fix.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易发生内存泄漏。例如，当在Java中执行TensorFlow模型时，算法执行和输入/输出参数对象都是在原生空间中创建的。这些对象不会被Java GC（垃圾回收）自动回收；我们必须手动释放它们。很容易忽略回收模型所声称的原生资源，并且由于原生对象的内存分配不在Java堆中跟踪，它们的内存使用难以观察和测量。因此，内存泄漏可能发生，并且难以修复。
- en: Note To troubleshoot native memory leaks, Jemalloc ([https://github.com/jemalloc/jemalloc/wiki/Background](https://github.com/jemalloc/jemalloc/wiki/Background))
    is a very handy tool. You can check out my blog post “Fix Memory Issues in Your
    Java Apps” ([http://mng.bz/lJ8o](http://mng.bz/lJ8o)) for further details.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了排查原生内存泄漏，Jemalloc ([https://github.com/jemalloc/jemalloc/wiki/Background](https://github.com/jemalloc/jemalloc/wiki/Background))
    是一个非常实用的工具。您可以查看我的博客文章“Fix Memory Issues in Your Java Apps” ([http://mng.bz/lJ8o](http://mng.bz/lJ8o))
    以获取更多详细信息。
- en: For the previously listed reasons, we highly recommend you adopt the model service
    approach for single model application use cases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述原因，我们强烈建议您为单模型应用程序用例采用模型服务方法。
- en: 6.3.2 Multitenant application
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 多租户应用程序
- en: We will use a chatbot application as an example to explain multitenant use cases.
    First, let’s set the context. A *tenant* is a company or organization (such as
    a school or a retail store) that uses the chatbot application to communicate with
    its customers. The tenants use the same software/service—the chatbot application—but
    have separate accounts with their data segregated. A *chat user* is the customer
    of a tenant and uses the chatbot to do business with the tenant.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以聊天机器人应用程序为例来解释多租户用例。首先，让我们设定上下文。一个*租户*是一个公司或组织（例如学校或零售店），它使用聊天机器人应用程序与其客户进行沟通。租户使用相同的软件/服务——聊天机器人应用程序——但拥有各自独立的账户，数据是隔离的。一个*聊天用户*是租户的客户，并使用聊天机器人与租户进行业务往来。
- en: By design, the chatbot application relies on an intent classification model
    to identify the user’s intention from his conversation, and then the chatbot redirects
    the user request to the correct service department of the tenant. Currently, this
    chatbot is taking a single model application approach, meaning it’s using a single
    intent classification model for every user and tenant.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 按照设计，聊天机器人应用程序依赖于一个意图分类模型来从用户的对话中识别用户的意图，然后聊天机器人将用户请求重定向到租户的正确服务部门。目前，这个聊天机器人采用单一模型应用方法，这意味着它为每个用户和租户使用一个意图分类模型。
- en: Now, because of customer feedback from tenants on the low prediction accuracy
    of the single intent classification model, we decide to let tenants use our training
    algorithm to build their own model with their own dataset. This way, the model
    can fit better with each tenant’s business situation. For model serving, we will
    let tenants use their own model for intent classification prediction requests.
    When a chatbot user now speaks to the chatbot application, the application will
    find the tenant’s specific model to answer the user’s question. The chatbot is
    changed to a multitenant application.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于租户对单一意图分类模型预测精度低的反馈，我们决定让租户使用我们的训练算法使用他们自己的数据集构建自己的模型。这样，模型可以更好地适应每个租户的商业情况。对于模型服务，我们将让租户使用他们自己的模型进行意图分类预测请求。当聊天机器人用户现在与聊天机器人应用程序交谈时，应用程序将找到租户的特定模型来回答用户的问题。聊天机器人已变为多租户应用程序。
- en: In this chatbot multitenant use case, although the models belong to different
    tenants and are trained with different datasets, they are the same type of model.
    Because these models are trained with the same algorithm, their model’s algorithm
    and predict function are all the same. We can extend the model service design
    in figure 6.4 to support multitenancy by adding a model cache. By caching model
    graphs and their dependent data in memory, we can perform multitenancy model serving
    in one service. Figure 6.6 illustrates this concept.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个聊天机器人多租户用例中，尽管模型属于不同的租户并且使用不同的数据集进行训练，但它们是同一类型的模型。因为这些模型使用相同的算法进行训练，所以它们的模型算法和预测函数都是相同的。我们可以通过添加模型缓存来扩展图6.4中的模型服务设计，以支持多租户。通过在内存中缓存模型图及其相关数据，我们可以在一个服务中执行多租户模型服务。图6.6说明了这个概念。
- en: Compared with the model service design in figure 6.4, the design in figure 6.6
    adds a model cache (component A) and a model file server (component B). Because
    we want to support multiple models in one service, we need a model cache in memory
    to host and execute different models. The model file server stores the model files
    that can be loaded into the prediction service’s model cache. The model server
    can also be shared among prediction service instances.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与图6.4中的模型服务设计相比，图6.6中的设计增加了模型缓存（组件A）和模型文件服务器（组件B）。因为我们希望在一个服务中支持多个模型，所以我们需要一个内存中的模型缓存来托管和执行不同的模型。模型文件服务器存储可以加载到预测服务模型缓存中的模型文件。模型服务器也可以在预测服务实例之间共享。
- en: To build a good model cache, we need to consider model cache management and
    memory resource management. For the model cache, we need to assign a unique model
    ID as a cache key to identify each model in the cache. For example, we can use
    the model training run ID as the model ID; the benefit is, for each model in the
    cache, we can trace which training run produced it. Another more flexible way
    of constructing the model ID is combining the model name (a customized string)
    and the model version. No matter which model ID style we choose, the ID has to
    be unique, and it must be provided in the prediction request.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个好的模型缓存，我们需要考虑模型缓存管理和内存资源管理。对于模型缓存，我们需要为每个缓存中的模型分配一个唯一的模型ID作为缓存键来标识每个模型。例如，我们可以使用模型训练运行ID作为模型ID；其好处是，对于缓存中的每个模型，我们可以追踪是哪个训练运行产生的。另一种更灵活构建模型ID的方法是结合模型名称（一个自定义字符串）和模型版本。无论我们选择哪种模型ID风格，ID都必须是唯一的，并且必须在预测请求中提供。
- en: '![](../Images/06-06.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-06.png)'
- en: Figure 6.6 A prediction service with model caching for multitenant applications
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 多租户应用的模型缓存预测服务
- en: For memory resource management, because each server has limited memory and GPU
    resources, we can’t load all the required models into memory. So we need to build
    model-swapping logic to the model cache. When the resource capacity is reached—for
    instance, the process is about to run out of memory—some models need to be evicted
    from the model cache to free some resources for new model prediction requests.
    Methods like LRU (least recently used) algorithm and model partition across different
    instances can help reduce the cache missing rate (the request model is not in
    the cache) and make model swapping less disruptive. The sample intent classification
    prediction service we build in section 7.1 demonstrates the model caching concept;
    you can explore the details there.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内存资源管理，因为每个服务器都有有限的内存和GPU资源，我们不能将所有必需的模型加载到内存中。因此，我们需要在模型缓存中构建模型交换逻辑。当资源容量达到极限时——例如，进程即将耗尽内存——一些模型需要从模型缓存中移除，为新模型的预测请求释放一些资源。像LRU（最近最少使用）算法和跨不同实例的模型分区这样的方法可以帮助减少缓存缺失率（请求的模型不在缓存中）并使模型交换不那么具有破坏性。我们在第7.1节中构建的示例意图分类预测服务展示了模型缓存的概念；你可以在那里探索细节。
- en: Can we extend the model caching design to multiple model types?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否将模型缓存设计扩展到多种模型类型？
- en: We don’t recommend extending the model caching design to multiple model types.
    The input/output data format and data process logic of various model types, such
    as the image classification model and intent classification model, are very different,
    so it’s hard to host and execute different types of models in the same model cache.
    To do that, we would need to build separate web interfaces and separate data preprocess
    and postprocess code for each type of model. At this point, you will find it’s
    easier to build separate prediction services for each model type—with each service
    having its own type of web interface and data process logic and managing the model
    cache for its own model type. For example, we can build an image classification
    prediction service and an intent classification prediction service for these two
    different model types separately.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不建议将模型缓存设计扩展到多种模型类型。不同模型类型，如图像分类模型和意图分类模型，其输入/输出数据格式和数据处理逻辑差异很大，因此在同一模型缓存中托管和执行不同类型的模型会很困难。为了做到这一点，我们需要为每种模型类型构建单独的Web界面和单独的数据预处理和后处理代码。此时，你会发现为每种模型类型构建单独的预测服务会更简单——每个服务都有自己的Web界面和数据处理逻辑，并管理其自身模型类型的模型缓存。例如，我们可以分别为这两种不同的模型类型构建图像分类预测服务和意图分类预测服务。
- en: This one service per model type approach works well when you only have a few
    model types. But if you have 20+ types of models, then it can’t scale. Building
    and maintaining web services—such as setting up a CI/CD pipeline, networking,
    and deployment—is costly. Also, the work of monitoring a service is nontrivial;
    we need to build monitoring and alerting mechanisms to ensure the service is running
    24/7\. Consider the costs of onboarding and maintenance work if we follow this
    design to support 100+ model types for the entire company. To scale up and serve
    lots of different model types in one system, we need to take the model server
    approach (section 6.2.3), which we will discuss further in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当您只有少数几种模型类型时，这种每个模型类型一个服务的方法效果很好。但是，如果您有20种以上的模型类型，那么它就无法扩展。构建和维护网络服务——例如设置CI/CD管道、网络和部署——是昂贵的。此外，监控服务的工作也不简单；我们需要构建监控和警报机制来确保服务24/7正常运行。如果我们按照这种设计来支持整个公司100多种模型类型，那么上线和维护工作的成本将会很高。为了在一个系统中扩展并服务于许多不同的模型类型，我们需要采用模型服务器方法（第6.2.3节），我们将在下一节中进一步讨论。
- en: 6.3.3 Supporting multiple applications in one system
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 在一个系统中支持多个应用程序
- en: 'You have successfully built multiple model serving services to support different
    applications, such as multitenant chatbot, face-swapping, flower recognition,
    and PDF document scanning. Now, you are given two more tasks: (1) building the
    model serving support for a new application that uses a voice recognition model
    and (2) reducing model serving costs for all applications.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经成功构建了多个模型服务来支持不同的应用程序，例如多租户聊天机器人、换脸、花卉识别和PDF文档扫描。现在，您面临两个新的任务：(1) 为使用语音识别模型的新应用程序构建模型服务支持，以及(2)
    降低所有应用程序的模型服务成本。
- en: So far, all the model serving implementations have been built with the model
    service approach. From previous discussions in sections 6.3.1 and 6.3.2, we know
    this approach can’t scale when we have more and more model types. When many products
    and applications have model serving requirements, it’s better to build just one
    centralized prediction service to address all the serving needs. We name this
    type of prediction service a *prediction platform*. It takes the model server
    approach (section 6.2.3) and handles all kinds of model serving in one place.
    This is the most cost-efficient approach for multiple application situations because
    the model onboarding and maintenance cost is limited to one system, which is much
    less than one prediction service per application approach (section 6.2.2).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有的模型服务实现都是基于模型服务方法构建的。从第6.3.1节和第6.3.2节中的先前讨论中，我们知道当模型类型越来越多时，这种方法无法扩展。当许多产品和应用程序有模型服务需求时，最好只构建一个集中的预测服务来满足所有服务需求。我们称这种类型的预测服务为*预测平台*。它采用模型服务器方法（第6.2.3节）并在一个地方处理所有类型的模型服务。这是多应用场景中最经济高效的方法，因为模型上线和维护成本限制在一个系统中，这比每个应用程序一个预测服务的方法（第6.2.2节）要少得多。
- en: To build such an omnipotent model serving system, we need to consider lots of
    elements, such as model file format, model libraries, model training frameworks,
    model caching, model versioning, model flow execution, model data processing,
    model management, and a unified prediction API that suits all model types. Figure
    6.7 illustrates the design and workflow of the prediction platform.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这样一个全能型的模型服务系统，我们需要考虑许多元素，例如模型文件格式、模型库、模型训练框架、模型缓存、模型版本控制、模型流程执行、模型数据处理、模型管理和适用于所有模型类型的统一预测API。图6.7展示了预测平台的设计和工作流程。
- en: The prediction platform design in figure 6.7 is much more complicated than the
    model service approach in figure 6.6\. This is because we need to combine multiple
    components and services to support arbitrary models. Let’s look at each component
    of the system and then the model prediction workflow.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7中的预测平台设计比图6.6中的模型服务方法复杂得多。这是因为我们需要结合多个组件和服务来支持任意模型。让我们看看系统的每个组件，然后是模型预测工作流程。
- en: '![](../Images/06-07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-07.png)'
- en: Figure 6.7 A general prediction service (platform) design that works with arbitrary
    model types
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 一个通用的预测服务（平台）设计，适用于任意模型类型
- en: Unified web API
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 统一网络API
- en: To support arbitrary models, we expect the public prediction APIs to be generic.
    No matter which model is called, the API’s spec—for instance, its payload schema
    of the prediction request and response—should be generic enough to satisfy the
    model’s algorithm requirement. One example of this kind of unified API is the
    KFServing predict protocol ([http://mng.bz/BlB2](http://mng.bz/BlB2)), which aims
    to standardize the prediction protocol that works for any models and various prediction
    backends.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持任意模型，我们期望公共预测API是通用的。无论调用哪种模型，API的规范——例如，预测请求和响应的有效载荷模式——应该足够通用，以满足模型算法的要求。这类统一API的一个例子是KFServing预测协议([http://mng.bz/BlB2](http://mng.bz/BlB2))，该协议旨在标准化适用于任何模型和各种预测后端的预测协议。
- en: 'The web APIs are also expected to be simple, so we can reduce the customer
    onboarding and maintenance effort. The prediction APIs can be categorized into
    three buckets: model prediction requests API, model metadata fetching API, and
    model deployment API. The model metadata fetching API and deployment API are very
    useful because they are agnostic about the model they are serving. We need these
    methods to check the model metadata, such as the model version and algorithm info,
    and to check the model deployment status.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 预期Web API也将很简单，这样我们可以减少客户入职和维护的工作量。预测API可以分为三个类别：模型预测请求API、模型元数据获取API和模型部署API。模型元数据获取API和部署API非常有用，因为它们对所服务的模型没有特定要求。我们需要这些方法来检查模型元数据，例如模型版本和算法信息，以及检查模型部署状态。
- en: Routing component
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 路由组件
- en: Normally, each type of serving backend can only handle a few types of models.
    To support arbitrary models, we need to have different kinds of serving backends,
    such as TensorFlow Serving backend for TensorFlow models and TorchServe backend
    for PyTorch models. When receiving a model prediction request, the system needs
    to know which backend can handle it. This is done with the routing component.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每种服务后端只能处理几种类型的模型。为了支持任意模型，我们需要有不同种类的服务后端，例如用于TensorFlow模型的TensorFlow Serving后端和用于PyTorch模型的TorchServe后端。当收到模型预测请求时，系统需要知道哪个后端可以处理它。这是通过路由组件来完成的。
- en: The routing component responds to route the prediction request to the correct
    backend inference server. For a given request, the routing component first fetches
    the model’s metadata; the metadata includes the model algorithm name and version,
    the model version, and the training framework. Then, by matching the model metadata
    with the routing config, it determines to which inference backend it should route
    the prediction request.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 路由组件负责将预测请求路由到正确的后端推理服务器。对于给定的请求，路由组件首先获取模型的元数据；元数据包括模型算法名称和版本、模型版本以及训练框架。然后，通过将模型元数据与路由配置进行匹配，它确定将预测请求路由到哪个推理后端。
- en: Graph execution component
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图执行组件
- en: 'The graph execution component handles the type of prediction that needs to
    execute a series of model predictions. For example, to automate the mortgage approval
    process, we have to run a loan approval prediction request following three models
    in a sequence: a PDF scanning model to parse the text from the PDF loan application,
    a named entity recognition model to recognize the keywords, and a loan-scoring
    model to score the loan application. To support such requirements, we can define
    a directed acyclic graph (DAG) to describe the model execution chain and build
    a graph execution engine to execute in one go.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图执行组件处理需要执行一系列模型预测的预测类型。例如，为了自动化抵押贷款审批流程，我们必须按照以下顺序运行贷款审批预测请求：一个PDF扫描模型从PDF贷款申请中解析文本，一个命名实体识别模型来识别关键词，以及一个贷款评分模型来评估贷款申请。为了支持这类需求，我们可以定义一个有向无环图（DAG）来描述模型执行链，并构建一个图执行引擎来一次性执行。
- en: Inference server
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 推理服务器
- en: The inference (model) server does the actual work to compute model prediction
    by managing model caching and model prediction execution. It’s similar to the
    prediction service shown in figure 6.6 but more sophisticated because it needs
    to support arbitrary model algorithms. Besides the predict API, the inference
    server should also offer model management API to register new models and remove
    models programmatically.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 推理（模型）服务器通过管理模型缓存和模型预测执行来完成计算模型预测的实际工作。它与图6.6中显示的预测服务类似，但更为复杂，因为它需要支持任意模型算法。除了预测API之外，推理服务器还应提供模型管理API，以注册新模型和程序化地删除模型。
- en: Building an inference server is much more complicated than building a predictor
    service; not many engineers want to try that. But luckily, there are multiple
    black-box, open source approaches that work out of the box, such as TensorFlow
    Serving, TorchServe, and NVIDIA Triton Inference Server. In practice, we often
    reuse these existing open source inference servers and integrate them into our
    own routing component and graph execution component. We will discuss more on the
    open source model server tools in chapter 7.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 构建推理服务器比构建预测服务要复杂得多；并不是很多工程师愿意尝试。但幸运的是，有多个开箱即用的黑盒开源方法，例如TensorFlow Serving、TorchServe和NVIDIA
    Triton推理服务器。在实践中，我们经常重用这些现有的开源推理服务器，并将它们集成到我们自己的路由组件和图执行组件中。我们将在第7章中进一步讨论开源模型服务器工具。
- en: Applications
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序
- en: In figure 6.7, we see applications A, B, and C are sharing the same model serving
    backend. The model serving for different applications occurs at the same place.
    Compared with the model service design in figure 6.6, the prediction platform
    is more scalable and more cost-efficient because there is almost no onboarding
    cost to add new application D.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.7中，我们看到应用程序A、B和C正在共享相同的模型服务后端。不同应用程序的模型服务发生在同一位置。与图6.6中的模型服务设计相比，预测平台更具有可扩展性和成本效益，因为添加新应用程序D几乎没有入门成本。
- en: For example, if we want to onboard new application D—a voice-to-text scripting
    application—we just need to upload the voice scripting model to the model file
    server and then let the application use the unified prediction web API of the
    prediction platform. There is no code change on the prediction platform side for
    supporting a new application.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想引入新应用程序D——一个语音到文本的脚本应用程序——我们只需将语音脚本模型上传到模型文件服务器，然后让应用程序使用预测平台的统一预测Web
    API。在预测平台侧不需要进行代码更改以支持新应用程序。
- en: Model prediction workflow
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测工作流程
- en: After explaining each key component, let’s look at a typical model prediction
    workflow (figure 6.7). First, we publish our model files to the model file server
    and update the config in the routing component, so the routing component knows
    to which inference server it should route the prediction request for this type
    of model. Second, applications send prediction requests to the prediction system’s
    web APIs, and then the request is routed by the routing component to the correct
    inference server. Third, the inference server will load the model from the model
    file server, convert the request payload to model input, run the model algorithm,
    and return the prediction result with a postprocess.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释完每个关键组件后，让我们看看一个典型的模型预测工作流程（图6.7）。首先，我们将我们的模型文件发布到模型文件服务器，并更新路由组件中的配置，以便路由组件知道应将此类模型的预测请求路由到哪个推理服务器。其次，应用程序将预测请求发送到预测系统的Web
    API，然后请求由路由组件路由到正确的推理服务器。第三，推理服务器将从模型文件服务器加载模型，将请求有效负载转换为模型输入，运行模型算法，并通过后处理返回预测结果。
- en: Note Prediction platform design is not always the best serving approach! In
    theory, the design in figure 6.7 can work for any model, but it does come with
    some extra cost. Its setup, maintenance, and debugging are way more complicated
    than the model service approach. This design is overkill for scenarios introduced
    in sections 6.3.1 and 6.3.2\. Because each design has its merits, we recommend
    not adhering to one serving approach but choosing the serving method based on
    your actual user scenarios.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：预测平台设计并不总是最佳的服务方法！从理论上讲，图6.7中的设计可以适用于任何模型，但它确实带来了一些额外的成本。其设置、维护和调试比模型服务方法复杂得多。对于6.3.1和6.3.2节中介绍的场景，这种设计过于冗余。因为每个设计都有其优点，我们建议不要固守一种服务方法，而是根据您的实际用户场景选择服务方法。
- en: 6.3.4 Common prediction service requirements
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 常见预测服务要求
- en: 'Although we state that designing prediction services should start from concrete
    use cases, different situations lead to different designs. Three common requirements
    exist among all model serving designs:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们强调设计预测服务应从具体用例开始，但不同的情况会导致不同的设计。在所有模型服务设计中都存在三个常见要求：
- en: '*Model deployment safety*—No matter what model rollout strategy and version
    strategy we choose, we must have a way to roll back a model to the previous state
    or version.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型部署安全*——无论我们选择什么模型发布策略和版本策略，我们都必须有一种方法可以将模型回滚到先前的状态或版本。'
- en: '*Latency*—Web request latency is a crucial factor in the success of many online
    businesses. Once we build the model serving support, the next step is to try our
    best to reduce the average prediction response time.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟*——Web请求延迟是许多在线业务成功的关键因素。一旦我们建立了模型服务支持，下一步就是尽最大努力减少平均预测响应时间。'
- en: '*Monitoring and alerting*—Model serving is the most critical service in a deep
    learning system; if it goes down, the business stops. Remember, actual businesses
    run on top of the model prediction in realtime. Customers are affected immediately
    if the service is down or serving latency increases. Prediction service should
    be the most-equipped service among other deep learning services in monitoring
    and alerting.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监控和警报*——模型服务是深度学习系统中最重要的服务；如果它崩溃，业务就会停止。记住，实际业务是在实时模型预测之上运行的。如果服务中断或服务延迟增加，客户将立即受到影响。预测服务应该是其他深度学习服务中在监控和警报方面最完善的。'
- en: In this chapter, we have reviewed concepts, definitions, and abstract high-level
    system designs of model serving. We hope you gain a clear picture of what model
    serving is and what to consider when designing model serving systems. In the next
    chapter, we will demo two sample prediction services and discuss the commonly
    used prediction open source tools. These examples will show you how the design
    concepts in this chapter are applied in real life.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了模型服务的概念、定义和抽象的高级系统设计。我们希望您对模型服务是什么以及设计模型服务系统时需要考虑什么有一个清晰的了解。在下一章中，我们将演示两个示例预测服务并讨论常用的预测开源工具。这些示例将展示本章中的设计概念如何在现实生活中应用。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'A model can be several files; it is composed of three elements: machine learning
    algorithm, model executor (wrapper), and model data.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型可以由几个文件组成；它由三个元素组成：机器学习算法、模型执行器（包装器）和模型数据。
- en: Model prediction and model inference have the same meaning in the model serving
    context.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型服务上下文中，模型预测和模型推理具有相同的意义。
- en: Direct model embedding, model service, and model server are the three common
    types of model serving strategies.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接模型嵌入、模型服务和模型服务器是三种常见的模型服务策略。
- en: The model service approach involves building a prediction service for each model,
    each version of a model, or each type of model.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务方法涉及为每个模型、每个模型的版本或每种类型的模型构建一个预测服务。
- en: The model server approach consists of building only one prediction service,
    but it can run models trained with different algorithms and frameworks and can
    run different versions of each model.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务器方法只涉及构建一个预测服务，但它可以运行使用不同算法和框架训练的模型，并且可以运行每个模型的不同版本。
- en: When designing a model serving system, the first component to understand is
    the use case, so we can decide which serving approach is most appropriate.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设计模型服务系统时，首先要理解的是用例，这样我们就可以决定哪种服务方法最合适。
- en: Cost efficiency is the primary goal for designing model serving systems; the
    cost includes service deployment, maintenance, monitoring, infrastructure, and
    service development.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本效益是设计模型服务系统的首要目标；成本包括服务部署、维护、监控、基础设施和服务开发。
- en: For single model applications, we recommend the model service approach.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于单模型应用程序，我们建议使用模型服务方法。
- en: For multitenant applications, we recommend the model service approach with an
    in-memory model cache.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多租户应用程序，我们建议使用具有内存中模型缓存的模型服务方法。
- en: For supporting multiple applications with different types of models, the model
    server and prediction platform are the most suitable approaches. They include
    a unified prediction API, a routing component, a graph execution component, and
    multiple model server backends.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于支持具有不同类型模型的多应用程序，模型服务器和预测平台是最合适的方法。它们包括统一的预测API、路由组件、图执行组件和多个模型服务器后端。
