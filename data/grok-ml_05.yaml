- en: 5 Using lines to split our points: The perceptron algorithm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 使用线来分割我们的点：感知器算法
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: what is classification
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是分类
- en: 'sentiment analysis: how to tell if a sentence is happy or sad using machine
    learning'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析：如何使用机器学习判断句子是快乐的还是悲伤
- en: how to draw a line that separates points of two colors
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何画一条线来分隔两种颜色的点
- en: what is a perceptron, and how do we train it
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器是什么，我们如何训练它
- en: coding the perceptron algorithm in Python and Turi Create
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 和 Turi Create 中编码感知器算法
- en: '![](../Images/5-unnumb.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-unnumb.png)'
- en: 'In this chapter, we learn a branch of machine learning called *classification*.
    Classification models are similar to regression models, in that their aim is to
    predict the labels of a dataset based on the features. The difference is that
    regression models aim to predict a number, whereas classification models aim to
    predict a state or a category. Classification models are often called *classifiers*,
    and we’ll use the terms interchangeably. Many classifiers predict one of two possible
    states (often yes/no), although it is possible to build classifiers that predict
    among a higher number of possible states. The following are popular examples of
    classifiers:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习机器学习的一个分支，称为 *分类*。分类模型与回归模型类似，它们的目的是根据特征预测数据集的标签。区别在于，回归模型旨在预测一个数字，而分类模型旨在预测一个状态或类别。分类模型通常被称为
    *分类器*，我们将交替使用这些术语。许多分类器预测两种可能的状态之一（通常是是/否），尽管也可以构建预测更多可能状态的分类器。以下是一些流行的分类器示例：
- en: A recommendation model that predicts whether a user will watch a certain movie
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测用户是否会观看特定电影的推荐模型
- en: An email model that predicts whether an email is spam or ham
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测电子邮件是垃圾邮件还是正常邮件的电子邮件模型
- en: A medical model that predicts whether a patient is sick or healthy
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测患者是生病还是健康的医疗模型
- en: An image-recognition model that predicts whether an image contains an automobile,
    a bird, a cat, or a dog
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测图像是否包含汽车、鸟、猫或狗的图像识别模型
- en: A voice recognition model that predicts whether the user said a particular command
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测用户是否说了特定命令的语音识别模型
- en: 'Classification is a popular area in machine learning, and the bulk of the chapters
    in this book (chapters 5, 6, 8, 9, 10, 11, and 12) talk about different classification
    models. In this chapter, we learn the *perceptron* model, also called the *perceptron
    classifier*, or simply the *perceptron*. A perceptron is similar to a linear regression
    model, in that it uses a linear combination of the features to make a prediction
    and is the building block of neural networks (which we learn in chapter 10). Furthermore,
    the process of training a perceptron is similar to that of training a linear regression
    model. Just as we did in chapter 3 with the linear regression algorithm, we develop
    the perceptron algorithm in two ways: using a trick that we can iterate many times,
    and defining an error function that we can minimize using gradient descent.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是机器学习中的一个热门领域，本书的大部分章节（第 5、6、8、9、10、11 和 12 章）都讨论了不同的分类模型。在本章中，我们学习 *感知器*
    模型，也称为 *感知器分类器*，或简单地称为 *感知器*。感知器类似于线性回归模型，因为它使用特征的线性组合来做出预测，并且是神经网络（我们在第 10 章中学习）的构建块。此外，训练感知器的过程与训练线性回归模型的过程类似。就像我们在第
    3 章中用线性回归算法做的那样，我们以两种方式开发感知器算法：使用我们可以多次迭代的技巧，以及定义一个我们可以通过梯度下降最小化的误差函数。
- en: The main example of classification models that we learn in this chapter is *sentiment
    analysis*. In sentiment analysis, the goal of the model is to predict the sentiment
    of a sentence. In other words, the model predicts whether the sentence is happy
    or sad. For example, a good sentiment analysis model can predict that the sentence
    “I feel wonderful!” is a happy sentence, and that the sentence “What an awful
    day!” is a sad sentence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们学习的主要分类模型示例是 *情感分析*。在情感分析中，模型的目的是预测句子的情感。换句话说，模型预测句子是快乐的还是悲伤的。例如，一个好的情感分析模型可以预测句子“我感觉太棒了！”是一个快乐的句子，而句子“多么糟糕的一天！”是一个悲伤的句子。
- en: 'Sentiment analysis is used in many practical applications, such as the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析在许多实际应用中被使用，例如以下：
- en: When a company analyzes the conversations between customers and technical support,
    to evaluate the quality of the conversation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当公司分析客户与技术支持之间的对话时，以评估对话的质量
- en: When analyzing the tone of a brand’s digital presence, such as comments on social
    media or reviews related to its products
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当分析品牌数字存在（如社交媒体上的评论或与其产品相关的评论）的语气时
- en: When a social platform like Twitter analyzes the overall mood of a certain population
    after an event
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当像Twitter这样的社交平台在事件发生后分析特定人群的整体情绪时
- en: When an investor uses public sentiment toward a company to predict its stock
    price
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当投资者使用对公司公开情绪的预测来预测其股价时
- en: How could we build a sentiment analysis classifier? In other words, how could
    we build a machine learning model that takes a sentence as an input and, as output,
    tells us whether the sentence is happy or sad. This model can make mistakes, of
    course, but the idea is to build it in such a way that it makes as few mistakes
    as possible. Let’s put down the book for a couple of minutes and think of how
    we would go about building this type of model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何构建一个情感分析分类器呢？换句话说，我们如何构建一个机器学习模型，它以句子作为输入，并以输出告诉我们句子是快乐还是悲伤。这个模型当然会犯错误，但我们的想法是构建它，使其尽可能少犯错误。让我们放下书本几分钟，想想我们如何构建这种类型的模型。
- en: Here is an idea. Happy sentences tend to contain happy words, such as *wonderful*,
    *happy*, or *joy*, whereas sad sentences tend to contain sad words, such as *awful*,
    *sad*, or *despair*. A classifier can consist of a “happiness” score for every
    single word in the dictionary. Happy words can be given positive scores, and sad
    words can be given negative scores. Neutral words such as *the* can be given a
    score of zero. When we feed a sentence into our classifier, the classifier simply
    adds the scores of all the words in the sentence. If the result is positive, then
    the classifier concludes that the sentence is happy. If the result is negative,
    then the classifier concludes that the sentence is sad. The goal now is to find
    scores for all the words in the dictionary. For this, we use machine learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个想法。快乐句子往往包含快乐词汇，如**wonderful**、**happy**或**joy**，而悲伤句子往往包含悲伤词汇，如**awful**、**sad**或**despair**。一个分类器可以由字典中每个单词的“快乐”分数组成。快乐词汇可以给予正分数，而悲伤词汇可以给予负分数。中性词汇如**the**可以给予零分。当我们将一个句子输入到我们的分类器中时，分类器只是简单地将句子中所有单词的分数相加。如果结果是正的，那么分类器就会得出结论，这个句子是快乐的。如果结果是负的，那么分类器就会得出结论，这个句子是悲伤的。现在的目标是找到字典中所有单词的分数。为此，我们使用机器学习。
- en: The type of model we just built is called a *perceptron model*. In this chapter,
    we learn the formal definition of a perceptron and how to train it by finding
    the perfect scores for all the words so that our classifier makes as few mistakes
    as possible.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才构建的模型类型被称为**感知器模型**。在本章中，我们将学习感知器的正式定义以及如何通过找到所有单词的完美分数来训练它，以便我们的分类器尽可能少犯错误。
- en: 'The process of training a perceptron is called the *perceptron algorithm*,
    and it is not that different from the linear regression algorithm we learned in
    chapter 3\. Here is the idea of the perceptron algorithm: To train the model,
    we first need a dataset containing many sentences together with their labels (happy/sad).
    We start building our classifier by assigning random scores to all the words.
    Then we go over all the sentences in our dataset several times. For every sentence,
    we slightly tweak the scores so that the classifier improves the prediction for
    that sentence. How do we tweak the scores? We do it using a trick called the *perceptron
    trick*, which we learn in the section “The perception trick.” An equivalent way
    to train perceptron models is to use an error function, just as we did in chapter
    3\. We then use gradient descent to minimize this function.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练感知器的过程被称为**感知器算法**，它与我们在第三章学到的线性回归算法并没有太大的区别。以下是感知器算法的思路：为了训练模型，我们首先需要一个包含许多句子及其标签（快乐/悲伤）的数据集。我们通过为所有单词分配随机分数来开始构建我们的分类器。然后，我们多次遍历数据集中的所有句子。对于每个句子，我们稍微调整分数，以便分类器改进对该句子的预测。我们如何调整分数呢？我们使用一种称为**感知器技巧**的方法来调整分数，这在“感知器技巧”这一节中会学到。另一种训练感知器模型的方法是使用误差函数，就像我们在第三章中所做的那样。然后我们使用梯度下降来最小化这个函数。
- en: However, language is complicated—it has nuances, double entendres, and sarcasm.
    Wouldn’t we lose too much information if we reduce a word to a simple score? The
    answer is yes—we do lose a lot of information, and we won’t be able to create
    a perfect classifier this way. The good news is that using this method, we can
    still create a classifier that is correct *most* of the time. Here is a proof
    that the method we are using can’t be correct all the time. The sentences, “I
    am not sad, I’m happy” and “I am not happy, I am sad” have the same words, yet
    completely different meanings. Therefore, no matter what scores we give the words,
    these two sentences will attain the exact same score, and, thus, the classifier
    will return the same prediction for them. They have different labels, so the classifier
    must have made a mistake with one of them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，语言很复杂——它有细微差别、双关语和讽刺。如果我们把一个词简化为一个简单的分数，我们不会丢失太多信息吗？答案是肯定的——我们会丢失很多信息，而且我们无法通过这种方式创建一个完美的分类器。好消息是，使用这种方法，我们仍然可以创建一个大多数时候都是正确的分类器。以下是一个证明我们使用的方法不可能总是正确的例子。句子“我不悲伤，我很快乐”和“我不快乐，我很悲伤”有相同的单词，但意义完全不同。因此，无论我们给单词赋予什么分数，这两个句子都将获得相同的分数，因此分类器将对他们做出相同的预测。它们有不同的标签，所以分类器必须在对其中一个句子做出错误判断。
- en: A solution for this problem is to build a classifier that takes the order of
    the words into account, or even other things such as punctuation or idioms. Some
    models such as *hidden Markov models* (HMM), *recurrent neural networks* (RNN),
    or *long short-term memory networks* (LSTM) have had great success with sequential
    data, but we won’t include them in this book. However, if you want to explore
    these models, in appendix C you can find some very useful references for that.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是为分类器构建一个考虑单词顺序的模型，或者甚至考虑其他事物，如标点符号或习语。一些模型，如*隐马尔可夫模型*（HMM）、*循环神经网络*（RNN）或*长短期记忆网络*（LSTM）在序列数据上取得了巨大成功，但我们将不会在本书中包括它们。然而，如果您想探索这些模型，附录C中您可以找到一些非常有用的参考资料。
- en: 'You can find all the code for this chapter in the following GitHub repository:
    [https://github.com/luisguiserrano/manning/tree/master/Chapter_5_Perceptron_Algorithm](https://github.com/luisguiserrano/manning/tree/master/Chapter_5_Perceptron_Algorithm).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下GitHub仓库中找到本章所有代码：[https://github.com/luisguiserrano/manning/tree/master/Chapter_5_Perceptron_Algorithm](https://github.com/luisguiserrano/manning/tree/master/Chapter_5_Perceptron_Algorithm).
- en: 'The problem: We are on an alien planet, and we don’t know their language!'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题：我们身处一个外星星球，而我们不知道他们的语言！
- en: 'Imagine the following scenario: we are astronauts and have just landed on a
    distant planet where a race of unknown aliens live. We would love to be able to
    communicate with the aliens, but they speak a strange language that we don’t understand.
    We notice that the aliens have two moods, happy and sad. Our first step in communicating
    with them is to figure out if they are happy or sad based on what they say. In
    other words, we want to build a sentiment analysis classifier.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象以下场景：我们是宇航员，刚刚降落在遥远的星球上，那里居住着一群未知的外星人。我们希望能与外星人交流，但他们说的是一种我们不懂的奇怪语言。我们注意到外星人有两种情绪，快乐和悲伤。我们与他们交流的第一步是确定他们情绪是快乐还是悲伤。换句话说，我们希望构建一个情感分析分类器。
- en: 'We manage to befriend four aliens, and we start observing their mood and studying
    what they say. We observe that two of them are happy and two of them are sad.
    They also keep repeating the same sentence over and over. Their language seems
    to only have two words: *aack* and *beep*. We form the following dataset with
    the sentence they say and their mood:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设法与四个外星人交上了朋友，并开始观察他们的情绪，研究他们说的话。我们注意到其中两个是快乐的，两个是悲伤的。他们也反复重复相同的句子。他们的语言似乎只有两个词：*aack*和*beep*。我们根据他们说的句子和他们的情绪形成了以下数据集：
- en: 'Dataset:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集：
- en: Alien 1
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外星人1
- en: 'Mood: Happy'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情绪：快乐
- en: 'Sentence: *“Aack, aack, aack!”*'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：*“Aack, aack, aack!”*
- en: 'Alien 2:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外星人2：
- en: 'Mood: Sad'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情绪：悲伤
- en: 'Sentence: *“Beep beep!”*'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：*“Beep beep!”*
- en: 'Alien 3:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外星人3：
- en: 'Mood: Happy'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情绪：快乐
- en: 'Sentence: *“Aack beep aack!”*'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：*“Aack beep aack!”*
- en: 'Alien 4:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外星人4：
- en: 'Mood: Sad'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情绪：悲伤
- en: 'Sentence: *“Aack beep beep beep!”*'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子：*“Aack beep beep beep!”*
- en: All of a sudden, a fifth alien comes in, and it says, “*Aack beep aack aack*!”
    We can’t really tell the mood of this alien. From what we know, how should we
    predict for the mood of the alien (figure 5.1)?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 突然，第五个外星人进来了，它说，“*Aack beep aack aack*！”我们真的无法判断这个外星人的情绪。根据我们所知，我们应该如何预测这个外星人的情绪（图5.1）？
- en: We predict that this alien is happy because, even though we don’t know the language,
    the word *aack* seems to appear more in happy sentences, whereas the word *beep*
    seems to appear more in sad sentences. Perhaps *aack* means something positive,
    such as “joy” or “happiness,” whereas *beep* may mean something sad, such as “despair”
    or “sadness.”
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测这个外星人很开心，尽管我们不知道他们的语言，但单词 *aack* 在快乐句子中似乎出现得更频繁，而单词 *beep* 在悲伤句子中似乎出现得更频繁。也许
    *aack* 代表着积极的意义，比如“快乐”或“幸福”，而 *beep* 可能代表着悲伤的意义，比如“绝望”或“悲伤”。
- en: '![](../Images/5-1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-1.png)'
- en: Figure 5.1 Our dataset of aliens. We have recorded their mood (happy or sad)
    and the sentence they keep repeating. Now a fifth alien comes in, saying a different
    sentence. Do we predict that this alien is happy or sad?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 我们的外星人数据集。我们记录了他们的情绪（快乐或悲伤）和他们反复说的句子。现在又来了一位第五个外星人，说了一个不同的句子。我们预测这个外星人快乐还是悲伤？
- en: 'This observation gives rise to our first sentiment analysis classifier. This
    classifier makes a prediction in the following way: it counts the number of appearances
    of the words *aack* and *beep*. If the number of appearances of *aack* is larger
    than that of *beep*, then the classifier predicts that the sentence is happy.
    If it is smaller, then the classifier predicts that the sentence is sad. What
    happens when both words appear the same number of times? We have no basis to tell,
    so let’s say that by default, the prediction is that the sentence is happy. In
    practice, these types of edge cases don’t happen often, so they won’t create a
    big problem for us.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个观察结果产生了我们的第一个情感分析分类器。这个分类器通过以下方式进行预测：它计算单词 *aack* 和 *beep* 出现的次数。如果 *aack*
    出现的次数多于 *beep*，则分类器预测句子是快乐的。如果它更少，则分类器预测句子是悲伤的。当两个单词出现次数相同会发生什么？我们没有依据来判断，所以我们说默认预测是句子是快乐的。实际上，这类边缘情况很少发生，所以它们不会给我们造成大问题。
- en: 'The classifier we just built is a perceptron (also called linear classifier).
    We can write it in terms of scores, or weights, in the following way:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构建的分类器是一个感知器（也称为线性分类器）。我们可以用分数或权重来表示它，如下所示：
- en: Sentiment analysis classifier
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析分类器
- en: 'Given a sentence, assign the following scores to the words:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个句子，为以下单词分配以下分数：
- en: 'Scores:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分数：
- en: '*Aack*:  1 point'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aack*: 1 分'
- en: '*Beep*: –1 points'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Beep*: -1 分'
- en: 'Rule:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 规则：
- en: 'Calculate the score of the sentence by adding the scores of all the words on
    it as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将句子上所有单词的分数相加来计算句子的分数，如下所示：
- en: If the score is positive or zero, predict that the sentence is happy.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数为正或零，预测句子是快乐的。
- en: If the score is negative, predict that the sentence is sad.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数为负，预测句子是悲伤的。
- en: In most situations, it is useful to plot our data, because sometimes nice patterns
    become visible. In table 5.1, we have our four aliens, as well as the number of
    times each said the words *aack* and *beep*, and their mood.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，绘制我们的数据是有用的，因为有时会出现漂亮的模式。在表 5.1 中，我们有四个外星人，以及他们说了多少次 *aack* 和 *beep*
    以及他们的情绪。
- en: Table 5.1 Our dataset of aliens, the sentences they said, and their mood. We
    have broken each sentence down to its number of appearances of the words aack
    and beep.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 我们的外星人数据集，他们说的句子和他们的情绪。我们将每个句子分解为单词 aack 和 beep 出现的次数。
- en: '| Sentence | Aack | Beep | Mood |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | Aack | Beep | 情绪 |'
- en: '| Aack aack aack! | 3 | 0 | Happy |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Aack aack aack! | 3 | 0 | 快乐 |'
- en: '| Beep beep! | 0 | 2 | Sad |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Beep beep! | 0 | 2 | 悲伤 |'
- en: '| Aack beep aack! | 2 | 1 | Happy |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Aack beep aack! | 2 | 1 | 快乐 |'
- en: '| Aack beep beep beep! | 1 | 3 | Sad |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Aack beep beep beep! | 1 | 3 | 悲伤 |'
- en: The plot consists of two axes, the horizontal (*x*) axis and the vertical (*y*)
    axis. In the horizontal axis, we record the number of appearances of *aack*, and
    in the vertical axis, the appearances of *beep*. This plot can be seen in figure
    5.2.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由两个轴组成，横轴（*x* 轴）和纵轴（*y* 轴）。在横轴上，我们记录了 *aack* 出现的次数，在纵轴上，记录了 *beep* 的出现次数。这个图表可以在图
    5.2 中看到。
- en: '![](../Images/5-2.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-2.png)'
- en: Figure 5.2 A plot of the dataset of aliens. In the horizontal axis we plot the
    number of appearances of the word *aack*, and in the vertical axis, the appearances
    of the word *beep*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 外星人的数据集图。在横轴上，我们绘制了单词 *aack* 出现的次数，在纵轴上，绘制了单词 *beep* 的出现次数。
- en: 'Note that in the plot in figure 5.2, the happy aliens are located on the bottom
    right, whereas the sad aliens are in the top left. This is because the bottom
    right is the area where the sentences have more appearances of *aack* than *beep*,
    and the top left area is the opposite. In fact, a line formed by all the sentences
    with the same number of appearances of *aack* and *beep* divides these two regions,
    as shown in figure 5.3\. This line has the following equation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在图 5.2 的图中，快乐的异形位于右下角，而悲伤的异形位于左上角。这是因为右下角是句子中 *aack* 出现次数多于 *beep* 的区域，而左上角区域则相反。实际上，所有
    *aack* 和 *beep* 出现次数相同的句子形成的线将这两个区域分开，如图 5.3 所示。这条线的方程如下：
- en: '#aack = #beep'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '#aack = #beep'
- en: 'Or equivalently, this equation:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 或者等价地，这个方程：
- en: '#aack – #beep = 0'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '#aack – #beep = 0'
- en: Throughout this chapter, we’ll use the variables *x* with different subscripts
    to indicate the number of appearances of a word in a sentence. In this case, *x*[aack]
    is the number of times the word *aack* appears, and *x*[beep] is the number of
    times the word *beep* appears.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用带有不同下标的变量 *x* 来表示一个词在句子中出现的次数。在这种情况下，*x*[aack] 表示单词 *aack* 出现的次数，而
    *x*[beep] 表示单词 *beep* 出现的次数。
- en: 'Using this notation, the equation of the classifier becomes *x*[aack] – *x*[beep]
    = 0, or equivalently, *x*[aack] = *x*[beep] *.* This is the equation of a line
    in the plane. If it doesn’t appear so, think of the equation of the line *y* =
    *x*, except instead of *x*, we have *x*[aack], and instead of *y*, we have *x*[beep]
    *.* Why not use *x* and *y* instead like we’ve done since high school? I would
    love to, but unfortunately we need the *y* for something else (the prediction)
    later. Thus, let’s think of the *x*[aack]-axis as the horizontal axis and the
    *x*[beep]-axis as the vertical axis. Together with this equation, we have two
    important areas, which we call the *positive zone* and the *negative zone*. They
    are defined as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种表示法，分类器的方程变为 *x*[aack] – *x*[beep] = 0，或者等价地，*x*[aack] = *x*[beep] *.* 这是平面上的一条直线方程。如果看起来不这样，想想直线方程
    *y* = *x*，除了用 *x* 代替，我们用 *x*[aack]，用 *y* 代替 *x*[beep] *.* 为什么不使用 *x* 和 *y* 像我们高中时做的那样呢？我愿意这样做，但不幸的是，我们稍后需要
    *y*（用于预测）。因此，让我们将 *x*[aack]-轴视为水平轴，将 *x*[beep]-轴视为垂直轴。与这个方程一起，我们有两个重要的区域，我们称之为
    *正区域* 和 *负区域*。它们的定义如下：
- en: '**Positive zone**: The area on the plane for which *x*[aack] – *x*[beep] ≥
    0\. This corresponds to the sentences in which the word *aack* appears at least
    as many times as the word *beep*.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**正区域**：平面上 *x*[aack] – *x*[beep] ≥ 0 的区域。这对应于单词 *aack* 出现的次数至少与单词 *beep* 相等的句子。'
- en: '**Negative zone**: The area on the plane for which *x*[aack] – *x*[beep] *<*
    0\. This corresponds to the sentences in which the word *aack* appears fewer times
    than the word *beep*.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**负区域**：平面上 *x*[aack] – *x*[beep] *<* 0 的区域。这对应于单词 *aack* 出现的次数少于单词 *beep*
    的句子。'
- en: The classifier we created predicts that every sentence in the positive zone
    is happy and every sentence in the negative zone is sad. Therefore, our goal is
    to find the classifier that can put as many happy sentences as possible in the
    positive area and as many sad sentences as possible in the negative area. For
    this small example, our classifier achieves this job to perfection. This is not
    always the case, but the perceptron algorithm will help us find a classifier that
    will perform this job really well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的分类器预测，正区域中的每个句子都是快乐的，而负区域中的每个句子都是悲伤的。因此，我们的目标是找到可以将尽可能多的快乐句子放入正区域，尽可能多的悲伤句子放入负区域的分类器。对于这个小型示例，我们的分类器完美地完成了这项工作。但这并不总是如此，但感知器算法将帮助我们找到能够真正出色完成这项工作的分类器。
- en: In figure 5.3, we can see the line that corresponds to the classifier and the
    positive and negative zones. If you compare figures 5.2 and 5.3, you can see that
    the current classifier is good, because all the happy sentences are in the positive
    zone and all the sad sentences are in the negative zone.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 5.3 中，我们可以看到对应于分类器和正负区域的线。如果你比较图 5.2 和图 5.3，你可以看到当前的分类器很好，因为所有快乐句子都在正区域，所有悲伤句子都在负区域。
- en: Now that we’ve built a simple sentiment analysis perceptron classifier, let’s
    look at a slightly more complex example.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了一个简单的情感分析感知器分类器，让我们看看一个稍微复杂一点的例子。
- en: '![](../Images/5-3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-3.png)'
- en: Figure 5.3 The classifier is the diagonal line that splits the happy and the
    sad points. The equation of this line is *x[aack]* = *x[beep]* (or equivalently,
    *x[aack]* – *x[beep]* = 0), because the line corresponds to all the points where
    the horizontal and the vertical coordinates are equal. The happy zone is the zone
    in which the number of appearances of *aack* is greater than or equal to the number
    of appearances of *beep*, and the sad zone is the zone in which the number of
    appearances of *aack* is less than that of *beep*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 分类器是分割快乐和悲伤点的对角线。这条线的方程是 *x[aack]* = *x[beep]*（或者等价地，*x[aack]* – *x[beep]*
    = 0），因为这条线对应于所有水平和垂直坐标相等的点。快乐区域是*aack*出现次数大于或等于*beep*出现次数的区域，而悲伤区域是*aack*出现次数少于*beep*出现次数的区域。
- en: A slightly more complicated planet
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个稍微复杂一些的星球
- en: 'In this section, we see a more complicated example, which introduces a new
    aspect of the perceptron: the bias. After we can communicate with the aliens on
    the first planet, we are sent on a mission to a second planet, where the aliens
    have a slightly more complicated language. Our goal is still the same: to create
    a sentiment analysis classifier in their language. The language in the new planet
    has two words: *crack* and *doink*. The dataset is shown in table 5.2.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到一个更复杂的例子，它引入了感知器的一个新方面：偏差。在我们能够与第一个星球上的外星人交流之后，我们被派往第二个星球，那里的外星人有稍微复杂一些的语言。我们的目标仍然是相同的：用他们的语言创建一个情感分析分类器。新星球上的语言有两种词：*crack*和*doink*。数据集显示在表5.2中。
- en: Building a classifier for this dataset seems to be a bit harder than for the
    previous dataset. First of all, should we assign positive or negative scores to
    the words *crack* and *doink*? Let’s take a pen and paper and try coming up with
    a classifier that can correctly separate the happy and sad sentences in this dataset.
    Looking at the plot of this dataset in figure 5.4 may be helpful.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为这个数据集构建分类器似乎比上一个数据集要困难一些。首先，我们应该给*crack*和*doink*这些词分配正分还是负分？让我们拿一支笔和一张纸，尝试构建一个可以正确分离这个数据集中快乐和悲伤句子的分类器。查看图5.4中的这个数据集的图表可能会有所帮助。
- en: Table 5.2 The new dataset of alien words. Again, we’ve recorded each sentence,
    the number of appearances of each word in that sentence, and the mood of the alien.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 外星词汇的新数据集。再次，我们记录了每个句子，该句子中每个单词出现的次数以及外星人的情绪。
- en: '| Sentence | Crack | Doink | Mood |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | Crack | Doink | 情绪 |'
- en: '| Crack! | 1 | 0 | Sad |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Crack! | 1 | 0 | 悲伤 |'
- en: '| Doink doink! | 0 | 2 | Sad |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Doink doink! | 0 | 2 | 悲伤 |'
- en: '| Crack doink! | 1 | 1 | Sad |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Crack doink! | 1 | 1 | 悲伤 |'
- en: '| Crack doink crack! | 2 | 1 | Sad |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Crack doink crack! | 2 | 1 | 悲伤 |'
- en: '| Doink crack doink doink! | 1 | 3 | Happy |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Doink crack doink doink! | 1 | 3 | 快乐 |'
- en: '| Crack doink doink crack! | 2 | 2 | Happy |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Crack doink doink crack! | 2 | 2 | 快乐 |'
- en: '| Doink doink crack crack crack! | 3 | 2 | Happy |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Doink doink crack crack crack! | 3 | 2 | 快乐 |'
- en: '| Crack doink doink crack doink! | 2 | 3 | Happy |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Crack doink doink crack doink! | 2 | 3 | 快乐 |'
- en: '![](../Images/5-4.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图5-4](../Images/5-4.png)'
- en: Figure 5.4 The plot of the new dataset of aliens. Notice that the happy ones
    tend to be above and to the right, and the sad ones below and to the left.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 外星词汇新数据集的图表。注意，快乐的点倾向于在上方和右侧，而悲伤的点在下方和左侧。
- en: The idea for this classifier is to count the number of words in a sentence.
    Notice that the sentences with one, two, or three words are all sad, and the sentences
    with four and five words are happy. That is the classifier! It classifies sentences
    with three words or fewer as sad, and the sentences with four words or more as
    happy. We can again write this in a more mathematical way.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器的想法是计算句子中的单词数量。注意，只有一个、两个或三个单词的句子都是悲伤的，而四个和五个单词的句子是快乐的。那就是分类器！它将三个单词或更少的句子分类为悲伤，将四个或更多单词的句子分类为快乐。我们还可以用更数学的方式表达这一点。
- en: Sentiment analysis classifier
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析分类器
- en: 'Given a sentence, assign the following scores to the words:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个句子，给以下单词分配以下分数：
- en: 'Scores:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 分数：
- en: '*Crack*: one point'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Crack*：一分'
- en: '*Doink*: one point'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Doink*：一分'
- en: 'Rule:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 规则：
- en: Calculate the score of the sentence by adding the scores of all the words on
    it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将句子上所有单词的分数相加来计算句子的分数。
- en: If the score is four or more, predict that the sentence is happy.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数是4或以上，预测该句子是快乐的。
- en: If the score is three or less, predict that the sentence is sad.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数是3或以下，预测该句子是悲伤的。
- en: To make it simpler, let’s slightly change the rule by using a cutoff of 3.5.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让它更简单，让我们通过使用3.5的截止值稍微改变一下规则。
- en: 'Rule:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 规则：
- en: Calculate the score of the sentence by adding the scores of all the words on
    it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算句子上所有单词的分数来计算句子的分数。
- en: If the score is 3.5 or more, predict that the sentence is happy.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数为 3.5 或更多，预测句子是快乐的。
- en: If the score is less than 3.5, predict that the sentence is sad.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数小于 3.5，预测句子是悲伤的。
- en: This classifier again corresponds to a line, and that line is illustrated in
    figure 5.5.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器再次对应于一条线，该线在图 5.5 中进行了说明。
- en: '![](../Images/5-5.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-5.png)'
- en: Figure 5.5 The classifier for the new dataset of aliens. It is again a line
    that splits the happy and the sad aliens.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 新数据集的外星人分类器。它再次是一条分割快乐和悲伤外星人的线。
- en: In the previous example, we concluded that the word *aack* was a happy word,
    and the word *beep* was a sad one. What happens in this example? It seems that
    both words *crack* and *doink* are happy, because their scores are both positive.
    Why, then, is the sentence “*Crack doink*” a sad sentence? It doesn’t have enough
    words. The aliens on this planet have a distinctive personality. The aliens that
    don’t speak much are sad, and those who speak a lot are happy. The way we can
    interpret it is that the aliens on this planet are inherently sad, but they can
    get out of the sadness by talking a lot.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，我们得出结论，单词 *aack* 是一个快乐的词，而单词 *beep* 是一个悲伤的词。在这个例子中会发生什么？看起来单词 *crack*
    和 *doink* 都是快乐的，因为它们的分数都是正的。那么，句子 “*Crack doink*” 为什么是一个悲伤的句子？因为它没有足够的单词。这个星球上的外星人有着独特的个性。说话不多的是悲伤的外星人，而说话很多的是快乐的外星人。我们可以这样解释：这个星球上的外星人天生就是悲伤的，但他们可以通过多说话来摆脱悲伤。
- en: 'Another important element in this classifier is the cutoff, or threshold, of
    3.5\. This threshold is used by the classifier to make the prediction, because
    sentences with scores higher than or equal to the threshold are classified as
    happy, and sentences with scores lower than the threshold are classified as sad.
    However, thresholds are not common, and instead we use the notion of a *bias*.
    The bias is the negative of the threshold, and we add it to the score. This way,
    the classifier can calculate the score and return a prediction of happy if the
    score is nonnegative, or sad if it is negative. As a final change in notation,
    we’ll call the scores of the words *weights*. Our classifier can be expressed
    as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个分类器中，另一个重要元素是截止值或阈值 3.5。分类器使用这个阈值进行预测，因为分数高于或等于阈值的句子被分类为快乐，而分数低于阈值的句子被分类为悲伤。然而，阈值并不常见，我们而是使用
    *偏置* 的概念。偏置是阈值的相反数，我们将其加到分数上。这样，分类器可以计算分数，如果分数是非负的，则返回快乐的预测，如果分数是负的，则返回悲伤的预测。作为最后的符号变化，我们将单词的分数称为
    *权重*。我们的分类器可以表示如下：
- en: Sentiment analysis classifier
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析分类器
- en: 'Given a sentence, assign the following weights and bias to the words:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个句子，为单词分配以下权重和偏置：
- en: 'Weights:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: '*Crack*: one point'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Crack*: 一分'
- en: '*Doink*: one point'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Doink*: 一分'
- en: '**Bias**: –3.5 points'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏置**： –3.5 分'
- en: 'Rule:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 规则：
- en: Calculate the score of the sentence by adding the weights of all the words on
    it and the bias.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将句子上所有单词的权重和偏置相加来计算句子的分数。
- en: If the score is greater than or equal to zero, predict that the sentence is
    happy.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数大于或等于零，预测句子是快乐的。
- en: If the score is less than zero, predict that the sentence is sad.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分数小于零，预测句子是悲伤的。
- en: 'The equation of the score of the classifier, and also of the line in figure
    5.5, follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的分数方程，以及图 5.5 中的直线方程如下：
- en: '#crack + #doink – 3.5 = 0'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#crack + #doink – 3.5 = 0'
- en: 'Notice that defining a perceptron classifier with a threshold of 3.5 and with
    a bias of –3.5 is the same thing, because the following two equations are equivalent:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，定义一个阈值为 3.5 且偏置为 –3.5 的感知器分类器与以下两个等式相同，因为这两个等式是等价的：
- en: '#crack + #doink ≥ 3.5'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#crack + #doink ≥ 3.5'
- en: '#crack + #doink – 3.5 ≥ 0'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#crack + #doink – 3.5 ≥ 0'
- en: We can use a similar notation as in the previous section, where *x*[crack] is
    the number of appearances of the word *crack* and *x*[doink] is the number of
    appearances of the word *doink*. Thus, the equation of the line in figure 3.5
    can be written as
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与上一节类似的表达法，其中 *x*[crack] 是单词 *crack* 出现的次数，*x*[doink] 是单词 *doink* 出现的次数。因此，图
    3.5 中的直线方程可以写成
- en: '*x*[crack] + *x*[doink] – 3.5 = 0.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[crack] + *x*[doink] – 3.5 = 0.'
- en: 'This line also divides the plane into positive and negative zones, defined
    as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这条线也将平面分为正区域和负区域，定义如下：
- en: '**Positive zone**: the area on the plane for which *x*[crack] + *x*[doink]
    – 3.5 ≥ 0'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**正区域**：平面上 *x*[crack] + *x*[doink] – 3.5 ≥ 0 的区域'
- en: '**Negative zone**: the area on the plane for which *x*[crack] + *x*[doink]
    – 3.5 < 0'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**负区域**：平面上满足 *x*[crack] + *x*[doink] – 3.5 < 0 的区域'
- en: Does our classifier need to be correct all the time? No
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器是否需要始终正确？不
- en: In the previous two examples, we built a classifier that was correct all the
    time. In other words, the classifier classified the two happy sentences as happy
    and the two sad sentences as sad. This is not something one finds often in practice,
    especially in datasets with many points. However, the goal of the classifier is
    to classify the points as best as possible. In figure 5.6, we can see a dataset
    with 17 points (eight happy and nine sad) that is impossible to perfectly split
    into two using a single line. However, the line in the picture does a good job,
    only classifying three points incorrectly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个例子中，我们构建了一个始终正确的分类器。换句话说，这个分类器将两个快乐的句子分类为快乐，将两个悲伤的句子分类为悲伤。这在实践中并不常见，尤其是在包含许多点的数据集中。然而，分类器的目标是将点尽可能准确地分类。在图5.6中，我们可以看到一个包含17个点（8个快乐和9个悲伤）的数据集，使用单条线无法完美地将其分成两部分。然而，图中的线做得很好，只错误地分类了三个点。
- en: '![](../Images/5-6.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-6.png)'
- en: 'Figure 5.6 This line splits the dataset well. Note that it makes only three
    mistakes: two on the happy zone and one on the sad zone.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 这条线很好地分割了数据集。注意，它只犯了一个错误：两个在快乐区域，一个在悲伤区域。
- en: A more general classifier and a slightly different way to define lines
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 更通用的分类器和定义线的不同方法
- en: 'In this section, we get a more general view of the perceptron classifier. For
    a moment, let’s call our words 1 and 2, and the variables keeping track of their
    appearances *x*[1] and *x*[2]. The equations of the two previous classifiers follow:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们获得了对感知器分类器的更一般性的看法。暂时让我们称我们的单词1和2，以及跟踪它们出现的变量为 *x*[1] 和 *x*[2]。前两个分类器的方程如下：
- en: '*x*[1] – *x*[2] = 0'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[1] – *x*[2] = 0'
- en: '*x*[1] + *x*[2] – 3.5 = 0'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[1] + *x*[2] – 3.5 = 0'
- en: 'The general form of the equation of a perceptron classifier is *ax*[1] + *bx*[2]
    + *c* = 0, where *a* is the score of the word 1, *b* the score of the word 2,
    and *c* is the bias. This equation corresponds to a line that splits the plane
    into two zones as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器分类器的方程的一般形式是 *ax*[1] + *bx*[2] + *c* = 0，其中 *a* 是单词1的得分，*b* 是单词2的得分，*c* 是偏置。这个方程对应于一条线，将平面分成两个区域，如下所示：
- en: '**Positive zone**: the zone on the plane for which *ax*[1] + *bx*[2] + *c*
    ≥ 0'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**正区域**：平面上满足 *ax*[1] + *bx*[2] + *c* ≥ 0 的区域'
- en: '**Negative zone**: the zone on the plane for which *ax*[1] + *bx*[2] + *c*
    < 0'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**负区域**：平面上满足 *ax*[1] + *bx*[2] + *c* < 0 的区域'
- en: For example, if the word 1 has a score of 4, the word 2 has a score of –2.5,
    and the bias is 1.8, then the equation of this classifier is
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果单词1的得分为4，单词2的得分为-2.5，偏置为1.8，那么这个分类器的方程是
- en: 4*x*[1] – 2.5*x*[2] + 1.8 = 0,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 4*x*[1] – 2.5*x*[2] + 1.8 = 0,
- en: and the positive and negative zones are those where 4*x*[1] – 2.5*x*[2] + 1.8
    ≥ 0 and 4*x*[1] – 2.5*x*[2] + 1.8 < 0, respectively.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以及正负区域分别是满足 4*x*[1] – 2.5*x*[2] + 1.8 ≥ 0 和 4*x*[1] – 2.5*x*[2] + 1.8 < 0 的区域。
- en: 'aside: Equations of lines and zones in the plane In chapter 3, we defined lines
    using the equation *y* = *mx* + *b* on a plane where the axes are *x* and *y*.
    In this chapter, we define them with the equation *ax*[1] + *bx*[2] + *c* = 0
    on a plane where the axes are *x*[1] and *x*[2]. How are they different? They
    are both perfectly valid ways to define a line. However, whereas the first equation
    is useful for linear regression models, the second equation is useful for perceptron
    models (and, in general, for other classification algorithms, such as logistic
    regression, neural networks, and support vector machines, that we’ll see in chapters
    6, 10, and 11, respectively). Why is this equation better for perceptron models?
    Some advantages follow:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 旁白：平面上线和区域的方程在第三章中，我们使用方程 *y* = *mx* + *b* 在 *x* 和 *y* 轴的平面上定义了线。在本章中，我们使用方程
    *ax*[1] + *bx*[2] + *c* = 0 在 *x*[1] 和 *x*[2] 轴的平面上定义它们。它们有什么不同？它们都是定义线的完全有效的方法。然而，第一个方程对线性回归模型很有用，而第二个方程对感知器模型（以及一般地，对其他分类算法，如逻辑回归、神经网络和支持向量机，我们将在第6、10和11章分别看到）很有用。为什么这个方程对感知器模型更好？以下是一些优点：
- en: The equation *ax*[1] + *bx*[2] + *c* = 0 not only defines a line but also clearly
    defines the two zones, positive and negative. If we wanted to have the same line,
    except with the positive and negative regions flipped, we would consider the equation
    –*ax*[1] – *bx*[2] – *c* = 0.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程*ax*[1] + *bx*[2] + *c* = 0不仅定义了一条线，而且清楚地定义了两个区域，正区域和负区域。如果我们想要有相同的线，但正负区域相反，我们会考虑方程
    –*ax*[1] – *bx*[2] – *c* = 0。
- en: Using the equation *ax*[1] + *bx*[2] + *c* = 0, we can draw vertical lines,
    because the equation of a vertical line is *x* = *c* or 1*x*[1] + 0*x*[2] – *c*
    = 0\. Although vertical lines don’t often show up in linear regression models,
    they do show up in classification models.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用方程*ax*[1] + *bx*[2] + *c* = 0，我们可以画出垂直线，因为垂直线的方程是*x* = *c*或1*x*[1] + 0*x*[2]
    – *c* = 0。尽管垂直线在线性回归模型中不常见，但在分类模型中确实会出现。
- en: '![](../Images/5-7.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-7.png)'
- en: Figure 5.7 A classifier is defined by a line with the equation *ax*[1] + *bx*[2]
    + *c* = 0, a positive zone, and a negative zone. If we want to flip the positive
    and negative zones, all we need to do is negate the weights and the bias. On the
    left we have the classifier with equation *ax*[1] + *bx*[2] + *c* = 0\. On the
    right, the classifier with flipped zones and the equation –*ax*[1] – *bx*[2] –
    *c* = 0.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 一个分类器由方程*ax*[1] + *bx*[2] + *c* = 0、正区域和负区域定义。如果我们想翻转正负区域，我们只需要取权重和偏差的相反数。在左边，我们有方程*ax*[1]
    + *bx*[2] + *c* = 0的分类器。在右边，区域翻转且方程为 –*ax*[1] – *bx*[2] – *c* = 0的分类器。
- en: 'The step function and activation functions: A condensed way to get predictions'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 步进函数和激活函数：获取预测的紧凑方式
- en: In this section, we learn a mathematical shortcut to obtain the predictions.
    Before learning this, however, we need to turn all our data into numbers. Notice
    that the labels in our dataset are “happy” and “sad.” We record these as 1 and
    0, respectively.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了一种数学捷径来获得预测。然而，在学习这个之前，我们需要将所有数据转换为数字。注意，我们数据集中的标签是“快乐”和“悲伤”。我们分别记录为1和0。
- en: 'Both perceptron classifiers that we’ve built in this chapter have been defined
    using an if statement. Namely, the classifier predicts “happy” or “sad” based
    on the total score of the sentence; if this score is positive or zero, the classifier
    predicts “happy,” and if it is negative, the classifier predicts “sad.” We have
    a more direct way to turn the score into a prediction: using the *step function*.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们构建的两个感知器分类器都是使用if语句定义的。具体来说，分类器根据句子的总得分预测“快乐”或“悲伤”；如果这个得分是正数或零，分类器预测“快乐”，如果是负数，分类器预测“悲伤”。我们有一个更直接的方法将得分转换为预测：使用*步进函数*。
- en: step function The function that returns a 1 if the output is nonnegative and
    a 0 if the output is negative. In other words, if the input is *x*, then
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 步进函数：当输出非负时返回1，当输出为负时返回0的函数。换句话说，如果输入是*x*，那么
- en: '*step*(*x*) = 1 if *x* ≥ 0'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*step*(*x*) = 1 if *x* ≥ 0'
- en: '*step*(*x*) = 0 if *x* < 0'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*step*(*x*) = 0 if *x* < 0'
- en: Figure 5.8 shows the graph of the step function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8显示了步进函数的图形。
- en: '![](../Images/5-8.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-8.png)'
- en: Figure 5.8 The step function is useful in the study of perceptron models. The
    output of the step function is 0 when the input is negative and 1 otherwise.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 步进函数在感知器模型的研究中很有用。当输入为负时，步进函数的输出为0，否则为1。
- en: With the step function, we can express the output of the perceptron classifier
    easily. In our dataset, we use the variable *y* to refer to the labels, just as
    we did in chapter 3\. The prediction that the model makes for the label is denoted
    *y**ˆ.* The output of the perceptron model is written in condensed form as
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步进函数，我们可以轻松地表达感知器分类器的输出。在我们的数据集中，我们使用变量*y*来指代标签，就像我们在第3章中所做的那样。模型对标签的预测表示为*y**ˆ*。感知器模型的输出以紧凑的形式表示为
- en: '*ŷ* = *step*(*ax*[1] + *bx*[2] + *c*).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *step*(*ax*[1] + *bx*[2] + *c*).'
- en: The step function is a specific case of an *activation function*. The activation
    function is an important concept in machine learning, especially in deep learning
    and will appear again in chapters 6 and 10\. The formal definition of an activation
    function will come later, because its full power is used in building neural networks.
    But for now, we can think of the activation function as a function we can use
    to turn the scores into a prediction.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 步进函数是*激活函数*的一个特例。激活函数是机器学习中的一个重要概念，尤其是在深度学习中，它将在第6章和第10章中再次出现。激活函数的正式定义将在稍后给出，因为它的全部力量是在构建神经网络时使用的。但就目前而言，我们可以将激活函数视为一个可以将得分转换为预测的函数。
- en: What happens if I have more than two words? General definition of the perceptron
    classifier
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我有超过两个单词会发生什么？感知器分类器的一般定义
- en: 'In the two alien examples at the beginning of this section, we built perceptron
    classifiers for languages with two words. But we can build classifiers with as
    many words as we want. For example, if we had a language with three words, say,
    *aack*, *beep*, and *crack*, the classifier would make predictions according to
    the following formula:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节开头提到的两个外星例子中，我们为拥有两个单词的语言构建了感知器分类器。但我们可以构建拥有任意多个单词的分类器。例如，如果我们有一个包含三个单词的语言，比如
    *aack*，*beep* 和 *crack*，分类器将根据以下公式进行预测：
- en: '*ŷ* = *step*(*ax*[aack] + *bx*[beep] + *cx*[crack] + *d*),'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *step*(*ax*[aack] + *bx*[beep] + *cx*[crack] + *d*),'
- en: where *a*, *b*, and *c* are the weights of the words *aack*, *beep*, and *crack*,
    respectively, and *d* is the bias.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a*，*b* 和 *c* 分别是单词 *aack*，*beep* 和 *crack* 的权重，*d* 是偏差。
- en: As we saw, the sentiment analysis perceptron classifiers for languages with
    two words can be expressed as a line in the plane that splits the happy and the
    sad points. Sentiment analysis classifiers for languages with three words can
    also be represented geometrically. We can imagine the points as living in three-dimensional
    space. In this case, each of the axes corresponds to each of the words *aack*,
    *beep*, and *crack*, and a sentence corresponds to a point in space for which
    its three coordinates are the number of appearances of the three words. Figure
    5.9 illustrates an example in which the sentence containing *aack* five times,
    *beep* eight times, and *crack* three times, corresponds to the point with coordinates
    (5, 8, 3).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，对于拥有两个单词的语言的情感分析感知器分类器可以表示为分割快乐和悲伤点的平面上的线。对于拥有三个单词的语言的情感分析分类器也可以用几何方式表示。我们可以想象这些点生活在三维空间中。在这种情况下，每个轴对应于单词
    *aack*，*beep* 和 *crack*，一个句子对应于空间中的一个点，其三个坐标是三个单词出现的次数。图 5.9 展示了一个例子，其中包含 *aack*
    五次，*beep* 八次和 *crack* 三次的句子对应于坐标为 (5, 8, 3) 的点。
- en: '![](../Images/5-9.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-9.png)'
- en: Figure 5.9 A sentence with three words can be plotted as a point in space. In
    this case, a sentence with the word aack five times, beep eight times, and crack
    three times is plotted in the point with coordinates (5,8,3).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 将包含三个单词的句子绘制为空间中的一个点。在这种情况下，包含单词 aack 五次，beep 八次和 crack 三次的句子绘制在坐标为 (5,8,3)
    的点上。
- en: The way to separate these points is using a plane. The equation with a plane
    is precisely *ax*[aack] + *bx*[beep] + *cx*[crack] + *d*, and this plane is illustrated
    in figure 5.10.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用平面来分离这些点。平面的方程是 *ax*[aack] + *bx*[beep] + *cx*[crack] + *d*，这个平面在图 5.10 中被展示出来。
- en: '![](../Images/5-10.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-10.png)'
- en: Figure 5.10 A dataset of sentences with three words is plotted in three dimensions.
    The classifier is represented by a plane that splits the space into two regions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 将三个单词的句子数据集绘制在三维空间中。分类器用一个分割空间的平面表示。
- en: We can build sentiment analysis perceptron classifiers for languages with as
    many words as we can. Say our language has *n* words, that we call 1, 2, … , *n*.
    Our dataset consists of *m* sentences, which we call *x*^((1)), *x*^((2)), … ,
    *x*^(^m^). Each sentence *x*^((1)) comes with a label *y*[i], which is 1 if the
    sentence is happy and 0 if it is sad. The way we record each sentence is using
    the number of appearances of each of the *n* words. Therefore, each sentence corresponds
    to a row in our dataset and can be seen as a vector, or an *n*-tuple of numbers
    *x*^(^i^) = (*x*[1]^(^i^), *x*[2]^(^i^), … , *x*[n]^(^i^)), where *x*[j]^(^i^)
    is the number of appearances of the word *j* in the *i*-th sentence.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为拥有尽可能多单词的语言构建情感分析感知器分类器。假设我们的语言有 *n* 个单词，我们称之为 1, 2, … , *n*。我们的数据集由 *m*
    个句子组成，我们称之为 *x*^((1)), *x*^((2)), … , *x*^(^m^)。每个句子 *x*^((1)) 都有一个标签 *y*[i]，如果句子是快乐的，则标签为
    1，如果是悲伤的，则标签为 0。我们记录每个句子的方式是使用每个 *n* 个单词出现的次数。因此，每个句子对应数据集中的一行，可以看作是一个向量，或一个由数字
    *x*^(^i^) = (*x*[1]^(^i^), *x*[2]^(^i^), … , *x*[n]^(^i^)) 组成的 *n*-元组，其中 *x*[j]^(^i^)
    是单词 *j* 在第 *i* 个句子中出现的次数。
- en: The perceptron classifier consists of *n* weights (scores), one for each of
    the *n* words in our language, and a bias. The weights are denoted *w*[i] and
    the bias *b*. Thus, the prediction that the classifier makes for the sentence
    *x*^(^i^) is
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器分类器由 *n* 个权重（分数）组成，每个权重对应我们语言中的 *n* 个单词，还有一个偏差。权重表示为 *w*[i] 和偏差 *b*。因此，分类器对句子
    *x*^(^i^) 的预测是
- en: '*ŷ*[i] = *step*(*w*[1]*x*[1]^(^i^) + *w*[2]*x*[2]^(^i^) + … +*w*[n]*x*[n]^(^i^)
    + *b*).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ*[i] = *step*(*w*[1]*x*[1]^(^i^) + *w*[2]*x*[2]^(^i^) + … +*w*[n]*x*[n]^(^i^)
    + *b*).'
- en: In the same way as the classifiers with two words can be represented geometrically
    as a line that cuts the plane into two regions, and the classifiers with three
    words can be represented as a plane that cuts the three-dimensional space into
    two regions, classifiers with *n* words can also be represented geometrically.
    Unfortunately, we need *n*-dimensions to see them. Humans can see only three dimensions,
    so we may have to imagine an (*n*-1)-dimensional plane (called a hyperplane) cutting
    the *n*-dimensional space into two regions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 与只有两个单词的分类器可以用一条线在平面上分割成两个区域一样，可以用一个平面在三维空间中分割成两个区域，具有*n*个单词的分类器也可以用几何表示。不幸的是，我们需要*n*维来看到它们。人类只能看到三维，所以我们可能需要想象一个(*n*-1)-维平面（称为超平面）将*n*-维空间分割成两个区域。
- en: 'However, the fact that we can’t imagine them geometrically doesn’t mean we
    can’t have a good idea of how they work. Imagine if our classifier is built on
    the English language. Every single word gets a weight assigned. That is equivalent
    to going through the dictionary and assigning a happiness score to each of the
    words. The result could look something like this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们无法从几何上想象它们并不意味着我们不能很好地理解它们是如何工作的。想象一下，如果我们的分类器建立在英语语言的基础上。每个单词都会被分配一个权重。这相当于遍历词典，并为每个单词分配一个快乐分数。结果可能看起来像这样：
- en: 'Weights (scores):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '权重（分数）:'
- en: 'A: 0.1 points'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'A: 0.1 points'
- en: 'Aardvark: 0.2 points'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aardvark: 0.2 points'
- en: 'Aargh: –4 points'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aargh: –4 points'
- en: …
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: …
- en: 'Joy: 10 points'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joy: 10 points'
- en: …
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: …
- en: 'Suffering: –8.5 points'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Suffering: –8.5 points'
- en: '...'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: 'Zygote: 0.4 points'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zygote: 0.4 points'
- en: 'Bias:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差：
- en: –2.3 points
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: –2.3 points
- en: If those were the weights and bias of the classifier, to predict whether a sentence
    is happy or sad, we add the scores of all the words on it (with repetitions).
    If the result is higher than or equal to 2.3 (the negative of the bias), the sentence
    is predicted as happy; otherwise, it is predicted as sad.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些是分类器的权重和偏差，要预测一个句子是快乐还是悲伤，我们只需加上句子上所有单词的分数（包括重复的）。如果结果是高于或等于2.3（偏差的负值），则预测该句子为快乐；否则，预测为悲伤。
- en: Furthermore, this notation works for any example, not only sentiment analysis.
    If we have a different problem with different data points, features, and labels,
    we can encode it using the same variables. For example, if we have a medical application
    where we are trying to predict whether a patient is sick or healthy using *n*
    weights and a bias, we can still call the labels *y*, the features *x*[i], the
    weights *w*[i], and the bias *b*.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种表示法适用于任何例子，而不仅仅是情感分析。如果我们有一个不同的问题，不同的数据点、特征和标签，我们可以使用相同的变量来编码它。例如，如果我们有一个医疗应用，我们试图使用*n*个权重和偏差来预测患者是生病还是健康，我们仍然可以称标签为*y*，特征为*x*[i]，权重为*w*[i]，偏差为*b*。
- en: The bias, the *y*-intercept, and the inherent mood of a quiet alien
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差、*y*-截距和安静外星人的内在情绪
- en: So far we have a good idea of what the weights of the classifier mean. Words
    with positive weights are happy, and words with negative words are sad. Words
    with very small weights (whether positive or negative) are more neutral words.
    However, what does the bias mean?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经很好地理解了分类器的权重意味着什么。具有正权重的单词是快乐的，具有负权重的单词是悲伤的。具有非常小的权重（无论是正还是负）的单词是更中性的单词。然而，偏差意味着什么呢？
- en: In chapter 3, we specified that the bias in a regression model for house prices
    was the base price of a house. In other words, it is the predicted price of a
    hypothetical house with zero rooms (a studio?). In the perceptron model, the bias
    can be interpreted as the score of the empty sentence. In other words, if an alien
    says absolutely nothing, is this alien happy or sad? If a sentence has no words,
    its score is precisely the bias. Thus, if the bias is positive, the alien that
    says nothing is happy, and if the bias is negative, that same alien is sad.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们指定了房价回归模型中的偏差是房屋的基础价格。换句话说，它是假设房屋房间数为零（一个工作室？）的假设房屋的预测价格。在感知器模型中，偏差可以解释为空句子的分数。换句话说，如果一个外星人什么都没说，这个外星人快乐还是悲伤？如果一个句子没有单词，它的分数就是偏差。因此，如果偏差是正的，那么什么都没说的外星人就是快乐的，如果偏差是负的，那么同样的外星人就是悲伤的。
- en: Geometrically, the difference between a positive and negative bias lies in the
    location of the origin (the point with coordinates (0,0)) with respect to the
    classifier. This is because the point with coordinates (0,0) corresponds to the
    sentence with no words. In classifiers with a positive bias, the origin lies in
    the positive zone, whereas in classifiers with a negative bias, the origin lies
    in the negative zone, as illustrated in figure 5.11.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 几何上，正偏见和负偏见的区别在于原点（坐标为(0,0)的点）相对于分类器的位置。这是因为坐标为(0,0)的点对应于没有单词的句子。在具有正偏见的分类器中，原点位于正区域，而在具有负偏见的分类器中，原点位于负区域，如图5.11所示。
- en: '![](../Images/5-11.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-11.png)'
- en: 'Figure 5.11 Left: the classifier has a negative bias, or a positive *threshold*
    (*y*-intercept). This means that the alien that doesn’t say anything falls in
    the sad zone and is classified as sad. Right: The classifier has a positive bias,
    or a negative threshold. This means that the alien that doesn’t say anything falls
    in the happy zone and is classified as happy.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 左：分类器具有负偏见，或正*阈值*（*y*截距）。这意味着不说任何话的外星人落入悲伤区域，并被分类为悲伤。右：分类器具有正偏见，或负阈值。这意味着不说任何话的外星人落入快乐区域，并被分类为快乐。
- en: 'Can we think of sentiment analysis datasets in which the bias is positive or
    negative? What about the following two examples:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否考虑具有正偏见或负偏见的情感分析数据集？以下两个例子如何？
- en: '**Example 1 (positive bias)**: a dataset of online reviews of a product'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例1（正偏见）**：一个产品的在线评论数据集'
- en: Imagine a dataset in which we record all the reviews of a particular product
    on Amazon. Some of them are positive, and some of them are negative, according
    to the number of stars they receive. What do you think the score would be for
    an empty review? From my experience, bad reviews tend to contain lots of words,
    because the customer is upset, and they describe their negative experience. However,
    many of the positive reviews are empty—the customer simply gives a good score,
    without the need to explain why they enjoyed the product. Therefore, this classifier
    probably has a positive bias.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个数据集，我们记录了在亚马逊上特定产品的所有评论。其中一些是正面的，一些是负面的，根据它们收到的星级来区分。你认为空评论的评分会是什么？根据我的经验，差评往往包含很多单词，因为客户感到沮丧，他们描述了他们的负面经历。然而，许多正面评论是空的——客户只是给出了一个好的评分，而不需要解释他们为什么喜欢这个产品。因此，这个分类器可能具有正偏见。
- en: '**Example 2 (negative bias)**: a dataset of conversations with friends'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例2（负偏见）**：与朋友对话的数据集'
- en: Imagine that we record all our conversations with friends and classify them
    as happy or sad conversations. If one day we bump into a friend, and our friend
    says absolutely nothing, we imagine that they are mad at us or that they are very
    upset. Therefore, the empty sentence is classified as sad. This means that this
    classifier probably has a negative bias.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们记录了我们与朋友的全部对话并将它们分类为快乐或悲伤的对话。如果有一天我们遇到一个朋友，而我们的朋友说绝对没有任何话，我们想象他们可能是在生我们的气，或者他们非常沮丧。因此，空句子被分类为悲伤。这意味着这个分类器可能具有负偏见。
- en: How do we determine whether a classifier is good or bad? The error function
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何确定一个分类器是好是坏？误差函数
- en: 'Now that we have defined what a perceptron classifier is, our next goal is
    to understand how to train it—in other words, how do we find the perceptron classifier
    that best fits our data? But before learning how to train perceptrons, we need
    to learn an important concept: how to evaluate them. More specifically, in this
    section we learn a useful error function that will tell us whether a perceptron
    classifier fits our data well. In the same way that the absolute and square errors
    worked for linear regression in chapter 3, this new error function will be large
    for classifiers that don’t fit the data well and small for those that fit the
    data well.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了什么是感知器分类器，我们的下一个目标是了解如何训练它——换句话说，我们如何找到最适合我们数据的感知器分类器？但在学习如何训练感知器之前，我们需要学习一个重要的概念：如何评估它们。更具体地说，在本节中，我们学习一个有用的误差函数，它将告诉我们感知器分类器是否适合我们的数据。与第3章中绝对误差和平方误差对线性回归有效一样，这个新的误差函数对于不适合数据的分类器将很大，而对于适合数据的分类器将很小。
- en: How to compare classifiers? The error function
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如何比较分类器？误差函数
- en: In this section, we learn how to build an effective error function that helps
    us determine how good a particular perceptron classifier is. First, let’s test
    our intuition. Figure 5.12 shows two different perceptron classifiers on the same
    dataset. The classifiers are represented as a line with two well-defined sides,
    happy and sad. Clearly, the one on the left is a bad classifier, and the one on
    the right is good. Can we come up with a measure of how good they are? In other
    words, can we assign a number to each one of them, in a way that the one on the
    left is assigned a high number and the one on the right a low number?
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习如何构建一个有效的错误函数，帮助我们确定特定感知器分类器的优劣。首先，让我们测试我们的直觉。图5.12显示了同一数据集上的两个不同的感知器分类器。分类器被表示为一条有两条明确边界的线，一边是快乐，另一边是悲伤。显然，左侧的是一个不好的分类器，而右侧的是一个好的分类器。我们能想出一个衡量它们好坏的方法吗？换句话说，我们能给每个分类器分配一个数字，使得左侧的分类器分配一个高数字，而右侧的分类器分配一个低数字？
- en: '![](../Images/5-12.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12](../Images/5-12.png)'
- en: 'Figure 5.12 Left: a bad classifier, which doesn’t really split the points well.
    Right: a good classifier. Can we think of an error function that assigns a high
    number to the bad classifier and a low number to the good one?'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 左：一个不好的分类器，它并没有很好地分割点。右：一个好的分类器。我们能想出一个错误函数，将高数字分配给不好的分类器，将低数字分配给好的分类器吗？
- en: 'Next, we see different answers to this question, all with some pros and cons.
    One of them (spoiler: the third one) is the one we use to train perceptrons.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看到对这个问题的不同回答，每个都有其优缺点。其中之一（剧透：第三个）是我们用来训练感知器的。
- en: 'Error function 1: Number of errors'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 错误函数1：错误数量
- en: The simplest way to evaluate a classifier is by counting the number of mistakes
    it makes—in other words, by counting the number of points that it classifies incorrectly.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 评估分类器的最简单方法是通过计算它犯的错误数量——换句话说，通过计算它错误分类的点数。
- en: In this case, the classifier on the left has an error of 8, because it erroneously
    predicts four happy points as sad, and four sad points as happy. The good classifier
    has an error of 3, because it erroneously predicts one happy point as sad, and
    two sad points as happy. This is illustrated in figure 5.13.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，左侧的分类器有一个8的错误率，因为它错误地将四个快乐点预测为悲伤，并将四个悲伤点预测为快乐。好的分类器有一个3的错误率，因为它错误地将一个快乐点预测为悲伤，并将两个悲伤点预测为快乐。这如图5.13所示。
- en: '![](../Images/5-13.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13](../Images/5-13.png)'
- en: Figure 5.13 We evaluate the two classifiers by counting the number of points
    that each one of them misclassifies. The classifier on the left misclassifies
    eight points, whereas the classifier on the right misclassifies three points.
    Thus, we conclude that the classifier on the right is a better one for our dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 我们通过计算每个分类器错误分类的点数来评估这两个分类器。左侧的分类器错误分类了八个点，而右侧的分类器错误分类了三个点。因此，我们得出结论，右侧的分类器对于我们的数据集来说是一个更好的选择。
- en: This is a good error function, but it’s not a great error function. Why? It
    tells us when there is an error, but it doesn’t measure the gravity of the error.
    For example, if a sentence is sad, and the classifier gave it a score of 1, the
    classifier made a mistake. However, if another classifier gave it a score of 100,
    this classifier made a much bigger mistake. The way to see this geometrically
    is in figure 5.14\. In this image, both classifiers misclassified a sad point
    by predicting that it is happy. However, the classifier on the left located the
    line close to the point, which means that the sad point is not too far from the
    sad zone. The classifier on the right, in contrast, has located the point very
    far from its sad zone.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好的错误函数，但并不是一个很好的错误函数。为什么？它告诉我们何时有错误，但它不衡量错误的严重程度。例如，如果一个句子是悲伤的，而分类器给它评分为1，那么分类器就犯了错误。然而，如果另一个分类器给它评分为100，那么这个分类器就犯了更大的错误。从几何上观察这一点，如图5.14所示。在这张图片中，两个分类器都将一个悲伤点错误分类为快乐。然而，左侧的分类器将线靠近点，这意味着悲伤点离悲伤区域不远。相比之下，右侧的分类器将点定位得非常远离其悲伤区域。
- en: '![](../Images/5-14.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14](../Images/5-14.png)'
- en: Figure 5.14 The two classifiers misclassify the point. However, the classifier
    on the right made a much bigger mistake than the classifier on the left. The point
    on the left is not far from the boundary, and thus, it is not very far from the
    sad zone. However, the point on the right is very far from the sad zone. Ideally,
    we would like an error function that assigns a higher error to the classifier
    in the right than to the classifier in the left.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 两个分类器误分类了点。然而，右侧的分类器犯的错误比左侧的分类器大得多。左侧的点离边界不远，因此它离悲伤区域也不远。然而，右侧的点离悲伤区域很远。理想情况下，我们希望一个误差函数将更高的误差分配给右侧的分类器，而不是左侧的分类器。
- en: Why do we care about measuring how bad an error is? Wouldn’t it be enough to
    count them? Recall what we did in chapter 3 with the linear regression algorithm.
    More specifically, recall the section “Gradient descent,” where we used gradient
    descent to reduce this error. The way to reduce an error is by decreasing it in
    small amounts, until we reach a point where the error is small. In the linear
    regression algorithm, we wiggled the line small amounts and picked the direction
    in which the error decreased the most. If our error is calculated by counting
    the number of misclassified points, then this error will take only integer values.
    If we wiggle the line a small amount, the error may not decrease at all, and we
    don’t know in which direction to move. The goal of gradient descent is to minimize
    a function by taking small steps in the direction in which the function decreases
    the most. If the function takes only integer values, this is equivalent to trying
    to descend from an Aztec staircase. When we are at a flat step, we don’t know
    what step to take, because the function doesn’t decrease in any direction. This
    is illustrated in figure 5.15.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么关心测量误差的严重程度呢？仅仅计数它们不是足够了吗？回想一下我们在第3章中用到的线性回归算法。更具体地说，回想一下“梯度下降”这一节，我们使用梯度下降来减少这个误差。减少误差的方法是通过逐步减小误差，直到我们达到一个误差很小的点。在线性回归算法中，我们小幅调整直线，并选择误差减少最多的方向。如果我们通过计数误分类点的数量来计算误差，那么这个误差将只取整数值。如果我们小幅调整直线，误差可能根本不会减少，我们也不知道该朝哪个方向移动。梯度下降的目标是通过在函数减少最多的方向上采取小步骤来最小化函数。如果函数只取整数值，这相当于试图从阿兹特克阶梯上下降。当我们处于一个平坦的台阶上时，我们不知道该走哪一步，因为函数在任意方向上都不会减少。这如图5.15所示。
- en: '![](../Images/5-15.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图5-15](../Images/5-15.png)'
- en: Figure 5.15 Performing gradient descent to minimize an error function is like
    descending from a mountain by taking small steps. However, for us to do that,
    the error function must not be flat (like the one on the right), because in a
    flat error function, taking a small step will not decrease the error. A good error
    function is like the one on the left, in which we can easily see the direction
    we must use to take a step to slightly decrease the error function.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 通过小步下降来最小化误差函数就像从山上走下来一样。然而，为了做到这一点，误差函数不能是平坦的（就像右侧的那个），因为在平坦的误差函数中，小步不会减少误差。一个好的误差函数就像左侧的那个，我们可以很容易地看到必须使用哪个方向来迈步以略微减少误差函数。
- en: We need a function that measures the magnitude of an error and that assigns
    a higher error to misclassified points that are far from the boundary than to
    those that are close to it.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来衡量误差的大小，并将更高的误差分配给远离边界的误分类点，而不是那些靠近边界的点。
- en: 'Error function 2: Distance'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数2：距离
- en: A way to tell the two classifiers apart in figure 5.16 is by considering the
    perpendicular distance from the point to the line. Notice that for the classifier
    on the left, this distance is small, whereas for the classifier on the right,
    the distance is large.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.16中区分两个分类器的方法是考虑点到直线的垂直距离。注意，对于左侧的分类器，这个距离很小，而对于右侧的分类器，距离很大。
- en: 'This error function is much more effective. What this error function does follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个误差函数更有效。这个误差函数所做的是：
- en: Points that are correctly classified produce an error of 0.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确分类的点会产生0误差。
- en: Points that are misclassified produce an error equal to the distance from that
    point to the line.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误分类的点会产生一个等于该点到直线距离的误差。
- en: '![](../Images/5-16.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图5-16](../Images/5-16.png)'
- en: Figure 5.16 An effective way to measure how bad a classifier misclassifies a
    point is by measuring the perpendicular distance from the point to the line. For
    the classifier on the left, this distance is small, whereas for the classifier
    on the right, the distance is large.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 一种有效衡量分类器如何误分类点的办法是测量该点到直线的垂直距离。对于左边的分类器，这个距离较小，而对于右边的分类器，这个距离较大。
- en: Let’s go back to the two classifiers we had at the beginning of this section.
    The way we calculate the total error is by adding the errors corresponding to
    all the data points, as illustrated in figure 5.17\. This means that we look only
    at the misclassified points and add the perpendicular distances from these points
    to the line. Notice that the bad classifier has a large error, and the good classifier
    has a small error.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本节开头我们拥有的两个分类器。我们计算总误差的方法是添加所有数据点的误差，如图5.17所示。这意味着我们只关注误分类点，并添加这些点到直线的垂直距离。请注意，不良分类器的误差较大，而良好分类器的误差较小。
- en: '![](../Images/5-17.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图5-17](../Images/5-17.png)'
- en: Figure 5.17 To calculate the total error of a classifier, we add up all the
    errors, which are the perpendicular distances from the misclassified points. The
    error is large for the classifier on the left and small for the classifier on
    the right. Thus, we conclude that the classifier on the right is better.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 为了计算分类器的总误差，我们将所有误差相加，这些误差是误分类点到直线的垂直距离。左边的分类器误差较大，而右边的分类器误差较小。因此，我们得出结论，右边的分类器更好。
- en: 'This is *almost* the error function we will use. Why don’t we use this one?
    Because the distance from a point to a line is a complicated formula. It contains
    a square root, because we calculate it using the Pythagorean theorem. Square roots
    have complicated derivatives, which adds unnecessary complexity the moment we
    apply the gradient descent algorithm. We don’t need to undertake this complication,
    because we can instead create an error function that is easier to calculate yet
    still manages to capture the essence of an error function: returning an error
    for points that are misclassified and varying the magnitude based on how far the
    misclassified point is from the boundary.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎是我们将要使用的误差函数。为什么我们不使用这个呢？因为点到直线的距离是一个复杂的公式。它包含一个平方根，因为我们使用勾股定理来计算它。平方根具有复杂的导数，一旦我们应用梯度下降算法，就会增加不必要的复杂性。我们不需要承担这种复杂性，因为我们可以创建一个更容易计算但仍然能够捕捉误差函数本质的误差函数：对误分类的点返回一个误差，并根据误分类点离边界的距离来改变误差的大小。
- en: 'Error function 3: Score'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数3：得分
- en: 'In this section, we see how to build the standard error function for perceptrons,
    which we call the *perceptron error function*. First, let’s summarize the properties
    we want in an error function as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何构建感知器的标准误差函数，我们称之为**感知器误差函数**。首先，让我们总结一下我们希望在误差函数中拥有的性质如下：
- en: The error function of a correctly classified point is 0.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确分类点的误差函数为0。
- en: The error function of an incorrectly classified point is a positive number.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误分类点的误差函数是一个正数。
- en: For misclassified points close to the boundary, the error function is small.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于接近边界的误分类点，误差函数值较小。
- en: For misclassified points far from the boundary, the error function is large.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于远离边界的误分类点，误差函数值较大。
- en: It is given by a simple formula.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它由一个简单的公式给出。
- en: Recall that the classifier predicts a label of 1 for points in the positive
    zone and a label of 0 for points in the negative zone. Therefore, a misclassified
    point is either a point with label 0 in the positive zone, or a point with label
    1 in the negative zone.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，分类器对正区间的点预测标签为1，对负区间的点预测标签为0。因此，一个误分类点要么是正区间的标签为0的点，要么是负区间的标签为1的点。
- en: 'To build the perceptron error function, we use the score. In particular, we
    use the following properties of the score:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建感知器误差函数，我们使用得分。具体来说，我们使用以下得分的性质：
- en: 'Properties of the score:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 得分的性质：
- en: The points in the boundary have a score of 0.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边界上的点得分为0。
- en: The points in the positive zone have positive scores.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正区间的点具有正得分。
- en: The points in the negative zone have negative scores.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负区间的点具有负得分。
- en: The points close to the boundary have scores of low magnitude (i.e., positive
    or negative scores of low absolute value).
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接近边界的点具有低幅度的得分（即低绝对值的正或负得分）。
- en: The points far from the boundary have scores of high magnitude (i.e., positive
    or negative scores of high absolute value).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 远离边界的点具有高幅值的得分（即，高绝对值的高或低得分）。
- en: For a misclassified point, the perceptron error wants to assign a value that
    is proportional to its distance to the boundary. Therefore, the error for misclassified
    points that are far from the boundary must be high, and the error for misclassified
    points that are close to the boundary must be low. Looking at properties 4 and
    5, we can see that the absolute value of the score is always high for points far
    from the boundary and low for points close to the boundary. Thus, we define the
    error as the absolute value of the score for misclassified points.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个错误分类的点，感知机错误希望分配一个与其到边界的距离成比例的值。因此，远离边界的错误分类点的错误必须很高，而接近边界的错误分类点的错误必须很低。观察性质
    4 和 5，我们可以看到，远离边界的点的得分绝对值总是很高，而接近边界的点的得分绝对值总是很低。因此，我们定义错误为错误分类点的得分的绝对值。
- en: 'More specifically, consider the classifier that assigns weights of *a* and
    *b* to the words *aack* and *beep*, and has a bias of *c*. This classifier makes
    the prediction *ŷ* = *step*(*ax*[aack] + *bx*[beep] + *c*) to the sentence with
    *x*[aack] appearances of the word *aack* and *x*[beep] appearances of the word
    *beep*. The perceptron error is defined as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，考虑将权重 *a* 和 *b* 分配给单词 *aack* 和 *beep* 的分类器，并具有偏差 *c*。此分类器对具有 *x*[aack]
    次出现的单词 *aack* 和 *x*[beep] 次出现的单词 *beep* 的句子进行预测 *ŷ* = *step*(*ax*[aack] + *bx*[beep]
    + *c*)。感知机错误定义为以下：
- en: Perceptron error for a sentence
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 句子的感知机错误
- en: If the sentence is correctly classified, the error is 0.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果句子被正确分类，错误为 0。
- en: If the sentence is misclassified, the error is |*x*[aack] + *bx*[beep] + *c*|.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果句子被错误分类，错误为 |*x*[aack] + *bx*[beep] + *c*|.
- en: 'In the general scenario, where the notation is defined as in the section “What
    happens if I have more than two words?,” the following is the definition of the
    perceptron error:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般场景中，其中符号定义如“如果我有超过两个单词会发生什么？”一节中所述，以下为感知机错误的定义：
- en: Perceptron error for a point (general)
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 点的感知机错误（一般）
- en: If the point is correctly classified, the error is 0.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果点被正确分类，错误为 0。
- en: If the point is misclassified, the error is |*w*[1] *x*[1] +*w*[2] *x*[2] +
    … +*w*[n]*x*[n] + *b*|.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果点被错误分类，错误为 |*w*[1] *x*[1] +*w*[2] *x*[2] + … +*w*[n]*x*[n] + *b*|.
- en: 'The mean perceptron error: A way to calculate the error of an entire dataset'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 平均感知机错误：计算整个数据集错误的一种方法
- en: To calculate the perceptron error for an entire dataset, we take the average
    of all the errors corresponding to all the points. We can also take the sum if
    we choose, although in this chapter we choose the average and call it the *mean
    perceptron error*.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算整个数据集的感知机错误，我们取所有点对应的所有错误的平均值。我们也可以选择取总和，尽管在本章中我们选择取平均值并称之为*平均感知机错误*。
- en: To illustrate the mean perceptron error, let’s look at an example.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明平均感知机错误，让我们看一个例子。
- en: Example
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: Consider the dataset made of four sentences, two labeled happy and two labeled
    sad, illustrated in table 5.3.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑由四个句子组成的语料库，其中两个标记为快乐，两个标记为悲伤，如表 5.3 所示。
- en: Table 5.3 The new dataset of aliens. Again, we’ve recorded each sentence, the
    number of appearances of each word in that sentence, and the mood of the alien.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.3 外星人新的数据集。同样，我们记录了每个句子，该句子中每个单词出现的次数以及外星人的情绪。
- en: '| Sentence | Aack | Beep | Label (mood) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | 啦 | 哔 | 标签（情绪） |'
- en: '| Aack | 1 | 0 | Sad |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 啦 | 1 | 0 | 悲伤 |'
- en: '| Beep | 0 | 1 | Happy |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 哔 | 0 | 1 | 快乐 |'
- en: '| Aack beep beep beep | 1 | 3 | Happy |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 啦哔哔哔 | 1 | 3 | 快乐 |'
- en: '| Aack beep beep aack aack | 3 | 2 | Sad |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 啦哔哔 啦 啦 | 3 | 2 | 悲伤 |'
- en: 'We’ll compare the following two classifiers on this dataset:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在该数据集上比较以下两个分类器：
- en: Classifier 1
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 1
- en: 'Weights:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: '*Aack*: *a* = 1'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* 啦*：*a* = 1'
- en: '*Beep*: *b* = 2'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哔*：*b* = 2'
- en: '**Bias**: *c* = –4'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**：*c* = –4'
- en: '**Score of a sentence**: 1*x*[aack] + 2*x*[beep] – 4'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子得分**： 1*x*[aack] + 2*x*[beep] – 4'
- en: '**Prediction**: *ŷ* = *step*(1*x*[aack] + 2*x*[beep] – 4)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*ŷ* = *step*(1*x*[aack] + 2*x*[beep] – 4)'
- en: Classifier 2
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器 2
- en: 'Weights:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: '*Aack*: *a* = –1'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* 啦*：*a* = –1'
- en: '*Beep*: *b* = 1'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哔*：*b* = 1'
- en: '**Bias**: *c* = 0'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**：*c* = 0'
- en: '**Score of a sentence**: –*x*[aack] + *x*[beep]'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子得分**： –*x*[aack] + *x*[beep]'
- en: '**Prediction**: *ŷ* = *step*(–*x*[aack] + *x*[beep])'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**： *ŷ* = *step*(–*x*[aack] + *x*[beep])'
- en: The points and the classifiers can be seen in figure 5.18\. At first glance,
    which one looks like a better classifier? It appears classifier 2 is better, because
    it classifies every point correctly, whereas classifier 1 makes two mistakes.
    Now let’s calculate the errors and make sure that classifier 1 has a higher error
    than classifier 2.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 点和分类器可以在图5.18中看到。乍一看，哪一个看起来像是一个更好的分类器？看起来分类器2更好，因为它正确地将所有点分类，而分类器1有两个错误。现在让我们计算错误，并确保分类器1的错误率高于分类器2。
- en: '![](../Images/5-18.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-18.png)'
- en: Figure 5.18 On the left we have classifier 1, and on the right we have classifier
    2.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18在左边我们有分类器1，在右边我们有分类器2。
- en: The predictions for both classifiers are calculated in table 5.4.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 两个分类器的预测结果在表5.4中计算。
- en: Table 5.4 Our dataset of four sentences with their labels. For each of the two
    classifiers, we have the score and the prediction.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.4我们的四个句子的数据集及其标签。对于两个分类器中的每一个，我们都有得分和预测。
- en: '| Sentence ( *x*[aack], *x*[beep]) | Label*y*   | Classifier 1 score1*x*[aack]
    + 2*x*[beep] – 4 | Classifier 1 prediction*step*(1*x*[aack] + 2*x*[beep] – 4)
    | Classifier 1 error | Classifier 2 score– *x*[aack] + 2*x*[beep] | Classifier 2
    prediction*step*(–*x*[aack] + 2 *x*[beep]) | Classifier 2 error |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 句子 (*x*[aack], *x*[beep]) | 标签*y*   | 分类器1得分1*x*[aack] + 2*x*[beep] – 4 |
    分类器1预测*step*(1*x*[aack] + 2*x*[beep] – 4) | 分类器1错误 | 分类器2得分– *x*[aack] + 2*x*[beep]
    | 分类器2预测*step*(–*x*[aack] + 2 *x*[beep]) | 分类器2错误 |'
- en: '| (1,0) | Sad (0) | –3 | 0 (correct) | 0 | –1 | 0 (correct) | 0 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| (1,0) | 悲伤 (0) | –3 | 0 (正确) | 0 | –1 | 0 (正确) | 0 |'
- en: '| (0,1) | Happy (1) | -2 | 0 (incorrect) | 2 | 1 | 1 (correct) | 0 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| (0,1) | 快乐 (1) | -2 | 0 (错误) | 2 | 1 | 1 (正确) | 0 |'
- en: '| (1,3) | Happy (1) | 3 | 1 (correct) | 3 | 2 | 1 (correct) | 0 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| (1,3) | 快乐 (1) | 3 | 1 (正确) | 3 | 2 | 1 (正确) | 0 |'
- en: '| (3,2) | Sad (0) | 3 | 1 (incorrect) | 0 | –1 | 0 (correct) | 0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| (3,2) | 悲伤 (0) | 3 | 1 (错误) | 0 | –1 | 0 (正确) | 0 |'
- en: '| Mean perceptron error |  | 1.25 |  | 0 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 平均感知器错误 |  | 1.25 |  | 0 |'
- en: Now on to calculate the errors. Note that classifier 1 misclassified only sentences
    2 and 4\. Sentence 2 is happy, but it is misclassified as sad, and sentence 4
    is sad, but it is misclassified as happy. The error of sentence 2 is the absolute
    value of the score, or |–2| = 2\. The error of sentence 4 is the absolute value
    of the score, or |3| = 3\. The other two sentences have an error of 0, because
    they are correctly classified. Thus, the mean perceptron error of classifier 1
    is
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来计算错误。注意，分类器1只错误地将句子2和句子4分类。句子2是快乐的，但它被错误地分类为悲伤的，而句子4是悲伤的，但它被错误地分类为快乐的。句子2的错误是得分的绝对值，即|–2|
    = 2。句子4的错误是得分的绝对值，即|3| = 3。其他两个句子没有错误，因为它们被正确分类。因此，分类器1的平均感知器错误为
- en: 1/4(0 + 2 + 0 + 3) = 1.25.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 1/4(0 + 2 + 0 + 3) = 1.25。
- en: Classifier 2 makes no errors—it correctly classifies all the points. Therefore,
    the mean perceptron error of classifier 2 is 0\. We then conclude that classifier
    2 is better than classifier 1\. The summary of these calculations is shown in
    table 5.4 and figure 5.19.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器2没有错误——它正确地分类了所有点。因此，分类器2的平均感知器错误为0。我们据此得出结论，分类器2优于分类器1。这些计算的总结在表5.4和图5.19中显示。
- en: '![](../Images/5-191.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-191.png)'
- en: Figure 5.19 Classifier 1 has an error of 1.25, whereas classifier 2 has an error
    of 0\. Thus, we conclude that classifier 2 is better than classifier 1.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19分类器1的错误率为1.25，而分类器2的错误率为0。因此，我们得出结论，分类器2优于分类器1。
- en: Now that we know how to compare classifiers, let’s move on to finding the best
    one of them, or at least a pretty good one.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何比较分类器，让我们继续寻找其中最好的一个，或者至少是一个相当好的一个。
- en: How to find a good classifier? The perceptron algorithm
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何找到一个好的分类器？感知器算法
- en: 'To build a good perceptron classifier, we’ll follow a similar approach as the
    one we followed with linear regression in chapter 3\. The process is called the
    *perceptron algorithm*, and it consists of starting with a random perceptron classifier
    and slowly improving it until we have a good one. The main steps of the perceptron
    algorithm follow:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个好的感知器分类器，我们将遵循与第3章中我们遵循的线性回归相似的方法。这个过程被称为*感知器算法*，它包括从一个随机的感知器分类器开始，并逐渐改进它，直到我们得到一个好的分类器。感知器算法的主要步骤如下：
- en: Start with a random perceptron classifier.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个随机的感知器分类器开始。
- en: Slightly improve the classifier. (Repeat many times).
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稍微改进分类器。（重复多次）。
- en: Measure the perceptron error to decide when to stop running the loop.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过测量感知器错误来决定何时停止循环运行。
- en: We start by developing the step inside the loop, a technique used to slightly
    improve a perceptron classifier called *the perceptron trick*. It is similar to
    the square and absolute tricks we learned in the sections “The square trick” and
    “The absolute trick” in chapter 3.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在循环内部开发步骤，这是一种用于略微改进感知器分类器的技术，称为**感知器技巧**。它与我们在第 3 章的“平方技巧”和“绝对技巧”部分学到的技巧类似。
- en: 'The perceptron trick: A way to slightly improve the perceptron'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器技巧：略微改进感知器的方法
- en: The perceptron trick is a tiny step that helps us go from a perceptron classifier
    to a slightly better perceptron classifier. However, we’ll start by describing
    a slightly less ambitious step. Just as we did in chapter 3, we’ll first focus
    on one point and try to improve the classifier for that one point.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器技巧是一个微小的步骤，帮助我们从一个感知器分类器到一个略微更好的感知器分类器。然而，我们将首先描述一个不那么雄心勃勃的步骤。就像我们在第 3 章中所做的那样，我们首先关注一个点，并尝试改进该点的分类器。
- en: There are two ways to see the perceptron step, although both are equivalent.
    The first way is the geometric way, where we think of the classifier as a line.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方式来观察感知器步骤，尽管两者都是等价的。第一种方式是几何方式，我们将分类器视为一条线。
- en: Pseudocode for the perceptron trick (geometric)
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器技巧的伪代码（几何）
- en: '**Case 1**: If the point is correctly classified, leave the line as it is.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况 1**：如果点被正确分类，保持线不变。'
- en: '**Case 2**: If the point is incorrectly classified, move the line a little
    closer to the point.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况 2**：如果点被错误分类，将线稍微移近该点。'
- en: Why does this work? Let’s think about it. If the point is misclassified, it
    means it is on the wrong side of the line. Moving the line closer to it may not
    put it on the right side, but at least it gets it closer to the line and, thus,
    closer to the correct side of the line. We repeat this process many times, so
    it is imaginable that one day we’ll be able to move the line past the point, thus
    correctly classifying it. This process is illustrated in figure 5.20.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这会起作用？让我们来思考一下。如果点被错误分类，这意味着它位于线的错误一侧。将线移近它可能不会将其移到正确的一侧，但至少会使它更接近线，因此更接近线的正确一侧。我们重复这个过程很多次，所以可以想象有一天我们能够将线移过点，从而正确分类它。这个过程在图
    5.20 中展示。
- en: We also have an algebraic way to see the perceptron trick.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一种代数方法来观察感知器技巧。
- en: '![](../Images/5-20.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.20](../Images/5-20.png)'
- en: 'Figure 5.20 Case 1 (left): A point that is correctly classified tells the line
    to stay where it is. Case 2 (right): A point that is misclassified tells the line
    to move closer toward it.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20 情况 1（左）：一个被正确分类的点告诉线保持原位。情况 2（右）：一个被错误分类的点告诉线向它靠近。
- en: Pseudocode for the perceptron trick (algebraic)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器技巧的伪代码（代数）
- en: '**Case 1**: If the point is correctly classified, leave the classifier as it
    is.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况 1**：如果点被正确分类，保持分类器不变。'
- en: '**Case 2**: If the point is incorrectly classified, that means it produces
    a positive error. Adjust the weights and the bias a small amount so that this
    error slightly decreases.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况 2**：如果点被错误分类，这意味着它产生了一个正误差。调整权重和偏置的一小部分，以便这个误差略微减小。'
- en: The geometric way is an easier way to visualize this trick, but the algebraic
    way is an easier way to develop it, so we’ll look at it the algebraic way. First,
    let’s use our intuition. Imagine that we have a classifier for the entire English
    language. We try this classifier on the sentence “I am sad,” and it predicts that
    the sentence is happy. This is clearly wrong. Where could we have gone wrong?
    If the prediction is that the sentence is happy, then the sentence must have received
    a positive score. This sentence shouldn’t receive a positive score—it should receive
    a negative score to be classified as sad. The score is calculated as the sum of
    the scores of its words *I*, *am*, and *sad*, plus the bias. We need to decrease
    this score, to make the sentence slightly sadder. It is OK if we decrease it only
    a little bit, and the score is still positive. Our hope is that running this process
    many times will one day turn the score into negative and correctly classify our
    sentence. The way to decrease the score is by decreasing all its parts, namely,
    the weights of the words *I*, *am*, and *sad* and the bias. By how much should
    we decrease them? We decrease them by an amount equal to the learning rate that
    we learned in the section “The square trick” in chapter 3.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 几何方法是一个更直观的方式来可视化这个技巧，但代数方法是一个更容易发展这个技巧的方式，所以我们将从代数方法来看。首先，让我们用我们的直觉来考虑。想象一下，我们有一个针对整个英语语言的分类器。我们尝试这个分类器对句子“我很悲伤”，它预测这个句子是快乐的。这显然是错误的。我们可能在哪里出错？如果预测这个句子是快乐的，那么这个句子必须得到了一个正分。这个句子不应该得到正分——它应该得到一个负分才能被分类为悲伤。分数是它的单词
    *我*、*是* 和 *悲伤* 的分数总和，加上偏差。我们需要降低这个分数，使句子稍微悲伤一些。如果我们只稍微降低它，分数仍然是正的，这也是可以的。我们的希望是，通过多次运行这个过程，总有一天能将分数变成负数，并正确地分类我们的句子。降低分数的方法是降低它的所有部分，即单词
    *我*、*是* 和 *悲伤* 的权重以及偏差。我们应该降低它们多少？我们通过一个等于我们在第三章“平方技巧”部分学到的学习率的量来降低它们。
- en: Similarly, if our classifier misclassifies the sentence “I am happy” as a sad
    sentence, then our procedure is to slightly increase the weights of the words
    *I*, *am*, and *happy* and the bias by an amount equal to the learning rate.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我们的分类器将句子“我很高兴”错误地分类为悲伤句子，那么我们的做法是略微增加单词 *我*、*是* 和 *高兴* 的权重以及偏差，增加的量等于学习率。
- en: Let’s illustrate this with a numerical example. In this example, we use a learning
    rate of *η* = 0.01\. Imagine that we have the same classifier that we had in the
    previous section, namely, the one with the following weights and bias. We’ll call
    it the bad classifier, because our goal is to improve it.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个数值例子来说明这一点。在这个例子中，我们使用的学习率为 *η* = 0.01。想象一下，我们有一个与上一节相同的分类器，即具有以下权重和偏差的分类器。我们将它称为“坏分类器”，因为我们的目标是改进它。
- en: Bad classifier
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 坏分类器
- en: 'Weights:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: '*Aack*: *a* = 1'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*啊哈*：*a* = 1'
- en: '*Beep*: *b* = 2'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哔哔*：*b* = 2'
- en: '**Bias**: *c* = –4'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**：*c* = –4'
- en: '**Prediction**: *ŷ* = *step*(*x*[aack] + 2*x*[beep] *–* 4)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*ŷ* = *step*(*x*[aack] + 2*x*[beep] *–* 4)'
- en: 'The following sentence is misclassified by the model, and we’ll use it to improve
    the weights:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 以下句子被模型错误分类，我们将用它来改进权重：
- en: '**Sentence 1**: “Beep aack aack beep beep beep beep.”'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子 1**： “哔哔 啊哈 啊哈 哔哔 哔哔 哔哔。”'
- en: '**Label**: Sad (0)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签**：悲伤（0）'
- en: For this sentence, the number of appearances of *aack* is *x*[aack] = 2, and
    the number of appearances of *beep* is *x*[beep] = 5\. Thus, the score is 1 ·
    *x*[aack] + 2 · *x*[beep] – 4 = 1 · 2 + 2 · 5 – 4 = 8, and the prediction is *ŷ*
    = *step*(8) = 1.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个句子，*aack* 出现的次数是 *x*[aack] = 2，*beep* 出现的次数是 *x*[beep] = 5。因此，分数是 1 · *x*[aack]
    + 2 · *x*[beep] – 4 = 1 · 2 + 2 · 5 – 4 = 8，预测是 *ŷ* = *step*(8) = 1。
- en: 'The sentence should have had a negative score, to be classified as sad. However,
    the classifier gave it a score of 8, which is positive. We need to decrease this
    score. One way to decrease it is to subtract the learning rate to the weight of
    *aack* to the weight of *beep* and to the bias, thus obtaining new weights, which
    we call *a**''* = 0.99, *b**''* = 1.99, and a new bias *c**''* = 4.01\. However,
    think about this: the word *beep* appeared many more times than the word *aack*.
    In some way, *beep* is more crucial to the score of the sentence than *aack*.
    We should probably decrease the weight of *beep* more than the score of *aack*.
    Let’s decrease the weight of each word by the learning rate times the number of
    times the word appears in the sentence. In other words:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 句子应该有一个负得分，以被分类为悲伤。然而，分类器给它一个得分为8，这是正的。我们需要降低这个得分。降低得分的一种方法是从 *aack* 的权重减去到
    *beep* 的权重，以及偏置，从而得到新的权重，我们称之为 *a**'* = 0.99，*b**'* = 1.99，以及新的偏置 *c**'* = 4.01。然而，考虑一下：单词
    *beep* 出现的次数比单词 *aack* 多得多。在某种程度上，*beep* 对句子的得分比 *aack* 更关键。我们可能需要比 *aack* 的得分降低
    *beep* 的权重更多。让我们通过将每个单词的权重减少学习率乘以单词在句子中出现的次数来降低每个单词的权重。换句话说：
- en: The word aack appears twice, so we’ll reduce its weight by two times the learning
    rate, or 0.02\. We obtain a new weight *a**'* = 1 – 2 · 0.01 = 0.98.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 aack 出现了两次，所以我们将它的权重减少两倍的学习率，即 0.02。我们得到新的权重 *a**'* = 1 – 2 · 0.01 = 0.98。
- en: The word *beep* appears five times, so we’ll reduce its weight by five times
    the learning rate, or 0.05\. We obtain a new weight *b**'* = 2 – 5 · 0.01 = 1.95.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 *beep* 出现了五次，所以我们将它的权重减少五倍的学习率，即0.05。我们得到新的权重 *b**'* = 2 – 5 · 0.01 = 1.95。
- en: The bias adds to the score only once, so we reduce the bias by the learning
    rate, or 0.01\. We obtain a new bias *c**'* = –4 – 0.01 = –4.01.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置只加一次到得分上，所以我们通过学习率，即0.01来减少偏置。我们得到新的偏置 *c**'* = –4 – 0.01 = –4.01。
- en: aside Instead of subtracting the learning rate from each weight, we subtracted
    the learning rate times the number of appearances of the word in the sentence.
    The true reason for this is calculus. In other words, when we develop the gradient
    descent method, the derivative of the error function forces us to do this. This
    process is detailed in appendix B, section “Using gradient descent to train classification
    models.”
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们不是从每个权重中减去学习率，而是减去学习率乘以单词在句子中出现的次数。真正的理由是微积分。换句话说，当我们开发梯度下降法时，误差函数的导数迫使我们这样做。这个过程在附录B的第“使用梯度下降训练分类模型”部分有详细说明。
- en: 'The new improved classifier follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 新的改进分类器如下：
- en: Improved classifier 1
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的分类器1
- en: 'Weights:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：
- en: '*Aack*: *a**''* = 0.98'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aack*：*a**''* = 0.98'
- en: '*Beep*: *b**''* = 1.95'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*哔哔声*：*b**''* = 1.95'
- en: '**Bias**: *c**''* = –4.01'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏置**：*c**''* = –4.01'
- en: '**Prediction**: *ŷ* = *step*(0.98*x*[aack] + 1.95*x*[beep] – 4.01)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*ŷ* = *step*(0.98*x*[aack] + 1.95*x*[beep] – 4.01)'
- en: Let’s verify the errors of both classifiers. Recall that the error is the absolute
    value of the score. Thus, the bad classifier produces an error of |1 · *x*[aack]
    + 2 · *x*[beep] – 4| = |1 · 2 + 2 · 5 – 4| = 8\. The improved classifier produces
    an error of |0.98 · *x*[aack] + 1.95 · *x*[beep] – 4.01| = |0.98 · 2 + 1.95 ·
    5 – 4.01| = 7.7\. That is a smaller error, so we have indeed improved the classifier
    for that point!
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证这两个分类器的错误。记住，错误是得分的绝对值。因此，糟糕的分类器产生的错误为 |1 · *x*[aack] + 2 · *x*[beep] –
    4| = |1 · 2 + 2 · 5 – 4| = 8。改进的分类器产生的错误为 |0.98 · *x*[aack] + 1.95 · *x*[beep]
    – 4.01| = |0.98 · 2 + 1.95 · 5 – 4.01| = 7.7。这是一个更小的错误，所以我们确实改进了该点的分类器！
- en: 'The case we just developed consists of a misclassified point with a negative
    label. What happens if the misclassified point has a positive label? The procedure
    is the same, except instead of subtracting an amount from the weights, we add
    it. Let’s go back to the bad classifier and consider the following sentence:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才开发的案例包含一个带有负标签的错误分类点。如果错误分类点有一个正标签会发生什么？程序是相同的，只是不是从权重中减去一个量，而是加上它。让我们回到糟糕的分类器，并考虑以下句子：
- en: '**Sentence 2**: “Aack aack.”'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子2**：“Aack aack。”'
- en: '**Label**: Happy'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签**：快乐'
- en: 'The prediction for this sentence is *ŷ* = *step*(*x*[aack] + 2*x*[beep] – 4)
    = *step*(2 + 2 · 0 – 4) = *step*(–2) = 0\. Because the prediction is sad, the
    sentence is misclassified. The score of this sentence is –2, and to classify this
    sentence as happy, we need the classifier to give it a positive score. The perceptron
    trick will increase this score of –2 by increasing the weights of the words and
    the bias as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子的预测是 *ŷ* = *step*(*x*[aack] + 2*x*[beep] – 4) = *step*(2 + 2 · 0 – 4) =
    *step*(–2) = 0。因为预测是悲伤的，所以句子被错误分类。这个句子的分数是 –2，为了将这个句子分类为快乐，我们需要分类器给它一个正分数。感知器技巧将通过以下方式增加这个
    –2 分数的权重和偏差：
- en: The word *aack* appears twice, so we’ll increase its weight by two times the
    learning rate, or 0.02\. We obtain a new weight *a'* = 1 + 2 · 0.01 = 1.02.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 *aack* 出现两次，所以我们将权重增加两倍的学习率，即 0.02。我们得到新的权重 *a'* = 1 + 2 · 0.01 = 1.02。
- en: The word *beep* appears zero times, so we won’t increase its weight, because
    this word is irrelevant to the sentence.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 *beep* 出现零次，所以我们不会增加其权重，因为这个词与句子无关。
- en: The bias adds to the score only once, so we increase the bias by the learning
    rate, or 0.01\. We obtain a new bias *c**'* = –4 + 0.01 = –3.99.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差只加到分数上一次，所以我们通过学习率，或0.01，增加偏差。我们得到新的偏差*c**' = –4 + 0.01 = –3.99。
- en: 'Thus, our new improved classifier follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们新的改进分类器如下：
- en: Improved classifier 2
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的分类器2
- en: 'Weights:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重**：'
- en: '*Aack*: *a**''* = 1.02'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aack*: *a**''* = 1.02'
- en: '*Beep*: *b**''* = 2'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Beep*: *b**''* = 2'
- en: '**Bias**: *c**''* = –3.99'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**: *c**'' = –3.99'
- en: '**Prediction**: *ŷ* = *step*(1.02*x*[aack] + 2*x*[beep] – 3.99)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*ŷ* = *step*(1.02*x*[aack] + 2*x*[beep] – 3.99)'
- en: Now let’s verify the errors. Because the bad classifier gave the sentence a
    score of –2, then the error is |–2| = 2\. The second classifier gave the sentence
    a score of 1.02*x*[aack] + 2*x*[beep] – 3.99 = 1.02 · 2 + 2 · 0 – 3.99 = –1.95,
    and an error of 1.95\. Thus, the improved classifier has a smaller error on that
    point than the bad classifier, which is exactly what we were expecting.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来验证错误。因为不良分类器给句子一个 –2 的分数，所以错误是 |–2| = 2。第二个分类器给句子一个分数为 1.02*x*[aack] +
    2*x*[beep] – 3.99 = 1.02 · 2 + 2 · 0 – 3.99 = –1.95，错误为 1.95。因此，改进的分类器在这个点上比不良分类器有更小的错误，这正是我们所期望的。
- en: Let’s summarize these two cases and obtain the pseudocode for the perceptron
    trick.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结这两个情况，并得到感知器技巧的伪代码。
- en: Pseudocode for the perceptron trick
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器技巧的伪代码
- en: 'Inputs:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: A perceptron with weights *a, b,* and bias *c*
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有权重 *a, b,* 和偏差 *c* 的感知器
- en: A point with coordinates (*x*[1], *x*[2]) and label *y*
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个坐标为 (*x*[1], *x*[2]) 和标签 *y* 的点
- en: A small positive value *η* (the learning rate)
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个小的正值 *η*（学习率）
- en: 'Output:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: A perceptron with new weights *a'*, *b'*, and bias *c'*
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有新权重 *a'*, *b'*, 和偏差 *c'* 的感知器
- en: 'Procedure:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤**：'
- en: The prediction the perceptron makes at the point is *ŷ* = *step*(*ax*[1] + *bx*[2]
    + *c*).
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器在这一点上的预测是 *ŷ* = *step*(*ax*[1] + *bx*[2] + *c*）。
- en: '**Case 1**: If *ŷ = y*:'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况1**：如果 *ŷ = y*：'
- en: '**Return** the original perceptron with weights *a''*, *b''*, and bias *c''*.'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**返回**原始感知器，权重 *a''*, *b''*, 和偏差 *c''*。'
- en: '**Case 2**: If *ŷ* = 1 and *y* = 0:'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况2**：如果 *ŷ* = 1 且 *y* = 0：'
- en: '**Return** the perceptron with the following weights and bias:'
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**返回**具有以下权重和偏差的感知器：'
- en: '*a'' = a – η**x*[1]'
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a'' = a – η**x*[1]'
- en: '*b'' = b – η**x*[2]'
  id: totrans-380
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b'' = b – η**x*[2]'
- en: '*c'' = c – η**x*[1]'
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c'' = c – η**x*[1]'
- en: '**Case 3**: If *ŷ* = 0 and *y* = 1:'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情况3**：如果 *ŷ* = 0 且 *y* = 1：'
- en: '**Return** the perceptron with the following weights and bias:'
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**返回**具有以下权重和偏差的感知器：'
- en: '*a'' = a + η**x*[1]'
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a'' = a + η**x*[1]'
- en: '*b'' = b – η**x*[2]'
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b'' = b – η**x*[2]'
- en: '*c'' = c + η**x*[1]'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c'' = c + η**x*[1]'
- en: If the perceptron correctly classifies the point, the output perceptron is the
    same as the input, and both of them produce an error of 0\. If the perceptron
    misclassifies the point, the output perceptron produces a smaller error than the
    input perceptron.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感知器正确分类了点，输出感知器与输入相同，它们都产生一个错误为 0。如果感知器错误分类了点，输出感知器产生的错误比输入感知器小。
- en: 'The following is a slick trick to condense the pseudocode. Note that the quantity
    *y* – *ŷ* is 0, –1, and +1 for the three cases in the perceptron trick. Thus,
    we can summarize it as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简洁的技巧来压缩伪代码。注意，*y* – *ŷ* 对于感知器技巧中的三个情况是 0, –1 和 +1。因此，我们可以总结如下：
- en: Pseudocode for the perceptron trick
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器技巧的伪代码
- en: 'Inputs:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: A perceptron with weights *a*, *b*, and bias *c*
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有权重 *a*, *b*, 和偏差 *c* 的感知器
- en: A point with coordinates (*x*[1], *x*[2]) and label *y*
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个坐标为 (*x*[1], *x*[2]) 和标签 *y* 的点
- en: A small value *η* (the learning rate)
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个小的值 *η*（学习率）
- en: 'Output:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: A perceptron with new weights *a'*, *b'*, and bias *c'*
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有新权重 *a'*, *b'*, 和偏差 *c'* 的感知器
- en: 'Procedure:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤**：'
- en: The prediction the perceptron makes at the point is *ŷ* = *step*(*ax*[1] + *bx*[2]
    + *c*).
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器在点上的预测是 *ŷ* = *step*(*ax*[1] + *bx*[2] + *c*)。
- en: '**Return** the perceptron with the following weights and bias:'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**返回**具有以下权重和偏差的感知器：'
- en: '*a'' = a* + *η*(*y* - *ŷ*)*x*[1]'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a'' = a* + *η*(*y* - *ŷ*)*x*[1]'
- en: '*b'' = b* + *η*(*y* - *ŷ*)*x*[2]'
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b'' = b* + *η*(*y* - *ŷ*)*x*[2]'
- en: '*c'' = c* + *η*(*y* - *ŷ*)'
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c'' = c* + *η*(*y* - *ŷ*)'
- en: 'Repeating the perceptron trick many times: The perceptron algorithm'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 重复感知器技巧多次：感知器算法
- en: In this section, we learn the *perceptron algorithm*, which is used to train
    a perceptron classifier on a dataset. Recall that the perceptron trick allows
    us to slightly improve a perceptron to make a better prediction on one point.
    The perceptron algorithm consists of starting with a random classifier and continuously
    improving it, using the perceptron trick many times.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习*感知器算法*，该算法用于在数据集上训练感知器分类器。回想一下，感知器技巧允许我们稍微改进一个感知器，以便在一点上做出更好的预测。感知器算法包括从一个随机的分类器开始，并连续改进它，多次使用感知器技巧。
- en: 'As we’ve seen in this chapter, we can study this problem in two ways: geometrically
    and algebraically. Geometrically, the dataset is given by points in the plane
    colored with two colors, and the classifier is a line that tries to split these
    points. Figure 5.21 contains a dataset of happy and sad sentences just like the
    ones we saw at the beginning of this chapter. The first step of the algorithm
    is to draw a random line. It’s clear that the line in figure 5.21 does not represent
    a great perceptron classifier, because it doesn’t do a good job of splitting the
    happy and the sad sentences.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中看到的，我们可以用两种方式来研究这个问题：几何和代数。在几何上，数据集由平面上用两种颜色着色的点给出，分类器是一条试图分割这些点的线。图5.21包含了一组快乐和悲伤的句子数据集，就像我们在本章开头看到的。算法的第一步是画一条随机的线。很明显，图5.21中的线并不代表一个很好的感知器分类器，因为它没有很好地分割快乐和悲伤的句子。
- en: '![](../Images/5-21.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-21.png)'
- en: Figure 5.21 Each point tells the classifier what to do to make life better for
    itself. The points that are correctly classified tell the line to stay still.
    The points that are misclassified tell the line to move slightly toward them.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21每个点都会告诉分类器如何让自己过得更好。被正确分类的点会告诉线保持不动。被错误分类的点会告诉线稍微向它们移动。
- en: The next step in the perceptron algorithm consists of picking one point at random,
    such as the one in figure 5.22\. If the point is correctly classified, the line
    is left alone. If it is misclassified, then the line gets moved slightly closer
    to the point, thus making the line a better fit for that point. It may become
    a worse fit for other points, but that doesn’t matter for now.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器算法的下一步是随机选择一个点，例如图5.22中的点。如果点被正确分类，则直线保持不变。如果点被错误分类，则直线会稍微靠近该点，从而使直线更好地适应该点。它可能对其他点不再是一个好的适应，但现在这并不重要。
- en: '![](../Images/5-22.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-22.png)'
- en: Figure 5.22 If we apply the perceptron trick to a classifier and a misclassified
    point, the classifier moves slightly toward the point.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22如果我们将感知器技巧应用于一个分类器和错误分类的点，分类器会稍微向该点移动。
- en: It is imaginable that if we were to repeat this process many times, eventually
    we will get to a good solution. This procedure doesn’t always get us to the best
    solution. But in practice, this method often reaches to a good solution as shown
    in figure 5.23\. We call this the *perceptron algorithm*.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 可以想象，如果我们重复这个过程很多次，最终我们会得到一个好的解决方案。这种方法并不总是能带我们到最好的解决方案。但在实践中，这种方法通常能找到一个好的解决方案，如图5.23所示。我们称之为*感知器算法*。
- en: '![](../Images/5-23.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-23.png)'
- en: Figure 5.23 If we apply the perceptron trick many times, each time picking a
    random point, we can imagine that we’ll obtain a classifier that classifies most
    points correctly.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23如果我们多次应用感知器技巧，每次随机选择一个点，我们可以想象我们会得到一个大多数点都能正确分类的分类器。
- en: 'The number of times we run the algorithm is the number of epochs. Therefore,
    this algorithm has two hyperparameters: the number of epochs, and the learning
    rate. The pseudocode of the perceptron algorithm follows:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行算法的次数是epoch的数量。因此，这个算法有两个超参数：epoch的数量和学习率。感知器算法的伪代码如下：
- en: Pseudocode for the perceptron algorithm
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器算法的伪代码
- en: 'Inputs:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A dataset of points, labeled 1 and 0
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由1和0标记的点集
- en: A number of epochs, *n*
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个epoch的数量，*n*
- en: A learning rate *η*
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个学习率 *η*
- en: 'Output:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A perceptron classifier consisting of a set of weights and a bias that fits
    the dataset
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由一组权重和偏差组成的感知器分类器，它适合数据集
- en: 'Procedure:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Start with random values for the weights and bias of the perceptron classifier.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从感知器分类器的权重和偏差的随机值开始。
- en: 'Repeat many times:'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Pick a random data point.
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机数据点。
- en: Update the weights and the bias using the perceptron trick.
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用感知器技巧更新权重和偏差。
- en: '**Return**: The perceptron classifier with the updated weights and bias.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回值**: 带有更新后的权重和偏差的感知器分类器。'
- en: 'How long should we run the loop? In other words, how many epochs should we
    use? Several criteria to help us make that decision follow:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该运行循环多长时间？换句话说，我们应该使用多少个周期？以下是一些帮助我们做出决定的准则：
- en: Run the loop a fixed number of times, which could be based on our computing
    power, or the amount of time we have.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行循环固定次数，这可能是基于我们的计算能力，或者我们拥有的时间量。
- en: Run the loop until the error is lower than a certain threshold we set beforehand.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行循环，直到错误低于我们事先设定的某个阈值。
- en: Run the loop until the error doesn’t change significantly for a certain number
    of epochs.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行循环，直到错误在一定的周期数内没有显著变化。
- en: Normally, if we have the computing power, it’s OK to run it many more times
    than needed, because once we have a well-fitted perceptron classifier, it tends
    to not change very much. In the “Coding the perceptron algorithm” section, we
    code the perceptron algorithm and analyze it by measuring the error in each step,
    so we’ll get a better idea of when to stop running it.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果我们有足够的计算能力，运行它比所需的次数多几次是可以的，因为一旦我们有一个拟合良好的感知器分类器，它往往不会改变很多。在“编码感知器算法”部分，我们编码感知器算法并通过测量每一步的错误来分析它，这样我们就能更好地了解何时停止运行它。
- en: 'Note that for some cases, such as the one shown in figure 5.24, it is impossible
    to find a line to separate the two classes in the dataset. That is OK: the goal
    is to find a line that separates the dataset with as few errors as possible (like
    the one in the figure), and the perceptron algorithm is good at this.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于某些情况，例如图5.24中所示的情况，在数据集中找到一条线来分开两个类别是不可能的。这是可以的：目标是找到一条尽可能少出错的线来分开数据集（如图中所示），感知器算法在这方面做得很好。
- en: '![](../Images/5-24.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![图5-24](../Images/5-24.png)'
- en: Figure 5.24 A dataset with two classes that are impossible to separate with
    a line. The perceptron algorithm is then trained to find a line that separates
    them as best as possible.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.24 一个包含两个类别的数据集，这两个类别无法用一条线分开。然后，感知器算法被训练以找到尽可能好地分开它们的线。
- en: Gradient descent
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降
- en: You may notice that the process for training this model looks very familiar.
    In fact, it is similar to what we did in chapter 3 with linear regression. Recall
    that the purpose of linear regression is to fit a line as close as possible to
    a set of points. In chapter 3, we trained our linear regression model by starting
    with a random line and taking small steps to get closer and closer to the points.
    We then used the analogy of descending from a mountain (Mount Errorest) by taking
    small steps toward the bottom. The height at each point in the mountain is the
    mean perceptron error function, which we defined as the absolute error, or the
    square error. Therefore, descending from the mountain is equivalent to minimizing
    the error, which is equivalent to finding the best line fit. We called this process
    gradient descent, because the gradient is precisely the vector that points in
    the direction of largest growth (so its negative points in the direction of largest
    decrease), and taking a step in this direction will get us to descend the most.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到训练这个模型的过程看起来非常熟悉。事实上，它与我们在第3章中用线性回归所做的是相似的。回想一下，线性回归的目的是将一条线尽可能接近一组点。在第3章中，我们通过从一个随机线开始，并逐步靠近点来训练我们的线性回归模型。我们用从山顶（埃里斯特山）向下走的小步来类比这个过程。山顶上每个点的海拔是平均感知器误差函数，我们将其定义为绝对误差，或平方误差。因此，从山顶向下走相当于最小化误差，这相当于找到最佳拟合线。我们称这个过程为梯度下降，因为梯度正是指向最大增长方向的向量（所以它的负方向指向最大减少方向），在这个方向上迈出一步将使我们向下走得更远。
- en: In this chapter, the same thing happens. Our problem is a little different because
    we don’t want to fit a line as close as possible to a set of points. Instead,
    we want to draw a line that separates two sets of points in the best possible
    way. The perceptron algorithm is the process that starts with a random line, and
    it slowly moves it step by step to build a better separator. The analogy of descending
    from a mountain also works here. The only difference is that in this mountain,
    the height at each point is the mean perceptron error that we learned in the section
    “How to compare classifiers? The error function.”
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，发生的是同样的事情。我们的问题有一点不同，因为我们不希望将直线尽可能接近一组点。相反，我们希望以最佳方式绘制一条将两组点分开的线。感知器算法是一个从随机线开始，并逐步移动以构建更好分离器的过程。从山上下降的类比也适用于这里。唯一的不同之处在于，在这个山上，每个点的海拔高度是我们在第“如何比较分类器？错误函数”节中学到的平均感知器误差。
- en: Stochastic and batch gradient descent
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降和批量梯度下降
- en: The way we developed the perceptron algorithm in this section is by repeatedly
    taking one point at a time and adjusting the perceptron (line) to be a better
    fit for that point. This is called an epoch. However, just as we did with linear
    regression in section “Do we train using one point at a time or many?” in chapter
    3, the better approach is to take a batch of points at a time and adjust the perceptron
    to be a better fit for those points in one step. The extreme case is to take all
    the points in the set at a time and adjust the perceptron to fit all of them better
    in one step. In section “Do we train using one point at a time or many?” in chapter
    3, we call these approaches *stochastic*, *mini-batch*, and *batch gradient descent*.
    In this section, we use the formal perceptron algorithm using mini-batch gradient
    descent. The mathematical details appear in appendix B, section “Using gradient
    descent to train classification models,” where the perceptron algorithm is described
    in full generality using mini-batch gradient descent.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们开发感知器算法的方式是每次取一个点，并调整感知器（直线）以更好地适应该点。这被称为一个epoch。然而，正如我们在第3章“我们是一次训练一个点还是多个点？”节中处理线性回归时所做的，更好的方法是每次取一批点，并一次性调整感知器以更好地适应这些点。极端情况是，一次性取集合中的所有点，并一次性调整感知器以更好地适应所有点。在第3章“我们是一次训练一个点还是多个点？”节中，我们称这些方法为*随机梯度下降*、*小批量梯度下降*和*批量梯度下降*。在本节中，我们使用带有小批量梯度下降的正式感知器算法。数学细节出现在附录B，“使用梯度下降训练分类模型”节中，其中使用小批量梯度下降对感知器算法进行了全面描述。
- en: Coding the perceptron algorithm
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写感知器算法
- en: 'Now that we have developed the perceptron algorithm for our sentiment analysis
    application, in this section we write the code for it. First we’ll write the code
    from scratch to fit our original dataset, and then we’ll use Turi Create. In real
    life, we always use a package and have little need to code our own algorithms.
    However, it’s good to code some of the algorithms at least once—think of it as
    doing long division. Although we usually don’t do long division without using
    a calculator, it’s good we had to in high school, because now when we do it using
    a calculator, we know what’s happening in the background. The code for this section
    follows, and the dataset we use is shown in table 5.5:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们的情感分析应用开发了感知器算法，在本节中我们将编写它的代码。首先，我们将从头开始编写代码以适应我们的原始数据集，然后我们将使用Turi
    Create。在现实生活中，我们总是使用一个包，几乎没有必要自己编写算法。然而，至少编写一些算法是好的——把它想象成做长除法。虽然我们通常不用计算器做长除法，但我们在高中时不得不这样做是好的，因为现在当我们用计算器做长除法时，我们知道背后发生了什么。本节的代码如下，我们使用的数据集显示在表5.5中：
- en: '**Notebook**: Coding_perceptron_algorithm.ipynb'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Notebook**: Coding_perceptron_algorithm.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_5_Perceptron_Algorithm/Coding_perceptron_algorithm.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_5_Perceptron_Algorithm/Coding_perceptron_algorithm.ipynb)'
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_5_Perceptron_Algorithm/Coding_perceptron_algorithm.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_5_Perceptron_Algorithm/Coding_perceptron_algorithm.ipynb)'
- en: Table 5.5 A dataset of aliens, the times they said each of the words, and their
    mood.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.5 外星人数据集，他们说过的话以及他们的情绪。
- en: '| Aack | Beep | Happy/Sad |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| Aack | Beep | Happy/Sad |'
- en: '| 1 | 0 | 0 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 |'
- en: '| 0 | 2 | 0 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2 | 0 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: '| 1 | 2 | 0 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 0 |'
- en: '| 1 | 3 | 1 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3 | 1 |'
- en: '| 2 | 2 | 1 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 1 |'
- en: '| 2 | 3 | 1 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 1 |'
- en: '| 3 | 2 | 1 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 1 |'
- en: Let’s begin by defining our dataset as a NumPy array. The features correspond
    to two numbers corresponding to the appearances of *aack* and *beep*. The labels
    are 1 for the happy sentences and 0 for the sad ones.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义我们的数据集为一个NumPy数组。特征对应于两个数字，分别对应*aack*和*beep*的出现。标签为快乐句子的1，悲伤句子的0。
- en: '[PRE0]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This gives us the plot in figure 5.25\. In this figure, the happy sentences
    are triangles, and the sad ones are squares.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了图5.25中的图表。在这个图表中，快乐句子是三角形，悲伤句子是正方形。
- en: '![](../Images/5-25.png)'
  id: totrans-457
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-25.png)'
- en: Figure 5.25 The plot of our dataset. Triangles are happy aliens, and squares
    are sad aliens.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.25 我们的数据集的图表。三角形是快乐的异形，正方形是悲伤的异形。
- en: Coding the perceptron trick
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 编写感知器技巧
- en: In this section, we code the perceptron trick. We’ll code it using stochastic
    gradient descent (one point at a time), but we could also code it using either
    mini-batch or batch gradient descent. We start by coding the score function and
    the prediction. Both functions receive the same input, which is the weights of
    the model, the bias, and the features of one data point. The score function returns
    the score that the model gives to that data point, and the prediction function
    returns a 1 if the score is greater than or equal to zero and a 0 if the score
    is less than zero. For this function we use the dot product defined in the section
    “Plotting the error function and knowing when to stop running the algorithm” in
    chapter 3.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们编写感知器技巧的代码。我们将使用随机梯度下降（一次一个点）来编写它，但我们也可以使用小批量或批量梯度下降来编写它。我们首先编写得分函数和预测函数。这两个函数接收相同的输入，即模型的权重、偏置和一个数据点的特征。得分函数返回模型对该数据点的评分，预测函数如果评分大于或等于零则返回1，如果评分小于零则返回0。对于此函数，我们使用第3章中“绘制误差函数和知道何时停止运行算法”部分中定义的点积。
- en: '[PRE1]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Calculates the dot product between the weights and the features, adds the
    bias, and applies the step function
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算权重和特征之间的点积，加上偏置，并应用步函数
- en: To write the prediction function, we first write the step function. The prediction
    is the step function of the score.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写预测函数，我们首先编写步函数。预测是分数的步函数。
- en: '[PRE2]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Looks at the score, and if it is positive or zero, returns 1; if it is negative,
    returns 0
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 查看分数，如果它是正数或零，则返回1；如果是负数，则返回0
- en: Next, we code the error function for one point. Recall that the error is zero
    if the point is correctly classified and the absolute value of the score if the
    point is misclassified. This function takes as input the weights and bias of the
    model and the features and label of the data point.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个点的误差函数。回想一下，如果点被正确分类，则误差为零；如果点被错误分类，则误差等于分数的绝对值。此函数以模型的权重和偏置以及数据点的特征和标签作为输入。
- en: '[PRE3]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ If the prediction is equal to the label, then the point is well classified,
    which means the error is zero.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果预测等于标签，则点被正确分类，这意味着误差为零。
- en: ❷ If the prediction is different from the label, then the point is misclassified,
    which means the error is equal to the absolute value of the score.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果预测与标签不同，则点被错误分类，这意味着误差等于分数的绝对值。
- en: We now write a function for the mean perceptron error. This function calculates
    the average of the errors of all the points in our dataset.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在编写一个用于平均感知器误差的函数。此函数计算我们数据集中所有点的误差的平均值。
- en: '[PRE4]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Loops through our data, and for each point, adds the error at that point,
    then returns this error
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历我们的数据，并对每个点，加上该点的误差，然后返回这个误差
- en: ❷ The sum of errors is divided by the number of points to get the mean perceptron
    error.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将误差总和除以点的数量以获得平均感知器误差。
- en: Now that we have the error function, we can go ahead and code the perceptron
    trick. We’ll code the condensed version of the algorithm found at the end of the
    section “The perceptron trick.” However, in the notebook, you can find it coded
    in both ways, the first one using an `if` statement that checks whether the point
    is well classified.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了误差函数，我们可以继续编写感知器技巧的代码。我们将编写该节“感知器技巧”末尾找到的算法的简化版本。然而，在笔记本中，你可以找到两种编码方式，第一种使用`if`语句检查点是否被正确分类。
- en: '[PRE5]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Updates the weights and biases using the perceptron trick
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用感知器技巧更新权重和偏置
- en: Coding the perceptron algorithm
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 编写感知器算法
- en: 'Now that we have the perceptron trick, we can code the perceptron algorithm.
    Recall that the perceptron algorithm consists of starting with a random perceptron
    classifier and repeating the perceptron trick many times (as many as the number
    of epochs). To track the performance of the algorithm, we’ll also keep track of
    the mean perceptron error at each epoch. As inputs, we have the data (features
    and labels), the learning rate, which we default to 0.01, and the number of epochs,
    which we default to 200\. The code for the perceptron algorithm follows:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了感知器技巧，我们可以编写感知器算法了。回想一下，感知器算法包括从一个随机的感知器分类器开始，并多次重复感知器技巧（多达epoch的数量）。为了跟踪算法的性能，我们还会在每个epoch跟踪平均感知器误差。作为输入，我们有数据（特征和标签）、学习率，我们默认设置为0.01，以及epoch的数量，我们默认设置为200。感知器算法的代码如下：
- en: '[PRE6]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Initializes the weights to 1 and the bias to 0\. Feel free to initialize them
    to small random numbers if you prefer.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将权重初始化为1，将偏置初始化为0。如果您愿意，也可以将它们初始化为小的随机数。
- en: ❷ An array to store the errors
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一个用于存储误差的数组
- en: ❸ Repeats the process as many times as the number of epochs
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 重复该过程，直到epoch的数量
- en: ❹ Calculates the mean perceptron error and stores it
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算平均感知器误差并将其存储
- en: ❺ Picks a random point in our dataset
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在我们的数据集中随机选择一个点
- en: ❻ Applies the perceptron algorithm to update the weights and the bias of our
    model based on that point
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将感知器算法应用于更新模型基于该点的权重和偏置
- en: Now let’s run the algorithm on our dataset!
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来在我们的数据集上运行算法吧！
- en: '[PRE7]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output shows that the weights and bias we obtained are the following:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们获得的权重和偏置如下：
- en: 'Weight of *aack*: 0.63'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*aack*的权重：0.63'
- en: 'Weight of *beep*: 0.18'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*beep*的权重：0.18'
- en: 'Bias: –1.04'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置：-1.04
- en: We could have a different answer, because of the randomness in our choice of
    points inside the algorithm. For the code in the repository to always return the
    same answer, the random seed is set to zero.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会有不同的答案，因为算法中我们选择点的随机性。为了使存储库中的代码始终返回相同的答案，随机种子被设置为零。
- en: 'Figure 5.26 shows two plots: on the left is the line fit, and on the right
    is the error function. The line corresponding to the resulting perceptron is the
    thick line, which classifies every point correctly. The thinner lines are the
    lines corresponding to the perceptrons obtained after each of the 200 epochs.
    Notice how at each epoch, the line becomes a better fit for the points. The error
    decreases (mostly) as we increase the number of epochs, until it reaches zero
    at around epoch 140, meaning that every point is correctly classified.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.26显示了两个图表：左边是线拟合，右边是误差函数。对应于结果感知器的线是粗线，它正确地分类了每个点。较细的线是对应于每个200个epoch后获得的感知器的线。注意，在每个epoch，线都成为点的更好拟合。随着epoch数量的增加，误差降低（主要是），直到在约140个epoch时达到零，这意味着每个点都被正确分类。
- en: '![](../Images/5-26.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5-26.png)'
- en: 'Figure 5.26 Left: The plot of our resulting classifier. Notice that it classifies
    each point correctly. Right: The error plot. Notice that the more epochs we run
    the perceptron algorithm for, the lower the error gets.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.26 左：我们结果分类器的图。注意，它正确地分类了每个点。右：误差图。注意，我们运行感知器算法的epoch越多，误差越低。
- en: That is the code for the perceptron algorithm! As I mentioned before, in practice,
    we don’t normally code algorithms by hand, but we use a package, such as Turi
    Create or Scikit-Learn. This is what we cover in the next section.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是感知器算法的代码！正如我之前提到的，在实践中，我们通常不会手动编写算法，而是使用一个包，如Turi Create或Scikit-Learn。这就是我们在下一节要介绍的内容。
- en: Coding the perceptron algorithm using Turi Create
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Turi Create编写感知器算法
- en: 'In this section, we learn to code the perceptron algorithm in Turi Create.
    The code is in the same notebook as the previous exercise. Our first task is to
    import Turi Create and create an SFrame with our data from a dictionary as follows:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习在Turi Create中编写感知器算法。代码与之前的练习在同一个笔记本中。我们的第一个任务是导入Turi Create并创建一个包含我们数据的SFrame，如下所示：
- en: '[PRE8]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we create and train our perceptron classifier, using the `logistic_classifier`
    object and the `create` method, as shown in the next code. The inputs are the
    dataset and the name of the column containing the labels (target).
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建并训练我们的感知器分类器，使用`logistic_classifier`对象和`create`方法，如以下代码所示。输入是数据集和包含标签（目标）的列名。
- en: '[PRE9]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Notice that the perceptron algorithm ran for four epochs, and in the last one
    (in fact, in all of them), it had a training accuracy of 1\. This means every
    point in the dataset was correctly classified.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，感知器算法运行了四个时期，在最后一个时期（实际上，在所有时期），它的训练准确率为1。这意味着数据集中的每个点都被正确分类。
- en: 'Finally, we can look at the weights and bias of the model, using the following
    command:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下命令查看模型的权重和偏差：
- en: '[PRE11]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of this function shows the following weights and bias for the resulting
    perceptron:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的输出显示了以下感知器的权重和偏差：
- en: 'Weight of *aack*: 2.70'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*aack*的权重：2.70'
- en: 'Weight of *beep*: 2.46'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*beep*的权重：2.46'
- en: 'Bias: –8.96'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差：-8.96
- en: These are different results from what we obtained by hand, but both perceptrons
    work well in the dataset.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与我们手动获得的结果不同，但两个感知器在数据集中都工作得很好。
- en: Applications of the perceptron algorithm
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器算法的应用
- en: The perceptron algorithm has many applications in real life. Virtually every
    time we need to answer a question with yes or no, where the answer is predicted
    from previous data, the perceptron algorithm can help us. Here are some examples
    of real-life applications of the perceptron algorithm.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器算法在现实生活中有许多应用。几乎每次我们需要用是或否来回答问题，而答案是从以前的数据中预测出来的，感知器算法都可以帮助我们。以下是感知器算法在现实生活中的应用实例。
- en: Spam email filters
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤器
- en: 'In a similar way as we predicted whether a sentence is happy or sad based on
    the words in the sentence, we can predict whether an email is spam or not spam
    based on the words in the email. We can also use other features, such as the following:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们根据句子中的单词预测句子是快乐还是悲伤一样，我们可以根据电子邮件中的单词预测电子邮件是否是垃圾邮件。我们还可以使用其他特征，如下所示：
- en: Length of the email
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件长度
- en: Size of attachments
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附件大小
- en: Number of senders
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发件人数量
- en: Whether any of our contacts is a sender
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否有任何联系人是发件人
- en: Currently, the perceptron algorithm (and its more advanced counterparts, logistic
    regression and neural networks) and other classification models are used as a
    part of spam classification pipelines by most of the biggest email providers,
    with great results.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，感知器算法（及其更高级的对应物，逻辑回归和神经网络）以及其他分类模型，大多数最大的电子邮件提供商都将其用作垃圾邮件分类管道的一部分，并取得了很好的效果。
- en: We can also categorize emails using classification algorithms like the perceptron
    algorithm. Classifying email into personal, subscriptions, and promotions is the
    exact same problem. Even coming up with potential responses to an email is also
    a classification problem, except now the labels that we use are responses to emails.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用感知器算法等分类算法对电子邮件进行分类。将电子邮件分类为个人、订阅和促销是同一个问题。甚至对一封电子邮件的潜在回复也是分类问题，只是现在我们使用的标签是电子邮件的回复。
- en: Recommendation Systems
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统
- en: 'In many recommendation systems, recommending a video, movie, song, or product
    to a user boils down to a yes or no answer. In these cases, the question can be
    any of the following:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多推荐系统中，向用户推荐视频、电影、歌曲或产品归结为一个是否的问题。在这些情况下，问题可以是以下任何一种：
- en: Will the user click on the video/movie we’re recommending?
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否会点击我们推荐的视频/电影？
- en: Will the user watch the entire video/movie we’re recommending?
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否会观看我们推荐的整个视频/电影？
- en: Will the user listen to the song we’re recommending?
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否会听我们推荐的歌？
- en: Will the user buy the product we’re recommending?
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否会购买我们推荐的商品？
- en: The features can be anything, from demographic (age, gender, location of the
    user), to behavioral (what videos did the user watch, what songs did they hear,
    what products did they buy?). You can imagine that the user vector would be a
    long one! For this, large computing power and very clever implementations of the
    algorithms are needed.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 特征可以是任何东西，从人口统计（用户的年龄、性别、位置），到行为（用户看了哪些视频，听了哪些歌曲，买了哪些产品？）。你可以想象用户向量会很长！为此，需要大量的计算能力和非常巧妙的算法实现。
- en: Companies such as Netflix, YouTube, and Amazon, among many others, use the perceptron
    algorithm or similar, more advanced classification models in their recommendation
    systems.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix、YouTube和Amazon等公司以及其他许多公司，在他们的推荐系统中使用感知器算法或更高级的分类模型。
- en: Health care
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗保健
- en: 'Many medical models also use classification algorithms such as the perceptron
    algorithm to answer questions such as the following:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 许多医疗模型也使用感知器算法等分类算法来回答以下问题：
- en: Does the patient suffer from a particular illness?
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患者是否患有特定的疾病？
- en: Will a certain treatment work for a patient?
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某种治疗方法对病人有效吗？
- en: The features for these models will normally be the symptoms the patient is suffering
    and their medical history. For these types of algorithms, one needs very high
    levels of performance. Recommending the wrong treatment for a patient is much
    more serious than recommending a video that a user won’t watch. For this type
    of analysis, refer to chapter 7, where we talk about accuracy and other ways to
    evaluate classification models.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的特征通常是病人所患的症状和他们的医疗史。对于这类算法，需要非常高的性能水平。为病人推荐错误的治疗方案比推荐一个用户不会观看的视频要严重得多。对于这种分析，请参考第7章，其中我们讨论了准确度以及其他评估分类模型的方法。
- en: Computer vision
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Classification algorithms such as the perceptron algorithm are widely used in
    computer vision, more specifically, in image recognition. Imagine that we have
    a picture, and we want to teach the computer to tell whether the picture contains
    a dog. This is a classification model in which the features are the pixels of
    the image.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器算法等分类算法在计算机视觉中得到了广泛应用，特别是在图像识别中。想象一下，我们有一张图片，我们想要教会计算机判断这张图片是否包含一只狗。这是一个分类模型，其特征是图像的像素。
- en: The perceptron algorithm has decent performance in curated image datasets such
    as MNIST, which is a dataset of handwritten digits. However, for more complicated
    images, it doesn’t do very well. For these, one uses models that consist of a
    combination of many perceptrons. These models are aptly called multilayer perceptrons,
    or neural networks, and we learn about them in detail in chapter 10.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器算法在MNIST等精心制作的图像数据集上表现良好，MNIST是一个手写数字数据集。然而，对于更复杂的图像，它表现不佳。对于这些，人们使用由许多感知器组合而成的模型。这些模型恰当地被称为多层感知器，或神经网络，我们将在第10章中详细学习它们。
- en: Summary
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Classification is an important part of machine learning. It is similar to regression
    in that it consists of training an algorithm with labeled data and using it to
    make predictions on future (unlabeled) data. The difference from regression is
    that in classification, the predictions are categories, such as yes/no, spam/ham,
    and so on.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类是机器学习的一个重要部分。它与回归类似，因为它包括使用标记数据训练算法，并使用它来对未来的（未标记）数据进行预测。与回归的不同之处在于，在分类中，预测是类别，例如是/否、垃圾邮件/非垃圾邮件等。
- en: Perceptron classifiers work by assigning a weight to each of the features and
    a bias. The score of a data point is calculated as the sum of products of the
    weights and features, plus the bias. If the score is greater than or equal to
    zero, the classifier predicts a yes. Otherwise, it predicts a no.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器分类器通过为每个特征和偏差分配一个权重来工作。数据点的得分是权重和特征的乘积之和，加上偏差。如果得分大于或等于零，分类器预测为是。否则，它预测为否。
- en: For sentiment analysis, a perceptron consists of a score for each of the words
    in the dictionary, together with a bias. Happy words normally end up with a positive
    score, and sad words with a negative score. Neutral words such as *the* likely
    end up with a score close to zero.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于情感分析，感知器由字典中每个单词的得分以及一个偏差组成。快乐词通常会有一个正得分，而悲伤词会有一个负得分。中性词如*the*可能最终会有一个接近零的得分。
- en: The bias helps us decide if the empty sentence is happy or sad. If the bias
    is positive, then the empty sentence is happy, and if it is negative, then the
    empty sentence is sad.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差帮助我们判断一个空句子是快乐还是悲伤。如果偏差是正的，那么空句子就是快乐的，如果是负的，那么空句子就是悲伤的。
- en: Graphically, we can see a perceptron as a line trying to separate two classes
    of points, which can be seen as points of two different colors. In higher dimensions,
    a perceptron is a hyperplane separating points.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图形上看，我们可以将感知器视为一条试图分离两类点的线，这些点可以看作是两种不同颜色的点。在更高维度的空间中，感知器是一个分隔点的超平面。
- en: The perceptron algorithm works by starting with a random line and then slowly
    moving it to separate the points well. In every iteration, it picks a random point.
    If the point is correctly classified, the line doesn’t move. If it is misclassified,
    then the line moves a little bit closer to the point to pass over it and classify
    it correctly.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器算法通过从一个随机的线开始，然后慢慢移动它以更好地分隔点来工作。在每一次迭代中，它选择一个随机点。如果点被正确分类，线不会移动。如果它被错误分类，那么线会稍微靠近一点，以越过它并正确分类。
- en: The perceptron algorithm has numerous applications, including spam email detection,
    recommendation systems, e-commerce, and health care.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器算法有众多应用，包括垃圾邮件检测、推荐系统、电子商务和医疗保健。
- en: Exercises
  id: totrans-546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 5.1
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.1
- en: The following is a dataset of patients who have tested positive or negative
    for COVID-19\. Their symptoms are cough (C), fever (F), difficulty breathing (B),
    and tiredness (T).
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个数据集，包含已测试 COVID-19 阳性或阴性的患者。他们的症状包括咳嗽（C）、发热（F）、呼吸困难（B）和疲劳（T）。
- en: '|  | Cough (C) | Fever (F) | Difficulty breathing (B) | Tiredness (T) | Diagnosis
    (D) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  | 咳嗽（C） | 发热（F） | 呼吸困难（B） | 疲劳（T） | 诊断（D） |'
- en: '| Patient 1 |  | X | X | X | Sick |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 1 |  | X | X | X | 病人 |'
- en: '| Patient 2 | X | X |  | X | Sick |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 2 | X | X |  | X | 病人 |'
- en: '| Patient 3 | X |  | X | X | Sick |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 3 | X |  | X | X | 病人 |'
- en: '| Patient 4 | X | X | X |  | Sick |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 4 | X | X | X |  | 病人 |'
- en: '| Patient 5 | X |  |  | X | Healthy |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 5 | X |  |  | X | 健康 |'
- en: '| Patient 6 |  | X | X |  | Healthy |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 6 |  | X | X |  | 健康 |'
- en: '| Patient 7 |  | X |  |  | Healthy |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 7 |  | X |  |  | 健康 |'
- en: '| Patient 8 |  |  |  | X | Healthy |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 患者编号 8 |  |  |  | X | 健康 |'
- en: Build a perceptron model that classifies this dataset.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个感知器模型来分类这个数据集。
- en: hint You can use the perceptron algorithm, but you may be able to eyeball a
    good perceptron model that works.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：你可以使用感知器算法，但你可能能够直接找到一个有效的感知器模型。
- en: Exercise 5.2
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.2
- en: Consider the perceptron model that assigns to the point (*x*[1], *x*[2]) the
    prediction *ŷ* = *step*(2*x*[1] + 3*x*[2] – 4). This model has as a boundary line
    with equation 2*x*[1] + 3*x*[2] – 4 = 0\. We have the point *p* = (1, 1) with
    label 0.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个感知器模型，该模型将点 (*x*[1], *x*[2]) 分配给预测 *ŷ* = *step*(2*x*[1] + 3*x*[2] – 4)。该模型具有方程
    2*x*[1] + 3*x*[2] – 4 = 0 的边界线。我们有一个点 *p* = (1, 1) 且标签为 0。
- en: Verify that the point *p* is misclassified by the model.
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证点 *p* 被模型错误分类。
- en: Calculate the perceptron error that the model produces at the point *p*.
  id: totrans-563
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在点 *p* 处产生的感知器误差。
- en: Use the perceptron trick to obtain a new model that still misclassifies *p*
    but produces a smaller error. You can use *η* = 0.01 as the learning rate.
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用感知器技巧获得一个新的模型，该模型仍然错误分类 *p* 但产生更小的误差。你可以使用 *η* = 0.01 作为学习率。
- en: Find the prediction given by the new model at the point *p*, and verify that
    the perceptron error obtained is smaller than the original.
  id: totrans-565
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新模型在点 *p* 处的预测，并验证感知器误差小于原始误差。
- en: Exercise 5.3
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.3
- en: Perceptrons are particularly useful for building logical gates such as AND and
    OR.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器在构建逻辑门（如 AND 和 OR）方面特别有用。
- en: 'Build a perceptron that models the AND gate. In other words, build a perceptron
    to fit the following dataset (where *x*[1], *x*[2] are the features and *y* is
    the label):'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个模拟 AND 门的感知器。换句话说，构建一个感知器来拟合以下数据集（其中 *x*[1], *x*[2] 是特征，*y* 是标签）：
- en: '| *x*[1] | *x*[2] | *y*   |'
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *y*   |'
- en: '| 0 | 0 | 0 |'
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 0 |'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 1 | 0 |'
- en: '| 1 | 0 | 0 |'
  id: totrans-572
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 0 | 0 |'
- en: '| 1 | 1 | 1 |'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 1 | 1 |'
- en: 'Similarly, build a perceptron that models the OR gate, given by the following
    dataset:'
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，根据以下数据集构建一个模拟 OR 门的感知器：
- en: '| *x*[1] | *x*[2] | *y*   |'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *y*   |'
- en: '| 0 | 0 | 0 |'
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-578
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 1 | 1 | 1 |'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 1 | 1 |'
- en: 'Show that there is no perceptron that models the XOR gate, given by the following
    dataset:'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 证明不存在一个感知器可以模拟以下数据集给出的 XOR 门：
- en: '| *x*[1] | *x*[2] | *y*   |'
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| *x*[1] | *x*[2] | *y*   |'
- en: '| 0 | 0 | 0 |'
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-584
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
