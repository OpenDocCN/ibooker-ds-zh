- en: 10 Sample-efficient value-based methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10种样本高效的基于价值的方法
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will implement a deep neural network architecture that exploits some of
    the nuances that exist in value-based deep reinforcement learning methods.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将实现一个深度神经网络架构，该架构利用了基于价值的深度强化学习方法中存在的一些细微差别。
- en: You will create a replay buffer that prioritizes experiences by how surprising
    they are.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将创建一个优先级回放缓冲区，根据经验的新颖性来优先处理。
- en: You will build an agent that trains to a near-optimal policy in fewer episodes
    than all the value-based deep reinforcement learning agents we’ve discussed.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将构建一个代理，它将在比我们讨论的所有基于价值的深度强化学习代理更少的回合中训练到接近最优策略。
- en: Intelligence is based on how efficient a species became at doing the things
    they need to survive.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 智力基于一个物种在完成生存所需的事情上的效率。
- en: — Charles Darwin English naturalist, geologist, and biologist Best known for
    his contributions to the science of evolution
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: —— 查尔斯·达尔文 英国自然学家、地质学家和生物学家，以对进化科学的贡献而闻名
- en: In the previous chapter, we improved on NFQ with the implementation of DQN and
    DDQN. In this chapter, we continue on this line of improvements to previous algorithms
    by presenting two additional techniques for improving value-based deep reinforcement
    learning methods. This time, though, the improvements aren’t so much about stability,
    although that could easily be a by-product. But more accurately, the techniques
    presented in this chapter improve the sample-efficiency of DQN and other value-based
    DRL methods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过实现DQN和DDQN改进了NFQ。在本章中，我们继续改进之前的算法，通过介绍两种额外的技术来提高基于价值的深度强化学习方法。然而，这次改进并不那么多的关于稳定性，尽管这很容易成为副产品。但更准确地说，本章中介绍的技术提高了DQN和其他基于价值的DRL方法的样本效率。
- en: First, we introduce a functional neural network architecture that splits the
    Q-function representation into two streams. One stream approximates the V-function,
    and the other stream approximates the A-function. V-functions are per-state values,
    while A-functions express the distance of each action from their V-functions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍了一种功能神经网络架构，该架构将Q函数表示分为两个流。一个流近似V函数，另一个流近似A函数。V函数是每个状态的价值，而A函数表示每个动作与它们的V函数的距离。
- en: This is a handy fact for designing RL-specialized architectures that are capable
    of squeezing information from samples coming from all action in a given state
    into the V-function for that same state. What that means is that a single experience
    tuple can help improve the value estimates of all the actions in that state. This
    improves the sample efficiency of the agent.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个设计RL专用架构的实用事实，这种架构能够从给定状态的所有动作中提取信息，并将其压缩到该状态的V函数中。这意味着一个单一的经验元组可以帮助提高该状态下所有动作的价值估计。这提高了代理的样本效率。
- en: The second improvement we introduce in this chapter is related to the replay
    buffer. As you remember from the previous chapter, the standard replay buffer
    in DQN samples experiences uniformly at random. It’s crucial to understand that
    sampling uniformly at random is a good thing for keeping gradients proportional
    to the true data-generating underlying distribution, and therefore keeping the
    updates unbiased. The issue is, however, that if we could devise a way for prioritizing
    experiences, we could use the samples that are the most promising for learning.
    Therefore, in this chapter, we introduce a different technique for sampling experiences
    that allows us to draw samples that appear to provide the most information to
    the agent for actually making improvements.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们引入的第二项改进与回放缓冲区有关。正如你从上一章中记得的那样，DQN中的标准回放缓冲区在随机均匀地采样经验。理解这一点至关重要，即随机均匀采样对于保持梯度与真实数据生成的基础分布成比例是好事，因此保持更新无偏。然而，问题在于，如果我们能够设计一种优先处理经验的方法，我们就可以使用最有希望用于学习的样本。因此，在本章中，我们介绍了一种不同的采样经验技术，使我们能够抽取那些似乎为代理提供最多信息的样本，以实际上进行改进。
- en: 'Dueling DDQN: A reinforcement-learning-aware neural network architecture'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dueling DDQN：一种强化学习感知的神经网络架构
- en: Let’s now dig into the details of this specialized neural network architecture
    called the *dueling network architecture*. The dueling network is an improvement
    that applies only to the network architecture and not the algorithm. That is,
    we won’t make any changes to the algorithm, but the only modifications go into
    the network architecture. This property allows dueling networks to be combined
    with virtually any of the improvements proposed over the years to the original
    DQN algorithm. For instance, we could have a dueling DQN agent, and a dueling
    double DQN agent (or the way I’m referring to it—dueling DDQN), and more. Many
    of these improvements are just plug-and-play, which we take advantage of in this
    chapter. Let’s now implement a dueling architecture to be used in our experiments
    and learn about it while building it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来深入探讨这种称为 *对抗网络架构* 的专用神经网络架构的细节。对抗网络是一种仅适用于网络架构而不适用于算法的改进。也就是说，我们不会对算法进行任何修改，但所有的修改都集中在网络架构上。这一特性使得对抗网络可以与多年来对原始
    DQN 算法提出的几乎所有改进相结合。例如，我们可以有一个对抗 DQN 代理，一个对抗双 DQN 代理（或者我所说的对抗 DDQN），等等。许多这些改进都是即插即用的，我们将在本章中利用这一点。现在让我们实现一个对抗架构，用于我们的实验，并在构建过程中了解它。
- en: Reinforcement learning isn’t a supervised learning problem
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习不是一个监督学习问题
- en: In the previous chapter, we concentrated our efforts on making reinforcement
    learning look more like a supervised learning problem. By using a replay buffer,
    we made online data, which is experienced and collected sequentially by the agent,
    look more like an independent and identically distributed dataset, such as those
    commonly found in supervised learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们集中精力使强化学习看起来更像一个监督学习问题。通过使用重放缓冲区，我们使在线数据，即代理按顺序体验和收集的数据，看起来更像一个独立同分布的数据集，这在监督学习中很常见。
- en: We also made targets look more static, which is also a common trait of supervised
    learning problems. This surely helps stabilize training, but ignoring the fact
    that reinforcement learning problems are problems of their own isn’t the smartest
    approach to solving these problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使目标看起来更静态，这也是监督学习问题的一个常见特征。这无疑有助于稳定训练，但忽视强化学习问题是其自身的问题这一事实，并不是解决这些问题的最佳方法。
- en: One of the subtleties value-based deep reinforcement learning agents have, and
    that we will exploit in this chapter, is in the way the value functions relate
    to one another. More specifically, we can use the fact that the state-value function
    *V*(*s*) and the action-value function *Q*(*s, a*) are related to each other through
    the action-advantage function *A*(*s, a*).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的深度强化学习代理具有的一个微妙之处，并且我们将在本章中利用这一点，就是价值函数之间的关系。更具体地说，我们可以利用状态值函数 *V*(*s*)
    和动作值函数 *Q*(*s, a*) 通过动作优势函数 *A*(*s, a*) 相互关联的事实。
- en: '| ![](../Images/icons_Memory.png) | Refresh My MemoryValue functions recap
    |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ![图标](../Images/icons_Memory.png) | 刷新我的记忆价值函数回顾 |'
- en: '|  | ![](../Images/10_00_Sidebar_01.png) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/10_00_Sidebar_01.png) |'
- en: Nuances of value-based deep reinforcement learning methods
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于价值的深度强化学习方法的细微差别
- en: 'The action-value function *Q*(*s, a*) can be defined as the sum of the state-value
    function *V*(*s*) and the action-advantage function *A*(*s, a*). This means that
    we can decompose a Q-function into two components: one that’s shared across all
    actions, and another that’s unique to each action; or, to say it another way,
    a component that is dependent on the action and another that isn’t.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值函数 *Q*(*s, a*) 可以定义为状态值函数 *V*(*s*) 和动作优势函数 *A*(*s, a*) 的和。这意味着我们可以将 Q 函数分解为两个部分：一个在所有动作中共享，另一个对每个动作都是独特的；或者说，一个依赖于动作，另一个不依赖于动作。
- en: Currently, we’re learning the action-value function *Q*(*s, a*) for each action
    separately, but that’s inefficient. Of course, there’s a bit of generalization
    happening because networks are internally connected. Therefore, information is
    shared between the nodes of a network. But, when learning about *Q*(*s, a*[1]),
    we’re ignoring the fact that we could use the same information to learn something
    about *Q*(*s, a*[2]), *Q*(*s, a*[3]), and all other actions available in state
    *s*. The fact is that *V*(*s*) is common to all actions *a*[1], *a*[2], *a*[3],
    ..., *a*[*N*].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们正在分别学习每个动作的动作-值函数 *Q*(*s, a*)，但这效率不高。当然，由于网络内部是相互连接的，所以会有一些泛化发生。因此，网络节点之间会共享信息。但是，当我们学习
    *Q*(*s, a*[1]) 时，我们忽略了这样一个事实：我们可以使用相同的信息来学习关于 *Q*(*s, a*[2])、*Q*(*s, a*[3]) 以及在状态
    *s* 中可用的所有其他动作的信息。事实上，*V*(*s*) 对所有动作 *a*[1]、*a*[2]、*a*[3]，...、*a*[*N*] 都是通用的。
- en: '![](../Images/10_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![10_01.png](../Images/10_01.png)'
- en: Efficient use of experiences
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有效利用经验
- en: '| ![](../Images/icons_Boil.png) | Boil It DownThe action-value function *Q*(*s,
    a*) depends on the state-value function *V*(*s*) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![煮](../Images/icons_Boil.png) | 简化它动作-值函数 *Q*(*s, a*) 依赖于状态-值函数 *V*(*s*)
    |'
- en: '|  | The bottom line is that the values of actions depend on the values of
    states, and it would be nice to leverage this fact. In the end, taking the worst
    action in a good state could be better than taking the best action in a bad state.
    You see how “the values of actions depend on values of states”?The dueling network
    architecture uses this dependency of the action-value function *Q*(*s, a*) on
    the state-value function *V*(*s*) such that every update improves the state-value
    function *V*(*s*) estimate, which is common to all actions. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | 核心问题是动作的值取决于状态的值，我们希望利用这一事实。最终，在好状态下采取最差动作可能比在坏状态下采取最佳动作更好。你看到了“动作的值取决于状态的值”吗？对抗网络架构利用动作-值函数
    *Q*(*s, a*) 对状态-值函数 *V*(*s*) 的这种依赖性，以便每次更新都改进状态-值函数 *V*(*s*) 的估计，这对于所有动作都是通用的。
    |'
- en: Advantage of using advantages
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用优势的优势
- en: Now, let me give you an example. In the cart-pole environment, when the pole
    is in the upright position, the values of the left and right actions are virtually
    the same. It doesn’t matter what you do when the pole is precisely upright (for
    the sake of argument, assume the cart is precisely in the middle of the track
    and that all velocities are 0). Going either left or right should have the same
    value in this perfect state.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我给你举一个例子。在购物车-杆环境中，当杆处于直立位置时，左右动作的值几乎相同。当杆精确直立时（为了论证，假设购物车精确位于轨道中间，并且所有速度都是
    0），无论你做什么都无关紧要。在这种完美的状态下，向左或向右移动应该具有相同的值。
- en: However, it does matter what action you take when the pole is tilted 10 degrees
    to the right, for instance. In this state, pushing the cart to the right to counter
    the tilt, is the best action the agent can take. Conversely, going left, and consequently,
    pronouncing the tilt is probably a bad idea.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当杆向右倾斜 10 度时，采取的动作确实很重要。在这种情况下，推动购物车向右以抵消倾斜是代理可以采取的最佳动作。相反，向左移动，并因此加剧倾斜，可能是一个糟糕的主意。
- en: 'Notice that this is what the action-advantage function *A*(*s, a*) represents:
    how much better than average is taking this particular action *a* in the current
    state *s*?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这正是动作-优势函数 *A*(*s, a*) 所表示的：在当前状态 *s* 中采取特定动作 *a* 比平均水平好多少？
- en: '![](../Images/10_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![10_02.png](../Images/10_02.png)'
- en: Relationship between value functions
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数之间的关系
- en: A reinforcement-learning-aware architecture
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习感知架构
- en: The dueling network architecture consists of creating two separate estimators,
    one of the state-value function *V*(*s*), and the other of the action-advantage
    function *A*(*s, a*). Before splitting up the network, though, you want to make
    sure your network shares internal nodes. For instance, if you’re using images
    as inputs, you want the convolutions to be shared so that feature-extraction layers
    are shared. In the cart-pole environment, we share the hidden layers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗网络架构包括创建两个独立的估计器，一个是状态-值函数 *V*(*s*)，另一个是动作-优势函数 *A*(*s, a*)。然而，在分割网络之前，你想要确保你的网络共享内部节点。例如，如果你使用图像作为输入，你想要共享卷积，以便特征提取层可以共享。在购物车-杆环境中，我们共享隐藏层。
- en: 'After sharing most of the internal nodes and layers, the layer before the output
    layers splits into two streams: a stream for the state-value function *V*(*s*),
    and another for the action-advantage function *A*(*s, a*). The V-function output
    layer always ends in a single node because the value of a state is always a single
    number. The output layer for the Q-function, however, outputs a vector of the
    same size as the number of actions. In the cart-pole environment, the output layer
    of the action-advantage function stream has two nodes, one for the left action,
    and the other for the right action.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享了大部分内部节点和层之后，输出层之前的层分为两个流：一个用于状态值函数*V*(*s*)的流，另一个用于动作优势函数*A*(*s, a*)的流。V函数输出层总是以单个节点结束，因为状态的价值始终是一个单一的数字。然而，Q函数的输出层输出一个与动作数量相同的向量。在cart-pole环境中，动作优势函数流的输出层有两个节点，一个用于左边的动作，另一个用于右边的动作。
- en: '![](../Images/10_03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10_03.png)'
- en: Dueling network architecture
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对弈网络架构
- en: '| 0001 | A Bit Of HistoryIntroduction of the dueling network architecture |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍对弈网络架构 |'
- en: '|  | The Dueling neural network architecture was introduced in 2015 in a paper
    called “Dueling Network Architectures for Deep Reinforcement Learning” by Ziyu
    Wang when he was a PhD student at the University of Oxford. This was arguably
    the first paper to introduce a custom deep neural network architecture designed
    specifically for value-based deep reinforcement learning methods.Ziyu is now a
    research scientist at Google DeepMind, where he continues to contribute to the
    field of deep reinforcement learning. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | 对弈神经网络架构是在2015年由Ziyu Wang在牛津大学攻读博士学位时，在名为“用于深度强化学习的对弈网络架构”的论文中提出的。这篇论文可以说是第一篇介绍专门为基于价值的深度强化学习方法设计的自定义深度神经网络架构的论文。Ziyu现在是Google
    DeepMind的研究科学家，在那里他继续为深度强化学习领域做出贡献。|'
- en: Building a dueling network
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建对弈网络
- en: Building the dueling network is straightforward. I noticed that you could split
    the network anywhere after the input layer, and it would work just fine. I can
    imagine you could even have two separate networks, but I don’t see the benefits
    of doing that. In general, my recommendation is to share as many layers as possible
    and split only in two heads, a layer before the output layer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 构建对弈网络很简单。我注意到你可以在输入层之后任何地方分割网络，它仍然可以正常工作。我可以想象你甚至可以有两个独立的网络，但我看不到这样做的好处。总的来说，我的建议是尽可能多地共享层，只在输出层之前的一个层中分割成两个头部。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonBuilding the dueling network
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python构建对弈网络 |'
- en: '|  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① The dueling network is similar to the regular network. We need variables for
    the number of nodes in the input and output layers, the shape of the hidden layers,
    and the activation function, the way we did before.② Next, we create the input
    layer and “connect” it to the first hidden layer. Here the input_dim variable
    is the number of input nodes, and hidden_dims[0] is the number of nodes of the
    first hidden layer. nn.Linear creates a layer with inputs and outputs.③ Here we
    create the hidden layers by creating layers as defined in the hidden_dims variable.
    For example, a value of (64, 32, 16) will create a layer with 64 input nodes and
    32 output nodes, and then a layer with 32 input nodes and 16 output nodes.④ Finally,
    we build the two output layers, both “connected” to the last hidden layer. The
    value_output has a single node output, and the advantage_output has output_dim
    nodes. In the cart-pole environment, that number is two. |
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ① 对弈网络与常规网络类似。我们需要为输入层和输出层的节点数量、隐藏层的形状以及激活函数设置变量，就像我们之前做的那样。② 接下来，我们创建输入层并将其“连接”到第一个隐藏层。在这里，input_dim变量是输入节点的数量，而hidden_dims[0]是第一个隐藏层的节点数量。nn.Linear创建了一个具有输入和输出的层。③
    我们通过创建隐藏层来创建隐藏层，这些层由hidden_dims变量定义。例如，值为(64, 32, 16)将创建一个具有64个输入节点和32个输出节点的层，然后是一个具有32个输入节点和16个输出节点的层。④
    最后，我们构建两个输出层，它们都“连接”到最后的隐藏层。value_output有一个节点输出，而advantage_output有output_dim个节点。在cart-pole环境中，这个数字是两个。|
- en: Reconstructing the action-value function
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重建动作值函数
- en: First, let me clarify that the motivation of the dueling architecture is to
    create a new network that improves on the previous network, but without having
    to change the underlying control method. We need changes that aren’t disruptive
    and that are compatible with previous methods. We want to swap the neural network
    and be done with it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我澄清一下，对抗架构的动机是创建一个新的网络，这个网络在改进先前网络的同时，无需改变底层控制方法。我们需要的是不破坏性的变化，并且与先前的方法兼容。我们希望替换神经网络，然后完成它。
- en: For this, we need to find a way to aggregate the two outputs from the network
    and reconstruct the action-value function *Q*(*s, a*), so that any of the previous
    methods could use the dueling network model. This way, we create the dueling DDQN
    agent when using the dueling architecture with the DDQN agent. A dueling network
    and the DQN agent would make the dueling DQN agent.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们需要找到一种方法来聚合网络输出的两个结果，并重建动作值函数 *Q*(*s, a*)，以便任何先前的方法都可以使用对抗网络模型。这样，当使用对抗架构与DDQN代理结合时，我们就创建了对抗DDQN代理。一个对抗网络和DQN代理将构成对抗DQN代理。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathDueling architecture aggregating
    equations |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式：对抗架构聚合方程'
- en: '|  | ![](../Images/10_03_Sidebar05.png) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/10_03_Sidebar05.png) |'
- en: But, how do we join the outputs? Some of you are thinking, add them up, right?
    I mean, that’s the definition that I provided, after all. Though, several of you
    may have noticed that there is no way to recover *V*(*s*) and *A*(*s, a*) uniquely
    given only *Q*(*s, a*). Think about it; if you add +10 to *V*(*s*) and remove
    it from *A*(*s, a*) you obtain the same *Q*(*s, a*) with two different values
    for *V*(*s*) and *A*(*s, a*).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们如何将输出合并呢？有些人可能会想，把它们加起来，对吧？我的意思是，毕竟这是我提供的定义。尽管如此，你们中的一些人可能已经注意到，仅凭 *Q*(*s,
    a*) 无法唯一地恢复 *V*(*s*) 和 *A*(*s, a*)。想想看；如果你给 *V*(*s*) 加上+10，并从 *A*(*s, a*) 中减去它，你将得到相同的
    *Q*(*s, a*)，但 *V*(*s*) 和 *A*(*s, a*) 的值却不同。
- en: The way we address this issue in the dueling architecture is by subtracting
    the mean of the advantages from the aggregated action-value function *Q*(*s, a*)
    estimate. Doing this shifts *V*(*s*) and *A*(*s, a*) off by a constant, but also
    stabilizes the optimization process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗架构中，我们通过从聚合的动作值函数 *Q*(*s, a*) 估计中减去优势的平均值来解决这个问题的。这样做会将 *V*(*s*) 和 *A*(*s,
    a*) 偏移一个常数，但也会稳定优化过程。
- en: While estimates are off by a constant, they do not change the relative rank
    of *A*(*s, a*), and therefore *Q*(*s, a*) also has the appropriate rank. All of
    this, while still using the same control algorithm. Big win.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当估计值有常数偏差时，它们不会改变 *A*(*s, a*) 的相对排名，因此 *Q*(*s, a*) 也具有适当的排名。所有这些，同时仍然使用相同的控制算法。这是一个大胜利。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe forward pass of a dueling
    network |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python：对抗网络的正向传播'
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Notice that this is the same class as before. I removed the code for building
    of the network for brevity.② In the forward pass, we start by making sure the
    input to the network, the ‘state,’ is of the expected type and shape. We do this
    because sometimes we input batches of states (training), sometimes single states
    (interacting). Sometimes these are NumPy vectors.③ At this point, we’ve prepped
    the input (again single or batch of states) variable *x* to what the network expects.
    we pass the variable *x* to the input layer, which, remember, takes in input_dim
    variables and outputs hidden_dim[0] variables; those will then pass through the
    activation function.④ We use that output as the input for our first hidden layer.
    We pass the variable *x*, which you can think of as the current state of a pulse
    wave that goes from the input to the output of the network, sequentially to each
    hidden layer and the activation function.⑤ *x* now contains the values that came
    out of the last hidden layer and its respective activation. We use those as the
    input to the advantage_output and the value_output layers. Since *v* is a single
    value that will be added to *a*, we expand it.⑥ Finally, we add *v* and *a* and
    subtract the mean of *a* from it. That’s our *Q*(*s*, .) estimate, containing
    the estimates of all actions for all states. |
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ① 注意，这与之前相同。为了简洁，我移除了构建网络的代码。② 在前向传递中，我们首先确保网络的输入，即‘状态’，是预期的类型和形状。我们这样做是因为有时我们输入的是状态批次（训练），有时是单个状态（交互）。有时这些是NumPy向量。③
    在这一点上，我们已经将输入变量*x*（再次是单个或状态批次）准备成网络所期望的。我们将变量*x*传递到输入层，记住，输入层接受input_dim变量并输出hidden_dim[0]变量；这些将随后通过激活函数。④
    我们使用那个输出作为我们第一个隐藏层的输入。我们将变量*x*（你可以将其视为脉冲波从输入到网络输出的当前状态）依次传递到每个隐藏层和激活函数。⑤ *x*现在包含了来自最后一个隐藏层及其相应激活的值。我们使用这些值作为优势输出和价值输出层的输入。由于*v*是一个将添加到*a*的单个值，我们将其扩展。⑥
    最后，我们将*v*和*a*相加，并从其中减去*a*的均值。这就是我们的*Q*(*s*, .)估计，包含了所有状态下所有动作的估计。|
- en: Continuously updating the target network
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持续更新目标网络
- en: Currently, our agent is using a target network that can be outdated for several
    steps before it gets a big weight update when syncing with the online network.
    In the cart-pole environment, that’s merely ~15 steps apart, but in more complex
    environments, that number can rise to tens of thousands.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的代理正在使用一个目标网络，在它与在线网络同步时，可能需要经过几步更新才能获得较大的权重更新。在CartPole环境中，这仅仅是相隔~15步，但在更复杂的环境中，这个数字可能会上升到数万。
- en: '![](../Images/10_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10_04.png)'
- en: Full target network update
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的目标网络更新
- en: There are at least a couple of issues with this approach. On the one hand, we’re
    freezing the weights for several steps and calculating estimates with progressively
    increasing stale data. As we reach the end of an update cycle, the likelihood
    of the estimates being of no benefit to the training progress of the network is
    higher. On the other hand, every so often, a huge update is made to the network.
    Making a big update likely changes the whole landscape of the loss function all
    at once. This update style seems to be both too conservative and too aggressive
    at the same time, if that’s possible.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法至少存在两个问题。一方面，我们在几步内冻结了权重，并使用逐渐增加的过时数据来计算估计值。当我们达到更新周期的末尾时，估计值对网络训练进展没有帮助的可能性更高。另一方面，网络每隔一段时间就会进行一次巨大的更新。进行一次大的更新可能会一次性改变损失函数的全局景观。这种更新风格似乎既过于保守又过于激进，如果可能的话。
- en: We got into this issue because we wanted our network not to move too quickly
    and therefore create instabilities, and we still want to preserve those desirable
    traits. But, can you think of other ways we can accomplish something similar but
    in a smooth manner? How about slowing down the target network, instead of freezing
    it?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到这个问题是因为我们希望我们的网络不要移动得太快，从而避免产生不稳定性，同时我们还想保留那些理想的特性。但是，你能想到其他我们可以以平滑方式实现类似目标的方法吗？比如，减缓目标网络的更新速度，而不是将其冻结？
- en: We can do that. The technique is called *Polyak Averaging,* and it consists
    of mixing in online network weights into the target network on every step. Another
    way of seeing it is that, every step, we create a new target network composed
    of a large percentage of the target network weights and a small percentage of
    the online network weights. We add ~1% of new information every step to the network.
    Therefore, the network always lags, but by a much smaller gap. Additionally, we
    can now update the network on each step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做到。这种技术被称为 *Polyak平均*，它包括在每一步将在线网络权重混合到目标网络中。另一种看待它的方法是，每一步，我们创建一个新的目标网络，该网络由大量目标网络权重和少量在线网络权重组成。我们每步向网络添加约1%的新信息。因此，网络总是落后，但差距要小得多。此外，我们现在可以在每一步更新网络。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathPolyak averaging |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学Polyak平均 |'
- en: '|  | ![](../Images/10_04_Sidebar07.png) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/10_04_Sidebar07.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonMixing in target and online
    network weights |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python混合目标和在线网络权重 |'
- en: '|  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① This is the same dueling DDQN class, but with most of the code removed for
    brevity.② *tau* is a variable representing the ratio of the online network that
    will be mixed into the target network. A value of 1 is equivalent to a full update.③
    zip takes iterables and returns an iterator of tuples.④ Now, we calculate the
    ratios we’re taking from the target and online weights.⑤ Finally, we mix the weights
    and copy the new values into the target network. |
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这与相同的对抗式DDQN类相同，但为了简洁起见，删除了大部分代码。② *tau* 是一个表示将混合到目标网络中的在线网络权重的比例的变量。值为1相当于完全更新。③
    zip 接受可迭代对象并返回一个元组的迭代器。④ 现在，我们计算从目标和在线权重中取出的比率。⑤ 最后，我们混合权重并将新值复制到目标网络中。 |
- en: What does the dueling network bring to the table?
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗式网络带来了什么？
- en: Action-advantages are particularly useful when you have many similarly valued
    actions, as you’ve seen for yourself. Technically speaking, the dueling architecture
    improves policy evaluation, especially in the face of many actions with similar
    values. Using a dueling network, our agent can more quickly and accurately compare
    similarly valued actions, which is something useful in the cart-pole environment.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 行动优势在您自己看到的情况下，当您有许多类似价值的行动时尤其有用。从技术上来说，对抗式架构提高了策略评估，尤其是在面对许多具有相似价值的行动时。使用对抗式网络，我们的智能体可以更快、更准确地比较具有相似价值的行动，这在小车-杆环境中是有用的。
- en: Function approximators, such as a neural network, have errors; that’s expected.
    In a network with the architecture we were using before, these errors are potentially
    different for all of the state-actions pairs, as they’re all separate. But, given
    the fact that the state-value function is the component of the action-value function
    that’s common to all actions in a state, by using a dueling architecture, we reduce
    the function error and error variance. This is because now the error in the component
    with the most significant magnitude in similarly valued actions (the state-value
    function *V*(*s*)) is now the same for all actions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 函数逼近器，如神经网络，都有误差；这是预期的。在我们之前使用的架构的网络中，这些误差对于所有状态-动作对可能是不同的，因为它们都是分开的。但是，鉴于状态值函数是动作值函数中在状态中所有动作都共有的部分，通过使用对抗式架构，我们减少了函数误差和误差方差。这是因为现在具有最大显著程度的相似价值动作的组成部分（状态值函数
    *V*(*s*)）对所有动作都是相同的。
- en: If the dueling network is improving policy evaluation in our agent, then a fully
    trained dueling DDQN agent should have better performance than the DDQN when the
    left and right actions have almost the same value. I ran an experiment by collecting
    the states of 100 episodes for both, the DDQN and the dueling DDQN agents. My
    intuition tells me that if one agent is better than the other at evaluating similarly
    valued actions, then the better agent should have a smaller range along the track.
    This is because a better agent should learn the difference between going left
    and right, even when the pole is exactly upright. Warning! I didn’t do ablation
    studies, but the results of my hand-wavy experiment suggest that the dueling DDQN
    agent is indeed able to evaluate in those states better.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对弈网络在我们的智能体中提高了策略评估，那么当左右动作几乎具有相同的价值时，一个完全训练好的对弈DDQN智能体应该比DDQN表现更好。我通过收集DDQN和对弈DDQN智能体的100个回合的状态来运行了一个实验。我的直觉告诉我，如果一个智能体在评估类似价值的动作方面比另一个智能体更好，那么更好的智能体在轨迹上的范围应该更小。这是因为更好的智能体应该学会在杆子完全直立时区分左右移动，警告！我没有做消融研究，但我的手波实验的结果表明，对弈DDQN智能体确实能够更好地评估这些状态。
- en: '![](../Images/10_05.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10_05.png)'
- en: State-space visited by fully trained cart-pole agents
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 完全训练的cart-pole智能体访问的状态空间
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsThe dueling double
    deep Q-network (dueling DDQN) algorithm |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Details.png) | 在细节中The dueling double deep Q-network
    (dueling DDQN)算法'
- en: '|  | Dueling DDQN is almost identical to DDQN, and DQN, with only a few tweaks.
    My intention is to keep the differences of the algorithms to a minimal while still
    showing you the many different improvements that can be made. I’m certain that
    changing only a few hyperparameters by a little bit has big effects in performance
    of many of these algorithms; therefore, I don’t optimize the agents. That being
    said, now let me go through the things that are still the same as before:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 对弈DDQN几乎与DDQN和DQN相同，只有一些小的调整。我的意图是在保持算法差异最小化的同时，仍然向您展示可以做出的许多不同改进。我确信，仅通过稍微调整几个超参数就可以对许多这些算法的性能产生重大影响；因此，我没有优化智能体。话虽如此，现在让我来谈谈仍然与之前相同的事情：'
- en: Network outputs the action-value function *Q*(*s, a; θ*).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络输出动作值函数 *Q*(*s, a; θ*）。
- en: Optimize the action-value function to approximate the optimal action-value function
    *q**(*s, a*).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化动作值函数以逼近最优动作值函数 *q**(*s, a*）。
- en: Use off-policy *TD* targets (*r + gamma*max_a’Q*(*s’,a’; θ*)) to evaluate policies.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用离线TD目标（*r + gamma*max_a’Q*(*s’，a’; θ*）来评估策略。
- en: Use an adjustable Huber loss, but still with the max_gradient_norm variable
    set to float(‘inf’). Therefore, we’re using MSE.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可调整的Huber损失，但仍然将max_gradient_norm变量设置为float（‘inf’）。因此，我们使用MSE。
- en: Use RMSprop as our optimizer with a learning rate of 0.0007.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RMSprop作为我们的优化器，学习率为0.0007。
- en: An exponentially decaying epsilon-greedy strategy (from 1.0 to 0.3 in roughly
    20,000 steps) to improve policies.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个指数衰减的epsilon-greedy策略（在约20,000步内从1.0衰减到0.3）来改进策略。
- en: A greedy action selection strategy for evaluation steps.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于评估步骤的贪婪动作选择策略。
- en: A replay buffer with 320 samples min, 50,000 max, and a batch of 64.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有最小样本数320、最大样本数50,000和批次大小64的重放缓冲区。
- en: We replaced
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们替换了
- en: 'The neural network architecture. We now use a state-in-values-out dueling network
    architecture (nodes: 4, 512,128, 1; 2, 2).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络架构。我们现在使用状态-值-输出对弈网络架构（节点：4, 512, 128, 1; 2, 2）。
- en: 'The target network that used to freeze for 15 steps and update fully now uses
    Polyak averaging: every time step, we mix in 0.1 of the online network and 0.9
    of the target network to form the new target network weights.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前用于冻结15步并完全更新的目标网络现在使用Polyak平均：每一步，我们将0.1的在线网络和0.9的目标网络混合形成新的目标网络权重。
- en: 'Dueling DDQN is the same exact algorithm as DDQN, but a different network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对弈DDQN与DDQN完全相同的算法，但网络不同：
- en: 'Collect experience: (*S*[*t*]*, A*[*t*]*, R*[*t+1*]*, S*[*t+1*]*, D*[*t+1*]),
    and insert into the replay buffer.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集经验：(*S*[*t*]*, A*[*t*]*, R*[*t+1*]*, S*[*t+1*]*, D*[*t+1*])，并将其插入重放缓冲区。
- en: 'Pull a batch out of the buffer and calculate the off-policy *TD* targets: *R
    + gamma*max_a’Q*(*s’,a’; θ*), using double learning.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从缓冲区中拉取一批并计算离线TD目标：*R + gamma*max_a’Q*(*s’，a’; θ*），使用双学习。
- en: Fit the action-value function *Q*(*s,a; θ*), using MSE and RMSprop.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MSE和RMSprop拟合动作值函数 *Q*(*s,a; θ*）。
- en: One cool thing to notice is that all of these improvements are like Lego™ blocks
    for you to get creative. Maybe you want to try dueling DQN, without the double
    learning; maybe you want the Huber loss to clip gradients; or maybe you like the
    Polyak averaging to mix 50:50 every 5 time steps. It’s up to you! Hopefully, the
    way I’ve organized the code will give you the freedom to try things out. |
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得注意的酷点是，所有这些改进都像是乐高™积木，供你发挥创意。也许你想尝试对抗式DQN，但不使用双重学习；也许你想让Huber损失函数剪辑梯度；或者也许你喜欢Polyak平均化，每5个时间步混合50:50。这取决于你！希望我组织代码的方式能给你尝试新事物的自由。
    |
- en: '| ![](../Images/icons_Tally.png) | Tally it UpDueling DDQN is more data efficient
    than all previous methods |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 累计优势对抗式DDQN比所有先前方法都更有效率'
- en: '|  | Dueling DDQN and DDQN have similar performance in the cart-pole environment.
    Dueling DDQN is slightly more data efficient. The number of samples DDQN needs
    to pass the environment is higher than that of dueling DDQN. However, dueling
    DDQN takes slightly longer than DDQN.![](../Images/10_05_Sidebar10.png) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 对抗式DDQN和DDQN在Cart-Pole环境中表现相似。对抗式DDQN在数据效率上略高。DDQN需要通过环境的样本数量高于对抗式DDQN。然而，对抗式DDQN的运行时间略长于DDQN。![](../Images/10_05_Sidebar10.png)
    |'
- en: 'PER: Prioritizing the replay of meaningful experiences'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PER：优先回放有意义经验
- en: In this section, we introduce a more intelligent experience replay technique.
    The goal is to allocate resources for experience tuples that have the most significant
    potential for learning. *prioritized experience replay* (PER) is a specialized
    replay buffer that does just that.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一种更智能的经验回放技术。目标是分配资源给那些具有最大学习潜力的经验元组。*优先经验回放*（PER）是一个专门用于此的回放缓冲区。
- en: A smarter way to replay experiences
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更智能的回放经验方式
- en: At the moment, our agent samples experience tuples from the replay buffer uniformly
    at random. Mathematically speaking, this feels right, and it is. But intuitively,
    this seems like an inferior way of replaying experiences. Replaying uniformly
    at random allocates resources to unimportant experiences. It doesn’t feel right
    that our agent spends time and compute power “learning” things that have nothing
    to offer to the current state of the agent
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的智能体从回放缓冲区中均匀随机地抽取经验元组。从数学上讲，这似乎是正确的，确实如此。但直观上，这似乎是一种回放经验的次优方式。均匀随机回放将资源分配给了不重要的经验。感觉上，我们的智能体花费时间和计算能力“学习”那些对当前智能体状态毫无帮助的事情是不合适的。
- en: 'But, let’s be careful here: while it’s evident that uniformly at random isn’t
    good enough, it’s also the case that human intuition might not work well in determining
    a better learning signal. When I first implemented a prioritized replay buffer,
    before reading the PER paper, my first thought was, “Well, I want the agent to
    get the highest cumulative discounted rewards possible; I should have it replay
    experiences with high reward only.” Yeah, that didn’t work. I then realized agents
    also need negative experiences, so I thought, “Aha! I should have the agent replay
    experiences with the highest reward magnitude! Besides, I love using that ‘abs’
    function!” But that didn’t work either. Can you think why these experiments didn’t
    work? It makes sense that if I want the agent to learn to experience rewarding
    states, I should have it replay those the most so that it learns to get there.
    Right?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但，在这里我们要小心：虽然均匀随机并不足够好，但人类直觉在确定更好的学习信号时可能也不太有效。当我第一次实现优先回放缓冲区，在阅读PER论文之前，我的第一个想法是，“嗯，我想让智能体获得尽可能高的累积折现奖励；我应该只让它回放高奖励的经验。”是的，这没有奏效。然后我意识到智能体也需要负面经验，所以我想到，“啊！我应该让智能体回放奖励幅度最高的经验！此外，我喜欢使用那个‘abs’函数！”但这也没有奏效。你能想出为什么这些实验没有奏效吗？如果我想让智能体学会体验奖励状态，我应该让它回放最多的那些，这样它才能学会到达那里。对吧？
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyHuman intuition and the
    relentless pursuit of happiness |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Miguel.png) | 米格尔的类比人类直觉和对幸福的执着追求'
- en: '|  | I love my daughter. I love her so much. In fact, so much that I want her
    to experience only the good things in life. No, seriously, if you’re a parent,
    you know what I mean.I noticed she likes chocolate a lot, or as she would say,
    “a bunch,” so, I started opening up to giving her candies every so often. And
    then more often than not. But, then she started getting mad at me when I didn’t
    think she should get a candy.Too much high-reward experiences, you think? You
    bet! Agents (maybe even humans) need to be reminded often of good and bad experiences
    alike, but they also need mundane experiences with low-magnitude rewards. In the
    end, none of these experiences give you the most learning, which is what we’re
    after. Isn’t that counterintuitive? |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 我爱我女儿。我非常爱她。事实上，如此之爱，以至于我希望她只体验生活中美好的事物。不，说真的，如果你是父母，你就知道我的意思。我发现她非常喜欢巧克力，或者像她说的那样，“一大堆”，所以我开始时不时地给她一些糖果。然后，大多数时候都是这样。但是，然后当她觉得我不应该给她糖果时，她开始生我的气。太多的高奖励体验，你认为吗？没错！代理（甚至可能是人类）需要经常提醒他们好与坏的经验，但他们也需要低强度奖励的平凡经验。最终，这些经验都没有给你带来最多的学习，这正是我们追求的目标。这不是反直觉吗？'
- en: Then, what’s a good measure of “important” experiences?
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，“重要”经验的好度量是什么？
- en: What we’re looking for is to learn from experiences with unexpected value, surprising
    experiences, experiences we thought should be valued this much, and ended up valued
    that much. That makes more sense; these experiences bring reality to us. We have
    a view of the world, we anticipate outcomes, and when the difference between expectation
    and reality is significant, we know we need to learn something from that.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所寻找的是从意外价值、令人惊讶的经验、我们认为应该这样评价的经验中学习，结果却评价了那样。这更有意义；这些经验将现实带给我们。我们有一个对世界的看法，我们预测结果，当期望与现实之间的差异很大时，我们知道我们需要从中学到一些东西。
- en: In reinforcement learning, this measure of surprise is given by the *TD* error!
    Well, technically, the *absolute* *TD* error. The *TD* error provides us with
    the difference between the agent’s current estimate and target value. The current
    estimate indicates the value our agent thinks it's going to get for acting in
    a specific way. The target value suggests a new estimate for the same state-action
    pair, which can be seen as a reality check. The absolute difference between these
    values indicates how far off we are, how unexpected this experience is, and how
    much new information we received, which makes it a good indicator for learning
    opportunity.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，这种惊喜的度量由 *TD* 错误给出！好吧，技术上讲，是 *绝对* *TD* 错误。*TD* 错误为我们提供了代理当前估计值和目标值之间的差异。当前的估计值表示代理认为它将以某种特定方式行动所能获得的价值。目标值建议对相同的状态-动作对的新估计，这可以看作是一种现实检查。这些值之间的绝对差异表明我们偏离了多少，这种体验有多么出乎意料，我们获得了多少新信息，这使得它成为学习机会的良好指标。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe absolute TD error is
    the priority |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式The absolute TD error is the priority
    |'
- en: '|  | ![](../Images/10_05_Sidebar12.png) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/10_05_Sidebar12.png) |'
- en: The *TD* error isn’t the perfect indicator of the highest learning opportunity,
    but maybe the best reasonable proxy for it. In reality, the best criterion for
    learning the most is inside the network and hidden behind parameter updates. But,
    it seems impractical to calculate gradients for all experiences in the replay
    buffer every time step. The good things about the *TD* error are that the machinery
    to calculate it is in there already, and of course, the fact that the *TD* error
    is still a good signal for prioritizing the replay of experiences.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*TD* 错误并不是最高学习机会的完美指标，但可能是最好的合理代理。实际上，学习最多最好的标准在网络的内部，隐藏在参数更新之后。但是，每次时间步都计算回放缓冲区中所有经验的梯度似乎不太实际。*TD*
    错误的好处是计算它的机制已经存在，当然，*TD* 错误仍然是优先回放经验的良好信号。'
- en: Greedy prioritization by *TD* error
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过 *TD* 错误进行贪婪优先级排序
- en: 'Let’s pretend we use *TD* errors for prioritizing experiences are follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们使用 *TD* 错误来优先处理经验如下：
- en: Take action a in state s and receive a new state *s'*, a reward *r*, and a done
    flag *d*.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在状态 s 中采取行动 a，并接收一个新的状态 *s'*)，一个奖励 *r*，以及一个完成标志 *d*。
- en: Query the network for the estimate of the current state *Q*(*s, a; θ*).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询网络以获取当前状态 *Q*(*s, a; θ*) 的估计。
- en: Calculate a new target value for that experience as *target = r + gamma*max_a'Q*
    (*s',a'; θ*).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为该经验计算一个新的目标值，即 *target = r + gamma*max_a'Q* (*s',a'; θ*)。
- en: Calculate the absolute *TD* error as *atd_err = abs*(*Q*(*s, a; θ*) – *target*).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算绝对*TD*误差为*atd_err = abs*(*Q*(*s, a; θ*) – *target*）。
- en: Insert experience into the replay buffer as a tuple (*s, a, r, s', d, atd_err*).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将经验作为一个元组（*s, a, r, s', d, atd_err*）插入到重放缓冲区中。
- en: Pull out the top experiences from the buffer when sorted by *atd_err*.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照排序后的*atd_err*从缓冲区中提取最优先的经验。
- en: Train with these experiences, and repeat.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些经验进行训练，并重复。
- en: 'There are multiple issues with this approach, but let’s try to get them one
    by one. First, we are calculating the *TD* errors twice: we calculate the *TD*
    error before inserting it into the buffer, but then again when we train with the
    network. In addition to this, we’re ignoring the fact that *TD* errors change
    every time the network changes because they’re calculated using the network. But,
    the solution can’t be updating all of the *TD* errors every time step. It’s simply
    not cost effective.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在多个问题，但让我们逐一解决。首先，我们正在计算*TD*误差两次：我们在将其插入缓冲区之前计算*TD*误差，但然后在用网络训练时再次计算。此外，我们忽略了*TD*误差会随着网络的变化而变化的事实，因为它们是使用网络计算的。但是，解决方案不能是每次时间步更新所有*TD*误差。这根本不划算。
- en: A workaround for both these problems is to update the *TD* errors only for experiences
    that are used to update the network (the replayed experiences) and insert new
    experiences with the highest magnitude *TD* error in the buffer to ensure they’re
    all replayed at least once.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这两个问题的方法是为用于更新网络（重放的经验）的经验更新*TD*误差，并在缓冲区中插入具有最大幅度*TD*误差的新经验，以确保它们至少被重放一次。
- en: However, from this workaround, other issues arise. First, a *TD* error of zero
    in the first update means that experience will likely never be replayed again.
    Second, when using function approximators, errors shrink slowly, and this means
    that updates concentrate heavily in a small subset of the replay buffer. And finally,
    *TD* errors are noisy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从这个解决方案中，其他问题也随之产生。首先，第一次更新中的*TD*误差为零意味着经验很可能永远不会再次被重放。其次，当使用函数逼近器时，误差缩小得很慢，这意味着更新主要集中在重放缓冲区的小子集。最后，*TD*误差是嘈杂的。
- en: For these reasons, we need a strategy for sampling experiences based on the
    *TD* errors, but stochastically, not greedily. If we sample prioritized experiences
    stochastically, we can simultaneously ensure all experiences have a chance of
    being replayed, and that the probabilities of sampling experiences are monotonic
    in the absolute *TD* error.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，我们需要一个基于*TD*误差的采样策略，但必须是随机的，而不是贪婪的。如果我们随机采样优先经验，我们就可以同时确保所有经验都有机会被重放，并且采样经验的概率在绝对*TD*误差上是单调的。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownTD errors, priorities, and probabilities
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化TD误差、优先级和概率 |'
- en: '|  | The most important takeaway from this page is that TD errors aren’t enough;
    we’ll use TD errors to calculate priorities, and from priorities we calculate
    probabilities. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | 本页最重要的启示是TD误差不足以解决问题；我们将使用TD误差来计算优先级，然后从优先级计算概率。 |'
- en: Sampling prioritized experiences stochastically
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机采样优先经验
- en: Allow me to dig deeper into why we need stochastic prioritization. In highly
    stochastic environments, learning from experiences sampled greedily based on the
    *TD* error may lead us to where the noise takes us.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我深入探讨一下为什么我们需要随机优先级。在高度随机的环境中，基于*TD*误差贪婪采样的经验可能会导致我们走向噪声所引导的方向。
- en: TD errors depend on the one-step reward and the action-value function of the
    next state, both of which can be highly stochastic. Highly stochastic environments
    can have higher variance *TD* errors. In such environments, we can get ourselves
    into trouble if we let our agents strictly follow the *TD* error. We don’t want
    our agents to get fixated with surprising situations; that’s not the point. An
    additional source of noise in the *TD* error is the neural network. Using highly
    non-linear function approximators also contributes to the noise in *TD* errors,
    especially early during training when errors are the highest. If we were to sample
    greedily solely based on *TD* errors, much of the training time would be spent
    on the experiences with potentially inaccurately large magnitude *TD* error.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: TD误差取决于一步奖励和下一个状态的动作值函数，这两者都可以是非常随机的。高度随机的环境可以有更高的方差*TD*误差。在这样的环境中，如果我们让我们的智能体严格遵循*TD*误差，我们可能会陷入麻烦。我们不希望我们的智能体对意外情况产生固定观念；那不是我们的目标。*TD*误差中的另一个噪声来源是神经网络。使用高度非线性的函数逼近器也导致了*TD*误差中的噪声，尤其是在训练早期，错误最高的时候。如果我们仅仅基于*TD*误差贪婪地采样，大部分的训练时间将花费在那些可能具有不准确的大幅度*TD*误差的经验上。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownSampling prioritized experiences
    stochastically |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化优先经验随机采样 |'
- en: '|  | TD errors are noisy and shrink slowly. We don’t want to stop replaying
    experiences that, due to noise, get a TD error value of zero. We don’t want to
    get stuck with noisy experiences that, due to noise, get a significant TD error.
    And, we don’t want to fixate on experiences with an initially high TD error. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | TD误差是有噪声且缓慢缩小的。我们不希望因为噪声而停止重放那些由于噪声而得到零TD误差值的经验。我们不希望陷入由于噪声而得到显著TD误差的噪声经验。而且，我们也不希望专注于初始TD误差高的经验。
    |'
- en: '| 0001 | A Bit Of HistoryIntroduction of the prioritized experience replay
    buffer |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 0001 | 一点历史介绍优先经验重放缓冲区 |'
- en: '|  | The paper “Prioritized Experience Replay”was introduced simultaneously
    with the dueling architecture paper in 2015 by the Google DeepMind folks.Tom Schaul,
    a Senior Research Scientist at Google DeepMind, is the main author of the PER
    paper. Tom obtained his PhD in 2011 from the Technical University of Munich. After
    two years as a postdoc at New York University, Tom joined DeepMind Technologies,
    which six months later would be acquired by Google and turned into what today
    is Google DeepMind.Tom is a core developer of the PyBrain framework, a modular
    machine learning library for Python. PyBrain was probably one of the earlier frameworks
    to implement machine learning, reinforcement learning and black-box optimization
    algorithms. He’s also a core developer of PyVGDL, a high-level video game description
    language built on top of pygame. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 2015年，与对抗网络论文同时由谷歌DeepMind团队引入的“优先经验重放”论文。汤姆·沙乌尔（Tom Schaul），谷歌DeepMind的高级研究科学家，是PER论文的主要作者。汤姆于2011年从慕尼黑工业大学获得博士学位。在纽约大学担任博士后两年后，汤姆加入了DeepMind
    Technologies，六个月后，该公司被谷歌收购，并发展成为今天的谷歌DeepMind。汤姆是PyBrain框架的核心开发者，这是一个用于Python的模块化机器学习库。PyBrain可能是最早实现机器学习、强化学习和黑盒优化算法的框架之一。他也是PyVGDL的核心开发者，这是一个基于pygame的高级视频游戏描述语言。
    |'
- en: Proportional prioritization
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比例优先级
- en: Let’s calculate priorities for each sample in the buffer based on *TD* errors.
    A first approach to do so is to sample experiences in proportion to their absolute
    *TD* error. We can use the absolute *TD* error of each experience and add a small
    constant, epsilon, to make sure zero *TD* error samples still have a chance of
    being replayed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据*TD*误差计算缓冲区中每个样本的优先级。一个这样做的方法是按比例根据它们的绝对*TD*误差来采样经验。我们可以使用每个经验的绝对*TD*误差并添加一个小的常数epsilon，以确保零*TD*误差样本仍有被重放的机会。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathProportional prioritization
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学比例优先级 |'
- en: '|  | ![](../Images/10_05_Sidebar16.png) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/10_05_Sidebar16.png) |'
- en: We scale this priority value by exponentiating it to alpha, a hyperparameter
    between zero and one. That allows us to interpolate between uniform and prioritized
    sampling. It allows us to perform the stochastic prioritization we discussed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将优先级值指数化为alpha（一个介于零和一之间的超参数）来缩放这个优先级值。这允许我们在均匀采样和优先级采样之间进行插值。它允许我们执行我们讨论过的随机优先级。
- en: When alpha is zero, all values become one, therefore, an equal priority. When
    alpha is one, all values stay the same as the absolute *TD* error; therefore,
    the priority is proportional to the absolute *TD* error—a value in between blends
    the two sampling strategies.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当 alpha 为零时，所有值都变为 1，因此，具有相等的优先级。当 alpha 为一时，所有值保持与绝对 *TD* 错误相同；因此，优先级与绝对 *TD*
    错误成比例——介于两者之间的值混合了两种采样策略。
- en: These scaled priorities are converted to actual probabilities only by dividing
    their values by the sum of the values. Then, we can use these probabilities for
    drawing samples from the replay buffer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这些缩放后的优先级只有通过将它们的值除以值的总和才能转换为实际概率。然后，我们可以使用这些概率从重放缓冲区中抽取样本。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathPriorities to probabilities
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 将优先级转换为概率 |'
- en: '|  | ![](../Images/10_05_Sidebar17.png) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/10_05_Sidebar17.png) |'
- en: Rank-based prioritization
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于排名的优先级
- en: One issue with the proportional-prioritization approach is that it’s sensitive
    to outliers. That means experiences with much higher *TD* error than the rest,
    whether by fact or noise, are sampled more often than those with low magnitudes,
    which may be an undesired side effect.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 比例优先级方法的一个问题是它对异常值敏感。这意味着具有比其他经验更高的 *TD* 错误（无论是事实还是噪声）的经验被采样的频率更高，这可能是不可期望的副作用。
- en: A slightly different experience prioritization approach to calculating priorities
    is to sample them using the rank of the samples when sorted by their absolute
    *TD* error.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算优先级时，一种略微不同的经验优先级方法是通过使用按绝对 *TD* 错误排序的样本的排名来采样。
- en: Rank here means the position of the sample when sorted in descending order by
    the absolute *TD* error—nothing else. For instance, prioritizing based on the
    rank makes the experience with the highest absolute *TD* error rank 1, the second
    rank 2, and so on.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的排名意味着按绝对 *TD* 错误降序排序的样本的位置——没有其他含义。例如，基于排名的优先级使得具有最高绝对 *TD* 错误的经验排名为 1，第二名为
    2，依此类推。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathRank-based prioritization
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学计算 |'
- en: '|  | ![](../Images/10_05_Sidebar18.png) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/10_05_Sidebar18.png) |'
- en: After we rank them by *TD* error, we calculate their priorities as the reciprocal
    of the rank. And again, for calculating priorities, we proceed by scaling the
    priorities with alpha, the same as with the proportional strategy. And then, we
    calculate actual probabilities from these priorities, also, as before, normalizing
    the values so that the sum is one.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们按 *TD* 错误对它们进行排名后，我们计算它们的优先级为排名的倒数。同样，在计算优先级时，我们通过使用 alpha（与比例策略相同）对优先级进行缩放来继续进行。然后，我们像以前一样从这些优先级计算实际概率，也进行归一化，使得总和为
    1。
- en: '| ![](../Images/icons_Boil.png) | Boil It DownRank-based prioritization |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Boil.png) | 简化优先级排序 |'
- en: '|  | While proportional prioritization uses the absolute TD error and a small
    constant for including zero TD error experiences, rank-based prioritization uses
    the reciprocal of the rank of the sample when sorted in descending order by absolute
    TD error.Both prioritization strategies then create probabilities from priorities
    the same way. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | 虽然比例优先级使用绝对 TD 错误和一个小常数来包含零 TD 错误经验，但基于排名的优先级使用按绝对 TD 错误降序排序的样本的排名的倒数。两种优先级策略随后以相同的方式从优先级创建概率。|'
- en: Prioritization bias
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优先级偏差
- en: Using one distribution for estimating another one introduces bias in the estimates.
    Because we’re sampling based on these probabilities, priorities, and *TD* errors,
    we need to account for that.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个分布来估计另一个分布会引入估计偏差。因为我们根据这些概率、优先级和 *TD* 错误进行采样，所以我们需要考虑这一点。
- en: First, let me explain the problem in more depth. The distribution of the updates
    must be from the same distribution as its expectation. When we update the action-value
    function of state *s* and an action *a*, we must be cognizant that we always update
    with targets.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我更深入地解释一下这个问题。更新的分布必须与其期望值来自相同的分布。当我们更新状态 *s* 和动作 *a* 的动作值函数时，我们必须意识到我们总是使用目标值进行更新。
- en: Targets are samples of expectations. That means the reward and state at the
    next step could be stochastic; there could be many possible different rewards
    and states when taking action *a* in a state *s*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是期望值的样本。这意味着下一步的奖励和状态可能是随机的；在状态 *s* 中采取动作 *a* 时，可能会有许多可能的不同的奖励和状态。
- en: If we were to ignore this fact and update a single sample more often than it
    appears in that expectation, we’d create a bias toward this value. This issue
    is particularly impactful at the end of training when our methods are near convergence.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们忽略这个事实，并且比在期望中出现得更频繁地更新单个样本，我们就会对这个值产生偏差。这个问题在训练结束时尤其有影响，那时我们的方法接近收敛。
- en: The way to mitigate this bias is to use a technique called *Weighted importance
    sampling*. It consists of scaling the *TD* errors by weights calculated with the
    probabilities of each sample.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要减轻这种偏差，可以使用一种称为*加权重要性采样*的技术。它包括通过每个样本的概率来计算权重，然后对*TD*误差进行缩放。
- en: What weighted importance sampling does is change the magnitude of the updates
    so that it appears the samples came from a uniform distribution.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 加权重要性采样所做的就是改变更新的幅度，使得样本看起来来自均匀分布。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathWeighted importance-sampling
    weights calculation |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式：加权重要性采样权重计算 |'
- en: '|  | ![](../Images/10_05_Sidebar20.png) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/10_05_Sidebar20.png) |'
- en: To do weighted importance-sampling effectively with a prioritized replay buffer,
    we add a convenient hyperparameter, beta, that allows us to tune the degree of
    the corrections. When beta is zero, there’s no correction; when beta is one, there’s
    a full correction of the bias.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地使用优先回放缓冲区进行加权重要性采样，我们添加了一个方便的超参数，beta，它允许我们调整校正的程度。当beta为零时，没有校正；当beta为一时，进行完全的偏差校正。
- en: Additionally, we want to normalize the weights by their max so that the max
    weight becomes one, and all other weights scale down the *TD* errors. This way,
    we keep *TD* errors from growing too much and keep training stable.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想通过它们的最大值来归一化权重，使得最大权重变为1，其他所有权重都按比例缩小*TD*误差。这样，我们就能防止*TD*误差增长过大，并保持训练的稳定性。
- en: These importance-sampling weights are used in the loss function. Instead of
    using the *TD* errors straight in the gradient updates, in PER, we multiply them
    by the importance-sampling weights and scale all *TD* errors down to compensate
    for the mismatch in the distributions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些重要性采样权重用于损失函数中。在PER中，我们不是直接在梯度更新中使用*TD*误差，而是将它们乘以重要性采样权重，并将所有*TD*误差缩小以补偿分布的不匹配。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathDueling DDQN with PER gradient
    update |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式：对抗性DDQN与PER梯度更新 |'
- en: '|  | ![](../Images/10_05_Sidebar21.png) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/10_05_Sidebar21.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPrioritized replay buffer
    1/2 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python：优先回放缓冲区 1/2 |'
- en: '|  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① The store function of the PrioritizedReplayBuffer class is straightforward.
    The first thing we do is calculate the priority for the sample. Remember, we set
    the priority to the maximum. The following code shows 1 as default; then it’s
    overwritten with the max value.② With the priority and sample (experience) in
    hand, we insert it into the memory.③ We increase the variable that indicates the
    number of experiences in the buffer, but we need to make sure the buffer doesn’t
    increase beyond the max_samples.④ This next variable indicates the index at which
    the next experience will be inserted. This variable loops back around from max_samples
    to 0 and goes back up.⑤ The update function takes an array of experiences ids,
    and new *TD* error values. Then, we insert the absolute *TD* errors into the right
    place.⑥ If we’re doing rank-based sampling, we additionally sort the array. Notice
    that arrays are sub-optimal for implementing a prioritized replay buffer, mainly
    because of this sort that depends on the number of samples. Not good for performance.
    |
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ① PrioritizedReplayBuffer类的存储函数很简单。我们首先做的事情是计算样本的优先级。记住，我们将优先级设置为最大值。下面的代码显示了默认值为1；然后它被覆盖为最大值。②
    拥有优先级和样本（经验）后，我们将它插入到内存中。③ 我们增加表示缓冲区中经验数量变量的值，但我们需要确保缓冲区不会超过max_samples。④ 下一个变量表示下一个经验将被插入的索引。这个变量从max_samples循环回0，然后再次上升。⑤
    更新函数接受一个经验ID数组和新*TD*误差值。然后，我们将绝对*TD*误差插入到正确的位置。⑥ 如果我们进行基于排名的采样，我们还会对数组进行排序。请注意，数组在实现优先回放缓冲区时不是最优的，主要是因为这个依赖于样本数量的排序。这对性能不好。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPrioritized replay buffer
    2/2 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python：优先回放缓冲区 2/2 |'
- en: '|  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Calculate the batch_size, anneal ‘beta,’ and remove zeroed rows from entries.②
    We now calculate priorities. If it’s a rank-based prioritization, it’s one over
    the rank (we sorted these in the update function). Proportional is the absolute
    *TD* error plus a small constant epsilon to avoid zero priorities.③ Now, we go
    from priorities to probabilities. First, we blend with uniform, then probs.④ We
    then calculate the importance-sampling weights using the probabilities.⑤ Normalize
    the weights. The maximum weight will be 1.⑥ We sample indices of the experiences
    in the buffer using the probabilities.⑦ Get the samples out of the buffer.⑧ Finally,
    stack the samples by ids, weights, and experience tuples, and return them. |
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ① 计算batch_size、衰减‘beta’并从条目中删除零行。② 现在，我们计算优先级。如果是基于排名的优先级，则是排名的倒数（我们在更新函数中已对这些进行排序）。比例是绝对TD误差加上一个小的常数epsilon，以避免优先级为零。③
    现在，我们将优先级转换为概率。首先，我们与均匀分布混合，然后是probs。④ 然后，我们使用概率计算重要性采样权重。⑤ 归一化权重。最大权重将是1。⑥ 我们使用概率从缓冲区中采样经验的索引。⑦
    从缓冲区中获取样本。⑧ 最后，通过ids、权重和经验元组堆叠样本，并返回它们。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPrioritized replay buffer
    loss function 1/2 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python优先级重放缓冲区损失函数1/2 |'
- en: '|  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '① As I’ve pointed out on other occasions, this is part of the code. These are
    snippets that I feel are worth showing here.② One thing to notice is that now
    we have ids and weights coming along with the experiences.③ We calculate the target
    values, as before.④ We query the current estimates: nothing new.⑤ We calculate
    the *TD* errors, the same way.⑥ But, now the loss function has *TD* errors downscaled
    by the weights.⑦ We continue the optimization as before.⑧ And we update the priorities
    of the replayed batch using the absolute *TD* errors. |'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ① 正如我在其他场合所指出的，这是代码的一部分。这些是我认为值得在这里展示的代码片段。② 注意现在我们有了与经验一起到来的ids和权重。③ 我们计算目标值，就像之前一样。④
    我们查询当前的估计：没有新内容。⑤ 我们以同样的方式计算TD误差。⑥ 但是，现在损失函数有通过权重缩放的TD误差。⑦ 我们继续之前的优化。⑧ 我们使用绝对TD误差更新重放批次的优先级。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPrioritized replay buffer
    loss function 2/2 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python优先级重放缓冲区损失函数2/2 |'
- en: '|  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① This is the same PER class, but we’re now in the train function.② Inside the
    episode loop③ Inside the time step loop④ Every time step during training time⑤
    Look how we pull the experiences from the buffer.⑥ From the experiences, we pull
    the idxs, weights, and experience tuple. Notice how we load the samples variables
    into the GPU.⑦ Then, we stack the variables again. Note that we did that only
    to load the samples into the GPU and have them ready for training.⑧ Then, we optimize
    the model (this is the function in the previous page).⑨ And, everything proceeds
    as usual. |
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是相同的PER类，但现在我们处于训练函数中。② 在剧集循环内。③ 在时间步循环内。④ 在训练时间中的每个时间步。⑤ 看看我们是怎样从缓冲区中提取经验的。⑥
    从经验中，我们提取idxs、权重和经验元组。注意我们是如何将样本变量加载到GPU中的。⑦ 然后，我们再次堆叠变量。请注意，我们这样做只是为了将样本加载到GPU中，并使其准备好训练。⑧
    然后，我们优化模型（这是上一页中的函数）。⑨ 然后，一切照常进行。|
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsThe dueling DDQN with
    the prioritized replay buffer algorithm |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Details.png) | 这就是细节双DQN与优先级重放缓冲区算法 |'
- en: '|  | One final time, we improve on all previous value-based deep reinforcement
    learning methods. This time, we do so by improving on the replay buffer. As you
    can imagine, most hyperparameters stay the same as the previous methods. Let’s
    go into the details. These are the things that are still the same as before:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 最后一次，我们改进了所有之前的价值型深度强化学习方法。这次，我们通过改进重放缓冲区来实现。正如你可以想象的那样，大多数超参数与之前的方法相同。让我们深入了解。这些是仍然与之前相同的事情：'
- en: Network outputs the action-value function *Q*(*s, a; θ*).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络输出动作值函数 *Q*(*s, a; θ*）。
- en: 'We use a state-in-values-out dueling network architecture (nodes: 4, 512,128,
    1; 2, 2).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用状态-值输出对冲网络架构（节点：4, 512,128, 1; 2, 2）。
- en: Optimize the action-value function to approximate the optimal action-value function
    *q**(*s, a*).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化动作值函数以逼近最优动作值函数 *q**(*s, a*）。
- en: Use off-policy *TD* targets (*r + gamma*max_a’Q*(*s’,a’; θ*)) to evaluate policies.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用离线策略TD目标（*r + gamma*max_a’Q*(*s’，a’; θ*）来评估策略。
- en: Use an adjustable Huber loss with max_gradient_norm variable set to float(‘inf’).
    Therefore, we're using MSE.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可调整的Huber损失，将max_gradient_norm变量设置为float(‘inf’)。因此，我们使用MSE。
- en: Use RMSprop as our optimizer with a learning rate of 0.0007.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用学习率为0.0007的RMSprop作为我们的优化器。
- en: An exponentially decaying epsilon-greedy strategy (from 1.0 to 0.3 in roughly
    20,000 steps) to improve policies.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种指数退化的ε-greedy策略（从1.0到0.3大约需要20,000步）来改进策略。
- en: A greedy action selection strategy for evaluation steps.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于评估步骤的贪婪动作选择策略。
- en: A target network that updates every time step using Polyak averaging with a
    tau (the mix-in factor) of 0.1.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个目标网络，每次时间步更新时使用Polyak平均，τ（混合因子）为0.1。
- en: A replay buffer with 320 samples minimum and a batch of 64.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少包含320个样本的回放缓冲区和64个批次的样本。
- en: 'Things we’ve changed:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的改变：
- en: Use weighted important sampling to adjust the *TD* errors (which changes the
    loss function).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用加权重要性采样来调整TD误差（这会改变损失函数）。
- en: Use a prioritized replay buffer with proportional prioritization, with a max
    number of samples of 10,000, an alpha (degree of prioritization versus uniform—1
    is full priority) value of 0.6, a beta (initial value of beta, which is bias correction—1
    is full correction) value of 0.1 and a beta annealing rate of 0.99992 (fully annealed
    in roughly 30,000 time steps).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有比例优先级的优先回放缓冲区，最大样本数为10,000，优先级（相对于均匀的优先级程度——1是完全优先级）的α值为0.6，β值（β的初始值，用于偏差校正——1是完全校正）为0.1，β退火率为0.99992（在大约30,000个时间步长后完全退火）。
- en: 'PER is the same base algorithm as dueling DDQN, DDQN, and DQN:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: PER与对抗DDQN、DDQN和DQN的基算法相同：
- en: 'Collect experience: (*S*[*t*]*, A*[*t*]*, R*[*t+*][*1*]*, S*[*t+*][*1*]*, D*[*t+1*]),
    and insert into the replay buffer.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集经验：(*S*[*t*]*, A*[*t*]*, R*[*t+*][*1*]*, S*[*t+*][*1*]*, D*[*t+1*])，并将其插入回放缓冲区。
- en: 'Pull a batch out of the buffer and calculate the off-policy *TD* targets: *r
    + gamma*max_a’Q(s’,a’; θ),* using double learning.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从缓冲区中抽取一个批次，并使用双学习计算离策略TD目标：*r + gamma*max_a’Q(s’，a’; θ)。
- en: Fit the action-value function *Q*(*s,a; θ*), using MSE and RMSprop.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MSE和RMSprop拟合动作价值函数*Q*(*s,a; θ*)。
- en: Adjust *TD* errors in the replay buffer.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整回放缓冲区中的TD误差。
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpPER improves data efficiency
    even more |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 累计它UPPER进一步提高数据效率'
- en: '|  | The prioritized replay buffer uses fewer samples than any of the previous
    methods. And as you can see it in the graphs below, it even makes things look
    more stable. Maybe?![](../Images/10_05_Sidebar27.png) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | 优先回放缓冲区使用的样本比之前任何方法都少。正如你在下面的图表中可以看到的，它甚至使事物看起来更加稳定。也许？！[](../Images/10_05_Sidebar27.png)
    |'
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This chapter concludes a survey of value-based DRL methods. In this chapter,
    we explored ways to make value-based methods more data efficient. You learned
    about the dueling architecture, and how it leverages the nuances of value-based
    RL by separating the *Q*(*s, a*) into its two components: the state-value function
    *V*(*s*) and the action-advantage function *A*(*s, a*). This separation allows
    every experience used for updating the network to add information to the estimate
    of the state-value function *V*(*s*), which is common to all actions. By doing
    this, we arrive at the correct estimates more quickly, reducing sample complexity.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了基于价值的DRL方法的调查。在本章中，我们探讨了使基于价值的方法更数据高效的方法。你了解了对抗架构，以及它是如何通过将*Q*(*s, a*)分解为其两个组成部分：状态价值函数*V*(*s*)和动作优势函数*A*(*s,
    a*)来利用基于价值的RL的细微差别。这种分离使得每个用于更新网络的体验都能为状态价值函数*V*(*s*)的估计添加信息，这对于所有动作都是共同的。通过这样做，我们能够更快地得到正确的估计，从而降低样本复杂度。
- en: You also looked into the prioritization of experiences. You learned that *TD*
    errors are a good criterion for creating priorities and that from priorities,
    you can calculate probabilities. You learned that we must compensate for changing
    the distribution of the expectation we’re estimating. Thus, we use importance
    sampling, which is a technique for correcting the bias.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你还研究了经验的优先级。你了解到TD误差是创建优先级的好标准，并且你可以从优先级中计算出概率。你了解到我们必须补偿我们估计的期望分布的变化。因此，我们使用重要性采样，这是一种纠正偏差的技术。
- en: In the past three chapters, we dove headfirst into the field of value-based
    DRL. We started with a simple approach, NFQ. Then, we made this technique more
    stable with the improvements presented in DQN and DDQN. Then, we made it more
    sample-efficient with dueling DDQN and PER. Overall, we have a pretty robust algorithm.
    But, as with everything, value-based methods also have cons. First, they’re sensitive
    to hyperparameters. This is well known, but you should try it for yourself; change
    any hyperparameter. You can find more values that don’t work than values that
    do. Second, value-based methods assume they interact with a Markovian environment,
    that the states contain all information required by the agent. This assumption
    dissipates as we move away from bootstrapping and value-based methods in general.
    Last, the combination of bootstrapping, off-policy learning, and function approximators
    are known conjointly as “the deadly triad.” While the deadly triad is known to
    produce divergence, researchers still don’t know exactly how to prevent it.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的三章中，我们深入研究了基于价值的DRL领域。我们从简单的方法NFQ开始。然后，我们通过DQN和DDQN的改进使这项技术更加稳定。然后，我们通过对抗DDQN和PER使其更高效地采样。总的来说，我们有一个相当稳健的算法。但是，就像所有事情一样，基于价值的方法也有缺点。首先，它们对超参数敏感。这是众所周知的，但你应该亲自尝试；改变任何超参数。你可以找到更多不工作的值，而不是工作的值。其次，基于价值的方法假设它们与马尔可夫环境交互，即状态包含代理所需的所有信息。当我们远离自举和基于价值的方法时，这种假设就会消失。最后，自举、离线学习和函数逼近器的组合共同被称为“致命的三位一体”。虽然已知致命的三位一体会产生发散，但研究人员仍然不知道如何确切地防止它。
- en: By no means am I saying that value-based methods are inferior to the methods
    we survey in future chapters. Those methods have issues of their own, too. The
    fundamental takeaway is to know that value-based deep reinforcement learning methods
    are well known to diverge, and that’s their weakness. How to fix it is still a
    research question, but sound practical advice is to use target networks, replay
    buffers, double learning, sufficiently small learning rates (but not too small),
    and maybe a little bit of patience. I’m sorry about that; I don’t make the rules.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对不是在说基于价值的算法比我们在未来章节中探讨的方法差。那些方法也有它们自己的问题。基本的启示是，基于价值的深度强化学习方法众所周知会发散，这是它们的弱点。如何修复它仍然是一个研究问题，但合理的实用建议是使用目标网络、重放缓冲区、双重学习、足够小的学习率（但不是太小），也许还需要一点耐心。对此我感到抱歉；我并不是制定规则的人。
- en: By now, you
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你
- en: Can solve reinforcement learning problems with continuous state spaces
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够解决具有连续状态空间的强化学习问题
- en: Know how to stabilize value-based DRL agents
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何稳定基于价值的DRL代理
- en: Know how to make value-based DRL agents more sample efficient
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何使基于价值的DRL代理更高效地采样
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ![推文图标](../Images/icons_Tweet.png) | 在自己的工作上努力并分享你的发现'
- en: '|  | Here are several ideas on how to take what you’ve learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you''ll take advantage of it.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些关于如何将你所学的内容提升到下一个层次的想法。如果你愿意，与世界分享你的结果，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能利用它。'
- en: '**#gdrl_ch10_tf01:** The replay buffers used in this and the previous chapter
    are sufficient for the cart-pole environment and other low-dimensional environments.
    However, you likely noticed that the prioritized buffer becomes the bottleneck
    for any more complex environment. Try rewriting all replay buffer code by yourself
    to speed it up. Do not look up other’s code on this yet; try making the replay
    buffers faster. In the prioritized buffer, you can see that the bottleneck is
    the sorting of the samples. Find ways to make this portion faster, too.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch10_tf01:** 本章和上一章中使用的重放缓冲区对于小车-杆环境和其他低维环境来说是足够的。然而，你可能已经注意到，优先级缓冲区成为任何更复杂环境的瓶颈。尝试自己重写所有重放缓冲区代码以加快速度。现在不要查找他人的代码；尝试使重放缓冲区更快。在优先级缓冲区中，你可以看到瓶颈是样本的排序。找到使这部分更快的方法。'
- en: '**#gdrl_ch10_tf02:** When trying to solve high-dimensional environments, such
    as Atari games, the replay buffer code in this and the previous chapter becomes
    prohibitively slow, and totally unpractical. Now what? Research how others solve
    this problem, which is a blocking issue for prioritized buffers. Share your findings
    and implement the data structures on your own. Understand them well, and create
    a blog post explaining the benefits of using them, in detail.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch10_tf02:** 当尝试解决高维环境，例如Atari游戏时，本章和上一章中的重放缓冲区代码变得极其缓慢，完全不实用。现在怎么办？研究其他人如何解决这个问题，这是优先级缓冲区的一个阻碍性问题。分享你的发现，并自己实现这些数据结构。深入了解它们，并创建一篇博客文章，详细解释使用它们的优点。'
- en: '**#gdrl_ch10_tf03:** In the last two chapters, you’ve learned about methods
    that can solve problems with high-dimensional and continuous state spaces, but
    how about action spaces? It seems lame that these algorithms can only select one
    action at a time, and these actions have discrete values. But wait, can DQN-like
    methods only solve problems with discrete action spaces of size one? Investigate
    and tell us!'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch10_tf03:** 在上一两章中，你已经学习了可以解决具有高维和连续状态空间问题的方法，但动作空间呢？这些算法一次只能选择一个动作，而且这些动作具有离散值，这似乎很无趣。但是等等，像DQN这样的方法只能解决大小为1的离散动作空间的问题吗？调查并告诉我们！'
- en: '**#gdrl_ch10_tf04:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch10_tf04:** 在每一章中，我都在使用最后一个标签作为通用的标签。请随意使用这个标签来讨论任何与本章相关的工作。没有比你自己创造的任务更令人兴奋的作业了。确保分享你打算调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from the list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '用你的发现写一条推文，@我 @mimoralea（我会转发），并使用列表中的特定标签，以帮助感兴趣的人找到你的结果。没有对错之分；你分享你的发现，并检查他人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源列表。查看它吧：<link>
    #gdrl_ch01_tf01”我会确保转发并帮助他人找到你的工作。|'
