- en: 1 Introduction to human-in-the-loop machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 闭环机器学习简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Annotating unlabeled data to create training, validation, and evaluation data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将未标记数据标注以创建训练、验证和评估数据
- en: Sampling the most important unlabeled data items (active learning)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽取最重要的未标记数据项（主动学习）
- en: Incorporating human–computer interaction principles into annotation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将人机交互原则融入标注
- en: Implementing transfer learning to take advantage of information in existing
    models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施迁移学习以利用现有模型中的信息
- en: Unlike robots in the movies, most of today’s artificial intelligence (AI) cannot
    learn by itself; instead, it relies on intensive human feedback. Probably 90%
    of machine learning applications today are powered by supervised machine learning.
    This figure covers a wide range of use cases. An autonomous vehicle can drive
    you safely down the street because humans have spent thousands of hours telling
    it when its sensors are seeing a pedestrian, moving vehicle, lane marking, or
    other relevant object. Your in-home device knows what to do when you say “Turn
    up the volume” because humans have spent thousands of hours telling it how to
    interpret different commands. And your machine translation service can translate
    between languages because it has been trained on thousands (or maybe millions)
    of human-translated texts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 与电影中的机器人不同，今天的大多数人工智能（AI）无法自主学习；相反，它依赖于密集的人类反馈。今天大约90%的机器学习应用都是由监督式机器学习驱动的。这个数字涵盖了广泛的使用案例。自动驾驶汽车可以安全地将你送下街道，因为人类已经花费数千小时告诉它当传感器看到行人、移动车辆、车道标记或其他相关物体时的情况。当你说出“调高音量”时，你的家庭设备知道该怎么做，因为人类已经花费数千小时告诉它如何解释不同的命令。而且，你的机器翻译服务可以翻译不同语言，因为它在数千（或可能数百万）个由人类翻译的文本上进行了训练。
- en: 'Compared with the past, our intelligent devices are learning less from programmers
    who are hardcoding rules and more from examples and feedback given by humans who
    do not need to code. These human-encoded examples—the training data—are used to
    train machine learning models and make them more accurate for their given tasks.
    But programmers still need to create the software that allows the feedback from
    nontechnical humans, which raises one of the most important questions in technology
    today: *What are the right ways for humans and machine learning algorithms to
    interact to solve problems*. After reading this book, you will be able to answer
    this question for many uses that you might face in machine learning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与过去相比，我们的智能设备从程序员那里学习到的规则越来越少，更多地是从不需要编码的人类提供的示例和反馈中学习。这些由人类编码的示例——训练数据——被用来训练机器学习模型，并使它们在特定任务上更加准确。但程序员仍然需要创建允许非技术性人类提供反馈的软件，这提出了当今技术领域中的一个最重要问题：*人类和机器学习算法如何正确互动以解决问题*。阅读这本书后，你将能够为你在机器学习中可能遇到的大多数用途回答这个问题。
- en: Annotation and active learning are the cornerstones of human-in-the-loop machine
    learning. They specify how you elicit training data from people and determine
    the right data to put in front of people when you don’t have the budget or time
    for human feedback on all your data. Transfer learning allows us to avoid a cold
    start, adapting existing machine learning models to our new task rather than starting
    at square one. We will introduce each of these concepts in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 标注和主动学习是闭环机器学习的基础。它们规定了如何从人们那里获取训练数据，以及在你没有预算或时间对所有数据进行人类反馈时，如何确定正确的数据展示给人们。迁移学习使我们能够避免从头开始，而是将现有的机器学习模型适应到我们的新任务中。我们将在本章中介绍这些概念。
- en: 1.1 The basic principles of human-in-the-loop machine learning
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 闭环机器学习的基本原则
- en: '*Human-in-the-loop* *machine learning* is a set of strategies for combining
    human and machine intelligence in applications that use AI. The goal typically
    is to do one or more of the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*闭环* *机器学习* 是一套策略，用于在应用人工智能的应用中结合人类和机器智能。通常的目标是完成以下一项或多项：'
- en: Increase the accuracy of a machine learning model.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高机器学习模型的准确性。
- en: Reach the target accuracy for a machine learning model faster.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快地达到机器学习模型的准确度目标。
- en: Combine human and machine intelligence to maximize accuracy.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合人类和机器智能以最大化准确性。
- en: Assist human tasks with machine learning to increase efficiency.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习辅助人类任务以提高效率。
- en: This book covers the most common active learning and annotation strategies and
    how to design the best interface for your data, task, and annotation workforce.
    The book gradually builds from simpler to more complicated examples and is written
    to be read in sequence. You are unlikely to apply all these techniques at the
    same time, however, so the book is also designed to be a reference for each specific
    technique.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书涵盖了最常见的主动学习和标注策略，以及如何为您的数据、任务和标注劳动力设计最佳界面。本书从简单的例子逐渐过渡到更复杂的例子，并编写成顺序阅读。然而，您不太可能同时应用所有这些技术，因此，本书也被设计成每个特定技术的参考。
- en: 'Figure 1.1 shows the human-in-the-loop machine learning process for adding
    labels to data. This process could be any labeling process: adding the topic to
    news stories, classifying sports photos according to the sport being played, identifying
    the sentiment of a social media comment, rating a video on how explicit the content
    is, and so on. In all cases, you could use machine learning to automate some of
    the process of labeling or to speed up the human process. In all cases, using
    best practices means implementing the cycle shown in figure 1.1: sampling the
    right data to label, using that data to train a model, and using that model to
    sample more data to annotate.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1展示了添加标签到数据的人机交互机器学习过程。这个过程可以是任何标注过程：将主题添加到新闻故事中，根据正在进行的运动对体育照片进行分类，识别社交媒体评论的情感，对视频内容是否露骨进行评分，等等。在所有情况下，您都可以使用机器学习来自动化标注过程的一部分或加快人工过程。在所有情况下，使用最佳实践意味着实施图1.1中所示的循环：采样正确的数据进行标注，使用这些数据来训练模型，并使用该模型来采样更多数据进行标注。
- en: '![](../Images/CH01_F01_Munro.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F01_Munro.png)'
- en: Figure 1.1 A mental model of the human-in-the-loop process for predicting labels
    on data
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 人机交互过程中预测数据标签的心理模型
- en: In some cases, you may want only some of the techniques. If you have a system
    that backs off to a human when the machine learning model is uncertain, for example,
    you would look at the relevant chapters and sections on uncertainty sampling,
    annotation quality, and interface design. Those topics still represent the majority
    of this book even if you aren’t completing the “loop.”
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能只需要一些技术。例如，如果您有一个在机器学习模型不确定时退回到人类的系统，您就会查看关于不确定性采样、标注质量和界面设计的相关章节和部分。即使您没有完成“循环”，这些主题仍然代表了本书的大部分内容。
- en: This book assumes that you have some familiarity with machine learning. Some
    concepts are especially important for human-in-the-loop systems, including deep
    understanding of softmax and its limitations. You also need to know how to calculate
    accuracy with metrics that take model confidence into consideration, calculate
    chance-adjusted accuracy, and measure the performance of machine learning from
    a human perspective. (The appendix contains a summary of this knowledge.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设您对机器学习有一定的了解。一些概念对于人机交互系统尤为重要，包括对softmax及其局限性的深入理解。您还需要知道如何使用考虑模型置信度的指标来计算准确率，计算调整后的准确率，以及从人类视角衡量机器学习的性能。（附录中包含这些知识的总结。）
- en: 1.2 Introducing annotation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 介绍标注
- en: '*Annotation* is the process of labeling raw data so that it becomes training
    data for machine learning. Most data scientists will tell you that they spend
    much more time curating and annotating datasets than they spend building the machine
    learning models. Quality control for human annotation relies on more complicated
    statistics than most machine learning models do, so it is important to take the
    necessary time to learn how to create quality training data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*标注*是将原始数据进行标记的过程，使其成为机器学习的训练数据。大多数数据科学家都会告诉你，他们花费在数据集整理和标注上的时间比构建机器学习模型的时间要多得多。人类标注的质量控制依赖于比大多数机器学习模型更复杂的统计方法，因此，花时间学习如何创建高质量的训练数据是非常重要的。'
- en: 1.2.1 Simple and more complicated annotation strategies
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 简单和更复杂的标注策略
- en: An annotation process can be simple. If you want to label social media posts
    about a product as positive, negative, or neutral to analyze broad trends in sentiment
    about that product, for example, you could build and deploy an HTML form in a
    few hours. A simple HTML form could allow someone to rate each social media post
    according to the sentiment option, and each rating would become the label on the
    social media post for your training data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 标注过程可以很简单。例如，如果您想对关于产品的社交媒体帖子进行标注，以分析该产品情绪趋势的广泛趋势，您可以在几小时内构建并部署一个HTML表单。一个简单的HTML表单可以允许某人根据情绪选项对每条社交媒体帖子进行评分，每个评分将成为您训练数据中社交媒体帖子的标签。
- en: An annotation process can also be complicated. If you want to label every object
    in a video with a bounding box, for example, a simple HTML form is not enough;
    you need a graphical interface that allows annotators to draw those boxes, and
    a good user experience might take months of engineering hours to build.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 标注过程也可以很复杂。例如，如果您想对视频中的每个对象进行标注并绘制边界框，简单的HTML表单就不够了；您需要一个允许标注者绘制这些框的图形界面，而良好的用户体验可能需要数月的工程时间来构建。
- en: 1.2.2 Plugging the gap in data science knowledge
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 填补数据科学知识空白
- en: Your machine learning algorithm strategy and your data annotation strategy can
    be optimized at the same time. The two strategies are closely intertwined, and
    you often get better accuracy from your models faster if you have a combined approach.
    Algorithms and annotation are equally important components of good machine learning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您的机器学习算法策略和数据标注策略可以同时优化。这两种策略紧密相连，如果您采用综合方法，您将更快地从模型中获得更高的准确度。算法和标注是优秀机器学习同等重要的组成部分。
- en: All computer science departments offer machine learning courses, but few offer
    courses on creating training data. At most, you might find one or two lectures
    about creating training data among hundreds of machine learning lectures across
    half a dozen courses. This situation is changing, but slowly. For historical reasons,
    academic machine learning researchers have tended to keep the datasets constant
    and evaluated their research only in terms of different algorithms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所有计算机科学系都提供机器学习课程，但很少有课程教授如何创建训练数据。在最多的情况下，您可能只能在六门课程中的数百个机器学习讲座中找到一两个关于创建训练数据的讲座。这种情况正在改变，但进展缓慢。由于历史原因，学术机器学习研究人员倾向于保持数据集不变，并仅从不同算法的角度评估他们的研究。
- en: By contrast with academic machine learning, it is more common in industry to
    improve model performance by annotating more training data. Especially when the
    nature of the data is changing over time (which is also common), using a handful
    of new annotations can be far more effective than trying to adapt an existing
    model to a new domain of data. But far more academic papers focus on how to adapt
    algorithms to new domains *without* new training data than on how to annotate
    the right new training data efficiently.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与学术机器学习相比，在工业界，通过标注更多训练数据来提高模型性能更为常见。尤其是在数据性质随时间变化（这也是很常见的情况）时，使用少量新的标注可能比尝试将现有模型适应新的数据领域更有效。但相比之下，更多的学术论文关注的是如何在没有新训练数据的情况下将算法适应新领域，而不是如何高效地标注正确的新训练数据。
- en: Because of this imbalance in academia, I’ve often seen people in industry make
    the same mistake. They hire a dozen smart PhDs who know how to build state-of-the-art
    algorithms but don’t have experience creating training data or thinking about
    the right interfaces for annotation. I saw exactly this situation recently at
    one of the world’s largest auto manufacturers. The company had hired a large number
    of recent machine learning graduates, but it couldn’t operationalize its autonomous
    vehicle technology because the new employees couldn’t scale their data annotation
    strategy. The company ended up letting that entire team go. During the aftermath,
    I advised the company how to rebuild its strategy by using algorithms and annotation
    as equally-important, intertwined components of good machine learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于学术界存在这种不平衡，我经常看到工业界的人犯同样的错误。他们雇佣了一打了解如何构建最先进算法的聪明博士，但他们没有创建训练数据或考虑标注正确接口的经验。我最近在世界最大的汽车制造商之一看到了这种情况。公司雇佣了大量最近毕业的机器学习毕业生，但由于新员工无法扩展他们的数据标注策略，公司无法实施其自动驾驶汽车技术。公司最终解雇了整个团队。在事件发生后，我建议公司如何通过将算法和标注作为优秀机器学习同等重要、相互交织的组成部分来重建其策略。
- en: '1.2.3 Quality human annotation: Why is it hard?'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 质量人工标注：为什么很难？
- en: To those who study it, annotation is a science that’s tied closely to machine
    learning. The most obvious example is that the humans who provide the labels can
    make errors, and overcoming these errors requires surprisingly sophisticated statistics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些研究它的人来说，标注是一门与机器学习紧密相关的科学。最明显的例子是，提供标签的人类可能会犯错误，克服这些错误需要惊人的复杂统计。
- en: Human errors in training data can be more or less important, depending on the
    use case. If a machine learning model is being used only to identify broad trends
    in consumer sentiment, it probably won’t matter whether errors propagate from
    1% bad training data. But if an algorithm that powers an autonomous vehicle doesn’t
    see 1% of pedestrians due to errors propagated from bad training data, the result
    will be disastrous. Some algorithms can handle a little noise in the training
    data, and random noise even helps some algorithms become more accurate by avoiding
    overfitting. But human errors tend not to be random noise; therefore, they tend
    to introduce irrecoverable bias into training data. No algorithm can survive truly
    bad training data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中的人类错误可能重要或不重要，这取决于用例。如果一个机器学习模型仅用于识别消费者情绪的广泛趋势，那么从1%的糟糕训练数据中传播的错误可能无关紧要。但如果一个驱动自动驾驶车辆的算法由于从糟糕的训练数据中传播的错误而没有看到1%的行人，结果将是灾难性的。一些算法可以处理训练数据中的一些噪声，随机噪声甚至有助于某些算法通过避免过拟合而变得更加准确。但人类错误往往不是随机噪声；因此，它们往往会向训练数据中引入不可恢复的偏差。没有任何算法能够从真正糟糕的训练数据中幸存。
- en: For simple tasks, such as binary labels on objective tasks, the statistics are
    fairly straightforward for deciding which label is correct when different annotators
    disagree. But for subjective tasks, or even objective tasks with continuous data,
    no simple heuristics exist for deciding the correct label. Think about the critical
    task of creating training data by putting a bounding box around every pedestrian
    recognized by a self-driving car. What if two annotators have slightly different
    boxes? Which box is the correct one? The answer is not necessarily either box
    or the average of the two boxes. In fact, the best way to aggregate the two boxes
    is to use machine learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的任务，例如在客观任务上的二进制标签，当不同的标注者意见不一致时，统计数据对于决定哪个标签是正确的相当直接。但对于主观任务，或者甚至具有连续数据的客观任务，不存在简单的启发式方法来决定正确的标签。想想通过在自动驾驶汽车识别到的每个行人周围放置边界框来创建训练数据的这项关键任务。如果两个标注者的边界框略有不同怎么办？哪个框是正确的？答案并不一定是任何一个框或者是两个框的平均值。实际上，聚合这两个框的最佳方式是使用机器学习。
- en: One of the best ways to ensure quality annotations is to ensure you have the
    right people making those annotations. Chapter 7 of this book is devoted to finding,
    teaching, and managing the best annotators. For an example of the importance of
    the right workforce combined with the right technology, see the following sidebar.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 确保质量标注的最佳方法之一是确保有合适的人来做这些标注。本书的第7章致力于寻找、培训和管理工作最佳的标注者。关于合适的劳动力与合适技术的结合的重要性，请参阅以下侧边栏。
- en: Human insights and scalable machine learning equal production AI
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人类洞察力和可扩展的机器学习等于生产型人工智能
- en: '*Expert anecdote by Radha Ramaswami Basu*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*由Radha Ramaswami Basu提供的专家轶事*'
- en: 'The outcome of AI is heavily dependent on the quality of the training data
    that goes into it. A small UI improvement like a magic wand to select regions
    in an image can realize large efficiencies when applied across millions of data
    points in conjunction with well-defined processes for quality control. An advanced
    workforce is the key factor: training and specialization increase quality, and
    insights from an expert workforce can inform model design in conjunction with
    domain experts. The best models are created by a constructive, ongoing partnership
    between machine and human intelligence.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的结果高度依赖于输入其内的训练数据的质量。一个小小的用户界面改进，比如一个魔法棒来选择图像中的区域，当应用于数百万数据点并配合良好的质量控制流程时，可以实现巨大的效率提升。一支先进的工作队伍是关键因素：培训和专业化提高质量，专家工作队伍的见解可以与领域专家一起指导模型设计。最佳模型是由机器和人类智能之间建设性、持续的合作创造的。
- en: We recently took on a project that required pixel-level annotation of the various
    anatomic structures within a robotic coronary artery bypass graft (CABG) video.
    Our annotation teams are not experts in anatomy or physiology, so we implemented
    teaching sessions in clinical knowledge to augment the existing core skills in
    3D spatial reasoning and precision annotation, led by a solutions architect who
    is a trained surgeon. The outcome for our customer was successful training and
    evaluation data. The outcome for us was to see people from under-resourced backgrounds
    in animated discussion about some of the most advanced uses of AI as they quickly
    became experts in one of the most important steps in medical image analysis.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近承接了一个项目，需要对机器人冠状动脉旁路移植术（CABG）视频中各种解剖结构的像素级标注。我们的标注团队并非解剖学或生理学专家，因此我们实施了临床知识教学课程，由一位受过培训的外科医生领导的解决方案架构师来增强现有的3D空间推理和精确标注的核心技能。对我们客户的结果是成功的培训和评估数据。对我们来说，结果是看到来自资源不足背景的人们在动画讨论中迅速成为医学图像分析最重要步骤之一的专家。
- en: '*Radha Basu is founder and CEO of iMerit. iMerit uses technology and an AI
    workforce consisting of 50% women and youth from underserved communities to create
    advanced technology workers for global clients. Radha previously worked at HP,
    took Supportsoft public as CEO, and founded the Frugal Innovation Lab at Santa
    Clara University*.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*拉达·巴苏是iMerit的创始人兼首席执行官。iMerit利用技术和由50%来自弱势社区的女性和青年组成的AI劳动力，为全球客户提供高级技术工作者。拉达之前在惠普工作，作为首席执行官将Supportsoft上市，并在圣克拉拉大学创立了节俭创新实验室*。'
- en: '1.3 Introducing active learning: Improving the speed and reducing the cost
    of training data'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 介绍主动学习：提高训练数据的速度和降低成本
- en: Supervised learning models almost always get more accurate with more labeled
    data. *Active learning* is the process of deciding which data to sample for human
    annotation. No one algorithm, architecture, or set of parameters makes one machine
    learning model more accurate in all cases, and no one strategy for active learning
    is optimal across all use cases and datasets. You should try certain approaches
    first, however, because they are more likely to be successful for your data and
    task.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习模型几乎总是随着标注数据的增加而变得更加准确。*主动学习*是决定哪些数据用于人工标注的过程。没有一种算法、架构或参数集能在所有情况下使机器学习模型更准确，也没有一种主动学习的策略在所有用例和数据集中都是最优的。然而，你应该首先尝试某些方法，因为它们更有可能适用于你的数据和任务。
- en: Most research papers on active learning focus on the number of training items,
    but speed can be an even more important factor in many cases. In disaster response,
    for example, I have often deployed machine learning models to filter and extract
    information from emerging disasters. Any delay in disaster response is potentially
    critical, so getting a usable model out quickly is more important than the number
    of labels that need to go into that model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于主动学习的研究论文都关注训练项的数量，但在许多情况下，速度可能是一个更重要的因素。例如，在灾害响应中，我经常部署机器学习模型从新兴灾害中过滤和提取信息。任何灾害响应的延误都可能至关重要，因此快速获得可用的模型比需要输入模型中的标签数量更重要。
- en: '1.3.1 Three broad active learning sampling strategies: Uncertainty, diversity,
    and random'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 三种广泛的主动学习抽样策略：不确定性、多样性和随机
- en: 'Many active learning strategies exist, but three basic approaches work well
    in most contexts: uncertainty, diversity, and random sampling. A combination of
    the three should almost always be the starting point.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多主动学习策略，但在大多数情况下，三种基本方法都适用：不确定性、多样性和随机抽样。这三种方法的组合几乎总是应该从起点开始。
- en: Random sampling sounds the simplest but can be the trickiest. What is random
    if your data is prefiltered, when your data is changing over time, or if you know
    for some other reason that a random sample will not be representative of the problem
    you are addressing? These questions are addressed in more detail in the following
    sections. Regardless of the strategy, you should always annotate some amount of
    random data to gauge the accuracy of your model and compare your active learning
    strategies with a baseline of randomly selected items.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随机抽样听起来最简单，但可能最难。如果你的数据预先过滤，或者数据随时间变化，或者你知道由于其他原因，随机样本不能代表你正在解决的问题，那么什么是随机呢？这些问题将在以下章节中更详细地讨论。无论策略如何，你都应该始终标注一定量的随机数据，以评估你模型的准确性，并将你的主动学习策略与随机选择的项目基线进行比较。
- en: Uncertainty and diversity sampling go by various names in the literature. They
    are often referred to as *exploitation* and *exploration*, which are clever names
    that alliterate and rhyme, but are not otherwise very transparent.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，不确定性和多样性采样有不同的名称。它们通常被称为 *利用* 和 *探索*，这两个名字听起来巧妙且押韵，但并不十分透明。
- en: '*Uncertainty sampling* is the set of strategies for identifying unlabeled items
    that are near a decision boundary in your current machine learning model. If you
    have a binary classification task, these items will have close to a 50% probability
    of belonging to either label; therefore, the model is called uncertain or confused.
    These items are most likely to be wrongly classified, so they are the most likely
    to result in a label that differs from the predicted label, moving the decision
    boundary after they have been added to the training data and the model has been
    retrained.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*不确定性采样* 是一组用于识别当前机器学习模型中接近决策边界的未标记项目的策略。如果你有一个二元分类任务，这些项目将有接近50%的概率属于任一标签；因此，模型被称为不确定或困惑。这些项目最有可能被错误分类，因此它们最有可能导致与预测标签不同的标签，在它们被添加到训练数据中并重新训练模型后，移动决策边界。'
- en: '*Diversity sampling* is the set of strategies for identifying unlabeled items
    that are underrepresented or unknown to the machine learning model in its current
    state. The items may have features that are rare in the training data, or they
    might represent real-world demographics that are currently under-represented in
    the model. In either case, the result can be poor or uneven performance when the
    model is applied, especially when the data is changing over time. The goal of
    diversity sampling is to target new, unusual, or underrepresented items for annotation
    to give the machine learning algorithm a more complete picture of the problem
    space.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*多样性采样* 是一组用于识别当前机器学习模型中未充分代表或未知的项目策略。这些项目可能具有在训练数据中罕见的特征，或者它们可能代表当前在模型中未充分代表的真实世界人口统计特征。在任何情况下，当模型应用时，结果都可能表现不佳或参差不齐，尤其是在数据随时间变化的情况下。多样性采样的目标是针对新的、不寻常的或未充分代表的项目进行标注，以便为机器学习算法提供一个更完整的对问题空间的了解。'
- en: Although the term *uncertainty sampling* is widely used, *diversity sampling*
    goes by different names in different fields, such as representative sampling,
    stratified sampling, outlier detection, and anomaly detection. For some use cases,
    such as identifying new phenomena in astronomical databases or detecting strange
    network activity for security, the goal of the task is to identify the outlier
    or anomaly, but we can adapt them here as a sampling strategy for active learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然术语 *不确定性采样* 被广泛使用，但 *多样性采样* 在不同领域有不同的名称，例如代表性采样、分层采样、异常检测和异常检测。对于某些用例，例如在天文数据库中识别新现象或检测异常网络活动以进行安全监控，任务的目标是识别异常或异常值，但我们可以将它们适应为主动学习的采样策略。
- en: Uncertainty sampling and diversity sampling have shortcomings in isolation (figure
    1.2). Uncertainty sampling might focus on one part of the decision boundary, for
    example, and diversity sampling might focus on outliers that are a long distance
    from the boundary. So the strategies are often used together to find a selection
    of unlabeled items that will maximize both uncertainty and diversity.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性采样和多样性采样在单独使用时存在缺点（图1.2）。不确定性采样可能专注于决策边界的一部分，例如，而多样性采样可能专注于远离边界的异常值。因此，这些策略通常一起使用，以找到最大化不确定性和多样性的未标记项目选择。
- en: '![](../Images/CH01_F02_Munro.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F02_Munro.png)'
- en: 'Figure 1.2 Pros and cons of different active learning strategies. Top left:
    The decision boundary from a machine learning algorithm between items, with some
    items labeled A and some labeled B. Top right: One possible result from uncertainty
    sampling. This active learning strategy is effective for selecting unlabeled items
    near the decision boundary. These items are the most likely to be wrongly predicted,
    and therefore, the most likely to get a label that moves the decision boundary.
    If all the uncertainty is in one part of the problem space, however, giving these
    items labels will not have a broad effect on the model. Bottom left: One possible
    result of diversity sampling. This active learning strategy is effective for selecting
    unlabeled items in different parts of the problem space. If the diversity is away
    from the decision boundary, however, these items are unlikely to be wrongly predicted,
    so they will not have a large effect on the model when a human gives them a label
    that is the same as the model predicted. Bottom right: One possible result from
    combining uncertainty sampling and diversity sampling. When the strategies are
    combined, items are selected that are near diverse sections of the decision boundary.
    Therefore, we are optimizing the chance of finding items that are likely to result
    in a changed decision boundary.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 不同主动学习策略的优缺点。左上角：机器学习算法在项目之间的决策边界，其中一些项目被标记为A，一些被标记为B。右上角：不确定性采样的一个可能结果。这种主动学习策略对于选择决策边界附近的无标签项目是有效的。这些项目最有可能被错误预测，因此，最有可能获得一个能够移动决策边界的标签。然而，如果所有的不确定性都集中在问题空间的一个部分，那么对这些项目进行标记将不会对模型产生广泛的影响。左下角：多样性采样的一个可能结果。这种主动学习策略对于选择问题空间不同部分的无标签项目是有效的。然而，如果多样性远离决策边界，那么这些项目不太可能被错误预测，因此，当人类给他们分配与模型预测相同的标签时，它们对模型的影响不会很大。右下角：结合不确定性采样和多样性采样的一个可能结果。当策略结合时，会选择靠近决策边界不同部分的项目。因此，我们正在优化找到可能导致决策边界变化的项目的机会。
- en: It is important to note that the active learning process is iterative. In each
    iteration of active learning, a selection of items is identified and receives
    a new human-generated label. Then the model is retrained with the new items, and
    the process is repeated. Figure 1.3 shows two iterations for selecting and annotating
    new items, resulting in a changing boundary.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，主动学习过程是迭代的。在主动学习的每一次迭代中，都会识别出一组项目并为其分配一个新的由人类生成的新标签。然后，使用新项目重新训练模型，并重复此过程。图1.3展示了选择和标注新项目的两个迭代过程，导致边界发生变化。
- en: '![](../Images/CH01_F03_Munro.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH01_F03_Munro.png)'
- en: 'Figure 1.3 The iterative active learning process. *Top left to bottom right*:
    Two iterations of active learning. In each iteration, items are selected along
    a diverse selection of the boundary, which in turn causes the boundary to move
    after retraining, resulting in a more accurate machine learning model. Ideally,
    we requested human labels for the minimum number of items as part of our active
    learning strategy. This request speeds the time to get an accurate model and reduces
    the overall cost of human annotation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 迭代主动学习过程。*从左上角到底右角*：两次主动学习的迭代。在每次迭代中，都会选择沿着边界进行多样化的选择，这反过来又会在重新训练后导致边界移动，从而得到一个更准确的机器学习模型。理想情况下，我们请求人类为我们的主动学习策略中所需的最少项目数量提供标签。这个请求加快了获得准确模型的时间，并减少了整体的人工标注成本。
- en: Iteration cycles can be a form of diversity sampling in themselves. Imagine
    that you used only uncertainty sampling, and sampled from only one part of the
    problem space in an iteration. You might solve all uncertainty in that part of
    the problem space; therefore, the next iteration would concentrate somewhere else.
    With enough iterations, you might not need diversity sampling at all. Each iteration
    from uncertainty sampling would focus on a different part of the problem space,
    and together, the iterations are enough to get a diverse sample of items for training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代周期本身可以是一种多样性采样。想象一下，你只使用了不确定性采样，并在一个迭代中只从问题空间的一个部分进行采样。你可能会解决该部分问题空间中的所有不确定性；因此，下一个迭代将集中在其他地方。经过足够的迭代，你可能根本不需要多样性采样。不确定性采样的每次迭代都会关注问题空间的不同部分，并且迭代在一起，足以获得用于训练的项目多样样本。
- en: 'Implemented properly, active learning has this self-correcting function: each
    iteration finds new aspects of the data that are best for human annotation. If
    some part of your data space is inherently ambiguous, however, each iteration
    could keep bringing you back to the same part of the problem space with those
    ambiguous items. So it is generally wise to consider both uncertainty and diversity
    sampling strategies to ensure that you are not focusing all your labeling efforts
    on a part of the problem space that your model might not be able to solve.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正确实施主动学习具有这种自我纠正功能：每次迭代都会找到最适合人工标注的数据的新方面。然而，如果你的数据空间中某些部分本质上是模糊的，每次迭代可能会不断地把你带回到问题空间中具有那些模糊项目的同一部分。因此，通常明智的做法是考虑不确定性和多样性采样策略，以确保你不会将所有的标注努力都集中在模型可能无法解决的问题空间的一部分。
- en: 'Figures 1.2 and 1.3 give you good intuition about the process for active learning.
    As anyone who has worked with high-dimensional or sequence data knows, it is not
    always straightforward to identify distance from a boundary or diversity. At least,
    the process is more complicated than the simple Euclidean distance in figures
    1.2 and 1.3\. But the same idea still applies: we are trying to reach an accurate
    model as quickly as possible with as few human labels as possible.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2和图1.3为你提供了关于主动学习过程的良好直观理解。正如任何处理过高维或序列数据的人都知道，从边界或多样性中识别距离并不总是直接的。至少，这个过程比图1.2和图1.3中的简单欧几里得距离要复杂。但同样的思想仍然适用：我们试图尽可能快地达到一个准确模型，同时尽可能少地使用人工标签。
- en: The number of iterations and the number of items that need to be labeled within
    each iteration depend on the task. When you’re working in adaptive machine+human
    translation, a single translated sentence is enough training data to require the
    model to update, ideally within a few seconds. It is easy to see why from a user-experience
    perspective. If a human translator corrects the machine prediction for some word,
    but the machine doesn’t adapt quickly, the human may need to (re)correct that
    machine output hundreds of times. This problem is common when you’re translating
    words that are highly context-specific. You may want to translate a person’s name
    literally in a news article, for example, but translate it into a localized name
    in a work of fiction. The user experience will be bad if the software keeps making
    the same mistake so soon after a human has corrected it, because we expect recency
    to help with adaptation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数和每次迭代中需要标注的项目数量取决于任务。当你从事自适应机器+人工翻译工作时，一个翻译过的句子就足以作为训练数据，要求模型更新，理想情况下在几秒钟内完成。从用户体验的角度来看，这一点很容易理解。如果人工翻译者纠正了某些单词的机器预测，但机器没有快速适应，人工翻译者可能需要（重新）纠正那个机器输出数百次。当你翻译高度上下文相关的单词时，这个问题很常见。例如，你可能想在新闻文章中直译一个人的名字，但在一部虚构作品中将其翻译成本地化名字。如果软件在人工纠正后不久就重复同样的错误，用户体验将会很糟糕，因为我们期望近期性有助于适应。
- en: On the technical side, of course, it is much more difficult to adapt a model
    quickly. Consider large machine translation models. Currently, it takes a week
    or more to train these models. From the experience of the translator, a software
    system that can adapt quickly is employing continuous learning. In most use cases
    I’ve worked on, such as identifying the sentiment in social media comments, I
    needed to iterate only every month or so to adapt to new data. Although few applications
    have real-time adaptive machine learning today, more are moving this way.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术层面上，当然，快速适应模型要困难得多。以大型机器翻译模型为例。目前，训练这些模型需要一周或更长时间。根据翻译者的经验，能够快速适应的软件系统正在采用持续学习。在我所工作的多数用例中，例如识别社交媒体评论中的情感，我只需要大约每月迭代一次来适应新的数据。尽管目前很少有应用具有实时自适应机器学习，但越来越多的应用正在朝这个方向发展。
- en: 1.3.2 What is a random selection of evaluation data?
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 什么是评估数据的随机选择？
- en: It is easy to *say* that you should always evaluate on a random sample of held-out
    data, but in practical terms, it is rarely easy to ensure that you have a truly
    random sample of data. If you prefiltered the data that you are working with by
    keyword, time, or some other factor, you already have a nonrepresentative sample.
    The accuracy of that sample is not necessarily indicative of the accuracy on the
    data where your model will be deployed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然容易*说*你应该始终在保留数据的随机样本上进行评估，但在实际操作中，确保你有一个真正随机的数据样本通常并不容易。如果你通过关键词、时间或其他因素预先过滤了你正在处理的数据，你已经有一个非代表性的样本。该样本的准确率并不一定能够表明你的模型部署的数据上的准确率。
- en: I’ve seen people use the well-known ImageNet dataset and apply machine learning
    models to a broad selection of data. The canonical ImageNet dataset has 1,000
    labels, each of which describes the category of that image, such as “Basketball,”
    “Taxi,” or “Swimming.” The ImageNet challenges evaluated held-out data from that
    dataset, and systems achieved near-human-level accuracy within that dataset. If
    you apply those same models to a random selection of images posted on a social
    media platform, however, accuracy immediately drops to something like 10%.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到人们使用广为人知的ImageNet数据集，并将机器学习模型应用于大量数据。标准的ImageNet数据集有1,000个标签，每个标签描述了该图像的类别，例如“篮球”、“出租车”或“游泳”。ImageNet挑战赛评估了该数据集的保留数据，系统在该数据集内达到了接近人类水平的准确率。然而，如果你将这些相同的模型应用于社交媒体平台上发布的随机选择图像，准确率会立即下降到大约10%。
- en: In most applications of machine learning, the data will change over time as
    well. If you’re working with language data, the topics that people talk about
    will change over time, and the languages themselves will innovate and evolve.
    If you’re working with computer vision data, the types of objects that you encounter
    will change over time. Equally important, the images themselves will change based
    on advances and changes in camera technology.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习的应用中，数据也会随时间而变化。如果你处理的是语言数据，人们讨论的话题会随时间变化，语言本身也会创新和演变。如果你处理的是计算机视觉数据，你遇到的物体类型也会随时间变化。同样重要的是，图像本身也会根据相机技术的进步和变化而变化。
- en: If you can’t define a meaningful random set of evaluation data, you should try
    to define a *representative* evaluation dataset. If you define a representative
    dataset, you are admitting that a truly random sample isn’t possible or meaningful
    for your dataset. It is up to you to define what is representative for your use
    case, based on how you are applying the data. You may want to select data points
    for every label that you care about, a certain number from every time period or
    a certain number from the output of a clustering algorithm to ensure diversity.
    (I discuss this topic more in chapter 4.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能定义一个有意义的随机评估数据集，你应该尝试定义一个*代表性*的评估数据集。如果你定义了一个代表性数据集，你是在承认对于你的数据集来说，真正随机的样本是不可能的或不具有意义的。根据你如何应用数据，你需要自己定义什么是有代表性的。你可能想要为每个你关心的标签选择数据点，从每个时间段选择一定数量的数据点，或者从聚类算法的输出中选择一定数量的数据点以确保多样性。（我在第4章中更详细地讨论了这一主题。）
- en: You may also want to have multiple evaluation datasets that are compiled through
    different criteria. One common strategy is to have one dataset drawn from the
    same data as the training data and at least one out-of-domain evaluation dataset
    drawn from a different source. Out-of-domain datasets are often drawn from different
    types of media or different time periods. If all the training data for a natural
    language processing (NLP) task comes from historical news articles, for example,
    an out-of-domain dataset might come from recent social media data. For most real-world
    applications, you should use an out-of-domain evaluation dataset, which is the
    best indicator of how well your model is truly generalizing to the problem and
    not simply overfitting quirks of that particular dataset. This practice can be
    tricky with active learning, however, because as soon as you start labeling that
    data, it is no longer out-of-domain. If doing so is practical, I recommend that
    you keep an out-of-domain dataset to which you *don’t* apply active learning.
    Then you can see how well your active learning strategy is generalizing the problem,
    not simply adapting and overfitting to the domains that it encounters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还希望拥有多个通过不同标准编制的评估数据集。一个常见的策略是有一个数据集是从与训练数据相同的数据中抽取的，并且至少有一个来自不同来源的域外评估数据集。域外数据集通常来自不同类型的媒体或不同时间段。例如，如果一个自然语言处理（NLP）任务的所有训练数据都来自历史新闻文章，那么域外数据集可能来自最近的社会媒体数据。对于大多数实际应用，你应该使用域外评估数据集，这是衡量你的模型真正泛化到问题并不仅仅是过度拟合特定数据集的特性的最佳指标。然而，这种做法在主动学习时可能会很棘手，因为一旦开始标注这些数据，它们就不再是域外的了。如果这样做是可行的，我建议你保留一个不应用主动学习的域外数据集。这样，你可以看到你的主动学习策略在泛化问题时表现如何，而不仅仅是适应和过度拟合它遇到的领域。
- en: 1.3.3 When to use active learning
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 何时使用主动学习
- en: You should use active learning when you can annotate only a small fraction of
    your data and when random sampling will not cover the diversity of data. This
    recommendation covers most real-world scenarios, as the scale of the data becomes
    an important factor in many use cases.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只能标注你数据的一小部分，并且随机抽样无法覆盖数据的多样性时，你应该使用主动学习。这个建议涵盖了大多数实际场景，因为数据的规模在许多用例中成为一个重要的因素。
- en: A good example is the amount of data present in videos. Putting a bounding box
    around every object in every frame of a video, for example, would be time-consuming.
    Suppose that this video is of a self-driving car on a street with about 20 objects
    you care about (cars, pedestrians, signs, and so on). At 30 frames a second, that’s
    30 frames * 60 seconds * 20 objects, so you would need to create *36,000* boxes
    for one minute of data! Even the fastest human annotator would need at least 12
    hours to annotate one minute’s worth of data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是视频中存在的数据量。例如，在视频的每一帧中为每个对象画一个边界框，这将非常耗时。假设这个视频是关于在街道上行驶的自动驾驶汽车，你关心大约20个对象（汽车、行人、标志等）。每秒30帧，那么就是30帧
    * 60秒 * 20对象，所以你需要为1分钟的数据创建**36,000**个框！即使是速度最快的人类标注员也需要至少12小时来标注1分钟的数据。
- en: If we run the numbers, we see how intractable this problem is. In the United
    States, people drive an average of 1 hour per day, which means that people in
    the United States drive 95,104,400,000 hours per year. Soon, every car will have
    a video camera on the front to assist with driving. So 1 year’s worth of driving
    in the United States alone would take 60,000,000,000 (60 trillion) hours to annotate.
    There are not enough people on Earth to annotate the videos of drivers in the
    United States today, even if the rest of the world did nothing but annotate data
    all day to make U.S. drivers safer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算一下，我们会看到这个问题有多么难以处理。在美国，人们平均每天开车1小时，这意味着美国人每年开车95,104,400,000小时。很快，每辆车都将配备前视摄像头来辅助驾驶。所以，仅在美国一年的驾驶时间就需要60,000,000,000（60万亿）小时来标注。地球上的人不足以标注今天美国驾驶员的视频，即使全世界的人除了标注数据外什么都不做，只是为了使美国驾驶员更安全。
- en: So any data scientists at an autonomous-vehicle company needs to answer a variety
    of questions about the annotation process. Is every *n*th frame in a video OK?
    Can we sample the videos so that we don’t have to annotate them all? Are there
    ways to design an interface for annotation to speed the process?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何在自动驾驶汽车公司工作的数据科学家都需要回答关于标注过程的各种问题。视频中的每**n**帧是否都合适？我们能否采样视频，这样我们就不需要标注所有视频？有没有设计标注界面的方法来加速这个过程？
- en: The intractability of annotation is true in most situations. There will be more
    data to annotate than there is budget or time to put each data point in front
    of a human. That’s probably why the task is using machine learning in the first
    place. If you have the budget and time to annotate all the data points manually,
    you probably don’t need to automate the task.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 标注的不可行性在大多数情况下都是真实的。将需要标注的数据量将超过预算或时间，将每个数据点展示给人类。这可能是最初使用机器学习的原因。如果您有预算和时间手动标注所有数据点，那么您可能不需要自动化这项任务。
- en: You don’t need active learning in every situation, although human-in-the-loop
    learning strategies might still be relevant. In some cases, humans are required
    by law to annotate every data point, such as a court-ordered audit that requires
    a human to look at every communication within a company for potential fraud. Although
    humans will ultimately need to look at every data point, active learning can help
    them find the fraud examples faster and determine the best user interface to use.
    It can also identify potential errors with human annotations. In fact, this process
    is how many audits are conducted today.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 并非在所有情况下都需要主动学习，尽管人机交互学习策略可能仍然相关。在某些情况下，法律要求对每个数据点进行标注，例如法院命令审计，要求人类检查公司内的每一条通信以寻找潜在的欺诈。尽管人类最终需要查看每个数据点，但主动学习可以帮助他们更快地找到欺诈示例并确定最佳用户界面。它还可以识别人类标注中的潜在错误。实际上，这就是今天许多审计是如何进行的。
- en: There are also some narrow use cases in which you almost certainly don’t need
    active learning. If you are monitoring equipment in a factory with consistent
    lighting, for example, it should be easy to implement a computer vision model
    to determine whether a given piece of machinery is on or off from a light or switch
    on that machine. As the machinery, lighting, camera, and the like are not changing
    over time, you probably don’t need to use active learning to keep getting training
    data after your model has been built. These use cases are rare, however. Fewer
    than 1% of the use cases that I have encountered in industry have no use for more
    training data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些狭窄的用例，在这些用例中，您几乎肯定不需要主动学习。例如，如果您在工厂中监控具有一致照明的设备，那么应该很容易实现一个计算机视觉模型，从机器上的灯光或开关确定给定的机械设备是开启还是关闭。由于机械设备、照明、摄像头等不会随时间变化，您可能不需要在模型构建后使用主动学习来持续获取训练数据。然而，这些用例很少见。我在工业界遇到的用例中，不到1%的用例不需要更多训练数据。
- en: Similarly, there might be use cases in which your baseline model is accurate
    enough for your business use case or the cost of more training data exceeds any
    value that a more accurate model might provide. This criterion could also be the
    stopping point for active learning iterations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，可能存在一些用例，其中您的基线模型对于您的业务用例来说足够准确，或者更多训练数据的成本超过了更准确模型可能带来的任何价值。这个标准也可能是主动学习迭代的停止点。
- en: 1.4 Machine learning and human–computer interaction
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 机器学习和人机交互
- en: For decades, a lot of smart people failed to make human translation faster and
    more accurate with the help of machine translation. It seems obvious that it should
    be possible to combine human translation and machine translation. As soon as a
    human translator needs to correct one or two errors in a sentence from machine
    translation output, however, it would be quicker for the translator to retype
    the whole sentence from scratch. Using the machine translation sentence as a reference
    when translating makes little difference in speed, and unless the human translator
    takes extra care, they will end up perpetuating errors in the machine translation,
    making their translation less accurate.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，许多聪明的人未能借助机器翻译使人类翻译更快、更准确。显然，应该有可能结合人类翻译和机器翻译。然而，当人类翻译需要从机器翻译输出中纠正一句话中的一个或两个错误时，翻译者重新从零开始键入整个句子会更快。在翻译时使用机器翻译句子作为参考对速度的影响很小，除非人类翻译者格外小心，否则他们最终会延续机器翻译中的错误，使他们的翻译不够准确。
- en: The eventual solution to this problem was not in the accuracy of the machine
    translation algorithms, but in the user interface. Instead of requiring human
    translators to retype whole sentences, modern translation systems let them use
    the same kind of predictive text that has become common in phones and (increasingly)
    in email and document composition tools. Human translators type translations as
    they always have, pressing Enter or Tab to accept the next word in the predicted
    translation, increasing their overall speed every time the machine translation
    prediction is correct. So the biggest breakthrough was in human–computer interaction,
    not the underlying machine learning algorithm.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的最终解决方案不在于机器翻译算法的准确性，而在于用户界面。现代翻译系统不再要求人类翻译员重新输入整个句子，而是允许他们使用在手机上（以及越来越多地）在电子邮件和文档编辑工具中常见的预测文本功能。人类翻译员像以前一样输入翻译内容，通过按下Enter或Tab键接受预测翻译中的下一个单词，每次机器翻译预测正确时都会提高他们的整体速度。因此，最大的突破在于人机交互，而不是底层机器学习算法。
- en: Human–computer interaction is an established field in computer science that
    has recently become especially important for machine learning. When you are building
    interfaces for humans to create training data, you are drawing on a field that
    is at the intersection of cognitive science, social sciences, psychology, user-experience
    design, and several other fields.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 人机交互是计算机科学中的一个成熟领域，最近它对于机器学习变得尤为重要。当你为人类创建用于生成训练数据的界面时，你正在利用一个位于认知科学、社会科学、心理学、用户体验设计以及其他几个领域交叉点的领域。
- en: '1.4.1 User interfaces: How do you create training data?'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.1 用户界面：你如何创建训练数据？
- en: 'Often, a simple web form is enough to collect training data. The human–computer
    interaction principles that underlie interaction with web forms are equally simple:
    people are accustomed to web forms because they see them all day. The forms are
    intuitive because a lot of smart people worked on and refined HTML forms. You
    are building on these conventions: people know how a simple HTML form works, so
    you don’t need to educate them. On the other hand, breaking these conventions
    would confuse people, so you are constrained to expected behavior. You might have
    some idea that dynamic text could speed some task, but that convention could confuse
    more people than it helps.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个简单的网页表单就足以收集训练数据。与网页表单交互背后的人机交互原则同样简单：人们因为整天都能看到它们，所以习惯了网页表单。表单直观，因为许多聪明的人对HTML表单进行了工作并进行了优化。你正在利用这些约定：人们知道简单的HTML表单是如何工作的，因此你不需要对他们进行教育。另一方面，打破这些约定可能会让人困惑，因此你被限制在预期的行为上。你可能有一些想法，认为动态文本可以加快某些任务，但这个约定可能会比它帮助的人更多。
- en: The simplest interface—binary responses—is also the best for quality control.
    If you can simplify or break your annotation project into binary tasks, it is
    a lot easier to design an intuitive interface and to implement the annotation
    quality control features covered in chapters 8–11.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的界面——二元响应——也是质量控制的最佳选择。如果你能将你的注释项目简化或分解为二元任务，那么设计直观界面和实现第8章至第11章中涵盖的注释质量控制功能就会容易得多。
- en: When you are dealing with more complicated interfaces, the conventions also
    become more complicated. Imagine that you are asking people to put polygons around
    certain objects in an image, which is a common use case for autonomous-vehicle
    companies. What modalities would an annotator expect? Would they expect freehand,
    lines, paintbrushes, smart selection by color/region, or other selection tools?
    If people are accustomed to working on images in programs such as Adobe Photoshop,
    they might expect the same functionality when annotating images. In the same way
    that you are building on and constrained by people’s expectations for web forms,
    you are also constrained by their expectations for selecting and editing images.
    Unfortunately, those expectations might require hundreds of hours of coding to
    build if you are offering full-featured interfaces.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理更复杂的界面时，约定也变得更加复杂。想象一下，你要求人们在一个图像中围绕某些对象绘制多边形，这是自动驾驶汽车公司常用的用例。注释者会期待哪些方式？他们会期待自由手绘、线条、画笔、通过颜色/区域进行智能选择，或其他选择工具吗？如果人们习惯于在Adobe
    Photoshop等程序中处理图像，他们可能会在注释图像时期待相同的功能。就像你在构建和受限于人们对网页表单的期望一样，你也在受限于他们对选择和编辑图像的期望。不幸的是，如果你提供功能齐全的界面，这些期望可能需要数百小时的编码才能实现。
- en: For anyone who is undertaking a repetitive task such as creating training data,
    moving a mouse is inefficient and should be avoided if possible. If the entire
    annotation process can happen on a keyboard, including the annotation itself and
    any form submissions or navigations, the rhythm of the annotators will be greatly
    improved. If you have to include a mouse, you should be getting rich annotations
    to make up for the slower inputs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何从事重复性任务，如创建训练数据的人来说，移动鼠标效率低下，如果可能的话应该避免。如果整个注释过程都可以在键盘上完成，包括注释本身以及任何表单提交或导航，注释者的节奏将大大提高。如果你必须包括鼠标，你应该获得丰富的注释来弥补较慢的输入。
- en: Some annotation tasks have specialized input devices. People who transcribe
    speech to text often use foot pedals to navigate backward and forward in time
    in the audio recording. The process allows them to leave their hands on the keyboard.
    Navigating a recording with their feet is much more efficient than navigating
    the recording with a mouse.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一些注释任务有专门的输入设备。将语音转录成文字的人通常使用脚踏板在音频录音中前后导航。这个过程允许他们将双手放在键盘上。用脚导航录音比用鼠标导航录音要高效得多。
- en: Exceptions such as transcription aside, the keyboard is still king. Most annotation
    tasks haven’t been popular for as long as transcription and therefore haven’t
    developed specialized input devices. For most tasks, using a keyboard on a laptop
    or PC is faster than using the screen of a tablet or phone. It’s not easy to type
    on a flat surface while keeping your eyes on inputs, so unless a task is a simple
    binary selection task or something similar, phones and tablets are not suited
    to high-volume data annotation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了转录等例外，键盘仍然是王者。大多数注释任务没有像转录那样流行了很长时间，因此没有发展出专门的输入设备。对于大多数任务来说，在笔记本电脑或PC上使用键盘比在平板电脑或手机屏幕上使用键盘要快。在保持眼睛关注输入的同时在平坦表面上打字并不容易，所以除非任务是一个简单的二元选择任务或类似的东西，否则手机和平板电脑不适合进行大量数据注释。
- en: '1.4.2 Priming: What can influence human perception?'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.2 前置条件：什么可以影响人类的感知？
- en: To get accurate training data, you have to take into account the focus of the
    human annotator, their attention span, and contextual effects that might cause
    them to make errors or to otherwise change their behavior. Consider a great example
    from linguistics research. In a study called “Stuffed toys and speech perception”
    ([https://doi.org/10.1515/ling.2010.027](https://doi.org/10.1515/ling.2010.027)),
    people were asked to distinguish between Australian and New Zealand accents. Researchers
    placed a stuffed toy kiwi bird or kangaroo (iconic animals for those countries)
    on a shelf in the room where participants undertook the study. The people who
    ran the study did not mention the stuffed toy to the participants; the toy was
    simply in the background. Incredibly, people interpreted an accent as sounding
    more New Zealand-like when a kiwi bird was present and more Australia-like when
    a kangaroo was present. Given this fact, it is easy to imagine that if you are
    building a machine learning model to detect accents (perhaps you are working on
    a smart home device that you want to work in as many accents as possible), you
    need to take context into account when collecting training data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得准确的学习数据，你必须考虑到人类注释者的注意力集中、注意力持续时间和可能使他们犯错误或改变行为的情境效应。考虑一个来自语言学研究的绝佳例子。在一项名为“填充玩具与语音感知”的研究中（[https://doi.org/10.1515/ling.2010.027](https://doi.org/10.1515/ling.2010.027)），研究人员要求参与者区分澳大利亚和新西兰口音。研究人员在参与者进行研究的房间里的架子上放置了一只填充玩具几维鸟或袋鼠（这些是那些国家的标志性动物）。进行研究的负责人没有向参与者提及这个填充玩具；玩具只是放在背景中。令人难以置信的是，当几维鸟在场时，人们认为口音听起来更像新西兰，而当袋鼠在场时，口音听起来更像澳大利亚。鉴于这一事实，很容易想象，如果你正在构建一个用于检测口音的机器学习模型（也许你正在开发一个希望尽可能在多种口音下工作的智能家居设备），在收集训练数据时需要考虑情境。
- en: When the context or sequence of events can influence human perception, this
    phenomenon is known as *priming* The most important type in creating training
    data is *repetition priming*, which occurs when the sequence of tasks can influence
    someone’s perception. If an annotator is labeling social media posts for sentiment,
    for example, and they encounter 99 negative sentiment posts in a row, they are
    more likely to make an error by labeling the hundredth post as negative when it
    is positive. The post may be inherently ambiguous (such as sarcasm) or a simple
    error caused by an annotator’s fading attention during repetitive work. In chapter
    11, I talk about the types of priming you need to control for.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当上下文或事件序列可以影响人类感知时，这种现象被称为**启动效应**。在创建训练数据时最重要的类型是**重复启动效应**，它发生在任务序列可以影响某人感知的情况下。例如，如果一个注释者正在为社交媒体帖子进行情感标注，并且他们连续遇到99条负面情感帖子，那么他们更有可能在第一百条帖子实际上是正面时将其错误地标注为负面。帖子可能是固有的模糊（例如讽刺）或注释者在重复工作中注意力下降所造成的简单错误。在第11章中，我讨论了需要控制的各种启动效应类型。
- en: 1.4.3 The pros and cons of creating labels by evaluating machine learning predictions
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.3 通过评估机器学习预测创建标签的优缺点
- en: One way to combine machine learning and ensure quality annotations is to use
    a simple binary-input form to have people evaluate a model prediction and confirm
    or reject that prediction. This technique can be a nice way to turn a more complicated
    task into a binary annotation task. You could ask someone whether a bounding box
    around an object is correct as a simple binary question that doesn’t involve a
    complicated editing/selection interface. Similarly, it is easier to ask an annotator
    whether some word is a location in a piece of text than it is to provide an interface
    to efficiently annotate phrases that are locations in free text.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习和确保质量注释相结合的一种方法是通过使用简单的二进制输入表来让人们评估模型预测并确认或拒绝该预测。这种方法可以将更复杂的任务转化为二进制注释任务。你可以问某人围绕一个对象的边界框是否正确，这是一个简单的二进制问题，不涉及复杂的编辑/选择界面。同样，询问注释者某个词是否是一段文本中的位置比提供一个界面来有效地注释自由文本中的位置短语要容易得多。
- en: When you do so, however, you run the risk of focusing on localized model uncertainty
    and missing important parts of the problem space. Although you can simplify the
    interface and annotation accuracy evaluation by having humans evaluate the predictions
    of machine learning models, you still need a diversity strategy for sampling,
    even if that strategy is merely ensuring that a random selection of items is also
    available.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你这样做的时候，你可能会专注于局部模型的不确定性，而错过问题空间的重要部分。尽管你可以通过让人类评估机器学习模型的预测来简化界面和注释准确度评估，但你仍然需要一个多样性策略来进行采样，即使这个策略仅仅是确保随机选择的项目也是可用的。
- en: 1.4.4 Basic principles for designing annotation interfaces
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4.4 设计注释界面的基本原则
- en: 'Based on what I’ve covered so far, here are some basic principles for designing
    annotation interfaces. I’ll go into more detail on these principles throughout
    the book:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我到目前为止所涵盖的内容，以下是一些设计注释界面的基本原则。我将在整本书中详细介绍这些原则：
- en: Cast your problems as binary choices wherever possible.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能将问题表述为二进制选择。
- en: Ensure that expected responses are diverse to avoid priming.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保预期的响应多样化，以避免启动效应。
- en: Use existing interaction conventions.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有的交互约定。
- en: Allow keyboard-driven responses.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许键盘驱动响应。
- en: 1.5 Machine-learning-assisted humans vs. human-assisted machine learning
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 机器学习辅助人类与人类辅助机器学习
- en: 'Human-in-the-loop machine learning can have two distinct goals: making a machine
    learning application more accurate with human input and improving a human task
    with the aid of machine learning. The two goals are sometimes combined, and machine
    translation is a good example. Human translation can be made faster by using machine
    translation to suggest words or phrases that a human can choose to accept or reject,
    much as your smartphone predicts the next word as you are typing. This task is
    a machine-learning-assisted human processing task. I’ve also worked with customers
    who use machine translation when human translation would be too expensive. Because
    the content is similar across the human- and machine-translated data, the machine
    translation system gets more accurate over time from the data that is human-translated.
    These systems are hitting both goals, making the humans more efficient and making
    the machines more accurate.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中的人类机器学习可以有两个不同的目标：通过人类输入使机器学习应用更准确，以及通过机器学习的辅助来提高人类任务。这两个目标有时会结合在一起，机器翻译就是一个很好的例子。通过使用机器翻译来建议单词或短语，人类可以选择接受或拒绝，就像你在打字时你的智能手机预测下一个单词一样，这样可以使人类翻译更快。这项任务是一个机器学习辅助的人类处理任务。我还与那些在人类翻译过于昂贵时使用机器翻译的客户合作过。由于人类翻译和机器翻译的数据内容相似，机器翻译系统随着时间的推移从人类翻译的数据中变得更加准确。这些系统正在实现两个目标，使人类更有效率，使机器更准确。
- en: Search engines are another great example of human-in-the-loop machine learning.
    People often forget that search engines are a form of AI despite being so ubiquitous
    for general search and for specific use cases such as e-commerce and navigation
    (online maps). When you search for a page online and click the fourth link that
    comes up instead of the first link, for example, you are probably training that
    search engine (information retrieval system) that the fourth link might be a better
    top response for your search query. There is a common misconception that search
    engines are trained only on feedback from end users. In fact, all the major search
    engines employ thousands of annotators to evaluate and tune their search engines.
    Evaluating search relevance is the single largest use case for human annotation
    in machine learning. Although there has been a recent rise in popularity of computer
    vision use cases, such as autonomous vehicles, and speech use cases, such as in-home
    devices and smartphones, search relevance is still the largest use case for professional
    human annotation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎是另一个人类在环机器学习的绝佳例子。人们常常忘记，尽管搜索引擎在通用搜索和特定用例（如电子商务和导航（在线地图））中无处不在，但它们仍然是一种人工智能形式。例如，当你在线搜索页面并点击出现的第四个链接而不是第一个链接时，你可能在训练那个搜索引擎（信息检索系统），使其第四个链接可能成为你搜索查询的更好顶部响应。有一种普遍的误解，即搜索引擎只根据最终用户的反馈进行训练。实际上，所有主要的搜索引擎都雇佣了成千上万的注释员来评估和调整他们的搜索引擎。评估搜索相关性是机器学习中人类注释的最大用例。尽管最近计算机视觉用例（如自动驾驶汽车）和语音用例（如家庭设备和智能手机）的流行有所上升，但搜索相关性仍然是专业人类注释的最大用例。
- en: However they appear at first glance, most human-in-the-loop machine learning
    tasks have some element of both machine-learning-assisted humans and human-assisted
    machine learning, so you need to design for both.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管乍一看它们可能有所不同，但大多数环境中的人类机器学习任务都包含机器学习辅助人类和人类辅助机器学习的元素，因此你需要为两者都进行设计。
- en: 1.6 Transfer learning to kick-start your models
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 转移学习以启动你的模型
- en: You don’t need to start building your training data from scratch in most cases.
    Often, existing datasets are close to what you need. If you are creating a sentiment
    analysis model for movie reviews, for example, you might have a sentiment analysis
    dataset from product reviews that you can start with and then adapt to your use
    cases. This process—taking a model from one use case and adapting it to another—is
    known as transfer learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你不需要从头开始构建你的训练数据。通常，现有的数据集接近你所需的内容。例如，如果你正在创建用于电影评论的情感分析模型，你可能有一个从产品评论开始的情感分析数据集，然后你可以将其适应到你的用例中。这个过程——从一个用例中提取模型并将其适应到另一个用例中——被称为迁移学习。
- en: Recently, there has been a large increase in the popularity of adapting general
    pretrained models to new, specific use cases. In other words, people are building
    models *specifically* to be used in transfer learning for many use cases. These
    models are often referred to as *pretrained* models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，将通用预训练模型适应新的、特定的用例的流行度大幅增加。换句话说，人们正在构建模型，*专门*用于在许多用例中进行迁移学习。这些模型通常被称为*预训练*模型。
- en: Historically, transfer learning has involved feeding the outputs of one process
    into another. An example in NLP might be
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，迁移学习涉及将一个过程的输出输入到另一个过程中。在自然语言处理（NLP）中的一个例子可能是
- en: General part-of-speech tagger > Syntactic parser > Sentiment analysis tagger
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通用词性标注器 > 语法解析器 > 情感分析标注器
- en: Today, transfer learning typically means
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，迁移学习通常意味着
- en: Retraining part of a neural model to adapt to a new task (pretrained models)
    or using the parameters of one neural model as inputs to another
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 重新训练神经网络模型的一部分以适应新的任务（预训练模型）或使用一个神经模型的参数作为另一个模型的输入
- en: Figure 1.4 shows an example of transfer learning. A model can be trained on
    one set of labels and then retrained on another set of labels by keeping the architecture
    the same and freezing part of the model, retraining only the last layer in this
    case.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4展示了迁移学习的一个例子。一个模型可以在一组标签上训练，然后通过保持相同的架构并冻结模型的一部分（在这种情况下仅重新训练最后一层）来在另一组标签上重新训练。
- en: '![](../Images/CH01_F04_Munro.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F04_Munro.png)'
- en: Figure 1.4 An example of transfer learning. A model was built to predict a label
    as “A,” “B,” “C,” or “D.” Retraining the last layer of the model and using far
    fewer human-labeled items than if we were training a model from scratch, the model
    is able to predict labels “Y” and “Z.”
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 迁移学习的一个例子。构建了一个模型来预测标签为“A”、“B”、“C”或“D”。通过重新训练模型的最后一层，并且使用比从头开始训练模型时少得多的人工标注项，模型能够预测标签“Y”和“Z”。
- en: 1.6.1 Transfer learning in computer vision
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6.1 计算机视觉中的迁移学习
- en: 'Transfer learning has seen the most progress recently in computer vision. A
    popular strategy is to start with the ImageNet dataset and build a model from
    the millions of examples to classify the 1,000 labels: sports, birds, human-made
    objects, and so on.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习在计算机视觉领域最近取得了最大的进展。一种流行的策略是从ImageNet数据集开始，构建一个模型，从数百万个示例中分类1,000个标签：运动、鸟类、人造物体等等。
- en: To learn to classify different types of sports, animals, and objects, the machine
    learning model is learning about the types of textures and edges that are needed
    to distinguish 1,000 types of items in images. Many of these textures and edges
    are more general than the 1,000 labels and can be used elsewhere. Because all
    the textures and edges are learned in the intermediate layers of the network,
    you can retrain only the last layer on a new set of labels. You may need only
    a few hundred or a few thousand examples for each new label, instead of millions,
    because you are already drawing on millions of images for the textures and edges.
    ImageNet has seen high success when people have retrained the final layer to new
    labels with little data, including objects such as cells in biology and geographic
    features from satellite views.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习分类不同类型的运动、动物和物体，机器学习模型正在学习区分图像中1,000种物品所需的纹理和边缘类型。许多这些纹理和边缘比1,000个标签更通用，可以在其他地方使用。因为所有这些纹理和边缘都是在网络的中间层中学习的，所以你只需要在新的标签集上重新训练最后一层。你可能只需要几百个或几千个示例，而不是数百万个，因为你已经从数百万张图像中提取了纹理和边缘。当人们用少量数据重新训练最终层到新的标签时，ImageNet取得了很高的成功，包括生物中的细胞和卫星视图中的地理特征等物体。
- en: It is also possible to retrain several layers instead of the last one and to
    add more layers to the model from which you are transferring. Transfer learning
    can be used with many architectures and parameters to adapt one model to a new
    use case, but with the same goal of limiting the number of human labels needed
    to build an accurate model on new data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能重新训练除最后一层之外的其他几层，并从你正在迁移的模型中添加更多层。迁移学习可以与许多架构和参数一起使用，以适应新的用例，但目标仍然是限制构建新数据集上准确模型所需的人工标注数量。
- en: Computer vision has been less successful to date for moving beyond image labeling.
    For tasks such as detecting objects within an image, it is difficult to create
    transfer learning systems that can adapt from one type of object to another. The
    problem is that objects are being detected as collections of edges and textures
    rather than as whole objects. Many people are working on the problem, however,
    so there is no doubt that breakthroughs will occur.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，计算机视觉在超越图像标注方面还不太成功。对于检测图像中物体等任务，很难创建能够从一个物体类型适应到另一个物体类型的迁移学习系统。问题在于物体被检测为边缘和纹理的集合，而不是整体物体。然而，许多人正在研究这个问题，因此毫无疑问，突破将会发生。
- en: 1.6.2 Transfer learning in NLP
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6.2 NLP中的迁移学习
- en: The big push for pretrained models for NLP is even more recent than for computer
    vision. transfer learning of this form has become popular for NLP only in the
    past two or three years, so it is one of the most cutting-edge technologies covered
    in this text, but it also might become out of date quickly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP来说，对预训练模型的推动甚至比计算机视觉还要晚。这种形式的迁移学习在过去两三年内才在NLP中变得流行，因此它是本文中涵盖的最前沿技术之一，但它也可能很快过时。
- en: ImageNet-like adaptation does not work for language data. Transfer learning
    for one sentiment analysis dataset to another sentiment analysis dataset provides
    an accuracy increase of only ~2–3%. Models that predict document-level labels
    don’t capture the breadth of human language to the extent that equivalent computer
    vision models capture textures and edges. But you can learn interesting properties
    of words by looking at the contexts in which they occur regularly. Words such
    as *doctor* and *surgeon* might occur in similar contexts, for example. Suppose
    that you found 10,000 contexts in which any English word occurs, looking at the
    set of words before and after. You can see how likely the word *doctor* is to
    occur in each of these 10,000 contexts. Some of these contexts will be medical-related,
    so *doctor* will have a high score in those contexts. But most of the 10,000 contexts
    will not be medical-related, so *doctor* will have a low score in those contexts.
    You can treat these 10,000 scores like a 10,000-long vector. The word *surgeon*
    is likely to have a vector similar to that of *doctor* because it often occurs
    in the same context.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于ImageNet的适应方法对语言数据不起作用。将一个情感分析数据集的迁移学习应用到另一个情感分析数据集上，只能提供大约 ~2–3% 的准确率提升。预测文档级标签的模型没有像等价的计算机视觉模型那样捕捉到人类语言的广度。但通过观察它们经常出现的上下文，你可以学习到单词的有趣属性。例如，*doctor*
    和 *surgeon* 可能会在相似的环境中出现。假设你找到了任何英语单词出现的10,000个上下文，查看其前后单词集合。你可以看到单词 *doctor*
    在这10,000个上下文中出现的可能性。其中一些上下文将与医学相关，因此 *doctor* 在这些上下文中的得分会很高。但大多数10,000个上下文不会与医学相关，因此
    *doctor* 在这些上下文中的得分会很低。你可以将这些10,000个得分视为一个10,000维的向量。单词 *surgeon* 很可能有一个与 *doctor*
    类似的向量，因为它经常出现在相同的环境中。
- en: 'The concept of understanding a word by its context is old and forms the basis
    of functional theories of linguistics:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上下文理解单词的概念很古老，构成了语言学功能理论的基石：
- en: '*You shall know a word by the company it keeps (Firth, J. R. 1957:11)*.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过它所伴随的词语来认识一个单词（Firth, J. R. 1957:11)*。'
- en: Strictly, we need to go below the word to get to the most important information.
    English is an outlier in that words tend to make good atomic units for machine
    learning. English allows for complex words such as *un-do-ing* it is obvious why
    we would want to interpret the separate parts (morphemes), but English does this
    much more rarely than a typical language. What English expresses with word order,
    such as subject-verb-object, is more frequently expressed with affixes that English
    limits to things such as present and past tense and singular/plural distinctions.
    So for machine learning tasks that are not biased toward a privileged language
    such as English, which is an outlier, we need to model subwords.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，我们需要深入到单词层面才能获取最重要的信息。英语是一个例外，因为单词往往对机器学习来说是非常好的原子单位。英语允许有复杂的单词，例如 *un-do-ing*，很明显我们为什么想要解释其单独的部分（词素），但英语这样做比典型语言要少得多。英语用词序表达的内容，比如主语-谓语-宾语，更频繁地用英语限制的词缀来表达，比如现在时和过去时以及单复数区别。因此，对于不偏向于像英语这样的特权语言的机器学习任务，我们需要对子词进行建模。
- en: Firth would appreciate this fact. He founded England’s first linguistics department
    at SOAS, where I worked for two years helping record and preserve endangered languages.
    It was clear from my time there that the full breadth of linguistic diversity
    means that we need more fine-grained features than words alone. Human-in-the-loop
    machine learning methods are necessary if we are going to adapt the world’s machine-learning
    capabilities to as many of the 7,000 world languages as possible.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 费尔思会欣赏这个事实。他创立了英格兰第一个语言学系，我在那里工作了两年，帮助记录和保护濒危语言。在那里的时候，很明显，语言的全部多样性意味着我们需要比仅仅词语更精细的特征。如果我们想要将世界的机器学习能力适应尽可能多的
    7,000 种世界语言，那么需要人参与的机器学习方法就是必要的。
- en: 'When transfer learning had its recent breakthrough moment, it followed the
    principle of understanding words (or word segments) in context. We can get millions
    of labels for our models for free if we predict the word from its context:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当迁移学习最近取得突破时，它遵循了在上下文中理解词语（或词语片段）的原则。如果我们能从上下文中预测词语，我们就可以免费为我们的模型获得数百万个标签：
- en: '*My ___ is cute. He ___ play-ing*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的 ___ 很可爱。他 ___ play-ing*'
- en: No human labeling is required. We can remove some percentage of the words in
    raw text and then turn the remaining text into a predictive machine-learning task.
    As you can guess, the first blank word might be *dog*, *puppy*, or *kitten*, and
    the second blank word is likely to be *is* or *was* As with *surgeon* and *doctor*
    we can predict words from context.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要人工标注。我们可以从原始文本中移除一部分词语，然后将剩余的文本转化为一个预测性机器学习任务。正如你可以猜到的，第一个空缺的词可能是 *狗*、*小狗*
    或 *小猫*，而第二个空缺的词很可能是 *是* 或 *曾经是*。就像 *外科医生* 和 *医生* 一样，我们可以根据上下文预测词语。
- en: Unlike the early example in which transfer learning from one type of sentiment
    to another failed, these kinds of pretrained models have been widely successful.
    With only minor tuning from a model that predicts a word in context, it is possible
    to build state-of-the-art systems with small amounts of human labeling for language
    tasks such as question answering, sentiment analysis, and textual entailment.
    Unlike computer vision, transfer learning is quickly becoming ubiquitous for complicated
    NLP tasks such as summarization and translation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期迁移学习从一种情感类型到另一种情感类型失败的例子不同，这类预训练模型已经取得了广泛的成功。通过仅对预测上下文中词语的模型进行少量调整，就有可能构建具有最先进技术的系统，这些系统只需要少量的人工标注来完成语言任务，如问答、情感分析和文本蕴涵。与计算机视觉不同，迁移学习正在迅速成为复杂
    NLP 任务（如摘要和翻译）的普遍方法。
- en: The pretrained models are not complicated. The most sophisticated ones today
    are trained to predict a word in context, the order of words in a sentence, and
    the order of sentences. From that baseline model of three types of predictions
    that are inherent in the data, we can build almost any NLP use case with a head
    start. Because word order and sentence order are inherent properties of the documents,
    the pretrained models don’t need human labels. They are still built like supervised
    machine learning tasks, but the training data is generated for free. The models
    might be asked to predict one in every ten words that have been removed from the
    data and to predict when certain sentences follow each other in the source documents,
    providing a powerful head start before any human labels are required for your
    task.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的模型并不复杂。目前最复杂的模型是训练来预测上下文中的一个词，句子中词语的顺序，以及句子之间的顺序。从这个数据中固有的三种预测类型的基础模型，我们可以构建几乎任何具有先发优势的
    NLP 应用场景。因为词语顺序和句子顺序是文档的固有属性，预训练的模型不需要人工标注。它们仍然像监督机器学习任务一样构建，但训练数据是免费生成的。模型可能需要预测数据中每十个被移除的词中的一个，并预测在源文档中某些句子是否依次出现，在你需要为任务添加人工标注之前，提供强大的先发优势。
- en: Pretrained models, however, are limited by how much unlabeled text is available.
    Much more unlabeled text is available in English than in other languages, even
    when you take the overall frequency of different languages into account. There
    will be cultural biases, too. The example *My dog is cute* might appear frequently
    in online text, which is the main source of data for pretrained models. But not
    everyone has a dog as a pet. When I briefly lived in the Amazon to study the Matsés
    language, monkeys were popular pets. The English phrase *My monkey is cute* rarely
    appears online, and the Matsés equivalent *Chuna bëdambo ikek* doesn’t occur at
    all. Word vectors and the contextual models in pretrained systems do allow multiple
    meanings to be expressed by one word, so they could capture both *dog* and *monkey*.
    in this context, but they are still biased toward the data on which they are trained,
    and the *monkey* context is unlikely to occur in large volumes in any language.
    We need to be aware that pretrained systems will tend to amplify cultural biases.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，预训练模型受可用未标记文本数量的限制。即使考虑不同语言的总体频率，英语中的未标记文本也比其他语言多得多。也会有文化偏见。例如，“我的狗很可爱”这句话在在线文本中可能经常出现，这是预训练模型的主要数据来源。但并非每个人都有宠物狗。当我短暂生活在亚马逊研究马赛语时，猴子是受欢迎的宠物。英语短语“我的猴子很可爱”很少在网上出现，马赛语的对应短语“Chuna
    bëdambo ikek”则根本不存在。预训练系统中的词向量以及上下文模型确实允许一个词表达多个含义，因此它们可以捕捉到这个上下文中的“狗”和“猴子”，但它们仍然偏向于它们训练的数据，并且“猴子”的上下文在任何语言中都不太可能大量出现。我们需要意识到预训练系统可能会放大文化偏见。
- en: Pretrained models still require additional human labels to achieve accurate
    results in their tasks, so transfer learning does not change our general architecture
    for human-in-the-loop machine learning. It can give us a substantial head start
    in labeling, however, which can influence the choice of active learning strategy
    that we use to sample additional data items for human annotation and even the
    interface by which humans provide that annotation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型仍然需要额外的手工标注才能在其任务中实现准确的结果，因此转移学习并没有改变我们用于人机交互机器学习的一般架构。然而，它可以给我们一个实质性的起点，在标注方面，这可能影响我们选择用于采样额外数据项以供人工标注的主动学习策略，甚至影响人类提供标注的界面。
- en: Transfer learning also forms the basis of some of the advanced active learning
    strategies discussed in chapter 5 and the advanced data annotation and augmentation
    strategies in chapter 9.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习也是第5章中讨论的一些高级主动学习策略和第9章中高级数据标注和增强策略的基础。
- en: 1.7 What to expect in this text
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.7 本文中可以期待的内容
- en: To think about how the pieces of this text fit together, it can be useful to
    think of the topics in terms of a knowledge quadrant (figure 1.5).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要思考文本各部分如何相互关联，可以将主题视为知识四象限（图1.5）。
- en: '![](../Images/CH01_F05_Munro.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH01_F05_Munro.png)'
- en: Figure 1.5 A machine learning knowledge quadrant, covering the topics in this
    book and expressing them in terms of what is known and unknown for your machine
    learning models
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5展示了机器学习知识四象限，涵盖了本书中的主题，并以您机器学习模型已知和未知的内容来表述。
- en: The four quadrants are
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 四个象限是：
- en: '*Known known*—What your machine learning model can confidently and accurately
    do today. This quadrant is your model in its current state.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*已知已知*——您机器学习模型今天可以自信且准确地执行的内容。这个象限是您模型当前的状态。'
- en: '*Known unknown*—What your machine learning model cannot confidently do today.
    You can apply uncertainty sampling to these items.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*已知未知*——您机器学习模型今天无法自信执行的内容。您可以对这些项目应用不确定性采样。'
- en: '*Unknown known*—Knowledge within pretrained models that can be adapted to your
    task. Transfer learning allows you to use this knowledge.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*已知未知*——预训练模型中可以适应您任务的已知知识。转移学习允许您使用这些知识。'
- en: '*Unknown unknown*—Gaps in your machine learning model. You can apply diversity
    sampling to these items.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*未知未知*——您机器学习模型中的差距。您可以对这些项目应用多样性采样。'
- en: 'The columns and rows are meaningful too, with the rows capturing knowledge
    of your model in its current state and the columns capturing the type of solutions
    needed:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列和行也很有意义，行捕捉了您模型当前状态的知识，而列捕捉了所需解决方案的类型：
- en: The top row captures your model’s knowledge.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶部行捕捉了您模型的知识。
- en: The bottom row captures knowledge outside your model.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底部行捕捉了模型外的知识。
- en: The left column can be addressed by the right algorithms.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左列可以通过合适的算法来解决。
- en: The right column can be addressed by human interaction.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右列可以通过人机交互来解决。
- en: This text covers a wide range of technologies, so it might help to keep this
    figure handy to know where everything fits in.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了广泛的技术，因此保留这张图可能有助于了解所有内容是如何相互关联的。
- en: The book has cheat sheets at the end of the first few chapters as a quick reference
    for the major concepts that were covered. You can keep these cheat sheets handy
    while reading later chapters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本书在第一、二章节的末尾提供了速查表，作为对已涵盖的主要概念的快速参考。阅读后续章节时，你可以将这些速查表随时备用。
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The broader human-in-the-loop machine learning architecture is an iterative
    process combining human and machine components. Understanding these components
    explains how the parts of this book come together.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更广泛的人机交互机器学习架构是一个结合人类和机器组件的迭代过程。理解这些组件可以解释本书各部分是如何结合在一起的。
- en: You can use some basic annotation techniques to start creating training data.
    Understanding these techniques ensures that you are getting annotations accurately
    and efficiently.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用一些基本的注释技术来开始创建训练数据。理解这些技术可以确保你准确且高效地进行注释。
- en: The two most common active learning strategies are uncertainty sampling and
    diversity sampling. Understanding the basic principles of each type helps you
    strategize about the right combination of approaches for your particular problems.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种最常见的主动学习策略是不确定性采样和多样性采样。理解每种类型的基本原理有助于你针对特定问题制定合适的策略组合。
- en: Human–computer interaction gives you a framework for designing the user-experience
    components of human-in-the-loop machine learning systems.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人机交互为你提供了一个框架，用于设计包含人类操作员的机器学习系统的用户体验组件。
- en: Transfer learning allows us to adapt models trained from one task to another
    and build more accurate models with fewer annotations.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习使我们能够将一个任务中训练的模型适应到另一个任务中，并使用更少的注释构建更准确的模型。
