- en: 'Chapter 5\. Learning multiple weights at a time: generalizing gradient descent'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 章\. 一次学习多个权重：泛化梯度下降
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容**'
- en: Gradient descent learning with multiple inputs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多输入梯度下降学习
- en: 'Freezing one weight: what does it do?'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冻结一个权重：它做什么？
- en: Gradient descent learning with multiple outputs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多输出梯度下降学习
- en: Gradient descent learning with multiple inputs and outputs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多输入和输出梯度下降学习
- en: Visualizing weight values
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化权重值
- en: Visualizing dot products
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化点积
- en: “You don’t learn to walk by following rules. You learn by doing and by falling
    over.”
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你不会通过遵循规则来学习走路。你是通过做和摔倒来学习的。”
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Richard Branson, [http://mng.bz/oVgd](http://mng.bz/oVgd)*'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*理查德·布兰森，[http://mng.bz/oVgd](http://mng.bz/oVgd)*'
- en: Gradient descent learning with multiple inputs
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多输入梯度下降学习
- en: Gradient descent also works with multiple inputs
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度下降也适用于多输入
- en: In the preceding chapter, you learned how to use gradient descent to update
    a weight. In this chapter, we’ll more or less reveal how the same techniques can
    be used to update a network that contains multiple weights. Let’s start by jumping
    in the deep end, shall we? The following diagram shows how a network with multiple
    inputs can learn.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何使用梯度下降来更新权重。在这一章中，我们将或多或少地揭示如何使用相同的技巧来更新包含多个权重的网络。让我们先从深入浅出开始，好吗？以下图表显示了具有多个输入的网络如何学习。
- en: '![](Images/f0080-01_alt.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0080-01_alt.jpg)'
- en: '![](Images/f0080-02_alt.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0080-02_alt.jpg)'
- en: '![](Images/f0081-01_alt.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0081-01_alt.jpg)'
- en: There’s nothing new in this diagram. Each `weight_delta` is calculated by taking
    its output `delta` and multiplying it by its `input`. In this case, because the
    three weights share the same output node, they also share that node’s `delta`.
    But the weights have different weight `delta`s owing to their different `input`
    values. Notice further that you can reuse the `ele_mul` function from before,
    because you’re multiplying each value in `weights` by the same value `delta`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图没有什么新东西。每个 `weight_delta` 都是通过将其输出 `delta` 乘以其 `input` 来计算的。在这种情况下，因为三个权重共享相同的输出节点，它们也共享该节点的
    `delta`。但由于它们的 `input` 值不同，权重有不同的 `weight_delta`。注意，你可以重用之前的 `ele_mul` 函数，因为你在
    `weights` 中的每个值都乘以相同的值 `delta`。
- en: '![](Images/f0081-02_alt.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0081-02_alt.jpg)'
- en: Gradient descent with multiple inputs explained
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多输入梯度下降解释
- en: Simple to execute, and fascinating to understand
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单易行，理解起来非常迷人
- en: When put side by side with the single-weight neural network, gradient descent
    with multiple inputs seems rather obvious in practice. But the properties involved
    are fascinating and worthy of discussion. First, let’s take a look at them side
    by side.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当与单权重神经网络并排放置时，多输入的梯度下降在实践中似乎相当明显。但涉及的属性非常迷人，值得讨论。首先，让我们并排看看它们。
- en: '![](Images/f0082-01_alt.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0082-01_alt.jpg)'
- en: '![](Images/f0082-02_alt.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0082-02_alt.jpg)'
- en: 'Up until the generation of `delta` on the output node, single input and multi-input
    gradient descent are identical (other than the prediction differences we studied
    in [chapter 3](kindle_split_011.xhtml#ch03)). You make a prediction and calculate
    `error` and `delta` in identical ways. But the following problem remains: when
    you had only one `weight`, you had only one `input` (one `weight_delta` to generate).
    Now you have three. How do you generate three `weight_delta`s?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出节点生成 `delta` 之前，单输入和多输入的梯度下降是相同的（除了我们在第 3 章中研究的预测差异）。你做出预测，并以相同的方式计算 `error`
    和 `delta`。但问题仍然存在：当你只有一个 `weight` 时，你只有一个 `input`（一个 `weight_delta` 生成）。现在你有三个。你怎么生成三个
    `weight_delta`？
- en: How do you turn a single delta (on the node) into three weight_delta values?
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你如何将单个 delta（在节点上）转换为三个 weight_delta 值？
- en: Remember the definition and purpose of `delta` versus `weight_delta`. `delta`
    is a measure of how much you want a node’s value to be different. In this case,
    you compute it by a direct subtraction between the node’s value and what you wanted
    the node’s value to be (`pred - true`). Positive `delta` indicates the node’s
    value was too high, and negative that it was too low.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 记住 `delta` 与 `weight_delta` 的定义和目的。`delta` 是衡量你希望节点值差异多少的度量。在这种情况下，你通过节点值和希望节点值是多少之间的直接减法来计算它（预测
    - 真实）。正 `delta` 表示节点的值太高，负 `delta` 表示太低。
- en: '|  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**delta**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**delta**'
- en: A measure of how much higher or lower you want a node’s value to be, to predict
    perfectly given the current training example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望节点值高多少或低多少的度量，以便在给定当前训练示例的情况下完美预测。
- en: '|  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '`weight_delta`, on the other hand, is an *estimate* of the direction and amount
    to move the weights to reduce `node_delta`, inferred by the derivative. How do
    you transform `delta` into a `weight_delta`? You multiply `delta` by a weight’s
    `input`.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`weight_delta`，另一方面，是减少`node_delta`的方向和数量的一个**估计**，通过导数推断得出。你是如何将`delta`转换成`weight_delta`的？你将`delta`乘以权重的`input`。'
- en: '|  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**weight_delta**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**weight_delta**'
- en: A derivative-based estimate of the direction and amount you should move a weight
    to reduce `node_delta`, accounting for scaling, negative reversal, and stopping.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于导数估计你应该移动权重以减少`node_delta`的方向和数量，考虑到缩放、负反转和停止。
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Consider this from the perspective of a single weight, highlighted at right:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从单个权重的角度来考虑，如右图所示：
- en: '**`delta`:** Hey, `input`s—yeah, you three. Next time, predict a little higher.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`delta`:** 嘿，`input`s——是的，你们三个。下次，预测得高一点。'
- en: '**Single weight:** Hmm: if my `input` was 0, then my weight wouldn’t have mattered,
    and I wouldn’t change a thing (*stopping*). If my `input` was negative, then I’d
    want to decrease my weight instead of increase it (*negative reversal*). But my
    `input` is positive and quite large, so I’m *guessing* that my personal prediction
    mattered a lot to the aggregated output. I’m going to move my weight up a lot
    to compensate (*scaling*).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单个权重：** 嗯：如果我的`input`是0，那么我的权重就不会起作用，我就不会改变任何东西（**停止**）。如果我的`input`是负数，那么我就会想减少我的权重而不是增加它（**负反转**）。但我的`input`是正的，而且相当大，所以我**猜测**我的个人预测对聚合输出非常重要。我将大大提高我的权重来补偿（**缩放**）。'
- en: '![](Images/f0083-01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0083-01.jpg)'
- en: The single weight increases its value.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 单个权重增加了其值。
- en: What did those three properties/statements really say? They all (stopping, negative
    reversal, and scaling) made an observation of how the weight’s role in `delta`
    was affected by its `input`. Thus, each `weight_delta` is a sort of input-modified
    version of `delta`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 那三个属性/陈述真正说了什么？它们都（停止、负反转和缩放）观察了权重在`delta`中的作用如何受到其`input`的影响。因此，每个`weight_delta`都是`delta`的一种输入修改版本。
- en: 'This brings us back to the original question: how do you turn one (node) `delta`
    into three `weight_delta` values? Well, because each weight has a unique input
    and a shared `delta`, you use each respective weight’s `input` multiplied by `delta`
    to create each respective `weight_delta`. Let’s see this process in action.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这又把我们带回了最初的问题：如何将一个（节点）`delta`转换成三个`weight_delta`值？嗯，因为每个权重都有一个独特的输入和一个共享的`delta`，你使用每个相应权重的`input`乘以`delta`来创建每个相应的`weight_delta`。让我们看看这个过程是如何实施的。
- en: In the next two figures, you can see the generation of `weight_delta` variables
    for the previous single-input architecture and for the new multi-input architecture.
    Perhaps the easiest way to see how similar they are is to read the pseudocode
    at the bottom of each figure. Notice that the multi-weight version multiplies
    `delta` (0.14) by every input to create the various `weight_delta`s. It’s a simple
    process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个图中，你可以看到为之前单个输入架构和新的多输入架构生成`weight_delta`变量。也许最容易看到它们多么相似的方法是阅读每个图底部的伪代码。注意，多权重版本将`delta`（0.14）乘以每个输入来创建各种`weight_delta`。这是一个简单的过程。
- en: '![](Images/f0084-01_alt.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0084-01_alt.jpg)'
- en: '![](Images/f0084-02_alt.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0084-02_alt.jpg)'
- en: '![](Images/f0085-01_alt.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0085-01_alt.jpg)'
- en: '![](Images/f0085-02_alt.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0085-02_alt.jpg)'
- en: The last step is also nearly identical to the single-input network. Once you
    have the `weight_delta` values, you multiply them by `alpha` and subtract them
    from the weights. It’s literally the same process as before, repeated across multiple
    weights instead of a single one.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步几乎与单个输入网络相同。一旦你有了`weight_delta`值，你将它们乘以`alpha`并从权重中减去。这实际上是之前相同的过程，只是在多个权重上重复，而不是单个权重。
- en: Let’s watch several steps of learning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们观察几个学习步骤
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](Images/f0086-01_alt.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0086-01_alt.jpg)'
- en: We can make three individual error/weight curves, one for each weight. As before,
    the slopes of these curves (the dotted lines) are reflected by the `weight_delta`
    values. Notice that ***a*** is steeper than the others. Why is `weight_delta`
    steeper for ***a*** than the others if they share the same output `delta` and
    `error` measure? Because ***a*** has an `input` value that’s significantly higher
    than the others and thus, a higher derivative.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为每个权重制作三个单独的错误/权重曲线，每个权重一个。和之前一样，这些曲线的斜率（虚线）反映了`weight_delta`值。注意，***a***比其他曲线更陡。为什么与其他共享相同输出`delta`和`error`测量的`weight_delta`对于***a***来说更陡？因为***a***的`input`值显著高于其他，因此导数更高。
- en: '![](Images/f0087-01_alt.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0087-01_alt.jpg)'
- en: '![](Images/f0087-02_alt.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0087-02_alt.jpg)'
- en: 'Here are a few additional takeaways. Most of the learning (weight changing)
    was performed on the weight with the largest input ***a***, because the input
    changes the slope significantly. This isn’t necessarily advantageous in all settings.
    A subfield called *normalization* helps encourage learning across all weights
    despite dataset characteristics such as this. This significant difference in slope
    forced me to set `alpha` lower than I wanted (0.01 instead of 0.1). Try setting
    `alpha` to 0.1: do you see how ***a*** causes it to diverge?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些额外的收获。大部分的学习（权重变化）都是在具有最大输入***a***的权重上进行的，因为输入会显著改变斜率。这并不一定在所有情况下都是有益的。一个名为*归一化*的子领域有助于鼓励在所有权重上学习，尽管有如这种数据集特征。这种显著的斜率差异迫使我将`alpha`设置得低于我想要的（0.01而不是0.1）。尝试将`alpha`设置为0.1：你是否看到***a***导致它发散？
- en: 'Freezing one weight: What does it do?'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冻结一个权重：它有什么作用？
- en: This experiment is a bit advanced in terms of theory, but I think it’s a great
    exercise to understand how the weights affect each other. You’re going to train
    again, except weight ***a*** won’t ever be adjusted. You’ll try to learn the training
    example using only weights ***b*** and ***c*** (`weights[1]` and `weights[2]`).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验在理论上有点高级，但我认为它是理解权重如何相互影响的一个很好的练习。你将再次进行训练，但权重***a***将永远不会调整。你将尝试仅使用权重***b***和***c***（`weights[1]`和`weights[2]`）来学习训练示例。
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](Images/f0088-01_alt.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0088-01_alt.jpg)'
- en: Perhaps you’re surprised to see that ***a*** still finds the bottom of the bowl.
    Why is this? Well, the curves are a measure of each individual weight relative
    to the global error. Thus, because `error` is shared, when one weight finds the
    bottom of the bowl, all the weights find the bottom of the bowl.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能很惊讶地看到***a***仍然找到了碗底。为什么？好吧，曲线是每个单独的权重相对于全局错误的度量。因此，因为`error`是共享的，当一个权重找到碗底时，所有权重都会找到碗底。
- en: 'This is an extremely important lesson. First, if you converged (reached `error`
    = 0) with ***b*** and ***c*** weights and then tried to train ***a***, ***a***
    wouldn’t move. Why? `error` = 0, which means `weight_delta` is 0\. This reveals
    a potentially damaging property of neural networks: ***a*** may be a powerful
    input with lots of predictive power, but if the network accidentally figures out
    how to predict accurately on the training data without it, then it will never
    learn to incorporate ***a*** into its prediction.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个极其重要的教训。首先，如果你用***b***和***c***权重收敛（达到`error` = 0），然后尝试训练***a***，***a***就不会移动。为什么？`error`
    = 0，这意味着`weight_delta`是0。这揭示了神经网络的一个潜在有害特性：***a***可能是一个强大的输入，具有大量的预测能力，但如果网络意外地发现如何在没有它的情况下准确预测训练数据，那么它将永远不会学会将其纳入其预测中。
- en: '![](Images/f0089-01.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0089-01.jpg)'
- en: Also notice how ***a*** finds the bottom of the bowl. Instead of the black dot
    moving, the curve seems to move to the left. What does this mean? The black dot
    can move horizontally only if the weight is updated. Because the weight for ***a***
    is frozen for this experiment, the dot must stay fixed. But `error` clearly goes
    to 0.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意***a***是如何找到碗底的。不是黑色点在移动，而是曲线似乎向左移动。这意味着什么？黑色点只有在权重更新时才能水平移动。因为在这个实验中***a***的权重被冻结了，所以点必须保持固定。但是`error`显然降到了0。
- en: This tells you what the graphs really are. In truth, these are 2D slices of
    a four-dimensional shape. Three of the dimensions are the weight values, and the
    fourth dimension is the error. This shape is called the *error plane*, and, believe
    it or not, its curvature is determined by the training data. Why is that the case?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉你了图表的真正含义。实际上，这些都是四维形状的二维切片。三个维度是权重值，第四个维度是错误。这个形状被称为*错误平面*，信不信由你，它的曲率是由训练数据决定的。为什么是这样？
- en: '![](Images/f0089-02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0089-02.jpg)'
- en: '`error` is determined by the training data. Any network can have any `weight`
    value, but the value of `error` given any particular weight configuration is 100%
    determined by data. You’ve already seen how the steepness of the U shape is affected
    by the input data (on several occasions). What you’re really trying to do with
    the neural network is find the lowest point on this big error plane, where the
    lowest point refers to the lowest `error`. Interesting, eh? We’ll come back to
    this idea later, so file it away for now.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`error` 由训练数据决定。任何网络都可以有任意的 `weight` 值，但给定任何特定的权重配置的 `error` 值是由数据100%决定的。你已经看到输入数据的陡峭程度是如何影响U形曲线的（在几次场合）。你真正试图用神经网络做到的是找到这个大错误平面上的最低点，这里的最低点指的是最低的
    `error`。有趣，对吧？我们稍后会回到这个想法，所以现在先把它记下来。'
- en: Gradient descent learning with multiple outputs
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多输出梯度下降学习
- en: Neural networks can also make multiple predictions using only a single input
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络也可以仅使用单个输入进行多次预测
- en: Perhaps this will seem a bit obvious. You calculate each `delta` the same way
    and then multiply them all by the same, single input. This becomes each weight’s
    `weight_delta`. At this point, I hope it’s clear that a simple mechanism (stochastic
    gradient descent) is consistently used to perform learning across a wide variety
    of architectures.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这看起来有点明显。你以相同的方式计算每个 `delta`，然后将它们都乘以相同的单个输入。这变成了每个权重的 `weight_delta`。到目前为止，我希望你已经清楚，一个简单的机制（随机梯度下降）被一致地用于在各种架构中进行学习。
- en: '![](Images/f0090-01_alt.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0090-01_alt.jpg)'
- en: '![](Images/f0090-02_alt.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0090-02_alt.jpg)'
- en: '![](Images/f0091-01_alt.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0091-01_alt.jpg)'
- en: '![](Images/f0091-02_alt.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0091-02_alt.jpg)'
- en: Gradient descent with multiple inputs and outputs
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多输入多输出的梯度下降
- en: Gradient descent generalizes to arbitrarily large networks
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度下降可以推广到任意大的网络
- en: '![](Images/f0092-01_alt.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0092-01_alt.jpg)'
- en: '![](Images/f0092-02_alt.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0092-02_alt.jpg)'
- en: '![](Images/f0093-01_alt.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0093-01_alt.jpg)'
- en: '![](Images/f0093-02_alt.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0093-02_alt.jpg)'
- en: What do these weights learn?
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这些权重学习了什么？
- en: Each weight tries to reduce the error, but what do they learn in aggregate?
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个权重都试图减少错误，但它们在总体上学习了什么？
- en: Congratulations! This is the part of the book where we move on to the first
    real-world dataset. As luck would have it, it’s one with historical significance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！这是本书中我们开始转向第一个真实世界数据集的部分。幸运的是，它具有历史意义。
- en: It’s called the Modified National Institute of Standards and Technology (MNIST)
    dataset, and it consists of digits that high school students and employees of
    the US Census Bureau handwrote some years ago. The interesting bit is that these
    handwritten digits are black-and-white images of people’s handwriting. Accompanying
    each digit image is the actual number they were writing (0–9). For the last few
    decades, people have been using this dataset to train neural networks to read
    human handwriting, and today, you’re going to do the same.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 它被称为修改后的国家标准与技术研究院（MNIST）数据集，它由一些年前高中生和美国人口普查局员工手写的数字组成。有趣的是，这些手写数字是人们手写的黑白图像。每个数字图像都附有他们实际写的数字（0-9）。在过去几十年里，人们一直使用这个数据集来训练神经网络读取人类手写，而今天，你将要做同样的工作。
- en: 'Each image is only 784 pixels (28 × 28). Given that you have 784 pixels as
    input and 10 possible labels as output, you can imagine the shape of the neural
    network: each training example contains 784 values (one for each pixel), so the
    neural network must have 784 input values. Pretty simple, eh? You adjust the number
    of input nodes to reflect how many datapoints are in each training example. You
    want to predict *10 probabilities*: one for each digit. Given an input drawing,
    the neural network will produce these 10 probabilities, telling you which digit
    is most likely to be what was drawn.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像只有784个像素（28 × 28）。鉴于你有784个像素作为输入和10个可能的标签作为输出，你可以想象神经网络的形状：每个训练示例包含784个值（每个像素一个），所以神经网络必须有784个输入值。很简单，对吧？你调整输入节点的数量以反映每个训练示例中的数据点数量。你想要预测
    *10 个概率*：每个数字一个。给定一个输入绘制，神经网络将产生这10个概率，告诉你哪个数字最有可能被绘制。
- en: '![](Images/f0094-01.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0094-01.jpg)'
- en: How do you configure the neural network to produce 10 probabilities? In the
    previous section, you saw a diagram for a neural network that could take multiple
    inputs at a time and make multiple predictions based on that input. You should
    be able to modify this network to have the correct number of inputs and outputs
    for the new MNIST task. You’ll tweak it to have 784 inputs and 10 outputs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何配置神经网络以产生10个概率？在上一节中，你看到了一个可以一次接受多个输入并基于该输入做出多个预测的神经网络图。你应该能够修改这个网络，使其具有正确数量的输入和输出，以适应新的MNIST任务。你需要调整它，使其有784个输入和10个输出。
- en: 'In the MNISTPreprocessor notebook is a script to preprocess the MNIST dataset
    and load the first 1,000 images and labels into two NumPy matrices called `images`
    and `labels`. You may be wondering, “Images are two-dimensional. How do I load
    the (28 × 28) pixels into a flat neural network?” For now, the answer is simple:
    flatten the images into a vector of 1 × 784\. You’ll take the first row of pixels
    and concatenate them with the second row, and the third row, and so on, until
    you have one list of pixels per image (784 pixels long).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNISTPreprocessor笔记本中有一个脚本，用于预处理MNIST数据集，并将前1000张图像和标签加载到两个名为`images`和`labels`的NumPy矩阵中。你可能想知道：“图像是二维的。我如何将(28
    × 28)像素加载到一个平坦的神经网络中？”目前，答案是简单的：将图像展平成一个1 × 784的向量。你将取像素的第一行，并将其与第二行、第三行等拼接起来，直到你有一个包含每张图像的像素列表（784个像素长）。
- en: This diagram represents the new MNIST classification neural network. It most
    closely resembles the network you trained with multiple inputs and outputs earlier.
    The only difference is the number of inputs and outputs, which has increased substantially.
    This network has 784 inputs (one for each pixel in a 28 × 28 image) and 10 outputs
    (one for each possible digit in the image).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表示新的MNIST分类神经网络。它与你之前用多个输入和输出训练的网络最相似。唯一的区别是输入和输出的数量，这已经大幅增加。此网络有784个输入（每个28
    × 28图像中的像素一个）和10个输出（每个可能的数字一个）。
- en: If this network could predict perfectly, it would take in an image’s pixels
    (say, a 2, like the one in the next figure) and predict a 1.0 in the correct output
    position (the third one) and a 0 everywhere else. If it were able to do this correctly
    for all the images in the dataset, it would have no error.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个网络能够完美预测，它将接受图像的像素（比如，下一个图中的2）并在正确的输出位置（第三个）预测1.0，在其他所有位置预测0。如果它能够对数据集中的所有图像都这样做，它将没有错误。
- en: '![](Images/f0095-01.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0095-01.jpg)'
- en: '![](Images/f0095-02_alt.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0095-02_alt.jpg)'
- en: Over the course of training, the network will adjust the weights between the
    input and prediction nodes so that `error` falls toward 0 in training. But what
    does this do? What does it mean to modify a bunch of weights to learn a pattern
    in aggregate?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，网络将调整输入和预测节点之间的权重，以便在训练中使`error`值接近0。但这意味着什么？修改大量权重以学习整体模式是什么意思？
- en: Visualizing weight values
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化权重值
- en: An interesting and intuitive practice in neural network research (particularly
    for image classifiers) is to visualize the weights as if they were an image. If
    you look at this diagram, you’ll see why.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络研究中（尤其是对于图像分类器）的一个有趣且直观的实践是将权重可视化成图像。如果你看这个图，你就会明白为什么。
- en: Each output node has a weight coming from every pixel. For example, the 2? node
    has 784 input weights, each mapping the relationship between a pixel and the number
    2.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出节点都有来自每个像素的权重。例如，2?节点有784个输入权重，每个权重映射像素和数字2之间的关系。
- en: What is this relationship? Well, if the weight is high, it means the model believes
    there’s a high degree of *correlation* between that pixel and the number 2\. If
    the number is very low (negative), then the network believes there is a very low
    correlation (perhaps even negative correlation) between that pixel and the number
    2.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关系是什么？嗯，如果权重高，这意味着模型认为该像素和数字2之间存在高度的相关性。如果数字非常低（负数），那么网络认为该像素和数字2之间的相关性非常低（甚至可能是负相关性）。
- en: '![](Images/f0096-01.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0096-01.jpg)'
- en: If you take the weights and print them out into an image that’s the same shape
    as the input dataset images, you can see which pixels have the highest correlation
    with a particular output node. In our example, a very vague 2 and 1 appear in
    the two images, which were created using the weights for 2 and 1, respectively.
    The bright areas are high weights, and the dark areas are negative weights. The
    neutral color (red, if you’re reading this in the eBook) represents 0 in the weight
    matrix. This illustrates that the network generally knows the shape of a 2 and
    of a 1.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将权重打印成与输入数据集图像相同形状的图像，你可以看到哪些像素与特定的输出节点相关性最高。在我们的例子中，使用 2 和 1 的权重分别创建的两个图像中，出现了非常模糊的
    2 和 1。亮区代表高权重，暗区代表负权重。中性颜色（如果你在电子书中阅读，则是红色）代表权重矩阵中的 0。这表明网络通常知道 2 和 1 的形状。
- en: Why does it turn out this way? This takes us back to the lesson on dot products.
    Let’s have a quick review.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样？这把我们带回到了点积的课程。让我们快速回顾一下。
- en: Visualizing dot products (weighted sums)
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化点积（加权求和）
- en: 'Recall how dot products work. They take two vectors, multiply them together
    (elementwise), and then sum over the output. Consider this example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下点积是如何工作的。它们取两个向量，逐元素相乘，然后对输出求和。考虑以下例子：
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Score**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 分**'
- en: First you multiply each element in `a` and `b` by each other, in this case creating
    a vector of 0s. The sum of this vector is also 0\. Why? Because the vectors have
    nothing in common.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要将 `a` 和 `b` 中的每个元素相互相乘，在这种情况下，创建了一个全为 0 的向量。这个向量的和也是 0。为什么？因为这两个向量没有共同点。
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: But the dot products between `c` and `d` return higher scores, because there’s
    overlap in the columns that have positive values. Performing dot products between
    two identical vectors tends to result in higher scores, as well. The takeaway?
    *A dot product is a loose measurement of similarity between two vectors.*
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 但 `c` 和 `d` 之间的点积返回更高的分数，因为具有正值的列之间存在重叠。两个相同向量的点积通常会导致更高的分数。结论是：*点积是衡量两个向量之间相似性的粗略度量。*
- en: What does this mean for the weights and inputs? Well, if the `weight` vector
    is similar to the `input` vector for 2, then it will output a high score because
    the two vectors are similar. Inversely, if the `weight` vector is *not* similar
    to the `input` vector for 2, it will output a low score. You can see this in action
    in the following figure. Why is the top score (0.98) higher than the lower one
    (0.01)?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这对权重和输入意味着什么？嗯，如果 `weight` 向量与 2 的 `input` 向量相似，那么它将输出一个高分，因为这两个向量相似。相反，如果 `weight`
    向量与 2 的 `input` 向量不相似，它将输出一个低分。你可以在以下图中看到这一点。为什么最高分（0.98）比较低分（0.01）高？
- en: '![](Images/f0097-01.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0097-01.jpg)'
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Gradient descent is a general learning algorithm
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度下降是一种通用学习算法
- en: Perhaps the most important subtext of this chapter is that gradient descent
    is a very flexible learning algorithm. If you combine weights in a way that allows
    you to calculate an error function and a `delta`, gradient descent can show you
    how to move the weights to reduce the error. We’ll spend the rest of this book
    exploring different types of weight combinations and error functions for which
    gradient descent is useful. The next chapter is no exception.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最重要的潜台词可能是梯度下降是一个非常灵活的学习算法。如果你以某种方式组合权重，可以计算误差函数和 `delta`，梯度下降可以告诉你如何移动权重以减少误差。本书的剩余部分将探讨梯度下降对不同的权重组合和误差函数有用的不同类型。下一章也不例外。
