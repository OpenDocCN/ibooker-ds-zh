- en: 10 Healthy microservices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 健康的微服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Techniques to ensure your microservices remain healthy
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保您的微服务保持健康的技术
- en: Logging and monitoring for microservices
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微服务的日志记录和监控
- en: Debugging microservices
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微服务调试
- en: Patterns for reliability and fault tolerance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可靠性和容错模式
- en: Errors happen. Code has bugs. Hardware, software, and networks are unreliable.
    Failures happen for all types of applications, not just for microservices. But
    microservices applications are more complex, and so problems can become considerably
    worse as we grow our application. The more microservices we maintain, the greater
    the chance at any given time that some of those microservices will be misbehaving!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 错误会发生。代码有错误。硬件、软件和网络是不可靠的。所有类型的应用程序都会发生故障，而不仅仅是微服务。但微服务应用程序更加复杂，因此随着我们应用程序的增长，问题可能会变得相当严重。我们维护的微服务越多，在特定时间某些微服务出现行为不当的可能性就越大！
- en: We can’t avoid problems entirely. It doesn’t matter if these are caused by human
    error or unreliable infrastructure. It’s a certainty-problems happen. But just
    because problems can’t always be avoided, doesn’t mean we shouldn’t try to mitigate
    against these. A well-engineered application anticipates and accounts for problems,
    even when the specific nature of some problems can’t be anticipated.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法完全避免问题。这些问题是由人为错误或不可靠的基础设施引起的无关紧要。问题是肯定会发生的。但仅仅因为问题无法总是避免，并不意味着我们不应该尝试减轻这些问题。一个精心设计的应用程序会预见并考虑到问题，即使某些问题的具体性质无法预见。
- en: As our application evolves to be more complex, we’ll need techniques to combat
    problems and keep our microservices healthy. Our industry has developed many “best”
    practices and patterns for dealing with problems. We’ll cover some of the most
    useful ones in this chapter. Following this guidance will make your application
    run more smoothly and be more reliable, resulting in less stress and easier recovery
    from problems when they do happen.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的应用程序变得更加复杂，我们需要技术来应对问题并保持微服务的健康。我们的行业已经发展了许多“最佳”实践和模式来处理问题。在本章中，我们将介绍其中一些最有用的。遵循这些指导原则将使您的应用程序运行更加顺畅，更加可靠，从而在问题发生时减少压力，更容易从问题中恢复。
- en: This chapter isn’t immediately practical; there’s no example code in GitHub
    and you can’t directly follow along. Think of it as a *toolbox of techniques*
    for you to try out in the future as you move forward and continue to develop your
    own microservices application.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章并不立即实用；GitHub中没有示例代码，您也不能直接跟随。将其视为一个*技术工具箱*，您可以在未来尝试，随着您继续前进并继续开发自己的微服务应用程序。
- en: 10.1 Maintaining healthy microservices
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 维护健康的微服务
- en: A healthy microservices application is composed of healthy microservices. A
    healthy microservice is one that is not experiencing problems such as bugs, CPU
    overload, or memory exhaustion. To understand the health of our application, we
    need to
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个健康的微服务应用程序由健康的微服务组成。一个健康的微服务是指没有遇到问题，如错误、CPU过载或内存耗尽。为了了解我们应用程序的健康状况，我们需要
- en: Monitor our microservices to understand their current state
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控我们的微服务以了解它们的当前状态
- en: Take action when problems occur to protect our customers
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当问题发生时采取行动以保护我们的客户
- en: Debug and apply fixes as issues arise
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在出现问题时进行调试并应用修复
- en: Using FlixTube’s metadata microservice as an example, figure 10.1 gives you
    an idea of the infrastructure for a healthy microservice in production. Notice
    that there are multiple replicas of the microservice, and that requests are evenly
    balanced between instances of the microservice using a *load balancer*. Should
    any single microservice go out of commission, the replicas can stand in while
    the failing instance is restarted.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以FlixTube的元数据微服务为例，图10.1展示了生产中健康微服务的架构。请注意，该微服务有多个副本，并且使用*负载均衡器*在微服务的实例之间均匀分配请求。如果任何单个微服务失效，副本可以替代，直到失败的实例重新启动。
- en: '![](../Images/CH10_F01_Davis4.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F01_Davis4.png)'
- en: Figure 10.1 Infrastructure for a healthy microservice in production
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 生产中健康微服务的架构
- en: This redundancy ensures the ongoing reliability of the microservice and the
    application. In this chapter, we’ll learn about replicating microservices on Kubernetes
    and other techniques to facilitate fault tolerance and recovery from errors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种冗余确保了微服务和应用程序的持续可靠性。在本章中，我们将学习在Kubernetes上复制微服务以及其他促进容错和从错误中恢复的技术。
- en: A microservice can suffer problems even without the dramatic effect of going
    out of commission. How do we know what’s going on in a microservice? It doesn’t
    have to be a black box. We need some kind of logging *aggregation* service (shown
    in figure 10.1) to combine the logging from all microservices presented in a way
    that we can understand.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有停机的大幅影响，微服务也可能出现问题。我们如何知道微服务中发生了什么？它不必是一个黑盒。我们需要某种类型的日志*聚合*服务（如图 10.1 所示）来以我们可以理解的方式组合所有展示的微服务的日志。
- en: What can we do to ensure that our microservices remain healthy? First, similar
    to a real medical professional, we must know how to *take the temperature* of
    our patients. We have numerous techniques at our disposal to help us diagnose
    the state and behavior of our microservices. Table 10.1 lists the main techniques
    we’ll learn in this chapter to take the temperature of our microservices.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做什么来确保我们的微服务保持健康？首先，类似于真正的医疗专业人士，我们必须知道如何*测量体温*。我们有多种技术可供我们诊断微服务的状态和行为。表
    10.1 列出了本章我们将学习的主要技巧来测量微服务的体温。
- en: Table 10.1 Techniques for monitoring the state of microservices
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.1 监控微服务状态的技巧
- en: '| Technique | Description |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 技巧 | 描述 |'
- en: '| Logging | Outputting information about the behavior of our microservices
    to show what is happening and when it happened. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 日志记录 | 将有关我们微服务行为的信息输出，以显示正在发生的事情以及何时发生。 |'
- en: '| Error handling | Having a strategy for managing errors to have a record of
    what failed and when it failed. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 错误处理 | 制定管理错误的策略，以记录失败的内容和失败的时间。 |'
- en: '| Aggregation | Combining the information from all microservices into a single
    stream so that we don’t have to go searching microservice-to-microservice for
    the information we need. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 聚合 | 将所有微服务的相关信息合并到单个流中，这样我们就不必在微服务之间搜索所需的信息。 |'
- en: '| Automatic health checks | Configuring Kubernetes to automatically find problems
    in our microservices. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 自动健康检查 | 配置 Kubernetes 以自动发现我们的微服务中的问题。 |'
- en: What happens when something has gone wrong? How do we fix it? Coping with problems
    that have occurred requires debugging. In this chapter, we’ll learn the techniques
    that we can use to find the cause of a problem so that we can fix it.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当出现问题时会发生什么？我们如何修复它？应对已经发生的问题需要调试。在本章中，我们将学习我们可以使用的技巧来找到问题的原因，以便我们可以修复它。
- en: 10.2 Monitoring your microservices
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 监控您的微服务
- en: Getting our application into production is just the first step. After that,
    we need to continually know if our application is functioning or not, especially
    as new updates to the code are rolled out.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们的应用程序部署到生产环境只是第一步。之后，我们需要持续了解我们的应用程序是否在运行，尤其是在代码的新更新推出之后。
- en: 'We must have transparency over what our application is doing, otherwise, we
    have no idea what’s going on in there, and we can’t fix problems unless we know
    about them. In this section, we’ll look at some techniques for monitoring the
    behavior of our microservices:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须了解我们的应用程序正在做什么，否则我们无法知道里面发生了什么，除非我们知道它们，否则我们无法修复问题。在本节中，我们将探讨一些监控我们微服务行为的技巧：
- en: Logging
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录日志
- en: Error handling
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误处理
- en: Log aggregation
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志聚合
- en: Automatic health checks
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动健康检查
- en: 10.2.1 Logging in development
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 开发中的登录
- en: Logging to the console is our most basic tool for understanding the ongoing
    behavior of our microservices. Through logging, we output a text stream showing
    the important events, activities, and actions that have taken place within our
    application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志记录到控制台是我们理解微服务持续行为的最低级工具。通过日志记录，我们输出一个文本流，显示应用程序内部发生的重要事件、活动和操作。
- en: The stream of logs coming from an application can be thought of as *the history
    of the application*, showing everything pertinent that has happened over its lifetime.
    We can use console logging in both development and production. Figure 10.2 illustrates
    how it works in development.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 来自应用程序的日志流可以被视为*应用程序的历史记录*，显示了在其整个生命周期中发生的所有相关事件。我们可以在开发和生产中使用控制台日志。图 10.2 展示了它在开发中的工作方式。
- en: '![](../Images/CH10_F02_Davis4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F02_Davis4.png)'
- en: Figure 10.2 Console logging during development
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 开发中的控制台日志
- en: 'Every microservice, like every process, has two output streams for logging:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个微服务，就像每个进程一样，都有两个用于日志记录的输出流：
- en: Standard output
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准输出
- en: Standard error
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准错误
- en: 'In JavaScript, we output logging to the standard output channel like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JavaScript 中，我们像这样将日志输出到标准输出通道：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We output errors to the standard error channel like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像这样将错误输出到标准错误通道：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note If you are using a language other than JavaScript, then it will have its
    own functions for outputting to standard output and standard error.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你使用的是除JavaScript之外的语言，那么它将有自己的函数来输出到标准输出和标准错误。
- en: That’s all we need to output to the console. We don’t really need a complex
    logging system. Modern logging aggregation systems usually automatically pick
    up the standard output and standard errors that are flowing out of a container.
    We’ll see how this works soon.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们输出到控制台所需的所有内容。我们实际上不需要复杂的日志系统。现代日志聚合系统通常会自动收集从容器中流出的标准输出和标准错误。我们很快就会看到它是如何工作的。
- en: What should be logged?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 应该记录什么？
- en: 'Given that logging has to be added explicitly by the developer and it’s always
    optional, how do we choose what to log? Here are a few examples:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于日志记录必须由开发者显式添加，并且始终是可选的，我们该如何选择要记录的内容呢？以下是一些示例：
- en: 'What to log:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该记录什么：
- en: Pertinent events in your application and details about those
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序中的相关事件及其细节
- en: Success/failure of important operations
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要操作的成功/失败
- en: 'What not to log:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不应该记录的内容：
- en: Things that can easily be ascertained from other sources
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从其他来源轻易确定的事情
- en: Anything that’s secret or sensitive
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何秘密或敏感信息
- en: Any personal details about your users
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关你用户的任何个人详细信息
- en: If you find yourself drowning in details from too much logging, feel free to
    go in and remove logging that isn’t useful. For every console log, you just have
    to ask the question, can I live without this detail? If you don’t need it, delete
    it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己被过多的日志细节淹没，你可以自由地进入并删除没有用的日志。对于每个控制台日志，你只需要问自己一个问题：没有这个细节我能活下去吗？如果你不需要它，就删除它。
- en: Generally speaking, though, more logs are better than fewer logs. When it comes
    to debugging in production, you need all the help you can get to understand why
    a problem occurred. Tracing back through the log is an important step in understanding
    the sequence of events that resulted in the problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然如此，但一般来说，更多的日志比更少的日志好。在生产环境中进行调试时，你需要尽可能多的帮助来了解问题发生的原因。通过日志回溯是理解导致问题的事件序列的重要步骤。
- en: You won’t be able to add more logging after the problem has occurred! Well,
    you can if you isolate and reproduce the problem, but that in itself can be difficult.
    More logging is better because when you do hit a problem, you want to have as
    much information as possible to help you solve it.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题发生之后，你将无法添加更多的日志！好吧，如果你能够隔离并重现问题，那么你可以这样做，但这本身可能很困难。更多的日志记录更好，因为当你遇到问题时，你希望拥有尽可能多的信息来帮助你解决问题。
- en: 10.2.2 Error handling
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 错误处理
- en: 'Errors happen. Our users suffer. It’s a fundamental law of computer programming!
    Here are some examples of errors:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 错误会发生。我们的用户会受到影响。这是计算机编程的基本法则！以下是一些错误的示例：
- en: Runtime errors (an exception is thrown that crashes our microservice)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行时错误（抛出异常导致我们的微服务崩溃）
- en: Bad data being input (from faulty sensors or human error in data entry)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入错误数据（来自故障传感器或数据输入中的人为错误）
- en: Code being used in unexpected combinations or ways
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以意外组合或方式使用的代码
- en: Third-party dependencies failing (RabbitMQ, for example)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三方依赖项失败（例如RabbitMQ）
- en: External dependencies failing (Azure Storage, for example)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部依赖项失败（例如Azure存储）
- en: How we deal with errors matters. We must plan to handle and recover from errors
    gracefully to minimize the harm caused to our users and our business. What happens
    when errors occur? How will our application deal with these? We must think through
    these questions and develop an error-handling strategy for our application.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何处理错误很重要。我们必须计划优雅地处理和恢复错误，以最大限度地减少对我们用户和业务的损害。当错误发生时会发生什么？我们的应用程序将如何处理这些问题？我们必须思考这些问题，并为我们的应用程序制定一个错误处理策略。
- en: Often in our JavaScript code, we’ll anticipate errors and handle these in our
    code using exceptions, callbacks, or promises. In those cases, we usually know
    what to do. We can retry the failed operation, or if possible, we might correct
    the issue and restart the operation if there isn’t any automatic corrective action
    that’s obvious. We even might have to report the error to the user or notify our
    operations staff.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的JavaScript代码中，我们通常会预测错误，并在我们的代码中使用异常、回调或承诺来处理这些错误。在这些情况下，我们通常知道该怎么做。我们可以重试失败的操作，或者如果可能，如果没有明显的自动纠正措施，我们可能需要向用户报告错误或通知我们的运维人员。
- en: Sometimes we can anticipate errors, other times not. We can miss errors because
    we didn’t know the error could occur or because certain types of errors (e.g.,
    a hard-drive failure) occur so infrequently that it’s not worth specifically handling
    these. To be safe, we must account for errors that we can’t even imagine!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们可以预见错误，有时则不能。我们可能会错过错误，因为我们不知道错误可能会发生，或者因为某些类型的错误（例如，硬盘故障）发生的频率如此之低，以至于不值得专门处理这些错误。为了安全起见，我们必须考虑到我们甚至无法想象的错误！
- en: 'What we need is a general strategy for how we handle unexpected errors. For
    any process, including individual microservices, this boils down to two main options:
    *abort* *and restart* or *resume operation**.* You can see these error-handling
    strategies illustrated in figure 10.3.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一种处理意外错误的一般策略。对于任何进程，包括单个微服务，这归结为两个主要选项：*中断* *和重启* 或 *恢复操作*。您可以在图 10.3
    中看到这些错误处理策略的说明。
- en: '![](../Images/CH10_F03_Davis4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F03_Davis4.png)'
- en: Figure 10.3 Strategies for handling unexpected errors
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 处理意外错误的策略
- en: Abort and restart
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 中断和重启
- en: The abort and restart strategy intercepts unexpected errors and responds by
    restarting the process. The simplest way to use this strategy is to just ignore
    any errors we don’t care about. Any exception that we don’t explicitly handle
    with a try/catch statement in our code results in the process being aborted.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 中断和重启策略拦截意外错误，并通过重启进程来响应。使用此策略的最简单方法是忽略我们不关心的任何错误。任何我们没有在代码中使用 try/catch 语句明确处理的异常都会导致进程被中断。
- en: This is the simplest error-handling strategy because it literally means *doing
    nothing*. Just allow unexpected errors to occur and let Node.js abort our program
    in response. When a production microservice is aborted, we’ll rely on Kubernetes
    to automatically restart it for us, which it does by default. (This behavior in
    Kubernetes is configurable as well.)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的错误处理策略，因为它实际上意味着*什么都不做*。只需允许意外错误发生，并让 Node.js 响应地中断我们的程序。当生产级微服务被中断时，我们将依赖
    Kubernetes 自动为我们重启它，这是它的默认行为。（在 Kubernetes 中，这种行为也是可配置的。）
- en: Resume operation
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复操作
- en: 'The resume operation strategy intercepts unexpected errors and responds by
    allowing the process to continue. We can implement this in Node.js by handling
    the `uncaughtException` event on the `process` object like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复操作策略拦截意外错误，并通过允许进程继续来响应。我们可以在 Node.js 中通过在 `process` 对象上处理 `uncaughtException`
    事件来实现这一点：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we handled the event like this, we take explicit control over unexpected
    errors. In that case, Node.js will not take its default action of aborting the
    process. It is simply left to continue as best it can, and we have to hope that
    the error has not left the process in a bad state.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们像这样处理事件，我们就明确控制了意外错误。在这种情况下，Node.js 不会采取默认的中断进程的操作。它只是简单地继续尽可能好地运行，我们必须希望错误没有使进程处于不良状态。
- en: Printing the error to the standard error channel means that it can be picked
    up by our production logging system, which we’ll discuss soon. This error can
    then be reported to our operations team, and it doesn’t have to go unnoticed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将错误打印到标准错误通道意味着它可以被我们的生产日志系统捕获，我们将在不久后讨论。然后，可以将此错误报告给我们的运维团队，而无需被忽视。
- en: 'Abort and restart: Version 2'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 中断和重启：版本 2
- en: 'Now that we understand how to handle uncaught exceptions in Node.js, we can
    implement a better version of the abort and restart strategy:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在 Node.js 中处理未捕获的异常，我们可以实现一个更好的版本的中断和重启策略：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this code, we take explicit control of the handler for unexpected errors.
    As before, we print the error so that it can be noticed by our operations team.
    Next, we explicitly terminate the program with a call to `process.exit`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们明确控制了意外错误的处理器。与之前一样，我们打印错误以便我们的运维团队能够注意到。接下来，我们通过调用 `process.exit` 明确终止程序。
- en: We pass a nonzero exit code to the `exit` function. This is a standard convention
    that indicates the process was terminated by an error. We can use different nonzero
    error codes here (any positive number) to indicate different types of errors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向 `exit` 函数传递一个非零退出码。这是一个标准约定，表示进程因错误而终止。我们可以在这里使用不同的非零错误码（任何正数）来指示不同类型的错误。
- en: Which error handling strategy should I use?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该使用哪种错误处理策略？
- en: To restart or not to restart, that is the question. Many developers swear by
    abort and restart, and in most situations, it’s a good idea to simply let our
    processes crash. Because trying to recover a microservice after a crash can leave
    it limping along in a damaged state.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 关于是否重启，这是一个问题。许多开发者坚信应该终止并重启，在大多数情况下，简单地让我们的进程崩溃是一个好主意。因为尝试在崩溃后恢复微服务可能会使其处于损坏状态而无法正常工作。
- en: With *abort and restart*, we can monitor for crashes to know which microservices
    have had problems that need to be resolved. If you couple this with good error
    reporting, it’s a good general strategy that you can apply by default.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *终止并重启*，我们可以监控崩溃，以了解哪些微服务出现了需要解决的问题。如果你结合良好的错误报告，这是一个好的通用策略，你可以默认应用。
- en: Sometimes, though, we might need to use the *resume operation* strategy. For
    some microservices (for example, microservices that deal with customer data),
    we must think through the implications of aborting the process.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时我们可能需要使用 *恢复操作* 策略。对于某些微服务（例如处理客户数据的微服务），我们必须仔细考虑终止进程的后果。
- en: As an example, let’s consider FlixTube’s video upload microservice. Is it OK
    for this microservice to be aborted at any time? At any given moment it might
    be accepting multiple video uploads from multiple users. Is it acceptable to abort
    this microservice, potentially losing user uploads? I would say no, but if this
    is your microservice, you might have a different opinion and that’s OK. There
    is no right way to do this.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以 FlixTube 的视频上传微服务为例，这个微服务在任何时候都可以被终止吗？在任何时刻，它可能正在接受来自多个用户的多个视频上传。终止这个微服务，可能会丢失用户上传，这是可以接受的吗？我会说不可以，但如果你是这个微服务，你可能有不同的看法，这也可以。没有一种正确的方式来处理这个问题。
- en: Note When deciding which strategy to use, it’s probably best to default to *abort
    and restart*, but occasionally *resume operation* will be more appropriate.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在决定使用哪种策略时，最好默认使用 *终止并重启*，但偶尔 *恢复操作* 可能会更合适。
- en: 10.2.3 Logging with Docker Compose
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 使用 Docker Compose 进行日志记录
- en: When using Docker Compose in development, we can see the logging from all our
    microservices in a single stream in our terminal window. Docker automatically
    collects the logging and aggregates it into a single stream as indicated in figure
    10.4\. Obviously, this is useful to get a broad overview of what our application
    is doing at any given time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当在开发中使用 Docker Compose 时，我们可以在终端窗口中看到所有微服务的日志输出合并为一个流。Docker 自动收集日志并将其汇总成一个单一的流，如图
    10.4 所示。显然，这有助于我们全面了解应用在任何给定时间正在做什么。
- en: '![](../Images/CH10_F04_Davis4.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F04_Davis4.png)'
- en: Figure 10.4 When using Docker Compose, Docker aggregates the logging from all
    our microservices into a single stream.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 使用 Docker Compose 时，Docker 将所有微服务的日志汇总成一个单一的流。
- en: Redirecting logging to a file
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志重定向到文件
- en: Here’s a trick that I find very useful. When we run Docker Compose, we can redirect
    its output and capture it to a log file. The `tee` command means we can display
    output both in the terminal as well as saving it to a file.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个我发现非常实用的技巧。当我们运行 Docker Compose 时，我们可以重定向其输出并将其捕获到日志文件中。`tee` 命令意味着我们可以在终端显示输出同时将其保存到文件中。
- en: '![](../Images/CH10_UN01_Davis4.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN01_Davis4.png)'
- en: Now we can load the log file (in this example, debug.log) in VS code and browse
    it at our leisure. We can search for particular strings of text. For example,
    if we are trying to find a problem with the database, we might search for logs
    that contain the word “database.”
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将日志文件（在这个例子中是 debug.log）加载到 VS code 中，并随意浏览它。我们可以搜索特定的文本字符串。例如，如果我们正在尝试找到数据库的问题，我们可能会搜索包含“database”一词的日志。
- en: I even like to put special codes (character sequences) in my logging to distinguish
    the logs for particular subsystems of a microservice. This makes it easier to
    search or filter for the types of logs you are interested in.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我甚至喜欢在我的日志中放入特殊的代码（字符序列），以区分微服务特定子系统的日志。这使得搜索或过滤你感兴趣的日志类型变得更容易。
- en: 10.2.4 Basic logging with Kubernetes
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4 Kubernetes 中的基本日志记录
- en: When running microservices in development under Docker Compose, we run the application
    locally on our development workstation. That makes it easy to see the logging
    from our application and understand what’s happening in our code.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Docker Compose 下开发中运行微服务时，我们在本地开发工作站上运行应用程序。这使得我们很容易看到应用程序的日志并理解代码中的情况。
- en: Retrieving logging from our production microservices running remotely on Kubernetes
    is much more difficult, however. To see the logging, we must be able to extract
    it from the cluster and pull it back to our development workstation for analysis.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从在 Kubernetes 上远程运行的生产微服务中检索日志要困难得多。要查看日志，我们必须能够从集群中提取它并将其拉回到我们的开发工作站进行分析。
- en: Assuming we can authenticate with our Kubernetes cluster, it’s fairly easy to
    retrieve logging separately for individual microservices using Kubectl or the
    Kubernetes dashboard. Revisit section 6.12 to remind yourself how to authenticate
    and start using these tools.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们能够验证我们的 Kubernetes 集群，使用 Kubectl 或 Kubernetes 仪表板分别检索单个微服务的日志相对容易。回顾第 6.12
    节以提醒自己如何进行验证并开始使用这些工具。
- en: Kubectl
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Kubectl
- en: We first met Kubectl in chapter 6, but we’ll use it again now to get logs from
    a particular container running on Kubernetes. Let’s say we are running FlixTube
    as it was at the end of chapter 9 (you can do this and follow along if you like).
    Imagine that we’d like to get logging from an instance of our metadata microservice.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第 6 章中首次遇到 Kubectl，但现在我们将再次使用它来从 Kubernetes 上运行的特定容器中获取日志。假设我们正在运行第 9 章末尾的
    FlixTube（如果您愿意，您可以这样做并跟随操作）。想象一下，我们想要从我们的元数据微服务的一个实例中获取日志。
- en: Given that we could have multiple instances of the metadata microservice (we
    don’t yet, but we’ll talk about creating replicas later in this chapter), we need
    to determine the unique name that Kubernetes has assigned to the particular microservice
    that we are interested in.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可能有多个元数据微服务的实例（我们目前还没有，但本章后面我们将讨论创建副本），我们需要确定 Kubernetes 分配给我们所感兴趣的特定微服务的唯一名称。
- en: 'What we are actually looking for here is the name of the *pod*. You might remember
    from chapter 6 that a Kubernetes pod is the thing that contains our containers.
    A pod can actually run multiple containers, even though for FlixTube, as yet,
    we are only running a single container in each pod. After authenticating Kubectl
    as described in section 6.12.1, now use the `get` `pods` command to see the full
    list of pods in our cluster as shown here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上在这里寻找的是 *pod* 的名称。您可能还记得第 6 章中提到的 Kubernetes pod 是包含我们的容器的东西。一个 pod 实际上可以运行多个容器，尽管对于
    FlixTube 来说，我们目前每个 pod 只运行一个容器。在按照第 6.12.1 节所述验证 Kubectl 之后，现在使用 `get pods` 命令查看集群中所有
    pod 的完整列表，如下所示：
- en: '![](../Images/CH10_UN02_Davis4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN02_Davis4.png)'
- en: Scan down the list to pick out the name of the pod for our metadata microservice
    and find its unique name. In this case, the name is `metadata-55bb6bdf58-7pjn2`.
    Now we can use the `logs` command to retrieve the logging for the metadata microservice.
    In this instance, there isn’t much to see, but it’s helpful that we know how to
    do this.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表中向下扫描以找到元数据微服务的 pod 名称，并找到其唯一名称。在这种情况下，名称是 `metadata-55bb6bdf58-7pjn2`。现在我们可以使用
    `logs` 命令来检索元数据微服务的日志。在这种情况下，没有太多可看的内容，但知道如何做是有帮助的。
- en: '![](../Images/CH10_UN03_Davis4.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_UN03_Davis4.png)'
- en: 'Just remember to replace the name of the pod with the name of an actual microservice
    from your cluster. The unique name is generated by Kubernetes, so the name for
    your metadata microservice won’t be the same as the name that is generated for
    my version of it. Here’s the general template for the command:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 只需记得将 pod 的名称替换为集群中实际微服务的名称。唯一名称是由 Kubernetes 生成的，因此您的元数据微服务的名称不会与我的版本生成的名称相同。以下是命令的一般模板：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Just insert the particular name of the pod from which you’d like to retrieve
    the logs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 只需插入您想要检索日志的特定 pod 名称。
- en: Kubernetes Dashboard
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Dashboard
- en: The other way to view logging for individual containers in your cluster is to
    use the Kubernetes dashboard. This is a visual way to inspect and explore your
    cluster, and you can even make modifications to it (although, I don’t recommend
    manually tweaking a production cluster!).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的集群中查看单个容器的日志的另一种方法是使用 Kubernetes 仪表板。这是一种视觉方式来检查和探索您的集群，您甚至可以对其进行修改（尽管，我不建议手动调整生产集群！）。
- en: We first met the Kubernetes dashboard in chapter 6\. If you’ve not already done
    so, you can follow the instructions in section 6.12 to install, authenticate,
    and connect to your own dashboard. Once connected, you can quickly drill down
    to any pod to see its log. Figures 10.5, 10.6, and 10.7 show this process. Note
    in figures 10.5 and 10.6 that there’s other useful information that can help us
    understand the state of our microservices.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第六章首次遇到Kubernetes仪表板。如果你还没有这样做，你可以按照第6.12节的说明来安装、验证和连接到你的仪表板。一旦连接，你可以快速钻取到任何pod以查看其日志。图10.5、10.6和10.7显示了此过程。注意在图10.5和10.6中，还有其他有用的信息可以帮助我们了解微服务的状态。
- en: '![](../Images/CH10_F05_Davis4.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5](../Images/CH10_F05_Davis4.png)'
- en: Figure 10.5 The Kubernetes dashboard showing all the pods in our cluster.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 显示我们集群中所有pod的Kubernetes仪表板。
- en: '![](../Images/CH10_F06_Davis4.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6](../Images/CH10_F06_Davis4.png)'
- en: Figure 10.6 Viewing the details of the pod that contains our metadata microservice
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 查看包含我们的元数据微服务的pod的详细信息
- en: '![](../Images/CH10_F07_Davis4.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图10.7](../Images/CH10_F07_Davis4.png)'
- en: Figure 10.7 Viewing the log for the metadata microservice
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 查看元数据微服务的日志
- en: 10.2.5 Roll your own log aggregation for Kubernetes
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.5 为Kubernetes手动实现日志聚合
- en: We can go a long way to finding problems by chasing up logging for each individual
    microservice as shown in the previous sections. I recommend you do this for as
    long as feasible because implementing aggregation of logging for Kubernetes is
    a difficult task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过追踪如前几节所示每个单独微服务的日志，我们可以走很长的路来找到问题。我建议你在可行的情况下尽可能这样做，因为为Kubernetes实现日志聚合是一个困难的任务。
- en: Eventually, as your application grows, you are going to get tired of chasing
    down logging separately for each microservice. It’s unfortunate that Kubernetes
    has no built-in way to aggregate logging from containers in the cluster. I do
    hope the Kubernetes developers provide a simple solution to this in the future;
    it would be nice if there was a simple way to enable a single stream of logging
    from the cluster that we can use to monitor the behavior of the whole application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，随着你的应用程序的增长，你可能会厌倦为每个微服务分别追踪日志。不幸的是，Kubernetes没有内置从集群容器中聚合日志的方法。我真心希望Kubernetes的开发者在未来能提供一个简单的解决方案；如果有一个简单的方法可以启用来自集群的单个日志流，我们可以用它来监控整个应用程序的行为，那将是非常好的。
- en: There are enterprise solutions to this, however, and we’ll look at one of those
    in the next section. The enterprise solutions can be heavyweight and expensive,
    and these don’t necessarily make things any easier. These can also be quite difficult
    and time-consuming to set up and configure. If you are looking for a lighter-weight
    solution, you can build your own Kubernetes aggregation system as illustrated
    in figure 10.8.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有企业解决方案，我们将在下一节中探讨其中之一。企业解决方案可能很重且成本高昂，而且这些解决方案并不一定使事情变得更容易。这些解决方案的设置和配置也可能相当困难且耗时。如果你在寻找一个更轻量级的解决方案，你可以根据图10.8所示自行构建Kubernetes聚合系统。
- en: '![](../Images/CH10_F08_Davis4.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8](../Images/CH10_F08_Davis4.png)'
- en: Figure 10.8 Rolling your log aggregation for Kubernetes
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 Kubernetes日志聚合的滚动
- en: The aggregation service shown in figure 10.8 is a lightweight microservice that
    runs within each Kubernetes node. The difficulty in implementing this is that
    you must deploy it as a *DaemonSet*. This is a type of Kubernetes deployment that
    runs a container on each and every node in the cluster. Why do we need this? It’s
    because we need access to the filesystem of each node, where the log files are
    stored. Kubernetes automatically records standard output and standard error for
    each container in log files, but those files are available only within the node.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8中显示的聚合服务是一个轻量级的微服务，它运行在每个Kubernetes节点上。实现这个服务的难点在于你必须将其部署为**DaemonSet**。这是一种Kubernetes部署类型，它在集群中的每个节点上运行一个容器。为什么我们需要这样做呢？这是因为我们需要访问每个节点的文件系统，其中存储着日志文件。Kubernetes会自动记录每个容器的标准输出和标准错误到日志文件中，但这些文件仅限于节点内部访问。
- en: The aggregation service forwards all logging from containers running on the
    node to an external *collection service*. The collection service is another lightweight
    microservice. Its only job is to receive incoming logs through HTTP requests and
    to store those in its database. The database of logs is then displayed to our
    developers and operations staff through a web-based dashboard.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合服务将节点上运行的容器中的所有日志转发到外部的 *收集服务*。收集服务是一个轻量级的微服务。它的唯一任务是接收通过 HTTP 请求传入的日志，并将这些日志存储在其数据库中。然后，通过基于
    Web 的仪表板将日志数据库显示给我们的开发人员和运维人员。
- en: Note that the collection service itself is outside the cluster. We could have
    put it inside the cluster, but then problems with the cluster (the exact thing
    we are trying to detect) can hinder our ability to collect logging. It can be
    difficult to debug problems in your cluster when your log collector is hosted
    within the cluster that is having the problems.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，收集服务本身位于集群之外。我们本可以将它放在集群内部，但那样的话，集群的问题（正是我们试图检测的问题）可能会妨碍我们收集日志的能力。当你的日志收集器托管在出现问题的集群内部时，调试集群中的问题可能会很困难。
- en: 'This kind of hand-rolled logging system actually works pretty well in the early
    days of your application. It’s a good learning experience to implement this, but
    only if you want to drill down deeper into the inner workings of Kubernetes. To
    learn more and to try building this for yourself, read my blog post on Kubernetes
    log aggregation:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这种手工打造的日志系统在应用程序的早期阶段实际上工作得相当不错。实现这个系统是一个很好的学习经历，但只有当你想深入了解 Kubernetes 的内部工作原理时才这样做。想了解更多信息并尝试自己构建这个系统，请阅读我关于
    Kubernetes 日志聚合的博客文章：
- en: '[http://www.the-data-wrangler.com/kubernetes-log-aggregation/](http://www.the-data-wrangler.com/kubernetes-log-aggregation/)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.the-data-wrangler.com/kubernetes-log-aggregation/](http://www.the-data-wrangler.com/kubernetes-log-aggregation/)'
- en: 10.2.6 Enterprise logging, monitoring and alerts
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.6 企业级日志、监控和警报
- en: A common solution for large scale enterprise monitoring of microservices is
    the combination of Fluentd, Elasticsearch, and Kibana. Other options specifically
    for monitoring metrics are Prometheus and Grafana. These are professional enterprise-scalable
    solutions for monitoring and alerting. But these can be heavyweight and resource-intensive,
    so don’t rush into implementing these for your application.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大规模企业级微服务监控的一个常见解决方案是 Fluentd、Elasticsearch 和 Kibana 的组合。其他专门用于监控指标的选择是 Prometheus
    和 Grafana。这些都是用于监控和警报的专业企业级可扩展解决方案。但它们可能很重，资源密集，所以不要急于将这些解决方案应用到你的应用程序中。
- en: We won’t dive into any details on these technologies here because it would be
    beyond the scope of this book. It’s enough for now to have a brief overview of
    each of these technologies.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不会深入探讨这些技术细节，因为这超出了本书的范围。现在，对每种技术有一个简要的了解就足够了。
- en: Fluentd
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd
- en: Fluentd is an open-source logging and data collection service written in Ruby.
    You can instantiate a Fluentd container within your cluster to forward your logs
    to external log collectors.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 是一个用 Ruby 编写的开源日志和数据收集服务。你可以在你的集群中实例化一个 Fluentd 容器，以便将你的日志转发到外部的日志收集器。
- en: 'Fluentd is flexible and can be extended by its many plugins. One such plugin
    is what allows us to forward our logging to Elasticsearch. Learn more about Fluentd
    by visiting the following web sites:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 是灵活的，可以通过其许多插件进行扩展。其中一个插件允许我们将日志转发到 Elasticsearch。想了解更多关于 Fluentd 的信息，请访问以下网站：
- en: '[https://www.fluentd.org/](https://www.fluentd.org/)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.fluentd.org/](https://www.fluentd.org/)'
- en: '[https://docs.fluentd.org/](https://docs.fluentd.org/)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.fluentd.org/](https://docs.fluentd.org/)'
- en: Elasticsearch
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: 'Elasticsearch is an open-source search engine written in Java. Elasticsearch
    is what you can use to store and retrieve logging, metrics, and other useful data.
    Learn more about Elastic search at their website:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是一个用 Java 编写的开源搜索引擎。你可以使用 Elasticsearch 来存储和检索日志、指标和其他有用的数据。更多关于
    Elasticsearch 的信息，请访问他们的网站：
- en: '[https://www.elastic.co/elasticsearch/](https://www.elastic.co/elasticsearch/)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.elastic.co/elasticsearch/](https://www.elastic.co/elasticsearch/)'
- en: Kibana
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is the most interesting option of all this. It’s an open-source visualization
    dashboard built on top of Elasticsearch.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 是所有这些选项中最有趣的一个。它是一个基于 Elasticsearch 的开源可视化仪表板。
- en: Kibana allows us to view, search, and visualize our logs and other metrics.
    You can create fantastic custom dashboards with Kibana. Figure 10.9 shows an example
    of a dashboard with metrics from a Kubernetes cluster.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 允许我们查看、搜索和可视化我们的日志和其他指标。您可以使用 Kibana 创建出色的自定义仪表板。图 10.9 展示了一个包含 Kubernetes
    集群指标的仪表板示例。
- en: '![](../Images/CH10_F09_Davis4.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F09_Davis4.png)'
- en: Figure 10.9 Screenshot of a demo Kibana dashboard with metrics from a Kubernetes
    cluster
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 演示 Kibana 仪表板截图，包含 Kubernetes 集群的指标
- en: The great thing about Kibana, and it can be a real lifesaver, is that you can
    configure it to automatically alert you when there are problems in your cluster.
    You specify the conditions under which the alert is raised and the action that
    is taken.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 的好处之一，它可以真正救命，是您可以配置它以在集群中出现问题时代自动提醒您。您指定触发警报的条件以及采取的操作。
- en: 'The paid version of Kibana also has support for email notifications and some
    other options including triggering of webhooks to invoke whatever custom response
    you need. Learn more about Kibana from the following websites:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana 的付费版本还支持电子邮件通知和一些其他选项，包括触发 webhook 以调用所需的任何自定义响应。有关 Kibana 的更多信息，请从以下网站了解：
- en: https://www.elastic.co/what-is/kibana
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: https://www.elastic.co/what-is/kibana
- en: '[https://www.elastic.co/kibana](https://www.elastic.co/kibana)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.elastic.co/kibana](https://www.elastic.co/kibana)'
- en: 'You can find Kibana demo dashboards here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下位置找到 Kibana 演示仪表板：
- en: '[https://www.elastic.co/demos](https://www.elastic.co/demos)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.elastic.co/demos](https://www.elastic.co/demos)'
- en: 'You can browse the supported notifications here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此浏览支持的提醒：
- en: '[https://www.elastic.co/guide/en/kibana/master/action-types.html](https://www.elastic.co/guide/en/kibana/master/action-types.html)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.elastic.co/guide/en/kibana/master/action-types.html](https://www.elastic.co/guide/en/kibana/master/action-types.html)'
- en: Prometheus
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus
- en: Prometheus is an open-source monitoring system and time series database. Alongside
    Kubernetes, Prometheus is a graduated project of the Cloud Native Computing Foundation
    (CNCF), which puts it with some very esteemed company.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 是一个开源的监控系统和时序数据库。与 Kubernetes 一起，Prometheus 是云原生计算基金会（CNCF）的一个毕业项目，这使得它与一些非常受尊敬的公司并列。
- en: 'We can configure Prometheus to scrape metrics from our microservices at regular
    intervals and automatically alert us when things are going wrong. Learn more about
    Prometheus here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以配置 Prometheus 定期从我们的微服务中抓取指标，并在出现问题时自动提醒我们。有关 Prometheus 的更多信息，请在此处了解：
- en: '[https://prometheus.io/](https://prometheus.io/)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://prometheus.io/](https://prometheus.io/)'
- en: Grafana
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana
- en: Whilst Prometheus is great for data collection, queries, and alerts, it’s not
    so good at visualization. We can create simple graphs with Prometheus, but it’s
    quite limited.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Prometheus 在数据收集、查询和警报方面很出色，但在可视化方面就不那么好了。我们可以用 Prometheus 创建简单的图表，但它相当有限。
- en: 'It’s fortunate then that Grafana, which allows us to create visual and interactive
    dashboards, is so easy to connect to Prometheus. Learn more about Grafana here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Grafana 允许我们创建视觉和交互式仪表板，并且连接到 Prometheus 非常容易。有关 Grafana 的更多信息，请在此处了解：
- en: '[https://grafana.com/](https://grafana.com/)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://grafana.com/](https://grafana.com/)'
- en: 10.2.7 Automatic restarts with Kubernetes health checks
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.7 使用 Kubernetes 健康检查自动重启
- en: Kubernetes has a great feature for automated health checks that allows us to
    automatically detect and restart unhealthy microservices. You may not need this
    particular feature because Kubernetes already defines an unhealthy microservice
    as one that has crashed or exited. By default, Kubernetes automatically restarts
    misbehaving containers.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 具有自动健康检查的出色功能，允许我们自动检测和重启不健康的微服务。您可能不需要这个特定功能，因为 Kubernetes 已经将不健康的微服务定义为崩溃或退出的微服务。默认情况下，Kubernetes
    会自动重启行为不当的容器。
- en: If we aren’t happy with the default, Kubernetes lets us create our own definition
    of “unhealthy” on a case-by-case basis. We can define a readiness probe and a
    liveness probe for each microservice for Kubernetes to query the health of the
    microservice. The *readiness probe* shows if the microservice has started and
    is ready to start accepting requests. The *liveness probe* then shows whether
    the microservice is still alive and is still accepting requests. Both are illustrated
    in figure 10.10.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对默认设置不满意，Kubernetes 允许我们根据具体情况创建自己的“不健康”定义。我们可以为每个微服务定义一个就绪探测和存活探测，以便 Kubernetes
    查询微服务的健康状况。*就绪探测*显示微服务是否已启动并准备好开始接受请求。*存活探测*随后显示微服务是否仍然存活并且仍在接受请求。两者都在图 10.10
    中展示。
- en: '![](../Images/CH10_F10_Davis4.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F10_Davis4.png)'
- en: Figure 10.10 Applying automated Kubernetes health checks to the metadata microservice
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 将自动 Kubernetes 健康检查应用于元数据微服务
- en: We can use these two Kubernetes features to elegantly solve the problem we discovered
    in chapter 5 when we first connected the history microservice to our RabbitMQ
    server (section 5.8.5). The problem was that the history microservice (or any
    other microservice that connects to an upstream dependency) must wait for its
    dependency (in this case, RabbitMQ) to boot up before it can connect and make
    use of it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以使用这两个 Kubernetes 功能优雅地解决我们在第 5 章首次将历史微服务连接到我们的 RabbitMQ 服务器时发现的问题（5.8.5
    节）。问题是历史微服务（或任何连接到上游依赖的微服务）必须在连接并使用它之前等待其依赖项（在这种情况下，是 RabbitMQ）启动。 '
- en: If the microservice tries to connect too early, it’s simply going to throw an
    exception that could abort the process. It would be better if we could simply
    make the history microservice wait quietly until RabbitMQ becomes available. That
    is why we used the `wait-port` npm module in chapter 5, but that was an awkward
    workaround. However, using Kubernetes, we now have the tools for an elegant fix.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果微服务尝试过早地连接，它将简单地抛出一个异常，这可能导致进程终止。如果我们能让历史微服务安静地等待直到 RabbitMQ 可用会更好。这就是为什么我们在第
    5 章中使用了 `wait-port` npm 模块，但这只是一个笨拙的解决方案。然而，使用 Kubernetes，我们现在有了优雅解决问题的工具。
- en: 'The problem as just described only really happens when a microservices appli-cation
    is first booted up. Once your production application is running and your RabbitMQ
    server is already started, you can easily and safely introduce new microservices
    that depend on RabbitMQ without them having to wait. But don’t start to think
    that it’s not an issue because there is another side to this problem:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述的问题仅在微服务应用首次启动时真正发生。一旦您的生产应用正在运行，并且 RabbitMQ 服务器已经启动，您就可以轻松且安全地引入依赖 RabbitMQ
    的新微服务，而无需它们等待。但不要认为这不是一个问题，因为这个问题还有另一面：
- en: What happens when RabbitMQ crashes and is then automatically restarted by Kubernetes?
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 RabbitMQ 崩溃并被 Kubernetes 自动重启时会发生什么？
- en: What happens if we’d like to take RabbitMQ down temporarily to upgrade or maintain
    it?
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们想暂时关闭 RabbitMQ 进行升级或维护，会发生什么情况？
- en: In both circumstances, RabbitMQ will go offline, and this breaks the connection
    for all the microservices that depend on it. The default for those microservices
    (unless we specifically handle it) is to throw an unhandled exception that most
    likely aborts the microservice. Now any microservices that depend on RabbitMQ
    will constantly crash and restart while RabbitMQ is down.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，RabbitMQ 都会离线，这会中断所有依赖它的微服务的连接。对于这些微服务来说，默认操作（除非我们特别处理）是抛出一个未处理的异常，这很可能会导致微服务终止。现在，任何依赖
    RabbitMQ 的微服务在 RabbitMQ 离线时都会不断崩溃和重启。
- en: This is also true of any system dependencies besides RabbitMQ. Generally speaking,
    we’d like to be able to take any service offline and have the downstream services
    wait quietly for that service to become available again. When the service comes
    back online, the downstream services can resume normal operation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这也适用于除了 RabbitMQ 之外的其他系统依赖。一般来说，我们希望能够将任何服务离线，并让下游服务安静地等待该服务再次可用。当服务上线时，下游服务可以恢复正常操作。
- en: We can now use the readiness and liveness probes to solve these problems. The
    following listing shows an update to the Terraform code from chapter 9 that defines
    readiness and liveness probes for our microservices.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用就绪和存活探针来解决这些问题。以下列表显示了第 9 章中 Terraform 代码的更新，该代码定义了微服务的就绪和存活探针。
- en: Listing 10.1 Implementing Kubernetes readiness and liveness probes for microservices
    (an update to chapter-9/example-1/scripts/modules/microservice/main.tf)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.1 为微服务实现 Kubernetes 就绪和存活探针（第 9 章的更新 - chapter-9/example-1/scripts/modules/microservice/main.tf）
- en: '[PRE5]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Defines a readiness probe for the microservice
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为微服务定义就绪探针
- en: ② Kubernetes makes a HTTP request to the /ready route to determine if the microservice
    is ready to accept requests.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ② Kubernetes 向 /ready 路由发送 HTTP 请求以确定微服务是否准备好接受请求。
- en: ③ Defines a liveness probe for the microservice
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 为微服务定义存活探针
- en: ④ Kubernetes makes a HTTP request to the /alive route to determine if the microservice
    is still accepting requests.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ④ Kubernetes 向 /alive 路由发送 HTTP 请求以确定微服务是否仍在接受请求。
- en: If you’d like to try out the code in listing 10.1 for yourself, you can type
    the updates into the code in the file chapter-9/example-1/scripts/modules/microservice/main.tf.
    You’ll then need to run `terraform apply` to apply the changes to the existing
    version of FlixTube that you deployed in chapter 9\. If you didn’t do that or
    if you have since taken down your production version of FlixTube, running `terraform
    apply` will deploy a fresh instance of FlixTube.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想亲自尝试列表10.1中的代码，你可以将更新输入到文件chapter-9/example-1/scripts/modules/microservice/main.tf中的代码。然后你需要运行`terraform
    apply`来将更改应用到你在第9章中部署的FlixTube的现有版本。如果你没有这样做，或者如果你已经关闭了你的FlixTube生产版本，运行`terraform
    apply`将部署FlixTube的新实例。
- en: To make this change, we also have to add HTTP GET route handlers for /ready
    and /alive to all of our microservices. But what should these routes do?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这个更改，我们还需要为所有我们的微服务添加针对/ready和/alive的HTTP GET路由处理程序。但这些路由应该做什么呢？
- en: In the simplest cases, we just have to return a HTTP status code of 200 to indicate
    success. That’s enough to pass both probes, and it lets Kubernetes know that a
    micro-service is both *ready* and *live*. In certain situations (for example,
    with the history microservice), we can then add additional code to customize the
    definition of what it means to be ready and live. In any microservice that depends
    on RabbitMQ, we would add code for
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，我们只需返回HTTP状态码200以表示成功。这足以通过这两个探测，并让Kubernetes知道微服务既*就绪*又*活跃*。在某些情况下（例如，对于历史微服务），我们还可以添加额外的代码来自定义就绪和活跃的定义。在依赖于RabbitMQ的任何微服务中，我们都会添加代码来
- en: '*A /ready route that returns status 200 only once RabbitMQ becomes available.*
    This tells Kubernetes that the microservice has entered its ready state.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当RabbitMQ可用时，仅返回状态码200的/ready路由。* 这告诉Kubernetes微服务已进入其就绪状态。'
- en: '*An /alive route that returns an error code when RabbitMQ becomes unavailable.*
    This causes the microservice to be restarted, but the new microservice (due to
    the /ready route) won’t be placed in a ready state until RabbitMQ comes back online.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*当RabbitMQ不可用时，返回错误代码的/alive路由。* 这会导致微服务重启，但由于/ready路由，新的微服务（直到RabbitMQ重新上线）不会处于就绪状态。'
- en: A strategy like this solves two problems. First, if we didn’t use readiness
    and liveness probes, our history microservice will constantly start up, crash,
    and restart while RabbitMQ is down. This constant restarting isn’t an efficient
    use of our resources, and it’s going to generate a ton of error logging that we’d
    have to analyze (in case there’s a real problem buried in there!).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略解决了两个问题。首先，如果我们没有使用就绪和存活探测，我们的历史微服务将在RabbitMQ故障时不断启动、崩溃和重启。这种不断的重启并不是我们资源的高效利用，而且它将生成大量的错误日志，我们需要分析这些日志（以防其中隐藏着真正的问题！）。
- en: 'Second, we could handle this explicitly in the microservice by detecting when
    RabbitMQ disconnects and then polling constantly to see if we can reconnect. This
    would save the microservice from constantly crashing and restarting, but it requires
    significantly more sophisticated code in our microservice to handle the disconnection
    and reconnection to RabbitMQ. We don’t need to write such sophisticated code because
    that‘s what the probes are doing for us. To learn more about pod lifecycle and
    the different kinds of probes, see the Kubernetes documentation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们可以在微服务中显式处理这个问题，通过检测RabbitMQ何时断开连接，然后不断轮询以查看我们是否可以重新连接。这将使微服务免于不断崩溃和重启，但这需要在我们的微服务中编写更复杂的代码来处理与RabbitMQ的断开和重新连接。我们不需要编写这样的复杂代码，因为这就是探测为我们所做的事情。要了解更多关于Pod生命周期和不同类型的探测的信息，请参阅Kubernetes文档：
- en: '[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/)'
- en: 10.2.8 Tracing across microservices
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.8 微服务间的跟踪
- en: I have one last thing to tell you about logging and microservices. It’s extremely
    useful to be able to correlate strings of requests through your cluster. We do
    this by generating a unique correlation ID (CID) that we can attach to our requests
    to relate them to each other.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有一件关于日志和微服务的事情要告诉你。能够通过你的集群关联请求字符串是非常有用的。我们通过生成一个唯一的关联ID（CID）来实现这一点，我们可以将其附加到我们的请求上，以便将它们相互关联。
- en: You can see how this works in figure 10.11\. When a HTTP request first arrives
    in our gateway microservice, a unique CID is generated and attached to the request.
    As the request is forwarded through the system (either by HTTP request or RabbitMQ
    message), the ID remains attached, and we can use that to trace the path of related
    requests through our application.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图 10.11 中看到这是如何工作的。当一个 HTTP 请求首先到达我们的网关微服务时，会生成一个唯一的 CID 并附加到请求上。随着请求通过系统（无论是通过
    HTTP 请求还是 RabbitMQ 消息）转发，ID 保持附加状态，我们可以使用它来追踪相关请求通过我们应用程序的路径。
- en: The CID relates all logging, errors, metrics, and other information for the
    complete chain of requests. This is useful information to have when monitoring
    or exploring the behavior of our application. If you don’t have this, it really
    isn’t obvious where and how a request has penetrated deep into our application.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: CID 关联了请求完整链的所有日志、错误、指标和其他信息。当监控或探索我们应用程序的行为时，这些信息非常有用。如果您没有这些信息，那么请求如何以及在哪里深入到我们的应用程序中就不是很明显了。
- en: '![](../Images/CH10_F11_Davis4.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F11_Davis4.png)'
- en: Figure 10.11 Using a correlation ID (CID) to correlate strings of requests through
    your cluster
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 使用关联 ID (CID) 通过您的集群关联请求字符串
- en: 'We can create unique IDs using the uuid library on npm. With that installed,
    we can create unique IDs like this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 npm 上的 uuid 库创建唯一的 ID。安装后，我们可以创建这样的唯一 ID：
- en: '[PRE6]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can then attach the unique ID to the headers of forwarded HTTP requests
    (easy to do with either `http.request` or Axios), or we can add the ID to the
    payload of RabbitMQ messages. To get serious about tracing your requests, you’ll
    need to get Zipkin. This is a tool that allows you to visually trace requests
    across your application. You can learn more about Zipkin online:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将唯一的 ID 附加到转发的 HTTP 请求的头部（使用 `http.request` 或 Axios 都很容易做到），或者我们可以将 ID
    添加到 RabbitMQ 消息的有效负载中。要严肃地追踪您的请求，您将需要获取 Zipkin。这是一个允许您在应用程序中可视追踪请求的工具。您可以在网上了解更多关于
    Zipkin 的信息：
- en: '[https://zipkin.io/](https://zipkin.io/)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://zipkin.io/](https://zipkin.io/)'
- en: 'The code for Zipkin can be found on GitHub:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Zipkin 的代码可以在 GitHub 上找到：
- en: '[https://github.com/openzipkin/zipkin](https://github.com/openzipkin/zipkin)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/openzipkin/zipkin](https://github.com/openzipkin/zipkin)'
- en: 10.3 Debugging microservices
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 调试微服务
- en: With some form of monitoring in place, we can see logging and metrics for our
    application. We use this to understand its current state and historical behavior.
    It is useful to have this kind of information in hand when problems occur.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置某种形式的监控后，我们可以看到我们应用程序的日志和指标。我们使用这些信息来了解其当前状态和历史行为。当出现问题时，手头上有这类信息非常有用。
- en: Once a problem has become apparent, we must now put on our detective hats. We
    need to analyze the information we have to understand *what* went wrong. We then
    track the clues back to the root cause to find out *why* it happened. Along the
    way, we’ll run experiments to further hone in on the culprit.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦问题变得明显，我们现在必须戴上侦探帽。我们需要分析我们所拥有的信息，以了解*什么*出了问题。然后，我们将线索追踪回根本原因，以找出*为什么*它发生了。在这个过程中，我们将进行实验，以进一步缩小问题根源。
- en: Usually, we can’t fix a problem until we have identified the cause. Of course,
    sometimes we can randomly stumble on a solution without knowing the cause. But
    it’s always sensible to be able to ascertain the root cause anyway. That way,
    we can be sure that the supposed *fix* has actually fixed the problem and not
    just hidden it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们无法修复问题，直到我们确定了原因。当然，有时我们可以在不知道原因的情况下随机找到解决方案。但无论如何，能够确定根本原因总是明智的。这样，我们可以确保所谓的*修复*实际上解决了问题，而不仅仅是掩盖了它。
- en: Debugging is the name of this process of tracking down the source of a problem
    and subsequently applying an appropriate fix. Debugging microservices is similar
    to debugging any other kind of application; it’s a form of troubleshooting that
    is part art and part science.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 调试是追踪问题根源并随后应用适当修复的过程的名称。调试微服务类似于调试任何其他类型的应用程序；它是一种既包含艺术又包含科学的问题解决形式。
- en: Debugging microservices, though, is more difficult due to the distributed nature
    of the application. Locating a problem in a single process can be difficult on
    its own, but finding a problem in an application composed of many interacting
    processes-that’s a whole lot more trouble.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于应用程序的分布式特性，调试微服务更为困难。在单个进程中定位问题本身就很难，但在由许多相互作用的进程组成的应用程序中找到问题则更加麻烦。
- en: As you might already suspect, searching for the source of a problem is actually
    the most difficult part of debugging. It’s like searching for the proverbial needle
    in the haystack. If you have any inkling of where to look for the problem, you
    stand a much greater chance of finding it quickly. That’s why developers who are
    experienced with a particular codebase can find bugs in it much more quickly than
    those who are less familiar.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经怀疑的那样，寻找问题的根源实际上是调试中最困难的部分。这就像在干草堆里寻找传说中的针。如果你有任何关于在哪里寻找问题的线索，你就有更大的机会快速找到它。这就是为什么熟悉特定代码库的开发者比不太熟悉的人更快地找到其中的错误。
- en: After finding the source of the problem, we must now fix it. Fortunately, it
    is often (but not always) much quicker to fix a bug than it was to find it in
    the first place.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在找到问题的根源之后，我们现在必须修复它。幸运的是，修复一个错误通常（但不总是）比最初找到它要快得多。
- en: 10.3.1 The debugging process
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 调试过程
- en: 'In an ideal world, we’d find and fix all problems during development and testing.
    Indeed, if you have a thorough testing practice and/or comprehensive automated
    test suite, you will find many of your bugs before production. If possible, that’s
    the best way because debugging is much easier in development (on your development
    workstation) instead of in production (where it’s likely distributed over multiple
    servers in a data center). To debug any code, we can follow this process:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的世界里，我们会在开发和测试期间找到并修复所有问题。事实上，如果你有彻底的测试实践和/或全面的自动化测试套件，你将在生产之前发现许多错误。如果可能的话，这是最好的方法，因为与在生产环境中（可能分布在数据中心的多台服务器上）相比，在开发环境中（在你的开发工作站上）调试要容易得多。要调试任何代码，我们可以遵循以下过程：
- en: Gather evidence
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集证据
- en: Mitigate customer impact
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减轻客户影响
- en: Isolate the problem
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隔离问题
- en: Reproduce the problem
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新复现问题
- en: Fix the problem
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决问题
- en: Reflection
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反思
- en: As with anything that’s part art and part science, this isn’t actually a strictly
    defined process. Sometimes, we must trace an iterative path through these steps
    in an unpredictable way. For the purposes of explanation though, let’s pretend
    that we can solve our problem by going through these steps in a straightforward
    linear manner.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正如任何既是艺术又是科学的东西一样，这实际上不是一个严格定义的过程。有时，我们必须以不可预测的方式在这些步骤中追踪一个迭代的路径。不过，为了解释的目的，让我们假设我们可以通过以直接线性方式通过这些步骤来解决我们的问题。
- en: Gather evidence
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 收集证据
- en: The start of the debugging process is always gathering as much evidence about
    the problem as possible. This is anything that can help direct us more quickly
    to the real location of the bug. If we start debugging close to where the problem
    actually is, we can narrow in on it much more quickly. We need to learn as much
    as we can about the problem as quickly as possible. That’s things like
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 调试过程的开始总是尽可能地收集有关问题的证据。这包括任何可以帮助我们更快地找到错误真正位置的东西。如果我们从问题实际发生的地方开始调试，我们就可以更快地缩小范围。我们需要尽可能快地了解尽可能多的关于问题的信息。这就像
- en: Logging and error reports
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志和错误报告
- en: Traces for relevant request paths through the system (as described in section
    10.2.8)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统中相关请求路径的跟踪（如第 10.2.8 节所述）
- en: Bug reports from users
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户提交的错误报告
- en: Information from the Kubernetes CLI tool or dashboard
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 Kubernetes CLI 工具或仪表板的信
- en: Call stacks for any crash that might have occurred
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何可能发生的崩溃的调用栈
- en: The implicated versions or branches of the code
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受影响的代码版本或分支
- en: Recently deployed code or microservices
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近部署的代码或微服务
- en: The reason we must compile this information immediately is that, often, the
    next thing we must do for the benefit of our customers is to make the problem
    just disappear as quickly as possible.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须立即编译这些信息的原因是，通常，为了我们客户的利益，我们接下来必须做的事情是尽可能快地让问题消失。
- en: Mitigate customer impact
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 减轻客户影响
- en: Before attempting to solve or find the cause of the problem, we must ensure
    that it is not adversely affecting our customers. If our customers are negatively
    affected, then we must take immediate action to rectify the situation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试解决问题或找到问题的原因之前，我们必须确保它不会对我们的客户产生不利影响。如果我们的客户受到负面影响，那么我们必须立即采取行动纠正这种情况。
- en: 'At this point, we don’t care what caused the problem or what the real long-term
    fix for it might be. We simply need the fastest possible way to restore the functionality
    that our customers depend upon. They’ll appreciate our immediate action to find
    a workaround that allows them to continue using our application. There are multiple
    ways we can do this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们不在乎问题是由什么引起的，或者它的真正长期解决方案可能是什么。我们只需要以最快的方式恢复客户所依赖的功能。他们会感激我们立即采取行动找到一种解决方案，让他们能够继续使用我们的应用程序。我们可以通过多种方式做到这一点：
- en: If the problem comes from a recent code update, revert that update and redeploy
    the code to production. This is often easier with microservices because if we
    know the microservice that was updated caused the problem, we can easily revert
    that single microservice and restore it to the previously working version, say,
    an earlier image in the container registry.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题来自最近的代码更新，则撤销该更新并将代码重新部署到生产环境中。由于我们知道更新的微服务导致了问题，因此使用微服务通常更容易这样做，我们可以轻松地撤销单个微服务并将其恢复到之前工作的版本，比如容器注册库中的一个更早的镜像。
- en: If the problem comes from a new or updated feature that isn’t urgently needed
    by the customer, we can disable that single feature to restore the application
    to a working state.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题来自客户不需要的新的或更新的功能，我们可以禁用该单个功能以恢复应用程序的工作状态。
- en: If the problem comes from a microservice that isn’t urgently needed, we can
    temporarily take that microservice out of commission.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题来自一个不是急需的微服务，我们可以暂时将该微服务停用。
- en: I can’t overstate the importance of this step! It could take hours or days (maybe
    even weeks at worst) to solve a problem. We can’t know ahead of time how long
    it will take, and we can’t expect our customers to standby and wait for that to
    happen. It’s more likely that they’ll head over to one of our competitors instead.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我无法过分强调这一步骤的重要性！解决问题可能需要几个小时或几天（在最坏的情况下甚至可能需要几周）。我们无法提前知道需要多长时间，也不能期望我们的客户等待。更有可能的是，他们会转向我们的竞争对手之一。
- en: What’s worse is that solving a problem under pressure (because our customers
    are waiting on us) is extremely stressful and results in poor decision making.
    Any fix we apply under stress is likely to add more bugs, which only compounds
    the problem.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，在压力下解决问题（因为我们的客户在等待我们）会非常紧张，并导致决策不佳。我们在压力下实施的任何修复都可能引入更多的错误，这只会使问题更加复杂。
- en: For the sake of our customers and ourselves, we must temporarily ignore the
    problem and find the fastest way to restore our application to a working state
    (as depicted in figure 10.12). This takes away the pressure, allows our customers
    to continue without interruption, and buys us the time we need to solve this problem.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们客户和我们自己，我们必须暂时忽略这个问题，找到最快的方法将我们的应用程序恢复到工作状态（如图10.12所示）。这样做可以减轻压力，让我们的客户能够不间断地继续使用，同时也为我们解决问题争取了时间。
- en: '![](../Images/CH10_F12_Davis4.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图10.12](../Images/CH10_F12_Davis4.png)'
- en: Figure 10.12 Mitigating the risk to our customer after a problem is found by
    immediately rolling back to a previous working version
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12 在发现问题后，通过立即回滚到先前的有效版本来减轻对我们客户的危害
- en: Reproduce the problem
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 重新复制问题
- en: After making sure that the application is working again for our customers, we
    can now move on to locating the cause of the problem and solving it. To do this,
    we must be able to reproduce the problem. Unless we can definitely and consistently
    repeat the problem, we can never be certain that we’ve fixed it. What we are aiming
    to do is create a *test case* that demonstrates the bug. That is a documented
    sequence of steps that we can follow to reliably cause the bug to show itself.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保应用程序再次为我们的客户正常工作后，我们现在可以继续寻找问题的原因并解决问题。为此，我们必须能够重新复制问题。除非我们能够肯定且一致地重复问题，否则我们永远无法确定我们已经修复了它。我们的目标是创建一个*测试用例*来演示这个错误。这是一个我们可以遵循的文档化步骤序列，以可靠地引起错误显示出来。
- en: Ideally, we’d like to reproduce the bug on our development workstation. That
    makes it easier to run experiments to track down the bug. Some problems, though,
    are so complex that we can’t easily reproduce those in development, especially
    when your application has grown so big (e.g., it has many microservices) that
    it no longer fits in its entirety on a single computer.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望在开发工作站上重新复制这个错误。这使我们可以更容易地运行实验来追踪错误。然而，有些问题非常复杂，我们无法在开发环境中轻松地复制这些问题，特别是当你的应用程序变得非常大（例如，它有多个微服务）以至于不再适合整个地放在一台计算机上。
- en: In this situation, we must reproduce the problem in a *test environment*. This
    is a production-like environment that is purely for testing (it’s not customer-facing).
    Debugging in the test environment (which is similar to debugging in production)
    can still be difficult, though, and ultimately, we’d still like to reproduce the
    problem in development.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们必须在*测试环境*中重现问题。这是一个类似生产环境的环境，但纯粹用于测试（它不面向客户）。尽管在测试环境中（与生产环境中的调试类似）调试仍然可能很困难，但最终，我们仍然希望在开发环境中重现问题。
- en: In the test environment, we can run experiments to further understand which
    components of the application are involved in the problem, and then safely remove
    any that aren’t contributing to it. Through a process of elimination, we can cut
    back our application to a point where it is small enough to run in development.
    At this point, we can transfer from the test environment to our development workstation.
    We’ll talk more about creating test environments in chapter 11.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试环境中，我们可以进行实验，进一步了解哪些应用程序组件参与了问题，然后安全地移除那些没有贡献的组件。通过排除法，我们可以将应用程序缩减到足够小，以便在开发环境中运行。在这个时候，我们可以从测试环境转移到我们的开发工作站。我们将在第11章中更多地讨论创建测试环境。
- en: If we are doing automated testing, this is the point where we should write an
    automated test that checks that the bug is fixed. Of course, this test fails initially-that’s
    the point of it. We’ll use it later as a reliable way to know that the problem
    has been fixed.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在进行自动化测试，这就是我们应该编写一个自动化测试来检查bug是否被修复的时刻。当然，这个测试最初会失败——这正是它的目的。我们稍后会用它作为一个可靠的方式来知道问题已经被修复。
- en: Writing an automated test also ensures that we can repeatedly reproduce the
    issue. Each and every time we run this test, it should fail, confirming that we
    have indeed found a reliable way to reproduce the bug.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 编写自动化测试也确保了我们能够反复重现问题。每次运行这个测试时，它都应该失败，从而确认我们确实找到了一种可靠的重现bug的方法。
- en: Isolate the problem
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离问题
- en: Once we have reproduced the problem in development, we now start the process
    of isolating it. We repeatedly run experiments and chip away at the application
    until we have narrowed down the scope and pinpointed the exact source of the bug.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在开发环境中重现了问题，我们现在开始隔离问题的过程。我们反复进行实验，逐步缩小应用程序的范围，直到我们缩小了范围并确定了bug的确切来源。
- en: We are effectively cutting away the space in which the problem can hide, progressively
    reducing the problem domain until the cause becomes obvious. We are using a *divide
    and* *conquer* -style of process as illustrated in figure 10.13.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上是在逐步缩小问题可能隐藏的空间，直到问题的根源变得明显。我们正在使用一种类似于图10.13所示的*分而治之*的过程。
- en: 'By the way, microservices are great for this. Our application is already nicely
    factored into easily separable components. This makes it much easier to pull our
    application into pieces. In general, it’s pretty easy to just drop an individual
    microservice out of the application (just comment it out of your Docker Compose
    file!). As you drop each microservice, ask the question: can you still reproduce
    the problem?'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，微服务非常适合这个。我们的应用程序已经很好地分解成易于分离的组件。这使得将我们的应用程序拆分成各个部分变得容易得多。一般来说，只需从Docker
    Compose文件中注释掉单个微服务即可将其从应用程序中移除！随着你移除每个微服务，问自己一个问题：你还能重现问题吗？
- en: Yes. That’s great. You’ve just reduced the problem domain by one microservice.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的。太好了。你已经通过减少一个微服务来缩小了问题域。
- en: No. That’s great. You’ve possibly just implicated that microservice in the problem.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不。太好了。你可能刚刚将那个微服务牵涉到问题中。
- en: Either way, you are iterating your way towards the cause of the problem.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，你都是在通过迭代的方式接近问题的根源。
- en: '![](../Images/CH10_F13_Davis4.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F13_Davis4.png)'
- en: Figure 10.13 Cutting away the problem space until we isolate the exact source
    of the bug
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13逐步缩小问题空间，直到我们隔离出bug的确切来源
- en: Sometimes we will quickly identify the source of a problem. At other times,
    debugging can be a painfully slow, time-consuming, and frustrating process. It
    depends a lot on our general level of experience, our familiarity with the code
    base, whether we have seen this *kind* of problem before, and the complexity of
    the particular problem itself.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们很快就能确定问题的根源。在其他时候，调试可能是一个痛苦缓慢、耗时且令人沮丧的过程。这很大程度上取决于我们的总体经验水平、我们对代码库的熟悉程度、我们是否以前见过这种类型的问题，以及特定问题的复杂性。
- en: Note Debugging at its worst requires persistence, patience, and commitment.
    Don’t be afraid to reach out for help. There’s nothing worse than being stuck
    on a problem that you can’t solve.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：调试最糟糕的情况需要毅力、耐心和承诺。不要害怕寻求帮助。没有什么比陷入一个无法解决的问题更糟糕的了。
- en: If you know where to start looking for a problem, then you already have a massive
    head start. You might also be able to take an educated guess at what is causing
    it. If that works out, you are quite right to skip much of this process and immediately
    focus your attention on the cause of the bug. However, if you don’t know where
    to look or if your guess turns out to be wrong, you will have to be more scientific
    about debugging and apply this full process.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道从哪里开始寻找问题，那么你已经取得了巨大的优势。你也可能能够对导致问题的原因做出有根据的猜测。如果这奏效了，你完全有理由跳过这个过程的大部分内容，并立即将注意力集中在问题的原因上。然而，如果你不知道该往哪里看，或者你的猜测是错误的，你将不得不更加科学地进行调试，并应用这个完整的过程。
- en: Fix the problem
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 修复问题
- en: You’ve identified the root cause of the problem. Now you just have to fix it!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经确定了问题的根本原因。现在你只需要修复它！
- en: Fortunately, fixing problems is much easier than finding them in the first place.
    Usually, identifying the broken code is enough to make you imagine what the solution
    would be. Sometimes, it’s more difficult, and you’ll have to invest some creative
    thinking to come up with a solution. The hardest part is definitely over though.
    You have found the needle in the haystack, and now you can work out the best way
    to remove it.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，解决问题比最初找到问题要容易得多。通常，识别出有问题的代码就足以让你想象出解决方案。有时，这会更困难，你可能需要投入一些创造性思维来想出解决方案。但最困难的部分肯定已经过去了。你已经找到了针尖上的麦芒，现在你可以找到最好的方法来移除它。
- en: If you are doing automated testing, and you have already written the failing
    test that reproduces the issue, then you have a convenient and reliable yardstick
    to show you when the bug has been fixed. Even if the fix turns out to be difficult,
    at least you have a way to know for sure that the problem is fixed. That’s a useful
    thing to have as you iterate and experiment your way towards a fix.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在进行自动化测试，并且已经编写了重现问题的失败的测试用例，那么你有一个方便且可靠的标尺来告诉你何时修复了错误。即使修复变得困难，至少你有一种方法可以确信问题已经解决。当你迭代和实验以找到解决方案时，这是一件很有用的事情。
- en: Reflection
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 反思
- en: Every time we solve a problem, we should pause for a moment to reflect on what
    can be done to prevent the problem from happening again in the future, or what
    could have been done to find and fix the problem more quickly. Reflection is important
    for us as individuals and as teams to continuously refine and improve our development
    process.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们解决问题时，我们都应该暂停片刻，反思如何防止问题在未来再次发生，或者如何更快地发现和修复问题。反思对我们个人和团队来说都很重要，以不断改进我们的开发过程。
- en: We might have written an automated test that will prevent this specific problem
    again in the future. But still, we need something more than that. We should seek
    practices and habits to help us eliminate, not just this specific problem, but
    all of this class or kind of problem.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能已经编写了一个自动化测试，可以防止未来再次出现这个问题。但仍然，我们需要更多。我们应该寻求实践和习惯来帮助我们消除，而不仅仅是这个具体的问题，而是所有这类或这类的问题。
- en: The amount of time that we spend reflecting and then the amount of time we invest
    in upgrading our development process depends a lot on the problem itself and the
    severity of it. We should ask questions like
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花在反思上的时间和我们投入在升级我们的开发过程中的时间在很大程度上取决于问题本身及其严重性。我们应该提出像以下问题：
- en: Is this kind of problem likely to happen in the future such that we should proactively
    mitigate against it?
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种问题未来可能再次发生，以至于我们应该主动减轻其影响吗？
- en: Are the effects of this problem severe enough that we should proactively mitigate
    against it?
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个问题的后果严重到足以让我们主动减轻其影响吗？
- en: Answering these questions helps us understand how much effort to expend combatting
    this type of problem in the future.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题有助于我们了解在将来应对这类问题需要投入多少努力。
- en: 10.3.2 Debugging production microservices
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 调试生产级微服务
- en: Sometimes we can’t get away from it; we literally have to debug our microservices
    in production. If we can’t reproduce the issue in test or development, then our
    only option is to further understand the problem as it is occurring in production.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们无法避免这种情况；我们实际上必须在生产中调试我们的微服务。如果我们无法在测试或开发中重现问题，那么我们唯一的选择就是进一步了解生产中正在发生的问题。
- en: 'If we need to make a deeper inspection than logging can give us, we can use
    the Kubernetes CLI tool (Kubectl) to open a terminal in any container (at least
    any that has a shell installed). Once you know the name of the pod (refer back
    to section 10.2.4), such as the pod that contains the metadata microservice, we
    can open a shell to it like this:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要进行比日志记录所能提供的更深入的检查，我们可以使用Kubernetes CLI工具（kubectl）在任何容器（至少是安装了shell的任何容器）中打开一个终端。一旦您知道了Pod的名称（请参阅第10.2.4节），例如包含元数据微服务的Pod，我们就可以像这样打开一个shell到它：
- en: '[PRE7]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You may have noticed back in figure 10.6 that you can also open a terminal to
    a pod using the Kubernetes dashboard. Now we can invoke shell commands inside
    a production microservice.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，在图10.6中，您还可以使用Kubernetes仪表板打开一个Pod的终端。现在我们可以在生产微服务内部调用shell命令。
- en: As you might be able to sense, we are in extremely dangerous territory here.
    When you are inside a microservice like this, there is the potential for much
    damage, and any mistakes could easily make the problem much worse! Don’t shell
    into a production microservice on a whim, and if you do, don’t change anything.
    There are better and safer ways to diagnose problems!
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经感受到的，我们正处于极其危险的领域。当您身处这样的微服务内部时，潜在的损害很大，任何错误都可能导致问题变得更加严重！不要随意在生产微服务上执行shell命令，如果您确实这样做，请不要更改任何内容。有更好的、更安全的方法来诊断问题！
- en: This only matters if it affects our customers. If you are instead debugging
    microservices in your own private cluster or in a test environment, then you aren’t
    affecting any customers; so feel free to push, prod, and poke your microservices
    however you like-it’s a great learning experience to do this! Just don’t do it
    to a production microservice.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这只有在它影响我们的客户时才重要。如果您在自己的私有集群或测试环境中调试微服务，那么您不会影响任何客户；因此，您可以随意推动、刺激和探索您的微服务——这是很好的学习经验！只是不要在生产微服务上这样做。
- en: 10.4 Reliability and recovery
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 可靠性和恢复
- en: We can’t avoid problems, but there are many ways that we can deal with these
    in our application to maintain service in the face of failures. With our application
    in production, we have an expectation that it will perform with a certain level
    of reliability, and there are many tactics we can employ to architect robust and
    reliable systems. This section overviews a small selection of practices and techniques
    that can help us build fault-tolerant systems that can quickly bounce back from
    failure.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法避免问题，但我们可以以多种方式在我们的应用程序中处理这些问题，以在面临故障时保持服务。在我们的应用程序在生产中，我们期望它以一定的可靠性运行，并且我们可以采用许多策略来构建健壮和可靠的系统。本节概述了一些实践和技术，可以帮助我们构建容错系统，这些系统能够快速从故障中恢复。
- en: 10.4.1 Practice defensive programming
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 实践防御性编程
- en: 'A first step is to code with the mindset of *defensive programming*. When working
    this way, we have the expectation that errors will occur, even if we can’t anticipate
    what those might be. We should always expect the following:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是带着*防御性编程*的心态进行编码。以这种方式工作时，我们期望会发生错误，即使我们无法预知这些错误可能是什么。我们始终应该期待以下情况：
- en: We’ll get bad inputs to our code.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们代码可能会接收到不良的输入。
- en: Our code contains bugs that haven’t manifested yet.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们代码中包含尚未显现的bug。
- en: Things we depend on (e.g., RabbitMQ) are not 100% reliable and, occasionally,
    have their own problems.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们所依赖的东西（例如，RabbitMQ）并不总是100%可靠的，偶尔也会出现他们自己的问题。
- en: When we adopt the defensive mindset, we’ll automatically start looking for ways
    to make our code behave more gracefully in the presence of unexpected situations.
    Fault tolerance starts at the coding level. It starts within each microservice.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们采取防御性思维时，我们会自动开始寻找让我们的代码在意外情况下表现得更加优雅的方法。容错始于编码层面。它始于每个微服务内部。
- en: 10.4.2 Practice defensive testing
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 实践防御性测试
- en: As you are probably aware, testing plays a huge role in building resilient and
    reliable systems. We covered testing in chapter 8, so all I’d like to say here
    is simply that testing the “normal” code paths is not enough. We should also be
    testing that the software we create can handle errors. This is the next step up
    from defensive programming.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，测试在构建弹性可靠系统中起着巨大的作用。我们在第8章中介绍了测试，所以在这里我想说的只是，测试“正常”代码路径是不够的。我们还应该测试我们创建的软件可以处理错误。这是防御编程的下一步。
- en: We should be writing tests that actively attack our code. This helps us identify
    fragile code that could do with some more attention. We need to make sure our
    code can recover gracefully, reporting errors, and handling unusual situations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该编写测试来积极攻击我们的代码。这有助于我们识别需要更多关注的脆弱代码。我们需要确保我们的代码可以优雅地恢复，报告错误，并处理不寻常的情况。
- en: 10.4.3 Protect your data
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.3 保护你的数据
- en: All applications deal with user data, and we must take necessary steps to protect
    our data in the event of failures. When unexpected failures occur, we need to
    be confident that our most important data is not damaged or lost. Bugs happen;
    losing our data should not.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 所有应用程序都处理用户数据，我们必须采取必要的步骤来保护我们的数据，以防发生故障。当意外故障发生时，我们需要有信心，我们最重要的数据没有被损坏或丢失。错误会发生；我们数据的丢失不应该发生。
- en: Not all data is equal. Data that is generated within our system (and can hence
    be regenerated) is less important than data that is captured from our customer.
    Although all data is important, it’s the source data that we must invest the most
    in protecting.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有数据都同等重要。在我们系统中生成（因此可以重新生成）的数据不如从我们的客户那里捕获的数据重要。尽管所有数据都很重要，但我们必须最重视保护源数据。
- en: The first step to protecting data, obviously, is to have a backup. The backup
    should be automated. Most cloud vendors provide facilities for this that you can
    enable.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 保护数据的第一步显然是备份。备份应该是自动化的。大多数云服务提供商都提供了你可以启用的此类功能。
- en: Note Don’t forget to practice restoring your backup! Backups are completely
    useless if we are unable to restore those.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不要忘记练习恢复你的备份！如果无法恢复备份，备份将完全无用。
- en: 'At least now, should the worst happen, we can restore lost or damaged data
    from the backup. In the industry, we have a saying: our data *doesn’t* exist *unless*
    it exists in at least three places. Here are some other guidelines we can follow
    to protect our data:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 至少现在，如果最坏的情况发生，我们可以从备份中恢复丢失或损坏的数据。在业界，我们有一句话：我们的数据*不存在*，除非它至少存在于三个地方。以下是一些我们可以遵循的其他保护数据的指南：
- en: Safely record data as soon as it’s captured
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦数据被捕获，就安全地记录数据
- en: Never have code that overwrites source data
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 永远不要编写覆盖源数据的代码
- en: Never have code that deletes source data
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 永远不要编写删除源数据的代码
- en: The code that captures our data is some of the most important code in our application,
    and we should treat it with an appropriate level of respect. It should be extremely
    well tested. It should also be minimal and as simple as possible, because simple
    code leaves less space where bugs and security issues can hide.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获我们数据的代码是我们应用程序中最重要的代码之一，我们应该以适当的尊重来对待它。它应该经过极其严格的测试。它还应该是最小化和尽可能简单的，因为简单的代码留给错误和安全问题隐藏的空间更少。
- en: The reason we should never overwrite or delete our source data is that a bug
    in that code can easily damage or destroy the data. We know bugs happen right?
    We are in the defensive mindset, so we are expecting unforeseen problems to happen.
    To learn more about working with and protecting your data, see my book, *Data
    Wrangling with JavaScript* (Manning, 2018).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们永远不应该覆盖或删除源数据的原因是，该代码中的错误可以轻易地损坏或破坏数据。我们知道错误会发生，对吧？我们处于防御心态，因此我们预计会发生不可预见的问题。要了解更多关于使用和保护数据的信息，请参阅我的书籍《使用JavaScript进行数据整理》（Manning，2018年）。
- en: 10.4.4 Replication and redundancy
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.4 复制和冗余
- en: The best way to tackle the failure of a microservice is by having *redundancy*.
    We do that by having multiple (usually at least three) instances of each microservice
    sitting behind a load balancer, which you can see in figure 10.14\. The load balancer
    is a service that shares incoming requests across multiple microservices so that
    the “load” is distributed evenly among them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 解决微服务失败的最佳方式是通过**冗余**。我们通过在负载均衡器后面拥有多个（通常至少三个）每个微服务的实例来实现这一点，如图10.14所示。负载均衡器是一种服务，它将传入的请求分配到多个微服务中，以便“负载”在它们之间均匀分布。
- en: If any microservice happens to fail, the load balancer immediately redirects
    incoming requests to the other instances. In the meantime, the failed instance
    is restarted by Kubernetes. This redundancy means that we can maintain a continuous
    level of service even in the face of intermittent failures.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何微服务发生故障，负载均衡器会立即将传入请求重定向到其他实例。在此期间，Kubernetes 会重启失败的实例。这种冗余意味着即使在间歇性故障的情况下，我们也能保持连续的服务水平。
- en: '![](../Images/CH10_F14_Davis4.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH10_F14_Davis4.png)'
- en: Figure 10.14 A load balancer distributes incoming requests across multiple redundant
    instances of a microservice.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 负载均衡器将传入请求分配到多个冗余的微服务实例。
- en: Redundancy is implemented by *replication*. We also use replication for increased
    performance, but we’ll save that until chapter 11.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余是通过**复制**实现的。我们也会使用复制来提高性能，但我们会留到第 11 章再讨论。
- en: Just because our system can handle failures doesn’t mean we should tolerate
    these. All failures should be logged and later investigated. We can use the debugging
    process from section 10.3 to find and fix the cause of the failure.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的系统可以处理故障，但这并不意味着我们应该容忍这些故障。所有故障都应该被记录下来，并在之后进行调查。我们可以使用第 10.3 节中的调试过程来找到并修复故障的原因。
- en: Implementing replication in Kubernetes
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中实现复制
- en: Each of the microservices that we have deployed for FlixTube so far (in chapters
    7 and 9) only had a single instance. This is perfectly OK when creating an application
    for learning (like we did with FlixTube) or when you are in the starting phase
    of developing your own microservices application. It’s just not going to be as
    fault-tolerant as it could be.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们为 FlixTube 部署的所有微服务（在第 7 章和第 9 章中）都只有一个实例。当创建一个用于学习的应用程序（就像我们用 FlixTube
    做的那样）或者你处于开发自己的微服务应用程序的初期阶段时，这是完全可以接受的。但这并不像它本可以做到的那样具有容错性。
- en: This is easily fixed though, because Kubernetes makes it easy for us to create
    replicas. The amazing thing is that it is just as simple as changing the value
    of a field in the Terraform code that we have already written-that’s the power
    of *infrastructure as code*.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易解决，因为 Kubernetes 让我们很容易创建副本。令人惊讶的是，这和更改我们已编写的 Terraform 代码中的一个字段的值一样简单——这就是**基础设施即代码**的力量。
- en: We can easily change the number of replicas by setting the value of the `replicas`
    property in our Kubernetes deployment. You can see an example of this in listing
    10.2, which is an update to the Terraform code from chapter 9.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设置 Kubernetes 部署中 `replicas` 属性的值来轻松更改副本的数量。您可以在列表 10.2 中看到这个例子，这是对第 9
    章中 Terraform 代码的更新。
- en: The number of replicas has been updated from one to three. We can apply this
    change by running `terraform apply`. Once completed, our microservices all have
    three redundant instances. With this small change, we have massively improved
    the reliability and fault tolerance of our application!
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 副本数量已从 1 更新到 3。我们可以通过运行 `terraform apply` 来应用这个更改。一旦完成，我们的所有微服务都将拥有三个冗余实例。通过这个小小的改变，我们极大地提高了应用程序的可靠性和容错性！
- en: The load balancer for our replicas is created by the Kubernetes service defined
    at the end of listing 10.2\. Working through chapters 7 and 9, we always had a
    load balancer for our microservices, but it distributed the load to only a single
    microservice! With the change we make in the listing, the load is now being distributed
    between three instances for each microservice.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们副本的负载均衡器是由列表 10.2 末尾定义的 Kubernetes 服务创建的。在处理第 7 章和第 9 章时，我们始终为我们的微服务提供了一个负载均衡器，但它只将负载分配给单个微服务！通过列表中我们做出的更改，现在负载正在三个实例之间分配给每个微服务。
- en: Listing 10.2 Creating load balanced replicas of a microservice on Kubernetes
    (an update to chapter-9/example-1/scripts/modules/microservice/main.tf)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.2 在 Kubernetes 上创建负载均衡的微服务副本（对第 9 章示例 1 的脚本 modules/microservice/main.tf
    的更新）
- en: '[PRE8]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Configures the Kubernetes deployment for each microservice
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ① 配置每个微服务的 Kubernetes 部署
- en: ② Sets the replica count to 3\. Running “terraform apply” again creates three
    replicas of each microservice.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将副本数量设置为 3。再次运行“terraform apply”将创建每个微服务的三个副本。
- en: 10.4.5 Fault isolation and graceful degradation
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.5 故障隔离和优雅降级
- en: One thing that microservices are really good at is fault isolation. We do have
    to take some care, however, to be able to make use of this. What we are aiming
    for is that problems within our cluster are isolated so that they have minimal
    effect on the user.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务真正擅长的一件事是故障隔离。然而，我们必须小心一些，以便能够利用这一点。我们的目标是使集群内部的问题得到隔离，以便它们对用户的影响最小化。
- en: With appropriate mechanisms in place, our application can gracefully handle
    faults and prevent these from manifesting as problems in the front end. The tools
    we need for this are timeouts, retries, circuit breakers, and bulkheads, which
    are described in the following sections.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在适当机制到位的情况下，我们的应用程序可以优雅地处理故障，并防止这些故障在前端表现为问题。我们需要的工具包括超时、重试、断路器和防波堤，这些将在以下章节中描述。
- en: As an example, let’s consider the video-upload microservice. Just imagine that
    something has gone wrong with it, and it is no longer functional. At this moment,
    we are working hard to rectify the situation and quickly restore it to a working
    state. In the meantime, our customers would like to continue using our product.
    If we didn’t have mechanisms to prevent it, errors might go all the way to the
    front end, bringing our service down, and badly disrupting our customers.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑视频上传微服务。想象一下，它出了问题，不再可用。在这个时候，我们正在努力纠正这种情况，并迅速将其恢复到工作状态。与此同时，我们的客户希望继续使用我们的产品。如果我们没有预防机制，错误可能会一直传播到前端，导致我们的服务中断，严重扰乱我们的客户。
- en: 'Instead, we should implement safeguards that stop this wholesale disruption
    of our user base. This is illustrated in figure 10.15\. The top part of the figure
    shows the error propagating all the way to the user and causing problems for them.
    The bottom part of figure 10.15 shows how this should work: the gateway stopping
    the error in its tracks, thus containing the fault within the cluster.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们应该实施保护措施，防止这种对用户群体的全面破坏。这如图10.15所示。图的上半部分显示了错误一直传播到用户并给他们带来问题。图10.15的下半部分显示了它应该如何工作：网关阻止错误传播，从而在集群内限制故障。
- en: We can then handle the situation by showing the user an error message saying
    that the video-upload feature is currently not available. Video upload might be
    broken, but our users can continue using the rest of the application.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过向用户显示错误消息来处理这种情况，说明视频上传功能目前不可用。视频上传可能已损坏，但我们的用户可以继续使用应用程序的其他部分。
- en: This is a huge benefit that microservices brings to the table. If we were using
    a monolith and one of its components (e.g., the video-upload component) was broken,
    that usually takes down the entire monolith, leaving our customers with nothing.
    With microservices, however, the fault can be isolated, and the application as
    a whole can continue to function, albeit in a degraded state.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是微服务带来的巨大好处。如果我们使用单体应用，并且其中一个组件（例如，视频上传组件）损坏，通常会导致整个单体崩溃，让我们的客户一无所有。然而，使用微服务，故障可以被隔离，整个应用程序可以继续运行，尽管是降级状态。
- en: This idea of fault isolation is often called *the bulkhead pattern*, so named
    because it is conceptually similar to the actual bulkheads that are used in large
    ships. When a leak occurs in a ship, it is the bulkheads that prevent the leak
    from escaping to other compartments and eventually sinking the ship. This is fault
    isolation in the real world, and you can see how it is similar to a microservices
    application.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这种故障隔离的想法通常被称为*防波堤模式*，之所以这样命名，是因为它在概念上与大型船舶中实际使用的防波堤相似。当船舶发生泄漏时，是防波堤阻止泄漏逃逸到其他舱室，并最终导致船舶沉没。这是现实世界中的故障隔离，你可以看到它如何与微服务应用程序相似。
- en: '![](../Images/CH10_F15_Davis4.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图10.15 集群内隔离故障](../Images/CH10_F15_Davis4.png)'
- en: Figure 10.15 Isolating faults within the cluster from the user
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15 集群内隔离故障于用户
- en: 10.4.6 Simple techniques for fault tolerance
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.6 容错简单技术
- en: Here are some simple techniques you can start using immediately to implement
    fault tolerance and fault isolation in your own microservices application.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些简单的技术，您可以立即开始使用，以在您自己的微服务应用程序中实现容错和故障隔离。
- en: Timeouts
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 超时
- en: In this book, we used the built-in Node.js `http.request` function and the Axios
    code library to make HTTP requests internally between microservices. We control
    our own microservices, and most of the time, we know those will respond quickly
    to requests that are internal to the cluster. There are times, however, when a
    problem manifests itself and an internal microservice stops responding.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们使用了内置的Node.js `http.request`函数和Axios代码库来在微服务之间进行内部HTTP请求。我们控制自己的微服务，大多数时候我们知道它们会快速响应集群内部的请求。然而，有时问题会显现出来，一个内部微服务停止响应。
- en: In the future, we’d also like to make requests to external services. Just imagine
    that we have integrated FlixTube with Dropbox as a means to import new videos.
    When making requests to an external service like Dropbox, we don’t have any control
    over how quickly these respond to our requests. Such external services will go
    down for maintenance occasionally, so it’s entirely likely that an external service
    like Dropbox will intermittently stop responding to our requests.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来，我们还想向外部服务发起请求。想象一下，我们将 FlixTube 与 Dropbox 集成，作为导入新视频的手段。当我们向类似 Dropbox 这样的外部服务发起请求时，我们无法控制这些服务对请求的响应速度。这样的外部服务偶尔会因维护而关闭，因此，外部服务如
    Dropbox 间歇性地停止响应我们的请求是完全可能的。
- en: We must consider how to handle requests to a service that doesn’t respond. If
    a request isn’t going to complete anytime soon, we’d like to have it aborted after
    some maximum amount of time. If we didn’t do that, it could take a long time (if
    ever) for the request to complete. We can’t very well have our customer waiting
    so long! We’d prefer to abort the request quickly and tell the customer something
    has gone wrong, rather than have them waiting indefinitely on it.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须考虑如何处理不响应的服务请求。如果请求在短时间内无法完成，我们希望在经过一定最大时间后将其终止。如果我们不这样做，请求可能需要很长时间（如果有的话）才能完成。我们实在无法让我们的客户等待那么久！我们宁愿快速终止请求，并告诉客户出了些问题，而不是让他们无限期地等待。
- en: We can deal with this using *timeouts*. A timeout is the maximum amount of time
    that can elapse before a request is automatically aborted with an error code.
    Setting timeouts for our requests allows us to control how quickly our application
    responds to failure. Failing quickly is what we want here because the alternative
    is to fail slowly, and if something is going to fail, we’d like to deal with it
    as quickly as possible so as not to waste our customer’s time.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 *超时* 来处理这个问题。超时是在请求自动因错误代码被终止之前可以经过的最大时间。为我们的请求设置超时允许我们控制我们的应用程序对失败的反应速度。快速失败是我们想要的，因为另一种选择是缓慢失败，如果某件事要失败，我们希望尽可能快地处理它，以免浪费客户的时间。
- en: Setting a timeout with Axios
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Axios 设置超时
- en: Reading the Axios documentation tells me that the default timeout is infinity!
    That means by default, an Axios request can literally go forever without being
    aborted. We definitely need to set the timeout for any requests we make with Axios.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读Axios 文档告诉我，默认超时是无限大！这意味着默认情况下，Axios 请求可以无限期地进行而不会被终止。我们绝对需要为任何使用 Axios 发起的请求设置超时。
- en: You can set the timeout for each request, but that requires repeated effort.
    Fortunately, with Axios, we can set a default timeout for *all* requests as shown
    in the following listing.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为每个请求设置超时，但这需要重复的努力。幸运的是，使用 Axios，我们可以为所有请求设置默认超时，如下面的列表所示。
- en: Listing 10.3 Setting a default timeout for HTTP requests with Axios
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.3 使用 Axios 设置 HTTP 请求的默认超时时间
- en: '[PRE9]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Sets the default timeout for requests to 2500 milliseconds or 2.5 seconds
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将请求的默认超时设置为 2500 毫秒或 2.5 秒
- en: Retries
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 重试
- en: We know that HTTP requests sometimes fail. We don’t control external services,
    and we can’t see the code for those. It’s difficult for us to determine how reliable
    these are and even the most reliable services can have intermittent failures.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 HTTP 请求有时会失败。我们无法控制外部服务，也无法看到那些服务的代码。对我们来说，很难确定这些服务的可靠性，即使是可靠性最高的服务也可能会有间歇性故障。
- en: '![](../Images/CH10_F16_Davis4.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH10_F16_Davis4.png)'
- en: Figure 10.16 Retrying a HTTP request until it succeeds
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 重试 HTTP 请求直到成功
- en: One simple way to deal with this is to simply retry the operation a number of
    times and hope that it succeeds on one of the subsequent attempts. This is illustrated
    in figure 10.16\. In this example, you can imagine FlixTube’s video storage microservice
    requesting a video to be retrieved from Azure storage. Occasionally such requests
    fail for indeterminable reasons. In figure 10.16, two successive requests have
    failed due to an intermittent connection error, but the third request succeeds.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的一个简单方法是在多次尝试中简单地重试操作，并希望它在后续尝试中成功。这如图 10.16 所示。在这个例子中，你可以想象 FlixTube
    的视频存储微服务请求从 Azure 存储检索视频。由于不可确定的原因，此类请求偶尔会失败。在图 10.16 中，连续两次请求因间歇性连接错误而失败，但第三次请求成功。
- en: Assuming that the network is reliable is one of the fallacies of distributed
    computing, we must take steps to mitigate against request failures. Implementation
    in JavaScript isn’t particularly difficult. In listing 10.4, you can see an implementation
    of a `retry` function that I’ve used across a number of projects. The `retry`
    function wraps other asynchronous operations such as HTTP requests so these can
    be attempted multiple times.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 假设网络是可靠的，这是分布式计算中的一个谬误，我们必须采取措施来减轻请求失败的影响。在JavaScript中的实现并不特别困难。在列表10.4中，你可以看到我已在多个项目中使用的一个`retry`函数的实现。`retry`函数包装了其他异步操作，如HTTP请求，这样就可以尝试多次。
- en: Listing 10.4 also includes a helpful `sleep` function used to make pauses between
    attempts. There’s no point immediately trying to make a request again. If we do
    it too quickly, it’s probably just going to fail again. In this case, we give
    it some time before making another attempt.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4还包括了一个有用的`sleep`函数，用于在尝试之间创建暂停。立即再次尝试请求是没有意义的。如果我们做得太快，它很可能会再次失败。在这种情况下，我们在再次尝试之前给它一些时间。
- en: Listing 10.5 is an example of how to call the `retry` function, showing how
    it can wrap a HTTP GET request. In this example, we allow the request to be retried
    three times with a pause of 5 milliseconds between each request.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.5展示了如何调用`retry`函数，展示了它如何包装HTTP GET请求。在这个例子中，我们允许请求重试三次，每次请求之间暂停5毫秒。
- en: Listing 10.4 Implementation of a retry function in JavaScript
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4 JavaScript中重试函数的实现
- en: '[PRE10]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Defines a “sleep” function that can be used to pause between retries
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ① 定义了一个“sleep”函数，可用于在重试之间暂停
- en: ② Wraps a call to “setTimeout” in a Promise so that we await the completion
    of the pause
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在Promise中包装对“setTimeout”的调用，以便我们等待暂停完成
- en: ③ Invokes a callback after the elapsed amount of time
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在经过一段时间后调用回调
- en: ④ The callback resolves the Promise.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 回调解决Promise。
- en: ⑤ Sets the duration of the pause
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置暂停的持续时间
- en: ⑥ Defines a “retry” function that we can use to make multiple attempts for any
    asynchronous operation
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 定义了一个“重试”函数，我们可以用它来对任何异步操作进行多次尝试
- en: ⑦ Loops for each attempt up until to the maximum number of retries
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 循环直到达到最大重试次数
- en: ⑧ Makes an attempt at the actual asynchronous operation
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 尝试实际异步操作
- en: ⑨ The operation was a success! This breaks out of the loop and returns the result
    of the asynchronous operation.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 操作成功！这会跳出循环并返回异步操作的结果。
- en: ⑩ Handles any error thrown by the asynchronous operation
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 处理异步操作抛出的任何错误
- en: ⑪ Records the error from the most recent attempt
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 记录最近尝试中的错误
- en: ⑫ Pauses for a brief moment before the next attempt (so long as we are not on
    the last attempt)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 在下一次尝试之前暂停片刻（只要我们不是最后一次尝试）
- en: ⑬ Throws the error from the last attempt. We ran out of retries, so we must
    let the error bubble up to the caller.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 抛出最后一次尝试的错误。我们已经用完了重试次数，所以我们必须让错误冒泡到调用者。
- en: Listing 10.5 Using the retry function (an example)
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.5 使用重试函数（示例）
- en: '[PRE11]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Calls our “retry” function
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ① 调用我们的“retry”函数
- en: ② The operation to be retried; in this example, it’s an HTTP GET request using
    Axios.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ② 要重试的操作；在这个例子中，它是一个使用Axios的HTTP GET请求。
- en: ③ Sets the maximum number of attempts to three
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将最大尝试次数设置为三次
- en: ④ Sets the time between retries to be five milliseconds
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将重试之间的时间设置为五毫秒
- en: 10.4.7 Advanced techniques for fault tolerance
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.7 容错的高级技术
- en: We have seen some simple techniques for improving the reliability and resilience
    of our application. Of course, there are many other more advanced techniques we
    could deploy for improved fault tolerance and recovery from failures.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一些提高我们应用程序可靠性和弹性的简单技术。当然，还有许多其他更高级的技术我们可以部署来提高容错性和从失败中恢复。
- en: We are almost beyond the scope of the book, but I’d still like to share with
    you a brief overview of some more advanced techniques. These will be useful in
    the future as you evolve a more robust architecture for your application.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎超出了本书的范围，但我仍然想与你分享一些更高级技术的简要概述。这些技术在你为应用程序构建更健壮的架构时将非常有用。
- en: Job queue
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 工作队列
- en: The *job queue* is a type of microservice found in many application architectures.
    This is a different kind of thing to the message queue we saw in RabbitMQ. It’s
    similar, but it’s a level of sophistication above that.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '*工作队列*是许多应用程序架构中找到的一种微服务。这与我们在RabbitMQ中看到的消息队列是不同的事物。它很相似，但它的复杂度更高。'
- en: We use a job queue to manage heavy-weight processing tasks. Let’s imagine how
    this could work for a future version of FlixTube. We can say that each video requires
    a lot of processing after it is uploaded. For example, we’d like to extract a
    thumbnail from videos. Or maybe, we’d like to convert videos to a lower resolution
    for better performance playback on mobile devices. These are the kinds of tasks
    that should happen after a video is uploaded.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用工作队列来管理重量级处理任务。让我们想象一下FlixTube未来版本的工作方式。我们可以这样说，每个视频在上传后都需要大量的处理。例如，我们希望从视频中提取缩略图。或者，也许我们希望将视频转换为较低分辨率，以便在移动设备上更好地播放性能。这些是在视频上传后应该发生的任务类型。
- en: Now imagine that 1,000 users have each uploaded a video, all roughly at the
    same time. We don’t have any kind of elastic scaling yet (we’ll talk about that
    in chapter 11). So how can we manage the huge processing workload resulting from
    so many videos landing in FlixTube at the same time? This is what the job queue
    does. You can see an illustration of how it works in figure 10.17.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，1,000个用户几乎同时上传了视频。我们还没有任何弹性扩展（我们将在第11章中讨论）。那么，我们如何管理这么多视频同时进入FlixTube产生的巨大处理工作负载呢？这就是工作队列的作用。您可以在图10.17中看到其工作原理的示意图。
- en: '![](../Images/CH10_F17_Davis4.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图10.17](../Images/CH10_F17_Davis4.png)'
- en: Figure 10.17 A job queue microservice manages a queue of video thumbnail generation
    jobs.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17 一个工作队列微服务管理着视频缩略图生成作业的队列。
- en: The job queue records the sequence of jobs that need to be performed to the
    database. This makes it resilient against failure. The entire application could
    crash and restart, but so long as the database survives, we can reload the job
    queue and continue processing where it left off. Individual jobs can also fail;
    for example, the microservice doing the processing crashes, but because failed
    jobs aren’t marked as complete, they’ll naturally be attempted again later.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 工作队列将需要执行的任务序列记录到数据库中。这使得它对故障具有弹性。整个应用程序可能会崩溃并重新启动，但只要数据库存活，我们就可以重新加载工作队列并继续处理之前中断的地方。个别任务也可能失败；例如，执行处理的微服务崩溃了，但由于失败的任务没有被标记为完成，它们会自然地在稍后再次尝试。
- en: The job queue also allows for control over the performance of this processing.
    Instead of maxing out our application to process the 1,000 uploaded videos all
    at once, we can spread out the load so that processing is scheduled over a longer
    time period. It can also be scheduled across off-peak hours. This means we won’t
    have to pay for the extra compute that might otherwise be required if we wanted
    to do the processing all at once in a massive burst.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 工作队列还允许控制此处理性能。我们不必一次性将应用程序性能最大化来处理1,000个上传的视频，我们可以将负载分散，以便在更长的时间段内进行调度。它也可以在非高峰时段进行调度。这意味着我们不必为可能需要的额外计算能力付费，如果我们想一次性在大量爆发中进行处理的话。
- en: Circuit breaker
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 电路断路器
- en: The *circuit breaker* is like a more advanced version of a timeout. It has some
    built-in smarts to understand when problems are occurring, so that it can deal
    with these more intelligently. Figure 10.18 illustrates how a circuit breaker
    works.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*电路断路器*类似于超时的更高级版本。它有一些内置的智能来理解何时出现问题，以便更智能地处理这些问题。图10.18说明了电路断路器的工作原理。'
- en: '![](../Images/CH10_F18_Davis4.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![图10.17](../Images/CH10_F18_Davis4.png)'
- en: Figure 10.18 Illustrating how the circuit breaker works
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18 电路断路器工作原理示意图
- en: In normal situations, the status of the circuit breaker is set to On, and it
    allows HTTP requests to go through as usual (1). If at some point a request to
    a particular resource fails (2), the circuit breaker flips to the Off state (3).
    While in the Off state, the circuit breaker always fails new requests immediately.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常情况下，电路断路器的状态设置为开启，并允许HTTP请求像往常一样通过（1）。如果在某个时刻对特定资源的请求失败（2），电路断路器将切换到关闭状态（3）。在关闭状态下，电路断路器总是立即拒绝新的请求。
- en: Think of this as a “super” timeout. The circuit breaker knows the upstream system
    is failing at the moment, so it doesn’t even bother checking. It immediately fails
    the incoming request!
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 将其视为一个“超级”超时。电路断路器知道上游系统目前正在失败，因此它甚至懒得检查。它立即拒绝传入的请求！
- en: This failing quickly is why we used timeouts. It’s better to fail quickly than
    to fail slowly. The circuit breaker works by already knowing that we are failing,
    and so, instead of just failing more quickly, it can fail *immediately*.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 快速失败是我们使用超时的原因。快速失败比缓慢失败要好。电路断路器通过已经知道我们正在失败，因此，它不仅可以更快地失败，而且可以立即失败。
- en: Periodically, on its own time (with a delay that you can configure), the circuit
    breaker checks the upstream service to see if it has resumed normal operation.
    When that happens, the circuit breaker flips back to the On state (4). Future
    incoming requests are now allowed through as normal. Implementing a circuit breaker
    is much more difficult than implementing a timeout or a retry, but it’s worth
    keeping in mind for future use, especially if you find yourself needing a more
    sophisticated technique.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 定期地，在它自己的时间（你可以配置延迟），断路器会检查上游服务是否已恢复正常操作。当这种情况发生时，断路器会切换回开启状态（4）。未来的入站请求现在可以正常通过。实现断路器比实现超时或重试要困难得多，但值得记住以备将来使用，特别是如果你发现自己需要更复杂的技术。
- en: 10.5 Continue your learning
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 继续你的学习
- en: 'You now have many techniques in your toolkit for keeping your microservices
    healthy and reliable! To learn more about building reliable microservices, Manning
    has released some free chapters relating to microservice stability from the books
    *Microservices in Action* by Morgan Bruce and Paulo A. Pereira (2018) and *The
    Tao of Microservices* by Richard Rodger (2017). You’ll find these available here:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在工具箱里有很多技术来保持你的微服务健康和可靠！要了解更多关于构建可靠微服务的信息，Manning 出版了一些关于微服务稳定性的免费章节，这些章节来自
    Morgan Bruce 和 Paulo A. Pereira 的书籍 *Microservices in Action*（2018年）以及 Richard
    Rodger 的 *The Tao of Microservices*（2017年）。你可以在以下链接找到这些内容：
- en: '[https://www.manning.com/books/microservices-stability](https://www.manning.com/books/microservices-stability)'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.manning.com/books/microservices-stability](https://www.manning.com/books/microservices-stability)'
- en: 'There’s also a great book about *crash testing* your application:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一本关于如何对应用程序进行*崩溃测试*的精彩书籍：
- en: '*Chaos Engineering* by Mikolaj Pawlikowski (Manning, 2020)'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Chaos Engineering* by Mikolaj Pawlikowski (Manning, 2020)'
- en: 'To learn more about logging and monitoring in production, read these books:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于生产中的日志记录和监控的信息，请阅读以下书籍：
- en: '*Unified Logging with Fluentd* by *Phil Wilkins (Manning, est Summer 2020)*'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Unified Logging with Fluentd* by *Phil Wilkins (Manning, est Summer 2020)*'
- en: '*Elasticsearch in Action* by *Radu Gheorghe, Matthew Lee Hinman, and Roy Russo
    (Manning, 2015)*'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elasticsearch in Action* by *Radu Gheorghe, Matthew Lee Hinman, and Roy Russo
    (Manning, 2015)*'
- en: Summary
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Logging and error handling outputs key information used to understand the behavior
    and state of our microservices.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logging and error handling outputs key information used to understand the behavior
    and state of our microservices.
- en: Monitoring is key to determining the health of our microservices and detecting
    problems.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控是确定我们的微服务健康状况和检测问题的关键。
- en: Aggregation combines the output of all our microservices into a single easily
    accessible stream of information.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aggregation combines the output of all our microservices into a single easily
    accessible stream of information.
- en: Kubernetes health checks can be used to automatically detect problems in our
    microservices.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 健康检查可以用来自动检测我们的微服务中的问题。
- en: When a problem is detected, we must go through a process of debugging to find
    the cause of the problem and to determine an appropriate fix for it.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当检测到问题时，我们必须通过调试过程来找出问题的原因，并确定适当的修复方案。
- en: There are many techniques for ensuring reliability and fault tolerance of our
    microservices, including replicas and load balancing, automatic restarts, timeouts,
    retries, job queues, and circuit breakers.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们的微服务可靠性和容错性的技术有很多，包括副本和负载均衡、自动重启、超时、重试、作业队列和断路器。
