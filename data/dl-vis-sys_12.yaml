- en: 9 DeepDream and neural style transfer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 DeepDream和神经风格迁移
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Visualizing CNN feature maps
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化CNN特征图
- en: Understanding the DeepDream algorithm and implementing your own dream
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解DeepDream算法并实现自己的梦想
- en: Using the neural style transfer algorithm to create artistic images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经风格迁移算法创建艺术图像
- en: In fine art, especially painting, humans have mastered the skill of creating
    unique visual experiences through composing a complex interplay between the content
    and style of an image. So far, the algorithmic basis of this process is unknown,
    and there exists no artificial system with similar capabilities. Nowadays, deep
    neural networks have demonstrated great promise in many areas of visual perception
    such as object classification and detection. Why not try using deep neural networks
    to create art? In this chapter, we introduce an artificial system based on a deep
    neural network that creates artistic images of high perceptual quality. The system
    uses neural representations to separate and recombine content and style of arbitrary
    images, providing a neural algorithm for the creation of artistic images.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯艺术，尤其是绘画中，人类已经掌握了通过组合图像的内容和风格之间的复杂相互作用来创造独特视觉体验的技能。到目前为止，这个过程算法基础尚不清楚，并且不存在具有类似能力的人工系统。如今，深度神经网络在视觉感知的许多领域，如对象分类和检测，已经显示出巨大的潜力。为什么不用深度神经网络来创造艺术呢？在本章中，我们介绍了一个基于深度神经网络的人工系统，该系统能够创建具有高感知质量的艺术图像。该系统使用神经网络表示来分离和重新组合任意图像的内容和风格，为艺术图像的创建提供了一种神经网络算法。
- en: 'In this chapter, we explore two new techniques to create artistic images using
    neural networks: DeepDream and neural style transfer. First, we examine how convolutional
    neural networks see the world. We’ve learned how CNNs are used to extract features
    in object classification and detection problems; here, we learn how to visualize
    the extracted feature maps. One reason is that we need this visualization technique
    in order to understand the DeepDream algorithm. Additionally, this will help us
    gain a better understanding of what our network learned during training; we can
    use that to improve the network’s performance when solving classification and
    detection problems.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了两种使用神经网络创建艺术图像的新技术：DeepDream和神经风格迁移。首先，我们检查卷积神经网络如何看世界。我们已经学习了CNN如何用于对象分类和检测问题中的特征提取；在这里，我们学习如何可视化提取的特征图。一个原因是我们需要这种可视化技术来理解DeepDream算法。此外，这将帮助我们更好地理解网络在训练期间学到了什么；我们可以利用这一点来提高网络在解决分类和检测问题时表现。
- en: Next, we discuss the DeepDream algorithm. The key idea of this technique is
    to print the features we visualize in a certain layer onto our input image, to
    create a dream-like hallucinogenic image. Finally, we explore the neural style
    transfer technique, which takes two images as inputs--a style image and a content
    image--and creates a new combined image that contains the layout from the content
    image and the texture, colors, and patterns from the style image.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论DeepDream算法。这种技术的关键思想是将我们在某一层可视化的特征打印到我们的输入图像上，以创建一种梦幻般的幻觉图像。最后，我们探索神经风格迁移技术，它接受两个图像作为输入——一个风格图像和一个内容图像——并创建一个包含内容图像的布局和风格图像的纹理、颜色和模式的新组合图像。
- en: Why is this discussion important? Because these techniques help us understand
    and visualize how neural networks are able to carry out difficult classification
    and detection tasks and check what the network has learned during training. Being
    able to see what the network thinks is an important feature to use when distinguishing
    objects will help you understand what is missing from your training set and thus
    improve the network’s performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这次讨论很重要？因为这些技术帮助我们理解和可视化神经网络如何执行困难的分类和检测任务，并检查网络在训练期间学到了什么。能够看到网络的想法，在区分物体时是一个重要的特征，这将帮助你了解你的训练集中缺少什么，从而提高网络的表现。
- en: These techniques also make us wonder whether neural networks could become tools
    for artists, give us a new way to combine visual concepts, or perhaps even shed
    a little light on the roots of the creative process in general. Moreover, these
    algorithms offer a path forward to an algorithmic understanding of how humans
    create and perceive artistic imagery.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术也让我们思考神经网络是否可能成为艺术家的工具，给我们提供一种新的方式来结合视觉概念，或者甚至可能对一般创造性过程的根源投下一些光。此外，这些算法为算法理解人类如何创造和感知艺术图像提供了一条前进的道路。
- en: 9.1 How convolutional neural networks see the world
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 卷积神经网络如何观察世界
- en: We have talked a lot in this book about all the amazing things deep neural networks
    can do. But despite all the exciting news about deep learning, the exact way neural
    networks see and interpret the world remains a black box. Yes, we have tried to
    explain how the training process works, and we explained intuitively and mathematically
    the backpropagation process that the network applies to update weights through
    many iterations to optimize the loss function. This all sounds good and makes
    sense on the scientific side of things. But how do CNNs see the world? How do
    they see the extracted features between all the layers?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中已经讨论了很多深度神经网络所能做到的令人惊叹的事情。但尽管关于深度学习的新闻都十分激动人心，神经网络确切地是如何观察和解释世界的仍然是一个黑箱。是的，我们尝试解释了训练过程是如何工作的，并且我们通过直观和数学的方式解释了网络通过多次迭代更新权重以优化损失函数的反向传播过程。这一切听起来都很好，在科学方面也很有道理。但是，CNN是如何观察世界的？它们是如何在所有层之间观察提取的特征的？
- en: 'A better understanding of exactly how they recognize specific patterns or objects
    and why they work so well might allow us to improve their performance even further.
    Additionally, on the business side, this would also solve the “AI explainability”
    problem. In many cases, business leaders feel unable to make decisions based on
    model predictions because nobody really understands what is happening inside the
    black box. This is what we do in this section: we open the black box and visualize
    what the network sees through its layers, to help make neural network decisions
    interpretable by humans.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 更好地理解它们是如何识别特定模式或对象以及为什么它们工作得如此之好，可能会让我们进一步提高它们的性能。此外，在商业方面，这也会解决“AI可解释性”问题。在许多情况下，商业领导者感到无法根据模型预测做出决策，因为没有人真正理解黑箱内部发生了什么。这就是我们在本节中要做的：我们打开黑箱，可视化网络通过其层所看到的内容，以帮助使神经网络决策对人类可解释。
- en: In computer vision problems, we can visualize the feature maps inside the convolutional
    network to understand how they see the world and what features they think are
    distinctive in an object for differentiating between classes. The idea of visualizing
    convolutional layers was proposed by Erhan et al. in 2009.[1](#pgfId-1193143)
    In this section, we will explain this concept and implement it in Keras.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉问题中，我们可以可视化卷积网络内部的特征图，以了解它们是如何观察世界的，以及它们认为哪些特征在区分不同类别时是独特的。可视化卷积层这一想法是由Erhan等人于2009年提出的。[1](#pgfId-1193143)
    在本节中，我们将解释这一概念并在Keras中实现它。
- en: 9.1.1 Revisiting how neural networks work
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 重新审视神经网络的工作原理
- en: Before we jump into the explanation of how we can visualize the activation maps
    (or feature maps) in a CNN, let’s revisit how neural networks work. We train a
    deep neural network by showing it millions of training examples. The network then
    gradually updates its parameters until it gives the classifications we want. The
    network typically consists of 10-30 stacked layers of artificial neurons. Each
    image is fed into the input layer, which then talks to the next layer, until eventually
    the “output” layer is reached. The network’s prediction is then produced by its
    final output layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳入如何可视化CNN中的激活图（或特征图）的解释之前，让我们重新审视神经网络的工作原理。我们通过展示数百万个训练示例来训练深度神经网络。然后，网络逐渐更新其参数，直到它给出我们想要的分类。网络通常由10-30层堆叠的人工神经元组成。每个图像被输入到输入层，然后与下一层通信，直到最终达到“输出”层。网络的预测由其最终输出层产生。
- en: One of the challenges of neural networks is understanding what exactly goes
    on at each layer. We know that after training, each layer progressively extracts
    image features at higher and higher levels, until the final layer essentially
    makes a decision about what the image contains. For example, the first layer may
    look for edges or corners, intermediate layers interpret the basic features to
    look for overall shapes or components, and the final few layers assemble those
    into complete interpretations. These neurons activate in response to very complex
    images such as a car or a bike.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个挑战是理解每个层究竟发生了什么。我们知道在训练之后，每个层逐渐提取更高层次上的图像特征，直到最终层基本上做出关于图像包含内容的决定。例如，第一层可能寻找边缘或角落，中间层将基本特征解释为寻找整体形状或组件，而最后几层将这些组合成完整的解释。这些神经元对非常复杂的图像，如汽车或自行车，做出反应。
- en: To understand what the network has learned through its training, we want to
    open this black box and visualize its feature maps. One way to visualize the extracted
    features is to turn the network upside down and ask it to enhance an input image
    in such a way as to elicit a particular interpretation. Say you want to know what
    sort of image would result in the output “Bird.” Start with an image full of random
    noise, and then gradually tweak the image toward what the neural net considers
    an important feature of a bird (figure 9.1).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解网络通过训练学到了什么，我们想要打开这个黑盒并可视化其特征图。可视化提取特征的一种方法是将网络颠倒过来，并要求它以某种方式增强输入图像，以引发特定的解释。比如说，你想知道什么样的图像会产生“鸟”的输出。从一个充满随机噪声的图像开始，然后逐渐调整图像，使其逐渐接近神经网络认为的鸟的重要特征（图9.1）。
- en: '![](../Images/9-1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-1.png)'
- en: Figure 9.1 Start with an image consisting of random noise, and tweak it until
    we visualize what the network considers important features of a bird.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 从一个由随机噪声组成的图像开始，调整它直到我们可视化网络认为的鸟的重要特征。
- en: We will dive deeper into the bird example and see how to visualize the network
    filters. The takeaway from this introduction is that neural networks are smart
    enough to understand which are the important features to pass along through its
    layers to be classified by its fully connected layers. Non-important features
    are discarded along the way. To put it simply, neural networks learn the features
    of the objects in the training dataset. If we are able to visualize these feature
    maps at the deeper layers of the network, we can find out where the neural network
    is paying attention and see the exact features that it uses to make its predictions.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更深入地探讨鸟的例子，看看如何可视化网络滤波器。从这个介绍中我们可以得出结论，神经网络足够聪明，能够理解哪些是重要的特征，并通过其层传递给全连接层进行分类。非重要特征在过程中被丢弃。简单来说，神经网络学习训练数据集中对象的特征。如果我们能够在网络的深层中可视化这些特征图，我们就可以找出神经网络关注的地方，并看到它用来做出预测的确切特征。
- en: 'NOTE This process is described best in François Chollet’s book, Deep Learning
    with Python (Manning, 2017; [www.manning.com/books/deep-learning-with-python](https://www.manning.com/books/deep-learning-with-python)):
    “You can think of a deep network as a multistage information-distillation operation,
    where information goes through successive filters and comes out increasingly purified.”'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这个过程在François Chollet的《Python深度学习》（Manning，2017；[www.manning.com/books/deep-learning-with-python](https://www.manning.com/books/deep-learning-with-python)）一书中描述得最好：“你可以将深度网络视为一个多阶段信息蒸馏操作，其中信息通过连续的滤波器，并逐渐变得更加纯净。”
- en: 9.1.2 Visualizing CNN features
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 可视化CNN特征
- en: An easy way to visualize the features learned by convolutional networks is to
    display the visual pattern that each filter is meant to respond to. This can be
    done with gradient ascent in input space. By applying gradient ascent to the value
    of the input image of a ConvNet, we can maximize the response of a specific filter,
    starting from a blank input image. The resulting input image will be one that
    the chosen filter is maximally responsive to.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化卷积网络学习到的特征的一个简单方法是通过显示每个滤波器旨在响应的视觉模式。这可以通过输入空间中的梯度上升来实现。通过对ConvNet的输入图像的值应用梯度上升，我们可以最大化特定滤波器的响应，从一个空白输入图像开始。结果输入图像将是那个滤波器响应最大的图像。
- en: Gradient ascent vs. gradient descent
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度上升与梯度下降
- en: As a reminder, the general definition of a gradient is that it is the function
    that defines the slope or rate of change of the line tangent to a curve at any
    given point. In simpler words, the gradient is the slope of the line at that point.
    Here are some example gradients at certain points on a curve.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，梯度的通用定义是：它是定义曲线在任意给定点的切线斜率或变化率的函数。用简单的话说，梯度是那个点的线的斜率。以下是一些曲线上的特定点的梯度示例。
- en: '![](../Images/9-unnumb.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-unnumb.png)'
- en: The gradient at different points on the curve
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线上不同点的梯度
- en: Whether we want to descend or ascend the curve is based on our project. We learned
    in chapter 2 that GD is the algorithm that descends the error function to find
    the local minimum (for example, minimize the loss function) by taking steps toward
    the negative of the gradient.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是想要下降还是上升曲线，这取决于我们的项目。我们在第二章中了解到，梯度下降法是寻找局部最小值（例如，最小化损失函数）的算法，它通过朝着梯度的负方向迈步来下降误差函数。
- en: To visualize feature maps, we want to maximize these features to make them show
    on the output image. In order to maximize the loss function, we want to reverse
    the GD process by using a gradient ascent algorithm. It takes steps proportional
    to the positive of the gradient to approach a local maximum of that function.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化特征图，我们希望最大化这些特征，使它们在输出图像上显示出来。为了最大化损失函数，我们希望通过使用梯度上升算法来反转GD过程。它通过正梯度的比例来接近该函数的局部最大值。
- en: Now comes the fun part of this section. In this exercise, we will see the visualized
    feature maps of a few examples at the beginning, middle, and end of a VGG16 network.
    The implementation is straightforward, and we will get to it soon. Before we go
    to the code implementation, let’s take a look at what these visualized filters
    look like.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是本节最有意思的部分。在这个练习中，我们将看到VGG16网络开始、中间和末尾的一些示例的可视化特征图。实现方法是直接的，我们很快就会看到。在我们进行代码实现之前，让我们看看这些可视化滤波器看起来像什么。
- en: 'From the VGG16 diagram we saw in figure 9.1, let’s visualize the output feature
    maps of the first, middle, and deep layers as follows: `block1_conv1`, `block3_conv2`,
    and `block5_conv3`. Figures 9.2, 9.3, and 9.4 show how the features evolve throughout
    the network layers.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从图9.1中我们看到的VGG16图中，让我们如下可视化第一层、中间层和深层层的输出特征图：`block1_conv1`、`block3_conv2`和`block5_conv3`。图9.2、9.3和9.4显示了特征在整个网络层中的演变过程。
- en: '![](../Images/9-2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-2.png)'
- en: Figure 9.2 Visualizing feature maps produced by `block1_conv1` filters
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 `block1_conv1` 滤波器生成的特征图可视化
- en: 'As you can see in figure 9.2, the early layers basically just encode low-level,
    generic features like direction and color. These direction and color filters then
    get combined into basic grid and spot textures in later layers. These textures
    are gradually combined into increasingly complex patterns (figure 9.3): the network
    starts to see some patterns that create basic shapes. These shapes are not very
    identifiable yet, but they are much clearer than the earlier ones.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如图9.2所示，你可以看到早期层基本上只是编码低级、通用的特征，如方向和颜色。这些方向和颜色滤波器然后在后续层中组合成基本的网格和点状纹理。这些纹理逐渐组合成越来越复杂的图案（图9.3）：网络开始看到一些创建基本形状的图案。这些形状目前还不太容易识别，但比早期的那些要清晰得多。
- en: '![](../Images/9-3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-3.png)'
- en: Figure 9.3 Visualizing feature maps produced by `block3_conv2` filters
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 `block3_conv2` 滤波器生成的特征图可视化
- en: Now this is the most exciting part. In figure 9.4, you see that the network
    was able to find patterns in patterns. These features contain identifiable shapes.
    While the network relies on more than one feature map to make its prediction,
    we can look at these maps and make a close guess about the content of these images.
    In the left image, I can see eyes and maybe a beak, and I would guess that this
    is a type of bird or fish. Even if our guess is not correct, we can easily eliminate
    most other classes like car, boat, building, bike, and so on, because we can clearly
    see eyes and none of those classes have eyes. Similarly, looking at the middle
    image, we can guess from the patterns that this is some kind of a chain. The right
    image feels more like food or fruit.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是最激动人心的部分。在图9.4中，你可以看到网络能够在模式中找到模式。这些特征包含可识别的形状。虽然网络依赖于多个特征图来做出预测，但我们可以查看这些图，并对这些图像的内容做出合理的猜测。在左边的图像中，我可以看到眼睛，也许还有喙，我会猜测这是一种鸟或鱼。即使我们的猜测不正确，我们也可以轻松排除大多数其他类别，如汽车、船只、建筑、自行车等，因为我们可以清楚地看到眼睛，而这些类别中没有一个有眼睛。同样，观察中间的图像，我们可以从模式中猜测这是一条链。右边的图像更像是食物或水果。
- en: '![](../Images/9-4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-4.png)'
- en: Figure 9.4 Visualizing feature maps produced by `block5_conv3` filters
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 `block5_conv3` 滤波器生成的特征图可视化
- en: How is this helpful in classification and detection problems? Let’s take the
    left feature map in figure 9.4 as an example. Looking at the visible features
    like eyes and beaks, I can interpret that the network relies on these two features
    to identify a bird. With this knowledge about what the network learned about birds,
    I will guess that it can detect the bird in figure 9.5, because the bird’s eye
    and beak are visible.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这在分类和检测问题中有什么帮助呢？让我们以图9.4中的左侧特征图为例。观察可见的特征，如眼睛和喙，我可以解释说网络依赖于这两个特征来识别一只鸟。有了关于网络学习到的关于鸟的知识，我将猜测它可以在图9.5中检测到鸟，因为鸟的眼睛和喙是可见的。
- en: '![](../Images/9-5.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-5.png)'
- en: Figure 9.5 Example of a bird image with visible eye and beak features
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5具有可见眼睛和喙特征的鸟的图像示例
- en: Now, let’s consider a more adversarial case where we can see the bird’s body
    but the eye and beak are covered by leaves (figure 9.6). Given that the network
    adds high weights on the eye and beak features to recognize a bird, there is a
    good chance that it might miss this bird because the bird’s main features are
    hidden. On the other hand, an average human can easily detect the bird in the
    image. The solution to this problem is using one of several data-augmentation
    techniques and collecting more adversarial cases in your training dataset to force
    the network to add higher weights on other features of a bird, like shape and
    color.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑一个更具对抗性的情况，其中我们可以看到鸟的身体，但眼睛和喙被树叶覆盖（图9.6）。鉴于网络在眼睛和喙特征上添加了高权重以识别鸟类，它可能会因为鸟类的主要特征被隐藏而错过这个鸟类。另一方面，一个普通的人可以很容易地在图像中检测到鸟类。解决这个问题的一个方法是使用几种数据增强技术之一，并在你的训练数据集中收集更多的对抗性案例，以迫使网络在鸟类的其他特征（如形状和颜色）上添加更高的权重。
- en: '![](../Images/9-6.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-6.png)'
- en: Figure 9.6 Example of an adversarial image of a bird where the eye and beak
    are not visible but the body is recognizable by a human
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 鸟类对抗性图像的示例，其中眼睛和喙不可见，但人体可以通过人类识别
- en: 9.1.3 Implementing a feature visualizer
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 实现特征可视化器
- en: Now that you’ve seen the visualized examples, it is time to get your hands dirty
    and develop the code to visualize these activation filters yourself. This section
    walks through the CNN visualization code implementation from the official Keras
    documentation, with minor tweaking.[2](#pgfId-1193223) You will learn how to generate
    patterns that maximize the mean activation of a chosen feature map. You can see
    the full code in Keras’s Github repository ([http://mng.bz/Md8n](http://mng.bz/Md8n)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了可视化的示例，是时候动手编写代码来自己可视化这些激活过滤器了。本节将介绍从官方Keras文档中实现的CNN可视化代码，略有调整。[2](#pgfId-1193223)
    你将学习如何生成最大化所选特征图平均激活的模式。你可以在Keras的GitHub仓库中看到完整的代码([http://mng.bz/Md8n](http://mng.bz/Md8n))。
- en: NOTE You will run into errors if you try to run the code snippets in this section.
    These snippets are just meant to illustrate the topic. You are encouraged to check
    out the full executable code that is downloadable with the book.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你尝试运行本节中的代码片段，你可能会遇到错误。这些片段只是为了说明主题。我们鼓励你查看与本书一起可下载的完整可执行代码。
- en: 'First, we load the VGG16 model from the Keras library. To do that, we first
    import VGG16 from Keras and then load the model, which is pretrained on the ImageNet
    dataset, without including the classification fully connected layers (top part)
    of the network:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从Keras库中加载VGG16模型。为此，我们首先从Keras导入VGG16，然后加载模型，该模型在ImageNet数据集上预训练，不包括网络分类全连接层（顶部部分）：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports the VGG model from Keras
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从Keras导入VGG模型
- en: ❷ Loads the model
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载模型
- en: 'Now, let’s view the names and output shape of all the VGG16 layers. We do that
    to pick the specific layer whose filters we want to visualize:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看所有VGG16层的名称和输出形状。我们这样做是为了选择我们想要可视化的特定层的过滤器：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Loops through the model layers
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历模型层
- en: ❷ Checks for a convolutional layer
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查卷积层
- en: ❸ Gets the filter weights
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取过滤器权重
- en: When you run this code cell, you will get the output shown in figure 9.7\. These
    are all the convolutional layers contained in the VGG16 network. You can visualize
    any of their outputs simply by referring to each layer by name, as you will see
    in the next code snippet.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个代码单元时，你会得到图9.7所示的输出。这些都是VGG16网络中包含的所有卷积层。你可以通过简单地按名称引用每个层来可视化它们的任何输出，正如你将在下一个代码片段中看到的那样。
- en: '![](../Images/9-7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-7.png)'
- en: Figure 9.7 Output showing convolution layers in the downloaded VGG16 network
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 显示下载的VGG16网络中卷积层的输出
- en: 'Let’s say we want to visualize the first conv layer: `block1_conv1`. Note that
    this layer has 64 filters, each of which has an index from 0 to 63 called `filter_index`.
    Now let’s define a loss function that seeks to maximize the activation of a specific
    filter (`filter` `_index`) in a specific layer (`layer_name`). We also want to
    compute the gradient using Keras’s backend function `gradients` and normalize
    the gradient to avoid very small and very large values, to ensure a smooth gradient
    ascent process.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要可视化第一个卷积层：`block1_conv1`。请注意，这个层有64个过滤器，每个过滤器都有一个从0到63的索引，称为`filter_index`。现在让我们定义一个损失函数，该函数试图最大化特定层（`layer_name`）中特定过滤器（`filter_index`）的激活。我们还想使用Keras的后端函数`gradients`来计算梯度，并将梯度归一化以避免非常小和非常大的值，以确保平滑的梯度上升过程。
- en: 'In this code snippet, we set the stage for gradient ascent. We define a loss
    function, compute the gradients, and normalize the gradients:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们为梯度上升设置了场景。我们定义了一个损失函数，计算了梯度，并归一化了梯度：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Identifies the filter that we want to visualize. This can be any integer from
    0 to 63, as there are 64 filters in that layer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 识别我们想要可视化的过滤器。这可以是该层中从0到63的任何整数，因为该层有64个过滤器。
- en: ❷ Gets the symbolic outputs of each key layer (we gave them unique names).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取每个关键层的符号输出（我们给它们起了独特的名字）。
- en: ❸ Builds a loss function that maximizes the activation of the nth filter of
    the layer considered
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建一个损失函数，该函数最大化考虑的层的第n个过滤器的激活
- en: ❹ Computes the gradient of the input picture with respect to this loss
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算输入图片相对于该损失的梯度
- en: ❺ Normalizes the gradient
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 归一化梯度
- en: ❻ This function returns the loss and grads given the input picture.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 此函数根据输入图片返回损失和梯度。
- en: 'We can use the Keras function that we just defined to do gradient ascent to
    our filter activation loss:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们刚刚定义的Keras函数来进行梯度上升，针对我们的过滤器激活损失：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Starts from a gray image with some noise
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从一个带有一些噪声的灰色图像开始
- en: ❷ Runs gradient ascent for 20 steps
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 进行20步的梯度上升
- en: 'Now that we have implemented the gradient ascent, we need to build a function
    that converts the tensor into a valid image. We will call it `deprocess_image(x``)`.
    Then we save the image on disk to view it:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了梯度上升，我们需要构建一个将张量转换为有效图像的函数。我们将称之为`deprocess_image(x)`。然后我们将图像保存到磁盘上以查看它：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '❶ Normalizes the tensor: centers on 0\. and ensures that std is 0.1'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 归一化张量：以0为中心，并确保std为0.1
- en: ❷ Clips to [0, 1]
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 截断到[0, 1]
- en: ❸ Converts to an RGB array
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 转换为RGB数组
- en: The result should be something like figure 9.8.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该类似于图9.8。
- en: '![](../Images/9-8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-8.png)'
- en: Figure 9.8 VGG16 layer `block1_conv1` visualized
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 VGG16层`block1_conv1`的可视化
- en: You can try to change the visualized filters to deeper layers in later blocks
    like `block2` and `block3` to see more defined features extracted as a result
    of the network recognizing patterns within patterns through its layers. In the
    highest layers (`block5_conv2`, `block5_conv3`) you will start to recognize textures
    similar to those found in the objects the network was trained to classify, such
    as feathers, eyes, and so on.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试将可视化的过滤器更改为后续块中的深层，例如`block2`和`block3`，以查看网络通过其层识别模式中的模式提取的更定义的特征。在最高层（`block5_conv2`，`block5_conv3`）中，你将开始识别与网络训练分类的对象中发现的纹理相似的纹理，例如羽毛、眼睛等。
- en: 9.2 DeepDream
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 DeepDream
- en: DeepDream was developed by Google researchers Alexander Mordvintsev et al. in
    2015.[3](#pgfId-1193345) It is an artistic image modification technique that creates
    dream-like, hallucinogenic images using CNNs, as shown in the example in figure
    9.9).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: DeepDream是由Google研究人员Alexander Mordvintsev等人于2015年开发的。[3](#pgfId-1193345) 它是一种艺术图像修改技术，使用CNN创建梦幻般的、致幻的图像，如图9.9所示的示例所示。
- en: '![](../Images/9-9.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-9.png)'
- en: Figure 9.9 DeepDream output image
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 DeepDream输出图像
- en: For comparison, the original input image is shown in figure 9.10\. The original
    is a scenic image from the ocean, containing two dolphins and other creatures.
    DeepDream merged both dolphins into one object and replaced one of the faces with
    what looks like a dog face. Other objects were also deformed in an artistic way,
    and the sea background has an edge-like texture.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，原始输入图像显示在图9.10中。原始图像是来自海洋的风景图像，包含两只海豚和其他生物。DeepDream将两只海豚合并成一个物体，并将其中一个面孔替换成看起来像狗脸的东西。其他物体也被以艺术的方式变形，海洋背景具有边缘状的纹理。
- en: '![](../Images/9-10.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-10.png)'
- en: Figure 9.10 DeepDream input image
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 DeepDream输入图像
- en: DeepDream quickly became an internet sensation, thanks to the trippy pictures
    it generates, full of algorithmic artifacts, bird feathers, dog faces, and eyes.
    These artifacts are byproducts of the fact that the DeepDream ConvNet was trained
    on ImageNet, where dog breeds and bird species are vastly overrepresented. If
    you tried another network that was pretrained on a dataset with a majority distribution
    of other objects, such as cars, you would see car features in your output image.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它生成的充满算法痕迹、鸟羽、狗脸和眼睛的奇异图片，DeepDream迅速成为互联网现象。这些痕迹是DeepDream ConvNet在ImageNet上训练的结果，在ImageNet中，狗的品种和鸟的种类被大量过度代表。如果你尝试另一个在具有多数分布的其他对象的数据集上预训练的网络，例如汽车，你将在输出图像中看到汽车特征。
- en: 'The project started as a fun experiment to run a CNN in reverse and visualize
    its activation maps using the same convolutional filter-visualization techniques
    explained in section 9.1: run a ConvNet in reverse, doing gradient ascent on the
    input in order to maximize the activation of a specific filter in an upper layer
    of the ConvNet. DeepDream uses this same idea, with a few alterations:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目最初是一个有趣的实验，旨在反向运行CNN并使用第9.1节中解释的相同的卷积滤波器可视化技术来可视化其激活图：反向运行卷积神经网络，对输入进行梯度上升，以最大化卷积神经网络上层特定滤波器的激活。DeepDream使用了这个相同的思想，但做了一些修改：
- en: Input image --In filter visualization, we don’t use an input image. We start
    from a blank image (or a slightly noisy one) and then maximize the filter activations
    of the convolutional layers to view their features. In DeepDream, we use an input
    image to the network because the goal is to print these visualized features on
    an image.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像--在滤波器可视化中，我们不使用输入图像。我们从一张空白图像（或稍微有点噪声的图像）开始，然后最大化卷积层的滤波器激活，以查看其特征。在DeepDream中，我们使用输入图像到网络，因为目标是将这些可视化的特征打印到图像上。
- en: Maximizing filters versus layers --In filter visualization, as the name implies,
    we only maximize activations of specific filters within the layer. But in DeepDream,
    we aim to maximize the activation of the entire layer to mix together a large
    number of features at once.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化滤波器与层--在滤波器可视化中，正如其名所示，我们只最大化层内特定滤波器的激活。但在DeepDream中，我们的目标是最大化整个层的激活，以一次混合大量特征。
- en: Octaves --In DeepDream, the input images are processed at different scales called
    octaves to improve the quality of the visualized features. This process will be
    explained next.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 八度音阶--在DeepDream中，输入图像以不同的尺度称为八度音阶进行处理，以提高可视化的特征质量。这个过程将在下一节中解释。
- en: 9.2.1 How the DeepDream algorithm works
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 DeepDream算法的工作原理
- en: 'Similar to the filter-visualization technique, DeepDream uses a pretrained
    network on a large dataset. The Keras library has many pretrained ConvNets available
    to use: VGG16, VGG19, Inception, ResNet, and so on. We can use any of these networks
    in the DeepDream implementation; we can even train a custom network on our own
    dataset and use it in the DeepDream algorithm. Intuitively, the choice of network
    and the data it is pretrained on will affect our visualizations because different
    ConvNet architectures result in different learned features; and, of course, different
    training datasets will create different features as well.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与过滤可视化技术类似，DeepDream使用在大数据集上预训练的网络。Keras库提供了许多可用的预训练卷积神经网络：VGG16、VGG19、Inception、ResNet等等。我们可以在DeepDream实现中使用这些网络中的任何一个；我们甚至可以在自己的数据集上训练一个自定义网络，并在DeepDream算法中使用它。直观地说，网络的选择以及它预训练的数据将影响我们的可视化，因为不同的卷积神经网络架构会导致不同的学习特征；当然，不同的训练数据集也会创建不同的特征。
- en: The creators of DeepDream used an Inception model because they found that in
    practice, it produces nice-looking dreams. So in this chapter, we will use the
    Inception v3 model. You are encouraged to try different models to observe the
    difference.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: DeepDream的创造者使用了Inception模型，因为他们发现实际上它产生的梦境看起来很美。所以在本章中，我们将使用Inception v3模型。我们鼓励你尝试不同的模型来观察差异。
- en: The overall idea with DeepDream is that we pass an input image through a pretrained
    neural network such as the Inception v3 model. At some layer, we calculate the
    gradient, which tells us how we should change the input image to maximize the
    value at this layer. We continue doing this for 10, 20, or 40 iterations until
    eventually, patterns start to emerge in the input image (figure 9.11).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DeepDream的整体思想是，我们将输入图像通过一个预训练的神经网络，如Inception v3模型。在某个层，我们计算梯度，它告诉我们如何改变输入图像以最大化该层的值。我们继续这样做，直到10、20或40次迭代，最终，输入图像中开始出现模式（图9.11）。
- en: '![](../Images/9-11.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-11.png)'
- en: Figure 9.11 DeepDream algorithm
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 DeepDream算法
- en: This works fine, except that if the pretrained network has been trained on fairly
    small image sizes, like ImageNet, then when our input image is large (say, 1000
    × 1000), the DeepDream algorithm will print a lot of small patterns in the image
    that look noisy rather than artistic. This happens because all the extracted features
    are small in size. To solve this problem, the DeepDream algorithm processes the
    input image at different scales called octaves.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这工作得很好，但是，如果预训练的网络是在相当小的图像尺寸上训练的，比如ImageNet，那么当我们的输入图像很大（比如1000 × 1000）时，DeepDream算法会在图像中打印出很多小的模式，看起来很嘈杂而不是艺术。这是因为所有提取的特征都很小。为了解决这个问题，DeepDream算法以不同的尺度称为八度音阶处理输入图像。
- en: 'Octave is just a fancy word for an interval. The idea is to apply the DeepDream
    algorithm on the input image through intervals. We first downscale the image several
    times into different scales. The number of scales is configurable, as you will
    see soon. For each interval, we do the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 八度只是一个表示区间的时髦词。想法是通过区间应用DeepDream算法于输入图像。我们首先将图像降级为几个不同的尺度。尺度数量是可配置的，你很快就会看到。对于每个区间，我们执行以下操作：
- en: 'Inject details: to avoid losing a lot of image details after each successive
    scale-up, we re-inject the lost details back into the image after each upscale
    process to create a blended image.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注入细节：为了避免在每次连续放大后丢失大量图像细节，我们在每次放大过程之后将丢失的细节重新注入到图像中，以创建混合图像。
- en: 'Apply the DeepDream algorithm: send the blended image through the DeepDream
    algorithm.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用DeepDream算法：将混合图像通过DeepDream算法。
- en: Upscale to the next interval.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 放大到下一个区间。
- en: As you can see in figure 9.12, we start with the large input image and then
    downscale two times to get a small image in octave 3\. For the first interval
    of applying DeepDream, we don’t need to do detail injection because the input
    image is the source image that hasn’t been upscaled before. We pass it through
    the DeepDream algorithm and then upscale the output. After upscaling, details
    are lost, which results in an increasingly blurry or pixelated image. This is
    why it is valuable to re-inject the image details from the input image in octave
    2 and then pass the blended image through the DeepDream algorithm. We apply the
    same process of upscale, detail injection, and DeepDream one more time to get
    the final result image. This process is run recursively for an identified number
    of iterations until we are satisfied with the output art.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如图9.12所示，我们从一个大输入图像开始，然后将其降级两次，以在3次方频域中得到一个小图像。在应用DeepDream的第一个区间，我们不需要进行细节注入，因为输入图像是尚未放大过的源图像。我们将其通过DeepDream算法，然后放大输出。放大后，细节丢失，导致图像越来越模糊或像素化。这就是为什么在2次方频域中重新注入输入图像的细节，然后将混合图像通过DeepDream算法非常有价值。我们再次应用放大、细节注入和DeepDream的过程，以获得最终的结果图像。这个过程以递归方式运行，直到我们对输出艺术满意为止。
- en: '![](../Images/9-12.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-12.png)'
- en: 'Figure 9.12 The DeepDream process: successive image downscales called octaves,
    detail re-injection, and then upscaling to the next octave'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 DeepDream过程：连续图像降级称为八度，细节重新注入，然后放大到下一个八度
- en: 'We set the DeepDream parameters as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将DeepDream参数设置为以下内容：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Number of scales
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 尺度数量
- en: ❷ Size ratio between scales. Each successive scale is larger than the previous
    one by a factor of 1.4 (40% larger).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 尺度之间的尺寸比。每个连续的尺度比前一个大1.4倍（即40%更大）。
- en: ❸ Number of iterations
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 迭代次数
- en: Now that you understand how the DeepDream algorithm works, let’s take a look
    at DeepDream in action using Keras.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了DeepDream算法的工作原理，让我们看看如何使用Keras实现DeepDream。
- en: 9.2.2 DeepDream implementation in Keras
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 Keras中的DeepDream实现
- en: 'The DeepDream implementation that we are going to implement is based on François
    Chollet’s code from the official Keras documentation ([https://keras.io/examples/
    generative/deep_dream/](https://keras.io/examples/generative/deep_dream/)) and
    his book, Deep Learning with Python. We’ll explain this code after adapting it
    to work on Jupyter Notebooks:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实现的DeepDream实现是基于François Chollet从官方Keras文档（[https://keras.io/examples/generative/deep_dream/](https://keras.io/examples/generative/deep_dream/））和他的书《Python深度学习》中的代码。我们将在此代码适应Jupyter
    Notebooks后解释此代码：
- en: '[PRE6]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Disables all training operations since we won’t be doing any training with
    the model
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 禁用所有训练操作，因为我们不会用该模型进行任何训练
- en: ❷ Downloads the pretrained Inception v3 model without its top part
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 下载预训练的Inception v3模型，不包括其顶部部分
- en: 'Now, we need to define a dictionary that specifies which layers are used to
    generate the dream. To do that, let’s print out the model summary to view all
    the layers and select the layers names:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义一个字典，指定用于生成梦境的层。为此，让我们打印出模型摘要以查看所有层并选择层名：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Inception v3 is very deep, and the summary printout is long. For simplicity,
    figure 9.13 shows a few layers of the network.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v3非常深，摘要打印很长。为了简单起见，图9.13显示了网络的一些层。
- en: 'The exact layers you choose and their contribution to the final loss have an
    important influence on the visuals you can produce in the dream image, so you
    want to make these parameters easily configurable. To define the layers that we
    want to contribute to the dream creation, we create a dictionary with the layer
    names and their respective weights. The larger the weight of the layer, the higher
    its level of contribution to the dream:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的精确层及其对最终损失的贡献对你在梦境图像中产生的视觉效果有重要影响，因此你希望这些参数易于配置。为了定义我们想要贡献于梦境创建的层，我们创建一个包含层名称及其相应权重的字典。层的权重越大，其对梦境的贡献级别越高：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/9-13.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-13.png)'
- en: Figure 9.13 Part of the Inception v3 model summary
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 Inception v3模型摘要的一部分
- en: 'These are the names of the layers for which we try to maximize activations.
    Note that when you change the layers in this dictionary, you will produce different
    dreams, so you are encouraged to experiment with different layers and their corresponding
    weights. For this project, we will start with a somewhat arbitrary configuration
    by adding four layers to our dictionary and their weights: `mixed2`, `mixed3`,
    `mixed4`, and `mixed5`. As a guide, remember from earlier in this chapter that
    lower layers can be used to generate edges and geometric patterns, while high
    layers result in the injection of trippy visual patterns, including hallucinations
    of dogs, cats, and birds.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们尝试最大化激活的层名称。请注意，当你更改此字典中的层时，你将产生不同的梦境，因此我们鼓励你尝试不同的层及其相应的权重。对于这个项目，我们将通过向我们的字典添加四个层及其权重（`mixed2`、`mixed3`、`mixed4`和`mixed5`）来从一个相对任意的配置开始。作为一个指导，记得从本章前面的内容中，较低层可以用来生成边缘和几何图案，而高层则导致注入令人着迷的视觉图案，包括狗、猫和鸟的幻觉。
- en: 'Now, let’s define a tensor that contains the loss: the weighted sum of the
    L2 norm of the activations of the layers:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个包含损失的张量：各层激活的L2范数的加权总和：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Dictionary that maps layer names to layer instances
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将层名称映射到层实例的字典
- en: ❷ Defines the loss by adding layer contributions to this scalar variable
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过添加层贡献到这个标量变量来定义损失
- en: ❸ Adds the L2 norm of the features of a layer to the loss. We avoid border artifacts
    by only involving non-border pixels in the loss.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将层的特征L2范数添加到损失中。我们通过只涉及非边界像素来避免边界伪影。
- en: 'Next, we compute the loss, which is the quantity we will try to maximize during
    the gradient ascent process. In filter visualization, we wanted to maximize the
    value of a specific filter in a specific layer. Here, we will simultaneously maximize
    the activation of all filters in a number of layers. Specifically, we will maximize
    a weighted sum of the L2 norm of the activations of a set of high-level layers:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算损失，这是我们将在梯度上升过程中尝试最大化的量。在滤波器可视化中，我们希望最大化特定层中特定滤波器的值。在这里，我们将同时最大化多个层中所有滤波器的激活。具体来说，我们将最大化一组高级层激活的L2范数的加权总和：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Tensors that holds the generated image
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 包含生成图像的张量
- en: ❷ Computes the gradients of the dream image with regard to the loss
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算梦境图像相对于损失的梯度
- en: ❸ Normalizes the gradients
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 归一化梯度
- en: ❹ Sets up a Keras function to retrieve the value of the loss and gradients given
    an input image
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置一个Keras函数以检索给定输入图像的损失和梯度值
- en: ❺ Runs the gradient ascent process for a number of iterations
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 运行梯度上升过程多次迭代
- en: 'Now we are ready to develop our DeepDream algorithm. The process is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开发我们的DeepDream算法。过程如下：
- en: Load the input image.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载输入图像。
- en: Define the number of scales, from smallest to largest.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义从最小到最大的尺度数量。
- en: Resize the input image to the smallest scale.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入图像调整到最小尺寸。
- en: 'For each scale, start with the smallest and apply the following:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个尺度，从最小开始，应用以下：
- en: Gradient ascent function
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度上升函数
- en: Upscaling to the next scale
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上采样到下一个尺度
- en: Re-injecting details that were lost during the upscale process
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上采样过程中重新注入丢失的细节
- en: Stop the process when we are back to the original size.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们回到原始大小时停止该过程。
- en: 'First, we set the algorithm parameters:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置算法参数：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Gradient ascent step size
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 梯度上升步长
- en: ❷ Number of scales at which we run gradient ascent
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行梯度上升的尺度数量
- en: ❸ Size ratio between scales
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 尺度之间的尺寸比
- en: ❹ Number of iterations
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 迭代次数
- en: Note that playing with these hyperparameters will allow you to achieve new effects.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，调整这些超参数将允许你实现新的效果。
- en: Let’s define the input image that we want to use to create our dream. For this
    example, I downloaded an image of the Golden Gate Bridge in San Francisco (see
    figure 9.14); feel free to use an image of your own. Figure 9.15 shows the DeepDream
    output image.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们想要用来创建梦境的输入图像。在这个例子中，我下载了一张旧金山金门大桥的图片（见图9.14）；你也可以使用你自己的图片。图9.15显示了DeepDream的输出图像。
- en: '![](../Images/9-14.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-14.png)'
- en: Figure 9.14 Example input image
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 示例输入图像
- en: '![](../Images/9-15.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-15.png)'
- en: Figure 9.15 DeepDream output
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 DeepDream输出
- en: 'Here’s the Keras code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Keras代码：
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Defines the path to the input image
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义输入图像的路径
- en: ❷ Saves the result to disk
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将结果保存到磁盘
- en: 9.3 Neural style transfer
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 神经风格迁移
- en: 'So far, we have learned how to visualize specific filters in a network. We
    also learned how to manipulate features of an input image to create dream-like
    hallucinogenic images using the DeepDream algorithm. In this section, we explore
    a new type of artistic image that ConvNets can create using neural style transfer
    : the technique of transferring the style from one image to another.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何在网络中可视化特定的过滤器。我们还学习了如何使用DeepDream算法操纵输入图像的特征来创建梦幻般的幻觉图像。在本节中，我们将探索一种新的艺术图像类型，即卷积神经网络可以通过神经风格迁移来创建的：将一种图像的风格转移到另一种图像的技术。
- en: The goal of the neural style transfer algorithm is to take the style of an image
    (style image) and apply it to the content of another image (content image). Style
    in this context means texture, colors, and other visual patterns in the image.
    And content is the higher-level macrostructure of the image. The result is a combined
    image that contains both the content of the content image and the style of the
    style image.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移算法的目标是将一张图像的风格（风格图像）应用到另一张图像的内容（内容图像）上。这里的风格指的是图像中的纹理、颜色和其他视觉模式。而内容则是图像的高级宏观结构。结果是包含内容图像的内容和风格图像的风格的综合图像。
- en: For example, let’s look at figure 9.16\. The objects in the content image (like
    dolphins, fish, and plants) are kept in the combined image but with the specific
    texture of the style image (blue and yellow brushstrokes).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看图9.16。内容图像中的对象（如海豚、鱼和植物）在综合图像中保持不变，但具有风格图像的特定纹理（蓝色和黄色的笔触）。
- en: '![](../Images/9-16.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-16.png)'
- en: Figure 9.16 Example of neural style transfer
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 神经风格迁移示例
- en: The idea of neural style transfer was introduced by Leon A. Gatys et al. in
    2015.[4](#pgfId-1193577) The concept of style transfer, which is tightly related
    to texture generation, had a long history in the image-processing community prior
    to that; but as it turns out, the DL-based implementations of style transfer offer
    results unparalleled by what had been previously achieved with traditional CV
    techniques, and they triggered an amazing renaissance in creative CV applications.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移的想法是由Leon A. Gatys等人于2015年提出的。[4](#pgfId-1193577) 在此之前，与纹理生成紧密相关的风格迁移概念在图像处理社区中有着悠久的历史；但事实证明，基于深度学习的风格迁移实现提供了传统计算机视觉技术所无法比拟的结果，并引发了一场在创造性计算机视觉应用中的惊人复兴。
- en: Among the different neural network techniques that create art (like DeepDream),
    style transfer is the closest to my heart. DeepDream can create cool hallucination-like
    images, but it can be disturbing sometimes. Plus, as a DL engineer, it is not
    easy to intentionally create a specific piece of art that you have in your mind.
    Style transfer, on the other hand, can use an artistic engineer to mix the content
    that you want from an image with your favorite painting to create something that
    you have imagined. It is a really cool technique that, if used by an artist engineer,
    can be used to create beautiful art on par with that produced by professional
    painters.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建艺术的不同神经网络技术（如DeepDream）中，风格迁移是我最喜欢的一种。DeepDream可以创建酷炫的幻觉图像，但有时可能会让人感到不安。此外，作为一个深度学习工程师，有意创造你心中所想的特定艺术品并不容易。另一方面，风格迁移可以使用艺术工程师将你从图像中想要的内容与你最喜欢的画作混合，创造出你想象中的东西。这是一个非常酷的技术，如果由艺术工程师使用，可以创造出与专业画家作品相媲美的美丽艺术。
- en: 'The main idea behind implementing style transfer is the same as the one central
    to all DL algorithms, as explained in chapter 2: we first define a loss function
    to define what we aim to achieve, and then we work on optimizing this function.
    In style-transfer problems, we know what we want to achieve: conserving the content
    of the original image while adopting the style of the reference image. Now all
    we need to do is to define both content and style in a mathematical representation,
    and then define an appropriate loss function to minimize.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 实现风格迁移背后的主要思想与第2章中解释的所有深度学习算法的核心思想相同：我们首先定义一个损失函数来定义我们想要实现的目标，然后我们致力于优化这个函数。在风格迁移问题中，我们知道我们想要实现的目标是：在保留原始图像内容的同时，采用参考图像的风格。现在我们所需做的就是以数学形式定义内容和风格，然后定义一个合适的损失函数来最小化它。
- en: 'The key notion in defining the loss function is to remember that we want to
    preserve content from one image and style from another:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 定义损失函数的关键思想是要记住，我们想要保留一张图像的内容和另一张图像的风格：
- en: Content loss --Calculate the loss between the content image and the combined
    image. Minimizing this loss means the combined image will have more content from
    the original image.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容损失--计算内容图像和组合图像之间的损失。最小化这个损失意味着组合图像将包含更多来自原始图像的内容。
- en: Style loss --Calculate the loss in style between the style image and the combined
    image. Minimizing this loss means the combined image will have style similar to
    the style image.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风格损失--计算风格图像和组合图像之间的风格损失。最小化这个损失意味着组合图像将具有与风格图像相似的风格。
- en: Noise loss --This is called the total variation loss. It measures the noise
    in the combined image. Minimizing this loss creates an image with a higher spatial
    smoothness.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声损失--这被称为总变差损失。它衡量了组合图像中的噪声。最小化这个损失会创建一个具有更高空间平滑性的图像。
- en: 'Here is the equation of the total loss:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是总损失的方程：
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: NOTE Gatys et al. (2015) on transfer learning does not include the total variation
    loss. After experimentation, the researchers found that the network generated
    better, more aesthetically-pleasing style transfers when they encouraged spatial
    smoothness across the output image.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Gatys等人（2015）在迁移学习的研究中不包括总变差损失。经过实验，研究人员发现，当他们在输出图像中鼓励空间平滑性时，生成的网络在风格迁移方面表现更好，更具有审美价值。
- en: Now that we have a big-picture idea of how the neural style transfer algorithm
    works, we are going to dive deeper into each type of loss to see how it is derived
    and coded in Keras. We will then understand how to train a neural style transfer
    network to minimize the `total_loss` function that we just defined.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对神经网络风格迁移算法的大致工作原理有了了解，我们将更深入地探讨每种类型的损失，以了解它是如何推导和编码在Keras中的。然后我们将了解如何训练一个神经网络风格迁移网络，以最小化我们刚刚定义的`total_loss`函数。
- en: 9.3.1 Content loss
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 内容损失
- en: 'The content loss measures how different two images are in terms of subject
    matter and the overall placement of content. In other words, two images that contain
    similar scenes should have a smaller loss value than two images that contain completely
    different scenes. Image subject matter and content placement are measured by scoring
    images based on higher-level feature representations in the ConvNet, such as dolphins,
    plants, and water. Identifying these features is the whole premise behind deep
    neural networks: these networks are trained to extract the content of an image
    and learn the higher-level features at the deeper layers by recognizing patterns
    in simpler features from the previous layers. With that said, we need a deep neural
    network that has been trained to extract the features of the content image so
    that we can tap into a deep layer of the network to extract high-level features.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失衡量了两个图像在主题和内容整体布局方面的差异。换句话说，包含相似场景的两个图像应该具有较小的损失值，而包含完全不同场景的两个图像应该具有较大的损失值。图像的主题和内容布局是通过根据ConvNet中的高级特征表示对图像进行评分来衡量的，例如海豚、植物和水。识别这些特征是深度神经网络背后的整个前提：这些网络被训练来提取图像的内容，并通过识别前一层简单特征中的模式来学习更深层的更高层次特征。因此，我们需要一个经过训练以提取内容图像特征的深度神经网络，以便我们可以挖掘网络的深层来提取高级特征。
- en: 'To calculate the content loss, we measure the mean squared error between the
    output for the content image and the combined image. By trying to minimize this
    error, the network tries to add more content to the combined image to make it
    more and more similar to the original content image:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算内容损失，我们测量内容图像和组合图像输出的均方误差。通过尝试最小化这个误差，网络试图向组合图像添加更多内容，使其越来越接近原始内容图像：
- en: Content loss = 1/2 Σ[content(original_image) - content(combined_image)]²
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失 = 1/2 Σ[内容(原始图像) - 内容(组合图像)]²
- en: Minimizing the content loss function ensures that we preserve the content of
    the original image and create it in the combined image.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化内容损失函数确保我们在组合图像中保留了原始图像的内容。
- en: To calculate the content loss, we feed both the content and style images into
    a pretrained network and select a deep layer from which to extract high-level
    features. We then calculate the mean squared error between both images. Let’s
    see how we calculate the content loss between two images in Keras.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算内容损失，我们将内容和风格图像输入到一个预训练的网络中，并从其中选择一个深层层来提取高级特征。然后我们计算两个图像之间的均方误差。让我们看看如何在Keras中计算两个图像之间的内容损失。
- en: NOTE The code snippets in this section are adapted from the neural style transfer
    example in the official Keras documentation ([https://keras.io/examples/ generative/neural_style_transfer/](https://keras.io/examples/generative/neural_style_transfer/)).
    If you want to re-create this project and experiment with different parameters,
    I suggest that you work from Keras’ Github repository as a starting point ([http://mng.bz/GVzv](http://mng.bz/GVzv))
    or run the adapted code available for download with this book.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节中的代码片段是从官方Keras文档中的神经风格迁移示例改编的（[https://keras.io/examples/generative/neural_style_transfer/](https://keras.io/examples/generative/neural_style_transfer/)）。如果你想要重新创建这个项目并尝试不同的参数，我建议你从Keras的Github仓库作为起点（[http://mng.bz/GVzv](http://mng.bz/GVzv)）或运行与此书一起提供的可下载的改编代码。
- en: 'First, we define two Keras variables to hold the content image and style image.
    And we create a placeholder tensor that will contain the generated combined image:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义两个Keras变量来保存内容图像和风格图像，并创建一个占位符张量，它将包含生成的组合图像：
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Paths to the content and style images
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 内容和风格图像的路径
- en: ❷ Gets tensor representations of our images
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取我们图像的张量表示
- en: 'Now, we concatenate the three images into one input tensor and feed it to the
    VGG19 neural network. Note that when we load the VGG19 model, we set the `include_top`
    parameter to `False` because we don’t need to include the classification fully
    connected layers for this task. This is because we are only interested in the
    feature-extraction part of the network:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将三张图像连接成一个输入张量，并将其输入到VGG19神经网络中。请注意，当我们加载VGG19模型时，我们将`include_top`参数设置为`False`，因为我们不需要包括用于此任务的分类全连接层。这是因为我们只对网络的特征提取部分感兴趣：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Combines the three images into a single Keras tensor
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将三张图像合并成一个单一的Keras张量
- en: ❷ Builds the VGG19 network with our three images as input. The model will be
    loaded with pretrained ImageNet weights.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用我们的三张图像作为输入构建VGG19网络。该模型将加载预训练的ImageNet权重。
- en: 'Similar to what we did in section 9.1, we now select the network layer we want
    to use to calculate the content loss. We wanted to choose a deep layer to make
    sure it contains higher-level features of the content image. If you choose an
    earlier layer of the network (like block 1 or block 2), the network won’t be able
    to transfer the full content from the original image because the earlier layers
    extract low-level features like lines, edges, and blobs. In this example, we choose
    the second convolutional layer in block 5 (`block5_conv2`):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在第9.1节中所做的一样，我们现在选择我们想要用来计算内容损失的神经网络层。我们想要选择一个深层层以确保它包含内容图像的高级特征。如果你选择网络的早期层（如块1或块2），网络将无法从原始图像中完全传递内容，因为早期层提取的是低级特征，如线条、边缘和块。在这个例子中，我们选择了块5中的第二个卷积层（`block5_conv2`）：
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Gets the symbolic outputs of each key layer (we gave them unique names)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取每个关键层的符号输出（我们给了它们独特的名称）
- en: 'Now we can extract the features from the layer that we chose from the input
    tensor:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以从输入张量中提取我们选择的层的特征：
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we create the `content_loss` function that calculates the mean squared
    error between the content image and the combined image. We create an auxiliary
    loss function designed to preserve the features of the `content_image` and transfer
    it to the `combined-image`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建`content_loss`函数，该函数计算内容图像和合成图像之间的均方误差。我们创建一个辅助损失函数，旨在保留`content_image`的特征并将其转移到`combined-image`：
- en: '[PRE18]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Mean square error function between the content image output and the combined
    image
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 内容图像输出与合成图像之间的均方误差函数
- en: ❷ content_loss is scaled by a weighting parameter.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 内容损失通过权重参数进行缩放。
- en: Weighting parameters
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 权重参数
- en: 'In this code implementation, you will see the following weighting parameters:
    `content` `_weight`, `style_weight`, and `total_variation_weight`. These are scaling
    parameters set by us as an input to the network as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码实现中，您将看到以下权重参数：`content_weight`、`style_weight` 和 `total_variation_weight`。这些是我们作为网络输入设置的缩放参数，如下所示：
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These weight parameters describe the importance of content, style, and noise
    in our output image. For example, if we set `style_weight` `=` `100` and `content_weight`
    `=` `1`, we are implying that we are willing to sacrifice a bit of the content
    for a more artistic style transfer. Also, a higher `total_variation_weight` implies
    higher spatial smoothness.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重参数描述了内容、风格和噪声在我们输出图像中的重要性。例如，如果我们设置`style_weight` `=` `100` 和 `content_weight`
    `=` `1`，这意味着我们愿意为了更艺术化的风格迁移而牺牲一点内容。此外，更高的`total_variation_weight`意味着更高的空间平滑度。
- en: 9.3.2 Style loss
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 风格损失
- en: As we mentioned before, style in this context means texture, colors, and other
    visual patterns in the image.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，在这个上下文中，风格指的是图像中的纹理、颜色和其他视觉模式。
- en: Multiple layers to represent style features
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多层表示风格特征
- en: Defining the style loss is a little more challenging than what we did with the
    content loss. In the content loss, we cared only about the higher-level features
    that are extracted at the deeper levels, so we only needed to choose one layer
    from the VGG19 network to preserve its features. In style loss, on the other hand,
    we want to choose multiple layers because we want to obtain a multi-scale representation
    of the image style. We want to capture the image style at lower-level layers,
    mid-level layers, and higher-level layers. This allows us to capture the texture
    and style of our style image and exclude the global arrangement of objects in
    the content image.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 定义风格损失比我们之前处理的内容损失更具挑战性。在内容损失中，我们只关心在更深层次提取的高级特征，所以我们只需要从VGG19网络中选择一层来保留其特征。而在风格损失中，另一方面，我们想要选择多个层，因为我们想要获得图像风格的多个尺度表示。我们想要捕捉低级层、中级层和高级层的图像风格。这使我们能够捕捉风格图像的纹理和风格，并排除内容图像中对象的全球排列。
- en: Gram matrix to measure jointly activated feature maps
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语法矩阵用于测量联合激活的特征图
- en: The gram matrix is a method that is used to numerically measure how much two
    feature maps are jointly activated. Our goal is to build a loss function that
    captures the style and texture of multiple layers in a CNN. To do that, we need
    to compute the correlations between the activation layers in our CNN. This correlation
    can be captured by computing the gram matrix--the feature-wise outer product--between
    the activations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 语法矩阵是一种用于数值测量两个特征图联合激活程度的方法。我们的目标是构建一个损失函数，以捕捉CNN中多个层的风格和纹理。为此，我们需要计算我们CNN中激活层之间的相关性。这种相关性可以通过计算激活之间的语法矩阵——特征外积——来捕捉。
- en: 'To calculate the gram matrix of the feature map, we flatten the feature map
    and calculate the dot product:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算特征图的语法矩阵，我们需要将特征图展平并计算点积：
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s build the `style_loss` function. It calculates the gram matrix for a
    set of layers throughout the network for both the style and combined images. It
    then compares the similarities of style and texture between them by calculating
    the sum of squared errors:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建`style_loss`函数。它计算网络中一系列层的风格和合成图像的语法矩阵。然后，通过计算平方误差之和来比较它们之间风格和纹理的相似性：
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this example, we are going to calculate the style loss over five layers:
    the first convolutional layer in each of the five blocks of the VGG19 network
    (note that if you change the feature layers, the network will preserve different
    styles):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将计算五个层的风格损失：VGG19网络中每个块的第一个卷积层（注意，如果您更改特征层，网络将保留不同的风格）：
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we loop through these `feature_layers` to calculate the style loss:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们遍历这些 `feature_layers` 来计算风格损失：
- en: '[PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Scales the style loss by a weighting parameter and the number of layers over
    which the style loss is calculated
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将风格损失乘以加权参数和计算风格损失所涉及的层数
- en: During training, the network works on minimizing the loss between the style
    of the output image (combined image) and the style of the input style image. This
    forces the style of the combined image to correlate with the style image.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，网络通过最小化输出图像（组合图像）的风格与输入风格图像的风格之间的损失来工作。这迫使组合图像的风格与风格图像相关联。
- en: 9.3.3 Total variance loss
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 总方差损失
- en: The total variance loss is the measure of noise in the combined image. The network’s
    goal is to minimize this loss function in order to minimize the noise in the output
    image.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 总方差损失是组合图像中噪声的度量。网络的目标是最小化这个损失函数，以最小化输出图像中的噪声。
- en: 'Let’s create the `total_variation_loss` function that calculates how noisy
    an image is. This is what we are going to do:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个 `total_variation_loss` 函数来计算图像的噪声程度。这是我们将要做的：
- en: Shift the image one pixel to the right, and calculate the sum of the squared
    error between the transferred image and the original.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像向右移动一个像素，并计算转移图像与原始图像之间的平方误差之和。
- en: Repeat step 1, this time shifting the image one pixel down.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1，这次将图像向下移动一个像素。
- en: 'The sum of these two terms (`a` and `b`) is the total variance loss:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个项（`a` 和 `b`）的和是总方差损失：
- en: '[PRE24]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Scales the total variance loss by the weighting parameter
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将总方差损失乘以加权参数
- en: 'Finally, we calculate the overall loss of our problem, which is the sum of
    the content, style, and total variance losses:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算我们问题的整体损失，这是内容、风格和总方差损失的加和：
- en: '[PRE25]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 9.3.4 Network training
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 网络训练
- en: 'Now that we have defined the total loss function for our problem, we can run
    the GD optimizer to minimize this loss function. First we create an object class
    `Evaluator` that contains methods that calculate the overall loss, as described
    previously, and gradients of the loss with respect to the input image:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们问题的总损失函数，我们可以运行 GD 优化器来最小化这个损失函数。首先我们创建一个对象类 `Evaluator`，它包含计算整体损失的方法，如前所述，以及相对于输入图像的损失梯度：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we use the methods in our evaluator class in the training process. To
    minimize the total loss function, we use the SciPy ([https://scipy.org/scipylib](https://scipy.org/scipylib))
    based optimization method `scipy.optimize.fmin_l_bfgs_b`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在训练过程中使用我们的评估器类中的方法。为了最小化总损失函数，我们使用基于 SciPy 的优化方法 `scipy.optimize.fmin_l_bfgs_b`：
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Trains for 1,000 iterations
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练 1,000 次迭代
- en: ❷ The training process is initialized with content_image as the first iteration
    of the combined image.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将内容图像初始化为组合图像的第一迭代。
- en: ❸ Runs scipy-based optimization (L-BFGS) over the pixels of the generated image
    to minimize total_loss.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在生成的图像的像素上运行基于 SciPy 的优化（L-BFGS）以最小化总损失。
- en: ❹ Saves the current generated image
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 保存当前生成的图像
- en: 'TIP When training your own neural style transfer network, keep in mind that
    content images that do not require high levels of detail work better and are known
    to create visually appealing or recognizable artistic images. In addition, style
    images that contain a lot of textures are better than flat style images: flat
    images (like a white background) will not produce aesthetically appealing results
    because there is not much texture to transfer.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：当训练自己的神经风格迁移网络时，请注意，不需要高细节水平的内容图像效果更好，并且已知可以创建视觉上吸引人或有辨识度的艺术图像。此外，包含大量纹理的风格图像比平面风格图像更好：平面图像（如白色背景）不会产生美观的结果，因为没有多少纹理可以转移。
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: CNNs learn the information in the training set through successive filters. Each
    layer of the network deals with features at a different level of abstraction,
    so the complexity of the features generated depends on the layer’s location in
    the network. Earlier layers learn low-level features; the deeper the layer is
    in the network, the more identifiable the extracted features are.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 通过连续的过滤器学习训练集中的信息。网络的每一层处理不同抽象级别的特征，因此生成的特征复杂度取决于层在网络中的位置。早期层学习低级特征；层在网络中的深度越深，提取的特征越容易识别。
- en: Once a network is trained, we can run it in reverse to adjust the original image
    slightly so that a given output neuron (such as the one for faces or certain animals)
    yields a higher confidence score. This technique can be used for visualizations
    to better understand the emergent structure of the neural network and is the basis
    for the DeepDream concept.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦网络被训练，我们可以反向运行它来稍微调整原始图像，以便给定的输出神经元（如面部或某些动物的面部）产生更高的置信度分数。这种技术可用于可视化，以更好地理解神经网络的涌现结构，并且是DeepDream概念的基础。
- en: DeepDream processes the input image at different scales called octaves. We pass
    each scale, re-inject image details, pass it through the DeepDream algorithm,
    and then upscale the image for the next octave.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDream在称为八度的不同尺度上处理输入图像。我们传递每个尺度，重新注入图像细节，通过DeepDream算法处理，然后将图像放大以供下一个八度使用。
- en: The DeepDream algorithm is similar to the filter-visualization algorithm. It
    runs the ConvNet in reverse to generate output based on the representations extracted
    by the network.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDream算法与滤波器可视化算法类似。它反向运行ConvNet，根据网络提取的表示生成输出。
- en: DeepDream differs from filter-visualization in that it needs an input image
    and maximizes the entire layer, not specific filters within the layer. This allows
    DeepDream to mix together a large number of features at once.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDream与滤波器可视化不同，因为它需要一个输入图像，并最大化整个层，而不是层内的特定滤波器。这使得DeepDream能够同时混合大量特征。
- en: DeepDream is not specific to images--it can be used for speech, music, and more.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepDream不仅限于图像——它还可以用于语音、音乐等。
- en: Neural style transfer is a technique that trains the network to preserve the
    style (texture, color, patterns) of the style image and preserve the content of
    the content image. The network then creates a new combined image that combines
    the style of the style image and the content from the content image.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经风格迁移是一种技术，它训练网络保留风格图像的风格（纹理、颜色、图案）并保留内容图像的内容。然后，网络创建一个新的组合图像，该图像结合了风格图像的风格和内容图像的内容。
- en: Intuitively, if we minimize the content, style, and variation losses, we get
    a new image that contains low variance in content and style from the content and
    style images, respectively, and low noise.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观地说，如果我们最小化内容、风格和变化损失，我们将得到一个新的图像，其中包含来自内容图像和风格图像的内容和风格的低方差，以及低噪声。
- en: Different values for content weight, style weight, and total variation weight
    will give you different results.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容权重、风格权重和总变化权重的不同值将给出不同的结果。
- en: '* * *'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '1.Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. “Visualizing
    Higher-Layer Features of a Deep Network.” University of Montreal 1341 (3): 1\.
    2009\. [http://mng.bz/yyMq](http://mng.bz/yyMq).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 1.杜米特鲁·埃尔汗，约书亚·本吉奥，阿隆·库维尔，和帕斯卡尔·文森特。“可视化深度网络的更高层特征。”蒙特利尔大学1341（3）：1。2009年。[http://mng.bz/yyMq](http://mng.bz/yyMq)。
- en: 2.François Chollet, “How convolutional neural networks see the world,” The Keras
    Blog, 2016, [https://blog .keras.io/category/demo.html](https://blog.keras.io/category/demo.html).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 2.弗朗索瓦·肖莱特，“卷积神经网络如何看世界”，Keras博客，2016年，[https://blog.keras.io/category/demo.html](https://blog.keras.io/category/demo.html)。
- en: 3.Alexander Mordvintsev, Christopher Olah, and Mike Tyka, “Deepdream--A Code
    Example for Visualizing Neural Networks,” Google AI Blog, 2015, [http://mng.bz/aROB](http://mng.bz/aROB).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 3.亚历山大·莫尔维茨夫，克里斯托弗·奥拉，和迈克·泰卡，“Deepdream——可视化神经网络的代码示例”，谷歌AI博客，2015年，[http://mng.bz/aROB](http://mng.bz/aROB)。
- en: 4.Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “A Neural Algorithm
    of Artistic Style,” 2015, [http:// arxiv.org/abs/1508.06576](http://arxiv.org/abs/1508.06576).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 4.莱昂·A·加蒂斯，亚历山大·S·埃克，和马蒂亚斯·贝特格，“艺术风格的神经网络算法”，2015年，[http://arxiv.org/abs/1508.06576](http://arxiv.org/abs/1508.06576)。
