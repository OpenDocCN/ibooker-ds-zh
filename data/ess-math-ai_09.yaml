- en: Chapter 9\. Graph Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。图模型
- en: 'Graphs, diagrams, and networks are all around us: Cities and roadmaps, airports
    and connecting flights, electrical networks, the power grid, the world wide web,
    molecular networks, biological networks such as our nervous system, social networks,
    terrorist organization networks, schematic represenations of mathematical models,
    artificial neural networks, and many, many others. They are easily recognizable,
    with distinct nodes representing some entities that we care for, which are then
    connected by directed or undirected edges, idicating the presence of some relationship
    between the connected nodes.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 图表和网络无处不在：城市和路线图，机场和连接航班，电力网络，电网，全球网络，分子网络，生物网络，如我们的神经系统，社交网络，恐怖组织网络，数学模型的图表表示，人工神经网络等等。它们很容易被识别，具有明显的节点代表我们关心的某些实体，然后由有向或无向边连接，表示连接节点之间存在某种关系。
- en: Data that has a natural graph structure is better understood by a mechanism
    that exploits and preserves that structure, building functions that operate directly
    on graphs (however they are mathematically represented), as opposed to feeding
    graph data into machine learning models that artificially reshape it before analyzing
    it. This inevitably leads to loss of valuable information. This is the same reason
    convolutional neural networks are successful with image data, recurrent neural
    networks are successful with sequential data, and so on.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具有自然图结构的数据通过利用和保留该结构的机制更容易理解，构建直接在图上操作的函数（无论它们如何在数学上表示），而不是将图数据输入到机器学习模型中，然后再分析它。这不可避免地导致有价值信息的丢失。这也是卷积神经网络在图像数据上成功的原因，循环神经网络在序列数据上成功的原因，等等。
- en: 'Graph based models are very attractive for data scientists and engineers: Graph
    structures offer a flexibility that is not afforded in spaces with a fixed underlying
    coordinate system such as in Euclidean spaces, or in relational databases, where
    the data along with its features is forced to adhere to a rigid and predetermined
    form. Moreover, graphs are the natural setting that allows us to investigate the
    relationships between the points in a data set. So far, our machine learning models
    consumed data represented as isolated data points. Graph models, on the other
    hand, consume isolated data points, *along with the connections between them*,
    allowing for deeper understanding and more expressive models.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的模型对数据科学家和工程师非常有吸引力：图结构提供了一种在具有固定底层坐标系统的空间中无法提供的灵活性，例如在欧几里得空间或关系数据库中，数据及其特征被迫遵循严格和预定的形式。此外，图是一种自然的设置，允许我们研究数据集中点之间的关系。到目前为止，我们的机器学习模型消耗的数据表示为孤立的数据点。另一方面，图模型消耗孤立的数据点，*以及它们之间的连接*，从而实现更深入的理解和更具表现力的模型。
- en: 'The human brain naturally internalizes graphical structures: It is able to
    model entities and their connections. It is also flexible enough to generate new
    networks, or expand and enhance existing ones, for example, when city planning,
    project planning, or when continuously updating transit networks. Moreover, humans
    can transition from natural language text to graph models and vice versa seamlessly:
    When we read something new, we find it natural to formulate a graphical represenation
    in order to better comprehend it or to illustrate it to other people. Conversely,
    when we see graph schematics, we are able to describe it via natural language.
    There are currently models that generate natural language text based on knowledge
    graphs and vice versa. This is called reasoning over knowledge graphs.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑自然内化了图形结构：它能够对实体及其连接进行建模。它也足够灵活，可以生成新的网络，或者扩展和增强现有的网络，例如在城市规划、项目规划或持续更新交通网络时。此外，人类可以无缝地从自然语言文本过渡到图模型，反之亦然：当我们阅读新内容时，我们自然会制定图形表示以更好地理解它或向其他人说明。相反，当我们看到图表时，我们能够通过自然语言描述它。目前有一些模型可以基于知识图生成自然语言文本，反之亦然。这被称为在知识图上推理。
- en: 'At this point we are pretty comfortable with the building blocks of neural
    networks, along with the types of data and tasks they are usually suited for:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们对神经网络的构建模块以及它们通常适用于的数据类型和任务已经非常熟悉：
- en: Multilayer perceptron or fully connected neural network ([Chapter 4](ch04.xhtml#ch04))
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多层感知器或全连接神经网络（[第4章](ch04.xhtml#ch04)）
- en: Convolutional layers ([Chapter 5](ch05.xhtml#ch05))
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积层（[第5章](ch05.xhtml#ch05)）
- en: Recurrent layers ([Chapter 7](ch07.xhtml#ch07))
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环层（[第7章](ch07.xhtml#ch07)）
- en: Encoder-decoder components ([Chapter 7](ch07.xhtml#ch07))
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器-解码器组件（[第7章](ch07.xhtml#ch07)）
- en: Adversarial components and two player zero sum games ([Chapter 8](ch08.xhtml#ch08))
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对抗组件和两人零和博弈（[第8章](ch08.xhtml#ch08)）
- en: Variational components ([Chapter 8](ch08.xhtml#ch08))
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变分组件（[第8章](ch08.xhtml#ch08)）
- en: 'The main tasks are mostly: Classification, regression, clustering, coding and
    decoding, or new data generation, where the model learns the joint probability
    distribution of the features of the data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 主要任务主要是：分类、回归、聚类、编码和解码，或新数据生成，其中模型学习数据特征的联合概率分布。
- en: We are also familiar with the fact that we can mix and match some of the components
    of neural networks in order to construct new models that are geared towards specific
    tasks. The good news is that graph neural networks use the same exact ingredients,
    so we do not need to go over any new machine learning concepts in this chapter.
    Once we undertsand how to mathematically represent graph data along with its features,
    in a way that can be fed into a neural network, either for analysis or for new
    network (graph) data generation, we are good to go. We will therefore avoid going
    down a maze of surveys for all the graph neural networks out there. Instead, we
    focus on the simple mathematical formulation, popular applications, common tasks
    for graph models, available data sets, and model evaluation methods. Our goal
    is to develop a strong intuition for the workings of the subject. The main challenge
    is, yet again, lowering the dimensionality of the problem in a way that makes
    it amenable to computation and analysis, while preserving the most amount of information.
    In other terms, for a network with millions of users, we cannot expect our models
    to take as input vectors or matrices with millions of dimensions. We need efficient
    representation methods for graph data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也熟悉这样一个事实，即我们可以混合和匹配神经网络的一些组件，以构建针对特定任务的新模型。好消息是，图神经网络使用完全相同的组件，因此我们不需要在本章中介绍任何新的机器学习概念。一旦我们了解了如何数学表示图数据及其特征，以一种可以输入到神经网络中进行分析或生成新网络（图）数据的方式，我们就可以开始了。因此，我们将避免深入研究所有图神经网络。相反，我们专注于简单的数学公式、流行的应用程序、图模型的常见任务、可用的数据集和模型评估方法。我们的目标是对这一主题的工作原理有一个深刻的直觉。主要挑战再次是以一种使问题降维并易于计算和分析的方式，同时保留最多信息。换句话说，对于拥有数百万用户的网络，我们不能期望我们的模型以数百万维的向量或矩阵作为输入。我们需要有效的图数据表示方法。
- en: 'For readers aiming to dive deeper and fast track into graph neural networks,
    the 2019 survey paper: [*A Comprehensive Survey On Graph Neural Networks*](https://arxiv.org/pdf/1901.00596.pdf?ref=https://githubhelp.com)
    is an excellent place to start (of course, only after carefully reading this chapter).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望深入了解和快速掌握图神经网络的读者，2019年的调查报告：[*关于图神经网络的综合调查*](https://arxiv.org/pdf/1901.00596.pdf?ref=https://githubhelp.com)
    是一个很好的起点（当然，在仔细阅读本章之后）。
- en: 'Graphs: Nodes, Edges, And Features For Each'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图：节点、边和每个节点的特征
- en: Graphs are naturally well suited to model any problem where the goal is to understand
    a discrete collection of objects (with emphasis on discrete and not continuous)
    through the relationships between them. Graph theory is a relatively young discipline
    in discrete mathematics and computer science with virtually unlimited applications.
    This field is in need of more brains to tackle its many unsolved problems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图自然适合模拟任何目标是通过它们之间的关系来理解离散对象集合（重点是离散而不是连续）的问题。图论是离散数学和计算机科学中一个相对年轻的学科，具有几乎无限的应用。这个领域需要更多的头脑来解决其许多未解决的问题。
- en: 'A graph (see [Figure 9-1](#Fig_nodes_edges)) is made up of:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图（见[图9-1](#Fig_nodes_edges)）由以下组成：
- en: 'Nodes or Vertices: Bundled together in a set <math alttext="upper N o d e s
    equals StartSet n o d e 1 comma n o d e 2 comma ellipsis comma n o d e Subscript
    n Baseline EndSet"><mrow><mi>N</mi> <mi>o</mi> <mi>d</mi> <mi>e</mi> <mi>s</mi>
    <mo>=</mo> <mo>{</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mn>1</mn></msub>
    <mo>,</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>n</mi></msub> <mo>}</mo></mrow></math> . This can be as little as a handful
    of nodes (or even one node), or as massive as billions of nodes.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点或顶点：捆绑在一个集合中 <math alttext="upper N o d e s equals StartSet n o d e 1 comma
    n o d e 2 comma ellipsis comma n o d e Subscript n Baseline EndSet"><mrow><mi>N</mi>
    <mi>o</mi> <mi>d</mi> <mi>e</mi> <mi>s</mi> <mo>=</mo> <mo>{</mo> <mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>,</mo> <mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>n</mi></msub> <mo>}</mo></mrow></math>
    。这可以是少数节点（甚至一个节点），也可以是数十亿个节点。
- en: 'Edges: Connecting any two nodes (this can include an edge from a node to itself,
    or multiple edges connecting the same two nodes) in a directed (pointing from
    one node to the other) or undirected way (the edge has no direction from either
    node to the other). The set of edges is <math alttext="upper E d g e s equals
    StartSet e d g e Subscript i j Baseline equals left-parenthesis n o d e Subscript
    i Baseline comma n o d e Subscript j Baseline right-parenthesis such that there
    is an edge pointing from n o d e Subscript i Baseline to n o d e Subscript j Baseline
    EndSet"><mrow><mi>E</mi> <mi>d</mi> <mi>g</mi> <mi>e</mi> <mi>s</mi> <mo>=</mo>
    <mo>{</mo> <mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <mrow><mo>(</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>i</mi></msub> <mo>,</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>such</mtext> <mtext>that</mtext> <mtext>there</mtext>
    <mtext>is</mtext> <mtext>an</mtext> <mtext>edge</mtext> <mtext>pointing</mtext>
    <mtext>from</mtext> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub>
    <mtext>to</mtext> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub>
    <mo>}</mo></mrow></math> .'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边：以有向（从一个节点指向另一个节点）或无向方式（边没有从任一节点指向另一个节点的方向）连接任意两个节点（这可以包括从一个节点到自身的边，或连接相同两个节点的多条边）。边的集合为<math
    alttext="upper E d g e s equals StartSet e d g e Subscript i j Baseline equals
    left-parenthesis n o d e Subscript i Baseline comma n o d e Subscript j Baseline
    right-parenthesis such that there is an edge pointing from n o d e Subscript i
    Baseline to n o d e Subscript j Baseline EndSet"><mrow><mi>E</mi> <mi>d</mi> <mi>g</mi>
    <mi>e</mi> <mi>s</mi> <mo>=</mo> <mo>{</mo> <mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mrow><mo>(</mo> <mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub> <mo>,</mo> <mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>such</mtext>
    <mtext>that</mtext> <mtext>there</mtext> <mtext>is</mtext> <mtext>an</mtext> <mtext>edge</mtext>
    <mtext>pointing</mtext> <mtext>from</mtext> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>i</mi></msub> <mtext>to</mtext> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>j</mi></msub> <mo>}</mo></mrow></math>。
- en: 'Node features: We can assign to each <math alttext="n o d e Subscript i"><mrow><mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math> a list
    of say *d* features (such as the age, gender and income level of a social media
    user) bundled together in a vector <math alttext="ModifyingAbove f e a t u r e
    s Subscript n o d e Sub Subscript i With right-arrow"><mover accent="true"><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><msub><mi>s</mi>
    <mrow><mi>n</mi><mi>o</mi><mi>d</mi><msub><mi>e</mi> <mi>i</mi></msub></mrow></msub></mrow>
    <mo>→</mo></mover></math> . We can then bundle all the feature vectors of all
    the *n* nodes of the graph in a matrix <math alttext="upper F e a t u r e s Subscript
    upper N o d e s"><mrow><mi>F</mi> <mi>e</mi> <mi>a</mi> <mi>t</mi> <mi>u</mi>
    <mi>r</mi> <mi>e</mi> <msub><mi>s</mi> <mrow><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow></math>
    of size <math alttext="d times n"><mrow><mi>d</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    .'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点特征：我们可以为每个<math alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>分配一个包含*d*个特征的列表（例如社交媒体用户的年龄、性别和收入水平），这些特征捆绑在一个向量<math
    alttext="ModifyingAbove f e a t u r e s Subscript n o d e Sub Subscript i With
    right-arrow"><mover accent="true"><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><msub><mi>s</mi>
    <mrow><mi>n</mi><mi>o</mi><mi>d</mi><msub><mi>e</mi> <mi>i</mi></msub></mrow></msub></mrow>
    <mo>→</mo></mover></math>中。然后，我们可以将图的所有*n*个节点的所有特征向量捆绑在一个大小为<math alttext="d times
    n"><mrow><mi>d</mi> <mo>×</mo> <mi>n</mi></mrow></math>的矩阵<math alttext="upper
    F e a t u r e s Subscript upper N o d e s"><mrow><mi>F</mi> <mi>e</mi> <mi>a</mi>
    <mi>t</mi> <mi>u</mi> <mi>r</mi> <mi>e</mi> <msub><mi>s</mi> <mrow><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow></math>中。
- en: 'Edge features: Similarly, we can assign to each <math alttext="e d g e Subscript
    i j"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
    a list of say *c* features (such as the length of a road, its speed limit, and
    whether it is a toll road or not) bundled together in a vector <math alttext="ModifyingAbove
    f e a t u r e s Subscript e d g e Sub Subscript i j With right-arrow"><mover accent="true"><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><msub><mi>s</mi>
    <mrow><mi>e</mi><mi>d</mi><mi>g</mi><msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></msub></mrow>
    <mo>→</mo></mover></math> . We can then bundle all the feature vectors of all
    the *m* edges of the graph in a matrix <math alttext="upper F e a t u r e s Subscript
    upper E d g e s"><mrow><mi>F</mi> <mi>e</mi> <mi>a</mi> <mi>t</mi> <mi>u</mi>
    <mi>r</mi> <mi>e</mi> <msub><mi>s</mi> <mrow><mi>E</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow></math>
    of size <math alttext="c times m"><mrow><mi>c</mi> <mo>×</mo> <mi>m</mi></mrow></math>
    .'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边特征：类似地，我们可以为每个<math alttext="e d g e Subscript i j"><mrow><mi>e</mi> <mi>d</mi>
    <mi>g</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>分配一个包含*c*个特征的列表（例如道路长度、速度限制以及是否是收费道路），这些特征捆绑在一个向量<math
    alttext="ModifyingAbove f e a t u r e s Subscript e d g e Sub Subscript i j With
    right-arrow"><mover accent="true"><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><msub><mi>s</mi>
    <mrow><mi>e</mi><mi>d</mi><mi>g</mi><msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></msub></mrow>
    <mo>→</mo></mover></math>中。然后，我们可以将图的所有*m*条边的所有特征向量捆绑在一个大小为<math alttext="c times
    m"><mrow><mi>c</mi> <mo>×</mo> <mi>m</mi></mrow></math>的矩阵<math alttext="upper
    F e a t u r e s Subscript upper E d g e s"><mrow><mi>F</mi> <mi>e</mi> <mi>a</mi>
    <mi>t</mi> <mi>u</mi> <mi>r</mi> <mi>e</mi> <msub><mi>s</mi> <mrow><mi>E</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow></math>中。
- en: '![300](assets/emai_0901.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0901.png)'
- en: Figure 9-1\. A graph is made up of nodes and directed or undirected edges connecting
    the nodes.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1。一个图由连接节点的有向或无向边和节点组成。
- en: 'Graph models are powerful because they are flexible and are not necessarily
    forced to adhere to a rigid grid-like structure. We can think of their nodes as
    *floating through space* with no coordinates whatsoever. They are only held together
    by the edges that connect them. However, we need a way to represent their intrinsic
    structure. There are software packages that visualize graphs given their sets
    of nodes and edges, but we cannot do analysis and computations on these pretty
    (and informative) pictures. There are two popular graph represenations that we
    can use as inputs to machine learning models: A graph’s *adjacency matrix* and
    its *incidence matrix*. There are other representations that are useful for graph
    theoretic algorithms, such as *edge listing*, *two linear arrays*, and *successor
    listing*. All of these represenations convey the same information but differ in
    their storage requirements and the efficiency of graph retrieval, search, and
    manipulation. Most graph neural networks take as input the adjacency matrix along
    with the feature matrices for the nodes and the edges. Many times, they must do
    a dimension reduction (called graph represenation or graph embedding) before feeding
    the graph data into a model. Other times, the dimension reduction step is part
    of the model itself.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图模型很强大，因为它们灵活，不一定要遵循严格的网格结构。我们可以将它们的节点想象为在空间中*漂浮*，根本没有任何坐标。它们只是通过连接它们的边保持在一起。然而，我们需要一种方法来表示它们的内在结构。有一些软件包可以根据它们的节点和边的集合来可视化图形，但我们不能在这些漂亮（和信息丰富）的图片上进行分析和计算。有两种流行的图表示可以作为机器学习模型的输入：图的*邻接矩阵*和*关联矩阵*。还有其他对于图论算法有用的表示，比如*边列表*、*两个线性数组*和*后继列表*。所有这些表示传达相同的信息，但在存储需求和图的检索、搜索和操作的效率方面有所不同。大多数图神经网络将邻接矩阵与节点和边的特征矩阵一起作为输入。许多时候，它们必须在将图数据输入模型之前进行维度缩减（称为图表示或图嵌入）。其他时候，维度缩减步骤是模型本身的一部分。
- en: '*Adjacency matrix*: One algebraic way to store the structure of a graph on
    a machine and study its properties is through an adjacency matrix, which is an
    <math alttext="n times n"><mrow><mi>n</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    whose entries <math alttext="a d j a c e n c y Subscript i j Baseline equals 1"><mrow><mi>a</mi>
    <mi>d</mi> <mi>j</mi> <mi>a</mi> <mi>c</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>y</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mn>1</mn></mrow></math> if
    there is an edge from <math alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math> <math alttext="n o
    d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math>
    and <math alttext="a d j a c e n c y Subscript i j Baseline equals 0"><mrow><mi>a</mi>
    <mi>d</mi> <mi>j</mi> <mi>a</mi> <mi>c</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>y</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math> if
    there is no edge from <math alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math> <math alttext="n o
    d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math>
    . Note that this definition is able to accomodate a self edge which is an edge
    from a vertex to itself, but not multiple edges between two distinct nodes, unless
    we decide to include the numbers 2, 3, *etc.* as entries in the adjacency matrix.
    This however can mess up some results that graph theorists have established using
    the adjacency matrix.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*邻接矩阵*：在计算机上存储图的结构并研究其属性的一种代数方法是通过邻接矩阵，它是一个<math alttext="n times n"><mrow><mi>n</mi>
    <mo>×</mo> <mi>n</mi></mrow></math>，如果从<math alttext="n o d e Subscript i"><mrow><mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>到<math alttext="n
    o d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math>有一条边，则其条目<math
    alttext="a d j a c e n c y Subscript i j Baseline equals 1"><mrow><mi>a</mi> <mi>d</mi>
    <mi>j</mi> <mi>a</mi> <mi>c</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>y</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mn>1</mn></mrow></math>，如果从<math
    alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>i</mi></msub></mrow></math>到<math alttext="n o d e Subscript j"><mrow><mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math>没有边，则其条目<math
    alttext="a d j a c e n c y Subscript i j Baseline equals 0"><mrow><mi>a</mi> <mi>d</mi>
    <mi>j</mi> <mi>a</mi> <mi>c</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>y</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>。请注意，此定义可以容纳自环，即从一个顶点到自身的边，但不包括两个不同节点之间的多条边，除非我们决定在邻接矩阵中包含数字2、3，*等*作为条目。然而，这可能会破坏一些图论学家使用邻接矩阵建立的结果。'
- en: '*Incidence matrix*: This is another algebraic way to store the structure of
    the graph and retain its full information. Here, we list both the nodes and the
    edges, then formulate a matrix whose rows correspond to the vertices and whose
    columns correspond to the edges. An entry <math alttext="i n c i d e n c e Subscript
    i j"><mrow><mi>i</mi> <mi>n</mi> <mi>c</mi> <mi>i</mi> <mi>d</mi> <mi>e</mi> <mi>n</mi>
    <mi>c</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
    of the matrix is 1 if <math alttext="e d g e Subscript j"><mrow><mi>e</mi> <mi>d</mi>
    <mi>g</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math> connects <math alttext="n
    o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>
    to some other node, and zero otherwise. Note that this definition is able to accomodate
    multiple edges between two distinct nodes, but not a self edge from a node to
    itself. Since many graphs have much more edges than vertices, this matrix tends
    to be very wide and larger in size than the adjacency matrix.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*关联矩阵*：这是另一种代数方式来存储图的结构并保留其完整信息。在这里，我们列出节点和边，然后制定一个矩阵，其行对应于顶点，列对应于边。如果矩阵的条目<math
    alttext="i n c i d e n c e Subscript i j"><mrow><mi>i</mi> <mi>n</mi> <mi>c</mi>
    <mi>i</mi> <mi>d</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>为1，则<math
    alttext="e d g e Subscript j"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi>
    <mi>j</mi></msub></mrow></math>连接<math alttext="n o d e Subscript i"><mrow><mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>到其他节点，否则为零。请注意，此定义能够容纳两个不同节点之间的多条边，但不能容纳节点自身到自身的自环。由于许多图的边远多于顶点，因此这个矩阵往往非常宽，比邻接矩阵更大。'
- en: The *Laplacian matrix* is another matrix that is associated with an undirected
    graph. It is an <math alttext="n times n"><mrow><mi>n</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    symmetric matrix where each node has a corresponding row and column. The diagonal
    entries of the Laplacian matrix equal to the degree of each node, and the off
    diagonal entries are zero if there is no edge between nodes corrsponding to that
    entry and -1 if there is an edge between them. This is the discrete analogue of
    the continuous Laplace operator from calculus and partial differential equations
    where the discretization happens at the nodes of the graph. The Laplacian takes
    into account the second derivatives of a continuous (and twice differentiable)
    function, which measure the concavity of a function, or how much its value at
    a point differ from its value at the surrounding points. Similar to the continuous
    Laplacian operator, the Laplacian matrix provides a measure of the extent a graph
    differs at one node from its values at nearby nodes. The Laplacian matrix of a
    graph appears when we investigate random walks on graphs and when we study electrical
    networks and resistances. We will see these later in this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*Laplacian矩阵*是与无向图相关联的另一个矩阵。它是一个对称矩阵，大小为<math alttext="n times n"><mrow><mi>n</mi>
    <mo>×</mo> <mi>n</mi></mrow></math>，其中每个节点都有对应的行和列。Laplacian矩阵的对角线条目等于每个节点的度，非对角线条目为零，如果两个节点之间没有边相连，则为-1。这是从微积分和偏微分方程中的连续拉普拉斯算子离散化的过程，其中离散化发生在图的节点上。Laplacian考虑了连续（且两次可微）函数的二阶导数，这些导数测量函数的凹凸性，或者函数在某一点的值与周围点的值有多大差异。与连续拉普拉斯算子类似，Laplacian矩阵提供了一个度量，衡量图中一个节点与其附近节点的值有多大差异。当我们研究图上的随机游走以及电网络和电阻时，图的Laplacian矩阵会出现。我们将在本章后面看到这些内容。'
- en: We can easily infer simple node and edge statistics from the adjacency and incidence
    matrices, such as the degrees of nodes (the degree of a node is the number of
    edges connected to this node). The degree distribution P(k) reflects the variability
    in the degrees of all the nodes. P(k) is the emperical probability that a node
    has exactly k edges. This is of interest for many networks, such as web connectivity
    and biological networks. For example, if the distribution of nodes of degree *k*
    in a graph follows a power law of the form <math alttext="upper P left-parenthesis
    k right-parenthesis equals k Superscript negative alpha"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>k</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>k</mi> <mrow><mo>-</mo><mi>α</mi></mrow></msup></mrow></math>
    then such graphs have few nodes of high connectivity, or hubs, which are central
    to the network topology, holding it together, along with many nodes with low connectivity,
    which connect to the hubs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从邻接矩阵和关联矩阵中轻松推断出简单的节点和边统计信息，例如节点的度数（节点的度数是连接到该节点的边的数量）。度分布P(k)反映了所有节点的度数的变异性。P(k)是节点恰好有k条边的经验概率。这对于许多网络都很重要，比如网络连接和生物网络。例如，如果图中度为*k*的节点的分布遵循形式为<math
    alttext="upper P left-parenthesis k right-parenthesis equals k Superscript negative
    alpha"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>k</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mi>k</mi> <mrow><mo>-</mo><mi>α</mi></mrow></msup></mrow></math>的幂律分布，则这样的图具有少量高连通性的节点，或者中心节点，这些节点对网络拓扑结构至关重要，它们与许多低连通性的节点连接在一起。
- en: We can also add time dependency, and think of dynamic graphs whose properties
    change as time evolves. Currently, there are models that add time dependency to
    the node and/or edge feature vectors (so each entry of these vectors becomes time
    dependent). For example, for a GPS system that predicts travel routes, the edge
    features connecting one point on the map to the other change with time depending
    on the traffic situation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以添加时间依赖性，并考虑随着时间演变而改变的动态图的属性。目前，有一些模型将时间依赖性添加到节点和/或边特征向量中（因此这些向量的每个条目都变得与时间有关）。例如，对于预测旅行路线的GPS系统，连接地图上一点到另一点的边特征会随着时间根据交通情况而改变。
- en: Now that we have a mathematical framework for graph objects along with their
    node and edge features, we can feed these represenative vectors, matrices (and
    labels for supervised models) into machine learning models and do business as
    usual. Most of the time half of the story is having a good representation for
    the objects at hand. The other half of the story is the expressive power of machine
    learning models in general, where we can get good results without encoding (or
    even having to learn) the rules that lead to these results. For the purposes of
    this chapter, this means that we can jump straight into graph neural networks
    *before* learning proper graph theory.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个数学框架，用于图对象以及它们的节点和边特征，我们可以将这些代表性向量、矩阵（以及用于监督模型的标签）输入到机器学习模型中，然后像往常一样进行业务。大多数时候，故事的一半是拥有一个良好的对象表示。另一半故事是机器学习模型的表达能力，我们可以在不编码（甚至不需要学习）导致这些结果的规则的情况下获得良好的结果。对于本章的目的，这意味着我们可以在学习适当的图论之前直接跳入图神经网络。
- en: Directed Graphs
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有向图
- en: For directed graphs, on one hand, we are interested in the same properties as
    undirected graphs, such as their spanning trees, fundamental circuits, cut sets,
    planarity, thickness, and others. On the other hand, directed graphs have their
    own unique properties which are different than undirected graphs, such as strong
    connectedness, arborescence (a directed form of rooted tree), decyclization, and
    others.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有向图，一方面，我们对于无向图感兴趣的属性也是一样的，比如它们的生成树、基本回路、割集、平面性、厚度等。另一方面，有向图有它们自己独特的属性，这些属性与无向图不同，比如强连通性、树形结构（根树的有向形式）、去环化等。
- en: 'Example: PageRank Algorithm'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：PageRank算法
- en: '[PageRank](https://en.wikipedia.org/wiki/PageRank) is a currently retired algorithm
    (expired in 2019) that Google used to rank web pages in their search engine results.
    It provides a measure for the importance of a webpage based on how many other
    pages link to it. In graph language, the nodes are the webpages and the directed
    edges are the links pointing from one page to another. According to PageRank node
    is important when it has many other webpages pointing to it, that is, when its
    incoming degree is large (see [Figure 9-2](#Fig_PageRank)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[PageRank](https://en.wikipedia.org/wiki/PageRank)是谷歌用于排名其搜索引擎结果中网页的一种当前已停用的算法（2019年过期）。它根据有多少其他页面链接到它来衡量网页的重要性。在图语言中，节点是网页，有向边是从一个页面指向另一个页面的链接。根据PageRank，当一个节点有许多其他网页指向它时，即其入度较大时，该节点是重要的（参见[图9-2](#Fig_PageRank)）。'
- en: '![250](assets/emai_0902.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0902.png)'
- en: Figure 9-2\. PageRank gives higher score for pages with more pages pointing
    (or linking) to them ([image source](https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg)).
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. PageRank为指向（或链接）到它们的页面给出更高的分数（[图片来源](https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg)）。
- en: As a concrete example involving graphs, adjacency matrix, linear algebra, and
    the web, let’s walk through PageRank algorithm for an absurdly simplified world
    wide web consisting of only four indexed webpages, such as in [Figure 9-3](#Fig_graph_page_rank),
    as opposed to billions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为涉及图、邻接矩阵、线性代数和网络的具体示例，让我们通过PageRank算法来走一遍一个仅包含四个索引网页的荒谬简化全球网络，如[图9-3](#Fig_graph_page_rank)中所示，而不是数十亿个。
- en: '![250](assets/emai_0903.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0903.png)'
- en: Figure 9-3\. A fictitious world wide web consisting of only four indexed webpages.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3\. 一个仅包含四个索引网页的虚构全球网络。
- en: In the graph of [Figure 9-3](#Fig_graph_page_rank), only B links to A;A and
    D link to B; A and D link to C; A, B, and C link to D; A links to B, C, and D;
    B links to A and D; C links to D; and D links to B and C.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图9-3](#Fig_graph_page_rank)的图中，只有B链接到A；A和D链接到B；A和D链接到C；A、B和C链接到D；A链接到B、C和D；B链接到A和D；C链接到D；D链接到B和C。
- en: Let’s think of a web surfer who starts at some page then randomly clicks on
    a link from that page, then a link from this new page, and so on. This surfer
    simulates a *random walk on the graph of the web*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个从某个页面开始然后随机点击该页面的链接，然后从这个新页面点击一个链接，依此类推的网络冲浪者。这个冲浪者模拟了*网络图上的随机游走*。
- en: In general, on the graph representing the world wide web, such a random surfer
    traverses the graph from a certain node to one of its neighbors (or back to itself
    if there are links pointing back to the page itself). We will encounter the world
    wide web one more time in this chapter and explore the kind questions that we
    like to understand about the nature its graph. We need a matrix for the random
    walk, which for this application we call the *linking matrix*, but in reality
    it is the adjacency matrix weighted by the degree of each vertex. We use this
    random walk matrix, or linking matrix, to understand the long term behavior of
    the random walk on the graph. Random walks on graphs will appear throughout this
    chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在代表全球网络的图上，这样一个随机冲浪者从某个节点遍历图到其邻居之一（或者如果有指向页面本身的链接，则返回到自身）。我们将在本章中再次遇到全球网络，并探讨我们想要了解有关其图性质的问题。我们需要一个用于随机游走的矩阵，对于这个应用我们称之为*链接矩阵*，但实际上它是由每个顶点的度加权的邻接矩阵。我们使用这个随机游走矩阵，或链接矩阵，来理解图上随机游走的长期行为。本章将在整个章节中出现图上的随机游走。
- en: Back to the four-page world wide web of [Figure 9-3](#Fig_graph_page_rank).
    If the web surfer is at page A, there is a one third chance the surfer will move
    to page B, one third to chance to move to C, and one third to move to D. Thus,
    the *outward linking* vector of page A is
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 回到[图9-3](#Fig_graph_page_rank)的四页全球网络。如果网络冲浪者在页面A，有三分之一的机会冲浪者将移动到页面B，三分之一的机会移动到C，三分之一的机会移动到D。因此，页面A的*外向链接*向量是
- en: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper A Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  1 slash 3 3rd
    Row  1 slash 3 4th Row  1 slash 3 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>A</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn> <mo>/</mo>
    <mn>3</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper A Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  1 slash 3 3rd
    Row  1 slash 3 4th Row  1 slash 3 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>A</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn> <mo>/</mo>
    <mn>3</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: If the web surfer is at page B, there is a one half chance they will move to
    page A and one half chance they will move to page D. Thus, the outward linking
    vector of page B is
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络冲浪者在页面B，有一半的机会他们会移动到页面A，一半的机会他们会移动到页面D。因此，页面B的外向链接向量是
- en: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper B Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  1 slash 2 2nd Row  0 3rd
    Row  0 4th Row  1 slash 2 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>B</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper B Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  1 slash 2 2nd Row  0 3rd
    Row  0 4th Row  1 slash 2 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>B</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: Similarly, the outward linking vectors of pages C and D are
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，页面C和D的外向链接向量是
- en: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper C Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  0 3rd Row  0 4th
    Row  1 EndMatrix and ModifyingAbove l i n k i n g Subscript upper D Baseline With
    right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  1 slash 2 3rd Row  1
    slash 2 4th Row  0 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>C</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mtext>and</mtext> <mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>D</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper C Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  0 3rd Row  0 4th
    Row  1 EndMatrix and ModifyingAbove l i n k i n g Subscript upper D Baseline With
    right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  1 slash 2 3rd Row  1
    slash 2 4th Row  0 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>C</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mtext>and</mtext> <mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>D</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'We bundle the linking vectors of all the webpages together to create a linking
    matrix:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有网页的链接向量捆绑在一起，创建一个链接矩阵：
- en: <math alttext="dollar-sign upper L i n k i n g equals Start 4 By 4 Matrix 1st
    Row 1st Column 0 2nd Column 1 slash 2 3rd Column 0 4th Column 0 2nd Row 1st Column
    1 slash 3 2nd Column 0 3rd Column 0 4th Column 1 slash 2 3rd Row 1st Column 1
    slash 3 2nd Column 0 3rd Column 0 4th Column 1 slash 2 4th Row 1st Column 1 slash
    3 2nd Column 1 slash 2 3rd Column 1 4th Column 0 EndMatrix dollar-sign"><mrow><mi>L</mi>
    <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd> <mtd><mrow><mn>1</mn> <mo>/</mo>
    <mn>2</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper L i n k i n g equals Start 4 By 4 Matrix 1st
    Row 1st Column 0 2nd Column 1 slash 2 3rd Column 0 4th Column 0 2nd Row 1st Column
    1 slash 3 2nd Column 0 3rd Column 0 4th Column 1 slash 2 3rd Row 1st Column 1
    slash 3 2nd Column 0 3rd Column 0 4th Column 1 slash 2 4th Row 1st Column 1 slash
    3 2nd Column 1 slash 2 3rd Column 1 4th Column 0 EndMatrix dollar-sign"><mrow><mi>L</mi>
    <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd> <mtd><mrow><mn>1</mn> <mo>/</mo>
    <mn>2</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Note that the columns of the linking matrix are the outward linking probabilities
    and the rows of A are the *inward linking* probabilities: How can a surfer end
    up at page A? They can only be at B, and from there there is only a 0.5 probability
    they will end up at A.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，链接矩阵的列是外向链接概率，A的行是*内向链接*概率：冲浪者如何到达页面A？他们只能在B，从那里只有0.5的概率他们最终会到达A。
- en: 'Now we can *rank* page A by adding up the ranks of all the pages pointing to
    A each weighted by the probability a surfer will end up at page A from that page,
    that is, a page with many highly ranked pages pointing to it will also rank high.
    The ranks of all four pages are therefore:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过将指向A的所有页面的排名加起来，每个页面的排名都加权，权重是冲浪者从该页面最终到达页面A的概率，也就是说，一个有许多高排名页面指向它的页面也会排名较高。因此，四个页面的排名是：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column r a n k Subscript
    upper A 2nd Column equals 0 r a n k Subscript upper A Baseline plus 1 slash 2
    r a n k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus
    0 r a n k Subscript upper D Baseline 2nd Row 1st Column r a n k Subscript upper
    B 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 0 r a n
    k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus 1
    slash 2 r a n k Subscript upper D Baseline 3rd Row 1st Column r a n k Subscript
    upper C 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 0
    r a n k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus
    1 slash 2 r a n k Subscript upper D Baseline 4th Row 1st Column r a n k Subscript
    upper D 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 1
    slash 2 r a n k Subscript upper B Baseline plus 1 r a n k Subscript upper C Baseline
    plus 0 r a n k Subscript upper D EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>A</mi></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi>
    <msub><mi>k</mi> <mi>A</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn>
    <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub> <mo>+</mo>
    <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>B</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>A</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub>
    <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>D</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn>1</mn> <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>A</mi></msub> <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>B</mi></msub> <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>C</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi>
    <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn>1</mn> <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>A</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi>
    <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub> <mo>+</mo> <mn>1</mn> <mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub> <mo>+</mo> <mn>0</mn>
    <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column r a n k Subscript
    upper A 2nd Column equals 0 r a n k Subscript upper A Baseline plus 1 slash 2
    r a n k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus
    0 r a n k Subscript upper D Baseline 2nd Row 1st Column r a n k Subscript upper
    B 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 0 r a n
    k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus 1
    slash 2 r a n k Subscript upper D Baseline 3rd Row 1st Column r a n k Subscript
    upper C 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 0
    r a n k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus
    1 slash 2 r a n k Subscript upper D Baseline 4th Row 1st Column r a n k Subscript
    upper D 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 1
    slash 2 r a n k Subscript upper B Baseline plus 1 r a n k Subscript upper C Baseline
    plus 0 r a n k Subscript upper D EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>A</mi></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi>
    <msub><mi>k</mi> <mi>A</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn>
    <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub> <mo>+</mo>
    <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>B</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>A</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub>
    <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>D</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn>1</mn> <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>A</mi></msub> <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>B</mi></msub> <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>C</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi>
    <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn>1</mn> <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>A</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi>
    <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub> <mo>+</mo> <mn>1</mn> <mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub> <mo>+</mo> <mn>0</mn>
    <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr></mtable></math>
- en: 'To find the numerical value for the rank of each webpage, we have to solve
    the above system of linear equations, which is the territory of linear algebra.
    In matrix vector notation, we write the above system as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到每个网页排名的数值，我们必须解决上述线性方程组，这是线性代数的领域。在矩阵向量表示中，我们将上述系统写为：
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow equals
    upper L i n k i n g ModifyingAbove r a n k s With right-arrow dollar-sign"><mrow><mover
    accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover></mrow></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow equals
    upper L i n k i n g ModifyingAbove r a n k s With right-arrow dollar-sign"><mrow><mover
    accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover></mrow></math>
- en: Therefore, the vector containing all the ranks of all the webpages is an eigenvector
    of the linking matrix of the graph of the webpages (where the node are the webpages
    and the directed edges are the links between them) with eigenvalue 1\. Recall
    that in reality, the graph of the web is enormous, which means that the linking
    matrix is enormous, and devising efficient ways to find its eigenvectors becomes
    of immediate interest.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，包含所有网页排名的向量是网页图的链接矩阵的特征向量（其中节点是网页，有向边是它们之间的链接）的特征值为1。请记住，实际上，网络的图是巨大的，这意味着链接矩阵是巨大的，设计有效的方法来找到它的特征向量变得非常重要。
- en: 'Computing eigenvectors and eigenvalues of a given matrix is one of the most
    important contributions of numerical linear algebra, with immediate applications
    in many field. A lot of the numerical methods for finding eigenvectors and eigenvalues
    involve repeatedly multiplying a matrix with a vector. When dealing with huge
    matrices, this is expensive and we have to use every trick in the book to make
    the operations cheaper: We take advantage of the sparsity of the matrix (many
    entries are zeros so it is a waste to multiply with these entries *then* discover
    that they are just zeros); we introduce randomization or stochasticity, and venture
    into the fields of high dimensional probability and large random matrices (we
    will get a flavor of these in [Chapter 11](ch11.xhtml#ch11) on Probability). For
    now, we re-emphasize the iterative method we introduced in [Chapter 6](ch06.xhtml#ch06)
    on Singular Value Decompoitions: We start with a random vector <math alttext="ModifyingAbove
    r a n k s With right-arrow Subscript 0"><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mn>0</mn></msub></math> then produce a sequence of vectors
    iteratively by multiplying by the linking matrix:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 计算给定矩阵的特征向量和特征值是数值线性代数的最重要贡献之一，具有许多领域的直接应用。许多用于找到特征向量和特征值的数值方法涉及反复将矩阵与向量相乘。当处理巨大矩阵时，这是昂贵的，我们必须使用书中的每一个技巧来使操作更便宜：我们利用矩阵的稀疏性（许多条目为零，因此与这些条目相乘是浪费，*然后*发现它们只是零）；我们引入随机化或随机性，并涉足高维概率和大型随机矩阵领域（我们将在[第11章](ch11.xhtml#ch11)中尝试这些）。目前，我们重申了我们在[第6章](ch06.xhtml#ch06)中介绍的迭代方法：我们从一个随机向量<math
    alttext="ModifyingAbove r a n k s With right-arrow Subscript 0"><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mn>0</mn></msub></math>开始，然后通过乘以链接矩阵迭代产生一系列向量：
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow Subscript
    i plus 1 Baseline equals upper L i n k i n g ModifyingAbove r a n k s With right-arrow
    Subscript i Baseline dollar-sign"><mrow><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <msub><mover
    accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mi>i</mi></msub></mrow></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow Subscript
    i plus 1 Baseline equals upper L i n k i n g ModifyingAbove r a n k s With right-arrow
    Subscript i Baseline dollar-sign"><mrow><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <msub><mover
    accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mi>i</mi></msub></mrow></math>
- en: For our four page world wide web, this converges to the vector
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的四页全球网络，这收敛到向量
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow equals
    Start 4 By 1 Matrix 1st Row  0.12 2nd Row  0.24 3rd Row  0.24 4th Row  0.4 EndMatrix
    dollar-sign"><mrow><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>12</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0</mn> <mo>.</mo>
    <mn>24</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>24</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>4</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow equals
    Start 4 By 1 Matrix 1st Row  0.12 2nd Row  0.24 3rd Row  0.24 4th Row  0.4 EndMatrix
    dollar-sign"><mrow><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>12</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0</mn> <mo>.</mo>
    <mn>24</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>24</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>4</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: which means that page D is ranked highest and in a search engine query with
    similar content it will be the first page returned. We can then redraw the diagram
    in [Figure 9-3](#Fig_graph_page_rank) with each circle’s size corresponding to
    the page’s importance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着页面D排名最高，在具有相似内容的搜索引擎查询中，它将是返回的第一个页面。然后，我们可以重新绘制[图9-3](#Fig_graph_page_rank)中的图表，其中每个圆的大小对应页面的重要性。
- en: 'When PageRank algorithm was in use, the real implementation included a damping
    factor *d* which is a number between zero and one, usually around 0.85, which
    takes into account only an 85 percent chance that the web surfer clicks on a link
    from the page they are currently at, and a fifteen percent chance that they start
    at a completely new page which had no links from the page they are currently at.
    This modifies the iterative process to find the rankings of the pages of the web
    in a straight forward way:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当PageRank算法在使用时，真正的实现包括一个阻尼因子*d*，它是介于零和一之间的数字，通常约为0.85，它考虑了只有85%的机会，网络冲浪者点击当前所在页面的链接，以及15%的机会，他们从当前所在页面开始访问一个完全新的页面，该页面没有从当前所在页面链接过来。这修改了找到网页排名的迭代过程的方式：
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow Subscript
    i plus 1 Baseline equals d left-parenthesis upper L i n k i n g ModifyingAbove
    r a n k s With right-arrow Subscript i Baseline right-parenthesis plus StartFraction
    1 minus d Over total number of pages EndFraction ModifyingAbove o n e s With right-arrow
    dollar-sign"><mrow><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>d</mi> <mrow><mo>(</mo> <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mfrac><mrow><mn>1</mn><mo>-</mo><mi>d</mi></mrow>
    <mrow><mtext>total</mtext><mtext>number</mtext><mtext>of</mtext><mtext>pages</mtext></mrow></mfrac>
    <mover accent="true"><mrow><mi>o</mi><mi>n</mi><mi>e</mi><mi>s</mi></mrow> <mo>→</mo></mover></mrow></math>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow Subscript
    i plus 1 Baseline equals d left-parenthesis upper L i n k i n g ModifyingAbove
    r a n k s With right-arrow Subscript i Baseline right-parenthesis plus StartFraction
    1 minus d Over total number of pages EndFraction ModifyingAbove o n e s With right-arrow
    dollar-sign"><mrow><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>d</mi> <mrow><mo>(</mo> <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mfrac><mrow><mn>1</mn><mo>-</mo><mi>d</mi></mrow>
    <mrow><mtext>total</mtext><mtext>number</mtext><mtext>of</mtext><mtext>pages</mtext></mrow></mfrac>
    <mover accent="true"><mrow><mi>o</mi><mi>n</mi><mi>e</mi><mi>s</mi></mrow> <mo>→</mo></mover></mrow></math>
- en: 'Finally, if you are wondering whether Google keeps searching the web for new
    webpages and indexing them, and does it keep checking all indexed webpages for
    new links? The answer is yes, and the following excerpts are from [Google’s How
    Search Works](https://developers.google.com/search/docs/advanced/guidelines/how-search-works):
    *Google Search is a fully-automated search engine that uses software known as
    web crawlers that explore the web regularly to find pages to add to our index.
    In fact, the vast majority of pages listed in our results aren’t manually submitted
    for inclusion, but are found and added automatically when our web crawlers explore
    the web. […] There isn’t a central registry of all web pages, so Google must constantly
    look for new and updated pages and add them to its list of known pages. This process
    is called “URL discovery”. Some pages are known because Google has already visited
    them. Other pages are discovered when Google follows a link from a known page
    to a new page: for example, a hub page, such as a category page, links to a new
    blog post. Still other pages are discovered when you submit a list of pages (a
    sitemap) for Google to crawl. […] When a user enters a query, our machines search
    the index for matching pages and return the results we believe are the highest
    quality and most relevant to the user. Relevancy is determined by hundreds of
    factors, which could include information such as the user’s location, language,
    and device (desktop or phone). For example, searching for “bicycle repair shops”
    would show different results to a user in Paris than it would to a user in Hong
    Kong.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你想知道谷歌是否一直在搜索网络上的新网页并对其进行索引，并且是否一直在检查所有已索引的网页是否有新链接？答案是肯定的，以下摘录来自[谷歌搜索工作原理](https://developers.google.com/search/docs/advanced/guidelines/how-search-works)：*谷歌搜索是一个完全自动化的搜索引擎，使用称为网络爬虫的软件定期探索网络以找到要添加到我们索引中的页面。事实上，我们结果中列出的绝大多数页面并非手动提交以进行包含，而是在我们的网络爬虫探索网络时自动发现并添加的。[...]并没有所有网页的中央注册表，因此谷歌必须不断寻找新的和更新的页面，并将它们添加到已知页面列表中。这个过程称为“URL发现”。有些页面是已知的，因为谷歌已经访问过它们。当谷歌从已知页面跟踪到新页面时，例如，一个中心页面，比如一个类别页面，链接到一个新的博客文章时，就会发现其他页面。当您提交一个页面列表（站点地图）供谷歌爬取时，也会发现其他页面。[...]当用户输入查询时，我们的机器在索引中搜索匹配的页面，并返回我们认为对用户最有价值和最相关的结果。相关性由数百个因素决定，这些因素可能包括用户的位置、语言和设备（台式机或手机）等信息。例如，搜索“自行车修理店”会向巴黎用户显示与向香港用户显示不同的结果。*
- en: The more data we collect the more complex searching it becomes. Google rolled
    RankBrain in 2015\. It uses machine learning to vectorize the text on the webpages,
    similar to what we did in [Chapter 7](ch07.xhtml#ch07). This process adds context
    and meaning to the indexed pages, so that the search returns more accurate results.
    The bad thing that this process adds is the much higher dimensions associated
    with meaning vectors. To circumvent the difficulty of checking every vector at
    every dimension before returning the webpages closest to the query, Google uses
    an approximate nearest neighbor algorithm, which helps return excellent results
    in milliseconds, the experience we have now.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集的数据越多，搜索就变得越复杂。谷歌在2015年推出了RankBrain。它使用机器学习来对网页上的文本进行向量化，类似于我们在[第7章](ch07.xhtml#ch07)中所做的。这个过程为索引页面添加了上下文和含义，使搜索结果更准确。这个过程增加的不好之处是与含义向量相关联的维度更高。为了避免在返回与查询最接近的网页之前检查每个维度上的每个向量的困难，谷歌使用了近似最近邻算法，帮助在毫秒内返回优秀的结果，这就是我们现在的体验。
- en: Inverting Matrices Using Graphs
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图来求逆矩阵
- en: Many problems in the applied sciences involve writing a discrete linear system
    <math alttext="upper A ModifyingAbove x With right-arrow equals ModifyingAbove
    b With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>=</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> and
    solving it, which is equivalent to inverting the matrix A and finding the solution
    <math alttext="ModifyingAbove x With right-arrow equals upper A Superscript negative
    1 Baseline ModifyingAbove b With right-arrow"><mrow><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> . But for large
    matrices, this is a computationally expensive operation, along with high storage
    requirements, and poor accuracy. We are always looking for efficient ways to invert
    matrices, sometimes leveraging the special characteristics of the particular matrices
    at hand.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 应用科学中的许多问题涉及编写一个离散线性系统 <math alttext="upper A ModifyingAbove x With right-arrow
    equals ModifyingAbove b With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    并解决它，这等同于求矩阵A的逆并找到解 <math alttext="ModifyingAbove x With right-arrow equals upper
    A Superscript negative 1 Baseline ModifyingAbove b With right-arrow"><mrow><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> 。但对于大矩阵来说，这是一个计算昂贵的操作，加上高存储要求和低精度。我们一直在寻找有效的方法来求逆矩阵，有时利用手头特定矩阵的特殊特性。
- en: 'The following is a graph theoretic method that computes the inverse of a matrix
    of a decent size (for example, a hundred rows and a hundred columns):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图论方法，用于计算一个相当大的矩阵（例如，一百行一百列）的逆：
- en: Replace each nonzero entry in the matrix A with a 1\. We obtain a binary matrix.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用1替换矩阵A中的每个非零元素。我们得到一个二进制矩阵。
- en: Permute the rows and the corresponding columns of the resulting binary matrix
    to make all diagonal entries 1’s.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新排列得到的二进制矩阵的行和相应的列，使所有对角线条目为1。
- en: We think of the matrix obtained as the adjacency matrix of a directed graph
    (where we delete the self-loops corresponding to 1’s along the diagonal from the
    graph).
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将得到的矩阵视为一个有向图的邻接矩阵（在图中删除对应于对角线上的1的自环）。
- en: The resulting directed graph is partitioned into its fragments.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 得到的有向图被分成了片段。
- en: If a fragment is too large, then we tear it further into smaller fragments by
    removing an appropriate edge.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个片段太大，那么我们通过删除适当的边将其进一步分解为更小的片段。
- en: We invert the smaller matrices.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们反转较小的矩阵。
- en: Apparently this leades to the inverse of the original matrix.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显然，这导致了原始矩阵的逆。
- en: We will not explain why and how, but this method is so cute so it made its way
    into this chpater.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会解释为什么和如何，但这种方法非常可爱，所以它进入了这一章节。
- en: 'Cayley Graphs Of Groups: Pure Algebra And Parallel Computing'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 群的凯莱图：纯代数和并行计算
- en: Graphs of groups, also called Cayley graphs or Cayley diagrams, can be helpful
    in designing and analyzing network architectures for parallel computers, routing
    problems, and routing algorithms for interconnected networks. The paper [Processor
    Interconnection Networks From Cayley Graphs](https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/cryptologic-quarterly/Processor_Interconnection.pdf)
    is an interesting and easy read on earlier designs applying Cayley graphs for
    parallel computing networks, and explains how to construct Cayley graphs that
    meet specific design parameters. Cayley graphs have also been applied for [classification
    of data](https://www.sciencedirect.com/science/article/pii/S0012365X08006869).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 群的图，也称为凯莱图或凯莱图表，对于设计和分析并行计算机的网络架构、路由问题以及互连网络的路由算法都是有帮助的。论文[Processor Interconnection
    Networks From Cayley Graphs](https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/cryptologic-quarterly/Processor_Interconnection.pdf)是一篇有趣且易读的早期设计应用凯莱图进行并行计算网络的论文，解释了如何构建符合特定设计参数的凯莱图。凯莱图也被应用于[数据分类](https://www.sciencedirect.com/science/article/pii/S0012365X08006869)。
- en: 'We can represent every group with n elements as a connected directed graph
    of n nodes, where each node corresponds to an element from the group, and each
    edge represents a multiplication by a generator from the group. The edges are
    labeled (or colored) depending on which generator from the group we are multiplying
    by (see [Figure 9-4](#Fig_graph_of_group)). This directed graph uniquely defines
    the group: Each product of elements in the group corresponds to following a sequence
    of directed edges on the graph. For example, the graph of a cyclic group of n
    elements is a directed circuit of n nodes in which every edge represents multiplication
    by one generator of the group.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将具有n个元素的每个群表示为具有n个节点的连接有向图，其中每个节点对应于群中的一个元素，每条边代表由群中的一个生成器相乘。边根据我们要乘以的群中的生成器进行标记（或着色）（参见[图9-4](#Fig_graph_of_group)）。这个有向图唯一定义了群：群中元素的每个乘积对应于在图上遵循一系列有向边。例如，具有n个元素的循环群的图是一个由n个节点组成的有向电路，其中每条边代表群的一个生成器的乘法。
- en: From a pure math perspective, Cayley graphs are useful for visualizing and studying
    abstract groups, encoding their full abstract structure and all of their elements
    in a visual diagram. The symmetry of Cayley graphs makes them useful for constructing
    more involved abstract objects. These are central tools for combinatorial and
    geometric group theory. For more Cayley graphs, check [this page](https://mathworld.wolfram.com/CayleyGraph.xhtml).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从纯数学的角度来看，凯莱图对于可视化和研究抽象群是有用的，它在视觉图表中编码了它们的完整抽象结构和所有元素。凯莱图的对称性使它们对于构建更复杂的抽象对象很有用。这些是组合和几何群论的中心工具。要了解更多凯莱图，请查看[此页面](https://mathworld.wolfram.com/CayleyGraph.xhtml)。
- en: '![250](assets/emai_0904.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0904.png)'
- en: 'Figure 9-4\. Cayley graph for the free group on two generators a and b: Each
    node represents an element of the free group, and each edge represents multiplication
    by a or b. ([image source](https://commons.wikimedia.org/wiki/File:F2_Cayley_Graph.png)).'
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。自由群a和b的凯莱图：每个节点代表自由群的一个元素，每条边代表乘以a或b。([图片来源](https://commons.wikimedia.org/wiki/File:F2_Cayley_Graph.png))。
- en: Message Passing Within A Graph
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图中的消息传递在图内传递
- en: A useful approach for modeling the propagation of information within graphs,
    as well as neatly aggregating the information conveyed in the nodes, edges, and
    the structure of the graph itself into vectors of a certain desired dimension
    is the *message passing framework*. Within this framework, we update every node
    with information from the feature vectors of its neighboring nodes and the edges
    connected to them. A graph neural network performs multiple rounds of message
    passing, each round propagates a single node’s information further. Finally, we
    combine the latent features of each individual node to obtain its unified vector
    representation, and to also represent the whole graph.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于模拟图中信息传播的有用方法，以及将节点、边和图结构传达的信息整齐地聚合到特定维度的向量中的方法是*消息传递框架*。在这个框架内，我们从相邻节点和连接到它们的边的特征向量中更新每个节点的信息。图神经网络执行多轮消息传递，每一轮传播一个节点的信息。最后，我们结合每个单独节点的潜在特征来获得其统一的向量表示，并表示整个图。
- en: More concretely, for a specific node, we choose a function that takes as input
    the node’s feature vector, the feature vector of one of its neighboring nodes
    (those connected to it by an edge), and the feature vector of the edge that connects
    it to this neighboring node, and outputs a new vector that contains within it
    information from the node, the neighbor, and their connecting edge. We apply the
    same function to all of the node’s neighbors, then add the resulting vectors together,
    producing a *message vector*. Finally, we update the feature vector of our node
    by combining its original feature vector with the message vector within an update
    function that we also choose. When we do this for each node in the graph, each
    node’s new feature vector will contain information from itself, all its neighbors,
    and all its connecting edges. Now when we repeat this process one more time, the
    node’s most recent feature vector will contain information from itself, all its
    neighbors *and its neighbor’s neighbors*, and all the corresponding connecting
    edges. Thus, the more message passing rounds we do, the more each node’s feature
    vector contains information from farther nodes within the graph, moving one edge
    separation at a time. The information diffuses successively across the entire
    network.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，对于一个特定的节点，我们选择一个函数，该函数以节点的特征向量、其中一个相邻节点的特征向量（通过边连接到它的节点）以及连接它到这个相邻节点的边的特征向量作为输入，并输出一个包含来自节点、相邻节点和它们连接的边的信息的新向量。我们将同样的函数应用于所有节点的相邻节点，然后将生成的向量相加，产生一个*消息向量*。最后，我们通过将原始特征向量与消息向量结合在一起的更新函数来更新我们节点的特征向量，这也是我们选择的。当我们对图中的每个节点都这样做时，每个节点的新特征向量将包含来自它自身、所有相邻节点和所有连接的边的信息。现在当我们再次重复这个过程时，节点的最新特征向量将包含来自它自身、所有相邻节点*及其相邻节点*，以及所有相应的连接边的信息。因此，我们进行的消息传递轮数越多，每个节点的特征向量就包含了来自图中更远节点的信息，每次移动一个边的距离。信息逐渐在整个网络中传播。
- en: The Limitless Applications Of Graphs
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的无限应用
- en: 'Applications for graph neural networks, and graph models in general, are ubiquitous
    and so important that I am a bit regretful I did not start the book with graphs.
    In any graph model, we start by answering the following questions:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络和图模型的应用普遍且重要，以至于我有点后悔没有从图开始这本书。在任何图模型中，我们首先回答以下问题：
- en: What are the nodes?
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点是什么？
- en: What is the relationship that links two nodes, that establishes directed or
    undirected edge(s) between them?
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是什么关系连接了两个节点，建立了它们之间的有向或无向边？
- en: Should the model include feature vectors for the nodes and/or the edges?
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否应该包括节点和/或边的特征向量？
- en: Is our model dynamic, where the nodes, edges, and their features evolve with
    time, or is it static in time?
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的模型是动态的吗，其中节点、边和它们的特征随时间演变，还是静态的？
- en: What are we interested in? Classifying (for example cancerous or noncancerous;
    fake news spreader or non fake news spreader)? Generating new graphs (for example
    for drug discovery)? Clustering? Embedding the graph into a lower dimensional
    and structured space?
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们感兴趣的是什么？分类（例如癌细胞或非癌细胞；虚假新闻传播者或非虚假新闻传播者）？生成新的图（例如用于药物发现）？聚类？将图嵌入到一个更低维度和结构化的空间中？
- en: What kind of the data is available or needed, and is the data organized and/or
    labeled? Does it need preprocessing?
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用或需要的数据是什么，数据是否组织和/或标记？是否需要预处理？
- en: 'We survey few applications in this section, but there are many more that genuinely
    lend themselves to a graph modeling structure. It is good to read the abstracts
    of the linked publications since they help capture common themes and ways of thinking
    about these models. The following list gives a good idea of the common tasks for
    graph neural networks, which include:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中我们调查了一些应用，但还有许多应用真正适合图模型结构。阅读链接出版物的摘要是很好的，因为它们有助于捕捉这些模型的共同主题和思考方式。以下列表提供了图神经网络的常见任务的一个很好的概念，包括：
- en: Node classification
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点分类
- en: Graph classification
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图分类
- en: Clustering and community detection
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类和社区检测
- en: Generation of new graphs
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成新的图
- en: Influence maximization
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 影响最大化
- en: Link prediction
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接预测
- en: Image data as graphs
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像数据作为图
- en: 'We might encounter graph neural networks tested on the [MNIST data set for
    handwritten digits](http://yann.lecun.com/exdb/mnist/), which is one of the benchmark
    sets for computer vision. If you wonder how image data (stored as three dimensional
    tensors of pixel intensities across each channel) manages to fit into a graph
    structure, here’s how it works: Each pixel is a node, its features are the respective
    intensities of its three channels (if it is a color image, otherwise it only has
    one feature). The edges connect each pixel to the three, five, or eight pixels
    surrounding it, depending on whether the pixel is located at a corner, edge, or
    in middle of the image.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会遇到在[手写数字MNIST数据集](http://yann.lecun.com/exdb/mnist/)上测试的图神经网络，这是计算机视觉的基准数据集之一。如果你想知道图像数据（存储为每个通道上的像素强度的三维张量）如何适应图结构，这里是它的工作原理：每个像素是一个节点，其特征是其三个通道的相应强度（如果是彩色图像，则只有一个特征）。边连接每个像素到其周围的三个、五个或八个像素，取决于像素是位于角落、边缘还是图像中间。
- en: Brain Networks
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大脑网络
- en: One of the main pursuits in neuroscience is understanding the network organization
    of the brain. Graph models provide a natural framework and many tools for analyzing
    the complex networks of the brain, both in terms of their anatomy and their functionality.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 神经科学中的主要追求之一是理解大脑的网络组织。图模型为分析大脑的复杂网络提供了一个自然的框架和许多工具，无论是从解剖学还是功能学的角度。
- en: To create artificial intelligence on par with human intelligence, we must understand
    the human brain on many levels. One aspect is the brain’s network connectivity,
    how connectivity affects the brain’s functionality, and how to replicate that,
    building up from small computational units up to modular components to a fully
    independent and functional system.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建与人类智能相媲美的人工智能，我们必须在许多层面上了解人类大脑。其中一个方面是大脑的网络连接性，连接性如何影响大脑的功能，以及如何复制这一点，从小的计算单元逐步构建到模块化组件，最终形成一个完全独立和功能完善的系统。
- en: 'Human brain anatomical networks, demonstrate short path length (conservation
    of wiring costs) along with high degree cortical *hubs*, that is, high clustering.
    This is on both the cellular scales and on the whole brain scale. In other words,
    the brain network seems to have organized itself in a way that maximizes the efficiency
    of information transfer and minimizes connection cost. The network also demonstrates
    modular and hierarchical topological structures and functionalities. The topological
    structures of the brain networks and their functionalities are interdependent
    over both short and long time scales: The dynamic properties of the networks are
    affected by their structural connectivity, and over a longer timescale, the dynamics
    affect the topological structure of the network.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑解剖网络展示了短路径长度（保持布线成本）以及高度皮层*中枢*，即高聚类。这既在细胞尺度上，也在整个大脑尺度上。换句话说，大脑网络似乎以一种最大化信息传输效率和最小化连接成本的方式组织自己。该网络还展示了模块化和分层的拓扑结构和功能。大脑网络的拓扑结构和功能在短时间尺度和长时间尺度上是相互依存的：网络的动态特性受其结构连接的影响，而在更长的时间尺度上，动态特性影响网络的拓扑结构。
- en: 'The most important questions are: What is the relationship between the network
    properties of the brain and its cognitive behavior? What is the relationship betweeen
    the network properties and brain and mental disoders? For example, we can view
    neuropsychiatric disorders as dysconnectivity syndromes, where graph theory can
    help quantify weaknesses, vulnerability to lesions, and abnormalities in the network
    structures. In fact, graph theory has been applied to study the structural and
    functional network properties in schizophrenia, Alzheimer’s, and other disorders.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的问题是：大脑的网络属性与其认知行为之间的关系是什么？网络属性与大脑和精神障碍之间的关系是什么？例如，我们可以将神经精神障碍视为失连接综合征，图论可以帮助量化网络结构中的弱点、对损伤的脆弱性和异常。事实上，图论已被应用于研究精神分裂症、阿尔茨海默病和其他疾病中的结构和功能网络属性。
- en: Spread Of Disease
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 疾病传播
- en: As we have all learned from the Covid19 pandemic, it is of crucial importance
    to be able to forecast disease incidents accurately and reliably, for mitigation
    purposes, quarantine measures, policy, and many other decision factors. A graph
    model can consider either individuals or entire geographic blocks as nodes, and
    contact occurences between these individuals or blocks as edges. Recent models
    for predicting Covid19 spread, for example the article [Combining graph neural
    networks and spatio-temporal disease models to improve the prediction of weekly
    COVID-19 cases in Germany (2022)](https://www.nature.com/articles/s41598-022-07757-5),
    incorporate human mobility data from Facebook, Apple, and Google to model interactions
    between the nodes in their models.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从Covid19大流行中学到的那样，准确可靠地预测疾病事件对于减轻目的、隔离措施、政策和许多其他决策因素至关重要。图模型可以将个体或整个地理区块视为节点，将这些个体或区块之间的接触事件视为边。最近用于预测Covid19传播的模型，例如文章[结合图神经网络和时空疾病模型以改善对德国每周COVID-19病例的预测（2022）](https://www.nature.com/articles/s41598-022-07757-5)，将来自Facebook、Apple和Google的人类流动数据纳入模型中，以模拟节点之间的相互作用。
- en: 'There is plenty of data that can be put to good use here: Facebook’s [Data
    For Good](https://dataforgood.facebook.com) resource has a wealth of data on population
    densities, social mobility and travel patterns, social connectedness, and others.
    Google’s [Covid 19 Community Mobility Reports](https://www.google.com/covid19/mobility/index.xhtml?hl=en)
    draw insights from Google Maps and other products into a data set that charts
    movement trends over time by geography, across different categories of places
    such as retail and recreation, groceries and pharmacies, parks, transit stations,
    workplaces, and residential areas. Similarly, Apple’s and Amazon’s mobility data
    serve similar purpose with the goal of aiding efforts to limit the spread of Covid19.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有大量数据可以加以利用：Facebook的[Data For Good](https://dataforgood.facebook.com)资源拥有大量关于人口密度、社会流动和旅行模式、社会联系等数据。谷歌的[Covid
    19社区流动报告](https://www.google.com/covid19/mobility/index.xhtml?hl=en)从谷歌地图和其他产品中获取见解，形成一个数据集，通过地理位置和不同类别的地点（如零售和娱乐、杂货店和药店、公园、交通站、工作场所和居住区）跟踪随时间变化的移动趋势。同样，苹果和亚马逊的流动数据也具有类似的目的，旨在帮助限制Covid19的传播。
- en: Spread Of Information
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息传播
- en: 'We can use graphs to model the spread of information, disease, rumors, gossip,
    computer viruses, innovative ideas, or others. Such a model is usually a directed
    graph, where each node corresponds to an individual, and the edges are tagged
    with information about the interaction between individuals. The edge tags, or
    weights, are usually probabilities: The weight <math alttext="w Subscript i j"><msub><mi>w</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> of the edge connecting <math alttext="n
    o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>
    to <math alttext="n o d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi>
    <msub><mi>e</mi> <mi>j</mi></msub></mrow></math> is the probability of a certain
    effect (disease, rumor, computer virus) propagating from <math alttext="n o d
    e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>
    to <math alttext="n o d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi>
    <msub><mi>e</mi> <mi>j</mi></msub></mrow></math> .'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用图形来模拟信息、疾病、谣言、八卦、计算机病毒、创新想法或其他事物的传播。这样的模型通常是一个有向图，其中每个节点对应一个个体，边缘带有关于个体之间互动的信息。边缘标签或权重通常是概率：连接<math
    alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>i</mi></msub></mrow></math> 和<math alttext="n o d e Subscript j"><mrow><mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math> 的边缘的权重<math
    alttext="w Subscript i j"><msub><mi>w</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    是某种效应（疾病、谣言、计算机病毒）从<math alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math> 传播到<math alttext="n
    o d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math>
    的概率。
- en: Detecting And Tracking Fake News Propagation
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检测和跟踪假新闻传播
- en: 'Graph neural networks perform better in the task of detecting fake news (see
    [Figure 9-5](#Fig_fake_news)) than content based natural language processing approaches.
    The abstract of the 2019 paper, [*Fake News Detection on Social Media using Geometric
    Deep Learning*](https://arxiv.org/pdf/1902.06673.pdf) is informative: *Social
    media are nowadays one of the main news sources for millions of people around
    the globe due to their low cost, easy access, and rapid dissemination. This however
    comes at the cost of dubious trustworthiness and significant risk of exposure
    to ‘fake news’, intentionally written to mislead the readers. Automatically detecting
    fake news poses challenges that defy existing content-based analysis approaches.
    One of the main reasons is that often the interpretation of the news requires
    the knowledge of political or social context or ‘common sense’, which current
    natural language processing algorithms are still missing. Recent studies have
    empirically shown that fake and real news spread differently on social media,
    forming propagation patterns that could be harnessed for the automatic fake news
    detection. Propagation based approaches have multiple advantages compared to their
    content based counterparts, among which is language independence and better resilience
    to adversarial attacks. In this paper, we show a novel automatic fake news detection
    model based on geometric deep learning. The underlying core algorithms are a generalization
    of classical convolutional neural networks to graphs, allowing the fusion of heterogeneous
    data such as content, user profile and activity, social graph, and news propagation.
    Our model was trained and tested on news stories, verified by professional fact
    checking organizations, that were spread on Twitter. Our experiments indicate
    that social network structure and propagation are important features allowing
    highly accurate (92.7% ROC AUC) fake news detection. Second, we observe that fake
    news can be reliably detected at an early stage, after just a few hours of propagation.
    Third, we test the aging of our model on training and testing data separated in
    time. Our results point to the promise of propagation based approaches for fake
    news detection as an alternative or complementary strategy to content based approaches.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络在检测假新闻的任务中表现比基于内容的自然语言处理方法更好（见[图9-5](#Fig_fake_news)）。2019年论文《在社交媒体上使用几何深度学习检测假新闻》的摘要很有信息量：*社交媒体如今是全球数百万人的主要新闻来源之一，因为它们成本低、易于访问且传播迅速。然而，这也带来了可疑的信任度和暴露于“假新闻”的风险，这些新闻是有意编写以误导读者的。自动检测假新闻面临挑战，这挑战超越了现有基于内容的分析方法。其中一个主要原因是，新闻的解释通常需要政治或社会背景知识或“常识”，而当前的自然语言处理算法仍然缺乏。最近的研究实证表明，假新闻和真实新闻在社交媒体上传播方式不同，形成了传播模式，这些模式可以用于自动检测假新闻。基于传播的方法与基于内容的方法相比具有多个优势，其中包括语言独立性和更好的抵抗对抗性攻击。在本文中，我们展示了一种基于几何深度学习的新型自动假新闻检测模型。底层核心算法是将经典卷积神经网络推广到图形，允许融合异构数据，如内容、用户资料和活动、社交图和新闻传播。我们的模型在Twitter上传播的经专业事实核查组织验证的新闻报道上进行了训练和测试。我们的实验表明，社交网络结构和传播是重要特征，允许高度准确（92.7%
    ROC AUC）的假新闻检测。其次，我们观察到假新闻可以在传播几小时后可靠地检测出来。第三，我们测试了我们模型在时间上分开的训练和测试数据的老化情况。我们的结果指向基于传播的方法对于假新闻检测的潜力，作为基于内容的方法的替代或补充策略。*
- en: '![250](assets/emai_0905.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0905.png)'
- en: Figure 9-5\. Nodes that spread fake news are labeled in red color. People who
    think alike cluster together in social networks ([image source](https://arxiv.org/pdf/1902.06673.pdf)).
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5。传播假新闻的节点标记为红色。思想相似的人在社交网络中聚集在一起（[图片来源](https://arxiv.org/pdf/1902.06673.pdf)）。
- en: Web-Scale Recommendation Systems
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Web-Scale Recommendation Systems
- en: 'Since 2018, Pinterest has been using [PinSage graph convolutional network](https://arxiv.org/pdf/1806.01973.pdf).
    This curates users’ homefeed and makes suggestions for new and relevant pins.
    The authors utilize *random walks on graphs* in their model, which we will discuss
    later in this chapter. Here is the full abstract: *Recent advancements in deep
    neural networks for graph-structured data have led to state-of-the-art performance
    on recommender system benchmarks. However, making these methods practical and
    scalable to web-scale recommendation tasks with billions of items and hundreds
    of millions of users remains a challenge. Here we describe a large-scale deep
    recommendation engine that we developed and deployed at Pinterest. We develop
    a data efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines
    efficient random walks and graph convolutions to generate embeddings of nodes
    (i.e., items) that incorporate both graph structure as well as node feature information.
    Compared to prior GCN approaches, we develop a novel method based on highly efficient
    random walks to structure the convolutions and design a novel training strategy
    that relies on harder-and-harder training examples to improve robustness and convergence
    of the model. We deploy PinSage at Pinterest and train it on 7.5 billion examples
    on a graph with 3 billion nodes representing pins and boards, and 18 billion edges.
    According to offline metrics, user studies and A/B tests, PinSage generates higher-quality
    recommendations than comparable deep learning and graph-based alternatives. To
    our knowledge, this is the largest application of deep graph embed- dings to date
    and paves the way for a new generation of web-scale recommender systems based
    on graph convolutional architectures.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自2018年以来，Pinterest一直在使用[PinSage图卷积网络](https://arxiv.org/pdf/1806.01973.pdf)。这个网络整理用户的主页并为新的相关的图钉提供建议。作者在他们的模型中使用了*图上的随机游走*，我们将在本章后面讨论。以下是完整的摘要：*针对图结构数据的深度神经网络的最新进展已经在推荐系统基准测试中取得了最先进的性能。然而，使这些方法实用并可扩展到拥有数十亿个项目和数亿用户的网络规模推荐任务仍然是一个挑战。在这里，我们描述了一个我们在Pinterest开发和部署的大规模深度推荐引擎。我们开发了一种数据高效的图卷积网络（GCN）算法PinSage，它结合了高效的随机游走和图卷积，生成了结合了图结构和节点特征信息的节点（即项目）嵌入。与先前的GCN方法相比，我们开发了一种基于高效随机游走的新方法来构造卷积，并设计了一种依赖于越来越难的训练示例来改善模型的鲁棒性和收敛性的新型训练策略。我们在Pinterest部署了PinSage，并在拥有30亿个代表图钉和画板的节点和180亿条边的图上对其进行了750亿个示例的训练。根据离线指标、用户研究和A/B测试，PinSage生成的推荐比可比的深度学习和基于图的替代方法具有更高的质量。据我们所知，这是迄今为止最大规模的深度图嵌入应用，并为基于图卷积架构的新一代网络规模推荐系统铺平了道路。*
- en: Fighting Cancer
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗癌症
- en: 'In the 2019 article, [*HyperFoods: Machine intelligent mapping of cancer-beating
    molecules in foods*](https://www.nature.com/articles/s41598-019-45349-y), the
    authors use protein, gene, and drug interaction data to identify the molecules
    that help prevent and beat cancer. They also map the foods that are the richest
    in cancer beating molecules (see [Figure 9-6](#Fig_HyperFoods)). Again, the authors
    utilize random walks on graphs. Here’s the abstract of the paper: *Recent data
    indicate that up to 30–40% of cancers can be prevented by dietary and lifestyle
    measures alone. Herein, we introduce a unique network based machine learning platform
    to identify putative food based cancer beating molecules. These have been identified
    through their molecular biological network commonality with clinically approved
    anti-cancer therapies. A machine-learning algorithm of random walks on graphs
    (operating within the supercomputing DreamLab platform) was used to simulate drug
    actions on human interactome networks to obtain genome-wide activity profiles
    of 1962 approved drugs (199 of which were classified as “anti-cancer” with their
    primary indications). A supervised approach was employed to predict cancer-beating
    molecules using these ‘learned’ interactome activity profiles. The validated model
    performance predicted anti-cancer therapeutics with classification accuracy of
    84–90%. A comprehensive database of 7962 bioactive molecules within foods was
    fed into the model, which predicted 110 cancer-beating molecules (defined by anti-cancer
    drug likeness threshold of >70%) with expected capacity comparable to clinically
    approved anti-cancer drugs from a variety of chemical classes including flavonoids,
    terpenoids, and polyphenols. This in turn was used to construct a ‘food map’ with
    anti-cancer potential of each ingredient defined by the number of cancer-beating
    molecules found therein. Our analysis underpins the design of next-generation
    cancer preventative and therapeutic nutrition strategies.*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '在2019年的文章[*HyperFoods: 机器智能映射食物中的抗癌分子*](https://www.nature.com/articles/s41598-019-45349-y)中，作者利用蛋白质、基因和药物相互作用数据来识别有助于预防和战胜癌症的分子。他们还绘制了富含抗癌分子的食物（见[图9-6](#Fig_HyperFoods)）。作者再次利用图上的随机游走。以下是论文的摘要：*最近的数据表明，30-40%的癌症可以仅通过饮食和生活方式措施来预防。在这里，我们介绍了一种独特的基于网络的机器学习平台，用于识别可能的基于食物的抗癌分子。这些分子是通过它们与临床批准的抗癌疗法的分子生物学网络共同性来识别的。使用图上的随机游走机器学习算法（在超级计算DreamLab平台内运行），模拟药物在人类相互作用网络上的作用，以获得1962种批准的药物的全基因组活性概况（其中199种被分类为“抗癌”并具有它们的主要适应症）。采用监督方法来预测抗癌分子，使用这些‘学习’相互作用活性概况。经验证的模型性能预测抗癌治疗药物的分类准确率为84-90%。7962种食物中的生物活性分子的全面数据库被输入模型，预测出110种抗癌分子（通过抗癌药物相似性阈值定义为>70%），其预期能力与来自各种化学类别的临床批准的抗癌药物相当。这反过来被用来构建一个‘食物地图’，其中每种成分的抗癌潜力由其中发现的抗癌分子数量来定义。我们的分析支持下一代癌症预防和治疗营养策略的设计。*'
- en: '![250](assets/emai_0906.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0906.png)'
- en: 'Figure 9-6\. HyperFoods: Machine intelligent mapping of cancer-beating molecules
    in foods. The bigger the node the more diverse the set of cancer beating molecules.
    ([image source](https://www.nature.com/articles/s41598-019-45349-y)).'
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 超级食物：食物中癌症抗击分子的机器智能映射。节点越大，癌症抗击分子集合越多样化。([图片来源](https://www.nature.com/articles/s41598-019-45349-y))。
- en: Biochemical Graphs
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生化图
- en: We can represent molecules and chemical compounds as graphs where the nodes
    are the atoms the edges are the chemical bonds between them. Data sets from this
    chemoinformatics domain are useful for assessing a classification model’s performance.
    For example, the [NCI-1](https://paperswithcode.com/dataset/nci1) dataset, containing
    around 4100 chemical compounds, is useful for anti-cancer screens where the chemicals
    are labeled as positive or negative to hinder cell lung cancer. Similar labeled
    graph data sets for proteins and other compounds are available on the same website,
    along with the papers that employ them and the performance of different models
    on these data sets.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将分子和化合物表示为图，其中节点是原子，边是它们之间的化学键。来自化学信息学领域的数据集对于评估分类模型的性能很有用。例如，包含约4100种化合物的[NCI-1](https://paperswithcode.com/dataset/nci1)数据集，对于抗癌筛选非常有用，其中化学品被标记为阻碍肺癌细胞的阳性或阴性。同一网站上还提供了蛋白质和其他化合物的类似标记图数据集，以及使用它们的论文以及不同模型在这些数据集上的性能。
- en: Molecular Graph Generation For Drug And Protein Structure Discovery
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 药物和蛋白质结构发现的分子图生成
- en: In the last chapter we learned how generative networks such as variational autoencoders
    and adversarial networks learn joint probability distributions from the data in
    order to generate similar looking data for various purposes. Generative networks
    for graphs build on similar ideas, however, they are a bit more involved than
    networks generating images per se. Generative graph networks generate new graphs
    either in a sequential manner, outputting nodes and edges step by step, or in
    a global manner, outputting a whole graph’s adjacency matrix at once. See for
    example the survey [paper on generative graph networks (2020)](https://arxiv.org/pdf/2007.06686.pdf)
    for details on the topic.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了生成网络如变分自动编码器和对抗网络如何从数据中学习联合概率分布，以便为各种目的生成外观相似的数据。生成图网络建立在类似的思想基础上，但是比单纯生成图像的网络更复杂。生成图网络以顺序方式生成新图，逐步输出节点和边，或者以全局方式一次性输出整个图的邻接矩阵。有关该主题的详细信息，请参见调查报告[生成图网络(2020)](https://arxiv.org/pdf/2007.06686.pdf)。
- en: Citation Networks
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引文网络
- en: 'In citation networks, the nodes could be the authors and the edges are their
    co-authorships, or the nodes are papers and the (directed) edges are the citations
    between them: Each paper has directed edges pointing to the papers it cites. Features
    for each paper include its abstract, authors, year, venue, title, field of study,
    and others. Tasks include node clustering, node classification, and link prediction.
    Popular data sets for paper citation networks include Cora, Citeseer and Pubmed.
    The [Cora data set](https://sites.google.com/site/semanticbasedregularization/home/software/experiments_on_cora)
    contains around three thousand machine learning publications grouped into seven
    categories. Each paper in citation networks is represented by a one-hot vector
    indicating the presence or absence of a word from a prespecified dictionary, or
    by a term frequency-inverse document frequency (TF-IDF) vector. These data sets
    are updated continuously as more papers join the networks.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在引文网络中，节点可以是作者，边是他们的合著关系，或者节点是论文，(有向)边是它们之间的引用：每篇论文都有指向它引用的论文的有向边。每篇论文的特征包括其摘要、作者、年份、会议、标题、研究领域等。任务包括节点聚类、节点分类和链接预测。用于论文引文网络的流行数据集包括Cora、Citeseer和Pubmed。[Cora数据集](https://sites.google.com/site/semanticbasedregularization/home/software/experiments_on_cora)包含约三千篇机器学习出版物，分为七个类别。引文网络中的每篇论文都由一个独热向量表示，指示预定字典中单词的存在或不存在，或者由词频-逆文档频率(TF-IDF)向量表示。随着更多论文加入网络，这些数据集将不断更新。
- en: Social Media Networks And Social Influence Prediction
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社交媒体网络和社交影响预测
- en: 'Social media networks such as Facebook, Twitter, Instagram, and Reddit are
    a distinctive feature of our time (after 2010). The [Reddit data set](https://paperswithcode.com/dataset/reddit)
    is an example of the available datasets: This is a graph where the nodes are the
    posts and the edges are between two posts which have comments from the same user.
    The posts are also labeled with the community to which they belong.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体网络如Facebook、Twitter、Instagram和Reddit是我们这个时代(2010年后)的一个独特特征。[Reddit数据集](https://paperswithcode.com/dataset/reddit)是可用数据集的一个例子：这是一个图，其中节点是帖子，边是两个帖子之间的评论来自同一用户。这些帖子还标有它们所属的社区。
- en: Social media networks and their social influence have a substantial impact on
    our societies, ranging from advertising to winning presidential elections to toppling
    political regimes. One important task for a graph model representing social networks
    is to predict the social influence of the nodes in the network. Here, the nodes
    are the users and their interactions are the edges. Features include users’ gender,
    age, sex, location, activity level, and others. One way to quantify social influence,
    the target variable, is through predicting the actions of a user given the actions
    of their near neighbors in the network. For example, if a user’s friends buy a
    product, what is the probability that they will buy the same product after a given
    period of time? Random walks on graphs help predict the social influence of certain
    nodes in a network.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体网络及其社会影响对我们的社会产生了重大影响，从广告到赢得总统选举再到推翻政治制度。代表社交网络的图模型的一个重要任务是预测网络中节点的社会影响。在这里，节点是用户，他们的互动是边缘。特征包括用户的性别、年龄、性别、位置、活动水平等。量化社会影响的一种方法是通过预测用户在网络中的邻近邻居的行为来预测目标变量。例如，如果用户的朋友购买了一个产品，那么在一段时间后他们购买相同产品的概率是多少？图上的随机游走有助于预测网络中某些节点的社会影响。
- en: Sociological Structures
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社会结构
- en: Social diagrams are directed graphs that represent relationships among individuals
    in a society or among groups of individuals. The nodes are the members of the
    society or the groups, and the directed edges are the relationships between these
    members, such as admiration, association, influence, and others. We are interested
    in the connectedness, separability, size of fragments, and so forth, in these
    social diagrams. One example is from anthropological studies where a number of
    tribes are classified according to their kinship structures.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 社会图是代表社会中个人或个人群体之间关系的有向图。节点是社会成员或群体，有向边是这些成员之间的关系，如钦佩、联合、影响等。我们对这些社会图中的连通性、可分离性、片段大小等感兴趣。一个例子来自人类学研究，根据他们的亲属结构对一些部落进行分类。
- en: Bayesian Networks
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯网络
- en: 'Soon in this chapter we will discuss Bayesian networks. These are probabilistic
    graph models whose goal is one we are very familiar with in the AI field: To learn
    joint probability distribution of the features of a data set. Bayesian networks
    consider this joint probability distribution as a product of single variable distributions
    *conditional only on a nodes’ parents* in a graph representing the relationships
    between the features of the data. That is, the nodes are the feature variables
    and the edges are between the features that *we believe* to be connected. Applications
    include spam filtering, voice recognition, coding and decoding, to name a few.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 很快在本章中我们将讨论贝叶斯网络。这些是概率图模型，其目标是我们在人工智能领域非常熟悉的目标：学习数据集特征的联合概率分布。贝叶斯网络将这个联合概率分布视为表示数据特征之间关系的图中*仅条件于节点的父节点*的单变量分布的乘积。也就是说，节点是特征变量，边缘是我们*认为*相连的特征之间的边缘。应用包括垃圾邮件过滤、语音识别、编码和解码等。
- en: Traffic Forecasting
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交通预测
- en: Traffic prediction is the task of predicting traffic volumes using historical
    road maps, road speed, and traffic volume data. There are [benchmark traffic data
    sets](https://paperswithcode.com/task/traffic-prediction) which we can use to
    track progress and compare models. For example, the [METR-LA](https://www.researchgate.net/figure/The-dataset-was-collected-from-the-highway-of-Los-Angeles-County-METR-LA-from-1-March_fig7_344150366)
    is a spatial-temporal graph data set, containing four months of traffic data collected
    by 207 sensors on the highways of LA County. The traffic network is a graph, where
    the nodes are the sensors, and the edges are the road segments between these sensors.
    At a certain time *t*, the features are traffic parameters such as velocity and
    volume. The task of a graph neural network is to predict the features of the graph
    after certain time has elapsed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 交通预测是使用历史道路地图、道路速度和交通量数据来预测交通量的任务。有[基准交通数据集](https://paperswithcode.com/task/traffic-prediction)，我们可以用来跟踪进展并比较模型。例如，[METR-LA](https://www.researchgate.net/figure/The-dataset-was-collected-from-the-highway-of-Los-Angeles-County-METR-LA-from-1-March_fig7_344150366)是一个时空图数据集，包含洛杉矶县高速公路上207个传感器收集的四个月的交通数据。交通网络是一个图，其中节点是传感器，边缘是这些传感器之间的道路段。在某个时间*t*，特征是交通参数，如速度和量。图神经网络的任务是在经过一定时间后预测图的特征。
- en: 'Other traffic forecasting models employ [Bayesian networks](http://bigeye.au.tsinghua.edu.cn/english/paper/A%20Bayesian%20Network%20Approach%20to.pdf):
    Traffic flows among adjacent road links. The model uses information from adjacent
    road links to analyze the trends of focus links.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 其他交通预测模型采用[贝叶斯网络](http://bigeye.au.tsinghua.edu.cn/english/paper/A%20Bayesian%20Network%20Approach%20to.pdf)：相邻道路链接之间的交通流量。该模型利用相邻道路链接的信息来分析焦点链接的趋势。
- en: Logistics And Operations Research
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 物流与运营研究
- en: We can model and solve many problems in operations research, such as transportation
    problems and activity networks, using graphs. The graphs involved are usually
    weighted directed graphs. Operations research problems are combinatorial in nature,
    and are always trivial if the network is small. However, for large real world
    networks, the challenge is finding efficient algorithms that can sift through
    the enormous search space and quickly rule out big parts of it out. A large part
    of the research literature deals with estimating the computational complexity
    of such algorithms. This is called combinatorial optmization. Typical problems
    include the *travelling salesman problem*, *supply chain optimization*, *shared
    rides routing and fares*, *job matching*, and others. Some of the graph methods
    and algorithms used for such problems are minimal spanning trees, shortest paths,
    max flow min cuts, and matching in graphs. We will visit operations research examples
    later in this book.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用图来建模和解决运筹学中的许多问题，例如运输问题和活动网络。涉及的图通常是加权有向图。运筹学问题具有组合性质，如果网络很小，问题总是微不足道的。然而，对于大型现实世界网络，挑战在于找到能够快速筛选出大部分搜索空间并迅速排除其中大部分内容的高效算法。研究文献的很大一部分涉及估计这些算法的计算复杂性。这被称为组合优化。典型问题包括“旅行推销员问题”、“供应链优化”、“共享乘车路线和票价”、“工作匹配”等。用于解决这些问题的一些图方法和算法包括最小生成树、最短路径、最大流最小割以及图中的匹配。我们将在本书的后面讨论运筹学示例。
- en: Language Models
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型
- en: Graph models are relevant for a variety of natural language tasks. These tasks
    seem different at the surface but many of them boil down to clustering, for which
    graph models are very well suited.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图模型与各种自然语言任务相关。这些任务在表面上看起来不同，但许多任务归结为聚类，而图模型非常适合这种任务。
- en: For any application we must first choose what the nodes, the edges, and the
    features for each represent. For natural language, these choices reveal hidden
    structures and regularities in the language and in the language corpora.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何应用程序，我们必须首先选择节点、边以及每个节点代表的特征。对于自然语言，这些选择揭示了语言和语言语料库中的隐藏结构和规律。
- en: Instead of representing a natural language sentence as a sequence of tokens
    for recurrent models or as a vector of tokens for transformers, in graph models,
    we embed sentences in a graph then employ graph deep learning (or graph neural
    networks).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与将自然语言句子表示为递归模型的令牌序列或变压器的令牌向量不同，在图模型中，我们将句子嵌入到图中，然后使用图深度学习（或图神经网络）。
- en: One example from computational linguistics is constructing diagrams for parsing
    language [Figure 9-7](#Fig_parsing).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 计算语言学中的一个例子是构建解析语言的图表[图9-7](#Fig_parsing)。
- en: '![300](assets/emai_0907.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0907.png)'
- en: Figure 9-7\. A parsed sentence.
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7。解析后的句子。
- en: The nodes are words, n-grams, or phrases, and the edges are the relationships
    between them, which depend on the language’s grammar or syntax (article, noun,
    verb, *etc.*). A language is defined as the set of all strings correctly generated
    from the language’s vocabulary according to its grammar rules. In that sense,
    computer languages are easy to parse (they are built that way), while natural
    languages are much harder to specify completely due to their complex nature.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 节点可以是单词、n-gram或短语，边是它们之间的关系，这取决于语言的语法或语法（冠词、名词、动词等）。语言被定义为根据其语法规则从语言词汇表中正确生成的所有字符串的集合。在这个意义上，计算机语言易于解析（它们是这样构建的），而自然语言由于其复杂性而很难完全指定。
- en: Parsing
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析
- en: Parsing means converting a stream of input into a structured or formal representation
    so it can be automatically processed. The input to a parser might be sentences,
    words, or even characters. The output is a tree diagram containing information
    about the function of each part of the input. Our brain is a great parser for
    language inputs. Computers parse programming languages.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 解析意味着将输入流转换为结构化或正式表示，以便可以自动处理。解析器的输入可以是句子、单词，甚至字符。输出是包含有关输入每个部分功能的信息的树形图。我们的大脑是语言输入的出色解析器。计算机解析编程语言。
- en: 'Another example is news clustering or article recommendations. Here, we use
    graph embeddings of text data to determine text similarity: The nodes can be words
    and the edges can be semantic relationships between the words, or just their co-occurences.
    Or the nodes can be words and documents, and the edges can again be semantic or
    co-occurence relationships. Features for nodes and edges can include authors,
    topics, time periods, and others. Clusters emerge naturally in such graphs.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是新闻聚类或文章推荐。在这里，我们使用文本数据的图嵌入来确定文本相似性：节点可以是单词，边可以是单词之间的语义关系，或者只是它们的共现关系。或者节点可以是单词和文档，边可以再次是语义或共现关系。节点和边的特征可以包括作者、主题、时间段等。在这样的图中，聚类自然而然地出现。
- en: Another type of parsing that does not depend on the syntax or grammar of a language
    is Abstract Meaning Representation (AMR). It relies instead on semantic representation,
    in the sense that sentences which are similar in meaning should be assigned the
    same abstract meaning represenation, even if they are not identically worded.
    Abstract meaning representation graphs are rooted, labeled, directed, acyclic
    graphs, representing full sentences. These are useful for machine translation
    and natural language understanding. There are packages and libraries for abstract
    meaning representation parsing, visualization, and surface generation as well
    as a publicly available data sets.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种不依赖于语言的语法或语法的解析类型是抽象意义表示（AMR）。它依赖于语义表示，即意义相似的句子应该被分配相同的抽象意义表示，即使它们的措辞不完全相同。抽象意义表示图是根据完整句子构建的有根、标记、有向、无环图。这对于机器翻译和自然语言理解非常有用。有用于抽象意义表示解析、可视化和表面生成的软件包和库，以及公开可用的数据集。
- en: 'For other natural language applications, the following survey paper is a nice
    reference that is easy to read and learn more on the subject: [A survey of graphs
    in natural language processing](https://web.eecs.umich.edu/~mihalcea/papers/nastase.jnle15.pdf)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他自然语言应用，以下调查论文是一个很好的参考，易于阅读并了解更多有关主题的信息：[自然语言处理中的图调查](https://web.eecs.umich.edu/~mihalcea/papers/nastase.jnle15.pdf)
- en: Graph Structure Of The Web
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络的图结构
- en: Since the inception of the *world wide web* (www) in 1989, it has grown enormously
    and has become an indespensible tool for billions of people around the world.
    It allows access to billions webpages, documents, and other resources using an
    internet web browser. With billions of pages linking to each other, it is of great
    interest to investigate the graph structure of the web. Mathematically, this vast
    and expansive graph is fascinating in its own right. But understanding this graph
    is important for more reasons than a beautiful mental exercise, providing insights
    into algorithms for crawling, indexing, and ranking the web (as in PageRank algorithm
    that we saw earlier in this chapter), searching communities, and discovering the
    social and other phenomena that characterize its growth or decay.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 自1989年*万维网*（www）的诞生以来，它已经得到了巨大的发展，并已成为全球数十亿人不可或缺的工具。它允许使用互联网浏览器访问数十亿个网页、文档和其他资源。由于数十亿个页面相互链接，研究网络的图结构具有极大的兴趣。从数学上讲，这个广阔而庞大的图本身就是迷人的。但理解这个图对于更多原因至关重要，它提供了对爬取、索引和排名网络的算法（如我们在本章前面看到的PageRank算法）、搜索社区以及发现表征其增长或衰退的社会和其他现象的洞察。
- en: 'The world wide web graph has:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 全球网络图具有：
- en: 'Nodes: Webpages, on the scale of billions.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点：网页，数量达数十亿。
- en: 'Edges: Are directed from one page linking to another page, on the scale of
    hundreds of billions.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边：从一个页面指向另一个页面，数量达数百亿。
- en: 'We are interested in:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的是：
- en: What is the average degree of the nodes?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的平均度是多少？
- en: Degree distributions of the nodes (for both the in degree and out degree, which
    can be very different). Are they power laws? Some other laws?
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的度分布（对于入度和出度，可能非常不同）。它们是否符合幂律？其他一些规律？
- en: 'Connectivity of the graph: What is the percentage of connected pairs?'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的连通性：连接对的百分比是多少？
- en: Average distances between the nodes.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点之间的平均距离。
- en: Is the observed structure of the web dependent or independent of the the particular
    crawl used?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察到的网络结构是否依赖于特定的爬取？
- en: Particular structures of weakly and of strongly connected components.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱连接组件和强连接组件的特定结构。
- en: Is there a giant strongly connected component? What is the proportion of nodes
    that can reach or can be reached from this giant component?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否存在一个巨大的强连通分量？可以到达或可以从这个巨大分量到达的节点的比例是多少？
- en: Automatically Analyzing Computer Programs
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动分析计算机程序
- en: 'We can use graphs for verification of computer programs, program reasoning,
    reliability theory, fault diagnosis in computers, and studying the structure of
    computer memory. The paper, [Graph Neural Networks on Program Analysis](https://miltos.allamanis.com/publicationfiles/allamanis2021graph/allamanis2021graph.pdf:),
    is one example: *Program analysis aims to determine if a program’s behavior complies
    with some specification. Commonly, program analyses need to be defined and tuned
    by humans. This is a costly process. Recently, machine learning methods have shown
    promise for probabilistically realizing a wide range of program analyses. Given
    the structured nature of programs, and the commonality of graph representations
    in program analysis, graph neural networks (GNN) offer an elegant way to represent,
    learn, and reason about programs and are commonly used in machine learning-based
    program analyses. This article discusses the use of graph neural networks for
    program analysis, highlighting two practical use cases: Variable misuse detection
    and type inference.*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用图来验证计算机程序、程序推理、可靠性理论、计算机故障诊断以及研究计算机内存的结构。文章[程序分析中的图神经网络](https://miltos.allamanis.com/publicationfiles/allamanis2021graph/allamanis2021graph.pdf:)是一个例子：*程序分析旨在确定程序的行为是否符合某些规范。通常，程序分析需要由人类定义和调整。这是一个昂贵的过程。最近，机器学习方法已经显示出在概率上实现各种程序分析的潜力。鉴于程序的结构化特性，以及程序分析中图表示的普遍性，图神经网络（GNN）提供了一种优雅的方式来表示、学习和推理程序，并广泛用于基于机器学习的程序分析。本文讨论了图神经网络在程序分析中的应用，重点介绍了两个实际用例：变量误用检测和类型推断。*
- en: Data Structures In Computer Science
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算机科学中的数据结构
- en: A data structure in computer science is a structure that stores, manages, and
    organizes data. There are different data structures and they are usually chosed
    in a way that makes it efficient to access the data (reads, writes, appends, infer
    or store relationships, *etc.*).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学中，数据结构是一种存储、管理和组织数据的结构。有不同的数据结构，它们通常被选择为使访问数据更有效（读取、写入、附加、推断或存储关系等）。
- en: Some data structures use graphs to organize data, computational devices in a
    cluster, and represent the flow of data and computation or the communication network.
    There are also graph databases geared towards storing and querying of graph data.
    Other databases transform graph data to more structured formats (such as relational
    formats).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据结构使用图来组织数据，计算设备在一个集群中，并表示数据和计算或通信网络的流动。还有一些面向存储和查询图数据的图数据库。其他数据库将图数据转换为更结构化的格式（如关系格式）。
- en: 'Here are some examples for graph data structures:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些图数据结构的示例：
- en: We have already encountered PageRank algorithm, along with the link structure
    of a website represented as a directed graph, where the nodes are the webpages
    and the edges represented the links from one page to another. A database keeping
    all the webpages along with their link structures can be either graph structured,
    where the graph is stored as is using the linking matrix or adjacency matrix,
    and no transformation necessary, or it can transformed to fit the structure of
    other nongraphical databases.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经遇到了PageRank算法，以及网站的链接结构表示为有向图，其中节点是网页，边表示从一个页面到另一个页面的链接。一个数据库保存所有网页以及它们的链接结构可以是图结构化的，其中图以链接矩阵或邻接矩阵的形式存储，无需进行任何转换，或者可以转换以适应其他非图形数据库的结构。
- en: 'Binary search trees for organizing files in a database: Binary search trees
    are ordered data structures that are efficient for both random and sequential
    access of records, and for modification of files. The inherent order of a binary
    search tree speeds up search time: We cut the amount of data to sort through by
    half at each level of the tree. It also speeds up insersion time: Unlike an array,
    when we add a node to the binary tree data structure, we create a new piece in
    memory and link to it. This is faster than creating a new large array then inserting
    the data from the smaller array to the new, larger one.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于组织数据库中文件的二叉搜索树：二叉搜索树是有序的数据结构，对于随机和顺序访问记录以及文件的修改都很高效。二叉搜索树的固有顺序加快了搜索时间：在树的每个级别，我们将要排序的数据量减少了一半。它还加快了插入时间：与数组不同，当我们向二叉树数据结构添加一个节点时，我们在内存中创建一个新的部分并链接到它。这比创建一个新的大数组然后将数据从较小的数组插入到新的更大数组要快。
- en: 'Graph based information retrieval systems: In some information retrieval systems
    we assign a certain number of index terms to each document. We can think of these
    as the documents’ indicators, descriptors, or keywords. These index terms are
    represented will be nodes of a graph. We connect two index terms with an undirected
    edge if these two happen to be closely related, such as the indices *graph* and
    *network*. The resulting *similarity graph* is very large, and is possibly disconnected.
    The maximally connected subgraphs of this graph are its *components*, and they
    naturally classify the documents in this system. For information retrieval, our
    query specifies some index terms, that is, certain nodes of the graph, and the
    sytem returns the maximal complete subgraph that includes the corresponding nodes.
    This gives the complete list of index terms which in turn specify the documents
    we are searching for.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于图的信息检索系统：在一些信息检索系统中，我们为每个文档分配一定数量的索引项。我们可以将这些视为文档的指示符、描述符或关键词。这些索引项将被表示为图的节点。如果两个索引项密切相关，例如索引*图*和*网络*，我们用无向边连接这两个索引项。结果的*相似性图*非常庞大，可能是不连通的。该图的最大连接子图是其*组件*，它们自然地对该系统中的文档进行分类。对于信息检索，我们的查询指定了一些索引项，即图的某些节点，系统返回包含相应节点的最大完整子图。这给出了指定我们正在搜索的文档的完整索引项列表。
- en: Load Balancing In Distributed Networks
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在分布式网络中的负载均衡
- en: 'The computational world has grown from Moore’s Law to parallel computing to
    cloud computing. In cloud computing, neither our data nor our files nor the machines
    executing our files and doing computations on our data are near us. They are not
    even near each other. As applications become more complex and as network traffic
    increases, we need the software or hardware analogue of a network traffic cop,
    that distributes network traffic across multiple servers, so that no single server
    bears a heavy load, enhancing performance in terms of application response times,
    end user experience, and others. As traffic increases, more applicances, or nodes,
    need to be added to handle the volume. Network traffic distributing needs to be
    done while preserving data security and privacy, and should be able to predict
    traffic bottlenecks before they happen. This is exactly what load balancers do.
    It is not hard to imagine the distributed network as a graph with the nodes as
    the connected servers and appliances. Load balancing is then a traffic flow problem
    on a given graph, and there are a variety of algorithms for allocating the load.
    All algorithms operate on the network’s graph. Some are static, allocating loads
    without updating the network’s current state in terms of loads or malfunctioning
    units, others are dynamic, but requiring constant communication within the network
    about the nodes’ statuses. The following are some algorithms:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 计算世界已经从摩尔定律发展到并行计算再到云计算。在云计算中，我们的数据、文件以及执行我们文件和对我们数据进行计算的机器都不在我们附近。它们甚至不彼此靠近。随着应用程序变得更加复杂，网络流量增加，我们需要软件或硬件的网络流量分配器，将网络流量分配到多个服务器上，以便没有单个服务器承担沉重负载，从而提高应用程序响应时间、最终用户体验等性能。随着流量增加，需要添加更多的设备或节点来处理体积。在保护数据安全和隐私的同时，需要进行网络流量分配，并且应该能够在发生之前预测流量瓶颈。这正是负载均衡器所做的。可以将分布式网络想象成一个图，其中节点是连接的服务器和设备。然后负载均衡是在给定图上的流量问题，有各种算法用于分配负载。所有算法都在网络的图上运行。有些是静态的，分配负载而不更新网络的当前状态，有些是动态的，但需要网络内关于节点状态的持续通信。以下是一些算法：
- en: 'Least connection algorithm: This method directs traffic to the server with
    the fewest active connections.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最少连接算法：这种方法将流量引导到具有最少活动连接的服务器。
- en: 'Least response time algorithm: This method directs traffic to the server with
    the fewest active connections and the lowest average response time.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小响应时间算法：这种方法将流量引导到具有最少活动连接和最低平均响应时间的服务器。
- en: 'Round Robin algorithm: This algorithm allocates load in a rotation on the servers:
    First it directs traffic to the first available server, then it moves that server
    to the bottom of the queue.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轮询算法：该算法在服务器上轮流分配负载：首先将流量引导到第一个可用的服务器，然后将该服务器移动到队列的底部。
- en: 'IP Hash: This method allocates servers based on the IP address of the client.'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IP哈希：这种方法根据客户端的IP地址分配服务器。
- en: Artificial Neural Networks
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Finally, artificial neural networks themselves are graphs where the nodes are
    the computational units and the edges are inputs and outputs of these units. [Figure 9-8](#Fig_neural_networks_graphs)
    summarizes popular artificial neural network models as graphs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，人工神经网络本身是图，其中节点是计算单元，边是这些单元的输入和输出。[图9-8](#Fig_neural_networks_graphs)总结了流行的人工神经网络模型作为图。
- en: '![250](assets/emai_0908.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0908.png)'
- en: Figure 9-8\. Neural networks as graphs ([image source](https://www.asimovinstitute.org/author/fjodorvanveen/)).
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. 神经网络作为图（[图片来源](https://www.asimovinstitute.org/author/fjodorvanveen/)）。
- en: Random Walks On Graphs
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图上的随机行走
- en: 'A random walk on a graph ([Figure 9-9](#Fig_random_walk_graph)) means exactly
    what it says: A sequence of steps that starts at some node, and at each time step
    chooses a neighboring node (using the adjacency matrix) with probability proportional
    to the weights of the edges, and moves there.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图上的随机行走（[图9-9](#Fig_random_walk_graph)）的意思就是：一系列步骤从某个节点开始，每个时间步骤选择一个相邻节点（使用邻接矩阵），概率与边的权重成比例，并移动到那里。
- en: '![300](assets/emai_0909.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0909.png)'
- en: Figure 9-9\. A random walker on an undirected graph.
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9\. 无向图上的随机行走者。
- en: 'If the edges are unweighted, then the neighboring nodes are all equally likely
    to be chosen for the walk’s move. At any time step, the walk can stay at the same
    node in the case there is a self edge, or when it is a lazy random walk, when
    there is a positive probability that the walker stays at a node instead of moving
    to one of its neigbours. We are interested in the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果边没有权重，那么相邻节点被选择作为行走移动的概率是相等的。在任何时间步骤中，如果存在自环，行走可以停留在同一节点，或者在进行懒惰随机行走时，行走者有一定概率停留在一个节点而不是移动到其邻居之一。我们感兴趣的是以下内容：
- en: What is the list of nodes visited by a random walk, in the order they are visited?
    Here, the starting point and the structure of the graph matter in how much a walker
    covers or whether the walk can ever reach certain regions of the graph. In graph
    neural networks, one is interested in learning a representation for a given node
    based on its neighbours’ features. In large graphs where nodes have more neighbors
    than to be computationally feasible, we employ random walks. However, we have
    to be careful that different parts of the graph have different random walk expansion
    speeds, so if we do not take that into account by adjusting the nummber of steps
    of a random walk according to the subgraph structure, we might end up with low
    quality represenations for the nodes, and undesirable outcomes as these go down
    the work pipeline.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机行走访问的节点列表，按访问顺序排列是什么？在这里，起点和图的结构对行走者覆盖范围或行走是否能到达图的某些区域有影响。在图神经网络中，人们希望根据邻居的特征学习给定节点的表示。在大图中，节点的邻居比可计算的要多，我们使用随机行走。然而，我们必须小心，图的不同部分具有不同的随机行走扩展速度，因此如果我们不根据子图结构调整随机行走的步数，我们可能会得到节点的低质量表示，并且随着这些表示进入工作流程，可能会产生不良后果。
- en: What is the expected behavior of a random walk, that is, the probability distribution
    over the visited nodes after a certain number of steps. We can study basic properties
    of a random walk, such as its long term behavior, by using *the spectrum of the
    graph* which is the set of eigenvalues of its adjacency matrix. In general, the
    spectrum of an operator helps us undertsand what happens when we repeatedly apply
    the operator. Randomly walking on a graph is equivalent to repeatedly applying
    a normalized version of the adjacency matrix to the node of the graph where we
    started. Every time we apply this *random walk matrix*, we walk one step further
    on the graph.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机行走的预期行为是什么，即在一定步数后访问节点的概率分布。我们可以通过使用*图的谱*，即其邻接矩阵的特征值集合，来研究随机行走的基本属性，比如其长期行为。一般来说，运算符的谱帮助我们理解当我们重复应用运算符时会发生什么。在图上随机行走等同于重复应用邻接矩阵的归一化版本到我们开始的图的节点上。每次应用这个*随机行走矩阵*，我们在图上向前走一步。
- en: How does a random walk behave on different types of graphs, such as paths, trees,
    two fully connected graphs joined by one edge, infinite graphs, and others?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机行走在不同类型的图上的行为如何，比如路径、树、两个完全连接的图通过一条边连接、无限图等？
- en: For a given graph, does the walk ever return to its starting point? If so how
    long do we have to walk until we return?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的图，行走是否会回到起点？如果是，我们需要走多久才能回到？
- en: How long do we have to walk until we reach a specific node?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要走多久才能到达特定节点？
- en: How long do we have to walk until we visit all the nodes?
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要走多久才能访问所有节点？
- en: How does a random walk *expand*? That is, what is the *influence distribution*
    of certain nodes that belong in certain regions of the graph? What is the size
    of their influence?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机行走如何*扩展*？也就是说，属于图中某些区域的特定节点的*影响分布*是什么？他们的影响大小是多少？
- en: Can we design algorithms based on random walks that are able to reach *obscure*
    parts of large graphs?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们能否设计基于随机行走的算法，能够到达大图的*隐秘*部分？
- en: Random walks and Brownian motion
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机行走和布朗运动
- en: In the limit where the size of steps of a random walk goes to zero, we obtain
    a Brownian motion. Brownian motion usually models the random fluctuations of particles
    suspended in a medium such as a fluid, or price fluctuations of derivatives in
    financial markets. We frequently encounter the term Brownian motion with the term
    *Weiner process*, which is a continuous stochastic process with clear mathematical
    definition on how the motion (of a particle or of a price fluctuation in finance)
    starts (at zero), how the next step is sampled (from the normal distribution and
    with independent increments), and assumptions about its continuity as a function
    of time (it is almost surely continuous). Another term it is associated with is
    *martingale*. We will see these in [Chapter 11](ch11.xhtml#ch11) on Probability.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当随机行走的步长大小趋近于零时，我们得到布朗运动。布朗运动通常模拟悬浮在介质中的粒子的随机波动，比如液体中的粒子或者金融市场中衍生品的价格波动。我们经常将布朗运动与*维纳过程*一起提及，维纳过程是一个连续随机过程，对于运动（粒子或金融中价格波动）的开始（从零开始）、下一步的采样（从正态分布中采样并具有独立增量）以及关于其时间连续性的假设（几乎肯定是连续的）有明确的数学定义。另一个相关的术语是*鞅*。我们将在[第11章](ch11.xhtml#ch11)中的概率部分看到这些内容。
- en: We did encounter a random walk once in this chapter when discussing PageRank
    algorithm, where a random webpage surfer randomly chooses to move from the page
    she’s at to a neighbouring page on the web. We noticed that the long term behavior
    of the walk is discovered when we repeatedly apply the linking matrix of the graph,
    which is the same as the adjacency matrix normalized by the degrees of each node.
    In the next section we see more uses of random walks for graph neural networks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中讨论PageRank算法时曾遇到过随机行走，其中一个随机网页浏览者随机选择从当前页面移动到网络中的相邻页面。我们注意到当我们反复应用图的链接矩阵时，随机行走的长期行为会被发现，这与每个节点的度数归一化的邻接矩阵相同。在下一节中，我们将看到更多关于图神经网络中随机行走的用途。
- en: We can use random walks (on directed or undirected graphs, weighted or unweighted
    graphs) for community detection and influence maximization in small networks,
    where we would only need the graph’s adjacency matrix (as opposed to node embedding
    into feature vectors then clustering).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用随机行走（在有向或无向图、加权或非加权图上）进行社区检测和影响最大化，其中我们只需要图的邻接矩阵（而不是将节点嵌入到特征向量中然后进行聚类）。
- en: Node Representation Learning
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点表示学习
- en: Before implementing any graph tasks on a machine, we must be able to reperesent
    the nodes of the graph as vectors that contain information about their position
    in the graph and their features relative to their locality within the graph. A
    node’s represenation vector is usually aggregated from the node’s own features
    and those features of the nodes surounding it.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器上实现任何图任务之前，我们必须能够将图的节点表示为包含有关其在图中位置和相对于图中局部性的特征的向量。节点的表示向量通常是从节点自身的特征和周围节点的特征聚合而来的。
- en: 'There are different ways to aggregate features, transform them, or even to
    choose which of the neighbouring nodes contribute to the feature represenation
    of a given node. Let’s go over few methods:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来聚合特征、转换特征，甚至选择哪些邻居节点对给定节点的特征表示有贡献。让我们看看一些方法：
- en: Traditional node representation methods rely on subgraph summary statistics.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的节点表示方法依赖于子图摘要统计。
- en: In other methods, nodes that occur together on short random walks will have
    similar vector representations.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其他方法中，一起出现在短随机行走中的节点将具有相似的向量表示。
- en: Other methods take into account that random walks tend to spread differently
    on different graph substructures, so a node’s represenation method adapt to the
    local substructure that the node belongs in, deciding on an appropriate radius
    on influence for each node depending on the topology of the subgraph it belongs
    to.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他方法考虑到随机行走在不同的图子结构上传播方式不同，因此节点的表示方法会根据节点所属的局部子结构进行调整，根据所属子图的拓扑结构决定每个节点的适当影响半径。
- en: Yet other methods produce a multi-scale representation by multiplying feature
    vector of a node with powers of a random walk matrice.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他方法通过将节点的特征向量与随机行走矩阵的幂相乘来产生多尺度表示。
- en: Other methods allow for nonlinear aggregation of the feature vectors of a node
    and its neighbours.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他方法允许对节点及其邻居的特征向量进行非线性聚合。
- en: It is also important to determine how large of a neighbourhood a node draws
    information from (influence distribution), that is, to find the range of nodes
    whose features affect a given node’s representation. This is analogous to sensitivity
    analysis in statistics, but here we need to determine the sensitivity of a node’s
    representation to changes in the features of the nodes surrounding it.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 确定节点从多大范围的邻域中获取信息（影响分布）也很重要，也就是找到哪些节点的特征会影响给定节点的表示。这类似于统计学中的敏感性分析，但在这里我们需要确定节点的表示对周围节点特征变化的敏感性。
- en: After creating a node represenation vector, we feed it into another machine
    learning model during training, such as a support vector machine model for classification,
    just like feeding other features of the data into the model. For example, We can
    learn the feature vector of every user in a social network, then pass these vectors
    into a classification model along with other features, to predict if the user
    is a fake news spreader or not. However, we do not have to rely on a machine learning
    model downstream to classify nodes. We can do that directly from the graph structural
    data where we can predict a node’s class depending on its association with other
    local nodes. The graph can be only partially labeled, and the task is to predict
    the rest of the labels. Moreover, the node representation step can either be a
    preprocessing step, or one part of an end to end model such as a graph neural
    network.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 创建节点表示向量后，在训练期间将其输入到另一个机器学习模型中，例如支持向量机模型进行分类，就像将数据的其他特征输入模型一样。例如，我们可以学习社交网络中每个用户的特征向量，然后将这些向量与其他特征一起传递到分类模型中，以预测用户是否是虚假新闻传播者。但是，我们不必依赖下游的机器学习模型来对节点进行分类。我们可以直接从图结构数据中进行分类，在那里我们可以根据其与其他局部节点的关联来预测节点的类别。图可能只有部分标记，任务是预测其余标签。此外，节点表示步骤可以是预处理步骤，也可以是端到端模型的一部分，例如图神经网络。
- en: Tasks For Graph Neural Networks
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图神经网络的任务
- en: After going through the linear algebra formulation of graphs, applications of
    graph models, random walks on graphs, and vector node representations that encode
    node features along with their zones of influence within the graph, we should
    have a good idea about the kind of tasks that graph neural networks can perform.
    Let’s go through some of these.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 经过线性代数图形式化、图模型应用、图上的随机游走以及编码节点特征及其在图中影响区域的向量节点表示，我们应该对图神经网络可以执行的任务有一个良好的了解。让我们来看看其中一些。
- en: Node Classification
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节点分类
- en: In articles citation network, such as in Citeseer or Cora, where the nodes are
    academic papers (given as bag-of-words vectors) and the directed edges are the
    citations between the papers, classify each paper into a specific discipline.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文章引用网络中，比如在Citeseer或Cora中，其中节点是学术论文（表示为词袋向量），有向边是论文之间的引用，将每篇论文分类到特定学科中。
- en: In the Reddit dataset, where the nodes are comment posts (given as word vectors)
    and undirected edges are between comments posted by the same user, classify each
    post according to the community it belongs to.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Reddit数据集中，其中节点是评论帖子（表示为词向量），无向边是由同一用户发布的评论之间的关系，根据其所属社区对每个帖子进行分类。
- en: The Protein-Protein Interaction network dataset contains 24 graphs where the
    nodes are labeled with the gene ontology sets (do not worry about the medical
    technical names, focus on the math instead. This is the nice thing about mathematical
    modeling, it works the same way for all kinds of applications from all kinds of
    fields, which validates it as a potential underlying language of the universe).
    Usually 20 graphs from the Protein-Protein Interaction network dataset are used
    for training, 2 graphs are used for validation and the rest for testing, each
    corresponding to a human tissue. The features associated with the nodes are positional
    gene sets, motif gene sets, and immunological signatures. Classify each node according
    to its gene ontology set.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蛋白质相互作用网络数据集包含24个图，其中节点标记有基因本体集（不用担心医学技术名称，专注于数学。这是数学建模的好处，它对来自各种领域的各种应用都起作用，这证实了它作为潜在的宇宙基础语言）。通常从蛋白质相互作用网络数据集中选取20个图用于训练，2个图用于验证，其余用于测试，每个对应一个人类组织。与节点相关的特征包括位置基因集、模体基因集和免疫特征。根据其基因本体集对每个节点进行分类。
- en: There are wildlife trade monitoring networks such as [Trade.org](https://www.traffic.org/),
    analyzing dynamic wildlife trade trends and using and updating data sets such
    as [CITES Wildlife Trade Database](https://www.kaggle.com/datasets/cites/cites-wildlife-trade-database)
    the [USADA US Department of Agriculture data commons](https://data.nal.usda.gov/dataset/data-united-states-wildlife-and-wildlife-product-imports-2000%E2%80%932014)
    (this data set includes more than million wildlife or wildlife product shipments,
    representing more than 60 biological classes and more than 3.2 billion live organisms).
    One classification task on the trade network is to classify each node, representing
    a trader (a buyer or a seller) as being engaged in illegal trade activity or not.
    The edges in the network would represent a trade transaction between those buyer
    and seller. Features for the nodes would include personal information of the traders,
    bank account numbers, locations, *etc.*, and features for the edges would include
    transaction idenstification numbers, dates, price tags, the species bought or
    sold, among others. If we already have a subset of the traders labeled as illegal
    traders, our model’s task would then be to predict the label of other nodes in
    the network based on their connections with other nodes (and their features) in
    the network.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有野生动物贸易监测网络，比如[Trade.org](https://www.traffic.org/)，分析动态的野生动物贸易趋势，并使用和更新数据集，比如[CITES野生动物贸易数据库](https://www.kaggle.com/datasets/cites/cites-wildlife-trade-database)和[USADA美国农业部数据共享](https://data.nal.usda.gov/dataset/data-united-states-wildlife-and-wildlife-product-imports-2000%E2%80%932014)（该数据集包括超过一百万个野生动物或野生动物产品运输，代表超过60个生物类别和超过32亿个活体生物）。贸易网络上的一个分类任务是将每个节点分类为从事非法贸易活动的交易者（买方或卖方）或不从事。网络中的边表示这些买方和卖方之间的交易。节点的特征包括交易者的个人信息、银行账号、位置等，边的特征包括交易标识号、日期、价格标签、购买或出售的物种等。如果我们已经有一部分交易者标记为非法交易者，那么我们模型的任务将是根据它们与网络中其他节点（及其特征）的连接来预测网络中其他节点的标签。
- en: Node classification examples lend themselves naturally to semi-supervised learning,
    where only few nodes in the data set come with their labels and the task is to
    label the rest of the nodes. Clean labeled data is what we all should be advocating
    for for our systems to be more accurate, reliable, and transparent.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 节点分类示例自然适合半监督学习，其中数据集中只有少数节点带有标签，任务是为其余节点标记。干净标记的数据是我们所有人都应该倡导的，这样我们的系统才能更准确、可靠和透明。
- en: Graph Classification
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图分类
- en: Sometimes we care for labeling a whole graph as opposed labeling the individual
    nodes of a graph. For example, in the PROTEINS dataset we have a collection of
    chemical compounds each represented as a graph and labeled as either an enzyme
    or not an enzyme. For a graph learning model we would input the nodes, edges,
    their features, the graph structure, and the label for each graph in the dataset,
    creating a whole graph representation or embedding, as opposed to a single node
    representation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们关心将整个图标记为与标记图的个别节点相对。例如，在PROTEINS数据集中，我们有一组化合物，每个化合物都表示为一个图，并标记为酶或非酶。对于图学习模型，我们将输入数据集中每个图的节点、边、它们的特征、图结构和标签，创建整个图表示或嵌入，而不是单个节点表示。
- en: Clustering And Community Detection
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类和社区检测
- en: Clustering in graphs is an important task which discovers communities or groups
    in networks, such as terrorist organizations. One way is to create node and graph
    representations then feed them into traditional clustering methods such as *k*
    means clustering. Other ways produce node and graph represenations that take into
    account the goal of clustering within their design. These can include encoder
    decoder designs and attention mechanisms similar to methods we encountered in
    previous chapters. Other methods are spectral, which means that they rely on the
    eigen values of the Laplacian matrix of the graph. Note that for nongraph data,
    principal component analysis is one method that we used for clustering which is
    also spectral, relying on the singular values of the data table. Computing eigenvalues
    of anything is always an expensive operation so the goal becomes finding ways
    around having to do it. For graphs we can employ graph theoretic methods such
    as *max flow min cut* (we will see this later in this chapter). Different methods
    have their own sets of strengths and shortcomings, for example some employ time
    proven graph theoretic results but fail to include the node or edge features because
    the theory was not developed with any features in mind, let alone a whole bunch
    of them. The message is to always be honest about what our models account and
    do not account for.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的聚类是一个重要的任务，它在网络中发现社区或群组，比如恐怖组织。一种方法是创建节点和图表示，然后将它们输入传统的聚类方法，如*k*均值聚类。其他方法产生节点和图表示，考虑到聚类目标在其设计中。这些方法可以包括编码器解码器设计和类似于我们在之前章节中遇到的方法的注意机制。其他方法是谱方法，这意味着它们依赖于图的拉普拉斯矩阵的特征值。请注意，对于非图数据，主成分分析是我们用于聚类的一种方法，也是谱方法，依赖于数据表的奇异值。计算任何东西的特征值总是一个昂贵的操作，所以目标是找到避免这样做的方法。对于图，我们可以采用图论方法，如*最大流最小割*（我们将在本章后面看到）。不同的方法有各自的优势和缺点，例如一些采用经过时间验证的图论结果，但未包括节点或边的特征，因为该理论并未考虑任何特征，更不用说一大堆特征了。重要的是要始终诚实地说明我们的模型考虑了什么，没有考虑什么。
- en: Graph Generation
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图生成
- en: Graph generation is very important for drug discovery, materials design, and
    other applications. Traditionally, graph generation approaches used hand crafted
    families of random graph models using a simple stochastic generation process.
    These models are well-understood mathematically due to their simple properties.
    However, due to the same reaon, they are limited in their ability to capture real
    world graphs with more complex dependencies, or even the correct statistical properties
    such as heavy tailed distribution for the node degrees that many real world networks
    exhibit. More recent approaches, such as generative graph neural networks, integrate
    graph and node representations with generative models that we went over in the
    previous chapter. These have a greater capacity to learn structural information
    from the data and generate complex graphs such as molecules and compounds.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图生成对于药物发现、材料设计和其他应用非常重要。传统上，图生成方法使用手工制作的随机图模型系列，使用简单的随机生成过程。由于它们的简单属性，这些模型在数学上是很容易理解的。然而，由于同样的原因，它们在捕捉具有更复杂依赖关系的真实世界图，甚至是许多真实世界网络所展示的正确统计属性，如节点度的重尾分布方面存在局限性。更近期的方法，如生成图神经网络，将图和节点表示与我们在前一章中介绍的生成模型相结合。这些方法具有更大的能力从数据中学习结构信息，并生成复杂的图，如分子和化合物。
- en: Influence maximization
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响最大化
- en: Influence maximization is a subfield of network diffusion, where the goal is
    to maximize the diffusion of something, such as information or a vaccine, through
    a network, while only giving the thing to few initial nodes, or seeds. So the
    objective is to locate the few nodes that have an overall maximal influence. Applications
    include information propagation such as job opennings, news, advertisements, and
    vaccinations. Traditional methods for locating the seeds choose the nodes based
    on highest degree, closeness, betweenness, and other graph structure properties.
    Others employ the field of discrete optimization, obtaining good results and proving
    the existence of approximate optimizers. More recent approaches employ graph neural
    networks and adversarial networks, when there are other objectives competing with
    the objective of maximizing a node’s influence, such as reaching a specific portion
    of the population, such as a certain minority group, who is not necessarily strongly
    connected with the natural hubs of the graph.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 影响最大化是网络扩散的一个子领域，其目标是通过网络最大化某种东西（如信息或疫苗）的扩散，同时只将该东西给予少数初始节点或种子。因此，目标是找到具有整体最大影响力的少数节点。应用包括信息传播，如职位空缺、新闻、广告和疫苗接种。传统方法选择种子节点的方式是基于最高度、接近度、介数和其他图结构属性。其他方法采用离散优化领域，获得良好结果并证明近似优化器的存在。最近的方法采用图神经网络和对抗网络，当存在其他与最大化节点影响力目标竞争的目标时，例如达到特定人口部分，如某个少数群体，他们不一定与图的自然中心紧密连接。
- en: Link prediction
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接预测
- en: 'Given two nodes of a graph, what is the probability that there is an edge linking
    them? Note that proximity in the sense of sharing common neighbours is not necessarily
    an indicator for a link (or an interaction). In a social network, people tend
    to run in the same circles so two people sharing many common friends are likely
    to interact so they are likely connected as well, but in some biological systems,
    such as in studying protein-protein interactions, the opposite is true: Proteins
    sharing more common neighbours are less likely to interact. Therefore, computing
    similarity scores based on basic properties such as graph distance, degrees, common
    neighbours, and others does not always produce correct results. We need neural
    networks to learn node and graph embeddings along with classifying whether two
    nodes are linked or not. One example of such networks is in this paper: [Link
    Prediction with Graph Neural Networks and Knowledge Extraction](http://cs230.stanford.edu/projects_spring_2020/reports/38854344.pdf).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 给定图中的两个节点，它们之间存在边的概率是多少？请注意，在共享共同邻居的意义上的接近性并不一定是连接（或互动）的指标。在社交网络中，人们倾向于在同一个圈子里活动，因此共享许多共同朋友的两个人很可能互动，因此它们也很可能相连，但在某些生物系统中，例如研究蛋白质相互作用时，情况恰恰相反：共享更多共同邻居的蛋白质更不可能相互作用。因此，基于基本属性（如图距离、度、共同邻居等）计算相似性分数并不总是产生正确结果。我们需要神经网络来学习节点和图的嵌入，并分类两个节点是否相连。这篇论文中有一个这样的网络示例：[Link
    Prediction with Graph Neural Networks and Knowledge Extraction](http://cs230.stanford.edu/projects_spring_2020/reports/38854344.pdf)。
- en: Dynamic Graph Models
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态图模型
- en: Many of the applications we have discussed in this chapter would benefit from
    including time dependency in our graph models, since they are dynamic in nature.
    Examples include traffic forecasting, load balance for distributed networks, simulating
    all kinds of interacting particle systems, and illegal wildlife trade monitoring.
    In dynamic graph models, node and edge features are allowed to evolve with time,
    and some nodes or edges can be added or removed. This modeling captures information
    such as latest trade trends in a market, fluctuations, new criminal activity in
    certain networks, and new routes or connections in transportation systems.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的许多应用都将受益于在我们的图模型中包含时间依赖性，因为它们具有动态性质。示例包括交通预测、分布式网络的负载平衡、模拟各种相互作用的粒子系统以及非法野生动物贸易监测。在动态图模型中，节点和边的特征允许随时间演变，一些节点或边可以被添加或移除。这种建模捕捉了市场中最新贸易趋势、波动、某些网络中的新犯罪活动以及交通系统中的新路线或连接等信息。
- en: Thinking about how to model dynamic graphs and extract information from them
    is not new, see for example the 1997 article [Dynamic graph models](https://www.sciencedirect.com/science/article/pii/S0895717797000502),
    however, the introduction of deep learning makes knowledge extraction from such
    systems more straightforward. Current approaches for dynamic graphs integrate
    graph convolutions to capture spatial dependencies with recurrent neural networks
    or convolutional neural networks to model temporal dependencies.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 思考如何建模动态图并从中提取信息并不是新鲜事，例如可以参考1997年的文章[Dynamic graph models](https://www.sciencedirect.com/science/article/pii/S0895717797000502)，然而，深度学习的引入使得从这些系统中提取知识更加简单。目前用于动态图的方法将图卷积与循环神经网络或卷积神经网络相结合，以捕获空间依赖性并建模时间依赖性。
- en: The paper [Learning to Simulate Complex Physics with Graph Networks (2020)](https://arxiv.org/pdf/2002.09405.pdf)
    is a great example with wonderful high resolution results that employs a dynamic
    graph neural networks to simulate any system of interacting particles on a much
    larger scale, in terms of both the number of involved particles and the time the
    system is allowed to (numerically) evolve, than what was done before. The particles,
    such as sand or water particles, are the nodes of the graph, with attributes such
    as position, velocity, pressure, external forces, etc., and the edges connect
    the particles that are allowed to interact with each other. The input to the neural
    network is a graph, and the output is a graph with the same nodes and edges but
    with updated attributes of particle positions and properties. The network learns
    the dynamics, or the update rule at each time step, via message passing. The update
    rule depends on the system’s state at the current time step, and on a parametrized
    function whose parameters are optimized for some training objective which depends
    on the specific application, which is the main step in any neural network. The
    prediction targets for supervised learning are the average acceleration for each
    particle.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文[Learning to Simulate Complex Physics with Graph Networks (2020)](https://arxiv.org/pdf/2002.09405.pdf)是一个很好的例子，展示了出色的高分辨率结果，利用动态图神经网络在更大的尺度上模拟任何相互作用粒子系统，无论是涉及的粒子数量还是系统允许（数值上）演化的时间，都比以前做的要多。粒子，如沙子或水粒子，是图的节点，具有属性如位置、速度、压力、外部力等，边连接允许相互作用的粒子。神经网络的输入是一个图，输出是一个具有相同节点和边的图，但具有更新后的粒子位置和属性。网络通过消息传递学习动态，或者说每个时间步的更新规则。更新规则取决于当前时间步系统的状态，以及一个参数化函数，其参数针对某个训练目标进行优化，该目标取决于特定应用，这是任何神经网络的主要步骤。监督学习的预测目标是每个粒子的平均加速度。
- en: Bayesian Networks
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯网络
- en: Bayesian networks are graphs that are perfectly equipped to deal with uncertainty,
    encoding probabilities in a mathematically sound way. [Figure 9-10](#Fig_Bayes_net_1)
    and [Figure 9-11](#Fig_Bayes_net_2) are examples of two Bayesian networks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络是完全适合处理不确定性的图形，以数学上合理的方式编码概率。[图9-10](#Fig_Bayes_net_1)和[图9-11](#Fig_Bayes_net_2)是两个贝叶斯网络的示例。
- en: '![300](assets/emai_0910.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0910.png)'
- en: Figure 9-10\. A Bayesian network
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10. 贝叶斯网络
- en: '![300](assets/emai_0911.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0911.png)'
- en: Figure 9-11\. Another Bayesian network
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11. 另一个贝叶斯网络
- en: 'In a Bayesian network, we have:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯网络中，我们有：
- en: The nodes are variables that we believe our model should include.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点是我们认为模型应该包含的变量。
- en: 'The edges are directed, pointing from the *parent* node to the *child* node,
    or from a *higher neuron* to a *lower neuron* in the sense that: We know the probability
    of the child variable conditional on observing the parent variable.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边是有向的，从*父*节点指向*子*节点，或者从*更高的神经元*指向*更低的神经元*，意思是：我们知道在观察到父变量的情况下子变量的概率。
- en: No cycles are allowed in the graph of the network.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络图中不允许循环。
- en: 'They rely heavily on Bayes’s rule: If there is an arrow from A to B, then P(B|A)
    is the forward probability and P(A|B) is the inverse probability. Think of this
    as P(evidence|hypothesis) or P(symptoms|disease). We can calculate the inverse
    probability from Bayes rule:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们严重依赖于贝叶斯规则：如果从A到B有一个箭头，那么P(B|A)是正向概率，P(A|B)是逆向概率。可以将其视为P(证据|假设)或P(症状|疾病)。我们可以根据贝叶斯规则计算逆向概率：
- en: <math alttext="dollar-sign upper P left-parenthesis upper A vertical-bar upper
    B right-parenthesis equals StartFraction upper P left-parenthesis upper B vertical-bar
    upper A right-parenthesis upper P left-parenthesis upper A right-parenthesis Over
    upper P left-parenthesis upper B right-parenthesis EndFraction period dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>|</mo><mi>A</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac> <mo>.</mo></mrow></math>
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper P left-parenthesis upper A vertical-bar upper
    B right-parenthesis equals StartFraction upper P left-parenthesis upper B vertical-bar
    upper A right-parenthesis upper P left-parenthesis upper A right-parenthesis Over
    upper P left-parenthesis upper B right-parenthesis EndFraction period dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>|</mo><mi>A</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac> <mo>.</mo></mrow></math>
- en: 'If there is no arrow pointing to a variable (if it has no parents), then all
    we need is the *prior* probability of that variable, which we compute from the
    data or from expert knowledge, such as: Thirteen percent of women in the United
    State develop breast cancer.'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有箭头指向一个变量（如果它没有父节点），那么我们只需要该变量的*先验*概率，我们可以从数据或专家知识中计算，比如：美国有百分之十三的女性患乳腺癌。
- en: 'If we happen to obtain more data on one of the variables in the model, or more
    *evidence*, then we update the node corresponding to that variable (the conditional
    probability), then propagate that information following the connections in the
    network, updating the conditional probabilities at each node, in two different
    ways, depending on whether the information is propagating from parent to child,
    or from child to parent. The update in each direction is very simple: Comply with
    Bayes’s rule.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在模型中的某个变量或更多*证据*上获得更多数据，那么我们更新对应于该变量的节点（条件概率），然后沿着网络中的连接传播该信息，更新每个节点的条件概率，有两种不同的方式，取决于信息是从父节点传播到子节点，还是从子节点传播到父节点。每个方向的更新非常简单：遵守贝叶斯规则。
- en: A Bayesian network represents is a compactified conditional probability table
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯网络代表的是一个紧凑的条件概率表。
- en: What a Bayesian network represents is a compactified conditional probability
    table. Usually when we model a real world scenario, each discrete variable can
    assume certain discrete values or categories, and each continuous variable can
    assume any value in a given continuous range. In theory, we can construct a giant
    conditional probability table which gives the probability of each variable assuming
    a certain state given fixed values of the other variables. In reality, even for
    a reasonable small number of variables, this is infeasible and expensive both
    for storage and computations. Moreover, we do not have access to all the information
    required to construct the table. The way Bayesian networks get around this hurdle
    is to allow variables to interact with only few neighbouring variables, so we
    only have to compute the probability of a variable given the states of only those
    directly connected to it in the graph of the network, albeit both forward and
    backward. If new evidence about any variable in the network arrives, then the
    graph structure, together with Bayes’s rule, guides us to update the probabilities
    of all the variables in the network in a systematic, explainable, and transparent
    way. *Sparsifying* the network this way is a feature of Bayesian networks that
    has allows for their success.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络代表的是一个紧凑的条件概率表。通常，当我们建模一个真实世界的场景时，每个离散变量可以假定某些离散值或类别，每个连续变量可以假定在给定连续范围内的任何值。理论上，我们可以构建一个巨大的条件概率表，该表给出了每个变量在固定其他变量值的情况下假定某个状态的概率。实际上，即使对于一个相对较小的变量数量，这也是不可行且昂贵的，无论是对于存储还是计算。此外，我们没有访问构建表所需的所有信息。贝叶斯网络克服这一障碍的方法是允许变量只与少数相邻变量交互，因此我们只需计算网络图中直接连接到它的变量的概率，无论是向前还是向后。如果网络中任何变量的新证据到达，那么网络的图结构，连同贝叶斯规则，将引导我们以系统化、可解释和透明的方式更新网络中所有变量的概率。以这种方式*稀疏化*网络是贝叶斯网络成功的一个特征。
- en: 'In summary, a Bayesian network’s graph specifies the joint probability distribution
    of the model’s variables (or the data’s features) as a product of local conditional
    probability distributions, one for each node:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，贝叶斯网络的图表明了模型变量（或数据特征）的联合概率分布，作为局部条件概率分布的乘积，每个节点一个：
- en: <math alttext="dollar-sign upper P left-parenthesis x 1 comma x 2 comma ellipsis
    comma x Subscript n Baseline right-parenthesis equals product Underscript i equals
    1 Overscript n Endscripts upper P left-parenthesis x Subscript i Baseline vertical-bar
    parents of x Subscript i Baseline right-parenthesis dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>|</mo> <mtext>parents</mtext> <mtext>of</mtext> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper P left-parenthesis x 1 comma x 2 comma ellipsis
    comma x Subscript n Baseline right-parenthesis equals product Underscript i equals
    1 Overscript n Endscripts upper P left-parenthesis x Subscript i Baseline vertical-bar
    parents of x Subscript i Baseline right-parenthesis dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>|</mo> <mtext>parents</mtext> <mtext>of</mtext> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: Making predictions using a Bayesian network
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用贝叶斯网络进行预测
- en: 'Once a Bayesian network is set and the conditional probabilities initiated
    (and continuously updated using more data), we can get quick results simply by
    searching these conditional probability distribution tables along Bayes’s rule
    or the product rule, depending on the query such as: What is the probability that
    this email is spam given the words it contains, the sender location, the time
    of the day, the links it includes, the history of interaction between the sender
    and the recepient, and other values of the spam detection variables? What is the
    probability that a patient has breast cancer given her mammogram test result,
    her family history, her symptoms, her blood tests, *etc.*? The best part here
    is that we do not need to consume energy executing large programs or employ large
    clusters of computers to get our results. That is, our phones’ and tablets’ batteries
    will last longer because they don’t have to spend much computational power coding
    and decoding messages, instead, they apply Bayesian networks for using [(*turbo
    code*)](http://authors.library.caltech.edu/6938/1/MCEieeejstc98.pdf) forward error
    correction algorithm.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦贝叶斯网络设置并初始化条件概率（并持续使用更多数据进行更新），我们可以通过沿着贝叶斯规则或乘积规则搜索这些条件概率分布表来快速获得结果，具体取决于查询，比如：给定邮件包含的单词、发件人位置、一天中的时间、包含的链接、发件人和收件人之间的互动历史以及垃圾邮件检测变量的其他值，这封电子邮件是垃圾邮件的概率是多少？给定患者的乳腺癌概率是多少，考虑到她的乳腺X光检查结果、家族史、症状、血液检测等？这里最好的部分是我们不需要执行大型程序或使用大型计算机集群来获得结果。也就是说，我们的手机和平板电脑的电池将持续更长时间，因为它们不需要花费大量计算能力来编码和解码消息，而是应用贝叶斯网络来使用[（*turbo
    code*）](http://authors.library.caltech.edu/6938/1/MCEieeejstc98.pdf)前向纠错算法。
- en: Bayesian networks are belief networks, not causal networks
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯网络是信念网络，而不是因果网络
- en: 'In Bayesian networks, though a pointed arrow from parent variable to child
    variable is preferably causal, in general it *is not* causal. All it means is
    that we we can model the probability distribution of a child variable given the
    states of its parent(s), and we can use Bayes rule to find the inverse probability:
    The probability distribution of a parent given the child. This is usually the
    more difficult direction because it is less intuitive and harder to observe. One
    way to think about this is that it is easier to calculate the probability distribution
    of a child’s traits given that we know his parents’ traits, *P(child|mother,father)*,
    even before having the child, than infering the parents’ traits given that we
    know the child’s traits *P(father|child)* and *P(mother|child)*. The Bayesian
    network in this example [Figure 9-12](#Fig_mother_father) has three nodes, mother,
    father, and child, with an edge pointing from the mother to the child, and another
    edge pointing from the father to the child.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯网络中，尽管从父变量指向子变量的箭头最好是因果关系，但一般来说*并不是*因果关系。这只是意味着我们可以模拟子变量的概率分布，考虑到其父变量的状态，我们可以使用贝叶斯规则找到逆概率：父变量给定子变量的概率分布。这通常是更困难的方向，因为它不太直观，更难观察。一种思考这个问题的方法是，在知道孩子的父母特征之前，计算孩子特征的概率分布更容易，*P(孩子|母亲，父亲)*，甚至在有孩子之前，而推断出父母特征，*P(父亲|孩子)*和*P(母亲|孩子)*。这个例子中的贝叶斯网络[图9-12](#Fig_mother_father)有三个节点，母亲、父亲和孩子，从母亲指向孩子的边，从父亲指向孩子的另一条边。
- en: '![300](assets/emai_0912.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0912.png)'
- en: Figure 9-12\. The child variable is a collider in this Bayesian network.
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12。在这个贝叶斯网络中，孩子变量是一个碰撞器。
- en: 'There is no edge between the mother and the father because there is no reason
    for their traits to be related. Knowing the mother’s traits gives us no information
    about the father’s traits, however, knowing the mother’s traits and the child’s
    traits allows us to know slightly more about the father’s traits, or the distribution
    P(father|mother,child). This means that the mother’s and father’s traits, which
    were originally independent, are conditionally dependent given knowing the child’s
    traits. Thus, a Bayesian network models the dependencies between variables in
    a graph structure, providing a map for how the variables *are believed* to relate
    to each other: Their conditional dependencies and independencies. This is why
    Bayesian networks are also called belief networks.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 母亲和父亲之间没有边，因为他们的特征之间没有关联。知道母亲的特征不会给我们关于父亲的特征提供任何信息，然而，知道母亲的特征和孩子的特征可以让我们稍微了解更多关于父亲的特征，或者分布
    P(父亲|母亲,孩子)。这意味着母亲和父亲的特征，最初是独立的，但在知道孩子的特征的情况下是有条件相关的。因此，贝叶斯网络模拟了图结构中变量之间的依赖关系，提供了一个关于这些变量之间关系的地图：它们的条件依赖和独立性。这就是为什么贝叶斯网络也被称为信念网络。
- en: Keep this in mind about Bayesian networks
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于贝叶斯网络，请记住以下内容
- en: 'Let’s keep the following in mind about Bayesian networks:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住关于贝叶斯网络的以下内容：
- en: 'Bayesian networks have no causal direction and are limited in answering causal
    questions, or why questions, such as: What caused the onset of a certain disease?
    That said, we will soon learn that we can use a Bayesian network for causal reasoning,
    and to predict the consequences of intervention. Whether used for causal reasoning
    or not, the way we update a Bayesian network, or the way we propagate the *belief*,
    always works in the same way.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯网络没有因果方向，并且在回答因果问题或为什么问题方面受到限制，比如：是什么导致了某种疾病的发作？也就是说，我们很快将会了解到，我们可以使用贝叶斯网络进行因果推理，并预测干预的后果。无论是用于因果推理还是其他用途，我们更新贝叶斯网络的方式，或者传播*信念*的方式，总是一样的。
- en: If some variables have missing data, Bayesian networks can handle it because
    they are designed in a way that propagates information efficiently from variables
    with abundant information about them to variables with less information.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一些变量有缺失数据，贝叶斯网络可以处理，因为它们被设计成有效地从具有关于它们的丰富信息的变量向具有较少信息的变量传播信息。
- en: Chains, Forks, and Colliders
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 链、分叉和碰撞器
- en: 'The building blocks of Bayesian networks (with three or more nodes) are three
    types of *junctions*: Chain, fork, and collider, illustrated in [Figure 9-13](#Fig_chain_fork_collider).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络的构建模块（具有三个或更多节点）是三种类型的*连接*：链、分叉和碰撞器，如[图 9-13](#Fig_chain_fork_collider)所示。
- en: '![300](assets/emai_0913.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0913.png)'
- en: Figure 9-13\. The three types of junctions in a Bayesian network.
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-13。贝叶斯网络中的三种连接类型。
- en: '*Chain*: A <math alttext="right-arrow"><mo>→</mo></math> B <math alttext="right-arrow"><mo>→</mo></math>
    C. In this chain, B is a mediator. If we know the value of B, then learning about
    A does not increase or decrease our belief in C. Thus, A and C are conditionally
    independent, given that we know the value of the mediator B. Conditional independence
    allows us, and a machine using a Bayesian network, to focus only on the relevant
    information.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*链*：A <math alttext="right-arrow"><mo>→</mo></math> B <math alttext="right-arrow"><mo>→</mo></math>
    C。在这个链中，B 是一个中介者。如果我们知道 B 的值，那么了解 A 不会增加或减少我们对 C 的信念。因此，在我们知道中介者 B 的值的情况下，A 和
    C 是有条件独立的。条件独立性使我们和使用贝叶斯网络的机器只需关注相关信息。'
- en: '*Fork*: B <math alttext="right-arrow"><mo>→</mo></math> A and B <math alttext="right-arrow"><mo>→</mo></math>
    C, that is, B is a common parent or a confounder of A and C. The data will show
    that A and C statistically correlated even though there is no causal relationship
    between them. We can expose this fake correlation by conditioning on the confounder
    B.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分叉*：B <math alttext="right-arrow"><mo>→</mo></math> A 和 B <math alttext="right-arrow"><mo>→</mo></math>
    C，也就是说，B 是 A 和 C 的共同父节点或混杂因素。数据将显示，即使它们之间没有因果关系，A 和 C 在统计上是相关的。我们可以通过对混杂因素 B 进行条件处理来暴露这种虚假相关性。'
- en: '*Collider*: A <math alttext="right-arrow"><mo>→</mo></math> B and C <math alttext="right-arrow"><mo>→</mo></math>
    B. Colliders are different than chains or forks when we condition on the variable
    in the middle. We saw this in the parents pointing to a child example above. If
    A and C are independent are originally independent, conditioning on B makes them
    dependent! This unexpected and noncausal transfer of information is one characteristic
    of Bayesian networks conditioning: Conditioning on a collider happens to open
    a dependence path between its parents.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*碰撞器*：A <math alttext="right-arrow"><mo>→</mo></math> B 和 C <math alttext="right-arrow"><mo>→</mo></math>
    B。当我们对中间变量进行条件处理时，碰撞器与链或分叉是不同的。我们在上面父母指向孩子的例子中看到了这一点。如果 A 和 C 最初是独立的，那么对 B 进行条件处理会使它们变得相关！这种意外的、非因果关系的信息传递是贝叶斯网络条件处理的一个特征：对碰撞器进行条件处理会打开其父节点之间的依赖路径。'
- en: 'Another thing we need to be careful about: When the outcome and mediator are
    confounded. Conditioning on the mediator in this case is different than holding
    it constant.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件我们需要小心的事情是：当结果和中介者混淆时。在这种情况下，对中介者进行条件处理与将其保持恒定是不同的。
- en: Given a data set, how do we set up a Bayesian network for the involved variables?
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 给定一个数据集，我们如何为涉及的变量建立一个贝叶斯网络？
- en: The graph structure of a Bayesian network can be decided on manually by us,
    or learned from the data by algorithms. Algorithms for Bayesian networks are very
    mature and there are commercial . Once the network’s structure is set in place,
    if a new piece of information about a certain variable in the network arrives,
    it is easy to update the conditional probabilities at each node, by following
    the diagram and propagating the information through the network, updating the
    belief about each variable in the network. The inventor of Bayesian networks,
    [Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl) likens this updating
    process to living organic tissue, and to a biological network of neurons, where
    if you excite one neuron, the whole network reacts, propagating the information
    from one neuron to its neighbours.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络的图结构可以由我们手动决定，也可以通过算法从数据中学习。贝叶斯网络的算法非常成熟，而且有商业化。一旦网络的结构确定，如果有关网络中某个变量的新信息到达，只需按照图表更新每个节点的条件概率，通过网络传播信息，更新对网络中每个变量的信念。贝叶斯网络的发明者朱迪亚·珀尔将这个更新过程比作生物有机组织，比作神经元的生物网络，如果激发一个神经元，整个网络会做出反应，将信息从一个神经元传播到其邻居。
- en: Finally, we can think of neural networks as Bayesian networks.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将神经网络视为贝叶斯网络。
- en: Models learning patterns from the data
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中学习模式
- en: 'It is important to note that Bayesian networks, and any other models learning
    the joint probability distributions of the features of the given data, such as
    the models we encountered in the previous chapter on generative models, and the
    deterministic models in earlier chapters, only detect patterns from the data and
    learn associations, as opposed to learning *what caused* these patterns to begin
    with. For an AI agent to be truly intelligent and reason like humans, they must
    ask the questions *how*, *why*, and *what if* about what they see and what they
    *do*, and they must seek answers, like humans do at a very early age. In fact,
    for humans, there is the age of *why* early in their development: It is the age
    when children drive their parents crazy asking *why* about everything and anything.
    An AI agent should have a *causal model*. This concept is so important for attaining
    general AI. We will visit it in the next section and one more time in [Chapter 11](ch11.xhtml#ch11)
    on Probability.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，贝叶斯网络以及学习给定数据特征的联合概率分布的任何其他模型，比如我们在前一章中遇到的生成模型以及早期章节中的确定性模型，只是从数据中检测模式并学习关联，而不是学习这些模式最初是如何产生的。要使AI代理真正智能并像人类一样推理，他们必须询问关于他们所看到的和他们所做的事情的“如何”、“为什么”和“如果”等问题，并且必须寻找答案，就像人类在很小的时候那样。事实上，对于人类来说，在他们的发展早期有一个“为什么”的年龄：那就是孩子们让他们的父母因为询问一切事情的“为什么”而发疯的年龄。AI代理应该有一个“因果模型”。这个概念对于获得通用人工智能非常重要。我们将在下一节和[第11章](ch11.xhtml#ch11)中再次讨论这个问题。
- en: Graph Diagrams For Probabilistic Causal Modeling
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率因果建模的图表
- en: 'Since taking our first steps in statistics, all we have heard was: *Correlation
    is not causation*. Then we go on and on about data, more data, and correlations
    in data. Alright then, we get the message, but what about causation? What is it?
    How do we quantify it? As humans we know exactly what *why* means? We conceptualize
    cause and effect intuitively, even as little as eight months olds. I actually
    think we function more on a natural and intuitive level in the world of *why*
    than in the world of association. *Why* then (see?), do our machines, who we expect
    that at some point will be able to reason like us, only function at an association
    and regression level? This is the point that mathematician and philosopher Judea
    Pearl argues for in the field of AI, and in his wonderful book *The Book of Why
    (2020)*. My favorite quote from this book: *Noncausal correlation violates our
    common sense*. The idea is that we need to both articulate and quantify which
    correlations are due to causation and which are due to some other factors.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们在统计学中迈出第一步以来，我们听到的一直是：“相关不意味着因果关系”。然后我们继续谈论数据、更多数据和数据中的相关性。好吧，我们明白了，但是因果关系呢？它是什么？我们如何量化它？作为人类，我们清楚知道“为什么”是什么意思？我们直观地概念化因果关系，即使是八个月大的婴儿也是如此。我实际上认为我们在“为什么”的世界中更多地依靠自然和直觉水平运作，而不是在关联的世界中。那么，我们期望有一天能像我们一样推理的机器，为什么只在关联和回归水平上运作？这是数学家和哲学家朱迪亚·珀尔在人工智能领域提出的观点，也是他在他出色的著作《为什么之书（2020）》中提出的观点。我从这本书中最喜欢的一句话是：“非因果关系违反了我们的常识”。这个想法是我们需要明确并量化哪些相关性是由因果关系引起的，哪些是由其他因素引起的。
- en: 'Pearl builds his mathematical causality models using diagrams (graphs), that
    are similar to Bayesian network graphs, but endowed with a probabilistic reasoning
    scheme based on *the do calculus*, or computing probabilities *given the do* operator
    as opposed to computing probabilities *given the observe* operator, which is very
    familiar from noncausal statistical models. The main point is this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 珀尔使用图表（图）构建他的数学因果模型，这些图表类似于贝叶斯网络图，但具有基于“do演算”的概率推理方案，或者计算概率“给定do”运算符，而不是计算概率“给定观察”运算符，这在非因果统计模型中非常熟悉。主要观点是：
- en: '*Observing is not the same as doing*. In math notation, <math alttext="upper
    P r o b left-parenthesis number of bus riders vertical-bar color coded routes
    right-parenthesis"><mrow><mi>P</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi> <mo>(</mo>
    <mtext>number</mtext> <mtext>of</mtext> <mtext>bus</mtext> <mtext>riders</mtext>
    <mo>|</mo> <mtext>color</mtext> <mtext>coded</mtext> <mtext>routes</mtext> <mo>)</mo></mrow></math>
    is not the same as <math alttext="upper P r o b left-parenthesis number of bus
    riders vertical-bar do color coded routes right-parenthesis"><mrow><mi>P</mi>
    <mi>r</mi> <mi>o</mi> <mi>b</mi> <mo>(</mo> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>bus</mtext> <mtext>riders</mtext> <mo>|</mo> <mtext>do</mtext> <mtext>color</mtext>
    <mtext>coded</mtext> <mtext>routes</mtext> <mo>)</mo></mrow></math> .'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察并不等同于行动。在数学符号中，<math alttext="上 P r o b left-parenthesis number of bus riders
    vertical-bar color coded routes right-parenthesis"><mrow><mi>P</mi> <mi>r</mi>
    <mi>o</mi> <mi>b</mi> <mo>(</mo> <mtext>number</mtext> <mtext>of</mtext> <mtext>bus</mtext>
    <mtext>riders</mtext> <mo>|</mo> <mtext>color</mtext> <mtext>coded</mtext> <mtext>routes</mtext>
    <mo>)</mo></mrow></math> 不同于 <math alttext="上 P r o b left-parenthesis number
    of bus riders vertical-bar do color coded routes right-parenthesis"><mrow><mi>P</mi>
    <mi>r</mi> <mi>o</mi> <mi>b</mi> <mo>(</mo> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>bus</mtext> <mtext>riders</mtext> <mo>|</mo> <mtext>do</mtext> <mtext>color</mtext>
    <mtext>coded</mtext> <mtext>routes</mtext> <mo>)</mo></mrow></math>。
- en: 'We can infer the first one from data: Look for the ridership numbers given
    that the bus routes in a certain city are color coded. This probability does not
    tell us the *effect* color coded routes has on the number of riders. The second
    probability, *with the do operator*, is different and the data alone, without
    a causal diagram, cannot tell us the answer. The difference is when we invoke
    the do operator, then we are deliberately changing the bus routes to color coded,
    and we want to assess the effect of that change on bus ridership. If the ridership
    increases after this deliberate *doing*, and given that we drew the correct graph
    including the variables and how they talk to each other, then we can assert: Using
    color coded bus routes *caused* the increase in ridership. When we *do* instead
    of *observe*, we manually block all the roads that could naturally lead to colored
    bus routes, such as a change in leadership, or a time of the year, that at the
    same time might affect ridership. We cannot block these roads if we were simply
    observing the data. Moreover, when we use the do operator, we intentionally and
    manually *set the value* of bus routes to color coded (as opposed to numbered,
    *etc.*)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从数据中推断第一个问题：查看某个城市的公交路线采用彩色编码时的乘客量。这种概率并不能告诉我们彩色编码路线对乘客数量的*影响*。第二种概率，*带有
    do 运算符*，是不同的，仅凭数据，没有因果图，我们无法得出答案。区别在于当我们调用 do 运算符时，我们故意改变了公交路线为彩色编码，并且我们想评估这种改变对乘客量的影响。如果在这种故意的*行动*之后乘客量增加了，并且考虑到我们绘制了正确的图表包括变量及它们之间的关系，那么我们可以断言：使用彩色编码的公交路线*导致*了乘客量的增加。当我们*行动*而不是*观察*时，我们手动阻止了所有可能导致公交路线变为彩色的道路，比如领导层的变化，或者一年中的某个时间，同时可能影响乘客量。如果我们仅仅观察数据，我们无法阻止这些道路。此外，当我们使用
    do 运算符时，我们故意和手动地*设置了*公交路线为彩色编码（而不是编号、*等等*）。
- en: 'Actually, the bus routes example is not made up. I am currently collaborating
    with the department of public transportation in Harrisonburg, Virginia, with the
    goals of increasing their ridership, improving their efficiency, and optimizing
    their operations given both limited resources and a drastic drop in the city’s
    population when the university is not in session. In 2019, the transportation
    department *deliberately* changed their routes from a number system to a color
    coded system, and at the same time *deliberately* changed their schedules from
    adaptive to the university’s class schedule to fixed schedules. Here is what happened:
    Their ridership increased a whopping 18%. You bet my students who are working
    on the project this summer (2022) will soon be drawing causal diagrams and writing
    probabilities that look like:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，公交路线的例子并非虚构。我目前正在与弗吉尼亚州哈里森堡市的公共交通部门合作，旨在增加他们的乘客量，提高他们的效率，并在资源有限且大学不在开学期间市内人口急剧减少的情况下优化他们的运营。2019年，交通部门*故意*将他们的路线从编号系统改为彩色编码系统，同时*故意*将他们的时间表从适应大学课程时间表改为固定时间表。结果如下：他们的乘客量增加了惊人的18%。可以肯定，我今年夏天正在参与该项目的学生们很快将绘制因果图并编写概率，看起来像：
- en: <math alttext="dollar-sign upper P left-parenthesis r i d e r s h i p vertical-bar
    do color coded routes comma do fixed schedules right-parenthesis period dollar-sign"><mrow><mi>P</mi>
    <mo>(</mo> <mi>r</mi> <mi>i</mi> <mi>d</mi> <mi>e</mi> <mi>r</mi> <mi>s</mi> <mi>h</mi>
    <mi>i</mi> <mi>p</mi> <mo>|</mo> <mtext>do</mtext> <mtext>color</mtext> <mtext>coded</mtext>
    <mtext>routes</mtext> <mo>,</mo> <mtext>do</mtext> <mtext>fixed</mtext> <mtext>schedules</mtext>
    <mo>)</mo> <mo>.</mo></mrow></math>
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper P left-parenthesis r i d e r s h i p vertical-bar
    do color coded routes comma do fixed schedules right-parenthesis period dollar-sign"><mrow><mi>P</mi>
    <mo>(</mo> <mi>r</mi> <mi>i</mi> <mi>d</mi> <mi>e</mi> <mi>r</mi> <mi>s</mi> <mi>h</mi>
    <mi>i</mi> <mi>p</mi> <mo>|</mo> <mtext>do</mtext> <mtext>color</mtext> <mtext>coded</mtext>
    <mtext>routes</mtext> <mo>,</mo> <mtext>do</mtext> <mtext>fixed</mtext> <mtext>schedules</mtext>
    <mo>)</mo> <mo>.</mo></mrow></math>
- en: 'A machine that is so good at detecting a pattern and acting on it, like a lizard
    observing a bug fly around, learning its pattern, then catching it and eating
    it, has a very different level of *intellect* than a machine that is able to reason
    on two higher levels than mere detection of patterns:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一台机器如果擅长发现模式并采取行动，就像蜥蜴观察一只昆虫飞来飞去，学习它的模式，然后捕捉并吃掉它，那么它的*智力*水平就与一台能够在两个比简单模式检测更高层次上推理的机器有很大不同：
- en: '*If I deliberately take this action, what will happen to [insert variable here]?*'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*如果我故意采取这个行动，[在此插入变量]会发生什么？*'
- en: '*What if I didn’t take this action, would [the variable taking a certain value]
    still have happened?* What if Harrisonburg City did not move to color coded routes
    and to fixed schedules, would the ridership still have increased? What if only
    one of these variables changed instead of both?'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*如果我不采取这个行动，[变量取某个值]是否仍然会发生？* 如果哈里森堡市没有转向彩色编码路线和固定时间表，乘客量是否仍然会增加？如果只有这两个变量中的一个改变而不是两者同时改变呢？'
- en: The data alone cannot answer these questions. In fact, carefully constructed
    causal diagrams help us tell apart the times when we can use the data alone to
    answer the above questions, and when we cannot answer them *irrespective of how
    much more data we collect*. Until our machines are endowed with graphs representing
    causal reasoning, our mahines have the same level of intellect as lizards. Amazingly,
    humans do all these computations instantaneously, albeit with arriving at wrong
    conclusions many times and arguing with each other about causes and effects for
    decades. We still need math and graphs to settle matters. In fact, the graph guides
    us and tells us which data we must look for and collect, which variables to condition
    on, and which variables to apply the do operator on. This intentional design and
    reasoning is very different than the culture of amassing big volumes of data,
    or aimlessly conditioning on all kinds of variables.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 单凭数据无法回答这些问题。事实上，精心构建的因果图帮助我们区分何时可以仅凭数据回答上述问题，以及何时无论我们收集多少数据都无法回答这些问题。直到我们的机器被赋予代表因果推理的图表之前，我们的机器的智力水平与蜥蜴相同。令人惊讶的是，人类可以即时进行所有这些计算，尽管很多时候会得出错误的结论，并且几十年来争论起因和结果。我们仍然需要数学和图表来解决问题。事实上，图表指导我们，告诉我们必须寻找和收集哪些数据，对哪些变量进行条件设置，以及对哪些变量应用do运算符。这种有意识的设计和推理与积累大量数据或毫无目的地对各种变量进行条件设置的文化非常不同。
- en: 'Now that we know this we can draw diagrams and design models that can help
    us settle all kinds of causal questions: Did I heal because of the doctor’s treatment
    or did I heal because time has passed and life has calmed down? We would still
    need to collect and organize data, but this process will now be intentional and
    guided. A machine endowed with these ways of reasoning: A causal diagram model,
    a long with the (very short) list of valid operations that go along with the causal
    diagram model, will be able to answer queries on all three levels of causation:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道这一点，我们可以绘制图表和设计模型，帮助我们解决各种因果问题：我是因为医生的治疗而康复，还是因为时间过去了，生活平静下来了？我们仍然需要收集和整理数据，但这个过程现在将是有意识和有指导性的。一个具有这些推理方式的机器：一个因果图模型，以及与因果图模型配套的（非常简短的）有效操作列表，将能够回答所有三个层面的因果问题：
- en: Are variables A and B correlated? Are ridership and labeling of bus routes correlated?
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量A和B是否相关？乘客量和公交路线的标记是否相关？
- en: If I set variable A to a specific value, how would variable B change? If I deliberately
    set color coded routes, would ridership increase?
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我将变量A设置为特定值，变量B会如何变化？如果我故意设置颜色编码的路线，乘客量会增加吗？
- en: If variable A did not take a certain value, would variable B have changed? If
    I did not change to color coded bus routes, would ridership still have increased?
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果变量A没有取某个值，变量B会发生变化吗？如果我没有改变颜色编码的公交路线，乘客量是否仍然会增加？
- en: 'We still need to learn how to deal with probability expressions that involve
    the *do* operator. We have established that seeing is not the same as doing: *Seeing*
    is in the data, while *doing* is deliberately running an experiment to assess
    the causal effect of a certain variable on another. It more costly than just counting
    proportions seen in the data. Pearl establishes three rules for manipulating probability
    expressions that involve the *do* operator. These help us move from expressions
    with *doing* to others with only *seeing*, where we can get the answers from the
    data. These rules are valuable because they enable us to quantify causal effects
    by *seeing*, by-passing *doing*. We go over these in [Chapter 11](ch11.xhtml#ch11)
    on Probability.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要学习如何处理涉及*do*运算符的概率表达式。我们已经确定看到和做不是一回事：*看到*是在数据中，而*做*是有意地进行实验来评估某个变量对另一个变量的因果效应。这比仅仅计算数据中看到的比例更昂贵。Pearl建立了三条关于操纵涉及*do*运算符的概率表达式的规则。这些规则帮助我们从涉及*做*的表达式转移到仅涉及*看到*的表达式，从中我们可以从数据中得到答案。这些规则很有价值，因为它们使我们能够通过*看到*来量化因果效应，绕过*做*。我们将在[第11章](ch11.xhtml#ch11)中详细介绍这些内容。
- en: A Brief History Of Graph Theory
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图论简史
- en: We cannot leave this chapter without a good overview of graph theory and the
    current state of the field. This area is built on such simple foundations, yet
    it is beautiful, stimulating, and with far reaching applications that it made
    me reassess my whole mathematical career path and try to convert urgently.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能在没有对图论和该领域当前状态进行良好概述的情况下结束本章。这个领域建立在如此简单的基础上，但它是美丽的、刺激的，并且具有广泛的应用，这使我重新评估了我的整个数学职业道路并试图迅速转变。
- en: 'The vocabulary of graph theory includes: Graphs, nodes, edges, degrees, connectivity,
    trees, spanning trees, circuits, fundamental circuits, vector space of a graph,
    rank and nullity (like in linear algebra), duality, path, walk, Euler line, Hamiltonian
    circuit, cut, network flow, traversing, coloring, enumerating, links, and vulnerability.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图论的词汇包括：图、节点、边、度、连通性、树、生成树、回路、基本回路、图的向量空间、秩和零度（类似于线性代数）、对偶、路径、行走、欧拉线、哈密顿回路、切割、网络流、遍历、着色、枚举、链接和脆弱性。
- en: 'The timeline of the development of graph theory is enlightening, with its roots
    in transportation systems, maps and geography, electric circuits, and molecular
    structures in chemistry:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图论发展的时间线是启发性的，它的根源在于交通系统、地图和地理、电路和化学中的分子结构：
- en: In 1736, Euler published the first paper in graph theory, solving the [Königsberg
    bridge problem](https://mathworld.wolfram.com/KoenigsbergBridgeProblem.xhtml).
    Then nothing happened in the field for more than a hundred years.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1736年，欧拉发表了图论的第一篇论文，解决了[Königsberg桥问题](https://mathworld.wolfram.com/KoenigsbergBridgeProblem.xhtml)。然后在这个领域里一百多年没有发生任何事情。
- en: In 1847, Kirchhoff developed the theory of trees while working on electrical
    networks.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1847年，基希霍夫在研究电网络时发展了树的理论。
- en: Shortly after, in the 1850’s, Cayley discovered trees as he was trying to enumerate
    the isomers of saturated hydrocarbons <math alttext="upper C Subscript n Baseline
    upper H Subscript 2 n plus 2"><mrow><msub><mi>C</mi> <mi>n</mi></msub> <msub><mi>H</mi>
    <mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow></math> . Arthur
    Cayley (1821-1895) is one of the founding fathers of graph theory. We find his
    name anywhere where there is graph data. More recently, [CayleyNet](https://arxiv.org/abs/1705.07664)
    uses complex rational functions called Cayley polynomials for a spectral domain
    approach for deep learning on graph data.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不久之后，在1850年代，凯利在尝试列举饱和烃异构体时发现了树。亚瑟·凯利（1821-1895）是图论的奠基人之一。无论何时涉及图数据，我们都会看到他的名字。最近，CayleyNet使用称为凯利多项式的复有理函数，用于图数据的深度学习的谱域方法。
- en: 'During the same time period, in 1850, Sir William Hamilton invented the game
    that became the basis of Hamiltonian circuits, and sold it in Dublin: We have
    a wooden, regular polyhedron with 12 faces and 20 corners, each face is a regular
    pentagon and three edges meeting at each corner. The 20 corners had the names
    of 20 cities, such as London, Rome, New York, Mumbai, Delhi, Paris, and others.
    We have to find a route along the edges of the polyhedron, passing through each
    of the 20 cities exactly once (a Hamiltonian circuit). The solution of this specific
    problem is easy, but until now we have no necessary and sufficient condition for
    the existence of such a route in an arbitrary graph.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一时间段内，1850年，威廉·汉密尔顿爵士发明了成为汉密尔顿回路基础的游戏，并在都柏林出售：我们有一个木制的正多面体，有12个面和20个角，每个面都是一个正五边形，每个角有三条边相交。20个角上有20个城市的名字，如伦敦、罗马、纽约、孟买、德里、巴黎等。我们必须找到一条路径沿着多面体的边，确保每个城市只经过一次（汉密尔顿回路）。这个特定问题的解决方案很容易，但到目前为止，我们还没有任意图中存在这样一条路径的必要和充分条件。
- en: Also during the same time period, at a lecture by Möbius (1840’s), a letter
    by De Morgan (1850’s), and in a publication by Cayley in the first volume of the
    Proceedings of the Royal Geographic Society (1879), the most famous problem in
    graph theory (solved in 1970), the four color theorem, came to life. This has
    occupied many mathematicians since then, leading to many interesting discoveries.
    It states that four colors are sufficient for coloring any map on a plane such
    that the countries with common boundaries have different colors. The interesting
    thing is that if we give ourselves more space by moving out of the flat plane,
    for example to the surface of a sphere, then we do have solutions for this conjecture.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一时间段内，莫比乌斯（1840年代）在一次讲座中，德摩根（1850年代）的一封信，以及凯利在皇家地理学会第一卷会议记录（1879年）中的一篇文章中，图论中最著名的问题（在1970年解决），即四色定理，开始引起人们的关注。自那时以来，这一问题一直困扰着许多数学家，导致了许多有趣的发现。四色定理指出，四种颜色足以为平面上的任何地图着色，使具有共同边界的国家使用不同的颜色。有趣的是，如果我们通过离开平面，例如到球面的表面，为自己提供更多的空间，那么我们确实可以找到这个猜想的解决方案。
- en: Unfortunately, nothing happened for another 70 years or so, until the 1920s
    when König wrote the first book on the subject and published it in 1936.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不幸的是，接下来的70年左右没有发生任何事情，直到20世纪20年代，König写了第一本关于这个主题的书，并于1936年出版。
- en: Things changed with the arrival of computers and their increasing ability to
    explore large problems of combinatorial nature. This spurred intense activity
    in both pure and applied graph theory. There are now thousands of papers and dozens
    of books on the subject, with significant contributers such as Claude Berge, Oystein
    Ore, Paul Erdös, William Tutte, and Frank Harary.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着计算机的出现和它们越来越能够探索组合性质的大问题，情况发生了变化。这激发了纯粹和应用图论的激烈活动。现在有成千上万篇论文和数十本书籍涉及这一主题，重要的贡献者包括克劳德·贝尔热、奥斯坦·奥尔、保罗·埃尔德什、威廉·图特和弗兰克·哈拉里。
- en: Main Considerations In Graph Theory
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图论的主要考虑因素
- en: 'Let’s organize the main topics in graph theory and aim for a bird’s eye view
    without diving into details:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们组织图论的主要主题，力求以鸟瞰的方式进行总览，而不深入细节：
- en: Spanning Trees And Shortest Spanning Trees
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成树和最短生成树
- en: These are of great importance and are used in network routing protocols, shortest
    path algorithms, and search algorithms. A spanning tree of a graph is a subgraph
    that is a tree (any two vertices can be connected using one unique path) including
    all of the vertices of the graph. That is, spanning trees keep the vertices of
    a graph together. The same graph can have many spanning trees.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都非常重要，并且在网络路由协议、最短路径算法和搜索算法中使用。图的生成树是一个子图，是一棵树（任意两个顶点可以使用唯一路径连接），包括图的所有顶点。也就是说，生成树将图的顶点保持在一起。同一个图可以有许多生成树。
- en: Cut Sets And Cut Vertices
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 割集和割点
- en: We can break any connected graph apart, disconnecting it, by cutting through
    enough edges, or sometimes by removing enough vertices. If we are able to find
    these cut sets in a given graph, such as in a communication network, an electrical
    grid, a transportation network, or others, we can cut all communication means
    between its disconnected parts. Usually we are interested in the smallest or minimal
    cut sets, which will accomplish the task of disconnecting the graph by removing
    the least amount of its edges or vertices. This helps us identify the weakest
    links in a network. In contrast to spanning trees, cut sets separate the vertices,
    as opposed to keeping all of them together. Thus, we would rightly expect a close
    relationship between spanning trees and cut sets. Moreover, if the graph represents
    a network with a source of some sort, such as fluid, traffic, electricity, or
    information, and a sink, where each edge allows only a certain amount to flow
    through it, then there is a close relationship between the maximum flow that can
    move from the source to the sink and the cut through the edges of the graph that
    disconnects the source from the sink, with minimal total capacity of the cut edges.
    This is the *max flow min cut theorem*, which states that in a flow network, the
    maximum amount of flow passing from the source to the sink is equal to the total
    weight of the edges in a minimal cut. In mathematics, when a maximization problem
    (max flow) becomes equivalent to a minimization problem (min cut) in a nontrivial
    way (for example by not just flipping the sign of the objective function), it
    signals duality. Indeed, the max flow min cut theorem for graphs is a special
    case of the duality theorem from linear optimization.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过切割足够多的边或者有时移除足够多的顶点，将任何连通图分开，使其断开连接。如果我们能够在给定的图中找到这些切割集合，比如在通信网络、电网、交通网络或其他网络中，我们可以切断其断开连接部分之间的所有通信方式。通常我们对最小或最小切割集感兴趣，这将通过移除最少数量的边或顶点来完成断开图的任务。这有助于我们识别网络中的最薄弱环节。与生成树相反，切割集将顶点分开，而不是将它们全部保持在一起。因此，我们可以合理地期望生成树和切割集之间存在密切关系。此外，如果图表示具有某种来源（如流体、交通、电力或信息）和汇点的网络，并且每条边只允许通过一定数量的流量，那么源到汇的最大流量与切割图的边之间存在密切关系，切割将源与汇断开，切割边的总容量最小。这就是*最大流最小切割定理*，它表明在流网络中，从源到汇的最大流量等于最小切割中边的总权重。在数学中，当最大化问题（最大流）以一种非平凡的方式等同于最小化问题（最小切割）时（例如不仅仅是改变目标函数的符号），这表明存在对偶性。事实上，图的最大流最小切割定理是线性优化对偶定理的一个特例。
- en: Planarity
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平面性
- en: Is the geometric representation of a graph planar or three dimensional? That
    is, can we draw the vertices of the graph and connect its edges, all in one plane,
    without its edges crossing each other? This is interesting for technological applications
    such as automatic wiring of complex systems, printed circuits, and large-scale
    integrated circuits. For nonplanar graphs, we are interested in properties such
    as the thickness of these graphs and the number of crossings between edges. An
    equivalent condition for a planar graph is the existence of a *dual graph*, where
    the relationship between a graph and its dual becomes clear in the context of
    the vector space of a graph. Linear algebra and graphs come together here, where
    algebraic and combinatoric representations answer questions about geometric figures
    and vice versa. For the planarity question, we only need to consider only simple,
    nonseparable graphs whose vertices all have degree three or more. Moreover, any
    graph with number of edges larger than three times the number of its vertices
    minus six is nonplanar. There are many unsolved problems in this field of study.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图的几何表示是平面的还是三维的？也就是说，我们能否在同一平面上绘制图的顶点并连接其边，而不让边相互交叉？这对于技术应用如复杂系统的自动布线、印刷电路和大规模集成电路非常有趣。对于非平面图，我们对这些图的厚度和边之间的交叉数量等属性感兴趣。对于平面图的等价条件是存在*对偶图*，在图的向量空间的背景下，图与其对偶之间的关系变得清晰。线性代数和图论在这里相结合，代数和组合表示回答了关于几何图形的问题，反之亦然。对于平面性问题，我们只需要考虑所有顶点的度数均为三或更高的简单、不可分离图。此外，任何边数大于其顶点数减去六的三倍的图都是非平面的。在这个研究领域中有许多未解决的问题。
- en: Graphs As Vector Spaces
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图作为向量空间
- en: It is important to understand a graph both as a geometric object and an algebraic
    object, along with the correspondence between the two representations. This is
    the case for graphs. Every graph corresponds to an *e* dimensional vector space
    over the field of integers modulo 2, where *e* is the number of edges of the graph.
    So if the graph only has three edges <math alttext="e d g e 1 comma e d g e 2
    comma e d g e 3"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>1</mn></msub>
    <mo>,</mo> <mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>,</mo> <mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>
    then it corresponds to the three dimensional vector space containing the vectors
    <math alttext="left-parenthesis 0 comma 0 comma 0 right-parenthesis comma left-parenthesis
    1 comma 0 comma 0 right-parenthesis comma left-parenthesis 0 comma 1 comma 0 right-parenthesis
    comma left-parenthesis 1 comma 1 comma 0 right-parenthesis comma left-parenthesis
    1 comma 0 comma 1 right-parenthesis comma left-parenthesis 0 comma 1 comma 1 right-parenthesis
    comma left-parenthesis 0 comma 0 comma 1 right-parenthesis comma left-parenthesis
    1 comma 1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math> . Here <math alttext="left-parenthesis
    0 comma 0 comma 0 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> corresponds to the null subgraph
    containing none of the three edges, <math alttext="left-parenthesis 1 comma 1
    comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo>
    <mn>1</mn> <mo>)</mo></mrow></math> corresponds to the full graph containing all
    three edges, <math alttext="left-parenthesis 0 comma 1 comma 1 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    corresponds to the subgraph containing only <math alttext="e d g e 2"><mrow><mi>e</mi>
    <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>2</mn></msub></mrow></math> , and <math
    alttext="e d g e 3"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>
    , and so on.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解图既作为几何对象又作为代数对象，以及两种表示之间的对应关系。这对于图来说是成立的。每个图对应于整数模2的一个*e*维向量空间，其中*e*是图的边数。因此，如果图只有三条边<math
    alttext="e d g e 1 comma e d g e 2 comma e d g e 3"><mrow><mi>e</mi> <mi>d</mi>
    <mi>g</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>,</mo> <mi>e</mi> <mi>d</mi>
    <mi>g</mi> <msub><mi>e</mi> <mn>2</mn></msub> <mo>,</mo> <mi>e</mi> <mi>d</mi>
    <mi>g</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>，那么它对应于包含向量的三维向量空间<math
    alttext="left-parenthesis 0 comma 0 comma 0 right-parenthesis comma left-parenthesis
    1 comma 0 comma 0 right-parenthesis comma left-parenthesis 0 comma 1 comma 0 right-parenthesis
    comma left-parenthesis 1 comma 1 comma 0 right-parenthesis comma left-parenthesis
    1 comma 0 comma 1 right-parenthesis comma left-parenthesis 0 comma 1 comma 1 right-parenthesis
    comma left-parenthesis 0 comma 0 comma 1 right-parenthesis comma left-parenthesis
    1 comma 1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>。这里<math alttext="left-parenthesis
    0 comma 0 comma 0 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math>对应于不包含这三条边的空子图，<math alttext="left-parenthesis
    1 comma 1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>对应于包含所有三条边的完整图，<math alttext="left-parenthesis
    0 comma 1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>对应于只包含<math alttext="e d g e 2"><mrow><mi>e</mi>
    <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>2</mn></msub></mrow></math>的子图，以及<math
    alttext="e d g e 3"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>，等等。
- en: Field of integers modulo 2
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整数模2的领域
- en: The field of integers modulo 2 only contains the two elements 0 and 1, with
    the operations + and <math alttext="times"><mo>×</mo></math> both happening modulo
    two. These are in fact equivalent to the logical operations *xor* (exclusive or
    operator) and *and* in Boolean logic. A vector space has to be defined over a
    field and has to be closed under multiplication of its vectors by *scalars* from
    that field. In this case, the scalars are only 0 and 1, and the multiplication
    happens modulo 2\. Graphs are therefore nice examples of vector spaces over finite
    fields, which are different than the usual real or complex numbers. The dimension
    of the vector space of a graph is the number of edges *e* of the graph, and the
    total number of vectors in this vector space is 2^(*e*). We can see here how graph
    theory is immediately applicable to switching circuits (with on and off switches),
    digital systems and signals, since all operate in the field of integers modulo
    2.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 整数模2的领域只包含两个元素0和1，加法和<math alttext="times"><mo>×</mo></math>运算都是模2进行的。实际上，这等同于布尔逻辑中的*异或*（异或运算符）和*与*操作。向量空间必须在一个领域上定义，并且必须对该领域的*标量*的向量乘法封闭。在这种情况下，标量只有0和1，乘法是模2进行的。因此，图是有限域上向量空间的良好示例，这与通常的实数或复数不同。图的向量空间的维数是图的边数*e*，这个向量空间中的向量总数是2^(*e*)。我们可以看到图论如何立即适用于开关电路（带有开关和关闭开关）、数字系统和信号，因为它们都在整数模2的领域中运行。
- en: With this simple correspondence, and backed by the whole field of linear algebra,
    it is natural to try to understand cut sets, circuits, fundamental circuits, spanning
    trees, and other important graph substructures, and relationships among them,
    in the context of vector subspaces, basis, intersections, orthogonality, and dimensions
    of these subspaces.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种简单的对应关系，并倚靠整个线性代数领域，自然而然地尝试理解切割集、回路、基本回路、生成树以及其他重要的图子结构，以及它们之间的关系，这些都是在向量子空间、基、交点、正交性以及这些子空间的维度的背景下进行的。
- en: Realizability
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可实现性
- en: We have already used the adjacency matrix and the incidence matrix as matrix
    representations that completely describe a graph. Others matrices decribe important
    features of the graph, such as the circuit matrix, the cut set matrix, and the
    path matrix. Then of course the relevant studies have to do how with how all these
    relate to each other and interact.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用邻接矩阵和关联矩阵作为完全描述图的矩阵表示。其他矩阵描述了图的重要特征，比如回路矩阵、割集矩阵和路径矩阵。然后当然相关研究必须涉及这些矩阵之间以及它们之间的相互关系和交互。
- en: 'Another very important topic is that of *realizability*: What conditions must
    a given matrix satisfy so that it is the circuit matrix of some graph?'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常重要的主题是*可实现性*：给定矩阵必须满足什么条件，以便它是某个图的回路矩阵？
- en: Coloring And Matching
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 着色和匹配
- en: In many situations we are interested in assigning labels, or colors, to the
    nodes, edges of a graph, or even regions in a planar graph. The famous graph coloring
    problem is when the colors we want to assign to each node are such that no neighboring
    vertices get same color, moreover, we want to do this using the minimal amount
    of colors. The smallest number of colors required to color a graph is called its
    *chromatic number*. Related to coloring are topics such as node partitioning,
    covering, and the chromatic polynomial.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们有兴趣为图的节点、边缘甚至平面图中的区域分配标签或颜色。著名的图着色问题是指我们要为每个节点分配的颜色是这样的，即相邻的顶点不会得到相同的颜色，而且我们希望使用最少的颜色来实现这一点。着色图所需的最少颜色数称为其*色数*。与着色相关的主题包括节点分区、覆盖和色多项式。
- en: A matching is a set of edges so that no two are adjacent. A maximal matching
    is a maximal set of edges where no two are adjacent. Matching in a general graph
    and in a bipartite graph have many applications, such as matching a minimal set
    of classes to satisfy graduation requirements, or matching job assignments to
    employees’ preferences (this ends up being a max flow min cut problem). We can
    use random walk based algorithms to find perfect matchings on large bipartite
    graphs.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配是一组边，使得任意两条边都不相邻。最大匹配是一组边的最大集合，其中任意两条边都不相邻。在一般图和二部图中的匹配有许多应用，比如匹配最小的一组课程以满足毕业要求，或者匹配工作分配给员工的偏好（这最终成为一个最大流最小割问题）。我们可以使用基于随机游走的算法来在大型二部图上找到完美匹配。
- en: Enumeration
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 枚举
- en: Cayley in 1857 was interested in counting the number of isomers of saturated
    hydrocarbon <math alttext="upper C Subscript n Baseline upper H Subscript 2 n
    plus 2"><mrow><msub><mi>C</mi> <mi>n</mi></msub> <msub><mi>H</mi> <mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow></math>
    , which led him to counting the number of different trees with n nodes, and to
    his contributions to graph theory. There are many types of graphs to be enumerated,
    and many have been their own research papers. Examples include enumerating all
    rooted trees, simple graphs, simple digraphs and others possessing specific properties.
    Enumeration is a huge area in graph theory. One important enumeration technique
    is Pólya’s counting theorem, where one needs to find an appropriate permutation
    group and then obtain its cycle index, which is nontrivial.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 1857年，Cayley对饱和烃异构体<mrow><msub><mi>C</mi> <mi>n</mi></msub> <msub><mi>H</mi>
    <mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow>的数量进行了计数，这使他开始计算具有n个节点的不同树的数量，并为他在图论方面的贡献。有许多类型的图需要枚举，许多都有自己的研究论文。例如，枚举所有根树、简单图、简单有向图以及具有特定属性的其他图。枚举是图论中的一个重要领域。一个重要的枚举技术是Pólya计数定理，其中需要找到一个适当的置换群，然后获得其循环指数，这是非平凡的。
- en: Algorithms And Computational Aspects Of Graphs
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的算法和计算方面
- en: 'Algorithms and computer implementations are of tremendous value for anyone
    working with graph modeling. Algorithms exist for traditional graph theoretical
    tasks such as:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何从事图模型工作的人来说，算法和计算机实现具有巨大的价值。存在用于传统图论任务的算法，例如：
- en: Find out if graph is separable.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出图是否可分。
- en: Find out if a graph is connected.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出图是否连通。
- en: Find out the components of a graph.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出图的组件。
- en: Find the spanning trees of a graph.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找图的生成树。
- en: Find a set of fundamental circuits.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到一组基本回路。
- en: Find cut sets.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到割集。
- en: Find the shortest path from a given node to another.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从给定节点到另一个节点找到最短路径。
- en: Test whether the graph is planar.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试图是否是平面的。
- en: Build a graph with specific properties.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建具有特定属性的图。
- en: 'Graph neural networks nowadays come with their own open source packages. As
    always for an algorithm to be of any practical use it must efficient: Its running
    time must not increase factorially or even exponentially with the number of nodes
    of the graph. It should be polynomial time, proportional to <math alttext="n Superscript
    k"><msup><mi>n</mi> <mi>k</mi></msup></math> , where *k* is preferably a low number.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，图神经网络已经有了自己的开源软件包。对于任何算法来说，要想实际应用，它必须是高效的：其运行时间不能随着图的节点数量的增加而阶乘甚至指数增长。它应该是多项式时间，与<msup><mi>n</mi>
    <mi>k</mi></msup>成正比，其中*k*最好是一个较低的数字。
- en: For anyone wishing to enter the field of graph modeling, it is of great use
    to familiarize themselves with both the theory and computational aspects of graphs.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望进入图模型领域的任何人来说，熟悉图的理论和计算方面是非常有用的。
- en: Summary And Looking Ahead
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和展望
- en: This chapter was a summary of various aspects of graphical modeling, with emphasis
    on examples, applications, and building intuition. There are many references for
    readers aiming to dive deeper. The main message is not to get lost in the weeds
    (and they are very thick) without an aim or an understanding of the big picture,
    the current state of the field, and how it relates to AI.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节总结了图形建模的各个方面，重点放在示例、应用和直觉上。对于希望深入了解的读者，有许多参考资料。主要信息是不要在没有目标或对大局、领域当前状态以及与人工智能的关系的理解的情况下迷失在细节中（而这些细节非常复杂）。
- en: We also introduced random walks on graphs, Bayesian networks and probabilistic
    causal models, which shifted our brain even more in the direction of probabilistic
    thinking, the main topic of [Chapter 11](ch11.xhtml#ch11). It was my intention
    all along to go over all kinds of uses for probability in AI before going into
    a math chapter on probability ([Chapter 11](ch11.xhtml#ch11)).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了图上的随机游走、贝叶斯网络和概率因果模型，这进一步引导我们的大脑朝着概率思维的方向发展，这也是第11章的主题。在进入关于概率的数学章节之前，我一直打算讨论人工智能中概率的各种用途。
- en: 'We leave this chapter with this very nice read: [Relational inductive biases,
    deep learning, and graph networks](https://arxiv.org/pdf/1806.01261.pdf) which
    makes the case for the deep learning community to adopt graph networks: *We present
    a new building block for the AI toolkit with a strong relational inductive bias—the
    graph network—which generalizes and extends various approaches for neural networks
    that operate on graphs, and provides a straightforward interface for manipulating
    structured knowledge and producing structured behaviors. We discuss how graph
    networks can support relational reasoning and combinatorial generalization, laying
    the foundation for more sophisticated, interpretable, and flexible patterns of
    reasoning.*'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们结束这一章节时推荐阅读这篇文章：《关系归纳偏差、深度学习和图网络》，该文章提出了深度学习社区应采用图网络的理由：*我们提出了一种具有强关系归纳偏差的人工智能工具包的新构建模块——图网络，它推广和扩展了各种在图上操作的神经网络方法，并提供了一个直观的界面来操作结构化知识和产生结构化行为。我们讨论了图网络如何支持关系推理和组合泛化，为更复杂、可解释和灵活的推理模式奠定了基础。*
