- en: 6 Operation patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 操作模式
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Recognizing areas of improvement in machine learning systems, such as job scheduling
    and metadata
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别机器学习系统中的改进领域，例如作业调度和元数据
- en: Preventing resource starvation and avoiding deadlocks using scheduling techniques,
    such as fair-share scheduling, priority scheduling, and gang scheduling
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用如公平共享调度、优先级调度和组调度等技术来防止资源饥饿和避免死锁
- en: Handling failures more effectively to reduce any negative effect on users via
    the metadata pattern
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过元数据模式更有效地处理故障，以减少对用户的任何负面影响
- en: In chapter 5, we focused on machine learning workflows and the challenges of
    building them in practice. Workflow is an essential component in machine learning
    systems as it connects all components in the system. A machine learning workflow
    can be as easy as chaining data ingestion, model training, and model serving.
    It can also be very complex when handling real-world scenarios, requiring additional
    steps and performance optimizations to be part of the entire workflow.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第五章中，我们关注了机器学习工作流程及其在实际构建中的挑战。工作流程是机器学习系统中的一个基本组成部分，因为它连接了系统中的所有组件。一个机器学习工作流程可以简单到只是将数据摄取、模型训练和模型服务串联起来。在处理现实世界场景时，它也可以非常复杂，需要额外的步骤和性能优化才能成为整个工作流程的一部分。
- en: Knowing the tradeoffs we may encounter when making design decisions to meet
    specific business and performance requirements is essential. I previously introduced
    a few established patterns commonly adopted in industry. Each pattern can be reused
    to build simple to complex machine learning workflows that are efficient and scalable.
    For example, we learned how to use the fan-in and fan-out patterns to build a
    system to execute complex machine learning workflows (section 5.2). This system
    can train multiple machine learning models and pick the most performant ones to
    provide good entity-tagging results. We also used synchronous and asynchronous
    patterns to make machine learning workflows more efficient and avoid delays due
    to the long-running model training steps that block other steps (section 5.3).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 了解在做出设计决策以满足特定业务和性能要求时可能遇到的权衡是至关重要的。我之前介绍了一些在工业界普遍采用的成熟模式。每个模式都可以被重用来构建从简单到复杂的、高效且可扩展的机器学习工作流程。例如，我们学习了如何使用扇入和扇出模式来构建一个执行复杂机器学习工作流程的系统（第5.2节）。这个系统可以训练多个机器学习模型，并选择性能最好的模型以提供良好的实体标记结果。我们还使用了同步和异步模式来提高机器学习工作流程的效率，并避免由于长时间运行的模型训练步骤而导致的延迟，这些步骤会阻塞其他步骤（第5.3节）。
- en: Since real-world distributed machine learning workflows can be extremely complex,
    as seen in chapter 5, a huge amount of *operational* work is involved to help
    maintain and manage the various components of the systems, such as improvements
    to system efficiency, observability, monitoring, deployment, etc. These operational
    work efforts usually require a lot of communication and collaboration between
    the DevOps and data science teams. For instance, the DevOps team may not have
    enough domain knowledge in machine learning algorithms used by the data science
    team to debug any encountered problems or optimize the underlying infrastructure
    to accelerate the machine learning workflows. For a data science team, the type
    of computational workload varies, depending on the team structure and the way
    team members collaborate. As a result, there’s no universal way for the DevOps
    team to handle the requests of different workloads from the data science team.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界的分布式机器学习工作流程可能非常复杂，正如第五章所见，维护和管理系统各个组件（如系统效率、可观察性、监控、部署等）需要大量的操作工作。这些操作工作通常需要DevOps团队和数据科学团队之间大量的沟通和协作。例如，DevOps团队可能没有足够的数据科学团队使用的机器学习算法的领域知识来调试遇到的问题或优化底层基础设施以加速机器学习工作流程。对于数据科学团队来说，计算工作负载的类型因团队结构和团队成员的协作方式而异。因此，DevOps团队没有处理来自数据科学团队的不同工作负载请求的通用方法。
- en: Fortunately, operational efforts and patterns can be used to greatly accelerate
    the end-to-end workflow. They can also reduce maintenance and communication efforts
    when engineering teams are collaborating with teams of data scientists or machine
    learning practitioners before the systems become production ready.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，操作努力和模式可以极大地加速端到端工作流程。它们还可以在系统成为生产就绪之前，当工程团队与数据科学家或机器学习实践者团队合作时，减少维护和沟通的努力。
- en: In this chapter, we’ll explore some of the challenges involved when performing
    operations on machine learning systems in practice and introduce a few commonly
    used patterns. For example, we’ll use scheduling techniques to prevent resource
    starvation and avoid deadlocks when many team members are working collaboratively
    in the same cluster with limited computational resources. We will also discuss
    the benefits of the metadata pattern, which can provide insights into the individual
    steps in machine learning workflows and help us handle failures more appropriately
    to reduce any negative effects on users.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在实际操作机器学习系统时遇到的挑战，并介绍一些常用的模式。例如，我们将使用调度技术来防止资源饥饿和避免死锁，当许多团队成员在有限的计算资源下在同一集群中协作时。我们还将讨论元数据模式的益处，它可以提供对机器学习工作流程中各个步骤的见解，并帮助我们更恰当地处理故障，以减少对用户产生的任何负面影响。
- en: 6.1 What are operations in machine learning systems?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 机器学习系统中的操作是什么？
- en: In this chapter, I will focus on operational techniques and patterns that are
    commonly seen in more than one component or step in a machine learning workflow,
    instead of patterns that are specific to each individual component. For example,
    the workflow shown in figure 6.1 includes three failed steps in the multiple model
    training steps that occur after data ingestion and in the multiple model serving
    steps that occur after the multiple model training steps. Unfortunately, each
    step is like a black box, and we don’t know many details about any of them yet.
    At this point, we only know whether they fail and whether the failures have affected
    the following steps. As a result, they are really hard to debug.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将重点关注在机器学习工作流程中常见于多个组件或步骤的操作技术和模式，而不是针对每个单独组件的模式。例如，图6.1所示的工作流程包括在数据摄入之后和多个模型训练步骤之后的多个模型服务步骤中发生的三个失败步骤。不幸的是，每个步骤都像一个黑盒，我们目前对它们的许多细节还一无所知。在这个阶段，我们只知道它们是否失败以及这些失败是否影响了后续步骤。因此，它们真的很难调试。
- en: '![06-01](../../OEBPS/Images/06-01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![06-01](../../OEBPS/Images/06-01.png)'
- en: Figure 6.1 An example workflow where multiple model training steps occur after
    data ingestion and multiple model serving steps occur after the multiple model
    training steps. Note the three failed steps.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 一个示例工作流程，其中在数据摄入之后发生多个模型训练步骤，在多个模型训练步骤之后发生多个模型服务步骤。注意三个失败的步骤。
- en: The operation patterns I introduce in this chapter can increase the visibility
    of the entire workflow to help us understand the root cause of the failures and
    give us some ideas on how to handle the failures properly. In addition, the increased
    observability may help us develop improvements in system efficiency that are beneficial
    to future executions of similar workflows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章中介绍的操作模式可以提高整个工作流程的可见性，帮助我们理解故障的根本原因，并给我们一些如何正确处理故障的想法。此外，提高的可观察性可能有助于我们开发对类似工作流程未来执行有益的系统效率改进。
- en: What about MLOps?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，MLOps是什么呢？
- en: We often hear about *MLOps* nowadays, which is a term derived from machine learning
    and operations. It usually means a collection of practices for managing machine
    learning lifecycles in production, including practices from machine learning and
    DevOps, to efficiently and reliably deploy and manage machine learning models
    in production.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在经常听到*MLOps*这个词，这是一个从机器学习和操作中衍生出来的术语。它通常意味着一组用于管理生产中机器学习生命周期的实践，包括来自机器学习和DevOps的实践，以高效和可靠地部署和管理生产中的机器学习模型。
- en: MLOps usually require communication and collaboration between DevOps and data
    science teams. It focuses on improving the quality of production machine learning
    and embracing automation while maintaining business requirements. The scope of
    MLOps can be extremely large and varies depending on the context.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps通常需要DevOps团队和数据科学团队之间的沟通和协作。它侧重于提高生产机器学习的质量，同时拥抱自动化，并保持业务需求。MLOps的范围可能非常大，并且根据上下文而变化。
- en: Given how large the scope of MLOps can be, depending on the context, I will
    only focus on a selected set of mature patterns at the time of writing. You can
    expect some updates to any future versions of this chapter as this field evolves.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MLOps的范围可能非常大，这取决于上下文，我在写作时将仅关注一组成熟的模式。您可以期待随着这个领域的演变，本章的任何未来版本都将有所更新。
- en: '6.2 Scheduling patterns: Assigning resources effectively in a shared cluster'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 调度模式：在共享集群中有效分配资源
- en: Let’s assume we have successfully set up the distributed infrastructure for
    users to submit distributed model training jobs that are scheduled to run on multiple
    CPUs by a default *scheduler*. A scheduler is responsible for assigning computational
    resources to perform tasks requested by the system. It is designed to keep computational
    resources busy and allow multiple users to collaborate with shared resources more
    easily. Multiple users are trying to build models using the shared computational
    resources in the cluster for different scenarios. For example, one user is working
    on a fraud detection model that tries to identify fraudulent financial behaviors
    such as international money laundering. Another user is working on a condition
    monitoring model that can generate a health score to represent the current condition
    for industrial assets such as components on trains, airplanes, wind turbines,
    etc.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经成功为用户设置了分布式基础设施，以便提交分布式模型训练作业，这些作业默认由*调度器*调度，在多个CPU上运行。调度器负责分配计算资源以执行系统请求的任务。它旨在保持计算资源忙碌，并允许多个用户更容易地共享资源进行协作。多个用户正在尝试使用集群中的共享计算资源构建模型，以适应不同的场景。例如，一位用户正在开发一个欺诈检测模型，试图识别诸如国际洗钱等欺诈金融行为。另一位用户正在开发一个条件监测模型，可以生成一个健康分数来表示工业资产（如火车、飞机、风力涡轮机等组件）的当前状态。
- en: Our beginning infrastructure only provides a simple scheduler, which schedules
    jobs on a first-come, first-served basis, as shown in figure 6.2\. For example,
    the third job is scheduled after the second job has been scheduled, and each job’s
    computational resources are allocated on scheduling.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基础设施最初只提供了一个简单的调度器，它按照先到先得的原则调度作业，如图6.2所示。例如，第三项作业是在第二项作业被调度之后调度的，并且每个作业的计算资源都是在调度时分配的。
- en: '![06-02](../../OEBPS/Images/06-02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![06-02](../../OEBPS/Images/06-02.png)'
- en: Figure 6.2 A diagram of an infrastructure that only provides a simple scheduler,
    which schedules jobs on a first-come, first-served basis
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 仅提供简单调度器的基础设施示意图，该调度器按照先到先得的原则调度作业
- en: In other words, the users who schedule jobs later must wait for all previously
    submitted jobs to finish before their model training jobs can start executing.
    Unfortunately, in the real world, users often want to submit multiple model training
    jobs to experiment with different sets of models or hyperparameters. These multiple
    models block other users’ model training jobs from executing since those previously
    submitted experiments are already utilizing all the available computational resources.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，调度作业较晚的用户必须等待所有先前提交的作业完成，然后他们的模型训练作业才能开始执行。不幸的是，在现实世界中，用户通常希望提交多个模型训练作业以实验不同的模型集或超参数集。这些多个模型阻塞了其他用户的模型训练作业的执行，因为先前提交的实验已经利用了所有可用的计算资源。
- en: In this case, users must compete for resources (e.g., waking up in the middle
    of the night to submit model training jobs when fewer users are using the system).
    As a result, collaboration among team members may not be pleasant. Some jobs include
    training very large machine learning models, which usually consume a lot of computational
    resources and thus increase the time other users have to wait for their jobs to
    execute.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，用户必须争夺资源（例如，在夜间唤醒以提交模型训练作业，因为此时使用系统的用户较少）。因此，团队成员之间的协作可能不会愉快。一些作业包括训练非常大的机器学习模型，这些模型通常消耗大量的计算资源，从而增加了其他用户等待其作业执行的时间。
- en: In addition, if we only schedule some of the requested workers for a distributed
    model training job, the model training cannot execute until all of the requested
    workers are ready; the nature of the distribution strategy is distributed training
    with the collective communication pattern. If necessary computational resources
    are lacking, the job will never start, and the already-allocated computational
    resources for the existing workers will be wasted.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们只为分布式模型训练作业调度部分请求的工人，则模型训练无法执行，直到所有请求的工人都准备好；分布策略的本质是具有集体通信模式的分布式训练。如果必要的计算资源不足，作业将永远不会启动，并且已分配给现有工人的计算资源将被浪费。
- en: 6.2.1 The problem
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 问题
- en: We have set up a distributed infrastructure for users to submit distributed
    model training jobs scheduled to run by a default scheduler responsible for assigning
    computational resources to perform various tasks requested by the users. However,
    the default scheduler only provides a simple scheduler that schedules jobs on
    a first-come, first-served basis. As a result, when multiple users attempt to
    use this cluster, they often need to wait a long time for available computational
    resources—that is, until the previously submitted jobs are completed. In addition,
    distributed model training jobs cannot begin to execute until all of the requested
    workers are ready due to the nature of the distributed training strategy, such
    as a collective communication strategy. Are there any alternatives to the existing
    default scheduler so we could assign the computational resources more effectively
    in a shared cluster?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为用户建立了一个分布式基础设施，用户可以通过默认调度器提交分布式模型训练作业，该调度器负责分配计算资源以执行用户请求的各种任务。然而，默认调度器仅提供一个简单的调度器，它按照先到先得的原则安排作业。因此，当多个用户尝试使用此集群时，他们通常需要等待很长时间才能获得可用的计算资源——也就是说，直到之前提交的作业完成。此外，由于分布式训练策略的性质，如集体通信策略，分布式模型训练作业必须等到所有请求的工人准备就绪后才能开始执行。那么，有没有任何替代现有的默认调度器的方法，以便我们可以在共享集群中更有效地分配计算资源？
- en: 6.2.2 The solution
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 解决方案
- en: In our scenario, the problem starts to occur when multiple users are trying
    to use the system to submit distributed model training jobs at the same time.
    Since the jobs are being executed on a first-come, first-served basis, the waiting
    times for jobs submitted later are long, even when those jobs are submitted by
    multiple users.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，当多个用户同时尝试使用系统提交分布式模型训练作业时，问题开始出现。由于作业是按照先到先得的原则执行的，因此后来提交的作业的等待时间很长，即使这些作业是由多个用户提交的。
- en: It’s easy to identify different users, so an intuitive solution would be to
    limit how much of the total computational resources each user is allotted. For
    example, say there are four users (A, B, C, and D). Once user A submits a job
    that uses 25% of the total available CPU cycles ([https://techterms.com/definition/clockcycle](https://techterms.com/definition/clockcycle)),
    they cannot submit another job until those allocated resources are released and
    ready to be allocated to new jobs. Other users could submit jobs independent of
    how much resources user A is using. For example, if user B starts two processes
    that use the same amount of resources, those processes will be attributed 12.5%
    of the total CPU cycles each, giving user B 25% of total resources. Each of the
    other users still receives 25% of the total cycles. Figure 6.3 illustrates the
    resource allocations for these four users.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 识别不同的用户很容易，因此一个直观的解决方案是限制每个用户分配的总计算资源量。例如，假设有四个用户（A、B、C和D）。一旦用户A提交了一个使用25%总可用CPU周期（[https://techterms.com/definition/clockcycle](https://techterms.com/definition/clockcycle)）的作业，他们就不能提交另一个作业，直到那些分配的资源被释放并准备好分配给新的作业。其他用户可以独立于用户A使用多少资源提交作业。例如，如果用户B启动了两个使用相同资源量的进程，这些进程将分别分配到12.5%的总CPU周期，从而给用户B分配25%的总资源。其他每个用户仍然获得25%的总周期。图6.3说明了这四个用户的资源分配情况。
- en: '![06-03](../../OEBPS/Images/06-03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![06-03](../../OEBPS/Images/06-03.png)'
- en: Figure 6.3 The resource allocations for the four users (A, B, C, and D)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 四个用户（A、B、C和D）的资源分配
- en: If a new user E starts a process on the system, the scheduler will reapportion
    the available CPU cycles so that each user gets 20% of the whole (100% / 5 = 20%).
    The way we schedule our workloads to execute in our cluster in figure 6.3 is called
    *fair-share scheduling*. It is a scheduling algorithm for computer operating systems
    in which the CPU usage is equally distributed among system users or groups, as
    opposed to equal distribution among processes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果新用户E在系统上启动一个进程，调度器将重新分配可用的CPU周期，以便每个用户获得整个的20%（100% / 5 = 20%）。我们在图6.3中安排工作负载在集群中执行的方式被称为*公平共享调度*。这是一种计算机操作系统的调度算法，其中CPU使用量在系统用户或组之间平均分配，而不是在进程之间平均分配。
- en: 'So far, we have only discussed partitioning resources among the users. When
    multiple teams are using the system to train their machine learning models and
    each team has multiple members, we can partition users into different groups and
    then apply the fair-share scheduling algorithm to both the users and the groups.
    Specifically, we first divide the available CPU cycles among the groups and then
    divide further among the users within each group. For example, if three groups
    contain three, two, and four users, respectively, each group will be able to use
    33.3% (100% / 3) of the total available CPU cycles. We can then calculate the
    available CPU cycles for each user in each group as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了在用户之间分配资源。当多个团队使用系统来训练他们的机器学习模型，并且每个团队有多名成员时，我们可以将用户划分为不同的组，然后对用户和组都应用公平共享调度算法。具体来说，我们首先将可用的CPU周期分配给组，然后在每个组内进一步分配给用户。例如，如果三个组分别包含三个、两个和四个用户，那么每个组将能够使用33.3%（100%
    / 3）的总可用CPU周期。然后我们可以按照以下方式计算每个组中每个用户的可用CPU周期：
- en: '*Group 1*—33.3% / 3 users = 11.1% per user'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*组1*—33.3% / 3用户 = 每用户11.1%'
- en: '*Group 2*—33.3% / 2 users = 16.7% per user'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*组2*—33.3% / 2用户 = 每用户16.7%'
- en: '*Group 3*—33.3% / 4 users = 8.3% per user'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*组3*—33.3% / 4用户 = 每用户8.3%'
- en: Figure 6.4 summarizes the resource allocation we calculated for each individual
    user in the three groups.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4总结了我们对三个组中每个个体用户计算的资源分配。
- en: '![06-04](../../OEBPS/Images/06-04.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![06-04](../../OEBPS/Images/06-04.png)'
- en: Figure 6.4 A summary of the resource allocation for each user in three groups
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 三组中每个用户的资源分配总结
- en: Fair-share scheduling would help us resolve the problem of multiple users running
    distributed training jobs concurrently. We can apply this scheduling strategy
    at each level of abstraction, such as processes, users, groups, etc. All users
    have their own pool of available resources without interfering with each other.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 公平共享调度可以帮助我们解决多个用户同时运行分布式训练作业的问题。我们可以在每个抽象级别应用这种调度策略，例如进程、用户、组等。所有用户都有自己的可用资源池，不会相互干扰。
- en: However, in some situations, certain jobs should be executed earlier. For example,
    a cluster administrator would like to submit jobs for cluster maintenance, such
    as deleting jobs that have been stuck and taking up resources for a long time.
    Executing these cluster maintenance jobs earlier would help make more computational
    resources available and thus unblock others from submitting new jobs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，某些作业应该先执行。例如，集群管理员可能希望提交集群维护作业，如删除长时间占用资源的挂起作业。提前执行这些集群维护作业可以帮助释放更多计算资源，从而让其他人能够提交新的作业。
- en: Let’s assume the cluster administrator is user 1 in group 1\. Two other nonadmin
    users are also in group 1, as in the previous example. User 2 is running job 1,
    which is using all of the 11.1% of the CPU cycles allocated to them based on the
    fair-share scheduling algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设集群管理员是组1中的用户1。另外两个非管理员用户也在组1中，就像之前的例子一样。用户2正在运行作业1，该作业正在使用基于公平共享调度算法分配给他们的11.1%的CPU周期。
- en: Even though user 2 has enough computational power to perform job 1, the job
    depends on the success of job 2 from user 3\. For example, job 2 from user 3 produces
    a table in the database that job 1 needs to perform a distributed model training
    task. Figure 6.5 summarizes the resource allocations and usages for each user
    in the first group.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管用户2有足够的计算能力来执行作业1，但该作业依赖于用户3的作业2的成功。例如，用户3的作业2在数据库中生成一个表格，作业1需要该表格来执行分布式模型训练任务。图6.5总结了第一组中每个用户的资源分配和利用率。
- en: '![06-05](../../OEBPS/Images/06-05.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![06-05](../../OEBPS/Images/06-05.png)'
- en: Figure 6.5 A summary of resource allocations and usages for each user in the
    first group
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 第一组中每个用户的资源分配和利用率总结
- en: Unfortunately, job 2 is stuck due to an unstable database connection and keeps
    trying to reconnect to produce the data that job 1 needs. To fix the problem,
    the administrator needs to submit job 3 that kills and then restarts the stuck
    job 2.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于数据库连接不稳定，作业2卡住了，并且一直在尝试重新连接以生成作业1所需的数据。为了解决这个问题，管理员需要提交作业3来终止并重新启动卡住的作业2。
- en: Now assume that the admin user 1 is already using 11.1% of the total CPU cycles
    available. As a result, since maintenance job 3 is submitted later than all previous
    jobs, it is added to the job queue and waits to be executed when resources are
    released, based on the first-come, first-served nature of our fair-share scheduling
    algorithm. As a result, we encounter a *deadlock* where no job can proceed, as
    illustrated in figure 6.6.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设管理员用户1已经使用了11.1%的总CPU周期。因此，由于维护作业3提交时间晚于所有之前的作业，它被添加到作业队列中，等待资源释放时执行，根据我们公平共享调度算法的先到先得性质。结果，我们遇到了一个*死锁*，没有任何作业可以继续进行，如图6.6所示。
- en: '![06-06](../../OEBPS/Images/06-06.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![06-06](../../OEBPS/Images/06-06.png)'
- en: Figure 6.6 The admin user (user 1) in group 1 is trying to schedule a job to
    restart the stuck job (job 3) but encounters a deadlock where no job can proceed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 在组1中的管理员用户（用户1）试图调度一个作业来重启挂起的作业（作业3），但遇到了一个死锁，没有任何作业可以继续进行。
- en: To fix this problem, we can allow users to assign *priorities* to each of the
    jobs so that jobs with higher priority are executed earlier, in contrast to the
    first-come, first-served nature of the fair-share scheduling algorithm. In addition,
    the jobs that are already running can be *preempted* or *evicted* to make room
    for jobs with higher priorities if not enough computational resources are available.
    This way of scheduling jobs based on priorities is called *priority scheduling*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以允许用户为每个作业分配*优先级*，这样具有更高优先级的作业将先执行，这与公平共享调度算法的先到先得性质相反。此外，如果可用计算资源不足，可以*抢占*或*驱逐*正在运行的作业，为具有更高优先级的作业腾出空间。这种基于优先级的作业调度方式称为*优先级调度*。
- en: Say, for example, four jobs (A, B, C, and D) have been submitted concurrently.
    Each job has been marked with priorities by the users. Jobs A and C are high priority,
    whereas job B is low priority, and job D is medium priority. With priority scheduling,
    jobs A and C will be executed first since they have the highest priorities, followed
    by the execution of job D with medium priority and, eventually low-priority job
    B. Figure 6.7 illustrates the order of execution for the four jobs (A, B, C, and
    D) when priority scheduling is used.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，有四个作业（A、B、C和D）同时提交。每个作业都由用户标记了优先级。作业A和C是高优先级，而作业B是低优先级，作业D是中等优先级。使用优先级调度，作业A和C将首先执行，因为它们的优先级最高，然后是具有中等优先级的作业D，最后是低优先级的作业B。图6.7说明了使用优先级调度时四个作业（A、B、C和D）的执行顺序。
- en: '![06-07](../../OEBPS/Images/06-07.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![06-07](../../OEBPS/Images/06-07.png)'
- en: Figure 6.7 The order of execution for the four concurrently submitted jobs (A,
    B, C, and D) when priority scheduling is used
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 使用优先级调度时，四个同时提交的作业（A、B、C和D）的执行顺序
- en: Let’s consider another example. Assume three jobs (B, C, and D) with different
    priorities are submitted concurrently and are executed based on their priorities,
    similar to the previous example. If another job (job A) with high priority is
    submitted after job B, which is low priority, has already started running, job
    B will be preempted, and then job A will start. The computational resources previously
    allocated to job B will be released and taken over by job A. Figure 6.8 summarizes
    the order of execution for the four jobs (A, B, C, and D) where the low-priority
    job B already running is preempted by a new job (job A) with higher priority.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个例子。假设有三个具有不同优先级的作业（B、C和D）同时提交，并按照它们的优先级执行，类似于前面的例子。如果提交了一个具有高优先级的作业（作业A），在低优先级的作业B已经开始运行之后，作业B将被抢占，然后作业A开始。之前分配给作业B的计算资源将被释放并由作业A接管。图6.8总结了四个作业（A、B、C和D）的执行顺序，其中正在运行的低优先级作业B被具有更高优先级的新作业（作业A）抢占。
- en: '![06-08](../../OEBPS/Images/06-08.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![06-08](../../OEBPS/Images/06-08.png)'
- en: Figure 6.8 The order of execution for the four jobs (A, B, C, and D) where the
    running low-priority job is preempted by a new job with higher priority
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 正在运行的低优先级作业被具有更高优先级的新作业抢占的四个作业（A、B、C和D）的执行顺序
- en: With priority scheduling, we can effectively eliminate the problem we previously
    encountered, where jobs can only be executed sequentially on a first-come, first-served
    basis. Jobs can now be preempted in favor of tasks with high priorities.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用优先级调度，我们可以有效地消除我们之前遇到的问题，即作业只能按先到先得的顺序依次执行。现在可以优先执行具有高优先级的任务。
- en: However, for distributed machine learning tasks—specifically, model training
    tasks—we want to ensure that all workers are ready before starting distributed
    training. Otherwise, the ones that are ready would be waiting for the remaining
    workers before the training can proceed, which wastes resources.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于分布式机器学习任务——特别是模型训练任务——我们希望在开始分布式训练之前确保所有工人都已准备好。否则，已经准备好的工人将等待剩余的工人，直到训练可以继续，这会浪费资源。
- en: For example, in figure 6.9, three worker processes in the same process group
    are performing an allreduce operation. However, two workers are not ready because
    the underlying distributed cluster is experiencing an unstable network. As a result,
    two of the processes (processes 1 and 3) that depend on those affected communications
    would not receive some of the calculated gradient values (v0 and v2) on time (denoted
    by question marks in figure 6.9), and the entire allreduce operation is stuck
    until everything is received.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在图6.9中，同一进程组中的三个工作进程正在进行allreduce操作。然而，由于底层分布式集群网络不稳定，有两个工人尚未准备好。因此，依赖于这些受影响通信的两个进程（进程1和3）将无法及时接收到一些计算出的梯度值（v0和v2）（如图6.9中的问号所示），整个allreduce操作将一直停滞，直到所有信息都接收完毕。
- en: '![06-09](../../OEBPS/Images/06-09.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![06-09](../../OEBPS/Images/06-09.png)'
- en: Figure 6.9 An example of the allreduce process with an unstable network between
    the worker processes that blocks the entire model training process
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9展示了工作进程之间网络不稳定导致整个模型训练过程受阻的allreduce过程示例
- en: '*Gang scheduling* is usually used to run distributed model training tasks.
    It ensures that if two or more workers communicate with each other, they will
    be ready to do so at the same time. In other words, gang scheduling only schedules
    workers when enough workers are available and ready to communicate.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*成组调度*通常用于运行分布式模型训练任务。它确保如果两个或多个工人相互通信，它们将同时准备好这样做。换句话说，成组调度仅在足够多的工人可用且准备好通信时调度工人。'
- en: If they are not gang scheduled, one worker may wait to send or receive a message
    while the other worker is sleeping, and vice versa. When the workers are waiting
    for other workers to be ready for communication, we are wasting allocated resources
    on the workers that are ready, and the entire distributed model training task
    is stuck.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它们没有被成组调度，一个工人可能需要在另一个工人睡眠时等待发送或接收消息，反之亦然。当工人在等待其他工人准备好通信时，我们正在浪费已经分配给准备好的工人的资源，整个分布式模型训练任务也因此停滞不前。
- en: For example, for collective communication-based distributed model training tasks,
    all workers must be ready to communicate the calculated gradients and update the
    models on each worker to complete an allreduce operation. I assume that the machine
    learning framework does not support elastic scheduling yet, which we will discuss
    in the next section. As shown in figure 6.10, the gradients are all denoted by
    question marks since they have not yet arrived in any of those worker processes
    in the second worker group. All worker processes have not yet started sending
    the gradients, and they won’t until they all move to the ready state after the
    network stabilizes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于基于集体通信的分布式模型训练任务，所有工人必须准备好通信计算出的梯度并更新每个工人上的模型，以完成allreduce操作。我假设机器学习框架目前还不支持弹性调度，我们将在下一节讨论。如图6.10所示，由于这些梯度尚未到达第二组中的任何工作进程，因此所有梯度都用问号表示。所有工作进程都尚未开始发送梯度，并且它们不会发送，直到网络稳定后所有进程都进入就绪状态。
- en: '![06-10](../../OEBPS/Images/06-10.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![06-10](../../OEBPS/Images/06-10.png)'
- en: Figure 6.10 With gang scheduling, the worker processes will not start sending
    the gradients until they are all in the ready state after the network becomes
    stable.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10展示了在成组调度下，工作进程将在网络稳定后所有进程都进入就绪状态时才开始发送梯度。
- en: With gang scheduling, we can make sure not to start any of the worker processes
    until all workers are ready, so none of them will be waiting for the remaining
    worker processes. As a result, we can avoid wasting computational resources. Once
    the network becomes stable, all of the gradients (v0, v1, and v2) arrive on each
    worker process after a successful allreduce operation, as shown in figure 6.11.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用成组调度，我们可以确保在所有工人都准备好之前不启动任何工作进程，这样就没有一个进程会等待剩余的工作进程。因此，我们可以避免浪费计算资源。一旦网络变得稳定，所有梯度（v0、v1和v2）在成功的allreduce操作后都会到达每个工作进程，如图6.11所示。
- en: '![06-11](../../OEBPS/Images/06-11.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![06-11](../../OEBPS/Images/06-11.png)'
- en: Figure 6.11 All of the gradients arrive on each of the worker processes after
    a successful allreduce operation once the network is stable.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 在网络稳定后，所有梯度都会在各个工作者进程上成功完成allreduce操作。
- en: NOTE The details of different types of gang scheduling and their algorithms
    are out of the scope of this book and will not be discussed here. However, we
    will be using an existing open source framework to integrate gang scheduling into
    distributed training in the last part of the book.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：不同类型群组调度的细节及其算法超出了本书的范围，这里不会进行讨论。然而，我们将在本书的最后部分使用现有的开源框架将群组调度集成到分布式训练中。
- en: By incorporating different scheduling patterns, we are able to address various
    problems that arise when multiple users are using the infrastructure to schedule
    different types of jobs. Although we looked at a few specific use cases for these
    scheduling patterns, the patterns can be found in many systems that require careful
    management of computational resources, especially when resources are scarce. Many
    scheduling techniques are applied to even lower-level operating systems to make
    sure the applications run efficiently and reasonably share resources.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合不同的调度模式，我们能够解决当多个用户使用基础设施调度不同类型的作业时出现的各种问题。尽管我们研究了这些调度模式的一些特定用例，但这些模式可以在许多需要仔细管理计算资源的系统中找到，尤其是在资源稀缺的情况下。许多调度技术甚至应用于更底层的操作系统，以确保应用程序高效运行并合理共享资源。
- en: 6.2.3 Discussion
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 讨论
- en: We’ve seen how fair-share scheduling can help us solve the problem of multiple
    users running distributed training jobs concurrently. Fair-share scheduling allows
    us to apply a scheduling strategy at each level of abstraction, such as processes,
    users, groups, etc. We also discussed priority scheduling, which can be used to
    effectively eliminate the problem we encounter when jobs can only be executed
    sequentially on a first-come, first-served basis. Priority scheduling allows jobs
    to be executed based on their priority levels, preempting low-priority jobs to
    make room for high-priority jobs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到公平共享调度如何帮助我们解决多个用户同时运行分布式训练作业时的问题。公平共享调度允许我们在每个抽象级别应用调度策略，例如进程、用户、组等。我们还讨论了优先级调度，它可以有效地消除当作业只能按先到先得的原则顺序执行时遇到的问题。优先级调度允许根据作业的优先级级别执行作业，抢占低优先级作业以腾出空间给高优先级作业。
- en: With priority scheduling, if a cluster is used by a large number of users, a
    malicious user could create jobs at the highest possible priority, causing other
    jobs to be evicted or not get scheduled at all. To deal with this potential problem,
    administrators of real-world clusters usually enforce certain rules and limits
    to prevent users from creating a huge number of jobs at high priorities.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用优先级调度时，如果集群被大量用户使用，恶意用户可能会创建具有最高可能优先级的作业，导致其他作业被驱逐或根本无法调度。为了处理这种潜在问题，现实世界中的集群管理员通常会强制执行某些规则和限制，以防止用户创建大量高优先级的作业。
- en: We also discussed gang scheduling, which ensures if two or more workers communicate
    with each other, they will all be ready to communicate at the same time. Gang
    scheduling is especially helpful for collective communication-based distributed
    model training jobs where all workers need to be ready to communicate the calculated
    gradients to avoid wasting computational resources.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了群组调度，它确保如果有两个或更多工作者相互通信，他们都将同时准备好进行通信。群组调度对于基于集体通信的分布式模型训练作业特别有帮助，在这些作业中，所有工作者都需要准备好通信计算梯度，以避免浪费计算资源。
- en: Some machine learning frameworks support elastic scheduling (see chapter 3),
    which allows distributed model training jobs to start with any number of workers
    available without waiting for all the requested workers to be ready. In this case,
    gang scheduling is not suitable because we would need to wait for all workers
    to be ready. Instead, we can begin making significant progress toward model training
    with elastic scheduling.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习框架支持弹性调度（见第3章），允许分布式模型训练作业在任何数量的工作者可用时启动，而无需等待所有请求的工作者都准备好。在这种情况下，群组调度不适用，因为我们需要等待所有工作者都准备好。相反，我们可以通过弹性调度开始取得模型训练的重大进展。
- en: Because the number of workers may change during model training, the batch size
    (sum of the size of mini-batches on each worker) will affect the model training
    accuracy. In that case, additional modifications to the model training strategy
    are needed. For example, we can support a customized learning rate scheduler that
    will account for epoch or batch or adjust the batch size dynamically based on
    the number of workers. Together with these algorithmic improvements, we can allocate
    and utilize existing computational resources more wisely and improve the user
    experience.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于工作进程的数量可能在模型训练期间发生变化，批大小（每个工作进程上迷你批次的尺寸之和）将影响模型训练的准确性。在这种情况下，需要对模型训练策略进行额外的修改。例如，我们可以支持一个定制的学习率调度器，该调度器将考虑纪元或批次，或根据工作进程的数量动态调整批大小。与这些算法改进相结合，我们可以更明智地分配和利用现有的计算资源，并提高用户体验。
- en: In practice, distributed model training jobs greatly benefit from scheduling
    patterns like gang scheduling. As a result, we can avoid wasting computational
    resources. However, one problem we might be neglecting is that any of these worker
    processes scheduled by gang scheduling may fail, leading to unexpected consequences.
    Often it’s hard to debug these types of failures. In the next section, I’ll introduce
    a pattern that will make debugging and handling failures easier.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，分布式模型训练作业极大地受益于像群调度这样的调度模式。因此，我们可以避免浪费计算资源。然而，我们可能忽视的一个问题是，任何由群调度安排的这些工作进程都可能失败，导致意外的后果。通常很难调试这类故障。在下一节中，我将介绍一种将使调试和处理故障更容易的模式。
- en: 6.2.4 Exercises
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 练习
- en: Can we only apply fair-share scheduling at the user level?
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们是否只能在用户级别应用公平共享调度？
- en: Is gang scheduling suitable for all distributed model training jobs?
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 群调度是否适合所有分布式模型训练作业？
- en: '6.3 Metadata pattern: Handle failures appropriately to minimize the negative
    effect on users'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 元数据模式：适当处理故障以最小化对用户的不利影响
- en: When building the most basic machine learning workflow that includes only data
    ingestion, model training, and model serving, where each component only appears
    once as an individual step in the workflow, everything seems pretty straightforward.
    Each step runs sequentially to reach completion. If any of these steps fail, we
    pick up where it’s left off. For example, imagine the model training step has
    failed to take the ingested data (e.g., lost the connection to the database where
    the ingested data is stored). We can retry the failed step and easily continue
    model training without rerunning the entire data ingestion process, as shown in
    figure 6.12.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建最基础的仅包含数据摄入、模型训练和模型服务的机器学习工作流程时，其中每个组件在流程中仅作为单个步骤出现一次，一切似乎都很直接。每个步骤按顺序运行以达到完成。如果这些步骤中的任何一个失败，我们就从它留下的地方继续。例如，想象一下模型训练步骤未能处理摄入的数据（例如，失去了存储摄入数据的数据库的连接）。我们可以重试失败的步骤，并如图6.12所示，轻松地继续模型训练而无需重新运行整个数据摄入过程。
- en: '![06-12](../../OEBPS/Images/06-12.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![06-12](../../OEBPS/Images/06-12.png)'
- en: Figure 6.12 A baseline workflow where the model training step has failed to
    take the ingested data. We retry the failed step and pick up from the failed step
    to continue model training without rerunning the entire data ingestion process.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 基线工作流程，其中模型训练步骤未能处理摄入的数据。我们重试失败的步骤，并从失败的步骤继续模型训练，而无需重新运行整个数据摄入过程。
- en: However, when the workflow gets more complicated, any failures are not trivial
    to handle. For example, consider the workflow from chapter 5\. This workflow trains
    models via three model training steps that arrive at different accuracies when
    tagging entities. Then, a model selection step picks the top two models with at
    least 90% accuracy trained from the first two model training steps, which will
    be used in the following two separate model serving steps. The results from the
    two model serving steps are then aggregated via a result aggregation step to present
    to users.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当工作流程变得更加复杂时，任何故障的处理都不再是微不足道的。例如，考虑第5章中的工作流程。该工作流程通过三个模型训练步骤来训练模型，这些步骤在标记实体时达到不同的准确性。然后，一个模型选择步骤从前两个模型训练步骤中至少训练出90%准确性的前两个模型中选择，这些模型将在接下来的两个单独的模型服务步骤中使用。然后，通过结果聚合步骤将这两个模型服务步骤的结果汇总，以呈现给用户。
- en: Now let’s consider the case where the second and the third model training steps
    have both failed during execution (e.g., some of the workers allocated for model
    training are preempted). These two model training steps would have provided both
    the most and the least accurate model if they had finished successfully, as shown
    in figure 6.13.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑第二种情况，即第二和第三模型训练步骤在执行过程中都失败了（例如，分配给模型训练的一些工作者被抢占）。如果这两个模型训练步骤成功完成，它们将提供最准确和最不准确的模型，如图6.13所示。
- en: '![06-13](../../OEBPS/Images/06-13.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![06-13](../../OEBPS/Images/06-13.png)'
- en: Figure 6.13 A machine learning workflow that trains models with different accuracies
    when tagging entities. The model selection step identifies the top two models
    with at least 90% accuracy to be used for model serving. The accuracies are crossed
    out in these two steps because the steps failed without arriving at the expected
    accuracies. The results from the two model serving steps are then aggregated to
    present to users.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 展示了一个机器学习工作流程，在标记实体时训练具有不同准确率的模型。模型选择步骤识别出至少90%准确率的两个顶级模型用于模型服务。这两个步骤中的准确率已被划掉，因为这些步骤在没有达到预期准确率的情况下失败了。然后，从两个模型服务步骤的结果中汇总以呈现给用户。
- en: At this point, one might think that we should rerun both steps to proceed to
    the model selection and model serving steps. However, in practice, since we already
    wasted some time training part of the models, we may not want to start everything
    from scratch. It would be much longer before our users can see the aggregated
    results from our best models. Is there a better way to handle such kinds of failures?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，有人可能会认为我们应该重新运行这两个步骤以继续进行模型选择和模型服务步骤。然而，在实践中，由于我们已经浪费了一些时间训练部分模型，我们可能不想从头开始。我们的用户需要更长的时间才能看到我们最佳模型的汇总结果。是否有更好的方法来处理这类失败？
- en: 6.3.1 The problem
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 问题
- en: For complicated machine learning workflows, such as the one we discussed in
    chapter 5, where we want to train multiple models and then select the top-performing
    models for model serving, the decision on which strategy to use to handle failures
    of certain steps due to real-world requirements is not always trivial. For example,
    when two out of three model training steps fail due to preempted workers, we don’t
    want to start training those models from scratch, which greatly increases the
    time needed to complete the workflow. How do we handle these failures appropriately
    so the negative effect on users can be minimized?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的机器学习工作流程，例如我们在第5章中讨论的，我们想要训练多个模型，然后选择表现最佳的模型进行模型服务，由于现实需求，处理某些步骤失败（例如，由于抢占式工作者）的策略决策并不总是简单的。例如，当三个模型训练步骤中有两个因抢占式工作者而失败时，我们不希望从头开始训练这些模型，这会大大增加完成工作流程所需的时间。我们如何适当地处理这些失败，以将对用户的影响降到最低？
- en: 6.3.2 The solution
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 解决方案
- en: Whenever we encounter a failure in a machine learning workflow, we should first
    understand the root cause (e.g., loss of network connections, lack of computational
    resources, etc). Knowing the root cause is important because we need to understand
    the nature of the failure to predict whether retrying the failed steps would help.
    If the failures are due to some long-lasting shortages that could very likely
    lead to repetitive failures when retrying, we could better utilize the computational
    resources to run some other tasks. Figure 6.14 illustrates the difference in the
    effectiveness of retrying for permanent and temporary failures. When we retry
    the model training step when encountering permanent failures, the retries are
    ineffective and lead to repetitive failures.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时我们在机器学习工作流程中遇到失败，我们首先应该了解根本原因（例如，网络连接丢失、计算资源不足等）。了解根本原因很重要，因为我们需要了解失败的性质，以预测重试失败的步骤是否会有帮助。如果失败是由于一些可能非常可能导致重试时重复失败的长期短缺，我们可以更好地利用计算资源来运行其他任务。图6.14说明了重试永久性失败和暂时性失败的有效性差异。当我们遇到永久性失败时重试模型训练步骤，重试是无效的，并导致重复失败。
- en: '![06-14](../../OEBPS/Images/06-14.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![06-14](../../OEBPS/Images/06-14.png)'
- en: Figure 6.14 The difference in the effectiveness of retrying for permanent and
    temporary failures
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 重试永久性失败和暂时性失败的有效性差异
- en: For example, in our case, we should first check whether the dependencies of
    a model training step are met, such as whether the ingested data from the previous
    step is still available. If the data has been persisted to a local disk to a database,
    we can proceed to model training. However, if the data was located in memory and
    lost when the model training step failed, we cannot start model training without
    ingesting the data again. Figure 6.15 shows the process of restarting the data
    ingestion step when there’s a permanent failure during model training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的案例中，我们首先应该检查模型训练步骤的依赖项是否满足，例如，之前步骤中摄入的数据是否仍然可用。如果数据已持久化到本地磁盘或数据库中，我们可以继续进行模型训练。然而，如果数据位于内存中并且在模型训练步骤失败时丢失，我们无法在不重新摄入数据的情况下开始模型训练。图6.15显示了在模型训练过程中出现永久性故障时重新启动数据摄入步骤的过程。
- en: '![06-15](../../OEBPS/Images/06-15.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![06-15](../../OEBPS/Images/06-15.png)'
- en: Figure 6.15 The process of restarting the data ingestion step when a permanent
    failure occurs during model training
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 在模型训练过程中发生永久性故障时重新启动数据摄入步骤的过程
- en: Similarly, if the model training step fails due to preempted training workers
    or out-of-memory problems, we need to make sure we still have sufficient computational
    resources allocated to rerun the model training step.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果模型训练步骤由于预占用的训练工作者或内存不足问题而失败，我们需要确保我们仍然有足够的计算资源分配来重新运行模型训练步骤。
- en: However, we won’t know what information to analyze to determine the root cause
    unless we intentionally record it as metadata during the runtime of each step
    in the entire machine learning workflow. For example, for each model training
    step, we can record metadata on the availability of the ingested data and whether
    different computational resources, such as memory and CPU usage, exceeded the
    limit before the step failed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除非我们故意在机器学习工作流程中每个步骤的运行时将其记录为元数据，否则我们不知道要分析哪些信息来确定根本原因。例如，对于每个模型训练步骤，我们可以在步骤失败之前记录有关摄入数据的可用性和不同计算资源（如内存和CPU使用）是否超过限制的元数据。
- en: Figure 6.16 is a workflow where the model training step failed. Metadata is
    collected every 5 minutes on memory usage (in megabytes) and the availability
    of the training data (yes/no) during the runtime of this step. We can notice a
    sudden huge memory spike from 23 MB to 200 MB after 30 minutes. In this case,
    we can retry this step with an increase in requested memory, and it would then
    successfully produce a trained model that will be used for the next model serving
    step.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16是一个模型训练步骤失败的工作流程。在此步骤的运行时，每5分钟收集一次内存使用情况（以兆字节为单位）和训练数据可用性（是/否）的元数据。我们可以注意到，在30分钟后，内存使用量从23
    MB突然增加到200 MB。在这种情况下，我们可以尝试增加请求的内存来重试此步骤，然后它将成功产生一个用于下一个模型服务步骤的训练模型。
- en: '![06-16](../../OEBPS/Images/06-16.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![06-16](../../OEBPS/Images/06-16.png)'
- en: Figure 6.16 An example workflow where the model training step failed, with the
    metadata collected showing an unexpected memory spike during runtime
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 模型训练步骤失败的一个示例工作流程，其中收集的元数据显示了运行时出现的意外内存峰值
- en: In practice, for complex workflows like in figure 6.13, even when we know all
    the dependencies of model training steps are met (e.g., we have enough computational
    resources and a good database connection to access the data source), we should
    also think about whether we want to handle the failures and how we’d like to handle
    them. We’ve spent a lot of time on the training steps already, but now, the steps
    have suddenly failed, and we’ve lost all the progress. In other words, we don’t
    want to start re-training all the models from scratch, which may add considerable
    time before we can deliver the aggregated results from our best models to users.
    Is there a better way to handle this without a huge effect on our user experience?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，对于如图6.13所示的复杂工作流程，即使我们知道模型训练步骤的所有依赖项都已满足（例如，我们有足够的计算资源和良好的数据库连接来访问数据源），我们也应该考虑我们是否想要处理这些失败以及我们希望如何处理它们。我们已经花费了大量时间在训练步骤上，但现在，步骤突然失败了，我们失去了所有的进度。换句话说，我们不希望从头开始重新训练所有模型，这可能会在我们能够将最佳模型的聚合结果交付给用户之前增加相当多的时间。有没有更好的方法来处理这个问题，而又不会对我们的用户体验产生巨大影响？
- en: In addition to the metadata we’ve recorded for each of the model training steps,
    we could save more useful metadata that can be used to figure out whether it’s
    worth rerunning all the model training steps. For example, the model accuracy
    over time indicates whether the model is being trained effectively.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为每个模型训练步骤记录的元数据外，我们还可以保存更多有用的元数据，这些元数据可以用来判断是否值得重新运行所有模型训练步骤。例如，模型准确率随时间的变化表明模型是否被有效地训练。
- en: Model accuracy that remains steady or even decreases (from 30% to 27%, as shown
    in figure 6.17) may indicate that the model already converges and continuing training
    would no longer improve model accuracy. In this example, even though two model
    training steps fail, it’s not necessary to retry the third model training step
    from scratch since it would lead to a model that converges fast but with low accuracy.
    Another example of metadata that can be potentially useful is the percentage of
    completed model training (e.g., if we’ve iterated through all the requested number
    of batches and epochs, the completion is 100%).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型准确率保持稳定甚至下降（如图6.17所示，从30%下降到27%）可能表明模型已经收敛，继续训练将不再提高模型准确率。在这个例子中，尽管有两个模型训练步骤失败，但不需要从头开始重试第三个模型训练步骤，因为这会导致一个快速收敛但准确率低的模型。另一个可能有用的元数据示例是模型训练完成的百分比（例如，如果我们已经迭代了所有请求的批次和epoch，完成度为100%）。
- en: '![06-17](../../OEBPS/Images/06-17.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![06-17](../../OEBPS/Images/06-17.png)'
- en: Figure 6.17 An example workflow where two model training steps fail and one
    has decreasing model accuracy
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 两个模型训练步骤失败且其中一个模型准确率下降的示例工作流程
- en: Once we have this additional metadata about model training steps, we can tell
    how well each started model training step progresses. For example, for the workflow
    in figure 6.18, we could potentially conclude ahead of time that the third model
    training step was progressing very slowly (only 1% of completion every 30 minutes)
    due to a smaller amount of allocated computational resources or more complex model
    architecture. We know that it’s highly likely that, given the limited time, we
    end up with a model with low accuracy. As a result, we can disregard this model
    training step in favor of allocating more computational resources to the other
    model training steps with more potential, which leads to more accurate models
    faster.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了关于模型训练步骤的额外元数据，我们就可以了解每个开始模型训练步骤的进展情况。例如，对于图6.18中的工作流程，我们可能提前得出结论，第三个模型训练步骤进展非常缓慢（每30分钟只完成1%），这是由于分配的计算资源较少或模型架构更复杂。我们知道，考虑到有限的时间，我们最终可能得到一个准确率很低的模型。因此，我们可以忽略这个模型训练步骤，转而将更多的计算资源分配给其他具有更大潜力的模型训练步骤，这有助于更快地得到更准确的模型。
- en: '![06-18](../../OEBPS/Images/06-18.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![06-18](../../OEBPS/Images/06-18.png)'
- en: Figure 6.18 An example workflow where two model training steps fail. One is
    disregarded because it is progressing very slowly, and the model will likely have
    low accuracy given the limited time.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 两个模型训练步骤失败的示例工作流程。其中一个被忽略，因为它进展非常缓慢，考虑到有限的时间，模型可能具有很低的准确率。
- en: Recording these metadata may help us derive more insights specific to each of
    the failed steps in the end-to-end machine learning workflow. We can then decide
    on a strategy to handle the failed steps appropriately to avoid wasting computational
    resources and minimize the effect on existing users. The metadata patterns provide
    great visibility into our machine learning pipelines. They can also be used to
    search, filter, and analyze the artifacts produced in each step in the future
    if we run a lot of pipelines on a regular basis. For example, we might want to
    know which models are performant or which datasets contribute the most to those
    models based on the historical training metrics.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 记录这些元数据可能有助于我们从端到端机器学习工作流程中的每个失败的步骤中得出更多特定的见解。然后我们可以决定采取适当的策略来处理失败的步骤，以避免浪费计算资源并最小化对现有用户的影响。元数据模式为我们提供了对机器学习管道的深入了解。如果我们在定期运行大量管道的情况下，它们还可以用于搜索、过滤和分析未来每个步骤中产生的工件。例如，我们可能想知道哪些模型表现良好，或者哪些数据集对那些模型贡献最大，基于历史训练指标。
- en: 6.3.3 Discussion
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 讨论
- en: With the help of the metadata pattern, we can gain additional insights into
    the individual steps in machine learning workflows. Then, if any fail, we can
    respond based on what’s beneficial to our users and thus reduce any negative effect
    due to the step failures.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作流程的各个步骤中，借助元数据模式，我们可以获得额外的见解。然后，如果任何步骤失败，我们可以根据对用户有益的方式做出响应，从而减少步骤失败带来的任何负面影响。
- en: One common type of metadata is the various network performance ([http://mng.bz/D4lR](http://mng.bz/D4lR))
    metrics while the model is being trained (e.g., bandwidth, throughput, latency).
    This type of information is very useful for detecting when certain workers experience
    poor network performance that blocks the entire training process. We can take
    down slow workers and start new workers to continue training, assuming the underlying
    machine learning frameworks support elastic scheduling and fault-tolerance (see
    chapter 3). For example, in figure 6.19, based on the metadata, the worker on
    the right-hand side has extremely high latency (10 times the latency of the other
    workers), which slows down the entire model training process. Ideally, this worker
    would be taken down and restarted.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的元数据类型是模型训练过程中的各种网络性能([http://mng.bz/D4lR](http://mng.bz/D4lR))指标（例如，带宽、吞吐量、延迟）。这类信息对于检测某些工作者遇到的网络性能不佳，从而阻塞整个训练过程非常有用。我们可以关闭速度较慢的工作者，并启动新的工作者以继续训练，前提是底层机器学习框架支持弹性调度和容错（参见第3章）。例如，在图6.19中，根据元数据，右侧的工作者具有极高的延迟（是其他工作者的10倍），这减缓了整个模型训练过程。理想情况下，这个工作者应该被关闭并重新启动。
- en: '![06-19](../../OEBPS/Images/06-19.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![06-19](../../OEBPS/Images/06-19.png)'
- en: Figure 6.19 An example parameter server-based model training where the worker
    on the right-hand side has extremely high latency (10 times the latency of the
    other workers), which slows down the entire model training process
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 一个基于参数服务器的模型训练示例，其中右侧的工作者具有极高的延迟（是其他工作者的10倍），这减缓了整个模型训练过程
- en: One additional benefit of introducing the metadata pattern to our machine learning
    workflows is to use the metadata recorded to establish relationships between the
    individual steps or across different workflows. For example, modern model management
    tools can use the recorded metadata to help users build the lineage of the trained
    models and visualize what individual steps/factors contributed to the model artifacts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将元数据模式引入我们的机器学习工作流程的另一个额外好处是，可以使用记录的元数据在各个步骤之间或不同工作流程之间建立关系。例如，现代模型管理工具可以使用记录的元数据帮助用户构建训练模型的谱系，并可视化哪些步骤/因素对模型工件做出了贡献。
- en: 6.3.4 Exercises
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 练习
- en: If the training step failed due to the loss of training data source, what should
    we do?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果训练步骤由于训练数据源丢失而失败，我们应该怎么办？
- en: What type of metadata can be collected if we look at individual workers or parameter
    servers?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们查看单个工作者或参数服务器，可以收集哪些类型的元数据？
- en: 6.4 Answers to exercises
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 练习答案
- en: Section 6.2
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第6.2节
- en: No, we can apply this scheduling strategy at each level of abstraction, such
    as processes, users, groups, etc.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，我们可以在抽象的每个级别应用这种调度策略，例如进程、用户、组等。
- en: No, some machine learning frameworks support elastic scheduling, which allows
    distributed model training jobs to start with any number of workers available
    without waiting for all the requested workers to be ready for communication. In
    this case, gang scheduling is not suitable.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，一些机器学习框架支持弹性调度，允许分布式模型训练作业在任何数量的工作者可用时启动，无需等待所有请求的工作者都准备好进行通信。在这种情况下，群组调度不适用。
- en: Section 6.3
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第6.3节
- en: We should rerun data ingestion before retrying the model training step since
    this failure is permanent, and simply retrying would lead to repetitive failures.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在重试模型训练步骤之前，我们应该重新运行数据摄取，因为这种失败是永久的，简单地重试会导致重复失败。
- en: Various network performance metrics while the model is being trained (e.g.,
    bandwidth, throughput, and latency). This type of information is very useful when
    we want to detect when workers experience poor network performance that blocks
    the entire training process.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练过程中的各种网络性能指标（例如，带宽、吞吐量和延迟）。当我们想要检测工作者遇到的网络性能不佳，从而阻塞整个训练过程时，这类信息非常有用。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: There are different areas of improvement related to operations in machine learning
    systems, such as job scheduling and metadata.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与机器学习系统中的操作相关，有多个改进领域，例如作业调度和元数据。
- en: Various scheduling patterns, such as fair-share scheduling, priority scheduling,
    and gang scheduling, can be used to prevent resource starvation and avoid deadlocks.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种调度模式，如公平共享调度、优先级调度和成组调度，可以用来防止资源饥饿和避免死锁。
- en: We can collect metadata to gain insights from machine learning workflows and
    handle failures more appropriately to reduce any negative effects on users.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以收集元数据，从机器学习工作流程中获得洞察力，并更恰当地处理故障，以减少对用户产生的任何负面影响。
