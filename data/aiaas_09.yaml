- en: 7 Applying AI to existing platforms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 将AI应用于现有平台
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Integration patterns for serverless AI
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无服务器AI的集成模式
- en: Improving identity verification with Textract
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Textract改进身份验证
- en: An AI-enabled data processing pipeline with Kinesis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Kinesis的AI赋能数据处理管道
- en: On-the-fly translation with Translate
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即时翻译使用Translate
- en: Sentiment analysis with Comprehend
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Comprehend进行情感分析
- en: Training a custom document classifier with Comprehend
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Comprehend训练自定义文档分类器
- en: In chapters 2-5 we created systems from scratch and applied AI services from
    the start. Of course, the real world is not always this clean and simple. Almost
    all of us have to deal with legacy systems and technical debt. In this chapter
    we are going to examine some strategies for applying AI services to existing systems.
    We will start by looking at some architectural patterns for this, and from there
    we will develop some specific examples drawn from real world experience.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2-5章中，我们从零开始创建系统，并从一开始就应用AI服务。当然，现实世界并不总是这么干净和简单。我们几乎所有人都要处理遗留系统和技术债务。在本章中，我们将探讨将AI服务应用于现有系统的一些策略。我们将首先查看一些适用于此的架构模式，然后从那里开发一些从实际经验中汲取的具体示例。
- en: 7.1 Integration patterns for serverless AI
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 无服务器AI的集成模式
- en: There is no escaping the fact that real world enterprise computing is “messy.”
    For a medium to large enterprise, the technology estate is typically large, sprawling,
    and has often grown organically over time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不可避免的事实是，现实世界的企业计算是“混乱”的。对于中型到大型企业，技术资产通常是庞大的、分散的，并且随着时间的推移通常是有机增长的。
- en: An organization’s compute infrastructure can be broken down along domain lines
    such as finance, HR, marketing, line-of-business systems, and so on. Each of these
    domains may be composed of many systems from various vendors, along with home-grown
    software, and will usually mix legacy with more modern Software as a Service (SaaS)
    delivered applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个组织的计算基础设施可以根据财务、人力资源、市场营销、业务线系统等域线进行分解。这些域中的每一个都可能由来自不同供应商的许多系统组成，包括自建软件，并且通常会将遗留系统与现代软件即服务（SaaS）交付的应用程序混合在一起。
- en: Concomitant to this, the various systems are typically operated in a hybrid
    model mixing on-premise, co-location, and cloud-based deployment. Furthermore,
    each of these operational elements must typically integrate with other systems
    both in and outside of the domain. These integrations can be by way of batch ETL
    jobs, point-to-point connections, or through some form of enterprise service bus
    (ESB)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，各种系统通常以混合模式运行，混合本地、共址和基于云的部署。此外，这些操作元素通常必须与域内和域外的其他系统进行集成。这些集成可以通过批量ETL作业、点对点连接或通过某种形式的企业服务总线（ESB）来实现
- en: ETL, point-to-point, and ESB
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ETL、点对点和ESB
- en: Enterprise system integration is a large topic which we won’t cover here, except
    to note that there are a number of ways in which systems can be connected together.
    For example, a company may need to export records from its HR database to match
    up with an expense tracking system. *Extract, transform, and load (ETL)* refers
    to the process of exporting records from one database (typically in CSV format),
    transforming, and then loading into another database.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 企业系统集成是一个大主题，我们在这里不会涉及，只是指出，有几种方式可以将系统连接在一起。例如，一家公司可能需要从其人力资源数据库中导出记录以与费用跟踪系统匹配。*提取、转换和加载（ETL）*指的是从数据库（通常为CSV格式）导出记录、转换，然后加载到另一个数据库的过程。
- en: Another method of connecting systems is to use a point-to-point integration.
    For example, some code can be created to call the API of one system and push data
    to another system’s API. This does, of course, depend on the provision of a suitable
    API. Over time, the use of ETL and point-to-point integration can accumulate into
    a very complex and difficult-to-manage system.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 连接系统的另一种方法是使用点对点集成。例如，可以创建一些代码来调用一个系统的API并将数据推送到另一个系统的API。当然，这取决于提供合适的API。随着时间的推移，ETL和点对点集成的使用可能会积累成一个非常复杂且难以管理的系统。
- en: An *enterprise service bus (ESB )* is an attempt to manage this complexity by
    providing a central system over which these connections can take place. The ESB
    approach suffers from its own particular pathologies, and has often caused as
    many problems as it has solved.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*企业服务总线（ESB）*试图通过提供一个中心系统来管理这种复杂性，这些连接可以在该系统中进行。ESB方法有其自身的特定病理，并且常常造成的问题与解决的问题一样多。'
- en: Figure 7.1 illustrates a typical mid-size organization’s technology estate.
    In this example, separate domains are connected together through a central bus.
    Within each domain, there are separate ETL and batch processes connecting systems
    together.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1说明了典型中型组织的技术地产。在这个例子中，通过一个中央总线将不同的领域连接在一起。在每一个领域内，都有独立的ETL和批量处理过程将系统连接起来。
- en: Needless to say, a description of all of this complexity is outside the scope
    of this book. The question we will concern ourselves with is how we can adopt
    and leverage serverless AI in this environment. Fortunately there are some simple
    patterns that we can follow to achieve our goals, but first let’s simplify the
    problem.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不言而喻，对所有这种复杂性的描述超出了本书的范围。我们将关注的问题是，我们如何在这个环境中采用和利用无服务器AI。幸运的是，有一些简单的模式我们可以遵循来实现我们的目标，但首先让我们简化这个问题。
- en: '![](../Images/CH07_F01_Elger.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F01_Elger.png)'
- en: Figure 7.1 Typical enterprise technology estate, broken down by logical domain.
    This image is intended to illustrate the complex nature of a typical technology
    estate. The detail of the architecture is not important.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 典型企业技术地产，按逻辑领域分解。此图像旨在说明典型技术地产的复杂性质。架构的细节并不重要。
- en: We will use figure 7.2 to represent our “enterprise estate” in the following
    discussion, to allow us to treat the rest of our infrastructure as a black box.
    In the next section we will examine four common patterns for connecting AI services.
    We will then build some concrete examples to show how AI services can be used
    to augment or replace existing business flows within an enterprise.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的讨论中，我们将使用图7.2来表示我们的“企业地产”，以便我们将其余的基础设施视为一个黑盒。在下一节中，我们将检查四种常见的连接AI服务模式。然后，我们将构建一些具体的例子来展示如何使用AI服务在企业内部增强或替换现有的业务流程。
- en: For example, if part of a company’s business workflow requires proof of identity
    via a utility bill or passport, that can be provided as an AI-enabled service,
    reducing the manual workload.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一家公司的业务流程需要通过水电费账单或护照证明身份，这可以作为一个AI赋能的服务提供，从而减少人工工作量。
- en: Another example is that of forecasting. Many organizations need to plan ahead
    to predict their required levels of inventory or staff over a given period of
    time. AI services could be integrated into this process to build more sophisticated
    and accurate models, saving the company money or opportunity costs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是预测。许多组织需要提前规划，以预测在特定时间段内所需的库存或员工水平。AI服务可以集成到这一过程中，以构建更复杂和准确的模型，为公司节省金钱或机会成本。
- en: '![](../Images/CH07_F02_Elger.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F02_Elger.png)'
- en: Figure 7.2 Simplified enterprise representation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 简化的企业表示
- en: 'We will examine four approaches:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查四种方法：
- en: Synchronous API
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步API
- en: Asynchronous API
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步API
- en: VPN Stream In
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VPN流入
- en: VPN Fully connected streaming
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VPN 全连接流
- en: Bear in mind that these approaches simply represent ways of getting the appropriate
    data into the required location to enable us to execute AI services to achieve
    a business goal.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些方法仅仅代表将适当的数据放入所需位置以使我们能够执行AI服务以实现业务目标的方式。
- en: '7.1.1 Pattern 1: Synchronous API'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 模式1：同步API
- en: The first and simplest approach is to create a small system, much like we did
    in the first few chapters, in isolation from the rest of the enterprise. Functionality
    is exposed through a secured API and accessed over the public internet. If a higher
    lever of security is required, a VPN connection can be established, through which
    the API can be called. This simple pattern is illustrated in figure 7.3.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种也是最简单的方法是创建一个小的系统，就像我们在前几章中所做的那样，与企业其他部分隔离。功能通过安全的API暴露，并通过公共互联网访问。如果需要更高层次的安全性，可以建立VPN连接，通过该连接调用API。这种简单的模式在图7.3中得到了说明。
- en: '![](../Images/CH07_F03_Elger.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F03_Elger.png)'
- en: 'Figure 7.3 Integration Pattern 1: Synchronous API'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 集成模式1：同步API
- en: In order to consume the service, a small piece of bridging code must be created
    to call the API and consume the results of the service. This pattern is appropriate
    when results can be obtained quickly and the API is called in a request/response
    manner.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用该服务，必须创建一小段桥接代码来调用API并消费服务的输出。当结果可以快速获得，并且API以请求/响应方式调用时，此模式是合适的。
- en: '7.1.2 Pattern 2: Asynchronous API'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 模式2：异步API
- en: Our second pattern is very similar, in that we expose functionality through
    an API; however, in this case the API acts asynchronously. This pattern, which
    is appropriate for longer-running AI services, is illustrated in figure 7.4.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个模式非常相似，即我们通过API公开功能；然而，在这种情况下，API是异步的。这种模式适用于运行时间较长的AI服务，如图7.4所示。
- en: '![](../Images/CH07_F04_Elger.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F04_Elger.png)'
- en: 'Figure 7.4 Integration Pattern 2: Asynchronous API'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 集成模式2：异步API
- en: 'Under this “fire and forget” model, the bridge code calls the API but does
    not receive results immediately, apart from status information. An example of
    this might be a document classification system that processes a large volume of
    text. The outputs of the system can potentially be consumed by the wider enterprise
    in a number of ways:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个“点火后即忘”模型下，桥梁代码调用API但不立即接收结果，除了状态信息。一个例子可能是一个处理大量文本的文档分类系统。该系统的输出可以通过多种方式被更广泛的企业所消费：
- en: By constructing a web application that users can interact with to see results
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过构建一个用户可以与之交互以查看结果的Web应用程序
- en: By the system messaging the results through email or other channels
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过系统通过电子邮件或其他渠道发送结果消息
- en: By the system calling an external API to forward details of any analysis
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过系统调用外部API以转发任何分析详情
- en: By the bridge code polling the API for results
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过桥梁代码轮询API以获取结果
- en: '7.1.3 Pattern 3: VPN Stream In'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 模式3：VPN流入
- en: A third approach is to connect the estate to cloud services through a VPN. Once
    a secure connection is established, the bridge code can interact more directly
    with cloud services. For example, rather than using an API Gateway to access the
    system, the bridge code could stream data directly into a Kinesis pipeline.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是通过VPN将企业连接到云服务。一旦建立了安全连接，桥梁代码可以更直接地与云服务交互。例如，而不是使用API网关来访问系统，桥梁代码可以直接将数据流入Kinesis管道。
- en: 'Results can be accessed in a number of ways: though an API, via outbound messaging,
    through a web GUI, or via an output stream. This is illustrated in figure 7.5.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以通过多种方式访问：通过API、通过出站消息、通过Web GUI或通过输出流。如图7.5所示。
- en: '![](../Images/CH07_F05_Elger.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F05_Elger.png)'
- en: 'Figure 7.5 Integration Pattern 3: Stream In'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 集成模式3：流入
- en: VPN
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: VPN
- en: A *virtual private network (VPN)* can be used to provide a secure network connection
    between devices or networks. VPNs typically use the IPSec protocol suite to provide
    authentication, authorization, and secure encrypted communications. Using IPSec
    lets you use insecure protocols, such as those used for file sharing between remote
    nodes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*虚拟专用网络（VPN）*可以用来在设备或网络之间提供安全的网络连接。VPN通常使用IPSec协议套件来提供身份验证、授权和安全的加密通信。使用IPSec可以让您使用不安全的协议，例如用于远程节点之间文件共享的协议。'
- en: A VPN can be used to provide secure access into a corporate network for remote
    workers, or to securely connect a corporate network into the cloud. Though there
    are a number of ways to set up and configure a VPN, we would recommend a serverless
    approach using the AWS VPN service.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: VPN可以用来为远程工作者提供对企业的安全访问，或者将企业网络安全地连接到云中。尽管有几种设置和配置VPN的方法，但我们建议使用AWS VPN服务来实现无服务器方法。
- en: '7.1.4 Pattern 4 VPN: Fully connected streaming'
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 模式4 VPN：完全连接的流式传输
- en: Our final pattern involves a much deeper connection between the estate and cloud
    AI services. Under this model, we establish a VPN connection as before, and use
    it to stream data in both directions. Though there are several streaming technologies
    available, we have had good results using Apache Kafka. This is illustrated in
    figure 7.6
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一个模式涉及企业与云AI服务之间更深层次的连接。在这个模型下，我们像以前一样建立VPN连接，并使用它来双向流数据。尽管有几种流式传输技术可用，但我们使用Apache
    Kafka取得了良好的效果。如图7.6所示
- en: '![](../Images/CH07_F06_Elger.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F06_Elger.png)'
- en: 'Figure 7.6 Integration Pattern 4: Full Streaming'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 集成模式4：全流式传输
- en: This approach involves operating a Kafka cluster on both ends of the VPN, and
    replicating data between the clusters. Within the cloud environment, services
    consume data by pulling from the appropriate Kafka topics and place results back
    onto a different topic for consumption by the wider enterprise.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法涉及在VPN两端运行Kafka集群，并在集群之间复制数据。在云环境中，服务通过从适当的Kafka主题中拉取数据来消费数据，并将结果放回不同的主题，以便更广泛的企业消费。
- en: Kafka
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka
- en: Apache Kafka is an open source, distributed streaming platform. Kafka was originally
    developed at LinkedIn, and later donated to the Apache Foundation. Though there
    are other streaming technologies available, Kafka’s design is unique in that it
    is implemented as a distributed commit log.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个开源的分布式流式平台。Kafka最初是在LinkedIn开发的，后来捐赠给了Apache基金会。尽管有其他流式技术可用，但Kafka的设计独特之处在于它被实现为一个分布式提交日志。
- en: Kafka is increasingly being adopted in high-throughput data streaming scenarios
    by companies such as Netflix and Uber. It is, of course, possible to install,
    run, and manage your own Kafka cluster; however, we would recommend that you take
    the serverless approach and adopt a system such as AWS Managed Streaming for Kafka
    (MSK).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka越来越多地被Netflix和Uber等公司用于高吞吐量数据流场景。当然，您可以安装、运行和管理自己的Kafka集群；然而，我们建议您采用无服务器方法，并采用AWS
    Managed Streaming for Kafka (MSK)等系统。
- en: A full discussion of the merits of this approach and Kafka in general is outside
    the scope of this book. If you are not familiar with Kafka, we recommend that
    you take a look at the Manning book *Kafka in Action* by Dylan Scott to get up
    to speed.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对这种方法以及Kafka的一般优点的全面讨论超出了本书的范围。如果您不熟悉Kafka，我们建议您阅读Dylan Scott所著的Manning出版社的《Kafka
    in Action》一书，以了解相关知识。
- en: 7.1.5 Which pattern?
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.5 哪种模式？
- en: As with all architectural decisions, which approach to take really depends on
    the use case. Our guiding principle is to keep things as simple as possible. If
    a simple API integration will achieve your goal, then go with that. If over time
    the external API set begins to grow, then consider changing the integration model
    to a streaming solution to avoid the proliferation of APIs. The key point is to
    keep the integration to AI services under constant review and be prepared to refactor
    as needs dictate.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有架构决策一样，采取哪种方法实际上取决于用例。我们的指导原则是尽可能保持简单。如果简单的API集成可以实现您的目标，那么就选择它。如果随着时间的推移，外部API集开始增长，那么考虑将集成模型改为流式解决方案，以避免API的过度扩散。关键点是持续审查AI服务的集成，并准备好根据需要重构。
- en: Table 7.1 summarizes the context and when each pattern should be applied.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1总结了上下文以及何时应用每种模式。
- en: Table 7.1 Applicability of AI as Service legacy integration patterns
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 AI as Service传统集成模式适用性
- en: '| Pattern | Context | Example |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 上下文 | 示例 |'
- en: '| 1: Synchronous API | Single service, fast response | Text extraction from
    document |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 1: 同步API | 单个服务，快速响应 | 从文档中提取文本 |'
- en: '| 2: Asynchronous API | Single service, longer running | Document transcription
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 2: 异步API | 单个服务，运行时间更长 | 文档转录 |'
- en: '| 3: VPN Stream In | Multiple services, results for human consumption | Sentiment
    analysis pipeline |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 3: VPN流入 | 多个服务，人类消费的结果 | 情感分析管道 |'
- en: '| 4: VPN Fully Connected | Multiple services, results for machine consumption
    | Batch translation of documents |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 4: VPN完全连接 | 多个服务，机器消费的结果 | 文档批处理翻译 |'
- en: 'In this chapter we will build two example systems:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建两个示例系统：
- en: 'Pattern 1: Synchronous API approach'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式1：同步API方法
- en: 'Pattern 2: Asynchronous API'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模式2：异步API
- en: Though we won’t look at the streaming approach in detail, bear in mind that
    both of our example systems could also be connected to an enterprise through this
    method by replacing the API layer with an appropriate technology such as Apache
    Kafka.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会详细探讨流式处理方法，但请记住，我们的两个示例系统也可以通过这种方法与企业连接，只需用适当的技术（如Apache Kafka）替换API层即可。
- en: 7.2 Improving identity verification with Textract
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 使用Textract改进身份验证
- en: For our first example, we will extend an existing platform by creating a small,
    self-contained API that can be called directly. Let’s imagine that an organization
    needs to validate identities. Most of us have had to go through this process at
    one time or another; for example, when applying for a mortgage or car loan.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个示例，我们将通过创建一个小的、自包含的API来扩展现有平台，该API可以直接调用。让我们想象一个组织需要验证身份。我们中的大多数人都在某个时候经历过这个过程；例如，在申请抵押贷款或汽车贷款时。
- en: This typically requires a scan of several pieces of documentation in order to
    prove your identity and address to the lender. Though a human will need to see
    these scans, extracting the information from them and manually entering the information
    into the lender’s system is time-consuming and error-prone. This is something
    that we can now achieve through the application of AI.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常需要扫描几份文件以证明您的身份和地址给贷款人。尽管需要有人查看这些扫描件，但从这些扫描件中提取信息并将信息手动输入到贷款人的系统中既耗时又容易出错。这是我们现在可以通过应用
    AI 来实现的事情。
- en: Our small, self-contained service is illustrated in figure 7.7\. It uses AWS
    Textract to grab the details from a scanned-in document. For this example, we
    will be using a passport, but other identifying documents would work just as well,
    such as utility bills or bank statements.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的小型、自包含的服务如图 7.7 所示。它使用 AWS Textract 从扫描的文档中提取详细信息。在这个例子中，我们将使用护照，但其他身份证明文件也可以同样工作，例如水电费账单或银行对账单。
- en: '![](../Images/CH07_F07_Elger.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F07_Elger.png)'
- en: Figure 7.7 Document recognition API
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 文档识别 API
- en: Our API has two parts. First we will need to upload a scanned image to S3\.
    The simplest way to do this is by using a pre-signed S3 URL, and our API provides
    a function to generate one of these and return it to the client. Once we have
    our image in S3, we will use a Lambda function to call Textract, which will analyze
    our scanned-in image, returning the data in a text format. Our API will return
    this data to the client for further processing.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 API 有两部分。首先，我们需要将扫描的图像上传到 S3。最简单的方法是使用预签名的 S3 URL，我们的 API 提供了一个生成此类 URL
    并将其返回给客户端的功能。一旦我们的图像在 S3 中，我们将使用 Lambda 函数调用 Textract，它将分析扫描的图像，并以文本格式返回数据。我们的
    API 将将这些数据返回给客户端以进行进一步处理。
- en: Personally identifiable information
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 个人可识别信息
- en: It goes without saying that any personally identifiable information must be
    handled with the utmost care. Whenever a system must deal with user-supplied information,
    particularly identification documents, it must comply with all of the statutory
    legal requirements for the territory in which the information is gathered.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 任何个人可识别信息都必须非常小心地处理。每当系统必须处理用户提供的个人信息时，尤其是身份证明文件时，它必须遵守信息收集地的所有法定法律要求。
- en: In the European Union, this means that the system must comply with the General
    Data Protection Regulation (GDPR). As developers and system architects, we need
    to be aware of these regulations and ensure compliance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在欧盟，这意味着系统必须遵守通用数据保护条例（GDPR）。作为开发人员和系统架构师，我们需要了解这些法规并确保合规。
- en: 7.2.1 Get the code
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 获取代码
- en: 'The code for the API is in the directory `chapter7/text-analysis`. This contains
    two directories: `text-analysis-api`, which has the code for our API service,
    and `client`, which has some code to exercise the API. We will walk through the
    system before deploying and testing with some sample data.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: API 的代码位于 `chapter7/text-analysis` 目录中。这包含两个目录：`text-analysis-api`，其中包含我们的 API
    服务的代码，以及 `client`，其中包含一些用于测试 API 的代码。在部署和测试之前，我们将通过一些示例数据来了解整个系统。
- en: 7.2.2 Text Analysis API
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 文本分析 API
- en: 'Our API codebase consists of a `serverless.yml` configuration file, `package.json`
    for our node module dependencies, and `handler.js` containing the logic for the
    API. The `serverless.yml` is fairly standard, defining two Lambda functions: `upload`
    and `analyze`, which are accessible through API Gateway. It also defines an S3
    bucket for the API, and sets up IAM permissions for the analysis service, as shown
    in the following listing.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 API 代码库包括一个 `serverless.yml` 配置文件，`package.json` 用于我们的节点模块依赖项，以及包含 API 逻辑的
    `handler.js`。`serverless.yml` 是相当标准的，定义了两个 Lambda 函数：`upload` 和 `analyze`，它们可以通过
    API Gateway 访问。它还定义了一个用于 API 的 S3 桶，并设置了分析服务的 IAM 权限，如下所示。
- en: Listing 7.1 Textract permissions
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 Textract 权限
- en: '[PRE0]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Enable bucket access to Lambda
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 启用桶对 Lambda 的访问
- en: ❷ Enable Textract permissions
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 启用 Textract 权限
- en: Bucket permissions are required for our Lambda functions to generate a valid
    pre-signed URL, in addition to enabling access for Textract to the uploaded documents.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为 Textract 允许访问上传的文档外，还需要桶权限以使我们的 Lambda 函数生成有效的预签名 URL。
- en: The next listing shows the call to the `S3` API to generate a pre-signed URL.
    The URL along with the bucket key is returned to the client, which performs a
    `PUT` request to upload the document in question.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了调用 `S3` API 生成预签名 URL 的调用。URL 以及桶密钥被返回给客户端，客户端执行 `PUT` 请求上传相关的文档。
- en: Listing 7.2 Get signed URL
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 获取预签名 URL
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Set expiry time of five minutes
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置五分钟的过期时间
- en: The pre-signed URL is restricted to a specific action for that given key and
    file only--in this case, a `PUT` request. Note also that we have set an expiry
    time of 300 seconds on the URL. This means that if the file transfer is not initiated
    within five minutes, the signed URL will become invalid, and no transfer will
    be possible without generating a fresh URL.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 预签名的 URL 仅限于为该特定键和文件指定的特定操作——在本例中为 `PUT` 请求。注意，我们还在 URL 上设置了 300 秒的过期时间。这意味着如果在五分钟内未启动文件传输，预签名
    URL 将失效，并且没有生成新的 URL 的情况下无法进行传输。
- en: Once the document is uploaded to the bucket, we can initiate the call to Textract
    to perform the analysis. The next listing shows how this is done in `handler.js`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文档上传到存储桶，我们就可以调用 Textract 来执行分析。下一列表显示了这是如何在 `handler.js` 中完成的。
- en: Listing 7.3 Calling Textract
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 调用 Textract
- en: '[PRE2]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Point to uploaded document
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指向上传的文档
- en: ❷ Set feature types
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置特征类型
- en: ❸ Call Textract
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 调用 Textract
- en: Textract can perform two types of analysis, `TABLES` and `FORMS`. The `TABLES`
    analysis type tells Textract to preserve tabular information in its analysis,
    whereas the `FORMS` type requests that Textract extract information as key-value
    pairs if possible. Both analysis types can be preformed in the same call if required.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Textract 可以执行两种类型的分析，`TABLES` 和 `FORMS`。`TABLES` 分析类型指示 Textract 在其分析中保留表格信息，而
    `FORMS` 类型则请求 Textract 在可能的情况下将信息提取为键值对。如果需要，这两种分析类型可以在同一调用中执行。
- en: On completion of the analysis, Textract returns a block of JSON containing the
    results. The result structure is illustrated in figure 7.8.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 分析完成后，Textract 返回一个包含结果的 JSON 块。结果结构如图 7.8 所示。
- en: '![](../Images/CH07_F08_Elger.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F08_Elger.png)'
- en: Figure 7.8 Textract output JSON
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 Textract 输出 JSON
- en: 'The structure should be fairly self-explanatory, in that it consists of a root
    `PAGE` element that links to child `LINE` elements, each of which links to a number
    of child `WORD` elements. Each `WORD` and `LINE` element has an associated confidence
    interval: a number between 0 and 100 indicating how accurate Textract thinks the
    analysis was for each element. Each `LINE` and `WORD` element also has a `Geometry`
    section; this contains coordinate information on the bounding box around the element.
    This can be useful for applications where some additional human verification is
    required. For example, a UI could display the scanned-in documents with an overlaid
    bounding box in order to confirm that the extracted text matches the expected
    document area.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结构应该是相当直观的，因为它由一个根 `PAGE` 元素组成，该元素链接到子 `LINE` 元素，每个子 `LINE` 元素又链接到多个子 `WORD`
    元素。每个 `WORD` 和 `LINE` 元素都有一个相关的置信区间：一个介于 0 和 100 之间的数字，表示 Textract 认为每个元素的分析有多准确。每个
    `LINE` 和 `WORD` 元素还有一个 `Geometry` 部分；这包含围绕元素边界框的坐标信息。这可以用于需要一些额外人工验证的应用程序。例如，一个
    UI 可以显示带有叠加边界框的扫描文档，以确认提取的文本与预期的文档区域相匹配。
- en: 7.2.3 Client code
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 客户端代码
- en: 'Code to exercise the API is in the `client` directory. The main API calling
    code is in `client.js`. There are three functions: `getSignedUrl`, `uploadImage`,
    and `analyze`. These functions map one-to-one with the API, as already described.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 API 的代码位于 `client` 目录中。主要的 API 调用代码在 `client.js` 中。有三个函数：`getSignedUrl`、`uploadImage`
    和 `analyze`。这些函数与 API 的一对一映射，如前所述。
- en: The following listing shows the `analyze` function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了 `analyze` 函数。
- en: Listing 7.4 Calling the API
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 调用 API
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Make POST request to the API
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向 API 发送 POST 请求
- en: ❷ Return results
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回结果
- en: The code uses the `request` module to execute a `POST` request to the `analyze`
    API, which returns the Textract results block to the client.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用 `request` 模块向 `analyze` API 发送 `POST` 请求，该请求将 Textract 的结果块返回给客户端。
- en: 7.2.4 Deploy the API
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 部署 API
- en: Before we deploy the API, we need to configure some environment variables. Both
    the API and the client read their configuration from an `.env` file in the `chapter7/text-analysis`
    directory. Open up your favorite editor and create this file with the contents,
    as shown in the next listing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署 API 之前，我们需要配置一些环境变量。API 和客户端都从 `chapter7/text-analysis` 目录中的 `.env` 文件中读取其配置。打开您最喜欢的编辑器并创建此文件，内容如下一列表所示。
- en: Listing 7.5 `.env` file for Textract example
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.5 Textract 示例的 `.env` 文件
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Replace `<your bucket name>` with a globally unique bucket name of your choice.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `<your bucket name>` 替换为您选择的全球唯一存储桶名称。
- en: To deploy the API, we need to use the Serverless Framework as before. Open a
    command shell, `cd` to the `chapter7/text-analysis/text-analysis-api` directory,
    and run
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了部署API，我们需要像之前一样使用Serverless Framework。打开命令行，`cd`到`chapter7/text-analysis/text-analysis-api`目录，并运行
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will create the document image bucket, set up API Gateway, and deploy our
    two Lambda functions. Once deployed, Serverless will output the Gateway URLs to
    our two functions, which will look similar to the output illustrated in the next
    listing.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建文档图像存储桶，设置API网关，并部署我们的两个Lambda函数。一旦部署，Serverless将输出两个函数的网关URL，这些URL将类似于下一条列表中所示。
- en: Listing 7.6 Endpoint URLs
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 端点URL
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Upload URL
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 上传URL
- en: ❷ Analyze URL
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分析URL
- en: We will use these URLs to call our text analysis API.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些URL来调用我们的文本分析API。
- en: 7.2.5 Test the API
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 测试API
- en: Now that we have deployed our API, it’s time to test it with some real data.
    The service that we have just deployed is able to read and identify text fields
    in documents such as utility bills or passports. We have provided some sample
    passport images in the `data` sub directory, one of which is shown in figure 7.9\.
    These are, of course, composed from dummy data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经部署了我们的API，是时候用一些真实数据来测试它了。我们刚刚部署的服务能够读取和识别文档中的文本字段，例如账单或护照。我们在`data`子目录中提供了一些示例护照图像，其中之一如图7.9所示。当然，这些是由模拟数据组成的。
- en: '![](../Images/CH07_F09_Elger.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F09_Elger.png)'
- en: Figure 7.9 Sample passport
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 示例护照
- en: To test the API, we first need to update our `.env` file. Open the file in a
    text editor and add the two URLs and bucket name, as shown in the next listing,
    using your specific names.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试API，我们首先需要更新我们的`.env`文件。在文本编辑器中打开文件，并添加两个URL和存储桶名称，如下所示，使用您特定的名称。
- en: Listing 7.7 Environment file
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 环境文件
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Replace with the analyze URL
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 替换为分析URL
- en: ❷ Replace with the upload URL
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 替换为上传URL
- en: Next, `cd` into the `chapter7/text-analysis/client` directory. There are some
    sample images in the `data` sub directory. Code to exercise the client is in `index.js`.
    To run the code, open a command shell and execute
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`cd`到`chapter7/text-analysis/client`目录。在`data`子目录中有一些示例图像。在`index.js`中有练习客户端的代码。要运行代码，打开命令行并执行
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The client code will use the API to upload an example document to the image
    bucket, and then call our `analyze` API. The `analyze` API will call Textract
    to analyze the image and return the results back to our client. Finally, the client
    code will parse through the output JSON structure and pick out a few key fields,
    displaying them to the console. You should see output similar to the following
    listing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端代码将使用API将示例文档上传到图像存储桶，然后调用我们的`analyze` API。`analyze` API将调用Textract来分析图像并将结果返回给我们的客户端。最后，客户端代码将解析输出JSON结构并挑选出几个关键字段，将它们显示在控制台上。你应该看到类似以下列表的输出。
- en: Listing 7.8 Client output
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.8 客户端输出
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is important to note that Textract is applying multiple techniques in order
    to extract this information for us. First it performs an *optical character recognition
    (OCR)* analysis to recognize the text in the image. As part of this analysis,
    it retains the coordinate information for the recognized characters, grouping
    them into blocks and lines. It then uses the coordinate information to associate
    form fields as name-value pairs.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，Textract正在应用多种技术来为我们提取这些信息。首先，它执行一个*光学字符识别（OCR）*分析来识别图像中的文本。作为分析的一部分，它保留了识别字符的坐标信息，将它们分组到块和行中。然后，它使用坐标信息将表单字段关联为名称-值对。
- en: 'To be accurate, we need to supply Textract with good-quality images: the better
    the quality, the better the result we will get from the analysis. You can test
    this by creating or downloading your own low-quality images and passing these
    to the API. You should find that Textract will struggle to identify the same fields
    in a low-quality image.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确起见，我们需要向Textract提供高质量的图像：图像质量越好，分析结果越好。你可以通过创建或下载自己的低质量图像并将这些图像传递给API来测试这一点。你应该会发现Textract在低质量图像中难以识别相同的字段。
- en: Listing 7.8 shows the fields that Textract has identified, and also a confidence
    level. Most AI services will return some associated confidence level, and it is
    up to us, as consumers of the service, to figure out how we should deal with this
    number. For example, if our use case is highly sensitive to errors, then perhaps
    it is correct to only accept a 99% or better confidence level. Results with lower
    levels should be sent off for human verification or correction. Many business
    use cases, however, can tolerate lower accuracy. This judgment is very domain-specific,
    and should involve both business and technical stakeholders.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 显示了 Textract 识别的字段，以及一个置信度级别。大多数人工智能服务都会返回一些相关的置信度级别，而作为服务的消费者，我们则需要弄清楚我们应该如何处理这个数字。例如，如果我们的用例对错误非常敏感，那么可能只接受
    99% 或更好的置信度级别是正确的。置信度较低的成果应发送给人类进行验证或修正。然而，许多商业用例可以容忍较低的准确性。这种判断非常特定于领域，应涉及业务和技术利益相关者。
- en: 'Think about the business processes at your own organization: are there areas
    that could be automated by this type of analysis? Do you need to collect and input
    information from documents supplied by your customers? Perhaps you could improve
    that process by adapting this example to your own needs.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑你自己的组织中的业务流程：是否有可以通过此类分析自动化的领域？你是否需要从客户提供的文档中收集和输入信息？也许你可以通过调整此示例以满足自己的需求来改进该流程。
- en: 7.2.6 Remove the API
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 删除 API
- en: Before moving on to the next section, we need to remove the API to avoid any
    additional charges. To do this, `cd` into the `chapter7/text-analysis/text-analysis
    -api` directory and run
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，我们需要删除 API 以避免产生额外费用。为此，请使用 `cd` 命令进入 `chapter7/text-analysis/text-analysis
    -api` 目录，并运行
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will remove all of the uploaded images from the bucket and tear down the
    stack.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这将删除桶中所有上传的图像并拆除堆栈。
- en: 7.3 An AI-enabled data processing pipeline with Kinesis
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 带有 Kinesis 的 AI 驱动数据处理管道
- en: 'For our second example, we will build a data processing pipeline. This pipeline
    will be exposed through an asynchronous API, and will serve as our pattern 2 example.
    In building this example, we will explore a number of new services and technologies
    in detail, including Kinesis, Translate, and Comprehend:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第二个示例，我们将构建一个数据处理管道。这个管道将通过异步 API 公开，并作为我们的模式 2 示例。在构建这个示例时，我们将详细探讨包括 Kinesis、Translate
    和 Comprehend 在内的一系列新服务和技术的使用：
- en: Kinesis is Amazon’s real-time streaming service, which is used to create data-
    and video-processing pipelines.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis 是亚马逊的实时流服务，用于创建数据和处理视频的管道。
- en: Translate is Amazon’s machine-driven language translation service.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Translate 是亚马逊的机器驱动语言翻译服务。
- en: Comprehend is Amazon’s natural language processing (NLP) service, which can
    be used to perform tasks like sentiment analysis or keyword detection.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comprehend 是亚马逊的自然语言处理（NLP）服务，可用于执行情感分析或关键词检测等任务。
- en: Consider the domain of retail and e-commerce. A large retail outlet might have
    multiple product departments such as “outdoor,” “automotive,” “pets,” and so on.
    Customer service is an important part of the retail trade. In particular, responding
    quickly and effectively to customer complaints is important, as it can transform
    a disgruntled customer into a brand advocate if done correctly. The problem is
    that customers have many channels on which to complain, including website product
    reviews, email, Twitter, Facebook, Instagram, blog posts, and so forth.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑零售和电子商务领域。大型零售店可能有多个产品部门，如“户外”、“汽车”、“宠物”等。客户服务是零售贸易的重要组成部分。特别是，快速有效地回应客户投诉非常重要，因为如果处理得当，它可以把一个不满的客户转变为品牌倡导者。问题是客户有多个渠道可以投诉，包括网站产品评论、电子邮件、Twitter、Facebook、Instagram、博客文章等等。
- en: Not only are there many channels on which product feedback can be placed, global
    retailers have to handle feedback in multiple languages as well. Though humans
    are needed to deal with the customers, detecting the negative feedback across
    all of these channels and geographic territories is something that is amenable
    to an AI-driven solution.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅有许多渠道可以放置产品反馈，全球零售商还必须处理多种语言的反馈。尽管需要人类来处理客户，但检测所有这些渠道和地理区域的负面反馈是适合人工智能驱动解决方案的。
- en: Our example system will be an AI-enabled pipeline that can be used to filter
    feedback from multiple channels in multiple languages. The aim of our pipeline
    is to alert the appropriate department when a piece of negative feedback is detected
    about one of their products.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例系统将是一个具有AI功能的管道，可以用于过滤来自多个渠道和多种语言的反馈。我们管道的目标是在检测到关于他们产品之一的负面反馈时，向适当的部门发出警报。
- en: This AI-enabled pipeline augments and extends the retailer’s digital capability,
    while not interfering directly with line-of-business systems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个具有AI功能的管道增强了零售商的数字能力，同时不会直接干扰业务线系统。
- en: Our pipeline is depicted in figure 7.10\. At the start of the pipe, raw data
    is sent to a collection API; this can be inbound from multiple feeds, such as
    a Twitter feed, Facebook comments, inbound emails, and other social channels.
    The API feeds the raw text into a Kinesis stream.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图7.10中展示了我们的管道。管道的起始处，原始数据被发送到一个收集API；这些数据可以来自多个来源，例如Twitter流、Facebook评论、入站电子邮件以及其他社交媒体渠道。API将原始文本输入到Kinesis流中。
- en: '![](../Images/CH07_F10_Elger.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F10_Elger.png)'
- en: Figure 7.10 Processing pipeline
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 处理管道
- en: 'AWS provides two key streaming technologies: Managed Streaming for Kafka (MSK)
    and Kinesis. Of these, Kinesis is the simplest to use, so we will focus on it
    for this system. Data in the stream triggers a downstream Lambda, which uses Comprehend
    to determine the language of the inbound text. If the language is not English,
    then the Lambda runs on-the-fly translation with AWS Translate before posting
    it down the pipe. The next downstream Lambda runs sentiment analysis against the
    translated text using Comprehend. If a positive sentiment is detected, then no
    further processing is carried out for the message. However, should the sentiment
    be strongly negative, the text is sent to a customer classifier built using AWS
    Comprehend. This analyzes the text and attempts to determine the product department
    pertaining to the message. Once a department has been identified, the message
    can be dispatched to the appropriate team to allow them to address the negative
    comment. In this case, we will output results to an S3 bucket.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了两种关键的流技术：管理的Kafka（MSK）和Kinesis。在这些技术中，Kinesis是最容易使用的，因此我们将专注于它来构建这个系统。流中的数据会触发下游的Lambda函数，该函数使用Comprehend来确定入站文本的语言。如果语言不是英语，Lambda函数将使用AWS
    Translate进行即时翻译，然后再将其发送到管道中。下一个下游的Lambda函数将对翻译后的文本进行情感分析，使用Comprehend。如果检测到积极情感，则不会对消息进行进一步处理。然而，如果情感非常负面，文本将被发送到使用AWS
    Comprehend构建的客户分类器。该分类器分析文本并尝试确定与消息相关的产品部门。一旦确定了部门，消息就可以被发送到适当的团队，以便他们处理负面评论。在这种情况下，我们将输出结果到S3存储桶。
- en: By using a combination of AI services in this way, a pipeline such as this can
    provide enormous cost savings for an enterprise, because the filtering and categorization
    of feedback is performed automatically without requiring a team of people.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式结合使用AI服务，这样的管道可以为企业节省巨大的成本，因为反馈的过滤和分类是自动完成的，无需团队人员。
- en: Kinesis vs. Kafka
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis与Kafka的比较
- en: Until recently, one of the reasons for choosing Kinesis over Kafka was that
    Kafka required installation, setup, and management on EC2 instances. With the
    release of AWS Managed Streaming for Kafka (MSK), this situation has changed.
    Though a full discussion on the merits of Kafka is outside the scope of this book,
    we would note that the technology is highly scalable and versatile. We suggest
    that you investigate Kafka in more depth if you are building a system that requires
    a lot of streaming at scale.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，选择Kinesis而不是Kafka的一个原因是Kafka需要在EC2实例上安装、设置和管理。随着AWS管理的Kafka（MSK）的发布，这种情况已经改变。尽管关于Kafka优点的全面讨论超出了本书的范围，但我们想指出，这项技术具有高度可扩展性和多功能性。我们建议，如果您正在构建一个需要大量流处理的系统，您应该更深入地研究Kafka。
- en: 'Even taking MSK into account, it is still true that Kinesis is more fully integrated
    into the AWS stack and is simpler to get up and running quickly, so we will use
    it for the purposes of our example system. Kinesis can be used in several ways:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 即使考虑到MSK，Kinesis仍然更完全地集成到AWS堆栈中，并且更容易快速启动，因此我们将使用它作为示例系统。Kinesis可以用几种方式使用：
- en: Kinesis Video Streams--For video and audio content
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis Video Streams--用于视频和音频内容
- en: Kinesis Data Streams--For general data streaming
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis Data Streams--用于通用数据流
- en: Kinesis Data Firehose--Supports streaming of Kinesis data to targets such as
    S3, Redshift, or Elasticsearch
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis Data Firehose--支持将Kinesis数据流式传输到S3、Redshift或Elasticsearch等目标
- en: Kinesis Analytics--Supports real-time stream processing with SQL
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis Analytics--支持使用SQL进行实时流处理
- en: In this chapter, we are using Kinesis Data Streams to build our pipeline.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用Kinesis Data Streams构建我们的管道。
- en: 7.3.1 Get the code
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 获取代码
- en: 'The code for our pipeline is in the book repository in the directory `chapter7/pipeline`.
    This contains the following sub directories that map to each stage in the process:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管道的代码位于书籍仓库中`chapter7/pipeline`目录下。该目录包含以下子目录，它们对应于处理过程中的每个阶段：
- en: '`pipeline-api`--Contains the API Gateway setup for the system'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pipeline-api`--包含系统的API Gateway设置'
- en: '`translate`--Contains the language detection and translation service'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`translate`--包含语言检测和翻译服务'
- en: '`sentiment`--Contains the sentiment analysis code'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentiment`--包含情感分析代码'
- en: '`training`--Contains utility scripts to help train a custom classifier'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training`--包含帮助训练自定义分类器的实用脚本'
- en: '`classify`--Contains the code that triggers our custom classifier'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classify`--包含触发我们的自定义分类器的代码'
- en: '`driver`--Contains code to exercise the pipeline'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`driver`--包含用于测试管道的代码'
- en: As with the preceding examples, we will briefly describe the code for each service
    before deploying. Once all of our units have been deployed, we will test our pipeline
    end to end. Let’s get started with the simple first step, deploying the API.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的示例一样，在部署之前，我们将简要描述每个服务的代码。一旦我们所有的单元都已部署，我们将对管道进行端到端测试。让我们从简单的第一步开始，部署API。
- en: 7.3.2 Deploying the API
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 部署API
- en: The code for the API is in the directory `chapter7/pipeline/pipeline-api` and
    consists of a `serverless.yml` file along with a simple API. The Serverless configuration
    defines a single `ingest` method, which pushes data posted to the API into Kinesis.
    The Kinesis stream is also defined in the Serverless configuration, which is shown
    in the following listing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: API的代码位于`chapter7/pipeline/pipeline-api`目录中，包括一个`serverless.yml`文件和一个简单的API。Serverless配置定义了一个单一的`ingest`方法，该方法将API发布的数据推送到Kinesis。Kinesis流也在Serverless配置中定义，如下所示。
- en: Listing 7.9 `serverless.yml` Kinesis definition
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9 `serverless.yml` Kinesis定义
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Define Kinesis stream
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义Kinesis流
- en: The code for the API is very simple in that it just forwards inbound data to
    the Kinesis stream. The API accepts inbound JSON `POST` requests and expects the
    format shown in the next listing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: API的代码非常简单，它只是将传入的数据转发到Kinesis流。API接受传入的JSON `POST`请求，并期望格式如下一列表所示。
- en: Listing 7.10 JSON data format for pipeline API
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10 管道API的JSON数据格式
- en: '[PRE12]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The original text
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 原始文本
- en: ❷ The source of the feedback
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 反馈来源
- en: ❸ The ID of the feedback originator
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 反馈发起者的ID
- en: Before deploying the API, we need to set up our environment. We have provided
    a template `.env` file in the `chapter7/pipeline` directory called `default-environment.env`.
    Make a copy of this file in the `chapter7/pipeline` directory with the filename
    `.env`. The file should have the contents outlined in the next listing.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署API之前，我们需要设置我们的环境。我们在`chapter7/pipeline`目录中提供了一个名为`default-environment.env`的模板`.env`文件。在`chapter7/pipeline`目录中创建此文件的副本，文件名为`.env`。该文件应包含下一列表中概述的内容。
- en: Listing 7.11 Environment file for Pipeline
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.11 管道的环境文件
- en: '[PRE13]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Kinesis shard count
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Kinesis分片数量
- en: ❷ Name of Kinesis translation stream
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Kinesis翻译流名称
- en: ❸ Name of Kinesis sentiment stream
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Kinesis情感流名称
- en: ❹ Name of Kinesis classify stream
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Kinesis分类流名称
- en: Next we can go ahead and deploy the API by opening a command shell in the `chapter7/pipeline/pipeline-api`
    directory and executing
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以在`chapter7/pipeline/pipeline-api`目录中打开命令行，并执行以下操作来部署API。
- en: '[PRE14]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This will create our first Kinesis stream, and also our ingestion API. Figure
    7.11 illustrates the state of our pipeline after deployment of the API. The highlighted
    section represents what has been deployed so far.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建我们的第一个Kinesis流，以及我们的摄取API。图7.11说明了API部署后的管道状态。高亮部分表示到目前为止已部署的内容。
- en: '![](../Images/CH07_F11_Elger.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11](../Images/CH07_F11_Elger.png)'
- en: Figure 7.11 Pipeline after API deployment
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 API部署后的管道
- en: On deployment, the framework will output the URL for our API. Before proceeding
    to the next stage, add this into the `.env` file as shown in the following listing,
    substituting your specific value.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署后，框架将输出我们API的URL。在进入下一阶段之前，请将其添加到`.env`文件中，如下所示，并用您的具体值替换。
- en: Listing 7.12 Additional entries in `.env` file following API deployment
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.12 API部署后的`.env`文件中的附加条目
- en: '[PRE15]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 7.4 On-the-fly translation with Translate
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 使用Translate进行即时翻译
- en: The first stage in our pipeline after ingestion is to detect the language and
    translate to English if needed. These tasks are handled by our translation service,
    the code for which is in the directory `chapter8/pipeline/translate`. The Serverless
    configuration is fairly standard, except that the main handler function is triggered
    by the Kinesis stream that we defined in our API deployment. This is shown in
    the following listing.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据摄取后的我们管道的第一个阶段是检测语言，如果需要则翻译成英语。这些任务由我们的翻译服务处理，其代码位于`chapter8/pipeline/translate`目录中。Serverless配置相当标准，除了主要处理函数是由我们在API部署中定义的Kinesis流触发的。这将在下面的列表中展示。
- en: Listing 7.13 Handler triggered by Kinesis
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.13 由Kinesis触发的处理程序
- en: '[PRE16]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Connect to the stream.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 连接到流。
- en: The configuration defines a second Kinesis stream that our Sentiment service
    will connect to, and also sets up the appropriate permissions to post to the stream
    and call the required translation services. This is shown in the following listing.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 该配置定义了一个第二个Kinesis流，我们的情感服务将连接到它，并设置了适当的权限来发布到流和调用所需的翻译服务。这将在下面的列表中展示。
- en: Listing 7.14 Handler IAM permissions
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.14 处理程序IAM权限
- en: '[PRE17]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Comprehend permissions
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Comprehend权限
- en: ❷ Translate permissions
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 翻译权限
- en: ❸ Kinesis permissions
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Kinesis权限
- en: The code for our translation service in `handler.js` is triggered with data
    from the Kinesis stream defined by our API. This is as a block of Base64-encoded
    records in the event parameter to our handler function. The next listing shows
    how our service consumes these records.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们翻译服务的代码在`handler.js`中，由我们的API定义的Kinesis流中的数据触发。这作为事件参数中Base64编码记录的一个块。下一个列表展示了我们的服务如何消费这些记录。
- en: Listing 7.15 Translation service
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.15 翻译服务
- en: '[PRE18]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Loop over each record.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历每个记录。
- en: ❷ Decode the record.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 解码记录。
- en: ❸ Convert to object
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 转换为对象
- en: Our service uses Comprehend and Translate in combination. Comprehend is used
    to detect the language in our message, and Translate is used to convert to English
    if the detected language requires it. The next listing shows the relevant calls
    from the source code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务结合使用Comprehend和Translate。Comprehend用于检测我们的消息中的语言，Translate用于在检测到的语言需要时将其转换为英语。下一个列表展示了源代码中的相关调用。
- en: Listing 7.16 Detect language and translate
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.16 检测语言和翻译
- en: '[PRE19]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Detect language
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检测语言
- en: ❷ Translate to English
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 翻译成英语
- en: Once the service has translated the text, if required, it posts an updated message
    into the second Kinesis stream. This will later be picked up by our sentiment
    detection service, which we will deploy shortly.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务翻译了文本，如果需要，它将更新后的消息发布到第二个Kinesis流。这将稍后被我们的情感检测服务获取，我们将在不久后部署它。
- en: To deploy the translation service, open a command shell in the `chapter7/pipeline/translate`
    directory and run
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署翻译服务，请在`chapter7/pipeline/translate`目录中打开命令行并运行
- en: '[PRE20]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will create the second stage in our pipeline. Figure 7.12 illustrates the
    state of our pipeline after the latest deployment.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在我们的管道中创建第二个阶段。图7.12展示了最新部署后我们的管道状态。
- en: '![](../Images/CH07_F12_Elger.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12 Elger](../Images/CH07_F12_Elger.png)'
- en: Figure 7.12 Pipeline after API deployment
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 API部署后的管道
- en: We’re halfway through the deployment of our pipeline. In the next section, we
    will check that everything is working so far.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了管道部署的一半。在下一节中，我们将检查到目前为止一切是否正常工作。
- en: 7.5 Testing the pipeline
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 测试管道
- en: Now that we have part of our pipeline deployed, let’s put some data through
    it to check that it’s working correctly. To do this, we are going to take advantage
    of a free open source public data set. Let’s grab some of this data now and use
    it to test our pipeline.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经部署了管道的一部分，让我们通过一些数据来检查它是否正常工作。为此，我们将利用一个免费的开源公共数据集。让我们现在获取一些这些数据并用来测试我们的管道。
- en: First `cd` into `chapter7/pipeline/testdata` directory. This contains a script
    that will download and unpack some test data, which you can run using
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先`cd`到`chapter7/pipeline/testdata`目录。这个目录包含一个脚本，它将下载并解压一些测试数据，你可以使用以下命令运行
- en: '[PRE21]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We are using a subset of the Amazon product review data held at [http://snap.stanford
    .edu/data/amazon/productGraph/](http://snap.stanford.edu/data/amazon/productGraph/).
    Specifically we are using data in the automotive, beauty, office, and pet categories.
    Once the script has completed, you will have four JSON files in the directory
    `testdata/data`. Each file contains a number of reviews, with review text and
    an overall score. You can open up the files in a text editor and take a look through
    them to get a feel for the data.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用存储在[http://snap.stanford.edu/data/amazon/productGraph/](http://snap.stanford.edu/data/amazon/productGraph/)的亚马逊产品评论数据的一个子集。具体来说，我们使用汽车、美容、办公和宠物类别的数据。一旦脚本完成，你将在`testdata/data`目录中拥有四个JSON文件。每个文件包含一定数量的评论，包括评论文本和总体评分。你可以用文本编辑器打开这些文件，浏览它们以了解数据。
- en: 'There is another script in the `testdata` directory called `preproc.sh`. This
    takes the downloaded review data and processes it into a format for training and
    testing our custom classifier. We will look at the classifier in the next section,
    but for now let’s process our data by running this script:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在`testdata`目录中还有一个名为`preproc.sh`的脚本。它将下载的评论数据处理成用于训练和测试我们自定义分类器的格式。我们将在下一节中查看分类器，但现在让我们通过运行此脚本来处理我们的数据：
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will create a number of additional files in the `data` directory. For each
    downloaded file, it creates a new JSON file with the structure shown in the next
    listing.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在`data`目录中创建多个附加文件。对于每个下载的文件，它创建一个新的JSON文件，其结构如下一列表所示。
- en: Listing 7.17 Amazon reviews data format
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.17 亚马逊评论数据格式
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Training data
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据
- en: ❷ Negative test data
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 负面测试数据
- en: ❸ Positive test data
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 正面测试数据
- en: What the script has done is to split the input data into two sets, one for training
    and one for testing, with the bulk of the records in the training set. Within
    the test set, we have used the `overall` field in the original data to determine
    if this review data is positive or negative. This will allow us to test our sentiment
    filter later on. The script has also created a CSV (comma separated value) file,
    `data/final/training.csv`. We will use this file in the next section to train
    our classifier.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本所做的是将输入数据分成两个集合，一个用于训练，一个用于测试，其中大部分记录在训练集中。在测试集中，我们使用原始数据中的`overall`字段来确定这些评论数据是正面还是负面。这将允许我们稍后测试我们的情感过滤器。脚本还创建了一个CSV（逗号分隔值）文件，`data/final/training.csv`。我们将在下一节中使用此文件来训练我们的分类器。
- en: 'Now that we have our data downloaded and prepared, we can check that our pipeline
    is functioning correctly so far. There is a test utility for this in the directory
    `pipeline/driver`. This has two small Node.js programs: `driver.js`, which calls
    our API with test data, and `streamReader.js`, which reads data from a nominated
    Kinesis stream so that we can see what data exists in that stream. We won’t go
    into the code here.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载并准备好了数据，我们可以检查我们的管道到目前为止是否正常工作。在`pipeline/driver`目录中有一个用于此目的的测试工具。这包含两个小的Node.js程序：`driver.js`，它使用测试数据调用我们的API，以及`streamReader.js`，它从指定的Kinesis流中读取数据，这样我们就可以看到该流中存在哪些数据。我们不会在这里详细介绍代码。
- en: 'Let’s first post some data to our API. Open a command shell in `pipeline/driver`,
    install dependencies, and then run the driver:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们向我们的API发送一些数据。在`pipeline/driver`目录中打开命令行，安装依赖项，然后运行驱动程序：
- en: '[PRE24]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will call the API with three random reviews: two from the office products
    data set, and one from the beauty data set. The driver also allows us to specify
    whether the data should be positive or negative. Next let’s check that the data
    is indeed in our Kinesis streams. First run'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用三个随机评论调用API：两个来自办公产品数据集，一个来自美容数据集。驱动程序还允许我们指定数据应该是正面还是负面。接下来，让我们检查数据是否确实在我们的Kinesis流中。首先运行
- en: '[PRE25]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will read data back from our translate stream and display it on the console.
    The stream reader code polls Kinesis every second to display the latest data.
    To stop the reader, press Ctrl-C. Next, repeat this exercise for the sentiment
    stream:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从我们的翻译流中读取数据并在控制台上显示。流读取器代码每秒轮询Kinesis以显示最新数据。要停止读取器，请按Ctrl-C。接下来，为情感流重复此练习：
- en: '[PRE26]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You should see the same data displayed to the console, with some additional
    fields that were added by the translation service.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会在控制台上看到相同的数据显示，还有一些由翻译服务添加的额外字段。
- en: 7.6 Sentiment analysis with Comprehend
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 使用Comprehend进行情感分析
- en: Now that we have tested our pipeline, it’s time to implement the next stage,
    which is detecting the sentiment of the inbound text. The code for this is in
    the directory `pipeline/sentiment` and uses AWS Comprehend to determine the sentiment.
    The Serverless configuration is very similar to the previous services, so we won’t
    cover it here, except to note that the configuration creates an S3 bucket to collect
    the negative review data for further processing.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经测试了我们的管道，是时候实施下一阶段了，即检测传入文本的情感。这个代码位于`pipeline/sentiment`目录中，并使用AWS Comprehend来确定情感。无服务器配置与之前的服务非常相似，所以我们在这里不会详细说明，只是要注意配置创建了一个S3存储桶来收集负面评论数据以供进一步处理。
- en: Sentiment analysis
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'Sentiment analysis is a complex process involving the use of natural language
    processing (NLP), text analysis, and computational linguistics. It is a difficult
    task for computers to perform, because at the root it involves the detection,
    to some extent, of emotions expressed in text form. Consider the following sentence
    that might be written by a reviewer about a hotel that they just stayed in:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一个复杂的过程，涉及自然语言处理（NLP）、文本分析和计算语言学。对于计算机来说，这是一个困难的任务，因为从根本上讲，它涉及到在一定程度上检测文本中表达的情感。考虑以下可能由评论者关于他们刚刚入住的酒店所写的句子：
- en: We hated leaving the hotel and felt sad to get home.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨厌离开酒店，回家时感到很悲伤。
- en: While this is in fact expressing a positive sentiment about the hotel, all of
    the words in this sentence are negative if taken in isolation. With the application
    of deep learning techniques, sentiment analysis is becoming more and more accurate.
    However sometimes a human is still required to make a judgement call.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这实际上是在表达对酒店的正面情感，但如果单独考虑这个句子中的所有单词，它们都是负面的。随着深度学习技术的应用，情感分析变得越来越准确。然而，有时仍然需要人工进行判断。
- en: By using AWS Comprehend, we don’t have to be concerned about all of this complexity;
    we merely need to process the results and call a human in when the API can’t make
    an accurate determination.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用AWS Comprehend，我们不必担心所有这些复杂性；我们只需处理结果，并在API无法做出准确判断时调用人工。
- en: The code for the service is in `handler.js`, and is illustrated in the next
    listing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的代码位于`handler.js`中，并在下一列表中展示。
- en: Listing 7.18 Sentiment analysis handler
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.18 情感分析处理器
- en: '[PRE27]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Unpack the message from Kinesis.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从Kinesis中提取消息。
- en: ❷ Detect sentiment
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检测情感
- en: ❸ Write negative, neutral, or mixed message to S3
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将负面、中性或混合消息写入S3
- en: ❹ Even if positive, write depending on confidence level
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 即使是正面的，也要根据置信度来写
- en: After unpacking the message, the code calls Comprehend to detect the message
    sentiment. Any negative messages are written to an S3 bucket for onward processing.
    Positive messages are dropped. However, you could do further computation at this
    point; for example, monitoring the ratio of positive to negative sentiment and
    alerting on anomalous conditions.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取消息后，代码调用Comprehend来检测消息的情感。任何负面消息都被写入S3存储桶以供进一步处理。正面消息被丢弃。然而，你可以在这一点上进行进一步的计算；例如，监控正面与负面情感的比例，并在异常条件下发出警报。
- en: As with all AI services, it is important to interpret the returned confidence
    level appropriately for the business problem. In this case, we have decided to
    err on the side of caution. This means
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 就像所有AI服务一样，正确解释返回的置信度对于业务问题非常重要。在这种情况下，我们决定采取谨慎的态度。这意味着
- en: Any overall negative, neutral, or mixed messages are treated as negative sentiments
    and sent on for classification.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何整体负面、中性或混合消息被视为负面情感，并继续进行分类。
- en: Any overall positive messages with a confidence level of more than 85% are discarded.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何整体正面消息，如果置信度超过85%，则被丢弃。
- en: Any overall positive message with a confidence level of less than 85% are treated
    as negative and sent on for classification.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何整体正面消息，如果置信度低于85%，则被视为负面，并继续进行分类。
- en: Remember that in this scenario, once classified, messages that aren’t discarded
    will be sent to a human for processing. We could easily change these rules to
    suit our business process--for example, by discarding the neutral and positive
    messages regardless of confidence level if we were less concerned about picking
    up all complaints, and only wanted to focus on strongly negative results. The
    point is to understand that our results come with an associated confidence level
    and to interpret this accordingly.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在这个场景中，一旦分类，未丢弃的消息将被发送给人工处理。我们可以轻松地更改这些规则以适应我们的业务流程——例如，如果我们不太关心收集所有投诉，只想关注强烈负面结果，我们可以丢弃中性和正面消息，无论置信度如何。关键是理解我们的结果都伴随着一个相关的置信度，并据此进行解释。
- en: 'Let’s now deploy the sentiment analysis service. `cd` into the `pipeline/sentiment`
    directory and run the following commands:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署情感分析服务。切换到`pipeline/sentiment`目录并运行以下命令：
- en: '[PRE28]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once the service is deployed, we can retest our pipeline by running the driver
    again to post some positive and negative messages, as shown in the next listing.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦服务部署完成，我们可以通过再次运行驱动程序来重新测试我们的管道，发送一些正面和负面的消息，如下一列表所示。
- en: Listing 7.19 Amazon reviews data format
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.19 Amazon评论数据格式
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Send positive message
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 发送正面消息
- en: ❷ Send negative message
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 发送负面消息
- en: 'To check that our pipeline is working correctly, run the `streamReader` utility
    in the `driver` directory, this time telling it to read from the classify stream:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的管道是否正常工作，在`driver`目录中运行`streamReader`实用程序，这次告诉它从分类流中读取：
- en: '[PRE30]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This will read data back from our classifier stream and display it on the console.
    The stream-reader code polls Kinesis every second to display the latest data.
    To stop the reader, hit Ctrl-C. You should see the message output, along with
    some additional data from the sentiment analysis. Note that strongly positive
    messages will be discarded, so not all of the messages sent by the driver will
    make it to the classifier stream.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从我们的分类器流中读取数据并显示在控制台上。流读取器代码每秒轮询Kinesis以显示最新数据。要停止读取器，请按Ctrl-C。你应该看到消息输出，以及一些来自情感分析的其他数据。注意，强烈正面消息将被丢弃，因此并非所有由驱动程序发送的消息都会到达分类器流。
- en: Following this deployment, the current state of our pipeline is shown in figure
    7.13.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在此部署之后，我们管道的当前状态如图7.13所示。
- en: Tip Though we are using translation and sentiment analysis services as part
    of a data pipeline, these can, of course, be used in isolation. Perhaps you can
    think of instances in your current work or organization where you could apply
    these services.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：尽管我们正在将翻译和情感分析服务作为数据管道的一部分使用，但当然也可以单独使用。也许你可以想想在你的当前工作或组织中可以应用这些服务的实例。
- en: '![](../Images/CH07_F13_Elger.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F13_Elger.png)'
- en: Figure 7.13 Pipeline after sentiment service deployment
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 情感服务部署后的管道
- en: 7.7 Training a custom document classifier
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7 训练自定义文档分类器
- en: 'For the final stage in our pipeline, we are going to use a custom classifier.
    From the inbound message text, our classifier will be able to determine which
    department the message is pertaining to: Automotive, Beauty, Office, or Pets.
    Training a classifier from scratch is a complex task that normally requires some
    level of in-depth knowledge of machine learning. Thankfully AWS Comprehend makes
    the job much easier. Figure 7.14 illustrates the training process.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们管道的最终阶段，我们将使用一个自定义分类器。从传入的消息文本中，我们的分类器将能够确定消息属于哪个部门：汽车、美容、办公室或宠物。从头开始训练分类器是一个复杂的任务，通常需要一定程度的机器学习深入知识。幸运的是，AWS
    Comprehend使这项工作变得容易得多。图7.14说明了训练过程。
- en: '![](../Images/CH07_F14_Elger.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F14_Elger.png)'
- en: Figure 7.14 Process for training a custom classifier with Comprehend
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 使用Comprehend训练自定义分类器的流程
- en: 'All of the code to train our custom classifier is in the directory `pipeline/training`.
    To train our classifier, we need to do the following:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 所有训练自定义分类器的代码都在`pipeline/training`目录中。为了训练我们的分类器，我们需要做以下几步：
- en: Create a data bucket.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个数据存储桶。
- en: Upload training data to the bucket.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练数据上传到存储桶。
- en: Create an IAM role for the classifier.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分类器创建一个IAM角色。
- en: Run the training data to create a classifier.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行训练数据以创建分类器。
- en: Create an endpoint to make the classifier available.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个端点以使分类器可用。
- en: Document classification models
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分类模型
- en: 'Document classification is the problem of assigning one or more classes or
    types to a document. In this context, a document can range from a large manuscript
    to a single sentence. This is typically performed using one of two approaches:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 文档分类是将一个或多个类别或类型分配给文档的问题。在这个上下文中，文档可以是从大型手稿到单个句子的任何内容。这通常使用两种方法之一来完成：
- en: Unsupervised classification--Clusters documents into types based on textual
    analysis
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督分类--根据文本分析将文档聚类成类型
- en: Supervised classification--Provides labeled data to a training process to build
    a model that is customized for our needs
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督分类--为训练过程提供标记数据以构建针对我们需求的定制模型
- en: In this chapter we are using supervised classification to train a model. By
    using Comprehend, we don’t need to get into the details of the training process;
    we just need to supply a labeled data set for Comprehend to train on.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用监督分类来训练模型。通过使用Comprehend，我们不需要深入了解训练过程；我们只需要为Comprehend提供一个标记的数据集进行训练。
- en: 7.7.1 Create a training bucket
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.1 创建训练存储桶
- en: Before we create our training bucket, we need to update our `.env` file. Open
    this in a text editor as before, and add the line indicated in the next listing,
    substituting your own unique bucket name.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建训练存储桶之前，我们需要更新我们的`.env`文件。像之前一样在文本编辑器中打开它，并添加下一个列表中指示的行，用您自己的唯一存储桶名称替换。
- en: Listing 7.20 Environment file for pipeline
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.20 管道环境文件
- en: '[PRE31]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To create the bucket, `cd` into the directory `pipeline/training` and run the
    following:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建存储桶，在`pipeline/training`目录下使用`cd`命令进入该目录并运行以下命令：
- en: '[PRE32]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 7.7.2 Upload training data
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.2 上传训练数据
- en: As you’ll recall from the previous section where we tested our pipeline, our
    data processing script created a CSV file for training. We now need to upload
    this to our training bucket. `cd` into the directory `pipeline/testdata` and run
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从上一节中测试我们的管道时回忆的那样，我们的数据处理脚本创建了一个用于训练的CSV文件。我们现在需要将其上传到我们的训练存储桶。在`pipeline/testdata`目录下使用`cd`命令进入该目录并运行
- en: '[PRE33]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This will push the training data set to S3\. Note that the training file is
    around 200MB, so this may take a while to upload depending on your outbound connection
    speed.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把训练数据集推送到S3。请注意，训练文件大约有200MB，因此根据您的出站连接速度，上传可能需要一段时间。
- en: The training data file is just a `csv` file containing a set of labels and associated
    text, as shown in the next listing.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据文件只是一个包含一组标签和相关文本的`csv`文件，如下所示。
- en: Listing 7.21 Training data file structure
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.21 训练数据文件结构
- en: '[PRE34]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In our case the label is one of `AUTO`, `BEAUTY`, `OFFICE`, or `PET`. Comprehend
    will use this file to build a custom classifier using the text data to train the
    model and match it to the appropriate label.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，标签是`AUTO`、`BEAUTY`、`OFFICE`或`PET`之一。Comprehend将使用此文件来构建一个自定义分类器，使用文本数据来训练模型并将其与适当的标签匹配。
- en: 7.7.3 Create an IAM role
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.3 创建IAM角色
- en: Next we have to create an Identity and Access Management (IAM) role for the
    classifier. This will restrict the AWS cloud services that the classifier can
    access. To create the role, `cd` into the directory `pipeline/training` and run
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须为分类器创建一个身份和访问管理（IAM）角色。这将限制分类器可以访问的AWS云服务。要创建该角色，在`pipeline/training`目录下使用`cd`命令进入该目录并运行
- en: '[PRE35]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This will create the role and write the newly created role ARN to the console.
    Add the role ARN to the `.env` file, as shown in the following listing.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建角色并将新创建的角色ARN写入控制台。将角色ARN添加到`.env`文件中，如下所示。
- en: Listing 7.22 Update pipeline environment with role ARN
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.22 使用角色ARN更新管道环境
- en: '[PRE36]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Note AWS Identity and Access Management (IAM) capabilities are pervasive throughout
    AWS. AWS IAM defines roles and access permissions across the platform. A full
    description is outside the scope of this book, but you can find the full AWS IAM
    documentation here: [http://mng.bz/NnAd](http://mng.bz/NnAd).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 注意AWS身份和访问管理（IAM）功能在AWS中无处不在。AWS IAM定义了整个平台上的角色和访问权限。完整的描述超出了本书的范围，但您可以在以下链接中找到完整的AWS
    IAM文档：[http://mng.bz/NnAd](http://mng.bz/NnAd)。
- en: 7.7.4 Run training
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.4 运行训练
- en: We’re now ready to start training the classifier. The code to do this is in
    `pipeline/training/train-classifier.js`. This code simply calls Comprehend’s `createDocumentClassifier`
    API, passing in the data access role, classifier name, and a link to the training
    bucket. This is shown in the next listing.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始训练分类器了。执行此操作的代码位于`pipeline/training/train-classifier.js`中。此代码简单地调用Comprehend的`createDocumentClassifier`
    API，传入数据访问角色、分类器名称和训练存储桶的链接。这将在下一个列表中展示。
- en: Listing 7.23 Training the classifier
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.23 训练分类器
- en: '[PRE37]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Set training parameters.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置训练参数。
- en: ❷ Start training.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 开始训练。
- en: To start training, `cd` into the directory `pipeline/training` and run
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练，请使用`cd`命令进入`pipeline/training`目录并运行
- en: '[PRE38]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: It should be noted at this point that the training process may take a while
    to complete, usually over an hour, so now might be a good time to take a break!
    You can check on the status of the training process by running the script `status.sh`
    in the same directory. This will output a status of `TRAINED` once the classifier
    is ready to use.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，训练过程可能需要一段时间才能完成，通常超过一小时，所以现在可能是一个休息的好时机！您可以通过在同一目录中运行脚本`status.sh`来检查训练过程的状态。一旦分类器准备好使用，它将输出状态`TRAINED`。
- en: 7.8 Using the custom classifier
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.8 使用自定义分类器
- en: 'Now that we have trained our classifier, we can complete the last stage in
    the pipeline: deploying a classification service to call our newly trained custom
    classifier. Recall that we have already determined the language of the message,
    translated to English if required, and filtered to include only negative messages
    in the processing bucket. Now we need to determine which department these messages
    are related to by running our newly trained classifier.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的分类器，我们可以在管道中完成最后阶段：部署一个分类服务来调用我们新训练的自定义分类器。回想一下，我们已经确定了消息的语言，如果需要，翻译成英语，并在处理存储桶中过滤出只有负面消息。现在我们需要通过运行我们新训练的分类器来确定这些消息与哪个部门相关。
- en: 'To make the classifier available, we need to create an endpoint. Do this by
    running the script `endpoint.sh` in the `pipeline/training` directory:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要使分类器可用，我们需要创建一个端点。通过在`pipeline/training`目录中运行脚本`endpoint.sh`来完成此操作：
- en: '[PRE39]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Warning Once the endpoint for the classifier is created, you will be charged
    per hour that it is available, so please ensure that you delete all resources
    for this chapter once you’re done!
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：一旦创建分类器的端点，您将按小时计费，因此请确保您完成此章节后删除所有资源！
- en: Before we deploy our classification service, we need to update the `.env` file
    to provide the name of our output bucket. Open this in a text editor and edit
    the line indicated in the next listing, substituting your own unique bucket name.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署分类服务之前，我们需要更新`.env`文件以提供输出存储桶的名称。在文本编辑器中打开它，并编辑下一列表中指示的行，用您自己的唯一存储桶名称替换。
- en: Listing 7.24 Pipeline processing bucket
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.24管道处理存储桶
- en: '[PRE40]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The code for out classification service is in the `pipeline/classify` directory.
    This holds the `serverless.yml` and `handler.js` files for the service. The following
    listing shows how the classifier is executed from the main handler function in
    the service.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类服务的代码位于`pipeline/classify`目录中。该目录包含服务的`serverless.yml`和`handler.js`文件。以下列表显示了如何从服务的主处理函数中执行分类器。
- en: Listing 7.25 Invoking the custom classifier endpoint
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.25调用自定义分类器端点
- en: '[PRE41]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Add the endpoint ARN to the parameters.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将端点ARN添加到参数中。
- en: ❷ Invoke the classifier through the endpoint.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过端点调用分类器。
- en: ❸ Process the results.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 处理结果。
- en: ❹ Write the message to the output bucket.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将消息写入输出存储桶。
- en: While we have trained our own custom classifier, the consumption pattern is
    similar to the other services that we have encountered previously, so the code
    should seem familiar. The function `determineClass` that is called in listing
    7.25 is shown in the following listing.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经训练了自己的自定义分类器，但其消耗模式与其他我们之前遇到的服务相似，因此代码应该看起来很熟悉。在列表7.25中调用的`determineClass`函数如下所示。
- en: Listing 7.26 Interpreting the custom classification results
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.26解释自定义分类结果
- en: '[PRE42]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Find the classification with the highest score.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 找到分数最高的分类。
- en: ❷ Only accept scores greater that 95%.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 只接受大于95%的分数。
- en: The function returns the classification class with the highest score, given
    that the score is greater than 95%. Otherwise it will return a result of `UNCLASSIFIED`.
    It is important to note that like the other services we have encountered, interpretation
    of the confidence level is domain-specific. In this case, we have opted for a
    high degree of accuracy (greater than 95%). Unclassified results will need to
    be processed by a human rather than sent directly to a department.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回分数最高的分类类别，前提是分数大于95%。否则，它将返回`UNCLASSIFIED`的结果。需要注意的是，与其他我们遇到的服务一样，置信水平的解释是特定领域的。在这种情况下，我们选择了高精度（大于95%）。未分类的结果需要由人工处理，而不是直接发送到部门。
- en: To deploy the classification service, `cd` into the `pipeline/classify` directory
    and run
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署分类服务，请使用`cd`命令进入`pipeline/classify`目录并运行
- en: '[PRE43]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We have now fully deployed our pipeline! For the final step in this chapter,
    let’s test it end to end.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完全部署了我们的管道！在本章的最后一步，让我们进行端到端测试。
- en: 7.9 Testing the pipeline end to end
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.9 端到端测试管道
- en: To test our full pipeline, let’s first push some data into it. We can do this
    by using the test driver as before. `cd` into the directory `pipeline/driver`,
    and push in some data by running
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的完整管道，让我们首先向其中推送一些数据。我们可以通过使用之前的测试驱动器来完成此操作。进入目录 `pipeline/driver`，并通过运行以下命令推送一些数据
- en: '[PRE44]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Do this several times, substituting random department names: `auto`, `beauty`,
    `office`, or `pet`. Also, randomly use both positive and negative values. The
    messages should flow through the pipeline, and negative messages will end up in
    the processing bucket under one of five possible paths: auto, beauty, office,
    pet, or unclassified. We have provided a script to help check the results. `cd`
    into the `pipeline/driver` directory and run'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做几次，用随机的部门名称替换：`auto`、`beauty`、`office` 或 `pet`。同时，随机使用正负值。信息应该通过管道流动，负面信息最终会出现在处理桶中的五个可能路径之一下：auto、beauty、office、pet
    或未分类。我们提供了一个脚本来帮助检查结果。进入 `pipeline/driver` 目录并运行
- en: '[PRE45]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This will fetch the output results from the bucket and print them to the console.
    You should see output similar to the following:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这将获取桶中的输出结果并将其打印到控制台。你应该会看到类似以下输出：
- en: '[PRE46]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Remember that only the negative messages will be in the results bucket; positive
    values should have been discarded by the sentiment filter. Take some time to review
    the results. Some messages will be unclassified, meaning that the confidence level
    of the classification step was below 95%.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，只有负面信息会出现在结果桶中；正面值应该已经被情感过滤器丢弃。花些时间来审查结果。一些信息将未被分类，这意味着分类步骤的置信水平低于 95%。
- en: A next logical step in the process would be to send alert emails, based on the
    pipeline output, to the appropriate department. This could easily be done using
    Amazon’s SES (Simple Email Service) service, and we leave this as an exercise
    for the reader to complete!
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中的下一个逻辑步骤可能是根据管道输出发送警报电子邮件到适当的部门。这可以很容易地使用 Amazon 的 SES（简单电子邮件服务）服务完成，我们将此作为练习留给读者来完成！
- en: As a further exercise, you could write a script to push a larger volume of data
    into the pipeline and see how the system behaves. You could also try making up
    your own comments or “tweets” and send them into the pipeline to determine how
    accurate the system is when presented with different data items.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 作为进一步练习，你可以编写一个脚本将大量数据推送到管道中，并观察系统如何表现。你也可以尝试编写自己的评论或“推文”并发送到管道中，以确定系统在呈现不同数据项时的准确性。
- en: 7.10 Removing the pipeline
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.10 移除管道
- en: Once you have finished with the pipeline, it’s important to remove it in order
    to avoid incurring additional costs from AWS. To do this, we have provided some
    scripts that will remove all of the elements of the pipeline in the directory
    `chapter7/pipeline`. `cd` into this directory and run
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了管道，重要的是要将其移除，以避免从 AWS 负担额外的费用。为此，我们提供了一些脚本，这些脚本将移除目录 `chapter7/pipeline`
    中管道的所有元素。进入此目录并运行
- en: '[PRE47]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This will remove the endpoint, which may take a few minutes to complete. You
    can re-run the `check-endpoint.sh` script; this will show a status of `DELETING`
    against our endpoint. Once the script no longer lists our endpoint, you can proceed
    to remove the rest of the system by running
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这将移除端点，这可能需要几分钟才能完成。你可以重新运行 `check-endpoint.sh` 脚本；这将显示针对我们的端点的状态为 `DELETING`。一旦脚本不再列出我们的端点，你可以通过运行以下命令来继续移除系统的其余部分
- en: '[PRE48]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This will remove the custom classifier and all of the other resources deployed
    in this section. Be sure to check that all of the resources were indeed removed
    by the script!
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这将移除自定义分类器以及本节中部署的所有其他资源。请确保所有资源确实已被脚本移除！
- en: 7.11 Benefits of automation
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.11 自动化的好处
- en: 'Let’s take a moment to think through how this type of processing could benefit
    an organization. As of April 2019, Amazon.com has a product catalog with hundreds
    of millions of listings. Let’s consider a smaller retailer that lists, say, 500,000
    items across a number of different departments. Let’s assume that customers provide
    feedback on the following five channels:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间思考一下这种类型的处理如何使组织受益。截至 2019 年 4 月，Amazon.com 拥有一个包含数亿个条目的产品目录。让我们考虑一个较小的零售商，该零售商在多个不同的部门中列出，例如，500,000
    个项目。让我们假设客户在以下五个渠道中提供反馈：
- en: Twitter
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter
- en: Facebook
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook
- en: Site reviews
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站评论
- en: Email
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件
- en: Other
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他
- en: Let’s also assume that on an average day, 2% of the products will receive some
    attention on each of these channels. That means that on a daily basis, the company
    has around 50,000 items of feedback to review and process. On an annual basis,
    that equates to 18,250,000 individual pieces of feedback.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再假设在平均每天，2% 的产品将在这些渠道中的每一个都获得一些关注。这意味着公司每天大约有 50,000 项反馈需要审查和处理。按年度计算，相当于
    18,250,000 项单独的反馈。
- en: Given that it would take a human an average of, say, two minutes to process
    each piece of feedback, an individual could process only 240 of these in a standard
    eight-hour work day. This means that a team of over 200 people would be needed
    to manually process all of the feedback items.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个人平均需要两分钟来处理每条反馈，那么在标准八小时工作日中，一个人只能处理 240 条这样的反馈。这意味着需要超过 200 人的团队来手动处理所有反馈项。
- en: Our AI-enabled pipeline can handle this load easily, 24 hours a day, 365 days
    a year, dramatically reducing costs and drudgery.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的人工智能管道可以轻松处理这种负载，每天24小时，每年365天，大幅降低成本和繁琐。
- en: Hopefully this chapter has inspired you to investigate further how you can apply
    AI as a Service to tackle problems like these in your own day-to-day work.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '希望这一章能够激发您进一步探索如何在日常工作中应用人工智能作为服务来解决这些问题。 '
- en: Summary
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'There are various architectural patterns for applying AI as a Service to existing
    systems:'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将人工智能作为服务应用于现有系统有多种架构模式：
- en: Synchronous API
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步 API
- en: Asynchronous API
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步 API
- en: Stream In
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流入
- en: Fully Connected Streaming
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接流
- en: Key text fields can be extracted from a document using AWS Textract. We demonstrated
    an example in the specific case of extracting information from a passport scan.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 AWS Textract 从文档中提取关键文本字段。我们在从护照扫描中提取信息的特定案例中演示了示例。
- en: Using the example of an existing e-commerce/retail platform, we can build an
    AI-enabled data processing pipeline using Kinesis and Lambda.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以现有的电子商务/零售平台为例，我们可以使用 Kinesis 和 Lambda 构建一个人工智能数据处理的管道。
- en: AWS Translate can be used to translate languages on the fly.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Translate 可以用于即时翻译语言。
- en: Using product review data from Amazon, it is possible to build a sentiment analysis
    service.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用亚马逊的产品评论数据，可以构建一个情感分析服务。
- en: A document classifier is built using Comprehend by splitting Amazon review data
    into a training and test set.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Comprehend 构建了一个文档分类器，通过将亚马逊评论数据分为训练集和测试集来实现。
- en: Combining all of these techniques into a data processing pipeline results in
    a system that translates, filters, and classifies data. This is an example of
    how to combine several AI services to achieve a business goal.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有这些技术结合到一个数据处理管道中，结果是一个能够翻译、过滤和分类数据的系统。这是如何结合几个人工智能服务以实现商业目标的一个示例。
- en: Warning Please ensure that you fully remove all cloud resources deployed in
    this chapter in order to avoid additional charges!
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 请确保您完全删除了本章中部署的所有云资源，以避免产生额外费用！
