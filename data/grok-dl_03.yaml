- en: 'Chapter 4\. Introduction to neural learning: gradient descent'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章。神经网络学习简介：梯度下降
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**在本章中**'
- en: Do neural networks make accurate predictions?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络能做出准确的预测吗？
- en: Why measure error?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要测量误差？
- en: Hot and cold learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热和冷学习
- en: Calculating both direction and amount from error
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从误差中计算方向和数量
- en: Gradient descent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Learning is just reducing error
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习就是减少误差
- en: Derivatives and how to use them to learn
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导数及其如何用于学习
- en: Divergence and alpha
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散度和alpha
- en: “The only relevant test of the validity of a hypothesis is comparison of its
    predictions with experience.”
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “对假设有效性的唯一相关检验是其预测与经验的比较。”
- en: ''
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Milton Friedman, *Essays in Positive Economics* (University of Chicago Press,
    1953)*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*米尔顿·弗里德曼，《正面经济学论文集》（芝加哥大学出版社，1953年）*'
- en: Predict, compare, and learn
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测、比较和学习
- en: 'In [chapter 3](kindle_split_011.xhtml#ch03), you learned about the paradigm
    “predict, compare, learn,” and we dove deep into the first step: *predict*. In
    the process, you learned a myriad of things, including the major parts of neural
    networks (nodes and weights), how datasets fit into networks (matching the number
    of datapoints coming in at one time), and how to use a neural network to make
    a prediction.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](kindle_split_011.xhtml#ch03)中，你学习了“预测、比较、学习”这一范式，并且我们深入探讨了第一步：*预测*。在这个过程中，你学到了许多东西，包括神经网络的主要部分（节点和权重）、数据集如何融入网络（匹配一次进入的数据点的数量），以及如何使用神经网络进行预测。
- en: 'Perhaps this process begged the question, “How do we set weight values so the
    network predicts accurately?” Answering this question is the main focus of this
    chapter, as we cover the next two steps of the paradigm: *compare* and *learn*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这个过程引发了这样的问题：“我们如何设置权重值，以便网络能够准确预测？”回答这个问题是本章的主要内容，因为我们涵盖了范式中的下一步：*比较*和*学习*。
- en: Compare
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较
- en: Comparing gives a measurement of how much a prediction “missed” by
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较提供了一个衡量预测“错过”多少的度量
- en: Once you’ve made a prediction, the next step is to evaluate how well you did.
    This may seem like a simple concept, but you’ll find that coming up with a good
    way to measure error is one of the most important and complicated subjects of
    deep learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你做出了预测，下一步就是评估你的表现如何。这听起来可能是一个简单的概念，但你很快会发现，想出一个好的方法来衡量误差是深度学习中最重要且最复杂的问题之一。
- en: 'There are many properties of measuring error that you’ve likely been doing
    your whole life without realizing it. Perhaps you (or someone you know) amplify
    bigger errors while ignoring very small ones. In this chapter, you’ll learn how
    to mathematically teach a network to do this. You’ll also learn that error is
    always positive! We’ll consider the analogy of an archer hitting a target: whether
    the shot is too low by an inch or too high by an inch, the error is still just
    1 inch. In the neural network *compare* step, you need to consider these kinds
    of properties when measuring error.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能一直在做测量误差的事情，而你可能没有意识到这一点。也许你（或你认识的人）放大了较大的误差，而忽略了非常小的误差。在本章中，你将学习如何用数学方法教会网络这样做。你还将了解到误差始终是正的！我们将考虑一个类比，即弓箭手射中靶心：无论射击是低了一英寸还是高了一英寸，误差仍然是1英寸。在神经网络的*比较*步骤中，你需要考虑这些属性来衡量误差。
- en: 'As a heads-up, in this chapter we evaluate only one simple way of measuring
    error: *mean squared error*. It’s but one of many ways to evaluate the accuracy
    of a neural network.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 提前提醒，在本章中，我们只评估了一种简单的误差测量方法：*均方误差*。这只是评估神经网络准确性的许多方法之一。
- en: This step will give you a sense for how much you missed, but that isn’t enough
    to be able to learn. The output of the *compare* logic is a “hot or cold” type
    signal. Given some prediction, you’ll calculate an error measure that says either
    “a lot” or “a little.” It won’t tell you why you missed, what direction you missed,
    or what you should do to fix the error. It more or less says “big miss,” “little
    miss,” or “perfect prediction.” What to do about the error is captured in the
    next step, *learn*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步将给你一个关于你错过了多少的感觉，但这还不足以进行学习。*比较*逻辑的输出是一个“热或冷”类型的信号。给定一些预测，你会计算一个误差度量，它会说“很多”或“很少”。它不会告诉你为什么你错过了，你错过了哪个方向，或者你应该做什么来纠正错误。它更多或更少地说“大错”、“小错”或“完美预测”。关于错误应该做什么，将在下一步，*学习*中解决。
- en: Learn
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习
- en: Learning tells each weight how it can change to reduce the error
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习告诉每个权重如何改变以减少误差
- en: 'Learning is all about *error attribution*, or the art of figuring out how each
    weight played its part in creating error. It’s the blame game of deep learning.
    In this chapter, we’ll spend many pages looking at the most popular version of
    the deep learning blame game: *gradient descent*.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的关键在于*错误归因*，或者说找出每个权重在产生错误中扮演了什么角色的艺术。这是深度学习的指责游戏。在本章中，我们将用许多页面来探讨深度学习指责游戏中最流行的版本：*梯度下降*。
- en: At the end of the day, it results in computing a number for each weight. That
    number represents how that weight should be higher or lower in order to reduce
    the error. Then you’ll move the weight according to that number, and you’ll be
    finished.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这会导致为每个权重计算一个数字。这个数字代表该权重应该更高或更低，以便减少错误。然后你将根据这个数字移动权重，任务就完成了。
- en: 'Compare: Does your network make good predictions?'
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较：你的网络是否做出了好的预测？
- en: Let’s measure the error and find out!
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们来测量错误并找出答案！
- en: 'Execute the following code in your Jupyter notebook. It should print `0.3025`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的Jupyter笔记本中执行以下代码。它应该打印`0.3025`：
- en: '![](Images/f0050-01_alt.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0050-01_alt.jpg)'
- en: '|  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**What is the goal_pred variable?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**goal_pred变量是什么？**'
- en: Much like `input`, `goal_pred` is a number you recorded in the real world somewhere.
    But it’s usually something hard to observe, like “the percentage of people who
    *did* wear sweatsuits,” given the temperature; or “whether the batter *did* hit
    a home run,” given his batting average.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '与`input`一样，`goal_pred`是在现实世界中记录的数字。但它通常是难以观察的东西，比如“给定温度，有多少百分比的人*确实*穿了运动服”；或者“给定他的打击率，击球手*是否*击出了全垒打”。 '
- en: '**Why is the error squared?**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么错误要平方？**'
- en: Think about an archer hitting a target. When the shot hits 2 inches too high,
    how much did the archer miss by? When the shot hits 2 inches too low, how much
    did the archer miss by? Both times, the archer missed by only 2 inches. The primary
    reason to *square* “how much you missed” is that it forces the output to be *positive*.
    `(pred - goal_pred)` could be negative in some situations, *unlike actual error*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个射箭手射中靶心。当箭射高了2英寸，射箭手错过了多少？当箭射低了2英寸，射箭手错过了多少？两种情况下，射箭手都只错过了2英寸。将“你错过了多少”平方的主要原因在于，它迫使输出为*正数*。`(pred
    - goal_pred)`在某些情况下可能是负数，*与实际错误不同*。
- en: '**Doesn’t squaring make big errors (>1) bigger and small errors (<1) smaller?**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**平方不会使大错误（>1）更大，小错误（<1）更小吗？**'
- en: 'Yeah ... It’s kind of a weird way of measuring error, but it turns out that
    *amplifying* big errors and *reducing* small errors is OK. Later, you’ll use this
    error to help the network learn, and you’d rather it *pay attention* to the big
    errors and not worry so much about the small ones. Good parents are like this,
    too: they practically ignore errors if they’re small enough (breaking the lead
    on your pencil) but may go nuclear for big errors (crashing the car). See why
    squaring is valuable?'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 呃 ... 这是一种有点奇怪的测量错误的方法，但结果证明，*放大*大错误和*减少*小错误是可以接受的。稍后，你将使用这个错误来帮助网络学习，你更希望它*关注*大错误，而不是过多地担心小错误。好父母也是如此：如果错误足够小（比如铅笔断了），他们实际上会忽略错误；但如果错误很大（比如车祸），他们可能会非常严厉。你明白为什么平方是有价值的吗？
- en: '|  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Why measure error?
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么要测量错误？
- en: Measuring error simplifies the problem
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测量错误简化了问题
- en: The goal of training a neural network is to make correct predictions. That’s
    what you want. And in the most pragmatic world (as mentioned in the preceding
    chapter), you want the network to take input that you can easily calculate (today’s
    stock price) and predict things that are hard to calculate (tomorrow’s stock price).
    That’s what makes a neural network useful.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的目的是做出正确的预测。这正是你想要的。在前面章节提到的最实用的情况下，你希望网络能够接受你容易计算的输入（今天的股价）并预测难以计算的事情（明天的股价）。这就是神经网络有用的原因。
- en: It turns out that changing `knob_weight` to make the network correctly predict
    `goal_prediction` is *slightly* more complicated than changing `knob_weight` to
    make `error == 0`. There’s something more concise about looking at the problem
    this way. Ultimately, both statements say the same thing, but trying to *get the
    error to 0* seems more straightforward.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，将`knob_weight`改为使网络正确预测`goal_prediction`比将其改为使`error == 0`要复杂一些。以这种方式看待问题有一些更简洁的地方。最终，这两个陈述说的是同一件事，但试图*将错误降到0*似乎更直接。
- en: Different ways of measuring error *prioritize error differently.*
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测量错误的不同方法*优先考虑错误的方式不同*。
- en: 'If this is a bit of a stretch right now, that’s OK, but think back to what
    I said earlier: by *squaring* the error, numbers that are less than 1 get *smaller*,
    whereas numbers that are greater than 1 get *bigger*. You’re going to change what
    I call *pure error* (`pred - goal_pred`) so that bigger errors become *very* big
    and smaller errors quickly become irrelevant.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在这有点牵强，那没关系，但回想一下我之前说的：通过**平方**错误，小于1的数字会变得更小，而大于1的数字会变得更大。你将改变我所说的**纯错误**（`pred
    - goal_pred`），使得大的错误变得非常大，而小的错误迅速变得无关紧要。
- en: By measuring error this way, you can *prioritize* big errors over smaller ones.
    When you have somewhat large pure errors (say, 10), you’ll tell yourself that
    you have *very* large error (10**2 == 100); and in contrast, when you have small
    pure errors (say, 0.01), you’ll tell yourself that you have *very* small error
    (0.01**2 == 0.0001). See what I mean about prioritizing? It’s just modifying what
    you *consider to be error* so that you amplify big ones and largely ignore small
    ones.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式测量错误，你可以将大错误优先于小错误。当你有相当大的纯错误（比如说，10）时，你会告诉自己你有**非常大的错误**（10**2 == 100）；而相比之下，当你有小的纯错误（比如说，0.01）时，你会告诉自己你有**非常小的错误**（0.01**2
    == 0.0001）。你明白我说的优先级是什么吗？这只是修改你**认为的错误**，以便放大大的错误，而大量忽略小的错误。
- en: In contrast, if you took the *absolute value* instead of squaring the error,
    you wouldn’t have this type of prioritization. The error would just be the positive
    version of the pure error—which would be fine, but different. More on this later.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果你取的是错误的绝对值而不是平方错误，你就不会有这种优先级。错误就只是纯错误的正值——这很好，但不同。关于这一点，稍后还会详细说明。
- en: Why do you want only *positive* error?
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你为什么只想看到**正**的错误？
- en: Eventually, you’ll be working with millions of `input` -> `goal_prediction`
    pairs, and we’ll still want to make accurate predictions. So, you’ll try to take
    the *average error* down to 0.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你将处理数百万个`input` -> `goal_prediction`对，我们仍然希望做出准确的预测。因此，你将尝试将**平均误差**降低到0。
- en: This presents a problem if the error can be positive and negative. Imagine if
    you were trying to get the neural network to correctly predict two datapoints—two
    `input` -> `goal_prediction` pairs. If the first had an error of 1,000 and the
    second had an error of –1,000, then the *average error* would be *zero*! You’d
    fool yourself into thinking you predicted perfectly, when you missed by 1,000
    each time! That would be really bad. Thus, you want the error of *each prediction*
    to always be *positive* so they don’t accidentally cancel each other out when
    you average them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果错误可以是正的也可以是负的，这就带来了问题。想象一下，如果你试图让神经网络正确预测两个数据点——两个`input` -> `goal_prediction`对。如果第一个有1000的错误，第二个有-1000的错误，那么**平均错误**将是**零**！你会让自己误以为你预测得完美，但实际上每次都差了1000！这会很糟糕。因此，你希望每个预测的错误始终是**正**的，这样在平均时它们就不会意外地相互抵消。
- en: What’s the simplest form of neural learning?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经学习最简单的形式是什么？
- en: Learning using the hot and cold method
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用热冷方法进行学习
- en: 'At the end of the day, learning is really about one thing: adjusting `knob_weight`
    either up or down so the error is reduced. If you keep doing this and the error
    goes to 0, you’re done learning! How do you know whether to turn the knob up or
    down? Well, you try *both up and down* and see which one reduces the error! Whichever
    one reduces the error is used to update `knob_weight`. It’s simple but effective.
    After you do this over and over again, eventually error == 0, which means the
    neural network is predicting with perfect accuracy.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，学习实际上就是关于一件事：调整`knob_weight`向上或向下，以减少错误。如果你一直这样做，错误降到0，你就完成了学习！你怎么知道是向上还是向下转动旋钮？嗯，你尝试**向上和向下**转动，看看哪一个可以减少错误！哪一个减少了错误，就用来更新`knob_weight`。这很简单但很有效。你一次又一次地这样做，最终错误等于0，这意味着神经网络正在以完美的准确性进行预测。
- en: '|  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Hot and cold learning**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**热冷学习**'
- en: '*Hot and cold learning* means wiggling the weights to see which direction reduces
    the error the most, moving the weights in that direction, and repeating until
    the error gets to 0.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**热冷学习**意味着调整权重以查看哪个方向可以最大程度地减少错误，然后在这个方向上移动权重，重复这个过程，直到错误达到0。'
- en: '|  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '![](Images/f0052-01_alt.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0052-01_alt.jpg)'
- en: '![](Images/f0052-02_alt.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0052-02_alt.jpg)'
- en: '![](Images/f0053-01_alt.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0053-01_alt.jpg)'
- en: '![](Images/f0053-02_alt.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0053-02_alt.jpg)'
- en: '![](Images/f0053-03_alt.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0053-03_alt.jpg)'
- en: These last five steps are one iteration of hot and cold learning. Fortunately,
    this iteration got us pretty close to the correct answer all by itself (the new
    error is only 0.004). But under normal circumstances, we’d have to repeat this
    process many times to find the correct weights. Some people have to train their
    networks for weeks or months before they find a good enough weight configuration.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后五步是热冷学习的一次迭代。幸运的是，这个迭代本身就让我们非常接近正确答案（新的错误仅为0.004）。但在正常情况下，我们可能需要重复这个过程很多次才能找到正确的权重。有些人可能需要训练他们的网络数周或数月才能找到足够好的权重配置。
- en: 'This reveals what learning in neural networks really is: a *search problem*.
    You’re *searching* for the best possible configuration of weights so the network’s
    error falls to 0 (and predicts perfectly). As with all other forms of search,
    you might not find exactly what you’re looking for, and even if you do, it may
    take some time. Next, we’ll use hot and cold learning for a slightly more difficult
    prediction so you can see this searching in action!'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这揭示了神经网络学习实际上是什么：一个*搜索问题*。你正在*搜索*最佳可能的权重配置，以便网络的错误降到0（并做出完美预测）。与其他所有形式的搜索一样，你可能找不到你想要的，即使你找到了，也可能需要一些时间。接下来，我们将使用热冷学习进行稍微困难一些的预测，以便你可以看到这个搜索过程！
- en: Hot and cold learning
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 热冷学习
- en: This is perhaps the simplest form of learning
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这可能是最简单的学习形式
- en: 'Execute the following code in your Jupyter notebook. (New neural network modifications
    are in **bold**.) This code attempts to correctly predict 0.8:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的Jupyter笔记本中执行以下代码。（新的神经网络修改用**粗体**标出。）此代码尝试正确预测0.8：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* How much to move the weights each iteration**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 每次迭代移动权重多少**'
- en: '***2* Repeat learning many times so the error can keep getting smaller.**'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 多次重复学习，以便错误可以持续减小。**'
- en: '***3* Try up!**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 尝试向上！**'
- en: '***4* Try down!**'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 尝试向下！**'
- en: '***5* If down is better, go down!**'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 如果向下更好，就向下走！**'
- en: '***6* If up is better, go up!**'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 如果向上更好，就向上走！**'
- en: 'When I run this code, I see the following output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这段代码时，我看到了以下输出：
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* The last step correctly predicts 0.8!**'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 最后一步正确预测了0.8！**'
- en: Characteristics of hot and cold learning
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 热冷学习的特点
- en: It’s simple
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这很简单
- en: Hot and cold learning is simple. After making a prediction, you predict two
    more times, once with a slightly higher weight and again with a slightly lower
    weight. You then move `weight` depending on which direction gave a smaller `error`.
    Repeating this enough times eventually reduces `error` to 0.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 热冷学习法很简单。在做出预测之后，你预测两次更多，一次稍微增加权重，再次稍微减少权重。然后根据哪个方向给出了更小的错误来移动`权重`。重复足够多次后，最终将`错误`减少到0。
- en: '|  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Why did I iterate exactly 1,101 times?**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么我迭代了正好1,101次？**'
- en: The neural network in the example reaches 0.8 after exactly that many iterations.
    If you go past that, it wiggles back and forth between 0.8 and just above or below
    0.8, making for a less pretty error log printed at the bottom of the left page.
    Feel free to try it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例子中的神经网络在经过恰好那么多次迭代后达到了0.8。如果你超过这个范围，它将在0.8和略高于或略低于0.8之间来回摆动，导致左页底部打印的错误日志不那么美观。你可以随意尝试。
- en: '|  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Problem 1: It’s inefficient'
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题1：它不够高效
- en: You have to predict *multiple times* to make a single `knob_weight` update.
    This seems very inefficient.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须预测*多次*才能更新单个`knob_weight`。这似乎非常低效。
- en: 'Problem 2: Sometimes it’s impossible to predict the exact goal prediction'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题2：有时无法预测确切的预期目标
- en: With a set `step_amount`, unless the perfect `weight` is exactly `n*step_amount`
    away, the network will eventually overshoot by some number less than `step_amount`.
    When it does, it will then start alternating back and forth between each side
    of `goal_prediction`. Set `step_amount` to 0.2 to see this in action. If you set
    `step_amount` to 10, you’ll really break it. When I try this, I see the following
    output. It never remotely comes close to 0.8!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`step_amount`后，除非完美的`权重`正好在`n*step_amount`处，否则网络最终会超过一些小于`step_amount`的数字。当它这样做时，它将开始交替在`goal_prediction`的每一侧之间。将`step_amount`设置为0.2来观察这个效果。如果你将`step_amount`设置为10，你真的会破坏它。当我尝试这样做时，我看到了以下输出。它从未接近0.8！
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The real problem is that even though you know the correct *direction* to move
    `weight`, you don’t know the correct *amount*. Instead, you pick a fixed one at
    random (`step_amount`). Furthermore, this amount has *nothing* to do with `error`.
    Whether `error` is big or tiny, `step_amount` is the same. So, hot and cold learning
    is kind of a bummer. It’s inefficient because you predict three times for each
    `weight` update, and `step_amount` is arbitrary, which can prevent you from learning
    the correct `weight` value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的问题是，即使你知道移动`weight`的正确*方向*，你也不知道正确的*数量*。相反，你随机选择一个固定的值（`step_amount`）。此外，这个数量与`error`没有任何关系。无论`error`是大是小，`step_amount`都是相同的。所以，冷热学习有点令人沮丧。它效率低下，因为对于每个`weight`更新，你需要预测三次，而且`step_amount`是任意的，这可能会阻止你学习正确的`weight`值。
- en: What if you had a way to compute both direction and amount for each `weight`
    without having to repeatedly make predictions?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能有一种方法来计算每个`weight`的方向和数量，而无需反复进行预测，会怎么样？
- en: Calculating both direction and amount from error
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从误差计算方向和数量
- en: Let’s measure the error and find the direction and amount!
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们来测量误差并找到方向和数量！
- en: 'Execute this code in your Jupyter notebook:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的Jupyter笔记本中执行此代码：
- en: '![](Images/f0056-01_alt.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0056-01_alt.jpg)'
- en: What you see here is a superior form of learning known as *gradient descent*.
    This method allows you to (in a single line of code, shown here in **bold**) calculate
    both the *direction* and the *amount* you should change `weight` to reduce `error`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里看到的是一种称为*梯度下降*的更高级的学习形式。这种方法允许你（在单行代码中，这里用**粗体**显示）计算你应该改变`weight`以减少`error`的*方向*和*数量*。
- en: '|  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**What is direction_and_amount?**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**方向和数量是什么？**'
- en: '`direction_and_amount` represents how you want to change `weight`. The first
    part ***1*** is what I call *pure error*, which equals (`pred - goal_pred`). (More
    about this shortly.) The second part ***2*** is the multiplication by the `input`
    that performs scaling, negative reversal, and stopping, modifying the pure error
    so it’s ready to update `weight`.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`direction_and_amount`代表你想要如何改变`weight`。第一部分***1***是我所说的*纯误差*，等于（`pred - goal_pred`）。（关于这一点，稍后会有更多介绍。）第二部分***2***是乘以`input`进行的缩放、负反转和停止操作，它修改了纯误差，使其准备好更新`weight`。'
- en: '**What is the pure error?**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**纯误差是什么？**'
- en: The pure error is (`pred - goal_pred`), which indicates the raw direction and
    amount you missed. If this is a *positive* number, you predicted too *high*, and
    vice versa. If this is a *big* number, you missed by a *big* amount, and so on.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 纯误差是（`pred - goal_pred`），它表示你错过的原始方向和数量。如果这是一个*正*数，你预测得太高，反之亦然。如果这是一个*大*数，你错得很多，等等。
- en: '**What are scaling, negative reversal, and stopping?**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**缩放、负反转和停止是什么？**'
- en: These three attributes have the combined effect of translating the pure error
    into the absolute amount you want to change `weight`. They do so by addressing
    three major edge cases where the pure error isn’t sufficient to make a good modification
    to `weight`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个属性共同作用，将纯误差转换为你要改变`weight`的绝对数量。它们通过解决纯误差不足以对`weight`进行良好修改的三个主要边缘情况来实现这一点。
- en: '**What is stopping?**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**停止是什么？**'
- en: Stopping is the first (and simplest) effect on the pure error caused by multiplying
    it by `input`. Imagine plugging a CD player into your stereo. If you turned the
    volume all the way up but the CD player was off, the volume change wouldn’t matter.
    Stopping addresses this in a neural network. If `input` is 0, then it will force
    `direction_and_amount` to also be 0\. You don’t learn (change the volume) when
    `input` is 0, because there’s nothing to learn. Every `weight` value has the same
    `error`, and moving it makes no difference because `pred` is always 0.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 停止是乘以`input`后对纯误差产生的第一个（也是最简单）的影响。想象一下将CD播放器插入你的立体声音响。如果你把音量开到最大，但CD播放器是关着的，音量变化就不会起作用。停止在神经网络中解决了这个问题。如果`input`是0，那么它将迫使`direction_and_amount`也变为0。当`input`为0时，你不会学习（改变音量），因为没有东西可以学习。每个`weight`值都有相同的`error`，移动它没有区别，因为`pred`始终为0。
- en: '**What is negative reversal?**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**负反转是什么？**'
- en: This is probably the most difficult and important effect. Normally (when `input`
    is positive), moving `weight` upward makes the prediction move upward. But if
    `input` is negative, then all of a sudden `weight` changes directions! When `input`
    is negative, moving `weight` *up* makes the prediction go *down*. It’s reversed!
    How do you address this? Well, multiplying the pure error by `input` will *reverse
    the sign* of `direction_and_amount` in the event that `input` is negative. This
    is *negative reversal*, ensuring that `weight` moves in the correct direction
    even if `input` is negative.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最困难和最重要的效果。通常（当 `input` 为正时），向上移动 `weight` 会使预测向上移动。但如果 `input` 为负，那么突然
    `weight` 的方向就改变了！当 `input` 为负时，向上移动 `weight` 会使预测向下移动。它是相反的！你该如何解决这个问题？嗯，如果 `input`
    为负，将纯错误乘以 `input` 将会反转 `direction_and_amount` 的符号。这是负反转，确保即使 `input` 为负，`weight`
    也能向正确的方向移动。
- en: '**What is scaling?**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是缩放？**'
- en: Scaling is the third effect on the pure error caused by multiplying it by `input`.
    Logically, if `input` is big, your `weight` update should also be big. This is
    more of a side effect, because it often goes out of control. Later, you’ll use
    *alpha* to address when that happens.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放是乘以 `input` 后对纯错误产生的第三个影响。从逻辑上讲，如果 `input` 很大，你的 `weight` 更新也应该很大。这更多的是一个副作用，因为它经常失控。稍后，你将使用
    *alpha* 来处理这种情况。
- en: '|  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'When you run the previous code, you should see the following output:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行前面的代码时，你应该看到以下输出：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* The last steps correctly approach 0.8!**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 最后的步骤正确地接近了 0.8！**'
- en: In this example, you saw gradient descent in action in a bit of an oversimplified
    environment. Next, you’ll see it in its more native environment. Some terminology
    will be different, but I’ll code it in a way that makes it more obviously applicable
    to other kinds of networks (such as those with multiple inputs and outputs).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你在一个简化的环境中看到了梯度下降的作用。接下来，你将在其更自然的环境中看到它。一些术语可能会有所不同，但我会用一种使其更明显适用于其他类型网络（如具有多个输入和输出的网络）的方式编写代码。
- en: One iteration of gradient descent
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降的一次迭代
- en: This performs a weight update on a single training example (input->true) pair
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这对一个训练示例（输入->真实）对进行权重更新
- en: '![](Images/f0058-01_alt.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0058-01_alt.jpg)'
- en: '![](Images/f0058-02_alt.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0058-02_alt.jpg)'
- en: '![](Images/f0058-03_alt.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0058-03_alt.jpg)'
- en: '`delta` is a measurement of how much this node missed. The true prediction
    is 1.0, and the network’s prediction was 0.85, so the network was too *low* by
    0.15\. Thus, `delta` is *negative* 0.15.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`delta` 是衡量这个节点错过多少的度量。真实预测是 1.0，而网络的预测是 0.85，所以网络低估了 0.15。因此，`delta` 是负值 0.15。'
- en: 'The primary difference between gradient descent and this implementation is
    the new variable `delta`. It’s the raw amount that the node was too high or too
    low. Instead of computing `direction_and_amount` directly, you first calculate
    how much you want the output node to be different. Only then do you compute `direction_and_amount`
    to change `weight` (in step 4, now renamed `weight_delta`):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降和这种实现之间的主要区别是新的变量 `delta`。它是节点过高或过低的原始量。你首先计算你希望输出节点有多大的不同，然后才计算 `direction_and_amount`
    来改变 `weight`（在第 4 步中，现在重命名为 `weight_delta`）：
- en: '![](Images/f0059-01_alt.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0059-01_alt.jpg)'
- en: '`weight_delta` is a measure of how much a weight caused the network to miss.
    You calculate it by multiplying the weight’s output node `delta` by the weight’s
    `input`. Thus, you create each `weight_delta` by *scaling* its output node `delta`
    by the weight’s `input`. This accounts for the three aforementioned properties
    of `direction_and_amount`: scaling, negative reversal, and stopping.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`weight_delta` 是衡量一个权重导致网络错过多少的度量。你通过将权重的输出节点 `delta` 乘以权重的 `input` 来计算它。因此，你通过
    *缩放* 输出节点 `delta` 的 `input` 来创建每个 `weight_delta`。这考虑了上述三个 `direction_and_amount`
    的属性：缩放、负反转和停止。'
- en: '![](Images/f0059-02_alt.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0059-02_alt.jpg)'
- en: You multiply `weight_delta` by a small number `alpha` before using it to update
    `weight`. This lets you control how fast the network learns. If it learns too
    fast, it can update weights too aggressively and overshoot. (More on this later.)
    Note that the weight update made the same change (small increase) as hot and cold
    learning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用它来更新 `weight` 之前，你将 `weight_delta` 乘以一个小的数字 `alpha`。这让你可以控制网络学习的速度。如果它学得太快，它可能会过于激进地更新权重并超出范围。（关于这一点稍后会有更多讨论。）请注意，权重更新与热学习和冷学习产生了相同的变化（小的增加）。
- en: Learning is just reducing error
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习就是减少错误
- en: You can modify weight to reduce error
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可以修改权重以减少错误
- en: 'Putting together the code from the previous pages, we now have the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将前几页的代码组合起来，我们现在有以下内容：
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* The last steps correctly approach 0.8!**'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 最后的步骤正确地接近了0.8！**'
- en: '|  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The golden method for learning**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习的黄金法则**'
- en: This approach adjusts each `weight` in the correct direction and by the correct
    amount so that `error` reduces to 0.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法调整每个`weight`的正确方向和正确数量，以便将`error`减少到0。
- en: '|  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'All you’re trying to do is figure out the right direction and amount to modify
    `weight` so that `error` goes down. The secret lies in the `pred` and `error`
    calculations. Notice that you use `pred` *inside* the `error` calculation. Let’s
    replace the `pred` variable with the code used to generate it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你所要做的一切就是找出正确的方向和数量来修改`weight`，以便`error`下降。秘密在于`pred`和`error`的计算。注意你在`error`计算中使用`pred`。让我们用生成它的代码替换`pred`变量：
- en: '[PRE5]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This doesn’t change the value of `error` at all! It just combines the two lines
    of code and computes `error` directly. Remember that `input` and `goal_prediction`
    are fixed at 0.5 and 0.8, respectively (you set them before the network starts
    training). So, if you replace their variables names with the values, the secret
    becomes clear:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这根本不会改变`error`的值！它只是将两行代码合并，并直接计算`error`。记住，`input`和`goal_prediction`分别固定在0.5和0.8（你在网络开始训练之前设置了它们）。所以，如果你用它们的值替换变量名，秘密就变得明显了：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The secret**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**秘密**'
- en: 'For any `input` and `goal_pred`, an *exact relationship* is defined between
    `error` and `weight`, found by combining the `prediction` and `error` formulas.
    In this case:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何`input`和`goal_pred`，`error`和`weight`之间定义了一种精确的关系，这是通过结合`prediction`和`error`公式找到的。在这种情况下：
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s say you increased `weight` by 0.5\. If there’s an exact relationship between
    `error` and `weight`, you should be able to calculate how much this also moves
    `error`. What if you wanted to move `error` in a specific direction? Could it
    be done?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你增加了`weight` 0.5。如果`error`和`weight`之间存在精确的关系，你应该能够计算出这也将如何移动`error`。如果你想将`error`移动到特定的方向，这能实现吗？
- en: '![](Images/f0061-01.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0061-01.jpg)'
- en: This graph represents every value of error for every weight according to the
    relationship in the previous formula. Notice it makes a nice bowl shape. The black
    dot is at the point of *both* the current `weight` and `error`. The dotted circle
    is where you want to be (`error` == 0).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表示根据前一个公式中的关系，每个`weight`的每个`error`值。注意它形成了一个漂亮的碗形。黑色圆点位于当前`weight`和`error`的点。虚线圆圈是你想要到达的地方（`error`等于0）。
- en: '|  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Key takeaway**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: The slope points to the *bottom* of the bowl (lowest `error`) no matter where
    you are in the bowl. You can use this slope to help the neural network reduce
    the error.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率指向碗的*底部*（最低`error`），无论你在碗的哪个位置。你可以使用这个斜率来帮助神经网络减少错误。
- en: '|  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s watch several steps of learning
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们观察学习过程中的几个步骤
- en: Will we eventually find the bottom of the bowl?
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们最终能找到碗底吗？
- en: '[PRE8]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](Images/f0062-01_alt.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0062-01_alt.jpg)'
- en: '![](Images/f0062-02_alt.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0062-02_alt.jpg)'
- en: '![](Images/f0063-01_alt.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0063-01_alt.jpg)'
- en: '![](Images/f0063-02_alt.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0063-02_alt.jpg)'
- en: '![](Images/f0063-03_alt.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0063-03_alt.jpg)'
- en: Why does this work? What is weight_delta, really?
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么这能工作？权重增量（weight_delta）到底是什么？
- en: Let’s back up and talk about functions. What is a function? How d- do you understand
    one?
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们回顾一下并讨论函数。什么是函数？你如何理解它？
- en: 'Consider this function:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个函数：
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A function takes some numbers as input and gives you another number as output.
    As you can imagine, this means the function defines some sort of relationship
    between the input number(s) and the output number(s). Perhaps you can also see
    why the ability to learn a function is so powerful: it lets you take some numbers
    (say, image pixels) and convert them into other numbers (say, the probability
    that the image contains a cat).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 函数接收一些数字作为输入，并给出另一个数字作为输出。正如你可以想象的那样，这意味着函数定义了输入数字和输出数字之间的一种关系。也许你也能看出学习函数的能力为什么如此强大：它让你能够将一些数字（比如图像像素）转换为其他数字（比如图像包含猫的概率）。
- en: 'Every function has what you might call *moving parts*: pieces you can tweak
    or change to make the output the function generates different. Consider `my_function`
    in the previous example. Ask yourself, “What’s controlling the relationship between
    the input and the output of this function?” The answer is, the 2\. Ask the same
    question about the following function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 每个函数都有你可能称之为*动态部分*的部分：你可以调整或改变这些部分来使函数生成的输出不同。考虑一下前一个例子中的`my_function`。问问自己，“是什么控制着这个函数的输入和输出之间的关系？”答案是，2。对于下面的函数，也提出同样的问题：
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: What’s controlling the relationship between `input` and the output (`error`)?
    Plenty of things are—this function is a bit more complicated! `goal_pred`, `input`,
    `**2`, `weight`, and all the parentheses and algebraic operations (addition, subtraction,
    and so on) play a part in calculating the error. Tweaking any one of them would
    *change* the error. This is important to consider.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 是什么控制着`input`和输出（`error`）之间的关系？有很多东西在起作用——这个函数稍微复杂一些！`goal_pred`、`input`、`**2`、`weight`以及所有括号和代数运算（加法、减法等等）都在计算误差中发挥作用。调整其中任何一个都会*改变*误差。这一点很重要。
- en: As a thought exercise, consider changing `goal_pred` to reduce the error. This
    is silly, but totally doable. In life, you might call this (setting goals to be
    whatever your capability is) “giving up.” You’re denying that you missed! That
    wouldn’t do.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种思维练习，考虑将`goal_pred`改为减少误差。这很愚蠢，但完全可行。在现实生活中，你可能会称这（设定目标为你的能力所及）为“放弃”。你是在否认你犯了错误！这也不行。
- en: What if you changed `input` until `error` went to 0? Well, that’s akin to seeing
    the world as you want to see it instead of as it actually is. You’re changing
    the input data until you’re predicting what you want to predict (this is loosely
    how *inceptionism* works).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你改变`input`直到`error`降到0会怎样？嗯，这就像是你想看到的世界而不是实际的世界。你正在改变输入数据，直到你预测你想预测的东西（这大致是*inceptionism*的工作方式）。
- en: Now consider changing the 2, or the additions, subtractions, or multiplications.
    This is just changing how you calculate `error` in the first place. The error
    calculation is meaningless if it doesn’t actually give a good measure of how much
    you missed (with the right properties mentioned a few pages ago). This won’t do,
    either.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑改变2，或者加法、减法或乘法。这只是在最初改变计算`error`的方式。如果误差计算实际上不能给出你遗漏了多少（如几页前提到的正确属性）的良好度量，那么这种计算是没有意义的。这也不行。
- en: What’s left? The only variable remaining is `weight`. Adjusting it doesn’t change
    your perception of the world, doesn’t change your goal, and doesn’t destroy your
    error measure. Changing `weight` means the function *conforms to the patterns
    in the data*. By forcing the rest of the function to be unchanging, you force
    the function to correctly model some pattern in the data. It’s only allowed to
    modify how the network *predicts*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下什么？唯一剩下的变量是`weight`。调整它不会改变你对世界的看法，也不会改变你的目标，更不会破坏你的误差度量。改变`weight`意味着函数*符合数据中的模式*。通过迫使函数的其他部分保持不变，你迫使函数正确地模拟数据中的某些模式。它只允许修改网络*预测*的方式。
- en: 'To sum up: you modify specific parts of an error function until the `error`
    value goes to 0\. This error function is calculated using a combination of variables,
    some of which you can change (weights) and some of which you can’t (input data,
    output data, and the error logic):'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：你修改误差函数的特定部分，直到`error`值降到0。这个误差函数是通过一系列变量计算得出的，其中一些你可以改变（权重），而另一些你不能（输入数据、输出数据和误差逻辑）：
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Key takeaway**'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: You can modify *anything* in the `pred` calculation except `input`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以修改`pred`计算中的任何东西，除了`input`。
- en: '|  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We’ll spend the rest of this book (and many deep learning researchers will spend
    the rest of their lives) trying everything you can imagine on that `pred` calculation
    so that it can make good predictions. Learning is all about automatically changing
    the prediction function so that it makes good predictions—aka, so that the subsequent
    `error` goes down to 0.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的剩余部分（以及许多深度学习研究人员将花费他们余生的时间）尝试所有你能想象到的关于那个`pred`计算的方法，以便它能做出良好的预测。学习就是自动改变预测函数，使其做出良好的预测——即，使后续的`error`降到0。
- en: Now that you know what you’re allowed to change, how do you go about doing the
    changing? That’s the good stuff. That’s the machine learning, right? In the next
    section, we’re going to talk about exactly that.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道你可以改变什么，那么你如何着手进行改变呢？这就是关键所在。这就是机器学习，对吧？在下一节中，我们将详细讨论这一点。
- en: Tunnel vision on one concept
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对一个概念的狭隘视野
- en: 'Concept: Learning is adjusting the weight to reduce the error to 0'
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概念：学习就是调整权重以将误差降至0
- en: So far in this chapter, we’ve been hammering on the idea that learning is really
    just about adjusting `weight` to reduce `error` to 0\. This is the secret sauce.
    Truth be told, knowing how to do this is all about understanding the *relationship*
    between `weight` and `error`. If you understand this relationship, you can know
    how to adjust `weight` to reduce `error`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们一直在强调学习实际上只是调整`weight`以将`error`降至0的想法。这是秘密配方。说实话，知道如何做到这一点完全关乎理解`weight`和`error`之间的*关系*。如果你理解这种关系，你就可以知道如何调整`weight`以减少`error`。
- en: What do I mean by “understand the relationship”? Well, to understand the relationship
    between two variables is to understand *how changing one variable changes the
    other*. In this case, what you’re really after is the *sensitivity* between these
    two variables. Sensitivity is another name for direction and amount. You want
    to know how sensitive `error` is to `weight`. You want to know the direction and
    the amount that `error` changes when you change `weight`. This is the goal. So
    far, you’ve seen two different methods that attempt to help you understand this
    relationship.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我所说的“理解关系”是什么意思呢？嗯，理解两个变量之间的关系就是理解*一个变量如何改变另一个变量*。在这种情况下，你真正追求的是这两个变量之间的*敏感性*。敏感性是方向和数量的另一种说法。你想要知道`error`对`weight`的敏感性。你想要知道当你改变`weight`时，`error`变化的方向和数量。这就是目标。到目前为止，你已经看到了两种不同的方法，试图帮助你理解这种关系。
- en: 'When you were wiggling `weight` (hot and cold learning) and studying its effect
    on `error`, you were experimentally studying the relationship between these two
    variables. It’s like walking into a room with 15 different unlabeled light switches.
    You start flipping them on and off to learn about their relationship to various
    lights in the room. You did the same thing to study the relationship between `weight`
    and `error`: you wiggled `weight` up and down and watched for how it changed `error`.
    Once you knew the relationship, you could move `weight` in the right direction
    using two simple `if` statements:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调整`weight`（热学习和冷学习）并研究它对`error`的影响时，你实际上是在实验性地研究这两个变量之间的关系。这就像走进一个有15个不同未标记的开关的房间。你开始打开和关闭它们，以了解它们与房间内各种灯光的关系。你做了同样的事情来研究`weight`和`error`之间的关系：你上下调整`weight`，观察它如何改变`error`。一旦你知道了关系，你就可以使用两个简单的`if`语句将`weight`移动到正确的方向：
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let’s go back to the earlier formula that combined the `pred` and `error`
    logic. As mentioned, they quietly define an exact relationship between `error`
    and `weight`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到之前结合了`pred`和`error`逻辑的公式。正如提到的，它们在`error`和`weight`之间定义了一个精确的关系：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This line of code, ladies and gentlemen, is the secret. This is a formula. This
    is the relationship between `error` and `weight`. This relationship is exact.
    It’s computable. It’s universal. It is and will always be.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码，女士们先生们，是秘密。这是一个公式。这是`error`和`weight`之间的关系。这种关系是精确的。它是可计算的。它是普遍的。它现在是，将来也始终是。
- en: Now, how can you use this formula to know how to change `weight` so that `error`
    moves in a particular direction? *That* is the right question. Stop. I beg you.
    Stop and appreciate this moment. This formula is the exact relationship between
    these two variables, and now you’re going to figure out how to change one variable
    to move the other variable in a particular direction.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你如何使用这个公式来知道如何改变`weight`，以便`error`向特定方向移动？*那*才是正确的问题。停下。我恳求你。停下并欣赏这个时刻。这个公式是这两个变量之间的精确关系，现在你将找出如何改变一个变量，以使另一个变量向特定方向移动。
- en: As it turns out, there’s a method for doing this for *any* formula. You’ll use
    it to reduce error.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，有一种方法可以用于*任何*公式。你会用它来减少误差。
- en: A box with rods poking out of it
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个有杆从里面伸出来的箱子
- en: Picture yourself sitting in front of a cardboard box that has two circular rods
    sticking through two little holes. The blue rod is sticking out of the box by
    2 inches, and the red rod is sticking out of the box by 4 inches. Imagine that
    I tell you these rods were connected, but I won’t tell you in what way. You have
    to experiment to figure it out.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你坐在一个纸箱前，这个纸箱有两个圆形杆从两个小孔中伸出。蓝色杆从箱子中伸出2英寸，红色杆从箱子中伸出4英寸。想象一下，我告诉你这些杆是连接的，但我不会告诉你连接的方式。你必须通过实验来找出答案。
- en: 'So, you take the blue rod and push it in 1 inch, and watch as, while you’re
    pushing, the red rod also moves into the box by 2 inches. Then, you pull the blue
    rod back out 1 inch, and the red rod follows again, pulling out by 2 inches. What
    did you learn? Well, there seems to be a *relationship* between the red and blue
    rods. However much you move the blue rod, the red rod will move by twice as much.
    You might say the following is true:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你拿起蓝色的杆子，向里推1英寸，然后观察，在你推的过程中，红色的杆子也向盒子里移动了2英寸。然后，你把蓝色的杆子拉回1英寸，红色的杆子又跟着拉出，拉出2英寸。你学到了什么？好吧，似乎红色和蓝色杆子之间存在一种关系。无论你移动蓝色杆子多少，红色杆子都会移动两倍的距离。你可能会说以下是真的：
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As it turns out, there’s a formal definition for “When I tug on this part, how
    much does this other part move?” It’s called a *derivative*, and all it really
    means is “How much does rod X move when I tug on rod Y?”
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，对于“当我拉这部分时，另一部分移动了多少？”有一个正式的定义，它被称为导数，它实际上意味着“当我拉杆Y时，杆X移动了多少？”
- en: 'In the case of the red and blue rods, the derivative for “How much does red
    move when I tug on blue?” is 2\. Just 2\. Why is it 2? That’s the *multiplicative*
    relationship determined by the formula:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在红色和蓝色杆子的例子中，“当我拉蓝色时红色移动了多少”的导数是2。仅仅是2。为什么是2？这是由公式确定的乘法关系：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* Derivative**'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 导数**'
- en: Notice that you always have the derivative *between two variables*. You’re always
    looking to know how one variable moves when you change another one. If the derivative
    is positive, then when you change one variable, the other will move in the *same*
    direction. If the derivative is *negative*, then when you change one variable,
    the other will move in the *opposite* direction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你总是有两个变量之间的导数。你总是在寻找当你改变一个变量时，另一个变量是如何移动的。如果导数是正的，那么当你改变一个变量时，另一个将朝着相同的方向移动。如果导数是负的，那么当你改变一个变量时，另一个将朝着相反的方向移动。
- en: Consider a few examples. Because the derivative of `red_length` compared to
    `blue_length` is 2, both numbers move in the same direction. More specifically,
    red will move twice as much as blue in the same direction. If the derivative had
    been –1, red would move in the opposite direction by the same amount. Thus, given
    a function, the derivative represents the direction and the amount that one variable
    changes if you change the other variable. This is exactly what we were looking
    for.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑几个例子。因为`red_length`相对于`blue_length`的导数是2，这两个数会朝着同一方向移动。更具体地说，红色会以两倍于蓝色的速度在同一方向上移动。如果导数是-1，红色将以相同的数量朝相反方向移动。因此，给定一个函数，导数表示当你改变另一个变量时，一个变量变化的方向和数量。这正是我们一直在寻找的。
- en: 'Derivatives: Take two'
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导数：取两个
- en: Still a little unsure about them? Let’s take another perspective
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你对它们还是有点不确定？让我们从另一个角度来考虑
- en: 'I’ve heard people explain derivatives two ways. One way is all about understanding
    how one variable in a function changes when you move another variable. The other
    way says that a derivative is the slope at a point on a line or curve. As it turns
    out, if you take a function and plot it (draw it), the slope of the line you plot
    is the *same thing* as “how much one variable changes when you change the other.”
    Let me show you by plotting our favorite function:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我听说人们有两种解释导数的方式。一种方式是关于理解当你移动另一个变量时，函数中的一个变量是如何变化的。另一种方式是说导数是直线或曲线上某一点的斜率。实际上，如果你取一个函数并绘制它（画出它），你绘制的线的斜率与“当你改变另一个变量时，一个变量变化了多少”是同一回事。让我通过绘制我们最喜欢的函数来展示给你看：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Remember, `goal_pred` and `input` are fixed, so you can rewrite this function:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`goal_pred`和`input`是固定的，所以你可以重写这个函数：
- en: '[PRE17]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Because there are only two variables left that change (all the rest of them
    are fixed), you can take every `weight` and compute the `error` that goes with
    it. Let’s plot them.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 因为只剩下两个变量会改变（其余的都是固定的），你可以计算每个`权重`及其对应的`误差`。让我们把它们画出来。
- en: As you can see, the plot looks like a big U-shaped curve. Notice that there’s
    also a point in the middle where `error` == 0\. Also notice that to the right
    of that point, the slope of the line is positive, and to the left of that point,
    the slope of the line is negative. Perhaps even more interesting, the farther
    away from the *goal weight* you move, the steeper the slope gets.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，图表看起来像一个大U形曲线。注意，中间也有一个点，其中`error`等于0。注意，在那个点右边，线的斜率是正的，而在那个点左边，线的斜率是负的。也许更有趣的是，离*目标权重*越远，斜率越陡。
- en: These are useful properties. The slope’s sign gives you direction, and the slope’s
    steepness gives you amount. You can use both of these to help find the goal `weight`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是有用的属性。斜率的符号给你方向，斜率的陡峭程度给你数量。你可以使用这两个属性来帮助找到目标`权重`。
- en: Even now, when I look at that curve, it’s easy for me to lose track of what
    it represents. It’s similar to the hot and cold method for learning. If you tried
    every possible value for `weight` and plotted it out, you’d get this curve.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 即使现在，当我看那条曲线时，我也很容易失去对它所代表的含义的跟踪。这类似于学习中的冷热法。如果你尝试了所有可能的`权重`值并绘制出来，你就会得到这条曲线。
- en: And what’s remarkable about derivatives is that they can see past the big formula
    for computing `error` (at the beginning of this section) and see this curve. You
    can compute the slope (derivative) of the line for any value of `weight`. You
    can then use this slope (derivative) to figure out which direction reduces the
    error. Even better, based on the steepness, you can get at least some idea of
    how far away you are from the optimal point where the slope is zero (although
    not an exact answer, as you’ll learn more about later).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 关于导数的显著之处在于，它们可以超越计算`误差`的大公式（本节开头），看到这条曲线。你可以计算任何`权重`值的线的斜率（导数）。然后你可以使用这个斜率（导数）来确定哪个方向可以减少误差。更好的是，根据斜率的陡峭程度，你可以至少得到一些关于你距离斜率为零的优化点的距离的想法（尽管这不是一个精确的答案，你会在后面学到更多）。
- en: '![](Images/f0068-01.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0068-01.jpg)'
- en: What you really need to know
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你真正需要知道的是
- en: With derivatives, you can pick any two variables in any formula, - and know
    how they interact
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用导数，你可以从任何公式中挑选任意两个变量，并了解它们是如何相互作用的
- en: 'Take a look at this *big whopper of a function*:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个*巨大的函数*：
- en: '[PRE18]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here’s what you need to know about derivatives. For any function (even this
    whopper), you can pick any two variables and understand their relationship with
    each other. For any function, you can pick two variables and plot them on an x-y
    graph as we did earlier. For any function, you can pick two variables and compute
    how much one changes when you change the other. Thus, for any function, you can
    learn how to change one variable so that you can move another variable in a direction.
    Sorry to harp on this point, but it’s important that you know this in your bones.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 关于导数，你需要知道的是。对于任何函数（即使是这个巨大的函数），你都可以挑选任意两个变量，并理解它们之间的关系。对于任何函数，你都可以挑选两个变量，像我们之前做的那样，将它们绘制在x-y图上。对于任何函数，你都可以挑选两个变量，并计算当你改变其中一个变量时，另一个变量会发生多少变化。因此，对于任何函数，你可以学习如何改变一个变量，以便你可以将另一个变量移动到某个方向。对不起，我再次强调这个观点，但重要的是你要在内心深处知道这一点。
- en: 'Bottom line: in this book, you’re going to build neural networks. A neural
    network is really just one thing: a bunch of weights you use to compute an error
    function. And for any error function (no matter how complicated), you can compute
    the relationship between any `weight` and the final `error` of the network. With
    this information, you can change each `weight` in the neural network to reduce
    `error` down to 0—and that’s exactly what you’re going to do.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：在这本书中，你将构建神经网络。神经网络实际上就是一件事情：你用来计算误差函数的一组权重。对于任何误差函数（无论多么复杂），你都可以计算任何`权重`与网络的最终`误差`之间的关系。有了这些信息，你可以改变神经网络中的每个`权重`，将`误差`降低到0——这正是你将要做的。
- en: What you don’t really need to know
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你真正不需要知道的是
- en: Calculus
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微积分
- en: So, it turns out that learning all the methods for taking any two variables
    in any function and computing their relationship takes about three semesters of
    college. Truth be told, if you went through all three semesters so that you could
    learn how to do deep learning, you’d use only a very small subset of what you
    learned. And really, calculus is just about memorizing and practicing every possible
    derivative rule for every possible function.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，学习如何从任何函数中取任意两个变量并计算它们之间关系的方法，大约需要大学三个学期的时间。说实话，如果你上完这三个学期是为了学习如何进行深度学习，你将只会用到你所学内容的一小部分。实际上，微积分只是关于记住并练习每个可能函数的每个可能的导数规则。
- en: 'In this book, I’m going to do what I typically do in real life (cuz I’m lazy—I
    mean, efficient): look up the derivative in a reference table. All you need to
    know is what the derivative represents. It’s the relationship between two variables
    in a function so you can know how much one changes when you change the other.
    It’s just the sensitivity between two variables.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我将做我在现实生活中通常做的事情（因为我懒惰——我是说，高效）：在参考表中查找导数。你需要知道的是导数代表什么。它是函数中两个变量之间的关系，这样你就可以知道当你改变另一个变量时，一个变量会改变多少。它只是两个变量之间的敏感性。
- en: I know that was a lot of information to say, “It’s the sensitivity between two
    variables,” but it is. Note that this can include *positive sensitivity* (when
    variables move together), *negative sensitivity* (when they move in opposite directions),
    and zero sensitivity (when one stays fixed regardless of what you do to the other).
    For example, *y* = 0 * *x*. Move *x*, and *y* is always 0.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这有很多信息要说，“它是两个变量之间的敏感性”，但确实是。请注意，这可以包括**正敏感性**（当变量一起移动时），**负敏感性**（当它们朝相反方向移动时），以及零敏感性（当一个变量保持固定，无论你对另一个变量做什么）。例如，*y*
    = 0 * *x*。移动 *x*，*y* 总是 0。
- en: Enough about derivatives. Let’s get back to gradient descent.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 关于导数就说到这里。让我们回到梯度下降。
- en: How to use a derivative to learn
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用导数进行学习
- en: weight_delta is your derivative
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权重变化量是你的导数
- en: What’s the difference between `error` and the derivative of `error` and `weight`?
    `error` is a measure of how much you missed. The derivative defines the relationship
    between each weight and how much you missed. In other words, it tells how much
    changing a weight contributed to the error. So, now that you know this, how do
    you use it to move the error in a particular direction?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 误差和误差的导数以及权重的区别是什么？误差是你错过多少的衡量标准。导数定义了每个权重和你的错过之间的关系。换句话说，它告诉改变权重对误差的贡献有多大。所以，现在你知道这个，你是如何使用它来将误差移动到特定方向的呢？
- en: 'You’ve learned the relationship between two variables in a function, but how
    do you exploit that relationship? As it turns out, this is incredibly visual and
    intuitive. Check out the `error` curve again. The black dot is where `weight`
    starts out: (0.5). The dotted circle is where you want it to go: the goal weight.
    Do you see the dotted line attached to the black dot? That’s the slope, otherwise
    known as the derivative. It tells you at that point in the curve how much `error`
    changes when you change `weight`. Notice that it’s pointed downward: it’s a negative
    slope.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了函数中两个变量之间的关系，但你如何利用这种关系呢？实际上，这是非常直观的。再次查看误差曲线。黑色点代表权重开始的地方：（0.5）。虚线圆圈是你想要它去的地方：目标权重。你看到连接黑色点的虚线吗？那就是斜率，也就是导数。它告诉你在这个曲线上，当你改变权重时，误差会改变多少。注意它是向下指的：它是一个负斜率。
- en: '![](Images/f0070-01.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0070-01.jpg)'
- en: The slope of a line or curve always points in the opposite direction of the
    lowest point of the line or curve. So, if you have a negative slope, you increase
    `weight` to find the minimum of `error`. Check it out.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 直线或曲线的斜率始终指向直线或曲线最低点的相反方向。所以，如果你有一个负斜率，你增加权重以找到误差的最小值。看看吧。
- en: 'So, how do you use the derivative to find the `error` minimum (lowest point
    in the `error` graph)? You move the opposite direction of the slope—the opposite
    direction of the derivative. You can take each `weight` value, calculate its derivative
    with respect to `error` (so you’re comparing two variables: `weight` and `error`),
    and then change `weight` in the opposite direction of that slope. That will move
    you to the minimum.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你是如何使用导数来找到误差最小值（误差图中的最低点）的呢？你移动到斜率的相反方向——导数的相反方向。你可以取每个权重值，计算它与误差的导数（这样你比较的是两个变量：权重和误差），然后改变权重，使其与斜率的相反方向。这样就会移动到最小值。
- en: 'Remember back to the goal again: you’re trying to figure out the direction
    and the amount to change the weight so the error goes down. A derivative gives
    you the relationship between any two variables in a function. You use the derivative
    to determine the relationship between any weight and error. You then move the
    weight in the opposite direction of the derivative to find the lowest weight.
    Voilà! The neural network learns.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 记住目标：你试图找出改变权重的方向和数量，以便误差下降。导数给出了函数中任何两个变量之间的关系。你使用导数来确定任何权重和误差之间的关系。然后你将权重移动到导数的相反方向，以找到最低的权重。哇！神经网络就学会了。
- en: This method for learning (finding error minimums) is called *gradient descent*.
    This name should seem intuitive. You move the `weight` value opposite the gradient
    value, which reduces `error` to 0\. By *opposite*, I mean you increase the weight
    when you have a negative gradient, and vice versa. It’s like gravity.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习方法（寻找误差最小值）被称为*梯度下降*。这个名字应该看起来很直观。你将权重值移动到梯度的相反方向，从而将误差减少到0。通过*相反*，我的意思是当你有一个负梯度时，增加权重，反之亦然。就像重力一样。
- en: Look familiar?
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 看起来熟悉吗？
- en: '[PRE19]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1* Derivative (how fast the error changes, given changes in the weight)**'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 导数（误差随权重变化的速度）**'
- en: '![](Images/f0071-01_alt.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0071-01_alt.jpg)'
- en: '![](Images/f0071-02_alt.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0071-02_alt.jpg)'
- en: Breaking gradient descent
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打破梯度下降
- en: Just give me the code!
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 只给我代码！
- en: '[PRE20]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When I run this code, I see the following output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这段代码时，我看到了以下输出：
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now that it works, let’s break it. Play around with the starting `weight`, `goal_pred`,
    and `input` numbers. You can set them all to just about anything, and the neural
    network will figure out how to predict the output given the input using the weight.
    See if you can find some combinations the neural network can’t predict. I find
    that trying to break something is a great way to learn about it.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它已经起作用了，让我们来破坏它。尝试调整起始`权重`、`goal_pred`和`input`数字。你可以将它们都设置为几乎任何值，神经网络将找出如何使用权重根据输入预测输出。看看你是否能找到神经网络无法预测的组合。我发现尝试破坏某物是了解它的好方法。
- en: 'Let’s try setting `input` equal to 2, but still try to get the algorithm to
    predict 0.8\. What happens? Take a look at the output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将`input`设置为2，但仍然尝试让算法预测0.8。会发生什么？看看输出：
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Whoa! That’s not what you want. The predictions exploded! They alternate from
    negative to positive and negative to positive, getting farther away from the true
    answer at every step. In other words, every update to the weight overcorrects.
    In the next section, you’ll learn more about how to combat this phenomenon.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这不是你想要的。预测爆炸了！它们在负数和正数之间交替，每一步都离真实答案越来越远。换句话说，每次更新权重都会过度校正。在下一节中，你将了解如何对抗这种现象。
- en: Visualizing the overcorrections
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化过度校正
- en: '![](Images/f0073-01_alt.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0073-01_alt.jpg)'
- en: '![](Images/f0073-02_alt.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0073-02_alt.jpg)'
- en: '![](Images/f0073-03_alt.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0073-03_alt.jpg)'
- en: Divergence
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发散
- en: Sometimes neural networks explode in value. Oops?
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 有时候神经网络的价值会爆炸。哎呀？
- en: '![](Images/f0074-01_alt.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0074-01_alt.jpg)'
- en: 'What really happened? The explosion in the error was caused by the fact that
    you made the input larger. Consider how you’re updating the weight:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 究竟发生了什么？误差爆炸是由你使输入变大的事实引起的。考虑你是如何更新权重的：
- en: '[PRE23]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If the input is sufficiently large, this can make the weight update large even
    when the error is small. What happens when you have a large weight update and
    a small error? The network overcorrects. If the new error is even bigger, the
    network overcorrects even more. This causes the phenomenon you saw earlier, called
    *divergence*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入足够大，即使误差很小，这也可能导致权重更新很大。当你有一个大的权重更新和小的误差时会发生什么？网络会过度校正。如果新的误差更大，网络会过度校正得更多。这导致了你之前看到的称为*发散*的现象。
- en: If you have a big input, the prediction is very sensitive to changes in the
    weight (because `pred = input * weight`). This can cause the network to overcorrect.
    In other words, even though the weight is still starting at 0.5, the derivative
    at that point is very steep. See how tight the U-shaped error curve is in the
    graph?
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个大的输入，预测对权重的变化非常敏感（因为`pred = input * weight`）。这可能导致网络过度校正。换句话说，即使权重仍然从0.5开始，该点的导数非常陡峭。看看图中U形误差曲线有多紧？
- en: This is really intuitive. How do you predict? By multiplying the input by the
    weight. So, if the input is huge, small changes in the weight will cause changes
    in the prediction. The error is very sensitive to the weight. In other words,
    the derivative is really big. How do you make it smaller?
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很直观。你是如何预测的？通过将输入乘以权重。所以，如果输入很大，权重的微小变化会导致预测的变化。误差对权重非常敏感。换句话说，导数非常大。你是如何使它变小的？
- en: Introducing alpha
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入alpha
- en: It’s the simplest way to prevent overcorrecting weight updates
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这是防止过度校正权重更新的最简单方法
- en: What’s the problem you’re trying to solve? That if the input is too big, then
    the weight update can overcorrect. What’s the symptom? That when you overcorrect,
    the new derivative is even larger in magnitude than when you started (although
    the sign will be the opposite).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你试图解决的问题是什么？如果输入太大，那么权重更新可能会过度纠正。症状是什么？当你过度纠正时，新的导数在幅度上甚至比开始时更大（尽管符号将是相反的）。
- en: Stop and consider this for a second. Look again at the graph in the previous
    section to understand the symptom. Step 2 is even farther away from the goal,
    which means the derivative is even greater in magnitude. This causes step 3 to
    be even farther from the goal than step 2, and the neural network continues like
    this, demonstrating divergence.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 停下来思考一下。再次看看上一节中的图表，以了解症状。第二步离目标更远，这意味着导数的幅度更大。这导致第三步比第二步更远离目标，神经网络就这样继续下去，表现出发散。
- en: 'The symptom is this overshooting. The solution is to multiply the weight update
    by a fraction to make it smaller. In most cases, this involves multiplying the
    weight update by a single real-valued number between 0 and 1, known as *alpha*.
    Note: this has no effect on the core issue, which is that the input is larger.
    It will also reduce the weight updates for inputs that aren’t too large.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 症状就是这种过度调整。解决方案是将权重更新乘以一个分数以使其更小。在大多数情况下，这涉及到将权重更新乘以一个介于0和1之间的单个实数，称为*alpha*。注意：这不会对核心问题产生影响，即输入较大。它也会减少不太大的输入的权重更新。
- en: Finding the appropriate alpha, even for state-of-the-art neural networks, is
    often done by guessing. You watch the error over time. If it starts diverging
    (going up), then the alpha is too high, and you decrease it. If learning is happening
    too slowly, then the alpha is too low, and you increase it. There are other methods
    than simple gradient descent that attempt to counter for this, but gradient descent
    is still very popular.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即使是最新技术的神经网络，找到合适的alpha通常是通过猜测来完成的。你观察错误随时间的变化。如果它开始发散（上升），那么alpha太高了，你就减小它。如果学习进展得太慢，那么alpha太低了，你就增加它。有其他方法比简单的梯度下降更有效地解决这个问题，但梯度下降仍然非常受欢迎。
- en: Alpha in code
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码中的alpha
- en: Where does our “alpha” parameter come into play?
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的“alpha”参数在哪里发挥作用？
- en: 'You just learned that alpha reduces the weight update so it doesn’t overshoot.
    How does this affect the code? Well, you were updating the weights according to
    the following formula:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚了解到alpha减少了权重更新，以防止它过度调整。这对代码有什么影响？嗯，你根据以下公式更新权重：
- en: '[PRE24]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Accounting for alpha is a rather small change, as shown next. Notice that if
    alpha is small (say, 0.01), it will reduce the weight update considerably, thus
    preventing it from overshooting:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到alpha的变化相对较小，如以下所示。注意，如果alpha很小（比如说，0.01），它将大大减少权重更新，从而防止它过度调整：
- en: '[PRE25]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'That was easy. Let’s install alpha into the tiny implementation from the beginning
    of this chapter and run it where `input` = 2 (which previously didn’t work):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。让我们将alpha安装到本章开头的小型实现中，并在`input` = 2的地方运行它（之前这不起作用）：
- en: '![](Images/f0076-01_alt.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0076-01_alt.jpg)'
- en: Voilà! The tiniest neural network can now make good predictions again. How did
    I know to set alpha to 0.1? To be honest, I tried it, and it worked. And despite
    all the crazy advancements of deep learning in the past few years, most people
    just try several orders of magnitude of alpha (10, 1, 0.1, 0.01, 0.001, 0.0001)
    and then tweak it from there to see what works best. It’s more art than science.
    There are more advanced ways to get to later, but for now, try various alphas
    until you get one that seems to work pretty well. Play with it.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！现在最小的神经网络又可以做出良好的预测了。我是怎么知道将alpha设置为0.1的呢？说实话，我试了，而且它有效。尽管过去几年深度学习取得了疯狂的发展，但大多数人只是尝试几个alpha的量级（10，1，0.1，0.01，0.001，0.0001），然后从那里调整以查看哪个效果最好。这更多的是艺术而不是科学。有更多高级的方法可以到达那里，但现在，尝试各种alpha，直到找到一个似乎效果很好的。玩一玩。
- en: Memorizing
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记忆
- en: It’s time to really learn this stuff
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 是时候真正学习这些内容了
- en: 'This may sound a bit intense, but I can’t stress enough the value I’ve found
    from this exercise: see if you can build the code from the previous section in
    a Jupyter notebook (or a .py file, if you must) from memory. I know that might
    seem like overkill, but I (personally) didn’t have my “click” moment with neural
    networks until I was able to perform this task.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有点过于激烈，但我无法强调我从这个练习中发现的价值的多少：尝试从记忆中在Jupyter笔记本（或者如果你必须的话，一个.py文件）中构建上一节中的代码。我知道这可能看起来有些过度，但（就我个人而言）直到我能够完成这个任务，我都没有在神经网络上有“顿悟”的时刻。
- en: Why does this work? Well, for starters, the only way to know you’ve gleaned
    all the information necessary from this chapter is to try to produce it from your
    head. Neural networks have lots of small moving parts, and it’s easy to miss one.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这会起作用呢？首先，唯一知道你是否已经从本章中获取了所有必要信息的方法就是尝试从你的脑海中复现它。神经网络有很多小的移动部件，很容易遗漏其中一个。
- en: Why is this important for the rest of the book? In the following chapters, I’ll
    be referring to the concepts discussed in this chapter at a faster pace so that
    I can spend plenty of time on the newer material. It’s vitally important that
    when I say something like “Add your alpha parameterization to the weight update,”
    you immediately recognize which concepts from this chapter I’m referring to.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于本书的其余部分为什么很重要呢？在接下来的章节中，我会以更快的速度引用本章讨论的概念，这样我就可以有更多的时间来讲解新的材料。当我提到“将你的alpha参数化添加到权重更新中”这样的内容时，你能够立即识别出我指的是本章中的哪些概念，这是至关重要的。
- en: All that is to say, memorizing small bits of neural network code has been hugely
    beneficial for me personally, as well as for many individuals who have taken my
    advice on this subject in the past.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我个人以及许多过去接受过我关于这个主题建议的人，通过记忆神经网络代码的小片段获得了巨大的益处。
