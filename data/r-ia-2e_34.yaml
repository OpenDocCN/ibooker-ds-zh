- en: Appendix F. Working with large datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录F. 处理大型数据集
- en: R holds all of its objects in virtual memory. For most of us, this design decision
    has led to a zippy interactive experience, but for analysts working with large
    datasets, it can lead to slow program execution and memory-related errors.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: R将其所有对象存储在虚拟内存中。对于我们大多数人来说，这个设计决策导致了快速的交互式体验，但对于处理大型数据集的分析师来说，它可能导致程序执行缓慢和内存相关错误。
- en: Memory limits depend primarily on the R build (32- versus 64-bit) and the OS
    version involved. Error messages starting with “cannot allocate vector of size”
    typically indicate a failure to obtain sufficient contiguous memory, whereas error
    messages starting with “cannot allocate vector of length” indicate that an address
    limit has been exceeded. When working with large datasets, try to use a 64-bit
    build if at all possible. See `help(Memory)` for more information.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 内存限制主要取决于R的构建版本（32位与64位）以及涉及的操作系统版本。以“无法分配大小为”开头的错误信息通常表明无法获得足够的连续内存，而以“无法分配长度为”开头的错误信息则表明已超出地址限制。当处理大型数据集时，如果可能的话，尽量使用64位构建版本。查看`help(Memory)`获取更多信息。
- en: 'There are three issues to consider when working with large datasets: efficient
    programming to speed execution, storing data externally to limit memory issues,
    and using specialized statistical routines designed to efficiently analyze massive
    amounts of data. First, we’ll consider simple solutions for each. Then we’ll turn
    to more comprehensive (and complex) solutions for working with *big* data.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大型数据集时，有三个问题需要考虑：提高编程效率以加快执行速度、外部存储数据以限制内存问题，以及使用专门设计的统计程序以有效地分析大量数据。首先，我们将考虑每个问题的简单解决方案。然后，我们将转向更全面（且更复杂）的解决方案，以处理*大数据*。
- en: F.1 Efficient programming
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F.1 高效编程
- en: 'Several programming tips can help you improve performance when working with
    large datasets:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 几个编程技巧可以帮助你在处理大型数据集时提高性能：
- en: Vectorize calculations when possible. Use R’s built-in functions for manipulating
    vectors, matrices, and lists (for example, `ifelse`, `colMeans`, and `rowSums`),
    and avoid loops (`for` and `while`) when feasible.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当可能时，将计算向量化。使用R内置的用于操作向量、矩阵和列表的函数（例如，`ifelse`、`colMeans`和`rowSums`），并在可行的情况下避免使用循环（`for`和`while`）。
- en: Use matrices rather than data frames (they have less overhead).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用矩阵而不是数据框（它们的开销更小）。
- en: When importing delimited text files, use an optimized function like `fread()`
    from the `data.table` package or `vroom()` from the `vroom` package. They are
    *considerably* faster than base R’s `read.table()` function.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当导入分隔符文本文件时，使用来自`data.table`包的优化函数`fread()`或来自`vroom`包的`vroom()`。它们比基础R的`read.table()`函数快得多。
- en: Correctly size objects initially, rather than growing them from smaller objects
    by appending values.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始时正确设置对象的大小，而不是通过追加值从小对象增长。
- en: Use parallelization for repetitive, independent, and numerically intensive tasks.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于重复的、独立的和数值密集型任务，使用并行化。
- en: Test programs on a sample of the data to optimize code and remove bugs before
    attempting a run on the full dataset.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在尝试在完整数据集上运行之前，在数据样本上测试程序以优化代码并删除错误。
- en: Delete temporary objects and objects that are no longer needed. The call `rm(list=ls())`
    removes all objects from memory, providing a clean slate. Specific objects can
    be removed with `rm(object)`. After removing large objects, a call to `gc()` will
    initiate garbage collection, ensuring that the objects are removed from memory.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除临时对象和不再需要的对象。调用`rm(list=ls())`将从内存中删除所有对象，提供一个干净的起点。可以使用`rm(object)`删除特定对象。删除大对象后，调用`gc()`将启动垃圾回收，确保对象从内存中删除。
- en: 'Use the function .`ls.objects()` described in Jeromy Anglim’s blog entry “Memory
    Management in R: A Few Tips and Tricks” ([http://mng.bz/xGpq](http://mng.bz/xGpq))
    to list all workspace objects sorted by size (MB). This function will help you
    find and deal with memory hogs.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Jeromy Anglim在其博客条目“R中的内存管理：一些技巧和窍门”中描述的函数`.ls.objects()`来按大小（MB）排序列出所有工作空间对象（[http://mng.bz/xGpq](http://mng.bz/xGpq)）。此函数将帮助你找到并处理内存消耗者。
- en: Profile your programs to see how much time is being spent in each function.
    You can accomplish this with the `Rprof()`and `summaryRprof()` functions. The
    `system.time()` function can also help. The `profr` and `prooftools` packages
    provide functions that can help you analyze profiling output.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析程序以查看每个函数花费了多少时间。你可以使用`Rprof()`和`summaryRprof()`函数完成此操作。`system.time()`函数也可以有所帮助。`profr`和`prooftools`包提供了可以帮助你分析分析输出的函数。
- en: Use compiled external routines to speed up program execution. You can use the
    `Rcpp` package to transfer R objects to C++ functions and back when more optimized
    subroutines are needed.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用编译的外部例程来加速程序执行。当需要更优化的子例程时，您可以使用`Rcpp`包将R对象传输到C++函数并返回。
- en: Section 20.5 offers examples of vectorization, efficient data input, correctly
    sizing objects, and parallelization.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第20.5节提供了向量化、高效数据输入、正确设置对象大小和并行化的示例。
- en: With large datasets, increasing code efficiency will only get you so far. When
    you bump up against memory limits or slow code execution, consider upgrading your
    hardware. You can also store your data externally and use specialized analysis
    routines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型数据集，提高代码效率只能走这么远。当您遇到内存限制或代码执行缓慢时，请考虑升级您的硬件。您还可以将数据存储在外部，并使用专门的分析例程。
- en: F.2 Storing data outside of RAM
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F.2 在RAM之外存储数据
- en: Several packages are available for storing data outside of R’s main memory.
    The strategy involves storing data in external databases or in binary flat files
    on disk and then accessing portions as needed. Table F.1 lists several useful
    packages.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个包可用于在R的主内存之外存储数据。该策略涉及将数据存储在外部数据库或磁盘上的二进制平面文件中，然后按需访问。表F.1列出了几个有用的包。
- en: Table F.1 R packages for accessing large datasets
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表F.1 R包用于访问大型数据集
- en: '| Package | Description |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 包 | 描述 |'
- en: '| `bigmemory` | Supports the creation, storage, access, and manipulation of
    massive matrices. Matrices are allocated to shared memory and memory-mapped files.
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `bigmemory` | 支持创建、存储、访问和操作庞大矩阵。矩阵分配到共享内存和内存映射文件。 |'
- en: '| `disk.frame` | A disk-based data manipulation framework for working with
    larger-than-RAM datasets |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `disk.frame` | 一个基于磁盘的数据操作框架，用于处理大于RAM的数据集 |'
- en: '| `ff` | Provides data structures that are stored on disk but behave as if
    they’re in RAM |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `ff` | 提供了存储在磁盘上的数据结构，但表现得像它们在RAM中一样 |'
- en: '| `filehash` | Implements a simple key-value database where character string
    keys are associated with data values stored on disk |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `filehash` | 实现了一个简单的键值数据库，其中字符串键与存储在磁盘上的数据值相关联 |'
- en: '| `ncdf`, `ncdf4` | Provide an interface to Unidata NetCDF data files |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `ncdf`、`ncdf4` | 提供了访问Unidata NetCDF数据文件的接口 |'
- en: '| `RODBC`, `RMySQL`, `ROracle`, `RPostgreSQL`, `RSQLite` | Each provides access
    to external relational database management systems. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `RODBC`、`RMySQL`、`ROracle`、`RPostgreSQL`、`RSQLite` | 每个都提供了访问外部关系型数据库管理系统的接口。
    |'
- en: These packages help overcome R’s memory limits on data storage. But you also
    need specialized methods when you attempt to analyze large datasets in a reasonable
    length of time. Some of the most useful are described next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包有助于克服R在数据存储上的内存限制。但是，当您尝试在合理的时间内分析大型数据集时，您还需要专门的方法。以下是一些最有用的方法。
- en: F.3 Analytic packages for out-of-memory data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F.3 内存外数据分析包
- en: 'R provides several packages for the analysis of large datasets:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: R提供了几个用于分析大型数据集的包：
- en: The `biglm` and `speedglm` packages fit linear and generalized linear models
    to large datasets in a memory-efficient manner. This offers `lm()` and `glm()`
    type functionality when dealing with massive datasets.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`biglm`和`speedglm`包以内存高效的方式对大型数据集进行线性模型和广义线性模型的拟合。这为处理大型数据集提供了`lm()`和`glm()`类型的功能。'
- en: Several packages offer analytic functions for working with the massive matrices
    produced by the `bigmemory` package. The `biganalytics` package offers k-means
    clustering, column statistics, and a wrapper to `biglm`. The `bigrf` package can
    be used to fit classification and regression forests. The `bigtabulate` package
    provides `table()`, `split()`, and `tapply()` functionality, and the `bigalgebra`
    package provides advanced linear algebra functions.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几个包提供了用于处理由`bigmemory`包生成的庞大矩阵的分析函数。`biganalytics`包提供了k-means聚类、列统计以及`biglm`的包装器。`bigrf`包可用于拟合分类和回归森林。`bigtabulate`包提供了`table()`、`split()`和`tapply()`功能，而`bigalgebra`包提供了高级线性代数函数。
- en: The `biglars` package offers least-angle regression, lasso, and stepwise regression
    for datasets that are too large to be held in memory, when used in conjunction
    with the `ff` package.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与`ff`包一起使用时，`biglars`包为太大而无法存储在内存中的数据集提供了最小角回归、lasso和逐步回归。
- en: The `data.table` package provides an enhanced version of `data.frame` that includes
    faster aggregation; faster ordered and overlapping range joins; and faster column
    addition, modification, and deletion by reference by group (without copies). You
    can use the `data.table` structure with large datasets (for example, 100 GB in
    RAM), and it’s compatible with any R function expecting a data frame.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.table` 包提供了一个 `data.frame` 的增强版本，包括更快的聚合；更快的有序和重叠范围连接；以及通过引用按组快速添加、修改和删除列（无需复制）。您可以使用
    `data.table` 结构处理大型数据集（例如，RAM 中 100 GB），并且它与任何期望数据框的 R 函数兼容。'
- en: Each of these packages accommodates large datasets for specific purposes and
    is relatively easy to use. More comprehensive solutions for analyzing data in
    the terabyte range are described next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包针对特定目的适应大型数据集，并且相对容易使用。接下来将描述用于分析千兆级数据的更全面解决方案。
- en: F.4 Comprehensive solutions for working with enormous datasets
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F.4 与海量数据集一起工作的综合解决方案
- en: When I wrote the first edition of this book, the most I could say in this section
    was, “Well, we’re trying.” Since that time, there has been an explosion of projects
    that combine *high-performance computing* (HPC) and the R language. This section
    provides pointers to some of the more popular approaches to using R with terabyte-class
    datasets. Each requires some familiarity with HPC and the use of other software
    platforms such as Hadoop (a free Java-based software framework for processing
    large datasets in a distributed computing environment).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我撰写这本书的第一版时，在这个部分我能说的最多就是，“好吧，我们正在尝试。”从那时起，结合高性能计算（HPC）和 R 语言的项目的数量激增。本节提供了使用
    R 处理千兆级数据集的一些更流行方法的指针。每种方法都需要对 HPC 和其他软件平台（如 Hadoop，一个用于在分布式计算环境中处理大型数据集的免费 Java
    软件框架）的使用有一定了解。
- en: Table F.2 describes open source approaches for using R with massive datasets.
    The most popular approaches are RHadoop and sparklyr.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表 F.2 描述了使用 R 处理大规模数据集的开源方法。最流行的方法是 RHadoop 和 sparklyr。
- en: Table F.2 R open source platforms for big data
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 F.2 R 开源大数据平台
- en: '| Approach | Description |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 |'
- en: '| RHadoop | Software for managing and analyzing data with Hadoop in an R environment.
    Consists of five interconnected packages: `rhbase`, `ravro`, `rhdfs`, `plyrmr`,
    and `rmr2`. See [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)
    for details. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| RHadoop | 在 R 环境中使用 Hadoop 管理和分析数据的软件。由五个相互连接的包组成：`rhbase`、`ravro`、`rhdfs`、`plyrmr`
    和 `rmr2`。有关详细信息，请参阅 [https://github.com/RevolutionAnalytics/RHadoop/wiki](https://github.com/RevolutionAnalytics/RHadoop/wiki)。
    |'
- en: '| RHIPE | *R and Hadoop Integrated Programming Environment*. An R package that
    allows users to run Hadoop MapReduce jobs from within R. See [https://github.com/delta-rho/RHIPE](https://github.com/delta-rho/RHIPE).
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| RHIPE | *R 和 Hadoop 集成编程环境*。一个 R 包，允许用户在 R 中运行 Hadoop MapReduce 作业。有关信息，请参阅
    [https://github.com/delta-rho/RHIPE](https://github.com/delta-rho/RHIPE)。 |'
- en: '| Hadoop Streaming | Hadoop streaming ([https://hadoop.apache.org/docs/r1.2.1/streaming.html](https://hadoop.apache.org/docs/r1.2.1/streaming.html))
    is a utility for creating and running Map/Reduce jobs with any language as the
    mapper and/or the reducer. The `HadoopStreaming` package supports writing these
    scripts in R. See [https://cran.rstudio.com/web/packages/HadoopStreaming/.](https://cran.rstudio.com/web/packages/HadoopStreaming/)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Hadoop Streaming | Hadoop streaming ([https://hadoop.apache.org/docs/r1.2.1/streaming.html](https://hadoop.apache.org/docs/r1.2.1/streaming.html))
    是一个用于创建和运行以任何语言作为映射器或/或归约器的 Map/Reduce 作业的实用程序。`HadoopStreaming` 包支持使用 R 编写这些脚本。有关信息，请参阅
    [https://cran.rstudio.com/web/packages/HadoopStreaming/](https://cran.rstudio.com/web/packages/HadoopStreaming/)。
    |'
- en: '| RHIVE | An R extension facilitating distributed computing via HIVE query.
    RHIVE supports easy use of HIVE SQL in R and the use of R objects and R functions
    in Hive. See [https://github.com/nexr/RHive](https://github.com/nexr/RHive) for
    information. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| RHIVE | 一个通过 HIVE 查询促进分布式计算的 R 扩展。RHIVE 支持在 R 中轻松使用 HIVE SQL，以及在 Hive 中使用
    R 对象和 R 函数。有关信息，请参阅 [https://github.com/nexr/RHive](https://github.com/nexr/RHive)。
    |'
- en: '| pbdR | “Programming with Big Data in R.” Packages enabling data parallelism
    in R through a simple interface to scalable, high-performance libraries (such
    as MPI, ZeroMQ, ScaLAPACK, and netCDF4, and PAPI). The pbdR software also supports
    the single program, multiple data (SPMD) model on large-scale computing clusters.
    See [http://r-pbd.org](http://r-pbd.org) or details. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| pbdR | “使用 R 进行大数据编程。”通过简单界面访问可扩展、高性能库（如 MPI、ZeroMQ、ScaLAPACK 和 netCDF4 以及
    PAPI）的包，实现 R 中的数据并行。pbdR 软件还支持在大型计算集群上使用单程序多数据（SPMD）模型。有关详细信息，请参阅 [http://r-pbd.org](http://r-pbd.org)。
    |'
- en: '| sparklyr | A package providing an R interface for Apache Spark. Supports
    connecting to local and remote Apache Spark clusters, provides a `''dplyr''`-compatible
    back end and an interface to Spark’s machine learning algorithms. See [https://cran.r-project.org/web/packages/sparklyr](https://cran.r-project.org/web/packages/sparklyr)
    for details. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| sparklyr | 一个提供 Apache Spark R 接口的包。支持连接到本地和远程 Apache Spark 集群，提供 `''dplyr''`
    兼容的后端和 Spark 机器学习算法的接口。有关详细信息，请参阅 [https://cran.r-project.org/web/packages/sparklyr](https://cran.r-project.org/web/packages/sparklyr)。|'
- en: Cloud services offer a ready-made, scalable infrastructure with potentially
    enormous memory and storage resources. The most popular cloud services for R users
    are provided by Amazon, Microsoft, and Google. While not a cloud solution, per
    se, Oracle also offers big data computing for R users (see table F.3).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供了一种现成的、可扩展的基础设施，具有可能巨大的内存和存储资源。为 R 用户最受欢迎的云服务由亚马逊、微软和谷歌提供。虽然不是云解决方案，但甲骨文也向
    R 用户提供了大数据计算（见表 F.3）。
- en: Table F.3 Commercial platforms for big data projects
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 F.3 大数据项目商业平台
- en: '| Approach | Description |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 |'
- en: '| Amazon Web Services (AWS) | There are several approaches to using R with
    AWS. The R package `paws` (package for Amazon Web Services in R) provides a full
    suite of AWS services from within R (see [https://paws-r.github.io/](https://paws-r.github.io/)).
    The `aws.ec2` package is a simple client package for the AWS EC2 REST API ([https://github.com/cloudyr/aws.ec2](https://github.com/cloudyr/aws.ec2)).
    Louis Aslett maintains Amazon Machine Images that make deploying an RStudio Server
    into the Amazon EC2 service relatively easy ([https://www.louisaslett.com/RStudio_AMI/](https://www.louisaslett.com/RStudio_AMI/)).
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊网络服务 (AWS) | 使用 R 与 AWS 有几种方法。R 包 `paws`（R 中 AWS 的包）提供了从 R 内部访问 AWS 服务的完整套件（见
    [https://paws-r.github.io/](https://paws-r.github.io/)）。`aws.ec2` 包是 AWS EC2 REST
    API 的简单客户端包（[https://github.com/cloudyr/aws.ec2](https://github.com/cloudyr/aws.ec2)）。Louis
    Aslett 维护的亚马逊机器镜像使得将 RStudio 服务器部署到亚马逊 EC2 服务变得相对简单（[https://www.louisaslett.com/RStudio_AMI/](https://www.louisaslett.com/RStudio_AMI/)）。|'
- en: '| Microsoft Azure | Data Science Virtual Machines are VM images on the Azure
    cloud platform, built for data science. They include Microsoft R Open, Microsoft
    Machine Learning Server, RStudio Desktop, and RStudio Server. See [https://azure.microsoft.com/en-us/
    services/virtual-machines/data-science-virtual-machines/](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/)
    for details. Also see `AzureR`, a family of packages for working with Azure from
    R ([https://github.com/Azure/AzureR](https://github.com/Azure/AzureR)). |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 微软 Azure | 数据科学虚拟机是 Azure 云平台上的虚拟机镜像，专为数据科学构建。它们包括 Microsoft R Open、Microsoft
    机器学习服务器、RStudio 桌面和 RStudio 服务器。有关详细信息，请参阅 [https://azure.microsoft.com/en-us/
    services/virtual-machines/data-science-virtual-machines/](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/)。另外，请参阅
    `AzureR`，这是一系列用于从 R 操作 Azure 的包（[https://github.com/Azure/AzureR](https://github.com/Azure/AzureR)）。|'
- en: '| Google Cloud Services | The `bigrquery` package provides an interface to
    Google’s BigQuery API ([https://github.com/r-dbi/bigrquery](https://github.com/r-dbi/bigrquery)).
    The `googleComputeEngineR` package provides an R interface to the Google Cloud
    Compute Engine API. It makes the deployment of cloud resources for R (including
    RStudio and Shiny) as simple as possible ([https://cloudyr.github.io/googleComputeEngineR/](https://cloudyr.github.io/googleComputeEngineR/)).
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 谷歌云服务 | `bigrquery` 包提供了对谷歌 BigQuery API 的接口（[https://github.com/r-dbi/bigrquery](https://github.com/r-dbi/bigrquery)）。`googleComputeEngineR`
    包提供了 R 对 Google Cloud Compute Engine API 的接口。它使得为 R（包括 RStudio 和 Shiny）部署云资源变得尽可能简单（[https://cloudyr.github.io/googleComputeEngineR/](https://cloudyr.github.io/googleComputeEngineR/)）。|'
- en: '| Oracle R Advanced Analytics for Hadoop | This is a collection of R packages
    that provide interfaces to HIVE tables, Hadoop infrastructure, Oracle database
    tables, and the local R environment. It also includes a wide range of predictive
    analytic techniques. See the Oracle help files ([http://mng.bz/K4jn](http://mng.bz/K4jn)).
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Oracle R 高级分析 Hadoop | 这是一个 R 包集合，提供了对 HIVE 表、Hadoop 基础设施、Oracle 数据库表和本地
    R 环境的接口。它还包括一系列预测分析技术。请参阅 Oracle 帮助文件（[http://mng.bz/K4jn](http://mng.bz/K4jn)）。|'
- en: In addition to the resources in table F.3, consider the cloudyr project ([https://cloudyr.github.io/](https://cloudyr.github.io/)),
    an initiative designed to make cloud computing with R easier. It contains a wide
    range of packages for melding the advantages of R and cloud services.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表F.3中的资源外，考虑cloudyr项目（[https://cloudyr.github.io/](https://cloudyr.github.io/）），这是一个旨在使使用R进行云计算更容易的倡议。它包含了一系列用于融合R和云服务优势的包。
- en: 'Working with datasets in the gigabyte-to-terabyte range can be challenging
    in any language. Each of these approaches comes with a significant learning curve.
    The book *Big Data Analytics with R and Hadoop* (Prajapati, 2013) is a useful
    resource for using R with Hadoop. For Spark, Mastering Spark with R ([https://therinspark.com/](https://therinspark.com/))
    and Using Spark from R for Performance with Arbitrary Code ([https://sparkfromr.com/](https://sparkfromr.com/))
    are very helpful. Finally, see the CRAN Task View: “*High-Performance and Parallel
    Computing with R*” ([https://cran.r-project.org/web/views/HighPerformanceComputing.html](https://cran.r-project.org/web/views/HighPerformanceComputing.html)).
    This is an area of rapid change and development, so be sure to check the task
    view often.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何语言中处理从千兆到太字节范围的数据库集都可能具有挑战性。每种方法都伴随着一个显著的学习曲线。书籍《使用R和Hadoop进行大数据分析》（Prajapati，2013）是使用R与Hadoop的一个有用资源。对于Spark，阅读《精通Spark与R》（[https://therinspark.com/](https://therinspark.com/））和《使用任意代码从R中利用Spark进行性能优化》（[https://sparkfromr.com/](https://sparkfromr.com/））都非常有帮助。最后，查看CRAN任务视图：“*使用R进行高性能和并行计算*”（[https://cran.r-project.org/web/views/HighPerformanceComputing.html](https://cran.r-project.org/web/views/HighPerformanceComputing.html)）。这是一个快速变化和发展的领域，所以请确保经常检查任务视图。
