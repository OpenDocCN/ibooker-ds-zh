- en: Chapter 3\. Recommending Music and the Audioscrobbler Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。推荐音乐与Audioscrobbler数据集
- en: The recommender engine is one of the most popular example of large-scale machine
    learning; for example, most people are familiar with Amazon’s. It is a common
    denominator because recommender engines are everywhere, from social networks to
    video sites to online retailers. We can also directly observe them in action.
    We’re aware that a computer is picking tracks to play on Spotify, in much the
    same way we don’t necessarily notice that Gmail is deciding whether inbound email
    is spam.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐引擎是大规模机器学习的最受欢迎的示例之一；例如，大多数人都熟悉亚马逊的推荐引擎。它是一个共同的基础，因为推荐引擎无处不在，从社交网络到视频网站再到在线零售商。我们也可以直接观察它们的运作。我们知道计算机正在挑选在Spotify上播放的曲目，这与我们不一定注意到Gmail是否决定入站邮件是否为垃圾邮件的方式类似。
- en: The output of a recommender is more intuitively understandable than other machine
    learning algorithms. It’s exciting, even. For as much as we think that musical
    taste is personal and inexplicable, recommenders do a surprisingly good job of
    identifying tracks we didn’t know we would like. For domains like music or movies,
    where recommenders are often deployed, it’s comparatively easy to reason why a
    recommended piece of music fits with someone’s listening history. Not all clustering
    or classification algorithms match that description. For example, a support vector
    machine classifier is a set of coefficients, and it’s hard even for practitioners
    to articulate what the numbers mean when they make predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐引擎的输出比其他机器学习算法更直观易懂。这甚至是令人兴奋的。尽管我们认为音乐口味是个人的且难以解释，推荐系统却出奇地能够很好地识别我们未曾预料到会喜欢的曲目。对于音乐或电影等领域，推荐系统经常被部署，我们可以比较容易地推断为什么推荐的音乐与某人的听歌历史相符。并非所有的聚类或分类算法都符合这一描述。例如，支持向量机分类器是一组系数，即使是从业者也很难表达这些数字在进行预测时的含义。
- en: It seems fitting to kick off the next three chapters, which will explore key
    machine learning algorithms on PySpark, with a chapter built around recommender
    engines, and recommending music in particular. It’s an accessible way to introduce
    real-world use of PySpark and MLlib and some basic machine learning ideas that
    will be developed in subsequent chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以推荐引擎为中心的下三章探讨PySpark上的关键机器学习算法似乎很合适，特别是推荐音乐。这是一个引入PySpark和MLlib的实际应用的可访问方式，并介绍一些基本的机器学习思想，这些思想将在随后的章节中进一步发展。
- en: In this chapter, we’ll implement a recommender system in PySpark. Specifically,
    we will use the Alternating Least Squares (ALS) algorithm on an open dataset provided
    by a music streaming service. We’ll start off by understanding the dataset and
    importing it in PySpark. Then we’ll discuss our motivation for choosing the ALS
    algorithm and its implementation in PySpark. This will be followed by data preparation
    and building our model using PySpark. We’ll finish up by making some user recommendations
    and discussing ways to improve our model through hyperparameter selection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在PySpark中实现一个推荐系统。具体而言，我们将使用一个音乐流媒体服务提供的开放数据集上的交替最小二乘（ALS）算法。我们将从理解数据集并在PySpark中导入开始。然后我们将讨论选择ALS算法的动机及其在PySpark中的实现。接下来是数据准备和使用PySpark构建模型。最后，我们将进行一些用户推荐，并讨论通过超参数选择改进我们的模型的方法。
- en: Setting Up the Data
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据设置
- en: We will use a dataset published by Audioscrobbler. Audioscrobbler was the first
    music recommendation system for [Last.fm](http://www.last.fm), one of the first
    internet streaming radio sites, founded in 2002\. Audioscrobbler provided an open
    API for “scrobbling,” or recording listeners’ song plays. Last.fm used this information
    to build a powerful music recommender engine. The system reached millions of users
    because third-party apps and sites could provide listening data back to the recommender
    engine.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由Audioscrobbler发布的数据集。Audioscrobbler是[Last.fm](http://www.last.fm)的第一个音乐推荐系统，是2002年成立的最早的互联网流媒体电台站点之一。Audioscrobbler提供了一个用于“scrobbling”（记录听众歌曲播放）的开放API。Last.fm利用这些信息构建了一个强大的音乐推荐引擎。该系统通过第三方应用程序和网站能够向推荐引擎提供听歌数据，达到了数百万用户。
- en: 'At that time, research on recommender engines was mostly confined to learning
    from rating-like data. That is, recommenders were usually viewed as tools that
    operated on input like “Bob rates Prince 3.5 stars.” The Audioscrobbler dataset
    is interesting because it merely records plays: “Bob played a Prince track.” A
    play carries less information than a rating. Just because Bob played the track
    doesn’t mean he actually liked it. You or I may occasionally play a song by an
    artist we don’t care for, or even play an album and walk out of the room.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那时，关于推荐引擎的研究大多局限于从类似评分的数据中学习。也就是说，推荐系统通常被视为在如“Bob 给 Prince 评了 3.5 星。”这样的输入上操作的工具。Audioscrobbler
    数据集很有趣，因为它仅记录了播放信息：“Bob 播放了 Prince 的一首曲目。”一个播放包含的信息比一个评分少。仅仅因为 Bob 播放了这首曲目，并不意味着他实际上喜欢它。你或者我偶尔也可能播放一个我们不喜欢的歌手的歌曲，甚至播放一个专辑然后离开房间。
- en: However, listeners rate music far less frequently than they play music. A dataset
    like this is therefore much larger, covers more users and artists, and contains
    more total information than a rating dataset, even if each individual data point
    carries less information. This type of data is often called *implicit feedback*
    data because the user-artist connections are implied as a side effect of other
    actions, and not given as explicit ratings or thumbs-up.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，听众评价音乐的频率远低于他们播放音乐的频率。因此，这样的数据集要大得多，涵盖的用户和艺术家更多，包含的总信息量也更多，即使每个个体数据点携带的信息较少。这种类型的数据通常被称为
    *隐式反馈* 数据，因为用户和艺术家之间的连接是作为其他行为的副产品而暗示的，并不是作为显式评分或赞的形式给出。
- en: A snapshot of a dataset distributed by Last.fm in 2005 can be found [online
    as a compressed archive](https://oreil.ly/Z7sfL). Download the archive, and find
    within it several files. First, the dataset’s files need to be made available.
    If you are using a remote cluster, copy all three data files into storage. This
    chapter will assume that the files are available at *data/*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2005 年 Last.fm 分发的数据集快照可以在 [在线压缩档案](https://oreil.ly/Z7sfL) 中找到。下载这个档案，并在其中找到几个文件。首先，需要使数据集的文件可用。如果你使用的是远程集群，请将所有三个数据文件复制到存储中。本章将假定这些文件在
    *data/* 目录下可用。
- en: 'Start `pyspark-shell`. Note that the computations in this chapter will take
    up more memory than simple applications. If you are running locally rather than
    on a cluster, for example, you will likely need to specify something like `--driver-memory
    4g` to have enough memory to complete these computations. The main dataset is
    in the *user_artist_data.txt* file. It contains about 141,000 unique users, and
    1.6 million unique artists. About 24.2 million users’ plays of artists are recorded,
    along with their counts. Let’s read this dataset into a DataFrame and have a look
    at it:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 `pyspark-shell`。请注意，本章中的计算将比简单应用程序消耗更多内存。例如，如果你是在本地而不是在集群上运行，可能需要指定类似 `--driver-memory
    4g` 这样的选项，以确保有足够的内存完成这些计算。主要数据集在 *user_artist_data.txt* 文件中。它包含约 141,000 位唯一用户和
    1.6 百万位唯一艺术家。记录了大约 2420 万位用户对艺术家的播放次数，以及它们的计数。让我们将这个数据集读入一个 DataFrame 并查看它：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Machine learning tasks like ALS are likely to be more compute-intensive than
    simple text processing. It may be better to break the data into smaller pieces—more
    partitions—for processing. You can chain a call to `.repartition(n)` after reading
    the text file to specify a different and larger number of partitions. You might
    set this higher to match the number of cores in your cluster, for example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像 ALS 这样的机器学习任务可能比简单的文本处理更需要计算资源。最好将数据分成更小的片段——更多的分区——进行处理。你可以在读取文本文件后链式调用 `.repartition(n)`，以指定一个不同且更大的分区数。例如，你可以将这个数设置得比集群中的核心数更高。
- en: 'The dataset also gives the names of each artist by ID in the *artist_data.txt*
    file. Note that when plays are scrobbled, the client application submits the name
    of the artist being played. This name could be misspelled or nonstandard, and
    this may only be detected later. For example, “The Smiths,” “Smiths, The,” and
    “the smiths” may appear as distinct artist IDs in the dataset even though they
    are plainly the same artist. So, the dataset also includes *artist_alias.txt*,
    which maps artist IDs that are known misspellings or variants to the canonical
    ID of that artist. Let’s read these two datasets into PySpark too:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还在 *artist_data.txt* 文件中按 ID 列出了每位艺术家的姓名。请注意，当播放时，客户端应用程序会提交正在播放的艺术家的名称。这个名称可能拼写错误或非标准，并且这可能只有在后期才能检测到。例如，“The
    Smiths”，“Smiths, The” 和 “the smiths” 可能会在数据集中出现为不同的艺术家 ID，尽管它们显然是同一位艺术家。因此，数据集还包括
    *artist_alias.txt*，其中映射了已知拼写错误或变体的艺术家 ID 到该艺术家的规范 ID。让我们也将这两个数据集读入 PySpark：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have a basic understanding of the datasets, we can discuss our requirements
    for a recommender algorithm and, subsequently, understand why the Alternating
    Least Squares algorithm is a good choice.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对数据集有了基本了解，我们可以讨论我们对推荐算法的需求，并且随后理解为什么交替最小二乘算法是一个不错的选择。
- en: Our Requirements for a Recommender System
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们对推荐系统的要求
- en: 'We need to choose a recommender algorithm that is suitable for our data. Here
    are our considerations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要选择一个适合我们数据的推荐算法。以下是我们的考虑：
- en: Implicit feedback
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式反馈
- en: The data is comprised entirely of interactions between users and artists’ songs.
    It contains no information about the users or about the artists other than their
    names. We need an algorithm that learns without access to user or artist attributes.
    These are typically called collaborative filtering algorithms. For example, deciding
    that two users might share similar tastes because they are the same age *is not*
    an example of collaborative filtering. Deciding that two users might both like
    the same song because they play many other songs that are the same *is* an example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据完全由用户和艺术家歌曲之间的互动组成。除了它们的名字外，没有关于用户或艺术家的其他信息。我们需要一种可以在没有用户或艺术家属性访问的情况下学习的算法。这些通常被称为协同过滤算法。例如，决定两个用户可能分享相似的口味，因为他们年龄相同*不是*协同过滤的例子。决定两个用户可能都喜欢同一首歌，因为他们播放了许多其他相同的歌曲*是*一个例子。
- en: Sparsity
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性
- en: Our dataset looks large because it contains tens of millions of play counts.
    But in a different sense, it is small and skimpy, because it is sparse. On average,
    each user has played songs from about 171 artists—out of 1.6 million. Some users
    have listened to only one artist. We need an algorithm that can provide decent
    recommendations to even these users. After all, every single listener must have
    started with just one play at some point!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集看起来很大，因为它包含数千万的播放次数。但从另一个角度来看，它又很小且稀疏，因为它是稀疏的。平均而言，每个用户从大约171位艺术家那里播放了歌曲——这些艺术家中有160万。有些用户只听过一个艺术家的歌曲。我们需要一种算法，即使对这些用户也能提供合理的推荐。毕竟，每个单独的听众最初肯定只是从一个播放开始的！
- en: Scalability and real-time predictions
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和实时预测
- en: Finally, we need an algorithm that scales, both in its ability to build large
    models and to create recommendations quickly. Recommendations are typically required
    in near real time—within a second, not tomorrow.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个在建立大模型和快速创建推荐方面都能扩展的算法。推荐通常需要几乎实时——不到一秒的时间，而不是明天。
- en: A broad class of algorithms that may be suitable is latent factor models. They
    try to explain *observed interactions* between large numbers of users and items
    through a relatively small number of *unobserved, underlying reasons*. For example,
    consider a customer who has bought albums by metal bands Megadeth and Pantera
    but also classical composer Mozart. It may be difficult to explain why exactly
    these albums were bought and nothing else. However, it’s probably a small window
    on a much larger set of tastes. Maybe the customer likes a coherent spectrum of
    music from metal to progressive rock to classical. That explanation is simpler
    and, as a bonus, suggests many other albums that would be of interest. In this
    example, “liking metal, progressive rock, and classical” are three latent factors
    that could explain tens of thousands of individual album preferences.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一类可能适合的广泛算法是潜在因子模型。它们试图通过相对较少的*未观察到的潜在原因*来解释大量用户和项目之间的*观察到的互动*。例如，考虑一个顾客购买了金属乐队Megadeth和Pantera的专辑，但同时也购买了古典作曲家莫扎特的专辑。可能很难解释为什么会购买这些专辑而不是其他的。然而，这可能只是更大音乐口味集合中的一个小窗口。也许这位顾客喜欢从金属到前卫摇滚再到古典的一致音乐谱系。这种解释更为简单，并且额外提出了许多其他可能感兴趣的专辑。在这个例子中，“喜欢金属、前卫摇滚和古典”这三个潜在因子可以解释成千上万个单独的专辑偏好。
- en: 'In our case, we will specifically use a type of matrix factorization model.
    Mathematically, these algorithms treat the user and product data as if it were
    a large matrix *A*, where the entry at row *i* and column *j* exists if user *i*
    has played artist *j*. *A* is sparse: most entries of *A* are 0, because only
    a few of all possible user-artist combinations actually appear in the data. They
    factor *A* as the matrix product of two smaller matrices, *X* and *Y*. They are
    very skinny—both have many rows because *A* has many rows and columns, but both
    have just a few columns (*k*). The *k* columns correspond to the latent factors
    that are being used to explain the interaction data.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将特别使用一种矩阵因子化模型。在数学上，这些算法将用户和产品数据视为一个大矩阵 *A*，如果用户 *i* 播放了艺术家 *j*，则第
    *i* 行和第 *j* 列的条目存在。*A* 是稀疏的：大多数 *A* 的条目都是 0，因为实际数据中只有少数所有可能的用户-艺术家组合。他们将 *A* 分解为两个较小矩阵的乘积，*X*
    和 *Y*。它们非常瘦长——因为 *A* 有很多行和列，但它们都只有几列（*k*）。*k* 列对应于被用来解释互动数据的潜在因素。
- en: The factorization can only be approximate because *k* is small, as shown in
    [Figure 3-1](#ALSFactorization).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *k* 较小，因此因子化只能是近似的，如[图 3-1](#ALSFactorization)所示。
- en: '![aaps 0301](assets/aaps_0301.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0301](assets/aaps_0301.png)'
- en: Figure 3-1\. Matrix factorization
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 矩阵因子化
- en: These algorithms are sometimes called matrix completion algorithms, because
    the original matrix *A* may be quite sparse, but the product *XY*^(*T*) is dense.
    Very few, if any, entries are 0, and therefore the model is only an approximation
    of *A*. It is a model in the sense that it produces (“completes”) a value for
    even the many entries that are missing (that is, 0) in the original *A*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法有时被称为矩阵完成算法，因为原始矩阵 *A* 可能非常稀疏，但乘积 *XY*^(*T*) 是密集的。很少有，如果有的话，条目是 0，因此模型仅是
    *A* 的近似。它是一个模型，因为它为原始 *A* 中许多缺失的条目（即 0）产生了一个值。
- en: This is a case where, happily, the linear algebra maps directly and elegantly
    to intuition. These two matrices contain a row for each user and each artist.
    The rows have few values—*k*. Each value corresponds to a latent feature in the
    model. So the rows express how much users and artists associate with these *k*
    latent features, which might correspond to tastes or genres. And it is simply
    the product of a user-feature and feature-artist matrix that yields a complete
    estimation of the entire, dense user-artist interaction matrix. This product might
    be thought of as mapping items to their attributes and then weighting those by
    user attributes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个令人高兴的例子，线性代数直接而优雅地映射到直觉中。这两个矩阵包含每个用户和每个艺术家的一行。这些行只有很少的值——*k*。每个值对应于模型中的一个潜在特征。因此，这些行表达了用户和艺术家与这些
    *k* 个潜在特征的关联程度，这些特征可能对应于品味或流派。它只是将用户特征和特征艺术家矩阵的乘积，得到了整个密集用户-艺术家互动矩阵的完整估计。这个乘积可以被认为是将项目映射到它们的属性，然后根据用户属性加权。
- en: 'The bad news is that *A* = *XY*^(*T*) generally has no exact solution at all,
    because *X* and *Y* aren’t large enough (technically speaking, too low [rank](https://oreil.ly/OfVj4))
    to perfectly represent *A*. This is actually a good thing. *A* is just a tiny
    sample of all interactions that *could* happen. In a way, we believe *A* is a
    terribly spotty and therefore hard-to-explain view of a simpler underlying reality
    that is well explained by just some small number of factors, *k*, of them. Think
    of a jigsaw puzzle depicting a cat. The final puzzle is simple to describe: a
    cat. When you’re holding just a few pieces, however, the picture you see is quite
    difficult to describe.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是 *A* = *XY*^(*T*) 通常根本没有确切的解，因为 *X* 和 *Y* 不够大（严格来说，[秩](https://oreil.ly/OfVj4)太低），无法完美地表示
    *A*。这实际上是件好事。*A* 只是所有可能发生的互动中的一个小样本。某种程度上，我们认为 *A* 是一个非常零散、因此难以解释的更简单的潜在现实的观点，它只能通过其中的一些少量因素，*k*
    个因素，进行很好的解释。想象一幅描绘猫的拼图。最终拼图很容易描述：一只猫。然而，当你手上只有几片时，你看到的图像却很难描述。
- en: '*XY*^(*T*) should still be as close to *A* as possible. After all, it’s all
    we’ve got to go on. It will not and should not reproduce it exactly. The bad news
    again is that this can’t be solved directly for both the best *X* and best *Y*
    at the same time. The good news is that it’s trivial to solve for the best *X*
    if *Y* is known, and vice versa. But neither is known beforehand!'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*XY*^(*T*) 应该尽可能接近 *A*。毕竟，这是我们所依赖的唯一依据。它不会也不应该完全复制它。坏消息是，这不能直接为同时获得最佳的 *X*
    和 *Y* 而解决。好消息是，如果已知 *Y*，那么解决最佳 *X* 就很简单，反之亦然。但事先都不知道！'
- en: Fortunately, there are algorithms that can escape this catch-22 and find a decent
    solution. One such algorithm that’s available in PySpark is the ALS algorithm.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些算法可以摆脱这种困境并找到一个合理的解决方案。 PySpark中可用的一个这样的算法是ALS算法。
- en: Alternating Least Squares Algorithm
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交替最小二乘法算法
- en: We will use the Alternating Least Squares algorithm to compute latent factors
    from our dataset. This type of approach was popularized around the time of the
    Netflix Prize competition by papers like [“Collaborative Filtering for Implicit
    Feedback Datasets”](https://oreil.ly/3pSzk) and [“Large-Scale Parallel Collaborative
    Filtering for the Netflix Prize”](https://oreil.ly/LULpp). PySpark MLlib’s ALS
    implementation draws on ideas from both of these papers and is the only recommender
    algorithm currently implemented in Spark MLlib.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用交替最小二乘法算法从我们的数据集中计算潜在因子。 这种方法在Netflix Prize竞赛期间由《“Collaborative Filtering
    for Implicit Feedback Datasets”》和《“Large-Scale Parallel Collaborative Filtering
    for the Netflix Prize”》等论文流行起来。 PySpark MLlib的ALS实现汲取了这两篇论文的思想，并且是目前在Spark MLlib中唯一实现的推荐算法。
- en: 'Here’s a code snippet (non-functional) to give you a peek at what lies ahead
    in the chapter:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一段代码片段（非功能性的），让你一窥后面章节的内容：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With ALS, we will treat our input data as a large, sparse matrix *A*, and find
    out *X* and *Y*, as discussed in previous section. At the start, *Y* isn’t known,
    but it can be initialized to a matrix full of randomly chosen row vectors. Then
    simple linear algebra gives the best solution for *X*, given *A* and *Y*. In fact,
    it’s trivial to compute each row *i* of *X* separately as a function of *Y* and
    of one row of *A*. Because it can be done separately, it can be done in parallel,
    and that is an excellent property for large-scale computation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过ALS，我们将把我们的输入数据视为一个大的稀疏矩阵*A*，并找出*X*和*Y*，就像前面的部分所讨论的那样。 起初，*Y*是未知的，但可以初始化为一个由随机选择的行向量组成的矩阵。
    然后，简单的线性代数给出了最佳的*X*解，给定*A*和*Y*。 实际上，可以分别计算*X*的每一行*i*作为*Y*和*A*的函数，并且可以并行进行。 对于大规模计算来说，这是一个很好的特性：
- en: <math display="block"><mrow><msub><mi>A</mi><mi>i</mi></msub><mi>Y</mi><mo>(</mo><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi><msup><mo>)</mo><mi>–1</mi></msup>
    <mo>=</mo> <msub><mi>X</mi><mi>i</mi></msub></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>A</mi><mi>i</mi></msub><mi>Y</mi><mo>(</mo><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi><msup><mo>)</mo><mi>–1</mi></msup>
    <mo>=</mo> <msub><mi>X</mi><mi>i</mi></msub></mrow></math>
- en: Equality can’t be achieved exactly, so in fact the goal is to minimize |*A*[*i*]*Y*(*Y*^(*T*)*Y*)^(*–1*)
    – *X*[*i*]|, or the sum of squared differences between the two matrices’ entries.
    This is where the “least squares” in the name comes from. In practice, this is
    never solved by actually computing inverses, but faster and more directly via
    methods like the QR decomposition. This equation simply elaborates on the theory
    of how the row vector is computed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上无法完全实现相等，因此目标实际上是将|*A*[*i*]*Y*(*Y*^(*T*)*Y*)^(*–1*) – *X*[*i*]|最小化，或者说是两个矩阵条目之间的平方差的总和。
    这就是名称中“最小二乘”的含义。 实际上，这从未通过计算逆矩阵来解决，而是通过QR分解等方法更快、更直接地解决。 这个方程只是详细说明了如何计算行向量的理论。
- en: 'The same thing can be done to compute each *Y*[*j*] from *X*. And again, to
    compute *X* from *Y*, and so on. This is where the “alternating” part comes from.
    There’s just one small problem: *Y* was made up—and random! *X* was computed optimally,
    yes, but gives a bogus solution for *Y*. Fortunately, if this process is repeated,
    *X* and *Y* do eventually converge to decent solutions.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过同样的方式可以计算每个*Y*[*j*]来自*X*。 同样地，来自*Y*的计算*X*，以此类推。 这就是“交替”部分的来源。 只有一个小问题：*Y*是编造出来的——而且是随机的！
    *X*被最佳计算了，是的，但对*Y*给出了一个虚假的解决方案。 幸运的是，如果这个过程重复进行，*X*和*Y*最终会收敛到合理的解决方案。
- en: When used to factor a matrix representing implicit data, there is a little more
    complexity to the ALS factorization. It is not factoring the input matrix *A*
    directly, but a matrix *P* of 0s and 1s, containing 1 where *A* contains a positive
    value and 0 elsewhere. The values in *A* are incorporated later as weights. This
    detail is beyond the scope of this book but is not necessary to understand how
    to use the algorithm.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当用于因子分解表示隐式数据的矩阵时，ALS因子分解会更加复杂一些。 它并不直接分解输入矩阵*A*，而是一个由0和1组成的矩阵*P*，其中包含*A*中包含正值的位置为1，其他位置为0。
    *A*中的值稍后将作为权重加入。 这个细节超出了本书的范围，但并不影响理解如何使用该算法。
- en: Finally, the ALS algorithm can take advantage of the sparsity of the input data
    as well. This, and its reliance on simple, optimized linear algebra and its data-parallel
    nature, make it very fast at large scale.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，ALS算法也可以利用输入数据的稀疏性。 这个特性以及它对简单、优化的线性代数和数据并行的依赖，使其在大规模情况下非常快速。
- en: Next, we will preprocess our dataset and make it suitable for use with the ALS
    algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将预处理我们的数据集，并使其适合与ALS算法一起使用。
- en: Preparing the Data
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: The first step in building a model is to understand the data that is available
    and parse or transform it into forms that are useful for analysis in Spark.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型的第一步是了解可用的数据，并将其解析或转换为在Spark中进行分析时有用的形式。
- en: Spark MLlib’s ALS implementation does not strictly require numeric IDs for users
    and items, but is more efficient when the IDs are in fact representable as 32-bit
    integers. That is the case because under the hood the data is being represented
    using the JVM’s data type. Does this dataset conform to this requirement already?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib的ALS实现在用户和项目的ID不是严格要求为数字时也可以工作，但当ID实际上可以表示为32位整数时效率更高。这是因为在底层使用JVM的数据类型来表示数据。这个数据集是否已经符合这个要求？
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each line of the file contains a user ID, an artist ID, and a play count, separated
    by spaces. To compute statistics on the user ID, we split the line by space characters
    and parse the values as integers. The result is conceptually three “columns”:
    a user ID, artist ID, and count as `int`s. It makes sense to transform this to
    a dataframe with columns named “user”, “artist”, and “count” because it then becomes
    simple to compute simple statistics like the maximum and minimum:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的每一行包含用户ID、艺术家ID和播放计数，用空格分隔。为了对用户ID进行统计，我们通过空格字符分割行并解析值为整数。结果概念上有三个“列”：用户ID、艺术家ID和计数，都是`int`类型。将其转换为列名为“user”、“artist”和“count”的数据框是有意义的，因为这样可以简单地计算最大值和最小值：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The maximum user and artist IDs are 2443548 and 10794401, respectively (and
    their minimums are 90 and 1; no negative values). These are comfortably smaller
    than 2147483647\. No additional transformation will be necessary to use these
    IDs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的用户和艺术家ID分别为2443548和10794401（它们的最小值分别为90和1；没有负值）。这些值明显小于2147483647。不需要额外的转换即可使用这些ID。
- en: 'It will be useful later in this example to know the artist names corresponding
    to the opaque numeric IDs. `raw_artist_data` contains the artist ID and name separated
    by a tab. PySpark’s split function accepts regular expression values for the `pattern`
    parameter. We can split using the whitespace character, `\s`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中稍后知道与不透明数字ID对应的艺术家名称将会很有用。`raw_artist_data`包含由制表符分隔的艺术家ID和名称。PySpark的split函数接受正则表达式作为`pattern`参数的值。我们可以使用空白字符`\s`进行分割：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This results in a dataframe with the artist ID and name as columns `id` and
    `name`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致一个数据框，其列为`id`和`name`，代表艺术家ID和名称。
- en: '`raw_artist_alias` maps artist IDs that may be misspelled or nonstandard to
    the ID of the artist’s canonical name. This dataset is relatively small, containing
    about 200,000 entries. It contains two IDs per line, separated by a tab. We will
    parse this in a similar manner as we did `raw_artist_data`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`raw_artist_alias`将可能拼写错误或非标准的艺术家ID映射到艺术家规范名称的ID。这个数据集相对较小，包含约200,000条目。每行包含两个ID，用制表符分隔。我们将以与`raw_artist_data`类似的方式解析它：'
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first entry maps ID 1092764 to 1000311\. We can look these up from the
    `artist_by_id` DataFrame:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个条目将ID 1092764映射到1000311。我们可以从`artist_by_id`数据框中查找这些信息：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This entry evidently maps “Winwood, Steve” to “Steve Winwood,” which is in fact
    the correct name for the artist.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此条目显然将“Winwood, Steve”映射为“Steve Winwood”，这实际上是艺术家的正确名称。
- en: Building a First Model
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建第一个模型
- en: 'Although the dataset is in nearly the right form for use with Spark MLlib’s
    ALS implementation, it requires a small, extra transformation. The aliases dataset
    should be applied to convert all artist IDs to a canonical ID, if a different
    canonical ID exists:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该数据集几乎适合与Spark MLlib的ALS实现一起使用，但它需要进行一个小的额外转换。应用别名数据集以将所有艺术家ID转换为规范ID（如果存在不同的规范ID）将会很有用：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO1-1)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO1-1)'
- en: Get artist’s alias if it exists; otherwise, get original artist.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在艺术家别名，则获取艺术家别名；否则，获取原始艺术家。
- en: We `broadcast` the `artist_alias` DataFrame created earlier. This makes Spark
    send and hold in memory just one copy for *each executor* in the cluster. When
    there are thousands of tasks and many execute in parallel on each executor, this
    can save significant network traffic and memory. As a rule of thumb, it’s helpful
    to broadcast a significantly smaller dataset when performing a join with a very
    big dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们广播先前创建的`artist_alias`数据框。这使得Spark在集群中的每个执行器上仅发送和保存一个副本。当有成千上万个任务并且许多任务并行执行时，这可以节省大量的网络流量和内存。作为经验法则，在与一个非常大的数据集进行连接时，广播一个显著较小的数据集是很有帮助的。
- en: The call to `cache` suggests to Spark that this DataFrame should be temporarily
    stored after being computed and, furthermore, kept in memory in the cluster. This
    is helpful because the ALS algorithm is iterative and will typically need to access
    this data 10 times or more. Without this, the DataFrame could be repeatedly recomputed
    from the original data each time it is accessed! The Storage tab in the Spark
    UI will show how much of the DataFrame is cached and how much memory it uses,
    as shown in [Figure 3-2](#Recommender_StorageTag). This one consumes about 120
    MB across the cluster.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`cache`告诉Spark，在计算后应该暂时存储这个DataFrame，并且在集群内存中保持。这很有帮助，因为ALS算法是迭代的，通常需要多次访问这些数据。如果没有这一步，每次访问时DataFrame都可能会从原始数据中重新计算！Spark
    UI中的存储选项卡将显示DataFrame的缓存量和内存使用量，如图[Figure 3-2](#Recommender_StorageTag)所示。这个DataFrame在整个集群中大约消耗了120
    MB。
- en: '![aaps 0302](assets/aaps_0302.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0302](assets/aaps_0302.png)'
- en: Figure 3-2\. Storage tab in the Spark UI, showing cached DataFrame memory usage
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2\. Spark UI中的存储选项卡，显示缓存的DataFrame内存使用情况
- en: When you use `cache` or `persist`, the DataFrame is not fully cached until you
    trigger an action that goes through every record (e.g., `count`). If you use an
    action like `show(1)`, only one partition will be cached. That is because PySpark’s
    optimizer will figure out that you do not need to compute all the partitions just
    to retrieve one record.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用`cache`或`persist`时，DataFrame不会完全缓存，直到您触发一个需要遍历每条记录的动作（例如`count`）。如果您使用像`show(1)`这样的操作，只有一个分区会被缓存。这是因为PySpark的优化器会发现您只需计算一个分区即可检索一条记录。
- en: Note that the label “Deserialized” in the UI in [Figure 3-2](#Recommender_StorageTag)
    is actually only relevant for RDDs, where “Serialized” means data is stored in
    memory, not as objects, but as serialized bytes. However, DataFrame instances
    like this one perform their own “encoding” of common data types in memory separately.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在UI中，“反序列化”标签在图[Figure 3-2](#Recommender_StorageTag)中实际上只与RDD相关，其中“序列化”表示数据存储在内存中，不是作为对象，而是作为序列化的字节。然而，像这种DataFrame实例会单独对常见数据类型在内存中进行“编码”。
- en: Actually, 120 MB is surprisingly small. Given that there are about 24 million
    plays stored here, a quick back-of-the-envelope calculation suggests that this
    would mean that each user-artist-count entry consumes only 5 bytes on average.
    However, the three 32-bit integers alone ought to consume 12 bytes. This is one
    of the advantages of a DataFrame. Because the types of data stored are primitive
    32-bit integers, their representation can be optimized in memory internally.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，120 MB的消耗量令人惊讶地小。考虑到这里存储了大约2400万次播放记录，一个快速的粗略计算表明，每个用户-艺术家-计数条目平均只消耗5字节。然而，仅三个32位整数本身应该消耗12字节。这是DataFrame的一个优点之一。因为存储的数据类型是基本的32位整数，它们在内存中的表示可以进行优化。
- en: 'Finally, we can build a model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以构建一个模型：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This constructs `model` as an `ALSModel` with some default configuration. The
    operation will likely take several minutes or more depending on your cluster.
    Compared to some machine learning models, whose final form may consist of just
    a few parameters or coefficients, this type of model is huge. It contains a feature
    vector of 10 values for each user and product in the model, and in this case there
    are more than 1.7 million of them. The model contains these large user-feature
    and product-feature matrices as DataFrames of their own.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用一些默认配置构建一个`ALSModel`作为`model`。根据您的集群不同，此操作可能需要几分钟甚至更长时间。与某些机器学习模型最终形式可能仅包含几个参数或系数不同，这种模型非常庞大。它包含模型中每个用户和产品的10个值的特征向量，在这种情况下超过170万个。该模型包含这些大型用户特征和产品特征矩阵作为它们自己的DataFrame。
- en: The values in your results may be somewhat different. The final model depends
    on a randomly chosen initial set of feature vectors. The default behavior of this
    and other components in MLlib, however, is to use the same set of random choices
    every time by defaulting to a fixed seed. This is unlike other libraries, where
    behavior of random elements is typically not fixed by default. So, here and elsewhere,
    a random seed is set with `(… seed=0,…)`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您的结果中的值可能会有所不同。最终模型取决于随机选择的初始特征向量集。然而，MLlib中此类组件的默认行为是使用相同的随机选择集合，默认情况下使用固定种子。这与其他库不同，其他库通常不会默认固定随机元素的行为。因此，在这里和其他地方，随机种子设置为`(…
    seed=0,…)。
- en: 'To see some feature vectors, try the following, which displays just one row
    and does not truncate the wide display of the feature vector:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看一些特征向量，请尝试以下操作，它仅显示一行并且不截断特征向量的宽显示：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The other methods invoked on `ALS`, like `setAlpha`, set *hyperparameters*
    whose values can affect the quality of the recommendations that the model makes.
    These will be explained later. The more important first question is this: is the
    model any good? Does it produce good recommendations? That is what we will try
    to answer in the next section.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ALS`上调用的其他方法，如`setAlpha`，设置的是可以影响模型推荐质量的*超参数*值。这些将在后面解释。更重要的第一个问题是：这个模型好吗？它能产生好的推荐吗？这是我们将在下一节试图回答的问题。
- en: Spot Checking Recommendations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽查推荐
- en: 'We should first see if the artist recommendations make any intuitive sense,
    by examining a user, plays, and recommendations for that user. Take, for example,
    user 2093760\. First, let’s look at his or her plays to get a sense of the person’s
    tastes. Extract the IDs of artists that this user has listened to and print their
    names. This means searching the input for artist IDs played by this user and then
    filtering the set of artists by these IDs to print the names in order:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该看看艺术家的推荐是否有直觉上的意义，通过检查一个用户、他或她的播放以及对该用户的推荐来判断。比如说，用户2093760。首先，让我们看看他或她的播放，以了解这个人的口味。提取此用户听过的艺术家ID并打印他们的名称。这意味着通过搜索该用户播放的艺术家ID来过滤艺术家集，并按顺序打印名称：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-1)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-1)'
- en: Find lines whose user is 2093760.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查找其用户为2093760的行。
- en: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-2)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-2)'
- en: Collect dataset of artist ID.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 收集艺术家ID数据集。
- en: '[![3](assets/3.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-3)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO2-3)'
- en: Filter in those artists.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤这些艺术家。
- en: The artists look like a mix of mainstream pop and hip-hop. A Jurassic 5 fan?
    Remember, it’s 2005. In case you’re wondering, the Saw Doctors is a very Irish
    rock band popular in Ireland.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些艺术家看起来像是主流流行音乐和嘻哈的混合。是不是恐龙5的粉丝？记住，那是2005年。如果你好奇的话，锯医生是一个非常受爱尔兰欢迎的爱尔兰摇滚乐队。
- en: 'Now, it’s simple to make recommendations for a user, though computing them
    this way will take a few moments. It’s suitable for batch scoring but not real-time
    use cases:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，简单地为用户做出推荐虽然用这种方式计算需要一些时间。它适用于批量评分，但不适合实时使用情况：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The resulting recommendations contain lists comprised of artist ID and, of course,
    “predictions.” For this type of ALS algorithm, the prediction is an opaque value
    normally between 0 and 1, where higher values mean a better recommendation. It
    is not a probability but can be thought of as an estimate of a 0/1 value indicating
    whether the user won’t or will interact with the artist, respectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果推荐包含由艺术家ID组成的列表，当然还有“预测”。对于这种ALS算法，预测是一个通常在0到1之间的不透明值，较高的值意味着更好的推荐。它不是概率，但可以看作是一个估计的0/1值，指示用户是否会与艺术家互动。
- en: 'After extracting the artist IDs for the recommendations, we can look up artist
    names in a similar way:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取了推荐的艺术家ID之后，我们可以类似地查找艺术家名称：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The result is all hip-hop. This doesn’t look like a great set of recommendations,
    at first glance. While these are generally popular artists, they don’t appear
    to be personalized to this user’s listening habits.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 结果全是嘻哈。乍一看，这看起来不是一个很好的推荐集。虽然这些通常是受欢迎的艺术家，但似乎并没有根据这位用户的听歌习惯来个性化推荐。
- en: Evaluating Recommendation Quality
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估推荐质量
- en: Of course, that’s just one subjective judgment about one user’s results. It’s
    hard for anyone but that user to quantify how good the recommendations are. Moreover,
    it’s infeasible to have any human manually score even a small sample of the output
    to evaluate the results.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是对一个用户结果的主观判断。除了该用户之外，其他人很难量化推荐的好坏。此外，人工对甚至是小样本输出进行评分以评估结果是不可行的。
- en: It’s reasonable to assume that users tend to play songs from artists who are
    appealing, and not play songs from artists who aren’t appealing. So, the plays
    for a user give a partial picture of “good” and “bad” artist recommendations.
    This is a problematic assumption but about the best that can be done without any
    other data. For example, presumably user 2093760 likes many more artists than
    the 5 listed previously, and of the 1.7 million other artists not played, a few
    are of interest, and not all are “bad” recommendations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设用户倾向于播放有吸引力的艺术家的歌曲，而不播放无吸引力的艺术家的歌曲，这是合理的。因此，用户的播放记录只能部分反映出“好”的和“坏”的艺术家推荐。这是一个有问题的假设，但在没有其他数据的情况下，这可能是最好的做法。例如，假设用户2093760喜欢的艺术家远不止之前列出的5个，并且在那170万名未播放的艺术家中，有些是感兴趣的，而不是所有的都是“坏”推荐。
- en: What if a recommender were evaluated on its ability to rank good artists high
    in a list of recommendations? This is one of several generic metrics that can
    be applied to a system that ranks things, like a recommender. The problem is that
    “good” is defined as “artists the user has listened to,” and the recommender system
    has already received all of this information as input. It could trivially return
    the user’s previously listened-to artists as top recommendations and score perfectly.
    But this is not useful, especially because the recommender’s role is to recommend
    artists that the user has never listened to.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个推荐系统的评估是根据其在推荐列表中高排名好艺术家的能力，那将是一个通用的度量标准之一，适用于像推荐系统这样排名物品的系统。问题在于，“好”被定义为“用户曾经听过的艺术家”，而推荐系统已经把所有这些信息作为输入接收了。它可以轻松地返回用户以前听过的艺术家作为顶级推荐，并得到完美的分数。但这并不实用，特别是因为推荐系统的角色是推荐用户以前从未听过的艺术家。
- en: To make this meaningful, some of the artist play data can be set aside and hidden
    from the ALS model-building process. Then, this held-out data can be interpreted
    as a collection of good recommendations for each user but one that the recommender
    has not already been given. The recommender is asked to rank all items in the
    model, and the ranks of the held-out artists are examined. Ideally, the recommender
    places all of them at or near the top of the list.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其具有意义，一些艺术家的播放数据可以被保留，并且隐藏在ALS模型构建过程之外。然后，这些保留的数据可以被解释为每个用户的一组好推荐，但是这些推荐并不是推荐系统已经给出的。推荐系统被要求对模型中的所有项目进行排名，并检查保留艺术家的排名。理想情况下，推荐系统会将它们全部或几乎全部排在列表的顶部。
- en: We can then compute the recommender’s score by comparing all held-out artists’
    ranks to the rest. (In practice, we compute this by examining only a sample of
    all such pairs, because a potentially huge number of such pairs may exist.) The
    fraction of pairs where the held-out artist is ranked higher is its score. A score
    of 1.0 is perfect, 0.0 is the worst possible score, and 0.5 is the expected value
    achieved from randomly ranking artists.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过比较所有保留艺术家的排名与其余艺术家的排名来计算推荐系统的分数。（在实践中，我们仅检查所有这些对中的一部分样本，因为可能存在大量这样的对。）保留艺术家排名较高的比例就是其分数。分数为1.0表示完美，0.0是最差的分数，0.5是从随机排名艺术家中达到的期望值。
- en: This metric is directly related to an information retrieval concept called the
    [receiver operating characteristic (ROC) curve](https://oreil.ly/Pt2bn). The metric
    in the preceding paragraph equals the area under this ROC curve and is indeed
    known as AUC, or area under the curve. AUC may be viewed as the probability that
    a randomly chosen good recommendation ranks above a randomly chosen bad recommendation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量标准直接与信息检索概念“接收者操作特征曲线（ROC曲线）”有关。上述段落中的度量标准等于这个ROC曲线下的面积，确实被称为AUC，即曲线下面积。AUC可以被视为随机选择的一个好推荐高于随机选择的一个坏推荐的概率。
- en: The AUC metric is also used in the evaluation of classifiers. It is implemented,
    along with related methods, in the MLlib class `BinaryClassificationMetrics`.
    For recommenders, we will compute AUC *per user* and average the result. The resulting
    metric is slightly different and might be called “mean AUC.” We will implement
    this, because it is not (quite) implemented in PySpark.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AUC指标也用于分类器的评估。它与相关方法一起实现在MLlib类`BinaryClassificationMetrics`中。对于推荐系统，我们将计算每个用户的AUC，并对结果取平均值。得到的度量标准略有不同，可能称为“平均AUC”。我们将实现这一点，因为它在PySpark中尚未（完全）实现。
- en: Other evaluation metrics that are relevant to systems that rank things are implemented
    in `RankingMetrics`. These include metrics like precision, recall, and [mean average
    precision (MAP)](https://oreil.ly/obbTT). MAP is also frequently used and focuses
    more narrowly on the quality of the top recommendations. However, AUC will be
    used here as a common and broad measure of the quality of the entire model output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其他与排名相关的评估指标实现在`RankingMetrics`中。这些包括精度、召回率和[平均精度均值（MAP）](https://oreil.ly/obbTT)等指标。MAP也经常被使用，更专注于顶部推荐的质量。然而，在这里AUC将作为衡量整个模型输出质量的常见和广泛的指标使用。
- en: 'In fact, the process of holding out some data to select a model and evaluate
    its accuracy is common practice in all of machine learning. Typically, data is
    divided into three subsets: training, cross-validation (CV), and test sets. For
    simplicity in this initial example, only two sets will be used: training and CV.
    This will be sufficient to choose a model. In [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests),
    this idea will be extended to include the test set.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在所有机器学习中，保留一些数据以选择模型并评估其准确性的过程都是常见的实践。通常，数据被分为三个子集：训练集、交叉验证（CV）集和测试集。在这个初始示例中为简单起见，只使用两个集合：训练集和CV集。这足以选择一个模型。在[第四章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)，这个想法将扩展到包括测试集。
- en: Computing AUC
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算AUC
- en: An implementation of mean AUC is provided in the source code accompanying this
    book. It is not reproduced here, but is explained in some detail in comments in
    the source code. It accepts the CV set as the “positive” or “good” artists for
    each user and a prediction function. This function translates a dataframe containing
    each user-artist pair into a dataframe that also contains its estimated strength
    of interaction as a “prediction,” a number wherein higher values mean higher rank
    in the recommendations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书附带的源代码中提供了平均AUC的实现。这里不会重复，但在源代码的注释中有详细解释。它接受CV集作为每个用户的“正向”或“好”的艺术家，并接受一个预测函数。这个函数将包含每个用户-艺术家对的数据框转换为另一个数据框，其中也包含其估计的互动强度作为“预测”，一个数字，较高的值意味着在推荐中排名更高。
- en: 'To use the input data, we must split it into a training set and a CV set. The
    ALS model will be trained on the training dataset only, and the CV set will be
    used to evaluate the model. Here, 90% of the data is used for training and the
    remaining 10% for cross-validation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用输入数据，我们必须将其分为训练集和CV集。ALS模型仅在训练数据集上进行训练，CV集用于评估模型。这里，90%的数据用于训练，剩余的10%用于交叉验证：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that `areaUnderCurve` accepts a function as its third argument. Here, the
    `transform` method from `ALSModel` is passed in, but it will shortly be swapped
    out for an alternative.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`areaUnderCurve`接受一个函数作为其第三个参数。在这里，从`ALSModel`中传入的`transform`方法被传递进去，但很快会被替换为另一种方法。
- en: The result is about 0.879\. Is this good? It is certainly higher than the 0.5
    that is expected from making recommendations randomly, and it’s close to 1.0,
    which is the maximum possible score. Generally, an AUC over 0.9 would be considered
    high.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果约为0.879。这好吗？它显然高于从随机推荐中预期的0.5，接近1.0，这是可能的最高分数。通常，AUC超过0.9会被认为是高的。
- en: But is it an accurate evaluation? This evaluation could be repeated with a different
    90% as the training set. The resulting AUC values’ average might be a better estimate
    of the algorithm’s performance on the dataset. In fact, one common practice is
    to divide the data into *k* subsets of similar size, use *k* – 1 subsets together
    for training, and evaluate on the remaining subset. We can repeat this *k* times,
    using a different set of subsets each time. This is called [*k-fold cross-validation*](https://oreil.ly/DolrQ).
    This won’t be implemented in examples here, for simplicity, but some support for
    this technique exists in MLlib in its `CrossValidator` API. The validation API
    will be revisited in [“Random Forests”](ch04.xhtml#RandomDecisionForests).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一个准确的评估吗？这个评估可以用不同的90%作为训练集重复进行。得到的AUC值的平均值可能更好地估计了算法在数据集上的性能。实际上，一个常见的做法是将数据分成*k*个大小相似的子集，使用*k*
    - 1个子集一起进行训练，并在剩余的子集上进行评估。我们可以重复这个过程*k*次，每次使用不同的子集。这被称为*k*折交叉验证，这里为简单起见不会在示例中实现，但MLlib中的`CrossValidator`API支持这种技术。验证API将在[“随机森林”](ch04.xhtml#RandomDecisionForests)中重新讨论。
- en: 'It’s helpful to benchmark this against a simpler approach. For example, consider
    recommending the globally most-played artists to every user. This is not personalized,
    but it is simple and may be effective. Define this simple prediction function
    and evaluate its AUC score:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与简单方法进行基准测试很有帮助。例如，考虑向每个用户推荐全球播放量最高的艺术家。这不是个性化的，但简单且可能有效。定义这个简单的预测函数并评估其AUC分数：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The result is also about 0.880\. This suggests that nonpersonalized recommendations
    are already fairly effective according to this metric. However, we’d expect the
    “personalized” recommendations to score better in comparison. Clearly, the model
    needs some tuning. Can it be made better?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 结果也约为0.880。这表明根据这个度量标准，非个性化的推荐已经相当有效。然而，我们预期“个性化”的推荐在比较中会得分更高。显然，模型需要一些调整。它可以做得更好吗？
- en: Hyperparameter Selection
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数选择
- en: 'So far, the hyperparameter values used to build the `ALSModel` were simply
    given without comment. They are not learned by the algorithm and must be chosen
    by the caller. The configured hyperparameters were:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，用于构建 `ALSModel` 的超参数值仅仅是给出而没有注释。它们不是由算法学习的，必须由调用者选择。配置的超参数是：
- en: '`setRank(10)`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`setRank(10)`'
- en: The number of latent factors in the model, or equivalently, the number of columns
    *k* in the user-feature and product-feature matrices. In nontrivial cases, this
    is also their rank.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的潜在因子数量，或者说用户特征和产品特征矩阵中的列数*k*。在非平凡情况下，这也是它们的秩。
- en: '`setMaxIter(5)`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`setMaxIter(5)`'
- en: The number of iterations that the factorization runs. More iterations take more
    time but may produce a better factorization.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分解运行的迭代次数。更多的迭代需要更多时间，但可能产生更好的因子分解。
- en: '`setRegParam(0.01)`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`setRegParam(0.01)`'
- en: A standard overfitting parameter, also usually called *lambda*. Higher values
    resist overfitting, but values that are too high hurt the factorization’s accuracy.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标准的过拟合参数，通常也称为*lambda*。较高的值抵抗过拟合，但是值过高会损害因子分解的准确性。
- en: '`setAlpha(1.0)`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`setAlpha(1.0)`'
- en: Controls the relative weight of observed versus unobserved user-product interactions
    in the factorization.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 控制观察到的用户产品交互与未观察到的交互在因子分解中的相对权重。
- en: '`rank`, `regParam`, and `alpha` can be considered *hyperparameters* to the
    model. (`maxIter` is more of a constraint on resources used in the factorization.)
    These are not values that end up in the matrices inside the `ALSModel`—those are
    simply its *parameters* and are chosen by the algorithm. These hyperparameters
    are instead parameters to the process of building itself.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`rank`、`regParam` 和 `alpha` 可以被视为模型的*超参数*。(`maxIter` 更多是对因子分解中资源使用的约束。) 这些不是最终出现在
    `ALSModel` 内部矩阵中的数值—那些只是其*参数*，由算法选择。这些超参数反而是构建过程的参数。'
- en: The values used in the preceding list are not necessarily optimal. Choosing
    good hyperparameter values is a common problem in machine learning. The most basic
    way to choose values is to simply try combinations of values and evaluate a metric
    for each of them, and choose the combination that produces the best value of the
    metric.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 前述列表中使用的值未必是最优的。选择好的超参数值是机器学习中的常见问题。选择值的最基本方法是简单地尝试不同的组合并评估每个组合的度量标准，选择产生最佳值的组合。
- en: 'In the following example, eight possible combinations are tried: `rank` = 5
    or 30, `regParam` = 4.0 or 0.0001, and `alpha` = 1.0 or 40.0\. These values are
    still something of a guess, but are chosen to cover a broad range of parameter
    values. The results are printed in order by top AUC score:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，尝试了八种可能的组合：`rank` = 5 或 30，`regParam` = 4.0 或 0.0001，以及 `alpha` = 1.0
    或 40.0。这些值仍然有些猜测性质，但被选择来覆盖广泛的参数值范围。结果按照最高AUC分数的顺序打印：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-1)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-1)'
- en: Free up model resources immediately.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 立即释放模型资源。
- en: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-2)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO3-2)'
- en: Sort by first value (AUC), descending, and print.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 按第一个值（AUC）降序排序，并打印。
- en: The differences are small in absolute terms, but are still somewhat significant
    for AUC values. Interestingly, the parameter `alpha` seems consistently better
    at 40 than 1\. (For the curious, 40 was a value proposed as a default in one of
    the original ALS papers mentioned earlier.) This can be interpreted as indicating
    that the model is better off focusing far more on what the user did listen to
    than what he or she did not listen to.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对值上的差异很小，但对于AUC值来说仍然有一定显著性。有趣的是，参数`alpha`在40时似乎比1要好得多。（对于感兴趣的人，40是在前面提到的原始ALS论文中作为默认值提出的一个值。）这可以解释为模型更好地集中于用户实际听过的内容，而不是未听过的内容。
- en: A higher `regParam` looks better too. This suggests the model is somewhat susceptible
    to overfitting, and so needs a higher `regParam` to resist trying to fit the sparse
    input given from each user too exactly. Overfitting will be revisited in more
    detail in [“Random Forests”](ch04.xhtml#RandomDecisionForests).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的`regParam`看起来也更好。这表明模型在某种程度上容易过拟合，因此需要更高的`regParam`来抵制试图精确拟合每个用户给出的稀疏输入。过拟合将在[“随机森林”](ch04.xhtml#RandomDecisionForests)中更详细地讨论。
- en: As expected, 5 features are pretty low for a model of this size, and it underperforms
    the model that uses 30 features to explain tastes. It’s possible that the best
    number of features is actually higher than 30 and that these values are alike
    in being too small.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 预期地，对于这种规模的模型来说，5个特征相当少，并且在解释品味方面表现不佳，相比使用30个特征的模型而言。可能最佳特征数量实际上比30更高，这些值在过小方面是相似的。
- en: Of course, this process can be repeated for different ranges of values or more
    values. It is a brute-force means of choosing hyperparameters. However, in a world
    where clusters with terabytes of memory and hundreds of cores are not uncommon,
    and with frameworks like Spark that can exploit parallelism and memory for speed,
    it becomes quite feasible.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个过程可以针对不同的数值范围或更多的值重复进行。这是一种选择超参数的蛮力方法。然而，在拥有数TB内存和数百核心的集群不罕见的世界中，并且像Spark这样的框架可以利用并行性和内存来提高速度，这变得非常可行。
- en: It is not strictly required to understand what the hyperparameters mean, although
    it is helpful to know what normal ranges of values are in order to start the search
    over a parameter space that is neither too large nor too tiny.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，不需要理解超参数的含义，尽管了解正常值范围有助于开始搜索既不太大也不太小的参数空间。
- en: This was a fairly manual way to loop over hyperparameters, build models, and
    evaluate them. In [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests),
    after learning more about the Spark ML API, we’ll find that there is a more automated
    way to compute this using `Pipeline`s and `TrainValidationSplit`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当手动的超参数循环、模型构建和评估方式。在[第4章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)中，了解更多关于Spark
    ML API之后，我们会发现有一种更自动化的方式可以使用`Pipeline`和`TrainValidationSplit`来计算这些。
- en: Making Recommendations
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做推荐
- en: Proceeding for the moment with the best set of hyperparameters, what does the
    new model recommend for user 2093760?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时使用最佳超参数集，新模型为用户2093760推荐什么？
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Anecdotally, this makes a bit more sense for this user, being dominated by pop
    rock instead of all hip-hop. `[unknown]` is plainly not an artist. Querying the
    original dataset reveals that it occurs 429,447 times, putting it nearly in the
    top 100! This is some default value for plays without an artist, maybe supplied
    by a certain scrobbling client. It is not useful information, and we should discard
    it from the input before starting again. It is an example of how the practice
    of data science is often iterative, with discoveries about the data occurring
    at every stage.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 据传闻，对于这个用户来说这更有意义，主要是流行摇滚而不是所有的嘻哈。`[unknown]`显然不是一个艺术家。查询原始数据集发现它出现了429,447次，几乎进入前100名！这是一个在没有艺术家的情况下播放的默认值，可能是由某个特定的scrobbling客户端提供的。这不是有用的信息，我们应该在重新开始前从输入中丢弃它。这是数据科学实践通常是迭代的一个例子，在每个阶段都会发现关于数据的发现。
- en: This model can be used to make recommendations for all users. This could be
    useful in a batch process that recomputes a model and recommendations for users
    every hour or even less, depending on the size of the data and speed of the cluster.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以用来为所有用户做推荐。这在一个批处理过程中可能会很有用，每小时甚至更频繁地重新计算模型和用户的推荐，具体取决于数据的规模和集群的速度。
- en: 'At the moment, however, Spark MLlib’s ALS implementation does not support a
    method to recommend to all users. It is possible to recommend to one user at a
    time, as shown above, although each will launch a short-lived distributed job
    that takes a few seconds. This may be suitable for rapidly recomputing recommendations
    for small groups of users. Here, recommendations are made to 100 users taken from
    the data and printed:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前为止，Spark MLlib的ALS实现不支持一种向所有用户推荐的方法。可以一次向一个用户推荐，如上所示，尽管每次推荐将启动一个持续几秒钟的短暂分布式作业。这可能适用于快速为小组用户重新计算推荐。这里，从数据中取出的100个用户将收到推荐并打印出来：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO4-1)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_recommending_music_and_the_audioscrobbler_dataset_CO4-1)'
- en: Subset of 100 distinct users
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 100个不同用户的子集
- en: Here, the recommendations are just printed. They could just as easily be written
    to an external store like [HBase](https://oreil.ly/SQImy), which provides fast
    lookup at runtime.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里仅仅打印了推荐内容。它们也可以轻松写入到像[HBase](https://oreil.ly/SQImy)这样的外部存储中，在运行时提供快速查找。
- en: Where to Go from Here
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来该做什么
- en: Naturally, it’s possible to spend more time tuning the model parameters and
    finding and fixing anomalies in the input, like the `[unknown]` artist. For example,
    a quick analysis of play counts reveals that user 2064012 played artist 4468 an
    astonishing 439,771 times! Artist 4468 is the implausibly successful alternate-metal
    band System of a Down, which turned up earlier in recommendations. Assuming an
    average song length of 4 minutes, this is over 33 years of playing hits like “Chop
    Suey!” and “B.Y.O.B.” Because the band started making records in 1998, this would
    require playing four or five tracks at once for seven years. It must be spam or
    a data error, and another example of the types of real-world data problems that
    a production system would have to address.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以花更多时间调整模型参数，并查找和修复输入中的异常，比如`[unknown]`艺术家。例如，对播放计数的快速分析显示，用户2064012惊人地播放了艺术家4468达到了439,771次！艺术家4468是不太可能成功的另类金属乐队System
    of a Down，早些时候出现在推荐中。假设平均歌曲长度为4分钟，这相当于播放“Chop Suey！”和“B.Y.O.B.”等多达33年的时间！因为该乐队1998年开始制作唱片，这意味着要同时播放四到五首歌曲七年时间。这必须是垃圾信息或数据错误，是生产系统必须解决的真实世界数据问题的又一个例子。
- en: ALS is not the only possible recommender algorithm, but at this time, it is
    the only one supported by Spark MLlib. However, MLlib also supports a variant
    of ALS for non-implicit data. Its use is identical, except that `ALS` is configured
    with `setImplicitPrefs(false)`. This is appropriate when data is rating-like,
    rather than count-like. For example, it is appropriate when the dataset is user
    ratings of artists on a 1–5 scale. The resulting `prediction` column returned
    from `ALSModel.transform` recommendation methods then really is an estimated rating.
    In this case, the simple RMSE (root mean squared error) metric is appropriate
    for evaluating the recommender.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ALS并不是唯一可能的推荐算法，但目前它是Spark MLlib支持的唯一算法。然而，MLlib也支持一种适用于非隐式数据的ALS变体。其使用方式相同，只是将`ALS`配置为`setImplicitPrefs(false)`。例如，当数据类似于评分而不是计数时，这种配置是合适的。从`ALSModel.transform`推荐方法返回的`prediction`列实际上是一个估计的评分。在这种情况下，简单的RMSE（均方根误差）指标适合用于评估推荐系统。
- en: Later, other recommender algorithms may be available in Spark MLlib or other
    libraries.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，Spark MLlib或其他库可能提供其他的推荐算法。
- en: In production, recommender engines often need to make recommendations in real
    time, because they are used in contexts like ecommerce sites where recommendations
    are requested frequently as customers browse product pages. Precomputing and storing
    recommendations is a reasonable way to make recommendations available at scale.
    One disadvantage of this approach is that it requires precomputing recommendations
    for all users who might need recommendations soon, which is potentially any of
    them. For example, if only 10,000 of 1 million users visit a site in a day, precomputing
    all million users’ recommendations each day is 99% wasted effort.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，推荐引擎通常需要实时进行推荐，因为它们被用于像电子商务网站这样的场景，客户在浏览产品页面时经常请求推荐。预先计算和存储推荐结果是一种在规模上提供推荐的合理方式。这种方法的一个缺点是它需要为可能很快需要推荐的所有用户预先计算推荐结果，而这些用户可能是任何用户中的一部分。例如，如果100万用户中只有10,000个用户在一天内访问网站，那么每天预先计算所有100万用户的推荐结果将浪费99%的努力。
- en: It would be nicer to compute recommendations on the fly, as needed. While we
    can compute recommendations for one user using the `ALSModel`, this is necessarily
    a distributed operation that takes several seconds, because `ALSModel` is uniquely
    large and therefore actually a distributed dataset. This is not true of other
    models, which afford much faster scoring.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最好根据需要即时计算推荐。虽然我们可以使用`ALSModel`为一个用户计算推荐，但这必然是一个分布式操作，需要几秒钟的时间，因为`ALSModel`实际上是一个非常庞大的分布式数据集。而其他模型则提供了更快的评分。
