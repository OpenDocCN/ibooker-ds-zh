- en: 2 Getting started with human-in-the-loop machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 人机交互机器学习入门
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Ranking predictions by model confidence to identify confusing items
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据模型置信度对预测进行排名以识别混淆项
- en: Finding unlabeled items with novel information
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找具有新颖信息的未标记项
- en: Building a simple interface to annotate training data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个简单的界面来标注训练数据
- en: Evaluating changes in model accuracy as you add more training data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着添加更多训练数据评估模型准确性的变化
- en: 'For any machine learning task, you should start with a simple but functional
    system and build out more sophisticated components as you go. This guideline applies
    to most technology: ship the minimum viable product (MVP) and then iterate on
    that product. The feedback you get from what you ship first will tell you which
    pieces are the most important to build out next.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何机器学习任务，你应该从一个简单但功能性的系统开始，随着你的进展构建更复杂的组件。这个指南适用于大多数技术：推出最小可行产品（MVP），然后对产品进行迭代。你从最初推出的产品中获得的反馈将告诉你哪些部分是构建下一个产品时最重要的。
- en: This chapter is dedicated to building your first human-in-the-loop machine learning
    MVP. We will build on this system as this book progresses, allowing you to learn
    about the different components that are needed to build more sophisticated data
    annotation interfaces, active learning algorithms, and evaluation strategies.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章致力于构建你的第一个人机交互机器学习MVP。随着本书的进展，我们将在此基础上构建系统，让你了解构建更复杂的数据标注界面、主动学习算法和评估策略所需的不同组件。
- en: Sometimes, a simple system is enough. Suppose that you work at a media company,
    and your job is to tag news articles according to their topic. You already have
    topics such as sports, politics, and entertainment. Natural disasters have been
    in the news lately, and your boss has asked you to annotate the relevant past
    news articles as disaster-related to allow better search for this new tag. You
    don’t have months to build out an optimal system; you want to get an MVP out as
    quickly as possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，一个简单的系统就足够了。假设你在一家媒体公司工作，你的工作是按照主题对新闻文章进行标记。你已经有了体育、政治和娱乐等主题。最近自然灾害的新闻报道很多，你的老板要求你标注相关的过去新闻文章为灾害相关，以便更好地搜索这个新标签。你没有几个月的时间来构建一个最优的系统；你希望尽快推出一个最小可行产品（MVP）。
- en: '2.1 Beyond hacktive learning: Your first active learning algorithm'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 超越黑客式学习：你的第一个主动学习算法
- en: You may not realize it, but you’ve probably used active learning before. As
    you learned in chapter 1, active learning is the process of selecting the right
    data for human review. Filtering your data by keyword or some other preprocessing
    step is a form of active learning, although not an especially principled one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能没有意识到，但你可能之前已经使用过主动学习了。正如你在第一章中学到的，主动学习是选择合适数据供人类审查的过程。通过关键词或其他预处理步骤过滤数据是一种主动学习形式，尽管它不是特别有原则性。
- en: 'If you have only recently started experimenting with machine learning, you
    have probably used common academic datasets such as ImageNet, the MNIST optical
    character recognition (OCR) dataset, and the CoNLL named entity recognition (NER)
    datasets. These datasets were heavily filtered with various sampling techniques
    before the actual training data was created. So if you randomly sample from any
    of these popular datasets, your sample is not truly random: it is a selection
    of data that conforms to whatever sampling strategies were used when these datasets
    were created. In other words, you unknowingly used a sampling strategy that was
    probably some handcrafted heuristic from more than a decade ago. You will learn
    more sophisticated methods in this text.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你最近才开始尝试机器学习实验，你可能已经使用了常见的学术数据集，例如ImageNet、MNIST光学字符识别（OCR）数据集和CoNLL命名实体识别（NER）数据集。在创建实际训练数据之前，这些数据集都经过了各种采样技术的严格筛选。因此，如果你从这些流行的数据集中随机采样，你的样本并不是真正随机的：它是对在创建这些数据集时使用的任何采样策略所符合的数据的选择。换句话说，你无意中使用了可能来自十多年前的一些手工制作的启发式方法作为采样策略。在这篇文章中，你将学习到更复杂的方法。
- en: 'There is a good chance that you have used ImageNet, MNIST OCR, or the CoNLL
    NER datasets without realizing how filtered they are. Little formal documentation
    is available, and not mentioned in most that use these datasets, as I know by
    chance. ImageNet was created by colleagues when I was at Stanford; I ran one of
    the 15 research teams in the original CoNLL NER task; and I learned about the
    limitations of MNIST when it was mentioned in a now-famous foundational Deep Learning
    paper. It is obviously not ideal that piecing together how an existing dataset
    was created is so difficult and arbitrary, but until this book, there is no place
    telling you: *don’t trust any existing dataset to be representative of data that
    you encounter in the real world*.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您很可能已经使用了ImageNet、MNIST OCR或CoNLL NER数据集，但没有意识到它们是如何被过滤的。关于这些数据集的正式文档很少，而且据我所知，在大多数使用这些数据集的人中都没有提及。ImageNet是在我在斯坦福大学时由同事创建的；我是原始CoNLL
    NER任务中的15个研究团队之一；当我看到一篇现在著名的深度学习基础论文中提到MNIST时，我了解到了MNIST的局限性。显然，拼凑现有数据集是如何创建的是如此困难且随意，但直到这本书，还没有一个地方告诉您：*不要相信任何现有的数据集能够代表您在现实世界中遇到的数据*。
- en: Because you are probably using filtered data by the time you build a machine
    learning model, it can be helpful to think of most machine learning problems as
    already being in the middle of the iteration process for active learning. Some
    decisions about data sampling have already been made; they led you to the current
    state of what data is annotated, and they probably weren’t entirely optimal. So
    one of the first things you need to worry about is how to start sampling the right
    data as you move forward.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您在构建机器学习模型时可能已经使用了过滤后的数据，因此将大多数机器学习问题视为已经处于主动学习迭代过程的中期可能会有所帮助。一些关于数据采样的决策已经做出；它们引导您到达当前的数据标注状态，并且可能并不完全最优。因此，您需要首先关注的是如何在前进的过程中开始正确地采样数据。
- en: If you aren’t explicitly implementing a good active learning strategy, instead
    employing ad hoc methods to sample your data, you are implementing hacktive learning.[¹](#pgfId-1005852)
    It’s fine to hack something together, but it is better to get the fundamentals
    right even if you are doing something quickly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有明确实现一个好的主动学习策略，而是采用临时方法来采样您的数据，那么您就是在实施“黑客式学习”。[¹](#pgfId-1005852) 组装一些东西是可以的，但即使您做得很快，也要确保基础正确。
- en: Your first human-in-the-loop machine learning system is going to look something
    like figure 2.1\. For the remainder of this chapter, you will be implementing
    this architecture. This chapter assumes that you will be using the dataset introduced
    in section 2.2, but you can easily use your own data instead. Alternatively, you
    can build the system described here; then, by making changes in the data and annotation
    instructions, you should be able to drop in your own text annotation task.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一个带人类交互的机器学习系统将类似于图2.1所示。在本章的剩余部分，您将实现这个架构。本章假设您将使用2.2节中介绍的数据集，但您也可以轻松地使用自己的数据。或者，您可以构建这里描述的系统；然后，通过更改数据和标注说明，您应该能够将您自己的文本标注任务插入其中。
- en: '![](../Images/CH02_F01_Munro.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F01_Munro.png)'
- en: Figure 2.1 The architecture of your first human-in-the-loop machine learning
    system
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 您的第一个带人类交互的机器学习系统架构
- en: 2.2 The architecture of your first system
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 您的第一个系统架构
- en: 'The first human-in-the-loop machine learning system that you will build in
    this text will label a set of news headlines as “disaster-related” or “not disaster-related.”
    This real-world task could have many application areas:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，您将构建的第一个带人类交互的机器学习系统将把一组新闻标题标注为“与灾难相关”或“非灾难相关”。这项现实世界的任务可能有多个应用领域：
- en: Using this dataset to build a machine learning model to help identify disaster-related
    news articles in real time to help with the response
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个数据集构建一个机器学习模型，以帮助实时识别与灾难相关的新闻文章，从而帮助应对
- en: Adding a new “disaster-related” tag to news articles to improve the searchability
    and indexability of a database
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为新闻文章添加一个新的“与灾难相关”标签，以提高数据库的可搜索性和可索引性
- en: Supporting a social study about how disasters are reported in the media by allowing
    someone to analyze the relevant headlines
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过允许某人分析相关标题来支持一项关于媒体如何报道灾难的社会研究
- en: In global epidemic tracking, identifying news articles about outbreaks is an
    important task. H5N1 (bird flu) was reported openly weeks before it was identified
    as a new strain of the flu, and H1N1 (swine flu) was reported openly months in
    advance. If these reports had been put in front of virologists and epidemiologists
    sooner, they would have recognized the patterns of new strains of the flu and
    could have reacted sooner. Although this use case for your first human-in-the-loop
    machine learning system is simple, it is a real-world use case that could save
    lives.[²](#pgfId-1005878)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在全球疫情追踪中，识别关于爆发的新闻文章是一项重要任务。H5N1（禽流感）在确定为流感新株之前几周就已经公开报道，而H1N1（猪流感）在几个月前就已经公开报道。如果这些报告能更早地呈现在病毒学家和流行病学家面前，他们就能识别流感新株的模式，并能够更快地做出反应。尽管您第一个带有人工智能的机器学习系统的用例很简单，但它是一个现实世界的用例，可能会挽救生命。[²](#pgfId-1005878)
- en: For data that you will be using throughout the book, you will use messages from
    several past disasters on which I worked as a professional disaster responder.
    In many of these cases, I ran the human-in-the-loop machine learning systems to
    process the data, so the examples are relevant to this text. The data includes
    messages sent following earthquakes in Haiti and Chile in 2010, floods in Pakistan
    in 2010, Hurricane Sandy in the United States in 2012, and a large collection
    of news headlines focused on disease outbreaks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您将在整本书中使用的数据，您将使用我在作为专业灾害响应者工作时处理的一些过去灾难的消息。在这些案例中，我运行了带有人工智能的机器学习系统来处理数据，因此这些示例与本文相关。数据包括2010年海地地震和智利地震后的消息、2010年巴基斯坦洪水、2012年美国飓风桑迪以及大量关注疾病爆发的新闻标题集合。
- en: 'You will be joining students in NLP at Stanford, data science students at Udacity,
    and high-school students enrolled in AI for All ([https://ai-4-all.org](http://ai-4-all.org/)),
    who are also using this dataset as part of their courses today. You will be doing
    the task introduced at the start of the chapter: classifying news headlines. You
    can download the code and data at [https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您将加入斯坦福大学NLP专业的学生、Udacity的数据科学学生以及报名参加“人工智能普及”（[https://ai-4-all.org](http://ai-4-all.org/））的高中生，他们今天也将使用这个数据集作为课程的一部分。您将执行本章开头介绍的任务：对新闻标题进行分类。您可以在[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)下载代码和数据。
- en: See the readme file for instructions on installing Python 3.6 or later and PyTorch
    on your machine. Versions of Python and PyTorch change rapidly, so I will keep
    the readme file updated with instructions for installation rather than try to
    include that information here.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅readme文件，了解如何在您的机器上安装Python 3.6或更高版本以及PyTorch的说明。Python和PyTorch的版本更新迅速，因此我将保持readme文件更新，提供安装说明，而不是尝试在此处包含该信息。
- en: 'If you are not familiar with PyTorch, start with the examples in this PyTorch
    tutorial: [http://mng.bz/6gy5](http://mng.bz/6gy5). The example in this chapter
    was adapted from a combination of this PyTorch example and the one in the PyTorch
    tutorial. If you become familiar with those two tutorials, all the code in this
    chapter should be clear to you. The data in the CSV files comprises two to five
    fields, depending on how processed it is, and looks something like the example
    in table 2.1.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉PyTorch，请从本PyTorch教程中的示例开始：[http://mng.bz/6gy5](http://mng.bz/6gy5)。本章中的示例是从PyTorch示例和PyTorch教程中的示例结合而来的。如果您熟悉这两个教程，本章中的所有代码都应该对您来说很清晰。CSV文件中的数据包括两个到五个字段，具体取决于其处理程度，看起来类似于表2.1中的示例。
- en: Table 2.1 An example data file, with the ID, actual text, active learning sampling
    strategy chosen, and score for that sampling strategy
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 一个示例数据文件，包含ID、实际文本、选定的主动学习采样策略和该策略的分数
- en: '| Text ID | Text | Label | Sampling strategy | Score |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 文本ID | 文本 | 标签 | 样本策略 | 分数 |'
- en: '| 596124 | Flood warning for Dolores Lake residents | 1 | Low confidence |
    0.5872 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 596124 | Dolores湖居民洪水预警 | 1 | 低置信度 | 0.5872 |'
- en: '| 58503 | First-aid workers arrive for earthquake relief | 1 | Random | 0.6234
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 58503 | 地震救援人员抵达 | 1 | 随机 | 0.6234 |'
- en: '| 23173 | Cyclists are lost trying to navigate new bike lanes | 0 | Random
    | 0.0937 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 23173 | 自行车手在尝试导航新自行车道时迷路 | 0 | 随机 | 0.0937 |'
- en: The data that you will be using in this chapter is from a large collection of
    news headlines. The articles span many years and hundreds of disasters, but most
    headlines *are not* disaster-related.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中您将使用的数据来自大量新闻标题的集合。这些文章跨越了许多年和数百次灾难，但大多数标题*并非*与灾难相关。
- en: 'There are four locations for data in the repo:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库中有四个数据位置：
- en: '*/training_data*—The data that your models will be trained on'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*/训练数据*—你的模型将用于训练的数据'
- en: '*/validation_data*—The data that your models will be tuned with'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*/验证数据*—你的模型将用于微调的数据'
- en: '*/evaluation_data*—The data that your models will be evaluated on for accuracy'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*/评估数据*—你的模型将用于评估准确性的数据'
- en: '*/unlabeled_data*—The large pool of data that you want to label'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*/未标记数据*—你想要标记的大量数据池'
- en: 'You will see the data in the CSV files in this repo, and they will have this
    format:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在这个存储库中的CSV文件中看到数据，并且它们将具有以下格式：
- en: 0\. Text ID (a unique ID for this item)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0\. 文本ID（此项目的唯一ID）
- en: 1\. Text (the text itself)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1\. 文本（文本本身）
- en: '2\. Label (the label: 1 = “disaster-related”; 0 = “not disaster-related”)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2\. 标签（标签：1 = “与灾难相关”；0 = “与灾难无关”）
- en: 3\. Sampling strategy (the active learning strategy that we used to sample this
    item)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3\. 样本策略（我们用来采样此项的主动学习策略）
- en: 4\. Confidence (the machine learning confidence that this item is “disaster-related”)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4\. 置信度（机器学习对此项是“与灾难相关”的置信度）
- en: (This list counts from 0 instead of 1 so that it will match the index of each
    field in the items/rows in the code).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: （此列表从0开始计数而不是1，以便它与代码中每个字段的项目/行的索引相匹配）。
- en: These fields are enough information for you to build your first model. You will
    see that the unlabeled data in the example does not yet have a label, sampling
    strategy, or confidence, for obvious reasons.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些字段足够你构建你的第一个模型。你会发现示例中的未标记数据还没有标签、样本策略或置信度，这是显而易见的原因。
- en: 'If you want to jump in right away, you can run this script:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要立即开始，你可以运行此脚本：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will initially be prompted to annotate messages as “disaster-related” or
    “not disaster-related” to create the evaluation data. Then you will be prompted
    to do the same again for the initial training data. Only then will you see models
    start being built on your data and the active learning process beginning. We will
    return to the code later in this chapter and introduce the strategy behind it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你最初将被提示将消息标注为“与灾难相关”或“与灾难无关”以创建评估数据。然后你将再次被提示对初始训练数据进行相同的操作。只有在这种情况下，你才会看到模型开始基于你的数据构建，并且主动学习过程开始。我们将在本章后面返回代码，并介绍其背后的策略。
- en: In an actual disaster, you would be classifying data into a large number of
    fine-grained categories. You might separate requests for food and water, for example,
    because people can go for much longer without food than without water, so requests
    for drinking water need to be responded to with more urgency than requests for
    food. On the other hand, you might be able to provide water locally with filtration,
    but food still needs to be shipped to the disaster-affected region for a longer
    period. As a result, different disaster relief organizations often focus on either
    food or water. The same is true for distinctions between types of medical aid,
    security, housing, and so on, all of which need fine-grained categories to be
    actionable. But in any of these situations, filtering between “relevant” and “not
    relevant” can be an important first step. If the volume of data is low enough,
    you might need machine learning assistance only to separate related from unrelated
    information; humans can take care of the rest of the categories. I have run disaster
    response efforts in which this was the case.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际灾难中，你将需要将数据分类到大量细粒度的类别中。例如，你可能需要区分食物和水的请求，因为人们没有食物可以忍受的时间比没有水要长得多，所以对饮用水的请求需要比食物的请求更紧急地响应。另一方面，你可能能够通过过滤在当地提供水，但食物仍需要运送到受灾地区，以更长时间的需求。因此，不同的救灾组织通常专注于食物或水。同样，对于医疗援助、安全、住房等类型的区分也是如此，所有这些都需要细粒度的类别才能实施。但在任何这些情况下，在“相关”和“不相关”之间进行过滤都可以是一个重要的第一步。如果数据量足够低，你可能只需要机器学习辅助来区分相关信息和非相关信息；人类可以处理其他类别。我参与过这样的救灾行动。
- en: 'Also, in most disasters, you wouldn’t be working in English. English makes
    up only about 5% of the world’s conversations daily, so around 95% of communications
    about disasters are not in English. The broader architecture could be applied
    to any language, however. The biggest difference is that English uses whitespace
    to break sentences into words. Most languages have more sophisticated prefixes,
    suffixes, and compounds that make individual words more complicated. Some languages,
    such as Chinese, don’t use whitespace between most words. Breaking words into
    their constituent parts (*morphemes*) is an important task in itself. In fact,
    this was part of my PhD thesis: automatically discovering word-internal boundaries
    for any language in disaster response communications. An interesting and important
    research area would be to make machine learning truly equal across the world,
    and I encourage people to pursue it!'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在大多数灾难中，你不会用英语工作。英语只占全球日常对话的约5%，因此大约95%关于灾难的交流不是用英语进行的。更广泛的结构可以应用于任何语言。最大的区别是英语使用空格将句子分割成单词。大多数语言都有更复杂的词缀、后缀和复合词，使得单个单词更加复杂。一些语言，如中文，在大多数单词之间不使用空格。将单词分解为其构成部分（*语素*）本身就是一个重要的任务。事实上，这是我博士论文的一部分：自动发现任何语言在灾难响应通信中的词内边界。一个有趣且重要的研究领域将是使机器学习真正在全球范围内平等，我鼓励人们追求这一目标！
- en: 'It helps to make your data assumptions explicit so that you can build and optimize
    the architecture that is best for your use case. It is good practice to include
    the assumptions in any machine learning system, so here are ours:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于使你的数据假设明确，这样你就可以构建和优化最适合你用例的架构。在任何机器学习系统中包含假设是一种好的实践，因此以下是我们的假设：
- en: The data is only in English.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据仅包含英语。
- en: The data is in different varieties of English (United Kingdom, United States,
    English as a second language).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据包含不同种类的英语（英国英语、美国英语、作为第二语言的英语）。
- en: We can use whitespace-delimited words as our features.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用空格分隔的单词作为我们的特征。
- en: A binary classification task is sufficient for the use case.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于这个用例，二元分类任务就足够了。
- en: It should be easy to see how the broader framework for human-in-the-loop machine
    learning will work for any similar use case. The framework in this chapter could
    be adapted to image classification almost as easily as to another text classification
    task, for example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 应该很容易看出，人类在循环机器学习的更广泛框架如何适用于任何类似的用例。本章中的框架几乎可以像对另一个文本分类任务一样容易地适应图像分类。
- en: 'If you have already jumped in, you will see that you are asked to annotate
    some additional data before you can build a model. This is good practice in general:
    looking at your data will give you better intuitions for every part of your model.
    See the following sidebar to learn why you should look at your data.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经开始了，你会看到在你可以构建模型之前，你需要标注一些额外的数据。这在一般情况下是一种好的实践：查看你的数据将为你模型的所有部分提供更好的直觉。参见以下侧边栏了解为什么你应该查看你的数据。
- en: Sunlight is the best disinfectant
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 阳光是最好的消毒剂
- en: '*Expert anecdote by Peter Skomoroch*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*彼得·斯科莫罗赫的专家轶事*'
- en: You need to look at real data in depth to know exactly what models to build.
    In addition to high-level charts and aggregate statistics, I recommend that data
    scientists go through a large selection of randomly selected, granular data regularly
    to let these examples wash over them. As executives look at company-level charts
    every week, and network engineers look over stats from system logs, data scientists
    should have intuition about their data and how it is changing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要深入查看真实数据，以确切知道要构建哪些模型。除了高级图表和汇总统计之外，我建议数据科学家定期浏览大量随机选择的、细粒度的数据，让这些例子影响他们。作为高管每周查看公司级图表，网络工程师查看系统日志的统计数据，数据科学家应该对其数据及其变化有直觉。
- en: When I was building LinkedIn’s Skill Recommendations feature, I built a simple
    web interface with a Random button that showed recommendation examples alongside
    the corresponding model inputs so that I could quickly view the data and get an
    intuition for the kinds of algorithms and annotation strategies that might be
    most successful. This approach is the best way to ensure that you have uncovered
    potential issues and obtained vital high-quality input data. You’re shining a
    light on your data, and sunlight is the best disinfectant.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我构建LinkedIn的技能推荐功能时，我创建了一个简单的网页界面，其中包含一个随机按钮，它显示了推荐示例以及相应的模型输入，这样我就可以快速查看数据，并了解可能最成功的算法和注释策略。这种方法是确保你已经发现潜在问题并获得了关键的高质量输入数据的最佳方式。你正在给你的数据带来光明，阳光是最好的消毒剂。
- en: '*Peter Skomoroch, the former CEO of SkipFlag (acquired by WorkDay), worked
    as a principal data scientist at LinkedIn on the team that invented the title
    “data scientist*.”'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*彼得·斯科莫罗赫（Peter Skomoroch），前SkipFlag（被WorkDay收购）的首席执行官，曾在LinkedIn担任首席数据科学家，是该团队发明“数据科学家”这一职位的人之一*。'
- en: 2.3 Interpreting model predictions and data to support active learning
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 解释模型预测和数据以支持主动学习
- en: 'Almost all supervised machine learning models will give you two things:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的监督机器学习模型都会给你两样东西：
- en: A predicted label (or set of predictions)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测标签（或一组预测标签）
- en: A number (or set of numbers) associated with each predicted label
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与每个预测标签相关联的数字（或一组数字）
- en: The numbers are generally interpreted as confidences in the prediction, although
    this can be more or less true depending on how the numbers are generated. If there
    are mutually exclusive categories with similar confidence, you have good evidence
    that the model is confused about its prediction and that human judgment would
    be valuable. Therefore, the model will benefit most when it learns to correctly
    predict the label of an item with an uncertain prediction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字通常被解释为预测的置信度，尽管这取决于数字是如何生成的，可能会有所不同。如果有相互排斥的类别并且具有相似的置信度，那么你有很好的证据表明模型对其预测感到困惑，并且人类的判断将是有价值的。因此，当模型学会正确预测具有不确定预测的项目的标签时，它将受益最大。
- en: 'Suppose that we have a message that might be disaster-related, and the prediction
    looks like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个可能与灾难相关的消息，预测看起来是这样的：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In this prediction, the message is predicted to be “Not Disaster-Related.”
    In the rest of supervised machine learning, this label is what people care about
    most: was the label prediction correct, and what is the overall accuracy of the
    model when predicting across a large held-out dataset?'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个预测中，消息被预测为“非灾难相关”。在其余的监督机器学习中，这个标签是人们最关心的：标签预测是否正确，以及模型在预测大量保留数据集时的整体准确率是多少？
- en: In active learning, however, the numbers associated with the prediction typically
    are what we care about most. You can see in the example that “Not Disaster-Related”
    is predicted with a 0.524 score. This score means that the system is 52.4% confident
    that the prediction was correct.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在主动学习过程中，与预测相关的数字通常是我们最关心的。你可以从例子中看到，“非灾难相关”被预测为0.524分。这个分数意味着系统有52.4%的信心认为预测是正确的。
- en: 'From the perspective of the task here, you can see why you might want a human
    to review the result anyway: there is still a relatively high chance that this
    is disaster-related. If it *is* disaster-related, your model is getting this example
    wrong for some reason, so it is likely that you want to add it to your training
    data so that you don’t miss similar examples.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里任务的视角来看，你可以看到为什么你可能仍然希望有人审查结果：这仍然有相对较高的可能是与灾难相关的。如果它*确实*与灾难相关，那么你的模型可能因为某些原因在这个例子上犯了错误，所以你很可能希望将其添加到你的训练数据中，以免错过类似的例子。
- en: In chapter 3, we will turn to the problem of how reliable a score of 0.524 is.
    Especially for neural models, these confidences can be widely off. For the sake
    of this chapter, we can assume that although the exact number may not be accurate,
    we can generally trust the relative differences in confidence across multiple
    predictions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们将转向如何可靠地评估0.524分的这个问题。特别是对于神经网络模型，这些置信度可能会相差很大。为了本章的目的，我们可以假设虽然确切数字可能不准确，但我们通常可以相信多个预测中置信度相对差异的准确性。
- en: 2.3.1 Confidence ranking
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 置信度排名
- en: 'Suppose that we had another message with this prediction:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们还有另一个带有这种预测的消息：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This item is also predicted as “Not Disaster-Related” but with 98.4% confidence,
    compared with 52.4% confidence for the first item. So the model is more confident
    about the second item than about the first. Therefore, it is reasonable to assume
    that the first item is more likely to be wrongly labeled and would benefit from
    human review. Even if we don’t trust the 52.4% and 98.4% numbers (and we probably
    shouldn’t, as you will learn in later chapters), it *is* reasonable to assume
    that the rank order of confidence will correlate with accuracy. This will generally
    be true of almost all machine learning algorithms and almost all ways of calculating
    accuracy: you can rank-order the items by the predicted confidence and sample
    the lowest-confidence items. For a probability distribution over a set of labels
    *y* for the item *x*, the confidence is given by the equation, where y* is the
    most confident (c) label:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此项也被预测为“非灾难相关”，但置信度为98.4%，而第一项的置信度仅为52.4%。因此，模型对第二项的信心比对第一项的信心更大。因此，合理地假设第一项更有可能被错误标记，并将从人工审查中受益。即使我们不信任52.4%和98.4%的数字（正如你将在后面的章节中了解到的那样），假设置信度排名将与准确性相关是合理的。这几乎适用于几乎所有机器学习算法和几乎所有计算准确性的方法：你可以根据预测的置信度对项目进行排名，并抽取置信度最低的项目。对于一个关于项目*x*的标签集合*y*的概率分布，置信度由以下方程给出，其中y*是最有信心（c）的标签：
- en: '![](../Images/CH02_F01_Munro_E01.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F01_Munro_E01.png)'
- en: For a binary prediction task like this example, you can simply rank by confidence
    and sample the items closest to 50% confidence. If you are attempting anything
    more complicated, however, such as predicting three or more mutually exclusive
    labels, labeling sequences of data, generating entire sentences (including translation
    and speech transcription), or identifying objects within images and videos, you
    have multiple ways to calculate confidence. We will return to other ways of calculating
    confidence in later chapters. The intuition about low confidence remains the same,
    and a binary task is easier for your first human-in-the-loop system.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像这个例子这样的二元预测任务，你可以简单地按置信度排名，并抽取置信度接近50%的项目。然而，如果你尝试更复杂的事情，比如预测三个或更多互斥的标签、标记数据序列、生成整个句子（包括翻译和语音转录）或在图像和视频中识别对象，你有多种方法来计算置信度。我们将在后面的章节中回到其他计算置信度的方法。关于低置信度的直觉保持不变，二元任务对于你的第一个人工系统来说更容易。
- en: 2.3.2 Identifying outliers
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 识别异常值
- en: 'As discussed in chapter 1, you often want to make sure that you are getting
    a diverse set of items for humans to label so that the newly sampled items aren’t
    all like each other. This task can include making sure that you are not missing
    any important outliers. Some disasters are rare, such as a large asteroid crashing
    into the Earth. If a news headline says “Asteroid flattens Walnut Creek,” and
    your machine learning model hasn’t learned what an asteroid is or that Walnut
    Creek is a city, it is easy to see why your machine learning model might not have
    predicted this headline as being disaster-related. You could call this sentence
    an outlier in this regard: it lies farthest from anything you’ve seen before.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如第1章所述，你通常想确保你为人工标记获得的项目种类多样，以便新抽取的项目不会都相似。这项任务可以包括确保你没有遗漏任何重要的异常值。一些灾难是罕见的，例如一颗大型小行星撞击地球。如果新闻标题说“小行星使核桃溪变平”，而你的机器学习模型还没有学会什么是小行星或核桃溪是一个城市，那么很容易理解为什么你的机器学习模型可能没有预测这个标题与灾难相关。你可以称这个句子在这个方面的一个异常值：它离你之前见过的任何东西都最远。
- en: 'As with confidence ranking, we have many ways to ensure that we are maximizing
    the diversity of the content that is selected for human review. You will learn
    more about such approaches in later chapters. For now, we will focus on a simple
    metric: the average training data frequency of words in each unlabeled item. Here
    is the strategy that we will implement in this chapter:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与置信度排名一样，我们有多种方法来确保我们最大化了为人工审查选择的内容的多样性。你将在后面的章节中了解更多关于这些方法的内容。现在，我们将关注一个简单的指标：每个未标记项目中单词的平均训练数据频率。以下是本章我们将实施的策略：
- en: For each item in the unlabeled data, count the average number of word matches
    it has with items already in the training data.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于未标记数据中的每个项目，计算它与训练数据中已有项目的平均单词匹配数。
- en: Rank the items by their average match.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据平均匹配数对项目进行排名。
- en: Sample the item with the lowest average number of matches.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抽取平均匹配数最低的项目。
- en: Add that item to the labeled data.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该项目添加到标记数据中。
- en: Repeat these steps until you have sampled enough for one iteration of human
    review.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这些步骤，直到你采样足够的数据进行一次人工审核迭代。
- en: Note that in step 4, when you have sampled the first item, you can treat that
    item as being labeled, because you know you are going to get a label for it later.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在第4步中，当你采样了第一个项目后，你可以将该项目视为已标记，因为你知道你稍后会为它获取一个标签。
- en: This method of determining outliers tends to favor small and novel headlines,
    so you will see that code adds 1 to the count as a smoothing factor. It also disfavors
    sentences with a lot of common words such as *the*, even if the other words are
    uncommon. So instead of using average matching, you could track the raw number
    of novel words to model the total amount of novel information in a headline instead
    of the overall average.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这种确定异常值的方法往往倾向于偏好小型和新颖的标题，因此你会看到代码将1加到计数中作为平滑因子。它还不喜欢包含许多常见单词（如*the*）的句子，即使其他单词不常见。因此，你与其使用平均匹配，不如跟踪新颖单词的原始数量，以模拟标题中新颖信息的总量，而不是整体平均值。
- en: You could also divide the number of matches in the training data by the total
    number of times that word occurs across all the data and multiply each of these
    fractions, which would more or less give you the Bayesian probability of the element’s
    being an outlier. Instead of using word matching, you could use more sophisticated
    edit-distance-based metrics that take the order of words within the sentence into
    account. Or you can use many other string-matching and other algorithms to determine
    outliers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将训练数据中的匹配数除以该词在所有数据中出现的总次数，然后将这些分数相乘，这大致会给你提供该元素是异常值的贝叶斯概率。你不仅可以使用词匹配，还可以使用更复杂的基于编辑距离的指标，这些指标会考虑句子中单词的顺序。或者，你可以使用许多其他字符串匹配和其他算法来确定异常值。
- en: 'As with everything else, you can start by implementing the simple example in
    this chapter and experiment with others later. The main goal is an insurance policy:
    is there something completely different that we haven’t seen yet? Probably not,
    but if there were, it would be the highest-value item to annotate correctly. We
    will look at ways of combining sampling by confidence and sampling by diversity
    in chapter 5.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他一切一样，你可以从实现本章中的简单示例开始，以后再尝试其他示例。主要目标是保险策略：是否有什么完全不同的事情我们还没有看到？可能没有，但如果有的话，那将是正确注释的最高价值项。我们将在第5章中探讨结合置信度采样和多样性采样的方法。
- en: 'We will also look at ways to combine your machine learning strategy with your
    annotation strategy. If you have worked in machine learning for a while but never
    in annotation or active learning, you have probably optimized models only for
    accuracy. For a complete architecture, you may want to take a more holistic approach
    in which your annotation, active learning, and machine learning strategies inform
    one another. You could decide to implement machine learning algorithms that can
    give more accurate estimates of their confidence at the expense of accuracy in
    label prediction. Or you might augment your machine learning models to have two
    types of inference: one to predict the labels and one to more accurately estimate
    the confidence of each prediction. If you are building models for more complicated
    tasks such as generating sequences of text (as in machine translation) or regions
    within images (as in object detection), the most common approach today is building
    separate inference capabilities for the task itself and interpreting the confidence.
    We will look at these architectures in chapters 9–11 of this book.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何将你的机器学习策略与你的注释策略相结合。如果你在机器学习领域工作了一段时间，但从未参与过注释或主动学习，你可能只优化了模型以实现准确性。对于完整的架构，你可能希望采取更全面的方法，其中你的注释、主动学习和机器学习策略相互告知。你可以决定实施可以提供更准确置信度估计的机器学习算法，但这可能会牺牲标签预测的准确性。或者，你可能增强你的机器学习模型以具有两种类型的推理：一种用于预测标签，另一种用于更准确地估计每个预测的置信度。如果你正在构建用于更复杂任务的模型，例如生成文本序列（如机器翻译）或图像中的区域（如目标检测），今天最常见的方法是为该任务本身构建单独的推理能力并解释置信度。我们将在本书的第9章到第11章中探讨这些架构。
- en: The process for building your first human-in-the-loop machine learning model
    is summarized in figure 2.2.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 构建你的第一个带人工审核的机器学习模型的过程总结在图2.2中。
- en: '![](../Images/CH02_F02_Munro.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F02_Munro.png)'
- en: Figure 2.2 The iterative process in your first human-in-the-loop machine learning
    system. Initially (top), you are annotating a random sample of unlabeled items
    to set aside as your evaluation data. Then you are labeling the first items to
    be used for training data (middle), also starting with a random selection. After
    this point, you start using active learning (bottom) to sample items that are
    low-confidence or outliers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 在你的第一个包含人类操作员的机器学习系统中，迭代过程。最初（顶部），你正在标注未标记项目的随机样本，将其作为评估数据。然后你开始标注用于训练数据的第一批项目（中间），同样从随机选择开始。在此之后，你开始使用主动学习（底部）来抽样低置信度或异常值的项目。
- en: 2.3.3 What to expect as you iterate
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 迭代过程中可以期待什么
- en: In our example code, after we have enough evaluation and initial training data,
    we will iterate on active learning every 100 items. This number is probably a
    little small in terms of the number of items per iteration, as you’ll be spending
    a lot of time waiting for the model to retrain for a relatively small number of
    new labeled items, but 100 is about right to get a feel for how much the sampled
    data changes in each iteration.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例代码中，在我们有足够的评估和初始训练数据后，我们将每100个项目迭代一次主动学习。从每次迭代的项数来看，这个数字可能有点小，因为你将花费大量时间等待模型重新训练相对较少的新标注项目，但100个是一个合适的数字，可以让你感受到每次迭代中采样数据的变化程度。
- en: 'Here are some things you may notice as you iterate through the active learning
    process:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在你通过主动学习过程迭代时，可能会注意到以下一些事情：
- en: '*First iteration*—You are annotating mostly “not disaster-related” headlines,
    which can feel tedious. The balance will improve when active learning kicks in,
    but for now, it is necessary to get the randomly sampled evaluation data. You
    should also notice that this problem is not trivial because journalists often
    use disaster metaphors for nondisasters, especially sports teams (declaring war,
    a scoring drought, and so on). You will also be challenged by edge cases. Is a
    plane crash a disaster, for example, or does its status depend on the size of
    the plane and/or the cause? These edge cases will help you refine the definition
    of your task and create the right instructions for engaging a larger workforce
    to annotate your data at scale.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第一次迭代*—你主要在标注“非灾难相关”的标题，这可能会感觉有些枯燥。当主动学习开始发挥作用时，这种平衡将会改善，但到目前为止，获取随机抽样的评估数据是必要的。你也应该注意到，这个问题并不简单，因为记者经常将灾难隐喻用于非灾难事件，尤其是体育团队（宣布战争、得分干旱等）。你还将面临边缘案例的挑战。例如，飞机坠毁是否算作灾难，或者其状态是否取决于飞机的大小和/或原因？这些边缘案例将帮助你细化任务定义，并为吸引更多劳动力大规模标注数据制定正确的指导方针。'
- en: '*Second iteration*—You have created your first model! Your F-score is probably
    terrible, maybe only 0.20\. Your area under the curve (AUC), however, might be
    around 0.75\. (See the appendix for more on F-score and AUC.) So despite the bad
    accuracy, you can find disaster-related messages better than chance. You could
    fix the F-score by playing with the model parameters and architecture, but more
    data is more important than model architecture right now, as will become clear
    when you start annotating: you will immediately notice on your second iteration
    that a large number of items is disaster-related. In fact, most of the items may
    be. Early on, your model will still try to predict most things as “not disaster-related,”
    so anything close to 50% confidence is at the “disaster-related” end of the scale.
    This example shows that active learning can be self-correcting: it is oversampling
    a lower-frequency label without requiring you to explicitly implement a targeted
    strategy for sampling important labels. You will also see evidence of overfitting.
    If your randomly selected items in the first iteration happened to have many headlines
    about floods, for example, you probably have *too* many headlines about floods
    and not enough about other types of disasters.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第二次迭代*—你已经创建了你的第一个模型！你的F分数可能非常糟糕，可能只有0.20。然而，你的曲线下面积（AUC）可能大约是0.75。（参见附录了解更多关于F分数和AUC的信息。）所以尽管准确度不高，你比随机选择能更好地找到与灾难相关的信息。你可以通过调整模型参数和架构来修复F分数，但更多数据比模型架构更重要，当你开始标注时这一点将变得清晰：你将在第二次迭代时立即注意到大量项目与灾难相关。实际上，大部分项目可能都是。在早期，你的模型仍然会尝试将大多数事情预测为“非灾难相关”，所以任何接近50%置信度的都位于“与灾难相关”的端点。这个例子表明，主动学习可以自我纠正：它在不要求你明确实现针对重要标签的采样策略的情况下，对低频标签进行了过度采样。你还将看到过度拟合的证据。例如，如果你的第一次迭代中随机选择的项目恰好有很多关于洪水的大标题，你可能有*太多*关于洪水的大标题，而关于其他类型灾难的则不够。'
- en: '*Third and fourth iterations*—You should start to see model accuracy improve,
    as you are now labeling many more “disaster-related” headlines, bringing the proposed
    annotation data closer to 50:50 for each label. If your model had overfitted some
    terms, such as the floods example, you should have seen some counterexamples,
    such as “New investment floods the marketplace,” These counterexamples help push
    your models back to more accurate predictions for headlines with these terms.
    If the data was genuinely disaster-related for everything with *floor* in it,
    these items are now predicted with high confidence and are no longer near 50%.
    Either way, the problem self-corrects, and the diversity of the headlines you
    are seeing should increase.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第三次和第四次迭代*—你应该开始看到模型准确率提高，因为你现在标注了更多与“灾难相关”的标题，使得每个标签的标注数据更接近50:50。如果你的模型对某些术语（如洪水示例）进行了过度拟合，你应该已经看到了一些反例，例如“新投资涌入市场”，这些反例有助于将你的模型推向对这些术语的标题进行更准确的预测。如果包含“地板”的所有内容确实与灾难相关，这些项目现在将以高置信度预测，并且不再接近50%。无论如何，问题会自我纠正，你所看到的标题多样性应该会增加。'
- en: '*Fifth to tenth iterations*—Your models start to reach reasonable levels of
    accuracy, and you should see more diversity in the headlines. As long as either
    the F-score or AUC goes up by a few percentage points for every 100 annotations,
    you are getting good gains in accuracy. You are probably wishing that you had
    annotated more evaluation data so that you could be calculating accuracy on a
    bigger variety of held-out data. Unfortunately, you can’t. It’s almost impossible
    to go back to truly random sampling unless you are prepared to give up a lot of
    your existing labels.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第五次到第十次迭代*—你的模型开始达到合理的准确率水平，你应该会看到标题的更多多样性。只要每100个标注中F分数或AUC提高几个百分点，你就在准确率上取得了良好的提升。你可能希望你有更多的标注评估数据，这样你就可以在更大的保留数据集上计算准确率。不幸的是，你做不到。除非你准备放弃大量的现有标签，否则几乎不可能回到真正的随机采样。'
- en: Although it feels simple, the system that you are building in this chapter follows
    the same strategy as the initial release of Amazon Web Services’s (AWS) SageMaker
    Ground Truth in 2018 (less than a year before this chapter was written). In fact,
    in the first version, SageMaker sampled only by confidence and didn’t look for
    outliers in that release. Although the system you are building is simple, it is
    beyond the level of algorithmic sophistication of an active learning tool that
    is currently offered by a major cloud provider. I worked briefly on SageMaker
    Ground Truth when I was at AWS, so this is not a criticism of that product or
    my colleagues who put much more work into it than I did. Although active learning
    is becoming part of large-scale commercial offerings for the first time, it is
    still in an early stage.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然感觉很简单，但本章中你构建的系统遵循与2018年（在本章撰写前不到一年）亚马逊网络服务（AWS）SageMaker Ground Truth初始发布相同的策略。事实上，在第一个版本中，SageMaker仅根据置信度进行采样，在该版本中没有寻找异常值。虽然你构建的系统很简单，但它超越了目前由主要云服务提供商提供的主动学习工具的算法复杂性水平。我在AWS工作时短暂地参与了SageMaker
    Ground Truth的开发，所以这并不是对该产品或我的同事的批评，他们比我投入了更多的工作。尽管主动学习首次成为大规模商业产品的一部分，但它仍然处于早期阶段。
- en: We will cover more sophisticated methods for sampling in part 2 of this book.
    For now, it is more important to focus on establishing the iterative process for
    active learning, along with the best practices for annotation and retraining and
    evaluating your models. If you don’t get your iteration and evaluation strategies
    correct, you can easily make your model worse instead of better and not even realize
    it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的第二部分介绍更复杂的采样方法。目前，更重要的是关注建立主动学习的迭代过程，以及标注、重新训练和评估你的模型的最佳实践。如果你没有正确地制定迭代和评估策略，你可能会轻易地使你的模型变得更差而不是更好，甚至可能没有意识到这一点。
- en: 2.4 Building an interface to get human labels
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 构建获取人工标签的界面
- en: To label your data, you need to start with the right interface. We’ll cover
    what that looks like for our example data in this section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标注你的数据，你需要从正确的界面开始。在本节中，我们将介绍我们的示例数据应该是什么样的。
- en: The right interface for human labeling is as important as the right sampling
    strategy. If you can make your interface 50% more efficient, that’s as good as
    improving your active learning sampling strategy by 50%. Out of respect for the
    people who are doing the labeling, you should do as much as you can to ensure
    that they feel they are as effective as possible. If you genuinely don’t know
    whether an interface or algorithm improvement is the best thing to focus on next,
    start with the interface to improve the work of the humans, and worry about your
    CPU’s feelings later.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 适合人工标注的正确界面与合适的采样策略一样重要。如果你能将你的界面效率提高50%，那就相当于将你的主动学习采样策略提高了50%。出于对进行标注的人的尊重，你应该尽可能确保他们觉得他们尽可能有效。如果你真的不知道界面或算法改进是下一步的最佳关注点，那么从改善人类工作的界面开始，以后再考虑CPU的感受。
- en: 'Part 3 of this book is dedicated to data annotation, so we will make a few
    assumptions to keep the discussion in this chapter simple:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本书第三部分致力于数据标注，因此我们将做出一些假设以保持本章讨论的简单性：
- en: Annotators aren’t making a significant number of errors in the labels, so we
    don’t have to implement quality control for annotations.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注者没有在标签上犯很多错误，所以我们不需要对标注实施质量控制。
- en: Annotators understand the task and labels perfectly, so they aren’t accidentally
    choosing the wrong labels.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注者完全理解任务和标签，所以他们不会意外地选择错误的标签。
- en: Only one annotator is working at a time, so we don’t have to keep track of any
    labeling in progress.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个标注者在同一时间工作，所以我们不需要跟踪任何正在进行的标注。
- en: These assumptions are big ones. In most deployed systems, you need to implement
    quality control to ensure that annotators are not making mistakes; you will most
    likely need several iterations of annotation to refine the definitions of the
    labels and instructions; and you will need a system to track work assigned to
    multiple people in parallel. A simple annotation interface like the one discussed
    here is enough if you want to annotate some data quickly for exploratory purposes,
    as you are doing here.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些假设是很大的。在大多数部署的系统中，你需要实施质量控制以确保标注者不会犯错误；你很可能会需要多次迭代标注来细化标签和指令的定义；并且你需要一个系统来跟踪分配给多个人并行工作的任务。如果你只想快速标注一些数据用于探索目的，就像你现在所做的那样，一个简单的标注界面就足够了。
- en: 2.4.1 A simple interface for labeling text
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 用于标注文本的简单界面
- en: 'The interface that you build is determined by your task and the distribution
    of your data. For a binary labeling task like the one we are implementing here,
    a simple command-line interface is enough (figure 2.3). You will see it immediately
    if you run the script that we introduced in this chapter:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你构建的界面取决于你的任务和数据分布。对于像我们在这里实施的二进制标注任务，一个简单的命令行界面就足够了（图2.3）。如果你运行本章中介绍的脚本，你会立即看到它：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/CH02_F03_Munro.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F03_Munro.png)'
- en: Figure 2.3 The command-line interface annotation tool for the example in this
    chapter
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 本章示例的命令行界面标注工具
- en: 'As discussed in the introduction, many human–computer interaction factors go
    into making a good interface for annotation. But if you have to build something
    quickly, do the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如引言中所述，许多人机交互因素都涉及到制作一个好的标注界面。但如果你必须快速构建一些东西，请执行以下操作：
- en: Build an interface that allows annotators to focus on one part of the screen.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个界面，让标注者能够专注于屏幕的一部分。
- en: Allow hot keys for all actions.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 允许所有操作的热键。
- en: Include a back/undo option.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含一个后退/撤销选项。
- en: Get those three things right first, and graphic design can come later.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先确保这三件事正确，然后图形设计可以稍后进行。
- en: To see exactly what the code is doing, look at the repo at [https://github.com/
    rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning),
    or clone it locally and experiment with it. Excerpts from that code will be shared
    in this book for illustrative purposes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要确切了解代码在做什么，请查看[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)上的仓库，或者将其本地克隆并对其进行实验。为了说明目的，本书将分享该代码的摘录。
- en: You can see the code to elicit annotations in the first 20 lines of the `get_annotations()`
    function in the following listing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下列表中的`get_annotations()`函数的前20行中看到引发标注的代码。
- en: Listing 2.1 Sampling the unlabeled items that we want to annotate
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 标注我们想要标注的无标签项
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The input() function prompts the user for input.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `input()`函数提示用户输入。
- en: 'For our data, the labels are a little unbalanced because most headlines are
    not related to disasters. This fact has interface-design implications. It would
    be inefficient and boring for someone to continually select “Not Disaster-Related.”
    You can make “Not Disaster-Related” the default option to improve efficiency so
    long as you have a back option when annotators inevitably get primed to select
    the default. You probably did this yourself: annotated quickly and then had to
    go back when you pressed the wrong answer. You should see this functionality in
    the next and final 20 lines of code of the `get_annotations()` function.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，标签有点不平衡，因为大多数标题与灾难无关。这一事实对界面设计有影响。如果有人不断选择“与灾难无关”，这将既低效又无聊。只要你有标注者不可避免地被诱导选择默认选项的后备选项，你就可以将“与灾难无关”作为默认选项以提高效率。你可能自己就是这样做的：快速标注后，当你按下错误答案时不得不返回。你应该在`get_annotations()`函数的下一部分和最后的20行代码中看到这个功能。
- en: Listing 2.2 Allowing the annotator to go back to avoid errors through repetition
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 允许标注者通过重复来避免错误
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 2.4.2 Managing machine learning data
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 管理机器学习数据
- en: For a deployed system, it is best to store your annotations in a database that
    takes care of backups, availability, and scalability. But you cannot always browse
    a database as easily as you can files on a local machine. In addition to adding
    training items to your database, or if you are building a simple system, it can
    help to have locally stored data and annotations that you can quickly spot-check.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个部署的系统，最好将你的注释存储在一个负责备份、可用性和可扩展性的数据库中。但你不能像在本地机器上浏览文件那样轻松地浏览数据库。除了将训练项目添加到你的数据库中，或者如果你正在构建一个简单的系统，拥有可以快速检查的本地存储数据和注释可能会有所帮助。
- en: In our example, we will separate the data into separate files according to the
    label, for additional redundancy. Unless you are working in an organization that
    already has good data-management processes in place for annotation and machine
    learning, you probably don’t have the same kind of quality control for your data
    as you do for your code, such as unit tests and good versioning. So it is wise
    to be redundant in how you store your data. Similarly, you will see that the code
    appends files but never writes over files. It also keeps the unlabeled_data.csv
    file untouched, checking for duplicates in the other datasets instead of deleting
    headlines from that file when the item has been labeled.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将根据标签将数据分开存储到单独的文件中，以增加冗余。除非你在一个已经为标注和机器学习建立了良好数据管理流程的组织中工作，否则你可能没有像对代码那样对数据进行相同的质量控制，例如单元测试和良好的版本控制。因此，在存储数据的方式上保持冗余是明智的。同样，你也会看到代码会附加文件，但永远不会覆盖文件。它还会保持未标记数据.csv文件不变，在项目被标记时检查其他数据集的重复项，而不是从该文件中删除标题。
- en: Redundancy in how you store labels and enforcing nondeletion of data will save
    you a lot of headaches when you start experimenting. I’ve never met a machine
    learning professional who hasn’t accidentally deleted labeled data at some point,
    so follow this advice! Also remember that if you are storing data on your local
    machine, that data may belong to someone else or have sensitive content. Make
    sure that you have permission to store the data, and delete the data when you
    no longer need it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储标签的方式上保持冗余并强制不删除数据，当你开始实验时将节省你很多麻烦。我从未遇到过一位机器学习专业人士没有在某个时候意外删除过标记数据，所以请遵循这些建议！同时，也请记住，如果你在本地机器上存储数据，这些数据可能属于其他人或包含敏感内容。确保你有权存储这些数据，并在不再需要时删除数据。
- en: Although the topic isn’t covered in this book, version control for your data
    is also important, especially if you are updating your instructions as you go.
    Some older labels may be incorrect, and you want to be able to reproduce them
    if you want to re-create your active learning iterations later.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个主题没有在本书中涵盖，但对你数据版本控制同样重要，尤其是当你边更新指令边进行时。一些较旧的标签可能是不正确的，如果你想在以后重新创建你的主动学习迭代，你希望能够重新生成它们。
- en: 2.5 Deploying your first human-in-the-loop machine learning system
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 部署你的第一个闭环机器学习系统
- en: Now let’s put all the pieces of your first human-in-the-loop system together!
    If you didn’t do so earlier in the chapter, download the code and data from [https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning),
    and see the readme file for installation instructions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将您第一个闭环系统中所有部件组合起来！如果您在本章的早期没有这样做，请从[https://github.com/rmunro/pytorch_active_learning](https://github.com/rmunro/pytorch_active_learning)下载代码和数据，并查看readme文件以获取安装说明。
- en: You can run this code immediately, and it will start prompting you to annotate
    data and automatically train after each iteration. You should experience the changes
    in data at each iteration that you learned in section 2.3.3.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以立即运行此代码，它将开始提示您标注数据，并在每次迭代后自动训练。您应该体验在2.3.3节中学到的每个迭代中的数据变化。
- en: To see what is happening under the hood, let’s go through the main components
    of this code and the strategies behind it. We use a simple PyTorch machine learning
    model for text classification. We will use a shallow model that can be retrained
    quickly to make our iterations fast. In PyTorch, this entire model definition
    is a dozen lines of code.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解底层发生了什么，让我们通过此代码的主要组件及其背后的策略来分析。我们使用一个简单的PyTorch机器学习模型进行文本分类。我们将使用一个可以快速重新训练的浅层模型来使我们的迭代快速进行。在PyTorch中，整个模型定义只有十几行代码。
- en: Listing 2.3 Simple PyTorch text classification model with one hidden layer
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3 具有一个隐藏层的简单PyTorch文本分类模型
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Hidden layer with 128 neurons/nodes
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 具有128个神经元/节点的隐藏层
- en: ❷ Output layer predicting each label
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输出层预测每个标签
- en: ❸ Optimizing our hidden layer with a ReLU activation function
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用ReLU激活函数优化我们的隐藏层
- en: ❹ Using linear activation function for our output layer
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用线性激活函数作为我们的输出层
- en: ❺ Returning the log softmax of our linear output to optimize our model in training
    and to return as a probability distribution for prediction
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将我们的线性输出的log softmax返回以优化训练中的模型，并将其作为预测的概率分布返回
- en: Our input layer contains the one-hot encoding for every word in our feature
    set (thousands), our output layer is the two labels, and our hidden layer is 128
    nodes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入层包含特征集中每个单词的一热编码（数千个），输出层是两个标签，隐藏层有128个节点。
- en: 'For training, we know that the data is imbalanced between the labels initially,
    so we want to ensure that we select something closer to an even number of items
    for each label. This specification is set in these variables at the start of the
    code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们知道标签之间的数据最初是不平衡的，因此我们想要确保我们为每个标签选择接近偶数个项目的某种东西。此规范在代码开始时设置在这些变量中：
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We are going to train our models for 10 epochs, and for each epoch, we are going
    to randomly select 200 items from each label. This approach won’t make our model
    completely even, because we are still selecting from a bigger variety of not-disaster-related
    text across all the epochs, but it will be enough that we get some signal from
    our data even when we have only 100 or so disaster-related examples.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练我们的模型10个周期，并且对于每个周期，我们将从每个标签随机选择200个项目。这种方法不会使我们的模型完全平衡，因为我们仍然是在所有周期中选择来自非灾难相关文本的更大种类，但即使我们只有大约100个与灾难相关的示例，这也将足够我们从数据中获得一些信号。
- en: (The hidden neurons, epochs, and items selected per epoch are sensible but otherwise
    arbitrary starting points. You can experiment with different hyperparameters,
    but at the start of the annotation process, you should be concentrating on the
    data.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: （隐藏神经元、周期和每个周期选择的项目是合理的，但其他方面是任意起点。您可以尝试不同的超参数，但在标注过程的开始，您应该专注于数据。）
- en: The code to train our model is the `train_model()` function shown next.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个展示的`train_model()`函数是训练我们模型的代码。
- en: Listing 2.4 Training the text classification model
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 训练文本分类模型
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Select an equal amount of items with each label to effectively oversample
    the smaller label, especially in early iterations of labeling.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为每个标签选择等量的项目，以有效地对较小的标签进行过采样，尤其是在标注的早期迭代中。
- en: You can see that we are keeping our training hyperparameters constant, such
    as the learning rate and type of activation functions. For an actual system, you
    would probably want to experiment with training hyperparameters and also with
    architectures that better model the sequence of words or could better model clusters
    of pixels if you are doing image classification.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们正在保持训练超参数不变，例如学习率和激活函数的类型。对于实际系统，您可能想要尝试训练超参数，以及更好地模拟单词序列或像素集群的架构，如果您正在进行图像分类。
- en: If you are doing any hyperparameter tuning at all, you should create validation
    data and use that data to tune your model, as you are already accustomed to doing
    in machine learning. In fact, you may want multiple kinds of validation datasets,
    including one drawn from your training data at each iteration, one drawn from
    your unlabeled data before you use active learning, and one drawn from the remaining
    unlabeled items at each iteration. We will return to validation data for active
    learning in chapter 3\. For now, we save you the additional annotations. If you
    want to tune your model in the example in this chapter, pull a random selection
    of data from your training data set at each iteration.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进行任何超参数调整，你应该创建验证数据并使用这些数据来调整你的模型，就像你在机器学习中已经习惯做的那样。实际上，你可能需要多种类型的验证数据集，包括在每个迭代中从你的训练数据中抽取的一个，一个在你使用主动学习之前从你的未标记数据中抽取的，以及一个在每个迭代中从剩余的未标记项目中抽取的。我们将在第3章中回到主动学习的验证数据。现在，我们省略了额外的标注。如果你想在本章的示例中调整模型，请在每个迭代中从你的训练数据集中抽取随机选择的数据。
- en: The remainder of the `train_model()` function evaluates the accuracy of the
    new model and saves it to file in models/. I cover evaluation in the next section.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_model()` 函数的剩余部分评估新模型的准确性并将其保存到 models/ 文件夹中。我将在下一节中介绍评估。'
- en: As stated earlier, you should become familiar with your data before you start
    building any machine learning system. Fortunately, this best practice applies
    to active learning too. You should select your evaluation data first, and you
    should be one of the people who labels it.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在开始构建任何机器学习系统之前，你应该熟悉你的数据。幸运的是，这个最佳实践也适用于主动学习。你应该首先选择评估数据，并且你应该成为负责标注这些数据的人之一。
- en: 2.5.1 Always get your evaluation data first
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 总是首先获取评估数据
- en: Evaluation data is often called a test set or held-out data, and for this task,
    it should be a random sample of headlines that we annotate. We will always hold
    out these headlines from our training data so that we can track the accuracy of
    our model after each iteration of active learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 评估数据通常被称为测试集或保留数据，对于这个任务，它应该是我们标注的标题的随机样本。我们将在每次主动学习迭代后始终保留这些标题，以便我们可以跟踪模型在每次迭代后的准确性。
- en: 'It is important to get the evaluation data first, as there are many ways to
    inadvertently bias your evaluation data after you have started other sampling
    techniques. Here are some of the things that can go wrong if you don’t pull out
    your evaluation data first:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先获取评估数据非常重要，因为在你开始其他采样技术之后，有很多方式会无意中使你的评估数据产生偏差。以下是一些如果你没有首先提取评估数据可能会出错的情况：
- en: If you forget to sample evaluation data from your unlabeled items until after
    you have sampled by low confidence, your evaluation data will be biased toward
    the remaining high-confidence items, and your model will appear to be more accurate
    than it is.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你忘记在低置信度采样之后从未标记的项目中采样评估数据，你的评估数据将偏向于剩余的高置信度项目，你的模型看起来会比实际更准确。
- en: If you forget to sample evaluation data and you pull evaluation data from your
    training data after you have sampled by confidence, your evaluation data will
    be biased toward low-confidence items, and your model will appear to be less accurate
    than it is.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你忘记采样评估数据，并在置信度采样之后从你的训练数据中抽取评估数据，你的评估数据将偏向于低置信度项目，你的模型看起来会比实际更不准确。
- en: If you have implemented outlier detection and later try to pull out evaluation
    data, it is almost impossible to avoid bias, as the items you pulled out have
    already contributed to the sampling of additional outliers.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你已经实现了异常值检测，后来又尝试提取评估数据，几乎不可避免地会产生偏差，因为你抽取的项目已经对额外异常值的采样做出了贡献。
- en: What happens if you don’t make evaluation data first?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有首先制作评估数据会发生什么？
- en: It’s difficult to know how accurate your model is if you don’t remember to get
    evaluation data first. This mistake is one of the biggest ones I’ve seen people
    make. As soon as data scientists get any new human labels, they naturally want
    to add those labels to their training data to see how much more accurate their
    models get. But if your evaluation data is an afterthought, and you aren’t careful
    about making it truly random, you won’t know how accurate your model is. I have
    seen companies building self-driving cars, social media feeds, and dating apps
    get evaluation data wrong. Know that the car that swerved past you today, the
    news article that was recommended to you, and the person you might one day marry
    may all have been determined by machine learning models of uncertain accuracy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有记住首先获取评估数据，就很难知道你的模型有多准确。这个错误是我看到人们犯的最大错误之一。一旦数据科学家获得任何新的手工标签，他们自然会想要将这些标签添加到他们的训练数据中，看看他们的模型能提高多少准确性。但如果你对评估数据没有给予足够的重视，并且没有小心地确保其真正随机，你就不会知道你的模型有多准确。我见过一些公司错误地构建自动驾驶汽车、社交媒体流和约会应用，评估数据出错。要知道，今天从你身边驶过的车辆、推荐给你的新闻文章以及你未来可能要嫁的人，都可能是由不确定准确性的机器学习模型决定的。
- en: If you want to start training right away, at least set the evaluation data aside
    first so that it doesn’t factor into your analysis. You can return to annotate
    that data later or annotate in parallel with your training and validation data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要立即开始训练，至少先设置好评估数据，这样它就不会影响你的分析。你可以稍后回来注释这些数据，或者与你的训练和验证数据并行注释。
- en: Finally, it may not be possible to select truly random data if you are applying
    your model to a continuously changing feed of information. In ongoing disaster-response
    situations, this will absolutely be the case, as new information is reported about
    the changing conditions and needs over time. For the example we are working on
    here, we are tasked with labeling a finite set of news headlines, so it is meaningful
    to select a random sample of the headlines to be in our training data. We will
    return to sampling strategies for evaluation data in more complicated contexts
    in chapter 3.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你将模型应用于一个持续变化的信息流，可能无法选择真正随机的数据。在持续的灾害响应情况下，这绝对会是这种情况，因为随着时间的推移，会报告有关变化条件和需求的新信息。对于我们在这里正在处理的例子，我们被要求标记一组有限的新闻标题，因此选择标题的随机样本作为我们的训练数据是有意义的。我们将在第3章中回到更复杂情境下的评估数据抽样策略。
- en: The code to evaluate the accuracy of your model at each iteration is the `evaluate_
    model()` function.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型在每个迭代中准确性的代码是`evaluate_model()`函数。
- en: Listing 2.5 Evaluating the model on held-out data
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5 在保留数据上评估模型
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ The PyTorch tensors are 2D, so we need to pull out only the predictive confidence.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ PyTorch张量是二维的，所以我们只需要提取预测的置信度。
- en: This code gets the predicted confidence that each item is “disaster-related”
    and tracks whether each prediction was correct or incorrect. Raw accuracy would
    not be a good metric to use here. Because the frequency of the two labels is unbalanced,
    you will get almost 95% accuracy from predicting “not disaster-related” each time.
    This result is not informative, and our task is specifically to find the disaster-related
    headlines, so we will calculate accuracy as the F-score of the disaster-related
    predictions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码获取每个项目被预测为“与灾害相关”的置信度，并跟踪每个预测是否正确或错误。原始准确率在这里不是一个好的度量标准。因为两个标签的频率不平衡，你每次预测“非灾害相关”都会得到近95%的准确率。这个结果没有信息量，我们的任务特别是要找到与灾害相关的标题，因此我们将计算准确率作为灾害相关预测的F分数。
- en: In addition to caring about the F-score, we care whether confidence correlates
    with accuracy, so we calculate the area under the ROC curve. A ROC (receiver operating
    characteristic) curve rank-orders a dataset by confidence and calculates the rate
    of true positives versus false positives.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关心F分数外，我们还关心置信度是否与准确性相关，因此我们计算ROC曲线下的面积。ROC（接收者操作特征）曲线按置信度对数据集进行排序，并计算真正例与假正例的比率。
- en: See the appendix for definitions and discussions of precision, recall, F-score,
    and AUC, all of which are implemented in the `evaluate_model()` function of our
    code.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅附录，其中定义了精确度、召回率、F分数和AUC的讨论，这些都在我们代码的`evaluate_model()`函数中实现。
- en: Listing 2.6 Calculating precision, recall, F-score, and AUC
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.6 计算精确度、召回率、F分数和AUC
- en: '[PRE10]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Harmonic mean of precision and recall
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 精确度和召回率的调和平均值
- en: ❷ For items with the label we care about (“related,” in this case), we want
    to know how many are predicted to have that label with greater confidence than
    the items without that label.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对于我们关心的标签（在这种情况下是“相关”），我们想知道有多少被预测为具有该标签，并且比没有该标签的项目具有更高的置信度。
- en: If you look at filenames for any models that you have built in the models directory,
    you will see that the filename includes a timestamp, the accuracy of the model
    by F-score and AUC, and the number of training items. It is good data-management
    practice to give your models verbose and transparent names, which will let you
    track accuracy over time with each iteration simply by looking at the directory
    listing.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看模型目录中任何你构建的模型的文件名，你会看到文件名包括时间戳、模型的F分数和AUC的准确度以及训练项的数量。为你的模型提供详细和透明的名称是良好的数据管理实践，这将让你通过查看目录列表简单地跟踪每个迭代的准确度。
- en: 2.5.2 Every data point gets a chance
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 每个数据点都有机会
- en: By including new randomly sampled items in each iteration of active learning,
    you get a baseline in that iteration. You can compare the accuracy from training
    on the random items with your other sampling strategies, which can tell you how
    effective your sampling strategies are compared with random sampling. You will
    already know how many newly annotated items are different from your model’s predicted
    label, but you won’t know how much they will change the model for future predictions
    after they have been added to the training data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在主动学习的每个迭代中包含新的随机采样项目，你得到该迭代的一个基线。你可以比较在随机项目上训练的准确性与你的其他采样策略，这可以告诉你你的采样策略与随机采样相比有多有效。你将已经知道有多少新标注的项目与你的模型预测的标签不同，但你不会知道它们在添加到训练数据后会对未来的预测造成多大的变化。
- en: Even if your other active learning strategies fail in the iteration, you will
    still get incremental improvement from the random sample, so random sampling is
    a nice fallback.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的其他主动学习策略在迭代中失败，你仍然会从随机样本中获得增量改进，因此随机采样是一个很好的后备方案。
- en: There is an ethical choice here too. We are acknowledging that all strategies
    are imperfect, so every data item still has some chance of being selected randomly
    and being reviewed by a human, even if none of the sampling strategies would have
    selected it. In an actual disaster scenario, would you want to eliminate the chance
    that someone would see an important headline because your sampling strategies
    would never select it? The ethical question is one you should ask yourself depending
    on the data and use case you are addressing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里也有一个道德选择。我们承认所有策略都不完美，因此每个数据项仍然有一定的机会被随机选择并由人工审查，即使没有任何采样策略会选择它。在实际的灾难场景中，你希望消除有人会看到重要标题的机会吗？因为你的采样策略永远不会选择它？道德问题是一个你应该根据你正在处理的数据和用例来问自己的问题。
- en: 2.5.3 Select the right strategies for your data
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 为您的数据选择合适的策略
- en: 'We know that disaster-related headlines are rare in our data, so the strategy
    of selecting outliers is not likely to select many disaster-related items. Therefore,
    the example code focuses on selecting by confidence and sampling data for each
    iteration according to the following strategy:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道在我们的数据中，灾难相关的标题很少，因此选择异常值的策略不太可能选择很多与灾难相关的项目。因此，示例代码专注于根据以下策略按置信度选择并针对每个迭代采样数据：
- en: 10% randomly selected from unlabeled items
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10% 从未标记的项目中随机选择
- en: 80% selected from the lowest confidence items
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 80% 从置信度最低的项目中选择
- en: 10% selected as outliers
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10% 被选为异常值
- en: Assuming that the low-confidence items are truly 50:50 disaster-related and
    not disaster-related, the annotators should see a little more than 4/10 disaster-related
    messages when a large number of items have been annotated and our models are stable.
    This result is close enough to equal that we don’t have to worry that ordering
    effects will prime the annotators in later iterations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设低置信度项目实际上是50:50与灾难相关和非灾难相关，当大量项目已被标注并且我们的模型稳定时，标注者应该看到略多于4/10的与灾难相关的消息。这个结果足够接近，以至于我们不必担心排序效应会在后续迭代中影响标注者。
- en: The following three listings contain the code for the three strategies. First,
    we get the low confidence predictions.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三个列表包含三个策略的代码。首先，我们获取低置信度预测。
- en: Listing 2.7 Sampling items with low confidence
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 使用低置信度采样项目
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Get the probabilities for each label for the item.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取每个标签的项目概率。
- en: ❷ Order the items by confidence.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 按置信度对项目进行排序。
- en: Next, we get the random items.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取随机项目。
- en: Listing 2.8 Sample random items
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.8 样本随机项
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finally, we get the outliers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了异常值。
- en: Listing 2.9 Sample outliers
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 样本异常值
- en: '[PRE13]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Count all features in the training data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算训练数据中所有特征的数量。
- en: ❷ Add the number of times this feature in the unlabeled data item occurred in
    the training data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将未标记数据项中此特征出现的次数添加到该特征中。
- en: ❸ Update the training data counts for this item to help with diversity for the
    next outlier that is sampled.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 更新此项目的训练数据计数，以帮助增加下一次采样的异常值的多样性。
- en: You can see that by default in the `get_low_conf_unlabeled()` function, we are
    predicting the confidence for only 10,000 unlabeled items, rather than from across
    the entire dataset. This example makes the time between iterations more manageable,
    as you would be waiting for many minutes or even hours for all predictions, depending
    on your machine. This example increases the diversity of the data too, as we are
    selecting low-confidence items from a different subset of unlabeled items each
    time.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在默认情况下，在 `get_low_conf_unlabeled()` 函数中，我们只对 10,000 个未标记项进行置信度预测，而不是对整个数据集进行预测。这个示例使得迭代之间的时间更加可控，因为你可能需要等待几分钟甚至几小时才能完成所有预测，这取决于你的机器。这个示例也增加了数据的多样性，因为我们每次都是从未标记项的不同子集中选择低置信度项。
- en: 2.5.4 Retrain the model and iterate
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 重新训练模型并迭代
- en: Now that you have your newly annotated items, you can add them to your training
    data and see the change in accuracy from your model. If you run the script that
    you downloaded at the start of this chapter, you will see that retraining happens
    automatically after you finish annotating each iteration.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了新标注的项，你可以将它们添加到训练数据中，并查看模型准确性的变化。如果你运行本章开头下载的脚本，你会看到在完成每个迭代的标注后，重新训练会自动进行。
- en: If you look at that code, you also see the controls that combine all the code
    that we went through in this chapter. This additional code is the hyperparameters,
    such as the number of annotations per iteration, and the code at the end of the
    file to make sure that you get the evaluation data first, train the models, and
    start iterating with active learning when you have enough evaluation data. The
    example in this chapter has fewer than 500 lines of unique code, so it is worth
    taking the time to understand what is going on in each step and thinking about
    how you might extend any part of the code.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看那段代码，你也会看到结合了本章中我们讨论的所有代码的控制部分。这段额外的代码是超参数，例如每次迭代的注释数量，以及文件末尾的代码，确保首先获取评估数据，然后训练模型，并在拥有足够的评估数据时开始使用主动学习进行迭代。本章的示例代码少于
    500 行，因此花时间理解每一步发生的事情，并思考如何扩展代码的任何部分是值得的。
- en: 'If you come from a machine learning background, the number of features will
    probably jump out at you. You probably have more than 10,000 features for only
    1,000 labeled training items. That is not what your model should look like if
    you are not labeling any more data: you would almost certainly get better accuracy
    if you reduced the number of features. But somewhat counterintuitively, you want
    a large number of features, especially in the early iterations of active learning,
    when you want to make every feature count for the rare disaster-related headlines.
    Otherwise, your early model would be even more biased toward the type of headlines
    that you happened to sample first randomly. There are many ways that you might
    want to combine your machine learning architecture and active learning strategies,
    and I will cover the major ones in chapters 9–11.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你来自机器学习背景，特征的数量可能会让你感到惊讶。你可能只有 1,000 个标记的训练项，就有超过 10,000 个特征。如果你不再标记任何数据，你的模型不应该看起来是这个样子：如果你减少特征数量，你几乎肯定能获得更高的准确性。但有些反直觉的是，你想要大量的特征，尤其是在主动学习的早期迭代中，当你希望每个特征都对罕见的灾难相关标题产生影响时。否则，你的早期模型可能会更加偏向于你随机首次采样的那种标题类型。你有多种方式可以将你的机器学习架构和主动学习策略结合起来，我将在第
    9 章到第 11 章中介绍主要的方法。
- en: After you complete 10 or so iterations of annotation, look at your training
    data. You will notice that most of the items were selected through low confidence,
    which is not a surprise. Look for ones that are listed as selected by outlier,
    and you might be surprised. There will probably be a few examples with words that
    are obvious (to you) as being disaster-related, which means that these examples
    increased the diversity of your dataset in a way that might otherwise have been
    missed.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成大约10次注释迭代后，查看您的训练数据。您会发现大多数项目都是通过低置信度选择的，这并不令人惊讶。寻找那些被列为异常值选择的项，可能会让您感到惊讶。可能有一些例子，其中的单词（对您来说）显然与灾难相关，这意味着这些例子以可能被遗漏的方式增加了您数据集的多样性。
- en: 'Although active learning can be self-correcting, can you see any evidence that
    it didn’t self-correct some bias? Common examples include oversampling extra-long
    or extra-short sentences. The computer vision equivalent would be oversampling
    images that are extra-large or extra-small, or high- or low-resolution. Your choice
    of outlier strategy and machine learning model might oversample based on features
    like these, which are not core to your goal. You might consider applying the methods
    in this chapter to different buckets of data in that case: lowest-confidence short
    sentences, lowest-confidence medium sentences, and lowest-confidence long sentences.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管主动学习可以自我纠正，但您是否能看到任何证据表明它没有纠正某些偏差？常见例子包括过度采样过长或过短的句子。计算机视觉的等效例子是过度采样过大或过小、高分辨率或低分辨率的图像。您选择的异常值策略和机器学习模型可能基于这些特征进行过度采样，而这些特征并不是您目标的核心。在这种情况下，您可能需要考虑将本章中的方法应用于不同数据桶中的数据：最低置信度短句、最低置信度中等句子和最低置信度长句。
- en: If you like, you can also experiment with variations on your sampling strategies
    within this code. Try retraining on only the randomly selected items, and compare
    the resulting accuracy with another system retrained on the same number of items
    selected by low confidence and using outlier sampling. Which strategy has the
    greatest impact, and by how much?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，也可以在这个代码中尝试采样策略的变体。尝试仅对随机选择的项进行重新训练，并将结果精度与另一个系统进行比较，该系统在相同数量的低置信度选择的项上重新训练，并使用异常值采样。哪种策略影响最大，以及影响有多大？
- en: 'You can also think about what you should develop next:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以考虑接下来应该开发什么：
- en: A more efficient interface for annotation
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高效的注释界面
- en: Quality controls to help stop errors in annotation
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助防止注释错误的质量控制
- en: Better active learning sampling strategies
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的主动学习采样策略
- en: More sophisticated neural architectures for the classification algorithm
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类算法的更复杂的神经网络架构
- en: Your subjective experience might be different from mine, and trying this example
    on your own data instead of the example dataset provided here might have changed
    things, too. But chances are good that you identified one of the first three options
    as the most important component to build out next. If you come from a machine
    learning background, your first instinct may be to keep the data constant and
    start experimenting with more sophisticated neural architectures. That task can
    be the best next step, but it’s rarely the most important one early on. Generally,
    you should get your data right first; tuning the machine learning architecture
    becomes more important later in the iterations.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 您的主观体验可能与我的不同，而且尝试在此处提供的示例数据集之外的数据集上的这个例子也可能改变了某些事情。但很可能您已经将前三个选项之一识别为下一个需要构建的最重要组件。如果您来自机器学习背景，您的第一直觉可能是保持数据不变，并开始尝试更复杂的神经网络架构。这项任务可能是最好的下一步，但通常在早期并不是最重要的。一般来说，您应该首先确保数据正确；调整机器学习架构在迭代后期变得更重要。
- en: The rest of this book helps you learn how to design better interfaces for annotation,
    implement better quality control for annotation, devise better active learning
    strategies, and arrive at better ways to combine these components.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本书剩余部分帮助您学习如何设计更好的注释界面，实施更好的注释质量控制，制定更好的主动学习策略，以及找到更好的方法来组合这些组件。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A simple human-in-the-loop machine learning system can cover the entire cycle,
    from sampling unlabeled data to updating the model. This approach lets you get
    started quickly with a complete MVP system that you can build out as needed.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的人机交互机器学习系统可以涵盖整个周期，从采样未标记数据到更新模型。这种方法让您可以快速开始使用一个完整的MVP系统，并根据需要构建。
- en: 'Two simple active learning strategies are easy to implement: sampling the least
    most confident items from predictions and sampling outliers. Understanding the
    basic goals of each of these strategies will help you dive deeper into uncertainty
    and diversity sampling later in this book.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种简单的主动学习策略易于实施：从预测中采样最不自信的项目和采样异常值。理解这些策略的基本目标将有助于你深入探索本书后面的不确定性采样和多样性采样。
- en: A simple command-line interface can allow humans to annotate data efficiently.
    Even a simple text-only interface can be efficient if it is built according to
    general human–computer interaction principles.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的命令行界面可以允许人类高效地标注数据。即使是一个简单的纯文本界面，如果按照一般的人机交互原则构建，也可以是高效的。
- en: Good data management, such as creating evaluation data as the first task, is
    important to get right. If you don’t get your evaluation data right, you may never
    know how accurate your model is.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的数据管理，例如将创建评估数据作为首要任务，对于正确执行至关重要。如果你没有正确处理你的评估数据，你可能永远不知道你的模型有多准确。
- en: Retraining a machine learning model with newly annotated data at regular iterations
    shows that your model gets more accurate over time. If designed correctly, the
    active learning iterations are naturally self-correcting, with overfitting in
    one iteration corrected by the sampling strategy in the following iterations.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期迭代使用新标注的数据重新训练机器学习模型表明，随着时间的推移，你的模型会变得更加准确。如果设计得当，主动学习迭代自然会自我纠正，一个迭代中的过拟合会在后续迭代中的采样策略中得到纠正。
- en: '* * *'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)Thanks to Jennifer Prendki (one of the authors of an anecdote in this text)
    for the term *hacktive learning*. While working together, we misheard each other
    due to our different accents, and both of us understood *active learning* to be
    *hacktive learning*, accidentally inventing this useful phrase.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ^（1.）感谢Jennifer Prendki（本文中一个轶事的作者）提出了“hacktive learning”这个术语。由于我们不同的口音，我们在合作时互相误解，我们都将“active
    learning”理解为“hacktive learning”，无意中发明了这个有用的短语。
- en: ^(2.)For more on how we were tracking epidemics, see [https://nlp.stanford.edu/pubs/Munro2012epidemics.pdf](https://nlp.stanford.edu/pubs/Munro2012epidemics.pdf).
    Since I *wrote* this note in early 2019, COVID-19 has made the importance of this
    use case more obvious.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^（2.）有关我们如何追踪流行病的信息，请参阅[https://nlp.stanford.edu/pubs/Munro2012epidemics.pdf](https://nlp.stanford.edu/pubs/Munro2012epidemics.pdf)。自从我在2019年初写下这个笔记以来，COVID-19使这个用例的重要性变得更加明显。
