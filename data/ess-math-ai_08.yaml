- en: Chapter 8\. Probabilistic Generative Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。概率生成模型
- en: '*AI ties up all the mathematics that I know together, and I have been getting
    to know mathematics for years.*'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*AI将我所知道的所有数学联系在一起，我已经学习数学多年了。*'
- en: If machines are ever to be endowed with an understanding of the world around
    them, and an ability to recreate it, like we do when we imagine, dream, draw,
    create songs, movies, or write books, then generative models are one significant
    step in that direction. We need to get these models right if we are ever going
    to achieve general artificial intelligence.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器要理解周围的世界，并且能够像我们想象、梦想、绘画、创作歌曲、电影或写书时那样重新创造它，那么生成模型是朝着这个方向迈出的一大步。如果我们要实现通用人工智能，就需要正确地构建这些模型。
- en: Generative models are built on the assumption that we can only interpret input
    data correctly if our model has learned the underlying statistical structure of
    this data. This is loosely analogous to our dreaming process, which points to
    the possibility that our brain has learned a model that is able to virtually recreate
    our environment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型建立在这样一个假设上，即只有当我们的模型学习了数据的潜在统计结构时，我们才能正确解释输入数据。这与我们的梦境过程有些类似，这表明我们的大脑可能学习了一个能够虚拟重现我们环境的模型。
- en: In this chapter, we still have the mathematical structure of training function,
    loss function, and optimization presented throughout the book. However, unlike
    in the first few chapters, we aim to learn probability distributions, instead
    of deterministic functions. The overarching theme is that there is training data,
    and we want to come up with a mathematical model that generates new data similar
    to it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们仍然有训练函数、损失函数和优化的数学结构，这些在整本书中都有提到。然而，与前几章不同的是，我们的目标是学习概率分布，而不是确定性函数。总体主题是有训练数据，我们想要提出一个能够生成类似数据的数学模型。
- en: 'There are two quantities of interest:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的两个数量是：
- en: The true (and unknown) joint probability distribution of the features of the
    input data <math alttext="p Subscript d a t a Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    .
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据特征的真实（未知）联合概率分布<math alttext="p Subscript d a t a Baseline left-parenthesis
    ModifyingAbove x With right-arrow right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>。
- en: 'The model joint probability distribution of the features of the data along
    with the parameters of the model: <math alttext="p Subscript m o d e l Baseline
    left-parenthesis ModifyingAbove x With right-arrow semicolon ModifyingAbove theta
    With right-arrow right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据特征的模型联合概率分布以及模型参数：<math alttext="p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow semicolon ModifyingAbove theta With right-arrow
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
- en: Ideally, we want these two as close as possible. In practice, we settle for
    parameter values <math alttext="ModifyingAbove theta With right-arrow"><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover></math> that allow <math alttext="p
    Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow
    semicolon ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> to work well for our particular
    use cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望这两者尽可能接近。在实践中，我们会接受参数值<math alttext="ModifyingAbove theta With right-arrow"><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover></math>，使得<math alttext="p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow semicolon
    ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>能够很好地适用于我们特定的用例。
- en: 'Throughout the chapter, we make use of three rules for probability distributions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个章节中，我们利用了三条概率分布的规则：
- en: The product rule that decomposes the multivariable joint probability distribution
    into a product of single variable conditional probability distributions.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将多变量联合概率分布分解为单变量条件概率分布的乘积的乘法规则。
- en: Bayes rule that allows us to flip between variables seemlessly.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯规则允许我们无缝地在变量之间切换。
- en: Independence or conditional independence assumptions on the features or on latent
    (hidden) variables that allow us to simplify the product of single variable conditional
    probabilities even further.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征或潜在（隐藏）变量的独立性或条件独立性假设，使我们能够进一步简化单变量条件概率的乘积。
- en: In previous chapters we were minimizing the *loss function*. In this chapter
    the analogous function is the *log likelihood function*, and the optimization
    process always attempts to *maximize* this log likelihood (careful, we are not
    minimizing a loss function, we are maximizing an objective function instead).
    More on this soon.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们在最小化*损失函数*。在本章中，类似的函数是*对数似然函数*，优化过程总是试图*最大化*这个对数似然（注意，我们不是在最小化损失函数，而是在最大化一个目标函数）。很快会详细介绍。
- en: 'Before we dive in, let’s make a note that puts our previous deterministic machine
    learning models into probability language: Our previous models learned a training
    function that mapped the features of the input data <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    to an output <math alttext="y"><mi>y</mi></math> (target or label), or <math alttext="f
    left-parenthesis ModifyingAbove x With right-arrow semicolon ModifyingAbove theta
    With right-arrow right-parenthesis equals y"><mrow><mi>f</mi> <mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo> <mo>=</mo> <mi>y</mi></mrow></math> . When our goal
    was classification, *f* returned the label *y* that had the highest probability.
    That is, a classifier learns a direct map from input data <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    to class labels *y*, in other words, they model the posterior probability <math
    alttext="p left-parenthesis y vertical-bar ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>y</mi> <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow></math> directly. We will elaborate on this later in the chapter.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论之前，让我们注意一下，将我们之前的确定性机器学习模型转换为概率语言：我们之前的模型学习了一个训练函数，将输入数据的特征映射到输出（目标或标签）或
    。当我们的目标是分类时，*f*返回具有最高概率的标签*y*。也就是说，分类器学习了从输入数据到类别标签*y*的直接映射，换句话说，它们直接对后验概率进行建模。我们将在本章后面详细阐述这一点。
- en: What Are Generative Models Useful For?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型有哪些用途？
- en: 'Generative models have made it possible to blurr the lines between true and
    computer generated data. They have been improving and are achieving impressive
    successes: Machine generated images, including those of humans, are increasingly
    more realistic. It is hard to tell whether an image of a model in the fashion
    industry is that of a real person or the output of a generative machine learnig
    model.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型使得真实数据和计算机生成数据之间的界限变得模糊。它们一直在改进，并取得了令人印象深刻的成功：机器生成的图像，包括人类的图像，变得越来越逼真。很难判断时尚行业模特的图像是真实人物还是生成机器学习模型的输出。
- en: The goal of a generative model is to use a machine to generate novel data, such
    as audio waveforms containing speech, images, videos, or natural language text.
    Generative models sample data from a learned probability distribution, where the
    samples mimic reality as much as possible. The assumption here is that there is
    some unknown probability distribution underlying the real life data that we want
    to mimic (otherwise our whole reality will be some random chaotic noise, lacking
    any coherence or structure), and the model’s goal is to learn an approximation
    of this probability distribution using the training data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的目标是使用机器生成新颖的数据，例如包含语音的音频波形，图像，视频或自然语言文本。生成模型从学习的概率分布中采样数据，其中样本尽可能地模拟现实。这里的假设是存在一些未知的概率分布潜在地支撑着我们想要模仿的真实生活数据（否则我们整个现实将是一种随机混乱的噪音，缺乏任何连贯性或结构），模型的目标是使用训练数据学习这个概率分布的近似值。
- en: After collecting a large amount of data from a specific domain, we train a generative
    model to generate data similar to the collected data. The collected data can be
    millions of images or videos, thousands of audio recordings, or entire corpuses
    of natural language.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在从特定领域收集了大量数据之后，我们训练一个生成模型来生成类似于收集数据的数据。收集的数据可以是数百万张图像或视频，数千个音频录音，或者整个自然语言语料库。
- en: Generative models are useful for many applications, including data augmentation
    when data is scarce and more of it is needed, imputing missing values for higher
    resolution images, simulating new data for reinforcement learning, or for semisupervised
    learning when only few labels are available. Another application is image-to-image
    translation, such as converting aerial images into maps or converting hand drawn
    sketches to images. More applications include image denoising, inpainting, super-resolution,
    images editing such as making smiles wider, cheeckbones higher, faces slimmer
    and others.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型在许多应用中非常有用，包括在数据稀缺且需要更多数据时进行数据增强，为高分辨率图像填充缺失值，为强化学习模拟新数据，或者在只有少量标签可用时进行半监督学习。另一个应用是图像到图像的转换，例如将航空图像转换为地图或将手绘草图转换为图像。更多的应用包括图像去噪、修补、超分辨率、图像编辑（如使微笑更宽，颧骨更高，脸部更苗条等）。
- en: Moreover, generative models are built to generate more than one acceptable output
    by drawing multiple samples from the desired probability distribution. This is
    different than our deterministic models that average over the output with different
    features during training using a mean squared error loss function or some other
    averaging loss function. The downside here is that a generative model can draw
    some bad samples as well.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，生成模型是建立在从所需概率分布中抽取多个样本以生成多个可接受输出的基础上的。这与我们的确定性模型不同，确定性模型在训练过程中使用均方误差损失函数或其他平均损失函数对具有不同特征的输出进行平均。这里的缺点是生成模型也可能生成一些不好的样本。
- en: One type of generative models, namely [*generative adversarial networks*](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
    (invented in 2014 by Ian Goodfellow *et al*), are incredibly promising and have
    a wide range of applications, from augmenting data sets to completing masked human
    faces to astrophysics and high energy physics, such as simulating data sets similar
    to those produced at the CERN Large Hardon Collider, or simulating distribution
    of dark matter and predicting gravitational lensing. Generative adversarial models
    sets up two neural networks that compete against each other in a zero sum game
    (think game theory in mathematics), until the machine itself cannot tell the difference
    between a real image and a computer generated one. This is why their outputs seem
    very close to reality.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种生成模型，即[*生成对抗网络*](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)（2014年由Ian
    Goodfellow等人发明），具有极大的潜力，并且具有广泛的应用，从增加数据集到完成遮蔽的人脸，再到天体物理学和高能物理学，比如模拟类似于CERN大型强子对撞机产生的数据集，或模拟暗物质的分布并预测引力透镜效应。生成对抗模型建立了两个神经网络，它们在一个零和博弈中相互竞争（想象数学中的博弈论），直到机器本身无法区分真实图像和计算机生成的图像之间的区别。这就是为什么它们的输出看起来非常接近现实。
- en: 'The previous chapter, which was heavily geared towards natural language processing,
    flirted with generative models without explicitly pointing them out. Most applications
    of natural language processing, which are not simple classification models (spam
    or not spam, positive sentiment or negative sentiment, part of speech tagging),
    include language generation. Such examples are: Auto-complete on our smart phones
    or email, machine translation, text summarization, chatbots, and image captioning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章，主要针对自然语言处理，涉及到生成模型，但没有明确指出。自然语言处理的大多数应用，不是简单的分类模型（垃圾邮件或非垃圾邮件，积极情绪或消极情绪，词性标注），包括语言生成。这些例子包括：我们智能手机或电子邮件上的自动完成，机器翻译，文本摘要，聊天机器人和图像标题。
- en: The Typical Mathematics Of Generative Models
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型的典型数学
- en: The way generative models perceive and represent the world is through probability
    distributions. That is, a color image is one sample from the joint probability
    distribution of pixels that together form a meaningful image (try to count the
    dimensions of such a joint probability distribution with all the red, green and
    blue channels included), an audio wave is one sample from the joint probability
    distribution of audio signals that together make up meaningful sounds (these are
    also extremely high dimensional), and a sentence is one sample from the joint
    probability distribution of words or characters that together represent coherent
    sentences.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型感知和表征世界的方式是通过概率分布。也就是说，一幅彩色图像是从像素的联合概率分布中取样得到的，这些像素一起形成了一个有意义的图像（尝试计算包括所有红色、绿色和蓝色通道的这种联合概率分布的维度），一段音频波形是从音频信号的联合概率分布中取样得到的，这些信号一起构成了有意义的声音（这些也是极高维度的），一句话是从单词或字符的联合概率分布中取样得到的，这些单词或字符一起构成了连贯的句子。
- en: 'The glaring question is then: How do we compute these amazingly representative
    joint probability distributions, that are able to capture the complexity of the
    world around us, but sadly happen to be extremely high dimensional?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，一个显著的问题是：我们如何计算这些能够捕捉我们周围世界复杂性的惊人代表性联合概率分布，但不幸的是这些分布往往是极高维度的呢？
- en: 'The machine learning answer is predictable at this point: Start with an easy
    probability distribution that we know of, such as the Gaussian distribution, then
    find a way to mold it into another distribution that well approximates the emperical
    distribution of the data at hand. But how do we mold one distribution into another?
    We can apply a deterministic function to its probability density. So we must understand
    the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的答案在这一点上是可预测的：从我们已知的简单概率分布开始，比如高斯分布，然后找到一种方法将其塑造成另一个很好地近似手头数据的经验分布的分布。但我们如何将一个分布塑造成另一个分布呢？我们可以对其概率密度应用一个确定性函数。因此，我们必须理解以下内容：
- en: 'How do we apply a deterministic function to a probability distribution, and
    what is the probability distribution of the resulting random variable? We use
    the following transformation formula:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何将确定性函数应用于概率分布，以及由此产生的随机变量的概率分布是什么？我们使用以下转换公式：
- en: <math alttext="dollar-sign p Subscript x Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis equals p Subscript z Baseline left-parenthesis
    g Superscript negative 1 Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis right-parenthesis StartAbsoluteValue det left-parenthesis StartFraction
    normal partial-differential g Superscript negative 1 Baseline ModifyingAbove x
    With right-arrow Over normal partial-differential ModifyingAbove x With right-arrow
    EndFraction right-parenthesis EndAbsoluteValue dollar-sign"><mrow><msub><mi>p</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>p</mi> <mi>z</mi></msub> <mrow><mo>(</mo>
    <msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mrow><mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mrow><mo>|</mo> <mo form="prefix" movablelimits="true">det</mo> <mrow><mo>(</mo>
    <mfrac><mrow><mi>∂</mi><msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover></mrow> <mrow><mi>∂</mi><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></mfrac> <mo>)</mo></mrow> <mo>|</mo></mrow></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p Subscript x Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis equals p Subscript z Baseline left-parenthesis
    g Superscript negative 1 Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis right-parenthesis StartAbsoluteValue det left-parenthesis StartFraction
    normal partial-differential g Superscript negative 1 Baseline ModifyingAbove x
    With right-arrow Over normal partial-differential ModifyingAbove x With right-arrow
    EndFraction right-parenthesis EndAbsoluteValue dollar-sign"><mrow><msub><mi>p</mi>
    <mi>x</mi></msub> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>p</mi> <mi>z</mi></msub> <mrow><mo>(</mo>
    <msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mrow><mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mrow><mo>|</mo> <mo form="prefix" movablelimits="true">det</mo> <mrow><mo>(</mo>
    <mfrac><mrow><mi>∂</mi><msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover></mrow> <mrow><mi>∂</mi><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></mfrac> <mo>)</mo></mrow> <mo>|</mo></mrow></mrow></math>
- en: This is very well documented in many probability books and we will extract what
    we need from there shortly. . What is the correct function that we must apply?
    One way is to train our model to *learn* it. We now know that neural networks
    have the capacity to represent a wide range of functions, so we can pass the simple
    probability distribution that we start with through a neural network (the neural
    network would be the formula of the deterministic function that we are looking
    for), then we learn the network’s parameters by minimizing the error between the
    emperical distribution of the given data, and the distribution output by the network.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这在许多概率书籍中都有很好的记录，我们很快就会从中提取我们需要的内容。我们必须应用的正确函数是什么？一种方法是训练我们的模型来*学习*它。我们现在知道神经网络有能力表示各种函数，因此我们可以通过神经网络传递我们开始的简单概率分布（神经网络将是我们正在寻找的确定性函数的公式），然后通过最小化给定数据的经验分布与网络输出的分布之间的误差来学习网络的参数。
- en: How do we measure errors between probability distributions? Probability theory
    provides us with some measures of how two probability distributions diverge from
    each other, such as the Kullback–Leibler (KL) divergence. This is also related
    to cross-entropy from information theory.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何衡量概率分布之间的误差？概率论为我们提供了一些衡量两个概率分布之间差异的方法，比如Kullback-Leibler（KL）散度。这也与信息论中的交叉熵有关。
- en: Do all generative models work this way? Yes and no. *Yes* in the sense that
    they are all trying to learn the joint probability distribution that presumably
    generated the training data. In other words, generative models attempt to learn
    the formula and the parameters of a joint probability distribution that maximizes
    the likelihood of the training data (or maximize the probability that the model
    assigns to the training data). *No* in the sense that we only outlined an *explicit*
    way to approximate our desired joint probability distribution. This is one school
    of thought. In general, a model that defines an explicit and tractable probability
    density function allows us to operate directly on the log-likelihood of the training
    data, compute its gradient, and apply available optimization algorithms to search
    for the maximum. There are other models that provide an explicit but intractable
    probability density function, in which case we must use approximations to maximize
    the likelihood. How do we solve an optimization problem *approximately*? We can
    either use a deterministic approximation, relying on *variational methods* (variational
    autoencoder models), or use a stochastic approximation, relying on Markov chain
    Monte Carlo methods. Finally, there are *implicit* ways to approximate our desired
    joint probability distribution. Implicit models learn to sample from the unknown
    distribution without ever explicitly defining a formula for it. Generative adversarial
    networks fall into this category.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有生成模型都是这样工作的吗？是和不是。是指它们都试图学习生成训练数据的联合概率分布。换句话说，生成模型试图学习最大化训练数据的似然性的联合概率分布的公式和参数。不是指我们只概述了一种明确的逼近我们所需的联合概率分布的方法。这是一种思维方式。一般来说，一个定义了明确且可处理的概率密度函数的模型允许我们直接操作训练数据的对数似然性，计算其梯度，并应用可用的优化算法来搜索最大值。还有其他提供明确但难以处理的概率密度函数的模型，这种情况下，我们必须使用逼近方法来最大化似然性。我们如何*近似*解决优化问题？我们可以使用确定性逼近，依赖于*变分方法*（变分自动编码器模型），或者使用随机逼近，依赖于马尔可夫链蒙特卡洛方法。最后，还有*隐式*的方法来逼近我们所需的联合概率分布。隐式模型学习从未明确定义公式的未知分布中抽样。生成对抗网络属于这一类别。
- en: 'Nowadays, the three most popular approaches to generative modeling are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，生成建模的三种最流行的方法是：
- en: Generative adversarial networks, which are implicit density models;
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络，这是隐式密度模型；
- en: Variational models which provide an explicit but intractable probability density
    function. We approximate the solution of the optimization problem within the framework
    of probabilistic graphical models where we maximize a lower bound on the log likelihood
    of the data, since immediately maximizing the log likelihood of the data is intractable;
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分模型提供明确但难以处理的概率密度函数。我们在概率图模型框架内逼近优化问题的解，其中我们最大化数据对数似然性的下界，因为直接最大化数据的对数似然性是难以处理的；
- en: '*Fully visible belief networks*, which provide explicit and tractable probability
    density functions, such as [Pixel Convolutional Neural Networks (PixelCNN) 2016](https://arxiv.org/pdf/1606.05328.pdf)
    and [WaveNet (2016)](https://arxiv.org/abs/1609.03499). These models learn the
    joint probability distribution by decomposing it into a product of one dimensional
    probability distributions for each individual dimension, conditioned on those
    that preceded it, and learning each of these distributions one at a time. This
    decomposition is thanks to the product rule or chain rule for probabilities. For
    example, PixelCNN trains a network that learns the conditional probability distribution
    of every individual pixel in an image given previous pixels (to the left and to
    the top of it), and WaveNet trains a network that learns the conditional probability
    distribution of every individual audio signal in a sound wave conditioned on those
    that preceded it. The drawbacks here are that these models generate the samples
    only one entry at a time, and they disallow parallelization. This slows down the
    generation process considerably. For example, it takes WaveNet two minutes of
    computation time to generate one second of audio, so we cannot use it for live
    back and forth conversations.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完全可见信念网络*，提供明确且可处理的概率密度函数，如[Pixel卷积神经网络（PixelCNN）2016](https://arxiv.org/pdf/1606.05328.pdf)和[WaveNet（2016）](https://arxiv.org/abs/1609.03499)。这些模型通过将联合概率分布分解为每个单独维度的概率分布的乘积，条件于之前的维度，并逐个学习这些分布来学习。这种分解得益于概率的乘法规则或链规则。例如，PixelCNN训练一个网络，学习图像中每个单独像素的条件概率分布，给定之前的像素（左侧和顶部），而WaveNet训练一个网络，学习声波中每个单独音频信号的条件概率分布，条件于之前的信号。这里的缺点是这些模型只能一次生成一个条目，并且不允许并行化。这显著减慢了生成过程。例如，WaveNet需要两分钟的计算时间才能生成一秒的音频，因此我们无法用它进行实时的来回对话。'
- en: There are other generative models that fall into the above categories but are
    less popular, due to expensive computational requirements or difficulties in selecting
    the density function and/or its transformations. These include models that require
    a change of variables, such as nonlinear independent component estimation (explicit
    and tractable density model), Boltzmann machine models (explicit and intractable
    density model, with a stochastic Markov chain approximation to the solution of
    the maximization problem), and generative stochastic network models (implicit
    density model again depending on a Markov chain to arrive at its approximate maximum
    likelihood). We survey these models briefly towards the end of this chapter. In
    practice and away from mathematical theory and analysis, Markov chain approaches
    are out of favor due their computational cost and relunctance to converge rapidly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他生成模型属于上述类别，但由于昂贵的计算要求或选择密度函数及/或其变换的困难而不太受欢迎。这些模型包括需要变量变换的模型，例如非线性独立成分估计（显式和可计算密度模型）、玻尔兹曼机模型（显式但不可计算密度模型，通过随机马尔可夫链近似解决最大化问题）、生成随机网络模型（隐式密度模型再次取决于马尔可夫链以达到其近似最大似然）。我们在本章末尾简要调查这些模型。在实践中，远离数学理论和分析，由于计算成本高昂且不愿意迅速收敛，马尔可夫链方法不受青睐。
- en: Shifting Our Brain From Deterministic Thinking To Probabilistic Thinking
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从确定性思维转变为概率性思维
- en: 'In this chapter, we are slowly shifting our brain from deterministic thinking
    to probabilistic thinking. So far in this book, we have only used determinstic
    functions to make our predictions. The training functions were linear combinations
    of data features, sometimes composed with nonlinear activators, the loss functions
    were deterministic dicriminators between the true values and the predicted ones,
    and the optimization methods were based on deterministic gradient descent methods.
    Stochasticity, or randomness, was only introduced when we needed to make the computations
    of the deterministic components of our model less expensive, such as stochastic
    gradient descent or stochastic singular value decomposition, when we split our
    data sets into training, validation, and test subsets, when we selected our minibatches,
    when we traversed some hyper-parameter spaces, or when we passed the scores of
    data samples into the softmax function, which is a deterministic function, and
    interpreted the resulting values as probabilities. In all of these settings, stochasticity
    and the associated probability distributions related only to specific components
    of the model, serving only as a means to an end: Enabling the practical implementation
    and computation of the deterministic model. They never constituted a model’s core
    makeup.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们正在慢慢将我们的大脑从确定性思维转变为概率性思维。到目前为止，在这本书中，我们只使用确定性函数来进行预测。训练函数是数据特征的线性组合，有时与非线性激活器组合，损失函数是真实值和预测值之间的确定性鉴别器，优化方法基于确定性梯度下降方法。随机性或随机性仅在我们需要使模型的确定性组件的计算变得更加经济时引入，例如随机梯度下降或随机奇异值分解，当我们将数据集分成训练、验证和测试子集时，当我们选择我们的小批量时，当我们遍历一些超参数空间时，或者当我们将数据样本的分数传递到softmax函数中时，这是一个确定性函数，并将结果值解释为概率。在所有这些设置中，随机性和相关的概率分布仅与模型的特定组件相关，仅作为实现和计算确定性模型的手段：从来没有构成模型的核心组成部分。
- en: Generative models are different than the models that we have seen in previous
    chapters in the sense that they are probabilistic at their core. Nevertheless,
    we still have the training, loss, and optimization structure, except that now
    the model learns a probability distribution (explicitly or implicitly) as opposed
    to learning a deterministic function. Our loss function then measures the error
    between the true and the predicted probability distributions (at least for the
    explicit density models), so we must understand how to define and compute some
    sort of error function between probabilities instead of deterministic values.
    We must also learn how to optimize and take derivatives in this probabilistic
    setting.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型与我们在前几章中看到的模型不同，因为它们在本质上是概率性的。然而，我们仍然有训练、损失和优化结构，只是现在模型学习一个概率分布（显式或隐式），而不是学习一个确定性函数。然后，我们的损失函数衡量真实和预测概率分布之间的误差（至少对于显式密度模型），因此我们必须了解如何定义和计算概率之间的某种误差函数，而不是确定性值。我们还必须学习如何在这种概率设置中进行优化和求导。
- en: 'In mathematics, it is a much easier problem to evaluate a given function (forward
    problem) than to find its inverse (inverse problem), let alone when we only have
    access to few observations of the function values, such as our data samples. In
    our probabilistic setting, the forward problem looks like: Given a certain probability
    distribution, sample some data. The inverse problem is the one we care for: Given
    this finite number of realizations (data samples) of a probability distribution
    that we do not know, find the probability distribution that most likely generated
    them. One first difficulty that comes to our mind is the issue of uniqueness:
    There could be more than one distribution that fit our data. Moreover, the inverse
    problem is usually much harder because in essence we have to act backwards and
    undo the process that the forward function followed to arrive at the given observations.
    The issue is that most processes cannot be undone, and this is somehow bigger
    than us, embedded in the laws of nature: The universe tends to increase entropy.
    On top of the hardship inherent to solving inverse problems, the probability distributions
    that we usually try to estimate for AI applications are high dimensional, with
    many variables, and we are not even sure that our probabilistic model has accounted
    for all the variables (but that is problematic for deterministic models as well).
    These difficulties should not deter us: Representing and manipulating high dimensional
    probability distributions is important for many math, science, finance, engineering
    and other applications. We must dive into generative models.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，评估给定函数（正向问题）要比找到其逆函数（逆向问题）容易得多，更不用说当我们只能访问少量函数值观测时，比如我们的数据样本。在我们的概率设置中，正向问题看起来是：给定某种概率分布，抽取一些数据。逆向问题是我们关心的问题：给定这个我们不知道的概率分布的有限数量的实现（数据样本），找到最有可能生成它们的概率分布。我们首先想到的一个困难是唯一性的问题：可能有多个分布适合我们的数据。此外，逆向问题通常更难，因为本质上我们必须向后操作并撤消正向函数遵循的过程，以到达给定的观测值。问题在于大多数过程无法撤消，这在某种程度上超出了我们的能力，嵌入在自然法则中：宇宙倾向于增加熵。除了解决逆问题固有的困难之外，我们通常尝试为AI应用估计的概率分布是高维的，具有许多变量，并且我们甚至不确定我们的概率模型是否已经考虑了所有变量（但这对确定性模型也是有问题的）。这些困难不应该使我们却步：表示和操作高维概率分布对于许多数学、科学、金融、工程和其他应用都很重要。我们必须深入研究生成模型。
- en: 'Throughout the rest of this chapter, we will differentiate the case when our
    estimated probability distribution is given with an explicit formula, and when
    we do not have a formula but instead we numerically generate new data samples
    from an implicit distribution. Note that in the previous chapters, with all of
    our deterministic models, we always had explicit formulas for our training functions,
    including the ones given by decision trees, fully connected neural networks, and
    convolutional neural networks. Back then, once we estimated these deterministic
    functions from the data, we could answer questions like: What is the predicted
    value of the target variable? In probabilistic models, we answer a different question:
    What is the probability that the target variable assumes a certain value, or lies
    in a certain interval? The difference is that we do not know how our model combined
    the variables to produce our result, as in the deterministic case. What we try
    to estimate in probabilistic models is the probability that the model’s variables
    occur together with the target variable (their joint probability), ideally for
    all ranges of all variables. This will give us the probability distribution of
    the target variable, without having to explicitly formulate how the model’s variables
    interact to produce this result. This purely depends on observing the data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将区分估计的概率分布是否具有显式公式的情况，以及当我们没有公式而是从隐式分布中数值生成新数据样本时的情况。请注意，在之前的章节中，对于所有我们的确定性模型，我们总是有明确的公式用于我们的训练函数，包括决策树、全连接神经网络和卷积神经网络给出的公式。那时，一旦我们从数据中估计出这些确定性函数，我们就可以回答问题，比如：目标变量的预测值是多少？在概率模型中，我们回答一个不同的问题：目标变量假定某个值的概率是多少，或者落在某个区间内的概率是多少？不同之处在于我们不知道我们的模型是如何组合变量以产生结果的，就像确定性情况下那样。在概率模型中，我们试图估计的是模型变量与目标变量一起发生的概率（它们的联合概率），理想情况下是对所有变量的所有范围。这将为我们提供目标变量的概率分布，而无需明确制定模型变量如何相互作用以产生这个结果。这完全取决于观察数据。
- en: Maximum Likelihood Estimation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大似然估计
- en: 'Many generative models either directly or indirectly rely on the *maximum likelihood
    principle*. For probabilistic models, the goal is to learn a probability distribution
    that approximates the true probability distribution of the observed data. One
    way to do this is to specify an explicit probability distribution <math alttext="p
    Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow
    semicolon ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> with some unknown parameters
    <math alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math> , then solve for the parameters <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    that make the training dataset as likely to be observed as possible. That is,
    we need to find the <math alttext="ModifyingAbove theta With right-arrow"><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover></math> that *maximizes the likelihood*
    of the training data, assigning a high probability for these samples. If there
    are *m* training data points, we assume that they are sampled independently, so
    that the probability of observing them together is just the product of the probabilities
    of all the individual samples. So we have:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 许多生成模型直接或间接地依赖于*最大似然原则*。对于概率模型，目标是学习一个概率分布，该分布近似于观测数据的真实概率分布。一种方法是指定一个明确的概率分布<p_subscript
    model Baseline left-parenthesis ModifyingAbove x With right-arrow semicolon ModifyingAbove
    theta With right-arrow right-parenthesis>，其中包含一些未知参数<ModifyingAbove theta With
    right-arrow>，然后解出使训练数据集尽可能被观察到的参数<ModifyingAbove theta With right-arrow>。也就是说，我们需要找到*最大化*训练数据的似然性的<ModifyingAbove
    theta With right-arrow>，为这些样本分配高概率。如果有*m*个训练数据点，我们假设它们是独立采样的，因此观察它们一起的概率就是所有单个样本概率的乘积。因此我们有：
- en: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg max Underscript ModifyingAbove theta With right-arrow
    Endscripts p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow Superscript 1 Baseline semicolon ModifyingAbove theta With right-arrow
    right-parenthesis p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow squared semicolon ModifyingAbove theta With right-arrow right-parenthesis
    ellipsis p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow Superscript m Baseline semicolon ModifyingAbove theta With right-arrow
    right-parenthesis dollar-sign"><mrow><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>1</mn></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>2</mn></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>⋯</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>m</mi></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg max Underscript ModifyingAbove theta With right-arrow
    Endscripts p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow Superscript 1 Baseline semicolon ModifyingAbove theta With right-arrow
    right-parenthesis p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow squared semicolon ModifyingAbove theta With right-arrow right-parenthesis
    ellipsis p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow Superscript m Baseline semicolon ModifyingAbove theta With right-arrow
    right-parenthesis dollar-sign"><mrow><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>1</mn></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>2</mn></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>⋯</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>m</mi></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
- en: 'Recall that each probability is a number between zero and one. If we multiply
    all of these probabilities together, we would obtain numbers extremely small in
    magnitude, which introduces numerical instabilities and we run the risk of underflow
    (when the machine stores a very small number as zero, essentially removing all
    significant digits). The *log* function always solves this problem, transforming
    all numbers whose magnitude are extremely large or extremely small back to the
    reasonable magnitude realm. The good news is that the *log* transformation for
    our probabilities does not affect the values of the optimal <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    , since the *log* function is an increasing function. That is, if <math alttext="f
    left-parenthesis ModifyingAbove theta With right-arrow Subscript o p t i m a l
    Baseline right-parenthesis greater-than-or-equal-to f left-parenthesis ModifyingAbove
    theta With right-arrow right-parenthesis"><mrow><mi>f</mi> <mrow><mo>(</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>≥</mo> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> for all <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    , then <math alttext="log left-parenthesis f left-parenthesis ModifyingAbove theta
    With right-arrow Subscript o p t i m a l Baseline right-parenthesis right-parenthesis
    greater-than-or-equal-to log left-parenthesis f left-parenthesis ModifyingAbove
    theta With right-arrow right-parenthesis right-parenthesis"><mrow><mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>≥</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> for all <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    as well. Composing with increasing functions does not change the inequality sign.
    The point is that the maximum likelihood solution becomes equivalent to the maximum
    log likelhood solution. Now recall that the *log* function transforms products
    to sums, we have:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，每个概率都是介于零和一之间的数字。如果我们将所有这些概率相乘，我们将得到数量极小的数字，这会引入数值不稳定性，我们面临下溢的风险（当机器将一个非常小的数字存储为零时，实质上删除了所有有效数字）。*对数*函数总是解决这个问题，将那些数量极大或极小的数字转换回合理的数量范围。好消息是，对我们的概率进行*对数*转换不会影响最优<math
    alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math> 的值，因为*对数*函数是一个增函数。也就是说，如果对于所有的<math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    ，都有<math alttext="f left-parenthesis ModifyingAbove theta With right-arrow Subscript
    o p t i m a l Baseline right-parenthesis greater-than-or-equal-to f left-parenthesis
    ModifyingAbove theta With right-arrow right-parenthesis"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>≥</mo> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> ，那么对于所有的<math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    ，也有<math alttext="log left-parenthesis f left-parenthesis ModifyingAbove theta
    With right-arrow Subscript o p t i m a l Baseline right-parenthesis right-parenthesis
    greater-than-or-equal-to log left-parenthesis f left-parenthesis ModifyingAbove
    theta With right-arrow right-parenthesis right-parenthesis"><mrow><mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>≥</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math> 。与增函数组合不会改变不等号。关键是，最大似然解变得等价于最大对数似然解。现在回想一下，*对数*函数将乘积转换为和，我们有：
- en: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg max Underscript ModifyingAbove theta With right-arrow
    Endscripts log left-parenthesis p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow Superscript 1 Baseline semicolon ModifyingAbove
    theta With right-arrow right-parenthesis right-parenthesis plus log left-parenthesis
    p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow
    squared semicolon ModifyingAbove theta With right-arrow right-parenthesis right-parenthesis
    plus ellipsis plus log left-parenthesis p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow Superscript m Baseline semicolon ModifyingAbove
    theta With right-arrow right-parenthesis right-parenthesis dollar-sign"><mrow><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>1</mn></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>+</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>2</mn></msup> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>m</mi></msup> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg max Underscript ModifyingAbove theta With right-arrow
    Endscripts log left-parenthesis p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow Superscript 1 Baseline semicolon ModifyingAbove
    theta With right-arrow right-parenthesis right-parenthesis plus log left-parenthesis
    p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow
    squared semicolon ModifyingAbove theta With right-arrow right-parenthesis right-parenthesis
    plus ellipsis plus log left-parenthesis p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow Superscript m Baseline semicolon ModifyingAbove
    theta With right-arrow right-parenthesis right-parenthesis dollar-sign"><mrow><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>1</mn></msup>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>+</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mn>2</mn></msup> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>m</mi></msup> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: 'Note that the above expression wants to increase each of <math alttext="p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow comma ModifyingAbove
    theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    for each data sample. That is, it prefers the values of <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    *push up* the graph of <math alttext="p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow comma ModifyingAbove theta With right-arrow
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    above each data point <math alttext="ModifyingAbove x With right-arrow Superscript
    i"><msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup></math>
    . However, we cannot push up indefinitely: There must be a downward compensation
    since the hyper-area of the region under the graph has to add up to one, knowing
    that <math alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> is a probability distribution.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，上述表达式希望增加每个数据样本的 <math alttext="p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow comma ModifyingAbove theta With right-arrow
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>。也就是说，它更喜欢
    <math alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math> 的值*推动* <math alttext="p Subscript m o d e l Baseline
    left-parenthesis ModifyingAbove x With right-arrow comma ModifyingAbove theta
    With right-arrow right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    在每个数据点 <math alttext="ModifyingAbove x With right-arrow Superscript i"><msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup></math> 上方。然而，我们不能无限制地向上推动：必须有向下的补偿，因为图形下方区域的超面积必须相加为一，知道
    <math alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> 是一个概率分布。
- en: 'We can reformulate the above expression in terms of expectation and conditional
    probabilities:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用期望和条件概率的术语重新表达上述表达式：
- en: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg max Underscript ModifyingAbove theta With right-arrow
    Endscripts double-struck upper E Subscript x tilde p Sub Subscript d a t a Baseline
    log left-parenthesis p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow vertical-bar ModifyingAbove theta With right-arrow right-parenthesis
    right-parenthesis dollar-sign"><mrow><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>|</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg max Underscript ModifyingAbove theta With right-arrow
    Endscripts double-struck upper E Subscript x tilde p Sub Subscript d a t a Baseline
    log left-parenthesis p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow vertical-bar ModifyingAbove theta With right-arrow right-parenthesis
    right-parenthesis dollar-sign"><mrow><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">max</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></mrow></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>|</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: 'The deterministic models that we discussed in the previous chapters find the
    models’ parameters (or weights) by minimizing a loss function that measures the
    error between the model’s predictions and the true values provided by the data
    labels, or in other words, between <math alttext="y Subscript m o d e l"><msub><mi>y</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    and <math alttext="y Subscript d a t a"><msub><mi>y</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    . In this chapter, we care to find the parameters that maximize the log-likelihood
    of the data. It would be nice if there is a formulation of log-likelihood maximization
    that is analogous to minimizing a quantity that measures an error between the
    probability distributions <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    and <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    , so that the analogy between this chapter and the previous chapters is obvious.
    Luckily, there is. The maximum likelihood estimation is the same as minimizing
    the [*Kullback Leibler (KL) divergence*](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)
    between the probability distribution that generated the data and the model’s probability
    distribution:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章讨论的确定性模型通过最小化衡量模型预测与数据标签提供的真实值之间误差的损失函数来找到模型的参数（或权重），或者换句话说，找到 <math alttext="y
    Subscript m o d e l"><msub><mi>y</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    和 <math alttext="y Subscript d a t a"><msub><mi>y</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    之间的误差。在本章中，我们关心找到最大化数据对数似然的参数。如果有一个类似于最小化衡量概率分布 <math alttext="p Subscript m o
    d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    和 <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    之间误差的量的公式，那么本章与前几章之间的类比就会很明显。幸运的是，有这样的公式。最大似然估计与最小化[*Kullback Leibler (KL) 散度*](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)之间是相同的，这个散度是生成数据的概率分布和模型的概率分布之间的差异：
- en: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg min Underscript ModifyingAbove theta With right-arrow
    Endscripts upper D i v e r g e n c e Subscript upper K upper L Baseline left-parenthesis
    p Subscript d a t a Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis StartAbsoluteValue EndAbsoluteValue p Subscript m o d e l Baseline
    left-parenthesis ModifyingAbove x With right-arrow semicolon ModifyingAbove theta
    With right-arrow right-parenthesis right-parenthesis dollar-sign"><mrow><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mi>D</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>r</mi> <mi>g</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>e</mi>
    <mrow><mi>K</mi><mi>L</mi></mrow></msub> <mrow><mo>(</mo></mrow> <msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mrow><mo>|</mo>
    <mo>|</mo></mrow> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mrow><mo>)</mo></mrow></mrow></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg min Underscript ModifyingAbove theta With right-arrow
    Endscripts upper D i v e r g e n c e Subscript upper K upper L Baseline left-parenthesis
    p Subscript d a t a Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis StartAbsoluteValue EndAbsoluteValue p Subscript m o d e l Baseline
    left-parenthesis ModifyingAbove x With right-arrow semicolon ModifyingAbove theta
    With right-arrow right-parenthesis right-parenthesis dollar-sign"><mrow><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mi>D</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>r</mi> <mi>g</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>e</mi>
    <mrow><mi>K</mi><mi>L</mi></mrow></msub> <mrow><mo>(</mo></mrow> <msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mrow><mo>|</mo>
    <mo>|</mo></mrow> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mrow><mo>)</mo></mrow></mrow></math>
- en: 'If <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    happens to be a member of the family of distributions <math alttext="p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow semicolon
    ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> and if we were able to perform
    the minimization precisely, then the we would recover the exact distribution that
    generated the data, namely <math alttext="p Subscript d a t a"><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> . However,
    in practice, we do not have access to the data generating distribution, in fact
    it is the distribution that we are trying to approximate. We only have access
    to *m* samples from <math alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    . These samples define the emperical distribution <math alttext="ModifyingAbove
    p With caret Subscript d a t a"><msub><mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math> that places
    mass only on exactly these *m* samples. Now maximizing the log-likelihood of the
    training set is exactly equivalent to minimizing the KL divergence between <math
    alttext="ModifyingAbove p With caret Subscript d a t a"><msub><mover accent="true"><mi>p</mi>
    <mo>^</mo></mover> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    and <math alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow semicolon ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> :'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果<p_data>恰好是分布家族<p_model Baseline left-parenthesis ModifyingAbove x With right-arrow
    semicolon ModifyingAbove theta With right-arrow right-parenthesis>的成员，并且如果我们能够精确执行最小化，那么我们将恢复生成数据的确切分布，即<p_data>。然而，在实践中，我们无法访问生成数据的分布，事实上，这是我们试图逼近的分布。我们只能访问<p_data>中的*m*个样本。这些样本定义了只在这些*m*个样本上放置质量的经验分布<ModifyingAbove
    p With caret Subscript d a t a>。现在，最大化训练集的对数似然恰好等同于最小化<ModifyingAbove p With caret
    Subscript d a t a>和<p_model Baseline left-parenthesis ModifyingAbove x With right-arrow
    semicolon ModifyingAbove theta With right-arrow right-parenthesis>之间的KL散度：
- en: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg min Underscript ModifyingAbove theta With right-arrow
    Endscripts upper D i v e r g e n c e Subscript upper K upper L Baseline left-parenthesis
    ModifyingAbove p With caret Subscript d a t a Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis StartAbsoluteValue EndAbsoluteValue p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow semicolon
    ModifyingAbove theta With right-arrow right-parenthesis right-parenthesis dollar-sign"><mrow><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mi>D</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>r</mi> <mi>g</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>e</mi>
    <mrow><mi>K</mi><mi>L</mi></mrow></msub> <mrow><mo>(</mo></mrow> <msub><mover
    accent="true"><mi>p</mi> <mo>^</mo></mover> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mrow><mo>)</mo></mrow></mrow></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove theta With right-arrow Subscript o
    p t i m a l Baseline equals arg min Underscript ModifyingAbove theta With right-arrow
    Endscripts upper D i v e r g e n c e Subscript upper K upper L Baseline left-parenthesis
    ModifyingAbove p With caret Subscript d a t a Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis StartAbsoluteValue EndAbsoluteValue p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow semicolon
    ModifyingAbove theta With right-arrow right-parenthesis right-parenthesis dollar-sign"><mrow><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mrow><mi>o</mi><mi>p</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub>
    <mo>=</mo> <mo form="prefix">arg</mo> <msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mi>D</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>r</mi> <mi>g</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>e</mi>
    <mrow><mi>K</mi><mi>L</mi></mrow></msub> <mrow><mo>(</mo></mrow> <msub><mover
    accent="true"><mi>p</mi> <mo>^</mo></mover> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mrow><mo>)</mo></mrow></mrow></math>
- en: 'At this point we might be confused between three optimization problems that
    are in fact mathematically equivalent, they just happen to come from different
    subdisciplines and subcultures of mathematics, statistics, natural sciences, and
    computer science:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可能会在三个优化问题之间感到困惑，实际上它们在数学上是等价的，它们只是来自数学、统计学、自然科学和计算机科学的不同子学科和子文化：
- en: Maximizing the log-likelihood of the training data
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大化训练数据的对数似然
- en: Minimizing the KL divergence between the emperical distribution of the training
    data and the model’s distribution
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化训练数据的经验分布与模型分布之间的KL散度
- en: Minimizing the cross entropy loss function between the training data labels
    and the model outputs, when we are classifying into multiple classes using composition
    with the sotmax function.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们使用softmax函数与多个类别进行组合进行分类时，最小化训练数据标签和模型输出之间的交叉熵损失函数。
- en: 'Do not be confused: The parameters that minimize the KL divergence are the
    same as the parameters that minimize the cross entropy and the negative log likelihood.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 不要混淆：最小化KL散度的参数与最小化交叉熵和负对数似然的参数相同。
- en: Explicit And Implicit Density Models
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式和隐式密度模型
- en: 'The goal of maximum log-likelihood estimation (or minimum KL-divergence) is
    to find a probability distribution <math alttext="p Subscript m o d e l Baseline
    left-parenthesis ModifyingAbove x With right-arrow semicolon ModifyingAbove theta
    With right-arrow right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    that best explains the observed data. Generative models use this learned <math
    alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow semicolon ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> to generate new data. There
    are two approaches here, one explicit and the other implicit:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最大对数似然估计（或最小KL散度）的目标是找到一个概率分布<math alttext="p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow semicolon ModifyingAbove theta With right-arrow
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>，最好地解释观察到的数据。生成模型使用这个学习的<math
    alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow semicolon ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>来生成新数据。这里有两种方法，一种是显式的，另一种是隐式的：
- en: '**Explicit density models**: Define the formula for the probability distribution
    *explicitly* in terms of <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math> and <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    , then find the values of <math alttext="ModifyingAbove theta With right-arrow"><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover></math> that maximize the log likelihood
    of the training data samples by following the gradient vector (the partial derivatives
    with respect to the components of <math alttext="ModifyingAbove theta With right-arrow"><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover></math> ) uphill. One glaring difficulty
    here is coming up with a formula for the probability density that is able to capture
    the complexity in the data, while at the same time staying amiable to computing
    the log likelihood with its gradient.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**显式密度模型**：根据<math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math>和<math alttext="ModifyingAbove theta With right-arrow"><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover></math>明确地定义概率分布的公式，然后通过沿着梯度向量（相对于<math
    alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math>的分量的偏导数）向上走，找到最大化训练数据样本的对数似然的值。这里一个明显的困难是提出一个能够捕捉数据复杂性的概率密度公式，同时又能友好地计算对数似然及其梯度。'
- en: '**Implicit density models**: Sample directly from <math alttext="p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow semicolon
    ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> without ever writing a formula
    for this distribution. Generative Stochastic Networks do this based on a Markov
    Chain framework, which is slow to converge and thus unpopular for practical applications.
    Using this approach, the model stochastically transforms an existing sample in
    order to obtain another sample from the same distribution. Generative Adversarial
    Networks interact indirectly with the model’s probability distribution without
    explicitly defining it. They set up a zero sum game between two networks, where
    one networks generates a sample and the other network acts like a classifier determining
    whether the generated sample is from the correct distribution or not.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐式密度模型**：直接从<math alttext="p Subscript m o d e l Baseline left-parenthesis
    ModifyingAbove x With right-arrow semicolon ModifyingAbove theta With right-arrow
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>进行采样，而无需为这个分布编写公式。生成随机网络基于马尔可夫链框架进行此操作，这种方法收敛速度慢，因此在实际应用中不受欢迎。使用这种方法，模型随机地转换现有样本，以获得来自相同分布的另一个样本。生成对抗网络间接与模型的概率分布交互，而不明确定义它。它们在两个网络之间建立一个零和博弈，其中一个网络生成一个样本，另一个网络充当分类器，确定生成的样本是否来自正确的分布。'
- en: 'Explicit Density- Tractable: Fully Visible Belief Networks'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式密度-易处理：完全可见信念网络
- en: 'These models admit an explicit probability density function with tractable
    log-likelihood optimization. They rely on the [chain rule of probability](https://en.wikipedia.org/wiki/Chain_rule_(probability))
    to decompose the joint probability distribution <math alttext="p Subscript m o
    d e l Baseline left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    into a product of one dimensional probability distributions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型具有明确的概率密度函数，可进行易处理的对数似然优化。它们依赖于[概率链规则](https://en.wikipedia.org/wiki/Chain_rule_(probability))来将联合概率分布分解为一维概率分布的乘积：
- en: <math alttext="dollar-sign p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis equals product Underscript i equals 1 Overscript
    n Endscripts p Subscript m o d e l Baseline left-parenthesis x Subscript i Baseline
    vertical-bar x 1 comma x 2 comma ellipsis comma x Subscript i minus 1 Baseline
    right-parenthesis dollar-sign"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis equals product Underscript i equals 1 Overscript
    n Endscripts p Subscript m o d e l Baseline left-parenthesis x Subscript i Baseline
    vertical-bar x 1 comma x 2 comma ellipsis comma x Subscript i minus 1 Baseline
    right-parenthesis dollar-sign"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: The main drawback here is that samples must be generated one component at a
    time (one pixel of an image, or one character of a word, or one entry of a discrete
    audio wave), therefore, the cost of generating one sample is *O(n)*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要缺点是样本必须逐个组件生成（图像的一个像素，单词的一个字符，或离散音频波形的一个条目），因此，生成一个样本的成本是 *O(n)*。
- en: 'Example: Generating Images via PixelCNN And Machine Audio via WaveNet'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：通过 PixelCNN 生成图像和通过 WaveNet 生成机器音频。
- en: '[PixelCNN](https://arxiv.org/pdf/1606.05328.pdf) trains a convolutional neural
    network that models the conditional distribution of every individual pixel given
    previous pixels (to the left and to the top of the target pixel). [Figure 8-1](#Fig_PixelCNN)
    illustrates this.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[PixelCNN](https://arxiv.org/pdf/1606.05328.pdf) 训练了一个卷积神经网络，模拟了给定先前像素（左侧和目标像素上方）的每个单独像素的条件分布。[图8-1](#Fig_PixelCNN)
    说明了这一点。'
- en: '![300](assets/emai_0801.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0801.png)'
- en: Figure 8-1\. PixelCNN learning the conditional distribution of the nth pixel
    conditioned on the previous n-1 pixels [(image source)](https://arxiv.org/pdf/1606.05328.pdf).
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. PixelCNN 学习第 n 个像素的条件分布，条件是前 n-1 个像素 [(图像来源)](https://arxiv.org/pdf/1606.05328.pdf)。
- en: WaveNet trains a convolutional neural network that models the conditional distribution
    of each entry of an audiowave given the previous entries. We will only elaborate
    on WaveNet. It is the one dimensional analogue of PixelCNN and it captures the
    essential ideas.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet 训练了一个卷积神经网络，模拟了音频波形中每个条目的条件分布，给定先前的条目。我们只会详细介绍 WaveNet。它是 PixelCNN 的一维模拟，并捕捉了基本思想。
- en: The goal of WaveNet is to generate wideband raw audio waveforms. So we must
    learn the joint probability distribution of an audio waveform <math alttext="ModifyingAbove
    x With right-arrow equals left-parenthesis x 1 comma x 2 comma ellipsis comma
    x Subscript upper T Baseline right-parenthesis"><mrow><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mi>T</mi></msub> <mo>)</mo></mrow></mrow></math> from a certain
    genre.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet 的目标是生成宽带原始音频波形。因此，我们必须学习音频波形的联合概率分布 <math alttext="ModifyingAbove x
    With right-arrow equals left-parenthesis x 1 comma x 2 comma ellipsis comma x
    Subscript upper T Baseline right-parenthesis"><mrow><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mi>T</mi></msub> <mo>)</mo></mrow></mrow></math> 来自某种类型。
- en: 'We use the product rule to decompose the joint distribution into a product
    of single variable distributions where we condition each entry of the audio waveform
    on those that preceded it:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用乘法规则将联合分布分解为单变量分布的乘积，其中我们将音频波形的每个条目条件化为其之前的条目：
- en: <math alttext="dollar-sign p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis equals product Underscript t equals 1 Overscript
    upper T Endscripts p Subscript m o d e l Baseline left-parenthesis x Subscript
    t Baseline vertical-bar x 1 comma x 2 comma ellipsis comma x Subscript t minus
    1 Baseline right-parenthesis dollar-sign"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>T</mi></msubsup>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis equals product Underscript t equals 1 Overscript
    upper T Endscripts p Subscript m o d e l Baseline left-parenthesis x Subscript
    t Baseline vertical-bar x 1 comma x 2 comma ellipsis comma x Subscript t minus
    1 Baseline right-parenthesis dollar-sign"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow> <mi>T</mi></msubsup>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: One difficulty is that audio waveforms have very high temporal resolution, with
    at least 16000 entries per one second of audio (so one data sample that is a minute
    long is a vector with *T*=960000 entries). Each of these entries represents one
    time step of discretized raw audio, and is usually stored as a 16 bit integer.
    That is, each entry can assume any value between zero and 65535\. If we keep this
    range, the network has to learn the probability for each entry so the softmax
    function at the output level has to output 65536 probability score for every single
    entry. The total number of entries we have to do this for, along with the computational
    complexity of the network itself, become very expensive. To make this more tractable,
    we must quantize, which in electronics means approximate a continuously varying
    signal by one whose amplitude is restricted to a prescribed set of values. WaveNet
    transforms the raw data to restrict the entries’ values to 256 options each, ranging
    from 0 to 255, similar to pixel range for digital images. Now, during training,
    the network must learn the probability distribution of each entry over these 256
    values, given the preceding entries, and during audio generation, it samples from
    these learned distributions one entry at a time.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个困难是音频波形具有非常高的时间分辨率，每秒至少有 16000 个条目（因此，一分钟长的一个数据样本是一个具有 *T*=960000 个条目的向量）。这些条目中的每一个代表离散原始音频的一个时间步，并通常存储为
    16 位整数。也就是说，每个条目可以假定为介于零和 65535 之间的任何值。如果我们保持这个范围，网络必须学习每个条目的概率，因此输出级别的 softmax
    函数必须为每个单个条目输出 65536 个概率分数。我们必须为这些条目的总数以及网络本身的计算复杂性变得非常昂贵。为了使这更易处理，我们必须量化，这在电子学中意味着通过其幅度受限于一组规定值的信号来近似连续变化的信号。WaveNet
    将原始数据转换为将条目值限制为每个 256 个选项的选项，范围从 0 到 255，类似于数字图像的像素范围。现在，在训练期间，网络必须学习每个条目在这 256
    个值上的概率分布，给定先前的条目，在音频生成期间，它逐个条目从这些学习的分布中采样。
- en: The last complication is that if the audio signal represents anything meaningful,
    then the vector representing it has long range dependencies over multiple time
    scales. In order to capture these long-range dependencies, WaveNet uses dilated
    convolutions. These are one dimensional kernels or filters that skip some entries
    to cover wider range without increasing the number of parameters (see [Figure 8-2](#Fig_dilated_convolution)
    for illustration).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个复杂性是，如果音频信号代表任何有意义的东西，那么表示它的向量在多个时间尺度上具有长距离依赖性。为了捕捉这些长距离依赖性，WaveNet 使用扩张卷积。这些是跳过一些条目以覆盖更广泛范围而不增加参数数量的一维核或滤波器（请参见
    [图8-2](#Fig_dilated_convolution) 进行说明）。
- en: '![300](assets/emai_0802.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0802.png)'
- en: Figure 8-2\. Dilated convolution with kernel size equals two. At each layer
    the kernel has only two parameters but it skips entries for larger coverage [(image
    source that has a nice animation)](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio).
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 具有核大小为两的扩张卷积。在每一层，核只有两个参数，但它会跳过一些条目以获得更广泛的覆盖范围 [(具有漂亮动画的图像来源)](https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio)。
- en: Note also that the network cannot peek into the future so the filters at each
    layer cannot use entries from the training sample that are ahead of the target
    entry. In one dimension we just stop filtering earlier at each convolutional layer,
    so it is a simple time shift. In two dimensions we use *masked* filters which
    have zeros to the right and to the bottom of the central entry.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，网络无法预知未来，因此每个层的滤波器不能使用训练样本中位于目标条目之前的条目。在一维中，我们只是在每个卷积层中较早地停止滤波，因此这是一个简单的时间偏移。在二维中，我们使用*掩码*滤波器，其右侧和底部的中心条目为零。
- en: 'WaveNet learns a total of *T* probability distributions, one for each entry
    of the audio waveform conditioned on those entries that preceded it: <math alttext="p
    Subscript m o d e l Baseline left-parenthesis x 1 right-parenthesis comma p Subscript
    m o d e l Baseline left-parenthesis x 2 vertical-bar x 1 right-parenthesis comma
    p Subscript m o d e l Baseline left-parenthesis x 3 vertical-bar x 1 comma x 2
    right-parenthesis comma d o t s"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>,</mo>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>,</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>,</mo> <mi>d</mi> <mi>o</mi> <mi>t</mi> <mi>s</mi></mrow></math> and <math
    alttext="p Subscript m o d e l Baseline left-parenthesis x Subscript upper T Baseline
    vertical-bar x 1 comma x 2 comma ellipsis comma x Subscript upper T minus 1 Baseline
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>T</mi></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>T</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math> . During training, these distributions can be
    computed in parallel.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet学习了总共*T*个概率分布，每个概率分布对应音频波形的一个条目，条件是前面的条目：<math alttext="p Subscript m
    o d e l Baseline left-parenthesis x 1 right-parenthesis comma p Subscript m o
    d e l Baseline left-parenthesis x 2 vertical-bar x 1 right-parenthesis comma p
    Subscript m o d e l Baseline left-parenthesis x 3 vertical-bar x 1 comma x 2 right-parenthesis
    comma d o t s"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>,</mo>
    <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>,</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>,</mo> <mi>d</mi> <mi>o</mi> <mi>t</mi> <mi>s</mi></mrow></math> 和 <math alttext="p
    Subscript m o d e l Baseline left-parenthesis x Subscript upper T Baseline vertical-bar
    x 1 comma x 2 comma ellipsis comma x Subscript upper T minus 1 Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>T</mi></msub> <mo>|</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mrow><mi>T</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>)</mo></mrow></mrow></math>
    。在训练过程中，这些分布可以并行计算。
- en: 'Now suppose we need to learn the probability distribution of the 100th entry,
    given the previous 99 entries. We input batches of audio samples from the training
    data, the convolutional network uses only the first 99 entries of each sample,
    computing linear combinations (the filters linearly combine), passing through
    nonlinear activation functions from one layer to the next to the next, and using
    some skip connections and residual layers to battle vanishing gradients, finally
    passing the result through a softmax function and outputing a vector of length
    256 containing probability scores for the value of the 100th entry. This is the
    probability distribution for the 100th entry output by the model. After comparing
    this output distribution with the emperical distribution of the data for the 100th
    entry from the training batch, the parameters of the network get adjusted to decrease
    the error (lower the cross entropy or increase the likelihood). As more batches
    of data and more epochs pass through the network, the probability distribution
    for the 100th entry given the previous 99 will appproach the emperical distribution
    from the training data. What we save within the network after training are the
    values of the parameters. Now we can use the trained network to generate machine
    audio, one entry at a time:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们需要学习第100个条目的概率分布，给定前面的99个条目。我们从训练数据中输入音频样本批次，卷积网络仅使用每个样本的前99个条目，计算线性组合（滤波器进行线性组合），通过非线性激活函数从一层传递到下一层，再到下一层，使用一些跳跃连接和残差层来对抗消失的梯度，最终通过softmax函数传递结果，并输出一个长度为256的向量，包含第100个条目的概率分数。这是模型输出的第100个条目的概率分布。将此输出分布与来自训练批次的第100个条目的经验分布进行比较后，网络的参数将被调整以减少错误（降低交叉熵或增加可能性）。随着更多数据批次和更多时代通过网络，给定前99个条目的第100个条目的概率分布将接近训练数据的经验分布。训练后我们在网络中保存的是参数的值。现在我们可以使用训练好的网络逐个生成机器音频条目：
- en: Sample a value <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>
    from the probability distribution late <math alttext="p Subscript m o d e l Baseline
    left-parenthesis x 1 right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
    .
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从概率分布中抽取一个值<math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math> 。
- en: Augment <math alttext="left-parenthesis x 1 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math> with zeros to establish
    the required length for the network’s input (check this) and pass the vector through
    the network. We will get as an output late <math alttext="p Subscript m o d e
    l Baseline left-parenthesis x 2 vertical-bar x 1 right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow></mrow></math> from which we can sample <math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math> .
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用零增加<math alttext="left-parenthesis x 1 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math>，以建立网络输入所需的长度（请检查），并通过网络传递向量。我们将得到一个输出，稍后<math
    alttext="p Subscript m o d e l Baseline left-parenthesis x 2 vertical-bar x 1
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>，从中我们可以对<math alttext="x 2"><msub><mi>x</mi>
    <mn>2</mn></msub></math>进行采样。
- en: Augment <math alttext="left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> with zeros (check this) and pass the vector through the
    network. We will get as an output late <math alttext="p Subscript m o d e l Baseline
    left-parenthesis x 3 vertical-bar x 1 comma x 2 right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>|</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    from which we can sample <math alttext="x 3"><msub><mi>x</mi> <mn>3</mn></msub></math>
    .
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用零增加<math alttext="left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math>，并通过网络传递向量。我们将得到一个输出，稍后<math alttext="p Subscript m o
    d e l Baseline left-parenthesis x 3 vertical-bar x 1 comma x 2 right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>|</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>，从中我们可以对<math
    alttext="x 3"><msub><mi>x</mi> <mn>3</mn></msub></math>进行采样。
- en: Keep going.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续。
- en: We can condition WaveNet on a certain speaker identity, so we can generate different
    voices using one model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据特定的说话者身份来调节WaveNet，因此我们可以使用一个模型生成不同的声音。
- en: The fact that we can train WaveNet in parallel but use it to generate audio
    only sequentially is a major shortcoming. This has been rectified since then with
    [Parallel WaveNet](https://arxiv.org/pdf/1711.10433.pdf) which is deployed online
    by Google Assistant, including serving multiple English and Japanese voices.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以并行训练WaveNet，但只能顺序生成音频是一个主要缺点。这已经得到了纠正，[Parallel WaveNet](https://arxiv.org/pdf/1711.10433.pdf)已经在线部署在Google
    Assistant上，包括提供多种英语和日语语音。
- en: To summarize and place the above discussion in the same mathematical context
    as this chapter, PixelCNN and WaveNet are models that aim to learn the joint probability
    distribution of image data or audio data from certain genres. They do so by decomposing
    the joint distribution into a product of one dimensional probability distributions
    for each entry of their data, conditioned on all the preceding entries. To find
    these one dimensional conditional distributions, they use a convolutional network
    to learn the way the observed entries interact together to produce a distribution
    of the next entry. This way, the input to the network is deterministic and its
    output is a probability mass function. The network itself is also a deterministic
    function. We can view the network together with its output as a probability distribution
    with parameters that we tweak. As the training evolves, the output gets adjusted
    until it reaches an acceptable agreement with the emperical distribution of the
    training data. Therefore, we are not applying a deterministic function to a probability
    distribution and tweaking the function’s parameters until we agree with the distribution
    of the training data. We are instead starting with an explicit formula for a probability
    distribution with many parameters (the network’s parameters), then tweaking the
    parameters until this explicit probability distribution reasonably agrees with
    training data. We do this for each conditional probability distribution corresponding
    to each entry.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总结并将上述讨论放在与本章相同的数学背景下，PixelCNN和WaveNet是旨在学习特定类型图像数据或音频数据的联合概率分布的模型。它们通过将联合分布分解为每个数据条目的一维概率分布的乘积，条件于所有先前的条目来实现这一目标。为了找到这些一维条件分布，它们使用卷积网络来学习观察到的条目如何相互作用以产生下一个条目的分布。这样，网络的输入是确定性的，其输出是一个概率质量函数。网络本身也是一个确定性函数。我们可以将网络及其输出视为具有我们调整参数的概率分布。随着训练的进行，输出会进行调整，直到与训练数据的经验分布达成可接受的一致。因此，我们不是将确定性函数应用于概率分布并调整函数的参数，直到我们同意训练数据的分布。相反，我们从一个具有许多参数（网络参数）的显式概率分布的公式开始，然后调整参数，直到这个显式概率分布与训练数据合理一致。我们对应于每个条目的每个条件概率分布都这样做。
- en: 'Explicit Density- Tractable: Change of Variables Nonlinear Independent Component
    Analysis'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密度显式-可计算：变量变换非线性独立分量分析
- en: The main idea here is that we have the random variable representing the observed
    training data <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> and we want to learn the source random variable <math
    alttext="ModifyingAbove s With right-arrow"><mover accent="true"><mi>s</mi> <mo>→</mo></mover></math>
    that generated it. We assume that there is a deterministic transformation <math
    alttext="g left-parenthesis ModifyingAbove s With right-arrow right-parenthesis
    equals ModifyingAbove x With right-arrow"><mrow><mi>g</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></math> that is invertible and
    differentiable that transforms the unknown <math alttext="ModifyingAbove s With
    right-arrow"><mover accent="true"><mi>s</mi> <mo>→</mo></mover></math> to the
    observed <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> . Then <math alttext="ModifyingAbove s With right-arrow
    equals g Superscript negative 1 Baseline left-parenthesis ModifyingAbove x With
    right-arrow right-parenthesis"><mrow><mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mo>=</mo> <msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    . Now we need to find an appropriate *g* and to find the probability distribution
    of <math alttext="ModifyingAbove s With right-arrow"><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover></math> . Moreover, we assume that <math alttext="ModifyingAbove
    s With right-arrow"><mover accent="true"><mi>s</mi> <mo>→</mo></mover></math>
    has independent entries, or components, so that its probability distribution is
    nothing but the product of the distributions of its components.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要思想是我们有代表观察到的训练数据的随机变量x→，我们想要学习生成它的源随机变量s→。我们假设存在一个可逆且可微的确定性转换g（s→）= x→，将未知的s→转换为观察到的x→。然后s→
    = g^-1（x→）。现在我们需要找到一个适当的g，并找到s→的概率分布。此外，我们假设s→具有独立的条目或组件，因此其概率分布仅仅是其组件的分布的乘积。
- en: 'The formula that relates the probability distribution of a random variable
    with the probability distribution of a deterministic transformation of it is:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将随机变量的概率分布与其确定性转换的概率分布相关联的公式是：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column p Subscript s Baseline
    left-parenthesis ModifyingAbove s With right-arrow right-parenthesis 2nd Column
    equals p Subscript x Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis times d e t e r m i n a n t left-parenthesis upper J a c o b
    i a n right-parenthesis 2nd Row 1st Column Blank 2nd Column equals p Subscript
    x Baseline left-parenthesis g left-parenthesis ModifyingAbove s With right-arrow
    right-parenthesis right-parenthesis StartAbsoluteValue det left-parenthesis StartFraction
    normal partial-differential g left-parenthesis ModifyingAbove s With right-arrow
    right-parenthesis Over normal partial-differential ModifyingAbove s With right-arrow
    EndFraction right-parenthesis EndAbsoluteValue EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>p</mi> <mi>s</mi></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>p</mi> <mi>x</mi></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>×</mo>
    <mi>d</mi> <mi>e</mi> <mi>t</mi> <mi>e</mi> <mi>r</mi> <mi>m</mi> <mi>i</mi> <mi>n</mi>
    <mi>a</mi> <mi>n</mi> <mi>t</mi> <mrow><mo>(</mo> <mi>J</mi> <mi>a</mi> <mi>c</mi>
    <mi>o</mi> <mi>b</mi> <mi>i</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>p</mi> <mi>x</mi></msub>
    <mrow><mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo form="prefix" movablelimits="true">det</mo>
    <mrow><mo>(</mo> <mfrac><mrow><mi>∂</mi><mi>g</mi><mo>(</mo><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover><mo>)</mo></mrow> <mrow><mi>∂</mi><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover></mrow></mfrac> <mo>)</mo></mrow> <mo>|</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column p Subscript s Baseline
    left-parenthesis ModifyingAbove s With right-arrow right-parenthesis 2nd Column
    equals p Subscript x Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis times d e t e r m i n a n t left-parenthesis upper J a c o b
    i a n right-parenthesis 2nd Row 1st Column Blank 2nd Column equals p Subscript
    x Baseline left-parenthesis g left-parenthesis ModifyingAbove s With right-arrow
    right-parenthesis right-parenthesis StartAbsoluteValue det left-parenthesis StartFraction
    normal partial-differential g left-parenthesis ModifyingAbove s With right-arrow
    right-parenthesis Over normal partial-differential ModifyingAbove s With right-arrow
    EndFraction right-parenthesis EndAbsoluteValue EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>p</mi> <mi>s</mi></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>s</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>p</mi> <mi>x</mi></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>×</mo>
    <mi>d</mi> <mi>e</mi> <mi>t</mi> <mi>e</mi> <mi>r</mi> <mi>m</mi> <mi>i</mi> <mi>n</mi>
    <mi>a</mi> <mi>n</mi> <mi>t</mi> <mrow><mo>(</mo> <mi>J</mi> <mi>a</mi> <mi>c</mi>
    <mi>o</mi> <mi>b</mi> <mi>i</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>p</mi> <mi>x</mi></msub>
    <mrow><mo>(</mo> <mi>g</mi> <mrow><mo>(</mo> <mover accent="true"><mi>s</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mrow><mo>|</mo> <mo form="prefix" movablelimits="true">det</mo>
    <mrow><mo>(</mo> <mfrac><mrow><mi>∂</mi><mi>g</mi><mo>(</mo><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover><mo>)</mo></mrow> <mrow><mi>∂</mi><mover accent="true"><mi>s</mi>
    <mo>→</mo></mover></mrow></mfrac> <mo>)</mo></mrow> <mo>|</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: Multiplying by the determinant of the Jacobian of the transformation accounts
    for the change in volume in space due to the transformation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以转换的雅可比行列式解释了由于转换而导致空间体积的变化。
- en: '[Nonlinear independent component estimation](https://arxiv.org/pdf/1410.8516.pdf)
    models the joint probability distribution as the nonlinear transformation of the
    data <math alttext="ModifyingAbove s With right-arrow equals g Superscript negative
    1 Baseline left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    . The transformation *g* is learned such that <math alttext="g Superscript negative
    1"><msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> maps the data
    to a latent space where it conforms to a factorized distribution, that is, the
    mapping results in independent latent variables. The transformation <math alttext="g
    Superscript negative 1"><msup><mi>g</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    is parametrized in a way that allows for easy compution of the determinant of
    the Jacobian and the inverse Jacobian. <math alttext="g Superscript negative 1"><msup><mi>g</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> is based on a deep neural network
    and its parameters are learned via optimizing the log-likelihood, which is tractable.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性独立成分估计模型将联合概率分布建模为数据的非线性转换s→ = g^-1（x→）。学习转换g，使得g^-1将数据映射到一个潜在空间，其中符合分解分布，即映射导致独立的潜在变量。转换g^-1的参数化方式允许轻松计算雅可比行列式和逆雅可比。g^-1基于深度神经网络，其参数通过优化对数似然来学习，这是可行的。
- en: Note that the requirement that the transformation *g* must be invertible means
    that the latent variables <math alttext="ModifyingAbove s With right-arrow"><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover></math> must have the same dimension
    as the data features (length of <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math> ). This imposes restrictions
    on the choice of the function *g* and it is a disadvanatage of nonlinear independent
    component analysis models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，要求变换*g*必须是可逆的意味着潜在变量<math alttext="ModifyingAbove s With right-arrow"><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover></math>必须具有与数据特征相同的维度（<math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>的长度）。这对函数*g*的选择施加了限制，这是非线性独立成分分析模型的一个缺点。
- en: In comparison, generative adversarial networks impose very few requirements
    on g, and, in particular, allow <math alttext="ModifyingAbove s With right-arrow"><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover></math> to have more dimensions than
    <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> .
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，生成对抗网络对g的要求很少，特别是允许<math alttext="ModifyingAbove s With right-arrow"><mover
    accent="true"><mi>s</mi> <mo>→</mo></mover></math>比<math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>具有更多维度。
- en: 'Explicit Density- Intractable: Variational Autoencoders- Approximation via
    Variational Methods'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式密度-难以处理：变分自动编码器-通过变分方法进行近似
- en: Deterministic autoencoders are composed of an encoder that maps the data from
    x space to latent z space of lower dimension, and a decoder that in turn maps
    the data from z space to <math alttext="ModifyingAbove x With caret"><mover accent="true"><mi>x</mi>
    <mo>^</mo></mover></math> space, with the objective of not losing much information,
    or reducing the reconstruction error, which means keeping x and <math alttext="ModifyingAbove
    x With caret"><mover accent="true"><mi>x</mi> <mo>^</mo></mover></math> close,
    for example in the Euclidean distance sense. In this sense, we can view *principal
    component analysis* which is based on the singular value decomposition <math alttext="upper
    X equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> as a linear
    encoder, where the decoder is simply the transpose of the encoding matrix. Encoding
    and decoding functions can be nonlinear and/or neural networks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 确定性自动编码器由一个将数据从x空间映射到较低维度的潜在z空间的编码器和一个将数据从z空间映射到<math alttext="ModifyingAbove
    x With caret"><mover accent="true"><mi>x</mi> <mo>^</mo></mover></math>空间的解码器组成，其目标是不丢失太多信息，或者减少重构误差，这意味着保持x和<math
    alttext="ModifyingAbove x With caret"><mover accent="true"><mi>x</mi> <mo>^</mo></mover></math>接近，例如在欧几里得距离意义上。在这个意义上，我们可以将基于奇异值分解<math
    alttext="upper X equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math>的*主成分分析*视为线性编码器，其中解码器简单地是编码矩阵的转置。编码和解码函数可以是非线性和/或神经网络。
- en: For deterministic autoencoders, we cannot use the decoder as a data generator.
    At least, if we do, then we have a to pick some z from latent z space and apply
    the decoder function to it. We are unlikely to get any <math alttext="ModifyingAbove
    x With caret"><mover accent="true"><mi>x</mi> <mo>^</mo></mover></math> that is
    close to how the desired data x looks, unless we picked a z which corresponds
    to a coded x, due to overfitting. We need a regularization that provides us with
    some control over z space, giving us the benefit of avoiding overfitting and using
    autoencoders as a data generator. We accomplish this by shifting from deterministic
    autoencoding to probabilistic autoencoding.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于确定性自动编码器，我们不能将解码器用作数据生成器。至少，如果我们这样做，那么我们必须从潜在z空间中选择一些z并将解码器函数应用于它。除非我们选择了一个对应于编码x的z，否则我们不太可能得到任何接近所需数据x外观的<math
    alttext="ModifyingAbove x With caret"><mover accent="true"><mi>x</mi> <mo>^</mo></mover></math>，由于过拟合。我们需要一种正则化方法，使我们能够对z空间进行一定程度的控制，从而避免过拟合并将自动编码器用作数据生成器。我们通过从确定性自动编码转向概率自动编码来实现这一点。
- en: 'Variational autoencoders are probabilistic autoencoders: The encoder outputs
    probability distributions over the latent space z instead of single points. Moreover,
    during training, the loss function includes an extra regularization term that
    controls the distribution over the latent space. Therefore, the loss function
    for variational autoencoders contains a reconstruction term (such as mean squared
    distance) and a regularization term to control the probability distribution output
    by the encoder. The regularization term can be a KL divergence from a Gaussian
    distribution, since the underlying assumption is that simple probabilistic models
    best describe the training data. In other words, complex relationships can be
    probabilistically simple. We have to be careful here, since this introduces a
    bias: The simple assumption on the data distribution in the latent variable can
    be a drawback if it is too weak. That is, when the assumption on the prior distribution
    or the assumption on the approximate posterior distribution are too weak, even
    with a perfect optimization algorithm and infinite training data, the gap between
    the estimate <math alttext="script upper L"><mi>ℒ</mi></math> and the true log
    likelihood can lead to <math alttext="p Subscript m o d e l"><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>
    learning a completely different distribution than the true <math alttext="p Subscript
    d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>
    .'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器是概率自动编码器：编码器输出概率分布而不是单个点。此外，在训练过程中，损失函数包括一个额外的正则化项，用于控制潜在空间上的分布。因此，变分自动编码器的损失函数包含一个重构项（如均方距离）和一个正则化项，用于控制编码器输出的概率分布。正则化项可以是与高斯分布的KL散度，因为基本假设是简单的概率模型最好地描述训练数据。换句话说，复杂的关系可以在概率上简单描述。在这里我们必须小心，因为这引入了一种偏见：潜变量中对数据分布的简单假设如果太弱可能会有缺陷。也就是说，如果对先验分布或近似后验分布的假设太弱，即使使用完美的优化算法和无限的训练数据，估计值和真实对数似然之间的差距可能导致<math
    alttext="p Subscript m o d e l"><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub></math>学习到一个与真实<math
    alttext="p Subscript d a t a"><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub></math>完全不同的分布。
- en: Mathematically, we maximize a lower bound <math alttext="script upper L"><mi>ℒ</mi></math>
    on the log likelihood of the data. In science, variational methods define lower
    bounds on an energy functional that we want to maximize, or upper bounds on an
    energy functional that we want to minimize. These bounds are usually easier to
    obtain and have tractable optimization algorithms, even when the log likelihood
    does not. At the same time, they provide good estimates for the optimal values
    that we are searching for.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，我们最大化数据对数似然的下界<math alttext="script upper L"><mi>ℒ</mi></math>。在科学中，变分方法定义我们要最大化的能量泛函的下界，或者我们要最小化的能量泛函的上界。即使对数似然不容易获得，这些界通常更容易获得，并且具有可处理的优化算法。同时，它们为我们正在寻找的最优值提供了良好的估计。
- en: <math alttext="dollar-sign script upper L left-parenthesis ModifyingAbove x
    With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis
    less-than-or-equal-to log p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis
    dollar-sign"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>≤</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign script upper L left-parenthesis ModifyingAbove x
    With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis
    less-than-or-equal-to log p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis
    dollar-sign"><mrow><mi>ℒ</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>≤</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
- en: Variational methods often achieve very good likelihood, but subjective evaluation
    of samples regard their generated samples as having lower quality. They are also
    considered more difficult to optimize than fully visible belief networks. Moreover,
    people find their mathematics more difficult than that of fully visible belief
    networks and of generative adversarial networks (discussed soon).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 变分方法通常能够获得非常好的似然度，但对样本的主观评价认为它们生成的样本质量较低。人们认为它们比完全可见的信念网络更难优化。此外，人们发现它们的数学比完全可见的信念网络和生成对抗网络更难理解（即将讨论）。
- en: 'Explicit Density- Intractable: Boltzman Machine- Approximation via Markov Chain'
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显式密度-难以处理：Boltzman机器-通过马尔可夫链近似
- en: 'Boltzmann machines (1980’s) are a family of generative models that rely on
    Markov chains to train generative model. This is a sampling technique, that happens
    to be more expensive than the simple sampling of a minibatch from a data set to
    estimate a loss function. We will discuss Markov chains in the context of reinforcement
    learning in [Chapter 14](ch14.xhtml#ch14). In the context of data generation,
    they have many disadvantages that caused them to fall out of favor: High computational
    cost, impratical and less efficient to extend to higher dimensions, slow to converge,
    and no clear way to know whether the model has converged or not, even when the
    theory says it must converge. Markov scale methods have not scaled to problems
    like ImageNet generation.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann机器（1980年代）是一类依赖于马尔可夫链训练生成模型的家族。这是一种采样技术，比起从数据集中简单采样一个小批量来估计损失函数更昂贵。我们将在[第14章](ch14.xhtml#ch14)中讨论马尔可夫链在强化学习中的上下文。在数据生成的背景下，它们有许多缺点导致它们不受青睐：计算成本高，难以扩展到更高维度，收敛速度慢，甚至在理论上必须收敛时也没有明确的方法知道模型是否已经收敛。马尔可夫链方法尚未扩展到像ImageNet生成这样的问题规模。
- en: A Markov chain has a transition operator *q* that encodes the probability of
    transitioning from one state of the system to another. This transition operator
    *q* needs to be explicitly defined. We can generate data samples by repeatedly
    drawing a sample <math alttext="x prime tilde q left-parenthesis x prime vertical-bar
    x right-parenthesis"><mrow><msup><mi>x</mi> <mo>'</mo></msup> <mo>∼</mo> <mi>q</mi>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    , updating <math alttext="x prime"><msup><mi>x</mi> <mo>'</mo></msup></math> sequentially
    according to the transition operator *q*. This sequential nature of generation
    is another disadvantage compared to single step generation. Markov chain methods
    can sometimes guarantee that x’ will eventually converge to a sample from <math
    alttext="p Subscript m o d e l Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> , even though the convergence might
    be slow.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链具有一个转移算子*q*，编码系统从一个状态过渡到另一个状态的概率。这个转移算子*q*需要明确定义。我们可以通过重复抽取样本<math alttext="x
    prime tilde q left-parenthesis x prime vertical-bar x right-parenthesis"><mrow><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>∼</mo> <mi>q</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，根据转移算子*q*依次更新<math alttext="x
    prime"><msup><mi>x</mi> <mo>'</mo></msup></math>。与单步生成相比，这种顺序生成的性质是另一个缺点。马尔可夫链方法有时可以保证x'最终会收敛到从<math
    alttext="p Subscript m o d e l Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math>中抽取的样本，尽管收敛可能很慢。
- en: Some models, such as deep Boltzman machines, employ both Markov chain and variational
    approximations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型，如深度玻尔兹曼机，同时使用马尔可夫链和变分逼近。
- en: 'Implicit Density- Markov Chain: Generative Stochastic Network'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐密度-马尔可夫链：生成随机网络
- en: Generative Stochastic Networks (Bengio et al., 2014) do not explicitly define
    a density function, and instead use a Markov chain transition operator interacts
    indirectly with <math alttext="p Subscript m o d e l Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> by sampling from the
    training data. This Markov chain operator must be run several times to obtain
    a sample from <math alttext="p Subscript m o d e l Baseline left-parenthesis x
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> . These methods still
    suffer from the shortcomings of Markov chain methods mentioned in the previous
    section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 生成随机网络（Bengio等人，2014）不明确定义密度函数，而是使用马尔可夫链转移算子与<math alttext="p Subscript m o
    d e l Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>间接交互，通过从训练数据中抽样。必须多次运行这个马尔可夫链算子才能获得从<math
    alttext="p Subscript m o d e l Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math>中抽取的样本。这些方法仍然存在前一节提到的马尔可夫链方法的缺点。
- en: 'Implicit Density- Direct: Generative Adversarial Networks'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐密度-直接：生成对抗网络
- en: 'Currently the most popular generative models are:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最流行的生成模型有：
- en: Fully visible deep belief networks, such as PixelCNN, WaveNet, and their variations.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可见的深度信念网络，如PixelCNN，WaveNet及其变体。
- en: Variational autoencoders, consisting of a probabilistic encoder-decoder architecture.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分自动编码器，由概率编码器-解码器架构组成。
- en: Generative adversarial networks, which have recieved a lot of attention from
    the scientific community, due to the simplicity of their concept and the good
    quality of their generated samples. We discuss them now.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络，由于其概念简单和生成样本质量好，受到科学界的关注。我们现在讨论它们。
- en: '[*Generative Adversarial Networks*](https://arxiv.org/pdf/1406.2661.pdf) were
    introduced in 2014 by Ian Goodfellow, et al. The mathematics involved is a beautiful
    mixture between probability and game theory. Generative adversarial networks avoid
    some disadvantages associated with other generative models:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[*生成对抗网络*](https://arxiv.org/pdf/1406.2661.pdf)由Ian Goodfellow等人于2014年提出。涉及的数学是概率和博弈论之间的美妙混合。生成对抗网络避免了与其他生成模型相关的一些缺点：'
- en: Generating samples all at once, in parallel, as opposed to feeding a new pixel
    back into the network to predict the one, such as in PixelCNN.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次性并行生成样本，而不是将新像素馈送回网络以预测像素，如PixelCNN。
- en: The generator function has few restrictions. This is an advantage relative to
    Boltzmann machines, for which few probability distributions admit tractable Markov
    chain sampling, and relative to nonlinear independent component analysis, for
    which the generator must be invertible and the latent variables z must have the
    same dimension as the samples x.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成函数的限制很少。这是与玻尔兹曼机相比的优势，对于玻尔兹曼机，很少有概率分布适合可计算的马尔可夫链抽样，相对于非线性独立成分分析，生成器必须是可逆的，潜在变量z必须与样本x具有相同的维度。
- en: Generative adversarial networks do not need Markov chains. This is an advantage
    relative to Boltzmann machines and to generative stochastic networks.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络不需要马尔可夫链。这是与玻尔兹曼机和生成随机网络相比的优势。
- en: While variational autoencoders might never converge to the true data generating
    distribution if they assume prior or posterior distributions that are too weak,
    generative adversarial networks converge to the true <math alttext="p Subscript
    d Baseline a t a"><mrow><msub><mi>p</mi> <mi>d</mi></msub> <mi>a</mi> <mi>t</mi>
    <mi>a</mi></mrow></math> , given that we have infinite training data and a large
    enough model. Moreover, generative adversarial networks do not need variational
    bounds, and the specific model families used within the generative adversarial
    network framework are already known to be universal approximators. Thus, generative
    adversarial networks are already known to be asymptotically consistent. On the
    other hand, some variational autoencoders are conjectured to be asymptotically
    consistent, but this still needs to be proven (check this).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然变分自动编码器可能永远无法收敛到真实数据生成分布，如果它们假设的先验或后验分布太弱，生成对抗网络会收敛到真实<p data baseline="d">，假设我们有无限的训练数据和足够大的模型。此外，生成对抗网络不需要变分界限，而且在生成对抗网络框架内使用的特定模型族已知是通用逼近器。因此，生成对抗网络已知是渐近一致的。另一方面，一些变分自动编码器被推测为渐近一致，但这仍然需要证明（请检查）。
- en: The disadvantage of generative adversarial networks is that training them requires
    spotting the Nash equilibrium of a game, which is more difficult than just optimizing
    an objective function. Moreover, the solution tends to be numerically unstable.
    This was improved in 2015 by Alec Radford, et al, [Deep Convolutional Generative
    Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf). This approach led
    to more stable models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络的缺点在于训练它们需要找到博弈的纳什均衡，这比仅仅优化一个目标函数更困难。此外，解决方案往往在数值上不稳定。2015年，Alec Radford等人改进了这一点，[深度卷积生成对抗网络](https://arxiv.org/pdf/1511.06434.pdf)。这种方法导致了更稳定的模型。
- en: 'During training, generative adversarial networks formulate a game between two
    separate networks: A generator network and a discriminator network that tries
    to classify genrator samples as either coming from the true distribution <math
    alttext="p Subscript d a t a Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> or from the model <math alttext="p
    Subscript m o d e l Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> . The loss functions of the two networks
    are related, so that the discriminator communicates the discrepancy between the
    two distributions and the generator adjusts its parameters accordingly, until
    the generator exactly reproduces the true data distribution (in theory) so that
    the discriminator’s classifications are no better than random guesses.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，生成对抗网络构建了两个独立网络之间的博弈：一个生成器网络和一个鉴别器网络，鉴别器试图将生成器样本分类为来自真实分布<p data baseline="x">或来自模型<p
    data baseline="x">。这两个网络的损失函数是相关的，因此鉴别器传达了两个分布之间的差异，生成器相应地调整其参数，直到生成器完全复制真实数据分布（理论上），使得鉴别器的分类不比随机猜测更好。
- en: The generator network wants to maximize the probability that the discriminator
    assigns the wrong label in its classification whether the sample is from the training
    data or from the model, while the discriminator network wants to minimize that
    probability. This is a two player zero sum game, where one player’s gain is another’s
    loss. We end up solving a minimax problem instead of a purely maximizing or minimizing
    problem. A unique solution exists.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络希望最大化鉴别器在分类中分配错误标签的概率，无论样本是来自训练数据还是模型，而鉴别器网络希望最小化该概率。这是一个两人零和博弈，其中一方的收益是另一方的损失。我们最终解决了一个极小极大问题，而不是一个纯粹的最大化或最小化问题。存在唯一的解决方案。
- en: How Do Generative Adversarial Networks Work?
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络是如何工作的？
- en: 'Keeping the goal of learning the generator’s probability distribution <math
    alttext="p Subscript g Baseline left-parenthesis ModifyingAbove x With right-arrow
    semicolon ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>;</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    over the data, here’s how the learning progresses for generative adversarial networks:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习生成器的概率分布<p data baseline="x">，这里是生成对抗网络学习过程的进展：
- en: Start with a random sample <math alttext="ModifyingAbove z With right-arrow"><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover></math> from a prior probability distribution
    <math alttext="p Subscript z Baseline left-parenthesis ModifyingAbove z With right-arrow
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>z</mi></msub> <mrow><mo>(</mo> <mover
    accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> ,
    which could be just uniform random noise for each component of <math alttext="ModifyingAbove
    z With right-arrow"><mover accent="true"><mi>z</mi> <mo>→</mo></mover></math>
    .
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从先验概率分布<p data baseline="z">中随机抽取一个样本<p data baseline="z">，对于<p data baseline="z">的每个分量，这可能只是均匀随机噪声。
- en: Start also with a random sample <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math> from the training data, so
    it is a sample from the probability distribution <math alttext="p Subscript d
    a t a Baseline left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    that the generator is trying to learn.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时从训练数据中随机抽取一个样本<math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math>，使其成为生成器试图学习的概率分布<math alttext="p Subscript d a t a Baseline
    left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>的样本。
- en: Apply to <math alttext="ModifyingAbove z With right-arrow"><mover accent="true"><mi>z</mi>
    <mo>→</mo></mover></math> the deterministic function <math alttext="upper G left-parenthesis
    ModifyingAbove z With right-arrow comma ModifyingAbove theta With right-arrow
    Subscript g Baseline right-parenthesis"><mrow><mi>G</mi> <mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>g</mi></msub> <mo>)</mo></mrow></math> representing the generative neural
    network. The parameters <math alttext="ModifyingAbove theta With right-arrow Subscript
    g"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub></math>
    are the ones we need to tweak via backpropagation until the output <math alttext="upper
    G left-parenthesis ModifyingAbove z With right-arrow comma ModifyingAbove theta
    With right-arrow Subscript g Baseline right-parenthesis"><mrow><mi>G</mi> <mo>(</mo>
    <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow></math> looks similar to
    samples from the training data set.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将确定性函数应用于<math alttext="ModifyingAbove z With right-arrow"><mover accent="true"><mi>z</mi>
    <mo>→</mo></mover></math>，表示生成神经网络的函数<math alttext="upper G left-parenthesis ModifyingAbove
    z With right-arrow comma ModifyingAbove theta With right-arrow Subscript g Baseline
    right-parenthesis"><mrow><mi>G</mi> <mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>g</mi></msub> <mo>)</mo></mrow></math>。参数<math alttext="ModifyingAbove theta
    With right-arrow Subscript g"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>g</mi></msub></math>是我们需要通过反向传播调整的参数，直到输出<math alttext="upper G left-parenthesis
    ModifyingAbove z With right-arrow comma ModifyingAbove theta With right-arrow
    Subscript g Baseline right-parenthesis"><mrow><mi>G</mi> <mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>g</mi></msub> <mo>)</mo></mrow></math>看起来类似于训练数据集中的样本。
- en: Pass the output <math alttext="upper G left-parenthesis ModifyingAbove z With
    right-arrow comma ModifyingAbove theta With right-arrow Subscript g Baseline right-parenthesis"><mrow><mi>G</mi>
    <mo>(</mo> <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow></math>
    into another deterministic function *D* representing the discriminative neural
    network. So now we have the new output <math alttext="upper D left-parenthesis
    upper G left-parenthesis ModifyingAbove z With right-arrow comma ModifyingAbove
    theta With right-arrow Subscript g Baseline right-parenthesis comma ModifyingAbove
    left-parenthesis With right-arrow theta right-parenthesis Subscript d Baseline
    right-parenthesis"><mrow><mi>D</mi> <msub><mrow><mo>(</mo><mi>G</mi><mrow><mo>(</mo><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover><mo>,</mo><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow><mo>,</mo><mover accent="true"><mo>(</mo>
    <mo>→</mo></mover><mi>θ</mi><mo>)</mo></mrow> <mi>d</mi></msub> <mrow><mo>)</mo></mrow></mrow></math>
    that is just a number closer to one or to zero, signifying whether this sample
    came from the generator or from the training data. Thus, for this input from the
    generator <math alttext="upper D left-parenthesis upper G left-parenthesis ModifyingAbove
    z With right-arrow comma ModifyingAbove theta With right-arrow Subscript g Baseline
    right-parenthesis comma ModifyingAbove left-parenthesis With right-arrow theta
    right-parenthesis Subscript d Baseline right-parenthesis"><mrow><mi>D</mi> <msub><mrow><mo>(</mo><mi>G</mi><mrow><mo>(</mo><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover><mo>,</mo><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow><mo>,</mo><mover accent="true"><mo>(</mo>
    <mo>→</mo></mover><mi>θ</mi><mo>)</mo></mrow> <mi>d</mi></msub> <mrow><mo>)</mo></mrow></mrow></math>
    must return a number close to one. The parameters <math alttext="ModifyingAbove
    theta With right-arrow Subscript d"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>d</mi></msub></math> are the ones we need to tweak via backpropagation until
    *D* returns the wrong classification around half of the time.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出<math alttext="upper G left-parenthesis ModifyingAbove z With right-arrow
    comma ModifyingAbove theta With right-arrow Subscript g Baseline right-parenthesis"><mrow><mi>G</mi>
    <mo>(</mo> <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow></math>传递到另一个确定性函数*D*，代表判别性神经网络。现在我们有新的输出<math
    alttext="upper D left-parenthesis upper G left-parenthesis ModifyingAbove z With
    right-arrow comma ModifyingAbove theta With right-arrow Subscript g Baseline right-parenthesis
    comma ModifyingAbove left-parenthesis With right-arrow theta right-parenthesis
    Subscript d Baseline right-parenthesis"><mrow><mi>D</mi> <msub><mrow><mo>(</mo><mi>G</mi><mrow><mo>(</mo><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover><mo>,</mo><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow><mo>,</mo><mover accent="true"><mo>(</mo>
    <mo>→</mo></mover><mi>θ</mi><mo>)</mo></mrow> <mi>d</mi></msub> <mrow><mo>)</mo></mrow></mrow></math>，这只是一个接近一或零的数字，表示这个样本是来自生成器还是训练数据。因此，对于来自生成器的这个输入<math
    alttext="upper D left-parenthesis upper G left-parenthesis ModifyingAbove z With
    right-arrow comma ModifyingAbove theta With right-arrow Subscript g Baseline right-parenthesis
    comma ModifyingAbove left-parenthesis With right-arrow theta right-parenthesis
    Subscript d Baseline right-parenthesis"><mrow><mi>D</mi> <msub><mrow><mo>(</mo><mi>G</mi><mrow><mo>(</mo><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover><mo>,</mo><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow><mo>,</mo><mover accent="true"><mo>(</mo>
    <mo>→</mo></mover><mi>θ</mi><mo>)</mo></mrow> <mi>d</mi></msub> <mrow><mo>)</mo></mrow></mrow></math>必须返回接近一的数字。参数<math
    alttext="ModifyingAbove theta With right-arrow Subscript d"><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mi>d</mi></msub></math>是我们需要通过反向传播调整的，直到*D*返回错误分类约一半的时间。
- en: Pass also the sample <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math> from the training data to *D*,
    so we evaluate <math alttext="upper D left-parenthesis ModifyingAbove x With right-arrow
    comma ModifyingAbove theta With right-arrow Subscript d Baseline right-parenthesis"><mrow><mi>D</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>d</mi></msub> <mo>)</mo></mrow></math>
    . For this input, <math alttext="upper D left-parenthesis ModifyingAbove x With
    right-arrow comma ModifyingAbove theta With right-arrow Subscript d Baseline right-parenthesis"><mrow><mi>D</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>d</mi></msub> <mo>)</mo></mrow></math>
    must return a number close to zero.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还将来自训练数据的样本<math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math>传递给*D*，以便我们评估<math alttext="upper D left-parenthesis
    ModifyingAbove x With right-arrow comma ModifyingAbove theta With right-arrow
    Subscript d Baseline right-parenthesis"><mrow><mi>D</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>d</mi></msub> <mo>)</mo></mrow></math>。对于这个输入，<math alttext="upper D left-parenthesis
    ModifyingAbove x With right-arrow comma ModifyingAbove theta With right-arrow
    Subscript d Baseline right-parenthesis"><mrow><mi>D</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>d</mi></msub> <mo>)</mo></mrow></math>必须返回接近零的数字。
- en: 'What is the loss function for these two networks, that has in its formula both
    sets of parameters <math alttext="ModifyingAbove theta With right-arrow Subscript
    g"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub></math>
    and <math alttext="ModifyingAbove theta With right-arrow Subscript d"><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>d</mi></msub></math> , along with
    the sampled vectors <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> and <math alttext="ModifyingAbove z With right-arrow"><mover
    accent="true"><mi>z</mi> <mo>→</mo></mover></math> ? The discriminator function
    *D* wants to get it right for both types of inputs, <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    and <math alttext="upper G left-parenthesis ModifyingAbove z With right-arrow
    comma ModifyingAbove theta With right-arrow Subscript g Baseline right-parenthesis"><mrow><mi>G</mi>
    <mo>(</mo> <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow></math>
    . So its parameters <math alttext="ModifyingAbove theta With right-arrow Subscript
    d"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>d</mi></msub></math>
    must be selected so that a number close to one is assigned a large score when
    the input is <math alttext="upper G left-parenthesis ModifyingAbove z With right-arrow
    comma ModifyingAbove theta With right-arrow Subscript g Baseline right-parenthesis"><mrow><mi>G</mi>
    <mo>(</mo> <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow></math>
    and a number close to zero is assigned a large value when the input is <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    . In both cases, we can use the negative of the *log* function since that is a
    function that is large near zero and small near one. Therefore, *D* needs the
    parameters <math alttext="ModifyingAbove theta With right-arrow Subscript d"><msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>d</mi></msub></math> that maximize:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个网络的损失函数是什么，其中的公式中包含两组参数<math alttext="ModifyingAbove theta With right-arrow
    Subscript g"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub></math>和<math
    alttext="ModifyingAbove theta With right-arrow Subscript d"><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mover> <mi>d</mi></msub></math>，以及采样向量<math alttext="ModifyingAbove x With
    right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>和<math alttext="ModifyingAbove
    z With right-arrow"><mover accent="true"><mi>z</mi> <mo>→</mo></mover></math>？鉴别器函数*D*希望对两种类型的输入<math
    alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>和<math
    alttext="upper G left-parenthesis ModifyingAbove z With right-arrow comma ModifyingAbove
    theta With right-arrow Subscript g Baseline right-parenthesis"><mrow><mi>G</mi>
    <mo>(</mo> <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub> <mo>)</mo></mrow></math>做出正确判断。因此，它的参数<math
    alttext="ModifyingAbove theta With right-arrow Subscript d"><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mover> <mi>d</mi></msub></math>必须被选择，以便在输入为<math alttext="upper G left-parenthesis
    ModifyingAbove z With right-arrow comma ModifyingAbove theta With right-arrow
    Subscript g Baseline right-parenthesis"><mrow><mi>G</mi> <mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mover>
    <mi>g</mi></msub> <mo>)</mo></mrow></math>时，接近1的数字被赋予较大的分数，而在输入为<math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>时，接近0的数字被赋予较大的值。在这两种情况下，我们可以使用*log*函数的负值，因为它在接近零时较大，在接近1时较小。因此，*D*需要最大化的参数<math
    alttext="ModifyingAbove theta With right-arrow Subscript d"><msub><mover accent="true"><mi>θ</mi>
    <mo>→</mover> <mi>d</mi></msub></math>是：
- en: <math alttext="dollar-sign double-struck upper E Subscript ModifyingAbove x
    With right-arrow tilde p Sub Subscript d a t a Subscript left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis Baseline left-bracket log upper D left-parenthesis
    ModifyingAbove x With right-arrow comma ModifyingAbove theta With right-arrow
    Subscript d Baseline right-parenthesis right-bracket plus double-struck upper
    E Subscript ModifyingAbove z With right-arrow tilde p Sub Subscript z Subscript
    left-parenthesis ModifyingAbove z With right-arrow right-parenthesis Baseline
    left-bracket log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    ModifyingAbove z With right-arrow comma ModifyingAbove theta Subscript g Baseline
    With right-arrow right-parenthesis comma ModifyingAbove theta Subscript d Baseline
    With right-arrow right-parenthesis right-parenthesis right-bracket dollar-sign"><mrow><msub><mi>𝔼</mi>
    <mrow><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mi>D</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>d</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mi>z</mi></msub> <mrow><mo>(</mo><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi> <mi>g</mi></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi>
    <mi>d</mi></msub> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign double-struck upper E Subscript ModifyingAbove x
    With right-arrow tilde p Sub Subscript d a t a Subscript left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis Baseline left-bracket log upper D left-parenthesis
    ModifyingAbove x With right-arrow comma ModifyingAbove theta With right-arrow
    Subscript d Baseline right-parenthesis right-bracket plus double-struck upper
    E Subscript ModifyingAbove z With right-arrow tilde p Sub Subscript z Subscript
    left-parenthesis ModifyingAbove z With right-arrow right-parenthesis Baseline
    left-bracket log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    ModifyingAbove z With right-arrow comma ModifyingAbove theta Subscript g Baseline
    With right-arrow right-parenthesis comma ModifyingAbove theta Subscript d Baseline
    With right-arrow right-parenthesis right-parenthesis right-bracket dollar-sign"><mrow><msub><mi>𝔼</mi>
    <mrow><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mi>D</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>d</mi></msub> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mi>z</mi></msub> <mrow><mo>(</mo><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi> <mi>g</mi></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi>
    <mi>d</mi></msub> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: 'At the same time, *G* needs the parameters <math alttext="ModifyingAbove theta
    With right-arrow Subscript g"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mi>g</mi></msub></math> that minimize <math alttext="log left-parenthesis 1 minus
    upper D left-parenthesis upper G left-parenthesis ModifyingAbove z With right-arrow
    comma ModifyingAbove theta Subscript g Baseline With right-arrow right-parenthesis
    comma ModifyingAbove theta Subscript d Baseline With right-arrow right-parenthesis
    right-parenthesis"><mrow><mo form="prefix">log</mo> <mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi> <mi>g</mi></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi>
    <mi>d</mi></msub> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
    . Combined together, *D* and *G* engage in a two-player minimax game with value
    function V (G, D):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，*G*需要最小化的参数<math alttext="ModifyingAbove theta With right-arrow Subscript
    g"><msub><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mi>g</mi></msub></math>，使得<math
    alttext="log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    ModifyingAbove z With right-arrow comma ModifyingAbove theta Subscript g Baseline
    With right-arrow right-parenthesis comma ModifyingAbove theta Subscript d Baseline
    With right-arrow right-parenthesis right-parenthesis"><mrow><mo form="prefix">log</mo>
    <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>z</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi>
    <mi>g</mi></msub> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>,</mo> <mover accent="true"><msub><mi>θ</mi>
    <mi>d</mi></msub> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math>。*D*和*G*一起参与一个具有值函数V(G,
    D)的两人零和博弈：
- en: <math alttext="dollar-sign min Underscript upper G Endscripts max Underscript
    upper D Endscripts upper V left-parenthesis upper D comma upper G right-parenthesis
    equals double-struck upper E Subscript ModifyingAbove x With right-arrow tilde
    p Sub Subscript d a t a Subscript left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis Baseline left-bracket log upper D left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis right-bracket plus double-struck upper E
    Subscript ModifyingAbove z With right-arrow tilde p Sub Subscript z Subscript
    left-parenthesis ModifyingAbove z With right-arrow right-parenthesis Baseline
    left-bracket log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    ModifyingAbove z With right-arrow right-parenthesis right-parenthesis right-parenthesis
    right-bracket dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mi>G</mi></msub> <msub><mo form="prefix" movablelimits="true">max</mo> <mi>D</mi></msub>
    <mi>V</mi> <mrow><mo>(</mo> <mi>D</mi> <mo>,</mo> <mi>G</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mi>D</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mi>z</mi></msub> <mrow><mo>(</mo><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript upper G Endscripts max Underscript
    upper D Endscripts upper V left-parenthesis upper D comma upper G right-parenthesis
    equals double-struck upper E Subscript ModifyingAbove x With right-arrow tilde
    p Sub Subscript d a t a Subscript left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis Baseline left-bracket log upper D left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis right-bracket plus double-struck upper E
    Subscript ModifyingAbove z With right-arrow tilde p Sub Subscript z Subscript
    left-parenthesis ModifyingAbove z With right-arrow right-parenthesis Baseline
    left-bracket log left-parenthesis 1 minus upper D left-parenthesis upper G left-parenthesis
    ModifyingAbove z With right-arrow right-parenthesis right-parenthesis right-parenthesis
    right-bracket dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mi>G</mi></msub> <msub><mo form="prefix" movablelimits="true">max</mo> <mi>D</mi></msub>
    <mi>V</mi> <mrow><mo>(</mo> <mi>D</mi> <mo>,</mo> <mi>G</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mi>D</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi>
    <mrow><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>∼</mo><msub><mi>p</mi>
    <mi>z</mi></msub> <mrow><mo>(</mo><mover accent="true"><mi>z</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>D</mi> <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mover accent="true"><mi>z</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
- en: This is a very simple mathematical structure, where setting up a discriminator
    network allows us to get closer to the true data distribution without ever explicitly
    defining it or assuming anything about it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的数学结构，通过设置鉴别器网络，我们可以更接近真实数据分布，而无需明确定义它或假设任何关于它的内容。
- en: 'Finally, we note that generative adversarial networks are highly promising
    for many applications. One example is the dramatic enhancement they have for semi-supervised
    learning, where [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)
    reports: *We introduce an approach for semi-supervised learning with generative
    adversarial networks that involves the discriminator producing an additional output
    indicating the label of the input. This approach allows us to obtain state of
    the art results on MNIST, SVHN, and CIFAR-10 in settings with very few labeled
    examples. On MNIST, for example, we achieve 99.14% accuracy with only 10 labeled
    examples per class with a fully connected neural network — a result that’s very
    close to the best known results with fully supervised approaches using all 60,000
    labeled examples. This is very promising because labeled examples can be quite
    expensive to obtain in practice.*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注意到生成对抗网络在许多应用中具有很高的潜力。一个例子是它们对半监督学习的显著增强，其中[NIPS 2016教程：生成对抗网络](https://arxiv.org/abs/1701.00160)报告：*我们引入了一种半监督学习的方法，使用生成对抗网络，其中鉴别器产生一个额外的输出，指示输入的标签。这种方法使我们能够在MNIST、SVHN和CIFAR-10上获得最先进的结果，即使只有很少的标记示例。例如，在MNIST上，我们只使用每类10个标记示例就实现了99.14%的准确率，使用全连接神经网络——这个结果非常接近使用所有60,000个标记示例的全监督方法的最佳已知结果。这是非常有希望的，因为在实践中获得标记示例可能非常昂贵。*
- en: Another far reaching application of generative adversarial networks (and machine
    learning in general) is simulating data for high energy physics. We discuss this
    next.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（以及机器学习一般）的另一个广泛应用是模拟高能物理数据。我们接下来讨论这个。
- en: 'Example: Machine Learning And Generative Networks For High Energy Physics'
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：机器学习和生成网络用于高能物理
- en: The following discussion is inspired by and borrows from [Machine Learning For
    Jet Physics Workshop 2020](https://iris-hep.org/2020/01/17/ml4jets-workshop.xhtml)
    and the two articles [Deep Learning and Its Application to Large Hadron Collider
    Physics](https://arxiv.org/pdf/1806.11484.pdf), and [Graph Generative Adversarial
    Networks for Sparse Data Generation in High Energy Physics](https://arxiv.org/pdf/2012.00173.pdf).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下讨论受到并借鉴了[2020年强子物理机器学习研讨会](https://iris-hep.org/2020/01/17/ml4jets-workshop.xhtml)和两篇文章[深度学习及其在大型强子对撞机物理中的应用](https://arxiv.org/pdf/1806.11484.pdf)和[用于高能物理中稀疏数据生成的图生成对抗网络](https://arxiv.org/pdf/2012.00173.pdf)。
- en: Before the deep learning revolution since 2012, the field of high energy physics
    traditionally relied in its analyses and computations on physical considerations
    and human intuition, boosted decision trees, hand crafted data feature engineering
    and dimensionality reduction, and traditional statistical analysis. These techniques,
    while insightful, are naturally far from optimal and hard to automate or extend
    to higher dimensions. Several studies have demonstrated that traditional shallow
    networks based on physics-inspired engineered *high-level* features are outperformed
    by deep networks based on the higher dimensional *lower-level* features which
    receive less pre-processing. Many areas of the [Large Hadron Collider](https://en.wikipedia.org/wiki/Large_Hadron_Collider)
    data analysis have suffered from long-standing sub-optimal feature engineering,
    and deserve re-examination. Thus, the high energy physics field is a breeding
    ground ripe for machine learning learning applications. A lot of progress is taking
    place on this front. The field is employing several machine learning techniques,
    including artificial neural networks, kernel density estimation, support vector
    machines, genetic algorithms, boosted decision trees, random forests, and generative
    networks.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年之前的深度学习革命之前，高能物理领域传统上依赖于物理考虑和人类直觉、增强决策树、手工制作的数据特征工程和降维、传统统计分析进行分析和计算。这些技术虽然具有洞察力，但自然远非最佳，难以自动化或扩展到更高维度。多项研究表明，基于物理启发的工程化*高级*特征的传统浅层网络被基于接受较少预处理的更高维度*低级*特征的深层网络超越。大型强子对撞机数据分析的许多领域长期以来一直受到次优特征工程的困扰，值得重新审视。因此，高能物理领域是机器学习应用的肥沃土壤。在这方面正在取得很多进展。该领域正在应用多种机器学习技术，包括人工神经网络、核密度估计、支持向量机、遗传算法、增强决策树、随机森林和生成网络。
- en: 'The experimental program of the Large Hadron Collider probes the most fundamental
    questions in modern physics: The nature of mass, the dimensionality of space,
    the unification of the fundamental forces, the particle nature of dark matter,
    and the fine-tuning of the [Standard Model](https://en.wikipedia.org/wiki/Standard_Model).
    One driving goal is to understand the most fundamental structure of matter. Part
    of that entails searching for and studying exotic particles, such as the top quark
    and Higgs boson, produced in collisions at accelerators such as the Large Hadron
    Collider. Specific benchmarks and challenges include: Mass resconstruction, jet
    substructure, and jet-flavor classification. For example, one can identify jets
    from heavy *(c, b, t)* or light *(u, d, s)* quarks, gluons, and *W*, *Z*, and
    *H* bosons.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 大型强子对撞机的实验计划探索现代物理学中最基本的问题：质量的本质、空间的维度、基本力的统一、暗物质的粒子性质以及[标准模型](https://en.wikipedia.org/wiki/Standard_Model)的微调。一个驱动目标是理解物质的最基本结构。其中一部分包括搜索和研究异域粒子，如在大型强子对撞机等加速器中产生的顶夸克和希格斯玻色子。具体的基准和挑战包括：质量重建、喷注亚结构和喷注味道分类。例如，可以识别来自重*(c、b、t)*或轻*(u、d、s)*夸克、胶子以及*W*、*Z*和*H*玻色子的喷注。
- en: Running high energy particle experiments and collecting the resulting data is
    extremely expensive. The data collected is enormous in terms of the number of
    collisions and in the complexity of each collision. In addition, the bulk of accelerator
    events do not produce interesting particles (signal particles *vs* background
    particles). Signal particles are rare, so high data rates are necessary. For example,
    the Large Hadron Collidor detectors have O(108) sensors used to record the large
    number of particles produced after each collision. It is thus of paramount importance
    to extract maximal information from experimental data (think regression and classification
    models), to accurately select and identify events for effective measurements,
    and to produce reliable methods for simulating new data similar to data produced
    by experiments (think generative models). High energy physics data is characterized
    by its high dimensionality, along with the complex topologies of many signal events.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 进行高能粒子实验并收集所得数据是非常昂贵的。所收集的数据在碰撞数量和每次碰撞的复杂性方面都是巨大的。此外，大部分加速器事件并不产生有趣的粒子（信号粒子与背景粒子）。信号粒子很少，因此需要高数据速率。例如，大型强子对撞机探测器有O(108)个传感器用于记录每次碰撞后产生的大量粒子。因此，从实验数据中提取最大信息（考虑回归和分类模型），准确选择和识别事件以进行有效测量，并为模拟新数据（考虑生成模型）产生可靠方法至关重要。高能物理数据的特点是其高维度，以及许多信号事件的复杂拓扑结构。
- en: 'The way this discussion ties into our chapter is through the nature of collisions
    and the interaction of their products with Large Hadron Collider detectors: They
    are quantum-mechanical, therefore, the observations resulting from a particular
    interaction are fundamentally probabilistic. The resulting data analysis must
    then be framed in statistical and probabilistic terms.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种讨论与我们的章节的联系在于碰撞的性质以及其产物与大型强子对撞机探测器的相互作用：它们是量子力学的，因此，由特定相互作用产生的观察结果基本上是概率性的。因此，随后的数据分析必须以统计和概率术语来构建。
- en: In our chapter, our aim is to learn the probability disribution <math alttext="p
    left-parenthesis ModifyingAbove theta With right-arrow vertical-bar ModifyingAbove
    x With right-arrow right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow></math> of the model’s parameters given the observed data. If
    the data was fairly low dimensional, such as less than five dimensions, the problem
    of estimating the unknown statistical model from the simulated samples would not
    be difficult, using histograms or kernel-based density estimates. However, we
    cannot easily extend these simple methods to higher dimensions, due to the curse
    of dimensionality. In a single dimension, we would need *N* samples to estimate
    the source probability density function, but in *d* dimensions, we would need
    <math alttext="upper O left-parenthesis upper N Superscript d Baseline right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <msup><mi>N</mi> <mi>d</mi></msup> <mo>)</mo></mrow></math> . The consequence
    is that if the dimension of the data is greater than ten or so, it is impractical
    or even impossible to use naive methods to estimate the probability distribution,
    requiring a prohibitive amount of computational resources.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的章节中，我们的目标是学习模型参数的概率分布<p>θ|x)给定观察到的数据。如果数据是相当低维的，例如小于五维，那么从模拟样本中估计未知的统计模型的问题并不困难，可以使用直方图或基于核的密度估计。然而，我们无法轻易将这些简单方法扩展到更高维度，这是由于维度诅咒。在单个维度中，我们需要N个样本来估计源概率密度函数，但在d个维度中，我们需要O(N^d)。其结果是，如果数据的维度大于十左右，使用朴素方法估计概率分布是不切实际甚至不可能的，需要大量的计算资源。
- en: 'High energy physicists have traditionally dealt with the curse of dimensionality
    by reducing the dimension of the data through a series of steps that operate both
    on individual collision events and on collections of events. These established
    approaches were based on specific, hand engineered features in the data to a number
    small enough to allow the estimation of the unknown probability distribution <math
    alttext="p left-parenthesis x vertical-bar theta right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></math> using samples
    generated by simulation tools. Obviously, due to complexity of the data and the
    rarity of potential new physics along with its subtle signatures, this traditional
    approach is probably sub-optimal. Machine learning eliminates the need for hand
    engineering features and manual dimensionality reduction, that can miss crucial
    information in the lower-level higher dimensional data. Moreover, the structure
    of lower-level data obtained directly from the sensors fits very well with well
    established neural network models, such as convolutional neural networks and graph
    neural networks: For example, the projective tower structure of calorimeters present
    in nearly all modern high energy physics detectors is similar to the pixels of
    an image.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 高能物理学家传统上通过一系列步骤来降低数据的维度，既对单个碰撞事件进行操作，也对事件集合进行操作，以应对维度诅咒。这些已建立的方法基于数据中的特定手工设计特征，将数据的维度降低到足够小的数量，以允许使用模拟工具生成的样本来估计未知概率分布<math
    alttext="p left-parenthesis x vertical-bar theta right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></math>。显然，由于数据的复杂性和潜在新物理的罕见性以及其微妙的特征，这种传统方法可能不是最佳选择。机器学习消除了手工设计特征和手动降维的需求，这可能会错过较低级别高维数据中的关键信息。此外，直接从传感器获得的较低级别数据的结构非常适合于已经建立的神经网络模型，例如卷积神经网络和图神经网络：例如，几乎所有现代高能物理探测器中存在的量能器的投影塔结构类似于图像的像素。
- en: Note however that while the image-based approach has been successful, the actual
    detector geometry is not perfectly regular, thus, some data preprocessing is required
    to represent jets images. In addition, jet images are typically very sparse. Both
    irregular geometry and sparsity can be addressed using *graph based convolutional
    networks* instead of the usual convolutional networks for our particle data modeling.
    Graph convolutional networks extend the application of convolutional neural networks
    to irregularly sampled data. They are able to handle sparse, permutation invariant
    data with complex geometries. We will discuss graph networks in the next chapter.
    They always come with nodes, edges, and a matrix encoding the relationships in
    the graph, called *adjacency matrix*. In the context of high energy physics, the
    particles of a jet represent the nodes of the graph and the edges encode how close
    the particles are in a learned adjacency matrix. In high energy physics, graph
    based networks have been successfully applied to classification, reconstruction
    and generation tasks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而需要注意的是，虽然基于图像的方法取得了成功，但实际探测器的几何形状并不完全规则，因此需要一些数据预处理来表示喷流图像。此外，喷流图像通常非常稀疏。不规则的几何形状和稀疏性可以使用*基于图的卷积网络*来解决，而不是通常用于粒子数据建模的卷积网络。图卷积网络将卷积神经网络的应用扩展到不规则采样的数据。它们能够处理稀疏、置换不变的数据和复杂几何形状。我们将在下一章讨论图网络。它们始终带有节点、边和编码图中关系的矩阵，称为*邻接矩阵*。在高能物理的背景下，喷流的粒子代表图的节点，边编码了粒子在学习的邻接矩阵中的接近程度。在高能物理中，基于图的网络已成功应用于分类、重建和生成任务。
- en: The subject of our chapter is generative models, or generating data similar
    to a given data set. Generating or simulating data faithful to the experimental
    data collected in high energy physics is of great importance. In [Graph Generative
    Adversarial Networks for Sparse Data Generation in High Energy Physics](https://arxiv.org/pdf/2012.00173.pdf),
    the authors develop graph-based generative models, using a generative adversarial
    network framework, for simulating sparse datasets like those produced at the [CERN](https://home.cern)
    Large Hadron Collider.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们章节的主题是生成模型，或者生成类似给定数据集的数据。在高能物理中生成或模拟与实验数据相符的数据是非常重要的。在[用于高能物理中稀疏数据生成的图生成对抗网络](https://arxiv.org/pdf/2012.00173.pdf)中，作者们开发了基于图的生成模型，使用生成对抗网络框架，用于模拟类似于[CERN](https://home.cern)大型强子对撞机产生的稀疏数据集。
- en: 'The authors illustrate their approach by training on and generating sparse
    representations of MNIST handwritten digit images and jets of particles in proton-proton
    collisions like those at the Large Hadron Collider. The model successfully generates
    sparse MNIST digits and particle jet data. The authors use two metrics to quantify
    agreement between real and generated data: A graph-based [Fréchet Inception distance](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance),
    and the particle and jet feature-level [1-Wasserstein distance](https://en.wikipedia.org/wiki/Wasserstein_metric).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们通过对MNIST手写数字图像和质子-质子碰撞中产生的粒子喷流等稀疏表示进行训练和生成来说明他们的方法，这些类似于大型强子对撞机中的情况。该模型成功生成了稀疏的MNIST数字和粒子喷流数据。作者们使用两个指标来量化真实数据和生成数据之间的一致性：基于图的[Fréchet
    Inception距离](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance)，以及粒子和喷流特征级别的[1-Wasserstein距离](https://en.wikipedia.org/wiki/Wasserstein_metric)。
- en: Other Generative Models
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他生成模型
- en: 'We have discussed state of the art generative models (as of 2022), but this
    chapter will be incomplete if we do not go over Naive Bayes, Gaussian mixture,
    and Boltzmann machine models. There are many others. That being said, Yann LeCun
    (VP and Chief AI Scientist at Meta) offers [his perspective](https://www.linkedin.com/posts/yann-lecun_yann-lecun-activity-6931321709572603904-Mj3v?utm_source=linkedin_share&utm_medium=member_desktop_web)
    on some of these models:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了最先进的生成模型（截至2022年），但如果我们不介绍朴素贝叶斯、高斯混合和玻尔兹曼机模型，这一章节将是不完整的。还有许多其他模型。话虽如此，Meta的副总裁兼首席人工智能科学家Yann
    LeCun在一些模型上提供了他的观点：
- en: '*Researchers in speech recognition, computer vision, and natural language processing
    in the 2000s were obsessed with accurate representations of uncertainty. This
    led to a flurry of work on probabilstic generative models such as Hidden Markov
    Models in speech, Markov random fields and constellation models in vision, and
    probabilistic topic models in NLP, e.g. with latent Dirichlet analysis. There
    were debates at computer vision workshops about generative models vs discriminative
    models. There were heroic-yet-futile attempts to build object recognition systems
    with non-parametric Bayesian methods. Much of this was riding on previous work
    on Bayesian networks, factor graphs and other graphical models. That’s how one
    learned about exponential family, belief propagation, loopy belief propagation,
    variational inference, etc, Chinese restaurant process, Indian buffet process,
    etc. But almost none of this work was concenred with the problem of learning representations.
    Features were assumed to be given. The structure of the graphical model, with
    its latent variables, was assumed to be given. All one had to do was to compute
    some sort of log likelihood by linearly combining features, and then use one of
    the above mentioned sophisticated inference methods to produce marginal distributions
    over the unknown variables, one of which being the answer, e.g. a category. In
    fact, exponential family pretty much means shallow: the log-likelihood can be
    expressed as a linearly parameterized function of features (or simple combinations
    thereof). Learning the parameters of the model was seen as just another variational
    inference problem. It’s interesting to observe that almost none of this is relevant
    to today’s top speech, vision, and NLP systems. As it turned out, solving the
    problem of learning hierarchical representations and complex functional dependencies
    was a much more important issue than being able to perform accurate probabilistic
    inference with shallow models. This is not to say that accurate probabilistic
    inference is not useful.*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代，语音识别、计算机视觉和自然语言处理领域的研究人员着迷于准确表示不确定性。这导致了大量关于概率生成模型的工作，如语音中的隐马尔可夫模型，视觉中的马尔可夫随机场和星座模型，以及自然语言处理中的概率主题模型，例如潜在狄利克雷分析。在计算机视觉研讨会上有关生成模型与判别模型的辩论。曾经有英勇而徒劳的尝试使用非参数贝叶斯方法构建物体识别系统。这在之前的贝叶斯网络、因子图和其他图形模型的基础上进行。这就是人们了解指数族、信念传播、循环信念传播、变分推断等的方式，中国餐馆过程、印度自助餐过程等。但几乎没有这项工作关注学习表示的问题。特征被认为是已知的。图形模型的结构，以及其中的潜在变量，被认为是已知的。人们所要做的就是通过线性组合特征计算某种对数似然，然后使用上述提到的一种复杂推断方法之一来生成未知变量的边际分布，其中之一就是答案，例如一个类别。事实上，指数族几乎意味着浅层：对数似然可以表示为特征（或其简单组合）的线性参数化函数。学习模型的参数被视为另一个变分推断问题。有趣的是，几乎没有这些内容与今天顶尖的语音、视觉和自然语言处理系统相关。事实证明，解决学习分层表示和复杂功能依赖的问题比能够使用浅层模型进行准确的概率推断更为重要。这并不是说准确的概率推断没有用处。
- en: 'In the same vein, he continues: *Generative Adversial Networks are nice for
    producing pretty pictures (though they are being edged out by diffusion models,
    or “multistep denoising auto-encoders” as I like to call them), but for recognition
    and representation learning, GANs have been a big disappointment.*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在同样的思路下，他继续说：生成对抗网络对于生成漂亮的图片很好（尽管它们正被扩散模型或“多步去噪自动编码器”所取代），但对于识别和表示学习，GANs是一个巨大的失望。
- en: Nevertheless, there is a lot of math to be learned from all these models. In
    my experience, we understand and retain math at a much deeper level when we see
    it developed and utilized for specific purposes, as opposed only to train the
    neurons of the brain. Many mathematicians claim to experience pleasure while proving
    theories that have yet to find applications. I was never one of those.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，从所有这些模型中可以学到很多数学知识。根据我的经验，当我们看到数学被开发和用于特定目的时，我们对数学的理解和保留会更加深入，而不仅仅是训练大脑的神经元。许多数学家声称在证明尚未找到应用的理论时会感到愉悦。我从来不是其中之一。
- en: Naive Bayes Classification Model
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类模型
- en: Naive Bayes model is a very simple classification model that we can also use
    as a generative model, since it ends up computing a joint probability distribution
    <math alttext="p left-parenthesis ModifyingAbove x With right-arrow comma y Subscript
    k Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow></math>
    for the data in order to determine its classification. The training data has features
    <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> and labels <math alttext="y Subscript k"><msub><mi>y</mi>
    <mi>k</mi></msub></math> . Therefore, we can use Naive Bayes model to generate
    new data points together with labels, by sampling from this joint probability
    distribution.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型是一个非常简单的分类模型，我们也可以将其用作生成模型，因为最终会计算数据的联合概率分布<math alttext="p left-parenthesis
    ModifyingAbove x With right-arrow comma y Subscript k Baseline right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></math>，以确定其分类。训练数据具有特征<math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>和标签<math
    alttext="y Subscript k"><msub><mi>y</mi> <mi>k</msub></math>。因此，我们可以使用朴素贝叶斯模型从这个联合概率分布中抽样生成新的数据点和标签。
- en: 'The goal of a Naive Bayes model is to compute the probability of the class
    <math alttext="y Subscript k"><msub><mi>y</mi> <mi>k</mi></msub></math> given
    the data features <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> , which is the conditional probability <math alttext="p
    left-parenthesis y Subscript k Baseline vertical-bar ModifyingAbove x With right-arrow
    right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    . For data with many features (high dimensional <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    ), this is expensive to compute, so we use Bayes rule and exploit the reverse
    conditional probability, which in turn leads to the joint probability distribution.
    That is:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型的目标是计算给定数据特征<math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math>的类别<math alttext="y Subscript
    k"><msub><mi>y</mi> <mi>k</msub></math>的概率，即条件概率<math alttext="p left-parenthesis
    y Subscript k Baseline vertical-bar ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>|</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math>。对于具有许多特征（高维<math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>）的数据，这种计算是昂贵的，因此我们使用贝叶斯规则并利用逆条件概率，从而导致联合概率分布。也就是说：
- en: <math alttext="dollar-sign p left-parenthesis y Subscript k Baseline vertical-bar
    ModifyingAbove x With right-arrow right-parenthesis equals StartFraction p left-parenthesis
    y Subscript k Baseline right-parenthesis p left-parenthesis ModifyingAbove x With
    right-arrow vertical-bar y Subscript k Baseline right-parenthesis Over p left-parenthesis
    ModifyingAbove x With right-arrow right-parenthesis EndFraction equals StartFraction
    p left-parenthesis ModifyingAbove x With right-arrow comma y Subscript k Baseline
    right-parenthesis Over p left-parenthesis ModifyingAbove x With right-arrow right-parenthesis
    EndFraction dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow><mi>p</mi><mrow><mo>(</mo><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>|</mo><msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></mrow> <mrow><mi>p</mi><mo>(</mo><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>,</mo><msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p left-parenthesis y Subscript k Baseline vertical-bar
    ModifyingAbove x With right-arrow right-parenthesis equals StartFraction p left-parenthesis
    y Subscript k Baseline right-parenthesis p left-parenthesis ModifyingAbove x With
    right-arrow vertical-bar y Subscript k Baseline right-parenthesis Over p left-parenthesis
    ModifyingAbove x With right-arrow right-parenthesis EndFraction equals StartFraction
    p left-parenthesis ModifyingAbove x With right-arrow comma y Subscript k Baseline
    right-parenthesis Over p left-parenthesis ModifyingAbove x With right-arrow right-parenthesis
    EndFraction dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mi>p</mi><mrow><mo>(</mo><msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow><mi>p</mi><mrow><mo>(</mo><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>|</mo><msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></mrow> <mrow><mi>p</mi><mo>(</mo><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover><mo>)</mo></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>,</mo><msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow> <mrow><mi>p</mi><mo>(</mo><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mfrac></mrow></math>
- en: 'Naive Bayes model makes the very strong and naive assumption, which in practice
    works better than one might expect, that the data features are mutually independent,
    when conditioned on the class label <math alttext="y Subscript k"><msub><mi>y</mi>
    <mi>k</mi></msub></math> . This assumption helps simplify the joint probability
    distribution in the numerator tremendously, especially when we expand it as a
    product of single variable conditional probabilities. The feature independence
    assumptions conditional on the class label <math alttext="y Subscript k"><msub><mi>y</mi>
    <mi>k</mi></msub></math> means:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型做出了非常强烈和天真的假设，实际上比人们预期的效果更好，即在给定类别标签<math alttext="y Subscript k"><msub><mi>y</mi>
    <mi>k</mi></msub></math>时，数据特征是相互独立的。这种假设有助于极大地简化分子中的联合概率分布，特别是当我们将其扩展为单变量条件概率的乘积时。在给定类别标签<math
    alttext="y Subscript k"><msub><mi>y</mi> <mi>k</mi></msub></math>的特征独立假设意味着：
- en: <math alttext="dollar-sign p left-parenthesis x Subscript i Baseline vertical-bar
    x Subscript i plus 1 Baseline comma x Subscript i plus 2 Baseline comma ellipsis
    comma x Subscript n Baseline comma y Subscript k Baseline right-parenthesis equals
    p left-parenthesis x Subscript i Baseline vertical-bar y Subscript k Baseline
    right-parenthesis dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mo>+</mo><mn>2</mn></mrow></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p left-parenthesis x Subscript i Baseline vertical-bar
    x Subscript i plus 1 Baseline comma x Subscript i plus 2 Baseline comma ellipsis
    comma x Subscript n Baseline comma y Subscript k Baseline right-parenthesis equals
    p left-parenthesis x Subscript i Baseline vertical-bar y Subscript k Baseline
    right-parenthesis dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>|</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>i</mi><mo>+</mo><mn>2</mn></mrow></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: 'Thus, the joint probability distribution factors into:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，联合概率分布分解为：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column p left-parenthesis
    ModifyingAbove x With right-arrow comma y Subscript k Baseline right-parenthesis
    2nd Column equals p left-parenthesis x 1 vertical-bar x 2 comma ellipsis comma
    x Subscript n Baseline comma y Subscript k Baseline right-parenthesis p left-parenthesis
    x 2 vertical-bar x 3 comma ellipsis comma x Subscript n Baseline comma y Subscript
    k Baseline right-parenthesis ellipsis p left-parenthesis x Subscript n Baseline
    vertical-bar y Subscript k Baseline right-parenthesis p left-parenthesis y Subscript
    k Baseline right-parenthesis 2nd Row 1st Column Blank 2nd Column equals p left-parenthesis
    x 1 vertical-bar y Subscript k Baseline right-parenthesis p left-parenthesis x
    2 vertical-bar y Subscript k Baseline right-parenthesis ellipsis p left-parenthesis
    x Subscript n Baseline vertical-bar y Subscript k Baseline right-parenthesis p
    left-parenthesis y Subscript k Baseline right-parenthesis EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>p</mi> <mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mo>⋯</mo> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow>
    <mo>⋯</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column p left-parenthesis
    ModifyingAbove x With right-arrow comma y Subscript k Baseline right-parenthesis
    2nd Column equals p left-parenthesis x 1 vertical-bar x 2 comma ellipsis comma
    x Subscript n Baseline comma y Subscript k Baseline right-parenthesis p left-parenthesis
    x 2 vertical-bar x 3 comma ellipsis comma x Subscript n Baseline comma y Subscript
    k Baseline right-parenthesis ellipsis p left-parenthesis x Subscript n Baseline
    vertical-bar y Subscript k Baseline right-parenthesis p left-parenthesis y Subscript
    k Baseline right-parenthesis 2nd Row 1st Column Blank 2nd Column equals p left-parenthesis
    x 1 vertical-bar y Subscript k Baseline right-parenthesis p left-parenthesis x
    2 vertical-bar y Subscript k Baseline right-parenthesis ellipsis p left-parenthesis
    x Subscript n Baseline vertical-bar y Subscript k Baseline right-parenthesis p
    left-parenthesis y Subscript k Baseline right-parenthesis EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>p</mi> <mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mo>⋯</mo> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow>
    <mo>⋯</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: We can now estimate these single feature probabilities conditioned on each category
    of the data easily from the training data. We can similarly estimate the probability
    of each class <math alttext="p left-parenthesis y Subscript k Baseline right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow></math> from the
    training data, or we can assume the classes are equally likely, so that <math
    alttext="p left-parenthesis y Subscript k Baseline right-parenthesis equals StartFraction
    1 Over number of classes EndFraction"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mtext>number</mtext><mtext>of</mtext><mtext>classes</mtext></mrow></mfrac></mrow></math>
    .
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以从训练数据中轻松估计这些单个特征概率，条件是每个数据类别。我们也可以从训练数据中估计每个类别<math alttext="p left-parenthesis
    y Subscript k Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow></math>的概率，或者我们可以假设类别是等可能的，因此<math alttext="p
    left-parenthesis y Subscript k Baseline right-parenthesis equals StartFraction
    1 Over number of classes EndFraction"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mtext>类别数</mtext></mrow></mfrac></mrow></math>。
- en: Note that in general, generative models find the joint probability distibution
    <math alttext="p left-parenthesis ModifyingAbove x With right-arrow comma ModifyingAbove
    y Subscript k Baseline With right-arrow right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>y</mi>
    <mi>k</mi></msub> <mo>→</mo></mover> <mo>)</mo></mrow></math> , between labels
    <math alttext="y Subscript k"><msub><mi>y</mi> <mi>k</mi></msub></math> and data
    <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> . Classification models, on the other hand, calculate
    the conditional probabilities <math alttext="p left-parenthesis y Subscript k
    Baseline vertical-bar ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>|</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> . They focus on calculating the decision
    boundaries between different classes in the data by returning the class <math
    alttext="y Subscript k"><msub><mi>y</mi> <mi>k</mi></msub></math> with the highest
    probability. So for Naive Bayes classifier, it returns the label <math alttext="y
    Subscript asterisk"><msub><mi>y</mi> <mo>*</mo></msub></math> with the highest
    value for <math alttext="p left-parenthesis y Subscript k Baseline vertical-bar
    ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>|</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> , which is the same as the highest
    value for <math alttext="p left-parenthesis x 1 vertical-bar y Subscript k Baseline
    right-parenthesis p left-parenthesis x 2 vertical-bar y Subscript k Baseline right-parenthesis
    ellipsis p left-parenthesis x Subscript n Baseline vertical-bar y Subscript k
    Baseline right-parenthesis p left-parenthesis y Subscript k Baseline right-parenthesis"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow>
    <mo>⋯</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></math> .
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一般来说，生成模型找到标签<math alttext="y Subscript k"><msub><mi>y</mi> <mi>k</mi></msub></math>和数据<math
    alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>之间的联合概率分布<math
    alttext="p left-parenthesis ModifyingAbove x With right-arrow comma ModifyingAbove
    y Subscript k Baseline With right-arrow right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><msub><mi>y</mi>
    <mi>k</mi></msub> <mo>→</mo></mover> <mo>)</mo></mrow></math>。另一方面，分类模型计算条件概率<math
    alttext="p left-parenthesis y Subscript k Baseline vertical-bar ModifyingAbove
    x With right-arrow right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow></math>。它们专注于通过返回具有最高概率的类<math alttext="y Subscript k"><msub><mi>y</mi>
    <mi>k</mi></msub></math>来计算数据中不同类之间的决策边界。因此，对于朴素贝叶斯分类器，它返回具有<math alttext="p left-parenthesis
    y Subscript asterisk"><msub><mi>y</mi> <mo>*</mo></msub></math>的最高值的标签<math alttext="y
    Subscript asterisk"><msub><mi>y</mi> <mo>*</mo></msub></math>，该值与<math alttext="p
    left-parenthesis y Subscript k Baseline vertical-bar ModifyingAbove x With right-arrow
    right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <msub><mi>y</mi> <mi>k</mi></msub>
    <mo>|</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>的最高值相同，这与<math
    alttext="p left-parenthesis x 1 vertical-bar y Subscript k Baseline right-parenthesis
    p left-parenthesis x 2 vertical-bar y Subscript k Baseline right-parenthesis ellipsis
    p left-parenthesis x Subscript n Baseline vertical-bar y Subscript k Baseline
    right-parenthesis p left-parenthesis y Subscript k Baseline right-parenthesis"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>|</mo> <msub><mi>y</mi>
    <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>|</mo> <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow>
    <mo>⋯</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>k</mi></msub> <mo>)</mo></mrow></mrow></math>的最高值相同。
- en: Gaussian Mixture Model
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: 'In a Gaussian mixture model, we assume that all the data points are generated
    from a mixture of a finite number of Gaussian distributions with unknown parameters
    (means and covariance matrices). We can think of mixture models as being similar
    to k-means clustering, but here we include information about the centers of the
    clusters (means of our Gaussians) along with the shape of the spread of the data
    in each cluster (determined by the covariance of the Gaussians). In order to determine
    the number of clusters in the data, Gaussian mixture models sometimes implement
    [the Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion).
    We can also retrict our model in order to control the covariance of the different
    Gaussians in the mixture: Full, Tied, Diagonal, Tied Diagonal, and Spherical (see
    [Figure 8-3](#Fig_Gaussian_covariance) for illustration).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯混合模型中，我们假设所有数据点都是从具有未知参数（均值和协方差矩阵）的有限数量的高斯分布混合中生成的。我们可以将混合模型视为类似于k均值聚类，但在这里我们包括有关聚类中心（高斯的均值）以及每个聚类中数据传播形状（由高斯的协方差确定）的信息。为了确定数据中的聚类数量，高斯混合模型有时会实现[贝叶斯信息准则](https://en.wikipedia.org/wiki/Bayesian_information_criterion)。我们还可以限制我们的模型以控制混合物中不同高斯的协方差：完全、绑定、对角线、绑定对角线和球形（参见[图8-3](#Fig_Gaussian_covariance)以进行说明）。
- en: '![250](assets/emai_0803.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0803.png)'
- en: Figure 8-3\. Gaussian mixture covariance types ([imagesource](https://i.stack.imgur.com/0zLpe.png)).
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3. 高斯混合协方差类型（[图片来源](https://i.stack.imgur.com/0zLpe.png)）。
- en: We finally need to maximize the likelihood of the data in order to estimate
    unknown parameters of the mixture (means and entries of the covariance matrices).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们需要最大化数据的似然以估计混合物的未知参数（均值和协方差矩阵的条目）。
- en: 'Maximum likelihood becomes intractable when there are latent or hidden variables
    in the data (variables that are not directly measured or observed). The way around
    this is to use an [expectation maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)
    in order to estimate the maximum likelihood. Expectation Maximization algorithm
    works as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据中存在潜在或隐藏变量（即未直接测量或观察的变量）时，最大似然变得难以处理。解决这个问题的方法是使用期望最大化（EM）算法来估计最大似然。期望最大化算法的工作原理如下：
- en: Estimate the values for the latent variables by creating a function for the
    expectation of the log-likelihood using the current estimate for the unknown parameters.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过创建一个关于当前未知参数估计的期望对数似然的函数，估计潜在变量的值。
- en: 'Optimize: Compute new parameters that maximize the expected log-likelihood
    evaluated in the previous step.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化：计算新参数，使得在前一步中评估的期望对数似然最大化。
- en: Repeat the above two steps until convergence.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上述两个步骤直至收敛。
- en: 'We can see how Gaussian Mixture models can be used as clustering, generative,
    or classification models. For clustering: This is the main part of the model build
    up. For generation: Sample new data points from the mixture after computing the
    unknown parameters via Expectation Maximization. For classification: Given a new
    data point, the model assigns it to the Gaussian it mostly probably belongs.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到高斯混合模型可以用作聚类、生成或分类模型。对于聚类：这是模型构建的主要部分。对于生成：通过期望最大化计算未知参数后，从混合中抽样新数据点。对于分类：给定一个新数据点，模型将其分配给它最有可能属于的高斯分布。
- en: The Evolution Of Generative Models
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型的演变
- en: 'In this section, we tell the story that contributed to ending the winter of
    neural networks, and ultimately led to modern probabilitistic deep learning models,
    such as variational autoencoders, fully visibile deep belief networks, and generative
    adversarial networks. We encounter the progression from Hopfield nets to Boltzmann
    Machines to Restricted Boltzmann Machines. I have a special affinity for these
    models: In addition to their historical value, learning the joint probability
    distribution of the features of the data by assembling a network of basic computional
    units, they employ the mathematical machinery of the extremely neat and well developed
    field of *statistical mechanics*, my initial area of research.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讲述了导致神经网络冬天结束的故事，最终导致了现代概率深度学习模型的出现，如变分自动编码器、完全可见深度信念网络和生成对抗网络。我们经历了从Hopfield网络到Boltzmann机器再到受限Boltzmann机器的发展过程。我对这些模型有特殊的偏爱：除了它们的历史价值外，通过组装基本计算单元网络来学习数据特征的联合概率分布，它们还利用了极其整洁和发展完善的*统计力学*领域的数学机制，这是我最初的研究领域。
- en: 'In statistical mechanics, we define probability distributions in terms of energy
    functions. The probability of us finding a system in a certain state <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    depends on its energy <math alttext="upper E left-parenthesis ModifyingAbove x
    With right-arrow right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> at that state. More precisely, high
    energy states are less probable, which manifests itself in the negative sign in
    the exponential in the following formula:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计力学中，我们通过能量函数来定义概率分布。我们找到系统处于某种状态<math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math>的概率取决于该状态的能量<math alttext="upper
    E left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>。更准确地说，高能态的概率较低，这在以下公式中的指数的负号中体现：
- en: <math alttext="dollar-sign p left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis equals StartFraction e x p left-parenthesis minus upper E left-parenthesis
    ModifyingAbove x With right-arrow right-parenthesis right-parenthesis Over upper
    Z EndFraction period dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>-</mo><mi>E</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow><mo>)</mo></mrow>
    <mi>Z</mi></mfrac> <mo>.</mo></mrow></math>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis equals StartFraction e x p left-parenthesis minus upper E left-parenthesis
    ModifyingAbove x With right-arrow right-parenthesis right-parenthesis Over upper
    Z EndFraction period dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>-</mo><mi>E</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow><mo>)</mo></mrow>
    <mi>Z</mi></mfrac> <mo>.</mo></mrow></math>
- en: 'The exponential function guarantees that *p* is positive, and the *partition
    function Z* in the denominator esures that the sum (or integral if *x* is continuous)
    of <math alttext="p left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    over all states <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> is one, making *p* a valid probability distribution.
    Machine learning models that define joint probability distributions this way are
    called *energy based models*, for the obvious reasons. They differ in how they
    assign the energy at each state, meaning in the specific formula they use for
    <math alttext="upper E left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    , which in turn affects the formula for the partition function *Z*. The formula
    for <math alttext="upper E left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> is the one that contains the paramters
    of the model <math alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math> , which we need to compute from the data using maximumum
    likelihood estimation. In fact, it is better if we have the dependence of *p*,
    *E*, and *Z* on <math alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math> explicit in the joint probability ditribution formula:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 指数函数确保*p*为正，分区函数*Z*在分母中确保对所有状态<math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math>的和（如果*x*是连续的，则是积分）为一，使*p*成为有效的概率分布。以这种方式定义联合概率分布的机器学习模型被称为*基于能量的模型*，原因显而易见。它们在如何分配每个状态的能量上有所不同，这意味着它们在<math
    alttext="upper E left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><mi>E</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    的具体公式中使用的公式不同，这反过来影响了分区函数*Z*的公式。<math alttext="upper E left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis"><mrow><mi>E</mi> <mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> 的公式包含模型参数<math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>，我们需要使用最大似然估计从数据中计算出这些参数。事实上，如果我们在联合概率分布公式中明确地考虑*p*、*E*和*Z*对<math
    alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math>的依赖性会更好：
- en: <math alttext="dollar-sign p left-parenthesis ModifyingAbove x With right-arrow
    comma ModifyingAbove theta With right-arrow right-parenthesis equals StartFraction
    e x p left-parenthesis minus upper E left-parenthesis ModifyingAbove x With right-arrow
    comma ModifyingAbove theta With right-arrow right-parenthesis right-parenthesis
    Over upper Z left-parenthesis ModifyingAbove theta With right-arrow right-parenthesis
    EndFraction period dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>-</mo><mi>E</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>,</mo><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover><mo>)</mo></mrow><mo>)</mo></mrow> <mrow><mi>Z</mi><mo>(</mo><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover><mo>)</mo></mrow></mfrac> <mo>.</mo></mrow></math>
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p left-parenthesis ModifyingAbove x With right-arrow
    comma ModifyingAbove theta With right-arrow right-parenthesis equals StartFraction
    e x p left-parenthesis minus upper E left-parenthesis ModifyingAbove x With right-arrow
    comma ModifyingAbove theta With right-arrow right-parenthesis right-parenthesis
    Over upper Z left-parenthesis ModifyingAbove theta With right-arrow right-parenthesis
    EndFraction period dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo>(</mo><mo>-</mo><mi>E</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>,</mo><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover><mo>)</mo></mrow><mo>)</mo></mrow> <mrow><mi>Z</mi><mo>(</mo><mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover><mo>)</mo></mrow></mfrac> <mo>.</mo></mrow></math>
- en: 'In most cases, it is not possible to compute a closed formula for the partition
    function *Z*, rendering the maximum likelihood estimation intractable. More precisely,
    when we maximize the loglikelihood, we need to compute its gradient, which includes
    computing its gradient with repect to the parameters <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    , which in turn forces us to compute the gradient of the partition function *Z*
    with repect to <math alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math> . The following quantity appears frequently in these
    computations:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，计算分区函数*Z*的闭合公式是不可能的，这使得最大似然估计变得棘手。更确切地说，当我们最大化对数似然时，我们需要计算其梯度，其中包括计算其对参数<math
    alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math>的梯度，这反过来又迫使我们计算分区函数*Z*对<math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>的梯度。在这些计算中，以下数量经常出现：
- en: <math alttext="dollar-sign normal nabla Subscript ModifyingAbove theta With
    right-arrow Baseline log upper Z left-parenthesis ModifyingAbove theta With right-arrow
    right-parenthesis equals double-struck upper E Subscript ModifyingAbove x With
    right-arrow tilde p left-parenthesis ModifyingAbove x With right-arrow right-parenthesis
    Baseline left-parenthesis normal nabla Subscript ModifyingAbove theta With right-arrow
    Baseline log left-parenthesis n u m e r a t o r left-parenthesis ModifyingAbove
    x With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis
    right-parenthesis dollar-sign"><mrow><msub><mi>∇</mi> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></msub> <mo form="prefix">log</mo> <mi>Z</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>∼</mo><mi>p</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub> <mrow><mo>(</mo></mrow>
    <msub><mi>∇</mi> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <mi>n</mi> <mi>u</mi> <mi>m</mi> <mi>e</mi>
    <mi>r</mi> <mi>a</mi> <mi>t</mi> <mi>o</mi> <mi>r</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign normal nabla Subscript ModifyingAbove theta With
    right-arrow Baseline log upper Z left-parenthesis ModifyingAbove theta With right-arrow
    right-parenthesis equals double-struck upper E Subscript ModifyingAbove x With
    right-arrow tilde p left-parenthesis ModifyingAbove x With right-arrow right-parenthesis
    Baseline left-parenthesis normal nabla Subscript ModifyingAbove theta With right-arrow
    Baseline log left-parenthesis n u m e r a t o r left-parenthesis ModifyingAbove
    x With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis
    right-parenthesis dollar-sign"><mrow><msub><mi>∇</mi> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></msub> <mo form="prefix">log</mo> <mi>Z</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mover accent="true"><mi>x</mi> <mo>→</mo></mover><mo>∼</mo><mi>p</mi><mrow><mo>(</mo><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover><mo>)</mo></mrow></mrow></msub> <mrow><mo>(</mo></mrow>
    <msub><mi>∇</mi> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover></msub> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <mi>n</mi> <mi>u</mi> <mi>m</mi> <mi>e</mi>
    <mi>r</mi> <mi>a</mi> <mi>t</mi> <mi>o</mi> <mi>r</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: where in our case the numerator in the formula of the energy based joint probability
    distribution is <math alttext="e x p left-parenthesis minus upper E left-parenthesis
    ModifyingAbove x With right-arrow comma ModifyingAbove theta With right-arrow
    right-parenthesis right-parenthesis"><mrow><mi>e</mi> <mi>x</mi> <mi>p</mi> <mo>(</mo>
    <mo>-</mo> <mi>E</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>,</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>)</mo></mrow></math> , but this can also differ among models.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，基于能量的联合概率分布公式中的分子是<math alttext="e x p left-parenthesis minus upper
    E left-parenthesis ModifyingAbove x With right-arrow comma ModifyingAbove theta
    With right-arrow right-parenthesis right-parenthesis"><mrow><mi>e</mi> <mi>x</mi>
    <mi>p</mi> <mo>(</mo> <mo>-</mo> <mi>E</mi> <mrow><mo>(</mo> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math>，但这在模型之间也可能有所不同。
- en: The cases where the partition function is intractable urges us to resort to
    approximation methods such as stochastic maximum likelihood and contrastive divergence.
    Other methods, sidestep approximating the partition function and compute conditional
    probabilities without knowledge of the partition function. They take advantage
    of the ratio definition of conditional probabilities along with the ratio in the
    definition of an energy based joint probability distribution, effectively canceling
    out the partition function. These methods include score matching, ratio matching,
    and denoising score matching.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 分区函数难以计算的情况促使我们求助于近似方法，如随机最大似然和对比散度。其他方法则避免近似计算分区函数，而是在不了解分区函数的情况下计算条件概率。它们利用条件概率的比率定义以及基于能量的联合概率分布定义中的比率，有效地消除了分区函数。这些方法包括得分匹配、比率匹配和去噪得分匹配。
- en: Other methods, such as noise contrastive estimation, annealed importance sampling,
    bridge sampling, or a combination of those relying on the strengths of each, approximate
    the partition function directly, not the log of its gradient.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法，如噪声对比估计、退火重要性采样、桥接采样，或者结合这些方法的优势，直接近似计算分区函数，而不是其梯度的对数。
- en: We will not discuss any of these methods here. Instead we refer interested readers
    to [Deep Learning Book by Ian Goodfellow (2016)](https://www.deeplearningbook.org/contents/partition.xhtml).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里讨论任何这些方法。相反，我们建议感兴趣的读者参考[Ian Goodfellow（2016）的《深度学习书》](https://www.deeplearningbook.org/contents/partition.xhtml)。
- en: 'Back to Hopfield nets and Boltzmann machines: These are the stepping stones
    to deep neural networks which are trained through back propagation that both recent
    deterministic and probabilistic deep learning models rely on. These methods form
    the original *connectionist* (of neurons) approach to learning arbitrary probability
    distributions, initially only over binary vectors of zeros and ones, and later
    over vectors with arbitrary real number values.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 回到霍普菲尔德网络和玻尔兹曼机：这些是深度神经网络的基石，通过反向传播进行训练，最近的确定性和概率深度学习模型都依赖于这种方法。这些方法构成了学习任意概率分布的原始*连接主义*（神经元）方法，最初只针对二进制零和一的向量，后来扩展到具有任意实数值的向量。
- en: Hopfield Nets
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 霍普菲尔德网络
- en: Hopfield nets take advantage of the elegant mathematics of statistical mechanics
    by identifying the states of neurons of an artificial neural network with the
    states of elements in a physical system. Even though Hopfield nets eventually
    proved to be computationally expensive and of limited practical use, they are
    the founding fathers of the modern era of neural networks, and they are worth
    exploring if only to gauge the historical evolution of AI field. Hopfield nets
    have no hidden units, and all their (visible) units are connected to each other.
    Each unit can be found in an *on* or *off* state (one or zero), and collectively
    they encode information about the whole network (or the system).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 霍普菲尔德网络利用统计力学的优雅数学，通过将人工神经网络的神经元状态与物理系统中元素的状态进行对应。尽管霍普菲尔德网络最终被证明计算昂贵且实用性有限，但它们是现代神经网络时代的奠基人，值得探索，即使只是为了衡量人工智能领域的历史演变。霍普菲尔德网络没有隐藏单元，所有（可见）单元都彼此连接。每个单元可以处于*开*或*关*状态（一或零），并且它们共同编码关于整个网络（或系统）的信息。
- en: Boltzmann Machine
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 玻尔兹曼机
- en: A Boltzmann machine is a Hopfield net, but with the addition of hidden units.
    We are already familiar with the structure of input units and hidden units in
    neural networks, so no need to explain them, but this is where they started. Similar
    to Hopfield net, both input and hidden units are binary, where the states that
    are either 0 or 1 (modern versions implement units that take real number values
    not only binary values).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机是霍普菲尔德网络，但增加了隐藏单元。我们已经熟悉神经网络中输入单元和隐藏单元的结构，所以不需要解释它们，但这是它们的起点。与霍普菲尔德网络类似，输入和隐藏单元都是二进制的，状态为0或1（现代版本实现了不仅是二进制值而且是实数值的单元）。
- en: All Boltzmann machines have an intractable partition function, so we approximate
    the maximum likelihood gradient using the techniques surveyed at the introduction
    of this section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所有玻尔兹曼机都有一个难以处理的分区函数，因此我们使用在本节介绍中调查的技术来近似最大似然梯度。
- en: Boltzmann machines rely only on the computationally intensive *Gibbs sampling*
    for their training. Gibbs is a name that appears repetitively in the statistical
    mechanics field. Gibbs sampling provides unbiased estimates of the weights of
    the network, but these estimates have high variance. In general, there is a tradeoff
    between bias and variance, and this tradeoff highlights the advantages and disadvantages
    of methods relying on each.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机仅依赖计算密集的*吉布斯抽样*进行训练。吉布斯是在统计力学领域中反复出现的名字。吉布斯抽样提供了网络权重的无偏估计，但这些估计具有很高的方差。一般来说，偏差和方差之间存在权衡，这种权衡突出了依赖于每种方法的优缺点。
- en: Restricted Boltzmann Machine (Explicit Density And Intractable)
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机（显式密度和难以处理）
- en: Boltzmann machines have a very slow learning rate due to the many inter-connections
    within visible layers and within hidden layers (think a very messy backpropagation).
    This makes their training very slow and prohibits their application to practical
    problems. Restricted Boltzmann machines, which restrict connections only to those
    between different layers, solve this problem. That is, there are no connections
    within each layer of a restricted Boltzmann machine, allowing all of the units
    in each layer can be updated simultaneously. Therefore, for two connected layers,
    we can collect co-occurrence statistics by alternately updating all of the units
    in each layer. In practice, there are larger savings because of minimal sampling
    procedures such as contrastive divergence.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机由于可见层内部和隐藏层内部之间的许多相互连接而具有非常缓慢的学习速率（想象一下非常混乱的反向传播）。这使得它们的训练非常缓慢，并且阻碍了它们在实际问题中的应用。限制玻尔兹曼机将连接仅限制在不同层之间，解决了这个问题。也就是说，在限制玻尔兹曼机的每一层内部没有连接，允许每一层的所有单元同时更新。因此，对于两个连接的层，我们可以通过交替更新每一层的所有单元来收集共现统计数据。在实践中，由于最小化抽样程序（如对比散度），可以实现更大的节省。
- en: Conditional Independence
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件独立性
- en: The lack of connections within each layer means that the states of all units
    in the hidden layer do not depend on each other, but they do depend on the states
    of units in the previous layer. In other words, given the states of the previous
    layer’s units, the state of each hidden unit is independent of the states of the
    other units in the hidden layer. This conditional independence allows us to factorize
    the joint probability of the state of a hidden layer <math alttext="p left-parenthesis
    ModifyingAbove h With right-arrow vertical-bar ModifyingAbove h Subscript p r
    e v i o u s Baseline With right-arrow right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mover accent="true"><mi>h</mi> <mo>→</mo></mover> <mo>|</mo> <mover accent="true"><msub><mi>h</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>o</mi><mi>u</mi><mi>s</mi></mrow></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> as the product of individual hidden
    unit’s state’s conditional probabilities. For example, if we have three units
    in a hidden layer, <math alttext="p left-parenthesis ModifyingAbove h With right-arrow
    vertical-bar ModifyingAbove h Subscript p r e v i o u s Baseline With right-arrow
    right-parenthesis equals p left-parenthesis h Baseline 1 vertical-bar ModifyingAbove
    h Subscript p r e v i o u s Baseline With right-arrow right-parenthesis p left-parenthesis
    h Baseline 2 vertical-bar ModifyingAbove h Subscript p r e v i o u s Baseline
    With right-arrow right-parenthesis p left-parenthesis h Baseline 3 vertical-bar
    ModifyingAbove h Subscript p r e v i o u s Baseline With right-arrow right-parenthesis"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>h</mi> <mo>→</mo></mover> <mo>|</mo>
    <mover accent="true"><msub><mi>h</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>o</mi><mi>u</mi><mi>s</mi></mrow></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>h</mi>
    <mn>1</mn> <mo>|</mo> <mover accent="true"><msub><mi>h</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>o</mi><mi>u</mi><mi>s</mi></mrow></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <mi>h</mi> <mn>2</mn>
    <mo>|</mo> <mover accent="true"><msub><mi>h</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>o</mi><mi>u</mi><mi>s</mi></mrow></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mi>p</mi> <mrow><mo>(</mo> <mi>h</mi> <mn>3</mn>
    <mo>|</mo> <mover accent="true"><msub><mi>h</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>i</mi><mi>o</mi><mi>u</mi><mi>s</mi></mrow></msub>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> . The other way is also true,
    the states of the units of a previous layer units are conditionally independent
    of each other given the states of the current layer. This conditional independence
    means that we can sample unit states instead of iteratively updating them for
    long periods of time.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层内部缺乏连接意味着隐藏层中所有单元的状态不相互依赖，但它们确实依赖于前一层单元的状态。换句话说，给定前一层单元的状态，每个隐藏单元的状态与隐藏层中其他单元的状态是独立的。这种条件独立性使我们能够将隐藏层状态的联合概率因子化为各个隐藏单元状态的条件概率的乘积。例如，如果我们在隐藏层中有三个单元，则等于
    。另一种方式也成立，即前一层单元的状态在给定当前层状态的情况下是条件独立的。这种条件独立性意味着我们可以对单元状态进行采样，而不是长时间迭代更新它们。
- en: Universal Approximation
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通用逼近
- en: The restricted connections in restricted Boltzmann machines allow for their
    stacking, that is, having a series of multiple hidden layers that are able to
    extract more complex features. We can see now how the architecture of the modern
    multilayer artificial neural network slowly emerged. Recall that in chapter four,
    we discussed the universal approximation of neural networks for a wide range of
    deterministic functions. In this chapter, we would like our networks to represent
    (or learn) joint probability distributions instead of deterministic function.
    In 2008, Le Roux and Bengio proved that Boltzmann machines can approximate any
    discrete probability distribution to an arbitrary accuracy. This result also applies
    to restricted Boltzmann machines. Moreover, under certain mild conditions, each
    additional hidden layer increases the value of the log likelihood function, thus
    gets allows the model distribution to be closer to the true joint probability
    distribution of the training set.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机中的受限连接允许它们堆叠，即具有一系列能够提取更复杂特征的多个隐藏层。我们现在可以看到现代多层人工神经网络的架构是如何逐渐出现的。回想一下，在第四章中，我们讨论了神经网络对各种确定性函数的通用逼近。在本章中，我们希望我们的网络代表（或学习）联合概率分布，而不是确定性函数。2008年，Le
    Roux和Bengio证明了玻尔兹曼机可以以任意精度逼近任何离散概率分布。这个结果也适用于受限玻尔兹曼机。此外，在某些温和条件下，每个额外的隐藏层都会增加对数似然函数的值，从而使模型分布更接近训练集的真实联合概率分布。
- en: In 2015, Eldan and Shamir verified emperically that increasing the number of
    layers of the neural networks is exponentially more valuable than increasing the
    width of network layers the number of units in each layer (depth *vs* width).
    We also know from practice (without proofs) that it is possible to train a network
    with hundreds of hidden layers, where deeper layers represent higher-order features.
    Historically, the problem of vanishing gradients had to be overcome in order to
    train deep networks.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，Eldan和Shamir经验性地验证了增加神经网络层数的价值远远超过增加网络层宽度中每层单元的数量（深度与宽度）。我们也从实践中知道（没有证明），可以训练一个具有数百个隐藏层的网络，其中更深层代表更高阶特征。从历史上看，必须克服梯度消失问题才能训练深度网络。
- en: The Original Autoencoder
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始自动编码器
- en: 'The autoencoder architecture aims to compress the information of the input
    into its lower dimensional hidden layers. The hidden layers should retain the
    same amount of information as the input layers, even when they have fewer units
    than the input layers. We have discussed modern variational autoencoders, which
    provide an efficient method for training autoencoder networks. During training,
    each vector should be mapped to itself (unsupervised), and the network tries to
    learn the best *encoding*. The input and the output layers must then have the
    same number of units. A Boltzman machine set up with a certain number of input
    units, fewer number of hidden units, and an output layer with the same number
    of units as the input layer, explains an original network autoencoder architecture.
    From a historical perspective this is significant: The autoencoder is of the first
    examples of a network successfully learning a code, implicit in the states of
    the hidden units, to represent its inputs. This made it possible to force a network
    to compress its input into a hidden layer with minimal loss of information. This
    is now an integral part of neural networks that we take for granted. The autoencoder
    architecture, with and without Boltzmann machines (with and without energy based
    joint probability distribution), is still very influential in the deep learning
    world.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器架构旨在将输入的信息压缩到其较低维度的隐藏层中。隐藏层应该保留与输入层相同数量的信息，即使它们的单元数少于输入层。我们已经讨论了现代变分自动编码器，为训练自动编码器网络提供了一种高效的方法。在训练过程中，每个向量应该被映射到自身（无监督），网络试图学习最佳的编码。然后输入层和输出层必须具有相同数量的单元。一个玻尔兹曼机设置了一定数量的输入单元，较少数量的隐藏单元，以及一个具有与输入层相同数量的单元的输出层，解释了原始网络自动编码器架构。从历史的角度来看，这是重要的：自动编码器是网络成功学习代码的第一个例子，隐含在隐藏单元的状态中，以表示其输入。这使得网络能够强制将其输入压缩到一个隐藏层中，而最小化信息损失。这现在是神经网络中我们理所当然的一个重要部分。自动编码器架构，无论是否使用玻尔兹曼机（是否使用基于能量的联合概率分布），在深度学习领域仍然具有很大影响力。
- en: Earlier in this chapter, we discussed variational autoencoders. From a historical
    point of view, these synthesize the ideas of Boltzmann machine autoencoders, deep
    autoencoder networks, *denoising autoencoders*, and the information bottleneck
    (Tishby et al., 2000), which have their roots in the idea of analysis by synthesis
    (Selfridge, 1958). Variational autoencoders use fast variational methods for their
    learning. In the context of bias-variance tradeoff, variational methods provide
    biased estimates for the network’s weights that have low variance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们讨论了变分自动编码器。从历史的角度来看，这些综合了玻尔兹曼机自动编码器、深度自动编码器网络、去噪自动编码器和信息瓶颈（Tishby等人，2000）的思想，这些思想源于分析合成的概念（Selfridge，1958）。变分自动编码器使用快速变分方法进行学习。在偏差-方差权衡的背景下，变分方法为网络的权重提供了偏差估计，具有较低的方差。
- en: Probabilistic Language Modeling
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率语言建模
- en: A natural connection between this chapter and [Chapter 6](ch06.xhtml#ch06),
    which focused almost exclusively on natural language processing and the various
    ways to extract meaning from natural language data, to this chapter, is to survey
    the fundamentals behind probabilistic language models, then to highlights the
    models from the previous chapter that adhere to these fundamentals.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章与第6章之间的自然联系，第6章几乎专注于自然语言处理以及从自然语言数据中提取含义的各种方法，这一章是调查概率语言模型背后的基本原理，然后强调前一章中遵循这些基本原理的模型。
- en: 'This chapter started with maximum likelihood estimation. One of the reasons
    this appears everywhere when we need to estimate probability distributions is
    that the probability distribution attained via maximum likelihood estimation is
    supported by mathematical theory, under couple conditions: Maximum likelihood
    estimation does converge to the true distribution <math alttext="p Subscript d
    a t a Baseline left-parenthesis ModifyingAbove x With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    that generated the data, in the limit as the number of data samples goes to infinity
    (that is, assuming we have a ton of data), and provided that the model probability
    distribution <math alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove
    x With right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> already includes the true probability
    distribution. That is, in the limit as the number of samples goes to infinity,
    the model parameters <math alttext="ModifyingAbove theta With right-arrow Superscript
    asterisk"><msup><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>*</mo></msup></math>
    that will maximize the likelihood of the data satisfy: <math alttext="p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow comma ModifyingAbove
    theta With right-arrow Superscript asterisk Baseline right-parenthesis equals
    p Subscript d a t a Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo>
    <msup><mover accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>*</mo></msup> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math>
    .'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以最大似然估计开始。当我们需要估计概率分布时，这种方法无处不在的原因之一是，通过最大似然估计获得的概率分布受到数学理论的支持，在一些条件下：最大似然估计会收敛到生成数据的真实分布<math
    alttext="p Subscript d a t a Baseline left-parenthesis ModifyingAbove x With right-arrow
    right-parenthesis">，当数据样本数量趋于无穷大时（也就是，假设我们有大量数据），并且假设模型概率分布<math alttext="p Subscript
    m o d e l Baseline left-parenthesis ModifyingAbove x With right-arrow comma ModifyingAbove
    theta With right-arrow right-parenthesis">已经包含了真实的概率分布。也就是说，当样本数量趋于无穷大时，最大化数据似然性的模型参数<math
    alttext="ModifyingAbove theta With right-arrow Superscript asterisk">将满足：<math
    alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow comma ModifyingAbove theta With right-arrow Superscript asterisk Baseline
    right-parenthesis equals p Subscript d a t a Baseline left-parenthesis ModifyingAbove
    x With right-arrow right-parenthesis">。
- en: In language models, the training data is samples of text from some corpus and/or
    genre, and we would like to learn its probability distribution so that we can
    generate similar text. It is important to keep in mind that the true data distribution
    is most likely *not* included in the family of distributions provided by <math
    alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>,</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mrow></math> , so the theoretical result
    in the previous paragraph might never hold in practice, however, this doesn’t
    deter us and we usually settle for models that are useful enough for our purposes.
    Our goal is to build a model that assigns probabilities to pieces of language.
    If we randomly assemble some pieces of language, we most likely end up with gibberish.
    What we actually want is to find the distribution of those sentences that mean
    something. A good language model is one that assigns high probabilities to sentences
    that are meaningful, even when these sentences are not among the training data.
    People usually compute the *perplexity* of a language model on the training data
    set to evaluate its performance.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型中，训练数据是来自某个语料库和/或流派的文本样本，我们希望学习其概率分布，以便生成类似的文本。重要的是要记住，真实的数据分布很可能*不*包含在由<math
    alttext="p Subscript m o d e l Baseline left-parenthesis ModifyingAbove x With
    right-arrow comma ModifyingAbove theta With right-arrow right-parenthesis">提供的分布族中，因此前一段中的理论结果在实践中可能永远不会成立，然而，这并不会阻止我们，我们通常会满足于对我们的目的足够有用的模型。我们的目标是构建一个能够为语言片段分配概率的模型。如果我们随机组合一些语言片段，我们很可能最终得到一些无意义的话。我们实际上想要找到那些有意义的句子的分布。一个好的语言模型是那种为有意义的句子分配高概率的模型，即使这些句子不在训练数据中。人们通常计算语言模型在训练数据集上的*困惑度*来评估其性能。
- en: Language models are based on the assumption that the probability distribution
    of the next word depends on the *n-1* words that preceded it, for some fixed *n*,
    so we care about calculating <math alttext="p Subscript m o d e l Baseline left-parenthesis
    x Subscript n Baseline vertical-bar x 1 comma x 2 comma ellipsis comma x Subscript
    n minus 1 Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow></mrow></math> . If we are using a word2vec model, that embeds
    the meaning of each word in vector, then each of these *x*’s is represented by
    a vector. Words which mean things or are frequently used in similar contexts tend
    to have similar vector values. We can use the transformer model from the previous
    chapter to predict the next word vector based on the preceding word vectors.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型基于这样一个假设，即下一个单词的概率分布取决于前面的*n-1*个单词，对于某个固定的*n*，因此我们关心计算<math alttext="p Subscript
    m o d e l Baseline left-parenthesis x Subscript n Baseline vertical-bar x 1 comma
    x 2 comma ellipsis comma x Subscript n minus 1 Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>)</mo></mrow></mrow></math>。如果我们使用一个word2vec模型，它将每个单词的含义嵌入到向量中，那么这些*x*中的每一个都由一个向量表示。那些有意义或在类似上下文中经常使用的单词往往具有相似的向量值。我们可以使用前一章的transformer模型来基于前面的单词向量预测下一个单词向量。
- en: 'Frequency based language models construct conditional probability tables by
    counting the number of times words appear together in the training corpus. For
    example, we can estimate the conditional probability p(morning|good) of the word
    *morning* appearing after the word *good*, by counting by the number of times
    *good morning* appears in the corpus divided by the number of times *good* appears
    in the corpus. That is:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 基于频率的语言模型通过计算训练语料库中单词一起出现的次数来构建条件概率表。例如，我们可以通过计算语料库中*good morning*出现的次数除以语料库中*good*出现的次数来估计单词*morning*在单词*good*之后出现的条件概率p(morning|good)。即：
- en: <math alttext="dollar-sign p left-parenthesis m o r n i n g vertical-bar g o
    o d right-parenthesis equals StartFraction p left-parenthesis g o o d comma m
    o r n i n g right-parenthesis Over p left-parenthesis g o o d right-parenthesis
    EndFraction dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mi>m</mi> <mi>o</mi>
    <mi>r</mi> <mi>n</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>|</mo> <mi>g</mi> <mi>o</mi>
    <mi>o</mi> <mi>d</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>g</mi><mi>o</mi><mi>o</mi><mi>d</mi><mo>,</mo><mi>m</mi><mi>o</mi><mi>r</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>)</mo></mrow>
    <mrow><mi>p</mi><mo>(</mo><mi>g</mi><mi>o</mi><mi>o</mi><mi>d</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p left-parenthesis m o r n i n g vertical-bar g o
    o d right-parenthesis equals StartFraction p left-parenthesis g o o d comma m
    o r n i n g right-parenthesis Over p left-parenthesis g o o d right-parenthesis
    EndFraction dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mi>m</mi> <mi>o</mi>
    <mi>r</mi> <mi>n</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>|</mo> <mi>g</mi> <mi>o</mi>
    <mi>o</mi> <mi>d</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>g</mi><mi>o</mi><mi>o</mi><mi>d</mi><mo>,</mo><mi>m</mi><mi>o</mi><mi>r</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>)</mo></mrow>
    <mrow><mi>p</mi><mo>(</mo><mi>g</mi><mi>o</mi><mi>o</mi><mi>d</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: This breaks down for very large corpuses or for unstructured text data such
    as tweets, facebook comments, or sms messages where the usual rules of grammar/spelling,
    *etc.* are not totally adhered to.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这在非常大的语料库或非结构化文本数据（如推特、Facebook评论或短信消息）中会出现问题，因为这些数据通常不完全遵循语法/拼写等规则。
- en: 'We can formalize the notion of a probabilistic language model this way:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样形式化概率语言模型的概念：
- en: Specify the vocabulary *V* of your language. This could be a set of characters,
    spaces, punctuations, symbols, unique words, and/ or n-grams. Mathematically,
    it is a finite discrete set that includes a stopping symbol signifying the end
    of a thought or sentence, like a period in English (even though a period does
    not always mean the end of a sentence in English, such as when it is used for
    abbreviations).
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定您语言的词汇表*V*。这可以是一组字符、空格、标点符号、符号、独特单词和/或n-gram。从数学上讲，它是一个包括一个停止符号的有限离散集合，该符号表示思想或句子的结束，就像英语中的句号（尽管句号并不总是表示句子的结束，比如在缩写中使用时）。
- en: Define a sentence (which could be meaningful or not) as a finite sequence of
    symbols <math alttext="ModifyingAbove x With right-arrow equals left-parenthesis
    x 1 comma x 2 comma ellipsis comma x Subscript m Baseline right-parenthesis"><mrow><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>m</mi></msub> <mo>)</mo></mrow></mrow></math>
    from the vocabulary *V* ending in the stop symbol. Each <math alttext="x Subscript
    i"><msub><mi>x</mi> <mi>i</mi></msub></math> can assume any value from the vocabulary
    *V*. We can specify *m* as a maximum length for our sentences.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个句子（可能有意义也可能没有）定义为一个以词汇表*V*结束的符号有限序列<math alttext="ModifyingAbove x With right-arrow
    equals left-parenthesis x 1 comma x 2 comma ellipsis comma x Subscript m Baseline
    right-parenthesis"><mrow><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>=</mo>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mrow></math>。每个<math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math>可以从词汇表*V*中任意取值。我们可以将*m*指定为我们句子的最大长度。
- en: 'Define our language space <math alttext="l Superscript m Baseline equals StartSet
    left-parenthesis x 1 comma x 2 comma ellipsis comma x Subscript m Baseline right-parenthesis
    comma x Subscript i Baseline element-of upper V EndSet"><mrow><msup><mi>l</mi>
    <mi>m</mi></msup> <mo>=</mo> <mrow><mo>{</mo> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>m</mi></msub> <mo>)</mo></mrow> <mo>,</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>∈</mo> <mi>V</mi> <mo>}</mo></mrow></mrow></math> as the
    set of all sentences of length less than or equal to *m*. The overwhelming majority
    of these sentences will mean nothing, and we need to define a language model that
    only captures the sentences that mean something: High probabilities for meaningful
    sentences and low probabilities for unmeaningful ones.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的语言空间<math alttext="l Superscript m Baseline equals StartSet left-parenthesis
    x 1 comma x 2 comma ellipsis comma x Subscript m Baseline right-parenthesis comma
    x Subscript i Baseline element-of upper V EndSet"><mrow><msup><mi>l</mi> <mi>m</mi></msup>
    <mo>=</mo> <mrow><mo>{</mo> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mi>m</mi></msub> <mo>)</mo></mrow> <mo>,</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>∈</mo> <mi>V</mi> <mo>}</mo></mrow></mrow></math>定义为所有长度小于或等于*m*的句子集合。这些句子中绝大多数将毫无意义，我们需要定义一个语言模型，只捕捉那些有意义的句子：对有意义句子的高概率和对无意义句子的低概率。
- en: Let <math alttext="script upper L"><mi>ℒ</mi></math> be the collection of all
    subsets of <math alttext="l Superscript m"><msup><mi>l</mi> <mi>m</mi></msup></math>
    . This accounts for collections of all meaningful and meaningless sentences of
    maximal length *m*.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让<math alttext="script upper L"><mi>ℒ</mi></math>表示<math alttext="l Superscript
    m"><msup><mi>l</mi> <mi>m</mi></msup></math>的所有子集的集合。这涵盖了所有最大长度为*m*的有意义和无意义句子的集合。
- en: 'In rigorous probability theory, we usually start with probability triples:
    A space, a *sigma algebra* containing some subsets of that space, and a probability
    measure assigned to each member of the chosen sigma algebra (do not worry about
    these details in this chapter). A language model, in this context, is the probability
    triple: The language space <math alttext="l Superscript m"><msup><mi>l</mi> <mi>m</mi></msup></math>
    , the sigma algebra made up of all the subsets of the language space <math alttext="script
    upper L"><mi>ℒ</mi></math> , and a probability measure *P* that we need to assign
    to each member of <math alttext="script upper L"><mi>ℒ</mi></math> . Since our
    language space is discrete and finite, it is easier to assign a probability *p*
    to each member of <math alttext="l Superscript m"><msup><mi>l</mi> <mi>m</mi></msup></math>
    instead, that is a probability to each sentence <math alttext="ModifyingAbove
    x With right-arrow equals left-parenthesis x 1 comma x 2 comma ellipsis comma
    x Subscript m Baseline right-parenthesis"><mrow><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>x</mi> <mi>m</mi></msub> <mo>)</mo></mrow></mrow></math> (since this
    will in turn induce a probability measure *P* on the collection of all subsets
    <math alttext="script upper L"><mi>ℒ</mi></math> , so we will never worry about
    this for language models). It is this *p* that we need to learn from the training
    data. The usual approach is to select a full family of probability distributions
    <math alttext="p left-parenthesis ModifyingAbove x With right-arrow semicolon
    ModifyingAbove theta With right-arrow right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></math> prametrized by <math alttext="ModifyingAbove
    theta With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math>
    .'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在严格的概率理论中，我们通常从概率三元组开始：一个空间，包含该空间一些子集的*sigma代数*，以及分配给所选sigma代数的概率测度（在本章中不必担心这些细节）。在这种情况下，语言模型是概率三元组：语言空间<math
    alttext="l Superscript m"><msup><mi>l</mi> <mi>m</mi></msup></math>，由语言空间<math
    alttext="script upper L"><mi>ℒ</mi></math>的所有子集组成的sigma代数，以及我们需要分配给<math alttext="script
    upper L"><mi>ℒ</mi></math>的每个成员的概率测度*P*。由于我们的语言空间是离散且有限的，更容易为<math alttext="l
    Superscript m"><msup><mi>l</mi> <mi>m</mi></msup></math>的每个成员分配一个概率*p*，即为每个句子<math
    alttext="ModifyingAbove x With right-arrow equals left-parenthesis x 1 comma x
    2 comma ellipsis comma x Subscript m Baseline right-parenthesis"><mrow><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>m</mi></msub> <mo>)</mo></mrow></mrow></math>分配一个概率（因为这将进而在所有子集<math
    alttext="script upper L"><mi>ℒ</mi></math>的集合上引入一个概率测度*P*，所以我们永远不会为语言模型担心这一点）。我们需要从训练数据中学习的就是这个*p*。通常的方法是选择一个由<math
    alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math>参数化的概率分布的完整系列<math alttext="p left-parenthesis ModifyingAbove
    x With right-arrow semicolon ModifyingAbove theta With right-arrow right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>;</mo> <mover
    accent="true"><mi>θ</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>。
- en: Finally, we need to estimate the parameters <math alttext="ModifyingAbove theta
    With right-arrow"><mover accent="true"><mi>θ</mi> <mo>→</mo></mover></math> by
    maximizing the likelihood of the training dataset that contains many sentence
    samples from <math alttext="l Superscript m"><msup><mi>l</mi> <mi>m</mi></msup></math>
    . Since the probabilities of meaningful sentences are very small numbers, we use
    the logarithm of these probabilities instead in order to avoid the risk of underflow.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要通过最大化包含许多来自<math alttext="l Superscript m"><msup><mi>l</mi> <mi>m</mi></msup></math>的句子样本的训练数据集的似然性来估计参数<math
    alttext="ModifyingAbove theta With right-arrow"><mover accent="true"><mi>θ</mi>
    <mo>→</mo></mover></math>。由于有意义的句子的概率非常小，我们使用这些概率的对数来避免下溢的风险。
- en: For consistency, it is a nice exercise to check Log-linear models and Log-bilinear
    models (GloVe) and latent Dirichlet allocation from [Chapter 7](ch07.xhtml#ch07)
    in the context of this section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性，在这一部分的背景下，检查对数线性模型和对数双线性模型（GloVe）以及来自[第7章](ch07.xhtml#ch07)的潜在狄利克雷分配是一个不错的练习。
- en: Summary And Looking Ahead
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和展望
- en: 'This was another foundational chapter in our journey to pinpointing the mathematics
    that is required for the state of the art AI models. We shifted from learning
    deterministic functions in earlier chapters to learning joint probability distributions
    of features of the data. The goal is to use those to generate new data similar
    to the training data. We learned, still without formalizing, a lot of properties
    and rules for probability distributions. We surveyed the most relevant models,
    along with some historical evolution that led us here. We made the distinction
    between models that provide explicit formulas for their joint distributions, and
    models which interact indirectly with the underlying distribution without explicitly
    writing down formulas. For models with explicit formulas, computing the log likelihood
    and its gradients can be tractable or intractable, each of which requires its
    own methods. The goal is always the same: Capture the underlying true joint probability
    ditribution of the data, by finding a model that maximizes its log likelihood.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在寻找最先进人工智能模型所需的数学基础的旅程中的又一个基础章节。我们从早期章节中学习确定性函数转向学习数据特征的联合概率分布。目标是利用这些数据生成类似于训练数据的新数据。我们学到了很多关于概率分布的性质和规则，尽管还没有正式化。我们调查了最相关的模型，以及一些历史演变，导致我们到达这里。我们区分了提供其联合分布的显式公式的模型和间接与底层分布交互而不明确写出公式的模型。对于具有显式公式的模型，计算对数似然及其梯度可能是可行的或不可行的，每种情况都需要自己的方法。目标始终相同：通过找到最大化其对数似然的模型来捕捉数据的潜在真实联合概率分布。
- en: None of this would have been necessary if our data was low dimensional, with
    one or two features. Histograms and kernel density estimators do a good job estimating
    probability distributions for low dimensional data. One of the best accomplishments
    in machine learning is the ability to model high-dimensional joint probability
    distributions from a big volume of data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的数据是低维的，只有一个或两个特征，那么这一切都是不必要的。直方图和核密度估计器很好地估计了低维数据的概率分布。机器学习中最重要的成就之一是能够从大量数据中建模高维联合概率分布。
- en: All of the approaches that we presented in this chapter have their pros and
    cons. For example, variational autoencoders allow us to perform both learning
    and efficient Bayesian inference in probabilistic graphical models with hidden
    (latent) variables. However, they generate lower quality samples. Generative adversarial
    networks generate better samples, but they are more difficult to optimize due
    to their unstable training dynamics. They search for an unstable saddle point
    instead of a stable maximum or minimum. Deep belief networks such as PixelCNN
    and WaveNet have a stable training process, optimizing the softmax loss function.
    However, they are inefficient during sampling and don’t organically embed data
    into lower dimensions, as autoencoders do.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们介绍的所有方法都有其优缺点。例如，变分自动编码器允许我们在具有隐藏（潜在）变量的概率图模型中执行学习和高效的贝叶斯推断。然而，它们生成的样本质量较低。生成对抗网络生成更好的样本，但由于其不稳定的训练动态，更难优化。它们寻找一个不稳定的鞍点，而不是一个稳定的最大值或最小值。像PixelCNN和WaveNet这样的深度信念网络具有稳定的训练过程，优化softmax损失函数。然而，在采样过程中效率低下，不像自动编码器那样有机地将数据嵌入到较低维度。
- en: Two player zero sum games from Game Theory appeared naturally in this chapter
    due to the set up of generative adversarial networks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中出现的博弈论中的两人零和游戏是由生成对抗网络的设置自然产生的。
- en: Looking ahead into the next chapter on *graphical modeling*, we note that the
    connections in the graph of a neural network dictate the way we can write conditional
    probabilities, easily pointing out the various dependencies and conditional independences.
    We saw this while discussing restricted Boltzmann machines in this chapter. In
    the next chapter, we focus exclusively on graphical modeling, which we have managed
    to avoid for a good three quarters of the book.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 展望下一章关于*图形建模*，我们注意到神经网络图中的连接决定了我们如何编写条件概率，轻松指出各种依赖关系和条件独立性。我们在本章讨论受限玻尔兹曼机时看到了这一点。在下一章中，我们将专注于图形建模，这是我们在本书的三分之三中设法避免的。
