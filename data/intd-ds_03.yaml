- en: Chapter 4\. Handling large data on a single computer
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章：在单台计算机上处理大型数据
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Working with large data sets on a single computer
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单台计算机上处理大型数据集
- en: Working with Python libraries suitable for larger data sets
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适合大型数据集的Python库
- en: Understanding the importance of choosing correct algorithms and data structures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解选择正确算法和数据结构的重要性
- en: Understanding how you can adapt algorithms to work inside databases
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何调整算法以在数据库内部工作
- en: What if you had so much data that it seems to outgrow you, and your techniques
    no longer seem to suffice? What do you do, surrender or adapt?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据量如此之大，以至于似乎超出了你的能力，你的技术似乎也不再足够，你会怎么办，屈服还是适应？
- en: Luckily you chose to adapt, because you’re still reading. This chapter introduces
    you to techniques and tools to handle larger data sets that are still manageable
    by a single computer if you adopt the right techniques.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你选择了适应，因为你还在阅读。本章将向你介绍一些技术和工具，以处理更大的数据集，如果你采用正确的技术，这些数据集仍然可以通过单台计算机进行管理。
- en: This chapter gives you the tools to perform the classifications and regressions
    when the data no longer fits into the RAM (random access memory) of your computer,
    whereas [chapter 3](kindle_split_011.xhtml#ch03) focused on in-memory data sets.
    [Chapter 5](kindle_split_013.xhtml#ch05) will go a step further and teach you
    how to deal with data sets that require multiple computers to be processed. When
    we refer to *large data* in this chapter we mean data that causes problems to
    work with in terms of memory or speed but can still be handled by a single computer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为你提供了在数据不再适合你计算机的RAM（随机访问内存）中进行分类和回归的工具，而[第三章](kindle_split_011.xhtml#ch03)则专注于内存数据集。[第五章](kindle_split_013.xhtml#ch05)将更进一步，教你如何处理需要多台计算机来处理的数据集。当我们在本章中提到“大型数据”时，我们指的是那些在内存或速度方面引起问题，但仍然可以通过单台计算机处理的数据。
- en: 'We start this chapter with an overview of the problems you face when handling
    large data sets. Then we offer three types of solutions to overcome these problems:
    adapt your algorithms, choose the right data structures, and pick the right tools.
    Data scientists aren’t the only ones who have to deal with large data volumes,
    so you can apply general best practices to tackle the large data problem. Finally,
    we apply this knowledge to two case studies. The first case shows you how to detect
    malicious URLs, and the second case demonstrates how to build a recommender engine
    inside a database.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从本章开始，概述你在处理大型数据集时面临的问题。然后我们提供三种类型的解决方案来克服这些问题：调整你的算法、选择正确的数据结构，以及选择正确的工具。数据科学家并不是唯一需要处理大量数据的人，因此你可以应用通用的最佳实践来解决大数据问题。最后，我们将这些知识应用于两个案例研究。第一个案例展示了如何检测恶意URL，第二个案例演示了如何在数据库内部构建推荐引擎。
- en: 4.1\. The problems you face when handling large data
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 处理大型数据时面临的问题
- en: 'A large volume of data poses new challenges, such as overloaded memory and
    algorithms that never stop running. It forces you to adapt and expand your repertoire
    of techniques. But even when you can perform your analysis, you should take care
    of issues such as I/O (input/output) and CPU starvation, because these can cause
    speed issues. [Figure 4.1](#ch04fig01) shows a mind map that will gradually unfold
    as we go through the steps: problems, solutions, and tips.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的数据带来了新的挑战，例如内存过载和永不停止运行的算法。这迫使你适应并扩展你的技术储备。但即使你能够执行分析，你也应该注意诸如I/O（输入/输出）和CPU饥饿等问题，因为这些可能会引起速度问题。[图4.1](#ch04fig01)
    展示了一个思维导图，随着我们逐步进行，它将逐渐展开：问题、解决方案和技巧。
- en: Figure 4.1\. Overview of problems encountered when working with more data than
    can fit in memory
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1：处理比内存能容纳更多的数据时遇到的问题概述
- en: '![](Images/04fig01_alt.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig01_alt.jpg)'
- en: A computer only has a limited amount of RAM. When you try to squeeze more data
    into this memory than actually fits, the OS will start swapping out memory blocks
    to disks, which is far less efficient than having it all in memory. But only a
    few algorithms are designed to handle large data sets; most of them load the whole
    data set into memory at once, which causes the out-of-memory error. Other algorithms
    need to hold multiple copies of the data in memory or store intermediate results.
    All of these aggravate the problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机只有有限的RAM。当你试图将比实际适合的更多数据挤进这个内存时，操作系统将开始将内存块交换到磁盘上，这比所有数据都在内存中要低效得多。但只有少数算法被设计来处理大数据集；其中大多数一次将整个数据集加载到内存中，这会导致内存不足错误。其他算法需要在内存中保留数据的多个副本或存储中间结果。所有这些都加剧了这个问题。
- en: 'Even when you cure the memory issues, you may need to deal with another limited
    resource: *time*. Although a computer may think you live for millions of years,
    in reality you won’t (unless you go into cryostasis until your PC is done). Certain
    algorithms don’t take time into account; they’ll keep running forever. Other algorithms
    can’t end in a reasonable amount of time when they need to process only a few
    megabytes of data.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你解决了内存问题，你可能还需要处理另一个有限的资源：*时间*。虽然计算机可能认为你活了几百万年，但现实中你不会（除非你进入冷冻睡眠直到你的PC完成）。某些算法没有考虑到时间；它们会永远运行。其他算法在需要处理仅几兆字节的数据时，无法在合理的时间内结束。
- en: A third thing you’ll observe when dealing with large data sets is that components
    of your computer can start to form a bottleneck while leaving other systems idle.
    Although this isn’t as severe as a never-ending algorithm or out-of-memory errors,
    it still incurs a serious cost. Think of the cost savings in terms of person days
    and computing infrastructure for CPU starvation. Certain programs don’t feed data
    fast enough to the processor because they have to read data from the hard drive,
    which is one of the slowest components on a computer. This has been addressed
    with the introduction of solid state drives (SSD), but SSDs are still much more
    expensive than the slower and more widespread hard disk drive (HDD) technology.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大数据集时，你还会观察到第三件事，那就是你的计算机组件可能会开始形成瓶颈，而其他系统则闲置。虽然这不像无限循环的算法或内存不足错误那样严重，但它仍然会带来严重的成本。从节省人天和计算基础设施的角度来考虑CPU饥饿的成本。某些程序因为必须从硬盘读取数据（这是计算机上速度最慢的组件之一）而无法快速向处理器提供数据。这个问题随着固态硬盘（SSD）的引入得到了解决，但SSD仍然比速度较慢且更广泛使用的硬盘驱动器（HDD）技术要贵得多。
- en: 4.2\. General techniques for handling large volumes of data
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 处理大量数据的通用技术
- en: Never-ending algorithms, out-of-memory errors, and speed issues are the most
    common challenges you face when working with large data. In this section, we’ll
    investigate solutions to overcome or alleviate these problems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无限循环的算法、内存不足错误和速度问题是你在处理大数据时面临的最常见挑战。在本节中，我们将探讨克服或减轻这些问题的解决方案。
- en: 'The solutions can be divided into three categories: using the correct algorithms,
    choosing the right data structure, and using the right tools ([figure 4.2](#ch04fig02)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案可以分为三类：使用正确的算法、选择合适的数据结构和使用合适的工具([图4.2](#ch04fig02))。
- en: Figure 4.2\. Overview of solutions for handling large data sets
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2\. 处理大数据集的解决方案概述
- en: '![](Images/04fig02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig02.jpg)'
- en: No clear one-to-one mapping exists between the problems and solutions because
    many solutions address both lack of memory and computational performance. For
    instance, data set compression will help you solve memory issues because the data
    set becomes smaller. But this also affects computation speed with a shift from
    the slow hard disk to the fast CPU. Contrary to RAM (random access memory), the
    hard disc will store everything even after the power goes down, but writing to
    disc costs more time than changing information in the fleeting RAM. When constantly
    changing the information, RAM is thus preferable over the (more durable) hard
    disc. With an unpacked data set, numerous read and write operations (I/O) are
    occurring, but the CPU remains largely idle, whereas with the compressed data
    set the CPU gets its fair share of the workload. Keep this in mind while we explore
    a few solutions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 问题与解决方案之间不存在明确的单一映射，因为许多解决方案同时解决了内存不足和计算性能问题。例如，数据集压缩可以帮助你解决内存问题，因为数据集变得更小。但这也会影响计算速度，从慢速硬盘转移到快速
    CPU。与 RAM（随机访问内存）不同，硬盘会在断电后存储一切，但写入硬盘比在短暂的 RAM 中更改信息花费更多时间。当不断更改信息时，RAM 因此比（更耐用的）硬盘更可取。在未打包的数据集中，会发生大量的读写操作（I/O），但
    CPU 仍然大部分空闲，而压缩数据集时，CPU 可以获得其应得的工作量。在我们探索一些解决方案时，请记住这一点。
- en: 4.2.1\. Choosing the right algorithm
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 选择合适的算法
- en: 'Choosing the right algorithm can solve more problems than adding more or better
    hardware. An algorithm that’s well suited for handling large data doesn’t need
    to load the entire data set into memory to make predictions. Ideally, the algorithm
    also supports parallelized calculations. In this section we’ll dig into three
    types of algorithms that can do that: *online algorithms*, *block algorithms*,
    and *MapReduce algorithms*, as shown in [figure 4.3](#ch04fig03).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的算法可以解决比增加更多或更好的硬件更多的问题。一个适合处理大量数据的算法不需要将整个数据集加载到内存中进行预测。理想情况下，该算法还支持并行计算。在本节中，我们将深入了解三种可以实现这一点的算法类型：*在线算法*、*块算法*和*MapReduce
    算法*，如图 4.3 所示。
- en: Figure 4.3\. Overview of techniques to adapt algorithms to large data sets
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.3\. 适应大数据集的技术概述
- en: '![](Images/04fig03_alt.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig03_alt.jpg)'
- en: Online learning algorithms
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在线学习算法
- en: Several, but not all, machine learning algorithms can be trained using one observation
    at a time instead of taking all the data into memory. Upon the arrival of a new
    data point, the model is trained and the observation can be forgotten; its effect
    is now incorporated into the model’s parameters. For example, a model used to
    predict the weather can use different parameters (like atmospheric pressure or
    temperature) in different regions. When the data from one region is loaded into
    the algorithm, it forgets about this raw data and moves on to the next region.
    This “use and forget” way of working is the perfect solution for the memory problem
    as a single observation is unlikely to ever be big enough to fill up all the memory
    of a modern-day computer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法可以通过每次使用一个观察值来训练，而不是将所有数据都加载到内存中。当新的数据点到达时，模型被训练，观察值可以被遗忘；其影响现在已纳入模型参数。例如，用于预测天气的模型可以使用不同的参数（如大气压力或温度）在不同地区。当来自一个地区的数据被加载到算法中时，它会忘记这些原始数据，然后转向下一个地区。这种“使用并遗忘”的工作方式是解决内存问题的完美解决方案，因为单个观察值不太可能大到足以填满现代计算机的所有内存。
- en: '[Listing 4.1](#ch04ex01) shows how to apply this principle to a perceptron
    with online learning. A *perceptron* is one of the least complex machine learning
    algorithms used for binary classification (0 or 1); for instance, will the customer
    buy or not?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 4.1](#ch04ex01) 展示了如何将此原理应用于具有在线学习的感知器。**感知器**是用于二元分类（0 或 1）的最简单机器学习算法之一；例如，顾客会购买还是不会购买？'
- en: Listing 4.1\. Training a perceptron by observation
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.1\. 通过观察训练感知器
- en: '![](Images/ch04ex01-0.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex01-0.jpg)'
- en: '![](Images/ch04ex01-1.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex01-1.jpg)'
- en: We’ll zoom in on parts of the code that might not be so evident to grasp without
    further explanation. We’ll start by explaining how the `train_observation()` function
    works. This function has two large parts. The first is to calculate the prediction
    of an observation and compare it to the actual value. The second part is to change
    the weights if the prediction seems to be wrong.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将放大代码中可能不是那么容易理解的部分。我们将首先解释 `train_observation()` 函数的工作原理。这个函数有两个主要部分。第一部分是计算观察值的预测并将其与实际值进行比较。第二部分是如果预测似乎错误，则更改权重。
- en: '![](Images/091fig01_alt.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/091fig01_alt.jpg)'
- en: 'The prediction (y) is calculated by multiplying the input vector of independent
    variables with their respective weights and summing up the terms (as in linear
    regression). Then this value is compared with the threshold. If it’s larger than
    the threshold, the algorithm will give a 1 as output, and if it’s less than the
    threshold, the algorithm gives 0 as output. Setting the threshold is a subjective
    thing and depends on your business case. Let’s say you’re predicting whether someone
    has a certain lethal disease, with 1 being positive and 0 negative. In this case
    it’s better to have a lower threshold: it’s not as bad to be found positive and
    do a second investigation than it is to overlook the disease and let the patient
    die. The error is calculated, which will give the direction to the change of the
    weights.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 预测（y）是通过将独立变量的输入向量与其相应的权重相乘并求和项（如线性回归中所示）来计算的。然后，将此值与阈值进行比较。如果它大于阈值，算法将输出 1，如果它小于阈值，算法将输出
    0。设置阈值是一个主观的事情，取决于你的业务案例。比如说，你正在预测某人是否患有某种致命疾病，其中 1 表示阳性，0 表示阴性。在这种情况下，较低的阈值更好：被发现阳性并进行第二次调查并不像忽视疾病并让患者死亡那样糟糕。计算误差，这将给出权重变化的指示。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The weights are changed according to the sign of the error. The update is done
    with the learning rule for perceptrons. For every weight in the weight vector,
    you update its value with the following rule:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的改变是根据误差的符号进行的。更新是通过感知器的学习规则完成的。对于权重向量中的每一个权重，你使用以下规则更新其值：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where Δ`w[i]` is the amount that the weight needs to be changed, α is the learning
    rate, ε is the error, and `x`^i is the i^(th) value in the input vector (the i^(th)
    predictor variable). The error count is a variable to keep track of how many observations
    are wrongly predicted in this epoch and is returned to the calling function. You
    add one observation to the error counter if the original prediction was wrong.
    An *epoch* is a single training run through all the observations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Δ`w[i]` 是权重需要改变的数量，α 是学习率，ε 是误差，`x`^i 是输入向量中的第 i 个值（第 i 个预测变量）。误差计数是一个变量，用于跟踪在这个时期内预测错误的观察数量，并将其返回给调用函数。如果原始预测错误，则将一个观察值添加到误差计数器中。一个
    *epoch* 是一次通过所有观察值的单个训练运行。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The second function that we’ll discuss in more detail is the `train()` function.
    This function has an internal loop that keeps on training the perceptron until
    it can either predict perfectly or until it has reached a certain number of training
    rounds (epochs), as shown in the following listing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地讨论的第二个函数是 `train()` 函数。这个函数有一个内部循环，它不断地训练感知器，直到它可以完美预测或者直到它达到了一定的训练轮数（epochs），如下所示。
- en: Listing 4.2\. Using `train` functions
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2\. 使用 `train` 函数
- en: '![](Images/092fig01_alt.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/092fig01_alt.jpg)'
- en: 'Most online algorithms can also handle mini-batches; this way, you can feed
    them batches of 10 to 1,000 observations at once while using a sliding window
    to go over your data. You have three options:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数在线算法也可以处理小批量；这样，你可以一次性输入 10 到 1,000 个观察值的批次，同时使用滑动窗口遍历你的数据。你有三个选项：
- en: '***Full batch learning (also called statistical learning)*** —Feed the algorithm
    all the data at once. This is what we did in [chapter 3](kindle_split_011.xhtml#ch03).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***全批量学习（也称为统计学习）*** — 一次性将所有数据输入算法。这正是我们在[第3章](kindle_split_011.xhtml#ch03)中做的事情。'
- en: '***Mini-batch learning*** —Feed the algorithm a spoonful (100, 1000, ..., depending
    on what your hardware can handle) of observations at a time.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***小批量学习*** — 一次输入算法一小部分观察值（100，1000，...，取决于你的硬件能处理多少）。'
- en: '***Online learning*** —Feed the algorithm one observation at a time.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***在线学习*** — 一次输入算法一个观察值。'
- en: 'Online learning techniques are related to *streaming algorithms*, where you
    see every data point only once. Think about incoming Twitter data: it gets loaded
    into the algorithms, and then the observation (tweet) is discarded because the
    sheer number of incoming tweets of data might soon overwhelm the hardware. Online
    learning algorithms differ from streaming algorithms in that they can see the
    same observations multiple times. True, the online learning algorithms and streaming
    algorithms can *both* learn from observations one by one. Where they differ is
    that *online algorithms* are also used on a static data source as well as on a
    streaming data source by presenting the data in small batches (as small as a single
    observation), which enables you to go over the data multiple times. This isn’t
    the case with a *streaming algorithm*, where data flows into the system and you
    need to do the calculations typically immediately. They’re similar in that they
    handle only a few at a time.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习技术相关于*流式算法*，其中你只能看到每个数据点一次。想想 incoming Twitter 数据：它被加载到算法中，然后观察（推文）被丢弃，因为
    incoming tweets 的数量可能会很快超过硬件的处理能力。在线学习算法与流式算法的不同之处在于，它们可以看到相同的观察结果多次。确实，在线学习算法和流式算法都可以逐个从观察结果中学习。它们的不同之处在于，*在线算法*还可以用于静态数据源以及流数据源，通过以小批量（小到单个观察结果）的形式呈现数据，这使得你可以多次遍历数据。而流式算法则不同，数据流入系统，你需要立即进行计算。它们相似之处在于，它们一次只处理少量数据。
- en: Dividing a large matrix into many small ones
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将大矩阵分割成许多小矩阵
- en: Whereas in the previous chapter we barely needed to deal with how exactly the
    algorithm estimates parameters, diving into this might sometimes help. By cutting
    a large data table into small matrices, for instance, we can still do a linear
    regression. The logic behind this matrix splitting and how a linear regression
    can be calculated with matrices can be found in the sidebar. It suffices to know
    for now that the Python libraries we’re about to use will take care of the matrix
    splitting, and linear regression variable weights can be calculated using matrix
    calculus.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 而在前一章中，我们几乎不需要处理算法如何精确估计参数的问题，深入探讨这个问题有时可能会有所帮助。例如，通过将大数据表切割成小矩阵，我们仍然可以进行线性回归。这种矩阵分割的逻辑以及如何使用矩阵进行线性回归的计算可以在侧边栏中找到。现在只需知道，我们即将使用的
    Python 库将负责矩阵分割，线性回归的变量权重可以使用矩阵微积分来计算。
- en: '|  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Block matrices and matrix formula of linear regression coefficient estimation**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**块矩阵和线性回归系数估计的矩阵公式**'
- en: Certain algorithms can be translated into algorithms that use blocks of matrices
    instead of full matrices. When you partition a matrix into a block matrix, you
    divide the full matrix into parts and work with the smaller parts instead of the
    full matrix. In this case you can load smaller matrices into memory and perform
    calculations, thereby avoiding an out-of-memory error. [Figure 4.4](#ch04fig04)
    shows how you can rewrite matrix addition A + B into submatrices.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 某些算法可以被转换成使用矩阵块而不是完整矩阵的算法。当你将矩阵分割成块矩阵时，你将完整矩阵分割成部分，并使用较小的部分而不是完整矩阵。在这种情况下，你可以将较小的矩阵加载到内存中并执行计算，从而避免内存不足错误。[图
    4.4](#ch04fig04) 展示了如何将矩阵加法 A + B 重写为子矩阵。
- en: Figure 4.4\. Block matrices can be used to calculate the sum of the matrices
    A and B.
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.4\. 块矩阵可以用来计算矩阵 A 和 B 的和。
- en: '![](Images/04fig04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig04.jpg)'
- en: The formula in [figure 4.4](#ch04fig04) shows that there’s no difference between
    adding matrices A and B together in one step or first adding the upper half of
    the matrices and then adding the lower half.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.4](#ch04fig04) 中的公式显示，将矩阵 A 和 B 一次性相加与先加矩阵的上半部分然后加下半部分没有区别。'
- en: All the common matrix and vector operations, such as multiplication, inversion,
    and singular value decomposition (a variable reduction technique like PCA), can
    be written in terms of block matrices.¹ Block matrix operations save memory by
    splitting the problem into smaller blocks and are easy to parallelize.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所有常见的矩阵和向量运算，如乘法、求逆和奇异值分解（类似于 PCA 的变量减少技术），都可以用块矩阵来表示。¹ 块矩阵运算通过将问题分割成更小的块来节省内存，并且易于并行化。
- en: Although most numerical packages have highly optimized code, they work only
    with matrices that can fit into memory and will use block matrices in memory when
    advantageous. With out-of-memory matrices, they don’t optimize this for you and
    it’s up to you to partition the matrix into smaller matrices and to implement
    the block matrix version.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数数值包都有高度优化的代码，但它们只处理可以放入内存的矩阵，并且在有利的条件下会在内存中使用块矩阵。对于超出内存的矩阵，它们不会为你优化这一点，你需要自己将矩阵分割成更小的矩阵，并实现块矩阵版本。
- en: A *linear regression* is a way to predict continuous variables with a linear
    combination of its predictors; one of the most basic ways to perform the calculations
    is with a technique called *ordinary least squares*. The formula in matrix form
    is
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性回归* 是一种使用预测变量的线性组合来预测连续变量的方法；执行计算的最基本方法之一是使用称为 *普通最小二乘法* 的技术。矩阵形式的公式是'
- en: β = (X^TX)^(-1)X^Ty
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: β = (X^TX)^(-1)X^Ty
- en: where β is the coefficients you want to retrieve, X is the predictors, and y
    is the target variable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 β 是你想要检索的系数，X 是预测变量，y 是目标变量。
- en: '|  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The Python tools we have at our disposal to accomplish our task are the following:^([[1](#ch04fn01)])
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来完成任务的Python工具如下:^([[1](#ch04fn01)])
- en: ¹
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For those who want to give it a try, Given transformations are easier to achieve
    than Householder transformations when calculating singular value decompositions.
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于那些想要尝试的人来说，在计算奇异值分解时，给定变换比Householder变换更容易实现。
- en: '*bcolz* is a Python library that can store data arrays compactly and uses the
    hard drive when the array no longer fits into the main memory.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*bcolz* 是一个Python库，可以紧凑地存储数据数组，并在数组不再适合主内存时使用硬盘。'
- en: '*Dask* is a library that enables you to optimize the flow of calculations and
    makes performing calculations in parallel easier. It doesn’t come packaged with
    the default Anaconda setup so make sure to use `conda install dask` on your virtual
    environment before running the code below. Note: some errors have been reported
    on importing Dask when using 64bit Python. Dask is dependent on a few other libraries
    (such as toolz), but the dependencies should be taken care of automatically by
    pip or conda.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dask* 是一个库，它使你能够优化计算流程，并使并行计算更容易。它不是Anaconda默认设置的一部分，所以在运行下面的代码之前，请确保在你的虚拟环境中使用
    `conda install dask`。注意：当使用64位Python导入Dask时，已经报告了一些错误。Dask依赖于一些其他库（如toolz），但依赖关系应该由pip或conda自动处理。'
- en: The following listing demonstrates block matrix calculations with these libraries.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了使用这些库进行块矩阵计算。
- en: Listing 4.3\. Block matrix calculations with bcolz and Dask libraries
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.3\. 使用bcolz和Dask库进行块矩阵计算
- en: '![](Images/ch04ex03-0.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex03-0.jpg)'
- en: '![](Images/ch04ex03-1.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex03-1.jpg)'
- en: Note that you don’t need to use a block matrix inversion because XTX is a square
    matrix with size nr. of predictors * nr. of predictors. This is fortunate because
    Dask doesn’t yet support block matrix inversion. You can find more general information
    on matrix arithmetic on the Wikipedia page at [https://en.wikipedia.org/wiki/Matrix_(mathematics)](https://en.wikipedia.org/wiki/Matrix_(mathematics)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你不需要使用块矩阵求逆，因为XTX是一个大小为预测变量数量 * 预测变量数量的平方矩阵。这是幸运的，因为Dask还不支持块矩阵求逆。你可以在维基百科的矩阵算术页面[https://en.wikipedia.org/wiki/Matrix_(mathematics)](https://en.wikipedia.org/wiki/Matrix_(mathematics))上找到更多一般信息。
- en: MapReduce
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce
- en: 'MapReduce algorithms are easy to understand with an analogy: Imagine that you
    were asked to count all the votes for the national elections. Your country has
    25 parties, 1,500 voting offices, and 2 million people. You could choose to gather
    all the voting tickets from every office individually and count them centrally,
    or you could ask the local offices to count the votes for the 25 parties and hand
    over the results to you, and you could then aggregate them by party.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类比来理解MapReduce算法是很容易的：想象一下，有人要求你统计全国选举的所有选票。你的国家有25个政党，1,500个投票办公室，和200万人。你可以选择从每个办公室单独收集所有选票并集中计数，或者你可以要求当地办公室为25个政党统计选票，并将结果交给你，然后你可以按政党进行汇总。
- en: Map reducers follow a similar process to the second way of working. They first
    map values to a key and then do an aggregation on that key during the reduce phase.
    Have a look at the following listing’s pseudo code to get a better feeling for
    this.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Map reducers遵循与第二种工作方式类似的过程。它们首先将值映射到键上，然后在reduce阶段对该键进行聚合。查看以下列表的伪代码，以更好地了解这种感觉。
- en: Listing 4.4\. MapReduce pseudo code example
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.4\. MapReduce伪代码示例
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One of the advantages of MapReduce algorithms is that they’re easy to parallelize
    and distribute. This explains their success in distributed environments such as
    Hadoop, but they can also be used on individual computers. We’ll take a more in-depth
    look at them in the next chapter, and an example (JavaScript) implementation is
    also provided in [chapter 9](kindle_split_017.xhtml#ch09). When implementing MapReduce
    in Python, you don’t need to start from scratch. A number of libraries have done
    most of the work for you, such as Hadoopy, Octopy, Disco, or Dumbo.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce算法的一个优点是它们易于并行化和分发。这解释了它们在Hadoop等分布式环境中的成功，但它们也可以在单个计算机上使用。我们将在下一章更深入地探讨它们，并在[第9章](kindle_split_017.xhtml#ch09)中提供了一个（JavaScript）实现示例。当在Python中实现MapReduce时，您不需要从头开始。许多库已经为您做了大部分工作，例如Hadoopy、Octopy、Disco或Dumbo。
- en: 4.2.2\. Choosing the right data structure
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 选择合适的数据结构
- en: Algorithms can make or break your program, but the way you store your data is
    of equal importance. Data structures have different storage requirements, but
    also influence the performance of *CRUD* (create, read, update, and delete) and
    other operations on the data set.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 算法可以成就或毁掉您的程序，但您存储数据的方式同样重要。数据结构有不同的存储需求，但也会影响数据集上*CRUD*（创建、读取、更新和删除）和其他操作的性能。
- en: '[Figure 4.5](#ch04fig05) shows you have many different data structures to choose
    from, three of which we’ll discuss here: sparse data, tree data, and hash data.
    Let’s first have a look at sparse data sets.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.5](#ch04fig05) 展示了您可以选择的许多不同的数据结构，其中我们将讨论三种：稀疏数据、树数据和哈希数据。让我们首先看看稀疏数据集。'
- en: Figure 4.5\. Overview of data structures often applied in data science when
    working with large data
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5\. 在处理大数据时，数据科学中常用数据结构的概述
- en: '![](Images/04fig05_alt.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig05_alt.jpg)'
- en: Sparse data
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 稀疏数据
- en: 'A sparse data set contains relatively little information compared to its entries
    (observations). Look at [figure 4.6](#ch04fig06): almost everything is “0” with
    just a single “1” present in the second observation on variable 9.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 与其条目（观测值）相比，稀疏数据集包含相对较少的信息。看看[图4.6](#ch04fig06)：几乎全是“0”，仅在第二个观测值变量9中有一个“1”。
- en: 'Figure 4.6\. Example of a sparse matrix: almost everything is 0; other values
    are the exception in a sparse matrix'
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6\. 稀疏矩阵的示例：几乎全是0；在稀疏矩阵中，其他值是例外
- en: '![](Images/04fig06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig06.jpg)'
- en: Data like this might look ridiculous, but this is often what you get when converting
    textual data to binary data. Imagine a set of 100,000 completely unrelated Twitter
    tweets. Most of them probably have fewer than 30 words, but together they might
    have hundreds or thousands of distinct words. In the chapter on text mining we’ll
    go through the process of cutting text documents into words and storing them as
    vectors. But for now imagine what you’d get if every word was converted to a binary
    variable, with “1” representing “present in this tweet,” and “0” meaning “not
    present in this tweet.” This would result in sparse data indeed. The resulting
    large matrix can cause memory problems even though it contains little information.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的数据可能看起来很荒谬，但将文本数据转换为二进制数据时，这通常是您得到的结果。想象一下一组10万个完全不相关的Twitter推文。其中大多数可能少于30个单词，但合在一起可能有数百或数千个不同的单词。在文本挖掘章节中，我们将介绍将文本文档切割成单词并将它们存储为向量的过程。但到目前为止，想象一下如果每个单词都被转换为一个二进制变量，其中“1”代表“存在于这条推文中”，而“0”表示“不存在于这条推文中”。这将确实导致稀疏数据。结果的大矩阵即使包含很少的信息，也可能导致内存问题。
- en: 'Luckily, data like this can be stored compacted. In the case of [figure 4.6](#ch04fig06)
    it could look like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这样的数据可以压缩存储。在[图4.6](#ch04fig06)的情况下，它可能看起来像这样：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Row 2, column 9 holds the value 1.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第2行第9列的值为1。
- en: Support for working with sparse matrices is growing in Python. Many algorithms
    now support or return sparse matrices.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，支持处理稀疏矩阵的功能正在增长。现在许多算法都支持或返回稀疏矩阵。
- en: Tree structures
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 树结构
- en: Trees are a class of data structure that allows you to retrieve information
    much faster than scanning through a table. A tree always has a root value and
    subtrees of children, each with its children, and so on. Simple examples would
    be your own family tree or a biological tree and the way it splits into branches,
    twigs, and leaves. Simple decision rules make it easy to find the child tree in
    which your data resides. Look at [figure 4.7](#ch04fig07) to see how a tree structure
    enables you to get to the relevant information quickly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 树是一种数据结构，它允许你比扫描整个表更快地检索信息。树总是有一个根值和子树，每个子树都有自己的子节点，以此类推。简单的例子可以是你的家族树或生物树以及它如何分裂成分支、枝条和叶子。简单的决策规则使得很容易找到你的数据所在的子树。查看[图
    4.7](#ch04fig07)以了解树结构如何帮助你快速获取相关信息。
- en: 'Figure 4.7\. Example of a tree data structure: decision rules such as age categories
    can be used to quickly locate a person in a family tree'
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.7\. 树数据结构的示例：可以使用如年龄类别这样的决策规则快速定位家族树中的人
- en: '![](Images/04fig07_alt.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig07_alt.jpg)'
- en: In [figure 4.7](#ch04fig07) you start your search at the top and first choose
    an age category, because apparently that’s the factor that cuts away the most
    alternatives. This goes on and on until you get what you’re looking for. For whoever
    isn’t acquainted with the Akinator, we recommend visiting [http://en.akinator.com/](http://en.akinator.com/).
    The Akinator is a djinn in a magical lamp that tries to guess a person in your
    mind by asking you a few questions about him or her. Try it out and be amazed
    . . . or see how this magic is a tree search.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 4.7](#ch04fig07)中，你从顶部开始搜索，首先选择一个年龄类别，因为显然这是剔除最多替代因素的因素。这个过程一直持续到找到你想要的东西。对于不熟悉
    Akinator 的人来说，我们建议访问[http://en.akinator.com/](http://en.akinator.com/)。Akinator
    是一个神奇灯中的精灵，通过询问你关于某人的几个问题来试图猜测你心中的那个人。试试看，你会感到惊讶……或者看看这种魔法是如何变成树搜索的。
- en: Trees are also popular in databases. Databases prefer not to scan the table
    from the first line until the last, but to use a device called an *index* to avoid
    this. Indices are often based on data structures such as trees and hash tables
    to find observations faster. The use of an index speeds up the process of finding
    data enormously. Let’s look at these hash tables.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 树在数据库中也非常受欢迎。数据库更喜欢不从头到尾扫描表，而是使用一个称为 *索引* 的设备来避免这种情况。索引通常基于树和哈希表等数据结构来更快地找到观察结果。使用索引极大地加快了查找数据的过程。让我们看看这些哈希表。
- en: Hash tables
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 哈希表
- en: Hash tables are data structures that calculate a key for every value in your
    data and put the keys in a bucket. This way you can quickly retrieve the information
    by looking in the right bucket when you encounter the data. Dictionaries in Python
    are a hash table implementation, and they’re a close relative of key-value stores.
    You’ll encounter them in the last example of this chapter when you build a recommender
    system within a database. Hash tables are used extensively in databases as indices
    for fast information retrieval.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希表是一种数据结构，为你的数据中的每个值计算一个键，并将这些键放入桶中。这样，当你遇到数据时，可以通过查看正确的桶来快速检索信息。Python 中的字典是哈希表实现，它们是键值存储的近亲。你将在本章的最后一个例子中遇到它们，当你在一个数据库中构建推荐系统时。哈希表在数据库中广泛用作索引，以实现快速信息检索。
- en: 4.2.3\. Selecting the right tools
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 选择合适的工具
- en: With the right class of algorithms and data structures in place, it’s time to
    choose the right tool for the job. The right tool can be a Python library or at
    least a tool that’s controlled from Python, as shown [figure 4.8](#ch04fig08).
    The number of helpful tools available is enormous, so we’ll look at only a handful
    of them.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当合适的算法和数据结构就位时，是时候选择合适的工具来完成这项工作了。合适的工具可以是 Python 库，或者至少是可以通过 Python 控制的工具，如图
    4.8 所示。可用的有用工具数量巨大，所以我们只看其中的一小部分。
- en: Figure 4.8\. Overview of tools that can be used when working with large data
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.8\. 在处理大量数据时可以使用的工具概述
- en: '![](Images/04fig08_alt.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/04fig08_alt.jpg)'
- en: Python tools
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Python 工具
- en: 'Python has a number of libraries that can help you deal with large data. They
    range from smarter data structures over code optimizers to just-in-time compilers.
    The following is a list of libraries we like to use when confronted with large
    data:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Python 有许多库可以帮助你处理大量数据。这些库从更智能的数据结构到代码优化器，再到即时编译器应有尽有。以下是我们面对大量数据时喜欢使用的库列表：
- en: '***Cython*** —The closer you get to the actual hardware of a computer, the
    more vital it is for the computer to know what types of data it has to process.
    For a computer, adding 1 + 1 is different from adding 1.00 + 1.00\. The first
    example consists of integers and the second consists of floats, and these calculations
    are performed by different parts of the CPU. In Python you don’t have to specify
    what data types you’re using, so the Python compiler has to infer them. But inferring
    data types is a slow operation and is partially why Python isn’t one of the fastest
    languages available. Cython, a superset of Python, solves this problem by forcing
    the programmer to specify the data type while developing the program. Once the
    compiler has this information, it runs programs much faster. See [http://cython.org/](http://cython.org/)
    for more information on Cython.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Cython*** —你越接近计算机的实际硬件，计算机知道它必须处理的数据类型就越重要。对于计算机来说，1 + 1 与 1.00 + 1.00
    的加法是不同的。第一个例子由整数组成，第二个例子由浮点数组成，这些计算由 CPU 的不同部分执行。在 Python 中，你不需要指定你使用的数据类型，因此
    Python 编译器必须推断它们。但是，推断数据类型是一个耗时的操作，这也是 Python 不是最快的语言之一的部分原因。Cython，Python 的超集，通过在开发程序时强制程序员指定数据类型来解决此问题。一旦编译器有了这些信息，它就会以更快的速度运行程序。有关
    Cython 的更多信息，请参阅 [http://cython.org/](http://cython.org/)。'
- en: '***Numexpr*** —Numexpr is at the core of many of the big data packages, as
    is NumPy for in-memory packages. Numexpr is a numerical expression evaluator for
    NumPy but can be many times faster than the original NumPy. To achieve this, it
    rewrites your expression and uses an internal (just-in-time) compiler. See [https://github.com/pydata/numexpr](https://github.com/pydata/numexpr)
    for details on Numexpr.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Numexpr*** —Numexpr 是许多大数据包的核心，就像 NumPy 对于内存中的包一样。Numexpr 是一个用于 NumPy 的数值表达式评估器，但可以比原始
    NumPy 快得多。为了实现这一点，它重写你的表达式并使用内部（即时）编译器。有关 Numexpr 的详细信息，请参阅 [https://github.com/pydata/numexpr](https://github.com/pydata/numexpr)。'
- en: '***Numba*** —Numba helps you to achieve greater speed by compiling your code
    right before you execute it, also known as *just-in-time compiling*. This gives
    you the advantage of writing high-level code but achieving speeds similar to those
    of C code. Using Numba is straightforward; see [http://numba.pydata.org/](http://numba.pydata.org/).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Numba*** —Numba 通过在执行前编译你的代码来帮助你实现更高的速度，这被称为 *即时编译*。这让你能够编写高级代码，但达到与 C 代码相似的速度。使用
    Numba 很简单；请参阅 [http://numba.pydata.org/](http://numba.pydata.org/) 了解更多信息。'
- en: '***Bcolz*** —Bcolz helps you overcome the out-of-memory problem that can occur
    when using NumPy. It can store and work with arrays in an optimal compressed form.
    It not only slims down your data need but also uses Numexpr in the background
    to reduce the calculations needed when performing calculations with bcolz arrays.
    See [http://bcolz.blosc.org/](http://bcolz.blosc.org/).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Bcolz*** —Bcolz 帮助你克服在使用 NumPy 时可能出现的内存不足问题。它可以以最优压缩形式存储和操作数组。它不仅减少了你的数据需求，还在后台使用
    Numexpr 来减少使用 bcolz 数组进行计算时所需的计算量。请参阅 [http://bcolz.blosc.org/](http://bcolz.blosc.org/)
    了解更多信息。'
- en: '***Blaze*** —Blaze is ideal if you want to use the power of a database backend
    but like the “Pythonic way” of working with data. Blaze will translate your Python
    code into SQL but can handle many more data stores than relational databases such
    as CSV, Spark, and others. Blaze delivers a unified way of working with many databases
    and data libraries. Blaze is still in development, though, so many features aren’t
    implemented yet. See [http://blaze.readthedocs.org/en/latest/index.html](http://blaze.readthedocs.org/en/latest/index.html).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Blaze*** —如果你想要使用数据库后端的力量，但又喜欢“Pythonic”的数据处理方式，Blaze 是理想的选择。Blaze 会将你的
    Python 代码转换为 SQL，但可以处理比关系数据库更多的数据存储，如 CSV、Spark 等。Blaze 提供了一种统一的方式来处理多个数据库和数据库。尽管如此，Blaze
    仍在开发中，因此许多功能尚未实现。请参阅 [http://blaze.readthedocs.org/en/latest/index.html](http://blaze.readthedocs.org/en/latest/index.html)
    了解更多信息。'
- en: '***Theano*** —Theano enables you to work directly with the graphical processing
    unit (GPU) and do symbolical simplifications whenever possible, and it comes with
    an excellent just-in-time compiler. On top of that it’s a great library for dealing
    with an advanced but useful mathematical concept: tensors. See [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Theano*** —Theano 允许你直接与图形处理单元 (GPU) 交互，并在可能的情况下进行符号化简化，它还附带了一个优秀的即时编译器。除此之外，它还是一个处理高级但有用的数学概念（张量）的出色库。请参阅
    [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)。'
- en: '***Dask*** —Dask enables you to optimize your flow of calculations and execute
    them efficiently. It also enables you to distribute calculations. See [http://dask.pydata.org/en/latest/](http://dask.pydata.org/en/latest/).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Dask*** — Dask 允许您优化计算流程并高效执行它们。它还允许您分发计算。请参阅 [http://dask.pydata.org/en/latest/](http://dask.pydata.org/en/latest/)。'
- en: These libraries are mostly about using Python itself for data processing (apart
    from Blaze, which also connects to databases). To achieve high-end performance,
    you can use Python to communicate with all sorts of databases or other software.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库主要关于使用 Python 本身进行数据处理（除了 Blaze，它还可以连接到数据库）。为了实现高端性能，您可以使用 Python 与各种数据库或其他软件进行通信。
- en: Use Python as a master to control other tools
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 Python 作为主控来控制其他工具
- en: Most software and tool producers support a Python interface to their software.
    This enables you to tap into specialized pieces of software with the ease and
    productivity that comes with Python. This way Python sets itself apart from other
    popular data science languages such as R and SAS. You should take advantage of
    this luxury and exploit the power of specialized tools to the fullest extent possible.
    [Chapter 6](kindle_split_014.xhtml#ch06) features a case study using Python to
    connect to a NoSQL database, as does [chapter 7](kindle_split_015.xhtml#ch07)
    with graph data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数软件和工具制造商都支持其软件的 Python 接口。这使得您能够轻松地利用 Python 的便捷性和生产力来访问专门的软件组件。这样，Python
    与其他流行的数据科学语言（如 R 和 SAS）区分开来。您应该充分利用这种奢侈，最大限度地发挥专用工具的威力。[第 6 章](kindle_split_014.xhtml#ch06)
    介绍了一个使用 Python 连接到 NoSQL 数据库的案例研究，[第 7 章](kindle_split_015.xhtml#ch07) 则介绍了与图数据相关的案例。
- en: Let’s now have a look at more general helpful tips when dealing with large data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下处理大数据时的一些更通用的有用提示。
- en: 4.3\. General programming tips for dealing with large data sets
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 处理大数据集的一般编程技巧
- en: The tricks that work in a general programming context still apply for data science.
    Several might be worded slightly differently, but the principles are essentially
    the same for all programmers. This section recapitulates those tricks that are
    important in a data science context.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般编程环境中有效的技巧在数据科学中仍然适用。其中一些可能措辞略有不同，但所有程序员的原则基本上是相同的。本节回顾了在数据科学环境中重要的这些技巧。
- en: 'You can divide the general tricks into three parts, as shown in the [figure
    4.9](#ch04fig09) mind map:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将一般技巧分为三个部分，如图 4.9 中的思维导图所示：
- en: Figure 4.9\. Overview of general programming best practices when working with
    large data
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.9\. 处理大数据时的一般编程最佳实践概述
- en: '![](Images/04fig09.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig09.jpg)'
- en: '***Don’t reinvent the wheel.*** Use tools and libraries developed by others.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***不要重复造轮子。*** 使用他人开发的工具和库。'
- en: '***Get the most out of your hardware.*** Your machine is never used to its
    full potential; with simple adaptions you can make it work harder.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***充分利用您的硬件。*** 您的机器从未被充分利用；通过简单的适配，您可以使其更努力地工作。'
- en: '***Reduce the computing need.*** Slim down your memory and processing needs
    as much as possible.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***减少计算需求。*** 尽可能地精简您的内存和处理需求。'
- en: “[Don’t reinvent the wheel](#ch04lev2sec4)” is easier said than done when confronted
    with a specific problem, but your first thought should always be, ‘Somebody else
    must have encountered this same problem before me.’
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对具体问题时，“[不要重复造轮子](#ch04lev2sec4)”可能说起来容易做起来难，但您的第一个想法始终应该是，“肯定有人在我之前遇到过这个问题。”
- en: 4.3.1\. Don’t reinvent the wheel
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 不要重复造轮子
- en: '“Don’t repeat anyone” is probably even better than “don’t repeat yourself.”
    Add value with your actions: make sure that they matter. Solving a problem that
    has already been solved is a waste of time. As a data scientist, you have two
    large rules that can help you deal with large data and make you much more productive,
    to boot:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: “不要重复别人的工作”可能比“不要重复自己”更好。通过您的行动增加价值：确保它们有意义。解决已经解决的问题是一种浪费时间的行为。作为一名数据科学家，您有两个重要的规则可以帮助您处理大数据并使您的工作效率更高：
- en: '***Exploit the power of databases.*** The first reaction most data scientists
    have when working with large data sets is to prepare their analytical base tables
    inside a database. This method works well when the features you want to prepare
    are fairly simple. When this preparation involves advanced modeling, find out
    if it’s possible to employ user-defined functions and procedures. The last example
    of this chapter is on integrating a database into your workflow.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***利用数据库的力量。*** 当数据科学家处理大数据集时，他们大多数人的第一反应是在数据库内部准备他们的分析基础表。当你要准备的特征相对简单时，这种方法效果很好。当这种准备涉及到高级建模时，找出是否可以采用用户定义的函数和过程。本章的最后一个例子是关于将数据库集成到你的工作流程中。'
- en: '***Use optimized libraries.*** Creating libraries like Mahout, Weka, and other
    machine-learning algorithms requires time and knowledge. They are highly optimized
    and incorporate best practices and state-of-the art technologies. Spend your time
    on getting things done, not on reinventing and repeating others people’s efforts,
    unless it’s for the sake of understanding how things work.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***使用优化库。*** 创建像Mahout、Weka和其他机器学习算法这样的库需要时间和知识。它们高度优化，并融合了最佳实践和最先进的技术。把时间花在完成任务上，而不是重复别人的工作，除非是为了理解事物是如何工作的。'
- en: Then you must consider your hardware limitation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你必须考虑你的硬件限制。
- en: 4.3.2\. Get the most out of your hardware
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 充分利用你的硬件
- en: 'Resources on a computer can be idle, whereas other resources are over-utilized.
    This slows down programs and can even make them fail. Sometimes it’s possible
    (and necessary) to shift the workload from an overtaxed resource to an underutilized
    resource using the following techniques:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机上的资源可能处于闲置状态，而其他资源则过度使用。这会减慢程序的速度，甚至可能导致它们失败。有时（并且是必要的）可以使用以下技术将工作负载从过度使用的资源转移到未充分利用的资源：
- en: '***Feed the CPU compressed data.*** A simple trick to avoid CPU starvation
    is to feed the CPU compressed data instead of the inflated (raw) data. This will
    shift more work from the hard disk to the CPU, which is exactly what you want
    to do, because a hard disk can’t follow the CPU in most modern computer architectures.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***向CPU提供压缩数据。*** 避免CPU饥饿的一个简单技巧是向CPU提供压缩数据而不是膨胀（原始）数据。这将使更多的工作从硬盘转移到CPU，这正是你想要的，因为在大多数现代计算机架构中，硬盘无法跟上CPU的速度。'
- en: '***Make use of the GPU.*** Sometimes your CPU and not your memory is the bottleneck.
    If your computations are parallelizable, you can benefit from switching to the
    GPU. This has a much higher throughput for computations than a CPU. The GPU is
    enormously efficient in parallelizable jobs but has less cache than the CPU. But
    it’s pointless to switch to the GPU when your hard disk is the problem. Several
    Python packages, such as Theano and NumbaPro, will use the GPU without much programming
    effort. If this doesn’t suffice, you can use a CUDA (Compute Unified Device Architecture)
    package such as PyCUDA. It’s also a well-known trick in bitcoin mining, if you’re
    interested in creating your own money.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***利用GPU的优势。*** 有时候，瓶颈不是你的内存，而是CPU。如果你的计算可以并行化，你可以从切换到GPU中受益。GPU在计算方面比CPU有更高的吞吐量。GPU在并行化工作中非常高效，但它的缓存比CPU少。但是，如果你的问题是硬盘，切换到GPU是没有意义的。几个Python包，如Theano和NumbaPro，可以在不费太多编程努力的情况下使用GPU。如果这还不够，你可以使用CUDA（计算统一设备架构）包，如PyCUDA。如果你对创建自己的货币感兴趣，这也是比特币挖矿中的一个众所周知的小技巧。'
- en: '***Use multiple threads.*** It’s still possible to parallelize computations
    on your CPU. You can achieve this with normal Python threads.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***使用多线程。*** 在你的CPU上并行化计算仍然是可能的。你可以通过正常的Python线程来实现这一点。'
- en: 4.3.3\. Reduce your computing needs
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 减少你的计算需求
- en: '“Working smart + hard = achievement.” This also applies to the programs you
    write. The best way to avoid having large data problems is by removing as much
    of the work as possible up front and letting the computer work only on the part
    that can’t be skipped. The following list contains methods to help you achieve
    this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: “聪明工作 + 努力工作 = 成就。” 这同样适用于你编写的程序。避免出现大量数据问题的最佳方式是在一开始就尽可能多地移除工作，并让计算机只处理那些无法跳过的部分。以下列表包含了一些帮助你实现这一目标的方法：
- en: '***Profile your code and remediate slow pieces of code.*** Not every piece
    of your code needs to be optimized; use a profiler to detect slow parts inside
    your program and remediate these parts.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分析你的代码并修复慢速代码部分。*** 并非你代码的每一部分都需要优化；使用分析器来检测程序中的慢速部分，并修复这些部分。'
- en: '***Use compiled code whenever possible, certainly when loops are involved.***
    Whenever possible use functions from packages that are optimized for numerical
    computations instead of implementing everything yourself. The code in these packages
    is often highly optimized and compiled.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***尽可能使用编译代码，特别是在涉及循环时。*** 无论如何可能，使用针对数值计算进行优化的包中的函数，而不是自己实现所有内容。这些包中的代码通常高度优化且已编译。'
- en: '***Otherwise, compile the code yourself.*** If you can’t use an existing package,
    use either a just-in-time compiler or implement the slowest parts of your code
    in a lower-level language such as C or Fortran and integrate this with your codebase.
    If you make the step to *lower-level languages* (languages that are closer to
    the universal computer bytecode), learn to work with computational libraries such
    as LAPACK, BLAST, Intel MKL, and ATLAS. These are highly optimized, and it’s difficult
    to achieve similar performance to them.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***否则，自己编译代码。*** 如果你不能使用现有的包，可以使用即时编译器，或者将代码中的最慢部分用 C 或 Fortran 等底层语言实现，并将其与代码库集成。如果你决定转向
    *底层语言*（更接近通用计算机字节码的语言），学习使用计算库，如 LAPACK、BLAST、Intel MKL 和 ATLAS。这些库高度优化，难以达到它们的性能水平。'
- en: '***Avoid pulling data into memory.*** When you work with data that doesn’t
    fit in your memory, avoid pulling everything into memory. A simple way of doing
    this is by reading data in chunks and parsing the data on the fly. This won’t
    work on every algorithm but enables calculations on extremely large data sets.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***避免将数据拉入内存。*** 当你处理无法装入内存的数据时，避免将所有数据都拉入内存。这样做的一个简单方法是通过分块读取数据并在飞行中解析数据。这并不适用于每个算法，但可以使对极其大型数据集的计算成为可能。'
- en: '***Use generators to avoid intermediate data storage.*** Generators help you
    return data per observation instead of in batches. This way you avoid storing
    intermediate results.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***使用生成器来避免中间数据存储。*** 生成器帮助你按观测值返回数据，而不是批量返回。这样，你就可以避免存储中间结果。'
- en: '***Use as little data as possible.*** If no large-scale algorithm is available
    and you aren’t willing to implement such a technique yourself, then you can still
    train your data on only a sample of the original data.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***尽可能少地使用数据。*** 如果没有可用的大规模算法，并且你不愿意自己实现此类技术，那么你仍然可以在原始数据的一个样本上训练你的数据。'
- en: '***Use your math skills to simplify calculations as much as possible.*** Take
    the following equation, for example: (*a* + *b*)² = *a*² + 2*ab* + *b*². The left
    side will be computed much faster than the right side of the equation; even for
    this trivial example, it could make a difference when talking about big chunks
    of data.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***尽可能使用你的数学技能来简化计算。*** 以以下方程为例：（*a* + *b*）² = *a*² + 2*ab* + *b*²。方程的左侧将比方程的右侧计算得更快；即使对于这个简单的例子，在处理大量数据时也可能有所区别。'
- en: '4.4\. Case study 1: Predicting malicious URLs'
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 案例研究 1：预测恶意 URL
- en: The internet is probably one of the greatest inventions of modern times. It
    has boosted humanity’s development, but not everyone uses this great invention
    with honorable intentions. Many companies (Google, for one) try to protect us
    from fraud by detecting malicious websites for us. Doing so is no easy task, because
    the internet has billions of web pages to scan. In this case study we’ll show
    how to work with a data set that no longer fits in memory.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网可能是现代最伟大的发明之一。它推动了人类的发展，但并非每个人都怀着崇高的意图使用这一伟大发明。许多公司（例如 Google）试图通过为我们检测恶意网站来保护我们免受欺诈。这样做并不容易，因为互联网上有数十亿个网页需要扫描。在本案例研究中，我们将展示如何处理不再适合内存的数据集。
- en: What we’ll use
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用
- en: '***Data*** —The data in this case study was made available as part of a research
    project. The project contains data from 120 days, and each observation has approximately
    3,200,000 features. The target variable contains 1 if it’s a malicious website
    and -1 otherwise. For more information, please see “Beyond Blacklists: Learning
    to Detect Malicious Web Sites from Suspicious URLs.”^([[2](#ch04fn02)])'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据*** — 本案例研究中的数据作为研究项目的一部分提供。该项目包含 120 天的数据，每个观测值大约有 3,200,000 个特征。目标变量包含
    1 表示恶意网站，否则为 -1。更多信息，请参阅“超越黑名单：从可疑 URL 中学习检测恶意网站。”^([[2](#ch04fn02)])'
- en: ²
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Justin Ma, Lawrence K. Saul, Stefan Savage, and Geoffrey M. Voelker, “Beyond
    Blacklists: Learning to Detect Malicious Web Sites from Suspicious URLs,” Proceedings
    of the ACM SIGKDD Conference, Paris (June 2009), 1245–53.'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Justin Ma, Lawrence K. Saul, Stefan Savage, and Geoffrey M. Voelker, “Beyond
    Blacklists: Learning to Detect Malicious Web Sites from Suspicious URLs,” Proceedings
    of the ACM SIGKDD Conference, Paris (June 2009), 1245–53.'
- en: '***The Scikit-learn library*** —You should have this library installed in your
    Python environment at this point, because we used it in the previous chapter.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Scikit-learn 库*** — 到目前为止，你应该在你的 Python 环境中安装了这个库，因为我们已经在上一章中使用过它了。'
- en: As you can see, we won’t be needing much for this case, so let’s dive into it.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，对于这个案例，我们不需要太多，让我们深入探讨。
- en: '4.4.1\. Step 1: Defining the research goal'
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 第 1 步：定义研究目标
- en: The goal of our project is to detect whether certain URLs can be trusted or
    not. Because the data is so large we aim to do this in a memory-friendly way.
    In the next step we’ll first look at what happens if we don’t concern ourselves
    with memory (RAM) issues.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们项目的目标是检测某些 URL 是否可信。由于数据量很大，我们旨在以内存友好的方式完成这项工作。在下一步中，我们将首先看看如果我们不关心内存（RAM）问题会发生什么。
- en: '4.4.2\. Step 2: Acquiring the URL data'
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 第 2 步：获取 URL 数据
- en: Start by downloading the data from [http://sysnet.ucsd.edu/projects/url/#datasets](http://sysnet.ucsd.edu/projects/url/#datasets)
    and place it in a folder. Choose the data in SVMLight format. SVMLight is a text-based
    format with one observation per row. To save space, it leaves out the zeros.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从 [http://sysnet.ucsd.edu/projects/url/#datasets](http://sysnet.ucsd.edu/projects/url/#datasets)
    下载数据并将其放置在一个文件夹中。选择 SVMLight 格式的数据。SVMLight 是一种基于文本的格式，每行一个观测值。为了节省空间，它省略了零。
- en: The following listing and [figure 4.10](#ch04fig10) show what happens when you
    try to read in 1 file out of the 120 and create the normal matrix as most algorithms
    expect. The `todense()` method changes the data from a special file format to
    a normal matrix where every entry contains a value.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表和[图 4.10](#ch04fig10) 展示了当你尝试读取 120 个文件中的 1 个并创建算法期望的正常矩阵时会发生什么。`todense()`
    方法将数据从特殊文件格式转换为包含每个条目值的正常矩阵。
- en: Figure 4.10\. Memory error when trying to take a large data set into memory
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.10\. 尝试将大量数据集加载到内存时出现的内存错误
- en: '![](Images/04fig10_alt.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片 04fig10_alt](Images/04fig10_alt.jpg)'
- en: Listing 4.5\. Generating an out-of-memory error
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.5\. 生成内存不足错误
- en: '![](Images/104fig01_alt.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片 104fig01_alt](Images/104fig01_alt.jpg)'
- en: Surprise, surprise, we get an out-of-memory error. That is, unless you run this
    code on a huge machine. After a few tricks you’ll no longer run into these memory
    problems and will detect 97% of the malicious sites.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 惊讶，惊讶，我们得到了一个内存不足错误。除非你在超级大的机器上运行此代码。经过一些技巧后，你将不再遇到这些内存问题，并将检测到 97% 的恶意网站。
- en: Tools and techniques
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 工具和技术
- en: 'We ran into a memory error while loading a single file—still 119 to go. Luckily,
    we have a few tricks up our sleeve. Let’s try these techniques over the course
    of the case study:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载单个文件时遇到了内存错误——还有 119 个文件要加载。幸运的是，我们有一些技巧。让我们在案例研究中尝试这些技术：
- en: Use a sparse representation of data.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据的稀疏表示。
- en: Feed the algorithm compressed data instead of raw data.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用压缩数据而不是原始数据喂给算法。
- en: Use an online algorithm to make predictions.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在线算法进行预测。
- en: We’ll go deeper into each “trick” when we get to use it. Now that we have our
    data locally, let’s access it. Step 3 of the data science process, data preparation
    and cleansing, isn’t necessary in this case because the URLs come pre-cleaned.
    We’ll need a form of exploration before unleashing our learning algorithm, though.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用它时，我们将更深入地探讨每个“技巧”。现在我们已经将数据本地化了，让我们访问它。数据科学流程的第 3 步，数据准备和清洗，在这个案例中不是必要的，因为
    URL 已经预先清洗过。然而，在我们释放学习算法之前，我们需要进行某种形式的探索。
- en: '4.4.3\. Step 4: Data exploration'
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 第 4 步：数据探索
- en: 'To see if we can even apply our first trick (sparse representation), we need
    to find out whether the data does indeed contain lots of zeros. We can check this
    with the following piece of code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看我们是否可以应用我们的第一个技巧（稀疏表示），我们需要找出数据是否确实包含大量零。我们可以使用以下代码片段进行检查：
- en: '[PRE5]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This outputs the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出以下内容：
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Data that contains little information compared to zeros is called *sparse data*.
    This can be saved more compactly if you store the data as `[(0,0,1),(4,4,1)]`
    instead of
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 与零相比信息量小的数据称为 *稀疏数据*。如果将数据存储为 `[(0,0,1),(4,4,1)]` 而不是
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: One of the file formats that implements this is SVMLight, and that’s exactly
    why we downloaded the data in this format. We’re not finished yet, though, because
    we need to get a feel of the dimensions within the data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的文件格式之一是 SVMLight，这正是我们下载此格式数据的原因。尽管如此，我们还没有完成，因为我们还需要了解数据内部的维度。
- en: To get this information we already need to keep the data compressed while checking
    for the maximum number of observations and variables. We also need to *read in
    data file by file*. This way you consume even less memory. A second trick is to
    feed the CPU compressed files. In our example, it’s already packed in the tar.gz
    format. You unpack a file only when you need it, without writing it to the hard
    disk (the slowest part of your computer).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取这些信息，我们已经在检查最大观测值和变量数时保持了数据压缩。我们还需要*逐个读取数据文件*。这样你消耗的内存会更少。第二个技巧是向CPU提供压缩文件。在我们的例子中，它已经打包在tar.gz格式中。你只有在需要时才解压文件，而不会将其写入硬盘（你电脑中最慢的部分）。
- en: For our example, shown in [listing 4.6](#ch04ex06), we’ll only work on the first
    5 files, but feel free to use all of them.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，[列表4.6](#ch04ex06)中，我们只会在前5个文件上工作，但你可以自由地使用所有文件。
- en: Listing 4.6\. Checking data size
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.6. 检查数据大小
- en: '![](Images/ch04ex06-0.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex06-0.jpg)'
- en: '![](Images/ch04ex06-1.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex06-1.jpg)'
- en: Part of the code needs some extra explanation. In this code we loop through
    the svm files inside the tar archive. We unpack the files one by one to reduce
    the memory needed. As these files are in the SVM format, we use a helper, `functionload_svmlight_file()`
    to load a specific file. Then we can see how many observations and variables the
    file has by checking the shape of the resulting data set.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 部分代码需要一些额外的解释。在这个代码中，我们遍历tar归档文件内的svm文件。我们逐个解压文件以减少所需的内存。由于这些文件是SVM格式，我们使用辅助函数`functionload_svmlight_file()`来加载特定文件。然后我们可以通过检查结果数据集的形状来查看文件包含多少观测值和变量。
- en: Armed with this information we can move on to model building.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些信息，我们可以继续进行模型构建。
- en: '4.4.4\. Step 5: Model building'
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4. 步骤 5：模型构建
- en: Now that we’re aware of the dimensions of our data, we can apply the same two
    tricks (sparse representation of compressed file) and add the third (using an
    online algorithm), in the following listing. Let’s find those harmful websites!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了数据的维度，我们可以在以下列表中应用相同的两个技巧（压缩文件的稀疏表示）并添加第三个（使用在线算法）。让我们找到那些有害网站！
- en: Listing 4.7\. Creating a model to distinguish the malicious from the normal
    URLs
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.7. 创建一个模型来区分恶意URL和正常URL
- en: '![](Images/ch04ex07-0.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex07-0.jpg)'
- en: '![](Images/ch04ex07-1.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex07-1.jpg)'
- en: The code in the previous listing looks fairly similar to what we did before,
    apart from the stochastic gradient descent classifier `SGDClassifier().`
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 之前列表中的代码看起来与我们之前所做的相当相似，除了随机梯度下降分类器`SGDClassifier()`。
- en: Here, we trained the algorithm iteratively by presenting the observations in
    one file with the `partial_fit()` function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过使用`partial_fit()`函数在一个文件中呈现观测值来迭代训练算法。
- en: 'Looping through only the first 5 files here gives the output shown in [table
    4.1](#ch04table01). The table shows classification diagnostic measures: precision,
    recall, F1-score, and support.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 只遍历前5个文件在这里给出了[表4.1](#ch04table01)中显示的输出。该表显示了分类诊断指标：精确度、召回率、F1分数和支持度。
- en: 'Table 4.1\. Classification problem: Can a website be trusted or not?'
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.1. 分类问题：网站是否可信？
- en: '|   | precision | recall | f1-score | support |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|   | 精确度 | 召回率 | F1分数 | 支持度 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| -1 | 0.97 | 0.99 | 0.98 | 14045 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| -1 | 0.97 | 0.99 | 0.98 | 14045 |'
- en: '| 1 | 0.97 | 0.94 | 0.96 | 5955 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.97 | 0.94 | 0.96 | 5955 |'
- en: '| avg/total | 0.97 | 0.97 | 0.97 | 20000 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 平均/总计 | 0.97 | 0.97 | 0.97 | 20000 |'
- en: Only 3% (1 - 0.97) of the malicious sites aren’t detected (*precision*), and
    6% (1 - 0.94) of the sites detected are falsely accused (*recall*). This is a
    decent result, so we can conclude that the methodology works. If we rerun the
    analysis, the result might be slightly different, because the algorithm could
    converge slightly differently. If you don’t mind waiting a while, you can go for
    the full data set. You can now handle all the data without problems. We won’t
    have a sixth step (presentation or automation) in this case study.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 只有3%（1 - 0.97）的恶意网站没有被检测到（*精确度*），6%（1 - 0.94）的检测到的网站被错误地指控（*召回率*）。这是一个不错的结果，因此我们可以得出结论，该方法有效。如果我们重新运行分析，结果可能会有所不同，因为算法可能会收敛得略有不同。如果你不介意等待一会儿，你可以使用完整的数据集。你现在可以处理所有数据而不会出现问题。在这种情况下研究，我们不会有第六步（展示或自动化）。
- en: 'Now let’s look at a second application of our techniques; this time you’ll
    build a recommender system inside a database. For a well-known example of recommender
    systems visit the Amazon website. While browsing, you’ll soon be confronted with
    recommendations: “People who bought this product also bought...”'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们技术的第二个应用；这次你将在数据库内构建一个推荐系统。要了解推荐系统的知名例子，请访问亚马逊网站。在浏览时，你很快就会遇到推荐：“购买此产品的用户还购买了...”
- en: '4.5\. Case study 2: Building a recommender system inside a database'
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 案例研究2：在数据库内构建推荐系统
- en: In reality most of the data you work with is stored in a relational database,
    but most databases aren’t suitable for data mining. But as shown in this example,
    it’s possible to adapt our techniques so you can do a large part of the analysis
    inside the database itself, thereby profiting from the database’s query optimizer,
    which will optimize the code for you. In this example we’ll go into how to use
    the hash table data structure and how to use Python to control other tools.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，你处理的大部分数据都存储在关系型数据库中，但大多数数据库并不适合数据挖掘。但正如这个例子所示，我们可以调整我们的技术，以便你可以在数据库内部进行大部分分析，从而利用数据库的查询优化器为你优化代码。在这个例子中，我们将探讨如何使用哈希表数据结构以及如何使用Python控制其他工具。
- en: 4.5.1\. Tools and techniques needed
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1. 需要的工具和技术
- en: Before going into the case study we need to have a quick look at the required
    tools and theoretical background to what we’re about to do here.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入案例研究之前，我们需要快速查看所需的工具和即将进行的操作的理论背景。
- en: Tools
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 工具
- en: '***MySQL database*** —Needs a MySQL database to work with. If you haven’t installed
    a MySQL community server, you can download one from [www.mysql.com](http://www.mysql.com).
    [Appendix C](kindle_split_020.xhtml#app03): “Installing a MySQL server” explains
    how to set it up.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MySQL数据库**——需要MySQL数据库来工作。如果你还没有安装MySQL社区服务器，你可以从[www.mysql.com](http://www.mysql.com)下载一个。[附录C](kindle_split_020.xhtml#app03)：“安装MySQL服务器”解释了如何设置它。'
- en: '***MySQL database connection Python library*** —To connect to this server from
    Python you’ll also need to install SQLAlchemy or another library capable of communicating
    with MySQL. We’re using MySQLdb. On Windows you can’t use Conda right off the
    bat to install it. First install Binstar (another package management service)
    and look for the appropriate mysql-python package for your Python setup.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MySQL数据库连接Python库**——要从Python连接到这个服务器，你还需要安装SQLAlchemy或其他能够与MySQL通信的库。我们使用MySQLdb。在Windows上，你不能直接使用Conda来安装它。首先安装Binstar（另一个包管理服务），然后寻找适合你的Python设置的适当mysql-python包。'
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following command entered into the Windows command line worked for us (after
    activating the Python environment):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows命令行中输入的以下命令对我们有效（在激活Python环境之后）：
- en: '[PRE9]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Again, feel free to go for the SQLAlchemy library if that’s something you’re
    more comfortable with.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，如果你更熟悉SQLAlchemy库，请随意使用。
- en: We will also need the *pandas* python library, but that should already be installed
    by now.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还需要*pandas* Python库，但到现在应该已经安装好了。
- en: With the infrastructure in place, let’s dive into a few of the techniques.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施就绪后，让我们深入探讨一些技术。
- en: Techniques
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 技术
- en: A simple recommender system will look for customers who’ve rented similar movies
    as you have and then suggest those that the others have watched but you haven’t
    seen yet. This technique is called *k-nearest neighbors* in machine learning.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的推荐系统会寻找与你租借过类似电影的用户，然后推荐那些其他人看过但你还没有看过的电影。在机器学习中，这种技术被称为**k-最近邻**。
- en: A customer who behaves similarly to you isn’t necessarily *the* most similar
    customer. You’ll use a technique to ensure that you can find similar customers
    (local optima) without guarantees that you’ve found the best customer (global
    optimum). A common technique used to solve this is called *Locality-Sensitive
    Hashing*. A good overview of papers on this topic can be found at [http://www.mit.edu/~andoni/LSH/](http://www.mit.edu/~andoni/LSH/).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一个行为与你相似的客户并不一定是**最相似**的客户。你需要使用一种技术来确保你能够找到相似的客户（局部最优解），而不保证你已经找到了最佳客户（全局最优解）。解决这个问题的常用技术被称为**局部敏感哈希**。关于这个主题的论文概述可以在[http://www.mit.edu/~andoni/LSH/](http://www.mit.edu/~andoni/LSH/)找到。
- en: 'The idea behind Locality-Sensitive Hashing is simple: Construct functions that
    map similar customers close together (they’re put in a bucket with the same label)
    and make sure that objects that are different are put in different buckets.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本地敏感哈希背后的思想很简单：构建函数，将相似的客户映射到一起（它们被放入具有相同标签的桶中），并确保不同的对象被放入不同的桶中。
- en: 'Central to this idea is a function that performs the mapping. This function
    is called a hash function: a function that maps any range of input to a fixed
    output. The simplest hash function concatenates the values from several random
    columns. It doesn’t matter how many columns (scalable input); it brings it back
    to a single column (fixed output).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法的核心是一个执行映射的函数。这个函数被称为哈希函数：一个将任何输入范围映射到固定输出的函数。最简单的哈希函数将来自几个随机列的值连接起来。列的数量（可扩展输入）无关紧要；它将其还原为单个列（固定输出）。
- en: 'You’ll set up three hash functions to find similar customers. The three functions
    take the values of three movies:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你将设置三个哈希函数来找到相似的客户。这三个函数取三部电影的值：
- en: The first function takes the values of movies 10, 15, and 28.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个函数取电影10、15和28的值。
- en: The second function takes the values of movies 7, 18, and 22.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个函数取电影7、18和22的值。
- en: The last function takes the values of movies 16, 19, and 30.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个函数取电影16、19和30的值。
- en: This will ensure that the customers who are in the same bucket share at least
    several movies. But the customers inside one bucket might still differ on the
    movies that weren’t included in the hashing functions. To solve this you still
    need to compare the customers within the bucket with each other. For this you
    need to create a new distance measure.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保处于同一桶的客户至少共享几部电影。但桶内的客户可能在未包含在哈希函数中的电影上有所不同。为了解决这个问题，你仍然需要比较桶内的客户。为此，你需要创建一个新的距离度量。
- en: The distance that you’ll use to compare customers is called the hamming distance.
    The hamming distance is used to calculate how much two strings differ. The distance
    is defined as the number of different characters in a string. [Table 4.2](#ch04table02)
    offers a few examples of the hamming distance.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你将用来比较客户的距离称为汉明距离。汉明距离用于计算两个字符串的差异程度。距离定义为字符串中不同字符的数量。[表4.2](#ch04table02)提供了汉明距离的一些示例。
- en: Table 4.2\. Examples of calculating the hamming distance
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.2\. 计算汉明距离的示例
- en: '| String 1 | String 2 | Hamming distance |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 字符串1 | 字符串2 | 汉明距离 |'
- en: '| --- | --- | --- |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Hat | Cat | 1 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 帽子 | 猫 | 1 |'
- en: '| Hat | Mad | 2 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 帽子 | 狂野 | 2 |'
- en: '| Tiger | Tigre | 2 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 老虎 | 狮子 | 2 |'
- en: '| Paris | Rome | 5 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 巴黎 | 罗马 | 5 |'
- en: Comparing multiple columns is an expensive operation, so you’ll need a trick
    to speed this up. Because the columns contain a binary (0 or 1) variable to indicate
    whether a customer has bought a movie or not, you can concatenate the information
    so that the same information is contained in a new column. [Table 4.3](#ch04table03)
    shows the “movies” variable that contains as much information as all the movie
    columns combined.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 比较多个列是一个昂贵的操作，所以你需要一个技巧来加快这个过程。因为列包含一个二进制（0或1）变量来指示客户是否购买过电影，你可以将信息连接起来，使得相同的信息包含在一个新的列中。[表4.3](#ch04table03)显示了包含所有电影列信息的“movies”变量。
- en: 'Table 4.3\. Combining the information from different columns into the movies
    column. This is also how DNA works: all information in a long string.'
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.3\. 将不同列的信息合并到movies列中。这也是DNA的工作方式：长字符串中的所有信息。
- en: '| Column 1 | Movie 1 | Movie 2 | Movie 3 | Movie 4 | movies |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 列1 | 电影1 | 电影2 | 电影3 | 电影4 | movies |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Customer 1 | 1 | 0 | 1 | 1 | 1011 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 客户1 | 1 | 0 | 1 | 1 | 1011 |'
- en: '| Customer 2 | 0 | 0 | 0 | 1 | 0001 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 客户2 | 0 | 0 | 0 | 1 | 0001 |'
- en: 'This allows you to calculate the hamming distance much more efficiently. By
    handling this operator as a bit, you can exploit the XOR operator. The outcome
    of the XOR operator (`^`) is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许你更有效地计算汉明距离。通过将此操作符作为位来处理，你可以利用XOR操作符。XOR操作符（`^`）的结果如下：
- en: '[PRE10]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With this in place, the process to find similar customers becomes very simple.
    Let’s first look at it in pseudo code:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，找到相似客户的过程变得非常简单。让我们首先用伪代码来看一下：
- en: '*Preprocessing*:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*预处理*:'
- en: '**1**.  Define *p* (for instance, 3) functions that select *k* (for instance,
    3) entries from the vector of movies. Here we take 3 functions (p) that each take
    3 (k) movies.'
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 定义 *p*（例如，3）个函数，从电影向量中选择 *k*（例如，3）个条目。这里我们取3个函数（p），每个函数取3部电影。'
- en: '**2**.  Apply these functions to every point and store them in a separate column.
    (In literature each function is called a hash function and each column will store
    a bucket.)'
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  将这些函数应用于每个点，并将它们存储在单独的列中。（在文献中，每个函数被称为哈希函数，每个列将存储一个桶。）'
- en: '*Querying point q:*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '*查询点q：*'
- en: '**1**.  Apply the same p functions to the point (observation) q you want to
    query.'
  id: totrans-247
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  将相同的p函数应用于你想要查询的点（观察）q。'
- en: '**2**.  Retrieve for every function the points that correspond to the result
    in the corresponding bucket.'
  id: totrans-248
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  为每个函数检索对应桶中结果的点。'
- en: ''
  id: totrans-249
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Stop when you’ve retrieved all the points in the buckets or reached 2p points
    (for example 10 if you have 5 functions).
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当你检索到桶中的所有点或达到2p点（例如，如果你有5个函数，则为10）时停止。
- en: '**3**.  Calculate the distance for each point and return the points with the
    minimum distance.'
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  计算每个点的距离，并返回距离最小的点。'
- en: Let’s look at an actual implementation in Python to make this all clearer.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Python中的一个实际实现，以使这一切更加清晰。
- en: '4.5.2\. Step 1: Research question'
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2\. 第1步：研究问题
- en: Let’s say you’re working in a video store and the manager asks you if it’s possible
    to use the information on what movies people rent to predict what other movies
    they might like. Your boss has stored the data in a MySQL database, and it’s up
    to you to do the analysis. What he is referring to is a recommender system, an
    automated system that learns people’s preferences and recommends movies and other
    products the customers haven’t tried yet. The goal of our case study is to create
    a memory-friendly recommender system. We’ll achieve this using a database and
    a few extra tricks. We’re going to create the data ourselves for this case study
    so we can skip the data retrieval step and move right into data preparation. And
    after that we can skip the data exploration step and move straight into model
    building.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一个录像店工作，经理问你是否可以使用人们租借电影的信息来预测他们可能喜欢其他哪些电影。您的老板已经将这些数据存储在MySQL数据库中，您需要进行分析。他所指的是推荐系统，这是一种自动学习人们偏好的系统，并为尚未尝试的客户推荐电影和其他产品。我们案例研究的目的是创建一个易于记忆的推荐系统。我们将使用数据库和一些额外的技巧来实现这一点。为了这个案例研究，我们将自己创建数据，这样我们就可以跳过数据检索步骤，直接进入数据准备阶段。然后我们可以跳过数据探索步骤，直接进入模型构建阶段。
- en: '4.5.3\. Step 3: Data preparation'
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.3\. 第3步：数据准备
- en: The data your boss has collected is shown in [table 4.4](#ch04table04). We’ll
    create this data ourselves for the sake of demonstration.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 您的老板收集的数据显示在[表4.4](#ch04table04)中。为了演示，我们将自己创建这些数据。
- en: Table 4.4\. Excerpt from the client database and the movies customers rented
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.4\. 客户数据库和客户租借的电影摘要
- en: '| Customer | Movie 1 | Movie 2 | Movie 3 | ... | Movie 32 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 客户 | 电影1 | 电影2 | 电影3 | ... | 电影32 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Jack Dani | 1 | 0 | 0 |   | 1 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Jack Dani | 1 | 0 | 0 |   | 1 |'
- en: '| Wilhelmson | 1 | 1 | 0 |   | 1 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Wilhelmson | 1 | 1 | 0 |   | 1 |'
- en: '| ... |   |   |   |   |   |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| ... |   |   |   |   |   |'
- en: '| Jane Dane | 0 | 0 | 1 |   | 0 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Jane Dane | 0 | 0 | 1 |   | 0 |'
- en: '| Xi Liu | 0 | 0 | 0 |   | 1 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Xi Liu | 0 | 0 | 0 |   | 1 |'
- en: '| Eros Mazo | 1 | 1 | 0 |   | 1 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Eros Mazo | 1 | 1 | 0 |   | 1 |'
- en: '| ... |   |   |   |   |   |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| ... |   |   |   |   |   |'
- en: For each customer you get an indication of whether they’ve rented the movie
    before (1) or not (0). Let’s see what else you’ll need so you can give your boss
    the recommender system he desires.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每位客户，你都会得到一个指示，表明他们之前是否租过这部电影（1）或者没有（0）。让我们看看你还需要什么，这样你就可以给你的老板提供他想要的推荐系统。
- en: First let’s connect Python to MySQL to create our data. Make a connection to
    MySQL using your username and password. In the following listing we used a database
    called “test”. Replace the user, password, and database name with the appropriate
    values for your setup and retrieve the connection and the cursor. A database cursor
    is a control structure that remembers where you are currently in the database.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将Python连接到MySQL来创建我们的数据。使用您的用户名和密码连接到MySQL。在下面的列表中，我们使用了一个名为“test”的数据库。将用户、密码和数据库名称替换为您的设置中的相应值，并检索连接和游标。数据库游标是一种控制结构，它记得你在数据库中的当前位置。
- en: Listing 4.8\. Creating customers in the database
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.8\. 在数据库中创建客户
- en: '![](Images/112fig01_alt.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/112fig01_alt.jpg)'
- en: 'We create 100 customers and randomly assign whether they did or didn’t see
    a certain movie, and we have 32 movies in total. The data is first created in
    a Pandas data frame but is then turned into SQL code. Note: You might run across
    a warning when running this code. The warning states: *The “mysql” flavor with
    DBAPI connection is deprecated and will be removed in future versions. MySQL will
    be further supported with SQLAlchemy engines*. Feel free to already switch to
    SQLAlchemy or another library. We’ll use SQLAlchemy in other chapters, but used
    MySQLdb here to broaden the examples.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了100个客户，并随机分配他们是否观看过某些电影，总共有32部电影。数据最初是在Pandas数据框中创建的，但随后被转换成SQL代码。注意：运行此代码时可能会遇到警告。警告指出：“带有DBAPI连接的“mysql”风味已被弃用，将在未来版本中删除。MySQL将通过SQLAlchemy引擎进一步支持”。您可以随时切换到SQLAlchemy或其他库。我们将在其他章节中使用SQLAlchemy，但在这里使用MySQLdb来扩展示例。
- en: 'To efficiently query our database later on we’ll need additional data preparation,
    including the following things:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以后高效查询我们的数据库，我们需要进行额外的数据准备，包括以下事项：
- en: Creating bit strings. The bit strings are compressed versions of the columns’
    content (0 and 1 values). First these binary values are concatenated; then the
    resulting bit string is reinterpreted as a number. This might sound abstract now
    but will become clearer in the code.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建位字符串。位字符串是列内容的压缩版本（0和1值）。首先，这些二进制值被连接起来；然后，得到的位字符串被重新解释为一个数字。现在这可能听起来很抽象，但在代码中会变得清晰。
- en: Defining hash functions. The hash functions will in fact create the bit strings.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义哈希函数。实际上，哈希函数将创建位字符串。
- en: Adding an index to the table, to quicken data retrieval.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向表中添加索引，以加快数据检索。
- en: Creating bit strings
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建位字符串
- en: Now you make an intermediate table suited for querying, apply the hash functions,
    and represent the sequence of bits as a decimal number. Finally, you can place
    them in a table.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你创建一个适合查询的中间表，应用哈希函数，并将位序列表示为十进制数。最后，你可以将它们放入表中。
- en: First, you need to create bit strings. You need to convert the string “11111111”
    to a binary or a numeric value to make the hamming function work. We opted for
    a numeric representation, as shown in the next listing.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要创建位字符串。你需要将字符串“11111111”转换成二进制或数值，以便汉明函数能够工作。我们选择了数值表示，如下一列表所示。
- en: Listing 4.9\. Creating bit strings
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.9\. 创建位字符串
- en: '![](Images/ch04ex09-0.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex09-0.jpg)'
- en: '![](Images/ch04ex09-1.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch04ex09-1.jpg)'
- en: By converting the information of 32 columns into 4 numbers, we compressed it
    for later lookup. [Figure 4.11](#ch04fig11) shows what we get when asking for
    the first 2 observations (customer movie view history) in this new format.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将32列的信息转换为4个数字，我们压缩了它以便后续查找。[图4.11](#ch04fig11)显示了在询问这种新格式下前两个观察结果（客户电影观看历史）时我们得到的结果。
- en: Figure 4.11\. First 2 customers’ information on all 32 movies after bit string
    to numeric conversion
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.11\. 位字符串转换成数字后前两位客户的32部电影信息
- en: '![](Images/04fig11.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig11.jpg)'
- en: '[PRE11]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The next step is to create the hash functions, because they’ll enable us to
    sample the data we’ll use to determine whether two customers have similar behavior.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建哈希函数，因为它们将使我们能够采样我们将使用的数据，以确定两位客户是否有相似的行为。
- en: Creating a hash function
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建哈希函数
- en: 'The hash functions we create take the values of movies for a customer. We decided
    in the theory part of this case study to create 3 hash functions: the first function
    combines the movies 10, 5, and 18; the second combines movies 7, 18, and 22; and
    the third one combines 16, 19, and 30\. It’s up to you if you want to pick others;
    this can be picked randomly. The following code listing shows how this is done.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的哈希函数取客户的电影值。在本案例研究的理论部分，我们决定创建3个哈希函数：第一个函数结合了电影10、5和18；第二个结合了电影7、18和22；第三个结合了16、19和30。如果您想选择其他函数，这可以是随机选择的。以下代码列表显示了如何实现这一点。
- en: Listing 4.10\. Creating hash functions
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.10\. 创建哈希函数
- en: '![](Images/114fig01_alt.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/114fig01_alt.jpg)'
- en: The hash function concatenates the values from the different movies into a binary
    value like what happened before in the `createNum()` function, only this time
    we don’t convert to numbers and we only take 3 movies instead of 8 as input. The
    assert function shows how it concatenates the 3 values for every observation.
    When the client has bought movie 10 but not movies 15 and 28, it will return b’100’
    for bucket 1\. When the client bought movies 7 and 18, but not 22, it will return
    b’110’ for bucket 2. If we look at the current result we see the 4 variables we
    created earlier (bit1, bit2, bit3, bit4) from the 9 handpicked movies ([figure
    4.12](#ch04fig12)).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希函数将不同电影的价值连接成一个二进制值，就像在`createNum()`函数中之前发生的那样，但这次我们不将其转换为数字，我们只取3部电影而不是8部电影作为输入。assert函数显示了它如何为每个观察值连接3个值。当客户购买了电影10但没有购买电影15和28时，它将为桶1返回b’100’。当客户购买了电影7和18但没有购买电影22时，它将为桶2返回b’110’。如果我们查看当前结果，我们会看到我们之前创建的4个变量（bit1，bit2，bit3，bit4）来自9个精选电影（[图4.12](#ch04fig12)）。
- en: Figure 4.12\. Information from the bit string compression and the 9 sampled
    movies
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.12\. 位字符串压缩和9个采样电影的信息
- en: '![](Images/04fig12.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/04fig12.jpg)'
- en: The last trick we’ll apply is indexing the customer table so lookups happen
    more quickly.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要应用的最后一个技巧是索引客户表，以便查找更快。
- en: Adding an index to the table
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向表中添加索引
- en: Now you must add indices to speed up retrieval as needed in a real-time system.
    This is shown in the next listing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您必须添加索引以加快检索速度，这在实时系统中是必需的。这将在下一个列表中显示。
- en: Listing 4.11\. Creating an index
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.11\. 创建索引
- en: '![](Images/115fig01_alt.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/115fig01_alt.jpg)'
- en: 'With the data indexed we can now move on to the “model building part.” In this
    case study no actual machine learning or statistical model is implemented. Instead
    we’ll use a far simpler technique: string distance calculation. Two strings can
    be compared using the hamming distance as explained earlier in the theoretical
    intro to the case study.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 数据索引后，我们现在可以继续到“模型构建部分”。在本案例研究中，没有实现实际的机器学习或统计模型。相反，我们将使用一种更简单的技术：字符串距离计算。可以使用之前在案例研究的理论介绍中解释的汉明距离来比较两个字符串。
- en: '4.5.4\. Step 5: Model building'
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.4\. 第5步：模型构建
- en: To use the hamming distance in the database we need to define it as a function.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 要在数据库中使用汉明距离，我们需要将其定义为函数。
- en: Creating the hamming distance function
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建汉明距离函数
- en: We implement this as *a user-defined function*. This function can calculate
    the distance for a 32-bit integer (actually 4*8), as shown in the following listing.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其实现为一个**用户定义函数**。此函数可以计算32位整数（实际上是4*8）的距离，如下所示。
- en: Listing 4.12\. Creating the hamming distance
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.12\. 创建汉明距离
- en: '![](Images/115fig02_alt.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/115fig02_alt.jpg)'
- en: If all is well, the output of this code should be 3.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，此代码的输出应该是3。
- en: 'Now that we have our hamming distance function in position, we can use it to
    find similar customers to a given customer, and this is exactly what we want our
    application to do. Let’s move on to the last part: utilizing our setup as a sort
    of application.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将汉明距离函数定位好，我们可以使用它来找到与给定客户相似的客户，这正是我们希望应用程序所做的。让我们继续到最后一个部分：利用我们的设置作为一种应用程序。
- en: '4.5.5\. Step 6: Presentation and automation'
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.5\. 第6步：展示和自动化
- en: 'Now that we have it all set up, our application needs to perform two steps
    when confronted with a given customer:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了所有内容，当面对一个给定的客户时，我们的应用程序需要执行两个步骤：
- en: Look for similar customers.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找类似客户。
- en: Suggest movies the customer has yet to see based on what he or she has already
    viewed and the viewing history of the similar customers.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据客户已经观看的内容和类似客户的观看历史，建议客户尚未观看的电影。
- en: 'First things first: select ourselves a lucky customer.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑：选择一个幸运的客户。
- en: Finding a similar customer
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 寻找类似客户
- en: Time to perform real-time queries. In the following listing, customer 27 is
    the happy one who’ll get his next movies selected for him. But first we need to
    select customers with a similar viewing history.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 进行实时查询的时间。在以下列表中，客户27是那位快乐的客户，他将得到为他选择的下一部电影。但首先我们需要选择具有类似观看历史的客户。
- en: Listing 4.13\. Finding similar customers
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.13\. 寻找类似客户
- en: '![](Images/116fig01_alt.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/116fig01_alt.jpg)'
- en: '[Table 4.5](#ch04table05) shows customers 2 and 97 to be the most similar to
    customer 27\. Don’t forget that the data was generated randomly, so anyone replicating
    this example might receive different results.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4.5](#ch04table05)显示客户2和97与客户27最为相似。不要忘记数据是随机生成的，所以任何复制此示例的人可能会得到不同的结果。'
- en: Table 4.5\. The most similar customers to customer 27
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.5. 与客户27最相似的客户
- en: '|   | cust_id | distance |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|   | cust_id | distance |'
- en: '| --- | --- | --- |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 27 | 0 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 27 | 0 |'
- en: '| 1 | 2 | 8 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 8 |'
- en: '| 2 | 97 | 9 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 97 | 9 |'
- en: Now we can finally select a movie for customer 27 to watch.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于可以为客户27选择一部电影来观看。
- en: Finding a new movie
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 寻找一部新电影
- en: We need to look at movies customer 27 hasn’t seen yet, but the nearest customer
    has, as shown in the following listing. This is also a good check to see if your
    distance function worked correctly. Although this may not be the closest customer,
    it’s a good match with customer 27\. By using the hashed indexes, you’ve gained
    enormous speed when querying large databases.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要查看客户27尚未观看但最近客户已经看过的电影，如下所示。这也是一个很好的检查，看看你的距离函数是否工作正确的机会。尽管这可能不是最近客户，但它与客户27非常匹配。通过使用哈希索引，你在查询大型数据库时获得了巨大的速度提升。
- en: Listing 4.14\. Finding an unseen movie
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.14. 寻找一部未观看的电影
- en: '![](Images/117fig01_alt.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/117fig01_alt.jpg)'
- en: '[Table 4.6](#ch04table06) shows you can recommend movie 12, 15, or 31 based
    on customer 2’s behavior.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4.6](#ch04table06) 显示你可以根据客户2的行为推荐电影12、15或31。'
- en: Table 4.6\. Movies from customer 2 can be used as suggestions for customer 27.
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.6. 客户2的电影可以作为客户27的建议。
- en: '|   | 0 | 1 | 2 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|   | 0 | 1 | 2 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Cust_id | 2 | 27 | 97 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Cust_id | 2 | 27 | 97 |'
- en: '| Movie3 | 0 | 1 | 1 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Movie3 | 0 | 1 | 1 |'
- en: '| Movie9 | 0 | 1 | 1 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Movie9 | 0 | 1 | 1 |'
- en: '| Movie11 | 0 | 1 | 1 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Movie11 | 0 | 1 | 1 |'
- en: '| Movie12 | 1 | 0 | 0 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Movie12 | 1 | 0 | 0 |'
- en: '| Movie15 | 1 | 0 | 0 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Movie15 | 1 | 0 | 0 |'
- en: '| Movie16 | 0 | 1 | 1 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Movie16 | 0 | 1 | 1 |'
- en: '| Movie25 | 0 | 1 | 1 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Movie25 | 0 | 1 | 1 |'
- en: '| Movie31 | 1 | 0 | 0 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Movie31 | 1 | 0 | 0 |'
- en: Mission accomplished. Our happy movie addict can now indulge himself with a
    new movie, tailored to his preferences.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 任务完成。我们这位快乐的影迷现在可以尽情享受一部新电影，这部电影是根据他的喜好定制的。
- en: In the next chapter we’ll look at even bigger data and see how we can handle
    that using the Horton Sandbox we downloaded in [chapter 1](kindle_split_009.xhtml#ch01).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将查看更大的数据，并看看我们如何使用我们在第1章中下载的 Horton 沙盒来处理这些数据。[第1章](kindle_split_009.xhtml#ch01)。
- en: 4.6\. Summary
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6. 摘要
- en: 'This chapter discussed the following topics:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了以下主题：
- en: 'The main *problems* you can run into when working with large data sets are
    these:'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理大数据集时，你可能会遇到的主要 *问题* 如下：
- en: Not enough memory
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存不足
- en: Long-running programs
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长运行程序
- en: Resources that form bottlenecks and cause speed problems
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形成瓶颈并导致速度问题的资源
- en: 'There are three main types of *solutions* to these problems:'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这些问题的三种主要类型 *解决方案* 如下：
- en: Adapt your algorithms.
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整你的算法。
- en: Use different data structures.
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的数据结构。
- en: Rely on tools and libraries.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖工具和库。
- en: 'Three main techniques can be used to *adapt an algorithm*:'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用三种主要技术来 *调整算法*：
- en: Present algorithm data *one observation at a time* instead of loading the full
    data set at once.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次只呈现算法数据的一个观察结果，而不是一次性加载整个数据集。
- en: '*Divide matrices into smaller matrices* and use these to make your calculations.'
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将矩阵划分为更小的矩阵* 并使用这些矩阵进行计算。'
- en: Implement the *MapReduce* algorithm (using Python libraries such as Hadoopy,
    Octopy, Disco, or Dumbo).
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现MapReduce算法（使用Python库如Hadoopy、Octopy、Disco或Dumbo）。
- en: 'Three main *data structures* are used in data science. The first is a type
    of matrix that contains relatively little information, the *sparse matrix*. The
    second and third are data structures that enable you to retrieve information quickly
    in a large data set: the *hash function* and *tree structure*.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学中使用了三种主要 *数据结构*。第一种是包含相对较少信息的一种矩阵类型，即 *稀疏矩阵*。第二种和第三种是数据结构，使你能够在大型数据集中快速检索信息：*哈希函数*
    和 *树结构*。
- en: Python has many *tools* that can help you deal with large data sets. Several
    tools will help you with the size of the volume, others will help you parallelize
    the computations, and still others overcome the relatively slow speed of Python
    itself. It’s also easy to use Python as a tool to control other data science tools
    because Python is often chosen as a language in which to implement an API.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python有许多可以帮助你处理大型数据集的 *工具*。几个工具可以帮助你处理数据量的大小，其他工具可以帮助你并行化计算，还有一些工具可以克服Python本身的相对较慢的速度。Python也易于用作控制其他数据科学工具的工具，因为Python通常被选为实现API的语言。
- en: The *best practices* from computer science are also valid in a data science
    context, so applying them can help you overcome the problems you face in a big
    data context.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机科学中的 *最佳实践* 也在数据科学环境中有效，因此应用它们可以帮助你克服在大数据环境中遇到的问题。
