- en: Chapter 5\. Text Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。文本分析
- en: In the last two chapters, we explored applications of dates and numbers with
    time series analysis and cohort analysis. But data sets are often more than just
    numeric values and associated timestamps. From qualitative attributes to free
    text, character fields are often loaded with potentially interesting information.
    Although databases excel at numeric calculations such as counting, summing, and
    averaging things, they are also quite good at performing operations on text data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，我们探讨了日期和数字的应用，包括时间序列分析和队列分析。但是数据集通常不仅仅是数字值和相关时间戳。从定性属性到自由文本，字符字段通常包含潜在有趣的信息。尽管数据库擅长于数字计算，如计数、求和和平均值，但它们在处理文本数据方面也非常擅长。
- en: I’ll begin this chapter by providing an overview of the types of text analysis
    tasks that SQL is good for, and of those for which another programming language
    is a better choice. Next, I’ll introduce our data set of UFO sightings. Then we’ll
    get into coding, covering text characteristics and profiling, parsing data with
    SQL, making various transformations, constructing new text from parts, and finally
    finding elements within larger blocks of text, including with regular expressions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从提供 SQL 擅长的文本分析任务概述开始这一章节，以及其他更适合使用其他编程语言的任务。接下来，我会介绍我们的 UFO 目击数据集。然后我们将进入编码部分，涵盖文本特征和分析、使用
    SQL 解析数据、进行各种转换、从部分构建新文本，最后在更大的文本块中查找元素，包括使用正则表达式。
- en: Why Text Analysis with SQL?
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要使用 SQL 进行文本分析？
- en: 'Among the huge volumes of data generated every day, a large portion consists
    of text: words, sentences, paragraphs, and even longer documents. Text data used
    for analysis can come from a variety of sources, including descriptors populated
    by humans or computer applications, log files, support tickets, customer surveys,
    social media posts, or news feeds. Text in databases ranges from *structured*
    (where data is in different table fields with distinct meanings) to *semistructured*
    (where the data is in separate columns but may need parsing or cleaning to be
    useful) or mostly *unstructured* (where long VARCHAR or BLOB fields hold arbitrary
    length strings that require extensive structuring before further analysis). Fortunately,
    SQL has a number of useful functions that can be combined to accomplish a range
    of text-structuring and analysis tasks.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在每天产生的海量数据中，有很大一部分是文本：单词、句子、段落，甚至更长的文档。用于分析的文本数据可以来自各种来源，包括人类或计算机应用程序填充的描述符、日志文件、支持票据、客户调查、社交媒体帖子或新闻订阅。数据库中的文本从*结构化*（数据位于不同表字段中，具有不同的含义）到*半结构化*（数据位于不同列中，但可能需要解析或清理才能有用）再到主要是*非结构化*（长VARCHAR或BLOB字段保存需要在进一步分析之前进行广泛结构化的任意长度字符串）。幸运的是，SQL具有许多有用的函数，可以结合使用以完成各种文本结构化和分析任务。
- en: What Is Text Analysis?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是文本分析？
- en: Text analysis is the process of deriving meaning and insight from text data.
    There are two broad categories of text analysis, which can be distinguished by
    whether the output is qualitative or quantitative. *Qualitative analysis*, which
    may also be called *textual analysis*, seeks to understand and synthesize the
    meaning from a single text or a set of texts, often applying other knowledge or
    unique conclusions. This work is often done by journalists, historians, and user
    experience researchers. *Quantitative analysis* of text also seeks to synthesize
    information from text data, but the output is quantitative. Tasks include categorization
    and data extraction, and analysis is usually in the form of counts or frequencies,
    often trended over time. SQL is much more suited to quantitative analysis, so
    that is what the rest of this chapter is concerned with. If you have the opportunity
    to work with a counterpart who specializes in the first type of text analysis,
    however, do take advantage of their expertise. Combining the qualitative with
    the quantitative is a great way to derive new insights and persuade reluctant
    colleagues.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析是从文本数据中提取意义和洞察的过程。文本分析大致分为两类，可以通过输出是定性还是定量来区分。*定性分析*，也可以称为*文本分析*，旨在理解和综合从单个文本或一组文本中获得的含义，通常应用其他知识或独特的结论。这项工作通常由记者、历史学家和用户体验研究人员完成。*定量分析*也旨在从文本数据中综合信息，但其输出是定量的。任务包括分类和数据提取，并且分析通常以计数或频率的形式进行，通常随时间趋势变化。SQL更适合于定量分析，因此本章的其余部分将关注此内容。然而，如果有机会与专注于第一类文本分析的同行合作，请务必利用他们的专业知识。将定性与定量结合是获得新洞见并说服不情愿的同事的好方法。
- en: Text analysis encompasses several goals or strategies. The first is text extraction,
    where a useful piece of data must be pulled from surrounding text. Another is
    categorization, where information is extracted or parsed from text data in order
    to assign tags or categories to rows in a database. Another strategy is sentiment
    analysis, where the goal is to understand the mood or intent of the writer on
    a scale from negative to positive.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析包括几个目标或策略。第一个是文本提取，其中必须从周围文本中提取有用的数据片段。另一个是分类，其中从文本数据中提取或解析信息，以便为数据库中的行分配标签或类别。另一种策略是情感分析，其目标是理解作者的情绪或意图，从负面到正面的范围。
- en: Although text analysis has been around for a while, interest and research in
    this area have taken off with the advent of machine learning and the computing
    resources that are often needed to work with large volumes of text data. *Natural
    language processing* (NLP) has made huge advances in recognizing, classifying,
    and even generating brand-new text data. Human language is incredibly complex,
    with different languages and dialects, grammars, and slang, not to mention the
    thousands and thousands of words, some that have overlapping meanings or subtly
    modify the meaning of other words. As we’ll see, SQL is good at some forms of
    text analysis, but for other, more advanced tasks, there are languages and tools
    that are better suited.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文本分析已经存在一段时间，但随着机器学习的出现以及处理大量文本数据通常需要的计算资源，对这一领域的兴趣和研究已经蓬勃发展。*自然语言处理*（NLP）在识别、分类甚至生成全新文本数据方面取得了巨大进展。人类语言极为复杂，包括不同的语言和方言、语法和俚语，更不用说成千上万的单词，有些单词具有重叠的含义或微妙地修改其他单词的含义。正如我们将看到的，SQL在某些形式的文本分析上表现良好，但对于其他更高级的任务，存在更适合的语言和工具。
- en: Why SQL Is a Good Choice for Text Analysis
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么SQL是进行文本分析的好选择
- en: There are a number of good reasons to use SQL for text analysis. One of the
    most obvious is when the data is already in a database. Modern databases have
    a lot of computing power that can be leveraged for text tasks in addition to the
    other tasks we’ve discussed so far. Moving data to a flat file for analysis with
    another language or tool is time consuming, so doing as much work as possible
    with SQL within the database has advantages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SQL进行文本分析有许多好处。其中一个最明显的好处是当数据已经在数据库中时。现代数据库具有大量的计算能力，可以用于文本任务，除了我们迄今讨论过的其他任务。将数据移动到平面文件中，再用其他语言或工具进行分析是耗时的，因此在数据库内尽可能多地使用SQL进行工作具有优势。
- en: If the data is not already in a database, for relatively large data sets, moving
    the data to a database may be worthwhile. Databases are more powerful than spreadsheets
    for processing transformations on many records. SQL is less error-prone than spreadsheets,
    since no copying and pasting is required, and the original data stays intact.
    Data could potentially be altered with an *UPDATE* command, but this is hard to
    do accidentally.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据尚未存储在数据库中，对于相对大的数据集，将数据移动到数据库可能是值得的。与电子表格相比，数据库在处理多条记录的转换时更为强大。SQL不像电子表格那样容易出错，因为不需要复制和粘贴，原始数据保持不变。数据可能会被*UPDATE*命令意外地更改，但这很难发生。
- en: SQL is also a good choice when the end goal is quantification of some sort.
    Counting how many support tickets contain a key phrase and parsing categories
    out of larger text that will be used to group records are good examples of when
    SQL shines. SQL is good at cleaning and structuring text fields. *Cleaning* includes
    removing extra characters or whitespace, fixing capitalization, and standardizing
    spellings. *Structuring* involves creating new columns from elements extracted
    or derived from other fields or constructing new fields from parts stored in different
    places. String functions can be nested or applied to the results of other functions,
    allowing for almost any manipulations that might be needed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: SQL在最终目标是某种形式的量化时也是一个不错的选择。计数多少支持票包含关键短语以及解析大文本中的类别，这些都是SQL发挥作用的好例子。SQL擅长清理和结构化文本字段。*清理*包括删除额外字符或空格，修正大小写，以及标准化拼写。*结构化*涉及从其他字段中提取或推导元素创建新列，或从不同位置存储的部分构建新字段。字符串函数可以嵌套或应用于其他函数的结果，几乎可以进行任何可能需要的操作。
- en: SQL code for text analysis can be simple or complex, but it is always rule based.
    In a rule-based system, the computer follows a set of rules or instructions—no
    more, no less. This can be contrasted with machine learning, in which the computer
    adapts based on the data. Rules are good because they are easy for humans to understand.
    They are written down in code form and can be checked to ensure they produce the
    desired output. The downside of rules is that they can become long and complicated,
    particularly when there are a lot of different cases to handle. This can also
    make them difficult to maintain. If the structure or type of data entered into
    the column changes, the rule set needs to be updated. On more than one occasion,
    I’ve started with what seemed like a simple CASE statement with 4 or 5 lines,
    only to have it grow to 50 or 100 lines as the application changed. Rules might
    still be the right approach, but keeping in sync with the development team on
    changes is a good idea.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本分析的SQL代码可以简单也可以复杂，但它总是基于规则的。在基于规则的系统中，计算机遵循一组规则或指令——既不多也不少。这与机器学习形成对比，后者根据数据进行调整。规则之所以好，是因为它们易于人类理解。它们以代码形式写下，并可以检查以确保它们产生期望的输出。规则的缺点在于它们可能变得冗长和复杂，特别是在需要处理许多不同情况时。这也可能使它们难以维护。如果输入到列中的数据的结构或类型发生变化，规则集就需要更新。我不止一次地以为自己开始用的是一个看起来简单的CASE语句，只有4或5行，结果随着应用程序的变化它就增长到50或100行。规则可能仍然是正确的方法，但与开发团队保持同步变更是个好主意。
- en: Finally, SQL is a good choice when you know in advance what you are looking
    for. There are a number of powerful functions, including regular expressions,
    that allow you to search for, extract, or replace specific pieces of information.
    “How many reviewers mention ‘short battery life’ in their reviews?” is a question
    SQL can help you answer. On the other hand, “Why are these customers angry?” is
    not going to be as easy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当您事先知道要查找的内容时，SQL是一个不错的选择。它有许多强大的功能，包括正则表达式，允许您搜索、提取或替换特定信息。例如，“有多少评论者提到‘短电池寿命’？”这是SQL可以帮助您回答的问题。然而，“为什么这些客户生气？”就不会那么容易了。
- en: When SQL Is Not a Good Choice
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SQL不适合的情况
- en: SQL essentially allows you to harness the power of the database to apply a set
    of rules, albeit often powerful rules, to a set of text to make it more useful
    for analysis. SQL is certainly not the only option for text analysis, and there
    are a number of use cases for which it’s not the best choice. It’s useful to be
    aware of these.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SQL基本上允许您利用数据库的力量，应用一组规则（尽管通常是强大的规则）对一组文本进行处理，使其在分析中更加有用。SQL显然不是文本分析的唯一选择，还有许多情况并非最佳选择。了解这些情况是很有用的。
- en: The first category encompasses use cases for which a human is more appropriate.
    When the data set is very small or very new, hand labeling can be faster and more
    informative. Additionally, if the goal is to read all the records and come up
    with a qualitative summary of key themes, a human is a better choice.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类涵盖了更适合人类处理的用例。当数据集非常小或非常新时，手动标记可能更快且更具信息性。此外，如果目标是阅读所有记录并得出关键主题的定性总结，选择人类更为合适。
- en: The second category is when there’s a need to search for and retrieve specific
    records that contain text strings with low latency. Tools like Elasticsearch or
    Splunk have been developed to index strings for these use cases. Performance will
    often be an issue with SQL and databases; this is one of the main reasons that
    we usually try to structure the data into discrete columns that can more easily
    be searched by the database engine.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类情况是当需要搜索和检索包含文本字符串的特定记录，并且需要低延迟时。像 Elasticsearch 或 Splunk 这样的工具已经开发出来为这些用例索引字符串。在
    SQL 和数据库中，性能通常是一个问题；这是我们通常尝试将数据结构化为可以更容易通过数据库引擎搜索的离散列的主要原因之一。
- en: The third category comprises tasks in the broader NLP category, where machine
    learning approaches and the languages that run them, such as Python, are a better
    choice. Sentiment analysis, used to analyze ranges of positive or negative feelings
    in texts, can be handled only in a simplistic way with SQL. For example, “love”
    and “hate” could be extracted and used to categorize records, but given the range
    of words that can express positive and negative emotions, as well as all the ways
    to negate those words, it would be nearly impossible to create a rule set with
    SQL to handle them all. Part-of-speech tagging, where words in a text are labeled
    as nouns, verbs, and so on, is better handled with libraries available in Python.
    Language generation, or creating brand-new text based on learnings from example
    texts, is another example best handled in other tools. We will see how we can
    create new text by concatenating pieces of data together, but SQL is still bound
    by rules and won’t automatically learn from and adapt to new examples in the data
    set.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第三类包括更广泛的 NLP 类别中的任务，其中机器学习方法以及运行它们的语言（如 Python）是更好的选择。情感分析，用于分析文本中的正面或负面情感范围，仅能以简单方式通过
    SQL 处理。例如，“爱”和“恨”可以被提取并用于分类记录，但考虑到可以表达正面和负面情绪的词语范围，以及否定这些词语的各种方式，使用 SQL 几乎不可能创建一个规则集来处理所有情况。词性标注，即将文本中的单词标记为名词、动词等，最好使用
    Python 中提供的库来处理。语言生成，即根据从示例文本中学到的内容创建全新文本，是另一个最好用其他工具处理的例子。我们将看到如何通过连接数据片段来创建新文本，但
    SQL 仍受规则约束，不会自动从数据集中学习并适应新的示例。
- en: Now that we’ve discussed the many good reasons to use SQL for text analysis,
    as well as the types of use cases to avoid, let’s take a look at the data set
    we’ll be using for the examples before launching into the SQL code itself.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经讨论了使用 SQL 进行文本分析的许多充分理由，以及需要避免的用例类型，接下来让我们在深入研究 SQL 代码之前，先看看我们将在示例中使用的数据集。
- en: The UFO Sightings Data Set
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UFO 目击数据集
- en: For the examples in this chapter, we’ll use a data set of UFO sightings compiled
    by the [National UFO Reporting Center](http://www.nuforc.org). The data set consists
    of approximately 95,000 reports posted between 2006 and 2020\. Reports come from
    individuals who can enter information through an online form.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的示例中，我们将使用由[国家不明飞行物报告中心](http://www.nuforc.org)编制的 UFO 目击数据集。该数据集包含从 2006
    年到 2020 年间发布的约 95,000 条报告。这些报告来自通过在线表单输入信息的个人。
- en: The table we will work with is `ufo`, and it has only two columns. The first
    is a composite column called `sighting_report` that contains information about
    when the sighting occurred, when it was reported, and when it was posted. It also
    contains metadata about the location, shape, and duration of the sighting event.
    The second column is a text field called `description` that contains the full
    description of the event. [Figure 5-1](#sample_of_the_ufo_table) shows a sample
    of the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的表是 `ufo`，它只有两列。第一列是一个名为 `sighting_report` 的复合列，其中包含目击事件发生、报告以及发布的详细信息。它还包含有关目击事件的位置、形状和持续时间的元数据。第二列是一个名为
    `description` 的文本字段，其中包含事件的完整描述。[图示 5-1](#sample_of_the_ufo_table) 显示了数据的一个示例。
- en: '![](Images/sfda_0501.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0501.png)'
- en: Figure 5-1\. Sample of the `ufo` table
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. `ufo` 表的示例
- en: Through the examples and discussion in this chapter, I will show how to parse
    the first column into structured dates and descriptors. I will also show how to
    perform various analyses on the `description` field. If I were working with this
    data on a continual basis, I might consider creating an ETL pipeline, a job that
    processes the data in the same way on a regular basis, and storing the resulting
    structured data in a new table. For the examples in this chapter, however, we’ll
    stick with the raw table.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章的示例和讨论，我将展示如何将第一列解析为结构化的日期和描述符。我还将展示如何对`description`字段执行各种分析。如果我持续处理这些数据，我可能会考虑创建一个ETL管道，这是一个定期以相同方式处理数据并将结果存储在新表中的作业。然而，在本章的示例中，我们将继续使用原始表格。
- en: Let’s get into the code, starting with SQL to explore and characterize the text
    from the sightings.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入代码，从SQL开始探索和特征化来自目击事件的文本。
- en: Text Characteristics
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本特性
- en: The most flexible data type in a database is VARCHAR, because almost any data
    can be put in fields of this type. As a result, text data in databases comes in
    a variety of shapes and sizes. As with other data sets, profiling and characterizing
    the data is one of the first things we do. From there we can develop a game plan
    for the kinds of cleaning and parsing that may be necessary for the analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库中最灵活的数据类型是VARCHAR，因为几乎任何数据都可以放入这种类型的字段中。因此，数据库中的文本数据呈现出各种形状和大小。与其他数据集一样，分析和特征化数据是我们首先要做的事情之一。从那里我们可以制定清理和解析分析可能需要的计划。
- en: 'One way we can get to know the text data is to find the number of characters
    in each value, which can be done with the `length` function (or `len` in some
    databases). This function takes the string or character field as an argument and
    is similar to functions found in other languages and spreadsheet programs:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解文本数据的一种方法是查找每个值中的字符数，可以使用`length`函数（或某些数据库中的`len`）。此函数以字符串或字符字段作为参数，并类似于其他语言和电子表格程序中的函数：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can create a distribution of field lengths to get a sense of the typical
    length and of whether there are any extreme outliers that might need to be handled
    in special ways:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建字段长度的分布来了解典型长度以及是否存在需要特殊处理的极端异常值：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can see in [Figure 5-2](#distribution_of_field_lengths_in_the_fi) that most
    of the records are between roughly 150 and 180 characters long, and very few are
    less than 140 or more than 200 characters. The lengths of the `description` field
    range from 5 to 64,921 characters. We can assume that there is much more variety
    in this field, even before doing any additional profiling.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[图 5-2](#distribution_of_field_lengths_in_the_fi)中看到，大多数记录的长度大约在150到180个字符之间，少数小于140或大于200个字符。`description`字段的长度范围从5到64,921个字符不等。我们可以假设即使在进行任何额外的分析之前，这个字段的变化也是非常多样的。
- en: '![](Images/sfda_0502.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0502.png)'
- en: Figure 5-2\. Distribution of field lengths in the first column of the `*ufo*`
    table
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. `*ufo*`表第一列字段长度的分布
- en: 'Let’s take a look at a few sample rows of the `sighting_report` column. In
    a query tool, I might scroll through a hundred or so rows to get familiar with
    the contents, but these are representative of the values in the column:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一看`sighting_report`列的几个示例行。在查询工具中，我可能会浏览大约一百行左右以熟悉内容，但这些行代表了列中的值的典型情况：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This data is what I would call semistructured, or overstuffed. It can’t be used
    in an analysis as is, but there are clearly distinct pieces of information stored
    here, and the pattern is similar between rows. For example, each row has the word
    “Occurred” followed by what looks like a timestamp, “Location” followed by a place,
    and “Duration” followed by an amount of time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据可以被称为半结构化或超负荷的。不能直接用于分析，但这里显然存储了明确的信息片段，并且在行之间的模式相似。例如，每一行都有“Occurred”后跟着类似时间戳的内容，“Location”后跟着地点，“Duration”后跟着时间长度。
- en: Note
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Data can end up in overstuffed fields for a variety of reasons, but there are
    two common ones I see. One is when there aren’t enough fields available in the
    source system or application to store all the attributes required, so multiple
    attributes are entered into the same field. Another is when the data is stored
    in a JSON blob in an application in order to accommodate sparse attributes or
    frequent additions of new attributes. Although both scenarios are less than ideal
    from an analysis perspective, as long as there is a consistent structure, we can
    usually handle these with SQL.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能因各种原因而最终存在于过多字段中，但我见过两种常见情况。一种情况是源系统或应用程序中没有足够的字段可用于存储所需的所有属性，因此多个属性输入到同一字段中。另一种情况是数据存储在应用程序中的JSON
    blob中，以适应稀疏属性或频繁添加新属性。虽然这两种情况在分析角度上不理想，但只要有一致的结构，通常可以通过SQL处理这些情况。
- en: 'Our next step is to make this field more usable by parsing it into several
    new fields, each of which contains a single piece of information. The steps in
    this process are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是通过将其解析为多个新字段使此字段更易于使用，每个字段都包含单个信息片段。此过程中的步骤包括：
- en: Plan the field(s) desired as output
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划所需的输出字段（或字段）
- en: Apply parsing functions
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用解析函数
- en: Apply transformations, including data type conversions
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用转换，包括数据类型转换
- en: Check the results when applied to the entire data set, since there will often
    be some records that don’t conform to the pattern
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当应用于整个数据集时，请检查结果，因为通常会有一些记录不符合模式
- en: Repeat these steps until the data is in the desired columns and formats
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复这些步骤，直到数据处于所需的列和格式中
- en: The new columns we will parse out of `sighting_report` are `occurred`, `entered_as`,
    `reported`, `posted`, `location`, `shape`, and `duration`. Next, we will learn
    about parsing functions and work on structuring the `ufo` data set.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`sighting_report`中解析出的新列是`occurred`、`entered_as`、`reported`、`posted`、`location`、`shape`和`duration`。接下来，我们将学习解析函数，并开始构造`ufo`数据集。
- en: Text Parsing
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本解析
- en: Parsing data with SQL is the process of extracting pieces of a text value to
    make them more useful for analysis. Parsing splits the data into the part that
    we want and “everything else,” though typically our code returns only the part
    we want.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SQL解析数据是从文本值中提取片段以使其更有用于分析的过程。解析将数据分割成我们想要的部分和“其余所有内容”，尽管通常我们的代码只返回我们想要的部分。
- en: 'The simplest parsing functions return a fixed number of characters from either
    the beginning or the end of a string. The `left` function returns characters from
    the left side or beginning of the string, while the `right` function returns characters
    from the right side or end of the string. They otherwise work in the same way,
    taking the value to parse as the first argument and the number of characters as
    the second. Either argument can be a database field or calculation, allowing for
    dynamic results:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解析函数从字符串的开头或结尾返回固定数量的字符。`left`函数从字符串的左侧或开头返回字符，而`right`函数从字符串的右侧或结尾返回字符。除此之外，它们的工作方式相同，第一个参数是要解析的值，第二个参数是字符数。任一参数可以是数据库字段或计算，允许动态结果：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the `ufo` data set, we can parse out the first word, “Occurred,” using the
    `left` function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ufo`数据集中，我们可以使用`left`函数解析出第一个单词“Occurred”：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can confirm that all records start with this word, which is good news because
    it means at least this part of the pattern is consistent. However, what we really
    want is the values for what occurred, not the word itself, so let’s try again.
    In the first example record, the end of the occurred timestamp is at character
    25\. In order to remove “Occurred” and retain only the actual timestamp, we can
    return the rightmost 14 characters using the `right` function. Note that the `right`
    and `left` functions are nested—the first argument of the `right` function is
    the result of the `left` function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确认所有记录都以此单词开头，这是个好消息，因为这意味着至少模式的这一部分是一致的。但是，我们真正想要的是发生了什么的值，而不是单词本身，所以让我们再试一次。在第一个示例记录中，发生的时间戳结束于第25个字符。为了删除“Occurred”并保留实际时间戳，我们可以使用`right`函数返回最右侧的14个字符。请注意，`right`和`left`函数是嵌套的—`right`函数的第一个参数是`left`函数的结果：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Although this returns the correct result for the first record, it unfortunately
    can’t handle the records that have two-digit month or day values. We could increase
    the number of characters returned by the `left` and `right` functions, but the
    result would then include too many characters for the first record.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可以为第一条记录返回正确结果，但不幸的是，它无法处理具有两位数月份或日期值的记录。我们可以增加`left`和`right`函数返回的字符数，但结果将包含第一条记录的太多字符。
- en: 'The `left` and `right` functions are useful for extracting fixed-length parts
    of a string, as in our extraction of the word “Occurred,” but for more complex
    patterns, a function called `split_part` is more useful. The idea behind this
    function is to split a string into parts based on a delimiter and then allow you
    to select a specific part. A *delimiter* is one or more characters that are used
    to specify the boundary between regions of text or other data. The comma delimiter
    and tab delimiter are probably the most common, as these are used in text files
    (with extensions such as *.csv*, *.tsv*, or *.txt*) to indicate where columns
    start and end. However, any sequence of characters can be used, which will come
    in handy for our parsing task. The form of the function is:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`left`和`right`函数对于提取字符串的固定长度部分非常有用，例如我们提取单词“Occurred”的操作，但对于更复杂的模式，名为`split_part`的函数更加实用。这个函数的想法是基于分隔符将字符串拆分为部分，然后允许您选择特定的部分。*分隔符*是用于指定文本或其他数据区域边界的一个或多个字符。逗号分隔符和制表符分隔符可能是最常见的，因为它们用于文本文件（如*.csv*、*.tsv*或*.txt*文件）中表示列的起始和结束位置。但是，任何字符序列都可以使用，这在我们的解析任务中会非常有用。函数的形式是：'
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The index is the position of the text to be returned, relative to the delimiter.
    So index = 1 returns all of the text to the left of the first instance of the
    delimiter, index = 2 returns the text between the first and second instance of
    the delimiter (or all of the text to the right of the delimiter if the delimiter
    appears only once), and so on. There is no zero index, and the values must be
    positive integers:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 索引是要返回的文本相对于分隔符的位置。因此，索引 = 1 返回分隔符的第一个实例左侧的所有文本，索引 = 2 返回第一个和第二个分隔符之间的文本（或者如果分隔符仅出现一次，则返回分隔符右侧的所有文本），依此类推。没有零索引，值必须是正整数：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tip
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: MySQL has a `substring_index` function instead of `split_part`. SQL Server does
    not have a `split_part` function at all
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL有一个`substring_index`函数，而SQL Server根本没有`split_part`函数。
- en: 'Note that spaces in the text will be retained unless specified as part of the
    delimiter. Let’s take a look at how we can parse the elements of the `sighting_report`
    column. As a reminder, a sample value looks like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意文本中的空格将保留，除非指定为分隔符的一部分。让我们看看如何解析`sighting_report`列的元素。作为提醒，样本值如下所示：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The value we want our query to return is the text between “Occurred : ” and
    “ (Entered”. That is, we want the string “6/3/2014 23:00”. Checking the sample
    text, “Occurred :” and “(Entered” both appear only once. A colon (:) appears several
    times, both to separate the label from the value and in the middle of timestamps.
    This might make parsing using the colon tricky. The open parenthesis character
    appears only once. We have some choices as to what to specify as the delimiter,
    choosing either longer strings or only the fewest characters required to split
    the string accurately. I tend to be a little more verbose to ensure that I get
    exactly the piece that I want, but it really depends on the situation.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '我们希望查询返回的值是“Occurred : ”和“ (Entered”之间的文本。也就是说，我们想要字符串“6/3/2014 23:00”。检查样本文本，“Occurred
    :”和“(Entered”只出现一次。冒号（:）多次出现，既用于将标签与值分隔开，又用于时间戳中间。这可能使得使用冒号进行解析变得棘手。开括号字符只出现一次。我们可以选择指定作为分隔符的内容，选择较长的字符串或仅包含拆分字符串所需的最少字符。我倾向于稍微冗长一些，以确保我确实获得我想要的那部分内容，但这真的取决于情况。'
- en: 'First, split `sighting_report` on “Occurred : ” and check the result:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，在“Occurred : ”上拆分`sighting_report`，并检查结果：'
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have successfully removed the label, but we still have a lot of extra text
    remaining. Let’s check the result when we split on “ (Entered”:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功移除标签，但仍有大量多余文本。让我们在“ (Entered”处拆分时检查结果：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is closer, but it still has the label in the result. Fortunately, nesting
    `split_part` functions will return only the desired date and time value:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这更接近了，但结果中仍然有标签。幸运的是，嵌套使用`split_part`函数将仅返回所需的日期和时间值：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now the result includes the desired values. Reviewing a few additional lines
    shows that two-digit day and month values are handled appropriately, as are dates
    that do not have a time value. It turns out that some records omit the “Entered
    as” value, so one additional split is required to handle records where the “Reported”
    label marks the end of the desired string:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在结果包含了所需的值。再查看一些额外的行显示，两位数的日和月值已经适当处理，没有时间值的日期也是如此。事实证明，一些记录省略了“输入为”值，因此需要额外的拆分以处理标记为所需字符串末尾的“报告”标签的记录：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The most common `occurred` values parsed out with the SQL code are graphed in
    [Figure 5-3](#top_onezero_most_common_occurred_values).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 用 SQL 代码解析出来的最常见的 `发生` 值在 [图 5-3](#top_onezero_most_common_occurred_values)
    中被绘制。
- en: '![](Images/sfda_0503.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0503.png)'
- en: Figure 5-3\. Top 10 most common `occurred` values for UFO sightings
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. UFO 目击事件中前 10 个最常见的 `发生` 值
- en: Tip
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Finding a set of functions that works for all values in the data set is one
    of the hardest parts of text parsing. It often takes several rounds of trial and
    error and profiling the results along the way to get it right.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 找到适用于数据集中所有值的函数集是文本解析中最难的部分之一。通常需要多轮尝试和逐步分析结果来正确处理它。
- en: 'The next step is to apply similar parsing rules to extract the other desired
    fields, using beginning and ending delimiters to isolate just the relevant part
    of the string. The final query uses `split_part` several times, with different
    arguments for each value:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是应用类似的解析规则，以提取其他所需字段，使用起始和结束定界符来隔离字符串的相关部分。最终查询在几个值中多次使用 `split_part`，每个值都有不同的参数：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With this SQL parsing, the data is now in a much more structured and usable
    format. Before we finish, however, there are a few transformations that will clean
    up the data a little further. We’ll take a look at these string transformation
    functions next.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种 SQL 解析，数据现在处于更加结构化和可用的格式中。然而，在我们完成之前，还有一些转换可以进一步清理数据。我们将接下来查看这些字符串转换函数。
- en: Text Transformations
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本转换
- en: Transformations change string values in some way. We saw a number of date and
    timestamp transformation functions in [Chapter 3](ch03.xhtml#time_series_analysis).
    There is a set of functions in SQL that specifically work on string values. These
    are useful for working with parsed data, but also for any text data in a database
    that needs to be adjusted or cleaned for analysis.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 转换会以某种方式更改字符串值。我们在 [第 3 章](ch03.xhtml#time_series_analysis) 中看到了许多日期和时间戳转换函数。SQL
    中有一组专门处理字符串值的函数。这些对于处理解析后的数据非常有用，也适用于需要调整或清理分析的任何数据库文本数据。
- en: 'Among the most common transformations are the ones that change capitalization.
    The `upper` function converts all letters to their uppercase form, while the `lower`
    function converts all letters to their lowercase form. For example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的转换之一是更改大小写的转换。`upper` 函数将所有字母转换为大写形式，而 `lower` 函数则将所有字母转换为小写形式。例如：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'These are useful for standardizing values that may have been entered in different
    ways. For example, any human will recognize that “California,” “caLiforNia,” and
    “CALIFORNIA” all refer to the same state, but a database will treat them as distinct
    values. If we were to count UFO sightings by states with these values, we would
    end up with three records for California, resulting in incorrect analysis conclusions.
    Converting them to all uppercase or all lowercase letters would solve this problem.
    Some databases, including Postgres, have an `initcap` function that capitalizes
    the first letter of each word in a string. This is useful for proper nouns, such
    as state names:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法对于标准化可能以不同方式输入的值非常有用。例如，任何人都会意识到，“加利福尼亚”，“caLiforNia”和“CALIFORNIA”指的是同一个州，但数据库会将它们视为不同的值。如果我们按这些值按州统计
    UFO 目击事件，我们会得到三条加利福尼亚州的记录，导致分析结论不正确。将它们全部转换为大写或小写字母可以解决这个问题。一些数据库，包括 Postgres，具有
    `initcap` 函数，该函数会将字符串中每个单词的首字母大写。这对于专有名词（例如州名）非常有用：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `shape` field in the data set we parsed contains one value that is in all
    capitals, “TRIANGULAR.” To clean this and standardize it with the other values,
    which all have only their first letter capitalized, apply the `initcap` function:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解析的数据集中的 `shape` 字段包含一个全大写的值，“TRIANGULAR”。为了清理并将其与其他只有首字母大写的值标准化，应用 `initcap`
    函数：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The number of sightings for each shape is shown in [Figure 5-4](#frequency_of_shapes_in_ufo_sightings).
    Light is by far the most common shape, followed by circle and triangle. Some sightings
    do not report a shape, so a count for null value appears in the graph as well.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每种形状的目击次数显示在[图5-4](#frequency_of_shapes_in_ufo_sightings)中。光是远远最常见的形状，其次是圆形和三角形。一些目击没有报告形状，因此在图表中也会出现空值计数。
- en: '![](Images/sfda_0504.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0504.png)'
- en: Figure 5-4\. Frequency of shapes in UFO sightings
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。UFO目击中形状的频率
- en: 'Another useful transformation function is one called `trim` that removes blank
    spaces at the beginning and end of a string. Extra whitespace characters are a
    common problem when parsing values out of longer strings, or when data is created
    by human entry or by copying data from one application to another. As an example,
    we can remove the leading spaces before “California” in the following string by
    using the `trim` function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的转换函数是称为`trim`的函数，它删除字符串开头和结尾的空格。在解析较长字符串的值或将数据从一个应用程序复制到另一个应用程序时，额外的空白字符是常见问题。例如，我们可以使用`trim`函数在以下字符串中的“California”之前去除前导空格：
- en: '[PRE17]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `trim` function has a few optional parameters that make it flexible for
    a variety of data-cleaning challenges. First, it can remove characters from the
    start of a string or from the end of a string, or both. Trimming from both ends
    is the default, but the other options can be specified with `leading` or `trailing`.
    Additionally, `trim` can remove any character, not just whitespace. So, for example,
    if an application placed a dollar sign ($) at the beginning of each state name
    for some reason, we could remove this with `trim`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`trim`有一些可选参数，使其在各种数据清理挑战中非常灵活。首先，它可以从字符串的开头或末尾（或两者）移除字符。从两端修剪是默认设置，但其他选项可以通过`leading`或`trailing`指定。另外，`trim`可以移除任何字符，而不仅仅是空白字符。例如，如果某个应用程序出于某种原因在每个州名开头放置了美元符号（$），我们可以使用`trim`来移除它：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A few of the values in the `duration` field have leading spaces, so applying
    `trim` will result in a cleaner output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`duration`字段中的一些值有前导空格，因此应用`trim`将产生更清晰的输出：'
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The number of sightings for the most common durations are graphed in [Figure 5-5](#top_onezero_most_common_durations_of_uf).
    Sightings lasting between 1 and 10 minutes are common. Some sightings do not report
    a duration, so a count for null value appears in the graph.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见持续时间的目击次数在[图5-5](#top_onezero_most_common_durations_of_uf)中绘制。持续1到10分钟的目击很常见。一些目击没有报告持续时间，因此在图表中显示空值计数。
- en: '![](Images/sfda_0505.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0505.png)'
- en: Figure 5-5\. Top 10 most common durations of UFO sightings
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5。UFO目击最常见的前10个持续时间
- en: 'The next type of transformation is a data type conversion. This type of transformation,
    discussed in [Chapter 2](ch02.xhtml#preparing_data_for_analysis), will be useful
    for ensuring that the results of our parsing have the intended data type. In our
    case, there are two fields that should be treated as timestamps—the `occurred`
    and `reported` columns—and the `posted` column should be a date type. The data
    types can be changed with casting, using either the double colon (::) operator
    or the `CAST field as type` syntax. We’ll leave the `entered_as`, `location`,
    `shape`, and `duration` values as VARCHAR:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下一种转换类型是数据类型转换。这种转换类型在[第2章](ch02.xhtml#preparing_data_for_analysis)中讨论，对于确保我们解析结果具有预期数据类型将非常有用。在我们的情况中，有两个字段应视为时间戳——`occurred`和`reported`列，`posted`列应为日期类型。数据类型可以通过强制转换来更改，可以使用双冒号(::)运算符或`CAST
    field as type`语法。我们将`entered_as`、`location`、`shape`和`duration`的值保留为VARCHAR：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A sample of the data converts to the new formats. Notice that the database
    adds the seconds to the timestamp, even though there were no seconds in the original
    value, and correctly recognizes dates that were in month/day/year (mm/dd/yyyy)
    format.^([1](ch05.xhtml#ch01fn7)) There is a problem when applying these transformations
    to the entire data set, however. A few records do not have values at all, appearing
    as an empty string, and some have the time value but no date associated with them.
    Although an empty string and null seem to contain the same information—nothing—databases
    treat them differently. An empty string is still a string and can’t be converted
    to another data type. Setting all the nonconforming records to null with a CASE
    statement allows the type conversion to work properly. Since we know that dates
    must contain at least eight characters (four digits for year, one or two digits
    each for month and day, and two “-” or “/” characters), one way to accomplish
    this is by setting any record with LENGTH less than 8 equal to null with a CASE
    statement:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的一个样本转换为新的格式。请注意，数据库将秒数添加到时间戳中，即使原始值中没有秒数，并且正确识别了以月/日/年（mm/dd/yyyy）格式存在的日期。^([1](ch05.xhtml#ch01fn7))
    然而，在将这些转换应用于整个数据集时存在问题。一些记录根本没有值，显示为空字符串，而一些记录具有时间值但没有与之关联的日期。尽管空字符串和null看起来包含相同的信息——什么都没有——但数据库对它们的处理方式不同。空字符串仍然是字符串，无法转换为另一种数据类型。通过使用CASE语句将所有不符合规范的记录设置为null，可以使类型转换正常工作。由于我们知道日期必须至少包含八个字符（年份四位数字，月份和日期各一到两位数字，以及两个“-”或“/”字符），因此可以通过使用CASE语句将长度小于8的任何记录设置为null来实现这一点：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The final transformation I’ll discuss in this section is the `replace` function.
    Sometimes there is a word, phrase, or other string within a field that we would
    like to change to another string or remove entirely. The `replace` function comes
    in handy for this task. It takes three arguments—the original text, the string
    to find, and the string to substitute in its place:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本节中讨论的最后一个转换是`replace`函数。有时，在字段中有一个词、短语或其他字符串，我们想要将其更改为另一个字符串或完全删除。`replace`函数在这种情况下非常有用。它接受三个参数——原始文本、要查找的字符串和要替换的字符串：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'So, for example, if we want to change references of “unidentified flying objects”
    to “UFOs,” we can use the `replace` function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，如果我们想要将“未识别飞行物体”的引用更改为“UFO”，我们可以使用`replace`函数：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This function will find and replace every instance of the string in the second
    argument, regardless of where it appears. An empty string can be used as the third
    argument, which is a good way to remove parts of a string that are not wanted.
    Like other string functions, `replace` can be nested, with the output from one
    `replace` becoming the input for another.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将查找并替换第二个参数中的每个实例，无论其出现在何处。可以使用空字符串作为第三个参数，这是删除不需要的字符串部分的好方法。与其他字符串函数一样，`replace`可以嵌套，其中一个`replace`的输出成为另一个的输入。
- en: 'In the parsed UFO-sighting data set we’ve been working with, some of the `location`
    values include qualifiers indicating that the sighting took place “near,” “close
    to,” or “outside of” a city or town. We can use `replace` to standardize these
    to “near”:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们正在处理的解析的UFO目击数据集中，一些`location`值包括指示目击发生在城市或镇“附近”、“靠近”或“外面”的修饰词。我们可以使用`replace`来将这些标准化为“near”：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The top 10 sighting locations are graphed in [Figure 5-6](#most_common_locations_of_ufo_sightings).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的UFO目击地点的前十名已在[图5-6](#most_common_locations_of_ufo_sightings)中绘制出来。
- en: '![](Images/sfda_0506.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0506.png)'
- en: Figure 5-6\. Most common locations of UFO sightings
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6. UFO目击最常见的地点
- en: 'Now we have parsed and cleaned all the elements of the `sighting_report` field
    into distinct, appropriately typed columns. The final code looks like this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经解析并清理了`sighting_report`字段的所有元素，将它们转换为了不同类型的列。最终的代码看起来像这样：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This piece of SQL code can be reused in other queries, or it can be used to
    copy the raw UFO data into a new, cleaned-up table. Alternatively, it could be
    turned into a view or put into a common table expression for reuse. [Chapter 8](ch08.xhtml#creating_complex_data_sets_for_analysis)
    will discuss these strategies in more detail.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这段SQL代码可以在其他查询中重复使用，或者可以用于将原始UFO数据复制到新的清理过的表中。或者，它可以转换为视图或放入通用表达式以供重用。[第8章](ch08.xhtml#creating_complex_data_sets_for_analysis)将更详细地讨论这些策略。
- en: We’ve seen how to apply parsing and transformation functions to clean and improve
    the analysis value of text data that has some amount of structure in it. Next,
    we’ll look at the other field in the UFO sightings data set, the free text `description`
    field, and learn how to use SQL functions to search for specific elements.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何应用解析和转换函数来清理和改善具有一定结构的文本数据的分析价值。接下来，我们将看看UFO目击数据集中的另一个字段，即自由文本`description`字段，并学习如何使用SQL函数来搜索特定元素。
- en: Finding Elements Within Larger Blocks of Text
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在更大的文本块中查找元素
- en: Parsing and transformations are common operations applied to text data to prepare
    it for analysis. Another common operation with text data is finding strings within
    larger blocks of text. This can be done to filter results, categorize records,
    or replace the searched-for strings with alternate values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 解析和转换是应用于文本数据的常见操作，以准备进行分析。另一个常见的操作是在更大的文本块中查找字符串。这可以用来过滤结果、分类记录或用替代值替换搜索的字符串。
- en: 'Wildcard Matches: LIKE, ILIKE'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通配符匹配：LIKE, ILIKE
- en: 'SQL has several functions for matching patterns within strings. The LIKE operator
    matches the specified pattern within the string.  In order to allow it to match
    a pattern and not just find an exact match, wildcard symbols can be added before,
    after, or in the middle of the pattern. The “%” wildcard matches zero or more
    characters, while the “_” wildcard matches exactly one character. If the goal
    is to match the “%” or “_” itself, place the backslash escape symbol (“\”) in
    front of that character:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: SQL具有几个用于在字符串中匹配模式的函数。LIKE运算符在字符串中匹配指定的模式。为了允许它匹配模式而不仅仅是找到精确匹配，可以在模式的前面、后面或中间添加通配符符号。“%”通配符匹配零个或多个字符，“_”通配符匹配正好一个字符。如果目标是匹配“%”或“_”本身，请在该字符前面放置反斜杠转义符（“\”）：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The LIKE operator can be used in a number of clauses within the SQL statement.
    It can be used to filter records in the *WHERE* clause. For example, some reporters
    mention that they were with a spouse at the time, and so we might want to find
    out how many reports mention the word “wife.” Since we want to find the string
    anywhere in the description text, we’ll place the “%” wildcard before and after
    “wife”:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: LIKE运算符可以在SQL语句的多个子句中使用。它可以用于在*WHERE*子句中过滤记录。例如，一些报告者提到他们当时与配偶在一起，因此我们可能想知道有多少份报告提到了“wife”。由于我们希望在描述文本的任何位置找到字符串，“%”通配符将在“wife”之前和之后放置：
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can see that more than six thousand reports mention “wife.” However, this
    will return only matches on the lowercase string. What if some reporters mention
    “Wife,” or they left Caps Lock on and typed in “WIFE” instead? There are two options
    for making the search case insensitive. One option is to transform the field to
    be searched using either the `upper` or `lower` function discussed in the previous
    section, which has the effect of making the search case insensitive since characters
    are all either uppercase or lowercase:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，超过六千份报告提到了“wife”。但是，这只会返回小写字符串的匹配项。如果有些报告者提到了“Wife”，或者他们在键入“WIFE”时忘记了大小写锁定键呢？有两种选项可以使搜索不区分大小写。一种选项是将要搜索的字段转换为前一节中讨论的`upper`或`lower`函数，这样做的效果是使搜索不区分大小写，因为字符都是大写或小写：
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Another way to accomplish this is with the ILIKE operator, which is effectively
    a case-insensitive LIKE operator. The drawback is that it is not available in
    every database; notably, MySQL and SQL Server do not support it. However, it’s
    a nice, compact syntax option if you are working in a database that does support
    it:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实现此目的的方法是使用ILIKE运算符，这实际上是一个不区分大小写的LIKE运算符。缺点是它不适用于每个数据库；特别是，MySQL和SQL Server不支持它。但是，如果您在支持它的数据库中工作，这是一个不错的、简洁的语法选项：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Any of these variations of LIKE and ILIKE can be negated with NOT. So, for
    example, to find the records that do not mention “wife,” we can use NOT LIKE:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: LIKE和ILIKE的任何这些变体都可以通过NOT进行否定。因此，例如，要找到不提到“wife”的记录，我们可以使用NOT LIKE：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Filtering on multiple strings is possible with AND and OR operators:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用AND和OR操作符对多个字符串进行过滤：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Be careful to use parentheses to control the order of operations when using
    OR in conjunction with AND operators, or you might get unexpected results. For
    example, these *WHERE* clauses do not return the same result since OR is evaluated
    before AND:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用OR与AND操作符结合时，请务必使用括号来控制操作的顺序，否则可能会得到意外的结果。例如，这些*WHERE*子句由于OR在AND之前进行评估，所以返回的结果不相同：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In addition to filtering in *WHERE* or *JOIN*...*ON* clauses, LIKE can be used
    in the *SELECT* clause to categorize or aggregate certain records. Let’s start
    with categorization. The LIKE operator can be used within a CASE statement to
    label and group records. Some of the descriptions mention an activity the observer
    was doing during or prior to the sighting, such as driving or walking. We can
    find out how many descriptions contain such terms by using a CASE statement with
    LIKE:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在*WHERE*或*JOIN*...*ON*子句中进行过滤之外，LIKE还可以在*SELECT*子句中用于对某些记录进行分类或聚合。让我们从分类开始。LIKE运算符可以在CASE语句内部使用，以标记和分组记录。一些描述中提到观察者在目击期间或之前正在进行的活动，如驾驶或步行。通过使用带有LIKE的CASE语句，我们可以找出有多少描述包含这些术语：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The most common activity was driving, whereas not many people report sightings
    while swimming or cycling. This is perhaps not surprising, since these activities
    are simply less common than driving.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的活动是驾驶，而不是很多人在游泳或骑行时报告目击。这或许并不令人惊讶，因为这些活动相对于驾驶来说较少见。
- en: Tip
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Although values derived through text-parsing transformation functions can be
    used in *JOIN* criteria, database performance is often a problem. Consider parsing
    and/or transforming in a subquery and then joining the result with an exact match
    in the *JOIN* clause.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过文本解析转换函数得到的值可以用于*JOIN*条件，但数据库性能通常是一个问题。考虑在子查询中进行解析和/或转换，然后将结果与*JOIN*子句中的精确匹配进行连接。
- en: Note that this CASE statement labels each description with only one of the activities
    and evaluates whether each record matches the pattern in the order in which the
    statement is written. A description that contains both “driving” and “walking”
    will be labeled as “driving.” This is appropriate in many cases, but particularly
    when analyzing longer text such as from reviews, survey comments, or support tickets,
    the ability to label records with multiple categories is important. For this type
    of use case, a series of binary or BOOLEAN flag columns is called for.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此CASE语句仅为每个描述标记一个活动，并评估每条记录是否与语句中写入的模式匹配。包含“驾驶”和“步行”等内容的描述将被标记为“驾驶”。在许多情况下，这是合适的，特别是在分析较长的文本（如评论、调查评论或支持票证）时，标记记录具有多个类别的能力至关重要。对于这种用例，需要一系列二进制或布尔标志列。
- en: 'We saw earlier that LIKE can be used to generate a BOOLEAN response of TRUE
    or FALSE, and we can use this to label rows. In the data set, a number of descriptions
    mention the direction in which the object was detected, such as north or south,
    and some mention more than one direction. We might want to label each record with
    a field indicating whether the description mentions each direction:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，LIKE可以用于生成TRUE或FALSE的布尔响应，并且我们可以使用它来标记行。在数据集中，许多描述提到了检测到对象的方向，如北或南，有些描述提到了多个方向。我们可能希望为每条记录添加一个字段，指示描述中是否提到了每个方向：
- en: '[PRE34]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The result is a matrix of BOOLEANs that can be used to find the frequency of
    various combinations of directions, or to find when a direction is used without
    any of the other directions in the same description.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个布尔矩阵，可以用来查找各种方向组合的频率，或者查找在同一描述中使用某个方向而不使用其他方向的情况。
- en: 'All of the combinations are useful in some contexts, particularly in building
    data sets that will be used by others to explore the data, or in a BI or visualization
    tool. However, sometimes it is more useful to summarize the data further and perform
    an aggregation on the records that contain a string pattern. Here we will count
    the records, but other aggregations such as `sum` and `average` can be used if
    the data set contains other numerical fields, such as sales figures:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些组合在某些情境中都很有用，特别是在构建将由其他人用于探索数据或在BI或可视化工具中使用的数据集时。然而，有时更有用的是进一步总结数据并对包含字符串模式的记录执行聚合。在这里，我们将计算记录的数量，但如果数据集包含其他数值字段（如销售数据），也可以使用诸如`sum`和`average`之类的其他聚合函数：
- en: '[PRE36]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We now have a much more compact summary of the frequency of direction terms
    in the description field, and we can see that “east” is mentioned more often than
    other directions. The results are graphed in [Figure 5-7](#frequency_of_compass_directions_mention).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对描述字段中方向术语的频率有了一个更紧凑的总结，可以看到“东”比其他方向提到得更频繁。结果在[图5-7](#frequency_of_compass_directions_mention)中绘制出来。
- en: '![](Images/sfda_0507.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0507.png)'
- en: Figure 5-7\. Frequency of compass directions mentioned in UFO sighting reports
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-7\. UFO目击报告中罗盘方向频率
- en: 'In the preceding query, we still allow a record that contains more than one
    direction to be counted more than once. However, there is no longer visibility
    into which specific combinations exist. Complexity can be added into the query
    as needed to handle such cases, with a statement such as:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的查询中，我们仍然允许包含多个方向的记录被计数多次。然而，现在无法看到具体的组合情况。可以根据需要在查询中添加复杂性，以处理此类情况，例如：
- en: '[PRE37]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Pattern matching with LIKE, NOT LIKE, and ILIKE is flexible and can be used
    in various places in a SQL query to filter, categorize, and aggregate data for
    a variety of output needs. These operators can be used in combination with the
    text-parsing and transformation functions we discussed earlier for even more flexibility.
    Next, I’ll discuss handling multiple elements when the matches are exact before
    returning to more patterns in a discussion of regular expressions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LIKE、NOT LIKE和ILIKE进行模式匹配是灵活的，可以在SQL查询的各种地方使用，以过滤、分类和聚合数据，以满足各种输出需求。这些操作符可以与我们之前讨论的文本解析和转换函数结合使用，提供更多的灵活性。接下来，我将讨论在匹配完全匹配时处理多个元素，然后返回讨论正则表达式中的更多模式。
- en: 'Exact Matches: IN, NOT IN'
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确匹配：IN、NOT IN
- en: Before we move on to more complex pattern matching with regular expressions,
    it’s worth looking at a couple of additional operators that are useful in text
    analysis. Although not strictly about pattern matching, they are often useful
    in combination with LIKE and its relatives in order to come up with a rule set
    that includes exactly the right set of results. The operators are IN and its negation,
    NOT IN. These allow you to specify a list of matches, resulting in more compact
    code.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论使用正则表达式进行更复杂的模式匹配之前，值得看看一些在文本分析中有用的额外操作符。虽然这些操作符不完全是关于模式匹配的，但它们通常与LIKE及其相关操作符一起使用，以制定一个包含准确结果集的规则集。操作符包括IN及其否定形式NOT
    IN。这些操作符允许你指定一个匹配列表，从而使代码更加紧凑。
- en: 'Let’s imagine we are interested in categorizing the sightings based on the
    first word of the `description`. We can find the first word using the `split_part`
    function, with a space character as the delimiter. Many reports start with a color
    as the first word. We might want to filter the records in order to take a look
    at reports that start by naming a color. This can be done by listing each color
    with an OR construction:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们有兴趣根据`description`的第一个单词来对目击事件进行分类。我们可以使用`split_part`函数，空格作为分隔符，找到第一个单词。许多报告以颜色作为第一个单词开始。我们可能想筛选记录，以查看以命名颜色开始的报告。这可以通过列出每种颜色并使用OR构造来完成：
- en: '[PRE38]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Using an IN list is more compact and often less error-prone, particularly when
    there are other elements in the *WHERE* clause. IN takes a comma-separated list
    of items to match. The data type of elements should match the data type of the
    column. If the data type is numeric, the elements should be numbers; if the data
    type is text, the elements should be quoted as text (even if the element is a
    number):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用IN列表更为紧凑，并且通常更不易出错，特别是在*WHERE*子句中有其他元素时。IN接受一个由逗号分隔的项列表进行匹配。元素的数据类型应与列的数据类型匹配。如果数据类型为数字，元素应为数字；如果数据类型为文本，元素应作为文本加引号（即使元素是数字）：
- en: '[PRE39]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The two forms are identical in their results, and the frequencies are shown
    in [Figure 5-8](#frequency_of_select_colors_used_as_the).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 两   这两种形式在结果上是相同的，频率显示在[图5-8](#frequency_of_select_colors_used_as_the)中。
- en: '![](Images/sfda_0508.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0508.png)'
- en: Figure 5-8\. Frequency of select colors used as the first word in UFO sighting
    descriptions
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-8。UFO目击描述中选择颜色作为第一个单词的频率
- en: 'The main benefit of IN and NOT IN is that they make code more compact and readable.
    This can come in handy when creating more complex categorizations in the *SELECT*
    clause. For example, imagine we wanted to categorize and count the records by
    the first word into colors, shapes, movements, or other possible words. We might
    come up with something like the following that combines elements of parsing, transformations,
    pattern matching, and IN lists:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: IN和NOT IN的主要好处是它们使代码更加紧凑和易读。当在*SELECT*子句中创建更复杂的分类时，这非常有用。例如，假设我们想按第一个单词将记录分类和计数为颜色、形状、移动或其他可能的单词。我们可能会想出类似以下内容的组合，结合了解析、转换、模式匹配和IN列表的元素：
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Of course, given the nature of this data set, it would likely take many more
    lines of code and rules to accurately categorize the reports by first word. SQL
    allows you to create a variety of complex and nuanced expressions to deal with
    text data. Next, we’ll turn to some even more sophisticated ways to work with
    text data in SQL, using regular expressions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，考虑到这个数据集的特性，可能需要更多的代码行和规则来准确分类报告的第一个单词。SQL允许你创建各种复杂和微妙的表达式来处理文本数据。接下来，我们将探讨在SQL中处理文本数据更加复杂的方法，使用正则表达式。
- en: Regular Expressions
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式
- en: There are a number of ways to match patterns in SQL. One of the most powerful
    methods, though it is also confusing, is the use of regular expressions (regex).
    I will admit to finding regular expressions intimidating, and I avoided using
    them for a long time in my data analysis career. In a pinch, I was lucky enough
    to have colleagues who were willing to share code snippets and get my work unstuck.
    It was only when I ended up with a big text analysis project that I decided it
    was finally time to learn about them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中有许多匹配模式的方法。其中一种最强大的方法，尽管也很令人困惑，是使用正则表达式（regex）。我承认，我对正则表达式感到有些害怕，而且在我的数据分析职业生涯中很长一段时间内避免使用它们。在紧急情况下，我很幸运有同事愿意分享代码片段，并帮助我解决工作中遇到的问题。直到我接手了一个大型文本分析项目，我才决定是时候学习它们了。
- en: '*Regular expressions* are sequences of characters, many with special meanings,
    that define search patterns. The main challenge in learning regex, and in using
    and maintaining code that contains it, is that the syntax is not particularly
    intuitive. Code snippets don’t read anything like a human language, or even like
    computer languages such as SQL or Python. With a working knowledge of the special
    characters, however, the code can be written and deciphered. Like the code for
    all our queries, it’s a good idea to start simple, build in complexity as needed,
    and check results as you go. And leave comments liberally, both for other analysts
    and for future you.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则表达式*是由许多具有特殊含义的字符序列组成，用于定义搜索模式。学习正则表达式的主要挑战，以及在使用和维护包含它的代码时，其语法并不特别直观。代码片段读起来不像人类语言，甚至不像SQL或Python等计算机语言。然而，只要掌握了特殊字符的工作原理，就可以编写和解密代码。与我们所有查询的代码一样，从简单开始，根据需要增加复杂性，并在进行过程中检查结果是个好主意。同时，为其他分析员和未来的你留下大量注释也很重要。'
- en: Regex is a language, but it’s one that is used only within other languages.
    For example, regular expressions can be called within Java, Python, and SQL, but
    there is no independent way to program with them. All major databases have some
    implementation of regex. The syntax isn’t always exactly the same, but as with
    other functions, once you have a sense of the possibilities, adjusting syntax
    to your environment should be possible.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式是一种语言，但仅在其他语言中使用。例如，正则表达式可以在Java、Python和SQL中调用，但没有独立的编程方式。所有主要的数据库都有某种形式的正则表达式实现。尽管语法不总是完全相同，但与其他函数一样，一旦你了解了可能性，调整语法以适应你的环境就是可能的。
- en: A full explanation, and all of the syntax and ways to use regex, is beyond the
    scope of this book, but I’ll show you enough for you to get started and accomplish
    a number of common tasks in SQL. For a more thorough introduction, *[Learning
    Regular Expressions](https://oreil.ly/5aYkb)* by Ben Forta (O’Reilly) is a good
    choice. Here I’ll start by introducing the ways to indicate to the database that
    you are using a regex, and then I’ll introduce the syntax, before moving on to
    some examples of how regex can be useful in the UFO sighting reports analysis.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的解释以及正则表达式的所有语法和用法超出了本书的范围，但我将展示足够的内容让你开始并完成一些SQL中的常见任务。如果你需要更详细的介绍，可以选择Ben
    Forta（O’Reilly）的*[学习正则表达式](https://oreil.ly/5aYkb)*。首先，我将介绍如何向数据库指示你正在使用正则表达式，然后介绍语法，并且示例说明正则表达式在UFO目击报告分析中的实际用途。
- en: 'Regex can be used in SQL statements in a couple of ways. The first is with
    POSIX comparators, and the second is with regex functions. POSIX stands for Portable
    Operating System Interface and refers to a set of IEEE standards, but you don’t
    need to know any more than that to use POSIX comparators in your SQL code. The
    first comparator is the ~ (tilde) symbol, which compares two statements and returns
    TRUE if one string is contained in the other. As a simple example, we can check
    to see whether the string “The data is about UFOs” contains the string “data”:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SQL 语句中可以用正则表达式有两种方式。第一种是使用 POSIX 比较器，第二种是使用正则表达式函数。POSIX 代表可移植操作系统接口，是一组
    IEEE 标准，但你只需知道这些即可在 SQL 代码中使用 POSIX 比较器。第一个比较器是 ~（波浪线）符号，用于比较两个语句，如果一个字符串包含在另一个字符串中则返回
    TRUE。举个简单例子，我们可以检查字符串 “The data is about UFOs” 是否包含字符串 “data”：
- en: '[PRE41]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The return value is a BOOLEAN, TRUE or FALSE. Note that, although it doesn’t
    contain any special syntax, “data” is a regex. Regular expressions can also contain
    normal text strings. This example is similar to what could be accomplished with
    a LIKE operator. The ~ comparator is case sensitive. To make it case insensitive,
    similar to ILIKE, use ~* (the tilde followed by an asterisk):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值是一个 BOOLEAN，TRUE 或 FALSE。注意，尽管它不包含任何特殊语法，“data” 是一个正则表达式。正则表达式也可以包含普通文本字符串。这个例子类似于使用
    LIKE 运算符完成的功能。~ 比较器是区分大小写的。要使它不区分大小写，类似于 ILIKE，使用 ~*（波浪线后跟一个星号）：
- en: '[PRE42]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To negate the comparator, place an ! (exclamation point) before the tilde or
    tilde-asterisk combination:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要对比较器取反，可以在波浪线或波浪线后跟星号的组合前加一个 !（感叹号）：
- en: '[PRE43]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[Table 5-1](#posix_comparators) summarizes the four POSIX comparators.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5-1](#posix_comparators) 总结了四个 POSIX 比较器。'
- en: Table 5-1\. POSIX comparators
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-1\. POSIX 比较器
- en: '| Syntax | What it does | Case sensitive? |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 语法 | 功能 | 区分大小写？ |'
- en: '| --- | --- | --- |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ~ | Compares two statements and returns TRUE if one is contained in the other
    | Yes |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ~ | 比较两个语句，如果一个包含在另一个中则返回 TRUE | 是 |'
- en: '| ~* | Compares two statements and returns TRUE if one is contained in the
    other | No |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ~* | 比较两个语句，如果一个包含在另一个中则返回 TRUE | 否 |'
- en: '| !~ | Compares two statements and returns FALSE if one is contained in the
    other | Yes |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| !~ | 比较两个语句，如果一个包含在另一个中则返回 FALSE | 是 |'
- en: '| !~* | Compares two statements and returns FALSE if one is contained in the
    other | No |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| !~* | 比较两个语句，如果一个包含在另一个中则返回 FALSE | 否 |'
- en: 'Now that we have a way to introduce regex into our SQL, let’s get familiar
    with some of the special pattern-matching syntax it offers. The first special
    character to know is the . (period) symbol, a wildcard that is used to match any
    single character:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种方法在 SQL 中引入正则表达式，让我们熟悉一些它提供的特殊模式匹配语法。要知道的第一个特殊字符是 .（句点）符号，它是用于匹配任何单个字符的通配符：
- en: '[PRE44]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Let’s break this down in order to understand what’s going on and develop our
    intuition about how regex works. In the first comparison, the pattern tries to
    match any character, indicated by the period, a space, and then the word “data.”
    This pattern matches the string “e data” in the example sentence, so TRUE is returned.
    If this seems counterintuitive, since there are additional characters before the
    letter “e” and after the word “data,” remember that the comparator is only looking
    for this pattern somewhere within the string, similar to a LIKE operator. In the
    second comparison, the pattern tries to match any character followed by “The.”
    Since in the example sentence “The” is the start of the string and there are no
    characters before it, the value FALSE is returned.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个过程，以便理解正在发生的事情，并培养我们对正则表达式如何工作的直觉。在第一个比较中，模式尝试匹配任何字符（由句点表示），然后是一个空格，然后是单词
    “data”。这个模式在示例句子中匹配字符串 “e data”，因此返回 TRUE。如果这看起来反直觉，因为在字母 “e” 前面和 “data” 后面有额外的字符，记住比较器只是在字符串中某处寻找这个模式，类似于
    LIKE 运算符。在第二个比较中，模式尝试匹配任何字符后跟 “The”。由于在示例句子中 “The” 是字符串的开头，并且它之前没有字符，因此返回 FALSE。
- en: 'To match multiple characters, use the * (asterisk) symbol. This will match
    zero or more characters, similar to using the % (percent) symbol in a LIKE statement. 
    This use of the asterisk is different from placing it immediately after the tilde
    (~*), which makes the match case insensitive. Notice, however, that in this case
    “%” is not a wildcard and is instead treated as a literal character to be matched:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要匹配多个字符，请使用 *（星号）符号。这将匹配零个或多个字符，类似于在LIKE语句中使用%（百分号）符号。 这种星号的用法不同于将其立即放在波浪号（~*）之后，后者使匹配不区分大小写。但是请注意，在这种情况下，“%”不是通配符，而是要匹配的文字字符：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The next special characters to know are [ and ] (left and right brackets).
    These are used to enclose a set of characters, any one of which must match. The
    brackets match a single character even though multiple characters can be between
    them, though we’ll see shortly how to match more than one time. One use for the
    brackets is to make part of a pattern case insensitive by enclosing the uppercase
    and lowercase letters within the brackets (do not use a comma, as that would match
    the comma character itself):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个需要了解的特殊字符是 [ 和 ]（左方括号和右方括号）。它们用于括住一组字符，其中任何一个必须匹配。尽管方括号之间可以有多个字符，但它们匹配单个字符，不过我们很快将看到如何多次匹配。方括号的一个用途是通过在方括号内将大写和小写字母括起来使模式部分不区分大小写（不要使用逗号，因为那会匹配逗号字符本身）：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In this example, the pattern will match either “the” or “The”; since this string
    starts the example sentence, the statement returns the value TRUE. This isn’t
    quite the same thing as the case-insensitive match ~*, because in this case variations
    such as “tHe” and “THE” do not match the pattern:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，该模式将匹配“the”或“The”；由于此字符串是例句的开头，语句返回TRUE值。 这与不区分大小写匹配~*并不完全相同，因为在此案例中，“tHe”和“THE”等变体不匹配该模式：
- en: '[PRE47]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Another use of the bracket set match is to match a pattern that includes a
    number, allowing for any number. For example, imagine we wanted to match any description
    that mentions “7 minutes,” “8 minutes,” or “9 minutes.” This could be accomplished
    with a CASE statement with several LIKE operators, but with regex the pattern
    syntax is more compact:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 方括号集合匹配的另一种用途是匹配包含数字的模式，允许任何数字。例如，想象一下我们想匹配任何提到“7分钟”，“8分钟”或“9分钟”的描述。这可以通过使用带有几个LIKE操作符的CASE语句来实现，但是使用正则表达式的模式语法更加紧凑：
- en: '[PRE48]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To match any number, we could enclose all the digits between the brackets:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要匹配任何数字，我们可以在方括号之间包含所有数字：
- en: '[PRE49]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'However, regex allows a range of characters to be entered with a - (dash) separator. 
    All of the numbers can be indicated by [0-9]. Any smaller range of numbers can
    be used as well, such as [0-3] or [4-9]. This pattern, with a range, is equivalent
    to the last example that listed out each number:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正则表达式允许使用 -（短划线）分隔符输入一系列字符。 所有数字可以用[0-9]表示。 也可以使用更小的数字范围，例如[0-3]或[4-9]。 此模式，与范围一起使用，相当于最后一个列出每个数字的示例：
- en: '[PRE50]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Ranges of letters can be matched in a similar way. [Table 5-2](#regex_range_patterns)
    summarizes the range patterns that are most useful in SQL analysis. Nonnumber
    and nonletter values can also be placed between brackets, as in [$%@].
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 字母范围可以以类似的方式匹配。 [表 5-2](#regex_range_patterns) 总结了在SQL分析中最有用的范围模式。非数字和非字母的值也可以放在方括号之间，例如
    [$%@]。
- en: Table 5-2\. Regex range patterns
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-2\. 正则表达式范围模式
- en: '| Range pattern | Purpose |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 范围模式 | 目的 |'
- en: '| --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [0-9] | Match any number |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| [0-9] | 匹配任何数字 |'
- en: '| [a-z] | Match any lowercase letter |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [a-z] | 匹配任何小写字母 |'
- en: '| [A-Z] | Match any uppercase letter |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| [A-Z] | 匹配任何大写字母 |'
- en: '| [A-Za-z0-9] | Match any lower- or uppercase letter, or any number |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [A-Za-z0-9] | 匹配任何小写字母或大写字母，或任何数字 |'
- en: '| [A-z] | Match any ASCII character; generally not used because it matches
    everything, including symbols |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| [A-z] | 匹配任何ASCII字符；通常不使用，因为它匹配所有内容，包括符号 |'
- en: 'If the desired pattern match contains more than one instance of a particular
    value or type of value, one option is to include as many ranges as needed, one
    after the other. For example, we can match a three-digit number by repeating the
    number range notation three times:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所需的模式匹配包含特定值或类型的值的多个实例，则一种选项是包括所需数量的范围，依次排列。例如，我们可以通过多次重复数字范围符号来匹配三位数：
- en: '[PRE51]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Another option is to use one of the optional special syntaxes for repeating
    a pattern multiple times. This can be useful when you don’t know exactly how many
    times the pattern will repeat, but be careful to check the results to make sure
    you don’t accidentally return more matches than intended. To match one or more
    times, place the + (plus) symbol after the pattern:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用可选的特殊语法之一来多次重复模式。当你不确定模式将重复多少次时，这可能很有用，但要小心检查结果，确保不要意外返回比预期更多的匹配项。要匹配一次或多次，请在模式后面加上+（加号）符号：
- en: '[PRE52]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[Table 5-3](#table_five_threedotregex_patterns_for_m) summarizes the other
    options for indicating the number of times to repeat a pattern.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5-3](#table_five_threedotregex_patterns_for_m) 总结了指示重复模式次数的其他选项。'
- en: Table 5-3\. Regex patterns for matching a character set multiple times; in each
    case, the symbol or symbols are placed immediately after the set expression
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-3\. 用于多次匹配字符集的正则表达式模式；在每种情况下，符号或符号紧跟在集合表达式之后
- en: '| Symbol | Purpose |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 目的 |'
- en: '| --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| + | Match the character set one or more times |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| + | 匹配字符集一次或多次 |'
- en: '| * | Match the character set zero or more times |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| * | 匹配字符集零次或多次 |'
- en: '| ? | Match the character set zero or one time |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| ? | 匹配字符集零次或一次 |'
- en: '| { } | Match the character set the number of times specified between the curly
    braces; for example, {3} matches exactly three times |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| { } | 匹配指定次数的字符集；例如，{3} 精确匹配三次'
- en: '| { , } | Match the character set any number of times in a range specified
    by the comma-separated numbers between the curly braces; for example, {3,5} matches
    between three and five times |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| { , } | 在花括号之间指定的逗号分隔数字范围内匹配字符集的任意次数；例如，{3,5} 匹配三到五次 |'
- en: 'Sometimes rather than matching a pattern, we want to find items that do *not*
    match a pattern. This can be done by placing the ^ (caret) symbol before the pattern,
    which serves to negate the pattern:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们不想匹配一个模式，而是想找到不匹配模式的项。可以在模式前面放置^（插入符）符号来执行此操作，该符号用于否定模式：
- en: '[PRE53]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We might want to match a pattern that includes one of the special characters,
    so we need a way to tell the database to check for that literal character and
    not treat it as special. To do this, we need an escape character, which is the
    \ (backslash) symbol in regex:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要匹配包含特殊字符之一的模式，因此我们需要一种方法告诉数据库检查该文字字符而不将其视为特殊字符。为此，我们需要一个转义字符，在正则表达式中是反斜杠（\）符号：
- en: '[PRE54]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In the first line, omitting the backslash before the question mark causes the
    database to return an “invalid regular expression” error (the exact wording of
    the error may be different depending on the database type). In the second line,
    even though ^ is followed by one or more digits ([0-9]+), the database interprets
    the ^ in the comparison `'^[0-9]+'` to be a negation and will evaluate whether
    the string does not include the specified digits. The third line escapes the caret
    with a backslash, and the database now interprets this as the literal ^ character.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，省略问号前面的反斜杠会导致数据库返回“无效的正则表达式”错误（错误的确切措辞可能因数据库类型而异）。在第二行中，即使^后面跟着一个或多个数字（[0-9]+），数据库也会将^解释为否定，并评估字符串是否不包含指定的数字。第三行使用反斜杠转义插入符号，现在数据库将其解释为字面插入符号。
- en: 'Text data usually includes whitespace characters. These range from the space,
    which our eyes notice, to the subtle and sometimes unprinted tab and newline characters.
    We will see later how to replace these with regex, but for now let’s stick to
    how to match them in a regex. Tabs are matched with \t. Newlines are matched with
    \r for a carriage return or \n for a line feed, and depending on the operating
    system, sometimes both are required: \r\n. Experiment with your environment by
    running a few simple queries to see what returns the desired result. To match
    any whitespace character, use \s, but note that this also matches the space character:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据通常包括空白字符。这些字符从我们眼睛注意到的空格开始，到微妙且有时未打印的制表符和换行符。稍后我们将看到如何使用正则表达式替换这些字符，但现在让我们关注如何在正则表达式中匹配它们。制表符用\t匹配。换行符用\r（回车）或\n（换行符）匹配，根据操作系统的不同，有时需要同时使用：\r\n。通过运行几个简单的查询来尝试您的环境，看看返回什么结果可以达到期望的效果。要匹配任何空白字符，请使用\s，但请注意这也会匹配空格字符：
- en: '[PRE55]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Tip
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: SQL query tools or SQL query parsers may have trouble interpreting new lines
    typed directly into them and thus may return an error. If this is the case, try
    copying and pasting the text from the source rather than typing it in. All SQL
    query tools should be able to work with newlines that exist in a database table,
    however.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 查询工具或 SQL 查询解析器可能难以解释直接键入它们的新行，因此可能会返回错误。如果是这种情况，请尝试从源中复制并粘贴文本，而不是直接键入。所有
    SQL 查询工具应能够处理存在于数据库表中的换行。
- en: 'Similar to mathematical expressions, parentheses can be used to enclose expressions
    that should be treated together. For example, we might want to match a somewhat
    complex pattern that repeats several times:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 与数学表达式类似，括号可以用于包含应一起处理的表达式。例如，我们可能希望匹配一个相对复杂的模式，该模式重复多次：
- en: '[PRE56]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: All three lines use the same regex pattern, `'([0-9]{2}[a-z]){3}'`, for matching.
    The pattern inside the parentheses, `[0-9]{2}[a-z]`, looks for two digits followed
    by a lowercase letter. Outside of the parentheses, `{3}` indicates that the whole
    pattern should be repeated three times. The first line follows this pattern, since
    it contains the string `12a34b56c`. The second line does not match the pattern;
    it does have two digits followed by a lowercase letter (`23a`) and then two more
    digits (`23a45`), but this second repetition is followed by a third digit rather
    than by another lowercase letter (`23a456`), so there is no match. The third line
    has a matching pattern, `99x66y33z`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三行使用相同的正则表达式模式，`'([0-9]{2}[a-z]){3}'`，用于匹配。括号内的模式 `[0-9]{2}[a-z]` 寻找两位数字后跟一个小写字母。括号外的
    `{3}` 表示整个模式应重复三次。第一行符合此模式，因为它包含字符串 `12a34b56c`。第二行不匹配该模式；它确实有两位数字后跟一个小写字母 (`23a`)，然后又有两位数字
    (`23a45`)，但这第二次重复后面跟着第三位数字而不是另一个小写字母 (`23a456`)，因此没有匹配。第三行具有匹配模式 `99x66y33z`。
- en: 'As we’ve just seen, regex can be used in any number of combinations with other
    expressions, both regex and normal text, to create pattern-matching code. In addition
    to specifying *what* to match, regex can be used to specify *where* to match.
    Use the special character \y to match a pattern starting at the beginning or end
    of a word (in some databases, this might be \b instead). As an example, imagine
    we were interested in finding the word “car” in the UFO sighting reports. We could
    write an expression like this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚刚看到的，正则表达式可以与其他表达式（包括正则表达式和普通文本）以任意组合使用，以创建模式匹配代码。除了指定 *要* 匹配的内容外，正则表达式还可用于指定
    *匹配* 的位置。使用特殊字符 \y 可以在单词的开头或结尾匹配模式（在某些数据库中，可能是 \b）。举个例子，想象一下我们有兴趣在 UFO 目击报告中找到单词“car”。我们可以写出这样的表达式：
- en: '[PRE57]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'It finds “car” in the string and returns TRUE as expected. However, let’s look
    at a few more strings from the data set, looking for the same expression:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 它在字符串中找到“car”并返回预期的 TRUE。然而，让我们从数据集中再看几个字符串，寻找相同的表达式：
- en: '[PRE58]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'All of these strings match the pattern “car” as well, even though “scares,”
    “carpenter,” and “boxcar” aren’t exactly what was intended when we went looking
    for mentions of cars. To fix this, we can add \y to the beginning and end of the
    “car” pattern in our expression:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些字符串也会匹配模式“car”，尽管“scares”、“carpenter”和“boxcar”并不完全符合我们寻找车辆提及时的意图。为了修正这个问题，我们可以在表达式中“car”模式的开头和结尾添加
    \y：
- en: '[PRE59]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Of course, in this simple example, we could have simply added spaces before
    and after the word “car” with the same result. The benefit of the pattern is that
    it will also pick up cases in which the pattern is at the beginning of a string
    and thus does not have a leading space:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们当然可以在单词“car”前后加上空格，结果也是一样的。这种模式的好处在于，它还会匹配那些模式位于字符串开头的情况，因此没有前导空格：
- en: '[PRE60]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The pattern `''\ycar\y''` makes a case-insensitive match when “Car” is the
    first word, but the pattern `'' car ''` does not. To match the  beginning of an
    entire string, use the \A special character, and to match the end of a string,
    use \Z:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 模式 `'\ycar\y'` 在“Car”是第一个单词时进行不区分大小写的匹配，但模式 `' car '` 则不会。要匹配整个字符串的开头，请使用特殊字符
    \A，要匹配字符串的结尾，请使用 \Z：
- en: '[PRE61]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: In the first line, the pattern matches “Car” at the beginning of the string.
    The second line starts with “I,” so the pattern does not match. In the third line,
    the pattern is looking for “car” at the end of the string and does match it. Finally,
    in the fourth line, the last word is “home,” so the pattern does not match.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，模式在字符串开头匹配“Car”。第二行以“I”开头，因此模式不匹配。在第三行中，模式在字符串末尾寻找“car”并匹配成功。最后，在第四行中，最后一个单词是“home”，因此模式不匹配。
- en: If this is your first time working with regular expressions, it may take a few
    read-throughs and some experimentation in your SQL editor to get the hang of them.
    There’s nothing like working with real examples to help solidify learning, so
    next I’ll go through some applications to our UFO sightings analysis, and I’ll
    also introduce a couple of specific regex SQL functions.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用正则表达式，可能需要通过 SQL 编辑器多次阅读并进行一些实验才能掌握它们的使用方法。没有什么比实际示例更有助于巩固学习，接下来我将介绍一些应用于
    UFO 目击分析的示例，并介绍一些特定的 regex SQL 函数。
- en: Note
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Regular expression implementations vary widely by database vendor. The POSIX
    operators in this section work in Postgres and in databases derived from Postgres
    such as Amazon Redshift, but not necessarily in others.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数据库供应商的正则表达式实现差异很大。本节中的 POSIX 运算符适用于 Postgres 及其衍生的数据库，如 Amazon Redshift，但不一定适用于其他数据库。
- en: 'An alternative to the ~ operator is the `rlike` or `regexp_like` function (depending
    on the database). These have the following format:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ~ 运算符的另一种选择是`rlike`或`regexp_like`函数（取决于数据库）。它们的格式如下：
- en: '[PRE62]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The first example in this section would be written as:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的第一个示例将写成：
- en: '[PRE63]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The optional parameters control matching type, such as whether the match is
    case insensitive.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 可选参数控制匹配类型，例如匹配是否不区分大小写。
- en: Many of these databases have additional functions not covered here, such as
    `regexp_substr` to find matching substrings, and `regexp_count` to find the number
    of times a pattern is matched. Postgres supports POSIX but unfortunately does
    not support these other functions. Organizations that expect to do a lot of text
    analysis will do well to choose a database type with a robust set of regular expression
    functions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据库中有许多其他未在此处涵盖的额外函数，例如`regexp_substr`用于查找匹配的子字符串，以及`regexp_count`用于计算模式匹配的次数。Postgres
    支持 POSIX，但遗憾的是不支持这些其他函数。希望进行大量文本分析的组织最好选择一个具有强大正则表达式函数集的数据库类型。
- en: Finding and replacing with regex
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 regex 进行查找和替换
- en: In the previous section, we discussed regular expressions and how to construct
    patterns with regex to match parts of strings in our data sets. Let’s apply this
    technique to the UFO sightings data set to see how it works in practice. Along
    the way, I’ll also introduce some additional regex SQL functions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了正则表达式以及如何使用 regex 构造模式来匹配数据集中字符串的部分。让我们将这一技术应用到 UFO 目击数据集中，看看它在实践中的效果。在此过程中，我还将介绍一些额外的
    regex SQL 函数。
- en: 'The sighting reports contain a variety of details, such as what the reporter
    was doing at the time of the sighting and when and where they were doing it. Another
    detail commonly mentioned is seeing some number of lights. As a first example,
    let’s find the descriptions that contain a number and the word “light” or “lights.”
    For the sake of display in this book, I’ll just check the first 100 characters,
    but this code can also work across the entire description field:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 目击报告包含各种细节，例如目击者在目击时正在做什么，何时何地进行目击。另一个常被提及的细节是看到一些数量的光。作为第一个示例，让我们找出包含数字和单词“light”或“lights”的描述。为了在本书中展示，我将只检查前100个字符，但此代码也可以在整个描述字段中运行：
- en: '[PRE64]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The regular expression pattern matches any number of digits ([0-9]+), followed
    by a space, then the string “light”, and finally either a letter “s,” a space,
    a comma, or a period. In addition to finding the relevant records, we might want
    to split out just the part that refers to the number and the word “lights.” To
    do this, we’ll use the regex function `regexp_matches`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式模式匹配任意数量的数字（[0-9]+），然后是一个空格，然后是字符串“light”，最后是字母“s”，空格，逗号或句号之一。除了找到相关记录外，我们可能还想分离出仅涉及数字和单词“lights”的部分。为此，我们将使用
    regex 函数`regexp_matches`。
- en: Tip
  id: totrans-261
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Regex function support varies widely by database vendor and sometimes by database
    software version. SQL Server does not support the functions, while MySQL has minimal
    support for them. Analytic databases such as Redshift, Snowflake, and Vertica
    support a variety of useful functions. Postgres has only match and replace functions.
    Explore the documentation for your database for specific function availability.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式函数的支持因数据库供应商而异，有时也因数据库软件版本而异。SQL Server不支持这些函数，而MySQL对它们的支持很有限。Redshift、Snowflake和Vertica等分析型数据库支持各种有用的函数。Postgres只支持匹配和替换函数。请查阅您的数据库文档以了解特定函数的可用性。
- en: 'The `regexp_matches` function takes two arguments: a string to search and a
    regex match pattern. It returns an array of the string(s) that matched the pattern.
    If there are no matches, a null value is returned. Since the return value is an
    array, we’ll use an index of [1] to return just a single value as a VARCHAR, which
    will allow for additional string manipulation as needed. If you are working in
    another type of database, the `regexp_substr` function is similar to `regexp_matches`,
    but it returns a VARCHAR value, so there is no need to add the [1] index.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`regexp_matches`函数接受两个参数：要搜索的字符串和正则表达式匹配模式。它返回一个匹配模式的字符串数组。如果没有匹配项，则返回空值。由于返回值是一个数组，我们将使用索引[1]只返回一个VARCHAR值，这将允许根据需要进行其他字符串操作。如果您在其他类型的数据库中工作，`regexp_substr`函数类似于`regexp_matches`，但它返回一个VARCHAR值，因此不需要添加[1]索引。'
- en: Tip
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: An *array* is a collection of objects stored together in the computer’s memory.
    In databases, arrays are enclosed in { } (curly braces), and this is a good way
    to spot that something in the database is not one of the regular data types we’ve
    been working with so far. Arrays have some advantages when storing and retrieving
    data, but they are not as easy to work with in SQL since they require special
    syntax. Elements in an array are accessed using [ ] (square brackets) notation.
    For our purposes here, it’s enough to know that the first element is found with
    [1], the second with [2], and so on.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*数组*是存储在计算机内存中的对象集合。在数据库中，数组被包含在{ }（大括号）中，这是一种识别数据库中不是我们迄今为止一直在处理的常规数据类型之一的好方法。数组在存储和检索数据时具有一些优势，但在SQL中使用起来不那么简单，因为它们需要特殊的语法。数组中的元素使用[
    ]（方括号）表示法访问。对于我们的目的，知道第一个元素可以用[1]找到，第二个可以用[2]，以此类推，已经足够了。'
- en: 'Building on our example, we can parse the desired value, the number, and the
    word “light(s)” from the description field and then *GROUP BY* this value and
    the most common variations:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们的例子，我们可以从描述字段中解析所需的值，即数字和“light(s)”一词，然后按此值和最常见的变体*GROUP BY*：
- en: '[PRE65]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The top 10 results are graphed in [Figure 5-9](#number_of_lights_mentioned_at_the_begin).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 前10个结果在[图 5-9](#number_of_lights_mentioned_at_the_begin)中绘制。
- en: '![](Images/sfda_0509.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0509.png)'
- en: Figure 5-9\. Number of lights mentioned at the beginning of UFO sighting descriptions
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. UFO目击描述开头提到的灯数
- en: 'Reports mentioning three lights are more than twice as common as the second
    most often mentioned number of lights, and from two to six lights are most commonly
    seen. To find the full range of the number of lights, we can parse the matched
    text and then find the `min` and `max` values:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 提到三个灯的报告比第二经常提到的灯数多两倍以上，从两到六盏灯最常见。要找到灯数的完整范围，我们可以解析匹配的文本，然后找到`min`和`max`值：
- en: '[PRE66]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: At least one report mentions two thousand lights, and the minimum value of zero
    lights is also mentioned. We might want to review these reports further to see
    if there is anything else interesting or unusual about these extreme values.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有一份报告提到了两千盏灯，而零灯的最小值也被提到。我们可能需要进一步审查这些报告，看看这些极端值是否还有其他有趣或不寻常的内容。
- en: 'In addition to finding matches, we might want to replace the matched text with
    some alternate text. This is particularly useful when trying to clean a data set
    of text that has multiple spellings for the same underlying thing. The `regexp_replace`
    function can accomplish this. It is similar to the `replace` function discussed
    earlier in the chapter, but it can take a regular expression argument as the pattern
    to match. The syntax is similar to the `replace` function:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 除了找到匹配项之外，我们可能希望用一些替代文本替换匹配的文本。当尝试清理数据集中具有同一基础内容多种拼写的文本时，这尤为有用。`regexp_replace`函数可以实现这一点。它类似于本章前面讨论过的`replace`函数，但它可以接受正则表达式作为匹配模式的参数。语法与`replace`函数类似：
- en: '[PRE67]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let’s put this to work to try to clean up the `duration` field that we parsed
    out of the `sighting_report` column earlier. This appears to be a free text entry
    field, and there are more than eight thousand different values. However, inspection
    reveals that there are common themes—most refer to some combination of seconds,
    minutes, and hours:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其应用于之前从`sighting_report`列中解析出来的`duration`字段的清理工作。这似乎是一个自由文本输入字段，有超过八千个不同的值。但是，检查后发现有共同的主题——大多数涉及秒、分钟和小时的某种组合：
- en: '[PRE68]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Within this sample, the durations of “10 minutes,” “10 min,” and “10 mins”
    all represent the same amount of time, but the database doesn’t know to combine
    them because the spellings are slightly different. We could use a series of nested
    `replace` functions to convert all these different spellings. However, we would
    also have to take into account other variations, such as capitalizations. Regex
    is handy in this situation, allowing us to create more compact code. The first
    step is to develop a pattern that matches the desired string, which we can do
    with the `regexp_matches` function. It’s a good idea to review this intermediate
    step to make sure we’re matching the correct text:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，“10 minutes”，“10 min”和“10 mins”的持续时间都表示相同的时间量，但是数据库并不知道如何将它们合并，因为拼写略有不同。我们可以使用一系列嵌套的`replace`函数来转换所有这些不同的拼写。但是，我们还必须考虑到其他变体，例如大小写。在这种情况下，正则表达式非常方便，允许我们创建更紧凑的代码。第一步是开发一个匹配所需字符串的模式，我们可以使用`regexp_matches`函数来实现。审查这个中间步骤以确保我们匹配到了正确的文本是个好主意：
- en: '[PRE69]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let’s break this down. In the subquery, the `duration` value is split out of
    the `sighting_report` field. Then the `regexp_matches` function looks for strings
    that match the pattern:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分解一下。在子查询中，`duration`值从`sighting_report`字段中分离出来。然后，`regexp_matches`函数查找与模式匹配的字符串：
- en: '[PRE70]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This pattern starts at the beginning of a word (\m) and then looks for any
    sequence of the letters “m,” “i,” and “n,” regardless of capitalization ([Mm]
    and so on). Next, it looks for zero or more instances of any other lowercase or
    uppercase letter ([A-Za-z]*), and then it finally checks for the end of a word
    (\y) so that only the word that includes the variation of “minutes” is included
    and not the rest of the string. Notice that the “+” and “?” characters are not
    matched. With this pattern, we can now replace all these variations with the standard
    value “min”:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式从单词的开头（\m）开始，并查找“m”，“i”和“n”这些字母的任意序列，不区分大小写（[Mm]等）。接下来，它查找零个或多个其他小写或大写字母（[A-Za-z]*），最后检查单词的结尾（\y），以便只包括包含“minutes”变体的单词，而不是字符串的其余部分。请注意，“+”和“?”字符不匹配。有了这个模式，我们现在可以用标准值“min”替换所有这些变体：
- en: '[PRE71]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The values in the `replaced_text` column are much more standardized now. The
    period, plus, and question mark characters could also be replaced by enhancing
    the regex. From an analytical standpoint, however, we might want to consider how
    to represent the uncertainty that the plus and question mark represent. The `regexp_replace`
    functions can be nested in order to achieve replacement of different parts or
    types of strings. For example, we can standardize both the minutes and the hours:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`replaced_text`列中的值现在更加标准化了。句号、加号和问号字符也可以通过增强正则表达式进行替换。然而，从分析的角度来看，我们可能需要考虑如何表示加号和问号所代表的不确定性。`regexp_replace`函数可以嵌套使用，以实现对不同部分或类型字符串的替换。例如，我们可以标准化分钟和小时：'
- en: '[PRE72]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The regex for hours is similar to the one for minutes, looking for case-insensitive
    matches of “hour” at the beginning of a word, followed by zero or more other letter
    characters before the end of the word. The intermediate hour and minutes matches
    may not be needed in the final result, but I find them helpful to review as I’m
    developing my SQL code to prevent errors later. A full cleaning of the `duration`
    column would likely involve many more lines of code, and it’s all too easy to
    lose track and introduce a typo.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 小时的正则表达式与分钟的相似，查找在单词开头的不区分大小写的“hour”，后跟零个或多个其他字母字符，直到单词的结尾。在最终结果中可能不需要中间的小时和分钟匹配，但我发现它们在开发SQL代码时有助于检查，以防后续出现错误。对`duration`列的完整清理可能需要更多行代码，很容易迷失并引入拼写错误。
- en: The `regexp_replace` function can be nested any number of times, or it can be
    combined with the basic `replace` function. Another use for `regexp_replace` is
    in CASE statements, for targeted replacement when conditions in the statement
    are met. Regex is a powerful and flexible tool within SQL that, as we’ve seen,
    can be used in a number of ways within an overall SQL query.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`regexp_replace` 函数可以嵌套任意次数，或者可以与基本的 `replace` 函数结合使用。`regexp_replace` 的另一个用途是在
    CASE 语句中，用于在满足语句中条件时进行有针对性的替换。正则表达式是 SQL 中强大且灵活的工具，正如我们所见，它可以在整体 SQL 查询中以多种方式使用。'
- en: 'In this section, I’ve introduced a number of ways to search for, find, and
    replace specific elements within longer texts, from wildcard matches with LIKE
    to IN lists and more complex pattern matching with regex. All of these, along
    with the text-parsing and transformation functions introduced earlier, allow us
    to create customized rule sets with as much complexity as needed to handle the
    data sets in hand. It’s worth keeping in mind the balance between complexity and
    maintenance burden, however. For one-time analysis of a data set, it can be worth
    the trouble to create complex rule sets that perfectly clean the data. For ongoing
    reporting and monitoring, it’s usually worthwhile to explore options for receiving
    cleaner data from data sources. Next, we’ll turn to several ways to construct
    new text strings with SQL: using constants, existing strings, and parsed strings.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我介绍了许多搜索、查找和替换长文本中特定元素的方法，从通配符匹配（LIKE）到 IN 列表，再到使用正则表达式进行更复杂的模式匹配。所有这些方法，连同之前介绍的文本解析和转换函数，使我们能够创建具有处理当前数据集所需复杂性的定制规则集。然而，值得注意的是，在一次性数据集分析中，创建完美清理数据的复杂规则集可能值得一试。对于持续报告和监控，通常值得探索从数据源获得更清洁数据的选项。接下来，我们将讨论几种使用
    SQL 构建新文本字符串的方法：使用常量、现有字符串和解析字符串。
- en: Constructing and Reshaping Text
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和重塑文本
- en: 'We’ve seen how to parse, transform, find, and replace elements of strings in
    order to perform a variety of cleaning and analysis tasks with SQL. In addition
    to these, SQL can be used to generate new combinations of text. In this section,
    I’ll first discuss *concatenation*, which allows different fields and types of
    data to be consolidated into a single field. Then I’ll discuss changing text shape
    with functions that combine multiple columns into a single row, as well as the
    opposite: breaking up a single string into multiple rows.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何解析、转换、查找和替换字符串的元素，以执行各种清理和分析任务。除了这些功能之外，SQL 还可以用于生成文本的新组合。在本节中，我将首先讨论
    *连接*，它允许不同字段和数据类型被合并为单个字段。然后我将讨论使用函数将多列合并为单行的文本形状变化，以及相反的操作：将单个字符串拆分为多行。
- en: Concatenation
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接
- en: 'New text can be created with SQL with concatenation. Any combination of constant
    or hardcoded text, database fields, and calculations on those fields can be joined
    together. There are a few ways to concatenate. Most databases support the `concat`
    function, which takes as arguments the fields or values to be concatenated:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 SQL 进行文本的连接来创建新文本。可以将常量或硬编码文本、数据库字段及这些字段上的计算结合在一起。有几种连接的方法。大多数数据库支持 `concat`
    函数，该函数将字段或值作为参数进行连接：
- en: '[PRE73]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Some databases support the `concat_ws` (concatenate with separator) function,
    which takes a separator value as the first argument, followed by the list of values
    to concatenate. This is useful when there are multiple values that you want to
    put together, using a comma, dash, or similar element to separate them:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据库支持 `concat_ws`（带分隔符的连接）函数，该函数以分隔符值作为第一个参数，后跟要连接的值列表。当有多个值需要使用逗号、破折号或类似元素分隔时，这非常有用：
- en: '[PRE74]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Finally, || (double pipe) can be used in  many databases to concatenate strings
    (SQL Server uses + instead):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，||（双竖线）可以在许多数据库中用于连接字符串（SQL Server 使用 + 代替）：
- en: '[PRE75]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Tip
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If any of the values in a concatenation are null, the database will return null.
    Be sure to use `coalesce` or CASE to replace null values with a default if you
    suspect they can occur.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果连接中的任何值为空，数据库将返回空。如果怀疑可能出现空值，请务必使用 `coalesce` 或 CASE 替换空值为默认值。
- en: 'Concatenation can bring together a field and a constant string. For example,
    imagine we wanted to label the shapes as such and add the word “reports” to the
    count of reports for each shape. The subquery parses the name of the shape from
    the `sighting_report` field and `count`s the number of records. The outer query
    concatenates the shapes with the string `'' (shape)''` and the `reports` with
    the string `'' reports''`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 连接可以将字段和常量字符串组合在一起。例如，想象一下，我们想要将形状标记为这样，并在每个形状的报告计数中添加“报告”一词。子查询从 `sighting_report`
    字段中解析形状的名称并计算记录数。外部查询使用字符串 `' (shape)'` 和 `' reports'` 连接形状和报告：
- en: '[PRE76]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We can also combine two fields together, optionally with a string separator.
    For example, we could unite the shape and location values into a single field:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将两个字段结合在一起，可选地使用字符串分隔符。例如，我们可以将形状和位置的值合并到一个字段中：
- en: '[PRE77]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The top 10 combinations are graphed in [Figure 5-10](#top_combinations_of_shape_and_location).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 前 10 个组合在 [图 5-10](#top_combinations_of_shape_and_location) 中绘制出来。
- en: '![](Images/sfda_0510.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0510.png)'
- en: Figure 5-10\. Top combinations of shape and location in UFO sightings
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. UFO 目击中形状和位置的顶级组合
- en: We saw earlier that “light” is the most common shape, so it’s not surprising
    that it appears in each of the top results. Phoenix is the most common location,
    while Las Vegas is the second most common overall.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，“光”是最常见的形状，因此它出现在每个顶级结果中并不奇怪。Phoenix 是最常见的位置，而拉斯维加斯则是整体第二常见的。
- en: 'In this case, since we went to so much trouble to parse out the different fields,
    it might not make as much sense to concatenate them back together. However, it
    can be useful to rearrange text or combine values into a single field for display
    in another tool. By combining various fields and text, we can also generate sentences
    that can function as summaries of the data, for use in emails or automated reports.
    In this example, subquery `a` parses the `occurred` and `shape` fields, as we’ve
    seen previously, and `count`s the records. Then in subquery `aa`, the `min` and
    `max` of `occurred` are calculated, along with the total number of `reports`,
    and the results are *GROUP*ed *BY* `shape`. Rows with `occurred` fields shorter
    than eight characters are excluded, to remove ones that don’t have properly formed
    dates and avoid errors in the `min` and `max` calculations. Finally, in the outer
    query, the final text is assembled with the `concat` function. The format of the
    dates is changed to read as long dates (April 9, 1957) for the earliest and latest
    dates:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于我们费了那么大的劲来解析不同的字段，将它们再次连接在一起可能就没有那么有意义了。但是，将文本重新排列或将值组合到单个字段中以便在其他工具中显示是很有用的。通过组合各种字段和文本，我们还可以生成作为数据摘要的句子，用于电子邮件或自动报告。在这个例子中，子查询
    `a` 解析了 `occurred` 和 `shape` 字段，正如我们之前所看到的，然后 `count` 记录。然后在子查询 `aa` 中，计算了 `occurred`
    的 `min` 和 `max`，以及 `reports` 的总数，并按 `shape` 进行了 *GROUP*。排除了 `occurred` 字段长度少于八个字符的行，以删除没有正确格式化日期并避免在
    `min` 和 `max` 计算中出现错误的行。最后，在外部查询中，使用 `concat` 函数组装最终的文本。日期的格式被更改为长日期形式（1957年4月9日），显示最早和最近的日期：
- en: '[PRE78]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: We could get even more creative with formatting the number of reports or adding
    `coalesce` or CASE statements to handle blank shape names, for example. Although
    these sentences are repetitive and are therefore no match for human (or AI) writers,
    they will be dynamic if the data source is frequently updated and thus can be
    useful in reporting applications.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更有创意地格式化报告的数量，或者添加 `coalesce` 或 CASE 语句来处理空白的形状名称，例如。尽管这些句子重复而且因此无法与人类（或
    AI）写作相比，但如果数据源经常更新，它们将是动态的，并且因此在报告应用程序中非常有用。
- en: Along with functions and operators for creating new text with concatenation,
    SQL has some special functions for reshaping text, which we’ll turn to next.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于通过连接创建新文本的函数和操作符外，SQL 还有一些专门用于重塑文本的特殊函数，我们将在下文讨论。
- en: Reshaping Text
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重塑文本
- en: As we saw in [Chapter 2](ch02.xhtml#preparing_data_for_analysis), changing the
    shape of the data—either pivoting from rows to columns or the reverse, changing
    the data from columns to rows—is sometimes useful. We saw how to do that with
    *GROUP BY* and aggregations, or with *UNION* statements. In SQL there are some
    special functions for reshaping text, however.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第2章](ch02.xhtml#preparing_data_for_analysis) 中所看到的，改变数据的形状——无论是从行到列的透视还是反过来，即从列到行的改变——有时是有用的。我们看到了如何通过
    *GROUP BY* 和聚合，或者 *UNION* 语句来实现这一点。在 SQL 中，还有一些专门用于重塑文本的特殊函数。
- en: 'One use case for reshaping text is when there are multiple rows with different
    text values for an entity and we would like to combine them into a single value.
    Combining values can make them more difficult to analyze, of course, but sometimes
    the use case requires a single record per entity in the output. Combining the
    individual values into a single field allows us to retain the detail. The `string_agg`
    function takes two arguments, a field or an expression, and a separator, which
    is commonly a comma but can be any separator character desired. The function aggregates
    only values that are not null, and the order can be controlled with an *ORDER
    BY* clause within the function as needed:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 文本重塑的一个用例是当实体具有多个行，每个行具有不同的文本值，我们希望将它们组合成一个单一值时。当然，将值合并可能会使其更难分析，但有时候用例需要输出中每个实体只有一个记录。将各个值合并为单个字段允许我们保留细节。`string_agg`
    函数接受两个参数，一个字段或表达式，以及一个分隔符，通常是逗号，但可以是任何所需的分隔符字符。该函数仅聚合非空值，并可以根据需要在函数内使用 *ORDER
    BY* 子句控制顺序：
- en: '[PRE79]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Since `string_agg` is an aggregate function, it requires a *GROUP BY* clause
    on the other fields in the query. In MySQL, an equivalent function is `group_concat`,
    and analytic databases such as Redshift and Snowflake have a similar function
    called `listagg`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `string_agg` 是一个聚合函数，它要求查询中其他字段上有一个 *GROUP BY* 子句。在 MySQL 中，一个等效的函数是 `group_concat`，而像
    Redshift 和 Snowflake 这样的分析数据库有一个类似的函数称为 `listagg`。
- en: 'Another use case is to do just the opposite of `string_agg` and instead split
    out a single field into multiple rows. There is a lot of inconsistency in how
    this is implemented in different databases, and even whether a function exists
    for this at all. Postgres has a function called `regexp_split_to_table`, while
    certain other databases have a `split_to_table` function that operates similarly
    (check documentation for availability and syntax in your database). The `regexp_split_to_table`
    function takes two arguments, a string value and a delimiter. The delimiter can
    be a regular expression, but keep in mind that a regex can also be a simple string
    such as a comma or space character. The function then splits the values into rows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用例是执行与 `string_agg` 完全相反的操作，而是将单个字段拆分为多行。在不同数据库中，这种实现方式存在很多不一致，甚至是否存在此类函数也不一定。Postgres
    提供了一个名为 `regexp_split_to_table` 的函数，而某些其他数据库则有类似操作的 `split_to_table` 函数（请查看数据库文档以确定可用性和语法）。`regexp_split_to_table`
    函数接受两个参数，一个字符串值和一个分隔符。分隔符可以是正则表达式，但请注意正则表达式也可以是一个简单的字符串，如逗号或空格字符。该函数然后将值拆分为行：
- en: '[PRE80]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The string to be split can include anything and doesn’t necessarily need to
    be a list. We can use the function to split up any string, including sentences.
    We can then use this to find the most common words used in text fields, a potentially
    useful tool for text analysis work. Let’s take a look at the most common words
    used in UFO sighting report descriptions:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 要拆分的字符串可以包含任何内容，不一定是列表。我们可以使用该函数拆分任何字符串，包括句子。然后，我们可以使用它来查找文本字段中使用最频繁的单词，这是文本分析工作中的一个潜在有用工具。让我们来看看
    UFO 目击报告描述中使用最频繁的单词：
- en: '[PRE81]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: The subquery first transforms the `description` into lowercase, since case variations
    are not interesting for this example. Next, the string is split using the regex
    `'\s+'`, which splits on any one or more whitespace characters.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 子查询首先将 `description` 转换为小写，因为对于本例来说大小写变化并不重要。接下来，使用正则表达式 `'\s+'` 对字符串进行拆分，该正则表达式在任何一个或多个空格字符上进行拆分。
- en: 'The most commonly used words are not surprising; however, they are not particularly
    useful since they are just commonly used words in general. To find a more meaningful
    list, we can remove what are called *stop words*. These are simply the most commonly
    used words in a language. Some databases have built-in lists in what are called
    dictionaries, but the implementations are not standard. There is also no single
    agreed-upon correct list of stop words, and it is common to adjust the particular
    list for the desired application; however, there are a number of lists of common
    stop words on the internet. For this example, I loaded a list of 421 common words
    into a table called `stop_words`, available on the book’s [GitHub site](https://oreil.ly/94jIS).
    The stop words are removed from the result set with a *LEFT JOIN* to the `stop_words`
    table, filtered to results that are not in that table:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的单词并不令人惊讶；然而，它们并不特别有用，因为它们只是一般使用的常见单词。要找到更有意义的列表，我们可以删除所谓的*停用词*。这些只是语言中最常用的单词。一些数据库在所谓的字典中内置了内置列表，但这些实现并不标准。没有单一一致的正确停用词列表，并且通常会根据所需应用调整特定列表；然而，在互联网上有许多常见停用词列表。例如，在这个示例中，我加载了一个包含421个常见词的表`stop_words`，可以在本书的[GitHub网站](https://oreil.ly/94jIS)上找到。使用*LEFT
    JOIN*到`stop_words`表将这些停用词从结果集中移除，结果集中包含不在该表中的结果。
- en: '[PRE82]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The top 10 most common words are graphed in [Figure 5-11](#most_common_words_in_ufo_sighting_descr).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的前10个单词在[UFO目击描述中的图5-11](#most_common_words_in_ufo_sighting_descr)中绘制成图表。
- en: '![](Images/sfda_0511.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0511.png)'
- en: Figure 5-11\. Most common words in UFO sighting descriptions, excluding stop
    words
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-11\. UFO目击描述中最常见的单词，不包括停用词
- en: We could continue to get more sophisticated by adding additional common words
    to the `stop_words` table or by *JOIN*ing the results with the descriptions to
    tag them with the interesting words they contain. Note that `regexp_split_to_table`
    and similar functions in other databases can be slow, depending on the length
    and number of records analyzed.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向`stop_words`表中添加更多常见词或将结果*JOIN*到描述中以标记包含有趣词汇的内容来进一步复杂化。请注意，`regexp_split_to_table`和其他数据库中类似的函数可能会很慢，具体取决于所分析记录的长度和数量。
- en: Constructing and reshaping text with SQL can be done in as simple or complex
    a way or ways as needed. Concatenation, string aggregation, and string-splitting
    functions can be used alone, in combination with each other, and with other SQL
    functions and operators to achieve the desired data output.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SQL进行文本构造和重塑可以根据需要以简单或复杂的方式进行。连接、字符串聚合以及字符串分割函数可以单独使用，也可以互相结合，或与其他SQL函数和操作符结合使用，以实现所需的数据输出。
- en: Conclusion
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Although SQL isn’t always the first tool mentioned when it comes to text analysis,
    it has many powerful functions and operators for accomplishing a variety of tasks.
    From parsing and transformations, to finding and replacing, to constructing and
    reshaping text, SQL can be used to both clean and prepare text data as well as
    perform analysis.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在文本分析中，SQL并不总是第一个提到的工具，但它具有许多强大的函数和操作符，可以完成各种任务。从解析和转换，到查找和替换，再到构造和重塑文本，SQL既可以用来清洁和准备文本数据，也可以进行分析。
- en: In the next chapter, we’ll turn to using SQL for anomaly detection, another
    topic in which SQL isn’t always the first tool mentioned but for which it has
    surprising capabilities.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用SQL进行异常检测，这是另一个SQL并非总是第一个提到的工具的主题，但它具有令人惊讶的能力。
- en: ^([1](ch05.xhtml#ch01fn7-marker)) Since the data set was created in the United
    States, it is in mm/dd/yyyy format. Many other parts of the world use the dd/mm/yyyy
    format instead. It’s always worth checking your source and adjusting your code
    as needed.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#ch01fn7-marker)) 由于数据集是在美国创建的，因此采用mm/dd/yyyy格式。世界其他地区多使用dd/mm/yyyy格式。检查源数据并根据需要调整代码始终是值得的。
