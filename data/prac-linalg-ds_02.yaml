- en: Chapter 2\. Vectors, Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。向量，第1部分
- en: Vectors provide the foundations upon which all of linear algebra (and therefore,
    the rest of this book) is built.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 向量提供了构建线性代数（因此也是本书其余部分）的基础。
- en: 'By the end of this chapter, you will know all about vectors: what they are,
    what they do, how to interpret them, and how to create and work with them in Python.
    You will understand the most important operations acting on vectors, including
    vector algebra and the dot product. Finally, you will learn about vector decompositions,
    which is one of the main goals of linear algebra.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将了解关于向量的一切：它们是什么，它们的作用是什么，如何解释它们，以及如何在Python中创建和操作它们。你将理解最重要的作用于向量的操作，包括向量代数和点积。最后，你将学习向量分解，这是线性代数的主要目标之一。
- en: Creating and Visualizing Vectors in NumPy
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在NumPy中创建和可视化向量
- en: In linear algebra, a *vector* is an ordered list of numbers. (In abstract linear
    algebra, vectors may contain other mathematical objects including functions; however,
    because this book is focused on applications, we will only consider vectors comprising
    numbers.)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，*向量* 是一组有序的数字列表。（在抽象的线性代数中，向量可以包含其他数学对象，包括函数；但由于本书专注于应用，我们只考虑由数字组成的向量。）
- en: 'Vectors have several important characteristics. The first two we will start
    with are:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 向量具有几个重要特征。我们将从前两个开始：
- en: Dimensionality
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 维度
- en: The number of numbers in the vector
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 向量中的数字数量
- en: Orientation
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 方向
- en: Whether the vector is in *column orientation* (standing up tall) or *row orientation*
    (laying flat and wide)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 向量是在 *列方向*（竖直）或 *行方向*（水平）的情况下
- en: Dimensionality is often indicated using a fancy-looking <math alttext="double-struck
    upper R Superscript upper N"><msup><mi>ℝ</mi> <mi>N</mi></msup></math> , where
    the <math alttext="double-struck upper R"><mi>ℝ</mi></math> indicates real-valued
    numbers (cf. <math alttext="double-struck upper C"><mi>ℂ</mi></math> for complex-valued
    numbers) and the <math alttext="Superscript upper N"><msup><mi>N</mi></msup></math>
    indicates the dimensionality. For example, a vector with two elements is said
    to be a member of <math alttext="double-struck upper R squared"><msup><mi>ℝ</mi>
    <mn>2</mn></msup></math> . That special <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    character is made using latex code, but you can also write R², R2, or R^2.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 维度常用一个看起来很复杂的 <math alttext="double-struck upper R Superscript upper N"><msup><mi>ℝ</mi>
    <mi>N</mi></msup></math> 表示，其中 <math alttext="double-struck upper R"><mi>ℝ</mi></math>
    表示实数（对比 <math alttext="double-struck upper C"><mi>ℂ</mi></math> 表示复数），而 <math
    alttext="Superscript upper N"><msup><mi>N</mi></msup></math> 表示维度。例如，具有两个元素的向量称为
    <math alttext="double-struck upper R squared"><msup><mi>ℝ</mi> <mn>2</mn></msup></math>
    的成员。这个特殊的 <math alttext="double-struck upper R"><mi>ℝ</mi></math> 字符是使用LaTeX代码制作的，但你也可以写成
    R²、R2 或者 R^2。
- en: '[Equation 2-1](#eq-1) shows a few examples of vectors; please determine their
    dimensionality and orientation before reading the subsequent paragraph.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程2-1](#eq-1) 展示了几个向量的例子；请在阅读后续段落之前确定它们的维度和方向。'
- en: Equation 2-1\. Examples of column vectors and row vectors
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程2-1。列向量和行向量的例子
- en: <math display="block"><mrow><mi>𝐱</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mi>𝐲</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mn>.3</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mo>-</mo> <mn>7</mn></mrow></mtd></mtr></mtable></mfenced> <mo>,</mo>
    <mi>𝐳</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>4</mn></mtd> <mtd><mn>5</mn></mtd> <mtd><mn>6</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>𝐱</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mi>𝐲</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mn>.3</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mo>-</mo> <mn>7</mn></mrow></mtd></mtr></mtable></mfenced> <mo>,</mo>
    <mi>𝐳</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>4</mn></mtd> <mtd><mn>5</mn></mtd> <mtd><mn>6</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Here are the answers: **x** is a 4D column vector, **y** is a 2D column vector,
    and **z** is a 4D row vector. You can also write, e.g., <math alttext="bold x
    element-of double-struck upper R Superscript 4"><mrow><mi>𝐱</mi> <mo>∈</mo> <msup><mi>ℝ</mi>
    <mn>4</mn></msup></mrow></math> , where the <math alttext="element-of"><mo>∈</mo></math>
    symbol means “is contained in the set of.”'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是答案：**x** 是一个4维列向量，**y** 是一个2维列向量，而 **z** 是一个4维行向量。你也可以写成，例如，<math alttext="bold
    x element-of double-struck upper R Superscript 4"><mrow><mi>𝐱</mi> <mo>∈</mo>
    <msup><mi>ℝ</mi> <mn>4</mn></msup></mrow></math>，其中 <math alttext="element-of"><mo>∈</mo></math>
    符号表示“属于集合”。
- en: Are **x** and **z** the same vector? Technically they are different, even though
    they have the same elements in the same order. See [“Does Vector Orientation Matter?”](#vector-orientation)
    for more discussion.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** 和 **z** 是同一个向量吗？从技术上讲，它们是不同的，即使它们的元素顺序相同。详细讨论请参见 [“向量方向是否重要？”](#vector-orientation)。'
- en: You will learn, in this book and throughout your adventures integrating math
    and coding, that there are differences between math “on the chalkboard” versus
    implemented in code. Some discrepancies are minor and inconsequential, while others
    cause confusion and errors. Let me now introduce you to a terminological difference
    between math and coding.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你将会在本书以及在整合数学和编程的冒险中学到，数学“在黑板上”的方式与在代码中实现的方式之间存在差异。有些差异微不足道，不重要，而其他则会导致混淆和错误。现在，让我向你介绍数学和编程之间术语上的差异。
- en: I wrote earlier that the *dimensionality* of a vector is the number of elements
    in that vector. However, in Python, the dimensionality of a vector or matrix is
    the number of geometric dimensions used to print out a numerical object. For example,
    all of the vectors shown above are considered “two-dimensional arrays” in Python,
    regardless of the number of elements contained in the vectors (which is the mathematical
    dimensionality). A list of numbers without a particular orientation is considered
    a 1D array in Python, regardless of the number of elements (that array will be
    printed out as a row, but, as you’ll see later, it is treated differently from
    row vectors). The mathematical dimensionality—the number of elements in the vector—is
    called the *length* or the *shape* of the vector in Python.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前写道，向量的*维数*是向量中元素的数量。然而，在 Python 中，向量或矩阵的维数是用于打印数值对象的几何维度的数量。例如，上面显示的所有向量在
    Python 中都被认为是“二维数组”，无论向量中包含的元素数量是多少（这是数学上的维数）。在 Python 中，没有特定方向的数字列表被认为是一维数组，无论元素数量如何（该数组将被打印为一行，但是，正如您稍后将看到的，它与行向量的处理方式不同）。在
    Python 中，向量的数学维数——向量中的元素数量——称为向量的*长度*或*形状*。
- en: This inconsistent and sometimes conflicting terminology can be confusing. Indeed,
    terminology is often a sticky issue at the intersection of different disciplines
    (in this case, mathematics and computer science). But don’t worry, you’ll get
    the hang of it with some experience.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不一致且有时冲突的术语可能会令人困惑。事实上，在不同学科（在本例中是数学和计算机科学）交汇处，术语通常是一个棘手的问题。但别担心，通过一些实践你会慢慢掌握。
- en: When referring to vectors, it is common to use lowercase bolded Roman letters,
    like **v** for “vector v.” Some texts use italics (*v*) or print an arrow on top
    ( <math alttext="ModifyingAbove v With right-arrow"><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover></math> ).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到向量时，通常使用小写加粗的罗马字母，比如**v**表示“向量 v”。有些文本使用斜体（*v*）或在顶部打印一个箭头（ <math alttext="ModifyingAbove
    v With right-arrow"><mover accent="true"><mi>v</mi> <mo>→</mo></mover></math>
    ）。
- en: Linear algebra convention is to assume that vectors are in column orientation
    unless otherwise specified. Row vectors are written as <math alttext="bold w Superscript
    upper T"><msup><mi>𝐰</mi> <mtext>T</mtext></msup></math> . The <math alttext="Superscript
    upper T"><msup><mtext>T</mtext></msup></math> indicates the *transpose operation*,
    which you’ll learn more about later; for now, suffice it to say that the transpose
    operation transforms a column vector into a row vector.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数的惯例是默认向量为列向量，除非另有说明。行向量写作 <math alttext="bold w Superscript upper T"><msup><mi>𝐰</mi>
    <mtext>T</mtext></msup></math> 。<math alttext="Superscript upper T"><msup><mtext>T</mtext></msup></math>
    表示*转置操作*，您稍后会了解更多；现在只需知道转置操作将列向量转换为行向量。
- en: Does Vector Orientation Matter?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量的方向重要吗？
- en: Do you really need to worry about whether vectors are column- or row-oriented,
    or orientationless 1D arrays? Sometimes yes, sometimes no. When using vectors
    to store data, orientation usually doesn’t matter. But some operations in Python
    can give errors or unexpected results if the orientation is wrong. Therefore,
    vector orientation is important to understand, because spending 30 minutes debugging
    code only to realize that a row vector needs to be a column vector is guaranteed
    to give you a headache.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您真的需要担心向量是列向量还是行向量，或者是无方向的一维数组吗？有时是，有时不是。在使用向量存储数据时，方向通常并不重要。但是，如果方向错误，Python
    中的某些操作可能会导致错误或意外结果。因此，理解向量的方向是很重要的，因为花费30分钟调试代码，最后发现一个行向量应该是一个列向量，肯定会让您头痛不已。
- en: 'Vectors in Python can be represented using several data types. The `list` type
    may seem like the simplest way to represent a vector—and it is for for some applications.
    But many linear algebra operations won’t work on Python lists. Therefore, most
    of the time it’s best to create vectors as NumPy arrays. The following code shows
    four ways of creating a vector:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，向量可以用几种数据类型表示。`list` 类型可能看起来是表示向量最简单的方式，并且对于某些应用来说确实如此。但是，许多线性代数运算在
    Python 列表上不起作用。因此，大多数时候最好将向量创建为 NumPy 数组。以下代码展示了创建向量的四种方式：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The variable `asArray` is an *orientationless* array, meaning it is neither
    a row nor a column vector but simply a 1D list of numbers in NumPy. Orientation
    in NumPy is given by brackets: the outermost brackets group all of the numbers
    together into one object. Then, each additional set of brackets indicates a row:
    a row vector (variable `rowVec`) has all numbers in one row, while a column vector
    (variable `colVec`) has multiple rows, with each row containing one number.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 `asArray` 是一个*无方向*数组，意味着它既不是行向量也不是列向量，只是 NumPy 中的一个一维数字列表。在 NumPy 中，方向由括号决定：最外层的括号将所有数字组合成一个对象。然后，每一组额外的括号表示一行：行向量（变量
    `rowVec`）将所有数字放在一行中，而列向量（变量 `colVec`）有多行，每行包含一个数字。
- en: 'We can explore these orientations by examining the shapes of the variables
    (inspecting variable shapes is often very useful while coding):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查变量的形状来探索这些方向（在编码时检查变量形状通常非常有用）：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here’s what the output looks like:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output shows that the 1D array `asArray` is of size (`3`), whereas the orientation-endowed
    vectors are 2D arrays and are stored as size (`1,3`) or (`3,1`) depending on the
    orientation. Dimensions are always listed as (rows,columns).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示一维数组 `asArray` 的大小为（`3`），而有方向的向量是二维数组，其大小为（`1,3`）或（`3,1`），具体取决于方向。维度总是按照（行数,列数）列出。
- en: Geometry of Vectors
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量的几何
- en: '*Ordered list of numbers* is the algebraic interpretation of a vector; the
    geometric interpretation of a vector is a straight line with a specific length
    (also called *magnitude*) and direction (also called *angle*; it is computed relative
    to the positive *x*-axis). The two points of a vector are called the tail (where
    it starts) and the head (where it ends); the head often has an arrow tip to disambiguate
    from the tail.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*有序数字列表* 是向量的代数解释；向量的几何解释是具有特定长度（也称为*大小*）和方向（也称为*角度*；相对于正 *x*-轴计算）。向量的两个点称为尾部（起始点）和头部（结束点）；头部通常带有箭头提示以区分尾部。'
- en: You may think that a vector encodes a geometric coordinate, but vectors and
    coordinates are actually different things. They are, however, concordant when
    the vector starts at the origin. This is called the *standard position* and is
    illustrated in [Figure 2-1](#fig_2_1).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为向量编码了一个几何坐标，但向量和坐标实际上是不同的东西。然而，它们在向量从原点开始时是协调的。这被称为*标准位置*，并且在 [图 2-1](#fig_2_1)
    中有所说明。
- en: '![A vector, repeated in space.](assets/plad_0201.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![空间中重复的向量。](assets/plad_0201.png)'
- en: Figure 2-1\. All arrows express the same vector. A vector in standard position
    has its tail at the origin and its head at the concordant geometric coordinate.
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 所有箭头表示相同的向量。位于标准位置的向量其尾部位于原点，其头部位于协调的几何坐标。
- en: Conceptualizing vectors either geometrically or algebraically facilitates intuition
    in different applications, but these are simply two sides of the same coin. For
    example, the geometric interpretation of a vector is useful in physics and engineering
    (e.g., representing physical forces), and the algebraic interpretation of a vector
    is useful in data science (e.g., storing sales data over time). Oftentimes, linear
    algebra concepts are learned geometrically in 2D graphs, and then are expanded
    to higher dimensions using algebra.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是几何上还是代数上构想向量都有助于在不同应用中形成直觉，但这只是同一个问题的两面。例如，向量的几何解释在物理学和工程学中很有用（例如表示物理力量），而向量的代数解释在数据科学中很有用（例如存储随时间变化的销售数据）。通常，线性代数概念在二维图表中以几何方式学习，然后通过代数扩展到更高维度。
- en: Operations on Vectors
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量的操作
- en: Vectors are like nouns; they are the characters in our linear algebra story.
    The fun in linear algebra comes from the verbs—the actions that breathe life into
    the characters. Those actions are called *operations*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 向量就像名词一样；它们是我们线性代数故事中的角色。线性代数的乐趣来自于动词——给这些角色注入生命的行动。这些行动称为*操作*。
- en: Some linear algebra operations are simple and intuitive and work exactly how
    you’d expect (e.g., addition), whereas others are more involved and require entire
    chapters to explain (e.g., singular value decomposition). Let’s begin with simple
    operations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一些线性代数操作简单直观，并且完全符合预期（例如加法），而其他一些操作则更复杂，需要整整几章来解释（例如奇异值分解）。让我们从简单的操作开始。
- en: Adding Two Vectors
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加两个向量
- en: 'To add two vectors, simply add each corresponding element. [Equation 2-2](#eq-2)
    shows an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加两个向量，只需将每个对应的元素相加。[方程 2-2](#eq-2) 展示了一个例子：
- en: Equation 2-2\. Adding two vectors
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 2-2\. 向量相加
- en: <math alttext="Start 3 By 1 Matrix 1st Row  4 2nd Row  5 3rd Row  6 EndMatrix
    plus Start 3 By 1 Matrix 1st Row  10 2nd Row  20 3rd Row  30 EndMatrix equals
    Start 3 By 1 Matrix 1st Row  14 2nd Row  25 3rd Row  36 EndMatrix" display="block"><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>5</mn></mtd></mtr>
    <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced> <mo>+</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mn>10</mn></mtd></mtr> <mtr><mtd><mn>20</mn></mtd></mtr>
    <mtr><mtd><mn>30</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mn>14</mn></mtd></mtr> <mtr><mtd><mn>25</mn></mtd></mtr>
    <mtr><mtd><mn>36</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 3 By 1 Matrix 1st Row  4 2nd Row  5 3rd Row  6 EndMatrix
    plus Start 3 By 1 Matrix 1st Row  10 2nd Row  20 3rd Row  30 EndMatrix equals
    Start 3 By 1 Matrix 1st Row  14 2nd Row  25 3rd Row  36 EndMatrix" display="block"><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>5</mn></mtd></mtr>
    <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced> <mo>+</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mn>10</mn></mtd></mtr> <mtr><mtd><mn>20</mn></mtd></mtr>
    <mtr><mtd><mn>30</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]"
    open="["><mtable><mtr><mtd><mn>14</mn></mtd></mtr> <mtr><mtd><mn>25</mn></mtd></mtr>
    <mtr><mtd><mn>36</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: As you might have guessed, vector addition is defined only for two vectors that
    have the same dimensionality; it is not possible to add, e.g., a vector in <math
    alttext="double-struck upper R cubed"><msup><mi>ℝ</mi> <mn>3</mn></msup></math>
    with a vector in <math alttext="double-struck upper R Superscript 5"><msup><mi>ℝ</mi>
    <mn>5</mn></msup></math> .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的那样，向量加法仅对具有相同维度的两个向量定义；例如，在 <math alttext="双划线上的 R 立方"><msup><mi>ℝ</mi>
    <mn>3</mn></msup></math> 中的向量和在 <math alttext="双划线上的 R 上标 5"><msup><mi>ℝ</mi>
    <mn>5</mn></msup></math> 中的向量不能相加。
- en: 'Vector subtraction is also what you’d expect: subtract the two vectors element-wise.
    [Equation 2-3](#eq-3) shows an example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 向量减法也与预期相同：逐元素减去两个向量。方程 2-3 示范了一个例子：
- en: Equation 2-3\. Subtracting two vectors
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 2-3\. 向量相减
- en: <math alttext="Start 3 By 1 Matrix 1st Row  4 2nd Row  5 3rd Row  6 EndMatrix
    minus Start 3 By 1 Matrix 1st Row  10 2nd Row  20 3rd Row  30 EndMatrix equals
    Start 3 By 1 Matrix 1st Row  negative 6 2nd Row  negative 15 3rd Row  negative
    24 EndMatrix" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>4</mn></mtd></mtr>
    <mtr><mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced>
    <mo>-</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd></mtr>
    <mtr><mtd><mn>20</mn></mtd></mtr> <mtr><mtd><mn>30</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>6</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mo>-</mo> <mn>15</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>24</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 3 By 1 Matrix 1st Row  4 2nd Row  5 3rd Row  6 EndMatrix
    minus Start 3 By 1 Matrix 1st Row  10 2nd Row  20 3rd Row  30 EndMatrix equals
    Start 3 By 1 Matrix 1st Row  negative 6 2nd Row  negative 15 3rd Row  negative
    24 EndMatrix" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>4</mn></mtd></mtr>
    <mtr><mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced>
    <mo>-</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd></mtr>
    <mtr><mtd><mn>20</mn></mtd></mtr> <mtr><mtd><mn>30</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>6</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mo>-</mo> <mn>15</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo>
    <mn>24</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Adding vectors is straightforward in Python:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，向量相加是直接的：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Does vector orientation matter for addition? Consider [Equation 2-4](#eq-4):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 向量加法是否受向量方向的影响？考虑方程 2-4：
- en: Equation 2-4\. Can you add a row vector to a column vector?
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 2-4\. 你能将行向量加到列向量中吗？
- en: <math alttext="Start 3 By 1 Matrix 1st Row  4 2nd Row  5 3rd Row  6 EndMatrix
    plus Start 1 By 3 Matrix 1st Row 1st Column 10 2nd Column 20 3rd Column 30 EndMatrix
    equals question-mark" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>4</mn></mtd></mtr>
    <mtr><mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd> <mtd><mn>20</mn></mtd>
    <mtd><mn>30</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>?</mo></mrow></math>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 3 By 1 Matrix 1st Row  4 2nd Row  5 3rd Row  6 EndMatrix
    plus Start 1 By 3 Matrix 1st Row 1st Column 10 2nd Column 20 3rd Column 30 EndMatrix
    equals question-mark" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>4</mn></mtd></mtr>
    <mtr><mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mn>6</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd> <mtd><mn>20</mn></mtd>
    <mtd><mn>30</mn></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>?</mo></mrow></math>
- en: 'You might think that there is no difference between this example and the one
    shown earlier—after all, both vectors have three elements. Let’s see what Python
    does:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能认为这个例子与之前显示的例子没有区别——毕竟，这两个向量都有三个元素。让我们看看 Python 做了什么：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result may seem confusing and inconsistent with the definition of vector
    addition given earlier. In fact, Python is implementing an operation called *broadcasting*.
    You will learn more about broadcasting later in this chapter, but I encourage
    you to spend a moment pondering the result and thinking about how it arose from
    adding a row and a column vector. Regardless, this example shows that orientation
    is indeed important: *two vectors can be added together only if they have the
    same dimensionality **and** the same orientation*.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能看起来令人困惑，并且与前文所述的向量加法定义不一致。事实上，Python 正在执行一种称为*广播*的操作。您将在本章后面更多地了解广播，但我鼓励您花一点时间思考这个结果，并思考它是如何由添加行向量和列向量而来的。无论如何，这个例子表明方向确实很重要：*只有具有相同维度
    **和** 相同方向的两个向量才能相加*。
- en: Geometry of Vector Addition and Subtraction
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量加法与减法的几何性质
- en: 'To add two vectors geometrically, place the vectors such that the tail of one
    vector is at the head of the other vector. The summed vector traverses from the
    tail of the first vector to the head of the second (graph A in [Figure 2-2](#fig_2_2)).
    You can extend this procedure to sum any number of vectors: simply stack all the
    vectors tail-to-head, and then the sum is the line that goes from the first tail
    to the final head.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要在几何上添加两个向量，请将向量放置在一个向量的尾部位于另一个向量的头部的位置（图 A 参见[图 2-2](#fig_2_2)）。您可以将此过程扩展到对任意数量的向量求和：简单地将所有向量依次堆叠，然后和向量是从第一个尾到最后一个头的线。
- en: '![What does this do?](assets/plad_0202.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![这是什么意思？](assets/plad_0202.png)'
- en: Figure 2-2\. The sum and difference of two vectors
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 两个向量的和与差
- en: 'Subtracting vectors geometrically is slightly different but equally straightforward:
    line up the two vectors such that their tails are at the same coordinate (this
    is easily accomplished by having both vectors in standard position); the difference
    vector is the line that goes from the head of the “negative” vector to the head
    of the “positive” vector (graph B in [Figure 2-2](#fig_2_2)).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 几何上减去向量略有不同，但同样直接：将两个向量排列在一起，使它们的尾部位于相同的坐标（这可以通过将两个向量放在标准位置轻松实现）；差向量是从“负”向量的头到“正”向量的头的线（图
    B 参见[图 2-2](#fig_2_2)）。
- en: 'Do not underestimate the importance of the geometry of vector subtraction:
    it is the basis for orthogonal vector decomposition, which in turn is the basis
    for linear least squares, which is one of the most important applications of linear
    algebra in science and engineering.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 不要低估向量减法的几何性质的重要性：它是正交向量分解的基础，而正交向量分解又是线性最小二乘的基础，这是科学和工程中线性代数的最重要应用之一。
- en: Vector-Scalar Multiplication
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量-标量乘法
- en: A *scalar* in linear algebra is a number on its own, not embedded in a vector
    or matrix. Scalars are typically indicated using lowercase Greek letters such
    as α or λ. Therefore, vector-scalar multiplication is indicated as, for example,
    β**u**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数中的*标量*是一个独立的数字，不嵌入在向量或矩阵中。标量通常用小写希腊字母表示，如α或λ。因此，向量-标量乘法表示为，例如，β**u**。
- en: 'Vector-scalar multiplication is very simple: multiply each vector element by
    the scalar. One numerical example ([Equation 2-5](#eq-5)) will suffice for understanding:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 向量-标量乘法非常简单：将每个向量元素乘以标量。一个数值示例（[方程 2-5](#eq-5)）足以理解：
- en: 'Equation 2-5\. Vector-scalar multiplication (or: scalar-vector multiplication)'
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 2-5\. 向量-标量乘法（或：标量-向量乘法）
- en: <math alttext="lamda equals 4 comma bold w equals Start 3 By 1 Matrix 1st Row  9
    2nd Row  4 3rd Row  1 EndMatrix comma lamda bold w equals Start 3 By 1 Matrix
    1st Row  36 2nd Row  16 3rd Row  4 EndMatrix" display="block"><mrow><mi>λ</mi>
    <mo>=</mo> <mn>4</mn> <mo>,</mo> <mi>𝐰</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>9</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mi>λ</mi> <mi>𝐰</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>36</mn></mtd></mtr>
    <mtr><mtd><mn>16</mn></mtd></mtr> <mtr><mtd><mn>4</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda equals 4 comma bold w equals Start 3 By 1 Matrix 1st Row  9
    2nd Row  4 3rd Row  1 EndMatrix comma lamda bold w equals Start 3 By 1 Matrix
    1st Row  36 2nd Row  16 3rd Row  4 EndMatrix" display="block"><mrow><mi>λ</mi>
    <mo>=</mo> <mn>4</mn> <mo>,</mo> <mi>𝐰</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>9</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>,</mo> <mi>λ</mi> <mi>𝐰</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>36</mn></mtd></mtr>
    <mtr><mtd><mn>16</mn></mtd></mtr> <mtr><mtd><mn>4</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'I wrote earlier that the data type of a variable storing a vector is sometimes
    important and sometimes unimportant. Vector-scalar multiplication is an example
    where data type matters:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到，存储向量的变量的数据类型有时重要，有时不重要。向量-标量乘法就是一个数据类型重要的例子：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The code creates a scalar (variable `s`) and a vector as a list (variable `a`),
    then converts that into a NumPy array (variable `b`). The asterisk is overloaded
    in Python, meaning its behavior depends on the variable type: scalar multiplying
    a list repeats the list `s` times (in this case, twice), which is definitely *not*
    the linear algebra operation of scalar-vector multiplication. When the vector
    is stored as a NumPy array, however, the asterisk is interpreted as element-wise
    multiplication. (Here’s a small exercise for you: what happens if you set `s =
    2.0`, and why?^([1](ch02.xhtml#idm45733300824976))) Both of these operations (list
    repetition and vector-scalar multiplication) are used in real-world coding, so
    be mindful of the distinction.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 代码创建了一个标量（变量 `s`）和一个作为列表的向量（变量 `a`），然后将其转换为NumPy数组（变量 `b`）。在Python中，星号的行为取决于变量类型：标量乘以列表会重复列表
    `s` 次（在本例中是两次），这显然*不是*标量-向量乘法的线性代数操作。然而，当向量存储为NumPy数组时，星号被解释为元素级乘法。（这里有个小练习：如果你设置
    `s = 2.0`，会发生什么？为什么？^([1](ch02.xhtml#idm45733300824976))）这两种操作（列表重复和向量-标量乘法）在实际编码中都有用到，所以要注意区别。
- en: Scalar-Vector Addition
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量-向量加法
- en: 'Adding a scalar to a vector is not formally defined in linear algebra: they
    are two separate kinds of mathematical objects and cannot be combined. However,
    numerical processing programs like Python will allow adding scalars to vectors,
    and the operation is comparable to scalar-vector multiplication: the scalar is
    added to each vector element. The following code illustrates the idea:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，向量加标量没有正式定义：它们是两种不同的数学对象，不能结合在一起。然而，像Python这样的数值处理程序允许将标量添加到向量中，操作类似于标量与向量的乘法：将标量添加到每个向量元素上。以下代码阐明了这个概念：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The geometry of vector-scalar multiplication
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量-标量乘法的几何学
- en: Why are scalars called “scalars”? That comes from the geometric interpretation.
    Scalars scale vectors without changing their direction. There are four effects
    of vector-scalar multiplication that depend on whether the scalar is greater than
    1, between 0 and 1, exactly 0, or negative. [Figure 2-3](#fig_2_3) illustrates
    the concept.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么标量被称为“标量”？这源于几何解释。标量调整向量的大小而不改变其方向。标量与向量相乘有四种效果，这取决于标量是大于1、在0和1之间、恰好为0还是负数。[图 2-3](#fig_2_3)
    阐述了这个概念。
- en: '![Scaling vectors](assets/plad_0203.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![缩放向量](assets/plad_0203.png)'
- en: Figure 2-3\. The same vector (black arrow) multiplied by different scalars <math
    alttext="sigma"><mi>σ</mi></math> (gray line; shifted slightly for visibility)
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 相同向量（黑色箭头）乘以不同标量 <math alttext="sigma"><mi>σ</mi></math> （灰线；略有位移以便看清楚）
- en: I wrote earlier that scalars do not change the direction of the vector. But
    the figure shows that the vector direction flips when the scalar is negative (that
    is, its angle rotates by 180°. That might seem a contradiction, but there is an
    interpretation of vectors as pointing along an infinitely long line that passes
    through the origin and goes to infinity in both directions (in the next chapter,
    I’ll call this a “one-dimensional subspace”). In that sense, the “rotated” vector
    still points along the same infinite line and thus the negative scalar does not
    change the direction. This interpretation is important for matrix spaces, eigenvectors,
    and singular vectors, all of which are introduced in later chapters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前写过，标量不会改变向量的方向。但图中显示，当标量为负数时向量方向会翻转（即其角度旋转180°）。这似乎是个矛盾，但对向量的解释是沿着一个通过原点并且无限延伸到两个方向的无限长线的线（在下一章中，我将称其为“一维子空间”）。从这个意义上讲，“旋转”的向量仍然沿着同一条无限线指向，因此负标量不会改变其方向。这种解释对于矩阵空间、特征向量和奇异向量都很重要，这些概念将在后面的章节中介绍。
- en: 'Vector-scalar multiplication in combination with vector addition leads directly
    to *vector averaging*. Averaging vectors is the same as averaging numbers: sum
    and divide by the number of numbers. So, to average two vectors, add them and
    then scalar multiply by .5\. In general, to average *N* vectors, sum them and
    scalar multiply the result by *1/N*.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 向量标量乘法与向量加法结合直接导致*向量平均*。对向量进行平均就像对数字进行平均一样：求和然后除以数字的数量。因此，要对两个向量进行平均，先将它们相加，然后乘以0.5。一般来说，要对*N*个向量进行平均，将它们求和，然后将结果乘以*1/N*。
- en: Transpose
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转置
- en: 'You already learned about the transpose operation: it converts column vectors
    into row vectors, and vice versa. Let me provide a slightly more formal definition
    that will generalize to transposing matrices (a topic in [Chapter 5](ch05.xhtml#Chapter_5)).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解了转置操作：它将列向量转换为行向量，反之亦然。让我提供一个稍微更正式的定义，将推广到转置矩阵（这是[第5章](ch05.xhtml#Chapter_5)中的一个主题）。
- en: 'A matrix has rows and columns; therefore, each matrix element has a (*row,column*)
    index. The transpose operation simply swaps those indices. This is formalized
    in [Equation 2-6](#eq-6):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵有行和列；因此，每个矩阵元素都有一个(*行,列*)索引。转置操作简单地交换这些索引。这在[方程2-6](#eq-6)中得到了正式化：
- en: Equation 2-6\. The transpose operation
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程2-6。转置操作
- en: <math alttext="bold m Subscript i comma j Superscript upper T Baseline equals
    bold m Subscript j comma i" display="block"><mrow><msubsup><mi>𝐦</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow>
    <mtext>T</mtext></msubsup> <mo>=</mo> <msub><mi>𝐦</mi> <mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></math>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold m Subscript i comma j Superscript upper T Baseline equals
    bold m Subscript j comma i" display="block"><mrow><msubsup><mi>𝐦</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow>
    <mtext>T</mtext></msubsup> <mo>=</mo> <msub><mi>𝐦</mi> <mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></math>
- en: Vectors have either one row or one column, depending on their orientation. For
    example, a 6D row vector has *i* = 1 and *j* indices from 1 to 6, whereas a 6D
    column vector has *i* indices from 1 to 6 and *j* = 1\. So swapping the *i,j*
    indices swaps the rows and columns.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的方向有一行或一列，这取决于它们的方向。例如，一个6维行向量有*i* = 1和*j*从1到6的索引，而一个6维列向量有*i*从1到6的索引和*j*
    = 1。因此，交换*i,j*索引会交换行和列。
- en: 'Here’s an important rule: transposing twice returns the vector to its original
    orientation. In other words, <math alttext="bold v Superscript TT Baseline equals
    bold v"><mrow><msup><mi>𝐯</mi> <mtext>TT</mtext></msup> <mo>=</mo> <mi>𝐯</mi></mrow></math>
    . That may seem obvious and trivial, but it is the keystone of several important
    proofs in data science and machine learning, including creating symmetric covariance
    matrices as the data matrix times its transpose (which in turn is the reason why
    a principal components analysis is an orthogonal rotation of the data space…don’t
    worry, that sentence will make sense later in the book!).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个重要的规则：两次转置将向量返回到其原始方向。换句话说，<math alttext="bold v Superscript TT Baseline
    equals bold v"><mrow><msup><mi>𝐯</mi> <mtext>TT</mtext></msup> <mo>=</mo> <mi>𝐯</mi></mrow></math>
    。这可能看起来显而易见和琐碎，但它是数据科学和机器学习中几个重要证明的基石，包括创建对称协方差矩阵作为数据矩阵乘以其转置（这也是主成分分析将数据空间进行正交旋转的原因……别担心，这个句子以后在本书中会有更多解释！）。
- en: Vector Broadcasting in Python
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python中的向量广播
- en: Broadcasting is an operation that exists only in modern computer-based linear
    algebra; this is not a procedure you would find in a traditional linear algebra
    textbook.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 广播是一种仅存在于现代基于计算机的线性代数中的操作；这不是传统线性代数教科书中会找到的过程。
- en: 'Broadcasting essentially means to repeat an operation multiple times between
    one vector and each element of another vector. Consider the following series of
    equations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 广播本质上意味着在一个向量和另一个向量的每个元素之间多次重复一个操作。考虑以下一系列方程：
- en: <math alttext="StartLayout 1st Row  Start 1 By 2 Matrix 1st Row 1st Column 1
    2nd Column 1 EndMatrix plus Start 1 By 2 Matrix 1st Row 1st Column 10 2nd Column
    20 EndMatrix 2nd Row  Start 1 By 2 Matrix 1st Row 1st Column 2 2nd Column 2 EndMatrix
    plus Start 1 By 2 Matrix 1st Row 1st Column 10 2nd Column 20 EndMatrix 3rd Row  Start
    1 By 2 Matrix 1st Row 1st Column 3 2nd Column 3 EndMatrix plus Start 1 By 2 Matrix
    1st Row 1st Column 10 2nd Column 20 EndMatrix EndLayout" display="block"><mtable><mtr><mtd><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd> <mtd><mn>20</mn></mtd></mtr></mtable></mfenced></mrow></mtd></mtr>
    <mtr><mtd><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>2</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr></mtable></mfenced> <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd>
    <mtd><mn>20</mn></mtd></mtr></mtable></mfenced></mrow></mtd></mtr> <mtr><mtd><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mn>3</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd> <mtd><mn>20</mn></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row  Start 1 By 2 Matrix 1st Row 1st Column 1
    2nd Column 1 EndMatrix plus Start 1 By 2 Matrix 1st Row 1st Column 10 2nd Column
    20 EndMatrix 2nd Row  Start 1 By 2 Matrix 1st Row 1st Column 2 2nd Column 2 EndMatrix
    plus Start 1 By 2 Matrix 1st Row 1st Column 10 2nd Column 20 EndMatrix 3rd Row  Start
    1 By 2 Matrix 1st Row 1st Column 3 2nd Column 3 EndMatrix plus Start 1 By 2 Matrix
    1st Row 1st Column 10 2nd Column 20 EndMatrix EndLayout" display="block"><mtable><mtr><mtd><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd> <mtd><mn>20</mn></mtd></mtr></mtable></mfenced></mrow></mtd></mtr>
    <mtr><mtd><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>2</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr></mtable></mfenced> <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd>
    <mtd><mn>20</mn></mtd></mtr></mtable></mfenced></mrow></mtd></mtr> <mtr><mtd><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mn>3</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>10</mn></mtd> <mtd><mn>20</mn></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
- en: 'Notice the patterns in the vectors. We can implement this set of equations
    compactly by condensing those patterns into vectors [1 2 3] and [10 20], and then
    broadcasting the addition. Here’s how it looks in Python:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意向量中的模式。我们可以将这组方程紧凑地实现为向量[1 2 3]和[10 20]，然后广播加法。以下是Python中的实现方式：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here again you can see the importance of orientation in linear algebra operations:
    try running the code above, changing `v` into a row vector and `w` into a column
    vector.^([2](ch02.xhtml#idm45733308090944))'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 再次可以看到线性代数操作中方向的重要性：尝试运行上面的代码，将`v`变成一个行向量和`w`变成一个列向量。^([2](ch02.xhtml#idm45733308090944))
- en: Because broadcasting allows for efficient and compact computations, it is used
    often in numerical coding. You’ll see several examples of broadcasting in this
    book, including in the section on *k*-means clustering ([Chapter 4](ch04.xhtml#Chapter_4)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因为广播允许进行高效紧凑的计算，它经常在数值编码中使用。您将在本书中看到几个广播的例子，包括* k * -均值聚类部分（[第4章](ch04.xhtml#Chapter_4)）。
- en: Vector Magnitude and Unit Vectors
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量大小和单位向量
- en: 'The *magnitude* of a vector—also called the *geometric length* or the *norm*—is
    the distance from tail to head of a vector, and is computed using the standard
    Euclidean distance formula: the square root of the sum of squared vector elements
    (see [Equation 2-7](#vector-norm)). Vector magnitude is indicated using double-vertical
    bars around the vector: <math alttext="parallel-to bold v parallel-to"><mrow><mo>∥</mo>
    <mi>𝐯</mi> <mo>∥</mo></mrow></math> .'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个向量的*大小* —— 也称为*几何长度* 或 *范数* —— 是从向量的尾到头的距离，并且使用标准欧几里德距离公式计算：向量元素的平方和的平方根（见
    [方程 2-7](#vector-norm)）。向量大小用双竖线标示在向量周围：<math alttext="parallel-to bold v parallel-to"><mrow><mo>∥</mo>
    <mi>𝐯</mi> <mo>∥</mo></mrow></math> 。
- en: Equation 2-7\. The norm of a vector
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 2-7\. 矢量的范数
- en: <math alttext="parallel-to bold v parallel-to equals StartRoot sigma-summation
    Underscript i equals 1 Overscript n Endscripts v Subscript i Superscript 2 Baseline
    EndRoot" display="block"><mrow><mrow><mo>∥</mo> <mi>𝐯</mi> <mo>∥</mo></mrow> <mo>=</mo>
    <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msubsup><mi>v</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="parallel-to bold v parallel-to equals StartRoot sigma-summation
    Underscript i equals 1 Overscript n Endscripts v Subscript i Superscript 2 Baseline
    EndRoot" display="block"><mrow><mrow><mo>∥</mo> <mi>𝐯</mi> <mo>∥</mo></mrow> <mo>=</mo>
    <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msubsup><mi>v</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></msqrt></mrow></math>
- en: Some applications use squared magnitudes (written ∥ 𝐯 ∥²), in which case the
    square root term on the right-hand side drops out.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有些应用程序使用平方幅度（写成 ∥ 𝐯 ∥² ），在这种情况下，右侧的平方根项消失。
- en: 'Before showing the Python code, let me explain some more terminological discrepancies
    between “chalkboard” linear algebra and Python linear algebra. In mathematics,
    the dimensionality of a vector is the number of elements in that vector, while
    the length is a geometric distance; in Python, the function `len()` (where `len`
    is short for *length*) returns the *dimensionality* of an array, while the function
    `np.norm()` returns the geometric length (magnitude). In this book, I will use
    the term *magnitude* (or *geometric length*) instead of *length* to avoid confusion:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示 Python 代码之前，让我解释一些“黑板”线性代数与 Python 线性代数之间的术语差异。在数学中，向量的维数是该向量中元素的数量，而长度是几何距离；在
    Python 中，函数 `len()`（其中 `len` 缩写为 *length*）返回数组的 *维数*，而函数 `np.norm()` 返回几何长度（大小）。在本书中，我将使用术语
    *大小*（或 *几何长度*）代替 *长度*，以避免混淆：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are some applications where we want a vector that has a geometric length
    of one, which is called a *unit vector*. Example applications include orthogonal
    matrices, rotation matrices, eigenvectors, and singular vectors.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有些应用程序需要一个几何长度为一的向量，这被称为*单位向量*。例如应用包括正交矩阵、旋转矩阵、特征向量和奇异向量。
- en: A unit vector is defined as <math alttext="parallel-to bold v parallel-to equals
    1"><mrow><mo>∥</mo> <mi>𝐯</mi> <mo>∥</mo> <mo>=</mo> <mn>1</mn></mrow></math>
    .
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 单位向量被定义为 <math alttext="parallel-to bold v parallel-to equals 1"><mrow><mo>∥</mo>
    <mi>𝐯</mi> <mo>∥</mo> <mo>=</mo> <mn>1</mn></mrow></math> 。
- en: 'Needless to say, lots of vectors are not unit vectors. (I’m tempted to write
    “most vectors are not unit vectors,” but there is an infinite number of unit vectors
    and nonunit vectors, although the set of infinite nonunit vectors is larger than
    the set of infinite unit vectors.) Fortunately, any nonunit vector has an associated
    unit vector. That means that we can create a unit vector in the same direction
    as a nonunit vector. Creating an associated unit vector is easy; you simply scalar
    multiply by the reciprocal of the vector norm ([Equation 2-8](#eq-vecnorm)):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，大量向量并非单位向量。（我很想写“大多数向量不是单位向量”，但是非单位向量和单位向量都有无穷多个，尽管非单位向量的集合大于单位向量的集合。）幸运的是，任何非单位向量都有一个相关联的单位向量。这意味着我们可以创建一个与非单位向量方向相同的单位向量。创建相关联的单位向量很容易；你只需乘以矢量范数的倒数（[方程
    2-8](#eq-vecnorm)）：
- en: Equation 2-8\. Creating a unit vector
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 2-8\. 创建单位向量
- en: <math alttext="ModifyingAbove bold v With caret equals StartFraction 1 Over
    parallel-to bold v parallel-to EndFraction bold v" display="block"><mrow><mover
    accent="true"><mi>𝐯</mi> <mo>^</mo></mover> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mo>∥</mo><mi>𝐯</mi><mo>∥</mo></mrow></mfrac>
    <mi>𝐯</mi></mrow></math>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove bold v With caret equals StartFraction 1 Over
    parallel-to bold v parallel-to EndFraction bold v" display="block"><mrow><mover
    accent="true"><mi>𝐯</mi> <mo>^</mo></mover> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mo>∥</mo><mi>𝐯</mi><mo>∥</mo></mrow></mfrac>
    <mi>𝐯</mi></mrow></math>
- en: You can see the common convention for indicating unit vectors ( <math alttext="ModifyingAbove
    bold v With caret"><mover accent="true"><mi>𝐯</mi> <mo>^</mo></mover></math> )
    in the same direction as their parent vector <math alttext="bold v"><mi>𝐯</mi></math>
    . [Figure 2-4](#fig_2_4) illustrates these cases.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到表示单位向量的常见约定（ <math alttext="ModifyingAbove bold v With caret"><mover accent="true"><mi>𝐯</mi>
    <mo>^</mo></mover></math> ）与其父向量 <math alttext="bold v"><mi>𝐯</mi></math> 在相同方向上的情况。[图
    2-4](#fig_2_4) 阐明了这些情况。
- en: '![Vector buddies](assets/plad_0204.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![矢量朋友们](assets/plad_0204.png)'
- en: Figure 2-4\. A unit vector (gray arrow) can be crafted from a nonunit vector
    (black arrow); both vectors have the same angle but different magnitudes
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 一个单位向量（灰色箭头）可以从一个非单位向量（黑色箭头）中制作出来；这两个向量有相同的角度但不同的大小。
- en: Actually, the claim that “*any* nonunit vector has an associated unit vector”
    is not entirely true. There is a vector that has nonunit length and yet has no
    associated unit vector. Can you guess which vector it is?^([3](ch02.xhtml#idm45733307857968))
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，“*任何* 非单位向量都有相关联的单位向量”这一说法并不完全正确。有一个矢量其长度为非单位长度，但没有相关联的单位向量。你能猜出这是哪个向量吗？^([3](ch02.xhtml#idm45733307857968))
- en: I’m not showing Python code to create unit vectors here, because that’s one
    of the exercises at the end of this chapter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里不展示创建单位向量的 Python 代码，因为这是本章末尾的一个练习之一。
- en: The Vector Dot Product
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量点积
- en: The *dot product* (also sometimes called the *inner product*) is one of the
    most important operations in all of linear algebra. It is the basic computational
    building block from which many operations and algorithms are built, including
    convolution, correlation, the Fourier transform, matrix multiplication, linear
    feature extraction, signal filtering, and so on.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*点积*（有时也称为*内积*）是线性代数中最重要的操作之一。它是许多操作和算法的基本计算模块，包括卷积、相关性、傅里叶变换、矩阵乘法、线性特征提取、信号过滤等。'
- en: There are several ways to indicate the dot product between two vectors. I will
    mostly use the common notation <math alttext="bold a Superscript upper T Baseline
    bold b"><mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐛</mi></mrow></math>
    for reasons that will become clear after learning about matrix multiplication.
    In other contexts you might see <math alttext="bold a dot bold b"><mrow><mi>𝐚</mi>
    <mo>·</mo> <mi>𝐛</mi></mrow></math> or <math alttext="mathematical left-angle
    bold a comma bold b mathematical right-angle"><mrow><mo>〈</mo> <mi>𝐚</mi> <mo>,</mo>
    <mi>𝐛</mi> <mo>〉</mo></mrow></math> .
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以表示两个向量之间的点积。出于某些原因，在学习矩阵乘法后，我将主要使用常见的符号 <math alttext="bold a Superscript
    upper T Baseline bold b"><mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐛</mi></mrow></math>
    。在其他情况下，您可能会看到 <math alttext="bold a dot bold b"><mrow><mi>𝐚</mi> <mo>·</mo> <mi>𝐛</mi></mrow></math>
    或 <math alttext="mathematical left-angle bold a comma bold b mathematical right-angle"><mrow><mo>〈</mo>
    <mi>𝐚</mi> <mo>,</mo> <mi>𝐛</mi> <mo>〉</mo></mrow></math> 。
- en: The dot product is a single number that provides information about the relationship
    between two vectors. Let’s first focus on the algorithm to compute the dot product,
    and then I’ll discuss how to interpret it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 点积是一个单一的数字，提供关于两个向量之间关系的信息。让我们首先专注于计算点积的算法，然后我会讨论如何解释它。
- en: 'To compute the dot product, you multiply the corresponding elements of the
    two vectors, and then sum over all the individual products. In other words: element-wise
    multiplication and sum. In [Equation 2-9](#dp-alg), **a** and **b** are vectors,
    and a[i] indicates the *i*th element of **a**.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算点积，您需要将两个向量的对应元素相乘，然后将所有单个乘积求和。换句话说：逐元素乘法和求和。在 [方程式 2-9](#dp-alg) 中，**a**
    和 **b** 是向量，a[i] 表示 **a** 的第 *i* 个元素。
- en: Equation 2-9\. Dot product formula
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 2-9\. 点积公式
- en: <math alttext="delta equals sigma-summation Underscript i equals 1 Overscript
    n Endscripts a Subscript i Baseline b Subscript i" display="block"><mrow><mi>δ</mi>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mtext>a</mtext> <mi>i</mi></msub> <msub><mtext>b</mtext>
    <mi>i</mi></msub></mrow></math>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="delta equals sigma-summation Underscript i equals 1 Overscript
    n Endscripts a Subscript i Baseline b Subscript i" display="block"><mrow><mi>δ</mi>
    <mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></munderover> <msub><mtext>a</mtext> <mi>i</mi></msub> <msub><mtext>b</mtext>
    <mi>i</mi></msub></mrow></math>
- en: 'You can tell from the formula that the dot product is valid only between two
    vectors of the same dimensionality. [Equation 2-10](#dp-cal) shows a numerical
    example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从公式可以看出，点积只在相同维度的两个向量之间有效。[方程式 2-10](#dp-cal) 展示了一个数值示例：
- en: Equation 2-10\. Example dot product calculation
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 2-10\. 示例点积计算
- en: <math alttext="StartLayout 1st Row 1st Column Start 1 By 4 Matrix 1st Row 1st
    Column 1 2nd Column 2 3rd Column 3 4th Column 4 EndMatrix dot Start 1 By 4 Matrix
    1st Row 1st Column 5 2nd Column 6 3rd Column 7 4th Column 8 EndMatrix 2nd Column
    equals 1 times 5 plus 2 times 6 plus 3 times 7 plus 4 times 8 2nd Row 1st Column
    Blank 2nd Column equals 5 plus 12 plus 21 plus 32 3rd Row 1st Column Blank 2nd
    Column equals 70 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd> <mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>5</mn></mtd> <mtd><mn>6</mn></mtd>
    <mtd><mn>7</mn></mtd> <mtd><mn>8</mn></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn> <mo>×</mo> <mn>5</mn> <mo>+</mo>
    <mn>2</mn> <mo>×</mo> <mn>6</mn> <mo>+</mo> <mn>3</mn> <mo>×</mo> <mn>7</mn> <mo>+</mo>
    <mn>4</mn> <mo>×</mo> <mn>8</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mn>5</mn> <mo>+</mo> <mn>12</mn> <mo>+</mo> <mn>21</mn> <mo>+</mo> <mn>32</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mn>70</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column Start 1 By 4 Matrix 1st Row 1st
    Column 1 2nd Column 2 3rd Column 3 4th Column 4 EndMatrix dot Start 1 By 4 Matrix
    1st Row 1st Column 5 2nd Column 6 3rd Column 7 4th Column 8 EndMatrix 2nd Column
    equals 1 times 5 plus 2 times 6 plus 3 times 7 plus 4 times 8 2nd Row 1st Column
    Blank 2nd Column equals 5 plus 12 plus 21 plus 32 3rd Row 1st Column Blank 2nd
    Column equals 70 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd> <mtd><mn>3</mn></mtd> <mtd><mn>4</mn></mtd></mtr></mtable></mfenced>
    <mo>·</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>5</mn></mtd> <mtd><mn>6</mn></mtd>
    <mtd><mn>7</mn></mtd> <mtd><mn>8</mn></mtd></mtr></mtable></mfenced></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn> <mo>×</mo> <mn>5</mn> <mo>+</mo>
    <mn>2</mn> <mo>×</mo> <mn>6</mn> <mo>+</mo> <mn>3</mn> <mo>×</mo> <mn>7</mn> <mo>+</mo>
    <mn>4</mn> <mo>×</mo> <mn>8</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mn>5</mn> <mo>+</mo> <mn>12</mn> <mo>+</mo> <mn>21</mn> <mo>+</mo> <mn>32</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mn>70</mn></mrow></mtd></mtr></mtable></math>
- en: Irritations of Indexing
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引的烦恼
- en: Standard mathematical notation, and some math-oriented numerical processing
    programs like MATLAB and Julia, start indexing at 1 and stop at *N*, whereas some
    programming languages like Python and Java start indexing at 0 and stop at *N*
    − 1\. We need not debate the merits and limitations of each convention—though
    I do sometimes wonder how many bugs this inconsistency has introduced into human
    civilization—but it is important to be mindful of this difference when translating
    formulas into Python code.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 标准数学表示法和一些数学导向的数值处理程序，如 MATLAB 和 Julia，从1开始索引，到 *N* 结束；而一些编程语言如 Python 和 Java
    从0开始索引，到 *N* − 1 结束。我们不需要辩论每种约定的优缺点 — 虽然我有时会想知道这种不一致性引入了多少错误到人类文明中 — 但在将公式翻译为
    Python 代码时注意这种差异是很重要的。
- en: 'There are multiple ways to implement the dot product in Python; the most straightforward
    way is to the use the `np.dot()` function:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中实现点积有多种方法；最直接的方法是使用 `np.dot()` 函数。
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note About np.dot()
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于 np.dot() 的注意事项
- en: The function `np.dot()` does not actually implement the vector dot product;
    it implements matrix multiplication, which is a collection of dot products. This
    will make more sense after learning about the rules and mechanisms of matrix multiplication
    ([Chapter 5](ch05.xhtml#Chapter_5)). If you want to explore this now, you can
    modify the previous code to endow both vectors with orientations (row versus column).
    You will discover that the output is the dot product only when the first input
    is a row vector and the second input is a column vector.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`np.dot()`实际上并没有实现向量的点积；它实现的是矩阵乘法，而矩阵乘法是一组点积的集合。在学习矩阵乘法的规则和机制之后会更加清晰（[第5章](ch05.xhtml#Chapter_5)）。如果你想现在就探索这一点，可以修改之前的代码，给两个向量分别赋予方向（行向量对比列向量）。你会发现，只有在第一个输入是行向量，第二个输入是列向量时，输出才是点积。
- en: 'Here is an interesting property of the dot product: scalar multiplying one
    vector scales the dot product by the same amount. We can explore this by expanding
    the previous code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 点积的一个有趣属性是：标量乘以一个向量会按相同的比例扩展点积。我们可以通过扩展之前的代码来探索这一点：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The dot product of `v` and `w` is 70, and the dot product using `s*v` (which,
    in math notation, would be written as <math alttext="sigma bold v Superscript
    upper T Baseline bold w"><mrow><mi>σ</mi> <msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <mi>𝐰</mi></mrow></math> ) is 700\. Now try it with a negative scalar, e.g., `s
    = -1`. You’ll see that the dot product magnitude is preserved but the sign is
    reversed. Of course, when `s = 0` then the dot product is zero.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 向量`v`和`w`的点积为70，使用`s*v`进行的点积（在数学符号中可以写为<math alttext="sigma bold v Superscript
    upper T Baseline bold w"><mrow><mi>σ</mi> <msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <mi>𝐰</mi></mrow></math>）为700。现在尝试一个负标量，例如`s = -1`。你会看到点积的大小保持不变，但符号被反转。当然，当`s
    = 0`时，点积为零。
- en: Now you know how to compute the dot product. What does the dot product mean
    and how do we interpret it?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何计算点积了。点积意味着什么，我们如何解释它？
- en: The dot product can be interpreted as a measure of *similarity* or *mapping*
    between two vectors. Imagine that you collected height and weight data from 20
    people, and you stored those data in two vectors. You would certainly expect those
    variables to be related to each other (taller people tend to weigh more), and
    therefore you could expect the dot product between those two vectors to be large.
    On the other hand, the magnitude of the dot product depends on the scale of the
    data, which means the dot product between data measured in grams and centimeters
    would be larger than the dot product between data measured in pounds and feet.
    This arbitrary scaling, however, can be eliminated with a normalization factor.
    In fact, the normalized dot product between two variables is called the *Pearson
    correlation coefficient*, and it is one of the most important analyses in data
    science. More on this in [Chapter 4](ch04.xhtml#Chapter_4)!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 点积可以被解释为两个向量之间的*相似性*或*映射*的度量。想象一下，你从20个人那里收集了身高和体重数据，并将这些数据存储在两个向量中。你肯定希望这些变量之间有关联（身高较高的人
    tend to weigh more），因此你可以期望这两个向量之间的点积较大。然而，点积的大小取决于数据的比例，这意味着以克和厘米测量的数据之间的点积将大于以磅和英尺测量的数据之间的点积。然而，这种任意的缩放可以通过归一化因子来消除。事实上，两个变量的归一化点积被称为*皮尔逊相关系数*，它是数据科学中最重要的分析之一。更多信息请参见[第4章](ch04.xhtml#Chapter_4)！
- en: The Dot Product Is Distributive
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 点积具有分配性质
- en: 'The distributive property of mathematics is that <math alttext="a left-parenthesis
    b plus c right-parenthesis equals a b plus a c"><mrow><mi>a</mi> <mo>(</mo> <mi>b</mi>
    <mo>+</mo> <mi>c</mi> <mo>)</mo> <mo>=</mo> <mi>a</mi> <mi>b</mi> <mo>+</mo> <mi>a</mi>
    <mi>c</mi></mrow></math> . Translated into vectors and the vector dot product,
    it means that:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 数学的分配性质是：<math alttext="a left-parenthesis b plus c right-parenthesis equals
    a b plus a c"><mrow><mi>a</mi> <mo>(</mo> <mi>b</mi> <mo>+</mo> <mi>c</mi> <mo>)</mo>
    <mo>=</mo> <mi>a</mi> <mi>b</mi> <mo>+</mo> <mi>a</mi> <mi>c</mi></mrow></math>。转化为向量和向量点积的术语，意味着：
- en: <math alttext="bold a Superscript upper T Baseline left-parenthesis bold b plus
    bold c right-parenthesis equals bold a Superscript upper T Baseline bold b plus
    bold a Superscript upper T Baseline bold c" display="block"><mrow><msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>𝐛</mi> <mo>+</mo> <mi>𝐜</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐛</mi> <mo>+</mo> <msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mi>𝐜</mi></mrow></math>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold a Superscript upper T Baseline left-parenthesis bold b plus
    bold c right-parenthesis equals bold a Superscript upper T Baseline bold b plus
    bold a Superscript upper T Baseline bold c" display="block"><mrow><msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>𝐛</mi> <mo>+</mo> <mi>𝐜</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐛</mi> <mo>+</mo> <msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mi>𝐜</mi></mrow></math>
- en: In words, you would say that the dot product of a vector sum equals the sum
    of the vector dot products.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，一个向量和的点积等于各个向量点积的和。
- en: 'The following Python code illustrates the distributivity property:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码展示了分配性质：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The two outcomes `res1` and `res2` are the same (with these vectors, the answer
    is 110), which illustrates the distributivity of the dot product. Notice how the
    mathematical formula is translated into Python code; translating formulas into
    code is an important skill in math-oriented coding.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 两个结果 `res1` 和 `res2` 是相同的（对于这些向量，答案是110），这说明了点积的分配性质。注意数学公式如何被转化为Python代码；将公式转化为代码是数学导向编程中的重要技能。
- en: Geometry of the Dot Product
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 点积的几何性质
- en: There is also a geometric definition of the dot product, which is the product
    of the magnitudes of the two vectors, scaled by the cosine of the angle between
    them ([Equation 2-11](#dp-geom)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 点积还有一个几何定义，即两个向量的大小乘积，乘以它们之间的余弦值（[方程式 2-11](#dp-geom)）。
- en: Equation 2-11\. Geometric definition of the vector dot product
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 2-11\. 向量点积的几何定义
- en: <math alttext="alpha equals cosine left-parenthesis theta Subscript bold v comma
    bold w Baseline right-parenthesis parallel-to bold v parallel-to parallel-to bold
    w parallel-to" display="block"><mrow><mi>α</mi> <mo>=</mo> <mo form="prefix">cos</mo>
    <mo>(</mo> <msub><mi>θ</mi> <mrow><mi>𝐯</mi><mo>,</mo><mi>𝐰</mi></mrow></msub>
    <mo>)</mo> <mo>∥</mo> <mi>𝐯</mi> <mo>∥</mo> <mo>∥</mo> <mi>𝐰</mi> <mo>∥</mo></mrow></math>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="alpha equals cosine left-parenthesis theta Subscript bold v comma
    bold w Baseline right-parenthesis parallel-to bold v parallel-to parallel-to bold
    w parallel-to" display="block"><mrow><mi>α</mi> <mo>=</mo> <mo form="prefix">cos</mo>
    <mo>(</mo> <msub><mi>θ</mi> <mrow><mi>𝐯</mi><mo>,</mo><mi>𝐰</mi></mrow></msub>
    <mo>)</mo> <mo>∥</mo> <mi>𝐯</mi> <mo>∥</mo> <mo>∥</mo> <mi>𝐰</mi> <mo>∥</mo></mrow></math>
- en: '[Equation 2-9](#dp-alg) and [Equation 2-11](#dp-geom) are mathematically equivalent
    but expressed in different forms. The proof of their equivalence is an interesting
    exercise in mathematical analysis, but would take about a page of text and relies
    on first proving other principles including the Law of Cosines. That proof is
    not relevant for this book and so is omitted.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式 2-9](#dp-alg) 和 [方程式 2-11](#dp-geom) 在数学上是等价的，但表达形式不同。它们的等价性证明是数学分析中有趣的练习，但需要大约一页的文本，依赖于首先证明包括余弦定理在内的其他原理。这个证明对本书不相关，因此被省略。'
- en: Notice that vector magnitudes are strictly positive quantities (except for the
    zeros vector, which has <math alttext="parallel-to bold 0 parallel-to equals 0"><mrow><mo>∥</mo>
    <mn mathvariant="bold">0</mn> <mo>∥</mo> <mo>=</mo> <mn>0</mn></mrow></math> ),
    while the cosine of an angle can range between −1 and +1\. This means that the
    sign of the dot product is determined entirely by the geometric relationship between
    the two vectors. [Figure 2-5](#fig_2_5) shows five cases of the dot product sign,
    depending on the angle between the two vectors (in 2D for visualization, but the
    principle holds for higher dimensions).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意向量的大小严格为正（除了零向量，其长度为 <math alttext="parallel-to bold 0 parallel-to equals
    0"><mrow><mo>∥</mo> <mn mathvariant="bold">0</mn> <mo>∥</mo> <mo>=</mo> <mn>0</mn></mrow></math>
    ），而角的余弦值可以在 -1 到 +1 之间。这意味着点积的符号完全由两个向量之间的几何关系决定。[图 2-5](#fig_2_5) 展示了两个向量之间角度的五种点积符号情况（在二维中用于可视化，但原理适用于更高维度）。
- en: '![What does this do?](assets/plad_0205.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![这个做什么？](assets/plad_0205.png)'
- en: Figure 2-5\. The sign of the dot product between two vectors reveals the geometric
    relationship between those vectors
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 两个向量之间的点积符号显示了这些向量之间的几何关系
- en: 'Memorize This: Orthogonal Vectors Have a Zero Dot Product'
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记住这个：正交向量的点积为零
- en: 'Some math teachers insist that you shouldn’t memorize formulas and terms, and
    instead should understand procedures and proofs. But let’s be honest: memorization
    is an important and inescapable part of learning mathematics. Fortunately, linear
    algebra isn’t excessively memorization-heavy, but there are a few things you’ll
    simply need to commit to memory.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数学老师坚持认为你不应该记忆公式和术语，而应该理解过程和证明。但让我们诚实地说：记忆是学习数学中重要且不可避免的一部分。幸运的是，线性代数并不需要过多的记忆负担，但有一些事情你必须简单地记住。
- en: 'Here is one: orthogonal vectors have a dot product of zero (that claim goes
    both ways—when the dot product is zero, then the two vectors are orthogonal).
    So, the following statements are equivalent: two vectors are orthogonal; two vectors
    have a dot product of zero; two vectors meet at a 90° angle. Repeat that equivalence
    until it’s permanently etched into your brain.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子：正交向量的点积为零（反之亦然——当点积为零时，两个向量是正交的）。因此，以下陈述是等价的：两个向量是正交的；两个向量的点积为零；两个向量成90°角。重复这种等价性，直到它永久地刻在你的大脑中。
- en: Other Vector Multiplications
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他向量乘法
- en: The dot product is perhaps the most important, and most frequently used, way
    to multiply vectors. But there are several other ways to multiply vectors.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 点乘可能是最重要、也是最经常使用的一种向量乘法方式。但还有几种其他向量乘法方式。
- en: Hadamard Multiplication
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哈达玛乘法
- en: 'This is just a fancy term for element-wise multiplication. To implement Hadamard
    multiplication, each corresponding element in the two vectors is multiplied. The
    product is a vector of the same dimensionality as the two multiplicands. For example:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个元素级乘法的花哨术语。要实现哈达玛乘法，需要将两个向量中对应的每个元素相乘。乘积是与两个乘数相同维度的向量。例如：
- en: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>5</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>8</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd></mtr></mtable></mfenced>
    <mo>⊙</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>.5</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>5</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>4</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>5</mn></mtd></mtr>
    <mtr><mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>8</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd></mtr></mtable></mfenced>
    <mo>⊙</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>1</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>.5</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>5</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>0</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>4</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'In Python, the asterisk indicates element-wise multiplication for two vectors
    or matrices:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，星号表示两个向量或矩阵的逐元素乘法：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Try running that code in Python and…uh oh! Python will give an error. Find and
    fix the bug. What have you learned about Hadamard multiplication from that error?
    Check the footnote for the answer.^([4](ch02.xhtml#idm45733308534784))
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在 Python 中运行该代码然后……哎呀！Python 会报错。找到并修复了错误。从该错误中，你学到了什么关于哈达玛乘法？查看脚注获取答案。^([4](ch02.xhtml#idm45733308534784))
- en: Hadamard multiplication is a convenient way to organize multiple scalar multiplications.
    For example, imagine you have data on the number of widgets sold in different
    shops and the price per widget at each shop. You could represent each variable
    as a vector, and then Hadamard-multiply those vectors to compute the widget revenue
    *per shop* (this is different from the total revenue across *all shops*, which
    would be computed as the dot product).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 哈达玛乘积是组织多个标量乘法的便捷方式。例如，想象一下你有不同商店销售的小部件数量以及每个商店每件小部件的价格的数据。你可以将每个变量表示为向量，然后对这些向量进行哈达玛乘法以计算每个商店的小部件收入（这与跨所有商店的总收入不同，后者可以通过点积计算）。
- en: Outer Product
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外积
- en: 'The outer product is a way to create a matrix from a column vector and a row
    vector. Each row in the outer product matrix is the row vector scalar multiplied
    by the corresponding element in the column vector. We could also say that each
    column in the product matrix is the column vector scalar multiplied by the corresponding
    element in the row vector. In [Chapter 6](ch06.xhtml#Chapter_6), I’ll call this
    a “rank-1 matrix,” but don’t worry about the term for now; instead, focus on the
    pattern illustrated in the following example:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 外积是通过一个列向量和一个行向量创建矩阵的一种方式。外积矩阵中的每一行是行向量与列向量中对应元素的标量乘积。我们也可以说外积矩阵中的每一列是列向量与行向量中对应元素的标量乘积。在[第
    6 章](ch06.xhtml#Chapter_6)中，我将称之为“秩-1 矩阵”，但现在不要担心这个术语；而是要专注于下面示例中展示的模式：
- en: <math alttext="Start 3 By 1 Matrix 1st Row  a 2nd Row  b 3rd Row  c EndMatrix
    Start 1 By 2 Matrix 1st Row 1st Column d 2nd Column e EndMatrix equals Start 3
    By 2 Matrix 1st Row 1st Column a d 2nd Column a e 2nd Row 1st Column b d 2nd Column
    b e 3rd Row 1st Column c d 2nd Column c e EndMatrix" display="block"><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mi>a</mi></mtd></mtr> <mtr><mtd><mi>b</mi></mtd></mtr>
    <mtr><mtd><mi>c</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mi>d</mi></mtd>
    <mtd><mi>e</mi></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mi>a</mi>
    <mi>d</mi></mrow></mtd> <mtd><mrow><mi>a</mi> <mi>e</mi></mrow></mtd></mtr> <mtr><mtd><mrow><mi>b</mi>
    <mi>d</mi></mrow></mtd> <mtd><mrow><mi>b</mi> <mi>e</mi></mrow></mtd></mtr> <mtr><mtd><mrow><mi>c</mi>
    <mi>d</mi></mrow></mtd> <mtd><mrow><mi>c</mi> <mi>e</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 3 By 1 Matrix 1st Row  a 2nd Row  b 3rd Row  c EndMatrix
    Start 1 By 2 Matrix 1st Row 1st Column d 2nd Column e EndMatrix equals Start 3
    By 2 Matrix 1st Row 1st Column a d 2nd Column a e 2nd Row 1st Column b d 2nd Column
    b e 3rd Row 1st Column c d 2nd Column c e EndMatrix" display="block"><mrow><mfenced
    close="]" open="["><mtable><mtr><mtd><mi>a</mi></mtd></mtr> <mtr><mtd><mi>b</mi></mtd></mtr>
    <mtr><mtd><mi>c</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mi>d</mi></mtd>
    <mtd><mi>e</mi></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mi>a</mi>
    <mi>d</mi></mrow></mtd> <mtd><mrow><mi>a</mi> <mi>e</mi></mrow></mtd></mtr> <mtr><mtd><mrow><mi>b</mi>
    <mi>d</mi></mrow></mtd> <mtd><mrow><mi>b</mi> <mi>e</mi></mrow></mtd></mtr> <mtr><mtd><mrow><mi>c</mi>
    <mi>d</mi></mrow></mtd> <mtd><mrow><mi>c</mi> <mi>e</mi></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'The outer product is quite different from the dot product: it produces a matrix
    instead of a scalar, and the two vectors in an outer product can have different
    dimensionalities, whereas the two vectors in a dot product must have the same
    dimensionality.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 外积与点积有很大的不同：它产生一个矩阵而不是一个标量，并且外积中的两个向量可以具有不同的维度，而点积中的两个向量必须具有相同的维度。
- en: The outer product is indicated as <math alttext="bold v bold w Superscript upper
    T"><mrow><mi>𝐯</mi> <msup><mi>𝐰</mi> <mtext>T</mtext></msup></mrow></math> (remember
    that we assume vectors are in column orientation; therefore, the outer product
    involves multiplying a column by a row). Note the subtle but important difference
    between notation for the dot product ( <math alttext="bold v Superscript upper
    T Baseline bold w"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup> <mi>𝐰</mi></mrow></math>
    ) and the outer product ( <math alttext="bold v bold w Superscript upper T"><mrow><mi>𝐯</mi>
    <msup><mi>𝐰</mi> <mtext>T</mtext></msup></mrow></math> ). This might seem strange
    and confusing now, but I promise it will make perfect sense after learning about
    matrix multiplication in [Chapter 5](ch05.xhtml#Chapter_5).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 外积表示为<math alttext="bold v bold w Superscript upper T"><mrow><mi>𝐯</mi> <msup><mi>𝐰</mi>
    <mtext>T</mtext></msup></mrow></math>（请记住我们假设向量是列向量，因此外积涉及将列乘以行）。注意点积（<math alttext="bold
    v Superscript upper T Baseline bold w"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <mi>𝐰</mi></mrow></math>）与外积（<math alttext="bold v bold w Superscript upper T"><mrow><mi>𝐯</mi>
    <msup><mi>𝐰</mi> <mtext>T</mtext></msup></mrow></math>）之间的微妙但重要的差异。现在这可能看起来奇怪和令人困惑，但我保证在学习第
    5 章关于矩阵乘法后，这将变得非常清晰。
- en: 'The outer product is similar to broadcasting, but they are not the same: *broadcasting*
    is a general coding operation that is used to expand vectors in arithmetic operations
    such as addition, multiplication, and division; the *outer product* is a specific
    mathematical procedure for multiplying two vectors.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 外积类似于广播，但它们并不相同：*广播*是一种通用的编码操作，用于在算术运算（如加法、乘法和除法）中扩展向量；*外积*是一种特定的数学过程，用于两个向量的乘法。
- en: NumPy can compute the outer product via the function `np.outer()` or the function
    `np.dot()` if the two input vectors are in, respectively, column and row orientation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 可以通过函数`np.outer()`或函数`np.dot()`计算外积，如果两个输入向量分别是列向量和行向量。
- en: Cross and Triple Products
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉乘积和三重积
- en: There are a few other ways to multiply vectors such as the cross product or
    triple product. Those methods are used in geometry and physics, but don’t come
    up often enough in tech-related applications to spend any time on in this book.
    I mention them here only so you have passing familiarity with the names.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '"还有其他几种向量乘法方法，如叉积或三重积。这些方法用于几何和物理学中，但在技术相关应用中很少出现，本书不会花时间介绍它们。我在这里提到它们只是让你对这些名词有所了解。"'
- en: Orthogonal Vector Decomposition
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '"正交向量分解"'
- en: To “decompose” a vector or matrix means to break up that matrix into multiple
    simpler pieces. Decompositions are used to reveal information that is “hidden”
    in a matrix, to make the matrix easier to work with, or for data compression.
    It is no understatement to write that much of linear algebra (in the abstract
    and in practice) involves matrix decompositions. Matrix decompositions are a big
    deal.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '"矩阵或向量的“分解”意味着将该矩阵分解为多个更简单的部分。分解用于揭示矩阵中“隐藏”的信息，使矩阵更易于处理，或用于数据压缩。可以毫不夸张地说，线性代数的很多内容（无论是抽象的还是实际的）都涉及矩阵分解。矩阵分解非常重要。"'
- en: 'Let me introduce the concept of a decomposition using two simple examples with
    scalars:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '"让我用两个简单的标量示例介绍分解的概念："'
- en: 'We can decompose the number 42.01 into two pieces: 42 and .01\. Perhaps .01
    is noise to be ignored, or perhaps the goal is to compress the data (the integer
    42 requires less memory than the floating-point 42.01). Regardless of the motivation,
    the decomposition involves representing one mathematical object as the sum of
    simpler objects (42 = 42 + .01).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"我们可以将数字42.01分解为两部分：42和.01。也许.01是要忽略的噪音，或者目标是压缩数据（整数42比浮点数42.01需要更少的内存）。无论动机如何，分解涉及将一个数学对象表示为更简单对象的和（42
    = 42 + .01）。"'
- en: 'We can decompose the number 42 into the product of prime numbers 2, 3, and
    7\. This decomposition is called *prime factorization* and has many applications
    in numerical processing and cryptography. This example involves products instead
    of sums, but the point is the same: decompose one mathematical object into smaller,
    simpler pieces.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"我们可以将数字42分解为质数2、3和7的乘积。这种分解称为*质因数分解*，在数值处理和密码学中有许多应用。这个例子涉及乘积而不是求和，但要点是相同的：将一个数学对象分解为更小、更简单的部分。"'
- en: In this section, we will begin exploring a simple yet important decomposition,
    which is to break up a vector into two separate vectors, one of which is orthogonal
    to a reference vector while the other is parallel to that reference vector. Orthogonal
    vector decomposition directly leads to the Gram-Schmidt procedure and QR decomposition,
    which is used frequently when solving inverse problems in statistics.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '"在本节中，我们将开始探讨一个简单但重要的分解方法，即将一个向量分解为两个分开的向量，其中一个与参考向量正交，另一个与该参考向量平行。正交向量分解直接导致了格拉姆-施密特过程和QR分解，这在解决统计学中的逆问题时经常使用。"'
- en: 'Let’s begin with a picture so you can visualize the goal of the decomposition.
    [Figure 2-6](#fig_2_6) illustrates the situation: we have two vectors <math alttext="bold
    a"><mi>𝐚</mi></math> and <math alttext="bold b"><mi>𝐛</mi></math> in standard
    position, and our goal is find the point on <math alttext="bold a"><mi>𝐚</mi></math>
    that is as close as possible to the head of <math alttext="bold b"><mi>𝐛</mi></math>
    . We could also express this as an optimization problem: project vector <math
    alttext="bold b"><mi>𝐛</mi></math> onto vector <math alttext="bold a"><mi>𝐚</mi></math>
    such that the projection distance is minimized. Of course, that point on <math
    alttext="bold a"><mi>𝐚</mi></math> will be a scaled version of <math alttext="bold
    a"><mi>𝐚</mi></math> ; in other words, <math alttext="beta bold a"><mrow><mi>β</mi>
    <mi>𝐚</mi></mrow></math> . So now our goal is to find the scalar <math alttext="beta"><mi>β</mi></math>
    . (The connection to orthogonal vector decomposition will soon be clear.)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '"让我们从一幅图开始，这样你可以看到分解的目标。[图2-6](#fig_2_6)说明了这种情况：我们有两个位于标准位置的向量 <math alttext="bold
    a"><mi>𝐚</mi></math> 和 <math alttext="bold b"><mi>𝐛</mi></math> ，我们的目标是找到 <math
    alttext="bold b"><mi>𝐛</mi></math> 头部最接近的 <math alttext="bold a"><mi>𝐚</mi></math>
    点。我们还可以将其表达为一个优化问题：将向量 <math alttext="bold b"><mi>𝐛</mi></math> 投影到向量 <math alttext="bold
    a"><mi>𝐚</mi></math> 上，使得投影距离最小化。当然，在 <math alttext="bold a"><mi>𝐚</mi></math>
    上的那一点将会是 <math alttext="beta bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math>
    的缩放版本。所以我们的目标是找到标量 <math alttext="beta"><mi>β</mi></math> 。（与正交向量分解的联系很快会变得清晰。）"'
- en: '![The picture.](assets/plad_0206.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '"![图片。](assets/plad_0206.png)"'
- en: Figure 2-6\. To project a point at the head of <math alttext="bold b"><mi>𝐛</mi></math>
    onto a vector <math alttext="bold a"><mi>𝐚</mi></math> with minimum distance,
    we need a formula to compute <math alttext="beta"><mi>β</mi></math> such that
    the length of the projection vector <math alttext="left-parenthesis bold b minus
    beta bold a right-parenthesis"><mrow><mo>(</mo> <mi>𝐛</mi> <mo>-</mo> <mi>β</mi>
    <mi>𝐚</mi> <mo>)</mo></mrow></math> is minimized
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-6\. 投影 <math alttext="bold b"><mi>𝐛</mi></math> 头部的点到向量 <math alttext="bold
    a"><mi>𝐚</mi></math> 上，以获得最小距离，我们需要一个计算 <math alttext="beta"><mi>β</mi></math>
    的公式，使投影向量 <math alttext="left-parenthesis bold b minus beta bold a right-parenthesis"><mrow><mo>(</mo>
    <mi>𝐛</mi> <mo>-</mo> <mi>β</mi> <mi>𝐚</mi> <mo>)</mo></mrow></math> 的长度最小化。
- en: Importantly, we can use vector subtraction to define the line from <math alttext="bold
    b"><mi>𝐛</mi></math> to <math alttext="beta bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math>
    . We could give this line its own letter, e.g., vector <math alttext="bold c"><mi>𝐜</mi></math>
    , but the subtraction is necessary for discovering the solution.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们可以使用向量减法来定义从 <math alttext="bold b"><mi>𝐛</mi></math> 到 <math alttext="beta
    bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math> 的直线。我们可以给这条线一个独立的字母，例如向量 <math
    alttext="bold c"><mi>𝐜</mi></math> ，但减法对于发现解决方案是必要的。
- en: The key insight that leads to the solution to this problem is that the point
    on <math alttext="bold a"><mi>𝐚</mi></math> that is closest to the head of <math
    alttext="bold b"><mi>𝐛</mi></math> is found by drawing a line from <math alttext="bold
    b"><mi>𝐛</mi></math> that meets <math alttext="bold a"><mi>𝐚</mi></math> at a
    right angle. The intuition here is to imagine a triangle formed by the origin,
    the head of <math alttext="bold b"><mi>𝐛</mi></math> , and <math alttext="beta
    bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math> ; the length of the line from
    <math alttext="bold b"><mi>𝐛</mi></math> to <math alttext="beta bold a"><mrow><mi>β</mi>
    <mi>𝐚</mi></mrow></math> gets longer as the angle <math alttext="measured-angle
    beta bold a"><mrow><mi>∡</mi> <mi>β</mi> <mi>𝐚</mi></mrow></math> gets smaller
    than <math alttext="90 Superscript ring"><msup><mn>90</mn> <mo>∘</mo></msup></math>
    or larger than <math alttext="90 Superscript ring"><msup><mn>90</mn> <mo>∘</mo></msup></math>
    .
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 导致解决此问题的关键见解是，距离 <math alttext="bold b"><mi>𝐛</mi></math> 头部最近的 <math alttext="bold
    a"><mi>𝐚</mi></math> 上的点是通过从 <math alttext="bold b"><mi>𝐛</mi></math> 到 <math
    alttext="beta bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math> 画一条直线来找到的，该直线与
    <math alttext="bold a"><mi>𝐚</mi></math> 垂直。这里的直觉是想象一个由原点、<math alttext="bold
    b"><mi>𝐛</mi></math> 头部和 <math alttext="beta bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math>
    组成的三角形；从 <math alttext="bold b"><mi>𝐛</mi></math> 到 <math alttext="beta bold a"><mrow><mi>β</mi>
    <mi>𝐚</mi></mrow></math> 的线段长度随角度 <math alttext="measured-angle beta bold a"><mrow><mi>∡</mi>
    <mi>β</mi> <mi>𝐚</mi></mrow></math> 小于 <math alttext="90 Superscript ring"><msup><mn>90</mn>
    <mo>∘</mo></msup></math> 或大于 <math alttext="90 Superscript ring"><msup><mn>90</mn>
    <mo>∘</mo></msup></math> 而变长。
- en: 'Putting this together, we have deduced that <math alttext="left-parenthesis
    bold b minus beta bold a right-parenthesis"><mrow><mo>(</mo> <mi>𝐛</mi> <mo>-</mo>
    <mi>β</mi> <mi>𝐚</mi> <mo>)</mo></mrow></math> is orthogonal to <math alttext="beta
    bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math> , which is the same thing as
    saying that those vectors are perpendicular. And that means that the dot product
    between them must be zero. Let’s transform those words into an equation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 把这些放在一起，我们推断出 <math alttext="left-parenthesis bold b minus beta bold a right-parenthesis"><mrow><mo>(</mo>
    <mi>𝐛</mi> <mo>-</mo> <mi>β</mi> <mi>𝐚</mi> <mo>)</mo></mrow></math> 垂直于 <math
    alttext="beta bold a"><mrow><mi>β</mi> <mi>𝐚</mi></mrow></math> ，这意味着这些向量是垂直的。这意味着它们的点积必须为零。让我们把这些话语转化为一个方程：
- en: <math alttext="bold a Superscript upper T Baseline left-parenthesis bold b minus
    beta bold a right-parenthesis equals 0" display="block"><mrow><msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>𝐛</mi> <mo>-</mo> <mi>β</mi> <mi>𝐚</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold a Superscript upper T Baseline left-parenthesis bold b minus
    beta bold a right-parenthesis equals 0" display="block"><mrow><msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>𝐛</mi> <mo>-</mo> <mi>β</mi> <mi>𝐚</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>
- en: 'From here, we can apply some algebra to solve for <math alttext="beta"><mi>β</mi></math>
    (note the application of the distributive property of dot products), which is
    shown in [Equation 2-12](#orth_proj_dot):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以应用一些代数来解 <math alttext="beta"><mi>β</mi></math> 的问题（注意点积的分配性质的应用），如
    [方程式 2-12](#orth_proj_dot) 所示：
- en: Equation 2-12\. Solving the orthogonal projection problem
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 2-12\. 解决正交投影问题
- en: <math alttext="StartLayout 1st Row 1st Column bold a Superscript upper T Baseline
    bold b minus beta bold a Superscript upper T Baseline bold a 2nd Column equals
    0 2nd Row 1st Column beta bold a Superscript upper T Baseline bold a 2nd Column
    equals bold a Superscript upper T Baseline bold b 3rd Row 1st Column beta 2nd
    Column equals StartFraction bold a Superscript upper T Baseline bold b Over bold
    a Superscript upper T Baseline bold a EndFraction EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup>
    <mi>𝐛</mi> <mo>-</mo> <mi>β</mi> <msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐚</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>β</mi> <msup><mi>𝐚</mi> <mtext>T</mtext></msup>
    <mi>𝐚</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mi>𝐛</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi>β</mi></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup>
    <mi>𝐛</mi></mrow> <mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐚</mi></mrow></mfrac></mrow></mtd></mtr></mtable></math>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold a Superscript upper T Baseline
    bold b minus beta bold a Superscript upper T Baseline bold a 2nd Column equals
    0 2nd Row 1st Column beta bold a Superscript upper T Baseline bold a 2nd Column
    equals bold a Superscript upper T Baseline bold b 3rd Row 1st Column beta 2nd
    Column equals StartFraction bold a Superscript upper T Baseline bold b Over bold
    a Superscript upper T Baseline bold a EndFraction EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup>
    <mi>𝐛</mi> <mo>-</mo> <mi>β</mi> <msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐚</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mi>β</mi> <msup><mi>𝐚</mi> <mtext>T</mtext></msup>
    <mi>𝐚</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mi>𝐛</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi>β</mi></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup>
    <mi>𝐛</mi></mrow> <mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup> <mi>𝐚</mi></mrow></mfrac></mrow></mtd></mtr></mtable></math>
- en: 'This is quite beautiful: we began with a simple geometric picture, explored
    the implications of the geometry, expressed those implications as a formula, and
    then applied a bit of algebra. And the upshot is that we discovered a formula
    for projecting a point onto a line with minimum distance. This is called *orthogonal
    projection*, and it is the basis for many applications in statistics and machine
    learning, including the famous least squares formula for solving linear models
    (you’ll see orthogonal projections in Chapters [9](ch09.xhtml#Chapter_09), [10](ch10.xhtml#Chapter_10),
    and [11](ch11.xhtml#Chapter_11)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太美妙了：我们从一个简单的几何图像开始，探索了几何图像的含义，将这些含义表达为一个公式，然后应用了一点代数。最终，我们发现了一个将点投影到具有最小距离的线上的公式。这被称为*正交投影*，它是统计学和机器学习中许多应用的基础，包括著名的最小二乘法公式（你将在第
    [9](ch09.xhtml#Chapter_09)、[10](ch10.xhtml#Chapter_10) 和 [11](ch11.xhtml#Chapter_11)
    章节中看到正交投影）。
- en: I can imagine that you’re super curious to see what the Python code would look
    like to implement this formula. But you’re going to have to write that code yourself
    in [Exercise 2-8](#exercise_2_8) at the end of this chapter. If you can’t wait
    until the end of the chapter, feel free to solve that exercise now, and then continue
    learning about orthogonal decomposition.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以想象你肯定非常想看看如何使用 Python 代码来实现这个公式。但你需要自己在本章末尾的 [练习 2-8](#exercise_2_8) 中编写这段代码。如果你等不及想要先解决这个练习，然后继续学习正交分解。
- en: You might be wondering how this is related to orthogonal vector decomposition,
    i.e., the title of this section. The minimum distance projection is the necessary
    grounding, and you’re now ready to learn the decomposition.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道这与正交向量分解，也就是本节标题有什么关系。最小距离投影是必要的基础，你现在已经准备好学习这种分解了。
- en: As usual, we start with the setup and the goal. We begin with two vectors, which
    I’ll call the “target vector” and the “reference vector.” Our goal is to decompose
    the target vector into two other vectors such that (1) those two vectors sum to
    the target vector, and (2) one vector is orthogonal to the reference vector while
    the other is parallel to the reference vector. The situation is illustrated in
    [Figure 2-7](#fig_2_7).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们从设置和目标开始。我们有两个向量，我将它们称为“目标向量”和“参考向量”。我们的目标是将目标向量分解为另外两个向量，这两个向量的和等于目标向量，并且其中一个向量垂直于参考向量，另一个平行于参考向量。这种情况在
    [图 2-7](#fig_2_7) 中有所说明。
- en: 'Before starting with the math, let’s get our terms straight: I will call the
    target vector <math alttext="bold t"><mi>𝐭</mi></math> and the reference vector
    <math alttext="bold r"><mi>𝐫</mi></math> . Then, the two vectors formed from the
    target vector will be called the *perpendicular component*, indicated as <math
    alttext="bold t Subscript up-tack bold r"><msub><mi>𝐭</mi> <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub></math>
    , and the *parallel component*, indicated as <math alttext="bold t Subscript parallel-to
    bold r"><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></math> .'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数学推导之前，让我们先搞清楚术语：我将称目标向量为 <math alttext="bold t"><mi>𝐭</mi></math>，参考向量为
    <math alttext="bold r"><mi>𝐫</mi></math>。然后，从目标向量形成的两个向量将被称为*垂直分量*，表示为 <math alttext="bold
    t Subscript up-tack bold r"><msub><mi>𝐭</mi> <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub></math>，以及*平行分量*，表示为
    <math alttext="bold t Subscript parallel-to bold r"><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></math>。
- en: '![What does this do?](assets/plad_0207.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![这是做什么的？](assets/plad_0207.png)'
- en: 'Figure 2-7\. Illustration of orthogonal vector decomposition: decompose vector
    <math alttext="bold t"><mi>𝐭</mi></math> into the sum of two other vectors that
    are orthogonal and parallel to vector <math alttext="bold r"><mi>𝐫</mi></math>'
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-7\. 正交向量分解的示例：将向量 <math alttext="bold t"><mi>𝐭</mi></math> 分解为与向量 <math
    alttext="bold r"><mi>𝐫</mi></math> 正交且平行的两个向量之和。
- en: 'We begin by defining the parallel component. What is a vector that is parallel
    to <math alttext="bold r"><mi>𝐫</mi></math> ? Well, any scaled version of <math
    alttext="bold r"><mi>𝐫</mi></math> is obviously parallel to <math alttext="bold
    r"><mi>𝐫</mi></math> . So, we find <math alttext="bold t Subscript parallel-to
    bold r"><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></math> simply
    by applying the orthogonal projection formula that we just discovered ([Equation
    2-13](#find-tParallel)):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从定义平行分量开始。什么是与<math alttext="bold r"><mi>𝐫</mi></math>平行的向量？显然，任何<math alttext="bold
    r"><mi>𝐫</mi></math>的缩放版本都与<math alttext="bold r"><mi>𝐫</mi></math>平行。因此，我们通过刚刚发现的正交投影公式（[Equation
    2-13](#find-tParallel)）来简单地找到<math alttext="bold t Subscript parallel-to bold
    r"><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></math>。
- en: Equation 2-13\. Computing the parallel component of **t** with respect to **r**
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式2-13。计算**t**相对于**r**的平行分量
- en: <math alttext="bold t Subscript parallel-to bold r Baseline equals bold r StartFraction
    bold t Superscript upper T Baseline bold r Over bold r Superscript upper T Baseline
    bold r EndFraction" display="block"><mrow><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub>
    <mo>=</mo> <mi>𝐫</mi> <mfrac><mrow><msup><mi>𝐭</mi> <mtext>T</mtext></msup> <mi>𝐫</mi></mrow>
    <mrow><msup><mi>𝐫</mi> <mtext>T</mtext></msup> <mi>𝐫</mi></mrow></mfrac></mrow></math>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold t Subscript parallel-to bold r Baseline equals bold r StartFraction
    bold t Superscript upper T Baseline bold r Over bold r Superscript upper T Baseline
    bold r EndFraction" display="block"><mrow><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub>
    <mo>=</mo> <mi>𝐫</mi> <mfrac><mrow><msup><mi>𝐭</mi> <mtext>T</mtext></msup> <mi>𝐫</mi></mrow>
    <mrow><msup><mi>𝐫</mi> <mtext>T</mtext></msup> <mi>𝐫</mi></mrow></mfrac></mrow></math>
- en: 'Note the subtle difference to [Equation 2-12](#orth_proj_dot): there we only
    computed the scalar <math alttext="beta"><mi>β</mi></math> ; here we want to compute
    the scaled vector <math alttext="beta bold r"><mrow><mi>β</mi> <mi>𝐫</mi></mrow></math>
    .'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意与[Equation 2-12](#orth_proj_dot)的微妙差别：那里我们只计算了标量<math alttext="beta"><mi>β</mi></math>；而这里我们要计算缩放向量<math
    alttext="beta bold r"><mrow><mi>β</mi> <mi>𝐫</mi></mrow></math>。
- en: 'That’s the parallel component. How do we find the perpendicular component?
    That one is easier, because we already know that the two vector components must
    sum to the original target vector. Thus:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是平行分量。我们如何找到垂直分量呢？这更容易，因为我们已经知道这两个向量分量必须等于原始目标向量。因此：
- en: <math alttext="StartLayout 1st Row 1st Column bold t 2nd Column equals bold
    t Subscript up-tack bold r Baseline plus bold t Subscript parallel-to bold r Baseline
    2nd Row 1st Column bold t Subscript up-tack bold r 2nd Column equals bold t minus
    bold t Subscript parallel-to bold r EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>𝐭</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi>𝐭</mi> <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub> <mo>+</mo> <msub><mi>𝐭</mi>
    <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msub><mi>𝐭</mi>
    <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐭</mi> <mo>-</mo> <msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold t 2nd Column equals bold
    t Subscript up-tack bold r Baseline plus bold t Subscript parallel-to bold r Baseline
    2nd Row 1st Column bold t Subscript up-tack bold r 2nd Column equals bold t minus
    bold t Subscript parallel-to bold r EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mi>𝐭</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi>𝐭</mi> <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub> <mo>+</mo> <msub><mi>𝐭</mi>
    <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><msub><mi>𝐭</mi>
    <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐭</mi> <mo>-</mo> <msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></mrow></mtd></mtr></mtable></math>
- en: In other words, we subtract off the parallel component from the original vector,
    and the residual is our perpendicular component.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们从原始向量中减去平行分量，剩余部分即为我们的垂直分量。
- en: 'But is that perpendicular component *really* orthogonal to the reference vector?
    Yes, it is! To prove it, you show that the dot product between the perpendicular
    component and the reference vector is zero:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 但是那个垂直分量*真的*与参考向量正交吗？是的，它是！为了证明这一点，你可以展示垂直分量与参考向量的点积为零：
- en: <math alttext="StartLayout 1st Row 1st Column left-parenthesis bold t Subscript
    up-tack bold r Baseline right-parenthesis Superscript upper T Baseline bold r
    2nd Column equals 0 2nd Row 1st Column left-parenthesis bold t minus bold r StartFraction
    bold t Superscript upper T Baseline bold r Over bold r Superscript upper T Baseline
    bold r EndFraction right-parenthesis Superscript upper T Baseline bold r 2nd Column
    equals 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><msub><mi>𝐭</mi>
    <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub> <mo>)</mo></mrow> <mtext>T</mtext></msup>
    <mi>𝐫</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><mi>𝐭</mi><mo>-</mo><mi>𝐫</mi><mfrac><mrow><msup><mi>𝐭</mi>
    <mtext>T</mtext></msup> <mi>𝐫</mi></mrow> <mrow><msup><mi>𝐫</mi> <mtext>T</mtext></msup>
    <mi>𝐫</mi></mrow></mfrac><mo>)</mo></mrow> <mtext>T</mtext></msup> <mi>𝐫</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column left-parenthesis bold t Subscript
    up-tack bold r Baseline right-parenthesis Superscript upper T Baseline bold r
    2nd Column equals 0 2nd Row 1st Column left-parenthesis bold t minus bold r StartFraction
    bold t Superscript upper T Baseline bold r Over bold r Superscript upper T Baseline
    bold r EndFraction right-parenthesis Superscript upper T Baseline bold r 2nd Column
    equals 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><msub><mi>𝐭</mi>
    <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub> <mo>)</mo></mrow> <mtext>T</mtext></msup>
    <mi>𝐫</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><mi>𝐭</mi><mo>-</mo><mi>𝐫</mi><mfrac><mrow><msup><mi>𝐭</mi>
    <mtext>T</mtext></msup> <mi>𝐫</mi></mrow> <mrow><msup><mi>𝐫</mi> <mtext>T</mtext></msup>
    <mi>𝐫</mi></mrow></mfrac><mo>)</mo></mrow> <mtext>T</mtext></msup> <mi>𝐫</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></math>
- en: Working through the algebra of this proof is straightforward but tedious, so
    I’ve omitted it. Instead, you’ll work on building intuition using Python code
    in the exercises.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个证明的代数过程是直接但乏味的，所以我省略了它。相反，你将通过Python代码来建立直觉。
- en: 'I hope you enjoyed learning about orthogonal vector decomposition. Note again
    the general principle: we break apart one mathematical object into a combination
    of other objects. The details of the decomposition depend on our constraints (in
    this case, orthogonal and parallel to a reference vector), which means that different
    constraints (that is, different goals of the analysis) can lead to different decompositions
    of the same vector.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢学习正交向量分解。再次注意一般原则：我们将一个数学对象分解为其他对象的组合。分解的细节取决于我们的约束条件（在本例中为正交和平行于参考向量），这意味着不同的约束条件（即分析目标的不同）可以导致相同向量的不同分解。
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The beauty of linear algebra is that even the most sophisticated and computationally
    intense operations on matrices are made up of simple operations, most of which
    can be understood with geometric intuition. Do not underestimate the importance
    of studying simple operations on vectors, because what you learned in this chapter
    will form the basis for the rest of the book—and the rest of your career as an
    *applied linear algebratician* (which is what you really are if you do anything
    with data science, machine learning, AI, deep learning, image processing, computational
    vision, statistics, blah blah blah).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数的美在于，即使是对矩阵进行最复杂和计算密集的操作，也可以分解为简单操作，其中大部分可以通过几何直觉来理解。不要低估对向量进行简单操作的重要性，因为你在本章学到的将构成本书以及你作为*应用线性代数学家*（如果你从事数据科学、机器学习、人工智能、深度学习、图像处理、计算视觉、统计等工作，这才是你真正的身份）职业生涯的基础。
- en: 'Here are the most important take-home messages of this chapter:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本章最重要的要点：
- en: A vector is an ordered list of numbers that is placed in a column or in a row.
    The number of elements in a vector is called its dimensionality, and a vector
    can be represented as a line in a geometric space with the number of axes equal
    to the dimensionality.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量是一个按顺序排列的数字列表，可以放置在列或行中。向量的元素数称为其维数，向量可以在几何空间中表示为具有与维数相等的轴数的线。
- en: Several arithmetic operations (addition, subtraction, and Hadamard multiplication)
    on vectors work element-wise.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的几个算术运算（加法、减法和Hadamard乘法）按元素逐个操作。
- en: The dot product is a single number that encodes the relationship between two
    vectors of the same dimensionality, and is computed as element-wise multiplication
    and sum.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积是一个单一的数字，编码了两个相同维度向量之间的关系，计算方法是元素逐个相乘并求和。
- en: The dot product is zero for vectors that are orthogonal, which geometrically
    means that the vectors meet at a right angle.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于正交向量，点积为零，几何上意味着向量在直角处相交。
- en: Orthogonal vector decomposition involves breaking up a vector into the sum of
    two other vectors that are orthogonal and parallel to a reference vector. The
    formula for this decomposition can be rederived from the geometry, but you should
    remember the phrase “mapping over magnitude” as the concept that that formula
    expresses.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正交向量分解涉及将一个向量分解为另外两个与参考向量正交且平行的向量之和。这种分解的公式可以从几何中重新推导出来，但你应该记住“映射到大小”的概念正是这个公式表达的概念。
- en: Code Exercises
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码练习
- en: I hope you don’t see these exercises as tedious work that you need to do. Instead,
    these exercises are opportunities to polish your math and coding skills, and to
    make sure that you really understand the material in this chapter.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你不要把这些练习看作是你需要完成的烦人工作。相反，这些练习是提升你的数学和编码技能的机会，确保你真正理解本章内容。
- en: 'I also want you to see these exercises as a springboard to continue exploring
    linear algebra using Python. Change the code to use different numbers, different
    dimensionalities, different orientations, etc. Write your own code to test other
    concepts mentioned in the chapter. Most importantly: have fun and embrace the
    learning experience.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我也希望你把这些练习看作是使用Python继续探索线性代数的跳板。更改代码以使用不同的数字、不同的维度、不同的方向等。编写自己的代码来测试章节中提到的其他概念。最重要的是：享受学习过程，拥抱学习的体验。
- en: 'As a reminder: the solutions to all the exercises can be viewed or downloaded
    from [*https://github.com/mikexcohen/LA4DataScience*](https://github.com/mikexcohen/LA4DataScience).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒：所有练习的解答可以在[*https://github.com/mikexcohen/LA4DataScience*](https://github.com/mikexcohen/LA4DataScience)上查看或下载。
- en: Exercise 2-1\.
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-1。
- en: The online code repository is “missing” code to create [Figure 2-2](#fig_2_2).
    (It’s not really *missing*—I moved it into the solution to this exercise.) So,
    your goal here is to write your own code to produce [Figure 2-2](#fig_2_2).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在线代码库“丢失”了创建[Figure 2-2](#fig_2_2)的代码。（实际上并不是真的“丢失” — 我把它移到了这个练习的解决方案中。）因此，你的目标是编写自己的代码来生成[Figure 2-2](#fig_2_2)。
- en: Exercise 2-2\.
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-2。
- en: Write an algorithm that computes the norm of a vector by translating [Equation
    2-7](#vector-norm) into code. Confirm, using random vectors with different dimensionalities
    and orientations, that you get the same result as `np.linalg.norm()`. This exercise
    is designed to give you more experience with indexing NumPy arrays and translating
    formulas into code; in practice, it’s often easier to use `np.linalg.norm()`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个算法，通过将[Equation 2-7](#vector-norm)翻译成代码来计算向量的范数。使用不同维度和方向的随机向量来确认，你得到的结果与`np.linalg.norm()`相同。这个练习旨在让你更多地练习索引NumPy数组和将公式转化为代码；实际上，使用`np.linalg.norm()`通常更容易。
- en: Exercise 2-3\.
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-3。
- en: Create a Python function that will take a vector as input and output a unit
    vector in the same direction. What happens when you input the zeros vector?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Python函数，将一个向量作为输入，并输出与之方向相同的单位向量。当你输入零向量时会发生什么？
- en: Exercise 2-4\.
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-4。
- en: You know how to create *unit* vectors; what if you want to create a vector of
    any arbitrary magnitude? Write a Python function that will take a vector and a
    desired magnitude as inputs and will return a vector in the same direction but
    with a magnitude corresponding to the second input.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道如何创建*单位*向量；如果你想创建任意大小的向量怎么办？编写一个Python函数，它将接受一个向量和一个期望的大小作为输入，并返回一个方向相同但大小对应于第二个输入的向量。
- en: Exercise 2-5\.
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-5。
- en: Write a `for` loop to transpose a row vector into a column vector without using
    a built-in function or method such as `np.transpose()` or `v.T`. This exercise
    will help you create and index orientation-endowed vectors.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个`for`循环，将行向量转置为列向量，而不使用如`np.transpose()`或`v.T`等内置函数或方法。这个练习将帮助你创建和索引具有方向性的向量。
- en: Exercise 2-6\.
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-6。
- en: 'Here is an interesting fact: you can compute the squared norm of a vector as
    the dot product of that vector with itself. Look back to [Equation 2-8](#eq-vecnorm)
    to convince yourself of this equivalence. Then confirm it using Python.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的事实：你可以计算向量的平方范数，就像将向量与自身的点积一样。回顾 [方程 2-8](#eq-vecnorm) 以确信这两者是等价的。然后使用
    Python 来确认它。
- en: Exercise 2-7\.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-7。
- en: Write code to demonstrate that the dot product is *commutative*. Commutative
    means that <math alttext="a times b equals b times a"><mrow><mi>a</mi> <mo>×</mo>
    <mi>b</mi> <mo>=</mo> <mi>b</mi> <mo>×</mo> <mi>a</mi></mrow></math> , which,
    for the vector dot product, means that <math alttext="bold a Superscript upper
    T Baseline bold b equals bold b Superscript upper T Baseline bold a"><mrow><msup><mi>𝐚</mi>
    <mtext>T</mtext></msup> <mi>𝐛</mi> <mo>=</mo> <msup><mi>𝐛</mi> <mtext>T</mtext></msup>
    <mi>𝐚</mi></mrow></math> . After demonstrating this in code, use equation [Equation
    2-9](#dp-alg) to understand why the dot product is commutative.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 编写代码来证明点积是*可交换*的。可交换意味着 <math alttext="a times b equals b times a"><mrow><mi>a</mi>
    <mo>×</mo> <mi>b</mi> <mo>=</mo> <mi>b</mi> <mo>×</mo> <mi>a</mi></mrow></math>
    ，对于向量的点积来说，意味着 <math alttext="bold a Superscript upper T Baseline bold b equals
    bold b Superscript upper T Baseline bold a"><mrow><msup><mi>𝐚</mi> <mtext>T</mtext></msup>
    <mi>𝐛</mi> <mo>=</mo> <msup><mi>𝐛</mi> <mtext>T</mtext></msup> <mi>𝐚</mi></mrow></math>
    。在用代码演示后，使用方程 [方程 2-9](#dp-alg) 来理解点积为何是可交换的。
- en: Exercise 2-8\.
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-8。
- en: Write code to produce [Figure 2-6](#fig_2_6). (Note that your solution doesn’t
    need to look *exactly* like the figure, as long as the key elements are present.)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 编写代码生成 [图 2-6](#fig_2_6)。 (注意，你的解决方案不需要与图完全相同，只要关键元素存在即可。)
- en: Exercise 2-9\.
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-9。
- en: Implement orthogonal vector decomposition. Start with two random-number vectors
    <math alttext="bold t"><mi>𝐭</mi></math> and <math alttext="bold r"><mi>𝐫</mi></math>
    , and reproduce [Figure 2-8](#fig_2_8) (note that your plot will look somewhat
    different due to random numbers). Next, confirm that the two components sum to
    <math alttext="bold t"><mi>𝐭</mi></math> and that <math alttext="bold t Subscript
    up-tack bold r"><msub><mi>𝐭</mi> <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub></math>
    and <math alttext="bold t Subscript parallel-to bold r"><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></math>
    are orthogonal.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 实现正交向量分解。从两个随机数向量 <math alttext="bold t"><mi>𝐭</mi></math> 和 <math alttext="bold
    r"><mi>𝐫</mi></math> 开始，并重现 [图 2-8](#fig_2_8)（注意，由于随机数的原因，你的图看起来可能有所不同）。接下来，确认这两个分量的和是
    <math alttext="bold t"><mi>𝐭</mi></math> ，并且 <math alttext="bold t Subscript up-tack
    bold r"><msub><mi>𝐭</mi> <mrow><mo>⊥</mo><mi>𝐫</mi></mrow></msub></math> 和 <math
    alttext="bold t Subscript parallel-to bold r"><msub><mi>𝐭</mi> <mrow><mo>∥</mo><mi>𝐫</mi></mrow></msub></math>
    是正交的。
- en: '![solution to Exercise 2-9](assets/plad_0208.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![练习 2-9 的解答](assets/plad_0208.png)'
- en: Figure 2-8\. Exercise 9
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-8。练习 9。
- en: Exercise 2-10\.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 2-10。
- en: An important skill in coding is finding bugs. Let’s say there is a bug in your
    code such that the denominator in the projection scalar of [Equation 2-13](#find-tParallel)
    is <math alttext="bold t Superscript upper T Baseline bold t"><mrow><msup><mi>𝐭</mi>
    <mtext>T</mtext></msup> <mi>𝐭</mi></mrow></math> instead of <math alttext="bold
    r Superscript upper T Baseline bold r"><mrow><msup><mi>𝐫</mi> <mtext>T</mtext></msup>
    <mi>𝐫</mi></mrow></math> (an easy mistake to make, speaking from personal experience
    while writing this chapter!). Implement this bug to check whether it really deviates
    from the accurate code. What can you do to check whether the result is correct
    or incorrect? (In coding, confirming code with known results is called *sanity-checking*.)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 编程中的一个重要技能是查找错误。假设你的代码中有一个 bug，导致 [方程 2-13](#find-tParallel) 中的投影标量的分母是 <math
    alttext="bold t Superscript upper T Baseline bold t"><mrow><msup><mi>𝐭</mi> <mtext>T</mtext></msup>
    <mi>𝐭</mi></mrow></math> 而不是 <math alttext="bold r Superscript upper T Baseline
    bold r"><mrow><msup><mi>𝐫</mi> <mtext>T</mtext></msup> <mi>𝐫</mi></mrow></math>
    （这是我在写这一章节时个人经历的一个容易犯的错误！）。实现这个 bug 来检查它是否真的导致了代码偏离正确结果。你可以做什么来确认结果是正确的还是错误的？（在编程中，用已知结果来确认代码的正确性被称为*合理检查*。）
- en: ^([1](ch02.xhtml#idm45733300824976-marker)) `a*s` throws an error, because list
    repetition can only be done using integers; it’s not possible to repeat a list
    2.72 times!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#idm45733300824976-marker)) `a*s` 抛出一个错误，因为列表重复只能使用整数；不可能将列表重复
    2.72 次！
- en: ^([2](ch02.xhtml#idm45733308090944-marker)) Python still broadcasts, but the
    result is a 3 × 2 matrix instead of a 2 × 3 matrix.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.xhtml#idm45733308090944-marker)) Python 仍然进行广播，但结果是一个 3 × 2 的矩阵，而不是一个
    2 × 3 的矩阵。
- en: ^([3](ch02.xhtml#idm45733307857968-marker)) The zeros vector has a length of
    0 but no associated unit vector, because it has no direction and because it is
    impossible to scale the zeros vector to have nonzero length.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.xhtml#idm45733307857968-marker)) 零向量的长度为0，但没有相关的单位向量，因为它没有方向，并且不可能将零向量缩放为非零长度。
- en: ^([4](ch02.xhtml#idm45733308534784-marker)) The error is that the two vectors
    have different dimensionalities, which shows that Hadamard multiplication is defined
    only for two vectors of equal dimensionality. You can fix the problem by removing
    one number from `a` or adding one number to `b`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.xhtml#idm45733308534784-marker)) 错误在于这两个向量的维度不同，这表明哈达玛乘积仅对维度相同的两个向量定义。你可以通过从`a`中移除一个数字或向`b`中添加一个数字来修复问题。
