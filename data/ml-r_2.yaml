- en: Part 3\. Regression
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三部分\. 回归
- en: Take a moment to look back on what you’ve learned so far. Assuming you’ve completed
    [parts 1](kindle_split_009.html#part01) and [2](kindle_split_012.html#part02)
    of this book, you now possess the skills you need to tackle a large range of classification
    problems. In this part of the book, we’ll shift our focus from predicting categorical
    variables to predicting continuous ones.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 抽空回顾一下你到目前为止学到的内容。假设你已经完成了本书的[第一部分](kindle_split_009.html#part01)和[第二部分](kindle_split_012.html#part02)，你现在拥有了处理大量分类问题的技能。在本部分书中，我们将把重点从预测分类变量转移到预测连续变量。
- en: As you learned in [chapter 1](kindle_split_010.html#ch01), we use the term *regression*
    for supervised machine learning that predicts a continuous outcome variable. In
    [chapters 9](kindle_split_020.html#ch09) through [12](kindle_split_023.html#ch12),
    you’re going to learn a variety of regression algorithms that will help you deal
    with different data situations. Some of them are suited to situations in which
    there are linear relationships between predictor variables and your outcome, and
    are highly interpretable. Others are able to model nonlinear relationships but
    may not be quite so interpretable.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[第1章](kindle_split_010.html#ch01)中学到的，我们使用术语*回归*来表示预测连续结果变量的监督机器学习。在第9章（kindle_split_020.html#ch09）至第12章（kindle_split_023.html#ch12）中，你将学习各种回归算法，这些算法将帮助你处理不同的数据情况。其中一些适合于预测变量与结果之间存在线性关系的场景，并且具有高度的可解释性。其他算法能够模拟非线性关系，但可能不太容易解释。
- en: We’ll start by covering linear regression—which, as you’ll learn, is closely
    related to the logistic regression we worked with in [chapter 4](kindle_split_014.html#ch04).
    In fact, if you’re already familiar with linear regression, you may be wondering
    why I’ve waited until now to cover linear regression, when the theory of logistic
    regression is built on it. It’s because to make your learning more simple and
    enjoyable, I wanted to cover classification, regression, dimension reduction,
    and clustering separately, so each of these topics is distinct in your mind. But
    I hope the theory we’ll cover in this next part will solidify your understanding
    of logistic regression.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍线性回归开始——正如你将要学习的，它与我们在[第4章](kindle_split_014.html#ch04)中使用的逻辑回归密切相关。事实上，如果你已经熟悉线性回归，你可能想知道为什么我等到现在才介绍线性回归，因为逻辑回归的理论建立在它之上。这是因为为了让你的学习更加简单和愉快，我想分别介绍分类、回归、降维和聚类，这样每个主题在你的脑海中都是独立的。但我希望我们在下一部分将要涵盖的理论将巩固你对逻辑回归的理解。
- en: Chapter 9\. Linear regression
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章\. 线性回归
- en: '*This chapter covers*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Working with linear regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用线性回归
- en: Performance metrics for regression tasks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归任务的性能指标
- en: Using machine learning algorithms to impute missing values
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习算法来填充缺失值
- en: Performing feature selection algorithmically
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法性地执行特征选择
- en: Combining preprocessing wrappers in mlr
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在mlr中组合预处理包装器
- en: Our first stop in [part 3](kindle_split_019.html#part03), “Regression,” brings
    us to *linear regression*. A classical and commonly used statistical method, linear
    regression builds predictive models by estimating the strength of the relationship
    between our predictor variables and our outcome variable. Linear regression is
    so named because it assumes the relationships between the predictor variables
    with the outcome variable are linear. Linear regression can handle both continuous
    and categorical predictor variables, and I’ll show you how in this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三部分](kindle_split_019.html#part03)“回归”的第一站，我们将来到*线性回归*。这是一种经典且常用的统计方法，通过估计预测变量与结果变量之间关系的强度来构建预测模型。线性回归之所以得名，是因为它假设预测变量与结果变量之间的关系是线性的。线性回归可以处理连续和分类预测变量，我将在本章中向你展示。
- en: By the end of this chapter, I hope you’ll understand a general approach to regression
    problems with mlr, and how this differs from classification. In particular, you’ll
    understand the different performance metrics we use for regression tasks, because
    mean misclassification error (MMCE) is no longer meaningful. I’ll also show you,
    as I promised in [chapter 4](kindle_split_014.html#ch04), more sophisticated approaches
    to missing value imputation and feature selection. Finally, I’ll cover how to
    combine as many preprocessing steps as we like using sequential wrappers, so we
    can include them in our cross-validation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望你能理解使用mlr解决回归问题的通用方法，以及这与分类有何不同。特别是，你将了解我们用于回归任务的不同性能指标，因为平均误分类误差（MMCE）不再有意义。我还会像在第4章[承诺的那样](kindle_split_014.html#ch04)，展示更复杂的缺失值插补和特征选择方法。最后，我将介绍如何使用序列包装器结合尽可能多的预处理步骤，这样我们就可以将它们包含在我们的交叉验证中。
- en: 9.1\. What is linear regression?
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1. 什么是线性回归？
- en: In this section, you’ll learn what linear regression is and how it uses the
    equation of a straight line to make predictions. Imagine that you want to predict
    the pH of batches of cider, based on the amount of apple content in each batch
    (in kilograms). An example of what this relationship might look like is shown
    in [figure 9.1](#ch09fig01).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习线性回归是什么以及它是如何使用直线的方程来进行预测的。想象一下，你想要根据每批苹果汁中苹果含量的数量（以千克为单位）来预测这些批次的pH值。这种关系可能的样子在[图9.1](#ch09fig01)中有展示。
- en: Figure 9.1\. Imaginary data of how the pH of cider batches changes with apple
    content
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1. 苹果含量与苹果汁批次pH值变化的假设数据
- en: '![](fig9-1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图9-1](fig9-1.jpg)'
- en: '|  |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from high school chemistry that the lower the pH, the more acidic a substance
    is.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下高中化学，pH值越低，物质越酸。
- en: '|  |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'The relationship between apple weight and cider pH appears linear, and we could
    model this relationship using a straight line. Recall from [chapter 1](kindle_split_010.html#ch01)
    that the only parameters needed to describe a straight line are the slope and
    intercept:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果重量与苹果汁pH值之间的关系看起来是线性的，我们可以用直线来模拟这种关系。回想一下[第1章](kindle_split_010.html#ch01)，描述一条直线所需的唯一参数是斜率和截距：
- en: '*y* = intercept + slope × *x*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = intercept + slope × *x*'
- en: '*y* is the outcome variable, *x* is the predictor variable, the intercept is
    the value of *y* when *x* is zero (where the line crosses the y-axis), and the
    slope is how much *y* changes when *x* increases by one unit.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* 是结果变量，*x* 是预测变量，截距是当 *x* 为零时 *y* 的值（即直线与y轴的交点），斜率是当 *x* 增加一个单位时 *y* 的变化量。'
- en: '|  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Interpreting the slope is useful because it tells us about how the outcome variable
    changes with the predictor(s), but interpreting the intercept is usually not so
    straightforward (or useful). For example, a model that predicts a spring’s tension
    from its length might have a positive intercept, suggesting that a spring of zero
    length has tension! If all the variables are centered to have a mean of zero,
    then the intercept can be interpreted as the value of *y* at the mean of *x* (which
    is often more useful information). Centering your variables like this doesn’t
    affect the slopes because the relationships between variables remain the same.
    Therefore, predictions made by linear regression models are unaffected by centering
    and scaling your data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 解释斜率是有用的，因为它告诉我们结果变量如何随着预测变量（们）的变化而变化，但解释截距通常并不那么直接（或不那么有用）。例如，一个预测弹簧张力的模型，其长度为零时可能有一个正的截距，这表明长度为零的弹簧有张力！如果所有变量都中心化，使得均值为零，那么截距可以解释为
    *x* 均值处的 *y* 值（这通常是更有用的信息）。以这种方式中心化变量不会影响斜率，因为变量之间的关系保持不变。因此，线性回归模型做出的预测不受数据中心化和缩放的影响。
- en: '|  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'If you were to read this out loud in plain English, you would say: “For any
    particular case, the value of the outcome variable, *y*, is the model intercept,
    plus the value of the predictor variable, *x*, times its slope.”'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你用普通英语大声读出来，你会说：“对于任何特定的情况，结果变量 *y* 的值是模型截距，加上预测变量 *x* 的值乘以其斜率。”
- en: Statisticians write this equation as
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家将这个方程写作
- en: '*y* = β[0] + β[1]*x*[1] + ϵ'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + β[1]*x*[1] + ϵ'
- en: where β[0] is the intercept, β[1] is the slope for variable *x*[1], and ϵ is
    the unobserved error unaccounted for by the model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 β[0] 是截距，β[1] 是变量 *x*[1] 的斜率，ϵ 是模型未观察到的、无法解释的误差。
- en: '|  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The parameters (also called *coefficients*) of a linear regression model are
    only estimates of the true values. This is because we are typically only working
    with a finite sample from the wider population. The only way to derive the true
    parameter values would be to measure the entire population, something that is
    usually impossible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型的参数（也称为*系数*）只是真实值的估计。这是因为我们通常只处理来自更广泛人群的有限样本。推导出真实参数值的方法只能是测量整个群体，而这通常是不可能的。
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: So to learn a model that can predict pH from apple weight, we need a way to
    estimate the intercept and slope of a straight line that best represents this
    relationship.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了学习一个可以预测pH值的模型，我们需要一种方法来估计最能代表这种关系的直线截距和斜率。
- en: Linear regression isn’t technically an algorithm. Rather, it’s the approach
    to modeling relationships using the straight-line equation. We could use a few
    different algorithms to estimate the intercept and slope of a straight line. For
    simple situations like our cider pH problem, the most common algorithm is *ordinary
    least squares* (OLS).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归在技术上不是一个算法。相反，它是使用直线方程建模关系的途径。我们可以使用几种不同的算法来估计直线的截距和斜率。对于像我们的苹果汁pH问题这样的简单情况，最常用的算法是*普通最小二乘法*（OLS）。
- en: 'The job of OLS is to learn the combination of values for the intercept and
    slope that minimizes the *residual sum of squares*. We came across the concept
    of a residual in [chapter 7](kindle_split_017.html#ch07) as the amount of information
    left unexplained by a model. In linear regression, we can visualize this as the
    vertical distance (along the y-axis) between a case and the straight line. But
    OLS doesn’t just consider the raw distances between each case and the line: it
    squares them first and then adds them all up (hence, *sum* of *squares*). This
    is illustrated for our cider example in [figure 9.2](#ch09fig02).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: OLS的职责是学习截距和斜率的值组合，以最小化*残差平方和*。我们在[第7章](kindle_split_017.html#ch07)中遇到了残差的概念，即模型未解释的信息量。在线性回归中，我们可以将此视为案例与直线之间的垂直距离（沿y轴）。但OLS不仅考虑每个案例与线之间的原始距离：它首先将它们平方，然后将它们全部加起来（因此，*平方和*）。这在我们苹果汁示例的[图9.2](#ch09fig02)中得到了说明。
- en: Figure 9.2\. Finding the least squares line through the data. Residuals are
    the vertical distances between the cases and the line. The area of the boxes represents
    the squared residuals for three of the cases. The intercept (β[*0*]) is where
    the line hits the y-axis when *x* = 0\. The slope is the change in *y* (Δ*y*)
    divided by the change in *x* (Δ*x*).
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2\. 通过数据找到最小二乘线。残差是案例与线之间的垂直距离。方框的面积代表三个案例的平方残差。截距（β[*0*]）是当*x* = 0时线与y轴相交的点。斜率是*y*（Δ*y*）的变化除以*x*（Δ*x*）的变化。
- en: '![](fig9-2.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig9-2.jpg)'
- en: Why does OLS square the distances? You may read that this is because it makes
    any negative residuals (for cases that lie below the line) positive, so they contribute
    to the sum of squares rather than subtract from it. This is certainly a handy
    by-product of squaring, but if that was true, we would simply use |*residual*|
    to denote the *absolute* value (removing the negative sign). We use the squared
    residuals so that we disproportionately penalize cases that are far away from
    their predicted value.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么OLS要平方距离？你可能读到这是因为它使得任何负残差（对于位于线下的案例）变为正数，因此它们对平方和的贡献而不是减去它。这当然是一个方便的副产品，但如果这是真的，我们就会简单地使用|*残差*|来表示*绝对值*（去除负号）。我们使用平方残差是为了不成比例地惩罚远离其预测值的案例。
- en: 9.1.1\. What if we have multiple predictors?
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1\. 如果我们有多个预测变量怎么办？
- en: 'OLS finds the combination of slope and intercept that minimizes the sum of
    squares, and the line learned in this way will be the one that best fits the data.
    But regression problems are rarely as simplistic as trying to predict an outcome
    with a single predictor; what about when we have multiple predictor variables?
    Let’s add another variable to our cider pH problem: fermentation time (see [figure
    9.3](#ch09fig03)).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法（OLS）找到斜率和截距的组合，使得平方和最小化，通过这种方式学习到的线将是最适合数据的线。但回归问题很少像尝试用一个预测变量预测一个结果那样简单；当我们有多个预测变量时怎么办？让我们给我们的苹果汁pH问题添加另一个变量：发酵时间（见[图9.3](#ch09fig03)）。
- en: 'Figure 9.3\. Adding an additional variable: the size of each dot corresponds
    to the fermentation time of each cider batch.'
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3\. 添加一个额外的变量：每个点的尺寸对应于每个苹果汁批次的发酵时间。
- en: '![](fig9-3.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig9-3.jpg)'
- en: 'When we have multiple predictors, a slope is estimated for each (using OLS),
    and the contributions of each variable are added together linearly, along with
    the model intercept (which is now the value of *y* when each predictor equals
    zero). The slopes in linear regression tell us how the outcome variable changes
    for a one-unit increase in each predictor *while holding all other predictors
    constant*. In other words, the slopes tell us how the outcome changes when we
    change the predictor variables, one at a time. For example, our two-predictor
    cider model would look like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有多个预测变量时，为每个变量估计一个斜率（使用OLS），并将每个变量的贡献线性相加，同时加上模型截距（现在每个预测变量等于零时*y*的值）。线性回归中的斜率告诉我们，在保持所有其他预测变量不变的情况下，每个预测变量增加一个单位时，因变量如何变化。换句话说，斜率告诉我们当我们逐个改变预测变量时，因变量如何变化。例如，我们的两个预测变量苹果酒模型看起来会是这样：
- en: '*y* = β[0] + β*[apples]* × *apples* + β*[fermentation]* × *fermentation* +
    ϵ'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + β*[apples]* × *apples* + β*[fermentation]* × *fermentation* +
    ϵ'
- en: '|  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: You will sometimes see linear regression with a single predictor and regression
    with multiple predictors described as *simple linear regression* and *multiple
    regression*, respectively. I find this distinction a little unnecessary, however,
    because we rarely work with only a single predictor.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你有时会看到单变量线性回归和多变量回归被描述为*简单线性回归*和*多元回归*，分别。然而，我认为这种区分有点不必要，因为我们很少只处理单个预测变量。
- en: '|  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When we have two predictors, our line becomes a surface/plane. You can see this
    illustrated for our cider example in [figure 9.4](#ch09fig04). When we have more
    than two predictors, our plane becomes a hyperplane. Indeed, our straight-line
    equation can be generalized to any number of predictors
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有两个预测变量时，我们的线就变成了一个表面/平面。你可以在[图9.4](#ch09fig04)中看到我们苹果酒示例的说明。当我们有超过两个预测变量时，我们的平面就变成了超平面。实际上，我们的直线方程可以推广到任何数量的预测变量
- en: '*y* = β[0] + β[1]*x*[1] + β[2]*x*[2] ... β[k]*x*[k] + ϵ'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + β[1]*x*[1] + β[2]*x*[2] ... β[k]*x*[k] + ϵ'
- en: Figure 9.4\. Representing a linear model with two predictors. Combining apple
    content and fermentation time in our linear model can be represented as a surface.
    The solid lines show the residual error for each case (its vertical distance from
    the surface).
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4\. 用两个预测变量表示线性模型。在我们的线性模型中将苹果含量和发酵时间结合可以表示为一个表面。实线显示了每个案例的残差误差（其垂直距离从表面）。
- en: '![](fig9-4_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](fig9-4_alt.jpg)'
- en: where there are *k* predictors in the model. This is called the *general linear
    model*, and it is the central equation of all linear models. If you’re coming
    from a traditional statistical modeling background, you may be familiar with *t*
    tests and analysis of variance. These approaches all use the general linear model
    to represent the relationships between the predictor variables and the outcome.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中模型中有 *k* 个预测变量。这被称为*一般线性模型*，它是所有线性模型的核心方程。如果你来自传统的统计建模背景，你可能熟悉*t*检验和方差分析。这些方法都使用一般线性模型来表示预测变量和因变量之间的关系。
- en: '|  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: The general linear model is not quite the same as the *generalized linear model*,
    which refers to a class of models that allow different distributions for the outcome
    variable. I’ll talk about the generalized linear model soon.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一般线性模型并不完全等同于广义线性模型，后者指的是一类允许因变量具有不同分布的模型。我很快就会谈到广义线性模型。
- en: '|  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Do you recognize the general linear model? You saw something similar to it when
    we covered logistic regression in [chapter 4](kindle_split_014.html#ch04). In
    fact, everything on the right side of the equation is identical. The only difference
    is what was on the left side of the equals sign. Recall that in logistic regression,
    we predict the log odds of a case belonging to a particular class. In linear regression,
    we simply predict the case’s value of the outcome variable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你能认出一般线性模型吗？在我们讲解逻辑回归的[第4章](kindle_split_014.html#ch04)时，你曾见过与之类似的东西。实际上，方程右侧的所有内容都是相同的。唯一的区别在于等号左侧的内容。回想一下，在逻辑回归中，我们预测一个案例属于特定类别的对数几率。而在线性回归中，我们只是预测案例的因变量值。
- en: '|  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**When interpretability is as or more important than performance**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**当可解释性与其性能同样重要或更重要时**'
- en: While another regression algorithm may perform better for a particular task,
    models formulated using the general linear model are often favored for how interpretable
    they are. The slopes tell you how much the outcome variable changes with a one-unit
    increase of each predictor variable, holding all other variables constant.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然另一个回归算法可能在特定任务上表现更好，但使用一般线性模型构建的模型通常因其可解释性而受到青睐。斜率告诉你，在保持所有其他变量不变的情况下，结果变量随着每个预测变量单位增加而变化的程度。
- en: There are other algorithms that may learn models that perform better on a particular
    task but aren’t as interpretable. Such models are often described as being black
    boxes, where the model takes input and gives output, but it’s not easy to see
    and/or interpret the rules inside the model that led to that particular output.
    Random forest, XGBoost, and SVMs are examples of black-box models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他算法可能学习到在特定任务上表现更好的模型，但可解释性较差。这样的模型通常被描述为黑盒，其中模型接受输入并给出输出，但很难看到和/或解释导致该特定输出的模型内部的规则。随机森林、XGBoost和SVMs是黑盒模型的例子。
- en: So when would we prefer an interpretable model (such as a linear regression
    model), over a black-box model that performs better? Well, one example is if our
    model has the potential to discriminate. Imagine if a model incorporated bias
    against women during training. It might be difficult to detect this immediately
    using a black-box model, whereas if we can interpret the rules, we can check for
    such biases. A similar consideration is safety, where it’s imperative to ensure
    that our model doesn’t give potentially dangerous outcomes (such as unnecessary
    medical intervention).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们什么时候会倾向于选择一个可解释的模型（例如线性回归模型），而不是表现更好的黑盒模型呢？嗯，一个例子是，如果我们的模型具有区分能力。想象一下，如果模型在训练过程中引入了对女性的偏见。使用黑盒模型可能很难立即检测到这种偏见，而如果我们能解释规则，我们就可以检查这种偏见。类似的考虑还有安全性，确保我们的模型不会给出可能危险的结果（例如不必要的医疗干预）是至关重要的。
- en: Another example is when we are using machine learning to better understand a
    system or nature. Getting predictions from a model might be useful, but understanding
    those rules to deepen our understanding and stimulate further research may be
    of more importance. Black boxes can make this difficult.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，当我们使用机器学习来更好地理解一个系统或自然时。从模型中获得预测可能是有用的，但理解这些规则以深化我们的理解和刺激进一步的研究可能更为重要。黑盒可能会使这变得困难。
- en: Finally, understanding the rules of our model allows us to make changes in the
    way we do things. Imagine that a business uses a linear regression model to predict
    demand for a particular product, based on things like its cost and how much the
    company spends on advertising. Not only can the company predict future demand,
    but it also can control it, by interpreting the rules of how the predictor variables
    impact the outcome.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，理解我们模型的规则允许我们改变做事的方式。想象一下，一家企业使用线性回归模型来预测特定产品的需求，基于其成本和公司在广告上的支出等因素。公司不仅能够预测未来的需求，而且还可以通过解释预测变量如何影响结果来控制需求。
- en: '|  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When modeling our data with the general linear model, we make the assumption
    that our residuals are normally distributed and *homoscedastic*. Homoscedastic
    is a ridiculous-sounding word (impress your friends with it) that simply means
    the variance of the outcome variable doesn’t increase as the predicted value of
    the outcome increases.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用一般线性模型来建模我们的数据时，我们假设我们的残差是正态分布的，并且*同方差*。同方差是一个听起来很荒谬的词（用这个词让你的朋友印象深刻），它仅仅意味着结果变量的方差不会随着结果预测值的增加而增加。
- en: '|  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: The opposite of homoscedastic is *heteroscedastic*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同样均方误差的对立面是*异方差*。
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We also make the assumption that there is a linear relationship between each
    predictor variable and the outcome, and that the effects of the predictor variables
    on the response variable are additive (rather than multiplicative).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设每个预测变量和结果之间存在线性关系，以及预测变量对响应变量的影响是可加的（而不是乘法的）。
- en: When these assumptions are valid, our model will make more accurate and unbiased
    predictions. However, the general linear model can be extended to handle situations
    in which the assumption of normally distributed residuals is violated (logistic
    regression is one such example).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些假设有效时，我们的模型将做出更准确和无偏的预测。然而，一般线性模型可以扩展以处理违反正态分布残差假设的情况（逻辑回归就是一个例子）。
- en: '|  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: I’ll show you how we can check the validity of these assumptions when we build
    our own linear regression model later in the chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我会在本章后面构建我们自己的线性回归模型时，向你展示如何检查这些假设的有效性。
- en: '|  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: In situations such as this, we turn to the *generalized linear model*. The generalized
    linear model is the same as the general linear model (in fact, the latter is a
    special case of the former), except that it uses various transformations called
    *link functions* to map the outcome variable to the linear predictions made by
    the right-hand side of the equals sign. For example, count data is rarely normally
    distributed, but by building a generalized model with an appropriate link function,
    we can transform linear predictions made by the model back into counts. I don’t
    intend to talk any further about generalized linear models here, but a good resource
    on this topic (if a little heavy) is *Generalized Linear Models With Examples
    in R* by Peter K. Dunn and Gordon K. Smyth (Springer, 2018).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们转向*广义线性模型*。广义线性模型与一般线性模型相同（实际上，后者是前者的一种特殊情况），只不过它使用各种称为*链接函数*的转换将结果变量映射到等号右侧做出的线性预测。例如，计数数据很少呈正态分布，但通过构建一个具有适当链接函数的广义模型，我们可以将模型做出的线性预测转换回计数。我不打算在这里进一步讨论广义线性模型，但关于这个主题的一个很好的资源（如果有点沉重）是Peter
    K. Dunn和Gordon K. Smyth合著的《带有R示例的广义线性模型》（Springer，2018年）。
- en: '|  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the residuals are heteroscedastic, it sometimes helps to build a model that
    predicts some transformation of the outcome variable instead. For example, predicting
    the log[10] of the response variable is a common choice. Predictions made by such
    a model can then be transformed back onto the original scale for interpretation.
    When the effect of multiple predictors on the outcome is not additive, we can
    add *interaction* terms to our model that state the effect of one predictor variable
    has on the outcome when the other predictor variable changes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果残差是异方差性的，有时构建一个预测结果变量某些转换的模型会有所帮助。例如，预测响应变量的对数[10]是一个常见的选择。这样的模型做出的预测可以转换回原始尺度进行解释。当多个预测变量对结果的影响不是加性的，我们可以在模型中添加*交互项*，说明当一个预测变量变化时，另一个预测变量对结果的影响。
- en: '|  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 9.1.2\. What if our predictors are categorical?
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2\. 如果我们的预测变量是分类的怎么办？
- en: So far, we’ve only considered the situation where our predictors are continuous.
    Because the general linear model is essentially the equation of a straight line,
    and we use it to find the slopes between variables, how can we find the slope
    of a categorical variable? Does this even make sense? Well, it turns out we can
    cheat by recoding categorical variables into *dummy variables*. Dummy variables
    are new representations of categorical variables that map the categories to 0
    and 1.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了我们的预测变量是连续的情况。因为一般线性模型本质上是一条直线的方程，我们用它来寻找变量之间的斜率，那么我们如何找到一个分类变量的斜率呢？这甚至有意义吗？好吧，结果是我们可以通过将分类变量重新编码为*虚拟变量*来作弊。虚拟变量是分类变量的新表示，将类别映射到0和1。
- en: 'Imagine that we want to predict the acidity of cider batches based on the type
    of apple: Gala or Braeburn. We want to find the intercept and slope that describes
    the relationship between these two apple types and acidity, but how do we do that?
    Remember earlier that the slope is how much *y* increases when *x* increases by
    one unit. If we recode our apple type variable such that Gala = 0 and Braeburn
    = 1, we can treat apple type as a continuous variable and find how much acidity
    changes as we go from 0 to 1\. Take a look at [figure 9.5](#ch09fig05): the intercept
    is the value of *y* when *x* is 0, which is the mean acidity when apple type =
    Gala. Gala is therefore said to be our *reference level*. The slope is the change
    in *y* with a one-unit increase in *x*, which is the difference between the mean
    acidity for Gala and the mean acidity with Braeburn. This may feel like cheating,
    but it works, and the slope with the least squares will be the one that connects
    the means of the categories.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要根据苹果类型（Gala或Braeburn）预测苹果酒的酸度。我们想要找到描述这两种苹果类型和酸度之间关系的截距和斜率，但我们如何做到这一点？记住，早些时候斜率是当*x*增加一个单位时*y*增加的量。如果我们重新编码我们的苹果类型变量，使得Gala
    = 0和Braeburn = 1，我们可以将苹果类型视为连续变量，并找出从0到1时酸度如何变化。查看[图9.5](#ch09fig05)：截距是*x*为0时*y*的值，即苹果类型
    = Gala时的平均酸度。因此，Gala被认为是我们的参考水平。斜率是*x*增加一个单位时*y*的变化，这是Gala的平均酸度与Braeburn的平均酸度之间的差异。这可能感觉像是在作弊，但它有效，并且最小二乘法中的斜率将是连接类别均值的斜率。
- en: '|  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Which category you choose as the reference level makes no difference to the
    predictions made by a model and is the first level of the factor (the first alphabetically
    by default).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择哪个类别作为参考水平对模型做出的预测没有影响，并且它是因子的第一级（默认情况下按字母顺序排列）。
- en: '|  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 9.5\. Finding the slope between two levels of a categorical variable
    using a dummy variable. The apple types are recoded as 0 and 1 and treated as
    a continuous variable. The slope now represents the difference in means between
    the two apple types, and the intercept represents the mean of the reference category
    (Gala).
  id: totrans-93
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5。使用虚拟变量在分类变量的两个水平之间找到斜率。苹果类型被重新编码为0和1，并被视为连续变量。现在的斜率代表两种苹果类型之间的均值差异，截距代表参考类别（Gala）的均值。
- en: '![](fig9-5.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图9-5](fig9-5.jpg)'
- en: Recoding dichotomous (two-level) factors into a single dummy variable with values
    of 0 and 1 makes sense, but what if we have a *polytomous* factor (a factor with
    more than two levels)? Do we code them as 1, 2, 3, 4, and so on, and treat this
    as a single continuous predictor? Well, this wouldn’t work because it’s unlikely
    that a single straight line would connect the means of the categories. Instead,
    we create *k* – 1 dummy variables, where *k* is the number of levels of the factor.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将二进制（两级）因子重新编码为单个虚拟变量，其值为0和1是有意义的，但如果我们有一个多级因子（具有超过两个级别的因子）怎么办？我们将它们编码为1、2、3、4等等，并将它们视为单个连续预测变量？嗯，这不会起作用，因为不太可能有一条直线能连接各个类别的均值。相反，我们创建*
    k* - 1个虚拟变量，其中* k* 是因子的级别数。
- en: 'Take a look at the example in [figure 9.6](#ch09fig06). We have four types
    of apples (Granny Smith is my favorite) and would like to predict pH based on
    the apple type used to make a particular batch of cider. To convert our four-level
    factor into dummy variables, we do the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图9.6中的示例。[图9.6](#ch09fig06)。我们有四种苹果类型（Granny Smith是我最喜欢的），并希望根据用于制作特定一批苹果酒的苹果类型来预测pH值。为了将我们的四级因子转换为虚拟变量，我们执行以下操作：
- en: Create a table of three columns, where each column represents a dummy variable.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个三列的表格，其中每一列代表一个虚拟变量。
- en: Choose a reference level (Gala, in this case).
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个参考水平（在本例中为Gala）。
- en: Set the value of each dummy variable to 0 for the reference level.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个虚拟变量的值设为0以表示参考水平。
- en: Set the value of each dummy variable to 1 for a particular factor level.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个虚拟变量的值设为1以表示特定的因子水平。
- en: Figure 9.6\. Recoding a polytomous categorical variable into *k* – 1 dummy variables.
    A four-level factor can be represented using three (*k* – 1) dummy variables.
    The reference level (Gala) has a value of 0 for each dummy variable. The other
    levels have a value of 1 for a particular dummy variable. A slope is estimated
    for each dummy.
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6。将多级分类变量重新编码为*k* - 1个虚拟变量。一个四级因子可以用三个(* k* - 1)虚拟变量表示。参考水平（Gala）在每个虚拟变量中的值为0。其他水平在特定虚拟变量中的值为1。为每个虚拟变量估计一个斜率。
- en: '![](fig9-6.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图9-6](fig9-6.jpg)'
- en: We’ve now turned our single variable of four levels into three distinct dummy
    variables that each take a value of 1 or 0. But how does this help us? Well, each
    dummy variable acts as a flag in the model formula to denote which level a particular
    case belongs to. The full model as shown in [figure 9.6](#ch09fig06) is
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将我们的四个级别的单一变量转换成了三个不同的虚拟变量，每个虚拟变量取值为1或0。但这如何帮助我们呢？嗯，每个虚拟变量在模型公式中充当一个标志，表示特定案例属于哪个级别。如图9.6所示，完整模型如下
- en: '*y* = β[0] + β[*d*1] *d*1 + β[*d*2] *d*2 + β[*d*3] *d*3 + ϵ'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + β[*d*1] *d*1 + β[*d*2] *d*2 + β[*d*3] *d*3 + ϵ'
- en: 'Now, because the intercept (β[0]) represents acidity when all predictors are
    equal to 0, this is now the mean of the reference level, Gala. The slopes in the
    model β[*d*1], β[*d*2], and so on) represent the difference between the mean of
    the reference level and the means of each of the other levels. If a batch of cider
    was made with a particular type of apple, its dummy variables will “switch on”
    the slope between that type of apple and the reference class, and “switch off”
    the others. For example, let’s say a particular batch was made with Braeburn apples.
    The model would look like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，因为截距（β[0]）代表当所有预测变量都等于0时的酸度，所以这现在是参考水平Gala的平均值。模型中的斜率（β[*d*1]，β[*d*2]，等等）代表参考水平与其他每个水平平均值的差异。如果一批苹果酒是用某种类型的苹果制作的，其虚拟变量将“开启”该类型苹果与参考类别之间的斜率，并“关闭”其他斜率。例如，假设一批苹果酒是用
    Braeburn 苹果制作的。模型将如下所示：
- en: '*y* = β[0] + β[*d*1] × 1 + β[*d*2] × 0 + β[*d*3] × 0 + ϵ'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + β[*d*1] × 1 + β[*d*2] × 0 + β[*d*3] × 0 + ϵ'
- en: The slopes of the other apple types are still in the model, but because their
    dummy variables are set to 0, they make no contribution to the predicted value!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其他苹果类型的斜率仍然在模型中，但由于它们的虚拟变量被设置为0，它们对预测值没有贡献！
- en: 'Models we build using the general linear model can mix both continuous and
    categorical predictors together. When we use our model to make predictions on
    new data, we simply do the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用广义线性模型建立的模型可以混合连续和分类预测变量。当我们使用我们的模型对新数据进行预测时，我们只需做以下几步：
- en: Take the values of each of the predictor variables for that data.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取该数据中每个预测变量的值。
- en: Multiply these values with the relevant slopes learned by the model.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些值乘以模型学习到的相关斜率。
- en: Add these values together.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些值相加。
- en: Add the intercept.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加截距。
- en: The result is our predicted value for that data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结果就是该数据的预测值。
- en: I hope by now you have a basic understanding of linear regression, so let’s
    turn this knowledge into skills by building your first linear regression model!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望到现在你已经对线性回归有了基本的理解，那么让我们通过建立你的第一个线性回归模型来将这一知识转化为技能吧！
- en: 9.2\. Building your first linear regression model
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 建立你的第一个线性回归模型
- en: In this section, I’ll teach you how to build, evaluate, and interpret a linear
    regression model to predict daily air pollution. I’ll also show other ways of
    imputing missing data and selecting relevant features, and how to bundle as many
    preprocessing steps into your cross-validation as you like.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将教你如何建立、评估和解释一个线性回归模型来预测每日空气污染。我还会展示其他填充缺失数据和选择相关特征的方法，以及如何将尽可能多的预处理步骤捆绑到你的交叉验证中。
- en: 'Imagine that you’re an environmental scientist interested in predicting daily
    levels of atmospheric ozone pollution in Los Angeles. Recall from high school
    chemistry that ozone is an *allotrope* (a fancy way of saying “another form”)
    of oxygen molecule that has three oxygen atoms instead of two (as in the dioxygen
    that you’re breathing right now). While ozone in the stratosphere protects us
    from the sun’s UV rays, products from burning fossil fuels can be converted into
    ozone at ground level, where it is toxic. Your job is to build a regression model
    that can predict ozone pollution levels based on the time of year and meteorological
    readings, such as humidity and temperature. Let’s start by loading the mlr and
    tidyverse packages:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一名对预测洛杉矶每日大气臭氧污染水平感兴趣的环境科学家。回想一下高中化学，臭氧是氧分子的一种同素异形体（一种说法是“另一种形式”），它有三个氧原子而不是两个（就像你现在呼吸的二氧化氧那样）。虽然平流层中的臭氧可以保护我们免受太阳紫外线的伤害，但燃烧化石燃料的产物可以在地面转化为臭氧，那里它是有毒的。你的任务是建立一个回归模型，可以根据年份和气象读数（如湿度和温度）预测臭氧污染水平。让我们先加载mlr和tidyverse包：
- en: '[PRE0]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 9.2.1\. Loading and exploring the Ozone dataset
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1. 加载和探索臭氧数据集
- en: Now let’s load the data, which is built into the mlbench package (I like the
    data examples in this package), convert it into a tibble (with `as_tibble()`),
    and explore it. We’re also going to give more readable names to the variables.
    We have a tibble containing 366 cases and 13 variables of daily meteorological
    and ozone readings.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来加载数据，这些数据内置在 mlbench 包中（我喜欢这个包中的数据示例），将其转换为 tibble（使用 `as_tibble()`），并对其进行探索。我们还将给变量起更易读的名字。我们有一个包含
    366 个案例和 13 个变量的 tibble，这些变量是每日气象和臭氧读数。
- en: Listing 9.1\. Loading and exploring the `Ozone` dataset
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1\. 加载和探索 `Ozone` 数据集
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At present, the `Month`, `Day`, and `Date` variables are factors. Arguably this
    may make sense, but we’re going to treat them as numerics for this exercise. To
    do this, we use the handy `mutate_all()` function, which takes the data as the
    first argument and a transformation/function as the second argument. Here, we
    use `as.numeric` to convert all the variables into the numeric class.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`Month`、`Day` 和 `Date` 变量是因子。可以说这可能有意义，但在这个练习中我们将它们视为数值。为此，我们使用方便的 `mutate_all()`
    函数，它将数据作为第一个参数，将转换/函数作为第二个参数。在这里，我们使用 `as.numeric` 将所有变量转换为数值类别。
- en: '|  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The `mutate_all()` function doesn’t alter the names of the variables, it just
    transforms them in place.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`mutate_all()` 函数不会改变变量的名称，它只是就地转换它们。'
- en: '|  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Next, we have some missing data in this dataset (use `map_dbl(ozoneTib, ~sum(is
    .na(.)))` to see how many). Missing data is okay in our predictor variables (we’ll
    deal with this later using imputation), but missing data for the variable we’re
    trying to predict is not okay. Therefore, we remove the cases without any ozone
    measurement by piping the result of the `mutate_all()` call into the `filter()`
    function, where we remove cases with an `NA` value for `Ozone`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这个数据集中有一些缺失数据（使用 `map_dbl(ozoneTib, ~sum(is .na(.)))` 来查看有多少）。在我们的预测变量中，缺失数据是可以接受的（我们稍后会使用插补来处理这个问题），但我们不能接受我们试图预测的变量的缺失数据。因此，我们通过将
    `mutate_all()` 调用的结果通过管道传递到 `filter()` 函数中，移除没有臭氧测量的案例，从而移除具有 `NA` 值的 `Ozone`
    的案例。
- en: Listing 9.2\. Cleaning the data
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2\. 清洗数据
- en: '[PRE2]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Could we have imputed missing data in our target variable? Yes we could, but
    this has the potential to introduce bias into our model. This is because we’ll
    be training a model to predict values that were themselves generated by a model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否在我们的目标变量中插补缺失数据？是的，我们可以，但这可能会将偏差引入我们的模型。这是因为我们将训练一个模型来预测由模型本身生成的值。
- en: '|  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s plot each of our predictor variables against `Ozone` to get an idea of
    the relationships in the data. We start with our usual trick of gathering the
    variables with the `gather()` function so we can plot them on separate facets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制每个预测变量与 `Ozone` 的关系图，以了解数据中的关系。我们首先使用 `gather()` 函数收集变量，这样我们就可以在单独的面板上绘制它们。
- en: Listing 9.3\. Plotting the data
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.3\. 绘制数据
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember we have to use `-Ozone` to prevent the `Ozone` variable from being
    gathered with the others.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们必须使用 `-Ozone` 来防止 `Ozone` 变量与其他变量一起收集。
- en: '|  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In our `ggplot()` call, we facet by `Variable` and allow the x-axes of the facets
    to vary by setting the `scale` argument equal to `"free_x"`. Then, along with
    a `geom_point` layer, we add two `geom_smooth` layers. The first `geom_smooth`
    is given no arguments and so uses the default settings. By default, `geom_smooth`
    will draw a LOESS curve to the data (a curvy, local regression line) if there
    are fewer than 1,000 cases, or a GAM curve if there are 1,000 or more cases. Either
    will give us an idea of the shape of the relationships. The second `geom_smooth`
    layer specifically asks for the `lm` method (linear model), which draws a linear
    regression line that best fits the data. Drawing both of these will help us identify
    if there are relationships in the data that are nonlinear.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 `ggplot()` 调用中，我们按 `Variable` 分面，并通过将 `scale` 参数设置为 `"free_x"` 允许面元的 x
    轴根据变量变化。然后，除了一个 `geom_point` 层，我们还添加了两个 `geom_smooth` 层。第一个 `geom_smooth` 没有给出任何参数，因此使用默认设置。默认情况下，如果案例少于
    1,000 个，`geom_smooth` 将在数据上绘制 LOESS 曲线（一条曲线，局部回归线），如果案例有 1,000 个或更多，则绘制 GAM 曲线。这两种方法都会给我们一个关于关系形状的线索。第二个
    `geom_smooth` 层专门要求 `lm` 方法（线性模型），它绘制最佳拟合数据的线性回归线。绘制这两个层将帮助我们识别数据中是否存在非线性关系。
- en: The resulting plot is shown in [figure 9.7](#ch09fig07). Hmm, some of the predictors
    have a linear relationship with ozone levels, some have a nonlinear relationship,
    and some seem to have no relationship at all!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图示在[图9.7](#ch09fig07)中。嗯，一些预测变量与臭氧水平呈线性关系，一些呈非线性关系，还有一些似乎完全没有关系！
- en: Figure 9.7\. Plotting each predictor variable in the `Ozone` dataset against
    the `Ozone` variable. The straight lines represent linear regression lines, and
    the curved lines represent GAM lines.
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7。在`Ozone`数据集中，将每个预测变量与`Ozone`变量进行绘图。直线代表线性回归线，曲线代表GAM线。
- en: '![](fig9-7_alt.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图9-7](fig9-7_alt.jpg)'
- en: 9.2.2\. Imputing missing values
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2. 插补缺失值
- en: Linear regression can’t handle missing values. Therefore, to avoid having to
    throw away a large portion of our dataset, we’re going to use imputation to fill
    in the gaps. In [chapter 4](kindle_split_014.html#ch04), we used mean imputation
    to replace missing values (`NA`s) with the mean of the variable. While this may
    work, it only uses the information within that single variable to predict missing
    values, and all missing values within a single variable will take the same value,
    potentially biasing the model. Instead, we can actually use machine learning algorithms
    to predict the value of a missing observation, using all of the other variables
    in the dataset! In this section, I’m going to show you how we can do this with
    mlr.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归无法处理缺失值。因此，为了避免丢弃大量数据集，我们将使用插补来填补空白。在[第4章](kindle_split_014.html#ch04)中，我们使用均值插补用变量的均值替换缺失值（`NA`）。虽然这可能有效，但它只使用了该单个变量内的信息来预测缺失值，并且一个变量中的所有缺失值都将取相同的值，这可能会对模型产生偏差。相反，我们实际上可以使用机器学习算法来预测缺失观察值的值，使用数据集中所有其他变量！在本节中，我将向你展示我们如何使用mlr来完成这项工作。
- en: 'If you run `?imputations`, you’ll be able to see the imputation methods that
    come packaged with mlr. These include methods such as `imputeMean()`, `imputeMedian()`,
    and `imputeMode()` (for replacing missing values with the mean, median, and mode
    of each variable, respectively). But the most important method is the one last
    on the list: `imputeLearner()`. The `imputeLearner()` function lets us specify
    a supervised machine learning algorithm to predict what the missing values would
    have been, based on the information held in all the other variables. For example,
    if we want to impute missing values of a continuous variable, the process proceeds
    as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行 `?imputations`，你将能够看到mlr附带的各种插补方法。这些方法包括例如 `imputeMean()`、`imputeMedian()`
    和 `imputeMode()`（分别用每个变量的均值、中位数和众数来替换缺失值）。但最重要的方法是列表中最后一个：`imputeLearner()`。`imputeLearner()`
    函数允许我们指定一个监督机器学习算法来预测缺失值可能是什么，基于所有其他变量中包含的信息。例如，如果我们想插补连续变量的缺失值，过程如下：
- en: Split the dataset into cases with and without missing values for this particular
    variable.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分为包含和不包含缺失值的特定变量案例。
- en: Decide on a regression algorithm to predict what the missing values would have
    been.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定一个回归算法来预测缺失值可能是什么。
- en: Considering only the cases *without* missing values, use the algorithm to predict
    the values of the variable with missing values, using the other variables in the
    dataset (including the dependent variable you’re trying to predict in your final
    model).
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅考虑没有缺失值的案例，使用算法预测变量缺失值的值，使用数据集中的其他变量（包括你最终模型中试图预测的因变量）。
- en: Considering only the cases *with* missing values, use the model learned in step
    3 to predict the missing values based on the values of the other predictors.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅考虑包含缺失值的案例，使用第3步中学习的模型根据其他预测变量的值来预测缺失值。
- en: We employ the same strategy when imputing categorical variables, except that
    we choose a classification algorithm instead of a regression one. So we end up
    using a supervised learning algorithm to fill in the blanks so that we can use
    another algorithm to train our final model!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在插补分类变量时，我们采用相同的策略，只是我们选择一个分类算法而不是回归算法。因此，我们最终使用一个监督学习算法来填补空白，以便我们可以使用另一个算法来训练我们的最终模型！
- en: So how do we choose an imputation algorithm? There are a few practical considerations,
    but as always it depends somewhat and it may pay off to try different methods
    and see which one gives you the best performance. We can at least initially whittle
    it down to either a classification or regression algorithm, depending on whether
    the variable with missing values is continuous or categorical. Next, whether we
    have missing values in one or multiple variables makes a difference because if
    it’s the latter, we will need to choose an algorithm that can itself handle missing
    values. For example, let’s say we try to use logistic regression to impute missing
    values of a categorical variable. We’ll get to step 3 in the previous procedure
    and stop because the other variables in the data (that the algorithm is trying
    to use to predict the categorical variable) also contain missing values. Logistic
    regression can’t handle that and will throw an error. If the only variable with
    missing values was the one we were trying to impute, this wouldn’t have been a
    problem. Finally, the only other consideration is computational budget. If the
    algorithm you’re using to learn your final model is already computationally expensive,
    using a computationally expensive algorithm to impute your missing values is added
    expense. Within these constraints, it’s often best to experiment with different
    imputation learners and see which one works best for the task at hand.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择一个插补算法呢？有几个实际考虑因素，但就像往常一样，这多少取决于具体情况，尝试不同的方法并看看哪种方法能给你带来最佳性能可能是有益的。我们至少最初可以将其缩小到分类或回归算法，这取决于缺失值的变量是连续的还是分类的。接下来，我们是否在一个或多个变量中存在缺失值是有区别的，因为如果是后者，我们需要选择一个可以自己处理缺失值的算法。例如，假设我们尝试使用逻辑回归来插补一个分类变量的缺失值。我们将到达上一个流程的第3步并停止，因为数据中的其他变量（算法试图用来预测分类变量的）也包含缺失值。逻辑回归无法处理这种情况，并将抛出错误。如果只有缺失值的变量是我们试图插补的，这就不会是问题。最后，唯一其他的考虑因素是计算预算。如果你用来学习最终模型的算法已经计算成本很高，使用一个计算成本高的算法来插补你的缺失值将增加额外的开销。在这些限制条件下，通常最好的做法是尝试不同的插补学习器，看看哪个最适合当前任务。
- en: When doing any form of missing-value imputation, it’s extremely important to
    ensure that the data is either *missing at random* (MAR) or *missing completely
    at random* (MCAR), and not *missing not at random* (MNAR). If data is MCAR, it
    means the likelihood of a missing value is not related to any variable in the
    dataset. If data is MAR, it means the likelihood of a missing value is related
    only to the value of the other variables in the dataset. For example, someone
    might be less likely to fill in their salary on a form because of their age. In
    either of these situations, we can still build models that are unbiased due to
    the presence of missing data. But consider the situation where someone is less
    likely to fill in their salary on a form because their salary is low. This is
    an example of data *missing not at random* (MNAR), where the likelihood of a missing
    value depends on the value of the variable itself. In such a situation, you would
    likely build a model that is biased to overestimate the salaries of the people
    in your survey.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行任何形式的缺失值插补时，确保数据是*随机缺失*（MAR）或*完全随机缺失*（MCAR），而不是*非随机缺失*（MNAR）这一点极为重要。如果数据是MCAR，这意味着缺失值的可能性与数据集中的任何变量无关。如果数据是MAR，这意味着缺失值的可能性仅与数据集中其他变量的值有关。例如，某人可能因为年龄而不太可能填写他们的薪水。在这两种情况下，我们仍然可以构建由于缺失数据的存在而具有无偏性的模型。但考虑这种情况，某人可能因为他们的薪水低而不太可能填写他们的薪水。这是一个数据*非随机缺失*（MNAR）的例子，其中缺失值的可能性取决于变量的值本身。在这种情况下，你可能会构建一个倾向于高估调查中人们薪水的模型。
- en: How do we tell if our data is MCAR, MAR, or MNAR? Not easily. There are methods
    for distinguishing MCAR and MAR. For example, you could build a classification
    model that predicts whether a case has a missing value for a particular variable.
    If the model does better at predicting missing values than a random guess, then
    the data is MAR. If the model can’t do much better than a random guess, then the
    data is probably MCAR. Is there a way to tell whether data is MNAR? Unfortunately
    not. Being confident that your data is not MNAR depends on good experiment design
    and thoughtful examination of your predictor variables.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何判断我们的数据是MCAR、MAR还是MNAR？并不容易。有方法可以区分MCAR和MAR。例如，您可以构建一个分类模型，预测一个案例是否对特定变量有缺失值。如果模型在预测缺失值方面比随机猜测做得更好，那么数据就是MAR。如果模型做得不比随机猜测好多少，那么数据可能就是MCAR。有没有办法判断数据是否是MNAR？很遗憾，没有。确信您的数据不是MNAR取决于良好的实验设计和对预测变量的深思熟虑。
- en: '|  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: There is a more powerful imputation technique called *multiple imputation*.
    The premise of multiple imputation is that you create many new datasets, replacing
    missing data with sensible values in each one. You then train a model on each
    of these imputed datasets and return the average model. While this is probably
    the most widely used imputation technique, sadly, it isn’t implemented yet in
    mlr, so we won’t use it here. However, I strongly suggest you read the documentation
    for the mice package in R.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种更强大的插补技术叫做**多重插补**。多重插补的前提是您创建许多新的数据集，在每个数据集中用合理的值替换缺失数据。然后，您在每个插补数据集上训练一个模型，并返回平均模型。虽然这可能是最广泛使用的插补技术，但遗憾的是，它还没有在mlr中实现，所以我们在这里不会使用它。然而，我强烈建议您阅读R中mice包的文档。
- en: '|  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'For our ozone data, we have missing values across several variables, and they’re
    all continuous variables. Therefore, I’m going to choose a regression algorithm
    that can handle missing data: rpart. Yep, you heard me right: we’re going to impute
    the missing values with the rpart decision tree algorithm. When we covered tree-based
    learners in [chapter 7](kindle_split_017.html#ch07), we only considered them for
    classification problems; but decision trees can be used to predict continuous
    variables, too. I’ll show you how this works in detail in [chapter 12](kindle_split_023.html#ch12);
    but for now, we’ll let rpart do its thing and impute our missing values for us.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的臭氧数据，我们在几个变量上都有缺失值，而且它们都是连续变量。因此，我将选择一个可以处理缺失数据的回归算法：rpart。是的，您没听错：我们将使用rpart决策树算法来插补缺失值。在我们讨论基于树的算法时[第7章](kindle_split_017.html#ch07)，我们只考虑了它们用于分类问题；但决策树也可以用来预测连续变量。我将在[第12章](kindle_split_023.html#ch12)中详细展示这是如何工作的；但现在，我们将让rpart做它的事情，为我们插补缺失值。
- en: Listing 9.4\. Using rpart to impute missing values
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.4\. 使用rpart插补缺失值
- en: '[PRE4]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We first use the `imputeLearner()` function to define what algorithm we’re going
    to use to impute the missing values. The only argument we supply to this function
    is the name of the learner, which in this case is `"regr.rpart"`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用`imputeLearner()`函数来定义我们将要使用什么算法来插补缺失值。我们提供给这个函数的唯一参数是学习者的名称，在这种情况下是`"regr.rpart"`。
- en: '|  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: There is an additional, optional argument, `features`, that lets us specify
    which variables in the dataset to use in the prediction of missing values. The
    default is to use all the other variables, but you can use this to specify variables
    without any missing values, allowing you to use algorithms that can’t themselves
    handle missing data. See `?imputeLearner` for more detail.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个可选的参数，`features`，允许我们指定在预测缺失值时使用数据集中的哪些变量。默认情况下，使用所有其他变量，但您可以使用它来指定没有任何缺失值的变量，这样您就可以使用无法处理缺失数据的算法。有关更多详细信息，请参阅`?imputeLearner`。
- en: '|  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Next, we use the `impute()` function to create the imputed dataset, to which
    the first argument is the data. We’ve wrapped our tibble inside the `as.data.frame()`
    function just to prevent repeated warnings about the data being a tibble and not
    a data frame (these can be safely ignored). We can specify different imputation
    techniques for different columns by supplying a named list to the `cols` argument.
    For example, we could say `cols = list(var1 = imputeMean(), var2 = imputeLearner("regr.lm"))`.
    We can also specify different imputation techniques for different classes of variable
    (one technique for numeric variables, another for factors) using the `classes`
    argument in the same way. In the following listing, we use the `classes` argument
    to impute all the variables (they are all numeric) using the `imputeMethod` we
    defined.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`impute()`函数创建填充后的数据集，其中第一个参数是数据。我们只是将我们的tibble包裹在`as.data.frame()`函数中，以防止重复警告数据是tibble而不是数据框（这些可以安全忽略）。我们可以通过向`cols`参数提供一个命名列表来为不同的列指定不同的填充技术。例如，我们可以说`cols
    = list(var1 = imputeMean(), var2 = imputeLearner("regr.lm"))`。我们还可以通过在相同的`classes`参数中使用相同的方式为不同类别的变量指定不同的填充技术（一个技术用于数值变量，另一个用于因子）。在下面的列表中，我们使用`classes`参数使用我们定义的`imputeMethod`来填充所有变量（它们都是数值变量）。
- en: This results in a dataset we can access using `ozoneImp$data`, whose missing
    values have been replaced with predictions from a model learned by the rpart algorithm.
    Now we can define our task and learner using the imputed dataset. By supplying
    `"regr.lm"` as an argument to the `makeLearner()` function, we’re telling mlr
    that we want to use linear regression.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个我们可以使用`ozoneImp$data`访问的数据集，其中缺失值已被rpart算法学习到的模型预测值所替换。现在我们可以使用填充后的数据集定义我们的任务和学习器。通过将`"regr.lm"`作为`makeLearner()`函数的参数，我们告诉mlr我们想要使用线性回归。
- en: Listing 9.5\. Defining our task and learner
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.5\. 定义我们的任务和学习器
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In [part 2](kindle_split_012.html#part02) of this book, we were used to defining
    learners as `classif .[ALGORITHM]`. In this part of the book, instead of `classif.`,
    the prefix will be `regr.`. This is important because the same algorithm can sometimes
    be used for classification *and* regression, so the prefix tells mlr which task
    we want to use the algorithm for.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二部分（[part 2](kindle_split_012.html#part02)），我们习惯于将学习器定义为`classif .[ALGORITHM]`。在本书的这一部分，而不是`classif.`，前缀将是`regr.`。这很重要，因为同一个算法有时可以用于分类和回归，所以前缀告诉mlr我们想要使用算法执行哪种任务。
- en: '|  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 9.2.3\. Automating feature selection
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.3\. 自动化特征选择
- en: Sometimes it may be obvious which variables have no predictive value and can
    be removed from the analysis. Domain knowledge is also very important here, where
    we include variables in the model that we, as experts, believe to have some predictive
    value for the outcome we’re studying. But it’s often better to take a less subjective
    approach to feature selection, and allow an algorithm to choose the relevant features
    for us. In this section, I’ll show you how we can implement this in mlr.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可能很明显哪些变量没有预测价值，可以从分析中移除。领域知识在这里也非常重要，我们在这里包括我们认为对我们要研究的输出结果有某些预测价值的变量。但通常更好的方法是采取更客观的特征选择方法，并允许算法为我们选择相关特征。在本节中，我将向您展示我们如何在mlr中实现这一点。
- en: 'There are two methods for automating feature selection:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化特征选择有两种方法：
- en: '***Filter methods—*** Filter methods compare each of the predictors against
    the outcome variable, and calculate a metric of how much the outcome varies with
    the predictor. This metric could be a correlation: for example, if both variables
    are continuous. The predictor variables are ranked in order of this metric (which,
    in theory, ranks them in order of how much information they can contribute to
    the model), and we can choose to drop a certain number or proportion of the worst-performing
    variables from our model. The number or proportion of variables we drop can be
    tuned as a hyperparameter during model building.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***过滤方法—*** 过滤方法将每个预测变量与结果变量进行比较，并计算结果变量随预测变量变化的度量。这个度量可以是相关系数：例如，如果两个变量都是连续的。预测变量将按照这个度量排序（理论上，按照它们可以贡献给模型的信息量排序），我们可以选择从我们的模型中删除一定数量或比例的最差表现变量。我们可以将删除变量数量或比例作为模型构建过程中的超参数进行调整。'
- en: '***Wrapper methods—*** With wrapper methods, rather than using a single, out-of-model
    statistic to estimate feature importance, we iteratively train our model with
    different predictor variables. Eventually, the combination of predictors that
    gives us the best performing model is chosen. There are different ways of doing
    this, but one such example is *sequential forward selection*. In sequential forward
    selection, we start with no predictors and then add predictors one by one. At
    each step of the algorithm, the feature that results in the best model performance
    is chosen. Finally, when the addition of any more predictors doesn’t result in
    an improvement in performance, feature addition stops, and the final model is
    trained on the selected predictors.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***包装方法—*** 在包装方法中，我们不是使用单个模型外的统计量来估计特征重要性，而是迭代地使用不同的预测变量训练我们的模型。最终，选择能够给出最佳性能的预测变量组合。有几种不同的方法可以做到这一点，但一个例子是*顺序前向选择*。在顺序前向选择中，我们从没有预测变量开始，然后逐个添加预测变量。在算法的每个步骤中，选择导致模型性能最佳的特性。最后，当添加任何更多预测变量都不会提高性能时，特征添加停止，并在选定的预测变量上训练最终模型。'
- en: 'Which method should we choose? It boils down to this: wrapper methods may result
    in models that perform better, because we are actually using the model we’re training
    to estimate predictor importance. However, because we’re training a fresh model
    at each iteration of the selection process (and each step may include other preprocessing
    steps such as imputation), wrapper methods tend to be computationally expensive.
    Filter methods, on the other hand, may or may not select the best-performing set
    of predictors but are much less computationally expensive.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择哪种方法？这归结为：包装方法可能会导致性能更好的模型，因为我们实际上正在使用我们正在训练的模型来估计预测变量的重要性。然而，由于我们在选择过程的每次迭代中（每个步骤可能包括其他预处理步骤，如插补），包装方法往往计算成本较高。另一方面，过滤方法可能或可能不会选择表现最佳的预测变量集，但计算成本要低得多。
- en: The filter method for feature selection
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征选择中的过滤方法
- en: 'I’m going to show you both methods for our ozone example, starting with the
    filter method. There are a number of metrics we can use to estimate predictor
    importance. To see the list of the available filter methods built into mlr, run
    `listFilterMethods()`. There are too many to describe in full, but common choices
    include these:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我将向您展示我们臭氧示例中的两种方法，首先是过滤方法。我们可以使用许多指标来估计预测变量的重要性。要查看 mlr 中内置的可用过滤方法列表，请运行 `listFilterMethods()`。描述全部内容太多，但常见的选项包括这些：
- en: '***Linear correlation—*** When both predictor and outcome are continuous'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***线性相关性—*** 当预测变量和结果变量都是连续变量时'
- en: '***ANOVA—*** When the predictor is categorical and the outcome is continuous'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***方差分析（ANOVA）—*** 当预测变量是分类变量且结果变量是连续变量时'
- en: '***Chi-squared—*** When both the predictor and outcome are continuous'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***卡方检验（Chi-squared）—*** 当预测变量和结果变量都是连续变量时'
- en: '***Random forest importance—*** Can be used whether the predictors and outcomes
    are categorical or continuous (the default)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***随机森林重要性—*** 可以用于预测变量和结果变量是分类变量或连续变量的情况（默认）'
- en: '|  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Feel free to experiment with the methods implemented in mlr. Many of them require
    you to first install the FSelector package: `install.packages ("FSelector")`.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎尝试 mlr 中实现的方法。其中许多方法需要您首先安装 FSelector 包：`install.packages("FSelector")`。
- en: '|  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The default method used by mlr (because it doesn’t depend on whether the variables
    are categorical or continuous) is to build a random forest to predict the outcome,
    and return the variables that contributed most to model predictions (using the
    out-of-bag error we discussed in [chapter 8](kindle_split_018.html#ch08)). In
    this example, because both the predictors and outcome variable are continuous,
    we’ll use linear correlation to estimate variable importance (it’s a little more
    interpretable than random forest importance).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: mlr 默认使用的方法（因为它不依赖于变量是否为分类变量）是构建随机森林来预测结果，并返回对模型预测贡献最大的变量（使用我们在第 8 章中讨论的袋外误差）。在这个例子中，由于预测变量和结果变量都是连续的，我们将使用线性相关性来估计变量重要性（它比随机森林的重要性更容易解释）。
- en: First, we use the `generateFilterValuesData()` function (longest function name
    ever!) to generate an importance metric for each predictor. The first argument
    is the task, which contains our dataset and lets the function know that `Ozone`
    is our target variable. The second, optional argument is `method`, to which we
    can supply one of the methods listed by `listFilterMethods()`. In this example,
    I’ve used `"linear.correlation"`. By extracting the `$data` component of this
    object, we get the table of predictors with their Pearson correlation coefficients.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `generateFilterValuesData()` 函数（最长的函数名！）为每个预测变量生成一个重要性度量。第一个参数是任务，其中包含我们的数据集，并让函数知道
    `Ozone` 是我们的目标变量。第二个可选参数是 `method`，我们可以向其中提供 `listFilterMethods()` 列出的方法之一。在这个例子中，我使用了
    `"linear.correlation"`。通过提取此对象中的 `$data` 组件，我们得到包含预测变量及其皮尔逊相关系数的表格。
- en: Listing 9.6\. Using a filter method for feature selection
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.6. 使用过滤方法进行特征选择
- en: '[PRE6]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It’s easier to interpret this information as a plot, which we can generate with
    the `plotFilterValues()` function, giving the object we saved the filter values
    to as its argument. The resulting plot is shown in [figure 9.8](#ch09fig08).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 将此信息解释为图表更为容易，我们可以使用 `plotFilterValues()` 函数生成图表，将保存过滤值的对象作为其参数。生成的图表显示在[图9.8](#ch09fig08)中。
- en: '|  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Generate and plot filter values for `ozoneTask`, but using the default method
    `randomForestSRC_importance` (don’t overwrite the `filterVals` object). Are the
    variables ranked in the same order of importance between the two methods?
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为 `ozoneTask` 生成和绘制过滤值，但使用默认方法 `randomForestSRC_importance`（不要覆盖 `filterVals`
    对象）。两种方法中变量的重要性排名是否相同？
- en: '|  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that we have a way of ranking our predictors in order of their estimated
    importance, we can decide how to “skim off” the least informative ones. We do
    this using the `filterFeatures()` function, which takes the task as the first
    argument, our `filterVals` object as the `fval` argument, and either the `abs`,
    `per`, or `threshold` argument. The `abs` argument allows us to specify the absolute
    number of best predictors to retain. The `per` argument allows us to specify a
    top percentage of best predictors to retain. The `threshold` argument allows us
    to specify a value of our filtering metric (in this case, correlation coefficient)
    that a predictor must exceed in order to be retained. We *could* manually filter
    our predictors using one of these three methods. This is shown in the following
    listing, but I’ve commented the lines out because we’re not going to do this.
    Instead, we can wrap together our learner (linear regression) and the filter method
    so that we can treat any of `abs`, `per`, and `threshold` as hyperparameters and
    tune them.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了按估计重要性对预测变量进行排名的方法，我们可以决定如何“去除”最不具信息量的变量。我们使用 `filterFeatures()` 函数来完成这项任务，该函数将任务作为第一个参数，将我们的
    `filterVals` 对象作为 `fval` 参数，以及 `abs`、`per` 或 `threshold` 参数。`abs` 参数允许我们指定要保留的最佳预测变量的绝对数量。`per`
    参数允许我们指定要保留的最佳预测变量的百分比。`threshold` 参数允许我们指定过滤度量（在本例中为相关系数）的值，预测变量必须超过此值才能被保留。我们可以手动使用这三种方法之一过滤我们的预测变量。这将在下面的列表中展示，但我已经注释掉了这些行，因为我们不会这样做。相反，我们可以将我们的学习器（线性回归）和过滤方法包装在一起，这样我们就可以将
    `abs`、`per` 和 `threshold` 视为超参数并进行调整。
- en: Figure 9.8\. Plotting the correlation of each predictor against the ozone level
    using `plotFilterValues()`
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8. 使用 `plotFilterValues()` 绘制每个预测变量与臭氧水平的相关性
- en: '![](fig9-8_alt.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![fig9-8_alt.jpg]'
- en: Listing 9.7\. Manually selecting which features to drop
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.7. 手动选择要删除的特征
- en: '[PRE7]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To wrap together our learner and filter method, we use the `makeFilterWrapper()`
    function, supplying the linear regression learner we defined as the `learner`
    argument and our filter metric as the `fw.method` argument.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的学习器和过滤方法包装在一起，我们使用 `makeFilterWrapper()` 函数，将我们定义的线性回归学习器作为 `learner`
    参数，并将我们的过滤度量作为 `fw.method` 参数。
- en: Listing 9.8\. Creating a filter wrapper
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.8. 创建过滤包装器
- en: '[PRE8]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Confusing terminology alert! We are still using the *filter method* for feature
    selection. It’s unfortunately confusing that we are making a *filter wrapper*,
    but this is not the *wrapper method* for feature selection. We will cover this
    shortly.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意术语混淆！我们仍在使用 *过滤方法* 进行特征选择。不幸的是，我们创建了一个 *过滤包装器*，但这并不是特征选择的 *包装方法*。我们将在稍后介绍这一点。
- en: '|  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When we wrap together a learner and a preprocessing step, the hyperparameters
    for both become available for tuning as part of our wrapped learner. In this situation,
    it means we can tune the *abs*, *per*, or *threshold* hyperparameter using cross-validation,
    to select the best-performing features. In this example, we’re going to tune the
    absolute number of features to retain.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将学习器和预处理步骤组合在一起时，两者的超参数都成为我们包装学习器的一部分可供调整。在这种情况下，这意味着我们可以使用交叉验证调整`abs`、`per`或`threshold`超参数，以选择最佳性能的特征。在这个例子中，我们将调整要保留的绝对特征数量。
- en: Listing 9.9\. Tuning the number of predictors to retain
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.9\. 调整要保留的预测因子数量
- en: '[PRE9]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you run `getParamSet(filterWrapper)`, you’ll see that the hyperparameter
    names for *abs*, *per*, and *threshold* have become *fw.abs*, *fw.per*, and *fw.threshold*,
    now that we’ve wrapped the filter method. Another useful hyperparameter, *fw.mandatory.feat*,
    allows you to force certain variables to be included regardless of their scores.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行`getParamSet(filterWrapper)`，你会看到由于我们包装了过滤方法，`abs`、`per`和`threshold`的超参数名称已经变成了`fw.abs`、`fw.per`和`fw.threshold`。现在，另一个有用的超参数`fw.mandatory.feat`允许你强制某些变量被包含，无论它们的分数如何。
- en: '|  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: First, we define the hyperparameter space, as usual, with `makeParamSet()`,
    and define *fw.abs* as an integer between 1 and 12 (the minimum and maximum number
    of features we’re going to retain). Next, we define our old friend, the grid search,
    using `makeTuneControlGrid()`. This will try every value of our hyperparameter.
    We define an ordinary 10-fold cross-validation strategy using `makeResampleDesc()`
    and then perform the tuning with `tuneParams()`. The first argument is our wrapped
    learner, and then we supply our task, cross-validation method, hyperparameter
    space, and search procedure.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们像往常一样使用`makeParamSet()`定义超参数空间，并将`fw.abs`定义为介于1到12之间的整数（我们将保留的最小和最大特征数量）。接下来，我们使用`makeTuneControlGrid()`定义我们熟悉的老朋友——网格搜索，这将尝试我们超参数的每个值。我们使用`makeResampleDesc()`定义一个普通的10折交叉验证策略，然后使用`tuneParams()`进行调优。第一个参数是我们的包装学习器，然后我们提供我们的任务、交叉验证方法、超参数空间和搜索过程。
- en: 'Our tuning procedure picks the 10 predictors with the highest correlation with
    ozone as the best-performing combination. But what’s `mse.test.mean`? You haven’t
    seen this performance metric before. Well, the performance metrics we used for
    classification, such as mean misclassification error, don’t make sense when we’re
    predicting continuous variables. For regression problems, there are three commonly
    used performance metrics:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的选择过程挑选出与臭氧相关性最高的10个预测因子作为最佳性能组合。但什么是`mse.test.mean`？你之前还没有见过这个性能指标。好吧，我们用于分类的性能指标，如平均误分类误差，在预测连续变量时没有意义。对于回归问题，有三个常用的性能指标：
- en: '***Mean absolute error (MAE)—*** Finds the absolute residual between each case
    and the model, adds them all up, and divides by the number of cases. We can interpret
    this as the mean absolute distance of the cases from the model.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***平均绝对误差 (MAE)***——找到每个案例与模型之间的绝对残差，将它们全部加起来，然后除以案例数量。我们可以将这解释为案例与模型之间的平均绝对距离。'
- en: '***Mean square error (MSE)—*** Similar to MAE but *squares* the residuals before
    finding their mean. This means MSE is more sensitive to outliers than MAE, because
    the size of the squared residual grows quadratically, the further from the model
    prediction it is. MSE is the default performance metric for regression learners
    in mlr. The choice of MSE or MAE depends on how you want to treat outliers in
    your data: if you want your model to be able to predict such cases, use MSE; otherwise,
    if you want your model to be less sensitive to outliers, use MAE.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***均方误差 (MSE)***——与MAE类似，但在找到平均值之前将残差平方。这意味着MSE比MAE对异常值更敏感，因为平方残差的大小随着与模型预测的距离的平方增长。MSE是mlr中回归学习者的默认性能指标。MSE或MAE的选择取决于你如何处理数据中的异常值：如果你想让你的模型能够预测这样的案例，使用MSE；否则，如果你想让你的模型对异常值不太敏感，使用MAE。'
- en: '***Root mean square error (RMSE)—*** Because MSE squares the residual, its
    value isn’t on the same scale as the outcome variable. Instead, if we take the
    square root of the MSE, we get the RMSE. When tuning hyperparameters and comparing
    models, MSE and RMSE will always select the same models (because RMSE is simply
    a transformation of MSE), but RMSE has the benefit of being on the same scale
    as our outcome variable and so is more interpretable.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***均方根误差（RMSE）——*** 由于MSE对残差进行平方，其值与结果变量的尺度不同。相反，如果我们对MSE取平方根，我们得到的是RMSE。在调整超参数和比较模型时，MSE和RMSE总是会选择相同的模型（因为RMSE只是MSE的一种变换），但RMSE的好处是它与我们的结果变量处于相同的尺度，因此更具可解释性。'
- en: '|  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Other regression performance metrics are available to us, such as the percentage
    versions of MAE and MSE. If you’re interested in reading about more of the performance
    metrics available in mlr (and there are a lot of them), run `?measures`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有其他回归性能指标可供选择，例如MAE和MSE的百分比版本。如果你对阅读mlr中可用的更多性能指标感兴趣（而且有很多），请运行`?measures`。
- en: '|  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: Repeat the feature-filtering process in [listings 9.8](#ch09ex08) and [9.9](#ch09ex09),
    but use the default `fw.method` argument (`randomForestSRC_importance`, or just
    don’t supply it). Does this select the same number of predictors as when we used
    linear correlation? Which method was faster?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表9.8](#ch09ex08)和[9.9](#ch09ex09)中重复特征过滤过程，但使用默认的`fw.method`参数（`randomForestSRC_importance`，或者不提供它）。这会选择与使用线性相关性相同的预测因子数量吗？哪种方法更快？
- en: '|  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Using the MSE performance metric, our tuned filter method has concluded that
    retaining the 10 features with the highest correlation with the ozone level results
    in the best-performing model. We can now train a final model that includes only
    these top 10 features in the task.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均方误差（MSE）性能指标，我们的调整滤波器方法得出结论，保留与臭氧水平相关性最高的10个特征会导致性能最佳的模型。我们现在可以训练一个只包含这些前10个特征的最终模型。
- en: Listing 9.10\. Training the model with filtered features
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.10\. 使用过滤特征训练模型
- en: '[PRE10]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: First, we create a new task that includes only the filtered features, using
    the `filterFeatures()` function. To this function, we supply the name of the existing
    task, the `filterVals` object we defined in [listing 9.6](#ch09ex06), and the
    number of features to retain as the argument to `abs`. This value can be accessed
    as the `$x` component of `tunedFeats` and needs to be wrapped in `unlist()`; otherwise,
    the function will throw an error. This creates a new task that contains only the
    filtered predictors and retains `Ozone` as the target variable. Finally, we train
    the linear model using this task.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`filterFeatures()`函数创建一个只包含过滤特征的新的任务。我们将现有任务的名称、我们在[列表9.6](#ch09ex06)中定义的`filterVals`对象以及要保留的特征数量作为`abs`函数的参数传递给这个函数。这个值可以作为`tunedFeats`的`$x`组件访问，并且需要用`unlist()`包装；否则，函数将抛出错误。这创建了一个只包含过滤预测因子并保留`Ozone`作为目标变量的新任务。最后，我们使用这个任务训练线性模型。
- en: The wrapper method for feature selection
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征选择的包装方法
- en: With the filter method, we generate univariate statistics describing how each
    predictor relates to the outcome variable. This may result in selecting the most
    informative predictors, but it isn’t guaranteed to. Instead, we can use the actual
    model we’re trying to train to determine which features help it make the best
    predictions. This has the potential to select a better-performing combination
    of predictors, but it is computationally more expensive as we’re training a fresh
    model for every permutation of predictor variables.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用过滤方法，我们生成描述每个预测因子如何与结果变量相关的单变量统计量。这可能会导致选择最有信息的预测因子，但并不保证。相反，我们可以使用我们试图训练的实际模型来确定哪些特征有助于它做出最佳预测。这有可能选择更好的预测因子组合，但因为它为预测变量的每个排列都训练一个新的模型，所以计算成本更高。
- en: 'Let’s start by defining how we’re going to search for the best combination
    of predictors. We have four options:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义我们将如何搜索最佳预测因子组合。我们有四种选择：
- en: '***Exhaustive search—*** This is basically a grid search. It will try every
    possible combination of predictor variables in your dataset and select the one
    that performs the best. This is guaranteed to find the best combination but can
    be prohibitively slow. For example, in our 12-predictor dataset, exhaustive search
    would need to try more than 1.3 × 10⁹ different variable combinations!'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***穷举搜索—*** 这基本上是一个网格搜索。它将尝试你数据集中预测变量可能的所有组合，并选择表现最好的那个。这保证找到最佳组合，但可能速度过慢。例如，在我们的12个预测变量数据集中，穷举搜索需要尝试超过1.3
    × 10⁹种不同的变量组合！'
- en: '***Random search—*** This is just like random search in hyperparameter tuning.
    We define a number of iterations and randomly select feature combinations. The
    best combination after the final iteration wins. This is usually less intensive
    (depending on how many iterations you choose), but it isn’t guaranteed to find
    the best combination of features.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***随机搜索—*** 这就像超参数调优中的随机搜索。我们定义一系列迭代次数，并随机选择特征组合。最终迭代后的最佳组合获胜。这通常不那么密集（取决于你选择的迭代次数），但它并不保证找到最佳特征组合。'
- en: '***Sequential search—*** From a particular starting point, we either add or
    remove features at each step that improve performance. This can be one of the
    following:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***顺序搜索—*** 从一个特定的起点开始，我们在每个步骤中添加或移除特征，以改善性能。这可以是以下之一：'
- en: '***Forward search—*** We start with an empty model and sequentially add the
    feature that improves the model most until additional features no longer improve
    the performance.'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***正向搜索—*** 我们从一个空模型开始，依次添加改进模型最多的特征，直到额外的特征不再提高性能。'
- en: '***Backward search—*** We start with all the features and remove the feature
    whose removal improves the model the most until additional removals no longer
    improve the performance.'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***反向搜索—*** 我们从所有特征开始，移除移除后模型改进最大的特征，直到额外的移除不再提高性能。'
- en: '***Floating forward search—*** Starting from an empty model, we either add
    one variable or remove one variable at each step, whichever improves the model
    the most, until neither an addition nor a removal improves model performance.'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***浮动正向搜索—*** 从一个空模型开始，我们在每个步骤中添加一个变量或移除一个变量， whichever improves the model
    the most，直到添加或移除都不再提高模型性能。'
- en: '***Floating backward search—*** The same as floating forward, except we start
    with a full model.'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***浮动反向搜索—*** 与浮动正向搜索相同，只是我们从完整的模型开始。'
- en: '***Genetic algorithm—*** This method, inspired by Darwinian evolution, finds
    pairs of feature combinations that act as “parents” to “offspring” variable combinations,
    which inherit the best-performing features. This method is very cool but can be
    computationally expensive as the feature space grows.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***遗传算法—*** 这种方法受到达尔文进化论的启发，找到作为“父母”的“后代”变量组合的特征组合对，这些组合继承了表现最好的特征。这种方法非常酷，但随着特征空间的扩大，计算成本可能会很高。'
- en: Wow! With so many options to choose from, where do we start? Well, I find the
    exhaustive and genetic searches prohibitively slow for a large feature space.
    While the random search can alleviate this problem, I find a sequential search
    to be a good compromise between computational cost and probability of finding
    the best-performing feature combination. Of its different variants, you may want
    to experiment with the various options to see which results in the best-performing
    model. I like the floating versions because they consider both addition and removal
    at each step, so for this example we’re going to use floating backward selection.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！有这么多选项可以选择，我们从哪里开始呢？嗯，我发现对于大特征空间，穷举搜索和遗传搜索速度过慢。虽然随机搜索可以缓解这个问题，但我发现顺序搜索在计算成本和找到最佳性能特征组合的概率之间是一个很好的折衷方案。在其不同的变体中，你可能想尝试不同的选项，看看哪个会产生最佳性能的模型。我喜欢浮动版本，因为它们在每个步骤都考虑了添加和移除，所以在这个例子中，我们将使用浮动反向选择。
- en: First, we define the search method using the `makeFeatSelControlSequential()`
    function (wow, the mlr authors really do love their long function names). We use
    `"sfbs"` as the method argument to use a sequential floating backward selection.
    Then, we use the `selectFeatures()` function to perform the feature selection.
    To this function we supply the learner, task, cross-validation strategy defined
    in [listing 9.9](#ch09ex09), and search method. It’s as easy as that. When we
    run the function, every permutation of predictor variables is cross-validated
    using our `kFold` strategy to get an estimate of its performance. By printing
    the result of this process, we can see the algorithm selected six predictors that
    had a slightly lower MSE value than the predictors selected by our filter method
    in [listing 9.9](#ch09ex09).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `makeFeatSelControlSequential()` 函数定义搜索方法（哇，mlr 的作者真的非常喜欢他们的长函数名）。我们使用
    `"sfbs"` 作为方法参数来使用序列浮点向后选择。然后，我们使用 `selectFeatures()` 函数执行特征选择。我们将学习者、任务、在[列表
    9.9](#ch09ex09)中定义的交叉验证策略和搜索方法提供给此函数。就这么简单。当我们运行函数时，使用我们的 `kFold` 策略对预测变量的每个排列进行交叉验证，以获得其性能的估计。通过打印此过程的结果，我们可以看到算法选择了六个预测变量，其
    MSE 值略低于[列表 9.9](#ch09ex09)中通过过滤器方法选择的预测变量。
- en: '|  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: To see all of the available wrapper methods and how to use them, run `?FeatSelControl`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看所有可用的包装器方法和如何使用它们，请运行 `?FeatSelControl`。
- en: '|  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Now I need to warn you about a frustrating bug with regard to the sequential
    floating forward search. As of this writing, using `"sffs"` as the feature-selection
    method will throw this error in some circumstances: `Error in sum(x) : invalid
    ''type'' (list) of argument`. If you try to use `"sffs"` as the search method
    in this example, you may get such an error. Therefore, while this is very frustrating,
    I’ve opted to use sequential floating *backward* search (`"sfbs"`) instead.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我需要提醒你关于序列浮点前向搜索的一个令人沮丧的错误。截至本文写作时，在某些情况下，使用 `"sffs"` 作为特征选择方法将抛出以下错误：`Error
    in sum(x) : invalid ''type'' (list) of argument`。如果你尝试在这个例子中使用 `"sffs"` 作为搜索方法，可能会遇到这样的错误。因此，虽然这非常令人沮丧，但我选择使用序列浮点*向后*搜索
    (`"sfbs"`) 代替。'
- en: Listing 9.11\. Using a wrapper method for feature selection
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.11\. 使用包装方法进行特征选择
- en: '[PRE11]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, just as we did for the filter method, we can create a new task using the
    imputed data that contains only those selected predictors, and train a model on
    it.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像我们对过滤器方法所做的那样，我们可以使用只包含所选预测因子的插补数据创建一个新的任务，并在其上训练模型。
- en: Listing 9.12\. Using a wrapper method for feature selection
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.12\. 使用包装方法进行特征选择
- en: '[PRE12]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 9.2.4\. Including imputation and feature selection in cross-validation
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.4\. 在交叉验证中包含插补和特征选择
- en: 'I’ve said it many times before, but I’m going to say it again: include all
    data-dependent preprocessing steps in your cross-validation! But up to this point,
    we’ve only needed to consider a single preprocessing step. How do we combine more
    than one? Well, mlr makes this process extremely simple. When we wrap together
    a learner and a preprocessing step, we have essentially created a new learner
    algorithm that includes that preprocessing. So to include an additional preprocessing
    step, we simply wrap the wrapped learner! I’ve illustrated this for our example
    in [figure 9.9](#ch09fig09). This results in a sort of Matryoshka doll of wrappers,
    where one is encapsulated by another, which is encapsulated by another, and so
    on.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经说过很多次了，但我要再说一遍：在交叉验证中包含所有数据相关的预处理步骤！但到目前为止，我们只需要考虑一个预处理步骤。我们如何组合多个步骤呢？嗯，mlr
    使得这个过程非常简单。当我们把一个学习者和一个预处理步骤包装在一起时，我们实际上创建了一个包含该预处理的新学习算法。因此，为了包含一个额外的预处理步骤，我们只需包装包装过的学习者！我在[图
    9.9](#ch09fig09)中展示了这一点。这导致了一种类似套娃的包装器，其中一个是被另一个封装的，然后是被另一个封装的，以此类推。
- en: Figure 9.9\. Combining multiple preprocessing wrappers. Once a learner and preprocessing
    step (such as imputation) have been combined in a wrapper, this wrapper can be
    used as the learner in another wrapper.
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.9\. 组合多个预处理包装器。一旦一个学习者和预处理步骤（如插补）在包装器中组合，这个包装器就可以用作另一个包装器中的学习者。
- en: '![](fig9-9.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 9-9](fig9-9.jpg)'
- en: Using this strategy, we can combine as many preprocessing steps as we like to
    create a pipeline. The innermost wrapper will always be used first, then the next
    innermost, and so on.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个策略，我们可以组合任意多的预处理步骤来创建一个管道。最内层的包装器将始终首先使用，然后是下一个内层的，依此类推。
- en: '|  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because the innermost wrapper is used first, through to the outermost, it’s
    important to think carefully about the order you wish the preprocessing steps
    to take.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因为最内层的包装器首先使用，一直到最外层，所以仔细思考你希望预处理步骤采取的顺序是很重要的。
- en: '|  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s reinforce this in your mind by actually doing it. We’re going to make
    an impute wrapper and then pass it as the learner to a feature-selection wrapper.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过实际操作来加强这一点。我们将创建一个填补包装器，然后将其作为学习器传递给一个特征选择包装器。
- en: Listing 9.13\. Combining imputation and feature selection wrappers
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.13\. 结合填补和特征选择包装器
- en: '[PRE13]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First, we redefine our imputation method using the `imputeLearner()` function
    (first defined in [listing 9.4](#ch09ex04)). Then, we create an imputation wrapper
    using the `makeImputeWrapper()` function, which takes the learner as the first
    argument. We use `list(numeric = imputeMethod)` as the `classes` argument to apply
    this imputation strategy to all of our numeric predictors (all of them, duh).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`imputeLearner()`函数（首次定义在[列表9.4](#ch09ex04)）重新定义我们的填补方法。然后，我们使用`makeImputeWrapper()`函数创建一个填补包装器，该函数将学习器作为第一个参数。我们使用`list(numeric
    = imputeMethod)`作为`classes`参数，将此填补策略应用于我们所有的数值预测因子（所有这些，duh）。
- en: 'Now here comes the neat bit: we create a feature-selection wrapper using `makeFeatSelWrapper()`,
    and supply the imputation wrapper we created as the learner. This is the crucial
    step because we’re creating a wrapper with another wrapper! We set the cross-validation
    method as `kFold` (defined in [listing 9.9](#ch09ex09)) and the method of searching
    feature combinations as `featSelControl` (defined in [listing 9.11](#ch09ex11)).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候展示一些巧妙的部分了：我们使用`makeFeatSelWrapper()`创建一个特征选择包装器，并提供我们创建的填补包装器作为学习器。这是关键步骤，因为我们正在创建一个包含另一个包装器的包装器！我们将交叉验证方法设置为`kFold`（在[列表9.9](#ch09ex09)中定义）和搜索特征组合的方法为`featSelControl`（在[列表9.11](#ch09ex11)中定义）。
- en: Now, let’s cross-validate our entire model-building process like good data scientists.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们像优秀的数据科学家一样交叉验证我们的整个模型构建过程。
- en: Listing 9.14\. Cross-validating the model-building process
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.14\. 交叉验证模型构建过程
- en: '[PRE14]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After loading our friends the parallel and parallelMap packages, we define a
    task using the `ozoneClean` tibble, which still contains missing data. Next, we
    define an ordinary 3-fold cross-validation strategy for our cross-validation procedure.
    Finally, we start parallelization with `parallelStartSocket()` and start the cross-validation
    procedure by supplying the learner (the wrapped wrapper), task, and cross-validation
    strategy to the `resample()` function. This took nearly 90 seconds on my four-core
    machine, so I suggest you start the process and then read on for a summary of
    what the code is doing.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了并行和parallelMap包的朋友之后，我们使用`ozoneClean` tibble定义一个任务，它仍然包含缺失数据。接下来，我们为交叉验证过程定义一个普通的3折交叉验证策略。最后，我们使用`parallelStartSocket()`开始并行化，并通过将学习器（包装的包装器）、任务和交叉验证策略提供给`resample()`函数来启动交叉验证过程。在我的四核机器上，这几乎花费了90秒，所以我建议你开始这个过程，然后继续阅读代码的总结。
- en: 'The cross-validation process proceeds like this:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证过程如下进行：
- en: Split the data into three folds.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为三个折叠。
- en: 'For each fold:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个折叠：
- en: Use the rpart algorithm to impute the missing values.
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用rpart算法来填补缺失值。
- en: 'Perform feature selection: Update template to support more than two levels
    of nested ordered lists.'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行特征选择：更新模板以支持超过两层嵌套有序列表。
- en: Use a selection method (such as backward search) to select combinations of features
    to train models on.
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用选择方法（例如反向搜索）来选择特征组合以训练模型。
- en: Use 10-fold cross-validation to evaluate the performance of each model.
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用10折交叉验证来评估每个模型的表现。
- en: Return the best-performing model for each of the three outer folds.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回三个外部折叠中每个表现最好的模型。
- en: Return the mean MSE to give us our estimate of performance.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回平均MSE以给出我们性能的估计。
- en: We can see that our model-building process gives us a mean MSE of 20.54, suggesting
    a mean residual error of 4.53 on the original ozone scale (taking the square root
    of 20.54).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的模型构建过程给我们一个平均均方误差（MSE）为20.54，这表明在原始臭氧尺度上的平均残差误差为4.53（取20.54的平方根）。
- en: 9.2.5\. Interpreting the model
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.5\. 解释模型
- en: Due to their simple structure, linear models are usually quite simple to interpret,
    because we can look at the slopes for each predictor to infer how much the outcome
    variable is affected by each. However, whether these interpretations are justified
    or not depends on whether some model assumptions have been met, so in this section
    I’ll show you how to interpret the model output and generate some diagnostic plots.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们的简单结构，线性模型通常很容易解释，因为我们可以通过查看每个预测因子的斜率来推断结果变量受到的影响程度。然而，这些解释是否合理取决于是否满足某些模型假设，因此在本节中，我将向您展示如何解释模型输出并生成一些诊断图。
- en: First, we need to extract the model information from our model object using
    the `getLearnerModel()` function. By calling `summary()` on the model data, we
    get an output with lots of information about our model. Take a look at the following
    listing.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用 `getLearnerModel()` 函数从我们的模型对象中提取模型信息。通过在模型数据上调用 `summary()`，我们得到一个包含大量关于我们模型信息的输出。请看下面的列表。
- en: Listing 9.15\. Interpreting the model
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.15\. 解释模型
- en: '[PRE15]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `Call` component would normally tell us the formula we used to create the
    model (which variables, and whether we added more complex relationships between
    them). Because we built this model using mlr, we unfortunately don’t get that
    information here; but the model formula is all of the selected predictors combined
    linearly together.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`Call` 组件通常会告诉我们创建模型所使用的公式（哪些变量，以及我们是否在它们之间添加了更复杂的关系）。由于我们使用 mlr 构建了这个模型，所以我们很遗憾地在这里没有获得这些信息；但模型公式是所有选定的预测因子线性组合在一起的结果。'
- en: The `Residuals` component gives us some summary statistics about the model residuals.
    Here we’re looking to see if the median is approximately 0 and that the first
    and third quartiles are approximately the same. If they aren’t, this might suggest
    the residuals are either not normally distributed, or heteroscedastic. In both
    situations, not only could this negatively impact model performance, but it could
    make our interpretation of the slopes incorrect.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '`Residuals` 组件为我们提供了关于模型残差的某些汇总统计信息。在这里，我们正在查看中位数是否大约为 0，以及第一四分位数和第三四分位数是否大约相同。如果它们不相同，这可能会表明残差要么不是正态分布的，要么是异方差性的。在这两种情况下，这不仅可能对模型性能产生负面影响，还可能使我们对斜率的解释不正确。'
- en: The `Coefficients` component shows us a table of model parameters and their
    standard errors. The intercept is 41.8, which is the estimate of the ozone level
    when all other variables are 0\. In this particular case it doesn’t really make
    sense for some of our variables to be 0 (`month`, for example) so we won’t draw
    too much interpretation from this. The estimates for the predictors are their
    slopes. For example, our model estimates that for a one-unit increase in the `Temp_Sand`
    variable, `Ozone` increases by 0.227 (holding all other variables constant). The
    `Pr(>|t|)` column contains the *p* values that, in theory, represent the probability
    of seeing a slope this large if the population slope was actually 0\. Use the
    *p* values to guide your model-building process, by all means; but there are some
    problems associated with *p* values, so don’t put too much faith in them.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`Coefficients` 组件显示了一个模型参数及其标准误差的表格。截距为 41.8，这是当所有其他变量都为 0 时臭氧水平的估计值。在这个特定情况下，某些变量为
    0（例如 `month`）实际上并没有太多意义，所以我们不会过多地从这个结果中得出解释。预测因子的估计值是它们的斜率。例如，我们的模型估计，当 `Temp_Sand`
    变量增加一个单位时，`Ozone` 增加 0.227（保持所有其他变量不变）。`Pr(>|t|)` 列包含的 *p* 值，在理论上，如果总体斜率实际上是 0，则表示看到如此大的斜率的概率。无论如何，请使用
    *p* 值来指导你的模型构建过程；但与 *p* 值相关的问题有一些，所以不要过分依赖它们。'
- en: Finally, `Residual standard error` is the same as RMSE, `Multiple R-squared`
    is an estimate of the proportion of variance in the data accounted for by our
    model (68.9%), and `F-statistic` is the ratio of variance explained by our model
    to the variance not explained by the model. The *p* value here is an estimate
    of the probability that our model is better than just using the mean of `Ozone`
    to make predictions.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`Residual standard error` 与 RMSE 相同，`Multiple R-squared` 是我们模型解释数据方差的估计比例（68.9%），而
    `F-statistic` 是模型解释的方差与模型未解释的方差的比率。这里的 *p* 值是对我们模型比仅使用 `Ozone` 的平均值进行预测更好的概率的估计。
- en: '|  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Notice the residual standard error value is close to but not the same as the
    RMSE estimated for the model-building process by cross-validation. This difference
    is because we cross-validated the model-building procedure, not this particular
    model itself.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 注意残差标准误差值接近但并不等于通过交叉验证估计的模型构建过程的RMSE。这种差异是因为我们交叉验证了模型构建过程，而不是这个特定的模型本身。
- en: '|  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We can very quickly and easily print diagnostic plots for linear models in R
    by supplying the model data as the argument to `plot()`. Ordinarily, this will
    prompt you to press Enter to cycle through the plots. I find this irritating and
    so prefer to split the plotting device into four parts using the `mfrow` argument
    to the `par()` function. This means when we create our diagnostic plots (there
    will be four of them), they will be tiled in the same plotting window. These plots
    may help us identify flaws in our model that impact predictive performance.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将模型数据作为`plot()`函数的参数来快速轻松地打印线性模型的诊断图。通常，这将提示你按Enter键循环浏览图表。我发现这很烦人，所以我更喜欢使用`par()`函数的`mfrow`参数将绘图设备分成四个部分。这意味着当我们创建我们的诊断图（将有四个）时，它们将在同一个绘图窗口中平铺。这些图表可能有助于我们识别影响预测性能的模型缺陷。
- en: '|  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-304
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: I change this back again with the `par()` function afterward.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我随后又用`par()`函数将其改回。
- en: '|  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 9.16\. Creating diagnostic plots of the model
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.16. 创建模型的诊断图
- en: '[PRE16]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The resulting plot is shown in [figure 9.10](#ch09fig10). The Residuals vs.
    Fitted plot shows the predicted ozone level on the x-axis and the residual on
    the y-axis for each case. We *hope* that there are no patterns in this plot. In
    other words, the amount of error shouldn’t depend on the predicted value. In this
    situation, we have a curved relationship. This indicates that we have nonlinear
    relationships between predictors and ozone, and/or heteroscedasticity.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如图9.10所示。残差与拟合值图显示了每个案例的预测臭氧水平（x轴）和残差（y轴）。我们*希望*在这个图中没有模式。换句话说，误差量不应取决于预测值。在这种情况下，我们有一个曲线关系。这表明我们在预测因子和臭氧之间存在非线性关系，以及/或异方差性。
- en: Figure 9.10\. Plotting diagnostic plots for our linear model. The Residuals
    vs. Fitted and Scale-Location plots help identify patterns that suggest nonlinearity
    and heteroscedasticity. The Normal Q-Q plot helps identify non-normality of residuals,
    and the Residuals vs. Leverage plot helps identify influential outliers.
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.10. 为我们的线性模型绘制诊断图。残差与拟合值图和尺度位置图有助于识别非线性异方差性的模式。正态Q-Q图有助于识别残差的非正态性，而残差与杠杆作用图有助于识别有影响力的异常值。
- en: '![](fig9-10_alt.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](fig9-10_alt.jpg)'
- en: 'The Normal Q-Q (quantile-quantile) plot shows the quantiles of the model residuals
    plotted against their quantiles if they were drawn from a theoretical normal distribution.
    If the data deviates considerably from a 1:1 diagonal line, this suggests the
    residuals are not normally distributed. This doesn’t seem to be a problem for
    this model: the residuals line up nicely on the diagonal.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 正态Q-Q（分位数-分位数）图显示了模型残差的分位数与如果它们是从理论正态分布中抽取的分位数之间的关系。如果数据与1:1对角线有显著偏差，这表明残差不是正态分布的。这似乎不是这个模型的问题：残差很好地排列在对角线上。
- en: The Scale-Location plot helps us identify heteroscedasticity of the residuals.
    There should be no patterns here, but it looks like the residuals are increasingly
    varied with larger predicted values, suggesting heteroscedasticity.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度位置图帮助我们识别残差的异方差性。这里不应该有模式，但看起来残差随着预测值的增大而越来越多样化，这表明存在异方差性。
- en: Finally, the Residuals vs. Leverage plot helps us to identify cases that have
    excessive influence on the model parameters (potential outliers). Cases that fall
    inside a dotted region of the plot called *Cook’s distance* may be outliers whose
    inclusion or exclusion makes a large difference to the model. Because we can’t
    even see Cook’s distance here (it is beyond the axis limits), we have no worries
    about outliers.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，残差与杠杆作用图帮助我们识别对模型参数有过度影响的案例（潜在的异常值）。落在称为*库克距离*的虚线区域内的案例可能是异常值，其包含或排除对模型有很大影响。因为我们甚至看不到库克距离在这里（它超出了坐标轴限制），所以我们不用担心异常值。
- en: These diagnostic plots (particularly the Residuals vs. Fitted plot) indicate
    the presence of nonlinear relationships between the predictor variables and the
    outcome variable. We may, therefore, be able to get better predictive performance
    from a model that doesn’t assume linearity. In the next chapter, I’ll show you
    how *generalized additive models* work, and we’ll train one to improve our model
    performance. I suggest you save your .R file, because we’re going to continue
    using the same dataset and task in the next chapter. This is so I can highlight
    to you how much nonlinearity can impact the performance of linear regression.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这些诊断图（尤其是残差与拟合图）表明预测变量和结果变量之间存在非线性关系。因此，我们可能能够从假设线性的模型中获得更好的预测性能。在下一章中，我将向您展示广义加性模型是如何工作的，我们将训练一个模型来提高我们的模型性能。我建议您保存您的
    .R 文件，因为我们将在下一章继续使用相同的dataset和任务。这样我可以向您强调非线性对线性回归性能的影响有多大。
- en: 9.3\. Strengths and weaknesses of linear regression
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3. 线性回归的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    linear regression will perform well for you.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常很难判断哪些算法会对特定任务表现良好，但以下是一些优缺点，这将帮助您决定线性回归是否适合您。
- en: 'The strengths of linear regression are as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的优点如下：
- en: It produces models that are very interpretable.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它产生的模型非常易于解释。
- en: It can handle both continuous and categorical predictors.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以处理连续和分类预测变量。
- en: It is very computationally inexpensive.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在计算上非常经济。
- en: 'The weaknesses of linear regression are these:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的缺点如下：
- en: It makes strong assumptions about the data, such as homoscedasticity, linearity,
    and the distribution of residuals (performance may suffer if these are violated).
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对数据做出了强烈的假设，例如同方差性、线性性和残差的分布（如果违反这些假设，性能可能会受到影响）。
- en: It can only learn linear relationships in the data.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只能学习数据中的线性关系。
- en: It cannot handle missing data.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法处理缺失数据。
- en: '|  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 3**'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: 'Instead of using a wrapper method, cross-validate the process of building our
    model using a filter method. Are the estimated MSE values similar? Which method
    is faster? Tips:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用包装方法，使用过滤器方法交叉验证构建我们的模型过程。估计的 MSE 值是否相似？哪种方法更快？提示：
- en: First, create a filter wrapper using our `imputeWrapper` as the learner.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用我们的 `imputeWrapper` 作为学习器创建一个过滤器包装器。
- en: Define a hyperparameter space to tune `"fw.abs"` using `makeParamSet()`.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个超参数空间，使用 `makeParamSet()` 调整 `"fw.abs"`。
- en: Define a tuning wrapper that takes the filter wrapper as a learner and performs
    a grid search.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个调整包装器，它将过滤器包装器作为学习器并执行网格搜索。
- en: Use `resample()` to perform cross-validation, using the tuning wrapper as the
    learner.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `resample()` 进行交叉验证，使用调整包装器作为学习器。
- en: '|  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Summary
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Linear regression can handle continuous and categorical predictors.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归可以处理连续和分类预测变量。
- en: Linear regression uses the equation of a straight line to model relationships
    in the data as straight lines.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归使用直线的方程来模拟数据中的关系，表现为直线。
- en: Missing values can be imputed using supervised learning algorithms that use
    the information from all the other variables.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用使用所有其他变量信息的监督学习算法来填充缺失值。
- en: 'Automated feature selection takes two forms: filter methods and wrapper methods.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动特征选择有两种形式：过滤器方法和包装方法。
- en: Filter methods of feature selection calculate univariate statistics outside
    of a model, to estimate how related predictors are to the outcome.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择的过滤器方法在模型之外计算单变量统计量，以估计预测变量与结果之间的关系。
- en: Wrapper methods actively train models on different permutations of the predictors
    to select the best-performing combination.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包装方法通过在预测变量的不同排列中主动训练模型来选择最佳性能的组合。
- en: Preprocessing steps can be combined together in mlr by sequential wrapping of
    wrapper functions.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 mlr 中可以通过包装函数的顺序包装将预处理步骤组合在一起。
- en: Solutions to exercises
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题的解答
- en: 'Generate filter values using the default `randomForestSRC_importance` method:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认的 `randomForestSRC_importance` 方法生成过滤器值：
- en: '[PRE17]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Repeat feature filtering using the default filter statistic:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认的过滤器统计量重复特征过滤：
- en: '[PRE18]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Cross-validate building a linear regression model, but using a filter method:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用过滤器方法交叉验证构建线性回归模型：
- en: '[PRE19]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Chapter 10\. Nonlinear regression with generalized additive models
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 10 章. 使用广义加性模型的非线性回归
- en: '*This chapter covers*'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Including polynomial terms in linear regression
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线性回归中包含多项式项
- en: Using splines in regression
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回归中使用样条
- en: Using generalized additive models (GAMs) for nonlinear regression
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用广义加性模型（GAMs）进行非线性回归
- en: In [chapter 9](kindle_split_020.html#ch09), I showed you how linear regression
    can be used to create very interpretable regression models. One of the strongest
    assumptions made by linear regression is that there is a linear relationship between
    each predictor variable and the outcome. This is often not the case, so in this
    chapter I’ll introduce you to a class of models that allows us to model nonlinear
    relationships in the data.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 9 章](kindle_split_020.html#ch09) 中，我向您展示了如何使用线性回归创建非常可解释的回归模型。线性回归做出的最强假设之一是每个预测变量和结果之间存在线性关系。这通常并不成立，因此在本章中，我将向您介绍一类模型，它允许我们模拟数据中的非线性关系。
- en: We’ll start by discussing how we can include *polynomial* terms in linear regression
    to model nonlinear relationships, and the advantages and disadvantages of doing
    this. We’ll then move on to the more sophisticated *generalized additive models*,
    which give us considerably more flexibility to model complex nonlinear relationships.
    I’ll also show you how these generalized additive models can handle both continuous
    and categorical variables, just like in linear regression.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论如何在线性回归中包含 *多项式* 项来模拟非线性关系，以及这样做的好处和坏处。然后，我们将转向更复杂的 *广义加性模型*，这些模型为我们提供了更多的灵活性来模拟复杂非线性关系。我还会向您展示这些广义加性模型如何处理连续变量和分类变量，就像在线性回归中一样。
- en: By the end of this chapter, I hope you’ll understand how to create nonlinear
    regression models that are still surprisingly interpretable. We will continue
    to work with the ozone dataset we were using in the previous chapter. If you no
    longer have the `ozoneClean` object defined in your global environment, just rerun
    [listings 9.1](kindle_split_020.html#ch09ex01) and [9.2](kindle_split_020.html#ch09ex02)
    from [chapter 9](kindle_split_020.html#ch09).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望您能理解如何创建仍然非常可解释的非线性回归模型。我们将继续使用我们在上一章中使用过的臭氧数据集。如果您在全局环境中不再有定义的 `ozoneClean`
    对象，只需重新运行 [第 9 章的列表 9.1](kindle_split_020.html#ch09ex01) 和 [9.2](kindle_split_020.html#ch09)。
- en: 10.1\. Making linear regression nonlinear with polynomial terms
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1\. 使用多项式项使线性回归非线性
- en: In this section, I’ll show you how we can take the general linear model we discussed
    in the previous chapter and extend it to include nonlinear, polynomial relationships
    between predictor variables and the outcome variable. Linear regression makes
    the strong assumption that there is a linear relationship between predictor variables
    and the outcome. Sometimes real-world variables have linear relationships, or
    can be sufficiently approximated by one, but often they do not. Surely the general
    linear model falls down when faced with nonlinear relationships, right? After
    all, it’s called the general *linear* model and uses the equation of a straight
    line. Well, it turns out that the general linear model is surprisingly flexible,
    and we can use it to model *polynomial* relationships.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何将我们在上一章讨论的通用线性模型扩展，以包括预测变量和结果变量之间的非线性、多项式关系。线性回归强加了一个假设，即预测变量和结果之间存在线性关系。有时现实世界的变量具有线性关系，或者可以被一个线性关系充分近似，但通常并非如此。当面对非线性关系时，通用线性模型显然会失效，对吧？毕竟，它被称为通用
    *线性* 模型，并使用直线方程。然而，结果出人意料，通用线性模型非常灵活，我们可以用它来模拟 *多项式* 关系。
- en: Recall from high school math that a polynomial equation is just an equation
    with multiple terms (single numbers or variables). If all the terms in the equation
    are raised to the power of 1 (an *exponent* of 1)—in other words, they are all
    equal to themselves—the equation is a *first-degree polynomial*. If the highest
    exponent in the equation is 2—in other words, one or some of the terms are squared
    but there are no higher exponents—the equation is a *second-degree polynomial*
    or *quadratic* polynomial. If the highest exponent is 3, the equation is a *cubic*
    polynomial; and if the highest exponent is 4, the equation is a *quartic* polynomial.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下高中数学，多项式方程只是一个包含多个项（单个数字或变量）的方程。如果方程中的所有项都提高到1的幂（即1的指数）——换句话说，它们都等于自身——那么这个方程就是一个*一次多项式*。如果方程中的最高指数是2——换句话说，一个或多个项是平方的，但没有更高的指数——那么这个方程就是一个*二次多项式*或*二次方程*。如果最高指数是3，那么方程就是一个*三次多项式*；如果最高指数是4，那么方程就是一个*四次多项式*。
- en: '|  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-361
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Although there are names for higher-degree polynomials, people usually just
    call them *n*th-degree polynomials (for example, a fifth-degree polynomial). This
    is, of course, unless you want to sound super-precocious!
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有更高次多项式的名称，但人们通常只是称它们为*n*次多项式（例如，五次多项式）。当然，除非你想显得特别聪明！
- en: '|  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Let’s have a look at some examples of *n*th-degree polynomials:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些*n*次多项式的例子：
- en: '*y* = *x*¹ (linear)'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = *x*¹ (线性)'
- en: '*y* = *x*² (quadratic)'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = *x*² (二次方)'
- en: '*y* = *x*³ (cubic)'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = *x*³ (三次方)'
- en: '*y* = *x*⁴ (quartic)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = *x*⁴ (四次方)'
- en: The shape of these functions is shown for values of *x* between –30 and 30 in
    [figure 10.1](#ch10fig01). When the exponent is 1, the function is a straight
    line; but when the exponent is greater than 1, the function is curvy.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的形状在图10.1中显示，*x*的值在-30到30之间。当指数为1时，函数是直线；但当指数大于1时，函数是曲线的。
- en: 'We can use this to our advantage: if the relationships between our predictor
    variables and our outcome variable have a curved relationship, we might be able
    to model this relationship by including *n*th-degree polynomials in our model
    definition. Think back to our cider example in [chapter 9](kindle_split_020.html#ch09).
    Imagine that instead of a linear relationship between apple content and cider
    batch pH, we have a downward curvilinear relationship like the one illustrated
    in [figure 10.2](#ch10fig02). A straight line no longer models this relationship
    very well, and predictions made by such a model are likely to have high bias.
    Instead, we can better model this relationship by including a quadratic term in
    the model definition.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这一点：如果我们的预测变量与结果变量之间的关系是曲线关系，我们可能可以通过在模型定义中包含*n*次多项式来模拟这种关系。回想一下我们在第9章中的苹果醋例子。想象一下，如果我们不是苹果含量与苹果醋批次pH值之间的线性关系，而是像图10.2中所示的下凹曲线关系，那么一条直线就不再很好地模拟这种关系，这种模型做出的预测很可能有很高的偏差。相反，我们可以在模型定义中包含一个二次项来更好地模拟这种关系。
- en: Figure 10.1\. Shapes of polynomial functions from the first to the fourth degree.
    When the *x* variable is raised to the first power, the equation models a straight
    line. As we increase the power that *x* is raised to, the equations model lines
    with varying degrees of flexibility.
  id: totrans-371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1. 从一次到四次多项式函数的形状。当*x*变量提高到一次幂时，方程模拟一条直线。随着*x*提高的幂次增加，方程模拟的线条具有不同程度的灵活性。
- en: '![](fig10-1_alt.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](fig10-1_alt.jpg)'
- en: Figure 10.2\. Comparing linear and quadratic fits to an imaginary nonlinear
    relationship between apple content and cider acidity
  id: totrans-373
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2. 比较线性拟合和二次拟合在苹果含量与苹果醋酸度之间的假设非线性关系
- en: '![](fig10-2.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](fig10-2.jpg)'
- en: The formula for the model shown in [figure 10.2](#ch10fig02) would be
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2中所示模型的公式将是
- en: '*y* = β*[apples]* × *apples* + β*[apples]*² × *apples*² + ϵ'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β*[apples]* × *apples* + β*[apples]*² × *apples*² + ϵ'
- en: where β*[apples]*² is the *slope* for the *apples*² term, which is more easily
    understood as how much the line curves as apple content increases (larger absolute
    values result in a more extreme curve). For a single predictor variable, we can
    generalize this for any *n*th-degree polynomial relationship as
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 其中β*[apples]*²是*apples*²项的*斜率*，这更容易理解为随着苹果含量的增加，线条弯曲的程度（绝对值越大，曲线越极端）。对于单个预测变量，我们可以将任何*n*次多项式关系推广为
- en: '*y* = β[0] + β[1]*x* + β[1]*x*² + ... β*[n]x^n* + ϵ'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + β[1]*x* + β[1]*x*² + ... β*[n]x^n* + ϵ'
- en: where *n* is the highest degree of polynomial you’re modeling. Notice that when
    performing polynomial regression, it’s usual to include all the lower-degree terms
    for that predictor variable as well. For example, if you’re modeling a quartic
    relationship between two variables, you would include *x*, *x*², *x*³, *and x*⁴
    terms in your model definition. Why is this? If we don’t include the lower-degree
    terms in the model, the *vertex* of the curve—the part of it that flattens out
    (either at the top or bottom of the curve, depending on which direction it curves)—is
    forced to pass through *x* = 0\. This might be a reasonable constraint to place
    on the model, but it usually isn’t. Instead, if we include the lower-degree terms
    in the model, the curve doesn’t need to pass through *x* = 0 and can “wiggle around”
    more to (hopefully) fit the data better. This is illustrated in [figure 10.3](#ch10fig03).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是你正在建模的多项式的最高次数。请注意，在进行多项式回归时，通常还会包括该预测变量的所有低次项。例如，如果你正在建模两个变量之间的四次关系，你会在你的模型定义中包含
    *x*，*x*²，*x*³和 *x*⁴ 项。为什么这样做呢？如果我们不在模型中包含低次项，曲线的**顶点**——它变平的部分（取决于曲线弯曲的方向，在曲线的顶部或底部）——被迫通过
    *x* = 0。这可能是对模型的一个合理的约束，但通常不是这样。相反，如果我们把低次项包含在模型中，曲线就不需要通过 *x* = 0，并且可以“摇摆”得更多，以（希望）更好地拟合数据。这通过[图10.3](#ch10fig03)进行了说明。
- en: Figure 10.3\. Comparing the shape of polynomial functions that do and do not
    include the first-degree term. Vertical dotted lines indicate the position of
    each function’s vertex on the x-axis.
  id: totrans-380
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3. 比较包含和不包含一次项的多项式函数的形状。垂直虚线表示每个函数顶点在x轴上的位置。
- en: '![](fig10-3_alt.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](fig10-3_alt.jpg)'
- en: Just as we saw in [chapter 9](kindle_split_020.html#ch09), when the model is
    given new data, it multiplies the values of the predictor variables (including
    the specified exponents) by their slopes and then adds them all together with
    the intercept to get the predicted value. The model we’re using is still the general
    linear model, because we’re *linearly* combining the model terms (adding them
    together).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第9章](kindle_split_020.html#ch09)中看到的，当模型接收到新数据时，它会将预测变量的值（包括指定的指数）乘以它们的斜率，然后将它们全部与截距相加以得到预测值。我们使用的模型仍然是广义线性模型，因为我们是以**线性**方式组合模型项（将它们相加）。
- en: '10.2\. More flexibility: Splines and generalized additive models'
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2. 更大的灵活性：样条函数和广义加性模型
- en: When using polynomial terms in linear regression, the higher the degree of polynomial
    we use, the more flexible our model will be. High-degree polynomials allow us
    to capture complicated nonlinear relationships in the data but are therefore more
    likely to overfit the training set. Sometimes, increasing the degree of the polynomials
    doesn’t help anyway, because the relationship between the predictor variable and
    outcome variable may not be the same across the range of the predictor variable.
    In such situations, instead of using high-degree polynomials, we can use *splines*.
    In this section, I’ll explain what splines are and how to use them, and how they
    relate to polynomials and a set of models called *generalized additive models*
    (GAMs).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 当在线性回归中使用多项式项时，我们使用的多项式次数越高，我们的模型就越灵活。高次多项式使我们能够捕捉数据中的复杂非线性关系，但因此也更可能过度拟合训练集。有时，增加多项式的次数也没有帮助，因为预测变量和结果变量之间的关系可能不会跨越预测变量的整个范围。在这种情况下，我们不是使用高次多项式，而是可以使用**样条函数**。在本节中，我将解释样条函数是什么，如何使用它们，以及它们与多项式和一组称为**广义加性模型**（GAMs）的模型之间的关系。
- en: A spline is a *piecewise* polynomial function. This means it splits the predictor
    variable into regions and fits a separate polynomial within each region, which
    regions connect to each other via knots. A *knot* is a position along the predictor
    variable that divides the regions within which the separate polynomials are fit.
    The polynomial curves in each region of the predictor pass through the knots that
    delimit that region. This allows us to model complex nonlinear relationships that
    are not constant across the range of the predictor variable. This is illustrated
    in [figure 10.4](#ch10fig04) using our cider example.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 样条函数是一种**分段**多项式函数。这意味着它将预测变量分割成若干区域，并在每个区域内拟合一个单独的多项式，这些区域通过节点相互连接。**节点**是沿着预测变量位置的点，它将区域分割成单独多项式拟合的区域。预测变量每个区域的多项式曲线都通过界定该区域的节点。这使我们能够模拟预测变量范围中不恒定的复杂非线性关系。这通过我们的苹果汁示例在[图10.4](#ch10fig04)中进行了说明。
- en: Figure 10.4\. Fitting a spline to a nonlinear relationship. The solid dots indicate
    the knots. Individual polynomial functions fit the data between the knots and
    connect to each other through them.
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4. 将样条拟合到非线性关系。实心点表示节点。单个多项式函数在节点之间拟合数据，并通过它们相互连接。
- en: '![](fig10-4.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](fig10-4.jpg)'
- en: 'Using splines is a great way of modeling complicated relationships such as
    the one shown in [figure 10.4](#ch10fig04), but this approach has some limitations:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 使用样条是一种建模复杂关系的极好方法，如[图10.4](#ch10fig04)中所示，但这种方法有一些局限性：
- en: The position and number of the knots need to be chosen manually. Both choices
    can make a big impact on the shape of the spline. The choice of knot position
    is typically either at obvious regions of change in the data or at regular intervals
    across the predictor, such as at the quartiles.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的位置和数量需要手动选择。这两个选择都会对样条形状产生重大影响。节点的位置选择通常是数据中变化明显的区域，或者在预测变量的常规间隔处，例如在四分位数处。
- en: The degree of the polynomials between knots needs to be chosen. We generally
    use cubic splines or higher, because these ensure that the polynomials connect
    with each other smoothly through the knots (quadratic polynomials may leave the
    spline disconnected at the knots).
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要选择节点之间多项式的次数。我们通常使用三次样条或更高次，因为这些确保多项式通过节点平滑地连接（二次多项式可能在节点处使样条断开）。
- en: It can become difficult to combine splines of different predictors.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不同预测器的样条组合起来可能会变得困难。
- en: So, can we do better than simple spline regression? Absolutely. The solution
    is GAMs. GAMs extend the general linear model such that instead of
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们能否比简单的样条回归做得更好？当然可以。解决方案是GAMs（广义加性模型）。GAMs扩展了广义线性模型，使得不再
- en: '*y* = β[0] + β[1]*x* + β[2]*x*[2] + ... β[2]*x*[2] + ϵ'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + β[1]*x* + β[2]*x*[2] + ... β[2]*x*[2] + ϵ'
- en: they take the form
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 它们具有以下形式
- en: '*y* = β[0] + *f*[1](*x*[1]) + *f*[2](*x*[2]) + ...*f*[k](*x*[k]) + ϵ'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = β[0] + *f*[1](*x*[1]) + *f*[2](*x*[2]) + ...*f*[k](*x*[k]) + ϵ'
- en: where each *f*(*x*) represents a function of a particular predictor variable.
    These functions can be any sort of smoothing function but will typically be a
    combination of multiple splines.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个*f*(*x*)代表特定预测变量的函数。这些函数可以是任何类型的平滑函数，但通常将是多个样条的组合。
- en: '|  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-398
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Can you see that the general linear model is a special case of the generalized
    additive model, where the function for each predictor variable is the *identity
    function* (*f*(*x*) = *x*)? We can go one step further then and say that the *generalized
    linear* model is a special case of the *generalized additive* model. This is because
    we can also use different link functions with GAMs that allow us to use them to
    predict categorical variables (as in logistic regression) or count variables.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看出广义线性模型是广义加性模型的一个特例，其中每个预测变量的函数是*恒等函数* (*f*(*x*) = *x*)吗？我们可以更进一步，并说*广义线性*模型是*广义加性*模型的一个特例。这是因为我们还可以使用GAMs中的不同连接函数，这使我们能够使用它们来预测分类变量（如逻辑回归）或计数变量。
- en: '|  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 10.2.1\. How GAMs learn their smoothing functions
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.1. GAMs如何学习它们的平滑函数
- en: Figure 10.5\. Smoothing functions for continuous variables in GAMs are commonly
    the sum of a series of basis functions, which are often splines. Three spline
    basis functions are summed at each value of *x* to predict the value of *y*. The
    dotted line shows the sum of the three basis functions, which models the nonlinear
    relationship in the data.
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5. GAMs中连续变量的平滑函数通常是多个基函数之和，这些基函数通常是样条。在每个*x*的值处，将三个样条基函数相加来预测*y*的值。虚线显示了三个基函数的和，它模拟了数据中的非线性关系。
- en: '![](fig10-5_alt.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](fig10-5_alt.jpg)'
- en: The most common method of constructing these smoothing functions is to use splines
    as *basis functions*. Basis functions are simple functions that can be combined
    to form a more complex function. Take a look at [figure 10.5](#ch10fig05). The
    nonlinear relationship between the *x* and *y* variables is modeled as a weighted
    sum of three splines. In other words, at each value of *x*, we sum the contributions
    from each of these basis functions to give us the function that models the relationship
    (the dotted line). The overall function is a *weighted* sum because each basis
    function has a corresponding weight, determining how much it contributes to the
    final function.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这些平滑函数最常见的方法是使用样条函数作为**基函数**。基函数是简单的函数，可以组合成更复杂的函数。请看[图10.5](#ch10fig05)。变量*x*和*y*之间的非线性关系被建模为三个样条函数的加权总和。换句话说，对于*x*的每个值，我们都会将这些基函数的每个贡献相加，从而得到一个模型关系的函数（虚线）。整体函数是一个**加权**的总和，因为每个基函数都有一个相应的权重，决定了它对最终函数的贡献程度。
- en: 'Let’s take another look at the GAM formula:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看GAM公式：
- en: y = β[0] + *f*[1](*x*[1]) + *f*[2](*x*[2]) + ... *f*[k](*x*[k]) + ϵ
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y = β[0] + *f*[1](*x*[1]) + *f*[2](*x*[2]) + ... *f*[k](*x*[k]) + ϵ
- en: So each *f[k]*(*x[k]*) is a smoothing function of that particular variable.
    When these smoothing functions use splines as basis functions, the function can
    be expressed as
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个*f[k]*(*x[k]*)是该特定变量的平滑函数。当这些平滑函数使用样条函数作为基函数时，该函数可以表示为
- en: '*f*(*x[i]*) = *a*[1]*b*[1](*x[i]*) + *a*[2]*b*[2](*x[i]*) + ... + *a[n]b[n]*(*x[i]*)'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*(*x[i]*) = *a*[1]*b*[1](*x[i]*) + *a*[2]*b*[2](*x[i]*) + ... + *a[n]b[n]*(*x[i]*)'
- en: where *b*[1](*x[i]*) is the value of the first basis function evaluated at a
    particular value of *x*, and *a*[1] is the weight of the first basis function.
    GAMs estimate the weights of these basis functions in order to minimize the residual
    square error of the model.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*b*[1](*x[i]*)是在*x*的特定值处评估的第一个基函数的值，而*a*[1]是第一个基函数的权重。GAMs通过最小化模型的残差平方误差来估计这些基函数的权重。
- en: 'GAMs automatically learn a nonlinear relationship between each predictor variable
    and the outcome variable, and then add these effects together linearly, along
    with the intercept. GAMs overcome the limitations of simply using splines in the
    general linear model by doing the following:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: GAMs自动学习每个预测变量和结果变量之间的非线性关系，然后将这些效应线性地加在一起，包括截距。GAMs通过以下方式克服了在一般线性模型中仅使用样条函数的局限性：
- en: Automatically selecting the knots for spline functions
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动选择样条函数的节点
- en: Automatically selecting the degree of flexibility of the smoothing functions
    by controlling the weights of the basis functions
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过控制基函数的权重来自动选择平滑函数的灵活性程度
- en: Allowing us to combine splines of multiple predictor variables simultaneously
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许我们同时组合多个预测变量的样条函数
- en: '|  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-415
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If I want to use linear modeling and the relationship between my predictors
    and outcome variable is nonlinear, GAMs are my go-to model. This is because of
    their flexibility and their ability to overcome the limitations of polynomial
    regression. The exception is if I have a theoretical reason to believe there is
    a specific polynomial relationship (say, quadratic) in the data. In such a situation,
    using linear regression with a polynomial term may result in a simpler model,
    where a GAM might overfit.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我想使用线性建模，并且我的预测变量和结果变量之间的关系是非线性的，那么GAMs是我的首选模型。这是因为它们的灵活性和克服多项式回归局限性的能力。例外情况是，如果我有理论上的理由相信数据中存在特定的多项式关系（比如，二次），在这种情况下，使用带有多项式项的线性回归可能会导致一个更简单的模型，而GAMs可能会过拟合。
- en: '|  |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 10.2.2\. How GAMs handle categorical variables
  id: totrans-418
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.2\. GAMs如何处理分类变量
- en: So far, I’ve shown you that GAMs learn nonlinear relationships between our predictor
    variables and our outcome. But what about when our predictor variables are categorical?
    Well, GAMs can handle categorical variables in two different ways.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我已经向你展示了GAMs如何学习预测变量和结果变量之间的非线性关系。但是，当我们的预测变量是分类变量时怎么办呢？嗯，GAMs可以通过两种不同的方式处理分类变量。
- en: One method is to treat categorical variables exactly the same way we do for
    the general linear model, and create *k* – 1 dummy variables that encode the effect
    of each level of the predictor on the outcome. When we use this method, the predicted
    value of a case is simply the sum of all of the smoothing functions, plus the
    contribution from the categorical variable effects. This method assumes independence
    between the categorical variable and the continuous variables (in other words,
    the smoothing functions are the same across each level of the categorical variable).
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是将分类变量处理得与一般线性模型完全相同，并为每个预测变量的每个水平创建* k* - 1个虚拟变量，以编码每个水平对结果的影响。当我们使用这种方法时，案例的预测值只是所有平滑函数的总和，加上分类变量效应的贡献。这种方法假设分类变量和连续变量之间是独立的（换句话说，平滑函数在分类变量的每个水平上都是相同的）。
- en: The other method is to model a separate smoothing function for each level of
    the categorical variable. This is important in situations where there are distinct
    nonlinear relationships between continuous variables and the outcome at each level
    of a categorical variable.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是为分类变量的每个水平建模一个单独的平滑函数。在连续变量和每个分类变量水平上的结果之间存在明显非线性关系的情况下，这一点很重要。
- en: '|  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-423
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: When specifying a GAM as our learner through mlr, the default method is the
    first approach.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过mlr指定GAM作为我们的学习器时，默认方法是第一种方法。
- en: '|  |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'GAMs are extraordinarily flexible and powerful for a huge range of machine
    learning problems. If you would like to delve deeper into the nuts and bolts of
    GAMs, I recommend *Generalized Additive Models: An Introduction with R* by Simon
    Wood (Chapman and Hall/CRC, 2017).'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: GAMs对于广泛的机器学习问题来说非常灵活且强大。如果您想深入了解GAMs的细节，我推荐Simon Wood的《广义加性模型：R语言导论》（Chapman
    and Hall/CRC，2017年）。
- en: I hope by now you have a basic understanding of polynomial regression and GAMs,
    so let’s turn this knowledge into skills by building your first nonlinear regression
    model!
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望到现在为止，您已经对多项式回归和GAMs有了基本的了解，所以让我们通过构建您的第一个非线性回归模型来将这种知识转化为技能！
- en: 10.3\. Building your first GAM
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3. 构建你的第一个GAM
- en: We finished [chapter 9](kindle_split_020.html#ch09) by interrogating the diagnostic
    plots of our linear regression model, and deciding it looked as though we have
    nonlinear relationships in the data. Therefore, in this section I’m going to show
    you how to model the data using a GAM, to account for the nonlinear relationships
    between the predictors and outcome.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过检查线性回归模型的诊断图完成了第9章的内容，并决定数据中似乎存在非线性关系。因此，在本节中，我将向您展示如何使用GAM（广义加性模型）来建模数据，以解释预测变量和结果之间的非线性关系。
- en: I’ll start with some feature engineering. From [figure 9.7](kindle_split_020.html#ch09fig07)
    in [chapter 9](kindle_split_020.html#ch09), it looks like there’s a curved relationship
    between `Month` and `Ozone`, peaking in summer and declining in winter. Because
    we also have access to the day of the month, let’s see if we can get a more predictive
    value by combining the two. Put another way, instead of getting month-of-the-year
    resolution, let’s get day-of-the-year resolution from our data.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从一些特征工程开始。从第9章[图9.7](kindle_split_020.html#ch09fig07)来看，`Month`和`Ozone`之间存在一种曲线关系，夏季达到峰值，冬季下降。因为我们也有月份的日信息，让我们看看是否可以通过结合这两个变量来获得更预测的价值。换句话说，我们不是从年月分辨率来获取数据，而是从年日分辨率来获取数据。
- en: To achieve this, we mutate a new column called `DayOfYear`. We use the `interaction()`
    function to generate a variable that contains the information from both the `Date`
    and `Month` variables. Because the `interaction()` function returns a factor,
    we wrap it inside the `as.numeric()` function to convert it into a numeric vector
    that represents the days of the year.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们创建了一个名为`DayOfYear`的新列。我们使用`interaction()`函数生成一个包含`Date`和`Month`变量信息的变量。因为`interaction()`函数返回一个因子，所以我们将其包裹在`as.numeric()`函数中，将其转换为表示年日的数值向量。
- en: '|  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: 'To get a better idea of what `interaction()` is doing, run the following:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解`interaction()`函数的作用，运行以下代码：
- en: '[PRE20]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Because the new variable contains the information from the `Date` and `Month`
    variables, we remove them from the data using the `select()` function—they are
    now redundant. We then plot our new variable to see how it relates to `Ozone`.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 因为新变量包含了`Date`和`Month`变量的信息，我们使用`select()`函数将它们从数据中删除——它们现在是多余的。然后我们绘制我们的新变量，以查看它与`Ozone`的关系。
- en: Listing 10.1\. Creating an interaction between `Date` and `Month`
  id: totrans-438
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.1\. 创建 `Date` 和 `Month` 之间的交互
- en: '[PRE21]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The resulting plot is shown in [figure 10.6](#ch10fig06). Aha! The relationship
    between ozone levels and the time of year is even clearer if we use day, instead
    of month, resolution.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在 [图 10.6](#ch10fig06) 中。啊哈！如果我们使用天而不是月来分辨，臭氧水平和年份之间的关系就更加清晰了。
- en: '|  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: 'Add another `geom_smooth()` layer to the plot, using these arguments to fit
    a quadratic polynomial line to the data:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中添加另一个 `geom_smooth()` 层，使用以下参数将二次多项式线拟合到数据：
- en: '`method = "lm"`'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`method = "lm"`'
- en: '`formula = "y ~` *x* `+ I(x^2)"`'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`formula = "y ~` *x* `+ I(x^2)"`'
- en: '`col = "red"`'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col = "red"`'
- en: Does this polynomial relationship fit the data well?
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多项式关系是否很好地拟合了数据？
- en: '|  |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 10.6\. Plotting the `DayOfYear` variable against ozone levels
  id: totrans-449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.6\. 将 `DayOfYear` 变量与臭氧水平绘制成图
- en: '![](fig10-6_alt.jpg)'
  id: totrans-450
  prefs: []
  type: TYPE_IMG
  zh: '![](fig10-6_alt.jpg)'
- en: Now let’s define our task, imputation wrapper, and feature-selection wrapper,
    just as we did for our linear regression model. Sadly, there isn’t yet an implementation
    of ordinary GAMs wrapped by mlr (such as from the mgcv package). Instead, however,
    we have access to the gamboost algorithm, which uses boosting (as you learned
    about in [chapter 8](kindle_split_018.html#ch08)) to learn an ensemble of GAM
    models. Therefore, for this exercise, we’ll use the `regr.gamboost` learner. Other
    than the different learner (`regr.gamboost` instead of `regr.lm`), we create our
    imputation and feature selection wrappers exactly the same way as in [listing
    9.13](kindle_split_020.html#ch09ex13).
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义我们的任务、插补包装器和特征选择包装器，就像我们为线性回归模型所做的那样。遗憾的是，mlr 的作者还没有实现普通 GAM 的包装（例如来自
    mgcv 包）。然而，我们仍然可以访问 gamboost 算法，该算法使用提升（正如你在第 8 章中了解的那样）来学习 GAM 模型的集成。因此，对于这个练习，我们将使用
    `regr.gamboost` 学习者。除了不同的学习者（`regr.gamboost` 而不是 `regr.lm`），我们创建插补和特征选择包装器的方式与
    [列表 9.13](kindle_split_020.html#ch09ex13) 中完全相同。
- en: Listing 10.2\. Defining the task and wrappers
  id: totrans-452
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.2\. 定义任务和包装器
- en: '[PRE22]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|  |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The authors of mlr wrote it to allow the incorporation of virtually any machine
    learning algorithm. If there is an algorithm from a package you want to use that
    isn’t yet wrapped by mlr, you can implement it yourself so that you can use mlr’s
    functionality with it. While doing so isn’t super-complicated, it does take a
    bit of explaining. Therefore, if you want to do this, I recommend following the
    mlr tutorial at [http://mng.bz/gV5x](http://mng.bz/gV5x), which does a good job
    of explaining the process.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: mlr 的作者编写它是为了允许几乎任何机器学习算法的集成。如果你想要使用尚未由 mlr 包装的包中的算法，你可以自己实现它，以便可以使用 mlr 的功能。虽然这样做并不超级复杂，但确实需要一些解释。因此，如果你想这样做，我建议遵循
    [http://mng.bz/gV5x](http://mng.bz/gV5x) 上的 mlr 教程，它很好地解释了整个过程。
- en: '|  |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: All that’s left to do is cross-validate the model-building process. Because
    the gamboost algorithm is much more computationally intense than linear regression,
    we’re only going to use `holdout` as the method for outer cross-validation.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作就是交叉验证模型构建过程。由于 gamboost 算法比线性回归计算量更大，所以我们只将 `holdout` 作为外部交叉验证的方法。
- en: '|  |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about 1.5 minutes to run on my four-core machine.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上运行大约需要 1.5 分钟。
- en: '|  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 10.3\. Cross-validating the GAM model-building process
  id: totrans-463
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.3\. 交叉验证 GAM 模型构建过程
- en: '[PRE23]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Great! Our cross-validation suggests that modeling the data using the gamboost
    algorithm will outperform a model learned by linear regression (the latter gave
    us a mean MSE of 22.8 in the previous chapter).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们的交叉验证表明，使用 gamboost 算法对数据进行建模将优于通过线性回归学习到的模型（后者在上一章中给出了平均 MSE 为 22.8）。
- en: Now let’s actually build a model so I can show you how to interrogate your GAM
    models to understand the nonlinear functions they’ve learned for your predictor
    variables.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实际上构建一个模型，这样我就可以向您展示如何调查您的 GAM 模型，以了解它们为预测变量学习到的非线性函数。
- en: '|  |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about 3 minutes to run on my four-core machine.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上运行大约需要 3 分钟。
- en: '|  |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 10.4\. Training a GAM
  id: totrans-471
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.4\. 训练 GAM
- en: '[PRE24]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: First, we train a boosted GAM using our `gamTask`. We can just use `gamFeatSelWrapper`
    as our learner, because this performs imputation and feature selection for us.
    To speed things along, we can parallelize the feature selection by running the
    `parallelStartSocket()` function before running the `train()` function to actually
    train the model.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`gamTask`训练一个增强型广义线性混合模型（GAM）。我们可以直接使用`gamFeatSelWrapper`作为我们的学习器，因为它会为我们执行插补和特征选择。为了加快速度，我们可以在运行`train()`函数实际训练模型之前，通过运行`parallelStartSocket()`函数来并行化特征选择。
- en: We then extract the model information using the `getLearnerModel()` function.
    This time, because our learner is a wrapper function, we need to supply an additional
    argument, `more.unwrap = TRUE`, to tell mlr that it needs to go all the way down
    through the wrappers to extract the base model information.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`getLearnerModel()`函数提取模型信息。这次，因为我们的学习器是一个包装函数，我们需要提供一个额外的参数，`more.unwrap
    = TRUE`，来告诉mlr它需要一路向下通过包装器来提取基础模型信息。
- en: Now, let’s understand our model a little better by plotting the functions it
    learned for each of the predictor variables. This is as easy as calling `plot()`
    on our model information. We can also look at the residuals from the model by
    extracting them with the `resid()` function. This allows us to plot the predicted
    values (by extracting the `$fitted()` component) against their residuals to look
    for patterns that suggest a poor fit. We can also plot the quantiles of the residuals
    against the quantiles of a theoretical normal distribution, using `qqnorm()` and
    `qqline()`, to see if they are normally distributed.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过绘制模型为每个预测变量学习到的函数来更好地理解我们的模型。这就像在我们的模型信息上调用`plot()`一样简单。我们还可以通过使用`resid()`函数提取残差来查看模型中的残差。这允许我们绘制预测值（通过提取`$fitted()`组件）与其残差的关系，以寻找表明拟合不良的模式。我们还可以使用`qqnorm()`和`qqline()`将残差的分位数与理论正态分布的分位数进行比较，以查看它们是否呈正态分布。
- en: Listing 10.5\. Plotting our GAM
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.5\. 绘制我们的GAM
- en: '[PRE25]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-479
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Because we’re about to create a subplot for every predictor, and two for the
    residuals, we first divide the plotting device into nine parts using the `mfrow`
    argument of the `par()` function. We set this back again using the same function.
    You may have a different number of predictors than I do, as returned from your
    feature selection.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们即将为每个预测变量创建一个子图，以及两个残差图，我们首先使用`par()`函数的`mfrow`参数将绘图设备分成九个部分。我们使用相同的函数将其恢复。您可能具有与我不同的预测变量数量，这是从您的特征选择中返回的。
- en: '|  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The resulting plot is shown in [figure 10.7](#ch10fig07). For each predictor,
    we get a plot of its value against how much that predictor contributes to the
    ozone estimate across its values. Lines show the shape of the functions learned
    by the algorithm, and we can see that they are all nonlinear.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图10.7](#ch10fig07)。对于每个预测变量，我们得到一个其值与该预测变量对其值范围内的臭氧估计贡献的图。线条显示了算法学习到的函数形状，我们可以看到它们都是非线性的。
- en: Figure 10.7\. Plotting the nonlinear relationships learned by our GAM. The rug
    at the base of each plot shows the position of each case along the x-axis. The
    residual vs. fitted plot (middle panel of the second row) shows a pattern suggestive
    of heteroscedasticity, and the normal Q-Q plot (right panel of the second row)
    shows the residuals are normally distributed.
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.7\. 绘制我们的GAM学习到的非线性关系。每个图底部的地毯显示了沿x轴的每个案例的位置。残差与拟合图（第二行的中间面板）显示了一个表明异方差性的模式，而正态Q-Q图（第二行的右侧面板）显示残差呈正态分布。
- en: '![](fig10-7_alt.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](fig10-7_alt.jpg)'
- en: '|  |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: The “rug” of tick marks at the base of each plot indicates the position of training
    cases. This helps us identify regions of each variable that have few cases, such
    as at the top end of the `Visib` variable. GAMs have the potential to overfit
    in regions with few cases.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图底部“地毯”状的刻度标记表示训练案例的位置。这有助于我们识别每个变量中案例数量较少的区域，例如在`Visib`变量的顶部。GAM在案例数量较少的区域有过度拟合的潜力。
- en: '|  |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Finally, looking at the residual plots, we can still see a pattern, which may
    indicate heteroscedasticity in the data. We could try training a model on a transformed
    `Ozone` variable (such as log[10]) to see if this helps, or use a model that doesn’t
    make this assumption. The quantile plot shows that most of the residuals lie close
    to the diagonal line, indicating that they approximate a normal distribution,
    with some deviation at the tails (which isn’t uncommon).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查看残差图，我们仍然可以看到一个模式，这可能表明数据中的异方差性。我们可以尝试在转换后的`Ozone`变量（如log[10]）上训练模型，看看这是否有帮助，或者使用不做出这种假设的模型。分位数图显示，大部分残差都靠近对角线，表明它们近似于正态分布，尾部有一些偏差（这是不常见的）。
- en: 10.4\. Strengths and weaknesses of GAMs
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4. GAMs的优势和劣势
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    GAMs will perform well for you.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪些算法会对给定的任务表现良好，但以下是一些优势和劣势，这将帮助您决定GAMs是否会对您表现良好。
- en: 'The strengths of GAMs are as follows:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: GAMs的优势如下：
- en: They produce models that are very interpretable, despite being nonlinear.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管是非线性的，但它们产生的模型非常可解释。
- en: They can handle both continuous and categorical predictors.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理连续和分类预测变量。
- en: They can automatically learn nonlinear relationships in the data.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以自动学习数据中的非线性关系。
- en: 'The weaknesses of GAMs are these:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: GAMs的劣势如下：
- en: They still make strong assumptions about the data, such as homoscedasticity
    and the distribution of residuals (performance may suffer if these are violated).
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们仍然对数据做出强烈的假设，例如同方差性和残差的分布（如果违反这些假设，性能可能会受到影响）。
- en: GAMs have a propensity to overfit the training set.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAMs倾向于过度拟合训练集。
- en: GAMs can be particularly poor at predicting data outside the range of values
    of the training set.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAMs在预测训练集值范围之外的数据时可能特别差。
- en: They cannot handle missing data.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们无法处理缺失数据。
- en: '|  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习3**'
- en: 'Just as in [exercise 3](kindle_split_020.html#ch09sb04) in [chapter 9](kindle_split_020.html#ch09),
    instead of using a wrapper method, cross-validate the process of building our
    GAM using a filter method. Are the estimated MSE values similar? Which method
    is faster? Tips:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第9章[练习3](kindle_split_020.html#ch09sb04)中所述，而不是使用包装器方法，使用filter方法交叉验证构建我们的GAM的过程。估计的MSE值是否相似？哪种方法更快？提示：
- en: First, create a filter wrapper, using `gamImputeWrapper` as the learner.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个filter包装器，使用`gamImputeWrapper`作为学习器。
- en: Define a hyperparameter space to tune `"fw.abs"` using `makeParamSet()`.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个超参数空间以调整`"fw.abs"`，使用`makeParamSet()`。
- en: Create a grid search definition using `makeTuneControlGrid()`.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`makeTuneControlGrid()`创建一个网格搜索定义。
- en: Define a tune wrapper that takes the filter wrapper as a learner and performs
    a grid search.
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个tune包装器，它接受filter包装器作为学习器并执行网格搜索。
- en: Use `resample()` to perform cross-validation, using the tune wrapper as the
    learner.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`resample()`执行交叉验证，使用tune包装器作为学习器。
- en: '|  |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Polynomial terms can be included in linear regression to model nonlinear relationships
    between the predictor variables and the outcome.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式项可以包含在线性回归中，以模拟预测变量和结果之间的非线性关系。
- en: Generalized additive models (GAMs) are supervised learners for regression problems
    that can handle continuous and categorical predictors.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义加性模型（GAMs）是用于回归问题的监督学习器，可以处理连续和分类预测变量。
- en: GAMs use the equation of a straight line, but allow nonlinear relationships
    between the predictor variables and the outcome.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAMs使用直线方程，但允许预测变量和结果之间存在非线性关系。
- en: The nonlinear functions learned by GAMs are often splines created from the sum
    of a series of basis functions.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAMs学习的非线性函数通常是通过对一系列基函数求和创建的样条函数。
- en: Solutions to exercises
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题解答
- en: 'Experiment with the `interaction()` function:'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用`interaction()`函数：
- en: '[PRE26]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Add a `geom_smooth()` layer, fitting a quadratic relationship to the data:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个`geom_smooth()`层，将二次关系拟合到数据中：
- en: '[PRE27]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Cross-validate building a GAM but using a filter method:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用filter方法交叉验证构建GAM：
- en: '[PRE28]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Chapter 11\. Preventing overfitting with ridge regression, LASSO, and elastic
    net
  id: totrans-522
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章. 使用岭回归、LASSO和弹性网络防止过度拟合
- en: '*This chapter covers*'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Managing overfitting in regression problems
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回归问题中管理过度拟合
- en: Understanding regularization
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解正则化
- en: Using the L1 and L2 norms to shrink parameters
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用L1和L2范数来收缩参数
- en: Our societies are full of checks and balances. In our political systems, parties
    balance each other (in theory) to find solutions that are at neither extreme of
    each other’s views. Professional areas, such as financial services, have regulatory
    bodies to prevent them from doing wrong and ensure that the things they say and
    do are truthful and correct. When it comes to machine learning, it turns out we
    can apply our own form of regulation to the learning process to prevent the algorithms
    from overfitting the training set. We call this regulation in machine learning
    *regularization*.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的社会充满了制衡。在我们的政治体系中，政党相互制衡（理论上）以找到既不是彼此观点的极端解决方案。专业领域，如金融服务，有监管机构来防止它们做错事，并确保他们所说的和所做的是真实和正确的。当涉及到机器学习时，我们发现我们可以将我们自己的形式应用于学习过程，以防止算法过度拟合训练集。我们将这种机器学习中的制衡称为*正则化*。
- en: 11.1\. What is regularization?
  id: totrans-528
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1. 什么是正则化？
- en: In this section, I’ll explain what regularization is and why it’s useful. Regularization
    (also sometimes called *shrinkage*) is a technique that prevents the parameters
    of a model from becoming too large and “shrinks” them toward 0\. The result of
    regularization is models that, when making predictions on new data, have less
    variance.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释正则化是什么以及为什么它有用。正则化（有时也称为*收缩*）是一种防止模型参数变得过大并将它们“缩小”到0的技术。正则化的结果是，当在新数据上做出预测时，模型具有更小的方差。
- en: '|  |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-531
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall that when we say a model has “less variance,” we mean it makes less-variable
    predictions on new data, because it is not as sensitive to the noise in the training
    set.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，当我们说一个模型有“更小的方差”时，我们的意思是它在新的数据上做出更稳定的预测，因为它对训练集中的噪声不太敏感。
- en: '|  |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'While we can apply regularization to most machine learning problems, it is
    most commonly used in linear modeling, where it shrinks the slope parameter of
    each predictor toward 0\. Three particularly well-known and commonly used regularization
    techniques for linear models are as follows:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以将正则化应用于大多数机器学习问题，但它最常用于线性建模，其中它将每个预测器的斜率参数缩小到0。以下三种特别著名且常用的线性模型正则化技术如下：
- en: Ridge regression
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归
- en: Least absolute shrinkage and selection operator (LASSO)
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小绝对收缩和选择算子（LASSO）
- en: Elastic net
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性网络
- en: These three techniques can be thought of as extensions to linear models that
    reduce overfitting. Because they shrink model parameters toward 0, they can also
    automatically perform feature selection by forcing predictors with little information
    to have no or negligible impact on predictions.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种技术可以被视为线性模型的扩展，用于减少过拟合。因为它们将模型参数缩小到0，它们还可以通过强制信息量少的预测器对预测没有或可忽略的影响来自动执行特征选择。
- en: '|  |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: When I say “linear modeling,” I’m referring to the modeling of data using the
    general linear model, generalized linear model, or generalized additive model
    that I showed you in [chapters 9](kindle_split_020.html#ch09) and [10](kindle_split_021.html#ch10).
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 当我说“线性建模”时，我指的是使用我在[第9章](kindle_split_020.html#ch09)和[第10章](kindle_split_021.html#ch10)中展示的广义线性模型、广义线性模型或广义加性模型对数据进行建模。
- en: '|  |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: By the end of this chapter, I hope you’ll have an intuitive understanding of
    what regularization is, how it works, and why it’s important. You’ll understand
    how ridge regression and LASSO work and how they’re useful, and how elastic net
    is a mixture of them both. Finally, you’ll build ridge regression, LASSO, and
    elastic net models, and use benchmarking to compare them to each other and to
    a linear regression model with no regularization.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望你能对正则化是什么、它是如何工作的以及为什么它很重要有一个直观的理解。你将了解岭回归和LASSO是如何工作的以及它们为什么有用，以及弹性网络是如何结合两者的。最后，你将构建岭回归、LASSO和弹性网络模型，并使用基准测试来比较它们以及与没有正则化的线性回归模型。
- en: 11.2\. What is ridge regression?
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2. 什么是岭回归？
- en: In this section, I’ll show you what ridge regression is, how it works, and why
    it’s useful. Take a look at the example in [figure 11.1](#ch11fig01), which I’ve
    reproduced from [chapter 3](kindle_split_013.html#ch03). I used this figure in
    [chapter 3](kindle_split_013.html#ch03) to show you what underfitting and overfitting
    look like for classification problems. When we underfit the problem, we partition
    the feature space in a way that doesn’t do a good job of capturing local differences
    near the decision boundary. When we overfit, we place too much importance on these
    local differences and end up with a decision boundary that captures much of the
    noise in the training set, resulting in an overly complex decision boundary.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示岭回归是什么，它是如何工作的，以及为什么它是有用的。请查看[图11.1](#ch11fig01)，这是我从[第3章](kindle_split_013.html#ch03)中复制的示例。我在[第3章](kindle_split_013.html#ch03)中使用这个图向您展示分类问题中欠拟合和过拟合的样子。当我们欠拟合问题时，我们会以一种无法很好地捕捉决策边界附近局部差异的方式划分特征空间。当我们过拟合时，我们过于重视这些局部差异，最终得到的决策边界捕捉了训练集中的大部分噪声，导致决策边界过于复杂。
- en: Figure 11.1\. Examples of underfitting, optimal fitting, and overfitting for
    a two-class classification problem. The dotted line represents a decision boundary.
    class classification problem. The dotted line represents a decision boundary.
  id: totrans-546
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1\. 对于二分类问题的欠拟合、最佳拟合和过拟合的示例。虚线代表决策边界。
- en: '![](fig11-1_alt.jpg)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-1_alt.jpg)'
- en: Now take a look at [figure 11.2](#ch11fig02), which shows an example of what
    underfitting and overfitting look like for regression problems. When we underfit
    the data, we miss local differences in the relationship and produce a model that
    has high bias (makes inaccurate predictions). When we overfit the data, our model
    is too sensitive to local differences in the relationship and has high variance
    (will make very variable predictions on new data).
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请看[图11.2](#ch11fig02)，它展示了欠拟合和过拟合在回归问题中的样子。当我们欠拟合数据时，我们会错过关系中的局部差异，并产生一个具有高偏差（做出不准确预测）的模型。当我们过拟合数据时，我们的模型对关系的局部差异过于敏感，具有高方差（将在新数据上做出非常变化的预测）。
- en: Figure 11.2\. Examples of underfitting, optimal fitting, and overfitting for
    a singlepredictor regression problem. The dotted line represents the regression
    line.
  id: totrans-549
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2\. 对于单预测回归问题的欠拟合、最佳拟合和过拟合的示例。虚线代表回归线。
- en: '![](fig11-2_alt.jpg)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-2_alt.jpg)'
- en: '|  |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-552
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The example I’ve used to labor this point is of a nonlinear relationship, but
    the sample applies to models of linear relationships too.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来阐述这个观点的例子是一个非线性关系，但这个例子也适用于线性关系模型。
- en: '|  |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'The principal job of regularization is to prevent algorithms from learning
    models that are overfit, by discouraging complexity. This is achieved by penalizing
    model parameters that are large, shrinking them toward 0\. This might sound counterintuitive:
    surely the model parameters learned by ordinary least squares (OLS from [chapter
    9](kindle_split_020.html#ch09)) are the best, as they minimize the residual error.
    The problem is that this is only necessarily true for the training set, and not
    the test set.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的主要任务是防止算法学习过拟合的模型，通过不鼓励复杂性来实现。这是通过惩罚大的模型参数，将它们缩小到0来实现的。这听起来可能有些反直觉：普通最小二乘法（OLS，见[第9章](kindle_split_020.html#ch09)）学习的模型参数无疑是最好的，因为它们最小化了残差误差。问题是，这只有在训练集上是必要的，而不是在测试集上。
- en: Consider the example in [figure 11.3](#ch11fig03). In the left-side plot, imagine
    that we only measured the two more darkly shaded cases. OLS would learn a line
    that passes through both cases, because this will minimize the sum of squares.
    We collect more cases in our study, and when we plot them on the right-side plot,
    we can see that the first model we trained doesn’t generalize well to the new
    data. This is due to *sampling error*, which is the difference between the distribution
    of data in our sample of cases and the distribution of data in the wider population
    we’re trying to make predictions on. In this (slightly contrived) case, because
    we only measured two cases, the sample doesn’t do a good job of representing the
    wider population, and we learned a model that overfit the training set.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图11.3中的例子。在左侧图中，假设我们只测量了两个较深阴影的案例。OLS会学习一条穿过这两个案例的线，因为这会最小化平方和。我们在研究中收集了更多的案例，当我们将它们绘制在右侧图中时，我们可以看到我们最初训练的模型对新数据推广得不好。这是由于*抽样误差*，即我们样本案例中的数据分布与我们要进行预测的更广泛群体中的数据分布之间的差异。在这个（稍微有些人为的）案例中，因为我们只测量了两个案例，样本并不能很好地代表更广泛的群体，我们学习到的模型过度拟合了训练集。
- en: This is where regularization comes in. While OLS will learn the model that best
    fits the training set, the training set probably isn’t perfectly representative
    of the wider population. Overfitting the training set is more likely to result
    in model parameters that are too large, so regularization adds a penalty to the
    least squares that grows bigger with larger estimated model parameters. This process
    usually adds a little bias to the model, because we’re intentionally underfitting
    the training set, but the reduction in model variance often results in a better
    model anyway. This is especially true in situations where the ratio of predictors
    to cases is large.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是正则化的作用所在。虽然OLS会学习最适合训练集的模型，但训练集可能并不能完美地代表更广泛的群体。过度拟合训练集更有可能导致模型参数过大，因此正则化会给最小二乘法添加一个惩罚项，该惩罚项随着估计的模型参数增大而增大。这个过程通常会给模型添加一点偏差，因为我们有意地欠拟合训练集，但模型方差减少通常会导致更好的模型。这在预测者与案例之间的比例很大的情况下尤其正确。
- en: Figure 11.3\. Sampling error leads to models that don’t generalize well to new
    data. In the left-side example, a regression line is fit, considering only the
    more darkly shaded cases. In the right-side example, all the cases are used to
    construct the regression line. The dotted lines help indicate that the magnitude
    of the slope is larger on the left side than on the right.
  id: totrans-558
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3. 抽样误差导致模型无法很好地推广到新数据。在左侧示例中，拟合回归线时只考虑了较深阴影的案例。在右侧示例中，使用了所有案例来构建回归线。虚线有助于表明左侧的斜率幅度大于右侧。
- en: '![](fig11-3.jpg)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![图片11-3](fig11-3.jpg)'
- en: '|  |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-561
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: How representative your dataset is of the wider population depends on carefully
    planning your data acquisition, avoiding introducing bias with experimental design
    (or identifying and correcting for it if the data already exists), and ensuring
    that your datasets are sufficiently large to learn real patterns. If your dataset
    poorly represents the wider population, no machine learning technique, including
    cross-validation, will be able to help you!
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据集代表更广泛群体的程度取决于你精心规划数据获取、避免在实验设计中引入偏差（或者如果数据已经存在，识别并纠正它），并确保你的数据集足够大，以便学习真实模式。如果你的数据集不能很好地代表更广泛的群体，没有任何机器学习技术，包括交叉验证，能够帮助你！
- en: '|  |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: So regularization can help prevent overfitting due to sampling error, but perhaps
    a more important use of regularization is in preventing the inclusion of spurious
    predictors. If we add predictors to an existing linear regression model, we’re
    likely to get better predictions on the training set. This might lead us (falsely)
    to believe we are creating a better model by including more predictors. This is
    sometimes called *kitchen-sink regression* (because everything goes in, including
    the kitchen sink). For example, imagine that you want to predict the number of
    people in a park on a given day, and you include the value of the FTSE 100 that
    day as a predictor. It’s unlikely (unless the park was near the London Stock Exchange,
    perhaps) that the value of the FTSE 100 has an influence on the number of people.
    Retaining this spurious predictor in the model has the potential to result in
    overfitting the training set. Because regularization will shrink this parameter,
    it will reduce the degree to which the model overfits the training set.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正则化可以帮助防止由于采样误差导致的过拟合，但正则化可能更为重要的用途是防止包含虚假预测因子。如果我们向现有的线性回归模型中添加预测因子，我们很可能会在训练集上得到更好的预测。这可能会让我们（错误地）相信，通过包含更多的预测因子，我们正在创建一个更好的模型。这有时被称为*厨房水槽回归*（因为所有东西都放进去，包括厨房水槽）。例如，想象一下，您想预测某一天公园的人数，并且您将那天FTSE
    100的值作为一个预测因子。除非公园靠近伦敦证券交易所，否则FTSE 100的值不太可能对公园的人数有影响。保留这个虚假预测因子在模型中可能会导致训练集过拟合。因为正则化会缩小这个参数，它将减少模型过拟合训练集的程度。
- en: 'Regularization can also help in situations that are *ill-posed*. An ill-posed
    problem in mathematics is one that does not satisfy these three conditions: having
    a solution, having a unique solution, and having a solution that depends on the
    initial conditions. In statistical modeling, a common ill-posed problem is when
    there is not one optimal parameter value, often encountered when the number of
    parameters is higher than the number of cases. In situations like this, regularization
    can make estimating the parameters a more stable problem.'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化也可以帮助解决那些*病态的*问题。数学中的病态问题是那些不满足以下三个条件的问题：有解、有唯一解以及解依赖于初始条件。在统计建模中，一个常见的病态问题是当参数的数量高于案例数量时，通常会遇到没有最优参数值的情况。在这种情况下，正则化可以使估计参数的问题更加稳定。
- en: 'What does this penalty look like that we add to the least squares estimate?
    Two penalties are frequently used: the L1 norm and the L2 norm. I’ll start by
    showing you what the L2 norm is and how it works, because this is the regularization
    method used in ridge regression. Then I’ll extend this to show you how LASSO uses
    the L1 norm method, and how elastic net combines both the L1 and L2 norms.'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加到最小二乘估计中的这种惩罚看起来是什么样子？常用的两种惩罚是L1范数和L2范数。我将首先向您展示L2范数是什么以及它是如何工作的，因为这是岭回归中使用的正则化方法。然后，我将扩展这个概念向您展示LASSO如何使用L1范数方法，以及弹性网络如何结合L1和L2范数。
- en: 11.3\. What is the L2 norm, and how does ridge regression use it?
  id: totrans-567
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3. L2范数是什么，岭回归是如何使用它的？
- en: In this section, I’ll show you a mathematical and graphical explanation of the
    L2 norm, how ridge regression uses it, and why you would use it. Imagine that
    you want to predict how busy your local park will be, depending on the temperature
    that day. An example of what this data might look like is shown in [figure 11.4](#ch11fig04).
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示L2范数的数学和图形解释，岭回归如何使用它，以及为什么您会使用它。想象一下，您想根据当天的温度预测您当地的公园会有多繁忙。这个数据可能看起来的一个例子如图11.4所示。
- en: Figure 11.4\. Calculating the sum of squares from a model that predicts the
    number of people in a park based on the temperature
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.4. 基于温度预测公园人数的模型计算平方和
- en: '![](fig11-4_alt.jpg)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-4_alt.jpg)'
- en: '|  |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-572
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: I realize that people may be reading this who are from countries that use Fahrenheit
    or Celsius to measure temperature, so I’ve shown the scale in Kelvin to irritate
    everyone equally.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 我意识到可能有人来自使用华氏度或摄氏度来测量温度的国家，所以我展示了开尔文尺度，让每个人都同样感到烦恼。
- en: '|  |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When using OLS, the residuals for a particular combination of intercept and
    slope are calculated for each case and squared. These squared residuals are then
    all added up to give the sum of squares. We can represent this in mathematical
    notation as in [equation 11.1](#ch11equ01).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用OLS时，对于每个截距和斜率的特定组合，都会计算每个案例的残差并平方。然后，将这些平方残差全部加起来，得到平方和。我们可以在数学符号中表示为[方程11.1](#ch11equ01)。
- en: equation 11.1\.
  id: totrans-576
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11.1.
- en: '![](eq11-1.jpg)'
  id: totrans-577
  prefs: []
  type: TYPE_IMG
  zh: '![方程11-1](eq11-1.jpg)'
- en: '*y[i]* is the value of the outcome variable for case *i*, and ŷ*[i]* is its
    value predicted by the model. This is the vertical distance of each case from
    the line. The Greek sigma ![](pg256.jpg) simply means that we calculate this vertical
    distance and square it for every case from the first one (*i* = 1) to the last
    one (*n*) and then add up all these values.'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '*y[i]*是案例*i*的因变量值，而ŷ*[i]*是模型预测的值。这是每个案例与线的垂直距离。希腊字母sigma ![](pg256.jpg)简单地意味着我们计算这个垂直距离并将其平方，对于从第一个案例(*i*
    = 1)到最后一个案例(*n*)的所有案例，然后将所有这些值加起来。'
- en: Mathematical functions that are minimized by machine learning algorithms to
    select the best combinations of parameters are called *loss functions*. Therefore,
    least squares is the loss function for the OLS algorithm.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法通过最小化参数的最佳组合来选择数学函数，这些函数被称为*损失函数*。因此，最小二乘法是OLS算法的损失函数。
- en: 'Ridge regression modifies the least squares loss function slightly to include
    a term that makes the function’s value larger, the larger the parameter estimates
    are. As a result, the algorithm now has to balance selecting the model parameters
    that minimize the sum of squares, and selecting parameters than minimize this
    new penalty. In ridge regression, this penalty is called the *L2 norm*, and it
    is very easy to calculate: we simply square all of the model parameters and add
    them up (all except the intercept). When we have only one continuous predictor,
    we have only one parameter (the slope), so the L2 norm is its square. When we
    have two predictors, we square the slopes for each and then add these squares
    together, and so on. This is illustrated for our park example in [figure 11.5](#ch11fig05).'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归稍微修改了最小二乘损失函数，以包括一个使函数值随着参数估计值的增大而增大的项。因此，算法现在必须平衡选择最小化平方和的模型参数，以及选择最小化这个新惩罚的参数。在岭回归中，这个惩罚被称为*L2范数*，它非常容易计算：我们只需将所有模型参数平方并相加（除了截距）。当我们只有一个连续预测因子时，我们只有一个参数（斜率），所以L2范数就是它的平方。当我们有两个预测因子时，我们分别平方每个斜率，然后将这些平方相加，依此类推。这在我们公园的例子中[图11.5](#ch11fig05)中得到了说明。
- en: Figure 11.5\. Calculating the sum of squares and the L2 norm for the slope between
    temperature and the number of people at the park.
  id: totrans-581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.5. 计算温度和公园人数之间的斜率的平方和与L2范数。
- en: '![](fig11-5_alt.jpg)'
  id: totrans-582
  prefs: []
  type: TYPE_IMG
  zh: '![图11-5](fig11-5_alt.jpg)'
- en: '|  |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-584
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Can you see that, in general, the more predictors a model has, the larger its
    L2 norm will be, because we are adding their squares together? Ridge regularization
    therefore penalizes models that are too complex (because they have too many predictors).
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看出，一般来说，模型拥有的预测因子越多，其L2范数就越大，因为我们正在将它们的平方相加吗？因此，岭回归正则化惩罚过于复杂的模型（因为它们有太多的预测因子）。
- en: '|  |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'So that we can control how much we want to penalize model complexity, we multiply
    the L2 norm by a value called *lambda* (λ, because Greek letters always sound
    cool). *Lambda* can be any value from 0 to infinity and acts as a volume knob:
    large values of *lambda* strongly penalize model complexity, while small values
    of *lambda* weakly penalize model complexity. *Lambda* cannot be estimated from
    the data, so it is a hyperparameter that we need to tune to achieve the best performance
    by cross-validation. Once we calculate the L2 norm and multiply it by *lambda*,
    we then add this product to the sum of squares to get our penalized least squares
    loss function.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们可以控制我们想要惩罚模型复杂性的程度，我们将L2范数乘以一个称为*lambda*（λ，因为希腊字母听起来总是很酷）的值。*Lambda*可以是0到无穷大之间的任何值，它充当一个音量旋钮：*lambda*的大值会强烈惩罚模型复杂性，而小值会弱化惩罚模型复杂性。*Lambda*不能从数据中估计出来，因此它是一个需要通过交叉验证调整的超参数，以实现最佳性能。一旦我们计算出L2范数并将其乘以*lambda*，然后我们再将这个乘积加到平方和中，以得到我们的惩罚最小二乘损失函数。
- en: '|  |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-589
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If we set *lambda* to 0, this removes the L2 norm penalty from the equation
    and we get back to the OLS loss function. If we set *lambda* to a very large value,
    all the slopes will shrink close to 0.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将*lambda*设为0，这将从方程中移除L2范数惩罚，我们就会回到OLS损失函数。如果我们将*lambda*设为一个非常大的值，所有斜率都会收缩到接近0。
- en: '|  |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If we’re mathematically minded, then we can represent this in mathematical notation
    as in [equation 11.2](#ch11equ02). Can you see that this is the same as the sum
    of squares as in [equation 11.1](#ch11equ01), but we’ve added the *lambda* and
    L2 norm terms?
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们数学思维，那么我们可以用数学符号表示，如[方程11.2](#ch11equ02)。你能看出这与[方程11.1](#ch11equ01)中的平方和相同，但我们增加了λ和L2范数项吗？
- en: equation 11.2\.
  id: totrans-593
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11.2。
- en: '![](eq11-2.jpg)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![](eq11-2.jpg)'
- en: So ridge regression learns a combination of model parameters that minimize this
    new loss function. Imagine a situation where we have many predictors. OLS might
    estimate a combination of model parameters that do a great job of minimizing the
    least squares loss function, but the L2 norm of this combination might be huge.
    In this situation, ridge regression would estimate a combination of parameters
    that have a slightly higher least squares value but a considerably lower L2 norm.
    Because the L2 norm gets smaller when model parameters are smaller, the slopes
    estimated by ridge regression will probably be smaller than those estimated by
    OLS.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，岭回归学习一组模型参数，以最小化这个新的损失函数。想象一下我们有很多预测因子的情况。OLS可能估计一组模型参数，这些参数在最小化平方损失函数方面做得很好，但这个组合的L2范数可能非常大。在这种情况下，岭回归将估计一组参数，这些参数的平方损失值略高，但L2范数显著降低。因为当模型参数较小时，L2范数会变小，所以岭回归估计的斜率可能会比OLS估计的斜率小。
- en: '|  |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Important
  id: totrans-597
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重要
- en: When using L2- or L1-penalized loss functions, it’s critical that the predictor
    variables are scaled first (divided by their standard deviation to put them on
    the same scale). This is because we are adding the squared slopes (in the case
    of L2 regularization), and this value is going to be considerably larger for predictors
    on larger scales (millimeters versus kilometers, for example). If we don’t scale
    the predictors first, they won’t all be given equal importance.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用L2或L1惩罚损失函数时，关键是要首先对预测变量进行缩放（除以它们的标准差，使它们处于相同的尺度）。这是因为我们在添加平方斜率（在L2正则化的情况下），这个值对于较大尺度的预测因子（例如毫米与千米）将会大得多。如果我们不首先缩放预测变量，它们将不会得到同等的重要性。
- en: '|  |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: If you prefer a more graphical explanation of the L2-penalized loss function
    (I know I do), take a look at [figure 11.6](#ch11fig06). The x- and y-axes show
    values for two slope parameters, (β[1] and β[2]). The shaded contour lines represent
    different sum of squares values for different combinations of the two parameters,
    where the combination resulting in the smallest sum of squares is at the center
    of the contours. The dashed circles centered at 0 represent the L2 norm multiplied
    by different values of *lambda*, for the combinations of β[1] and β[2] the dashed
    lines pass through.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢图形化的解释L2惩罚损失函数（我知道我确实如此），请查看[图11.6](#ch11fig06)。x轴和y轴显示两个斜率参数（β[1]和β[2]）的值。阴影轮廓线代表两个参数不同组合的不同平方和值，其中导致最小平方和的组合位于轮廓的中心。以0为中心的虚线圆代表乘以不同λ值的L2范数，对于β[1]和β[2]的组合，虚线穿过。
- en: Figure 11.6\. A graphical representation of the ridge regression penalty. The
    x- and y-axes represent the values of two model parameters. The solid, concentric
    circles represent the sum of squares value for different combinations of the parameters.
    The dashed circles represent the L2 norm multiplied by *lambda*.
  id: totrans-601
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.6。岭回归惩罚的图形表示。x轴和y轴代表两个模型参数的值。实心、同心圆代表参数不同组合的平方和值。虚线圆代表乘以λ值的L2范数。
- en: '![](fig11-6.jpg)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-6.jpg)'
- en: Notice that when *lambda* = 0, the circle passes through the combination of
    β[1] and β[2] that minimizes the sum of squares. When *lambda* is increased, the
    circle shrinks symmetrically toward 0\. Now the combination of parameters that
    minimizes the penalized loss function is the combination with the smallest sum
    of squares *that lies on the circle*. Put another way, the optimal solution when
    using ridge regression is always at the intersection of the circle and the ellipse
    around the OLS estimate. Can you see then that as we increase *lambda*, the circle
    shrinks and the selected combination of model parameters gets sucked toward 0?
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当λ = 0时，圆通过最小化平方和的β[1]和β[2]的组合。当λ增加时，圆对称地向0缩小。现在，最小化惩罚损失函数的参数组合是位于圆上的最小平方和组合。换句话说，使用岭回归的最优解总是在圆和围绕OLS估计的椭圆的交点处。你能看出随着λ的增加，圆缩小，所选的模型参数组合被吸向0吗？
- en: '|  |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-605
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: In this example, I’ve illustrated L2 regularization for two slope parameters.
    If we had only one slope, we would represent the same process on a number line.
    If we had three parameters, the same would apply in a three-dimensional space,
    and the penalty circle would become a penalty sphere. This continues in as many
    dimensions as you have non-intercept parameters (where the penalty becomes a hypersphere).
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我展示了 L2 正则化对两个斜率参数的应用。如果我们只有一个斜率，我们将在数轴上表示相同的过程。如果我们有三个参数，在三维空间中也会同样适用，惩罚圆将变成惩罚球体。这将继续适用于你拥有的非截距参数的维度数（其中惩罚变成超球体）。
- en: '|  |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: So, by using the L2-penalized loss function to learn the slope parameters, ridge
    regression prevents us from training models that overfit the training data.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用 L2 惩罚的损失函数来学习斜率参数，岭回归阻止我们训练过度拟合训练数据的模型。
- en: '|  |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-610
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The intercept isn’t included when calculating the L2 norm because it is defined
    as the value of the outcome variable when all the slope parameters are equal to
    0.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 L2 范数时，不包括截距，因为它是当所有斜率参数都等于 0 时结果变量的值。
- en: '|  |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 11.4\. What is the L1 norm, and how does LASSO use it?
  id: totrans-613
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4\. L1 范数是什么，LASSO 如何使用它？
- en: Now that you know about ridge regression, learning how LASSO works will be a
    simple extension of what you’ve already learned. In this section, I’ll show you
    what the L1 norm is, how it differs from the L2 norm, and how the least absolute
    shrinkage and selection operator (LASSO) uses it to shrink parameter estimates.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了岭回归，学习 LASSO 的工作原理将是你对已学知识的简单扩展。在本节中，我将向你展示 L1 范数是什么，它与 L2 范数有何不同，以及最小绝对收缩和选择算子（LASSO）如何使用它来收缩参数估计。
- en: Let’s remind ourselves what the L2 norm looks like, in [equation 11.3](#ch11equ03).
    Recall that we square the value of each of the slope parameters and add them all
    up. We then multiply this L2 norm by *lambda* to get the penalty we add to the
    sum of squares loss function.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下 L2 范数在 [方程式 11.3](#ch11equ03) 中的样子。回想一下，我们平方每个斜率参数的值并将它们全部加起来。然后我们将这个
    L2 范数乘以 *lambda* 以得到我们添加到平方损失函数总和的惩罚项。
- en: equation 11.3\.
  id: totrans-616
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 11.3\.
- en: '![](eq11-3.jpg)'
  id: totrans-617
  prefs: []
  type: TYPE_IMG
  zh: '![](eq11-3.jpg)'
- en: The L1 norm is only slightly different than the L2 norm. Instead of squaring
    the parameter values, we take their absolute value instead and *then* sum them.
    This is shown in [equation 11.4](#ch11equ04) by the vertical lines around β*[j]*.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: L1 范数与 L2 范数只有细微的差别。我们不是平方参数值，而是取它们的绝对值，然后求和。这通过 [方程式 11.4](#ch11equ04) 中的 β*[j]*
    周围的垂直线来表示。
- en: equation 11.4\.
  id: totrans-619
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 11.4\.
- en: '![](eq11-4.jpg)'
  id: totrans-620
  prefs: []
  type: TYPE_IMG
  zh: '![](eq11-4.jpg)'
- en: 'We then create the loss function for LASSO (the L1-penalized loss function)
    in exactly the same way we did for ridge regression: we multiply the L1 norm by
    *lambda* (which has the same meaning) and add it to the sum of squares. The L1-penalized
    loss function is shown in [equation 11.5](#ch11equ05). Notice that the only difference
    between this equation and [equation 11.2](#ch11equ02) is that we take the absolute
    value of the parameters before summing them, instead of squaring them. Say we
    had three slopes, one of which was negative: 2.2, –3.1, 0.8\. The L1 norm of these
    three slopes would be 2.2 + 3.1 + 0.8 = 6.1.'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与岭回归相同的方式创建 LASSO（L1 惩罚的损失函数）：我们将 L1 范数乘以 *lambda*（具有相同的意义）并将其添加到平方和。L1 惩罚的损失函数在
    [方程式 11.5](#ch11equ05) 中显示。注意，这个方程与 [方程式 11.2](#ch11equ02) 的唯一区别在于，我们在求和之前取参数的绝对值，而不是平方它们。假设我们有三个斜率，其中一个是负数：2.2，–3.1，0.8\.
    这三个斜率的 L1 范数将是 2.2 + 3.1 + 0.8 = 6.1。
- en: equation 11.5\.
  id: totrans-622
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 11.5\.
- en: '![](eq11-5.jpg)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![](eq11-5.jpg)'
- en: I can already hear you thinking, “So what? What’s the benefit/difference of
    using the L1 norm instead of the L2 norm?” Well, ridge regression can shrink parameter
    estimates toward 0, but they will never actually *be* 0 (unless the OLS estimate
    is 0 to begin with). So if you have a machine learning task where you believe
    all the variables should have some degree of predictive value, ridge regression
    is great because it won’t remove any variables. But what if you have a large number
    of variables and/or you want an algorithm that will perform feature selection
    for you? LASSO is helpful here because unlike ridge regression, LASSO *is* able
    to shrink small parameter values to 0, effectively removing that predictor from
    the model.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经能听到你在想，“那又怎样？使用L1范数而不是L2范数有什么好处/区别？”好吧，岭回归可以将参数估计值缩小到0，但它们永远不会真正**是**0（除非OLS估计一开始就是0）。所以如果你有一个机器学习任务，你相信所有变量都应该有一定的预测价值，岭回归是非常好的，因为它不会移除任何变量。但如果你有很多变量，或者你想要一个能够为你进行特征选择的算法呢？LASSO在这里很有帮助，因为与岭回归不同，LASSO**确实**能够将小的参数值缩小到0，从而有效地从模型中移除那个预测因子。
- en: Let’s represent this graphically the same way we did for ridge regression. [Figure
    11.7](#ch11fig07) shows the contours of the sum of squares for the same two imaginary
    parameters as in [figure 11.6](#ch11fig06). Instead of forming a circle, the LASSO
    penalty forms a square, rotated 45^o such that its vertices lie along the axes
    (I guess you could call this a diamond). Can you see that, for the same *lambda*
    as in our ridge regression example, the combination of parameters with the smallest
    sum of squares that touches the diamond is one where parameter β[2] is 0? This
    means the predictor represented by this parameter has been removed from the model.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以与岭回归相同的方式图形化地表示这一点。[图11.7](#ch11fig07)显示了与[图11.6](#ch11fig06)中相同的两个假设参数的平方和的等高线。LASSO惩罚形成的是一个正方形，旋转了45度，使得其顶点沿着轴（我想你可以称之为一个菱形）。你能看到，对于与我们的岭回归示例中相同的*lambda*，接触菱形的具有最小平方和的参数组合是参数β[2]为0的那个吗？这意味着代表这个参数的预测因子已经被从模型中移除了。
- en: Figure 11.7\. A graphical representation of the LASSO penalty. The x- and y-axes
    represent the values of two model parameters. The solid, concentric circles represent
    the sum of squares value for different combinations of the parameters. The dashed
    diamonds represent the L2 norm multiplied by *lambda.*
  id: totrans-626
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.7\. LASSO惩罚的图形表示。x轴和y轴代表两个模型参数的值。实心、同心圆代表参数的不同组合的平方和值。虚线菱形代表乘以*lambda*的L2范数。
- en: '![](fig11-7.jpg)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-7.jpg)'
- en: '|  |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-629
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If we had three parameters, we could represent the LASSO penalty as a cube (with
    its vertices aligned with the axes). It’s hard to visualize this in more than
    three dimensions, but the LASSO penalty would be a hypercube.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有三个参数，我们可以将LASSO惩罚表示为一个立方体（其顶点与轴对齐）。在超过三个维度中可视化这一点是困难的，但LASSO惩罚将是一个超立方体。
- en: '|  |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Just to make this extra clear, I’ve overlaid the LASSO and ridge penalties in
    [figure 11.8](#ch11fig08), including dotted lines that highlight the parameter
    values chosen by each method.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更加清晰，我在[图11.8](#ch11fig08)中叠加了LASSO和岭回归的惩罚，包括突出显示每种方法选择的参数值的虚线。
- en: 11.5\. What is elastic net?
  id: totrans-633
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5\. 弹性网络是什么？
- en: 'In this section, I’ll show you what elastic net is and how it mixes L2 and
    L1 regularization to find a compromise between ridge regression and LASSO parameter
    estimates. Sometimes you may have a prior justification for why you wish to use
    ridge regression or LASSO. If it’s important that you include all your predictors
    in the model, however small their contribution, use ridge regression. If you want
    the algorithm to perform feature selection for you by shrinking uninformative
    slopes to 0, use LASSO. More often than not, though, the decision between ridge
    regression and LASSO isn’t a clear one. In such situations, *don’t* choose between
    them: use elastic net, instead.'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示弹性网络是什么，以及它是如何混合L2和L1正则化，在岭回归和LASSO参数估计之间找到一个折衷方案的。有时你可能有一个先验的理由来解释为什么你想使用岭回归或LASSO。然而，如果你认为必须将所有预测因子包括在模型中，无论它们的贡献有多小，那么使用岭回归。如果你想算法通过将无信息斜率缩小到0来为你进行特征选择，那么使用LASSO。但通常情况下，岭回归和LASSO之间的选择并不是一个明确的选择。在这种情况下，**不要**在它们之间做出选择：使用弹性网络。
- en: Figure 11.8\. Comparing the ridge regression and LASSO penalties
  id: totrans-635
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.8\. 比较岭回归和LASSO惩罚
- en: '![](fig11-8.jpg)'
  id: totrans-636
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-8.jpg)'
- en: '|  |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-638
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: One important limitation of LASSO is that if you have more predictors than cases,
    it will select at most a number of predictors equal to the number of cases in
    the data. Put another way, if your dataset contains 100 predictors and 50 cases,
    LASSO will set the slopes of at least 50 predictors to 0!
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO的一个重要限制是，如果你有比案例更多的预测变量，它最多会选择与数据中案例数量相等的预测变量数量。换句话说，如果你的数据集包含100个预测变量和50个案例，LASSO将至少将50个预测变量的斜率设置为0！
- en: '|  |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Elastic net is an extension of linear modeling that includes both L2 *and* L1
    regularization in its loss function. It finds a combination of parameter estimates
    somewhere between those found by ridge regression and LASSO. We’re also able to
    control just how much importance we place on the L2 versus the L1 norms using
    the hyperparameter *alpha*.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性网络是线性建模的扩展，其损失函数中包括L2和L1正则化。它找到岭回归和LASSO找到的参数估计值之间的组合。我们还可以使用超参数*alpha*来控制我们对L2范数和L1范数的重视程度。
- en: 'Take a look at [equation 11.6](#ch11equ06). We multiply the L2 norm by 1 –
    α, multiply the L1 norm by α, and add up these values. We multiply this value
    by *lambda* and add it to the sum of squares. *Alpha* here can take any value
    between 0 and 1:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下[方程11.6](#ch11equ06)。我们乘以L2范数和1-α，乘以L1范数和α，然后将这些值相加。我们将这个值乘以*lambda*并加到平方和上。*Alpha*在这里可以取0到1之间的任何值：
- en: When *alpha* is 0, the L1 norm becomes 0, and we get ridge regression.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*alpha*为0时，L1范数变为0，我们得到岭回归。
- en: When *alpha* is 1, the L2 norm becomes 0, and we get LASSO.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*alpha*为1时，L2范数变为0，我们得到LASSO。
- en: When *alpha* is between 0 and 1, we get a mixture of ridge regression and LASSO.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当*alpha*介于0和1之间时，我们得到岭回归和LASSO的混合。
- en: How do we choose *alpha*? We don’t! We tune it as a hyperparameter and let cross-validation
    choose the best-performing value for us.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择*alpha*？我们不做！我们将其作为超参数进行调整，让交叉验证为我们选择最佳性能的值。
- en: equation 11.6\.
  id: totrans-647
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11.6.
- en: '![](eq11-6_alt.jpg)'
  id: totrans-648
  prefs: []
  type: TYPE_IMG
  zh: '![](eq11-6_alt.jpg)'
- en: If you’re more mathematically inclined, the full elastic net loss function is
    shown in [equation 11.7](#ch11equ07). If you’re not mathematically inclined, feel
    free to skip over this; but if you look carefully, I’m sure you’ll be able to
    see how the elastic net loss function combines the ridge and LASSO loss functions.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更倾向于数学，完整的弹性网络损失函数在[方程11.7](#ch11equ07)中显示。如果你不倾向于数学，请随意跳过；但如果你仔细看，我相信你一定能看到弹性网络损失函数是如何结合岭回归和LASSO损失函数的。
- en: equation 11.7\.
  id: totrans-650
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程11.7.
- en: '![](eq11-7_alt.jpg)'
  id: totrans-651
  prefs: []
  type: TYPE_IMG
  zh: '![](eq11-7_alt.jpg)'
- en: Prefer a graphical explanation? Yep, me too. [Figure 11.9](#ch11fig09) compares
    the shapes of the ridge, LASSO, and elastic net penalties. Because the elastic
    net penalty is somewhere between the ridge and LASSO penalties, it looks like
    a square with rounded sides.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 倾向于图形解释？是的，我也是。[图11.9](#ch11fig09)比较了岭回归、LASSO和弹性网络惩罚的形状。因为弹性网络惩罚介于岭回归和LASSO惩罚之间，它看起来像一个边角圆润的方形。
- en: Figure 11.9\. Comparing the shape of the ridge regression, LASSO, and elastic
    net penalties
  id: totrans-653
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.9. 比较岭回归、LASSO和弹性网络惩罚的形状
- en: '![](fig11-9.jpg)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-9.jpg)'
- en: So why might we prefer elastic net over ridge regression or LASSO? Well, elastic
    net can shrink parameter estimates to 0, allowing it to perform feature selection
    like LASSO. But it also circumvents LASSO’s limitation of not being able to select
    more variables than there are cases. Another limitation of LASSO is that if there
    is a group of predictors that are correlated with each other, LASSO will only
    select one of the predictors. Elastic net, on the other hand, is able to retain
    the group of predictors.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么我们可能更倾向于弹性网络而不是岭回归或LASSO呢？嗯，弹性网络可以将参数估计值缩小到0，使其能够像LASSO一样进行特征选择。但它也避免了LASSO无法选择比案例更多的变量的限制。LASSO的另一个限制是，如果有一组预测变量彼此相关，LASSO只会选择其中一个预测变量。另一方面，弹性网络能够保留这组预测变量。
- en: For these reasons, I usually dive straight in with elastic net as my regularization
    method of choice. Even if pure ridge or LASSO will result in the best-performing
    model, the ability to tune *alpha* as a hyperparameter still allows the possibility
    of selecting ridge or LASSO, although the optimal solution is usually somewhere
    between them. An exception to this is when we have prior knowledge about the effect
    of the predictors we’ve included in our model. If we have very strong domain knowledge
    that predictors ought to be included in the model, then we may have a preference
    for ridge regression. Conversely, if we have a strong prior belief that there
    are variables that probably don’t contribute anything (but we don’t know which),
    we may prefer LASSO.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，我通常直接选择弹性网络作为我的正则化方法。即使纯岭回归或 LASSO 会产生性能最佳的模型，调整 *alpha* 作为超参数的能力仍然允许选择岭回归或
    LASSO，尽管最佳解通常位于它们之间。一个例外是当我们对我们模型中包含的预测变量的影响有先验知识时。如果我们有非常强的领域知识，认为预测变量应该包含在模型中，那么我们可能更喜欢岭回归。相反，如果我们有一个强烈的先验信念，认为有一些变量可能没有任何贡献（但我们不知道是哪些），我们可能更喜欢
    LASSO。
- en: I hope I’ve conveyed how regularization can be used to extend linear models
    to avoid overfitting. You should now also have a conceptual understanding of ridge
    regression, LASSO, and elastic net, so let’s turn concepts into experience by
    training a model of each!
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望我已经传达了如何使用正则化来扩展线性模型以避免过拟合。现在你也应该对岭回归、LASSO 和弹性网络有一个概念性的理解，所以让我们通过训练每个模型来将概念转化为经验！
- en: 11.6\. Building your first ridge, LASSO, and elastic net models
  id: totrans-658
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.6\. 构建你的第一个岭回归、LASSO 和弹性网络模型
- en: 'In this section, we’re going to build ridge, LASSO, and elastic net models
    on the same dataset, and use benchmarking to compare how they perform against
    each other and against a vanilla (unregularized) linear model. Imagine that you’re
    trying to estimate the market price of wheat for the coming year in Iowa. The
    market price depends on the yield for that particular year, so you’re trying to
    predict the yield of wheat from rain and temperature measurements. Let’s start
    by loading the mlr and tidyverse packages:'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用相同的数据集构建岭回归、LASSO 和弹性网络模型，并使用基准测试来比较它们之间的性能以及与普通（未正则化）线性模型的性能。想象一下，你正在尝试估算爱荷华州下一年度的市场小麦价格。市场价格取决于那一年的产量，因此你正在尝试通过降雨量和温度测量来预测小麦产量。让我们首先加载
    mlr 和 tidyverse 包：
- en: '[PRE29]'
  id: totrans-660
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 11.6.1\. Loading and exploring the Iowa dataset
  id: totrans-661
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.6.1\. 加载和探索 Iowa 数据集
- en: Now let’s load the data, which is built into the lasso2 package, convert it
    into a tibble (with `as_tibble()`), and explore it.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载数据，该数据内置在 lasso2 包中，将其转换为 tibble（使用 `as_tibble()`），并对其进行探索。
- en: '|  |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-664
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You may need to install the lasso2 package first with `install.packages ("lasso2")`.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要首先使用 `install.packages("lasso2")` 安装 lasso2 包。
- en: '|  |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We have a tibble containing only 33 cases and 10 variables of various rainfall
    and temperature measurements, the year, and the wheat yield.
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含仅 33 个案例和 10 个变量的 tibble，这些变量包括各种降雨量和温度测量值、年份和小麦产量。
- en: Listing 11.1\. Loading and exploring the `Iowa` dataset
  id: totrans-668
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.1\. 加载和探索 `Iowa` 数据集
- en: '[PRE30]'
  id: totrans-669
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s plot the data to get a better understanding of the relationships within
    it. We’ll use our usual trick of gathering the data so we can facet by each variable,
    supplying `"free_x"` as the `scales` argument to allow the x-axis to vary between
    facets. To get an indication as to any linear relationships with `Yield`, I also
    applied a `geom_smooth` layer, using `"lm"` as the argument to `method` to get
    linear fits.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据以更好地理解其中的关系。我们将使用我们常用的技巧来收集数据，以便我们可以按每个变量进行分面，将 `"free_x"` 作为 `scales`
    参数，以允许 x 轴在分面之间变化。为了得到与 `Yield` 之间任何线性关系的指示，我还应用了一个 `geom_smooth` 层，使用 `"lm"`
    作为 `method` 参数以获取线性拟合。
- en: Listing 11.2\. Plotting the data
  id: totrans-671
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.2\. 绘制数据
- en: '[PRE31]'
  id: totrans-672
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The resulting plot is shown in [figure 11.10](#ch11fig10). It looks like some
    of the variables correlate with `Yield`; but notice that because we don’t have
    a large number of cases, the slopes of some of these relationships could drastically
    change if we only removed a couple of cases near the extremes of the x-axis. For
    example, would the slope between `Rain2` and `Yield` be nearly as steep if we
    hadn’t measured those three cases with the highest rainfall? We’re going to need
    regularization to prevent overfitting for this dataset.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表显示在[图11.10](#ch11fig10)中。看起来一些变量与`Yield`相关联；但请注意，因为我们没有大量的案例，如果我们只移除x轴极端附近的一两个案例，这些关系中的某些斜率可能会急剧变化。例如，如果我们没有测量那三个降雨量最高的案例，`Rain2`和`Yield`之间的斜率会接近那么陡峭吗？我们需要正则化来防止这个数据集过拟合。
- en: Figure 11.10\. Plotting each of the predictors against wheat yield for the `Iowa`
    dataset. Lines represent linear model fits between each predictor and yield.
  id: totrans-674
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.10\. 将每个预测变量与`Iowa`数据集中的小麦产量进行绘图。线条代表每个预测变量和产量之间的线性模型拟合。
- en: '![](fig11-10_alt.jpg)'
  id: totrans-675
  prefs: []
  type: TYPE_IMG
  zh: '![图11-10替代](fig11-10_alt.jpg)'
- en: 11.6.2\. Training the ridge regression model
  id: totrans-676
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.6.2\. 训练岭回归模型
- en: In this section, I’ll walk you through training a ridge regression model to
    predict `Yield` from our `Iowa` dataset. We’ll tune the *lambda* hyperparameter
    and train a model using its optimal value.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将指导你训练一个岭回归模型来从我们的`Iowa`数据集中预测`Yield`。我们将调整*lambda*超参数，并使用其最佳值来训练模型。
- en: 'Let’s define our task and learner, this time supplying `"regr.glmnet"` as the
    argument to `makeLearner()`. Handily, the `glmnet` function (from the package
    of the same name) allows us to create ridge, LASSO, and elastic net models using
    the same function. Notice that we set the value of *alpha* equal to 0 here. This
    is how we specify that we want to use pure ridge regression with the `glmnet`
    function. We also supply an argument that you haven’t seen before: `id`. The `id`
    argument just lets us supply a unique name to every learner. The reason we need
    this now is that later in the chapter, we’re going to benchmark our ridge, LASSO,
    and elastic net learners against each other. Because we create each of these with
    the same `glmnet` function, we’ll get an error because they won’t each have a
    unique identifier.'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们的任务和学习器，这次将`"regr.glmnet"`作为`makeLearner()`的参数。方便的是，`glmnet`函数（来自同名包）允许我们使用相同的函数创建岭回归、LASSO和弹性网络模型。请注意，我们在这里将*alpha*的值设为0。这就是我们如何指定我们想要使用纯岭回归的`glmnet`函数。我们还提供了一个你之前没有见过的参数：`id`。`id`参数只是让我们为每个学习器提供一个唯一的名称。我们需要这个参数的原因是，在本章的后面部分，我们将基准测试我们的岭回归、LASSO和弹性网络学习器。因为我们使用相同的`glmnet`函数创建它们，所以我们会得到一个错误，因为它们不会有唯一的标识符。
- en: Listing 11.3\. Creating the task and learner
  id: totrans-679
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.3\. 创建任务和学习器
- en: '[PRE32]'
  id: totrans-680
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Let’s get an idea of how much each predictor would contribute to a model’s ability
    to predict `Yield`. We can use the `generateFilterValuesData()` and `plotFilterValues()`
    functions we used in [chapter 9](kindle_split_020.html#ch09) when performing feature
    selection using the filter method.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解每个预测变量对模型预测`Yield`能力的贡献程度。我们可以使用我们在[第9章](kindle_split_020.html#ch09)中使用的`generateFilterValuesData()`和`plotFilterValues()`函数，当时我们使用过滤方法进行特征选择。
- en: Listing 11.4\. Generating and plotting filter values
  id: totrans-682
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.4\. 生成和绘制过滤值
- en: '[PRE33]'
  id: totrans-683
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The resulting plot is shown in [figure 11.11](#ch11fig11). We can see that `Year`
    contains the most predictive information about `Yield`; `Rain3`, `Rain1`, and
    `Rain0` seem to contribute very little; and `Temp1` seems to make a negative contribution,
    suggesting that including it in the model will be to the detriment of predictive
    accuracy.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表显示在[图11.11](#ch11fig11)中。我们可以看到`Year`包含关于`Yield`的最多的预测信息；`Rain3`、`Rain1`和`Rain0`似乎贡献很小；而`Temp1`似乎有负贡献，这表明将其包含在模型中将损害预测精度。
- en: But we’re not going to perform feature selection. Instead, we’re going to enter
    all the predictors and let the algorithm shrink the ones that contribute less
    to the model. The first thing we need to do is tune the *lambda* hyperparameter
    that controls just how big a penalty to apply to the parameter estimates.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不会执行特征选择。相反，我们将输入所有预测变量，并让算法缩小对模型贡献较小的那些变量。我们需要做的第一件事是调整控制参数估计惩罚大小的*lambda*超参数。
- en: '|  |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-687
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: Remember that when *lambda* equals 0, we are applying no penalty and get the
    OLS parameter estimates. The larger *lambda* is, the more the parameters are shrunk
    toward 0.
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当*lambda*等于0时，我们正在应用无惩罚并得到OLS参数估计。*lambda*越大，参数就越会被压缩到0。
- en: '|  |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 11.11\. Plotting the result of `generateFilterValuesData()`. Bar height
    represents how much information each predictor contains about wheat yield.
  id: totrans-690
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.11。绘制`generateFilterValuesData()`的结果。条形高度表示每个预测变量包含多少关于小麦产量的信息。
- en: '![](fig11-11_alt.jpg)'
  id: totrans-691
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-11_alt.jpg)'
- en: We’ll start by defining the hyperparameter space we’re going to search to find
    the optimal value of *lambda*. Recall that to do this, we use the `makeParamSet()`
    function, supplying each of our hyperparameters to search, separated by commas.
    Because we only have one hyperparameter to tune, and because *lambda* can take
    any numeric value between 0 and infinity, we use the `makeNumericParam()` function
    to specify that we want to search for numeric values of *lambda* between 0 and
    15.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义我们要搜索的超参数空间，以找到*lambda*的最佳值。回想一下，为了做到这一点，我们使用`makeParamSet()`函数，将每个要搜索的超参数通过逗号分隔开来。因为我们只有一个超参数需要调整，而且因为*lambda*可以取0到无穷大之间的任何数值，所以我们使用`makeNumericParam()`函数来指定我们想要搜索0到15之间的*lambda*的数值。
- en: '|  |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-694
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that I’ve called the hyperparameter `"s"` instead of `"lambda"`. If you
    run `getParamSet(ridge)`, you will indeed see a tunable hyperparameter called
    *lambda*, so what’s with the `"s"`? The authors of glmnet helpfully wrote it so
    that it will build models for a range of *lambda*s for us. Then we can plot the
    *lambda*s to see which one gives the best cross-validated performance. This is
    handy, but seeing as we’re using mlr as a universal interface to many machine
    learning packages, it makes sense for us to tune *lambda* ourselves the way we’re
    used to. The glmnet *lambda* hyperparameter is used for specifying a *sequence*
    of *lambda* values to try, and the authors specifically recommend *not* supplying
    a single value for this hyperparameter. Instead, the *s* hyperparameter is used
    to train a model with a single, specific *lambda*, so this is what we will tune
    when using mlr. For more information, I suggest reading the documentation for
    glmnet by running `?glmnet::glmnet`.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我使用的是超参数`"s"`而不是`"lambda"`。如果你运行`getParamSet(ridge)`，你确实会看到一个名为*lambda*的可调整超参数，那么`"s"`是怎么回事呢？glmnet的作者们很贴心地这样写，以便为我们构建一系列*lambda*值的模型。然后我们可以绘制*lambda*值，看看哪一个给出了最佳的交叉验证性能。这很方便，但鉴于我们正在使用mlr作为许多机器学习包的通用接口，我们以我们习惯的方式自己调整*lambda*是有意义的。glmnet的*lambda*超参数用于指定要尝试的*lambda*值的*序列*，作者们特别建议不要为这个超参数提供一个单一值。相反，*s*超参数用于训练一个使用单个、特定*lambda*的模型，因此这就是我们在使用mlr时将要调整的内容。有关更多信息，我建议通过运行`?glmnet::glmnet`来阅读glmnet的文档。
- en: '|  |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Next, let’s define our search method as a random search with 200 iterations
    using `makeTuneControlRandom()`, and define our cross-validation method as 3-fold
    cross-validation repeated 5 times, using `makeResampleDesc()`. Finally, we run
    our hyperparameter tuning process with the `tuneParams()` function. To speed things
    up a little, let’s use `parallelStartSocket()` to parallelize the search.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用`makeTuneControlRandom()`定义我们的搜索方法为随机搜索，200次迭代，并定义我们的交叉验证方法为重复5次的3折交叉验证，使用`makeResampleDesc()`。最后，我们使用`tuneParams()`函数运行我们的超参数调整过程。为了加快速度，让我们使用`parallelStartSocket()`来并行化搜索。
- en: '|  |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-699
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about 30 seconds on my four-core machine.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上大约需要30秒。
- en: '|  |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 11.5\. Tuning the *lambda* (*s*) hyperparameter
  id: totrans-702
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.5。调整*lambda* (*s*)超参数
- en: '[PRE34]'
  id: totrans-703
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Our tuning process selected 6.04 as the best-performing *lambda* (yours might
    be a little different due to the random search). But how can we be sure we searched
    over a large enough range of *lambda*s? Let’s plot each value of *lambda* against
    the mean MSE of its models and see if it looks like there may be a better value
    outside of our search space (greater than 15).
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的选择过程选择了6.04作为最佳性能的*lambda*（由于随机搜索，你的可能略有不同）。但我们如何确保我们搜索了足够大的*lambda*值范围？让我们绘制每个*lambda*值与其模型平均MSE的对应关系，看看是否可能存在搜索空间之外（大于15）的更好值。
- en: First, we extract the *lambda* and mean MSE values for each iteration of the
    random search by supplying our tuning object as the argument to the `generateHyperParsEffectData()`
    function. Then, we supply this data as the first argument of the `plotHyperParsEffect()`
    function and tell it we want to plot the values of *s* on the x-axis and the mean
    MSE (`"mse.test.mean"`) on the y-axis, and that we want a line that connects the
    data points.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过将我们的调整对象作为 `generateHyperParsEffectData()` 函数的参数，提取每次随机搜索的 *lambda* 和均方误差
    (MSE) 值。然后，我们将这些数据作为 `plotHyperParsEffect()` 函数的第一个参数，并告诉它我们想在 x 轴上绘制 *s* 的值，在
    y 轴上绘制均方误差 ("mse.test.mean")，并且我们想要一条连接数据点的线。
- en: Listing 11.6\. Plotting the hyperparameter tuning process
  id: totrans-706
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.6\. 绘制超参数调整过程
- en: '[PRE35]'
  id: totrans-707
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The resulting plot is shown in [figure 11.12](#ch11fig12). We can see that the
    MSE is minimized for *lambda*s between 5 and 6, and it seems that increasing *lambda*
    beyond 6 results in models that perform worse. If the MSE seemed to be still decreasing
    at the edge of our search space, we would need to expand the search in case we’re
    missing better hyperparameter values. Because we appear to be at the minimum,
    we’re going to stop our search here.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在 [图 11.12](#ch11fig12) 中。我们可以看到，当 *lambda* 在 5 和 6 之间时，MSE 最小化，并且看起来当
    *lambda* 超过 6 时，模型的表现会变差。如果 MSE 在搜索空间的边缘似乎仍在下降，我们就需要扩大搜索范围，以防我们错过了更好的超参数值。因为我们看起来已经达到了最小值，所以我们将在这里停止搜索。
- en: Figure 11.12\. Plotting the ridge regression *lambda*-tuning process. The x-axis
    represents *lambda*, and the y-axis represents the mean MSE. Dots represent values
    of *lambda* sampled by the random search. The line connects the dots.
  id: totrans-709
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.12\. 绘制岭回归 *lambda* 调整过程。x 轴代表 *lambda*，y 轴代表均方误差。点代表随机搜索中采样的 *lambda*
    值。线连接这些点。
- en: '![](fig11-12_alt.jpg)'
  id: totrans-710
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-12_alt.jpg)'
- en: '|  |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-712
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Maybe I’ve been too hasty, because it’s possible we are only in a *local minimum*,
    the smallest MSE value compared to the values of *lambda* around it. When searching
    a hyperparameter space, there may be many local minima (plural of *minimum*);
    but we really want to find the *global minimum*, which is the lowest MSE value
    across all possible hyperparameter values. For example, imagine that if we keep
    increasing *lambda*, the MSE gets higher but then starts to come down again, forming
    a hill. It’s possible that this hill continues to decrease even more than the
    minimum shown in [figure 11.12](#ch11fig12). Therefore, it’s a good idea to really
    search your hyperparameter space well to try to find that global minimum.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我太急躁了，因为我们可能只是处于一个 *局部最小值*，即与周围 *lambda* 值相比最小的 MSE 值。在搜索超参数空间时，可能会有许多局部最小值（*minimum*
    的复数形式）；但我们真正想要找到的是 *全局最小值*，这是所有可能的超参数值中的最低 MSE 值。例如，想象一下，如果我们继续增加 *lambda*，MSE
    会先升高然后开始下降，形成一个山丘。这个山丘可能比 [图 11.12](#ch11fig12) 中显示的最小值下降得更多。因此，真正搜索超参数空间以尝试找到那个全局最小值是一个好主意。
- en: '|  |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-715
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 1**'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: Repeat the tuning process, but this time expand the search space to include
    values of *s* between 0 and 50 (don’t overwrite anything). Did our original search
    find a local minimum or the global minimum?
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 重复调整过程，但这次将搜索空间扩大到包括 0 到 50 之间的 *s* 值（不要覆盖任何内容）。我们的原始搜索是否找到了局部最小值或全局最小值？
- en: '|  |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Okay, now that we think we’ve selected the best-performing value of *lambda*,
    let’s train a model using that value. First, we use the `setHyperPars()` function
    to define a new learner using our tuned *lambda* value. Then, we use the `train()`
    function to train the model on our `iowaTask`.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，既然我们认为我们已经选择了最佳性能的 *lambda* 值，那么我们就用这个值来训练一个模型。首先，我们使用 `setHyperPars()` 函数定义一个新的学习器，使用我们调整过的
    *lambda* 值。然后，我们使用 `train()` 函数在 `iowaTask` 上训练模型。
- en: Listing 11.7\. Training a ridge regression model using the tuned *lambda*
  id: totrans-720
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.7\. 使用调整过的 *lambda* 训练岭回归模型
- en: '[PRE36]'
  id: totrans-721
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: One of the main motivations for using linear models is that we can interpret
    the slopes to get an idea of how much the outcome variable changes with each predictor.
    So let’s extract the parameter estimates from our ridge regression model. First,
    we extract the model data using the `getLearnerModel()` function. Then, we use
    the `coef()` function (short for *coefficients*) to extract the parameter estimates.
    Note that because of the way glmnet works, we need to supply the value of *lambda*
    to get the parameters for that model.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性模型的主要动机之一是我们可以通过解释斜率来了解结果变量随着每个预测变量的变化情况。所以让我们从岭回归模型中提取参数估计。首先，我们使用`getLearnerModel()`函数提取模型数据。然后，我们使用`coef()`函数（简称*系数*）来提取参数估计。请注意，由于glmnet的工作方式，我们需要提供*lambda*的值来获取该模型的参数。
- en: When we print `ridgeCoefs`, we get a matrix containing the name of each parameter
    and its slope. The intercept is the estimated `Yield` when all the predictors
    are 0\. Of course, it doesn’t make much sense to have negative wheat yield, but
    because it doesn’t make sense for all the predictors to be 0 (such as the year),
    we won’t interpret this. We’re more interested in interpreting the slopes, which
    are reported on the predictor’s original scale. We can see that for every additional
    year, wheat yield increased by 0.533 bushels per acre. For a one-inch increase
    in `Rain1`, wheat yield *decreased* by 0.703, and so on.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印`ridgeCoefs`时，我们得到一个包含每个参数名称及其斜率的矩阵。截距是当所有预测变量都为0时的估计`Yield`。当然，负小麦产量没有多少意义，但因为我们不能让所有预测变量都为0（例如年份），所以我们不会对此进行解释。我们更感兴趣的是解释斜率，这些斜率报告在预测变量的原始尺度上。我们可以看到，对于每增加一年，小麦产量每英亩增加0.533蒲式耳。对于`Rain1`每增加一英寸，小麦产量*减少*了0.703，等等。
- en: '|  |'
  id: totrans-724
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-725
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall that I mentioned how important it is to scale our predictors so that
    they are weighted equally when calculating the L1 and/or L2 norms. Well, glmnet
    does this for us by default, using its `standardize = TRUE` argument. This is
    handy, but it’s important to remember that the parameter estimates are transformed
    back onto the variables’ original scale.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我提到过如何重要地对我们预测变量进行缩放，以便在计算L1和/或L2范数时它们被同等加权。嗯，glmnet默认为我们做了这件事，使用其`standardize
    = TRUE`参数。这很方便，但重要的是要记住，参数估计被转换回变量的原始尺度。
- en: '|  |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 11.8\. Extracting the model parameters
  id: totrans-728
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.8\. 提取模型参数
- en: '[PRE37]'
  id: totrans-729
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Let’s plot these parameter estimates against the estimates from unregularized
    linear regression, so you can see the effect of parameter shrinkage. First, we
    need to train a linear model using OLS. We could do this with mlr, but as we’re
    not going to do anything fancy with this model, we can create one quickly using
    the `lm()` function. The first argument to `lm()` is the formula `Yield ~ .`,
    which means `Yield` is our outcome variable, and we want to model it (`~`) using
    all other variables in the data (`.`). We tell the function where to find the
    data, and wrap the whole `lm()` function inside the `coef()` function to extract
    its parameter estimates.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这些参数估计与无正则化线性回归估计的对比图，这样你可以看到参数收缩的影响。首先，我们需要使用OLS训练一个线性模型。我们可以使用mlr来做这件事，但因为我们不会对这个模型做任何复杂的事情，我们可以快速使用`lm()`函数创建一个。`lm()`的第一个参数是公式`Yield
    ~ .`，这意味着`Yield`是我们的结果变量，我们希望使用数据中的所有其他变量（`~`）来对其进行建模。我们告诉函数在哪里找到数据，并将整个`lm()`函数包裹在`coef()`函数中，以提取其参数估计。
- en: 'Next, we create a tibble containing three variables:'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含三个变量的tibble：
- en: The parameter names
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数名称
- en: The ridge regression parameter values
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归参数值
- en: The `lm` parameter values
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lm`参数值'
- en: Because we want to exclude the intercepts, we use `[-1]` to subset all the parameters
    except the first one (the intercept).
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想排除截距，所以我们使用`[-1]`来选择除了第一个参数（截距）之外的所有参数。
- en: So that we can facet by model, we `gather()` the data and then plot it using
    `ggplot()`. Because it’s nice to see things in ascending or descending order,
    we supply `reorder(Coef, Beta)`, which will use the `Coef` variable as the x aesthetic
    ordered by the `Beta` variable. By default, `geom_bar()` tries to plot frequencies,
    but because we want bars to represent the actual value of each parameter, we set
    the `stat = "identity"` argument.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够按模型进行分解，我们使用`gather()`函数收集数据，然后使用`ggplot()`函数进行绘图。因为按升序或降序查看事物很方便，所以我们提供了`reorder(Coef,
    Beta)`，这将使用`Coef`变量作为x美学，按`Beta`变量排序。默认情况下，`geom_bar()`试图绘制频率，但因为我们希望条形图代表每个参数的实际值，所以我们设置了`stat
    = "identity"`参数。
- en: Listing 11.9\. Plotting the model parameters
  id: totrans-737
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.9\. 绘制模型参数
- en: '[PRE38]'
  id: totrans-738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The resulting plot is shown in [figure 11.13](#ch11fig13). In the left facet,
    we have the parameter estimates for the unregularized model; and in the right
    facet, we have the estimates for our ridge regression model. Can you see that
    most of the ridge regression parameters (though not all) are smaller than those
    for the unregularized model? This is the effect of regularization.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表显示在[图11.13](#ch11fig13)中。在左侧部分，我们有未正则化模型的参数估计；在右侧部分，我们有岭回归模型的估计。你能看到大多数岭回归参数（尽管不是全部）比未正则化模型的参数要小吗？这就是正则化的效果。
- en: '|  |'
  id: totrans-740
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: Create another plot exactly the same as in [figure 11.13](#ch11fig13), but this
    time *include* the intercepts. Are they the same between the two models? Why?
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 创建另一个与[图11.13](#ch11fig13)完全相同的图表，但这次*包含*截距。它们在两个模型中是否相同？为什么？
- en: '|  |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 11.13\. Comparing the parameter estimates of our ridge regression model
    to our OLS regression model
  id: totrans-744
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.13\. 比较我们的岭回归模型和OLS回归模型的参数估计
- en: '![](fig11-13_alt.jpg)'
  id: totrans-745
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-13_alt.jpg)'
- en: 11.6.3\. Training the LASSO model
  id: totrans-746
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.6.3\. 训练LASSO模型
- en: In this section, we’ll repeat the model-building process of the previous section,
    but using LASSO instead. Once we’ve trained our model, we’ll add to our figure,
    so we can compare parameter estimates between the models, to give you a better
    understanding of how the techniques differ.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重复上一节的模型构建过程，但这次使用LASSO。一旦我们训练了模型，我们将将其添加到我们的图中，这样我们就可以比较模型之间的参数估计，以便更好地理解这些技术之间的差异。
- en: 'We start by defining the LASSO learner, this time setting *alpha* equal to
    1 (to make it pure LASSO). And we give the learner an ID, which we’ll use when
    we benchmark the models later:'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义LASSO学习器，这次将*alpha*设置为1（使其成为纯LASSO）。我们给学习器一个ID，稍后我们将用它来基准测试模型：
- en: '[PRE39]'
  id: totrans-749
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now, let’s tune *lambda* as we did before for ridge regression.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们像之前对岭回归那样调整*lambda*。
- en: '|  |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-752
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about 30 seconds on my four-core machine.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上大约需要30秒。
- en: '|  |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 11.10\. Tuning *lambda* for LASSO
  id: totrans-755
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.10\. 调整LASSO的*lambda*
- en: '[PRE40]'
  id: totrans-756
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now we plot the tuning process to see if we need to expand our search.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们绘制调整过程，以查看是否需要扩大搜索范围。
- en: Listing 11.11\. Plotting the hyperparameter tuning process
  id: totrans-758
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.11\. 绘制超参数调整过程
- en: '[PRE41]'
  id: totrans-759
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The resulting plot is shown in [figure 11.14](#ch11fig14). Once again, we can
    see that the selected value of *lambda* falls at the bottom of the valley of mean
    MSE values. Notice that the mean MSE flat-lines after *lambda* values of 10: this
    is because the penalty is so large here that all the predictors have been removed
    from the model, and we get the mean MSE of an intercept-only model.'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表显示在[图11.14](#ch11fig14)中。再次，我们可以看到所选的*lambda*值位于平均均方误差值低谷的底部。请注意，当*lambda*值达到10时，平均均方误差变得平坦：这是因为这里的惩罚如此之大，以至于所有预测变量都被从模型中移除，我们得到了仅包含截距项的模型的平均均方误差。
- en: Figure 11.14\. Plotting the LASSO *lambda*-tuning process. The x-axis represents
    *lambda*, and the y-axis represents the mean MSE. Dots represent values of *lambda*
    sampled by the random search. The line connects the dots.
  id: totrans-761
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.14\. 绘制LASSO *lambda*调整过程。x轴代表*lambda*，y轴代表平均均方误差。点代表随机搜索采样的*lambda*值。线连接这些点。
- en: '![](fig11-14_alt.jpg)'
  id: totrans-762
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-14_alt.jpg)'
- en: Let’s train a LASSO model using our tuned value of *lambda*.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用调整后的*lambda*值训练一个LASSO模型。
- en: Listing 11.12\. Training a LASSO model using the tuned *lambda*
  id: totrans-764
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.12\. 使用调整后的*lambda*训练LASSO模型
- en: '[PRE42]'
  id: totrans-765
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now let’s look at the parameter estimates from our tuned LASSO model and see
    how they compare to the ridge and OLS estimates. Once again, we use the `getLearnerModel()`
    function to extract the model data and then the `coef()` function to extract the
    parameter estimates. Notice something unusual? Three of our parameter estimates
    are just dots. Well, those dots actually represent 0.0\. Zilch. Nada. Nothing.
    The slopes of these parameters in the dataset have been set to exactly 0\. This
    means they have been removed from the model completely. This is how LASSO can
    be used for performing feature selection.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看调整后的LASSO模型的参数估计，并看看它们与岭回归和OLS估计相比如何。再次，我们使用`getLearnerModel()`函数提取模型数据，然后使用`coef()`函数提取参数估计。有什么不寻常的地方吗？我们的三个参数估计只是点。实际上，这些点代表0.0。什么都没有。零。什么都没有。这些参数在数据集中的斜率被设置为正好为0。这意味着它们已经被完全从模型中移除。这就是LASSO可以用于执行特征选择的方法。
- en: Listing 11.13\. Extracting the model parameters
  id: totrans-767
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.13\. 提取模型参数
- en: '[PRE43]'
  id: totrans-768
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Let’s plot these parameter estimates alongside those from our ridge and OLS
    models to give a more graphical comparison. To do this, we simply add a new column
    to our `coefTib` tibble using `$LASSO`; it contains the parameter estimates from
    our LASSO model (excluding the intercept). We then gather this data so we can
    facet by model, and plot it as before using `ggplot()`.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些参数估计与我们的岭回归和OLS模型的参数估计并排放置，以进行更直观的比较。为此，我们只需在`coefTib` tibble中添加一个新的列，使用`$LASSO`；它包含我们的LASSO模型的参数估计（不包括截距）。然后我们收集这些数据，以便我们可以按模型进行细分，并使用`ggplot()`像以前一样绘制它。
- en: Listing 11.14\. Plotting the model parameters
  id: totrans-770
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.14\. 绘制模型参数
- en: '[PRE44]'
  id: totrans-771
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The resulting plot is shown in [figure 11.15](#ch11fig15). The plot nicely highlights
    the difference between ridge, which shrinks parameters toward 0 (but never actually
    to 0), and LASSO, which can shrink parameters to exactly 0.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图11.15](#ch11fig15)中。该图很好地突出了岭回归（将参数缩小到0，但永远不会真正缩小到0）和LASSO（可以将参数缩小到正好为0）之间的差异。
- en: Figure 11.15\. Comparing the parameter estimates of our ridge regression model,
    LASSO model, and OLS regression model
  id: totrans-773
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.15\. 比较我们的岭回归模型、LASSO模型和OLS回归模型的参数估计
- en: '![](fig11-15_alt.jpg)'
  id: totrans-774
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-15_alt.jpg)'
- en: 11.6.4\. Training the elastic net model
  id: totrans-775
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.6.4\. 训练弹性网络模型
- en: 'This section is going to look a lot like the previous two, but I’ll show you
    how to train an elastic net model by tuning both *lambda* and *alpha*. We’ll start
    by creating an elastic net learner; this time we won’t supply a value of *alpha*,
    because we’re going to tune it to find the best trade-off between L1 and L2 regularization.
    We also give it an ID that we can use later when benchmarking:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节将非常类似于前两个章节，但我将向您展示如何通过调整*lambda*和*alpha*来训练弹性网络模型。我们将首先创建一个弹性网络学习器；这次我们不会提供一个*alpha*的值，因为我们打算调整它以找到L1和L2正则化之间的最佳权衡。我们还给它一个ID，我们可以在以后进行基准测试时使用：
- en: '[PRE45]'
  id: totrans-777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now let’s define the hyperparameter space we’re going to tune over, this time
    including *alpha* as a numeric hyperparameter bounded between 0 and 1\. Because
    we’re now tuning two hyperparameters, let’s increase the number of iterations
    of our random search to get a little more coverage of the search space. Finally,
    we run the tuning process as before and print the optimal result.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义我们将要调整的超参数空间，这次包括*alpha*作为一个介于0和1之间的数值超参数。因为我们现在正在调整两个超参数，所以让我们增加随机搜索的迭代次数，以获得对搜索空间的更多覆盖。最后，我们像以前一样运行调整过程并打印出最佳结果。
- en: '|  |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Warning
  id: totrans-780
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about a minute on my four-core machine.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上大约需要一分钟。
- en: '|  |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 11.15\. Tuning *lambda* and *alpha* for elastic net
  id: totrans-783
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.15\. 调整弹性网络中的*lambda*和*alpha*
- en: '[PRE46]'
  id: totrans-784
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now let’s plot our tuning process to confirm that our search space was large
    enough. This time, because we are tuning two hyperparameters simultaneously, we
    supply *lambda* and *alpha* as the x- and y-axes, and mean MSE (`"mse.test.mean"`)
    as the z-axis. Setting the `plot.type` argument equal to `"heatmap"` will draw
    a heatmap where the color is mapped to whatever we set as the z-axis. For this
    to work, though, we need to fill in the gaps between our 1,000 search iterations.
    To do this, we supply the name of any regression algorithm to the `interpolate`
    argument. Here, I’ve used `"regr.kknn"`, which uses k-nearest neighbors to fill
    in the gaps based on the MSE values of the nearest search iterations. We add a
    single `geom_point` to the plot to indicate the combination of *lambda* and *alpha*
    that were selected by our tuning process.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制我们的调整过程以确认我们的搜索空间是否足够大。这次，因为我们同时调整两个超参数，我们将*lambda*和*alpha*作为x轴和y轴，将平均均方误差（`"mse.test.mean"`）作为z轴。将`plot.type`参数设置为`"heatmap"`将绘制一个热图，其中颜色映射到我们设置为z轴的任何内容。但是，为了使其工作，我们需要填充我们1,000次搜索迭代之间的空白。为此，我们将任何回归算法的名称提供给`interpolate`参数。在这里，我使用了`"regr.kknn"`，它使用k最近邻根据最近的搜索迭代的MSE值来填充空白。我们向图中添加一个`geom_point`来指示由我们的调整过程选择的*lambda*和*alpha*的组合。
- en: '|  |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-787
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: This interpolation is for visualization only, so while choosing different interpolation
    learners may change the tuning plot, it won’t affect our selected hyperparameters.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 这种插值仅用于可视化，因此选择不同的插值学习器可能会改变调整图，但不会影响我们选择的超参数。
- en: '|  |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 11.16\. Plotting the tuning process
  id: totrans-790
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.16\. 绘制调整过程
- en: '[PRE47]'
  id: totrans-791
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The resulting plot is shown in [figure 11.16](#ch11fig16). Beautiful! You could
    hang this on your wall and call it art. Notice that the selected combination of
    *lambda* and *alpha* (the white dot) falls in a valley of mean MSE values, suggesting
    our hyperparameter search space was wide enough.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在[图 11.16](#ch11fig16)中。太棒了！你可以把它挂在墙上，称之为艺术。注意，选择的*lambda*和*alpha*组合（白色点）落在平均均方误差值的山谷中，这表明我们的超参数搜索空间足够宽。
- en: Figure 11.16\. Plotting the hyperparameter tuning process for our elastic net
    model. The x-axis represents *lambda*, the y-axis represents *alpha*, and the
    shading represents mean MSE. The white dot represents the combination of hyperparameters
    chosen by our tuning process.
  id: totrans-793
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.16\. 绘制我们的弹性网络模型超参数调整过程。x轴代表*lambda*，y轴代表*alpha*，阴影代表平均均方误差。白色点代表我们的调整过程选择的超参数组合。
- en: '![](fig11-16_alt.jpg)'
  id: totrans-794
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-16_alt.jpg)'
- en: '|  |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: Let’s experiment with the `plotHyperParsEffect()` function. Change the `plot.type`
    argument to `"contour"`, add the argument `show.experiments = TRUE`, and redraw
    the plot. Next, change `plot.type` to `"scatter"`, remove the `interpolate` and
    `show .experiments` arguments, and remove the `scale_fill_gradientn()` layer.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实验一下`plotHyperParsEffect()`函数。将`plot.type`参数更改为`"contour"`，添加参数`show.experiments
    = TRUE`，并重新绘制图表。接下来，将`plot.type`更改为`"scatter"`，移除`interpolate`和`show .experiments`参数，并移除`scale_fill_gradientn()`层。
- en: '|  |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now let’s train the final elastic net model using our tuned hyperparameters.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用调整后的超参数来训练最终的弹性网络模型。
- en: Listing 11.17\. Training an elastic net model using tuned hyperparameters
  id: totrans-800
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.17\. 使用调整后的超参数训练弹性网络模型
- en: '[PRE48]'
  id: totrans-801
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Next, we can extract the model parameters and plot them alongside the other
    three models, as we did in [listings 11.9](#ch11ex09) and [11.14](#ch11ex14).
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以提取模型参数，并将它们与其他三个模型一起绘制，就像我们在[列表 11.9](#ch11ex09)和[11.14](#ch11ex14)中所做的那样。
- en: Listing 11.18\. Plotting the model parameters
  id: totrans-803
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.18\. 绘制模型参数
- en: '[PRE49]'
  id: totrans-804
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The resulting plot is shown in [figure 11.17](#ch11fig17). Notice that our elastic
    net model’s parameter estimates are something of a compromise between those estimated
    by ridge regression and those estimated by LASSO. The elastic net model’s parameters
    are more similar to those estimated by pure LASSO, however, because our tuned
    value of *alpha* was close to 1 (remember that when *alpha* equals 1, we get pure
    LASSO).
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在[图 11.17](#ch11fig17)中。注意，我们的弹性网络模型的参数估计介于岭回归估计和LASSO估计之间。然而，由于我们的调整值*alpha*接近1（记住当*alpha*等于1时，我们得到纯LASSO），弹性网络模型的参数与纯LASSO估计的参数更相似。
- en: '|  |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 4**'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4**'
- en: Redraw the plot in [figure 11.17](#ch11fig17), but remove the `facet_wrap()`
    layer and set the position argument of `geom_bar()` equal to `"dodge"`. Which
    visualization do you prefer?
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: 重新绘制[图 11.17](#ch11fig17)中的图表，但移除`facet_wrap()`层，并将`geom_bar()`的位置参数设置为`"dodge"`。你更喜欢哪种可视化？
- en: '|  |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 11.17\. Comparing the parameter estimates of our ridge regression model,
    LASSO model, elastic net model, and OLS regression model
  id: totrans-810
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.17\. 比较我们的岭回归模型、LASSO模型、弹性网络模型和OLS回归模型的参数估计
- en: '![](fig11-17_alt.jpg)'
  id: totrans-811
  prefs: []
  type: TYPE_IMG
  zh: '![](fig11-17_alt.jpg)'
- en: 11.7\. Benchmarking ridge, LASSO, elastic net, and OLS against each other
  id: totrans-812
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.7\. 将岭回归、LASSO、弹性网络和OLS相互基准测试
- en: Let’s use benchmarking to simultaneously cross-validate and compare the performance
    of our ridge, LASSO, elastic net, and OLS modeling processes. Recall from [chapter
    8](kindle_split_018.html#ch08) that benchmarking takes a list of learners, a task,
    and a cross-validation procedure. Then, for each iteration/fold of the cross-validation
    process, a model is trained using each learner on the same training set, and evaluated
    on the same test set. Once the entire cross-validation process is complete, we
    get the mean performance metric (MSE, in this case) for each learner, allowing
    us to compare which would perform best.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用基准测试来同时交叉验证并比较我们的岭回归、LASSO、弹性网络和OLS建模过程的表现。回想一下[第 8 章](kindle_split_018.html#ch08)，基准测试需要一个学习者列表、一个任务和一个交叉验证过程。然后，对于交叉验证过程的每个迭代/折，使用每个学习者在相同的训练集上训练一个模型，并在相同的测试集上评估。一旦整个交叉验证过程完成，我们得到每个学习者的平均性能指标（在这种情况下是均方误差），这使我们能够比较哪个表现最好。
- en: Listing 11.19\. Plotting the model parameters
  id: totrans-814
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.19\. 绘制模型参数
- en: '[PRE50]'
  id: totrans-815
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We start by defining tuning wrappers for each learner so we can include hyperparameter
    tuning inside our cross-validation loop. For each wrapper (one each for ridge,
    LASSO, and elastic net), we supply the learner, cross-validation strategy, the
    parameter space for that learner, and the search procedure for that learner (notice
    that we use a difference search procedure for elastic net). OLS regression doesn’t
    need hyperparameter tuning, so we don’t make a wrapper for it. Because the `benchmark()`
    function requires a list of learners, we next create a list of these wrappers
    (and `"regr.lm"`, our OLS regression learner).
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为每个学习器定义调整包装器，这样我们就可以在我们的交叉验证循环中包含超参数调整。对于每个包装器（分别为Ridge、LASSO和弹性网络），我们提供学习器、交叉验证策略、该学习器的参数空间以及该学习器的搜索过程（注意，我们为弹性网络使用不同的搜索过程）。OLS回归不需要超参数调整，所以我们不为它创建包装器。因为`benchmark()`函数需要一个学习器列表，所以我们接下来创建一个这些包装器的列表（以及`"regr.lm"`，我们的OLS回归学习器）。
- en: To run the benchmarking experiment, let’s define our outer resampling strategy
    to be 3-fold cross-validation. After starting parallelization, we run the benchmarking
    experiment by supplying the list of learners, task, and outer cross-validation
    strategy to the `benchmark()` experiment.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行基准实验，让我们定义我们的外部重采样策略为3折交叉验证。在启动并行化后，我们通过向`benchmark()`实验提供学习器列表、任务和外部交叉验证策略来运行基准实验。
- en: '|  |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-819
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This took almost 6 minutes on my four-core machine.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上几乎花费了6分钟。
- en: '|  |'
  id: totrans-821
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 11.20\. Plotting the model parameters
  id: totrans-822
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.20\. 绘制模型参数
- en: '[PRE51]'
  id: totrans-823
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Perhaps surprisingly, ridge and LASSO regression both outperformed elastic net,
    although all three regularization techniques outperformed OLS regression. Because
    elastic net has the potential to select both pure ridge or pure LASSO (based on
    the value of the *alpha* hyperparameter), increasing the number of iterations
    of the random search could end up putting elastic net on top.
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 比起弹性网络，Ridge和LASSO回归都表现更好，尽管所有三种正则化技术都比OLS回归表现更好。因为弹性网络有可能选择纯Ridge或纯LASSO（基于*alpha*超参数的值），增加随机搜索的迭代次数可能会导致弹性网络处于领先地位。
- en: 11.8\. Strengths and weaknesses of ridge, LASSO, and elastic net
  id: totrans-825
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.8\. Ridge、LASSO和弹性网络的优势与劣势
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    ridge regression, LASSO, and elastic net will perform well for you.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于给定的任务通常不容易判断哪些算法会表现良好，但以下是一些优势和劣势，可以帮助你决定Ridge回归、LASSO和弹性网络是否适合你。
- en: 'The strengths of ridge, LASSO, and elastic net are as follows:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge、LASSO和弹性网络的优势如下：
- en: They produce models that are very interpretable.
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们产生的模型非常易于解释。
- en: They can handle both continuous and categorical predictors.
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理连续和分类预测器。
- en: They are computationally inexpensive.
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在计算上成本较低。
- en: They often outperform OLS regression.
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们通常优于OLS回归。
- en: LASSO and elastic net can perform feature selection by setting the slopes of
    uninformative predictors equal to 0.
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LASSO和弹性网络可以通过将无信息预测器的斜率设置为0来执行特征选择。
- en: They can also be applied to generalized linear models (such as logistic regression).
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们也可以应用于广义线性模型（如逻辑回归）。
- en: 'The weaknesses of ridge, LASSO, and elastic net are these:'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge、LASSO和弹性网络的劣势如下：
- en: They make strong assumptions about the data, such as homoscedasticity (constant
    variance) and the distribution of residuals (performance may suffer if these are
    violated).
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们对数据做出了强烈的假设，例如同方差性（常数方差）和残差的分布（如果违反这些假设，性能可能会受到影响）。
- en: Ridge regression cannot perform feature selection automatically.
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ridge回归无法自动执行特征选择。
- en: LASSO cannot estimate more parameters than cases in the training set.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LASSO无法估计比训练集中案例更多的参数。
- en: They cannot handle missing data.
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们无法处理缺失数据。
- en: '|  |'
  id: totrans-839
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 5**'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习5**'
- en: Create a new tibble that contains only the `Yield` variable, and make a new
    regression task using this data, with `Yield` set as the target.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的tibble，其中只包含`Yield`变量，并使用这些数据创建一个新的回归任务，将`Yield`设置为目标变量。
- en: Train an ordinary OLS model on this data (a model with no predictors).
  id: totrans-842
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此数据上训练一个普通OLS模型（一个没有预测器的模型）。
- en: Train a LASSO model on the original `iowaTask` with a *lambda* value of 500.
  id: totrans-843
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在原始`iowaTask`上训练一个*lambda*值为500的LASSO模型。
- en: Cross-validate both models using leave-one-out cross-validation (`makeResampleDesc("LOO")`).
  id: totrans-844
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用留一法交叉验证（`makeResampleDesc("LOO")`）对两个模型进行交叉验证。
- en: How do the mean MSE values of both models compare? Why?
  id: totrans-845
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个模型的平均均方误差值如何比较？为什么？
- en: '|  |'
  id: totrans-846
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-847
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 6**'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习6**'
- en: Calling `plot()` on a `glmnet` model object doesn’t plot model residuals. Install
    the plotmo package and use its `plotres()` function, passing the model data objects
    for the ridge, LASSO, and elastic net models as arguments.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 在`glmnet`模型对象上调用`plot()`不会绘制模型残差。安装plotmo包并使用其`plotres()`函数，将岭回归、LASSO和弹性网络模型的模型数据对象作为参数传递。
- en: '|  |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-851
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Regularization is a set of techniques that prevents overfitting by shrinking
    model parameter estimates.
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化是一组通过缩小模型参数估计来防止过拟合的技术。
- en: 'There are three regularization techniques for linear models: ridge regression,
    LASSO, and elastic net.'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型有三种正则化技术：岭回归、LASSO和弹性网络。
- en: Ridge regression uses the L2 norm to shrink parameter estimates toward 0 (but
    never exactly to 0, unless they were 0 to begin with).
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归使用L2范数将参数估计缩小到0（但永远不会精确到0，除非它们一开始就是0）。
- en: LASSO uses the L1 norm to shrink parameter estimates toward 0 (and possibly
    exactly to 0, resulting in feature selection).
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LASSO使用L1范数将参数估计缩小到0（并且可能精确到0，从而实现特征选择）。
- en: Elastic net combines both L2 and L1 regularization, the ratio of which is controlled
    by the *alpha* hyperparameter.
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性网络结合了L2和L1正则化，其比例由*alpha*超参数控制。
- en: For all three, the *lambda* hyperparameter controls the strength of shrinkage.
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有三个模型，*lambda*超参数控制收缩的强度。
- en: Solutions to exercises
  id: totrans-858
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题解答
- en: 'Expand the search space to include values of *lambda* from 0 to 50:'
  id: totrans-859
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将搜索空间扩展到包括从0到50的*lambda*值：
- en: '[PRE52]'
  id: totrans-860
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Plot the intercepts for the ridge and LASSO models:'
  id: totrans-861
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制岭回归和LASSO模型的截距：
- en: '[PRE53]'
  id: totrans-862
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Experiment with different ways of plotting the hyperparameter tuning process:'
  id: totrans-863
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试不同的方法来绘制超参数调整过程：
- en: '[PRE54]'
  id: totrans-864
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Plot the model coefficients using horizontally dodged bars instead of facets:'
  id: totrans-865
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用水平错位条形图而不是小面元绘制模型系数：
- en: '[PRE55]'
  id: totrans-866
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Compare the performance of a LASSO model with a high *lambda*, and an OLS model
    with no predictors:'
  id: totrans-867
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较具有高*lambda*的LASSO模型和没有预测变量的OLS模型的性能：
- en: '[PRE56]'
  id: totrans-868
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Use the `plotres()` function to plot model diagnostics for glmnet models:'
  id: totrans-869
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plotres()`函数绘制glmnet模型的模型诊断：
- en: '[PRE57]'
  id: totrans-870
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Chapter 12\. Regression with kNN, random forest, and XGBoost
  id: totrans-871
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12章\. 使用kNN、随机森林和XGBoost进行回归
- en: '*This chapter covers*'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Using the k-nearest neighbors algorithm for regression
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用k最近邻算法进行回归
- en: Using tree-based algorithms for regression
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于树的回归算法
- en: Comparing k-nearest neighbors, random forest, and XGBoost models
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较k最近邻、随机森林和XGBoost模型
- en: You’re going to find this chapter a breeze. This is because you’ve done everything
    in it before (sort of). In [chapter 3](kindle_split_013.html#ch03), I introduced
    you to the k-nearest neighbors (kNN) algorithm as a tool for classification. In
    [chapter 7](kindle_split_017.html#ch07), I introduced you to decision trees and
    then expanded on this in [chapter 8](kindle_split_018.html#ch08) to cover random
    forest and XGBoost for classification. Well, conveniently, these algorithms can
    also be used to predict continuous variables. So in this chapter, I’ll help you
    extend these skills to solve regression problems.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现本章内容轻松易懂。这是因为你之前已经做过其中的所有内容（某种程度上）。在[第3章](kindle_split_013.html#ch03)中，我向你介绍了k最近邻（kNN）算法作为分类工具。在[第7章](kindle_split_017.html#ch07)中，我向你介绍了决策树，然后在[第8章](kindle_split_018.html#ch08)中扩展了这一内容，涵盖了用于分类的随机森林和XGBoost。方便的是，这些算法也可以用于预测连续变量。因此，在本章中，我将帮助你扩展这些技能来解决回归问题。
- en: By the end of this chapter, I hope you’ll understand how kNN and tree-based
    algorithms can be extended to predict continuous variables. As you learned in
    [chapter 7](kindle_split_017.html#ch07), decision trees suffer from a tendency
    to overfit their training data and so are often vastly improved by using ensemble
    techniques. Therefore, in this chapter, you’ll train a random forest model and
    an XGBoost model, and benchmark their performance against the kNN algorithm.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我希望你能理解如何将kNN和基于树的算法扩展到预测连续变量。正如你在[第7章](kindle_split_017.html#ch07)中学到的，决策树倾向于过拟合其训练数据，因此通常通过使用集成技术来大幅改进。因此，在本章中，你将训练一个随机森林模型和一个XGBoost模型，并将它们的性能与kNN算法进行比较。
- en: '|  |'
  id: totrans-878
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-879
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 8](kindle_split_018.html#ch08) that random forest and XGBoost
    are two tree-based learners that create an ensemble of many trees to improve prediction
    accuracy. Random forest trains many trees in parallel on different bootstrap samples
    from the data, and XGBoost trains sequential trees that prioritize misclassified
    cases.
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第8章](kindle_split_018.html#ch08)回顾，随机森林和XGBoost是两种基于树的算法，它们通过创建许多树的集成来提高预测精度。随机森林在数据的不同自助样本上并行训练许多树，而XGBoost则优先训练那些被错误分类的序列树。
- en: '|  |'
  id: totrans-881
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 12.1\. Using k-nearest neighbors to predict a continuous variable
  id: totrans-882
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1. 使用k近邻预测连续变量
- en: In this section, I’ll show you how you can use the kNN algorithm for regression,
    graphically and intuitively. Imagine that you’re not a morning person (perhaps,
    like me, you don’t have to imagine very hard), and you like to spend as much time
    in bed as possible. To maximize the amount of time you spend sleeping, you decide
    to train a machine learning model to predict how long it takes you to commute
    to work, based on the time you leave the house. It takes you 40 minutes to get
    ready in the morning, so you hope this model will tell you what time you need
    to leave the house to get to work on time, and therefore what time you need to
    wake up.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示如何使用kNN算法进行回归，图形化和直观化。想象一下，你不是个早睡早起的人（也许，像我一样，你不需要非常努力地想象），你尽可能地想多在床上多待一会儿。为了最大化你睡眠的时间，你决定训练一个机器学习模型来预测你通勤到工作地点所需的时间，基于你离开家的时间。你早上需要40分钟来准备，所以你希望这个模型能告诉你你需要什么时候离开家才能准时到达工作地点，因此你需要什么时候起床。
- en: Every day for two weeks, you record the time you leave the house and how long
    your journey takes. Your journey time is affected by the traffic (which varies
    across the morning), so your journey length changes, depending on when you leave.
    An example of what the relationship between departure time and journey length
    might look like is shown in [figure 12.1](#ch12fig01).
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两周内，你记录你离开家的时间和你的旅程所需的时间。你的旅程时间受交通状况（在早晨变化）的影响，所以你的旅程长度取决于你离开的时间。离开时间和旅程长度之间可能存在的关系示例在[图12.1](#ch12fig01)中展示。
- en: Figure 12.1\. An example relationship for how long your commute to work takes,
    depending on what time you leave the house
  id: totrans-885
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.1. 根据你离开家的时间，通勤所需时间的示例关系
- en: '![](fig12-1.jpg)'
  id: totrans-886
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-1.jpg)'
- en: Recall from [chapter 3](kindle_split_013.html#ch03) that the kNN algorithm is
    a lazy learner. In other words, it doesn’t do any work during model training (instead,
    it just stores the training data); it does all of its work when it makes predictions.
    When making predictions, the kNN algorithm looks in the training set for the *k*
    cases most similar to each of the new, unlabeled data values. Each of those *k*
    most similar cases votes on the predicted value of the new data. When using kNN
    for classification, these votes are for class membership, and the winning vote
    selects the class the model outputs for the new data. To remind you how this process
    works, I’ve reproduced a modified version of [figure 3.4](kindle_split_013.html#ch03fig04)
    from [chapter 3](kindle_split_013.html#ch03), in [figure 12.2](#ch12fig02).
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第3章](kindle_split_013.html#ch03)回顾，kNN算法是一种懒惰学习器。换句话说，它在模型训练期间不做任何工作（相反，它只是存储训练数据）；它所有的工都在进行预测时完成。在做出预测时，kNN算法会在训练集中寻找与每个新的、未标记的数据值最相似的*k*个案例。这些*k*个最相似的案例会对新数据的预测值进行投票。当使用kNN进行分类时，这些投票是关于类成员的，并且获得多数票的投票选择模型为新的数据输出的类别。为了提醒你这个过程是如何工作的，我重新绘制了[第3章的图3.4](kindle_split_013.html#ch03fig04)的修改版，即[图12.2](#ch12fig02)。
- en: 'Figure 12.2\. The kNN algorithm for classification: identifying the *k* nearest
    neighbors and taking the majority vote. Lines connect the unlabeled data with
    their one, three, and five nearest neighbors. The majority vote in each scenario
    is indicated by the shape drawn under each cross.'
  id: totrans-888
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.2. kNN算法进行分类：识别*k*个最近邻并采取多数投票。线条将未标记的数据与它们的单个、三个和五个最近邻连接起来。每种情况下的多数投票由每个十字下方的形状表示。
- en: '![](fig12-2.jpg)'
  id: totrans-889
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-2.jpg)'
- en: The voting process when using kNN for regression is very similar, except that
    we take the mean of these *k* votes as the predicted value for the new data.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 使用kNN进行回归时的投票过程非常相似，只是我们取这些*k*个投票的平均值作为新数据的预测值。
- en: 'This process is illustrated for our commuting example in [figure 12.3](#ch12fig03).
    The crosses on the x-axis represent new data: times we left the house and for
    which we want to predict journey length. If we train a one-nearest neighbor model,
    the model finds the single case from the training set that is closest to the departure
    time of each of the new data points, and uses that value as the predicted journey
    length. If we train a three-nearest neighbor model, the model finds the three
    training cases with departure times most similar to each of the new data points,
    takes the mean journey length of those nearest cases, and outputs this as the
    predicted value for the new data. The same applies to any number of *k* we use
    to train the model.'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程在[图12.3](#ch12fig03)中用我们的通勤示例进行了说明。x轴上的交叉点代表新的数据：我们离开家的时刻，我们想要预测行程长度。如果我们训练一个一最近邻模型，模型会找到训练集中与每个新数据点的出发时间最接近的单个案例，并使用该值作为预测的行程长度。如果我们训练一个三最近邻模型，模型会找到与每个新数据点的出发时间最相似的三个训练案例，取这些最近案例的平均行程长度，并将此作为新数据的预测值。对于任何我们用于训练模型的k值，情况都是相同的。
- en: Figure 12.3\. How the kNN algorithm predicts continuous variables. The crosses
    represent new data points for which we wish to predict the journey length. For
    the one-, three-, and five-nearest neighbor models, the nearest neighbors to each
    new data point are highlighted in a lighter shade. In each case, the predicted
    value is the mean journey length of the nearest neighbors.
  id: totrans-892
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.3\. kNN算法如何预测连续变量。交叉点代表我们希望预测行程长度的新的数据点。对于一、三和五个最近邻模型，每个新数据点的最近邻被以较浅的阴影突出显示。在每种情况下，预测值是最近邻的平均行程长度。
- en: '![](fig12-3_alt.jpg)'
  id: totrans-893
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-3_alt.jpg)'
- en: '|  |'
  id: totrans-894
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-895
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: Just like when we used kNN for classification, selecting the best-performing-
    value of *k* is critical to model performance. If we select a *k* that is too
    low, we may produce a model that is overfitted and makes predictions with high
    variance. If we select a *k* that is too high, we may produce a model that is
    underfitted and makes predictions with high bias.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 就像当我们使用kNN进行分类时，选择最佳性能的*k*值对于模型性能至关重要。如果我们选择一个太低的*k*，我们可能会产生一个过度拟合的模型，并做出具有高方差预测。如果我们选择一个太高的*k*，我们可能会产生一个欠拟合的模型，并做出具有高偏差的预测。
- en: '|  |'
  id: totrans-897
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 12.2\. Using tree-based learners to predict a continuous variable
  id: totrans-898
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2\. 使用基于树的算法预测连续变量
- en: In this section, I’ll show you how you can use tree-based algorithms to predict
    a continuous outcome variable. Back in [chapter 7](kindle_split_017.html#ch07),
    I showed you how tree-based algorithms (such as the rpart algorithm) split a feature
    space into separate regions, one binary split at a time. The algorithm tries to
    partition the feature space such that each region contains only cases from a particular
    class. Put another way, the algorithm tries to learn binary splits that result
    in regions that are as pure as possible.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用基于树的算法来预测连续结果变量。在[第7章](kindle_split_017.html#ch07)中，我向您展示了基于树的算法（如rpart算法）如何一次分割特征空间成单独的区域，每次一个二分分割。算法试图将特征空间分割成每个区域只包含特定类别的案例。换句话说，算法试图学习导致尽可能纯净区域的二分分割。
- en: '|  |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-901
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: Remember that the *feature space* refers to all possible combinations of predictor
    variable values, and that *purity* refers to how homogeneous the cases are within
    a single region.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，*特征空间*指的是预测变量值的所有可能组合，而*purity*指的是单个区域内案例的均匀程度。
- en: '|  |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: To refresh your memory, I’ve reproduced [figure 7.4](kindle_split_017.html#ch07fig04)
    in [figure 12.4](#ch12fig04), showing how a feature space of two predictor variables
    can be partitioned to predict the membership of three classes.
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 为了刷新您的记忆，我在[图12.4](#ch12fig04)中重新绘制了[图7.4](kindle_split_017.html#ch07fig04)，展示了如何将两个预测变量的特征空间分割以预测三个类别的成员资格。
- en: Figure 12.4\. How splitting is performed for classification problems. Cases
    belonging to three classes are plotted against two continuous variables. The first
    node splits the feature space into rectangles based on the value of variable 2\.
    The second node further splits the variable 2 ≥ 20 feature space into rectangles
    based on the value of variable 1.
  id: totrans-905
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.4\. 如何对分类问题进行分割。属于三个类别的案例被绘制在两个连续变量上。第一个节点根据变量2的值将特征空间分割成矩形。第二个节点进一步根据变量1的值将变量2
    ≥ 20的特征空间分割成矩形。
- en: '![](fig12-4_alt.jpg)'
  id: totrans-906
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-4_alt.jpg)'
- en: Classification with tree-based algorithms is a bit like herding animals into
    their pens on a farm. It’s quite obvious that we want one pen for the chickens,
    one for the cows, and one for the alpacas (I don’t think you see many alpacas
    on farms, but I’m particularly fond of them). So conceptually, it’s quite easy
    for us to picture splitting regions of the feature space into different pens for
    different categories. But perhaps it’s not so easy to picture splitting the feature
    space to predict a continuous variable.
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法进行分类有点像在农场把动物赶到不同的围栏里。很明显，我们想要一个围栏放鸡，一个放牛，一个放羊驼（我不认为你在农场看到很多羊驼，但我特别喜欢它们）。所以从概念上讲，我们很容易想象将特征空间的区域分割成不同类别的不同围栏。但也许想象将特征空间分割来预测一个连续变量并不那么容易。
- en: 'So how does this partitioning work for regression problems? In exactly the
    same way: the only difference is that instead of each region representing a class,
    it represents a value of the continuous outcome variable. Take a look at [figure
    12.5](#ch12fig05), where we’re creating a regression tree using our journey length
    example. The nodes of the regression tree split the feature space (departure time)
    into distinct regions. Each region represents the mean of the outcome variable
    of the cases inside it. When making predictions on new data, the model will predict
    the value of the region the new data falls into. The leaves of the tree are no
    longer classes, but numbers. This is illustrated for situations with one and two
    predictor variables in [figure 12.5](#ch12fig05), but it extends to any number
    of predictors.'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种分割对于回归问题是如何工作的呢？完全一样：唯一的区别是，每个区域代表的是连续结果变量的值，而不是一个类别。看看[图12.5](#ch12fig05)，在那里我们使用旅程长度示例创建回归树。回归树的节点将特征空间（出发时间）分割成不同的区域。每个区域代表其中案例的结果变量的平均值。当对新数据进行预测时，模型将预测新数据所属区域的值。树的叶子不再是类别，而是数字。这在[图12.5](#ch12fig05)中用有一个和两个预测变量的情况进行了说明，但它可以扩展到任何数量的预测变量。
- en: Just as for classification, regression trees can handle both continuous and
    categorical predictor variables (with the exception of XGBoost, which requires
    categorical variables to be numerically encoded). The way splits are decided for
    continuous and categorical variables is the same as for classification trees,
    except that instead of finding the split that has the highest Gini gain, the algorithm
    looks for the split with the lowest sum of squares.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: 正如分类一样，回归树可以处理连续和分类预测变量（除了XGBoost，它要求分类变量进行数值编码）。对于连续和分类变量决定分割的方式与分类树相同，只是算法寻找的是具有最低平方和的分割，而不是具有最高Gini增益的分割。
- en: Figure 12.5\. How splitting is performed for regression problems. The feature
    space is split into shaded regions based on the nodes of the tree next to each
    plot. The predicted journey length is shown inside each region. The dashed line
    in the top plot demonstrates how journey length is predicted from departure time
    based on the tree. The bottom plot shows a two-predictor situation.
  id: totrans-910
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.5. 对于回归问题，如何进行分割。特征空间根据每个图表旁边的树节点分割成阴影区域。每个区域内部显示了预测的旅程长度。顶部图表中的虚线展示了如何根据树从出发时间预测旅程长度。底部图表显示了两个预测变量的情况。
- en: '![](fig12-5_alt.jpg)'
  id: totrans-911
  prefs: []
  type: TYPE_IMG
  zh: '![图12-5](fig12-5_alt.jpg)'
- en: '|  |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-913
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 7](kindle_split_017.html#ch07) that the Gini gain is the
    difference between the Gini indices of the parent node and of the split. The Gini
    index is a measure of impurity and is equal to 1 – (*p*(*A*)² + *p*(*B*)²), where
    *p*(*A*) and *p*(*B*) are the proportions of cases belonging to classes *A* and
    *B*, respectively.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第7章](kindle_split_017.html#ch07)，Gini增益是父节点和分割的Gini指数之间的差异。Gini指数是不纯度的度量，等于1
    – (*p*(*A*)² + *p*(*B*)²)，其中*p*(*A*)和*p*(*B*)分别是属于类别*A*和*B*的案例的比例。
- en: '|  |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: For each candidate split, the algorithm calculates the sum of squared residuals
    for the left and right split, and adds them together to form the sum of squared
    residuals for the split as a whole. In [figure 12.6](#ch12fig06), the algorithm
    is considering the candidate split of a departure time before 7:45\. For each
    case where the departure time was before 7:45, the algorithm calculates the mean
    journey length, finds the residual error (the difference between each case’s journey
    length and the mean), and squares it. The same is done for the cases where you
    left the house after 7:45, with their respective mean. These two sums of squared
    residual values are added together to give the sum of squares for the split. If
    you prefer to see this in mathematical notation, it’s shown in [equation 12.1](#ch12equ01).
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个候选分割，算法计算左右分割的平方残差之和，并将它们相加以形成整个分割的平方残差之和。在 [图 12.6](#ch12fig06) 中，算法正在考虑
    7:45 之前的出发时间的候选分割。对于每个出发时间在 7:45 之前的案例，算法计算平均旅程长度，找到残差误差（每个案例的旅程长度与平均值的差异），并将其平方。对于你在
    7:45 之后离开房子的案例，也以它们各自的平均值进行同样的操作。这两个平方残差之和给出了分割的平方和。如果您更喜欢用数学符号表示，它显示在 [方程式 12.1](#ch12equ01)
    中。
- en: Figure 12.6\. How candidate splits are chosen for regression problems. The measure
    of purity is the sum of squares for the split, which is the combined sums of squares
    for the left and right nodes. Each sum of squares is the vertical distance between
    each case and the predicted value for the leaf it belongs to.
  id: totrans-917
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.6\. 如何为回归问题选择候选分割。纯度的度量是分割的平方和，即左右节点的组合平方和。每个平方和是每个案例与其所属叶子的预测值之间的垂直距离。
- en: '![](fig12-6_alt.jpg)'
  id: totrans-918
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-6_alt.jpg)'
- en: equation 12.1\.
  id: totrans-919
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 12.1。
- en: '![](eq12-1.jpg)'
  id: totrans-920
  prefs: []
  type: TYPE_IMG
  zh: '![](eq12-1.jpg)'
- en: where *i* ∈ *left* and *i* ∈ *right* indicate cases belonging to the left and
    right splits, respectively.
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *i* ∈ *left* 和 *i* ∈ *right* 分别表示属于左分割和右分割的案例。
- en: The candidate split with the lowest sum of squares is chosen as the split for
    any particular point in the tree. So, for regression trees, *purity* refers to
    how spread the data are around the mean of the node.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最低平方和的候选分割被选为树中任何特定点的分割。因此，对于回归树来说，*纯度*指的是数据围绕节点均值的分布程度。
- en: 12.3\. Building your first kNN regression model
  id: totrans-923
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3\. 构建您的第一个 kNN 回归模型
- en: In this section, I’ll teach you how to define a kNN learner for regression,
    tune the *k* hyperparameter, and train a model so you can use it to predict a
    continuous variable. Imagine that you’re a chemical engineer trying to predict
    the amount of heat released by various batches of fuel, based on measurements
    you made on each batch. We’re first going to train a kNN model on this task and
    then compare how it performs to a random forest and an XGBoost model, later in
    the chapter.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将教你如何定义一个用于回归的 kNN 学习者，调整 *k* 超参数，并训练一个模型，这样你就可以用它来预测一个连续变量。想象一下，你是一位化学工程师，试图根据对每个批次的测量来预测各种批次燃料释放的热量。我们将首先在这个任务上训练一个
    kNN 模型，然后在章节的后面部分将其性能与随机森林和 XGBoost 模型进行比较。
- en: 'Let’s start by loading the mlr and tidyverse packages:'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载 mlr 和 tidyverse 包开始：
- en: '[PRE58]'
  id: totrans-926
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 12.3.1\. Loading and exploring the fuel dataset
  id: totrans-927
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.1\. 加载和探索燃料数据集
- en: 'The mlr package, conveniently, comes with several predefined tasks to help
    you experiment with different learners and processes. The dataset we’re going
    to work with in this chapter is contained inside mlr’s `fuelsubset.task`. We load
    this task into our R session the same way we would any built-in dataset: using
    the `data()` function. We can then use mlr’s `getTaskData()` function to extract
    the data from the task, so we can explore it. As always, we use the `as_tibble()`
    function to convert the data frame into a tibble.'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: mlr 包方便地包含几个预定义的任务，以帮助您尝试不同的学习者和过程。我们将在本章中使用的数据集包含在 mlr 的 `fuelsubset.task`
    中。我们以加载任何内置数据集相同的方式将此任务加载到我们的 R 会话中：使用 `data()` 函数。然后我们可以使用 mlr 的 `getTaskData()`
    函数从任务中提取数据，以便我们可以探索它。像往常一样，我们使用 `as_tibble()` 函数将数据框转换为 tibble。
- en: Listing 12.1\. Loading and exploring the fuel dataset
  id: totrans-929
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.1\. 加载和探索燃料数据集
- en: '[PRE59]'
  id: totrans-930
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We have a tibble containing 129 different batches of fuel and 367 variables/features!
    In fact, there are so many variables that I’ve truncated the printout of the tibble
    to remove the names of the variables that didn’t fit on my console.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含 129 个不同批次的燃料和 367 个变量/特征的 tibble！实际上，变量如此之多，以至于我已经截断了 tibble 的打印输出，以删除不适合我控制台的变量名称。
- en: '|  |'
  id: totrans-932
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-933
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Run `names(fuelTib)` to return the names of all the variables in the dataset.
    This is useful when working with large datasets with too many columns to visualize
    on the console.
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`names(fuelTib)`以返回数据集中所有变量的名称。当处理具有太多列而无法在控制台可视化的大型数据集时，这很有用。
- en: '|  |'
  id: totrans-935
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The `heatan` variable is the amount of energy released by a certain quantity
    of fuel when it is combusted (measured in megajoules). The `h20` variable is the
    percentage of humidity in the fuel’s container. The remaining variables show how
    much ultraviolet or near-infrared light of a particular wavelength each batch
    of fuel absorbs (each variable represents a different wavelength).
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: '`heatan`变量是当一定量的燃料燃烧时释放的能量量（以兆焦耳计量）。`h20`变量是燃料容器中湿度的百分比。其余变量显示了每批燃料吸收特定波长的紫外或近红外光量（每个变量代表不同的波长）。'
- en: '|  |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-938
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: To see all the tasks that come built into mlr, use `data(package = "mlr")`.
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看mlr中内置的所有任务，请使用`data(package = "mlr")`。
- en: '|  |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Let’s plot the data to get an idea of how the `heatan` variable correlates
    with the `absorbance` variable at various wavelengths of ultraviolet and near-infrared
    light. We’ll up our tidyverse game by doing some more-complicated operations,
    so let me take you step by step through the process in [listing 12.2](#ch12ex02):'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据，以了解`heatan`变量在紫外和近红外光的各个波长下与`absorbance`变量之间的相关性。我们将通过进行一些更复杂的操作来提升tidyverse的使用水平，所以让我一步步带你通过[列表12.2](#ch12ex02)中的过程：
- en: Because we want to plot a separate `geom_smooth()` line for every case in the
    data, we first pipe the data into a `mutate()` function call, where we create
    an `id` variable that just acts as a row index. We use `nrow(.)` to specify the
    number of rows in the data object piped into `mutate()`.
  id: totrans-942
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们要为数据中的每个案例绘制一个单独的`geom_smooth()`线，所以我们首先将数据通过管道传递给一个`mutate()`函数调用，在那里我们创建一个仅作为行索引的`id`变量。我们使用`nrow(.)`来指定通过`mutate()`传递的数据对象中的行数。
- en: We pipe the result of step 1 into a `gather()` function to create a key-value
    pair of variables containing the spectral information (`wavelength` as the key,
    `absorbance` at that wavelength as the value). We omit the `heatan`, `h20`, and
    `id` variables from the gathering process (`c(-heatan, -h20, -id)`).
  id: totrans-943
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将步骤1的结果通过管道传递给一个`gather()`函数，以创建包含光谱信息的键值对变量（`wavelength`作为键，该波长的`absorbance`作为值）。我们在收集过程中省略了`heatan`、`h20`和`id`变量（`c(-heatan,
    -h20, -id)`）。
- en: 'We pipe the result of step 2 into another `mutate()` function to create two
    new variables:'
  id: totrans-944
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将步骤2的结果通过管道传递给另一个`mutate()`函数，以创建两个新变量：
- en: A character vector that indicates whether the row shows absorbance of ultraviolet
    or near-infrared spectra
  id: totrans-945
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个表示行是否显示紫外或近红外光谱吸收率的字符向量
- en: A numeric vector that indicates the wavelength of that particular spectrum
  id: totrans-946
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个表示该特定光谱波长的数值向量
- en: 'I’ve introduced two functions here from the stringr tidyverse package: `str_sub()`
    and `str_extract()`. The `str_sub()` function splits a character string into its
    individual alphanumeric characters and symbols, and returns the ones that are
    between the `start` and `end` arguments. For example, `str_sub("UVVIS.UVVIS.1",
    1, 3)` returns `"UVV"`. We use this function to mutate a column with the value
    `"UVV"` when the spectrum is ultraviolet and `"NIR"` when the spectrum is near-infrared.'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里介绍了来自stringr tidyverse包的两个函数：`str_sub()`和`str_extract()`。`str_sub()`函数将字符字符串分割成其单个字母数字字符和符号，并返回位于`start`和`end`参数之间的那些。例如，`str_sub("UVVIS.UVVIS.1",
    1, 3)`返回`"UVV"`。我们使用此函数在光谱为紫外时将列值更改为`"UVV"`，在光谱为近红外时更改为`"NIR"`。
- en: The `str_extract()` function looks for a particular pattern in a character string,
    and returns that pattern. In the example in [listing 12.2](#ch12ex02), we asked
    the function to look for any numerical digits, using `\\d`. The `+` after `\\d`
    tells the function that the pattern may be matched more than once. For example,
    compare the output of `str_extract ("hello123", "\\d")` and `str_extract("hello123",
    "\\d+")`.
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '`str_extract()`函数在字符字符串中查找特定的模式，并返回该模式。在[列表12.2](#ch12ex02)的例子中，我们要求该函数查找任何数字，使用`\\d`。`\\d`后面的`+`告诉函数该模式可能被匹配多次。例如，比较`str_extract("hello123",
    "\\d")`和`str_extract("hello123", "\\d+")`的输出。'
- en: Listing 12.2\. Preparing the data for plotting
  id: totrans-949
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.2\. 准备绘图数据
- en: '[PRE60]'
  id: totrans-950
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This was some reasonably complex data manipulation, so run the code and take
    a look at the resulting tibble, and make sure you understand how we created it.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些相当复杂的数据处理，所以运行代码并查看生成的tibble，确保你理解我们是如何创建它的。
- en: '|  |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-953
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: We search for patterns in character vectors by specifying *regular expressions*,
    such as `"\\d+"` in [listing 12.2](#ch12ex02). A regular expression is a special
    text string for describing a search pattern. Regular expressions are very useful
    tools for extracting (sometimes-complex) patterns from character strings. If I’ve
    piqued your interest in regular expressions, you can learn more about how to use
    them in R by running `?regex`.
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过指定**正则表达式**来在字符向量中寻找模式，例如[列表12.2](#ch12ex02)中的`"\\d+"`。正则表达式是用来描述搜索模式的一种特殊文本字符串。正则表达式是提取（有时是复杂的）字符字符串模式非常有用的工具。如果我对您对正则表达式产生了兴趣，您可以通过运行`?regex`来了解更多关于如何在R中使用它们的信息。
- en: '|  |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Now that we’ve formatted our data for plotting, we’re going to draw three plots:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经格式化好了数据以供绘图，我们将绘制三个图：
- en: '`absorbance` versus `heatan`, with a separate curve for every wavelength'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`吸光度`与`加热量`的关系，每个波长都有单独的曲线'
- en: '`wavelength` versus `absorbance`, with a separate curve for every case'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`波长`与`吸光度`的关系，每个案例都有单独的曲线'
- en: Humidity (`h20`) versus `heatan`
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 湿度(`h20`)与`加热量`的关系
- en: In the plot for `absorbance` versus `heatan`, we wrap `wavelength` inside the
    `as.factor()` function, so that each wavelength will be drawn with a discrete
    color (rather than a gradient of colors from low to high wavelengths). To prevent
    the `ggplot()` function from drawing a huge legend showing the color of each of
    the lines, we suppress the legend by adding `theme(legend.position = "none")`.
    We facet by spectrum to create subplots for the ultraviolet and near-infrared
    spectra, allowing the x-axis to vary between subplots using the `scales = "free_x"`
    argument.
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 在`吸光度`与`加热量`的图中，我们将`波长`放在`as.factor()`函数内，这样每个波长都会用离散的颜色绘制（而不是从低波长到高波长的颜色渐变）。为了防止`ggplot()`函数绘制一个显示每条线颜色的巨大图例，我们通过添加`theme(legend.position
    = "none")`来抑制图例。我们通过光谱进行分面，为紫外光谱和近红外光谱创建子图，允许使用`scales = "free_x"`参数在子图之间变化x轴。
- en: I don’t know about you, but I was always told in school to add titles to my
    plots. We can do this in ggplot2 using the `ggtitle()` function, supplying the
    title we want in quotes.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道你们是否如此，但我在学校总是被告知要在我的图中添加标题。我们可以在ggplot2中使用`ggtitle()`函数，在引号中提供我们想要的标题。
- en: '|  |'
  id: totrans-962
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-963
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The `theme()` function allows you to customize almost anything about the appearance
    of your ggplots, including font sizes and the presence/absence of grid lines.
    I won’t discuss this in depth, but I recommend taking a look at the help page
    using `?theme` to find out what you can do.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: '`theme()`函数允许您几乎可以自定义ggplot的任何外观，包括字体大小和网格线的存在/不存在。我不会深入讨论这个问题，但我建议您使用`?theme`查看帮助页面，以了解您可以做什么。'
- en: '|  |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the plot for `wavelength` versus `absorbance`, we set the `group` aesthetic
    equal to the `id` variable we created, so that the `geom_smooth()` layer will
    draw a separate curve for each batch of fuel.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 在`波长`与`吸光度`的图中，我们将`group`美学设置为创建的`id`变量，这样`geom_smooth()`层将为每批燃料绘制单独的曲线。
- en: Listing 12.3\. Plotting the data
  id: totrans-967
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.3\. 绘制数据
- en: '[PRE61]'
  id: totrans-968
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The resulting plots are shown in [figure 12.7](#ch12fig07) (I’ve combined them
    into a single figure to save space). Data really is beautiful sometimes, isn’t
    it? In the plots of `absorbance` against `heatan`, each line corresponds to a
    particular wavelength. The relationship between each predictor variable and the
    outcome variable is complex and nonlinear. There is also a nonlinear relationship
    between `h20` and `heatan`.
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图示显示在[图12.7](#ch12fig07)中（我将它们合并成单个图以节省空间）。有时数据确实很美，不是吗？在`吸光度`对`加热量`的图中，每条线对应一个特定的波长。每个预测变量与结果变量之间的关系是复杂且非线性的。`h20`与`加热量`之间也存在非线性关系。
- en: In the plots of `wavelength` against `absorbance`, each line corresponds to
    a particular batch of fuel, and the lines show its `absorbance` of ultraviolet
    and near-infrared light. The shading of the line corresponds to the `heatan` value
    of that batch. It’s difficult to identify patterns in these plots, but certain
    `absorbance` profiles seem to correlate with higher and lower `heatan` values.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 在`波长`对`吸光度`的图中，每条线对应一个特定的燃料批次，线显示了其紫外和近红外光的`吸光度`。线的阴影对应于该批次的`加热量`值。在这些图中识别模式很困难，但某些`吸光度`轮廓似乎与更高的`加热量`值和更低的`加热量`值相关联。
- en: '|  |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-972
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: While you can certainly overfit your data, you can never over-plot it. When
    starting an exploratory analysis, I will plot my dataset in multiple different
    ways to get a better understanding of it from different angles/perspectives.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你当然可以过拟合你的数据，但你永远不能过拟合你的图。在开始探索性分析时，我会以多种不同的方式绘制我的数据集，以便从不同的角度/视角更好地理解它。
- en: '|  |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: 'Add an additional `geom_smooth()` layer to the plot of `absorbance` versus
    `heatan` with these arguments:'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 向`absorbance`与`heatan`的图中添加一个额外的`geom_smooth()`层，并使用以下参数：
- en: '`group = 1`'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group = 1`'
- en: '`col = "blue"`'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col = "blue"`'
- en: Using the argument `group = 1`, create a single smoothing line that models *all*
    of the data, ignoring groups.
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`group = 1`参数，创建一条平滑线，以建模*所有*数据，忽略分组。
- en: '|  |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 12.7\. Plotting the relationships in the fuelTib dataset. The topmost
    plots show `absorbance` against `heatan` with separate lines drawn for each `wavelength`,
    faceted by near-infrared (NIR) or ultraviolet (UVV) light. The middle plots show
    `wavelength` against `absorbance` shaded by `heatan` with separate lines drawn
    for each batch of fuel, faceted by NIR or UVV light. The bottom plot shows `h20`
    against `heatan`.
  id: totrans-982
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.7\. 在fuelTib数据集中绘制关系。最上面的图显示了`absorbance`与`heatan`的关系，每个`wavelength`都有单独的线条，按近红外（NIR）或紫外（UVV）光进行分面。中间的图显示了`wavelength`与`absorbance`的关系，按`heatan`着色，每个燃料批次都有单独的线条，按NIR或UVV光进行分面。最下面的图显示了`h20`与`heatan`的关系。
- en: '![](fig12-7_alt.jpg)'
  id: totrans-983
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-7_alt.jpg)'
- en: '|  |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Modeling spectral data**'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: '**建模光谱数据**'
- en: The dataset we’re working with is an example of *spectral data*. Spectral data
    contains observations made across a range of (usually) wavelengths. For example,
    we might measure how much a substance absorbs light from a range of different
    colors.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理的数据集是*光谱数据*的一个例子。光谱数据包含在一系列（通常是）波长范围内进行的观测。例如，我们可能会测量一种物质从一系列不同颜色中吸收光量的多少。
- en: Statisticians and data scientists call this kind of data *functional data*,
    where there are many dimensions in the dataset (the wavelengths we measure across)
    and there is a particular order to those dimensions (starting by measuring the
    absorbance at the lowest wavelength and working our way to the highest wavelength).
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家和数据科学家将此类数据称为*泛函数据*，其中数据集具有许多维度（我们测量的波长），并且这些维度具有特定的顺序（从测量最低波长处的吸光度开始，逐步到最高波长）。
- en: A branch of statistics called *functional data analysis* is dedicated to modeling
    data like this. In functional data analysis, each predictor variable is turned
    into a function (for example, a function that describes how absorbance changes
    over ultraviolet and near-infrared wavelengths). That function is then used in
    the model as a predictor, to predict the outcome variable. We won’t apply this
    kind of technique to this data, but if you’re interested in functional data analysis,
    check out *Functional Data Analysis* by James Ramsay (Springer, 2005).
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学的一个分支，称为*泛函数据分析*，致力于建模此类数据。在泛函数据分析中，每个预测变量都被转换为一个函数（例如，一个描述紫外和近红外波长范围内吸光度变化的函数）。然后，这个函数在模型中用作预测变量，以预测结果变量。我们不会将此类技术应用于这些数据，但如果你对泛函数据分析感兴趣，可以查看James
    Ramsay所著的*泛函数据分析*（Springer，2005年）。
- en: '|  |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Because the predefined `fuelsubset.task` defines the ultraviolet and near-infrared
    spectra as functional variables, we’re going to define our own task, treating
    each wavelength as a separate predictor. We do this, as usual, with the `makeRegrTask()`
    function, setting the `heatan` variable as our target. We then define our kNN
    learner using the `makeLearner()` function.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 因为预定义的`fuelsubset.task`将紫外和近红外光谱定义为泛函变量，我们将定义自己的任务，将每个波长视为一个单独的预测变量。我们像往常一样使用`makeRegrTask()`函数做这件事，将`heatan`变量作为我们的目标。然后我们使用`makeLearner()`函数定义我们的kNN学习器。
- en: Listing 12.4\. Defining the task and kNN learner
  id: totrans-991
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.4\. 定义任务和kNN学习器
- en: '[PRE62]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '|  |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-994
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that for regression, the name of the learner is `"regr.kknn"` with two
    k’s, rather than the `"classif.knn"` we used in [chapter 3](kindle_split_013.html#ch03).
    This is because this function is taken from the kknn package, which allows us
    to perform *kernel k-nearest neighbors*, where we use a kernel function (just
    like with SVMs in [chapter 6](kindle_split_016.html#ch06)) to find a linear decision
    boundary between classes.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，对于回归，学习器的名称是`"regr.kknn"`，有两个k，而不是我们在第3章中使用的`"classif.knn"`。这是因为此函数来自kknn包，它允许我们执行*核k最近邻*，其中我们使用核函数（就像在第6章中使用的SVMs一样）在类别之间找到线性决策边界。
- en: '|  |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 12.3.2\. Tuning the k hyperparameter
  id: totrans-997
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 12.3.2\. 调整 k 超参数
- en: In this section, we’re going to tune *k* to get the best-performing kNN model
    possible. Remember that for regression, the value of *k* determines how many of
    the nearest neighbors’ outcome values to average when making predictions on new
    cases. We first define the hyperparameter search space using the `makeParamSet()`
    function, and define *k* as a discrete hyperparameter with possible values 1 through
    12\. Then we define our search procedure as a grid search (so that we will try
    every value in the search space), and define a 10-fold cross-validation strategy.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将调整 *k* 以获得最佳性能的 kNN 模型。记住，对于回归，*k* 的值决定了在为新案例进行预测时平均多少个最近邻的输出值。我们首先使用
    `makeParamSet()` 函数定义超参数搜索空间，并将 *k* 定义为一个离散超参数，其可能值为 1 到 12。然后，我们将搜索过程定义为网格搜索（这样我们将尝试搜索空间中的每个值），并定义一个
    10 折交叉验证策略。
- en: As we’ve done many times before, we run the tuning process using the `tuneParams()`
    function, supplying the learner, task, cross-validation method, hyperparameter
    space, and search procedure as arguments.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前多次做的那样，我们使用 `tuneParams()` 函数运行调整过程，将学习器、任务、交叉验证方法、超参数空间和搜索过程作为参数传递。
- en: Listing 12.5\. Tuning *k*
  id: totrans-1000
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.5\. 调整 *k*
- en: '[PRE63]'
  id: totrans-1001
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We can plot the hyperparameter tuning process by extracting the tuning data
    with the `generateHyperParsEffectData()` function and passing this to the `plotHyperParsEffect()`
    function, supplying our hyperparameter (`"k"`) as the x-axis and MSE (`"mse.test.mean"`)
    as the y-axis. Setting the `plot.type` argument equal to `"line"` connects the
    samples with a line.
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 `generateHyperParsEffectData()` 函数提取调整数据，并将其传递给 `plotHyperParsEffect()`
    函数来绘制超参数调整过程，将我们的超参数（`"k"`) 作为 x 轴，MSE（`"mse.test.mean"`) 作为 y 轴。将 `plot.type`
    参数设置为 `"line"` 将样本用线连接起来。
- en: Listing 12.6\. Plotting the tuning process
  id: totrans-1003
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.6\. 绘制调整过程
- en: '[PRE64]'
  id: totrans-1004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The resulting plot is shown in [figure 12.8](#ch12fig08). We can see that the
    mean MSE starts to rise as *k* increases beyond 7, so it looks like our search
    space was appropriate.
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在 [图 12.8](#ch12fig08)。我们可以看到，随着 *k* 超过 7，平均 MSE 开始上升，所以看起来我们的搜索空间是合适的。
- en: '|  |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: Let’s make sure our search space was large enough. Repeat the tuning process,
    but search values of *k* from 1 to 50\. Plot this tuning process just like we
    did in [figure 12.8](#ch12fig08). Was our original search space large enough?
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确保我们的搜索空间足够大。重复调整过程，但搜索 *k* 的值从 1 到 50。像我们在 [图 12.8](#ch12fig08) 中做的那样绘制这个调整过程。我们的原始搜索空间是否足够大？
- en: '|  |'
  id: totrans-1009
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that we have our tuned value of *k*, we can define a learner using that
    value, with the `setHyperPars()` function, and train a model using it.
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了调整后的 *k* 值，我们可以使用 `setHyperPars()` 函数定义一个使用该值的学习者，并使用它来训练一个模型。
- en: Figure 12.8\. Plotting our hyperparameter tuning process. The average MSE (`mse.test.mean`)
    is shown for each value of *k*.
  id: totrans-1011
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 12.8\. 绘制我们的超参数调整过程。显示了每个 *k* 值的平均 MSE（`mse.test.mean`）。
- en: '![](fig12-8_alt.jpg)'
  id: totrans-1012
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-8_alt.jpg)'
- en: Listing 12.7\. Training the final, tuned kNN model
  id: totrans-1013
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 12.7\. 训练最终的、调整后的 kNN 模型
- en: '[PRE65]'
  id: totrans-1014
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 12.4\. Building your first random forest regression model
  id: totrans-1015
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4\. 构建你的第一个随机森林回归模型
- en: In this section, I’ll teach you how to define a random forest learner for regression,
    tune its many hyperparameters, and train a model for our fuel task.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将教你如何定义一个用于回归的随机森林学习者，调整其许多超参数，并为我们燃料任务训练一个模型。
- en: '|  |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1018
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: We can also use the rpart algorithm to build a regression tree, but as it is
    almost always outperformed by bagged and boosted learners, we’re going to skip
    over it and dive straight in with random forest and XGBoost. Recall that bagged
    (bootstrap-aggregated) learners train multiple models on bootstrap samples of
    the data, and return the majority vote. Boosted learners train models sequentially,
    putting more emphasis on correcting the mistakes of the previous ensemble of models.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 rpart 算法构建回归树，但由于它几乎总是被集成学习和提升学习所超越，我们将跳过它，直接进入随机森林和 XGBoost。回想一下，集成（自助聚合）学习者在数据的自助样本上训练多个模型，并返回多数投票。提升学习器按顺序训练模型，更加重视纠正先前集成模型的错误。
- en: '|  |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'We’ll start by defining our random forest learner. Notice that rather than
    `"classif .randomForest"` as in [chapter 8](kindle_split_018.html#ch08), the regression
    equivalent is `"regr.randomForest"`:'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义我们的随机森林学习者。请注意，与 [第 8 章](kindle_split_018.html#ch08) 中的 `"classif .randomForest"`
    不同，回归的等效是 `"regr.randomForest"`：
- en: '[PRE66]'
  id: totrans-1022
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we’re going to tune the hyperparameters of our random forest learner:
    `ntree`, `mtry`, `nodesize`, and `maxnodes`. I first defined what these hyperparameters
    do in [chapter 8](kindle_split_018.html#ch08), but let’s recap each one here:'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调整我们的随机森林学习者的超参数：`ntree`、`mtry`、`nodesize`和`maxnodes`。我首先在[第8章](kindle_split_018.html#ch08)中定义了这些超参数的作用，但让我们在这里回顾一下每个参数：
- en: '`ntree` controls the number of individual trees to train. More trees is usually
    better until adding more doesn’t improve performance further.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ntree`控制要训练的单独树的数量。更多的树通常更好，直到增加更多的树不再进一步提高性能。'
- en: '`mtry` controls the number of predictor variables that are randomly sampled
    for each individual tree. Training each individual tree on a random selection
    of predictor variables helps keep the trees uncorrelated and therefore helps prevent
    the ensemble model from overfitting the training set.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mtry`控制为每个单独的树随机采样的预测变量数量。在每个单独的树上使用预测变量的随机选择进行训练有助于保持树的不相关性，从而有助于防止集成模型过度拟合训练集。'
- en: '`nodesize` defines the minimum number of cases allowed in a leaf node. For
    example, setting `nodesize` equal to 1 would allow each case in the training set
    to have its own leaf.'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nodesize`定义了在叶节点中允许的最小案例数。例如，将`nodesize`设置为1将允许训练集中的每个案例都有自己的叶节点。'
- en: '`maxnodes` defines the maximum number of nodes in each individual tree.'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxnodes`定义了每个单独树中的最大节点数。'
- en: As usual, we create our hyperparameter search space using the `makeParamSet()`
    function, defining each hyperparameter as an integer with sensible lower and upper
    bounds.
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们使用`makeParamSet()`函数创建我们的超参数搜索空间，将每个超参数定义为具有合理上下限的整数。
- en: We define a random search with 100 iterations and start the tuning procedure
    with our forest learner, fuel task, and `holdout` cross-validation strategy.
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个具有100次迭代的随机搜索，并使用森林学习者、fuel任务和`holdout`交叉验证策略开始调整过程。
- en: '|  |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-1031
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This tuning process takes a little while, so let’s use our good friends the
    parallel and parallelMap packages. Using parallelization, this takes 2 minutes
    on my four-core machine.
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调整过程需要一点时间，所以让我们使用我们的好朋友并行和parallelMap包。使用并行化，在我的四核机器上这需要2分钟。
- en: '|  |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 12.8\. Hyperparameter tuning for random forest
  id: totrans-1034
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.8. 随机森林的超参数调整
- en: '[PRE67]'
  id: totrans-1035
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Next, let’s train the random forest model using our tuned hyperparameters. Once
    we’ve trained the model, it’s a good idea to extract the model information and
    pass this to the `plot()` function to plot the out-of-bag error. Recall from [chapter
    8](kindle_split_018.html#ch08) that the out-of-bag error is the mean prediction
    error for each case by trees that *did not* include that case in their bootstrap
    sample. The only difference between the out-of-bag error for classification and
    regression random forests is that in classification, the error was the proportion
    of cases that were misclassified; but in regression, the error is the mean squared
    error.
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用调整好的超参数来训练随机森林模型。一旦我们训练了模型，提取模型信息并将其传递给`plot()`函数以绘制袋外误差是个好主意。回想一下[第8章](kindle_split_018.html#ch08)，袋外误差是由没有包含在该树的bootstrap样本中的树对每个案例的平均预测误差。分类和回归随机森林的袋外误差之间的唯一区别在于，在分类中，误差是错误分类的案例比例；但在回归中，误差是均方误差。
- en: Listing 12.9\. Training the model and plotting the out-of-bag error
  id: totrans-1037
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.9. 训练模型并绘制袋外误差
- en: '[PRE68]'
  id: totrans-1038
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The resulting plot is shown in [figure 12.9](#ch12fig09). It looks like the
    out-of-bag error stabilizes after 30–40 bagged trees, so we can be satisfied that
    we have included enough trees in our forest.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图12.9](#ch12fig09)中。看起来袋外误差在30-40个袋装树之后稳定下来，因此我们可以满意地认为我们已经包含了足够多的树在我们的森林中。
- en: Figure 12.9\. Plotting the out-of-bag error for our random forest model. The
    Error y-axis shows the mean square error for all cases, predicted by trees that
    didn’t include that case in the training set. This is shown for varying numbers
    of trees in the ensemble. The flattening out of the line suggests we have included
    enough individual trees in the forest.
  id: totrans-1040
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.9. 绘制我们的随机森林模型的袋外误差。误差y轴显示了所有情况预测的均方误差，这些预测是由没有包含该案例的训练集的树做出的。这显示了集成中不同树的数量。线条变平表明我们已经包含了足够单独的树在森林中。
- en: '![](fig12-9_alt.jpg)'
  id: totrans-1041
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-9_alt.jpg)'
- en: 12.5\. Building your first XGBoost regression model
  id: totrans-1042
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5. 构建你的第一个XGBoost回归模型
- en: 'In this section, I’ll teach you how to define an XGBoost learner for regression,
    tune its many hyperparameters, and train a model for our fuel task. We’ll start
    by defining our XGBoost learner. Just like for the kNN and random forest learners,
    instead of using `"classif.xgboost"` as in [chapter 8](kindle_split_018.html#ch08),
    the regression equivalent is `"regr.xgboost"`:'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您介绍如何定义一个用于回归的XGBoost学习器，调整其众多超参数，并为我们燃料任务训练一个模型。我们将首先定义我们的XGBoost学习器。就像在[第8章](kindle_split_018.html#ch08)中使用的kNN和随机森林学习器一样，我们不是使用`"classif.xgboost"`，而是使用回归等效的`"regr.xgboost"`：
- en: '[PRE69]'
  id: totrans-1044
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we’re going to tune the hyperparameters of our XGBoost learner: `eta`,
    `gamma`, `max_depth`, `min_child_weight`, `subsample`, `colsample_bytree`, and
    `nrounds`. I first defined what these hyperparameters do in [chapter 8](kindle_split_018.html#ch08),
    but again, let’s recap each one here:'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调整我们的XGBoost学习器的超参数：`eta`、`gamma`、`max_depth`、`min_child_weight`、`subsample`、`colsample_bytree`和`nrounds`。我在[第8章](kindle_split_018.html#ch08)中首先定义了这些超参数的作用，但再次，让我们在这里回顾每个参数：
- en: '`eta` is known as the *learning rate*. It takes a value between 0 and 1, which
    is multiplied by the model weight of each tree to slow down the learning process
    to prevent overfitting.'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`被称为*学习率*。它取0到1之间的值，乘以每棵树的模型权重，以减慢学习过程，防止过拟合。'
- en: '`gamma` is the minimum amount of splitting by which a node must improve the
    loss function (MSE in the case of regression).'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`是节点必须通过分割改进损失函数（在回归的情况下为MSE）的最小分割量。'
- en: '`max_depth` is the maximum number of levels deep that each tree can grow.'
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`是每棵树可以生长的最大深度。'
- en: '`min_child_weight` is the minimum degree of impurity needed in a node before
    attempting to split it (if a node is pure enough, don’t try to split it again).'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_child_weight`是在尝试分割节点之前，节点中需要的最小不纯度（如果一个节点足够纯净，则不要再次尝试分割它）。'
- en: '`subsample` is the proportion of cases to be randomly sampled (without replacement)
    for each tree. Setting this to 1 uses all the cases in the training set.'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`是每个树随机采样（不重复）的案例比例。将此设置为1将使用训练集中的所有案例。'
- en: '`colsample_bytree` is the proportion of predictor variables sampled for each
    tree. We could also tune `colsample_bylevel` and `colsample_bynode`, which instead
    sample predictors for each level of depth in a tree and at each node, respectively.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`是每个树中采样的预测变量比例。我们还可以调整`colsample_bylevel`和`colsample_bynode`，它们分别在每个树的每个深度级别和每个节点处采样预测变量。'
- en: '`nrounds` is the number of sequentially built trees in the model.'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nrounds`是模型中按顺序构建的树的数目。'
- en: '|  |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1054
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: When we used XGBoost for classification problems, we could also tune the `eval_metric`
    hyperparameter to select between the log loss and classification error loss functions.
    For regression problems, we only have one loss function available to us—RMSE—so
    there is no need to tune this hyperparameter.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用XGBoost进行分类问题时，我们还可以调整`eval_metric`超参数，以在日志损失和分类错误损失函数之间进行选择。对于回归问题，我们只有一个可用的损失函数——RMSE，因此不需要调整此超参数。
- en: '|  |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In [listing 12.10](#ch12ex10), we define the type and upper and lower bounds
    of each of these hyperparameters that we’ll search over. We define `max_depth`
    and `nrounds` as integer hyperparameters, and all the others as numerics. I’ve
    chosen sensible starting values for the upper and lower bounds of each hyperparameter,
    but you may find in your own projects you need to adjust your search space to
    find the optimal combination of values. I usually fix the `nrounds` hyperparameter
    as a single value that fits my computational budget to start with, and then plot
    the loss function (RMSE) against the tree number to see if the model error has
    flattened out. If it hasn’t, I increase the `nrounds` hyperparameter until it
    does. We’ll perform this in [listing 12.11](#ch12ex11).
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表12.10](#ch12ex10)中，我们定义了我们将搜索的每个这些超参数的类型和上下限。我们将`max_depth`和`nrounds`定义为整数超参数，其余的为数值。我为每个超参数的上限和下限选择了合理的起始值，但您可能在自己的项目中发现需要调整搜索空间以找到最佳值的组合。我通常将`nrounds`超参数固定为一个适合我的计算预算的单个值，然后绘制损失函数（RMSE）与树数的关系图，以查看模型误差是否已经平坦化。如果没有，我将增加`nrounds`超参数，直到它平坦化。我们将在[列表12.11](#ch12ex11)中执行此操作。
- en: Once the search space is defined, we start the tuning process just like we have
    the previous two times in this chapter.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了搜索空间，我们就开始调整过程，就像我们在本章的前两次一样。
- en: '|  |'
  id: totrans-1059
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-1060
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes around 1.5 minutes on my four-core machine.
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上大约需要1.5分钟。
- en: '|  |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 12.10\. Hyperparameter tuning for XGBoost
  id: totrans-1063
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.10\. XGBoost的超参数调整
- en: '[PRE70]'
  id: totrans-1064
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Now that we have our tuned combination of hyperparameters, let’s train the final
    model using this combination. Once we’ve done this, we can extract the model information
    and use it to plot the iteration number (tree number) against the RMSE to see
    if we included enough trees in our ensemble. The RMSE information for each tree
    number is contained in the `$evaluation_log` component of the model information,
    so we use this as the data argument for the `ggplot()` function, specifying `iter`
    and `train_rmse` to plot the tree number and its RMSE as the x and y aesthetics,
    respectively.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了调整后的超参数组合，让我们使用这个组合来训练最终的模型。一旦我们这样做，我们就可以提取模型信息，并使用它来绘制迭代次数（树数）与RMSE的关系，以查看我们是否在我们的集成中包含了足够的树。每个树数的RMSE信息包含在模型信息的`$evaluation_log`组件中，因此我们使用这个作为`ggplot()`函数的数据参数，指定`iter`和`train_rmse`来分别绘制树数及其RMSE作为x和y的美学。
- en: Listing 12.11\. Training the model and plotting RMSE against tree number
  id: totrans-1066
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.11\. 训练模型并绘制RMSE与树数的关系
- en: '[PRE71]'
  id: totrans-1067
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The resulting plot is shown in [figure 12.10](#ch12fig10). We can see that 30
    iterations/trees is just about enough for the RMSE to have flattened out (including
    more iterations won’t result in a better model).
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图12.10](#ch12fig10)中。我们可以看到，30次迭代/树数几乎足以使RMSE平缓（增加更多的迭代不会导致更好的模型）。
- en: 12.6\. Benchmarking the kNN, random forest, and XGBoost model-building processes
  id: totrans-1069
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6\. 基准测试kNN、随机森林和XGBoost模型构建过程
- en: I love a bit of healthy competition. In this section, we’re going to benchmark
    the kNN, random forest, and XGBoost model-building processes against each other.
    We start by creating tuning wrappers that wrap together each learner with its
    hyperparameter tuning process. Then we create a list of these wrapper learners
    to pass into `benchmark()`. As this process will take some time, we’re going to
    define and use a `holdout` cross-validation procedure to evaluate the performance
    of each wrapper (ideally we would use k-fold or repeated k-fold).
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢一点健康的竞争。在本节中，我们将对kNN、随机森林和XGBoost模型构建过程进行相互基准测试。我们首先创建调整包装器，将每个学习器及其超参数调整过程包装在一起。然后我们创建一个这些包装学习器的列表，将其传递给`benchmark()`函数。由于这个过程将花费一些时间，我们将定义并使用`holdout`交叉验证程序来评估每个包装器的性能（理想情况下我们会使用k折或重复k折）。
- en: Figure 12.10\. Plotting the average root mean square error (`train_rmse`) against
    the iteration of the boosting process. The curve flattens out just before 30 iterations,
    suggesting that we have included enough trees in our ensemble.
  id: totrans-1071
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图12.10\. 绘制平均均方根误差(`train_rmse`)与提升过程迭代的对比。曲线在30次迭代前变平，表明我们已经在我们的集成中包含了足够的树。
- en: '![](fig12-10_alt.jpg)'
  id: totrans-1072
  prefs: []
  type: TYPE_IMG
  zh: '![](fig12-10_alt.jpg)'
- en: '|  |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Warning
  id: totrans-1074
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: It’s tea and cake time! This takes around 7 minutes to run on my four-core machine.
    Using the parallelMap package won’t help because we’re training XGBoost models
    as part of the benchmark, and XGBoost works fastest if you allow it to perform
    its own internal parallelization.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候喝茶吃蛋糕了！在我的四核机器上运行大约需要7分钟。使用parallelMap包不会有所帮助，因为我们正在将XGBoost模型作为基准测试的一部分进行训练，而XGBoost在允许其进行内部并行化时运行最快。
- en: '|  |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 12.12\. Benchmarking kNN, random forest, and XGBoost
  id: totrans-1077
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表12.12\. 基准测试kNN、随机森林和XGBoost
- en: '[PRE72]'
  id: totrans-1078
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: According to this benchmark result, the random forest algorithm is likely to
    give us the best-performing model, with a mean prediction error of 2.485 (the
    square root of 6.174).
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个基准结果，随机森林算法可能为我们提供性能最好的模型，平均预测误差为2.485（6.174的平方根）。
- en: 12.7\. Strengths and weaknesses of kNN, random forest, and XGBoost
  id: totrans-1080
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.7\. kNN、随机森林和XGBoost的优缺点
- en: The strengths and weaknesses of the kNN, random forest, and XGBoost algorithms
    are the same for regression as they were for classification.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: kNN、随机森林和XGBoost算法的优缺点在回归方面与在分类方面相同。
- en: '|  |'
  id: totrans-1082
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 3**'
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习3**'
- en: 'Get a more accurate estimate of each of our model-building processes by rerunning
    the benchmark experiment, changing our `holdout` cross-validation object to our
    `kFold` object. Warning: This took nearly an hour on my four-core machine! Save
    the benchmark result to an object, and pass that object as the only argument to
    the `plotBMRBoxplots()` function.'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新运行基准实验，将我们的`holdout`交叉验证对象更改为`kFold`对象，我们可以更准确地估计我们的模型构建过程。警告：在我的四核机器上这几乎花了一个小时！将基准结果保存到对象中，并将该对象作为唯一参数传递给`plotBMRBoxplots()`函数。
- en: '|  |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 4**'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习4**'
- en: 'Cross-validate the model-building process of the model that won the benchmark
    in [exercise 3](#ch12sb04), but perform 2,000 iterations of the random search
    during hyperparameter tuning. Use `holdout` as the inner cross-validation loop
    and 10-fold cross-validation as the outer loop. Warning: I’d suggest you use parallelization
    and leave this running during lunch or overnight.'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 对在[练习3](#ch12sb04)中赢得基准的模型的建模过程进行交叉验证，但在超参数调整期间进行2,000次随机搜索迭代。使用`holdout`作为内部交叉验证循环，10折交叉验证作为外部循环。警告：我建议您使用并行化，并在午餐或夜间运行此操作。
- en: '|  |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-1090
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The k-nearest neighbors (kNN) and tree-based algorithms can be used for regression
    as well as classification.
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-最近邻（kNN）和基于树的算法可以用于回归以及分类。
- en: When predicting a continuous outcome variable, the predictions made by kNN are
    the mean outcome values of the k-nearest neighbors.
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当预测连续的因变量时，kNN做出的预测是k个最近邻的平均结果值。
- en: When predicting a continuous outcome variable, the leaves of tree-based algorithms
    are the mean of the cases within that leaf.
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当预测连续的因变量时，基于树的算法的叶子是那个叶子内案例的平均值。
- en: Out-of-bag error and RMSE can still be used to identify whether random forest
    and XGBoost ensembles have enough trees, respectively, in regression problems.
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回归问题中，可以使用袋外误差和RMSE来识别随机森林和XGBoost集成是否具有足够的树。
- en: Solutions to exercises
  id: totrans-1095
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题的解决方案
- en: 'Plot `absorbance` versus `heatan` with an additional `geom_smooth()` layer
    that models the whole dataset:'
  id: totrans-1096
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用额外的`geom_smooth()`层绘制`absorbance`与`heatan`的关系图，以模拟整个数据集：
- en: '[PRE73]'
  id: totrans-1097
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Expand the kNN search space to include values between 1 and 50:'
  id: totrans-1098
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将kNN搜索空间扩展到包括1到50之间的值：
- en: '[PRE74]'
  id: totrans-1099
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Use 10-fold cross-validation as the outer cross-validation loop for the benchmark
    experiment:'
  id: totrans-1100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基准实验中，使用10折交叉验证作为外部交叉验证循环：
- en: '[PRE75]'
  id: totrans-1101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Cross-validate the model-building process for the algorithm that won the benchmark,
    performing 2,000 iterations of the random search and using `holdout` as the inner
    cross-validation strategy (inside the tuning wrapper):'
  id: totrans-1102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对赢得基准的算法的建模过程进行交叉验证，执行2,000次随机搜索迭代，并使用`holdout`作为内部交叉验证策略（在调整包装器内部）：
- en: '[PRE76]'
  id: totrans-1103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
