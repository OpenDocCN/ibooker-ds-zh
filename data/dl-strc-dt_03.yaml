- en: '3 Preparing the data, part 1: Exploring and cleansing the data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 准备数据（第一部分）：探索和清洗数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using config files in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中使用配置文件
- en: Ingesting XLS files into Pandas dataframes and saving dataframes with pickle
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 XLS 文件导入 Pandas 数据框并使用 pickle 保存数据框
- en: Exploring the input dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索输入数据集
- en: Categorizing data into continuous, categorical, and text categories
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分类为连续型、分类型和文本型
- en: Correcting gaps and errors in the dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纠正数据集中的缺失值和错误
- en: Calculating data volume for a successful deep learning project
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算成功深度学习项目的数据量
- en: 'In this chapter, you’ll learn how to bring tabular structured data from an
    XLS file into your Python program and how to use the pickle facility in Python
    to save your data structure between Python sessions. You’ll learn how to categorize
    the structured data in the three categories needed by the deep learning model:
    continuous, categorical, and text. You will learn how to detect and deal with
    gaps and errors in the dataset that must be corrected before it can be used to
    train a deep learning model. Finally, you will get some pointers on how to assess
    whether a given dataset is large enough to be applicable to deep learning.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何将表格结构化数据从 XLS 文件导入到你的 Python 程序中，以及如何使用 Python 中的 pickle 功能在 Python
    会话之间保存你的数据结构。你将学习如何将结构化数据分类为深度学习模型所需的三个类别：连续型、分类型和文本型。你将学习如何检测和处理数据集中必须纠正的缺失值和错误，以便在训练深度学习模型之前可以使用。最后，你将获得一些关于如何评估给定的数据集是否足够大，可以应用于深度学习的指导。
- en: 3.1 Code for exploring and cleansing the data
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 探索和清洗数据的代码
- en: After you have cloned the GitHub repo associated with this book ([http://mng.bz/
    v95x](http://mng.bz/v95x)), the code related to exploring and cleansing the data
    will be in the notebooks subdirectory. The next listing shows the files that contain
    the code described in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在你克隆了与本书相关的 GitHub 仓库（[http://mng.bz/v95x](http://mng.bz/v95x)）之后，探索和清洗数据的相关代码将位于
    notebooks 子目录中。下面的列表显示了包含本章描述的代码的文件。
- en: Listing 3.1 Code in the repo related to exploring and cleansing the data
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 仓库中与探索和清洗数据相关的代码
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Directory for pickled input and output dataframes
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于 pickled 输入和输出数据框的目录
- en: ❷ Main data exploration notebook
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 主要数据探索笔记本
- en: ❸ Data preparation notebook
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据准备笔记本
- en: '❹ Config file for the data preparation notebook: whether to load the input
    data from scratch, save the transformed output dataframe, and remove bad values
    as well as the filenames for the pickled input and output dataframes'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 数据准备笔记本的配置文件：是否从头开始加载数据，保存转换后的输出数据框，以及删除坏值以及 pickled 输入和输出数据框的文件名
- en: ❺ Notebook for data exploration using time series approaches
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用时间序列方法进行数据探索的笔记本
- en: 3.2 Using config files with Python
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 使用 Python 配置文件
- en: The main code to clean up the dataset is contained in the notebook streetcar_
    data_preparation.ipynb. This notebook has a companion config file, streetcar_
    data_preparation_config.yml , that is used to set the main parameters. Before
    exploring the data preparation code, let’s examine how this config file and the
    other config files are used in the streetcar delay prediction example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗数据集的主要代码包含在 streetcar_data_preparation.ipynb 笔记本中。这个笔记本有一个配套的配置文件，即 streetcar_data_preparation_config.yml，用于设置主要参数。在探索数据准备代码之前，让我们看看这个配置文件和其他配置文件在电车延误预测示例中的应用。
- en: '*Config files* are files that are external to your Python program where you
    can set parameter values. Your Python program reads the config file and uses the
    values from it for the parameters in the Python program. With config files, you
    separate setting the parameter values from the Python code, so you can update
    the parameter values and rerun your code without having to make updates to the
    code itself. This technique reduces the risk of introducing errors into your code
    when you update parameter values and keeps your code neater. For the streetcar
    delay prediction example, we have config files for all the major Python programs,
    as shown in figure 3.1.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*配置文件* 是位于你的 Python 程序之外的外部文件，你可以在其中设置参数值。你的 Python 程序读取配置文件，并使用其中的值来设置 Python
    程序中的参数。使用配置文件，你可以将设置参数值与 Python 代码分离，这样你就可以更新参数值并重新运行你的代码，而无需对代码本身进行更新。这种技术可以降低在更新参数值时将错误引入代码的风险，并使你的代码更整洁。对于电车延误预测示例，我们为所有主要的
    Python 程序都提供了配置文件，如图 3.1 所示。'
- en: '![CH03_F01_Ryan](../Images/CH03_F01_Ryan.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F01_Ryan](../Images/CH03_F01_Ryan.png)'
- en: Figure 3.1 Summary of the config files used in the streetcar delay prediction
    example
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1电车延误预测示例中使用的配置文件摘要
- en: You can define config files for your Python program as JSON ([https://www .json.org
    json-en.html](https://www.json.org/json-en.html)) or YAML ([https://yaml.org](https://yaml.org/)).
    For the streetcar delay prediction example in this book, we use YAML. Listing
    3.2 shows the config file used by the streetcar_ data_preparation notebook, streetcar_data_preparation_config.yml.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为您的Python程序定义JSON ([https://www.json.org/json-en.html](https://www.json.org/json-en.html))
    或 YAML ([https://yaml.org](https://yaml.org/)) 格式的配置文件。对于本书中的电车延误预测示例，我们使用YAML。列表3.2显示了streetcar_data_preparation笔记本使用的配置文件，streetcar_data_preparation_config.yml。
- en: Listing 3.2 Config file for data preparation streetcar_data_preparation_config.yml
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2数据准备配置文件streetcar_data_preparation_config.yml
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ You can organize your config files with categories. This config file has one
    category for general parameters and another for filenames.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 您可以使用类别来组织您的配置文件。此配置文件有一个用于一般参数的类别，另一个用于文件名。
- en: ❷ Parameter used to control whether the raw XLS files are read directly. If
    this parameter is True, the raw dataset is read from the original XLS files. If
    this parameter is False, the dataset is read from a pickled dataframe.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于控制是否直接读取原始XLS文件的参数。如果此参数为True，则从原始XLS文件中读取原始数据集。如果此参数为False，则从pickle数据框中读取数据集。
- en: ❸ Parameter used to control whether the output dataframe is saved to a pickle
    file
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于控制是否将输出数据框保存到pickle文件的参数
- en: ❹ Parameter used to control whether bad values are included in the output dataframe
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用于控制是否将坏值包含在输出数据框中的参数
- en: ❺ Filename of the pickled data frame to read if load_from_scratch is set to
    False
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果load_from_scratch设置为False，则读取的pickle数据框的文件名
- en: ❻ Filename of the pickled data frame to write if save_transformed_dataframe
    is True
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 如果save_transformed_dataframe为True，则写入的pickle数据框的文件名
- en: The next listing is the code in the streetcar_data_preparation notebook that
    reads the config file and sets the parameters based on the values in the config
    file.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表是streetcar_data_preparation笔记本中读取配置文件并根据配置文件中的值设置参数的代码。
- en: Listing 3.3 Code in the data preparation notebook that ingests the config file
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.3数据准备笔记本中处理配置文件的代码
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Get the path for the notebook.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取笔记本的路径。
- en: ❷ Define the path for the fully qualified config file. Note that the name of
    the config file is one parameter that needs to be hardcoded in the Python code,
    although this should not be a problem because the name of the config file should
    not change. Also note that we use os.path.join to combine the directory and filename
    into a single path. We use this function because it makes the pathname platform
    independent.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义完全合格的配置文件路径。请注意，配置文件的名称是需要在Python代码中硬编码的一个参数，尽管这不应该是一个问题，因为配置文件的名称不应该改变。另外请注意，我们使用os.path.join将目录和文件名组合成一个单独的路径。我们使用此函数是因为它使路径名与平台无关。
- en: ❸ Define the Python dictionary config containing the key/value pairs from the
    config file.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义包含配置文件中键/值对的Python字典config。
- en: ❹ Copy the values from the config dictionary into the variables used throughout
    the rest of the program.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将配置字典中的值复制到程序其余部分使用的变量中。
- en: In this section, you have seen why we use config files with the major Python
    programs in the streetcar delay prediction example along with a description of
    the details of the config file for the data preparation code. Config files are
    particularly handy for this example because we are using pickle files to save
    the interim results. By setting the values for the filenames of these pickle files
    in config files, we can rerun the code on different interim result sets without
    having to modify the code itself.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您已经看到了为什么我们在电车延误预测示例中使用配置文件，以及数据准备代码配置文件详细描述。对于这个例子来说，配置文件特别有用，因为我们使用pickle文件来保存中间结果。通过在配置文件中设置这些pickle文件名的值，我们可以在不同的中间结果集上重新运行代码，而无需修改代码本身。
- en: 3.3 Ingesting XLS files into a Pandas dataframe
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 将XLS文件导入Pandas数据框
- en: In chapter 2, we examined the format of the input dataset for the streetcar
    delay problem. In this section, we will go over how to ingest this dataset into
    a Pandas dataframe in Python. The input dataset is made up of multiple XLS files.
    To start, let’s go through the process of ingesting a single XLS file into a Pandas
    dataframe as shown in the notebook for this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章中，我们检查了电车延误问题的输入数据集的格式。在本节中，我们将介绍如何在Python中将此数据集导入Pandas数据框。输入数据集由多个XLS文件组成。首先，让我们按照本章笔记本中的过程将单个XLS文件导入到Pandas数据框中。
- en: 'First, you need to install the library to read Excel files:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要安装一个库来读取Excel文件：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then you need to get the metadata (tab names) for the XLS file and iterate through
    the list of tab names to load all the tabs into one dataframe, as shown in the
    next listing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要获取XLS文件的元数据（标签名称）并遍历标签名称列表，将所有标签加载到一个数据框中，如以下列表所示。
- en: Listing 3.4 Code to iterate through the tabs in the XLS file
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 遍历XLS文件标签的代码
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Function that returns the path of the data directory
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回数据目录路径的函数
- en: ❷ Import the pandas library.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入pandas库。
- en: ❸ Define the base path and the filename.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义基本路径和文件名。
- en: ❹ Load metadata about the Excel file; then load the first sheet of the XLS file
    into a dataframe.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 加载Excel文件的相关元数据；然后加载XLS文件的第一个工作表到数据框中。
- en: ❺ Iterate through the remaining sheets in the XLS file, appending their contents
    to the dataframe.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 遍历XLS文件中剩余的工作表，并将它们的内 容追加到数据框中。
- en: ❻ Load the current sheet into a dataframe.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将当前工作表加载到数据框中。
- en: ❼ Append this dataframe to the aggregated dataframe.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将此数据框追加到聚合数据框中。
- en: 'The output shows the tab names starting with the second tab name (because the
    first name is loaded before the `for` loop):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示从第二个标签开始显示标签名称（因为第一个名称在`for`循环之前加载）：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After the dataframe is created from the input XLS files, you will notice some
    unexpected columns in the `head` `()` output for the dataframe (figure 3.2).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入的XLS文件创建数据框后，你会在数据框的`head` `()`输出中注意到一些意外的列（如图3.2所示）。
- en: '![CH03_F02_Ryan](../Images/CH03_F02_Ryan.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F02_Ryan](../Images/CH03_F02_Ryan.png)'
- en: Figure 3.2 Loaded dataframe with extraneous columns
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 加载的数据框中包含多余列
- en: 'In addition to the expected Min Delay and Min Gap columns, there are unexpected
    Delay and Gap columns, as well as an unexpected Incident ID column. It turns out
    that the source dataset has some anomalies introduced in the April and June tabs
    of the 2019 XLS file, as shown in figure 3.3\. The April tab of the 2019 XLS file
    has columns called Delay and Gap (rather than Min Delay and Min Gap, like all
    the other tabs in the original dataset), along with an Incident ID column. The
    June tab of the 2019 XLS file has a different issue: instead of the Min Delay
    and Min Gap columns, it has columns called Delay and Gap.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预期的最小延迟和最小间隔列之外，还有意外的延迟和间隔列，以及一个意外的事件ID列。实际上，源数据集在2019 XLS文件的4月和6月标签中引入了一些异常，如图3.3所示。2019
    XLS文件的4月标签有称为延迟和间隔的列（而不是所有其他标签中的最小延迟和最小间隔），以及一个事件ID列。2019 XLS文件的6月标签有一个不同的问题：它没有最小延迟和最小间隔列，而是有称为延迟和间隔的列。
- en: '![CH03_F03a_Ryan](../Images/CH03_F03a_Ryan.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F03a_Ryan](../Images/CH03_F03a_Ryan.png)'
- en: '![CH03_F03b_Ryan](../Images/CH03_F03b_Ryan.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F03b_Ryan](../Images/CH03_F03b_Ryan.png)'
- en: Figure 3.3 Anomalies in the April 2019 and June 2019 tabs of the 2019 XLS file
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 2019 XLS文件4月和6月标签中的异常
- en: Because these anomalous columns appear in two tabs in the 2019 XLS file, if
    you read in the complete dataset including the XLS file for 2019, then the overall
    dataframe gets these columns as well. The data preparation notebook includes the
    code in the following listing, which corrects the problem by copying over the
    needed data and then deleting the extraneous columns.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些异常列出现在2019 XLS文件的两个标签中，如果你读取包括2019年XLS文件的完整数据集，那么整体数据框也会包含这些列。数据准备笔记本包括以下列表中的代码，通过复制所需数据然后删除多余的列来纠正问题。
- en: Listing 3.5 Code to correct the anomalous columns
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.5 修正异常列的代码
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ If there is NaN in the Min Delay or Min Gap columns, copy over the value from
    Delay or Gap.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果最小延迟或最小间隔列中存在NaN，则从延迟或间隔中复制值。
- en: ❷ Now that the useful values have been copied from Delay and Gap, remove the
    Delay and Gap columns.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在已经从延迟和间隔中复制了有用的值，删除延迟和间隔列。
- en: ❸ Remove the Incident ID column; it’s extraneous.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 删除事件ID列；它是多余的。
- en: ❹ Return the updated dataframe.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 返回更新后的数据框。
- en: After this cleanup, the output of `head` `()` confirms that the anomalous columns
    have been eliminated and that the dataframe has the expected columns (see figure
    3.4).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在此清理之后，`head` 函数的输出确认了异常列已被消除，并且数据框具有预期的列（见图 3.4）。
- en: '![CH03_F04_Ryan](../Images/CH03_F04_Ryan.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F04_Ryan](../Images/CH03_F04_Ryan.png)'
- en: Figure 3.4 The beginning of the dataframe containing all tabs of the input XLS
    file
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 包含输入 XLS 文件所有标签页的数据框的开始部分
- en: We check the output of `tail` `()` to confirm that the end of the dataframe
    is also as expected (see figure 3.5).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查 `tail` 函数的输出以确认数据框的末尾也符合预期（见图 3.5）。
- en: '![CH03_F05_Ryan](../Images/CH03_F05_Ryan.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F05_Ryan](../Images/CH03_F05_Ryan.png)'
- en: Figure 3.5 The end of the dataframe containing all tabs of the input XLS file
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 包含输入 XLS 文件所有标签页的数据框的结束部分
- en: There’s an important lesson to be learned from these anomalies in the source
    streetcar dataset. With a live, real-world dataset, we need to be prepared to
    be nimble to account for changes in the dataset. While I was writing this book,
    the anomalies described in this section were introduced into the dataset, so I
    needed to be prepared to update the data preparation code to address these anomalies.
    When you are dealing with a dataset that you don’t control, you need to be able
    to take unintended changes in the dataset in stride.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从源电车数据集中的这些异常中可以学到重要的一课。在使用实时、真实世界的数据集时，我们需要准备好灵活应对数据集的变化。在我撰写这本书的时候，本节中描述的异常被引入到数据集中，因此我需要准备好更新数据准备代码以解决这些异常。当你处理一个你无法控制的数据集时，你需要能够从容应对数据集中的意外变化。
- en: Now that you know how to load a single XLS file, let’s work through how to ingest
    the entire input dataset by bringing data from multiple XLS files into a single
    Pandas dataframe.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何加载单个 XLS 文件，让我们通过将来自多个 XLS 文件的数据引入单个 Pandas 数据框来了解如何导入整个输入数据集。
- en: The code examples in this section come from the streetcar_data_preparation notebook.
    This notebook assumes that you have copied all the XLS files from the raw input
    dataset ([http://mng.bz/ry6y](http://mng.bz/ry6y)) to a directory called data,
    which is a sibling of the directory that contains the notebook.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码示例来自 streetcar_data_preparation 笔记本。该笔记本假设你已经将所有 XLS 文件从原始输入数据集（[http://mng.bz/ry6y](http://mng.bz/ry6y)）复制到了名为
    data 的目录中，该目录是包含笔记本的目录的兄弟目录。
- en: The code in the streetcar data preparation notebook uses two functions to ingest
    multiple XLS files into a single dataframe. The `reloader` function primes the
    process by loading the first tab of the first file into a dataframe and then calls
    the `load_xls` function to load the remaining tabs of the first file, along with
    all the tabs of all the other XLS files. The code in the next listing assumes
    that the XLS files in the data directory are exactly the XLS files that make up
    the dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 街车数据准备笔记本中的代码使用两个函数将多个 XLS 文件导入到单个数据框中。`reloader` 函数通过将第一个文件的第一个标签页加载到数据框中启动这个过程，然后调用
    `load_xls` 函数来加载第一个文件的其余标签页以及所有其他 XLS 文件的所有标签页。下一列表中的代码假设数据目录中的 XLS 文件正好是构成数据集的
    XLS 文件。
- en: Listing 3.6 Code to ingest the XLS file
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 导入 XLS 文件的代码
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Load all the tabs of all the XLS files in a list of XLS files, minus the tab
    that has the seeded dataframe.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将列表中所有 XLS 文件的所有标签页加载到一个数据框中，但不包括包含已播种数据框的标签页。
- en: ❷ Iterate through all the XLS files in the directory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历目录中的所有 XLS 文件。
- en: ❸ Iterate through all the sheets in the current XLS file.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历当前 XLS 文件中的所有标签页。
- en: ❹ Append the dataframe for the current sheet to the overall dataframe.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将当前工作表的 data frame 追加到整体数据框中。
- en: The code in the following listing shows the `reloader` function that calls the
    `load_xls` function to ingest all XLS files and saves the result in a pickled
    dataframe.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表代码展示了 `reloader` 函数，该函数调用 `load_xls` 函数来导入所有 XLS 文件，并将结果保存为 pickle 格式的数据框。
- en: Listing 3.7 Code to ingest multiple XLS files and save the result in a pickled
    dataframe
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 导入多个 XLS 文件并将结果保存为 pickle 格式数据框的代码
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Given a path and a filename, load all the XLS files in the path into a dataframe.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 给定一个路径和一个文件名，将路径中的所有 XLS 文件加载到一个数据框中。
- en: ❷ Get the list of all XLS files in the path.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取路径中所有 XLS 文件的列表。
- en: ❸ Seed the initial tab on the initial XLS file.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在初始 XLS 文件的初始标签页上播种。
- en: ❹ Get the list of sheets in the first file.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取第一个文件中所有标签页的列表。
- en: ❺ Load the remaining tabs from all the other XLS files.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从所有其他 XLS 文件中加载剩余的标签页。
- en: ❻ Save the dataframe to a pickle file.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将数据框保存到 pickle 文件中。
- en: ❼ Return the dataframe loaded with all tabs of all XLS files.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 返回加载了所有 XLS 文件所有标签的数据框。
- en: How do you get the correct value for path, the directory where you have copied
    the XLS files that make up the dataset? The code assumes that all the data exists
    in a directory called data, which is a sibling of the directory that contains
    the notebook. The next listing is the code snippet introduced in chapter 2 that
    gets the correct path for the directory containing the XLS files; it gets the
    current directory (where the notebook resides) and the path for the directory
    called data that is a sibling of this directory.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何获取正确的路径值，即你复制了构成数据集的 XLS 文件的目录？代码假设所有数据都存在于一个名为 data 的目录中，这个目录是这个笔记本所在目录的兄弟目录。接下来的列表是第
    2 章中引入的代码片段，它获取包含 XLS 文件的目录的正确路径；它获取当前目录（笔记本所在的位置）以及这个目录兄弟目录 data 的路径。
- en: Listing 3.8 Code to get the path of the data directory
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.8 获取数据目录路径的代码
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Get the directory that this notebook is in.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取这个笔记本所在的目录。
- en: ❷ Get the fully qualified path of the directory data that is a sibling of the
    directory that this notebook is in.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取与这个笔记本所在的目录同级的目录 data 的完整合格路径。
- en: 3.4 Using pickle to save your Pandas dataframe from one session to another
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 使用 pickle 在一个会话到另一个会话之间保存你的 Pandas 数据框
- en: A Pandas dataframe exists for the life of your notebook session. This consideration
    is an important one, particularly when you are using a cloud environment such
    as Paperspace. When you have shut down the notebook (either explicitly or by closing
    your cloud session), you lose the dataframes you created during your session.
    The next time you want to do more work, you have to reload the data from scratch.
    What can you do if you want your Pandas dataframe to persist between sessions,
    to save you from having to reload the data from its source every time, or if you
    want to share a dataframe between two notebooks?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 数据框存在于你的笔记本会话期间。这个考虑因素非常重要，尤其是当你使用 Paperspace 这样的云环境时。当你关闭笔记本（无论是明确关闭还是通过关闭云会话）时，你将丢失在会话期间创建的数据框。下次你想做更多的工作时，你必须从头开始重新加载数据。如果你想让
    Pandas 数据框在会话之间持续存在，以避免每次都必须从其源重新加载数据，或者如果你想在两个笔记本之间共享数据框，你该怎么办？
- en: For modest-sized datasets, the answer to the problem of keeping a dataframe
    beyond the life of a session is pickle. This extremely useful standard Python
    library allows you to save your Python objects (including Pandas dataframes) as
    files in your filesystem that you can later read back into Python. Before I go
    into the details of how to use pickle, it behooves me to acknowledge that not
    everybody is a fan of pickle. Ben Frederickson, for example, argues that pickle
    is less efficient than serialization alternatives like JSON and can expose you
    to security issues if you unpickle files whose origin is not known ([https://www.benfrederickson.com/dont-pickle-your-data](https://www.benfrederickson.com/dont-pickle-your-data)).
    In addition, pickle isn’t the right choice for every use case. It isn’t recommended
    if you need to share serialized objects between programming languages (pickle
    is for Python only), for example, or between levels of Python. You can run into
    issues if you pickle an object in one level of Python and then try to bring it
    into another piece of code that is running at a different Python level. For the
    purposes of the example described in this book, I am sticking with pickle because
    it simplifies the serialization process and because the provenance of all the
    pickle files is known.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于适度大小的数据集，保持数据框超过会话生命周期的答案是 pickle。这个极其有用的标准 Python 库允许你将你的 Python 对象（包括 Pandas
    数据框）保存为文件系统中的文件，你可以在以后将其读回到 Python 中。在详细介绍如何使用 pickle 之前，我必须承认并不是每个人都喜欢 pickle。例如，Ben
    Frederickson 认为 pickle 的效率不如 JSON 这样的序列化替代方案，如果解包来源不明的文件，可能会暴露你于安全风险（[https://www.benfrederickson.com/dont-pickle-your-data](https://www.benfrederickson.com/dont-pickle-your-data)）。此外，pickle
    也不是每个用例的正确选择。如果你需要在编程语言之间共享序列化对象（pickle 仅适用于 Python），例如，或在 Python 的不同级别之间，那么它是不推荐的。如果你在一个
    Python 级别中 pickle 一个对象，然后尝试将其带入运行在不同 Python 级别的另一段代码中，你可能会遇到问题。对于本书中描述的示例目的，我坚持使用
    pickle，因为它简化了序列化过程，并且所有 pickle 文件的来源都是已知的。
- en: Suppose that you want to work with the publicly available Iris dataset rather
    than copy it to your filesystem, but you are working in an environment in which
    your internet connection is not reliable. For the sake of this example, assume
    that you are using a locally installed machine learning framework, such as Jupyter
    Notebooks installed in your local system, so you can work on notebooks when you
    are offline. You want to be able to load the dataframe while you are connected
    to the internet and then save the dataframe to your filesystem so you can reload
    the dataframe and keep working when you are offline.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要使用公开可用的Iris数据集，而不是将其复制到你的文件系统中，但你正在一个网络连接不可靠的环境中工作。为了这个例子，假设你正在使用本地安装的机器学习框架，例如在你的本地系统中安装的Jupyter
    Notebooks，这样你就可以在离线时工作在notebooks上。你希望在连接到互联网时能够加载dataframe，然后将dataframe保存到你的文件系统中，这样你就可以在离线时重新加载dataframe并继续工作。
- en: First, you load the Iris dataset into a dataframe, just as you did in chapter
    2 (list-ing 3.9), as shown next.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将Iris数据集加载到dataframe中，就像你在第2章中做的那样（列表3.9），如下所示。
- en: Listing 3.9 Code to ingest a CSV file using a URL reference
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.9 使用URL引用导入CSV文件的代码
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Raw GitHub URL for Iris dataset
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Iris数据集的原始GitHub URL
- en: ❷ Read the contents of the URL into a Pandas dataframe.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将URL的内容读取到Pandas dataframe中。
- en: Next, you call the `to_pickle()` method to save the dataframe to a file in your
    filesystem, as shown in the following listing. By convention, pickle files have
    the extension `pkl`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你调用`to_pickle()`方法将dataframe保存到你的文件系统中的一个文件中，如下所示。按照惯例，pickle文件的扩展名为`pkl`。
- en: Listing 3.10 Code to save a dataframe as a pickle file
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.10 保存dataframe为pickle文件的代码
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Define a filename for the pickle file.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为pickle文件定义一个文件名。
- en: ❷ Write the dataframe to the named pickle file.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将dataframe写入命名的pickle文件。
- en: Now when you are on a flight with no internet connection and want to keep working
    with this dataset, all you have to do is call the `read_pickle` method with the
    pickle file you saved as the argument, as the next listing shows.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你正在没有互联网连接的航班上，并想继续使用这个数据集，你所要做的就是调用`read_pickle`方法，并将你保存的pickle文件作为参数，如下所示。
- en: Listing 3.11 Code to read a pickle file into a dataframe
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.11 将pickle文件读取到dataframe中的代码
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Call the read_pickle function to read the pickle file into a dataframe.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调用read_pickle函数将pickle文件读取到dataframe中。
- en: The output of `head` `()` shows that you have the data loaded into a dataframe
    again without having to go back to the original source dataset (figure 3.6).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`head()`函数的输出显示，你已将数据重新加载到dataframe中，而无需回到原始数据源（图3.6）。'
- en: '![CH03_F06_Ryan](../Images/CH03_F06_Ryan.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_Ryan](../Images/CH03_F06_Ryan.png)'
- en: Figure 3.6 Results of unpickling a dataframe that was saved in a pickle file
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 将保存为pickle文件的dataframe反序列化的结果
- en: Figure 3.7 summarizes the flow from the original source dataset (CSV file) to
    a Pandas dataframe, then to a pickle file, and finally back to the Pandas dataframe.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 从原始源数据集（CSV文件）到Pandas dataframe，再到pickle文件，最后回到Pandas dataframe的流程总结。
- en: '![CH03_F07_Ryan](../Images/CH03_F07_Ryan.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_Ryan](../Images/CH03_F07_Ryan.png)'
- en: Figure 3.7 Life cycle of a dataset
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 数据集的生命周期
- en: Pickling is extremely useful if you have a large dataset that takes some time
    to load into a dataframe from its external source. For large datasets, unpickling
    a saved dataframe can be much faster than reloading the dataframe from its external
    source.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有大量数据集，且将其从外部源加载到dataframe中需要一些时间，那么pickle操作将非常有用。对于大型数据集，从保存的dataframe中反序列化通常比重新从外部源加载数据更快。
- en: 3.5 Exploring the data
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 探索数据
- en: Now that we’ve learned how to ingest the complete input dataset into a Pandas
    dataframe and how to make this dataset persistent between sessions, we need to
    explore the data to understand its characteristics. By using the facilities that
    are available in Python to visualize the data, we can explore the data to find
    patterns and anomalies that help us make good choices for the downstream processes.
    You can find the code for this section in the streetcar_data_exploration notebook
    and in the streetcar _time_series notebook.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何将完整输入数据集导入到Pandas dataframe中，以及如何使数据集在会话之间持久化，我们需要探索数据以了解其特征。通过使用Python中可用的数据可视化工具，我们可以探索数据以找到模式和异常，这有助于我们为下游过程做出良好的选择。你可以在这个部分的代码在streetcar_data_exploration笔记本和streetcar
    _time_series笔记本中找到。
- en: First let’s use `describe` `()` on the raw dataframe (see figure 3.8).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们对原始dataframe使用`describe()`函数（见图3.8）。
- en: '![CH03_F08_Ryan](../Images/CH03_F08_Ryan.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Ryan](../Images/CH03_F08_Ryan.png)'
- en: Figure 3.8 Output of describe()
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 describe()函数的输出
- en: 'Here are a few items to note:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些需要注意的事项：
- en: Route and Vehicle are being interpreted as continuous; we need to correct the
    types of these two columns.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路线和车辆被解释为连续的；我们需要纠正这两个列的类型。
- en: The maximum delay is 23 hours, and the maximum gap is 72 hours. Both values
    look incorrect. We need to check the records to confirm whether they are incorrect.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大延迟为23小时，最大间隔为72小时。这两个值看起来都不正确。我们需要检查记录以确认它们是否不正确。
- en: The average delay is 12 minutes; the average gap is 18 minutes.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均延迟时间为12分钟；平均间隔时间为18分钟。
- en: Taking a random sample of the dataset by using `sample` `()` , we get the output
    shown in figure 3.9.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`sample()`函数随机抽样数据集，我们得到图3.9所示的输出。
- en: '![CH03_F09_Ryan](../Images/CH03_F09_Ryan.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Ryan](../Images/CH03_F09_Ryan.png)'
- en: Figure 3.9 Output of sample() on the raw input dataframe
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 对原始输入数据框的sample()函数的输出
- en: What does this output tell us?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出告诉我们什么？
- en: Some incidents have zero-length delays and gaps, which isn’t what we’d expect
    from a dataset that is supposed to record delays. We need to review these records
    to determine whether the zero-length delays and gaps are intentional or errors.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些事件有零长度的延迟和间隔，这与预期记录延迟的数据集不符。我们需要审查这些记录以确定零长度的延迟和间隔是故意的还是错误。
- en: 'Location values do not have consistent junction words: `at` , `and` , and `&`
    all appear in this random sample'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置值没有一致的连接词：`at`、`and`和`&`都出现在这个随机样本中
- en: Incident may be a categorical column. We should count the number of unique values
    to determine whether it makes sense to treat Incident as a categorical column.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件可能是一个分类列。我们应该计算唯一值的数量，以确定是否可以将事件视为一个分类列。
- en: 'If we look at the number of unique values for Incident, we see that it is a
    small enough number for Incident to be a categorical column:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看事件唯一值的数量，我们会发现这个数字足够小，使得事件可以被视为一个分类列：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This confirms that Incident should be treated as a categorical column rather
    than a freeform text column.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了事件应该被视为一个分类列，而不是一个自由文本列。
- en: Let’s explore the relative size of Min Delay and Min Gap in the next listing
    by counting how many times one value is larger than the other for a given incident.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一个列表中通过计算给定事件中一个值比另一个值大多少来探索最小延迟和最小间隔的相对大小。
- en: Listing 3.12 Code to count how many times Min Delay is bigger or smaller than
    Min Gap
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.12 计算最小延迟比最小间隔大或小的次数的代码
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Get the number of records in which Min Gap is greater than Min Delay.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取最小间隔大于最小延迟的记录数量。
- en: ❷ Number of these records
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这些记录的数量
- en: ❸ Get the number of records in which Min Gap is less than Min Delay.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取最小间隔小于最小延迟的记录数量。
- en: ❹ Number of these records
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这些记录的数量
- en: The result tells us that Min Gap is usually longer than Min Delay for a given
    incident, but about 3% of the time, Min Delay is longer. This isn’t what we’d
    expect. We need to review these records where Min Delay is longer than Min Gap
    to determine whether they are errors.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结果告诉我们，对于给定的事件，最小间隔通常比最小延迟长，但大约3%的时间，最小延迟比最小间隔长。这不是我们预期的。我们需要审查这些最小延迟比最小间隔长的记录，以确定它们是否是错误。
- en: Next, let’s look at a view that clusters the number of incidents by month (see
    figure 3.10). You can find the code to generate the charts in this section in
    the streetcar_time _series notebook. Each dot in this view represents the total
    incidents in a month for a given year. Tight vertical clusters of dots mean that
    the number of incidents doesn’t vary much from year to year.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看按月聚类事件数量的视图（见图3.10）。你可以在这个部分的streetcar_time_series笔记本中找到生成图表的代码。在这个视图中，每个点代表给定年份一个月内的事件总数。点的紧密垂直簇意味着事件数量在年份间变化不大。
- en: '![CH03_F10_Ryan](../Images/CH03_F10_Ryan.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F10_Ryan](../Images/CH03_F10_Ryan.png)'
- en: Figure 3.10 Delay incidents by month
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 按月延迟事件
- en: The next listing shows the code that generates this chart.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了生成此图表的代码。
- en: Listing 3.13 Code to generate the delay incidents by month chart
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.13 生成按月图表的延迟事件的代码
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Plot with the month of the year on the x axis and count of delays on the y
    axis
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在x轴上按年月绘制，在y轴上绘制延迟计数
- en: ❷ Render the plot.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 渲染图表。
- en: What does this view tell us?
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视图告诉我们什么？
- en: The number of incidents in March, April, September, and October (and arguably
    July) doesn’t vary as much from year to year as in other months.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三月、四月、九月和十月（或许还有七月）的事件数量在年份间的变化不如其他月份。
- en: January, February, and December have the ranges with the highest top ends.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一月、二月和十二月拥有最高的最高值范围。
- en: Can we draw any conclusions from these observations? Perhaps there are more
    incidents during the months with more extreme temperatures. Perhaps the number
    of incidents is more variable during months with unpredictable weather. Both of
    these conclusions are reasonable, but neither is certain. We need to let the data
    drive the conclusions. It is possible that weather is a contributing factor to
    the number of delays, but we need to be careful not to assign causality without
    supporting data. In chapter 9, we discuss adding weather as an additional data
    source to the streetcar prediction model. The delays by month chart indicates
    this might be a useful exercise.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能从这些观察结果中得出任何结论吗？或许在温度极端的月份会有更多事件发生。或许在天气不可预测的月份，事件的数量变化更大。这两个结论都是合理的，但都不确定。我们需要让数据来驱动结论。有可能天气是导致延误数量的一个影响因素，但我们需要小心不要在没有支持数据的情况下归因。在第9章中，我们讨论了将天气作为额外数据源添加到电车预测模型中。按月延迟图表表明这可能是一项有用的练习。
- en: Now let’s look at a rolling average of delay duration. The data point for each
    month on this chart is the average of the delay duration for the six preceding
    months (see figure 3.11).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看延迟持续时间的滚动平均值。此图表上每个月的数据点是前六个月延迟持续时间的平均值（见图3.11）。
- en: '![CH03_F11_Ryan](../Images/CH03_F11_Ryan.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F11_Ryan](../Images/CH03_F11_Ryan.png)'
- en: Figure 3.11 Rolling average delay duration
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 滚动平均延迟持续时间
- en: The following listing is the code that generates this chart.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表是生成此图表的代码。
- en: Listing 3.14 Code to generate the rolling average delay duration chart
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.14 生成滚动平均延迟持续时间图表的代码
- en: '[PRE16]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Plot the six-month rolling average of delay duration with data points by month.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以月份为数据点绘制延迟持续时间的六个月滚动平均值。
- en: ❷ Render the plot.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 渲染图表。
- en: Figure 3.11 tells us that there has been an overall trend of delay incidents
    getting shorter, with an uptick in 2019.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11告诉我们，延迟事件的整体趋势是缩短，但在2019年有所上升。
- en: Python offers many options for exploring a dataset. This section showed a useful
    subset of these options along with possible actions to take as a result of the
    exploration.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Python提供了许多探索数据集的选项。本节展示了这些选项的有用子集，以及探索结果后可能采取的行动。
- en: The next listing is the code for the chart in figure 3.12.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表是图3.12中图表的代码。
- en: Listing 3.15 Code to generate the rolling average delay count chart
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.15 生成滚动平均延迟计数图表的代码
- en: '[PRE17]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Plot the six-month rolling average of delay counts with data points by month.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以月份为数据点绘制延迟计数的六个月滚动平均值。
- en: ❷ Render the plot.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 渲染图表。
- en: '![CH03_F12_Ryan](../Images/CH03_F12_Ryan.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_Ryan](../Images/CH03_F12_Ryan.png)'
- en: Figure 3.12 Rolling average delay count
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 滚动平均延迟计数
- en: Figure 3.12 suggests that the trend toward delay count is increasing, opposite
    the trend toward delay duration.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12表明，延迟计数的趋势是增加的，与延迟持续时间的趋势相反。
- en: 3.6 Categorizing data into continuous, categorical, and text categories
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 将数据分类为连续、分类和文本类别
- en: 'Now that you have explored the data, it’s time to tackle the problem of how
    to categorize the columns in the dataset. The approach described in this book
    is based on grouping the input columns into three categories:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经探索了数据，是时候解决如何对数据集中的列进行分类的问题了。本书中描述的方法是基于将输入列分为三个类别：
- en: '*Continuous* —These values are numeric and are values to which arithmetic can
    be applied. Examples of continuous values include temperatures, currency values,
    time spans (such as elapsed hours), and counts of objects and activities.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连续* —这些值是数值，并且可以对这些值进行算术运算。连续值的例子包括温度、货币价值、时间跨度（如已过小时数）和对象和活动的计数。'
- en: '*Categorical* —These values can be single strings, such as the days of the
    week, or collections of one or more strings that constitute an identifier, such
    as the names of the U.S. states. The number of distinct values in categorical
    columns can range from two to several thousand.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类* —这些值可以是单个字符串，例如一周中的某一天，或者构成标识符的一组一个或多个字符串，例如美国各州的名称。分类列中不同值的数量可以从两个到几千个不等。'
- en: '*Text* —These values are sets of strings.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本* —这些值是字符串集合。'
- en: 'This categorization is critical for two reasons:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类对于两个原因至关重要：
- en: Like other machine learning algorithms, deep learning algorithms work on numerical
    values. The final data stream that is fed into the deep learning model needs to
    consist entirely of numerical values, so all non-numerical values need to be transformed
    into numerical values. The categorization tells us whether such a transformation
    is needed for a column and, if so, what kind of transformation is needed.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与其他机器学习算法一样，深度学习算法在数值上工作。最终输入到深度学习模型中的数据流需要完全由数值组成，因此所有非数值都需要转换成数值。分类告诉我们是否需要为一个列进行这种转换，如果是的话，需要什么样的转换。
- en: As you will see in chapter 5, the layers of the deep learning models are built
    up automatically based on the categories of the input columns. Each type of column
    (continuous, categorical, and text) generates layers with different characteristics.
    By categorizing the input columns, you make it possible for the code in the streetcar_model_training
    notebook to build a deep learning model automatically, which means that if you
    add columns to or drop columns from your dataset, the model will be updated automatically
    when you rerun the notebook.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如你在第5章中将会看到的，深度学习模型的层是根据输入列的分类自动构建的。每种类型的列（连续型、分类型和文本型）都会生成具有不同特征的层。通过分类输入列，你使得streetcar_model_training笔记本中的代码能够自动构建深度学习模型，这意味着如果你向你的数据集中添加列或从数据集中删除列，当你重新运行笔记本时，模型将自动更新。
- en: 'Figure 3.13 shows how the columns can be categorized for the input streetcar
    delay dataset:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13展示了如何对输入的街车延误数据集的列进行分类：
- en: '![CH03_F13_Ryan](../Images/CH03_F13_Ryan.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F13_Ryan](../Images/CH03_F13_Ryan.png)'
- en: Figure 3.13 Categorizing columns in the input dataset for the main example in
    this book
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 本书中主要示例的输入数据集列的分类
- en: 'Here is a description of the categorization of each column:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个列分类的描述：
- en: Min Delay and Min Gap are measurements of elapsed time and thus are continuous
    columns.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小延误和最小间隔是已过时间的测量，因此是连续列。
- en: Incident is a categorical column that describes what caused the interruption
    of service. There are nine valid values.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件是一个分类列，描述了导致服务中断的原因。有九个有效值。
- en: Route is a categorical column. There are 12 valid values.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路线是一个分类列，有12个有效值。
- en: Day is a categorical column with seven valid values.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期是一个具有七个有效值的分类列。
- en: Location is a categorical column, even though it is a freeform field when the
    data is entered. This column is an example that requires preprocessing to map
    a large number of potential values to a smaller set of unique values. In chapter
    4, we’ll discuss the pros and cons of mapping these location strings to strictly
    defined longitude and latitude values.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置是一个分类列，尽管在数据输入时它是一个自由格式字段。这个列是一个需要预处理以将大量潜在值映射到更小的一组唯一值的例子。在第4章中，我们将讨论将这些位置字符串映射到严格定义的经纬度值的优势和劣势。
- en: 'Direction is a categorical column with five valid values: the four compass
    points and a value to indicate multiple directions.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方向是一个具有五个有效值的分类列：四个方向和表示多个方向的一个值。
- en: Vehicle is a categorical column, even though the values in it look like floating-point
    values when they are initially brought into a dataframe. The values in this column
    are actually four-character identifiers for 300 or so active streetcar vehicles.
    In chapter 4, we will deal with the mismatch between the type that Python assigns
    to this column and the actual type of the data.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆是一个分类列，尽管当这些值最初被引入到dataframe中时，它们看起来像浮点值。这个列中的值实际上是大约300辆活跃街车的四字符标识符。在第4章中，我们将处理Python分配给这个列的类型与实际数据类型之间的不匹配问题。
- en: 'What about the temporal columns Report Date and Time? In chapter 5, we’ll describe
    an approach to parsing the values in these columns into new categorical columns
    that identify the most interesting temporal aspects of the incidents: year, month,
    day of month, and hour.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 那么时间列“报告日期和时间”呢？在第5章中，我们将描述一种方法，将这些列中的值解析成新的分类列，以识别事件中最有趣的时序方面：年份、月份、月份中的日期和小时。
- en: Note that in some cases, a column could legitimately fit into more than one
    category. A column containing timestamps, for example, could be treated as continuous
    or categorical, depending on the requirements of your business problem. The good
    news is that the approach described in this book is flexible enough that you can
    change your mind about the category for a column and retrain the model with minimal
    disruption.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在某些情况下，一列可能合法地属于多个类别。例如，包含时间戳的列可以根据你的业务问题的需求被处理为连续或分类。好消息是，本书中描述的方法足够灵活，你可以改变对某一列类别的看法，并以最小的干扰重新训练模型。
- en: '3.7 Cleaning up problems in the dataset: missing data, errors, and guesses'
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 清理数据集中的问题：缺失数据、错误和猜测
- en: The streetcar delay problem is a good illustration of applying deep learning
    to tabular structured data because the input dataset is messy, with lots of missing,
    invalid, and extraneous values. Real-world problems that we want to solve with
    deep learning involve these kinds of messy datasets, so cleaning up messy data
    is one thing we need to learn how to do if we want to harness deep learning to
    solve practical problems with tabular, structured data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 电车延误问题是应用深度学习到表格结构数据的良好示例，因为输入数据集很混乱，有很多缺失、无效和多余的值。我们希望用深度学习解决的问题涉及这些类型的混乱数据集，因此清理混乱数据是我们需要学会如何做的一件事，如果我们想利用深度学习来解决使用表格结构数据解决的实际问题。
- en: 'We need to clean up these problems in the dataset for a variety of reasons:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要清理数据集中的这些问题，原因有很多：
- en: The missing values need to be dealt with because the deep learning model cannot
    be trained on datasets that have missing values.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值需要被处理，因为深度学习模型不能在包含缺失值的数据集上进行训练。
- en: The invalid values need to be dealt with because, as you will see in chapter
    7, training the deep learning model with the invalid values still in place reduces
    the performance of the trained model.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无效值需要被处理，因为正如你将在第7章中看到的，在无效值仍然存在的情况下训练深度学习模型会降低训练模型的性能。
- en: The extraneous values—multiple distinct tokens for the same real-world characteristic
    (such as `E/B,` `e/b,` and `eb` for *eastbound*)—need to be dealt with because
    carrying them through the rest of the process increases the complexity of the
    model without providing any additional signal to be picked up in the training
    process.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要处理这些额外的值——对于同一现实世界特征的多个不同标记（例如，`E/B,` `e/b,` 和 `eb` 代表 *向东行驶*）——因为这些值在后续过程中的携带会增加模型的复杂性，而不会在训练过程中提供任何额外的信号。
- en: 'Following are the characteristics we want the dataset to have when the cleanup
    process is complete:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在清理过程完成后我们希望数据集具有的特征：
- en: '*All values are numeric* *.* Machine learning algorithms depend on all the
    data being numeric. Missing values need to be replaced, and non-numeric values
    (in categorical or text columns) need to be replaced by numeric identifiers.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*所有值都是数值型* *.* 机器学习算法依赖于所有数据都是数值型。缺失值需要被替换，非数值型值（在分类或文本列中）需要被数值标识符替换。'
- en: '*Records with invalid values* *are identified and eliminated.* The reason for
    eliminating invalid values (such as locations that are outside the geographic
    area of the streetcar network or vehicle IDs that are not valid streetcar IDs)
    is to prevent the model from being trained with data that isn’t a reflection of
    the real-world problem.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*包含无效值的记录* *将被识别并消除*。消除无效值（例如，位于电车网络地理区域之外的位置或无效的电车ID）的原因是为了防止模型在训练过程中使用不反映现实世界问题的数据进行训练。'
- en: '*Extraneous categories* *are identified and eliminated.* We know that the Direction
    column should have only five valid values (the compass points plus an identifier
    to indicate both directions). All records need to use the same consistent categories.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多余的类别* *将被识别并消除*。我们知道，方向列应该只有五个有效值（方向指示符加上表示两个方向的标识符）。所有记录都需要使用相同的统一类别。'
- en: 'First, let’s take a look at the missing values in each column. Deep learning
    models work only with numbers as input. All the values that are used to train
    deep learning models must be numbers. The reason is that the data that is used
    to train the deep learning model needs to repeatedly undergo the process described
    in chapter 1: multiplication by weights, addition of bias values, and application
    of an activation function. These operations cannot work with missing values, character
    values, or anything else that is not a number, so dealing with missing values
    is a fundamental part of cleaning up the input dataset.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看每个列中的缺失值。深度学习模型只处理数字作为输入。用于训练深度学习模型的所有值都必须是数字。原因是用于训练深度学习模型的数据需要反复进行第1章中描述的过程：乘以权重、添加偏差值以及应用激活函数。这些操作不能与缺失值、字符值或任何不是数字的东西一起工作，因此处理缺失值是清理输入数据集的基本部分。
- en: 'How can you tell which columns are missing values and how many rows in each
    column are missing values? Here’s a simple command whose output is the number
    of missing values in each column:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何知道哪些列有缺失值以及每个列中有多少行缺失值？这里有一个简单的命令，其输出是每个列的缺失值数量：
- en: '[PRE18]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output tells us that Location, Min Delay, Min Gap, and Direction all have
    missing values that need to be dealt with. The `fill_missing` function iterates
    through all the columns in a dataframe and replaces empty values with a placeholder
    value depending on the column category, as the next listing shows.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 输出告诉我们，位置、最小延迟、最小间隔和方向都有需要处理的缺失值。`fill_missing`函数遍历数据框中的所有列，并根据列类别用占位符值替换空值，如下所示。
- en: Listing 3.16 Code to replace missing values with placeholder values
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.16 用占位符值替换缺失值的代码
- en: '[PRE19]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Fill missing values according to the column category.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据列类别填充缺失值。
- en: ❷ We are going to fill missing values in continuous columns with zeros. Note
    that for some columns, the mean of the values in the column would be a valid substitute
    for missing values.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们将用零填充连续列中的缺失值。请注意，对于某些列，该列值的平均值可以作为缺失值的有效替代。
- en: If we call this function with the dataframe where we loaded the input dataset
    as the argument
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用加载输入数据集的数据框作为参数调用此函数
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'and then rerun the command to count empty values, we can see that all the missing
    values have been dealt with:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然后重新运行命令来计数空值，我们可以看到所有缺失值都已处理：
- en: '[PRE21]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that we have fixed the missing values, let’s explore in depth the remaining
    cleanup action for one of the columns of the input dataset: Direction.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经修复了缺失值，让我们深入探讨输入数据集的一个列的剩余清理操作：方向。
- en: 'The Direction column indicates which direction of traffic was affected by the
    incident in a given record. According to the readme file that accompanies the
    dataset, there are seven valid values for Direction:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 方向列表示在给定记录中事故影响了哪种交通方向。根据随数据集附带的readme文件，方向有七个有效值：
- en: '`B` , `b` , or `BW` for incidents that affect traffic going in both directions.
    When we clean up the values in this column, we will use a single value to indicate
    both directions.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于影响双向交通的事故，使用`B`、`b`或`BW`。当我们清理此列的值时，我们将使用单个值来表示两个方向。
- en: '`NB` , `SB,` `EB` , and `WB` for incidents that affect traffic going in a single
    direction (northbound, southbound, eastbound, or westbound).'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于影响单方向交通的事故（北行、南行、东行或西行），使用`NB`、`SB`、`EB`和`WB`。
- en: 'Now let’s look at the actual number of unique values in the Direction column
    in the dataframe where we’ve loaded the input data:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在已加载输入数据的DataFrame中方向列的实际唯一值数量：
- en: '[PRE22]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'What’s going on? How can there be 95 distinct values in a column that has only
    seven legitimate values? Let’s take a look at the output of `value_counts` for
    the Direction column to get an idea of where all these unique values are coming
    from:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？为什么一个只有七个合法值的列会有95个不同的值？让我们看看`value_counts`的输出，以了解所有这些唯一值来自哪里：
- en: '[PRE23]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see, the 95 values in the Direction column come from a combination
    of redundant tokens and errors. To correct these problems, we need to
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，方向列中的95个值来自冗余标记和错误的组合。为了纠正这些问题，我们需要
- en: Get a consistent case for the values in this column to avoid problems like `EB`
    and `eb` being treated as distinct values.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取该列值的统一情况，以避免像`EB`和`eb`这样的值被当作不同的值处理。
- en: Remove `/` from values in this column to avoid problems like `wb` and `w/b`
    being treated as distinct values.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从该列的值中移除 `/` 以避免像 `wb` 和 `w/b` 被视为不同值的问题。
- en: 'Make the following substitutions to eliminate redundant tokens for compass
    points:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行以下替换以消除对指南针标记的冗余标记：
- en: '`e` for `eb` and `eastbound`'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`e` 代表 `eb` 和 `eastbound`'
- en: '`w` for `wb` and `westbound`'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w` 代表 `wb` 和 `westbound`'
- en: '`n` for `nb` and `northbound`'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n` 代表 `nb` 和 `northbound`'
- en: '`s` for `sb` and `southbound`'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s` 代表 `sb` 和 `southbound`'
- en: '`b` for b`w`'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b` 代表 b`w`'
- en: Replace all remaining tokens (including the `missing` token that we inserted
    for missing values) with a single token, `bad direction` , to signify a value
    that could not be mapped to any of the valid directions.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有剩余的标记（包括我们为缺失值插入的 `missing` 标记）替换为单个标记，`bad direction`，以表示无法映射到任何有效方向的价值。
- en: We apply the `direction_cleanup` function in the next listing to the Direction
    column to make these changes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下一列表中应用了 `direction_cleanup` 函数到方向列以实现这些更改。
- en: Listing 3.17 Code to clean up the Direction column
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.17 清理方向列的代码
- en: '[PRE24]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Function to replace invalid direction values with a common string
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 函数用于将无效的方向值替换为通用字符串
- en: ❷ Function to clean up direction values
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 清理方向值的函数
- en: ❸ Lowercase all values.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将所有值转换为小写。
- en: ❹ Remove / from all values.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从所有值中移除 `/`。
- en: ❺ Replace eastbound, westbound, and so on with single letter direction tokens.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将东向、西向等替换为单个字母方向标记。
- en: ❻ Remove extraneous b from strings like eb and nb.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 从 eb 和 nb 等字符串中移除多余的 b。
- en: ❼ Call check_direction to replace any remaining invalid values with a common
    string.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 调用 check_direction 将任何剩余的无效值替换为通用字符串。
- en: 'The output shows the effect of these changes:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了这些变化的效果：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here are the counts by the remaining unique values in the Direction column:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是按方向列剩余的唯一值计数的：
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that now we have only six valid values instead of the seven specified in
    the readme. We have combined the three “both way” tokens—`B` , `b` , and `BW`
    —into `b` and added a new token, `bad` `direction` , for erroneous direction values.
    This cleanup is essential for the refactoring of the input dataset that is described
    in chapter 5 to get a dataset with a record for every route/direction/time-slot
    combination. This refactoring depends on there being at most five direction values
    for any route, and thanks to the cleanup of the Direction column, we know that
    this condition is true.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在我们只有六个有效值，而不是说明中指定的七个。我们已经将三个“双向”标记——“B”、“b”和“BW”——合并为“b”，并为错误的方向值添加了一个新标记“bad
    direction”。这种清理对于第 5 章中描述的输入数据集重构至关重要，以获得每个路线/方向/时间段组合的记录。这种重构依赖于任何路线最多有五个方向值，多亏了方向列的清理，我们知道这个条件是成立的。
- en: The Direction column is only one of the columns that need to be cleaned up.
    Compared with other columns, Direction is relatively straightforward to clean
    up because it has a small number of valid values. Also, the operations to convert
    the input values to the set of valid values are relatively straightforward. We
    will go through a more complicated cleanup process for other columns in chapter
    4.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 方向列只是需要清理的列之一。与其他列相比，方向列相对简单，因为它有较少的有效值。此外，将输入值转换为有效值集的操作相对简单。我们将在第 4 章中详细介绍其他列的更复杂的清理过程。
- en: 3.8 Finding out how much data deep learning needs
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.8 查找深度学习需要多少数据
- en: If you do a naïve search to find out how much data you need to train a deep
    learning model, you will not get satisfying results. The answers are all variations
    on “It depends.” Some famous deep learning models are trained on millions of examples.
    Generally speaking, nonlinear methods like deep learning require more data than
    linear methods such as linear regression to achieve adequate results. Like other
    machine learning methods, deep learning requires a training dataset that covers
    the combinations of inputs that could be encountered when the model is deployed.
    In the case of the streetcar delay prediction problem, we need to ensure that
    the training set includes records for all the streetcar routes for which a user
    might want predictions. If a new streetcar route were added to the system, for
    example, we would need to retrain the model with a dataset that includes delay
    information for that route.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您进行简单的搜索以找出训练深度学习模型所需的数据量，您将不会得到令人满意的结果。所有答案都是“这取决于”的各种变体。一些著名的深度学习模型是在数百万个示例上训练的。一般来说，非线性方法如深度学习需要比线性方法如线性回归更多的数据才能达到适当的结果。与其他机器学习方法一样，深度学习需要一个训练数据集，该数据集涵盖了模型部署时可能遇到的输入组合。在街车延误预测问题的情况下，我们需要确保训练集包括用户可能希望预测的所有街车路线的记录。例如，如果系统添加了新的街车路线，我们就需要使用包含该路线延误信息的数据集重新训练模型。
- en: The real question for us isn’t the broad question “How much data does a deep
    learning model need to be trained adequately?” The question is “Does the streetcar
    dataset have enough data so that we can apply deep learning to it?” Ultimately,
    the answer depends on how well our model performs, and there is a lesson in this
    observation that the amount of data needed depends on the performance of the trained
    model. If you have a dataset with tens of thousands of entries, like the streetcar
    dataset, it is neither so small that deep learning is out of the question, nor
    so big as to guarantee that data volume won’t be a problem. The only way to know
    for sure whether deep learning is applicable to your problem is to try it out
    and look at the performance. As described in chapter 6, for the streetcar delay
    prediction model, we measure test accuracy (that is, how well the model predicts
    delays for trips that it did not see during the training process) and other measurements
    that are connected to a good user experience with the model. The good news is
    that after you have completed this book, you will have all the tools you need
    to assess the performance of deep learning on tabular structured datasets.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，真正的问题不是广泛的“深度学习模型需要多少数据才能充分训练”的问题，而是“街车数据集是否有足够的数据，以便我们可以将其应用于深度学习？”最终，答案取决于我们的模型表现如何，这个观察结果中有一个教训，即所需的数据量取决于训练模型的性能。如果您有一个包含数万个条目的数据集，如街车数据集，它既不是如此之小以至于深度学习不可行，也不是如此之大以至于数据量不会成为问题。确定深度学习是否适用于您的问题的唯一方法就是尝试它并查看其性能。正如第6章所述，对于街车延误预测模型，我们测量测试精度（即模型在训练过程中未看到的行程中预测延误的能力）以及其他与模型良好用户体验相关的测量。好消息是，完成这本书后，您将拥有评估深度学习在表格结构数据集上性能所需的所有工具。
- en: We will take a closer look at model accuracy in chapter 6, but it’s worth briefly
    reflecting on the question of accuracy now. How good does the accuracy of the
    streetcar prediction model need to be? For this problem, anything better than
    70% will help, because it will make it possible for passengers to avoid long delays
    most of the time.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第6章中更详细地讨论模型精度，但现在简要地反思一下精度问题。街车预测模型的精度需要有多好？对于这个问题，任何超过70%的精度都会有所帮助，因为这将使乘客在大多数情况下能够避免长时间的延误。
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A fundamental step in a deep learning project is ingesting the raw dataset so
    that it is available for you to manipulate in your code. When you have ingested
    the data, you can explore it and begin to clean it up.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习项目的一个基本步骤是摄取原始数据集，以便您可以在代码中对其进行操作。当您摄取了数据后，您可以探索它并开始对其进行清理。
- en: You can use config files in conjunction with your Python programs to keep your
    parameters organized and to make it easy to update parameters without having to
    touch the Python code.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用配置文件与您的Python程序结合使用，以保持您的参数组织有序，并使更新参数变得容易，而无需修改Python代码。
- en: You can ingest a CSV file directly into a Pandas dataframe with a single function
    call. With a simple Python function, you can also ingest all the tabs in an XLS
    file or even in multiple XLS files into a dataframe.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过单个函数调用直接将 CSV 文件导入 Pandas 数据框。使用简单的 Python 函数，您还可以将 XLS 文件中的所有标签或甚至多个 XLS
    文件中的所有标签导入到数据框中。
- en: Python’s pickle facility is an easy way to save objects in a Python program
    so you can use them across Python sessions. If you have a Python program that
    performs a series of transformations on a dataset, you can pickle the transformed
    dataframe, read the pickled dataframe in another Python program, and continue
    working on the dataset without having to redo the transformations.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 的 pickle 工具是保存 Python 程序中对象的一种简单方法，这样您就可以在 Python 会话之间使用它们。如果您有一个对数据集执行一系列转换的
    Python 程序，您可以 pickle 转换后的数据框，在另一个 Python 程序中读取 pickle 的数据框，然后继续在数据集上工作，而无需重新执行转换。
- en: When you have ingested the dataset into a dataframe, Python offers many convenient
    functions for exploring the dataset to determine the type and range of values
    in each column as well as trends in the values in columns. This exploration can
    help you detect anomalies in the dataset and avoid making unsubstantiated assumptions
    about the data.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您将数据集导入到数据框中后，Python 提供了许多方便的函数来探索数据集，以确定每列值的类型和范围，以及列中值的趋势。这种探索可以帮助您检测数据集中的异常，并避免对数据进行未经证实的假设。
- en: When you clean up a dataset to prepare it for training a deep learning model,
    the problems that you need to fix include missing values, invalid values, and
    values indicated by multiple distinct tokens (such as `e` , `e/b` , and `EB` all
    indicating `eastbound`).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您清理数据集以准备训练深度学习模型时，您需要解决的问题包括缺失值、无效值以及由多个不同标记表示的值（例如 `e`、`e/b` 和 `EB` 都表示 `eastbound`）。
- en: The answer to the question “How much data do you need to train a deep learning
    model?” is “Enough data to train the model to meet your performance criteria.”
    In most cases involving structured data, you need at least tens of thousands of
    records.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于“您需要多少数据来训练深度学习模型？”这个问题，答案是“足够的数据来训练模型以满足您的性能标准。”在大多数涉及结构化数据的情况下，您至少需要数万条记录。
