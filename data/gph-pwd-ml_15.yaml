- en: 11 Graph-based natural language processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 基于图的自然语言处理
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A simple approach to decompose a text and store it in a graph
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种将文本分解并存储在图中的简单方法
- en: How to extract the hidden structure of unstructured data via natural language
    processing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过自然语言处理提取非结构化数据的隐藏结构
- en: An advanced graph model for taming text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种用于驯服文本的高级图模型
- en: Let’s start this new topic by considering the most common applications that
    deal with natural language (in different format) for providing services to end
    users. You likely use them every day, probably without even noticing how complex
    and useful they are.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从考虑最常见的一些应用开始，这些应用处理自然语言（以不同格式）为最终用户提供服务。你可能每天都在使用它们，可能甚至没有意识到它们的复杂性和实用性。
- en: Chapter 4 dealt with text to implement a recommendation engine that uses the
    content related to the items, such as the description of a product or a movie
    plot. In that case, this data was used to compare items or user profiles, find
    commonalities (specifically, similarities) among users or items, and use them
    to suggest something that might be of interest to the current user. Figure 11.1
    presents the high-level structure of a content-based recommendation engine taken
    from chapter 4.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章处理文本以实现一个推荐引擎，该引擎使用与项目相关的信息，例如产品的描述或电影情节。在这种情况下，这些数据被用来比较项目或用户配置文件，找出用户或项目之间的共同点（特别是相似性），并利用这些共同点向当前用户推荐可能感兴趣的内容。图11.1展示了第4章中摘取的内容推荐引擎的高级结构。
- en: '![CH11_F01_Negro](../Images/CH11_F01_Negro.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F01_Negro](../Images/CH11_F01_Negro.png)'
- en: Figure 11.1 A content-based recommendation engine, as presented in chapter 4
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 如第4章所述的内容推荐引擎
- en: The item analyzer and user profiles builder deal with text to make it available
    during the recommendation phase. The result of their analysis is stored in such
    a way that is it easy to access and query during the model generation and the
    recommendation process.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 项目分析器和用户配置文件构建器处理文本，以便在推荐阶段使其可用。他们的分析结果以易于在模型生成和推荐过程中访问和查询的方式存储。
- en: For many years now, the search engine has been perhaps the most critical type
    of application dealing with text, providing relevant results to users while they
    are looking for something. Think about Google and Yahoo! Without search engines
    like these two, the internet wouldn’t have been the same; there would have been
    no useful way to discover new content or access the huge wealth of resources the
    internet offers, and as a result, it might never have grown to the scale it has.
    Searching is the most common technique we use to interact with a variety of content
    sources (news sites, retail sites, databases, and more). It helps us gain access
    to relevant data in an intuitive and effective way. Figure 11.2 shows a simplified
    schema of a search engine [Turnbull and Berryman, 2016].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，搜索引擎可能一直是处理文本的最关键类型的应用，在用户寻找某物时提供相关结果。想想谷歌和雅虎！如果没有像这两个搜索引擎这样的工具，互联网可能就不会是现在这样；将没有一种有效的方法来发现新内容或访问互联网提供的巨大资源宝库，结果可能是它永远无法发展到现在的规模。搜索是我们用来与各种内容来源（新闻网站、零售网站、数据库等）互动的最常见技术。它帮助我们以直观和有效的方式获取相关数据。图11.2展示了搜索引擎的一个简化架构[Turnbull和Berryman，2016]。
- en: '![CH11_F02_Negro](../Images/CH11_F02_Negro.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F02_Negro](../Images/CH11_F02_Negro.png)'
- en: Figure 11.2 An oversimplified schema of a search engine
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 搜索引擎的一个过于简化的架构
- en: The search engine is linked to a document store. It indexes the documents in
    that store in such a way that queries from users will be performed quickly, and
    the results will be accurate.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎与文档存储库相连。它以这种方式索引存储库中的文档，即用户的查询将快速执行，结果将准确无误。
- en: A more complex scenario occurs when an application gets a question in natural
    language and offers a reply or interacts with the user in some way. Most of us
    have experience with a digital voice assistant. You start a sentence with “Siri
    . . . ,” “Okay Google . . . ,” or “Alexa . . . ,” and ask the assistant to perform
    some simple task such as “Find the nearest restaurant,” “Play a romantic song,”
    or “Tell me the weather forecast.” Excited by this new technology, you’ve probably
    experimented more, asking it to perform more complicated tasks and trying different
    questions in different formats. Chances are that you were frustrated by the agent’s
    inability to deal with queries that go beyond the predefined set of skills.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个应用收到一个自然语言问题并提供回复或以某种方式与用户互动时，会出现一个更复杂的场景。我们大多数人都有使用数字语音助手的经验。你以“Siri . .
    . ，”“Okay Google . . . ，”或“Alexa . . . ，”开始一句话，并要求助手执行一些简单任务，比如“找到最近的餐厅”，“播放一首浪漫的歌曲”，或“告诉我天气预报”。由于对这项新技术感到兴奋，你可能已经尝试得更多，要求它执行更复杂的任务，并尝试不同格式的不同问题。很可能会因为代理无法处理超出预定义技能集的查询而感到沮丧。
- en: These applications are doing their best, but the goal they are trying to achieve
    is difficult and complex. As figure 11.3 shows, answering even a simple question
    requires completing a whole set of tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用正在尽力而为，但他们试图实现的目标是困难和复杂的。如图 11.3 所示，回答甚至一个简单的问题也需要完成一系列的任务。
- en: '![CH11_F03_Negro](../Images/CH11_F03_Negro.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F03_Negro](../Images/CH11_F03_Negro.png)'
- en: Figure 11.3 An example of the set of tasks performed by a conversational agent
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 对话式代理执行的任务集示例
- en: These tasks are described in detail in table 11.1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务在表 11.1 中有详细描述。
- en: Table 11.1 Tasks performed by a conversational agent replying to a user query
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.1 对话式代理执行的任务
- en: '| Task/component | Description |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 任务/组件 | 描述 |'
- en: '| Automatic speech recognition (ASR) | ASR takes audio (voice) input from the
    user and outputs a transcribed string of words that represent the query. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 自动语音识别 (ASR) | ASR 从用户那里接收音频（语音）输入，并输出代表查询的转录单词字符串。 |'
- en: '| Natural language understanding (NLU) | The goal of the NLU component is to
    extract three things from the user’s utterance. The first task is domain classification.
    Is the user (for example) talking about booking a flight, programming an alarm,
    or dealing with their calendar? this 1-of-*n* classification task is unnecessary
    for single-domain systems that focus only on, say, calendar management, but multidomain
    dialogue systems are the modern standard. The second task is user intent identification.
    What general task or goal is the user trying to accomplish? The task could be
    to find a movie, show a flight, or remove a calendar appointment. Finally, entity
    extraction entails extracting the particular concepts that the user intends the
    system to understand from their utterance with respect to their intent. These
    entities are used to circumstantiate the intent: Where would they like to book
    a flight to? Which event are they looking for in the calendar? |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言理解 (NLU) | NLU 组件的目标是从用户的表述中提取三件事。第一项任务是领域分类。用户（例如）是在谈论预订航班、设置闹钟还是处理日历？这种
    1-of-*n* 分类任务对于只关注，比如说，日历管理的单领域系统是不必要的，但多领域对话系统是现代标准。第二项任务是用户意图识别。用户试图完成什么一般任务或目标？任务可能是查找电影、显示航班或删除日历预约。最后，实体提取涉及从用户的表述中提取用户意图系统应理解的具体概念。这些实体用于具体化意图：他们想在哪里预订航班？他们在日历中寻找哪个事件？
    |'
- en: '| Searching for answer | This step is the meat of the process. The system,
    having received the domain, the intent, and the entities, accesses some knowledge
    base and determines a possible set of answers. Then it has to rank the answers
    and provide them as input to the following steps. In some cases, this process
    means finding passages in documents; in others, it means retrieving information
    that will be used to generate a proper answer.In this context, the knowledge base
    can be created from structured and unstructured data coming from multiple data
    sources.In conversational agents, this component also takes into account the previous
    questions from the user to narrow the context of the current question. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 搜索答案 | 这一步是整个过程的精髓。系统在接收到领域、意图和实体后，访问某些知识库并确定可能的答案集。然后它必须对答案进行排序并将它们作为输入提供给后续步骤。在某些情况下，这个过程意味着在文档中查找段落；在另一些情况下，意味着检索将用于生成适当答案的信息。在此背景下，知识库可以由来自多个数据源的结构化和非结构化数据创建。在对话代理中，此组件还会考虑用户先前的问题，以缩小当前问题的上下文。|'
- en: '| Natural language generation (NLG) | (Optional) Next, the NLG component generates
    the text of a response to the user. The task of NLG in the information-state architecture
    is often modeled in two stages: content planning (what to say) and sentence realization
    (how to say it).This component is optional. In most cases, the sentences are extracted
    as they are from existing documents in the knowledge base. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言生成 (NLG) | （可选）接下来，NLG 组件生成对用户的响应文本。在信息状态架构中，NLG 的任务通常分为两个阶段：内容规划（说什么）和句子实现（如何说）。此组件是可选的。在大多数情况下，句子直接从知识库中的现有文档中提取。|'
- en: '| Text to speech (TTS) | TTS converts the answer to audio that the user can
    listen to instead of reading. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 文本转语音 (TTS) | TTS 将答案转换为用户可以听到的音频，而不是阅读。'
- en: Due to the complexity of this scenario, it represents one of the most exciting
    and active research areas in machine learning. In the future, we will be able
    to interact with all the devices around us by using only our voices, but for now,
    this type of interaction is a dream.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此场景的复杂性，它代表了机器学习中最激动人心和最活跃的研究领域之一。在未来，我们只需使用我们的声音就能与周围的所有设备进行交互，但到目前为止，这种交互仍然是一个梦想。
- en: 'Although recommendation agents, search engines, and conversational agents seem
    to be different, they have critical aspects in common. The underlying requirements
    can be summarized in the following way:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管推荐代理、搜索引擎和对话代理似乎不同，但它们有共同的关键方面。基本要求可以总结如下：
- en: The use of text for building a knowledge base.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文本构建知识库。
- en: Recommendation engines use item descriptions to create the recommendation model
    (for instance, identifying similarities among items).
  id: totrans-29
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐引擎使用项目描述来创建推荐模型（例如，识别项目之间的相似性）。
- en: Search engines preprocess the documents via indexing.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎通过索引对文档进行预处理。
- en: Chat bots and conversational agents create the knowledge base using unstructured
    data (documents and previous questions).
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人和对话代理使用非结构化数据（文档和先前问题）来创建知识库。
- en: A proper knowledge representation that stores all the information necessary
    for the scope of the application, providing efficient access to it.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当的知识表示存储了应用范围内所需的所有信息，并提供对其的高效访问。
- en: Interaction with the user, which in most cases happens via natural language.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与用户的交互，这在大多数情况下是通过自然语言进行的。
- en: The following sections and chapter 12 deal with those critical elements, proposing
    different graph-based approaches to accomplishing those tasks or tackling some
    of the related problems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节和第12章处理这些关键元素，提出不同的基于图的方法来完成这些任务或解决一些相关问题。
- en: '11.1 A basic approach: Store and access sequence of words'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 基本方法：存储和访问单词序列
- en: As usual, let’s start with a basic graph model to illustrate the high-level
    concepts and the main issues related to graph-based natural language processing.
    Later in the chapter, starting in section 11.2, we’ll discuss more advanced techniques
    and models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，让我们从一个基本的图模型开始，以说明基于图的自然语言处理的高级概念和主要问题。在本章的后续部分，从第11.2节开始，我们将讨论更高级的技术和模型。
- en: 'It’s worth noting that each graph model should be designed with the application’s
    purpose in mind. Despite their simplicity, the models designed in this case are
    the right fit for the scope of the scenario described in this section. The models
    will serve their purpose appropriately without too many complications, showcasing
    a critical aspect of graph modeling: start simple and adapt by introducing new
    concepts and complexity only when necessary. With that said, it’s time to begin
    our journey.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，每个图模型都应该根据应用的目的来设计。尽管这些模型很简单，但在这个案例中设计的模型非常适合本节所述场景的范围。这些模型将适当地发挥作用，而不会过于复杂，展示了图建模的一个关键方面：从简单开始，并在必要时通过引入新的概念和复杂性来适应。话虽如此，现在是时候开始我们的旅程了。
- en: Suppose that you would like to implement a tool that supports message writing,
    suggesting the next word while you are typing. Moreover, suppose that you would
    like the tool to learn from you or from a specific set of documents. Such a tool
    could be useful not only for providing message-writing assistance, but also for
    supporting spell checking, extracting common phrases, summarizing, and so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想实现一个支持消息编写、在输入时建议下一个单词的工具。此外，假设你希望这个工具能够从你或特定的文档集中学习。这样的工具不仅可以帮助提供消息编写辅助，还可以支持拼写检查、提取常用短语、总结等功能。
- en: The first step is splitting the input into words. The simplest approach in occidental
    languages is to use whitespace. (Other languages, such as Chinese, require different
    approaches.) When they are extracted, these words have to be stored in a way that
    keeps track of their order in the original text. The basic approach outlined here
    is inspired by a blog post[¹](#pgfId-1006674) by Michael Hunger. A suitable graph
    model would look like figure 11.4 (using the sample phrase *you will not be able*).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将输入拆分为单词。在西方语言中，最简单的方法是使用空格。（其他语言，如中文，需要不同的方法。）当它们被提取出来时，这些单词必须以某种方式存储，以跟踪它们在原始文本中的顺序。这里概述的基本方法受到了Michael
    Hunger的一篇博客文章[¹](#pgfId-1006674)的启发。一个合适的图模型将类似于图11.4（使用示例短语“你将不能”）。
- en: '![CH11_F04_Negro](../Images/CH11_F04_Negro.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F04_Negro](../Images/CH11_F04_Negro.png)'
- en: Figure 11.4 Basic schema applied to the phrase you will not be able
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 应用到短语“你将不能”的基本方案
- en: In this schema, the words themselves are considered to be unique, but not the
    relationships between the words, so if some words are used elsewhere in other
    sentences, they will not be replicated; instead, new relationships will be created.
    If we were to also process the phrase *you will be able*, the resulting graph
    would look like figure 11.5.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，单词本身被认为是唯一的，但单词之间的关系则不是，因此如果某些单词在其他句子中也被使用，它们不会被复制；相反，会创建新的关系。如果我们也处理短语“你将能够”，生成的图将类似于图11.5。
- en: '![CH11_F05_Negro](../Images/CH11_F05_Negro.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F05_Negro](../Images/CH11_F05_Negro.png)'
- en: Figure 11.5 Our schema applied to the phrase *you will be able*
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 我们对短语“你将能够”应用的方案
- en: New relationships are created because of the new input but no new words are
    created, because we have already stored all these words. This model and approach,
    in which we keep the words unique and create new relationships for each sentence,
    has pros and cons that we are going to explore and analyze. The model may be useful
    in some cases but not in others. This example will illustrate how, in the evolution
    of your project, you can change your mind about certain aspects of your solution.
    Your needs may change, requiring you to develop a new schema that suits them better.
    Graphs are helpful for this purpose because they are flexible data structures
    that can evolve in response to your project’s changing constraints and requirements.
    The following Cypher query shows how to process the text and obtain the expected
    graph database.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于新的输入创建了新的关系，但没有创建新的单词，因为我们已经存储了所有这些单词。这种模型和我们在每个句子中保持单词唯一并创建新关系的方法既有优点也有缺点，我们将对其进行探讨和分析。在某些情况下，这个模型可能很有用，但在其他情况下则不然。这个例子将说明，在你的项目发展中，你可以改变对解决方案某些方面的看法。你的需求可能会变化，需要你开发一个更适合它们的新的方案。图对于这个目的很有帮助，因为它们是灵活的数据结构，可以根据你项目的变化约束和要求进行演变。以下Cypher查询显示了如何处理文本并获得预期的图数据库。
- en: Listing 11.1 Splitting a sentence by using whitespace and storing it
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.1 使用空格拆分句子并存储它
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this query, the WITH clause at the beginning provides data to the next query
    statement. Note that split() defines the tokenization process based on whitespace,
    which is specified as the delimiter in the second parameter. The range() function
    creates a range of numbers, in this case from 0 to the size of the sentence—obtained
    by using the size() function on the text—minus 2\. The UNWIND clause turns the
    range collection into result rows with the index value to use to get the right
    word. The MERGE clause, as usual, helps us avoid creating the same node (the same
    word, in this scenario) twice. At the end, we use CREATE to store the relationships
    between two consecutive words. As a side note, for MERGE to work efficiently,
    you would want to create a constraint in your graph, as in the following query.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个查询中，开头的 WITH 子句为下一个查询语句提供了数据。注意，split() 函数根据空白字符定义了分词过程，这被指定为第二个参数中的分隔符。range()
    函数创建一个数字范围，在这种情况下是从 0 到句子的大小——通过在文本上使用 size() 函数获得——减去 2。UNWIND 子句将范围集合转换为具有索引值的结果行，以获取正确的单词。MERGE
    子句，如通常一样，帮助我们避免创建相同的节点（在这种情况下是相同的单词）两次。最后，我们使用 CREATE 来存储两个连续单词之间的关系。作为旁注，为了使
    MERGE 高效工作，你希望在图中创建一个约束，如下面的查询所示。
- en: Listing 11.2 Creating unique constraints for a word’s value
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 为单词的值创建唯一约束
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you would like to explore this first graph, the next query shows you the
    path.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要探索这个第一个图，下一个查询将显示路径。
- en: Listing 11.3 Returning the path for the graph
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.3 返回图的路径
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result will look like figure 11.6.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将看起来像图 11.6。
- en: '![CH11_F06_Negro](../Images/CH11_F06_Negro.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F06_Negro](../Images/CH11_F06_Negro.png)'
- en: Figure 11.6 The resulting graph after processing the sentence “You will not
    be able to send new mail until you upgrade your email.”
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 处理句子“在你升级电子邮件之前，你将无法发送新邮件”后的结果图
- en: So far, so good—this result is exactly what we expected. But the model can be
    improved. As discussed previously, the same word can be followed by multiple other
    words in different sentences. By adhering to the model described, listing 11.1
    produces multiple relationships between the same couple of words due to the last
    CREATE clause. If we ingest a new sentence like “He says it’s OK for Bill and
    Hillary Clinton to send their kid to a private school,” the resulting graph will
    look like figure 11.7.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利——这个结果正是我们预期的。但是，模型可以改进。如前所述，同一个单词可以在不同的句子中后面跟多个其他单词。通过遵循所描述的模型，列表
    11.1 由于最后的 CREATE 子句，在相同的两个单词之间产生了多个关系。如果我们摄入一个新句子，如“他说比尔·克林顿和希拉里·克林顿送他们的孩子去私立学校是可以的”，生成的图将看起来像图
    11.7。
- en: '![CH11_F07_Negro](../Images/CH11_F07_Negro.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F07_Negro](../Images/CH11_F07_Negro.png)'
- en: Figure 11.7 The resulting graph after also processing “He says it’s OK for Bill
    and Hillary Clinton to send their kid to a private school.”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 也处理“他说比尔·克林顿和希拉里·克林顿送他们的孩子去私立学校是可以的”后的结果图
- en: Notice that we have multiple relationships between *to* and *send*. This result
    is correct and definitely useful in some scenarios, but in our case, we want to
    prioritize the words that are most likely to be next to the current one. This
    schema requires us to compute the number of relationships for each combination
    of words in which the current word appears first.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在 *to* 和 *send* 之间有多个关系。这个结果是正确的，并且在某些情况下肯定是有用的，但在这个案例中，我们想要优先考虑最有可能与当前单词相邻的单词。这个架构要求我们计算当前单词出现在每个单词组合中的关系数量。
- en: We can modify our graph model by adding a weight to the relationship between
    two words and making it unique when it connects the same couple of words. The
    resulting schema is shown in figure 11.8.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在两个单词之间的关系上添加权重并使其在连接相同的两个单词时唯一来修改我们的图模型。结果架构如图 11.8 所示。
- en: '![CH11_F08_Negro](../Images/CH11_F08_Negro.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F08_Negro](../Images/CH11_F08_Negro.png)'
- en: Figure 11.8 New schema model with weight property on the relationships
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 在关系上具有权重属性的新架构模型
- en: The new schema removes the need for multiple relationships by adding a weight
    property on the relationship connecting the words. This change in the schema requires
    a small change in the query.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 新架构通过在连接单词的关系上添加权重属性来消除了多个关系的需要。这种架构的变化需要在查询中做小的修改。
- en: Listing 11.4 Storing frequency of word pairs
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.4 存储词对频率
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the last CREATE has been replaced by a MERGE that either creates (ON
    CREATE) or updates (ON MATCH) the weight property on the NEXT relationship.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最后的 CREATE 已经被 MERGE 替换，它要么在创建时（ON CREATE）创建，要么在匹配时（ON MATCH）更新 NEXT 关系上的权重属性。
- en: Exercise
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 练习
- en: Try the new query with the previous sentences, and check the resulting graph.
    Remember to clean up your database first.[²](#pgfId-1006689) Check the weight
    of the relationship between *to* and *send*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用之前的句子运行新的查询，并检查生成的图。请记住先清理您的数据库。[²](#pgfId-1006689) 检查 *to* 和 *send* 之间关系的权重。
- en: With a proper model in hand, we can now approach our original problem. Because
    it would be hard for us now to train a model with our personal messages, let’s
    use a generic dataset containing some text. We will use as a corpus the Manually
    Annotated Sub-Corpus (MASC) dataset,[³](#pgfId-1006705) a balanced subset of 500,000
    words of written texts and transcribed speech drawn primarily from the Open American
    National Corpus (OANC). Table 11.2 shows a few examples of documents in the dataset.
    For space reasons, I’ve copied only the key columns.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个合适的模型在手，我们现在可以接近我们的原始问题。因为我们现在很难用我们个人的信息来训练一个模型，所以让我们使用包含一些文本的通用数据集。我们将使用手动注释子语料库（MASC）数据集，[³](#pgfId-1006705)
    这是一个从开放美国国家语料库（OANC）中抽取的包含 50 万个单词的书面文本和转录语音的平衡子集。表 11.2 显示了数据集中的一些文档示例。由于空间原因，我只复制了关键列。
- en: Table 11.2 Sample items from the MASC dataset
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.2 MASC 数据集的样本项目
- en: '| Filename | Content |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 文件名 | 内容 |'
- en: '| [MASC]/data/written/110CYL200.txt | It took some time and hard work, but
    with the help of Goodwill, Jerry was able to work out a payment plan with the
    prosecutor’s office, find housing and conduct a more thorough job search. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| [MASC]/data/written/110CYL200.txt | 这需要一些时间和努力，但在 Goodwill 的帮助下，Jerry 能够与检察官办公室协商出一个付款计划，找到住处，并进行更彻底的求职。|'
- en: '| [MASC]/data/written/111348.txt | The above figure was given to me as my share
    and to conceal this kind of money became a problem for me, so with the help of
    a British contact working with the UN here (his office enjoys some immunity) I
    was able to get the package out to a safe location entirely out of trouble spot.
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| [MASC]/data/written/111348.txt | 我得到了上面的那份文件作为我的份额，为了隐藏这种钱，这对我来说成了问题，所以在我这里与联合国合作的英国联系人（他的办公室享有某些豁免权）的帮助下，我能够将包裹安全地送到一个完全远离麻烦的地方。|'
- en: '| [MASC]/data/written/111364.txt | Please l know very well that this mail might
    come to you as a surprise, I am rs Dagmar a dying woman who has decided to donate
    what I have to the Church, Mosque or any Charity Organization round your community
    through your assistance since l will not be able to do this here in my community
    for the reason which l will explain to you later. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| [MASC]/data/written/111364.txt | 我非常清楚这封信可能会让你感到惊讶，我是 Dagmar，一个决定将我所拥有的一切捐给教堂、清真寺或你所在社区的任何慈善组织的垂死之人，通过你的帮助，因为在这里我的社区里我无法做到这一点，原因我稍后会向你解释。|'
- en: '| [MASC]/data/written/111371.txt | Imagine the feeling of being able to offer
    your opportunity and products to millions of people in North America and other
    parts of the world from your own home-based niche. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| [MASC]/data/written/111371.txt | 想象一下，从你自己的家庭工作室中向北美和其他地区的数百万人们提供你的机会和产品的感觉。|'
- en: We will use only the tab-separated sentence dataset available in the file masc_sentences
    .tsv. For the following queries, please copy that file into the import directory
    in your Neo4j installation. To import all the sentences in the file and decompose
    them as described earlier, you’ll need to run the following query (remembering
    to clean up your database first).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只使用文件 masc_sentences.tsv 中可用的制表符分隔的句子数据集。对于以下查询，请将此文件复制到您的 Neo4j 安装中的导入目录。为了导入文件中的所有句子并将它们分解，您需要运行以下查询（请记住先清理您的数据库）。
- en: Listing 11.5 Importing the MASC dataset and processing its contents
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.5 导入 MASC 数据集并处理其内容
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pro Tip To go through the words in the sentence, this query replaces UNWIND
    with FOREACH (using the same range on the indices). The UNWIND clause turns the
    range collection into result rows and in this case returns a lot of data. FOREACH
    instead executes the MERGEs inside without returning anything. This clause simplifies
    execution and improves performance dramatically.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 高级技巧 为了遍历句子中的单词，这个查询将 UNWIND 替换为 FOREACH（使用相同的索引范围）。UNWIND 子句将范围集合转换为结果行，在这种情况下返回大量数据。相反，FOREACH
    执行 MERGE 操作而不返回任何内容。这个子句简化了执行并显著提高了性能。
- en: Let’s take a quick look at the database. We can search for the 10 most frequent
    combinations of words via the following query.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看数据库。我们可以通过以下查询搜索最常见的 10 个单词组合。
- en: Listing 11.6 Find the 10 most common pairs of words
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.6 查找最常见的 10 个词对
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result is shown in figure 11.9.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如图 11.9 所示。
- en: '![CH11_F09_Negro](../Images/CH11_F09_Negro.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F09_Negro](../Images/CH11_F09_Negro.png)'
- en: Figure 11.9 The 10 most common pairs of words in the MASC dataset
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9 MASC数据集中最常见的10个单词对
- en: 'Now we have all the components to satisfy our initial requirement: implement
    a tool that supports message writing, suggesting the next word. The idea is to
    take the current word and query our new graph to find the three most probable
    next words, using the weight on the relationship.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了满足我们最初要求的所有组件：实现一个支持消息编写并建议下一个单词的工具。想法是取当前单词并查询我们的新图，以找到三个最可能的后继单词，使用关系上的权重。
- en: Listing 11.7 Query that suggests the most probable word
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.7 建议最可能单词的查询
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This query will give us the results shown in figure 11.10.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询将给我们展示在图11.10中的结果。
- en: '![CH11_F10_Negro](../Images/CH11_F10_Negro.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F10_Negro](../Images/CH11_F10_Negro.png)'
- en: Figure 11.10 The next most probable words to follow “how”
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10 接下来最可能跟随“how”的单词
- en: Apparently, the three best words to suggest to a user who wrote *how* as the
    last word are *to,* *the*, and *much*. Not bad!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于以*how*作为最后一个单词的用户，建议的前三个最佳单词是*to*、*the*和*much*。不错！
- en: Exercise
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 练习
- en: Play with the database, checking the results for other words. Do they make sense
    to you?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据库中尝试其他单词的结果。它们对您有意义吗？
- en: As you can see from the results obtained so far, the results are pretty good,
    but we can do better. Instead of considering only the last word, we can consider
    the previous two or even three words. This approach will give us the opportunity
    to improve the quality of the suggestions, but we’ll have to change the structure
    of the database a little.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从目前获得的结果中看到，结果相当不错，但我们还可以做得更好。我们不仅考虑最后一个单词，还可以考虑前两个甚至三个单词。这种方法将给我们提供提高建议质量的机会，但我们需要稍微改变数据库的结构。
- en: 'Again, there’s nothing bad about changing your mind and refining the model
    of your database as you develop your solution. This situation happens quite often:
    you start with an idea in mind; design your model accordingly; and then realize
    that with a small change, you can get better or faster results, so you change
    the model and test. You should follow this process all the time; you shouldn’t
    consider your model to be definitive. In this sense, graphs offer you all the
    flexibility you need. In some cases, you can even adapt your model without reingesting
    everything.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，改变主意并随着解决方案的开发而细化数据库模型并没有什么不好。这种情况很常见：你心中有一个想法；相应地设计你的模型；然后意识到通过小小的改变，你可以得到更好或更快的成果，所以你改变模型并测试。你应该始终遵循这个过程；你不应该认为你的模型是最终的。在这方面，图提供了你需要的一切灵活性。在某些情况下，你甚至可以在不重新摄入所有内容的情况下调整你的模型。
- en: 'Let’s try out this model refinement by considering the last two (or three)
    words the user wrote instead of only one. The idea for improving the quality of
    the recommendations by considering the two (or three) previous words can be formalized
    as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过考虑用户写的最后两个（或三个）单词而不是一个单词来尝试这种模型细化。通过考虑两个（或三个）前一个单词来提高推荐质量的想法可以形式化为以下内容：
- en: Take the last two (or three) words the user wrote.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取用户最后写的两个（或三个）单词。
- en: Search in the database for all sentences in which those words appear in the
    same order.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据库中搜索所有包含这些单词以相同顺序出现的句子。
- en: Find what the next possible words are.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出下一个可能出现的单词。
- en: Group the words and compute the frequency of each.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词分组并计算每个单词的频率。
- en: Order by frequency (descending).
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按频率（降序）排序。
- en: Recommend the top three.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推荐前三名。
- en: This process is illustrated in figure 11.11.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在图11.11中得到了说明。
- en: '![CH11_F11_Negro](../Images/CH11_F11_Negro.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F11_Negro](../Images/CH11_F11_Negro.png)'
- en: Figure 11.11 Schema for improving the quality of next-word recommendation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 提高下一个单词推荐质量的模式
- en: The new model should allow us to reconstruct sentences so that we can identify
    in which of them the words appear in the specific order we want. The current model
    cannot do this because it merges the sentences, updating only the weights. The
    original information is lost.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型应该允许我们重建句子，以便我们可以确定在哪些句子中单词以我们想要的特定顺序出现。当前模型无法做到这一点，因为它合并了句子，只更新了权重。原始信息丢失了。
- en: At this point, we have a few options. One option is to remove the unique constraint
    on Word and replicate all the words each time they appear (and do the same for
    the relationships), but this solution requires a lot of disk space without adding
    any concrete value. We can do better by using the model in figure 11.12.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一些选择。一个选择是移除 Word 上的唯一约束，并在每次出现时复制所有单词（以及关系），但这种解决方案需要大量的磁盘空间，而不会增加任何具体的价值。我们可以通过使用图
    11.12 中的模型做得更好。
- en: '![CH11_F12_Negro](../Images/CH11_F12_Negro.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F12_Negro](../Images/CH11_F12_Negro.png)'
- en: Figure 11.12 The third version of our schema
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 我们模式的第三个版本
- en: This model keeps the words unique but creates relationships specific to each
    sentence by adding an ID on the relationships. In this way, it is possible to
    reconstruct an original sentence by filtering the relationships by sentenceId.
    This approach uses less disk space than replicating the words, and the result
    obtained will be exactly the same. So let’s clean up our database and reload with
    the new model. The query for cleaning up the database follows.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型保持了单词的唯一性，但通过在关系上添加一个 ID 来创建每个句子特有的关系。这样，通过通过 sentenceId 过滤关系，就有可能重建原始句子。这种方法比复制单词使用的磁盘空间更少，并且获得的结果将完全相同。因此，让我们清理我们的数据库，并使用新的模型重新加载。清理数据库的查询如下。
- en: Listing 11.8 Cleaning up the database by using APOC’s iterate procedure
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.8 使用 APOC 的 iterate 程序清理数据库
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this case, it’s better to use apoc.periodic.iterate because the database
    is fairly big; removing it in a single transaction could take a while, and the
    transaction could fail. The iterate() function in the APOC plugin allows you to
    split the big commit into smaller commits, and the operations can be done in parallel,
    which will be much faster. When the graph database is empty, we can reimport and
    process the text.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最好使用 apoc.periodic.iterate，因为数据库相当大；在单个事务中删除它可能需要一段时间，并且事务可能会失败。APOC
    插件中的 iterate() 函数允许您将大提交拆分成更小的提交，并且操作可以并行执行，这将更快。当图数据库为空时，我们可以重新导入并处理文本。
- en: Listing 11.9 New importing query that uses the sentence identifier
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.9 使用句子标识符的新导入查询
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the MERGE used to create the relationships between words has
    been replaced by the CREATE we had in the first example. But in this case, a new
    property, sentence, contains a sentence identifier. If you look at the graph now,
    you’ll see a lot of relationships coming out and in of almost every node. On the
    other hand, now you can execute a query like the following to suggest the next
    word based on the current and previous words.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，用于创建单词之间关系的 MERGE 已被第一个示例中的 CREATE 替换。但在这个情况下，一个新的属性，即句子，包含了一个句子标识符。如果您现在查看图，您会看到几乎每个节点都有很多关系进出。另一方面，现在您可以使用以下查询来根据当前和前面的单词建议下一个单词。
- en: Listing 11.10 Query to suggest the next word considering the last two words
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.10 考虑最后两个单词来建议下一个单词的查询
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To suggest a word based on the last three words, you can use a query like the
    following.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要根据最后三个单词建议一个单词，您可以使用以下查询。
- en: Listing 11.11 Query to suggest the next word considering the last three words
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.11 考虑最后三个单词来建议下一个单词的查询
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As expected, the quality of the suggestions is much higher, but at the cost
    of a bigger and more complex database. The last two queries search for specific
    patterns in the database. The Cypher language in this case helps you define at
    a high level the graph pattern you are looking for; the engine will return all
    the nodes and relationships matching that pattern.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，建议的质量要高得多，但代价是数据库更大、更复杂。最后两个查询在数据库中搜索特定的模式。在这种情况下，Cypher 语言帮助您在较高层次上定义您正在寻找的图模式；引擎将返回所有匹配该模式的节点和关系。
- en: 'It is worth mentioning a subtle drawback to the last schema we defined. The
    word nodes are unique, so if you have millions of sentences, this schema will
    create supernodes—that is, nodes with millions of relationships coming in, going
    out, or both. In most cases, these dense nodes represent the so-called *stop words*:
    words that appear frequently in most texts, such as articles (a, *the*), pronouns
    (*he*, *she*), and auxiliary verbs (*do,* *does*, *will*, *should*). As mentioned
    previously, such dense nodes can be an issue during query execution because the
    time it takes to traverse them is high. For the purposes of this scenario and
    the solutions presented in this section, this situation will not be a big problem,
    but in section 11.2, we’ll examine how to recognize and process these words properly.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们最后定义的最后一个模式有一个微小的缺点。单词节点是唯一的，所以如果你有数百万个句子，这个模式将创建超级节点——即有数百万个关系进入、出去或两者都有的节点。在大多数情况下，这些密集节点代表了所谓的*停用词*：在大多数文本中频繁出现的词语，如冠词（a，*the*）、代词（*he*，*she*）和助动词（*do*，*does*，*will*，*should*）。如前所述，这样的密集节点在查询执行期间可能会成为一个问题，因为遍历它们所需的时间很高。对于本场景和本节中提出的解决方案，这种情况不会是一个大问题，但在第11.2节中，我们将探讨如何正确识别和处理这些词语。
- en: 11.1.1 Advantages of the graph approach
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 图方法的优势
- en: The first scenario showed how to represent text in the form of a graph. Sentences
    are split into simple tokens, these tokens are represented by nodes, and the sequence
    of words is maintained by relationships.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个场景展示了如何将文本表示为图的形式。句子被分割成简单的标记，这些标记由节点表示，而单词的顺序则由关系来维持。
- en: Despite its simplicity, the graph model designed in the end served perfectly
    the purpose we had in mind. You also saw how to evolve your model to respond to
    new needs or fulfill new constraints.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其简单性，最终设计的图模型完美地实现了我们心中的目的。你也看到了如何使你的模型适应新的需求或满足新的约束。
- en: 'The last few queries showed how to search for specific patterns in the graph—an
    extremely powerful feature of graphs made available via a proper graph query language
    such as Cypher. With other approaches, such as a relational database, expressing
    the same concepts would have been much more complicated. Simplicity, flexibility
    (adaptability to changes), and power: these graph features emerged clearly in
    this simple scenario.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几个查询展示了如何在图中搜索特定的模式——这是通过像Cypher这样的适当图查询语言提供的图的一个极其强大的功能。使用其他方法，例如关系数据库，表达相同的概念将会复杂得多。简单性、灵活性（适应变化的能力）和强大：这些图特性在这个简单场景中表现得非常明显。
- en: 11.2 NLP and graphs
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 自然语言处理和图
- en: The basic approach discussed in section 11.1 has a lot of limitations, some
    of which we discussed along the way. It serves well the intended purpose of suggesting
    the next word based on the user’s previous input, but it would not be suitable
    for advanced scenarios that require detailed analysis and understanding of the
    text, as in the case of the conversational agents, chatbots, and advanced recommendation
    engines mentioned in this chapter’s introduction. Some of the aspects not considered
    in the previous examples are
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11.1节中讨论的基本方法有很多局限性，其中一些我们在讨论过程中已经提到了。它很好地服务于建议下一个词基于用户先前输入的预期目的，但它不适合需要详细分析和理解文本的高级场景，例如本章引言中提到的对话代理、聊天机器人和高级推荐引擎。前例中未考虑的一些方面包括
- en: The words are not normalized to their base form (such as removing plurals or
    considering the base form of a declined verb).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语没有被归一化到其基本形式（例如去除复数或考虑动词的变位的基本形式）。
- en: We are not taking into account dependencies among words (such as the connection
    between adjectives and nouns).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有考虑词语之间的依赖关系（例如形容词和名词之间的联系）。
- en: Some words make more sense together because they represent a single entity.
    (*Alessandro Negro* is a person and should be treated as a single token, for example.)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些词语放在一起更有意义，因为它们代表了一个单一实体。（例如，*Alessandro Negro* 是一个人，应该被视为一个单独的标记。）
- en: The stop words are not properly identified and (eventually) removed to prevent
    dense nodes. A few examples were given, but lists are available based on the language
    and the domain.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停用词没有被正确识别和（最终）移除以防止密集节点。给出了一些例子，但基于语言和领域，有可用的列表。
- en: Splitting by using only whitespace typically is not good enough. (Consider,
    for example, words like *can’t* and all the types of punctuation that might be
    attached to the last word in a sentence.)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用空白字符进行分割通常是不够的。（例如，考虑像 *can’t* 这样的词以及可能附加在句子最后一个词上的所有类型的标点符号。）
- en: This section describes more sophisticated scenarios that require advanced NLP
    tasks. It walks through various techniques and tools for decomposing and properly
    analyzing textual data, as well as the graph models for storing the results of
    such analyses. This phase represents the fundamental step on top of which more
    advanced tasks can be accomplished.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了需要高级NLP任务的高级场景。它探讨了分解和正确分析文本数据的各种技术和工具，以及存储此类分析结果的图模型。这一阶段代表了在之上可以完成更高级任务的基本步骤。
- en: Text is often thought of as being unstructured data, but free text has a lot
    of structure. The difficulty is that most of that structure isn’t explicit, which
    makes it hard to search for or analyze information contained within the text [Grishman,
    2015]. NLP uses concepts from computer science, artificial intelligence, and linguistics
    to analyze natural language, with the aim of deriving meaningful and useful information
    from text.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 文本通常被认为是无结构数据，但自由文本有很多结构。困难在于，大部分这种结构都不是显式的，这使得在文本中搜索或分析信息变得困难 [Grishman, 2015]。NLP使用来自计算机科学、人工智能和语言学的概念来分析自然语言，目的是从文本中提取有意义的和有用的信息。
- en: '*Information extraction (IE**)* is the first step in the process of understanding
    text and building a sophisticated, engaging machine learning application. It can
    be described as the process of analyzing text, decomposing it, and identifying
    semantically defined entities and relationships within it with the goal of making
    the text’s semantic structure explicit. The results of this analysis can be recorded
    in a database for querying and inference or used for further analysis.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*信息提取（IE**）* 是理解文本和构建复杂、引人入胜的机器学习应用过程中的第一步。它可以描述为分析文本、分解它，并识别其中语义定义的实体和关系的过程，目的是使文本的语义结构明确。这种分析的结果可以记录在数据库中以便查询和推理，或者用于进一步分析。'
- en: 'IE is a multistep process involving several analysis components that are combined
    to extract the most valuable information from text, making it available for further
    processing. Following are the main tasks, some of which we’ll consider in detail
    in the remainder of this section:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: IE是一个多步骤的过程，涉及几个分析组件的组合，以从文本中提取最有价值的信息，使其可用于进一步处理。以下是一些主要任务，其中一些我们将在本节剩余部分详细讨论：
- en: Tokenization, part of speech (PoS) tagging, stemming/lemmatization, and stop-word
    removal
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词、词性标注（PoS）、词干提取/词形还原和停用词去除
- en: Named entity recognition (NER)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）
- en: Entity relationship extraction (ERE)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体关系提取（ERE）
- en: Syntactic analysis
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法分析
- en: Coreference resolution
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共指消解
- en: Semantic analysis
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义分析
- en: This list is not an exhaustive one—other tasks can be considered to be part
    of the core information extraction process—but those items are the most common
    and, I would say, the most valuable and useful in terms of the amount of information,
    structure, and knowledge they enable you to extract from the text. The results
    of each IE task have to be organized and stored properly so that they are useful
    for other processes and analyses or to be queried. The model used to store these
    results is critical because it affects the performance of the subsequent operations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表并不是详尽的——其他任务也可以被认为是核心信息提取过程的一部分——但这些项目是最常见的，而且就信息量、结构和知识而言，我认为它们是最有价值且最有用的。每个IE任务的结果都必须被适当组织和存储，以便它们对其他过程和分析有用，或者可以查询。用于存储这些结果的模型至关重要，因为它会影响后续操作的性能。
- en: In this section, we will explore how graphs offer a perfect data model for taming
    text because they allow us to organize the structures and information within text
    so that they are immediately available for querying, analyzing, or extracting
    the sets of features necessary to feed other processes. For each task, a graph
    model is proposed to store the results properly. The proposed model will grow
    consistently from the first task to the last, and the resulting graph will incorporate
    all the knowledge that it is possible to distill from the text in a homogeneous
    data structure.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨图如何为驯服文本提供一个完美的数据模型，因为它们允许我们组织文本中的结构和信息，以便它们可以立即用于查询、分析或提取其他过程所需的特征集。对于每个任务，都提出了一个图模型来正确存储结果。所提出的模型将从第一个任务持续增长到最后一个任务，并且生成的图将包含从文本中提炼出的所有可能的知识，并以统一的数据结构呈现。
- en: 'To simplify the description and make it more concrete, I will describe some
    of these tasks by starting with a real scenario that requires that technique.
    The process will be incremental: at each stage, new information will be added
    to the graph, and at the end, we will have the full corpus processed, structured,
    accessible, and ready for the next steps. Let’s start our journey!'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化描述并使其更具体，我将通过描述一个需要该技术的真实场景来描述这些任务中的几个。这个过程将是逐步的：在每一个阶段，新的信息将被添加到图中，到最后，我们将拥有处理过的完整语料库，结构化、可访问，并准备好进行下一步。让我们开始我们的旅程！
- en: Suppose that you would like to decompose a text into its main elements (eventually
    normalizing them to the base form), get the role of each entity in the text, remove
    the useless words, and store the result in such a way that it is easy to navigate
    and query for searching purposes or for further analysis.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要将文本分解为其主要元素（最终将它们归一化到基本形式），获取文本中每个实体的角色，删除无用的单词，并以便于导航和查询搜索目的或进一步分析的方式存储结果。
- en: This scenario is quite common, because the first step of almost any IE process
    consists of breaking the content into small, usable chunks of text, called *tokens*.
    This process is called *tokenization*. Generally, tokens represent single words,
    but as you’ll soon see, what constitutes a small, usable chunk can be specific
    to an application. As noted in section 11.1, the simplest approach to tokenizing
    English text is to split a string based on the occurrence of whitespace such as
    spaces and line breaks, as our simple tokenizer will do. Using this approach on
    the sentence
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这种场景相当常见，因为几乎所有信息提取（IE）过程的第一个步骤都是将内容分解成小的、可用的文本块，这些文本块被称为*标记*。这个过程被称为*分词*。通常，标记代表单个单词，但正如你很快就会看到的，构成一个小而可用的块可以针对特定应用。如第11.1节所述，对英语文本进行分词的最简单方法是基于空白（如空格和换行符）的出现来分割字符串，正如我们的简单分词器所做的那样。使用这种方法对以下句子进行处理
- en: '*I couldn’t believe my eyes when I saw my favorite team winning the 2019-2020
    cup.*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*我简直不敢相信自己的眼睛，当我看到我最喜欢的球队赢得2019-2020赛季的奖杯时。*'
- en: 'yields the following list:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 得到以下列表：
- en: '[“I”, “couldn’t”, “believe”, “my”, “eyes”, “when”, “I”, “saw”, “my”, “favorite”,
    “team”, “winning”, “the”, “2019-2020”, “cup.”]'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[“我”， “couldn’t”， “believe”， “my”， “eyes”， “when”， “I”， “saw”， “my”， “favorite”，
    “team”， “winning”， “the”， “2019-2020”， “cup.”]'
- en: 'This approach is exactly the approach we used earlier, but clearly, it is not
    enough. To have better tokenization, we need to handle things such as punctuation,
    acronyms, email addresses, URLs, and numbers. If we apply a more complex tokenization
    approach that uses token classes such as alphabetic, numeric, and whitespace,
    the output should be something like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法正是我们之前使用的方法，但显然，这还不够。为了获得更好的分词效果，我们需要处理诸如标点符号、缩写、电子邮件地址、URL和数字等问题。如果我们应用一个更复杂的分词方法，该方法使用诸如字母、数字和空白等标记类，输出应该类似于以下这样：
- en: '[“I”, “couldn”, “’”, “t”, “believe”, “my”, “eyes”, “when”, “I”, “saw”, “my”,
    “favorite”, “team”, “winning”, “the”, “2019”, “-“, “2020”, “cup”, “.”]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[“我”， “couldn”， “’”， “t”， “believe”， “my”， “eyes”， “when”， “I”， “saw”， “my”，
    “favorite”， “team”， “winning”， “the”， “2019”， “-”， “2020”， “cup”， “.”]'
- en: Much better!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！
- en: In this example, we considered a single sentence, but in many scenarios, it
    is relevant to split the document into sentences first. In English, we can perform
    this task by considering punctuation such as periods and question marks. Tokenization
    and sentence splitting are greatly affected by several factors, the two most critical
    of which are
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们考虑了一个单独的句子，但在许多情况下，首先将文档分解成句子是相关的。在英语中，我们可以通过考虑诸如句号和问号之类的标点符号来完成这项任务。分词和句子分割受到几个因素的影响，其中两个最关键的因素是
- en: '*Language*—Different languages have different rules. These rules can dramatically
    affect the way in which you perform even a simple task like tokenization. There
    is no whitespace in between phrases in Chinese, for example, so splitting based
    on whitespace, as you’d do in English, may not work.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言*—不同的语言有不同的规则。这些规则可以显著影响你执行甚至像分词这样简单的任务的方式。例如，中文中没有短语之间的空格，所以像英语中那样基于空格分割可能不起作用。'
- en: '*Domain*—Some domains have specific elements that have a specific structure.
    Consider molecular names such as *3-(furan-2-yl)-[1,2,4]triazolo[3,4-b][1,3,4]thiadiazole*[⁴](#pgfId-1006723)
    in the chemistry domain and sizes such as *60 in. X 32 in.* in a home improvement
    retail domain.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域*—某些领域有特定的元素，这些元素具有特定的结构。例如，在化学领域，考虑像 *3-(furan-2-yl)-[1,2,4]triazolo[3,4-b][1,3,4]thiadiazole*
    这样的分子名称，以及在家庭装修零售领域的大小，如 *60 in. X 32 in.*。'
- en: Even using the more sophisticated approach, tokenization on its own is not enough
    in most cases. When you have the list of tokens, you can apply multiple techniques
    to get a better representation. The most common techniques [Farris et al., 2013]
    are
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用更复杂的方法，单独的分词在大多数情况下也不够。当你有了标记列表时，你可以应用多种技术来获得更好的表示。最常见的技巧 [Farris 等人，2013]
    是
- en: '*Case alterations*—This task involves changing the case of the tokens to a
    common case so that the tokens are uniform. The process is often more complex
    than lowercasing everything, though: some tokens should have the first letter
    uppercased because they appear at the beginning of a sentence, and others should
    be capitalized because they are proper nouns (such as names of people or locations).
    A proper case alteration considers these factors. Note that this task is language-specific;
    in Arabic, for example, there is no lowercasing or uppercasing.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大小写更改*—这项任务涉及将标记的大小写更改为通用大小写，以便标记是一致的。然而，这个过程通常比将所有内容转换为小写要复杂得多：一些标记应该首字母大写，因为它们出现在句子的开头，而其他标记应该大写，因为它们是专有名词（如人名或地点名）。适当的案例更改会考虑这些因素。请注意，这项任务是特定于语言的；例如，在阿拉伯语中，没有小写或大写。'
- en: '*Stop-word removal*—This task filters out common words such as *the*, *and*,
    and a. Commonly occurring words like these often add little value (note that I
    didn’t say *no* value) to applications that don’t rely on sentence structure.
    This list of stop words is also application-specific. If you’re processing a book
    like this one, you might want to filter out other words that add minimal value
    to the content but occur frequently, such as *chapter*, *section*, and *figure*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*停用词去除*—这项任务会过滤掉诸如 *the*、*and* 和 *a* 这样的常见词汇。这类经常出现的词汇通常对不依赖句子结构的应用程序价值不大（注意，我说的是
    *little* 价值，而不是 *no* 价值）。这个停用词列表也是特定于应用程序的。如果你在处理像这样一本书，你可能想过滤掉那些对内容价值最小但出现频率较高的其他词汇，例如
    *chapter*、*section* 和 *figure*。'
- en: '*Expansion*—Some tokens can be further extended or clarified by adding synonyms
    or expanding acronyms and abbreviations in a token stream. This task can allow
    applications to handle alternative inputs from users.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*扩展*—一些标记可以通过添加同义词或扩展标记流中的缩写和缩写词来进一步扩展或澄清。这项任务可以使应用程序能够处理来自用户的替代输入。'
- en: '*PoS tagging*—The purpose of this task is to identify a word’s part of speech—whether
    it’s a noun, verb, or adjective, for example. The task is highly valuable because
    it is used later in the process to enhance the quality of the results. PoS tagging
    can help determine the important keywords in a document, for example (we’ll see
    how later in this section), or support proper casing (such as *Will* as a proper
    noun versus *will* the modal verb).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词性标注*—这项任务的目的在于识别一个词的词性——比如它是名词、动词还是形容词等。这项任务非常有价值，因为它在后续处理过程中被用来提高结果的质量。词性标注可以帮助确定文档中的重要关键词，例如（我们将在本节后面看到），或者支持正确的首字母大写（例如，*Will*
    作为专有名词与 *will* 作为情态动词的区别）。'
- en: '*Lemmatization* *and stemming*—Suppose that you would like to search in a bunch
    of documents for the verb *take*. A simple string match will not work because,
    as you know, such a verb can appear in many forms, such as *take*, *took*, *taken*,
    *taking*, and *takes*. These forms are known as *surface forms*. The verb is the
    same, but it is conjugated in different ways according to the role it plays in
    the text and other syntactic rules. Lemmatization and stemming are tasks that
    allow us to reduce words to their root or base form, such as by converting all
    the surface forms of *take* to their root form. The difference between lemmatization
    and stemming is in the approach used to produce the root forms of words and the
    words produced. Generally speaking, stemming uses grammar rules, whereas lemmatization
    uses a dictionary-based approach. For this reason, stemming is faster but less
    accurate; lemmatization is slower but more precise.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词形还原* 和 *词干提取*——假设你想要在一组文档中搜索动词 *take*。简单的字符串匹配将不起作用，因为正如你所知，这样的动词可以以多种形式出现，如
    *take*、*took*、*taken*、*taking* 和 *takes*。这些形式被称为 *表面形式*。动词是相同的，但它根据它在文本中扮演的角色和其他句法规则以不同的方式进行屈折。词形还原和词干提取是允许我们将单词还原到其根或基本形式的任务，例如通过将
    *take* 的所有表面形式转换为它们的根形式。词形还原和词干提取之间的区别在于产生单词根形式的方法以及产生的单词。一般来说，词干提取使用语法规则，而词形还原使用基于字典的方法。因此，词干提取更快但准确性较低；词形还原较慢但更精确。'
- en: For further reading, I recommend the excellent practical books on these topics
    mentioned in the references section at the end of this chapter [Lane et al., 2019;
    Farris et al., 2013]. The books cover broadly the steps outlined here, with concrete
    examples.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 关于进一步阅读，我推荐本章末尾参考文献部分提到的这些主题的优秀实用书籍 [Lane 等人，2019；Farris 等人，2013]。这些书籍广泛涵盖了此处概述的步骤，并提供了具体的示例。
- en: Figure 11.13 shows some of the tasks described in the preceding list applied
    to a simple sentence.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 展示了前面列表中描述的一些任务应用于简单句子的情况。
- en: '![CH11_F13_Negro](../Images/CH11_F13_Negro.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F13_Negro](../Images/CH11_F13_Negro.png)'
- en: Figure 11.13 Tokenization, stop-word removal, and lemmatization applied to a
    sample sentence
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 对示例句子应用分词、停用词去除和词形还原
- en: 'A lot of tasks seem to be involved in information extraction, but the reality
    is that (unless you would like to implement heavy customizations) plenty of software
    and libraries for various programming languages can perform all these tasks for
    you. The following example demonstrates the use of one of the most common libraries
    for such purposes: the spaCy[⁵](#pgfId-1006743) Python library.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 许多任务似乎都涉及信息提取，但现实是（除非你想实施大量自定义），各种编程语言的许多软件和库可以为你执行所有这些任务。以下示例演示了用于此类目的最常用的库之一：spaCy[⁵](#pgfId-1006743)
    Python 库。
- en: Listing 11.12 Basic text processing with spaCy
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.12 使用 spaCy 进行基本文本处理
- en: '[PRE11]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Prefers the GPU (which is faster) whenever possible
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在可能的情况下优先使用 GPU（因为它更快）
- en: ❷ Loads the English-language model
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载英语语言模型
- en: ❸ Processes the text
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 处理文本
- en: ❹ Loops over the sentences
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历句子
- en: ❺ Loops over the tokens
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 遍历标记
- en: ❻ Prints the index (the token start position), the text as it is, and the lemmatized
    version
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印索引（标记起始位置）、文本本身和词形还原版本
- en: 'This listing[⁶](#pgfId-1006758) implements a basic example that prints the
    result of the tokenization, which looks like this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表[⁶](#pgfId-1006758) 实现了一个基本示例，打印出分词的结果，如下所示：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How should we store the result of this first step in a graph model? As usual,
    there is no single right answer; the answer depends on what you want to do with
    it. What I’m going to present is the schema that we use in GraphAware Hume, which
    has proved to be flexible enough to cover a plethora of scenarios without any
    particular difficulties. The only issue is that it is quite verbose, in the sense
    that it stores a lot of data that is sometimes not required. As you’ll see, it
    provides a starting point; you can prune some parts or add others.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何在图模型中存储这个第一步的结果？通常情况下，没有唯一的正确答案；答案取决于你打算如何使用它。我将要介绍的是我们在 GraphAware Hume
    中使用的方案，它已经证明足够灵活，可以覆盖大量场景而不会遇到任何特别困难。唯一的问题是它相当冗长，从某种意义上说，它存储了有时并不需要的大量数据。正如你将看到的，它提供了一个起点；你可以修剪一些部分或添加其他部分。
- en: 'The first schema presented is the minimum needed to fulfill a significant number
    of scenarios and requirements. The following aspects of this model are critical
    for many applications and uses:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先提出的方案是满足大量场景和需求所需的最小方案。此模型以下方面对于许多应用和使用至关重要：
- en: '*Sentence nodes*—The main text is split into sentences, which are key elements
    in most text-analysis use cases (such as summarization and similarity computation).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*句子节点*—主文本被分割成句子，这是大多数文本分析用例（如摘要和相似度计算）中的关键元素。'
- en: '*TagOccurrence nodes*—These nodes contain details on how the tag appears in
    the text, such as start and end position, actual value, and lemma (PoS value).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签出现节点*—这些节点包含有关标签在文本中如何出现的信息，例如起始和结束位置、实际值和词元（词性值）。'
- en: HAS_NEXT *relationships*—There are relationships of type HAS_NEXT between TagOccurrence
    nodes, which have the same scope as in section 11.1\. In this way, this new schema
    incorporates—and heavily extends—the schema produced earlier so that the previous
    scenarios can also be resolved by using this new model.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAS_NEXT *关系*—标签出现节点之间存在类型为 HAS_NEXT 的关系，其范围与第 11.1 节中相同。通过这种方式，这个新架构结合并大量扩展了之前生成的架构，以便可以使用这个新模型解决之前的场景。
- en: Figure 11.14 shows the schema. The comments in it should help you read it properly,
    even though it may appear to be complex.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14 展示了架构。其中的注释应该能帮助你正确阅读，即使它可能看起来很复杂。
- en: '![CH11_F14_Negro](../Images/CH11_F14_Negro.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F14_Negro](../Images/CH11_F14_Negro.png)'
- en: Figure 11.14 First schema for dealing with text properly
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14 处理文本的正确第一个架构
- en: This schema can be improved slightly by adding Tag nodes to represent the lemmatized
    versions of the tokens. These nodes are unique; they are stored once, and all
    the sentences that contain such tags point to them via a TagOccurrence.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加标签节点来表示词元的归一化版本，可以略微改进此架构。这些节点是唯一的；它们只存储一次，包含此类标签的所有句子都通过标签出现节点指向它们。
- en: For the reasons mentioned earlier, common words can generate dense nodes. To
    mitigate this issue, only non-stop words are stored as Tag nodes. The resulting
    model will look like figure 11.15.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面提到的原因，常见单词可以生成密集节点。为了减轻这个问题，只有非停用词被存储为标签节点。结果模型将类似于图 11.15。
- en: '![CH11_F15_Negro](../Images/CH11_F15_Negro.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F15_Negro](../Images/CH11_F15_Negro.png)'
- en: Figure 11.15 Extended schema with Tag nodes
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15 带有标签节点的扩展架构
- en: The Tag nodes simplify navigation through the graph database when you would
    like to access it by using some key term(s) as an entry point. (Think about search,
    for example.) You could also achieve this task by using an index on the TagOccurrence
    nodes, but some queries are much easier to perform when you access Tag nodes directly
    and then use the relationships to TagOccurrence nodes. Because these nodes are
    not critical for our purposes, we will ignore them in our schema, examples, and
    exercises to make the graph easier to read, but keep them in mind as an option
    for specific access patterns.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想使用一些关键术语作为入口点来访问图数据库时，标签节点简化了导航。例如，你可以通过在标签节点上直接进行查询并使用关系来访问标签出现节点来实现这个任务。因为这些节点对我们来说不是关键，所以我们将忽略它们在架构、示例和练习中的内容，以使图更容易阅读，但请记住它们作为特定访问模式的选择。
- en: With this new model in hand, we can extend our code for processing text and
    storing it in a graph. The following listing is a bit more sophisticated than
    listing 11.12, which converted the text to a graph by using the model described
    in figures 11.11 and 11.12.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个新模型后，我们可以扩展我们的文本处理代码并将其存储在图中。下面的列表比列表 11.12 更为复杂，它通过使用图 11.11 和 11.12 中描述的模型将文本转换为图。
- en: Listing 11.13 Creating the first graph out of text
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.13 创建文本的第一个图
- en: '[PRE13]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Processes the text without NER (improves performance)
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不使用 NER 处理文本（提高性能）
- en: ❷ Loops over the processed docs. The pipe accepts a list of documents and returns
    a list of processed docs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历处理过的文档。管道接受文档列表并返回处理过的文档列表。
- en: ❸ Loops over the sentences, calling the store function on each
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历句子，对每个句子调用存储函数
- en: ❹ This function creates the main node with the label AnnotatedText, to which
    all the other nodes will be connected.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 这个函数创建带有标签 AnnotatedText 的主要节点，所有其他节点都将连接到它。
- en: ❺ The query for creating the AnnotatedText node is simple, storing only an ID
    to identify the original document.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建 AnnotatedText 节点的查询很简单，只存储一个 ID 来标识原始文档。
- en: ❻ This common function executes all the queries in the code. It requires the
    query and the parameters, and executes the query in a transaction.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 这个常用函数执行代码中的所有查询。它需要查询和参数，并在事务中执行查询。
- en: ❼ This function stores sentences along with the tag occurrences and the tags.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 这个函数存储句子以及标签出现和标签。
- en: ❽ This query searches for the AnnotatedText node created by id, creates a sentence,
    and connects it to the AnnotatedText node.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 此查询通过id搜索创建的AnnotatedText节点，创建一个句子，并将其连接到AnnotatedText节点。
- en: ❾ This query stores the TagOccurrence nodes, connecting them to the sentence
    and to one another.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 此查询存储TagOccurrence节点，将它们连接到句子和彼此。
- en: ❿ This query stores the TagOccurrence nodes, connecting them to the sentence
    and to one another and the Tag nodes. It is used as an alternative to the previous
    tag_occurrence_ query to store the Tag nodes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 此查询存储TagOccurrence节点，将它们连接到句子和彼此以及Tag节点。它用作存储Tag节点的替代方案，用于之前的tag_occurrence_查询。
- en: ⓫ Runs the query for storing the sentence
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 运行查询以存储句子
- en: ⓬ Loops over the tokens extracted for each sentence and creates an array of
    dict to be used as a parameter for the query that stores the sentence
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 遍历为每个句子提取的标记，并创建一个字典数组，用作存储句子的查询参数
- en: ⓭ This filter avoids storing punctuation and spaces.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 此过滤器避免存储标点和空格。
- en: ⓮ Executes the query with Tag
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 使用Tag执行查询
- en: ⓯ Executes the query without Tag
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⓯ 不使用Tag执行查询
- en: In this code, with a single parameter (storeTag), it is possible to decide whether
    to store the Tag nodes. Because Tag nodes are not necessary in the rest of this
    section, this flag is set to false, which will result in a less verbose database
    and help us avoid dense node issues.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，通过一个参数（storeTag），可以决定是否存储Tag节点。因为Tag节点在本节的其余部分中不是必需的，所以此标志设置为false，这将导致数据库更加简洁，并帮助我们避免密集节点问题。
- en: The tokenization breaks up the text according to specific splitting rules, which
    are generally a bit more complex than using whitespace and punctuation. Nevertheless,
    you might like to get more from your text. Tokens in a sentence are not isolated
    components; they are related through linguistic relationships. Syntactic relationships,
    for example, capture the role of a word in modifying the semantics of other words
    in the sentence and are helpful for determining the subject and predicate. In
    the example used earlier,
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是根据特定的分割规则来拆分文本的，这些规则通常比使用空格和标点符号要复杂一些。尽管如此，你可能希望从文本中获得更多信息。句子中的标记不是孤立的组件；它们通过语言关系相互关联。例如，句法关系可以捕捉一个词在句子中改变其他词语义的角色，并有助于确定主语和谓语。在前面使用的例子中，
- en: '*I couldn’t believe my eyes when I saw my favorite team winning the 2019-2020
    cup.*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*当我看到我最喜欢的队伍赢得2019-2020赛季的奖杯时，我简直不敢相信自己的眼睛。*'
- en: '*I* is syntactically related to *believe* because it is the subject of that
    verb. Capturing these types of dependencies is critical for further understanding
    the text: they allow us to determine semantic relations (who did what to whom)
    later in the pipeline. Generally speaking, richer analysis at this stage simplifies
    the semantic analysis that follows.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*I* 与 *believe* 在句法上相关，因为它是这个动词的主语。捕捉这些类型的依赖关系对于进一步理解文本至关重要：它们允许我们在管道的后续步骤中确定语义关系（谁对谁做了什么）。一般来说，这一阶段的丰富分析简化了后续的语义分析。'
- en: 'So let’s extend our previous scenario, adding a new requirement: you would
    like to recognize key syntactic elements in the text (such as verbs and their
    subjects and predicates) to improve understanding of the text for further analysis.
    Among the various parsing methods that have been proposed to date, dependency
    parsing—which is concerned with the identification of dependency structures in
    the text—has gained the greatest attention. Figure 11.16 shows the dependency
    parse obtained for our sample sentence by using the CoreNLP test service.[7](#pgfId-1006772)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们扩展我们之前的场景，添加一个新的要求：你希望识别文本中的关键句法元素（如动词及其主语和谓语），以改善对文本的理解，以便进行进一步分析。在迄今为止提出的各种解析方法中，关注于识别文本中依赖结构的依赖解析受到了最多的关注。图11.16显示了通过使用CoreNLP测试服务为我们示例句子获得的依赖解析。[7](#pgfId-1006772)
- en: '![CH11_F16_Negro](../Images/CH11_F16_Negro.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F16_Negro](../Images/CH11_F16_Negro.png)'
- en: Figure 11.16 Dependencies among tokens obtained via corenlp.run
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16 通过corenlp.run获得的标记之间的依赖关系
- en: 'I hope that at this point in the book, you can recognize immediately where
    a graph can be applied. In this case, it is a special type of graph: a tree. In
    dependency parsing, each sentence is represented as a tree, which has as its root
    either the main predicate of the sentence or a dummy node labeled root with the
    main predicate as its sole child [Mihalcea and Radev, 2011]. Edges are used to
    connect each word to its dependency parent. In the sentence “John likes green
    apples,” the predicate is *likes*. It takes two arguments: the liker (*John*)
    and the liked (*apples*). The word *green* modifies *apples*, so it is added to
    the tree as a child of *apples*. The final tree is shown in fig-ure 11.17.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望到这本书的这一部分，你能够立即识别出哪里可以应用图。在这种情况下，它是一种特殊类型的图：一棵树。在依存句法分析中，每个句子都表示为一棵树，其根是句子的主要谓语或标记为根的虚拟节点，该虚拟节点以主要谓语作为其唯一的子节点
    [Mihalcea and Radev, 2011]。边用于将每个词与其依存父节点连接起来。在句子“John likes green apples”中，谓语是
    *likes*。它有两个论元：喜欢者（*John*）和被喜欢者（*apples*）。单词 *green* 修饰 *apples*，因此它被添加到树中作为 *apples*
    的子节点。最终的树如图 11.17 所示。
- en: '![CH11_F17_Negro](../Images/CH11_F17_Negro.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F17_Negro](../Images/CH11_F17_Negro.png)'
- en: Figure 11.17 An example of a dependency tree
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17 依赖树的示例
- en: Adding these new syntactic relationships to our graph model is straightforward.
    Figure 11.18 shows how it could work.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些新的句法关系到我们的图模型中是直接的。图 11.18 展示了它是如何工作的。
- en: '![CH11_F18_Negro](../Images/CH11_F18_Negro.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![CH11_F18_Negro](../Images/CH11_F18_Negro.png)'
- en: Figure 11.18 Extended schema with the dependency relationships
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 带有依赖关系的扩展模式
- en: In the resulting graph model, the new relations connect TagOccurrence nodes
    to the dependent nodes. This connection is necessary because the same Tag can
    have different relationships in different sentences (*John* might be the subject
    in some sentences and the object in others), whereas a TagOccurrence represents
    the tag in a specific sentence context and can have only a specific role. The
    direction of the relationships follows the schema in figure 11.14, and the root
    (the main verb) of the dependency tree is recognizable via the self loop. The
    following listing is the code for extracting and storing the dependencies in the
    graph.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的图模型中，新的关系将连接 TagOccurrence 节点与依赖节点。这种连接是必要的，因为同一个标签在不同的句子中可能具有不同的关系（例如，“John”可能在某些句子中是主语，在其他句子中是宾语），而
    TagOccurrence 代表特定句子上下文中的标签，并且只能具有特定的角色。关系的方向遵循图 11.14 中的模式，依赖树的根（主动词）可以通过自环识别。以下列表是提取和存储图中的依赖关系的代码。
- en: Listing 11.14 Extracting and storing the dependencies
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.14 提取和存储依赖关系
- en: '[PRE14]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ The piece of code affected by the changes to store the dependencies
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 受存储依赖关系更改影响的代码片段
- en: ❷ The dict with the dependency information is prepared and then appended to
    the list of dependencies.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 准备包含依赖信息的字典，并将其附加到依赖关系列表中。
- en: ❸ A specific function is called after the creation of the TagOccurrence nodes
    to store the dependencies among them.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在创建 TagOccurrence 节点之后，会调用一个特定函数来存储它们之间的依赖关系。
- en: ❹ The query goes through the dependencies, searches for TagOccurrence nodes,
    and connects them.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 查询通过依赖关系进行，搜索 TagOccurrence 节点，并将它们连接起来。
- en: 'At this stage, the resulting graph contains sentences, tokens—lemmatized, marked
    as stop words, and with PoS information—and relationships between tokens that
    describe their role in the sentence. That’s a lot of information that can serve
    a wide range of use cases, such as the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，生成的图包含句子、标记（词形还原、标记为停用词，并带有词性信息）以及描述它们在句子中角色的标记之间的关系。这是一些可以服务于广泛用例的大量信息，例如以下内容：
- en: '*Next-word suggestion*—As in section 11.1 with the next schema model, it is
    possible to suggest the next word considering the current one or any number of
    previous words.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*下一词建议*——与 11.1 节中的下一个模式模型一样，考虑到当前词或任何数量的前一个词，可以建议下一个词。'
- en: '*Advanced search engines*—When we have the information about the order of the
    words together with dependencies among them, we can implement advanced search
    capabilities in which, apart from checking for the exact order of the words, it
    is possible to consider cases with some words between our target and provide some
    suggestion. A concrete example follows this list.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高级搜索引擎*——当我们有了关于词的顺序以及它们之间依存关系的信息时，我们可以实现高级搜索功能，除了检查词的精确顺序外，还可以考虑目标词之间有一些词的情况，并提供一些建议。具体示例如下。'
- en: '*Content-based recommendation*—By decomposing the text into components, we
    can compare item descriptions (movies, products, and so on). This step is one
    of the first required for providing content-based recommendations. In this case,
    having the lemmatization and other normalization in place (stop-word removal,
    punctuation handling, and so on) will make the comparisons even more accurate.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于内容的推荐*——通过将文本分解成组件，我们可以比较项目描述（电影、产品等）。这一步是提供基于内容推荐的第一步要求。在这种情况下，如果已经实现了词干提取和其他规范化（如停用词去除、标点符号处理等），将使比较更加准确。'
- en: 'With the schema in mind and the code in hand, let’s try to accomplish a concrete
    task. Suppose that you have the following three sentences:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 带着模式和手头的代码，让我们尝试完成一个具体任务。假设你有以下三个句子：
- en: “John likes green apples.”
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “John likes green apples.”
- en: “Melissa picked up three small red apples.”
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “Melissa picked up three small red apples.”
- en: “That small tree produces tasty yellow apples.”
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “That small tree produces tasty yellow apples.”
- en: Exercise
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 练习
- en: Import the three sentences into the graph, using listings 11.13 and 11.14\.
    To look for all the documents containing the word *apples*, you can use the following
    query.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表 11.13 和 11.14 将三个句子导入图表。要查找包含单词 *apples* 的所有文档，可以使用以下查询。
- en: Listing 11.15 Searching for documents with the word apples
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.15 搜索包含单词 apples 的文档
- en: '[PRE15]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Easy—but you can do this with any search engine. Now let’s consider a more
    complex use case: searching for *small apples*. With a search engine, you have
    two options: search for the words in that specific order, or search for both words
    in the document in any order. In the first case, you will not get any results
    (because *red* appears between the two words), whereas in the second case, you
    will get two documents (because both words also appear in the third document).
    This scenario is where the graph model we’ve created shows its power. Here is
    the query to perform this search.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可以用任何搜索引擎完成，但现在让我们考虑一个更复杂的使用案例：搜索 *小苹果*。使用搜索引擎，你有两种选择：按特定顺序搜索这些词，或者搜索文档中这两个词的任何顺序。在前一种情况下，你将不会得到任何结果（因为
    *red* 出现在这两个词之间），而在后一种情况下，你将得到两个文档（因为这两个词也出现在第三个文档中）。这种场景正是我们创建的图模型显示其力量的地方。以下是执行此搜索的查询。
- en: Listing 11.16 Searching for small apples
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.16 搜索小苹果
- en: '[PRE16]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ This line checks whether a syntactic dependency exists between the two tokens.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这一行检查两个标记之间是否存在句法依存关系。
- en: Exercise
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 练习
- en: With the graph, we can express queries in the same way we would in a search
    engine. Write the queries that find documents that contain the exact phrase *small
    apples* and find documents that contain those two words in any order.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图表，我们可以用与在搜索引擎中相同的方式表达查询。编写查询以找到包含确切短语 *小苹果* 的文档，以及找到包含这两个词以任何顺序出现的文档。
- en: Let’s look at one more example that shows the power of NLP combined with a graph
    approach. As the following query demonstrates, we can use the graph to answer
    even more complex questions, forming the basis for applications such as information
    retrieval, chatbots, and conversational platforms.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再来看一个例子，这个例子展示了自然语言处理（NLP）与图表方法的结合力量。正如以下查询所展示的，我们可以使用图表来回答甚至更复杂的问题，这为信息检索、聊天机器人和对话平台等应用奠定了基础。
- en: Listing 11.17 Answering the question “What are the apples like?”
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.17 回答问题“苹果是什么样的？”
- en: '[PRE17]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A graph is capable of answering complex questions without any human effort in
    terms of training. Decomposing the text and establishing the proper graph structure
    allow us to do a lot. Chapter 12 extends this idea by building a proper knowledge
    graph that provides support for more complicated scenarios.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图形能够无需任何人工训练努力回答复杂问题。分解文本并建立适当的图结构使我们能够做很多事情。第 12 章通过构建适当的知识图来扩展这一想法，该知识图为更复杂的场景提供支持。
- en: 11.2.1 Advantages of the graph approach
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 图形方法的优点
- en: This section showed clearly how well NLP and graphs work together. Due to the
    highly connected nature of the data produced by NLP tasks, storing their results
    in a graph model appears to be a logical, rational choice. In some cases, as with
    syntactic dependencies, the relationships are generated as output of the NLP task,
    and the graph only has to store them. In other cases, the model has been designed
    to serve multiple scopes at the same time, providing easy-to-navigate data structures.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本节清楚地展示了自然语言处理（NLP）和图如何很好地协同工作。由于NLP任务产生数据的强连接性，将这些结果存储在图模型中似乎是一个逻辑上合理的、理性的选择。在某些情况下，例如与句法依赖关系一样，关系作为NLP任务的输出生成，图模型只需存储它们。在其他情况下，模型被设计为同时服务于多个范围，提供易于导航的数据结构。
- en: 'The graph model proposed here not only stores the main data and relationships
    extracted during the IE process, but also allows further extension by adding new
    information computed in a postprocessing phase: similarity computation, sentiment
    extraction, and so on. As a result, with relatively little effort, we can serve
    the word-suggestion use case as well as more complex search needs and even question
    answering (“What are the apples like?”).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提出的图模型不仅存储了信息提取过程中提取的主要数据和关系，还允许通过在后期处理阶段添加新信息进行扩展：相似度计算、情感提取等。因此，我们只需付出相对较小的努力，就可以服务于词建议用例以及更复杂的搜索需求，甚至问答（“苹果是什么样的？”）。
- en: This model is further extended in chapter 12 to include more information extracted
    from the text and the results of postprocessing, allowing it to serve even more
    scenarios and applications.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在第12章中进一步扩展了此模型，包括从文本和后处理结果中提取的更多信息，使其能够服务于更多场景和应用。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced key concepts related to NLP and knowledge representation,
    matching them with graphs and graph models. The main point is that graphs can
    not only store textual data, but also provide a conceptual toolset for processing
    text and deliver advanced features—enabling complex search scenarios, for example—with
    minimal effort.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了与NLP和知识表示相关的关键概念，将它们与图和图模型相匹配。主要观点是，图不仅可以存储文本数据，还可以提供处理文本的概念工具集，并通过最小努力提供高级功能——例如，使复杂的搜索场景成为可能。
- en: Topics covered included
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖的主题包括
- en: How to store text in the simplest way possible by decomposing it into chunks
    that are easy to navigate
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何以最简单的方式存储文本，通过将其分解成易于导航的块
- en: How to extract a meaningful set of information from a text
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从文本中提取有意义的信息集
- en: How to design a powerful graph model for storing text and accessing it in different
    applications
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何设计一个强大的图模型来存储文本并在不同的应用程序中访问它
- en: How to query a graph for different purposes such as search, question answering,
    and word suggestion
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何根据不同的目的查询图，例如搜索、问答和词建议
- en: References
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Farris et al., 2013] Farris, Andrew L., Grant S. Ingersoll, and Thomas S.
    Morton. *Taming Text: How to Find, Organize, and Manipulate It*. Shelter Island,
    NY: Manning, 2013.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[Farris et al., 2013] Farris, Andrew L., Grant S. Ingersoll, 和 Thomas S. Morton.
    *文本驯服：如何查找、组织和操作它*. 纽约州舍托岛：Manning, 2013。'
- en: '[Grishman, 2015] Grishman, Ralph. “Information Extraction.” *IEEE Intelligent
    Systems* 30:5 (2015): 8-15.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[Grishman, 2015] Grishman, Ralph. “信息提取。” *IEEE智能系统* 30:5 (2015): 8-15.'
- en: '[Lane et al., 2019] Lane, Hobson, Cole Howard, and Hannes Hapke. *Natural Language
    Processing in Action*. Shelter Island, NY: Manning, 2019.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lane et al., 2019] Lane, Hobson, Cole Howard, 和 Hannes Hapke. *自然语言处理实战*.
    纽约州舍托岛：Manning, 2019。'
- en: '[Mihalcea and Radev, 2011] Mihalcea, Rada, and Dragomir Radev. *Graph-Based
    Natural Language Processing and Information Retrieval*. New York: Cambridge University
    Press, 2011.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mihalcea and Radev, 2011] Mihalcea, Rada, 和 Dragomir Radev. *基于图的自然语言处理和信息检索*.
    纽约：剑桥大学出版社，2011。'
- en: '[Turnbull and Berryman, 2016] Turnbull, Doug, and John Berryman. *Relevant
    Search: With Applications for Solr and Elasticsearch*. Shelter Island, NY: Manning,
    2016.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[Turnbull and Berryman, 2016] Turnbull, Doug, 和 John Berryman. *相关搜索：适用于Solr和Elasticsearch的应用*.
    纽约州舍托岛：Manning, 2016。'
- en: '* * *'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)[http://mng.bz/y9Dq](https://shortener.manning.com/y9Dq).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: (1.)[http://mng.bz/y9Dq](https://shortener.manning.com/y9Dq).
- en: ^(2.)With MATCH (n) DETACH DELETE n.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: (2.)使用MATCH (n) DETACH DELETE n.
- en: '^(3.)The file can be downloaded here: [http://mng.bz/MgKn](https://shortener.manning.com/MgKn).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: (3.)文件可以在此处下载：[http://mng.bz/MgKn](https://shortener.manning.com/MgKn).
- en: '^(4.)I explicitly searched for a complex name and found it here: [http://mng.bz/aKzB](https://shortener.manning.com/aKzB).'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: (4.)我明确搜索了一个复杂名称，并在这里找到了它：[http://mng.bz/aKzB](https://shortener.manning.com/aKzB).
- en: ^(5.)[https://spacy.io](https://spacy.io).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spacy.io](https://spacy.io).'
- en: ^(6.)The code is available in the book’s repository, at ch11/basic_nlp_examples/01_spacy_basic_nlp_tasks.py.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可在本书的仓库中找到，位于 ch11/basic_nlp_examples/01_spacy_basic_nlp_tasks.py。
- en: ^(7.)[http://corenlp.run](http://corenlp.run).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://corenlp.run](http://corenlp.run).'
