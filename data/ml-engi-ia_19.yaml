- en: Appendix A. Big O(no) and how to think about runtime performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录A. 大O(no)以及如何考虑运行性能
- en: Runtime complexity, for ML use cases, is no different than it is for any other
    piece of software. The impact of inefficient and poorly optimized code affects
    processing tasks in ML jobs the same as it does any other engineering project.
    The only material difference that sets ML tasks apart from traditional software
    is in the algorithms employed to solve problems. The computational and space complexity
    of these algorithms is typically obscured by high-level APIs that encapsulate
    recursive iterations, which can dramatically increase runtimes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习用例，运行时间复杂度与其他任何软件没有区别。不高效和优化不良的代码对机器学习任务中的处理任务的影响与对任何其他工程项目的影响相同。唯一将机器学习任务与传统软件区分开来的实质性差异在于解决问题的算法。这些算法的计算和空间复杂度通常被封装递归迭代的通用API所掩盖，这可能会显著增加运行时间。
- en: The goal of this appendix is to focus on understanding both the runtime characteristics
    of *control code* (all the code in your project that isn’t involved in training
    a model) and the ML algorithm itself that is being trained.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录的目的是专注于理解*控制代码*（项目中所有不涉及训练模型的代码）的运行特性以及正在训练的机器学习算法本身。
- en: A.1 What is Big O, anyway?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.1 什么是大O？
- en: Let’s suppose we’re working on a project that is set to release soon to production.
    The results are spectacular, and the business unit for whom the project was built
    is happy with the attribution results. However, not everyone is happy. The costs
    of running the solution are incredibly high.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在开发一个即将投入生产的项目的项目。结果是令人瞩目的，为该项目构建的业务单元对归因结果感到满意。然而，并非所有人都满意。运行解决方案的成本非常高。
- en: As we step through the code, we discover that the vast majority of the execution
    time is centered around our feature-engineering preprocessing stages. One particular
    portion of the code seems to take far longer than we originally expected. Based
    on initial testing, shown in the following listing, we had imagined that this
    function wouldn’t be much of a problem.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们逐步通过代码的过程中，我们发现大部分执行时间都集中在我们的特征工程预处理阶段。代码的某个特定部分似乎比我们最初预期的要花费更长的时间。根据以下列表中的初始测试，我们原本认为这个函数不会造成太大问题。
- en: Listing A.1 Nested loop name-reconciliation example
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.1 嵌套循环名称协调示例
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The list of registered names of dogs in our database (small sample)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们数据库中狗的注册名称列表（小样本）
- en: ❷ The parsed names from the free-text field ratings that we get from our customer’s
    humans
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 来自我们客户的人类从自由文本字段评分中解析的名称
- en: ❸ Looping through every one of our registered names
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历我们所有的注册名称
- en: ❹ The O(n²) nested loop, going through each of the parsed names
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ O(n²)嵌套循环，遍历每个解析名称
- en: ❺ Calculates the Levenshtein distance between the names after removing spaces
    and forcing lowercase on both strings
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算去除空格并在两个字符串上强制转换为小写后的名称之间的Levenshtein距离
- en: ❻ Loops through the pairwise distance measurements to return the most likely
    match for each parsed name. This is O(n).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 遍历成对距离测量以返回每个解析名称的最可能匹配项。这是O(n)。
- en: ❼ Runs the algorithm against the two lists of registered names and parsed names
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 对注册名称列表和解析名称列表运行算法
- en: ❽ The results of closest match by Levenshtein distance
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 通过Levenshtein距离找到的最近匹配结果
- en: On the small dataset used for validation and development, the execution time
    was in milliseconds. However, when running against our full dataset of 5 million
    registered dogs and 10 billion name reference extractions, we simply have too
    many dogs in our data to run through this algorithm. (Yes, there can be such a
    thing as too many dogs, believe it or not.).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于验证和开发的较小数据集上，执行时间以毫秒计。然而，当针对我们包含500万注册狗和1000亿名称参考提取的完整数据集运行时，我们数据中的狗太多，无法运行此算法。（是的，可能会有太多狗的情况，信不信由你。）
- en: The reason is that the computational complexity of this algorithm is O(*n*²).
    For each registered name, we’re testing its distance to each of the name extracts,
    as shown in figure A.1.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是此算法的计算复杂度为O(*n*²)。对于每个注册名称，我们正在测试其与每个名称提取的距离，如图A.1所示。
- en: '![A-01](../Images/A-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![A-01](../Images/A-01.png)'
- en: Figure A.1 The computational complexity of our feature engineering
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.1 我们特征工程的计算复杂度
- en: The following listing shows an alternative approach to reducing the looped searching.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了减少循环搜索的另一种方法。
- en: Listing A.2 A slightly better approach (but still not perfect)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.2 一种稍微好一点的方法（但仍不完美）
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Creates a pandas DataFrame from the client names list
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从客户端名称列表创建pandas DataFrame
- en: ❷ Generates a static join key to support our Cartesian join we’ll be doing
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成静态连接键以支持我们将要执行的笛卡尔连接
- en: ❸ Cleans up the names so that our Levenshtein calculation can be as accurate
    as possible (function defined in listing A.1)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 清理名称，以便我们的Levenshtein计算尽可能准确（在列表A.1中定义的函数）
- en: ❹ Generates the same static join key in the right-side table to effect the Cartesian
    join
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在右侧表中生成相同的静态连接键以实现笛卡尔连接
- en: ❺ Performs the Cartesian join (which is O(n²) space complexity)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 执行笛卡尔连接（空间复杂度为O(n²)）
- en: ❻ Calculates the Levenshtein distance by using the thoroughly useful NLTK package
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 通过使用非常有用的NLTK包计算Levenshtein距离
- en: ❼ Removes any potential non-matches from the DataFrame
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 从DataFrame中移除任何潜在的非匹配项
- en: ❽ Returns the rows for each potential match key that has the lowest Levenshtein
    distance score
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 返回具有最低Levenshtein距离得分的每个潜在匹配键的行
- en: NOTE if you’re curious about the NLTK package and all of the fantastic things
    that it can do for natural language processing in Python, I highly encourage you
    to read *Natural Language Processing with Python* (O’Reilly, 2009) by Steven Bird,
    Ewan Klein, and Edward Loper, the original authors of the open source project.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您对NLTK包及其在Python中为自然语言处理所能做的所有奇妙事情感兴趣，我强烈建议您阅读Steven Bird、Ewan Klein和Edward
    Loper所著的《Python自然语言处理》（O’Reilly，2009年），他们是开源项目的原始作者。
- en: Utilizing this DataFrame approach can remarkably speed up the runtime. Listing
    A.2 is not a perfect solution, since the space complexity will increase, but refactoring
    in this manner can dramatically reduce the runtime of the project and reduce costs.
    Figure A.2 shows the results of calling the function defined in listing A.2.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这种DataFrame方法可以显著加快运行时间。列表A.2不是一个完美的解决方案，因为空间复杂度会增加，但以这种方式重构可以显著减少项目的运行时间并降低成本。图A.2显示了调用列表A.2中定义的函数的结果。
- en: '![A-02](../Images/A-02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![A-02](../Images/A-02.png)'
- en: Figure A.2 Reducing computational complexity at the expense of space complexity
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.2 以空间复杂性的代价降低计算复杂度
- en: 'The important thing to remember about this example is that scalability is *relative*.
    Here we are trading computational complexity for space complexity: we originally
    were sequentially looping through two arrays, which takes a long time to do, but
    has a very low memory footprint; then, working with the matrix-like structure
    of pandas is orders of magnitude faster but requires a great deal of RAM. In actual
    practice, with the data volumes involved here, the best solution is to process
    this problem in a mix of looped processing (preferably in Spark `DataFrame` s)
    while leveraging Cartesian joins in chunks to find a good balance between computation
    and space pressure.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个示例的重要事情是要记住可扩展性是相对的。在这里，我们是在计算复杂性和空间复杂性之间进行权衡：我们最初是顺序遍历两个数组，这需要很长时间，但内存占用非常低；然后，使用pandas的矩阵结构要快得多，但需要大量的RAM。在实际操作中，考虑到这里涉及的数据量，最佳解决方案是在循环处理（最好在Spark
    `DataFrame`中）的同时，分块利用笛卡尔连接来找到计算和空间压力之间的良好平衡。
- en: Refactoring for performance and cost
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 优化性能和成本
- en: Most refactoring of code bases is done to enhance their testability and extensibility.
    But in ML code bases, a frequent activity that prompts enhancements is runtime
    efficiency. This generally is focused more on the training and retraining of models
    than on prediction aspects of ML, but incredibly complex feature engineering is
    involved in these jobs. Many times, the root cause of nonperformant code in ML
    projects is in the feature-processing and control logic, rather than in the training
    of the model(s) (except for the case of extensive hyperparameter tuning, which
    will likely dominate the total runtime).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数代码库重构是为了提高其可测试性和可扩展性。但在机器学习代码库中，一个常见的活动是提高运行时效率。这通常更多地关注模型的训练和再训练，而不是机器学习的预测方面，但涉及的工作中包含着极其复杂的特征工程。很多时候，机器学习项目中代码性能不佳的根本原因在于特征处理和控制逻辑，而不是模型（s）的训练（除非是广泛的超参数调整，这可能会主导总运行时间）。
- en: Primarily because of the long-running nature of these jobs, identifying and
    optimizing runtime performance can have a dramatic impact on the total cost of
    ownership of an ML solution. To effectively optimize, however, it’s critical to
    analyze the computational complexity (affecting total runtime) and space complexity
    (affecting the size or number of machines required to run the code).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 主要由于这些工作的长时间运行特性，识别和优化运行时性能可以对机器学习解决方案的总拥有成本产生重大影响。然而，为了有效地优化，分析计算复杂度（影响总运行时间）和空间复杂度（影响运行代码所需的机器大小或数量）是至关重要的。
- en: The analysis of runtime issues is, from both a practical and theoretical stance,
    handled through evaluating computational complexity and space complexity, referred
    to in shorthand as *Big O*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从实用和理论的角度来看，运行时问题的分析是通过评估计算复杂度和空间复杂度来处理的，简称为*大O*。
- en: A.1.1 A gentle introduction to complexity
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1.1 复杂度的温和介绍
- en: '*Computational* *complexity* is, at its heart, a worst-case estimation of how
    long it will take for a computer to work through an algorithm. *Space complexity*,
    on the other hand, is the worst-case burden to a system’s memory that an algorithm
    can cause.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算复杂度*本质上是对计算机执行算法所需时间的最坏情况估计。另一方面，*空间复杂度*是指算法可能对系统内存造成的最坏情况负担。'
- en: While computational complexity typically impacts the CPU, space complexity involves
    the memory (RAM) you need to have in the system to process the algorithm without
    incurring disk spills (pagination to a hard drive or solid-state drive). Figure
    A.3 shows how operating on a collection of data points can have different space
    and computational complexity, depending on the algorithm that you’re using.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算复杂度通常影响CPU，但空间复杂度涉及系统（RAM）中您需要处理的内存，以避免磁盘溢出（将分页到硬盘或固态硬盘）。图A.3显示了操作数据点集合时，根据所使用的算法，可以具有不同的空间和计算复杂度。
- en: The different actions being performed on collections of data affect the amount
    of time and space complexity involved. As you move from top to bottom in figure
    A.3, both space and computational complexity increase for different operations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据集合执行的不同操作会影响所需的时间和空间复杂度。如图A.3从上到下移动时，不同操作的空间和计算复杂度都会增加。
- en: '![A-03](../Images/A-03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![A-03](../Images/A-03.png)'
- en: Figure A.3 Comparison of computational and space complexities for operating
    on a collection of data
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.3 对操作数据集合的计算复杂性和空间复杂性的比较
- en: Many other complexities are considered standard for assessing complexities in
    algorithms. Figure A.4 shows these standard assessments on a linear scale, while
    figure A.5 shows them on a logarithmic y-scale to illustrate just how much some
    of them should be avoided.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估算法的复杂度时，考虑了许多其他复杂度作为标准。图A.4显示了这些标准评估在线性尺度上的情况，而图A.5则显示它们在对数y尺度上的情况，以说明其中一些应该避免到何种程度。
- en: '![A-04](../Images/A-04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![A-04](../Images/A-04.png)'
- en: Figure A.4 Linear y-axis scale of different computational complexities filtered
    to 150 iterations
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.4 不同计算复杂度的线性y轴尺度，过滤到150次迭代
- en: As figures A.4 and A.5 show, the relationship between collection size and algorithm
    type can dramatically affect the runtime of your code. Understanding these relationships
    (of both space and computational complexity) within the non-ML aspects of your
    code, outside of model training and inference, is absolutely essential.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如图A.4和A.5所示，集合大小与算法类型之间的关系可以显著影响代码的运行时间。理解这些关系（包括空间和计算复杂度）在代码的非机器学习方面，即在模型训练和推理之外，是绝对必要的。
- en: '![A-05](../Images/A-05.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![A-05](../Images/A-05.png)'
- en: Figure A.5 Logarithmic y-axis scale of computational complexities. Pay close
    attention to the size of the y-axis toward the top of the graph. Exponential and
    factorial complexities can truly bring the pain.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.5 计算复杂度的对数y轴尺度。请注意图表顶部y轴的大小。指数和阶乘复杂度确实会带来痛苦。
- en: Let’s imagine what the costs would be for implementing something as simple as
    a collection traversal in project orchestration code. If we were trying to evaluate
    relationships between two arrays of numbers in a brute-force manner (looping through
    each in a nested fashion), we’d be looking at O(*n*²) complexity. If we were to
    merge the lists instead through an optimized join, we could reduce the complexity
    significantly. Moving from complexities like O(*n*²) to something closer to O(*n*),
    as shown in figures A.4 and A.5, when dealing with large collections, can translate
    to significant cost and time savings.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，在项目编排代码中实现如此简单的东西，如集合遍历的成本。如果我们试图以暴力方式评估两个数字数组之间的关系（以嵌套方式遍历每个数组），我们将面临O(*n*²)的复杂度。如果我们通过优化的连接合并列表，我们可以显著降低复杂度。从O(*n*²)这样的复杂度转移到接近O(*n*)，如图A.4和A.5所示，在处理大型集合时，可以转化为显著的成本和时间节省。
- en: A.2 Complexity by example
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.2 通过示例分析复杂度
- en: Analyzing code for performance issues can be daunting. Many times, we’re so
    focused on getting all of the details surrounding feature engineering, model tuning,
    metric evaluation, and statistical evaluation ironed out that the concept of evaluating
    how we’re iterating over collections doesn’t enter our minds.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 分析代码以查找性能问题可能会令人畏惧。很多时候，我们如此专注于解决围绕特征工程、模型调整、指标评估和统计评估的所有细节，以至于评估我们如何迭代集合的概念并没有进入我们的脑海。
- en: If we were to take a look at the control code that directs the execution of
    those elements of a project, thinking of their execution as a factor of complexity,
    we would be able to estimate the relative runtime effects that will occur. Armed
    with this knowledge, we could decouple inefficient operations (such as overly
    nested looping statements that could be collapsed into a single indexed traversal)
    and help reduce the burden on both the CPU and the memory of the system running
    our code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们审视控制代码，这些代码指导着项目元素的执行，将它们的执行视为复杂性的一个因素，我们就能估计出将发生的相对运行时间效应。有了这些知识，我们可以解耦低效的操作（例如，可以折叠成单个索引遍历的过度嵌套循环语句），并帮助减轻运行我们代码的系统的CPU和内存负担。
- en: Now that you’ve seen the theory of Big O, let’s take a look at some code examples
    using these algorithms. Being able to see how differences in the number of elements
    in a collection can affect timing of operations is important in order to fully
    understand these concepts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了大O理论，让我们看看一些使用这些算法的代码示例。能够看到集合中元素数量的差异如何影响操作的时间，对于完全理解这些概念非常重要。
- en: I’m going to present these topics in a somewhat less-than-traditional manner,
    using dogs as an example, followed by showing code examples of the relationships.
    Why? Because dogs are fun.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我将以一种不太传统的方式介绍这些主题，以狗为例，然后展示代码示例来展示这些关系。为什么？因为狗很有趣。
- en: 'A.2.1 O(1): The “It doesn’t matter how big the data is” algorithm'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.1 O(1)：不关心数据大小的“它的大小无关紧要”算法
- en: Let’s imagine that we’re in a room. A very, very large room. In the center of
    the room is a ring of food bowls. For dogs. And we’ve filled these bowls with
    some pasta Bolognese. It’s been a torturous day of making it (for the dogs, smelling
    the entire time), but we’ve ladled the food into five separate bowls and are ready
    with our notepads to record data about the event. After all is said and done (bowls
    are cleaner than before they were ladled with pasta), we have collections of ordered
    lists representing different actions that our panel of dogs took.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们身处一个房间。一个非常非常大的房间。房间的中心是一个食物碗的环形。为狗准备的。我们已经在这些碗里装了一些博洛尼亚面食。对于狗来说，这是一天痛苦的等待（整个过程中都在闻味），但我们已经把食物分装进了五个单独的碗里，并准备好了我们的笔记本来记录这个事件的数据。一切结束后（碗比倒面食之前干净），我们有了代表我们的狗小组采取的不同行动的有序列表集合。
- en: When we wish to answer questions about the facts that we observed, we’re operating
    on these lists, but retrieving a single indexed value associated with the order
    in which these events occurred. Regardless of the size of these lists, the O(1)-type
    questions are simply acquiring data based on positional reference, and thus, the
    operations all take the same amount of time. Let’s take a look at this scenario
    in figure A.6.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望回答关于我们观察到的事实的问题时，我们正在操作这些列表，但检索与这些事件发生的顺序相关的单个索引值。无论这些列表的大小如何，O(1)类型的问题只是基于位置参考获取数据，因此，所有操作所需的时间相同。让我们看看图A.6中的这个场景。
- en: '![A-06](../Images/A-06.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![A-06](../Images/A-06.png)'
- en: Figure A.6 O(1) search by means of hungry dogs
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.6 通过饥饿的狗实现O(1)搜索
- en: O(1) doesn’t care how big the data is, as figure A.6 shows. These algorithms
    simply operate in ways that don’t traverse collections, but rather access positions
    of data within collections.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: O(1)不关心数据有多大，如图A.6所示。这些算法以不遍历集合，而是访问集合中数据的位置的方式进行操作。
- en: To show this relationship in a computational sense, listing A.3 illustrates
    a comparison of performing an O(1) task on two differently sized collections of
    data—with a similar runtime performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在计算意义上展示这种关系，列表A.3展示了在两个不同大小的数据集上执行O(1)任务的比较——具有相似的运行时性能。
- en: Listing A.3 Demonstration of O(1) complexity
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.3 O(1)复杂性的演示
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Generates an array of integers between -100 and 100
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成介于-100和100之间的整数数组
- en: ❷ Runs through 100,000 iterations of the operation to allow for per-run variance
    to be minimized to see the access speed
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行100,000次操作的迭代，以最小化每次运行的方差，从而查看访问速度
- en: ❸ The absolute value of average speed per iteration is highly dependent on the
    hardware that the code is running on. 269 nanoseconds is pretty fast for using
    a single core from an 8-core laptop CPU, though.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每次迭代的平均速度的绝对值高度依赖于代码运行的硬件。尽管如此，对于使用8核心笔记本电脑CPU的单个核心来说，269纳秒已经非常快了。
- en: ❹ Generates an array that is just slightly larger than the first one
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成一个比第一个稍大的数组
- en: ❺ 261 nanoseconds. Even with 100,000 times more data, the execution time is
    the same.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 261纳秒。即使有100,000倍的数据，执行时间也是相同的。
- en: ❻ Quadratic equation to illustrate mathematical operations on a single value
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 二次方程用于说明对单个值进行的数学运算
- en: ❼ Executes 5.31 microseconds on a single value from the array
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 在数组中的单个值上执行5.31微秒
- en: ❽ Executes 1.55 microseconds on a single value from the array (less time than
    the previous due to indexing operations in NumPy for accessing larger arrays)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在数组中的单个值上执行1.55微秒（由于NumPy访问较大数组中的索引操作，比之前少）
- en: The first array (`sequential_array`) is a scant 200 elements in length, and
    its access time for retrieving an element from its indexed c-based-struct type
    is very fast. As we increase the size of the array (`massive_array`, containing
    2 million elements), the runtime doesn’t change for a positional retrieval. This
    is due to an optimized storage paradigm of the array; we can directly look up
    the memory address location for the element in constant O(1) time through the
    index registry.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数组（`sequential_array`）长度仅为200个元素，从其索引的c-based-struct类型中检索元素的访问时间非常快。随着我们增加数组的大小（包含200万个元素的`massive_array`），位置检索的运行时间不会改变。这是由于数组的优化存储模式；我们可以通过索引注册表直接在常数O(1)时间内查找元素的内存地址位置。
- en: 'The control code of ML projects has many examples of O(1) complexity:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习项目的控制代码有许多O(1)复杂性的例子：
- en: '*Getting the last entry in a sorted, ranked collection of aggregated data points*—For
    example, from a window function with events arranged by time of occurrence. However,
    the process of building the windowed aggregation is typically O(*n* log *n*) because
    of the sort involved.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*获取排序、排名的聚合数据点的最后一个条目*——例如，从按发生时间排序的事件窗口函数中。然而，由于涉及排序，构建窗口聚合的过程通常是O(*n* log
    *n*)。'
- en: '*A modulo function*—This indicates the remainder after dividing one number
    by another and is useful in pattern generation in collection traversals. (The
    traversal will be O(*n*), though.)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*取模函数*——这表示除以另一个数后的余数，在收集遍历中的模式生成中很有用。（遍历将是O(*n*）。）'
- en: '*Equivalency test*—Equal, greater than, less than, and so forth.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*等价测试*——等于、大于、小于等等。'
- en: 'A.2.2 O(n): The linear relationship algorithm'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.2 O(n)：线性关系算法
- en: What if we want to know the status of our canine test subjects at a particular
    point in time? Let’s say that we really want to find out the rate at which they
    are wolfing down their food. Suppose that we decide to collect data at 30 seconds
    into the feast to see the state of the food bowls for each dog.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在特定时间点了解我们的犬类测试对象的状态怎么办？比如说，我们真的想找出他们狼吞虎咽食物的速度。假设我们决定在盛宴开始后的30秒收集数据，以查看每只狗的食物碗状态。
- en: 'The data that we collect for each dog would involve a key-value pairing. In
    Python, we would be collecting a dictionary containing the names of the dogs and
    the amount of food remaining in their bowls:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每只狗收集的数据将涉及键值对。在Python中，我们将收集一个包含狗的名字和它们碗中剩余食物量的字典：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This operation, walking around and estimating the amount of food left in the
    bowls, recording it in this (key, value) pairing, would be O(*n*), as illustrated
    in figure A.7.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作，绕着走并估计碗中的剩余食物量，将其记录在这个（键，值）对中，将是O(*n*)，如图A.7所示。
- en: '![A-07](../Images/A-07.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![A-07](../Images/A-07.png)'
- en: Figure A.7 O(*n*) search through all of the dogs’ consumption rates
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.7 O(*n*)搜索所有狗的消耗率
- en: As you can see, in order to measure the amount remaining, we need to walk around
    to each dog and check the state of their bowl. For the five dogs we’re showing,
    that may take a few seconds. But what if we had 500 dogs? That would take a few
    minutes of walking around to measure. The O(*n*) indicates a linear relationship
    between the algorithm (checking the amount of Bolognese eaten) and the size of
    the data (number of dogs) as a reflection of computational complexity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，为了测量剩余量，我们需要绕到每只狗那里检查它们的碗的状态。对于我们展示的五只狗，这可能需要几秒钟。但如果我们有500只狗呢？那可能需要几分钟的时间来测量。O(*n*)表示算法（检查吃掉的博洛尼亚肉的数量）与数据大小（狗的数量）之间的线性关系，这是计算复杂度的反映。
- en: From a software perspective, the same relationship holds true. Listing A.4 shows
    an iterative usage of the `quadratic()` method defined in listing A.3, operating
    over each element in the two NumPy arrays defined in that listing. As the size
    of the array increases, the runtime increases in a linear manner.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从软件的角度来看，同样的关系也成立。列表A.4显示了列表A.3中定义的`quadratic()`方法的迭代使用，在该列表中操作每个元素。随着数组大小的增加，运行时间以线性方式增加。
- en: Listing A.4 Demonstration of O(*n*) complexity
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.4 O(*n*)复杂度演示
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Mapping over the small (-100, 100) array and applying a function to each value
    takes a bit longer than retrieving a single value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在小数组（-100，100）上映射并应用每个值比检索单个值花费的时间要长。
- en: ❷ Increases the size by a factor of 10 for the array, and the runtime increases
    by a factor of 10
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数组的大小增加一个10倍，运行时间也增加一个10倍
- en: ❸ Increases again by a factor of 10, and the runtime follows suit. This is O(n).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 再次增加一个10倍的因素，运行时间也相应增加。这是O(*n*)。
- en: ❹ Increases by a factor of 10, and the runtime increases by a factor of 30?!
    This is due to the size of the values being calculated and the shift to an alternative
    form of multiplication in Cython (the underlying compiled C* code that optimized
    calculations use).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 增加一个10倍的因素，运行时间增加一个30倍的因素？！这归因于被计算值的尺寸以及Cython（优化计算使用的底层编译C*代码）中乘法替代形式的转变。
- en: As can be seen in the results, the relationship between collection size and
    computational complexity is for the most part *relatively uniform*(see the following
    callout for why it’s not perfectly uniform here).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如结果所示，集合大小与计算复杂度之间的关系在大多数情况下是相对均匀的（见以下说明，了解为什么这里不是完全均匀的）。
- en: When computational complexity breaks patterns at massive scale
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算复杂度在巨大规模上打破模式时
- en: In listing A.4, the final collection doesn’t follow the same pattern as the
    preceding ones. Behavior such as this (a breakdown in assumed expected performance
    when dealing with massive data) is present in any system, particularly in distributed
    systems.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表A.4中，最终的集合不遵循前面集合的相同模式。这种行为（在处理大量数据时，假设的预期性能崩溃）在任何系统中都存在，尤其是在分布式系统中。
- en: When some algorithms start processing data that is of a sufficient size, reallocation
    of memory may be a limiting factor in the performance of those algorithms. Similarly,
    garbage-collection operations in ML-focused languages (Python, or anything running
    in the JVM) can cause substantial disruptions to runtime performance because the
    system has to free space in memory in order to continue the operation that you
    are instructing it to perform.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当一些算法开始处理足够大的数据时，内存重新分配可能成为这些算法性能的限制因素。同样，在以机器学习为重点的语言（Python或任何在JVM上运行的东西）中的垃圾回收操作可能会对运行时性能造成重大干扰，因为系统必须释放内存空间以继续执行您指示它执行的操作。
- en: 'O(*n*) is a fact of life in our world of DS work. However, we should all pause
    and reconsider our implementations if we’re building software that employs our
    next relationship in this list: O(*n*²). This is where things can get a little
    crazy.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: O(*n*)是我们DS工作世界中的一种生活事实。然而，如果我们正在构建使用我们列表中下一个关系的软件：O(*n*²)，我们应该都停下来重新考虑我们的实现。这是事情可能变得有点疯狂的地方。
- en: 'A.2.3 O(n²): A polynomial relationship to the size of the collection'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2.3 O(n²)：与集合大小的多项式关系
- en: Now that our dogs are fed, satiated, and thoroughly pleased with their meal,
    they could use a bit of exercise. We take them all to a dog park and let them
    enter at the same time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们喂饱了狗，它们对食物感到满意，它们需要一点锻炼。我们带它们去狗公园，让它们同时进入。
- en: As in any social hour involving dogs, the first order of business is going to
    be formal introductions by way of behind-sniffing. Figure A.8 shows the combinations
    of greetings among our five dogs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何涉及狗的社会时刻一样，首要任务是正式介绍，通过背后嗅探。图A.8展示了我们五只狗之间的招呼组合。
- en: '![A-08](../Images/A-08.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![A-08](../Images/A-08.png)'
- en: Figure A.8 The dog park meet and greet. While not precisely O(*n*²), it bears
    a similar relationship.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.8狗公园的相遇和问候。虽然不是精确的O(*n*²)，但它有相似的关系。
- en: NOTE Combination calculations are, in the strictest sense, O(*n* choose *k*)
    in complexity. For simplicity’s sake, let’s imagine brute-forcing the solution
    by interacting all possible permutations and then filtering, which would be O(*n*²)
    in complexity.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：组合计算在严格意义上是O(*n* choose *k*)的复杂度。为了简单起见，让我们想象通过交互所有可能的排列并过滤来暴力破解解决方案，这将具有O(*n*²)的复杂度。
- en: This combination-based traversal of paired relationships is not strictly O(*n*²);
    it is actually O(*n* choose *k*). But we can apply the concept and show the number
    of operations as combinatorial operations. Likewise, we can show the relationship
    between runtime duration and collection size by operating on permutations
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于组合的配对关系遍历不是严格的O(*n*²)；实际上它是O(*n* choose *k*)。但我们可以应用这个概念，并通过组合运算来展示操作的数量。同样，我们可以通过操作排列来展示运行时间与集合大小之间的关系
- en: Table A.1 shows the number of total butt-sniff interactions that will happen
    in this dog park based on the number of dogs let in through the gate (combinations),
    as well as the potential greetings. We’re assuming the dogs feel the need for
    a formal introduction, wherein each acts as the initiator (a behavior that I have
    witnessed with my dog on numerous occasions).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.1显示了在这个狗公园中，根据通过大门进入的狗的数量（组合）以及可能的招呼，将发生的总屁股嗅探交互次数。我们假设狗感到需要正式介绍，其中每只狗都作为发起者（这是我多次目睹我的狗的行为）。
- en: Table A.1 Dog greeting by number of dogs
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.1狗的问候次数
- en: '| Number of dogs present | Number of greetings (combinations) | Potential greetings
    (permutations) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 狗的数量 | 招呼的数量（组合） | 可能的招呼（排列） |'
- en: '| 2 | 1 | 2 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 2 |'
- en: '| 5 | 10 | 20 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 10 | 20 |'
- en: '| 10 | 90 | 45 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 90 | 45 |'
- en: '| 100 | 4,950 | 9,900 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 4,950 | 9,900 |'
- en: '| 500 | 124,750 | 249,500 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 500 | 124,750 | 249,500 |'
- en: '| 1,000 | 499,500 | 999,000 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 1,000 | 499,500 | 999,000 |'
- en: '| 2,000 | 1,999,000 | 3,998,000 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 2,000 | 1,999,000 | 3,998,000 |'
- en: To illustrate what this relationship of friendly dog greetings looks like for
    both the combinations and permutations, figure A.9 shows the incredible growth
    in complexity as the number of dogs increases.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明友好狗的招呼关系在组合和排列中的样子，图A.9展示了随着狗数量的增加，复杂性的惊人增长。
- en: '![A-09](../Images/A-09.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![A-09](../Images/A-09.png)'
- en: Figure A.9 The explosion of sniffing as the number of dogs increases in our
    dog park. Exponential relationships in complexity are just as bad in code as they
    are for dogs when talking about efficiency.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.9随着我们狗公园中狗数量的增加，嗅探的爆炸性增长。在代码中，复杂度的指数关系和狗在谈论效率时一样糟糕。
- en: For the vast majority of ML algorithms (the models that are built through a
    training process), this level of computational complexity is just the beginning.
    Most are far more complex than O(*n* ²).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数机器学习算法（通过训练过程构建的模型），这种计算复杂度只是开始。大多数比O(*n*²)复杂得多。
- en: Listing A.5 shows an implementation of *n* ² complexity. For each element of
    the source array, we’re going to be generating an offset curve that rotates the
    element by the iteration index value. The visualizations for each section following
    will show what is going on in the code to make it clearer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.5展示了**n**²复杂度的实现。对于源数组的每个元素，我们将生成一个偏移曲线，该曲线将元素旋转到迭代索引值。接下来的每个部分的可视化将展示代码中的操作，以便更清晰。
- en: Listing A.5 An example of O(*n*²) complexity
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.5 O(*n*²)复杂度的示例
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Function for modifying the quadratic solution to the array by a value from
    the array
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过数组中的一个值修改数组的二次解函数
- en: ❷ Function for generating the collection of quadratic evaluation series values
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成二次评估系列值的集合函数
- en: ❸ Acquires the range around 0 for array generation (size + 1 for symmetry)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取数组生成周围的0范围（为了对称性，大小加1）
- en: ❹ Catches warnings related to dividing by zero (since we’re crossing that boundary
    of integers in the array)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 捕获与除以零相关的警告（因为我们正在跨越数组中整数的边界）
- en: ❺ The n² traversal to generate the array of arrays by mapping over the collection
    twice
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过映射集合两次生成数组数组的 n² 遍历
- en: ❻ Transposes and melts the resultant matrix of data to a normalized form for
    plotting purposes
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将生成的数据矩阵转换为归一化形式，以便进行绘图
- en: ❼ Plots each curve with a different color to illustrate the complexity differences
    in the algorithm
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 用不同的颜色绘制每条曲线，以说明算法中的复杂度差异
- en: For the algorithm defined in listing A.5, if we were to call it with different
    values for an effective collection size, we would get the timed results shown
    in the next listing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于列表 A.5 中定义的算法，如果我们用不同的有效集合大小值调用它，我们将在下一个列表中看到计时结果。
- en: Listing A.6 Results of evaluating an O(*n*²) complex algorithm
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 A.6 评估 O(*n*²) 复杂度算法的结果
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ With only 121 operations, this executes pretty quickly.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅需 121 次操作，这个执行速度相当快。
- en: ❷ At 10 times the array size, 10,201 operations take significantly longer.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在数组大小增加 10 倍时，10,201 次操作需要显著更长的时间。
- en: ❸ At 1,002,001 operations, the exponential relationship becomes clear.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在 1,002,001 次操作时，指数关系变得明显。
- en: The relationship to the input array size from listing A.5 and the results shown
    in listing A.6 can be seen a bit more clearly in figure A.10\. If we were to continue
    increasing the size of the array generation parameter value to 100,000, we would
    be looking at 10,000,200,001 operations (while our first iteration of size 10
    generates 121 operations). More important, though, is the memory pressure of generating
    so many arrays of data. The size complexity will rapidly become the limiting factor
    here, resulting in an out-of-memory (OOM) exception long before we get annoyed
    at how long it’s taking to calculate.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表 A.5 中的输入数组大小和列表 A.6 中显示的结果可以看出，图 A.10 中可以更清楚地看到它们之间的关系。如果我们继续增加数组生成参数值的规模到
    100,000，我们将看到 10,000,200,001 次操作（而我们的第一次迭代大小为 10 生成 121 次操作）。然而，更重要的是，生成如此多的数据数组所带来的内存压力。大小复杂度将迅速成为这里的限制因素，导致在我们对计算所需时间感到厌烦之前，就可能出现内存不足（OOM）异常。
- en: '![A-10](../Images/A-10.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![A-10](../Images/A-10.png)'
- en: Figure A.10 Computational complexity of different collection sizes to the algorithm
    in listing A.5
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.10 列表 A.5 中算法对不同集合大小的计算复杂度
- en: To illustrate what this code is doing, we can see the result of the first iteration
    (using `10` as the function argument) in figure A.11.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明此代码正在做什么，我们可以查看第一次迭代的结果（使用 `10` 作为函数参数），如图 A.11 所示。
- en: '![A-11](../Images/A-11.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![A-11](../Images/A-11.png)'
- en: 'Figure A.11 The data generated from our O(*n*²) algorithm operating on an array
    of size 11 (execution time: 433 ms, ~26 KB space required)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.11 在大小为 11 的数组上运行的 O(*n*²) 算法生成数据（执行时间：433 毫秒，约需 26 KB 空间）
- en: Figure A.12 shows the progression in complexity from an array size of 201 (top)
    to a much more extreme size (2,001, at bottom) when run through this algorithm.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.12 显示了通过此算法运行时，从数组大小 201（顶部）到更极端的大小（底部 2,001）的复杂度进展。
- en: '![A-12](../Images/A-12.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![A-12](../Images/A-12.png)'
- en: 'Figure A.12 Comparison of array sizes 201 (time: 8.58 s, space: ~5.82 MB) and
    2,001 (time: 1,367 s, space: ~576.57 MB).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.12 比较数组大小 201（时间：8.58 秒，空间：约 5.82 MB）和 2,001（时间：1,367 秒，空间：约 576.57 MB）。
- en: As you can see (keep in mind that these plots are generating a series that is
    plotted for each index position of the input array), a seemingly innocuous collection
    size can become very large, very quickly, when run through such an algorithm.
    It’s not too much of a stretch to imagine how much this will affect the runtime
    performance of a project if code is written with this level of complexity.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见（请记住，这些图表是为输入数组的每个索引位置生成的一系列），当通过这样的算法运行时，看似无害的集合大小可以迅速变得非常大。如果代码以这种复杂度级别编写，想象这将对项目的运行时性能产生多大的影响并不困难。
- en: Complexity smells in code
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的复杂度问题
- en: In the grand scheme of code smells, computational complexity is generally one
    of the easier ones to spot. This complexity typically manifests itself in nested
    loops. Whether it be a declarative-style `for` loop that has additional `for`
    loops within it, a `while` loop with nested iteration or mapping within it, or
    a nested list comprehension, these structures in code stand out as potentially
    dangerous.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码恶臭的大背景下，计算复杂性通常是更容易识别的一个。这种复杂性通常表现为嵌套循环。无论是具有内部额外`for`循环的声明式`for`循环，还是包含嵌套迭代或映射的`while`循环，或者是嵌套列表推导式，这些代码结构都显得可能存在危险。
- en: This isn’t to say that the logic within nested loops and complex `while` statements
    is guaranteed to be worst-case scenarios of O(*n*²), O(2*n*), or O(*n*!), but
    these are places to spend more time when evaluating code. They’re the smoke, so
    to speak, that needs to be investigated to ensure that a potential fire is not
    about to erupt when you run the code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说嵌套循环和复杂的`while`语句中的逻辑一定是O(*n*²)、O(2*n*)或O(*n*!)的最坏情况，但在评估代码时，这些地方需要花费更多的时间。它们就像是烟雾，需要调查以确保在运行代码时不会突然爆发潜在的火灾。
- en: Seeing these in a code base simply means that you should spend extra time looking
    through the logic and running through scenarios. The best way to do that is to
    imagine what would happen if the collection being iterated over doubles in size.
    What if it increases by an order of magnitude?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码库中看到这些意味着你应该花额外的时间检查逻辑并运行各种场景。最好的方法是想象如果正在迭代的集合大小加倍会发生什么。如果它增加一个数量级会怎样？
- en: Will the code scale? Will it take so long to run that an SLA is missed? Will
    the system that it’s running on OOM? Thinking of how to identify, refactor, and
    change the logic of your code can help to prevent stability issues and cost considerations
    later.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 代码是否会扩展？运行时间是否会过长以至于错过SLA？运行的系统是否会OOM？思考如何识别、重构和更改代码的逻辑可以帮助预防后续的稳定性问题和成本考虑。
- en: A.3 Analyzing decision-tree complexity
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.3 分析决策树复杂性
- en: Let’s imagine that we’re in the process of building a solution to a problem
    that, as a core requirement, needs a highly interpretable model structure as its
    output. Because of this requirement, we choose to use a decision-tree regressor
    to build a predictive solution.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们正在构建一个解决方案来解决问题，其核心需求需要一个高度可解释的模型结构作为输出。由于这个要求，我们选择使用决策树回归器来构建预测解决方案。
- en: Since we’re a company that is dedicated to our customers (dogs) and their pets
    (humans), we need a way to translate our model results into direct and actionable
    results that can be quickly understood and applied. We’re not looking for a black-box
    prediction; rather, we’re looking to understand the nature of the correlations
    in our data and see how the predictions are influenced by the complex system of
    our features.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们是一家致力于客户（狗）及其宠物（人）的公司，我们需要一种方法将我们的模型结果转化为直接且可操作的结果，这些结果可以快速理解和应用。我们不是在寻找黑盒预测；相反，我们希望理解数据中相关性的本质，并了解预测如何受到特征复杂系统的影响。
- en: After feature engineering is complete and the prototype has been built, we’re
    in the process of exploring our hyperparameter space in order to hone the automated
    tuning space for the MVP. After starting the run (with tens of thousands of tuning
    experiments), we notice that the training of different hyperparameters results
    in different completion times. In fact, depending on the tested hyperparameters,
    each test’s runtime can be off by more than an order of magnitude from another.
    Why?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征工程完成并构建了原型之后，我们正在探索超参数空间，以优化MVP的自动化调整空间。在启动运行（成千上万的调整实验）后，我们注意到不同超参数的训练结果导致不同的完成时间。事实上，根据测试的超参数，每个测试的运行时间可能比另一个高一个数量级。为什么？
- en: To explore this concept, let’s step through how a decision-tree regressor works
    (in the sense of complexity) and evaluate how changing a few hyperparameter settings
    can affect the runtime. Figure A.13 shows a rather high-level view of what is
    happening in the algorithm when it is fit upon training data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这个概念，让我们逐步了解决策树回归器（在复杂性的意义上）是如何工作的，并评估更改一些超参数设置如何影响运行时间。图A.13展示了算法在训练数据上拟合时发生的情况的高级视图。
- en: '![A-13](../Images/A-13.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![A-13](../Images/A-13.png)'
- en: Figure A.13 A high-level explanation of a decision tree algorithm
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.13 决策树算法的高级解释
- en: This diagram is likely familiar to you. The basic structure, functionality,
    and behavior of the algorithm is covered in great detail in blog posts, other
    books, and as a foundational concept in learning the basics of ML. What is of
    interest to our discussion is what affects the computational and space complexity
    while training the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表可能对你来说很熟悉。算法的基本结构、功能和行为在博客文章、其他书籍中都有详细的介绍，并且是学习机器学习基础的一个基本概念。对我们讨论感兴趣的是，在训练模型时，哪些因素会影响计算和空间复杂度。
- en: NOTE Figure A.13 is an example only. This model is incredibly overfit and would
    likely do very poorly against a validation split. With a more realistic data-split
    volume and a depth restriction, the predictions would be the average of split-branch
    membership.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：图A.13仅作为示例。这个模型过度拟合，可能在对验证分割的测试中表现非常糟糕。在有更现实的数据分割体积和深度限制的情况下，预测将是分割分支成员的平均值。
- en: To start, we can see that the initial split at the root of the tree requires
    a determination of which feature to choose to split on first. A scalability factor
    exists right out of the gate with this algorithm. To determine where to split,
    we need to measure each feature, split it based on the criterion that the library’s
    implementation chooses, and calculate the information gain between those splits.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到，在树的根节点上的初始分割需要确定首先选择哪个特征进行分割。这个算法一开始就存在一个可扩展性因素。为了确定分割的位置，我们需要测量每个特征，根据库的实现选择的准则进行分割，并计算这些分割之间的信息增益。
- en: For the purposes of computational complexity, we will refer to the number of
    features as *k*. Another component to the calculation of information gain involves
    the estimation of entropy, based on the size of the dataset being trained upon.
    This is the *n* in traditional non-ML complexity. To add to this complexity, we
    have to traverse each level of the tree. Once a split is realized as the best
    path, we have to continue to iterate on the features present in the subset of
    the data, repeatedly, until we hit the criteria set in the hyperparameter that
    equates to the minimum number of elements to populate a leaf (prediction) node.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算复杂度的目的，我们将特征的数量称为 *k*。信息增益的计算还包括基于训练数据集大小的熵的估计。这是传统非机器学习复杂度中的 *n*。为了增加这种复杂性，我们必须遍历树的每一层。一旦实现了最佳分割路径，我们必须继续在数据子集中的现有特征上迭代，反复进行，直到达到超参数中设定的标准，即填充叶（预测）节点所需的最小元素数量。
- en: Iterating through these nodes represents a computational complexity of O(*n*log(*n*)),
    as the splits are restricted in size as we move closer to leaf nodes. However,
    because we are forced to iterate through all features at each of the adjudication
    nodes, our final computational complexity becomes more akin to O(*k* × *n*× log(*n*)).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历这些节点代表了一个计算复杂度为 O(*n*log(*n*))，因为当我们接近叶节点时，分割的大小受到限制。然而，由于我们必须在每个裁决节点上迭代所有特征，我们的最终计算复杂度更接近于
    O(*k* × *n*× log(*n*))。
- en: We can directly affect the real-world behavior of this worst-case runtime performance
    by adjusting hyperparameters (remember that O() notation is *the worst-case*).
    Of particular note is that some of the hyperparameters can be beneficial to computational
    complexity and model efficacy (minimum count to create a leaf, maximum depth of
    tree), while others are negatively correlated (learning rate in other algorithms
    that utilize stochastic gradient descent, or SGD, for instance).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调整超参数直接影响这种最坏情况运行时性能的真实世界行为（记住，O()表示法是*最坏情况*）。特别值得注意的是，一些超参数可能对计算复杂度和模型有效性有益（例如创建叶节点的最小计数，树的深度限制），而其他则可能呈负相关（例如，在其他使用随机梯度下降（SGD）的算法中的学习率）。
- en: To illustrate the relationship between a hyperparameter and the runtime performance
    of a model, let’s look at modifying the maximum depth of a tree in listing A.7\.
    For this example, we’re going to use a freely available open source dataset to
    illustrate the effect of hyperparameter values that directly influence computational
    and space complexity of a model. (My apologies for not collecting a dataset regarding
    dog characteristics and general hunger levels. If anyone wants to create that
    dataset and release it for common use, please let me know.)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明超参数与模型运行时性能之间的关系，让我们看看修改列表A.7中树的最大深度。在这个例子中，我们将使用一个免费可用的开源数据集来展示直接影响模型计算和空间复杂性的超参数值的影响。（对于没有收集关于狗的特征和一般饥饿水平的数据集，我表示歉意。如果有人想创建这个数据集并公开发布，请告诉我。）
- en: NOTE In listing A.7, in order to demonstrate an excessive depth, I break the
    rules of tree-based models by one-hot-encoding categorical values. Encoding categorical
    values in this manner risks a very high chance of preferentially splitting only
    on the Boolean fields, making a dramatically underfit model if the depth is not
    sufficient to utilize other fields. Validation of the feature set should always
    be conducted thoroughly when encoding values to determine if they will create
    a poor model (or a difficult-to-explain model). Look to bucketing, k-leveling,
    binary encoding, or enforced-order indexing to solve your ordinal or nominal categorical
    woes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在列表A.7中，为了展示过深的深度，我通过独热编码分类值来违反基于树的模型规则。以这种方式编码分类值存在高度风险，可能会优先在布尔字段上分割，如果深度不足以利用其他字段，将导致模型显著欠拟合。在编码值时，应始终彻底验证特征集，以确定它们是否会创建一个糟糕的模型（或难以解释的模型）。考虑使用分桶、k级划分、二进制编码或强制顺序索引来解决你的有序或名义分类问题。
- en: Listing A.7 Demonstrating the effects of tree depth on runtime performance
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表A.7 展示树深度对运行时性能的影响
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Pulls in an open source dataset to test against
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 引入一个开源数据集进行测试
- en: ❷ One-hot-encodes the month and day columns to ensure that we have sufficient
    features to achieve the necessary depth for this exercise. (See note preceding
    this listing.)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将月份和日期列进行独热编码，以确保我们有足够多的特征来实现本练习所需的深度。（参见本列表之前的说明。）
- en: ❸ Gets the train and test split data
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取训练集和测试集数据
- en: ❹ A shallow depth of 3 (potentially underfit) reduces the runtime to a minimum
    baseline.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 浅深度3（可能欠拟合）将运行时间降低到最小基线。
- en: ❺ Moving from a depth of 3 to 5 increases runtime by 17% (some branches will
    have terminated, limiting the additional time).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从深度3增加到5，运行时间增加了17%（一些分支将已终止，限制了额外的时间）。
- en: ❻ Moving to a depth of 30 (actual realized depth of 21 based on this dataset)
    and reducing the minimum leaf size to 1 captures the worst possible runtime complexity.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将深度移动到30（根据此数据集实际实现的深度为21）并将最小叶子大小减少到1，可以捕捉到最糟糕的运行时复杂度。
- en: As you can see in the timed results of manipulating the hyperparameters, a seemingly
    insignificant relationship exists between the depth of the tree and the runtime.
    When we think about this as a percentage change, though, we can start to understand
    how this could be problematic.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在调整超参数的计时结果中所看到的，树深度和运行时间之间存在一种看似微不足道的关系。然而，当我们将其视为百分比变化时，我们就可以开始理解这可能会造成的问题。
- en: To illustrate the complexity of this tree-based approach, figure A.14 shows
    the steps that are being taken at each candidate split as the tree is being produced.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种基于树的方法的复杂性，图A.14显示了在生成树的过程中，在每个候选分割步骤所采取的步骤。
- en: '![A-14](../Images/A-14.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![A-14](../Images/A-14.png)'
- en: Figure A.14 The computational complexity of a decision tree
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.14 决策树的计算复杂性
- en: Not only do multiple tasks need to be accomplished in order to decide where
    to split and what to split on, but this entire block of tasks shown on the right
    side of figure A.14 needs to be completed for each feature on the subset of data
    that fulfills the prior split conditions at each candidate node. With a tree depth
    of 30, 40, or 50, we can imagine that this tree becomes quite large rather fast.
    The runtime will increase comparatively as well.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅需要完成多个任务来决定在哪里分割以及分割什么，而且还需要完成图A.14右侧显示的整个任务块，以针对每个在候选节点满足先前分割条件的子数据集的特征进行操作。当树深度为30、40或50时，我们可以想象这棵树会很快变得相当大。运行时间也会相应增加。
- en: What happens when the dataset is not, as in this toy example, 517 rows? What
    happens when we’re training on 500 million rows of data? Setting aside the model
    performance implications (generalization capabilities) of running to too deep
    of a tree, when we think about a 68% increase in runtime from a single hyperparameter,
    the differences in training time can be extremely significant (and costly) if
    we’re not careful about how we’re controlling the hyperparameters of the model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集不是像这个玩具示例中的517行时会发生什么？当我们训练5000万行数据时会发生什么？抛开模型性能影响（泛化能力）的模型运行到过深树的含义，当我们考虑单个超参数导致的68%的运行时间增加时，如果我们不仔细控制模型的超参数，训练时间的差异可能会非常显著（并且代价高昂）。
- en: Now that you’ve seen how computationally expensive hyperparameter tuning is,
    in the next section we’ll look at the computational and space complexities of
    different model families.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了超参数调整的计算成本，在下一节中，我们将探讨不同模型家族的计算和空间复杂度。
- en: A.4 General algorithmic complexity for ML
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A.4 机器学习的一般算法复杂度
- en: While we won’t be talking about the implementation details of any other ML algorithm
    (as I’ve mentioned before, there are books devoted to that subject), we can look
    at one further example. Let’s suppose we are dealing with an incredibly large
    dataset. It has 10 million rows of training data, 1 million rows of test data,
    and a feature set of 15 elements.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会讨论任何其他机器学习算法的实现细节（正如我之前提到的，有专门的书籍介绍这个主题），但我们可以看看另一个更进一步的例子。假设我们正在处理一个非常大的数据集。它包含1000万行训练数据，100万行测试数据，以及15个特征元素。
- en: With a dataset this big, we’re obviously going to be using distributed ML with
    the SparkML packages. After doing some initial testing on the 15 features within
    the vector, a decision is made to start improving the performance to try to get
    better error-metric performance. Since we’re using a generalized linear model
    for the project, we’re handling collinearity checks on all features and are scaling
    the features appropriately.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如此大的数据集，我们显然会使用分布式机器学习与SparkML包。在测试向量中的15个特征之后，我们决定开始提高性能，以尝试获得更好的错误度量性能。由于我们在这个项目中使用的是广义线性模型，我们对所有特征进行共线性检查，并适当地缩放特征。
- en: For this work, we split the team into two groups. Group 1 works on adding a
    single validated feature at a time, checking the improvement or degradation of
    the prediction performance against the test set at each iteration. While this
    is slow going, group 1 is able to cull or add potential candidates one at a time
    and have a relatively predictable runtime performance from run to run.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项工作，我们将团队分成两组。第1组一次添加一个经过验证的特征，并在每次迭代中检查预测性能的改进或下降。虽然这个过程很慢，但第1组能够一次淘汰或添加一个潜在候选者，并且从一次运行到下一次运行的运行时间相对可预测。
- en: The members of group 2, on the other hand, add 100 potential features that they
    think will make the model better. They execute the training run and wait. They
    go to lunch, have a delightful conversation, and return back to the office. Six
    hours later, the Spark cluster is still running with all executors pegged at >90%
    CPU. It continues to run overnight as well.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，第2组的成员添加了他们认为会使模型更好的100个潜在特征。他们执行训练运行并等待。他们去吃午饭，进行愉快的交谈，然后回到办公室。六小时后，Spark集群仍在运行，所有执行器都达到了>90%的CPU使用率。它还继续运行了一整夜。
- en: The main problem here is the *increase in computational complexity*. While the
    *n* of the model hasn’t changed at all (training data is still the exact same
    size), the reason for the longer runtime is simply the increased feature size.
    For large datasets, this becomes a bit of a problem because of the way that the
    optimizer functions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要问题是**计算复杂度的增加**。尽管模型的**n**值完全没有改变（训练数据的大小仍然是完全相同的），运行时间变长的原因仅仅是特征大小的增加。对于大数据集来说，这会由于优化器的工作方式而成为一个问题。
- en: While traditional linear solvers (ordinary least squares, for instance) can
    rely on solving a best fit through a closed-form solution involving matrix inversion,
    on large datasets that need to be distributed, this isn’t an option. Other solvers
    have to be employed for optimizing in a distributed system. Since we’re using
    a distributed system, we’re looking at SGD. Being an iterative process, SGD will
    perform optimization by taking steps along a local gradient of the tuning history.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统的线性求解器（例如普通最小二乘法）可以依赖于通过涉及矩阵逆的闭式解来求解最佳拟合，但在需要分布的大型数据集中，这不是一个选项。其他求解器必须用于在分布式系统中进行优化。由于我们正在使用分布式系统，我们正在考虑SGD。作为一个迭代过程，SGD将通过沿着调整历史的局部梯度进行步骤来执行优化。
- en: To get a simplified sense of how SGD works, see figure A.15\. This 3D plot represents
    a walk of the solver along a series of gradients in its attempt to find the global
    minima error for a particular set of coefficients for the linear equation being
    generated.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得SGD工作原理的简化理解，请参阅图A.15。这个3D图表示求解器沿着一系列梯度进行搜索，以找到生成线性方程特定系数集的全局最小误差。
- en: '![A-15](../Images/A-15.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![A-15](../Images/A-15.png)'
- en: Figure A.15 A visual representation (with artistic liberties) of the SGD process
    of searching for minimization during optimization
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.15 优化过程中搜索最小化的SGD过程的视觉表示（带有艺术自由度）
- en: Note Stochastic gradient descent will proceed along fixed-distance adjustments
    to attempt to arrive at the best fit to the test data (error minimization). It
    will stop when either the descent flattens to a slope of 0 and subsequent iterations
    within a threshold show no improvement or maximum iterations is reached.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意随机梯度下降将沿着固定距离的调整进行，试图达到测试数据（误差最小化）的最佳拟合。它将在下降变得平坦到斜率为0，并且在阈值内后续迭代没有改进或达到最大迭代次数时停止。
- en: Notice the iterative search that is occurring. This series of attempts to get
    the best fit of the equation to the target variable involves making adjustments
    to each of the coefficients for each element of the feature vector. Naturally,
    as the size of the vector increases, the number of coefficient evaluations grows.
    This process needs to occur for each iterative walk that is occurring.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 注意正在发生的迭代搜索。这一系列尝试将方程的最佳拟合与目标变量相匹配，包括调整特征向量每个元素的所有系数。自然地，随着向量大小的增加，系数评估的数量也会增加。这个过程需要为每个正在进行的迭代遍历发生。
- en: However, a bit of a wrench is being thrown into this situation. SGD and iterative
    methodologies of its ilk (such as genetic algorithms) don’t have a simple solution
    for determining computational complexity.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个情况中出现了一些问题。SGD及其类似迭代方法（如遗传算法）没有简单的解决方案来确定计算复杂性。
- en: The reason for this (which is also true for other comparable iterating solvers,
    like limited memory Broyden-Fletcher-Goldfarb-Shanno, or L-BFGS) is that the nature
    of the optimization minima in both a local and global sense is highly dependent
    on the composition of the feature data (distributions and inferred structural
    types), the nature of the target, and the complexity of the feature space (feature
    count).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这（对于其他类似的迭代求解器，如有限内存Broyden-Fletcher-Goldfarb-Shanno，或L-BFGS也是正确的）的原因是，在局部和全局意义上，优化最小值的性质高度依赖于特征数据的组成（分布和推断的结构类型）、目标性质以及特征空间的复杂性。
- en: These algorithms all have settings for maximum number of iterations to achieve
    a best-effort optimization to a global minima state, but there is no guarantee
    that optimization will happen before hitting that iterator maximum count. Conversely,
    one of the challenges that can arise when determining how long training is going
    to take revolves around the complexity of optimization. If SGD (or other iterative
    optimizers) can arrive at a (hopefully, global) minima in a relatively short number
    of iterations, training will terminate long before the maximum iteration count
    is reached.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法都有最大迭代次数的设置，以实现最佳努力的全局最小化状态优化，但无法保证优化会在达到迭代器最大计数之前发生。相反，确定训练将花费多长时间时可能出现的挑战之一是优化的复杂性。如果SGD（或其他迭代优化器）可以在相对较少的迭代次数内达到（希望是全局的）最小值，则训练将在达到最大迭代计数之前很久就终止。
- en: Because of these considerations, table A.2 is a rough estimate of theoretical
    worst-case computational complexity in common traditional ML algorithms.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些考虑，表A.2是对常见传统机器学习算法理论最坏情况计算复杂度的一个粗略估计。
- en: Table A.2 Estimation of computational complexity for different model families
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.2 不同模型家族的计算复杂度估计
- en: '| Model family | Training complexity | Prediction complexity |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 模型家族 | 训练复杂度 | 预测复杂度 |'
- en: '| Decision trees | O(*kn*log(*n*)) | O(*k*) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | O(*kn*log(*n*)) | O(*k*) |'
- en: '| Random forest | O(*kn*log(*n*)*m* | O(*km*) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | O(*kn*log(*n*)*m*) | O(*km*) |'
- en: '| Gradient boosted trees | O(*knm*) | O(*km*) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 梯度提升树 | O(*knm*) | O(*km*) |'
- en: '| Linear models (OLS) | O(*k2n*) | O(*k*) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 线性模型（OLS） | O(*k2n*) | O(*k*) |'
- en: '| Linear models (non-OLS) | O(*k*²*n*+ *k*³) | O(*k*) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 线性模型（非OLS） | O(*k*²*n*+ *k*³) | O(*k*) |'
- en: '| Support vector machines | O(*kn*²+ *n*³) | O(*km*) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机 | O(*kn*²+ *n*³) | O(*km*) |'
- en: '| K-nearest neighbors | O(*kmn*)* | O(*kn*) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| K-最近邻算法 | O(*kmn*)* | O(*kn*) |'
- en: '| K-means | O(*mni*)** | O(*m*) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| K-均值算法 | O(*mni*)** | O(*m*) |'
- en: '| Alternating least squares | O(*mni*)** | O(*ni*) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 交替最小二乘法 | O(*mni*)** | O(*ni*) |'
- en: '| *n* = Number of rows in training set*k* = Number of features in vector*m*
    = Number of ensemble members*i* = Number of iterations to converge* m in this
    case is the restriction of number of neighbors to consider for definition of a
    boundary.** m here refers to the number of k-centroids being considered. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| *n* = 训练集行数*k* = 向量中的特征数*m* = 集成成员数*i* = 收敛所需的迭代次数* m在此情况下是指考虑边界定义时邻居数量的限制。**
    m在此处指的是考虑的k-质心数量。 |'
- en: 'The most common aspect of all of these complexities involves separate factors:
    the number of vectors used for training (row count in a DataFrame) and the number
    of features in the vector. An increase in the count of either of these has a direct
    effect on the runtime performance. Many ML algorithms have an exponential relationship
    between computational time and the size of the input feature vector. Setting aside
    the intricacies of different optimization methodologies, the solver of a given
    algorithm can have its performance impacted adversely as a feature set size grows.
    While each algorithm family has its own nuanced relationships to both feature
    size and training sample sizes, understanding the impact that feature counts have
    in general is an important concept to remember while in the early stages of a
    project''s development.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些复杂性的最常见方面都涉及单独的因素：用于训练的向量数量（DataFrame中的行数）和向量中的特征数。这两种数量的增加都会直接影响到运行时的性能。许多机器学习算法在计算时间和输入特征向量大小之间有指数关系。在忽略不同优化方法复杂性的情况下，给定算法的求解器可能会因为特征集大小的增加而受到不利影响。虽然每个算法家族都与特征大小和训练样本大小有自己独特的细微关系，但在项目开发初期，了解特征计数的一般影响是一个重要的概念。
- en: As we saw in the previous section, the depth of a decision tree can influence
    the runtime performance, since it is searching through more splits, and thus taking
    more time to do so. Nearly all models have parameters that give flexibility to
    the application’s practitioner that will directly influence the predictive power
    of a model (typically at the expense of runtime and memory pressure).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '如前节所述，决策树的深度可以影响运行时的性能，因为它需要搜索更多的分割，因此需要更多的时间。几乎所有模型都有参数，这些参数可以给应用实践者提供灵活性，这将直接影响到模型的预测能力（通常是以运行时间和内存压力为代价）。 '
- en: In general, it’s a good idea to become familiar with the computational and space
    complexities of ML models. Knowing the impact to the business of selecting one
    type of model over another (provided that they’re capable of solving the problem
    in a similar manner) can make a difference of orders of magnitude in cost after
    everything is in production. I’ve personally made the decision many times to use
    something that was slightly worse in predictive capabilities because it could
    run in a fraction of the time of an alternative that was many times more expensive
    to execute.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，熟悉机器学习模型的计算和空间复杂度是个好主意。了解选择一种模型而不是另一种模型对业务的影响（假设它们能够以类似的方式解决问题）可以在生产完成后在成本上产生数量级的差异。我个人已经多次决定使用预测能力略差的方法，因为它可以以比替代方案执行成本低得多的时间运行。
- en: Remember, at the end of the day, we’re here to solve problems for the business.
    Getting a 1% increase in prediction accuracy at the expense of 50 times the cost
    creates a new type of problem for the business while solving what you set out
    to do.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，最终，我们在这里是为了解决业务问题。以50倍的成本为代价，在预测精度上提高1%，在解决你最初想要解决的问题的同时，为业务创造了一个新的问题。
