- en: Chapter 20\. Numerical Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 20 章\. 数值优化
- en: 'At this point in the book, our modeling procedure should feel familiar: we
    define a model, choose a loss function, and fit the model by minimizing the average
    loss over our training data. We’ve seen several techniques to minimize loss. For
    example, we used both calculus and a geometric argument in [Chapter 15](ch15.html#ch-linear)
    to find a simple expression for fitting linear models using squared loss.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，我们的建模过程应该感到很熟悉：我们定义一个模型，选择一个损失函数，并通过最小化训练数据上的平均损失来拟合模型。我们已经看到了几种最小化损失的技术。例如，我们在[第
    15 章](ch15.html#ch-linear)中使用了微积分和几何论证，找到了使用平方损失拟合线性模型的简单表达式。
- en: But empirical loss minimization isn’t always so straightforward. Lasso regression,
    with the addition of the <math><msub><mi>L</mi> <mn>1</mn></msub></math> penalty
    to the average squared loss, no longer has a closed-form solution, and logistic
    regression uses cross-entropy loss to fit a nonlinear model. In these cases, we
    use *numerical optimization* to fit the model, where we systematically choose
    parameter values to evaluate the average loss in search of the minimizing value.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但经验损失最小化并不总是那么简单。**Lasso 回归**在平均平方损失中加入<math><msub><mi>L</mi> <mn>1</mn></msub></math>惩罚后，不再有闭合形式的解，而逻辑回归使用交叉熵损失来拟合非线性模型。在这些情况下，我们使用*数值优化*来拟合模型，系统地选择参数值以评估平均损失，以寻找最小化的值。
- en: 'When we introduced loss functions in [Chapter 4](ch04.html#ch-modeling), we
    performed a simple numerical optimization to find the minimizer of the average
    loss. We created a grid of <math><mi>θ</mi></math> values and evaluated the average
    loss at all points in the grid (see [Figure 20-1](#grid-diagram)). The grid point
    with the smallest average loss we took as the best fit. Unfortunately, this sort
    of grid search quickly becomes impractical, for the following reasons:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第 4 章](ch04.html#ch-modeling)介绍损失函数时，我们执行了一个简单的数值优化，以找到平均损失的最小化者。我们创建了一个<math><mi>θ</mi></math>值的网格，并在网格中的所有点上评估平均损失（见[图
    20-1](#grid-diagram)）。具有最小平均损失的网格点我们认为是最佳拟合。不幸的是，这种网格搜索很快变得不切实际，原因如下：
- en: For complex models with many features, the grid becomes unwieldy. With only
    four features and a grid of 100 values for each feature, we must evaluate the
    average loss at <math><msup><mn>100</mn> <mn>4</mn></msup> <mo>=</mo> <mn>100,000,000</mn></math>
    grid points.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有许多特征的复杂模型，网格变得难以管理。对于仅具有四个特征和每个特征 100 个值的网格，我们必须评估<math><msup><mn>100</mn>
    <mn>4</mn></msup> <mo>=</mo> <mn>100,000,000</mn></math>个网格点上的平均损失。
- en: The range of parameter values to search over must be specified in advance to
    create the grid, and when we don’t have a good sense of the range, we need to
    start with a wide grid and possibly repeat the grid search over narrower ranges.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须事先指定要搜索的参数值范围，以创建网格。当我们对范围没有很好的感觉时，我们需要从一个宽网格开始，可能需要在更窄的范围内重复网格搜索。
- en: With a large number of observations, the evaluation of the average loss over
    the grid points can be slow.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大量观测值，评估网格点上的平均损失可能会很慢。
- en: '![](assets/leds_2001.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_2001.png)'
- en: Figure 20-1\. Searching over a grid of points can be computationally slow or
    inexact
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-1\. 在网格点上搜索可能计算速度慢或不准确
- en: In this chapter, we introduce numerical optimization techniques that take advantage
    of the shape and smoothness of the loss function in the search for the minimizing
    parameter values. We first introduce the basic idea behind the technique of gradient
    descent, then we give an example and describe the properties of the loss function
    that make gradient descent work, and finally, we provide a few extensions of gradient
    descent.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍利用损失函数的形状和平滑性寻找最小化参数值的数值优化技术。我们首先介绍梯度下降技术的基本思想，然后给出一个示例，描述使梯度下降起作用的损失函数的特性，最后提供了梯度下降的几个扩展。
- en: Gradient Descent Basics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降基础知识
- en: Gradient descent is based on the notion that for many loss functions, the function
    is roughly linear in small neighborhoods of the parameter. [Figure 20-2](#gd-diagram)
    gives a diagram of the basic idea.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降基于以下观念：对于许多损失函数，函数在参数的小邻域内大致是线性的。[图 20-2](#gd-diagram) 给出了这个基本思想的示意图。
- en: '![](assets/leds_2002.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_2002.png)'
- en: Figure 20-2\. The technique of gradient descent moves in small increments toward
    the minimizing parameter value
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-2\. 梯度下降技术是向最小化参数值的方向进行小增量移动的技术。
- en: 'In the diagram, we have drawn the tangent line to the loss curve <math><mi>L</mi></math>
    at some point <math><mrow><mi>θ</mi></mrow></math> to the left of the minimizing
    value, <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    . Notice that the slope of the tangent line is negative. A short step to the right
    of <math><mrow><mi>θ</mi></mrow></math> to <math><mrow><mi>θ</mi></mrow> <mo>+</mo>
    <mtext>s</mtext></math> , for some small amount <math><mtext>s</mtext></math>
    , gives a point on the tangent line close to the loss at <math><mrow><mi>θ</mi></mrow>
    <mo>+</mo> <mtext>s</mtext></math> , and this loss is smaller than <math><mi>L</mi>
    <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">~</mo></mover></mrow>
    <mo stretchy="false">)</mo></math> . That is, since the slope, <math><mi>b</mi></math>
    , is negative, and the tangent line approximates the loss function in a neighborhood
    of <math><mrow><mi>θ</mi></mrow></math> , we have:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们画出了损失曲线<math><mi>L</mi></math>在最小化值<math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    左侧某点的切线。注意到切线的斜率为负。在<math><mrow><mi>θ</mi></mrow></math> 右侧前进一个短距离<math><mrow><mi>θ</mi></mrow>
    <mo>+</mo> <mtext>s</mtext></math> （其中<math><mtext>s</mtext></math> 是一小量）给出切线上接近<math><mrow><mi>θ</mi></mrow>
    <mo>+</mo> <mtext>s</mtext></math> 处的损失，且该损失小于<math><mi>L</mi> <mo stretchy="false">(</mo>
    <mrow><mover><mi>θ</mi> <mo stretchy="false">~</mo></mover></mrow> <mo stretchy="false">)</mo></math>
    。也就是说，由于斜率<math><mi>b</mi></math> 为负，并且切线在<math><mrow><mi>θ</mi></mrow></math>
    附近近似损失函数，我们有：
- en: <math display="block"><mi>L</mi> <mo stretchy="false">(</mo> <mrow><mi>θ</mi></mrow>
    <mo>+</mo> <mtext>s</mtext> <mo stretchy="false">)</mo> <mo>≈</mo> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi>θ</mi> <mo stretchy="false">)</mo> <mo>+</mo>
    <mi>b</mi> <mo>×</mo> <mtext>s</mtext> <mo><</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>L</mi> <mo stretchy="false">(</mo> <mrow><mi>θ</mi></mrow>
    <mo>+</mo> <mtext>s</mtext> <mo stretchy="false">)</mo> <mo>≈</mo> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi>θ</mi> <mo stretchy="false">)</mo> <mo>+</mo>
    <mi>b</mi> <mo>×</mo> <mtext>s</mtext> <mo><</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo></math>
- en: So, taking a small step to the right of this <math><mrow><mi>θ</mi></mrow></math>
    decreases the loss. On the other hand, on the other side of <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> in the diagram in [Figure 20-2](#gd-diagram),
    the slope is positive, and taking a small step to the left decreases the loss.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，向<math><mrow><mi>θ</mi></mrow></math> 的右侧小步骤会减小损失。另一方面，在图[Figure 20-2](#gd-diagram)中<math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> 的另一侧，切线是正的，向左侧小步骤也会减小损失。
- en: When we take repeated small steps in the direction indicated by whether the
    slope of the tangent line is positive or negative at each new step, this leads
    to smaller and smaller values of the average loss and eventually brings us to
    the minimizing value <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    (or very close to it). This is the basic idea behind gradient descent.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们根据切线斜率的正负指示重复采取小步骤时，这导致平均损失值越来越小，最终使我们接近或达到最小化值<math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> 。这就是梯度下降背后的基本思想。
- en: 'More formally, to minimize <math><mi>L</mi> <mo stretchy="false">(</mo> <mi
    mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> for a general
    vector of parameters, <math><mi mathvariant="bold-italic">θ</mi></math> , the
    gradient (first-order partial derivative) determines the direction and size of
    the step to take. If we write the gradient, <math><msub><mi mathvariant="normal">∇</mi>
    <mi>θ</mi></msub> <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> , as simply <math><mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> , then
    gradient descent says the increment or step is <math><mo>−</mo> <mi>α</mi> <mi>g</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    for some small positive <math><mi>α</mi></math> . Then the average loss at the
    new position is:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，为了最小化一般参数向量<math><mi mathvariant="bold-italic">θ</mi></math> 的损失函数<math><mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    ，梯度（一阶偏导数）决定了应该采取的步长和方向。如果我们将梯度<math><msub><mi mathvariant="normal">∇</mi> <mi>θ</mi></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></math> 简单写作<math><mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> ，那么梯度下降法指出每次增量或步长为<math><mo>−</mo>
    <mi>α</mi> <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> ，其中<math><mi>α</mi></math> 是某个小正数。然后，在新位置的平均损失是：
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>+</mo> <mo stretchy="false">(</mo> <mo>−</mo> <mi>α</mi> <mi>g</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>≈</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mi>α</mi> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <msup><mo stretchy="false">)</mo> <mi>T</mi></msup>
    <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mo><</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>+</mo> <mo stretchy="false">(</mo> <mo>−</mo> <mi>α</mi> <mi>g</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>≈</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mi>α</mi> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <msup><mo stretchy="false">)</mo> <mi>T</mi></msup>
    <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mo><</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
- en: Note that <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> is a <math><mi>p</mi> <mo>×</mo> <mn>1</mn></math>
    vector and <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <msup><mo stretchy="false">)</mo> <mi>T</mi></msup> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> is positive.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> 是一个 <math><mi>p</mi> <mo>×</mo> <mn>1</mn></math>
    向量，而 <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <msup><mo stretchy="false">)</mo> <mi>T</mi></msup> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> 是正的。
- en: 'The steps in the gradient descent algorithm go as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法的步骤如下：
- en: Choose a starting value, called <math><msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mn>0</mn> <mo stretchy="false">)</mo></mrow></msup></math>
    (a common choice is <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mn>0</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <mn>0</mn></math> ).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个起始值，称为 <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mn>0</mn> <mo stretchy="false">)</mo></mrow></msup></math>（一个常见的选择是 <math><msup><mi
    mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mn>0</mn>
    <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo> <mn>0</mn></math>）。
- en: Compute <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mi>g</mi> <mo
    stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    .
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mi>g</mi> <mo
    stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    。
- en: Repeat step 2 until <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup></math>
    doesn’t change (or changes little) between iterations.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 直到 <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup></math>
    不再改变（或变化很小）为止。
- en: The quantity <math><mi>α</mi></math> is called the *learning rate*. Setting
    <math><mi>α</mi></math> can be tricky. It needs to be small enough to not overshoot
    the minimum but large enough to arrive at the minimum in reasonably few steps
    (see [Figure 20-3](#gd-learning-rate)). There are many strategies for setting
    <math><mi>α</mi></math> . For example, it can be useful to decrease <math><mi>α</mi></math>
    over time. When <math><mi>α</mi></math> changes between iterations, we use the
    notation <math><msup><mi>α</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo
    stretchy="false">)</mo></mrow></msup></math> to indicate that the learning rate
    varies during the search.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数量 <math><mi>α</mi></math> 被称为*学习率*。设置 <math><mi>α</mi></math> 可能比较棘手。它需要足够小以避免超过最小值，但足够大以在合理步数内达到最小值（见
    [图 20-3](#gd-learning-rate)）。有许多设置 <math><mi>α</mi></math> 的策略。例如，随着时间的推移减少 <math><mi>α</mi></math>
    可能会很有用。当 <math><mi>α</mi></math> 在迭代之间变化时，我们使用符号 <math><msup><mi>α</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    表示学习率在搜索过程中变化。
- en: '![](assets/leds_2003.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_2003.png)'
- en: Figure 20-3\. A small learning rate requires many steps to converge (left),
    and a large learning rate can diverge (right); choosing the learning rate well
    leads to fast convergence on the minimizing value (middle)
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-3\. 较小的学习率需要许多步骤才能收敛（左），较大的学习率可能会发散（右）；选择适当的学习率可以快速收敛到最小值（中）。
- en: The gradient descent algorithm is simple yet powerful since we can use it for
    many types of models and many types of loss functions. It is the computational
    tool of choice for fitting many models, including linear regression on large datasets
    and logistic regression. We demonstrate the algorithm to fit a constant to the
    bus delay data (from [Chapter 4](ch04.html#ch-modeling)) next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法简单而强大，因为我们可以用它来拟合许多类型的模型和许多类型的损失函数。它是拟合许多模型的计算工具首选，包括大数据集上的线性回归和逻辑回归。接下来，我们演示使用该算法来拟合巴士延误数据中的常量（来自
    [第 4 章](ch04.html#ch-modeling)）。
- en: Minimizing Huber Loss
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小化 Huber 损失
- en: '*Huber loss* combines absolute loss and squared loss to get a function that
    is differentiable (like squared loss) and less sensitive to outliers (like absolute
    loss):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*Huber 损失* 结合了绝对损失和平方损失，得到一个既可微（像平方损失）又对异常值不那么敏感（像绝对损失）的函数：'
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>{</mo> <mtable columnalign="left left" columnspacing="1em"
    rowspacing=".2em"><mtr><mtd><mfrac><mn>1</mn> <mn>2</mn></mfrac> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></mtd> <mtd><mo stretchy="false">|</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow> <mo>≤</mo> <mi>γ</mi></mtd></mtr>
    <mtr><mtd><mi>γ</mi> <mo stretchy="false">(</mo> <mrow><mo stretchy="false">|</mo></mrow>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow>
    <mo>−</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mi>γ</mi> <mo stretchy="false">)</mo></mtd>
    <mtd><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>{</mo> <mtable columnalign="left left" columnspacing="1em"
    rowspacing=".2em"><mtr><mtd><mfrac><mn>1</mn> <mn>2</mn></mfrac> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></mtd> <mtd><mo stretchy="false">|</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow> <mo>≤</mo> <mi>γ</mi></mtd></mtr>
    <mtr><mtd><mi>γ</mi> <mo stretchy="false">(</mo> <mrow><mo stretchy="false">|</mo></mrow>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow>
    <mo>−</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mi>γ</mi> <mo stretchy="false">)</mo></mtd>
    <mtd><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math>
- en: 'Since Huber loss is differentiable, we can use gradient descent. We first find
    the gradient of the average Huber loss:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Huber 损失是可微的，我们可以使用梯度下降。我们首先找到平均 Huber 损失的梯度：
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mi>n</mi></munderover> <mrow><mo>{</mo>
    <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em"><mtr><mtd><mo>−</mo>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi>
    <mo stretchy="false">)</mo></mtd> <mtd><mo stretchy="false">|</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow>
    <mo>≤</mo> <mi>γ</mi></mtd></mtr> <mtr><mtd><mo>−</mo> <mi>γ</mi> <mo>⋅</mo> <mtext>sign</mtext>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi>
    <mo stretchy="false">)</mo></mtd> <mtd><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math>
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mi>n</mi></munderover> <mrow><mo>{</mo>
    <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em"><mtr><mtd><mo>−</mo>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi>
    <mo stretchy="false">)</mo></mtd> <mtd><mo stretchy="false">|</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow>
    <mo>≤</mo> <mi>γ</mi></mtd></mtr> <mtr><mtd><mo>−</mo> <mi>γ</mi> <mo>⋅</mo> <mtext>sign</mtext>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi>
    <mo stretchy="false">)</mo></mtd> <mtd><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math>
- en: 'We create the functions `huber_loss` and `grad_huber_loss` to compute the average
    loss and its gradient. We write these functions to have signatures that enable
    us to specify the parameter as well as the observed data that we average over
    and the transition point of the loss function:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了 `huber_loss` 和 `grad_huber_loss` 函数来计算平均损失及其梯度。我们编写这些函数时签名设计使我们能够指定参数以及我们平均的观察数据和损失函数的转折点：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we write a simple implementation of gradient descent. The signature of
    our function includes the loss function, its gradient, and the data to average
    over. We also supply the learning rate.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写了梯度下降的简单实现。我们的函数签名包括损失函数、其梯度和要平均的数据。我们还提供学习率。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Recall that the bus delays dataset consists of over 1,000 measurements of how
    many minutes late the northbound C-line buses are in arriving at the stop at 3rd
    Avenue and Pike Street in Seattle:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想一下，公交车延误数据集包含超过 1,000 个测量值，即北行 C 线公交车在抵达西雅图第三大道和派克街站点时晚多少分钟：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In [Chapter 4](ch04.html#ch-modeling), we fit a constant model to these data
    for absolute loss and squared loss. We found that absolute loss yielded the median
    and square the mean of the data:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 4 章](ch04.html#ch-modeling) 中，我们为这些数据拟合了一个常数模型，以得到绝对损失和平方损失。我们发现绝对损失产生了数据的中位数，而平方损失产生了数据的均值：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we use the gradient descent algorithm to find the minimizing constant model
    for Huber loss:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用梯度下降算法来找到最小化 Huber 损失的常数模型：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The optimizing constant for Huber loss is close to the value that minimizes
    absolute loss. This comes from the shape of the Huber loss function. It is linear
    in the tails and so is not affected by outliers like with absolute loss and unlike
    with squared loss.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Huber 损失的优化常数接近最小化绝对损失的值。这是由于 Huber 损失函数的形状决定的。它在尾部是线性的，因此不像绝对损失那样受到异常值的影响，也不像平方损失那样受到影响。
- en: Warning
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: We wrote our `minimize` function to demonstrate the idea behind the algorithm.
    In practice, you will want to use well-tested, numerically sound implementations
    of an optimization algorithm. For example, the `scipy` package has a `minimize`
    method that we can use to find the minimizer of average loss, and we don’t even
    need to compute the gradient. This algorithm is likely to be much faster than
    any one that we might write. In fact, we used it in [Chapter 18](ch18.html#ch-donkey)
    when we created our own asymmetric modification of quadratic loss for the special
    case where we wanted the loss to be greater for errors on one side of the minimum
    than the other.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了我们的 `minimize` 函数来演示算法背后的思想。在实践中，您应该使用经过充分测试的、数值上稳定的优化算法的实现。例如，`scipy`
    包中有一个 `minimize` 方法，我们可以用它来找到平均损失的最小化器，甚至不需要计算梯度。这个算法可能比我们可能编写的任何一个算法都要快得多。事实上，我们在
    [第 18 章](ch18.html#ch-donkey) 中使用它来创建我们自己的二次损失的非对称修改，特别是在我们希望损失对于最小值的一侧的错误更大而另一侧的影响较小的特殊情况下。
- en: More generally, we typically stop the algorithm when <math><msup><mi>θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    doesn’t change much between iterations. In our function, we stop when <math><msup><mi>θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>−</mo> <msup><mi>θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    is less than 0.001\. It is also common to stop the search after a large number
    of steps, such as 1,000\. If the algorithm has not arrived at the minimizing value
    after 1,000 iterations, then the algorithm might be diverging because the learning
    rate is too large or the minimum might exist in the limit at <math><mo>±</mo>
    <mi mathvariant="normal">∞</mi></math> .
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，我们通常在迭代之间<math><msup><mi>θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup></math> 不太改变时停止算法。在我们的函数中，我们停止当<math><msup><mi>θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>−</mo> <msup><mi>θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    小于 0.001 时。在迭代次数较多时，例如 1,000 次，停止搜索也很常见。如果算法在 1,000 次迭代后仍未到达最小值，则可能是因为学习率过大，或者最小值可能存在于极限处<math><mo>±</mo>
    <mi mathvariant="normal">∞</mi></math>。
- en: 'Gradient descent gives us a general way to minimize average loss when we cannot
    easily solve for the minimizing value analytically or when the minimization is
    computationally expensive. The algorithm relies on two important properties of
    the average loss function: it is both convex and differentiable in <math><mi mathvariant="bold-italic">θ</mi></math>
    . We discuss how the algorithm relies on these properties next.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们无法通过解析方法轻松求解最小值或者最小化计算成本很高时，梯度下降给了我们一个通用的最小化平均损失的方法。该算法依赖于平均损失函数的两个重要属性：它在<math><mi
    mathvariant="bold-italic">θ</mi></math> 上既是凸的又是可微的。我们接下来讨论算法如何依赖这些属性。
- en: Convex and Differentiable Loss Functions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 凸和可微损失函数
- en: As its name suggests, the gradient descent algorithm requires the function being
    minimized to be differentiable. The gradient, <math><msub><mi mathvariant="normal">∇</mi>
    <mi>θ</mi></msub> <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> , allows us to make a linear approximation
    to the average loss in small neighborhoods of <math><mi mathvariant="bold-italic">θ</mi></math>
    . This approximation gives us the direction (and size) of the step, and as long
    as we don’t overshoot the minimum, <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> , we are
    bound to eventually reach it. Well, as long as the loss function is also convex.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名字所示，梯度下降算法要求被最小化的函数是可微的。梯度<math><msub><mi mathvariant="normal">∇</mi> <mi>θ</mi></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></math> ，使我们能够对<math><mi mathvariant="bold-italic">θ</mi></math>
    的小邻域内的平均损失进行线性近似。这个近似给出了我们的步长的方向（和大小），只要我们不超过最小值<math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> ，我们就一定会最终到达它。嗯，只要损失函数也是凸的。
- en: The step-by-step search for the minimum also relies on the loss function being
    convex. The function in the diagram on the left in [Figure 20-4](#gd-convex) is
    convex, but the function on the right is not. The function on the right has a
    local minimum, and depending on where the algorithm starts, it might converge
    to this local minimum and miss the real minimum entirely. The property of convexity
    avoids this problem. A *convex function* avoids the problem of local minima. So,
    with an appropriate step size, gradient descent finds the globally optimal <math><mi>θ</mi></math>
    for any convex, differentiable function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最小值的逐步搜索也依赖于损失函数是凸函数。左图中的函数是凸的，但右图中的函数不是。右图中的函数有一个局部最小值，根据算法开始的位置，它可能会收敛到这个局部最小值并完全错过真正的最小值。凸性质避免了这个问题。*凸函数*避免了局部最小值的问题。所以，通过适当的步长，梯度下降可以找到任何凸、可微函数的全局最优解<math><mi>θ</mi></math>。
- en: '![](assets/leds_2004.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_2004.png)'
- en: Figure 20-4\. With nonconvex functions (right), gradient descent might locate
    a local minimum rather than a global minimum, which is not possible with convex
    functions (left)
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 20-4。对于非凸函数（右图），梯度下降可能会找到局部最小值而不是全局最小值，而凸函数（左图）不可能出现这种情况。
- en: 'Formally, a function <math><mi>f</mi></math> is convex if for any two input
    values, <math><msub><mi mathvariant="bold-italic">θ</mi> <mi>a</mi></msub></math>
    and <math><msub><mi mathvariant="bold-italic">θ</mi> <mi>b</mi></msub></math>
    , and any <math><mi>q</mi></math> between 0 and 1:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，函数 <math><mi>f</mi></math> 如果对于任意的输入值 <math><msub><mi mathvariant="bold-italic">θ</mi>
    <mi>a</mi></msub></math> 和 <math><msub><mi mathvariant="bold-italic">θ</mi> <mi>b</mi></msub></math>，以及介于
    0 和 1 之间的任意 <math><mi>q</mi></math> 都是凸的：
- en: <math display="block"><mi>q</mi> <mi>f</mi> <mo stretchy="false">(</mo> <msub><mi
    mathvariant="bold-italic">θ</mi> <mi>a</mi></msub> <mo stretchy="false">)</mo>
    <mo>+</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>q</mi> <mo stretchy="false">)</mo>
    <mi>f</mi> <mo stretchy="false">(</mo> <msub><mi mathvariant="bold-italic">θ</mi>
    <mi>b</mi></msub> <mo stretchy="false">)</mo> <mo>≥</mo> <mi>f</mi> <mo stretchy="false">(</mo>
    <mi>q</mi> <msub><mi mathvariant="bold-italic">θ</mi> <mi>a</mi></msub> <mo>+</mo>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>q</mi> <mo stretchy="false">)</mo>
    <msub><mi mathvariant="bold-italic">θ</mi> <mi>b</mi></msub> <mo stretchy="false">)</mo></math>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mi>q</mi> <mi>f</mi> <mo stretchy="false">(</mo> <msub><mi
    mathvariant="bold-italic">θ</mi> <mi>a</mi></msub> <mo stretchy="false">)</mo>
    <mo>+</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>q</mi> <mo stretchy="false">)</mo>
    <mi>f</mi> <mo stretchy="false">(</mo> <msub><mi mathvariant="bold-italic">θ</mi>
    <mi>b</mi></msub> <mo stretchy="false">)</mo> <mo>≥</mo> <mi>f</mi> <mo stretchy="false">(</mo>
    <mi>q</mi> <msub><mi mathvariant="bold-italic">θ</mi> <mi>a</mi></msub> <mo>+</mo>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>q</mi> <mo stretchy="false">)</mo>
    <msub><mi mathvariant="bold-italic">θ</mi> <mi>b</mi></msub> <mo stretchy="false">)</mo></math>
- en: This inequality implies that any line segment that connects two points of the
    function must reside on or above the function itself. Heuristically, this means
    that whenever we take a small enough step to the right when the gradient is negative
    or to the left when the gradient is positive, we will head in the direction of
    the function’s minimum.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个不等式意味着连接函数的任意两个点的线段必须位于或位于函数本身之上。从启发式的角度来看，这意味着无论我们在梯度为负时向右走还是在梯度为正时向左走，只要我们采取足够小的步伐，我们就会朝向函数的最小值方向前进。
- en: 'The formal definition of convexity gives us a precise way to determine whether
    a function is convex. And we can use this definition to connect the convexity
    of the average loss <math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> to the loss function <math><mrow><mi mathvariant="script">l</mi></mrow>
    <mo mathvariant="script" stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="script" stretchy="false">)</mo></math> . We have so far in this
    chapter simplified the representation of <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> by not
    mentioning the data. Recall:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 凸性的正式定义为我们提供了确定一个函数是否为凸函数的精确方式。我们可以利用这个定义来将平均损失函数 <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> 的凸性与损失函数
    <math><mrow><mi mathvariant="script">l</mi></mrow> <mo mathvariant="script" stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="script" stretchy="false">)</mo></math>
    连接起来。迄今为止，在本章中，我们通过不提及数据来简化了 <math><mi>L</mi> <mo stretchy="false">(</mo> <mi
    mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> 的表示。回顾一下：
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
- en: 'where <math><mtext mathvariant="bold">X</mtext></math> is an <math><mi>n</mi>
    <mo>×</mo> <mi>p</mi></math> design matrix and <math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mi>i</mi></msub></math> is the <math><mi>i</mi></math> th row of the design matrix,
    which corresponds to the <math><mi>i</mi></math> th observation in the dataset.
    This means that the gradient can be expressed as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math><mtext mathvariant="bold">X</mtext></math> 是一个 <math><mi>n</mi> <mo>×</mo>
    <mi>p</mi></math> 设计矩阵，<math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mi>i</mi></msub></math> 是设计矩阵的第 <math><mi>i</mi></math> 行，对应于数据集中的第 <math><mi>i</mi></math>
    个观测值。这意味着梯度可以表示为：
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
- en: 'If <math><mrow><mi mathvariant="script">l</mi></mrow> <mo mathvariant="script"
    stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="script">,</mo>
    <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi mathvariant="script">i</mi></msub>
    <mo mathvariant="script">,</mo> <msub><mi mathvariant="script">y</mi> <mi mathvariant="script">i</mi></msub>
    <mo mathvariant="script" stretchy="false">)</mo></math> is a convex function of
    <math><mi mathvariant="bold-italic">θ</mi></math> , then the average loss is also
    convex. And similarly for the derivative: the derivative of <math><mrow><mi mathvariant="script">l</mi></mrow>
    <mo mathvariant="script" stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="script">,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mi mathvariant="script">i</mi></msub> <mo mathvariant="script">,</mo> <msub><mi
    mathvariant="script">y</mi> <mi mathvariant="script">i</mi></msub> <mo mathvariant="script"
    stretchy="false">)</mo></math> is averaged over the data to evaluate the derivative
    of <math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></math> . We walk through a proof of the convexity
    property in the exercises.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果<math><mrow><mi mathvariant="script">l</mi></mrow> <mo mathvariant="script"
    stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="script">,</mo>
    <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi mathvariant="script">i</mi></msub>
    <mo mathvariant="script">,</mo> <msub><mi mathvariant="script">y</mi> <mi mathvariant="script">i</mi></msub>
    <mo mathvariant="script" stretchy="false">)</mo></math>是关于<math><mi mathvariant="bold-italic">θ</mi></math>的凸函数，则平均损失也是凸的。对于导数也是类似的：<math><mrow><mi
    mathvariant="script">l</mi></mrow> <mo mathvariant="script" stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="script">,</mo> <msub><mrow><mi
    mathvariant="bold">x</mi></mrow> <mi mathvariant="script">i</mi></msub> <mo mathvariant="script">,</mo>
    <msub><mi mathvariant="script">y</mi> <mi mathvariant="script">i</mi></msub> <mo
    mathvariant="script" stretchy="false">)</mo></math>的导数被平均到数据上以评估<math><mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo> <mtext
    mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></math>的导数。我们将在练习中详细讨论凸性质。
- en: Now, with a large amount of data, calculating <math><msup><mi>θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    can be computationally expensive since it involves the average of the gradient
    <math><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow></math> over all the <math><mo stretchy="false">(</mo>
    <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo></math> . We next consider variants
    of gradient descent that can be computationally faster because they don’t average
    over all of the data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了大量数据，计算<math><msup><mi>θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup></math>可能会非常耗时，因为它涉及到所有<math><mo stretchy="false">(</mo>
    <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo></math>上的梯度<math><msub><mi mathvariant="normal">∇</mi>
    <mrow><mi>θ</mi></mrow></msub> <mrow><mi mathvariant="script">l</mi></mrow></math>的平均值。接下来，我们考虑梯度下降的变体，这些变体可以更快地计算，因为它们不会对所有数据进行平均。
- en: Variants of Gradient Descent
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降的变体
- en: Two variants of gradient descent, stochastic gradient descent and mini-batch
    gradient descent, use subsets of the data when computing the gradient of the average
    loss and are useful for optimization problems with large datasets. A third alternative,
    Newton’s method, assumes the loss function is twice differentiable and uses a
    quadratic approximation to the loss function, rather than the linear approximation
    used in gradient descent.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的两个变体，随机梯度下降和小批量梯度下降，在计算平均损失的梯度时使用数据子集，并且对于具有大型数据集的优化问题很有用。第三个选择是牛顿法，它假设损失函数是两次可微的，并且使用损失函数的二次近似，而不是梯度下降中使用的线性近似。
- en: 'Recall that gradient descent takes steps based on the gradient. At step <math><mi>t</mi></math>
    , we move from <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math> to:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，梯度下降是根据梯度采取步骤的。在步骤<math><mi>t</mi></math>，我们从<math><msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>移动到：
- en: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mi>L</mi>
    <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mi>L</mi>
    <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></math>
- en: 'And since <math><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></math> can be expressed as the average gradient of
    the loss function <math><mi mathvariant="script">l</mi></math> , we have:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 <math><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></math> 可以表示为损失函数 <math><mi mathvariant="script">l</mi></math>
    的平均梯度，我们有：
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
- en: This representation of the gradient of the average loss in terms of the average
    of the gradient of loss at each point in the data shows why the algorithm is also
    called *batch gradient descent*. Two variants to batch gradient descent use smaller
    amounts of the data rather than the complete “batch.” The first, stochastic gradient
    descent, uses only one observation in each step of the algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种根据数据中每个点处损失的梯度的平均来表示平均损失梯度的方法说明了为什么这个算法也被称为*批梯度下降*。批梯度下降的两个变体使用较小数量的数据而不是完整的“批次”。第一个，随机梯度下降，在算法的每一步中只使用一个观察。
- en: Stochastic Gradient Descent
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: Although batch gradient descent can often find an optimal <math><mi mathvariant="bold-italic">θ</mi></math>
    in relatively few iterations, each iteration can take a long time to compute if
    the dataset contains many observations. To get around this difficulty, stochastic
    gradient descent approximates the overall gradient by a single, randomly chosen
    data point. Since this observation is chosen randomly, we expect that using the
    gradient at randomly chosen observations will, on average, move in the correct
    direction and so eventually converge to the minimizing parameter.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管批梯度下降通常能在相对较少的迭代中找到最优的 <math><mi mathvariant="bold-italic">θ</mi></math>，但如果数据集包含许多观察结果，每次迭代可能需要很长时间来计算。为了克服这个困难，随机梯度下降通过单个、随机选择的数据点来近似整体梯度。由于此观察是随机选择的，我们期望使用在随机选择观察点处的梯度，平均而言会朝着正确的方向移动，从而最终收敛到最小化参数。
- en: 'In short, to conduct stochastic gradient descent, we replace the average gradient
    with the gradient at a single data point. So, the updated formula is just:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，为了进行随机梯度下降，我们将平均梯度替换为单个数据点处的梯度。因此，更新后的公式就是：
- en: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <msup><mrow><mi
    mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <msup><mrow><mi
    mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
- en: In this formula, the <math><msup><mi>i</mi> <mrow><mi>t</mi> <mi>h</mi></mrow></msup></math>
    observations <math><mo stretchy="false">(</mo> <msub><mtext mathvariant="bold">x</mtext>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
    are chosen randomly from the data. Choosing the points randomly is critical to
    the success of stochastic gradient descent. If the points are not chosen randomly,
    the algorithm may produce significantly worse results than batch gradient descent.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，<math><msup><mi>i</mi> <mrow><mi>t</mi> <mi>h</mi></mrow></msup></math>
    观测值 <math><mo stretchy="false">(</mo> <msub><mtext mathvariant="bold">x</mtext>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
    是从数据中随机选择的。随机选择点对于随机梯度下降的成功至关重要。如果点不是随机选择的，算法可能会产生比批梯度下降更差的结果。
- en: We most commonly run stochastic gradient descent by randomly shuffling all of
    the data points and using each point in its shuffled order until we complete one
    entire pass through the data. If the algorithm hasn’t converged yet, then we reshuffle
    the points and run another pass through the data. Each *iteration* of stochastic
    gradient descent looks at one data point; each complete pass through the data
    is called an *epoch*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常通过随机重新排列所有数据点并按照它们的重新排列顺序使用每个点，直到完成一整个数据的遍历来运行随机梯度下降。如果算法尚未收敛，那么我们会重新洗牌数据并再次遍历数据。每个*迭代*的随机梯度下降看一个数据点；每个完整的数据遍历称为*epoch*。
- en: Since stochastic descent only examines a single data point at a time, at times
    it takes steps away from the minimizer, <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> , but on average these steps
    are in the right direction. And since the algorithm computes an update much more
    quickly than batch gradient descent, it can make significant progress toward the
    optimal <math><mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold"
    stretchy="false">^</mo></mover></mrow></math> by the time batch gradient descent
    finishes a single update.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机下降每次只检查一个数据点，有时会朝着极小化器<math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math>的方向迈出步伐，但平均而言这些步骤是朝着正确的方向。由于该算法的更新速度比批量梯度下降快得多，因此它可以在批量梯度下降完成单次更新时朝着最优<math><mrow><mover><mi
    mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math>取得显著进展。
- en: Mini-Batch Gradient Descent
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: 'As its name suggests, *mini-batch gradient descent* strikes a balance between
    batch gradient descent and stochastic gradient descent by increasing the number
    of observations selected at random in each iteration. In mini-batch gradient descent,
    we average the gradient of the loss function at a few data points instead of at
    a single point or all the points. We let <math><mrow><mi mathvariant="script">B</mi></mrow></math>
    represent the mini-batch of data points that are randomly sampled from the dataset,
    and we define the algorithm’s next step as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，*小批量梯度下降* 在批量梯度下降和随机梯度下降之间取得平衡，通过在每次迭代中随机选择更多的观测值来增加样本数。在小批量梯度下降中，我们对少量数据点的损失函数梯度进行平均，而不是单个点或所有点的梯度。我们让<math><mrow><mi
    mathvariant="script">B</mi></mrow></math>表示从数据集中随机抽取的小批次数据点，并定义算法的下一步为：
- en: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <mfrac><mn>1</mn> <mrow><mo stretchy="false">|</mo> <mrow><mi mathvariant="script">B</mi></mrow>
    <mo stretchy="false">|</mo></mrow></mfrac> <munder><mo>∑</mo> <mrow><mrow><mi>i</mi>
    <mo>∈</mo> <mrow><mi mathvariant="script">B</mi></mrow></mrow></mrow></munder>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <mfrac><mn>1</mn> <mrow><mo stretchy="false">|</mo> <mrow><mi mathvariant="script">B</mi></mrow>
    <mo stretchy="false">|</mo></mrow></mfrac> <munder><mo>∑</mo> <mrow><mrow><mi>i</mi>
    <mo>∈</mo> <mrow><mi mathvariant="script">B</mi></mrow></mrow></mrow></munder>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
- en: As with stochastic gradient descent, we perform mini-batch gradient descent
    by randomly shuffling the data. Then we split the data into consecutive mini-batches
    and iterate through the batches in sequence. After each epoch, we reshuffle our
    data and select new mini-batches.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机梯度下降类似，我们通过随机洗牌数据来执行小批量梯度下降。然后我们将数据分割成连续的小批次，并按顺序迭代这些批次。每个 epoch 后，重新洗牌数据并选择新的小批次。
- en: While we have made the distinction between stochastic and mini-batch gradient
    descent, *stochastic gradient descent* is sometimes used as an umbrella term that
    encompasses the selection of a mini-batch of any size.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经区分了随机梯度下降和小批量梯度下降，*随机梯度下降* 有时被用作一个总称，包括任意大小的小批次的选择。
- en: Another common optimization technique is Newton’s method.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的优化技术是牛顿法。
- en: Newton’s Method
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 牛顿法
- en: 'Newton’s method uses the second derivative to optimize the loss. The basic
    idea is to approximate the average loss, <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> , in small
    neighborhoods of <math><mi mathvariant="bold-italic">θ</mi></math> , with a quadratic
    curve rather than a linear approximation. The approximation looks as follows for
    a small step <math><mrow><mi mathvariant="bold">s</mi></mrow></math> :'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿法利用二阶导数优化损失。其基本思想是在<math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> 的小邻域内，用二次曲线而不是线性逼近来近似平均损失。对于一个小步长<math><mrow><mi
    mathvariant="bold">s</mi></mrow></math>，逼近如下所示：
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>+</mo> <mrow><mi
    mathvariant="bold">s</mi></mrow> <mo stretchy="false">)</mo> <mo>≈</mo> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo>
    <mo>+</mo> <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <msup><mo stretchy="false">)</mo> <mi>T</mi></msup> <mrow><mi mathvariant="bold">s</mi></mrow>
    <mo>+</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <msup><mrow><mi mathvariant="bold">s</mi></mrow>
    <mi>T</mi></msup> <mi>H</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mrow><mi mathvariant="bold">s</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>+</mo> <mrow><mi
    mathvariant="bold">s</mi></mrow> <mo stretchy="false">)</mo> <mo>≈</mo> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo>
    <mo>+</mo> <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <msup><mo stretchy="false">)</mo> <mi>T</mi></msup> <mrow><mi mathvariant="bold">s</mi></mrow>
    <mo>+</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <msup><mrow><mi mathvariant="bold">s</mi></mrow>
    <mi>T</mi></msup> <mi>H</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mrow><mi mathvariant="bold">s</mi></mrow></mtd></mtr></mtable></math>
- en: 'where <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mo>=</mo> <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></math> is the gradient and <math><mi>H</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo> <mo>=</mo> <msubsup><mi
    mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow> <mn>2</mn></msubsup> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    is the Hessian of <math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> . More specifically, <math><mi>H</mi></math>
    is a <math><mi>p</mi> <mo>×</mo> <mi>p</mi></math> matrix of second-order partial
    derivatives in <math><mi mathvariant="bold-italic">θ</mi></math> with <math><mi>i</mi></math>
    , <math><mi>j</mi></math> elements:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mo>=</mo> <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></math>是梯度，<math><mi>H</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo> <mo>=</mo> <msubsup><mi
    mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow> <mn>2</mn></msubsup> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>是<math><mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>的海森矩阵。更具体地说，<math><mi>H</mi></math>是<math><mi
    mathvariant="bold-italic">θ</mi></math>中的二阶偏导数的<math><mi>p</mi> <mo>×</mo> <mi>p</mi></math>矩阵，具有<math><mi>i</mi></math>，<math><mi>j</mi></math>元素：
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mi>H</mi>
    <mrow><mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msup><mi>∂</mi>
    <mn>2</mn></msup> <mrow><mi mathvariant="script">l</mi></mrow></mrow> <mrow><mi>∂</mi>
    <msub><mi>θ</mi> <mi>i</mi></msub> <mi>∂</mi> <msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mtd></mtr></mtable></math>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mi>H</mi>
    <mrow><mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msup><mi>∂</mi>
    <mn>2</mn></msup> <mrow><mi mathvariant="script">l</mi></mrow></mrow> <mrow><mi>∂</mi>
    <msub><mi>θ</mi> <mi>i</mi></msub> <mi>∂</mi> <msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mtd></mtr></mtable></math>
- en: 'This quadratic approximation to <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo>+</mo> <mrow><mi mathvariant="bold">s</mi></mrow>
    <mo stretchy="false">)</mo></math> has a minimum at <math><mrow><mi mathvariant="bold">s</mi></mrow>
    <mo>=</mo> <mo>−</mo> <mo stretchy="false">[</mo> <msup><mi>H</mi> <mrow><mo>−</mo>
    <mn>1</mn></mrow></msup> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> . (Convexity
    implies that <math><mi>H</mi></math> is a symmetric square matrix that can be
    inverted.) Then a step in the algorithm moves from <math><msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    to:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对<math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>+</mo> <mrow><mi mathvariant="bold">s</mi></mrow> <mo stretchy="false">)</mo></math>的二次逼近在<math><mrow><mi
    mathvariant="bold">s</mi></mrow> <mo>=</mo> <mo>−</mo> <mo stretchy="false">[</mo>
    <msup><mi>H</mi> <mrow><mo>−</mo> <mn>1</mn></mrow></msup> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo>
    <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></math> 处具有最小值。（凸性意味着<math><mi>H</mi></math>是对称方阵，可以被反转。）然后算法中的一步从<math><msup><mi
    mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup></math>移动到：
- en: <math display="block"><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>+</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo>−</mo> <mo stretchy="false">[</mo> <msup><mi>H</mi> <mrow><mo>−</mo> <mn>1</mn></mrow></msup>
    <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo
    stretchy="false">]</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup>
    <mo stretchy="false">)</mo></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>+</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo>−</mo> <mo stretchy="false">[</mo> <msup><mi>H</mi> <mrow><mo>−</mo> <mn>1</mn></mrow></msup>
    <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo
    stretchy="false">]</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup>
    <mo stretchy="false">)</mo></math>
- en: '[Figure 20-5](#newton-diagram) gives the idea behind Newton’s method of optimization.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[图20-5](#newton-diagram)展示了牛顿法优化的思想。'
- en: '![](assets/leds_2005.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_2005.png)'
- en: Figure 20-5\. Newton’s method uses a local quadratic approximation to the curve
    to take steps toward the minimizing value of a convex, twice-differentiable function
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图20-5。牛顿法使用对曲线的局部二次逼近来朝着凸、两次可微函数的最小值迈出步伐
- en: This technique converges quickly if the approximation is accurate and the steps
    are small. Otherwise, Newton’s method can diverge, which often happens if the
    function is nearly flat in a dimension. When the function is relatively flat,
    the derivative is near zero and its inverse can be quite large. Large steps can
    move to <math><mi mathvariant="bold-italic">θ</mi></math> that are far from where
    the approximation is accurate. (Unlike with gradient descent, there is no learning
    rate that keeps steps small.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此技术在逼近准确且步长小的情况下会快速收敛。否则，牛顿法可能会发散，这通常发生在函数在某个维度上几乎平坦的情况下。当函数相对平坦时，导数接近于零，其倒数可能非常大。大步长可能会移动到离逼近准确点很远的<math><mi
    mathvariant="bold-italic">θ</mi></math>处。（与梯度下降不同，没有学习率可以保持步长小。）
- en: Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced several techniques for numerical optimization
    that take advantage of the shape and smoothness of the loss function in the search
    for the minimizing parameter values. We first introduced gradient descent, which
    relies on the differentiability of loss function. Gradient descent, also called
    batch gradient descent, iteratively improves model parameters until the model
    achieves minimal loss. Since batch gradient descent is computationally intractable
    with large datasets, we often instead use stochastic gradient descent to fit models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了几种利用损失函数的形状和平滑性进行数值优化的技术，以搜索最小化参数值。我们首先介绍了梯度下降，它依赖于损失函数的可微性。梯度下降，也称为批量梯度下降，通过迭代改善模型参数，直到模型达到最小损失。由于批量梯度下降在处理大数据集时计算复杂度高，我们通常改用随机梯度下降来拟合模型。
- en: Mini-batch gradient descent is most optimal when running on a graphical processing
    unit (GPU) chip found in some computers. Since computations on these types of
    hardware can be executed in parallel, using a mini-batch can increase the accuracy
    of the gradient without increasing computation time. Depending on the memory size
    of the GPU, the mini-batch size is often set between 10 and 100 observations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降在运行在某些计算机上找到的图形处理单元（GPU）芯片时最为优化。由于这些硬件类型可以并行执行计算，使用小批量可以提高梯度的准确性，而不增加计算时间。根据GPU的内存大小，小批量大小通常设置在10到100个观测之间。
- en: Alternatively, if the loss function is twice differentiable, then Newton’s method
    can converge very quickly, even though it is more expensive to compute one step
    in the iteration. A hybrid approach is also popular, beginning with gradient descent
    (of some kind) and then switching the algorithm to Newton’s method. This approach
    can avoid divergence and be faster than gradient descent alone. Typically, the
    second-order approximation used by Newton’s method is more appropriate near the
    optimum and converges quickly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果损失函数是二次可微的，则牛顿法可以非常快速地收敛，尽管在迭代中计算一步较为昂贵。混合方法也很受欢迎，先用梯度下降（某种类型），然后切换算法至牛顿法。这种方法可以避免发散，并且比单独使用梯度下降更快。通常，在最优点附近，牛顿法使用的二阶近似更为合适且收敛速度快。
- en: Lastly, another option is to set the step size adaptively. Additionally, setting
    different learning rates for different features can be important if they are of
    different scale or vary in frequency. For example, word counts can differ a lot
    across common words and rare words.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一个选项是自适应设置步长。此外，如果不同特征的规模不同或频率不同，则设置不同的学习率可能很重要。例如，单词计数在常见单词和罕见单词之间可能会有很大差异。
- en: The logistic regression model introduced in [Chapter 19](ch19.html#ch-logistic)
    is fitted using numerical optimization methods like those described in this chapter.
    We wrap up with one final case study that uses logistic regression to fit a complex
    model with thousands of features.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第19章](ch19.html#ch-logistic)介绍的逻辑回归模型是通过本章描述的数值优化方法拟合的。我们最后介绍了一个案例研究，使用逻辑回归来拟合一个具有数千个特征的复杂模型。
