- en: 9 Compressive privacy for machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 压缩隐私在机器学习中的应用
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding compressive privacy
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解压缩隐私
- en: Introducing compressive privacy for machine learning applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习应用引入压缩隐私
- en: Implementing compressive privacy from theory to practice
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从理论到实践的压缩隐私实现
- en: A compressive privacy solution for privacy-preserving machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于隐私保护机器学习的压缩隐私解决方案
- en: In previous chapters we’ve looked into differential privacy, local differential
    privacy, privacy-preserving synthetic data generation, privacy-preserving data
    mining, and their application when designing privacy-preserving machine learning
    solutions. As you’ll recall, in differential privacy a trusted data curator collects
    data from individuals and produces differentially private results by adding precisely
    computed noise to the aggregation of individuals’ data. In local differential
    privacy, individuals privatize their own data by perturbation before sending it
    to the data aggregator, which eliminates the need to have a trusted data curator
    collect the data from individuals. In data mining, we looked into various privacy-preserving
    techniques and operations that can be used when collecting information and publishing
    the data. We also discussed strategies for regulating data mining output. Privacy-
    preserving synthetic data generation provides a promising solution for private
    data sharing, where synthetic yet representative data can be generated and then
    shared among multiple parties safely and securely.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了差分隐私、局部差分隐私、隐私保护合成数据生成、隐私保护数据挖掘以及在设计隐私保护机器学习解决方案时的应用。如您所回忆的，在差分隐私中，一个可信的数据管理员从个人那里收集数据，并通过向个人数据的聚合中添加精确计算的噪声来产生差分隐私的结果。在局部差分隐私中，个人在将数据发送到数据聚合器之前通过扰动来隐私化自己的数据，从而消除了需要一个可信的数据管理员从个人那里收集数据的需要。在数据挖掘中，我们探讨了在收集信息和发布数据时可以使用的各种隐私保护技术和操作。我们还讨论了调节数据挖掘输出的策略。隐私保护合成数据生成提供了私有数据共享的可行解决方案，其中可以生成合成且具有代表性的数据，然后安全且安全地共享给多个当事人。
- en: '![CH09_00_UN01_Zhuang](../../OEBPS/Images/CH09_00_UN01_Zhuang.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_00_UN01_Zhuang](../../OEBPS/Images/CH09_00_UN01_Zhuang.png)'
- en: As you can see, most of the techniques we have discussed are based on the definition
    of differential privacy (DP), which does not make any assumptions about the abilities
    of the adversaries, thus providing an extremely strong privacy guarantee. However,
    to enable this strong privacy guarantee, DP-based mechanisms usually add excessive
    noise to the private data, causing a somewhat inevitable utility drop. This prevents
    DP approaches from being applied for many real-world applications, and especially
    for practical applications using machine learning (ML) or deep learning. This
    prompts us to explore other perturbation-based approaches to privacy preservation.
    Compressive privacy (CP) is one alternative approach we can look into. In this
    chapter we will explore the concept, mechanisms, and applications of compressive
    privacy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们讨论的大多数技术都是基于差分隐私（DP）的定义，它不对对手的能力做出任何假设，因此提供了极其强大的隐私保证。然而，为了实现这种强大的隐私保证，基于DP的机制通常会在私有数据上添加过多的噪声，导致某种程度的不可避免的有效性下降。这阻止了DP方法应用于许多现实世界应用，尤其是使用机器学习（ML）或深度学习的实际应用。这促使我们探索其他基于扰动的隐私保护方法。压缩隐私（CP）是我们可以探讨的一种替代方法。在本章中，我们将探讨压缩隐私的概念、机制和应用。
- en: 9.1 Introducing compressive privacy
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 引入压缩隐私
- en: '*Compressive privacy* (CP) is an approach that perturbs the data by projecting
    it to a lower-dimensional hyperplane via compression and dimensionality-reduction
    (DR) techniques. To better understand the concept and benefits of CP, let’s compare
    it with the idea of differential privacy (DP), which we discussed in chapter 2.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*压缩隐私*（CP）是一种通过压缩和降维（DR）技术将数据投影到低维超平面上来扰动数据的方法。为了更好地理解压缩隐私的概念和优势，让我们将其与我们在第二章中讨论的差分隐私（DP）理念进行比较。'
- en: According to the definition of DP, a randomized algorithm *M* is said to be
    ε-DP of the input data if it satisfies Pr[*M*(*D*) ∈ *S*] ≤ *e*^ε ∙ Pr[*M*(*D*')
    ∈ *S*] for all *S* ∈ *Range*(*M*) and all datasets *D* and *D*' differing by one
    item, where *Range*(*M*) is the output set of *M*. In other words, the idea behind
    DP is that if the effect of making an arbitrary single change or substitution
    in a database is small enough, the query result cannot be used to infer much about
    any single individual, and it therefore provides privacy. As you can see, DP guarantees
    that the distribution of the query result from a dataset should be indistinguishable
    (modulo by a factor of *e* ^ε) whether or not any single item in that dataset
    is changed. The DP definition does not make any assumptions about adversaries
    in advance. For instance, adversaries could have unbounded auxiliary information
    and unlimited computing resources, and DP mechanisms can still provide privacy
    guarantees under this definition of DP.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 根据DP的定义，如果一个随机算法*M*满足对于所有*S*属于*M*的输出集*Range*(*M*)以及所有只相差一个项目的数据集*D*和*D*'，都有Pr[*M*(*D*)
    ∈ *S*] ≤ *e*^ε ∙ Pr[*M*(*D*') ∈ *S*]，则称该算法是输入数据的ε-DP算法。其中，*Range*(*M*)是算法*M*的输出集。换句话说，DP背后的思想是，如果对数据库进行任意单个更改或替换的影响足够小，查询结果就不能用来推断任何单个个体的信息，因此它提供了隐私保护。正如你所看到的，DP保证了从数据集查询结果的分布，无论数据集中的单个项目是否更改，都应该是不可区分的（模*e*^ε因子）。DP的定义没有对对手做出任何先前的假设。例如，对手可能拥有无界的辅助信息和无限的计算资源，但DP机制仍然可以在这种DP定义下提供隐私保证。
- en: This shows the bright side of DP—it provides strong privacy guarantees via rigorous
    theoretical analysis. However, the DP definition and mechanisms do not make any
    assumptions about utility. As such, DP mechanisms usually cannot promise good
    performance in terms of utility. This is particularly true when applying DP approaches
    to real-world applications that require complex calculations, such as data mining
    and machine learning. That’s why we also need to explore other privacy-enhancing
    techniques that consider utility while relaxing the theoretical privacy guarantees
    somewhat. CP is one such alternative that can be used with practical applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了DP的积极一面——它通过严格的理论分析提供了强大的隐私保证。然而，DP的定义和机制并没有对效用做出任何假设。因此，DP机制通常不能在效用方面提供良好的性能。这在将DP方法应用于需要复杂计算的实际应用（如数据挖掘和机器学习）时尤其如此。这就是为什么我们还需要探索其他考虑效用同时放松理论隐私保证的隐私增强技术。CP就是这样一种可以用于实际应用的替代方案。
- en: Unlike DP, the CP approach allows the query to be tailored to the known utility
    and privacy tasks. Specifically, for datasets that have samples with two labels,
    a utility label and a privacy label, CP allows the data owner to project their
    data to a lower dimension in a way that maximizes the accuracy of learning for
    the utility labels while decreasing the accuracy of learning for the privacy labels.
    We will discuss these labels when we get into the details. It is also noteworthy
    that, although CP does not eliminate all data privacy risks, it offers some control
    over the misuse of data when the privacy task is known. In addition, CP guarantees
    that the original data can never be fully recovered, mainly due to the dimensionality
    reduction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与DP不同，CP方法允许查询根据已知的效用和隐私任务进行定制。具体来说，对于具有两个标签的样本数据集，即效用标签和隐私标签，CP允许数据所有者以最大化效用标签学习准确性的方式将数据投影到低维空间，同时降低隐私标签学习的准确性。我们将在详细讨论时讨论这些标签。值得注意的是，尽管CP不能消除所有数据隐私风险，但在隐私任务已知的情况下，它提供了一些对数据滥用的控制。此外，CP保证原始数据永远不会被完全恢复，这主要是由于降维。
- en: Now let’s dig into how CP works. Figure 9.1 illustrates the threat model of
    CP. The adversaries are all the data users with full access to the public datasets
    (e.g., background and auxiliary information).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来深入探讨CP是如何工作的。图9.1展示了CP的威胁模型。对手是所有拥有对公共数据集（例如，背景和辅助信息）完全访问权限的数据用户。
- en: '![CH09_F01_Zhuang](../../OEBPS/Images/CH09_F01_Zhuang.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_Zhuang](../../OEBPS/Images/CH09_F01_Zhuang.png)'
- en: Figure 9.1 The threat model of compressive privacy. The real challenge is balancing
    the utility and privacy tradeoff.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 压缩隐私的威胁模型。真正的挑战在于平衡效用和隐私之间的权衡。
- en: In this scenario, let’s assume that the privacy task is a two-class {+,-} classification
    problem (utility tasks are independent of the privacy task), and *X*[+], *X*[-]
    are two public datasets. Suppose *X*[S] is the private data of the data owner,
    where *s* ∈{+,-} is its original class (privacy task) and *t* ∈ {+,-} is its expected
    class (privacy task) after applying the CP perturbation. The data owner could
    publish *z* (the perturbed version of *X*[S]) using the CP mechanisms. However,
    there could also be an adversary out there, using their approach *z*' = *A*(*z*,
    *X*[+], *X*[-]) to inference the original (privacy task) class *s*. Thus, what
    we want to achieve here is to minimize the probability difference, |*Pr* (*z*'
    = +|*z*) - *P r* (*z*' = -|*z*)|, so that the adversary cannot learn any valuable
    information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，让我们假设隐私任务是二分类{+,-}问题（实用任务与隐私任务独立），*X*[+]和*X*[-]是两个公开数据集。假设*X*[S]是数据所有者的私有数据，其中*s*
    ∈{+,-}是其原始类别（隐私任务），而*t* ∈ {+,-}是在应用CP扰动后的预期类别（隐私任务）。数据所有者可以使用CP机制发布*z*（*X*[S]的扰动版本）。然而，也可能存在一个对手，使用他们的方法*z*'
    = *A*(*z*, *X*[+], *X*[-])来推断原始（隐私任务）类别*s*。因此，我们想要实现的目标是最小化概率差异，|*Pr* (*z*' =
    +|*z*) - *P r* (*z*' = -|*z*)|，这样对手就不能学习到任何有价值的信息。
- en: In figure 9.1 Alice (the data owner) has some private data, and she wants to
    publish it for a utility task. Let’s say the utility task is to allow a data user
    to perform an ML classification with the data. Since it contains personal information,
    Alice can perturb the data using CP. Whenever Bob (the data user) wants to use
    this compressed data, he needs to recover the information, which he can do using
    the statistical properties of a publicly available dataset in a similar domain
    (which we call the *utility task feature space*). The problem is that someone
    else, Eve (an adversary), might also use the compressed data and try to recover
    it using another publicly available dataset, which might lead to a privacy breach.
    Thus, the real challenge with CP is balancing this utility/privacy tradeoff. We
    can compress data to perform the utility task, but it still needs to be challenging
    for someone to recover the data to identify personal information.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在图9.1中，Alice（数据所有者）有一些私有数据，她希望为了一个实用任务发布这些数据。假设这个实用任务是允许数据用户使用这些数据执行机器学习分类。由于它包含个人信息，Alice可以使用CP对数据进行扰动。每当Bob（数据用户）想要使用这个压缩数据时，他需要恢复信息，这可以通过使用在类似领域公开可用的数据集的统计特性（我们称之为*实用任务特征空间*）来实现。问题是，其他人，比如Eve（一个对手），也可能使用压缩数据并尝试使用另一个公开可用的数据集来恢复它，这可能导致隐私泄露。因此，CP的真正挑战在于平衡这种实用/隐私权衡。我们可以压缩数据以执行实用任务，但仍然需要让其他人难以恢复数据以识别个人信息。
- en: The following section will walk you through several useful components that enable
    CP for privacy-preserving data sharing or ML applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将向您介绍几个有用的组件，这些组件使CP能够用于隐私保护的数据共享或机器学习应用。
- en: 9.2 The mechanisms of compressive privacy
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 压缩隐私机制
- en: An important building block of CP is the supervised dimensionality reduction
    technique, which relies on data labels. Principal component analysis (PCA) is
    a widely used method that aims to project the data on the principal components
    with the highest variance, thus preserving most of the information in the data
    while reducing the data dimensions. We briefly discussed this in section 3.4.1,
    but let’s recap it quickly.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CP（差分隐私）的一个重要组成部分是监督降维技术，它依赖于数据标签。主成分分析（PCA）是一种广泛使用的方法，旨在将数据投影到具有最高方差的主成分上，从而在降低数据维度的同时保留数据中的大部分信息。我们曾在3.4.1节中简要讨论过这一点，但让我们快速回顾一下。
- en: 9.2.1 Principal component analysis (PCA)
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 主成分分析（PCA）
- en: Let’s first understand what principal components are. *Principal components*
    are new variables constructed as linear combinations of the initial variables
    in a dataset. These combinations are created in such a way that the new variables
    are uncorrelated, and most of the information within the initial variables is
    compressed into the first components (which is why we call it *compressive*).
    So, for instance, when 10-dimensional data gives you 10 principal components,
    PCA tries to put the maximum possible information in the first component, the
    maximum remaining information in the second component, and so on. When you organize
    information in principal components like this, you reduce dimensionality without
    losing much of the critical information.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解一下主成分是什么。*主成分* 是通过数据集中初始变量的线性组合构造的新变量。这些组合是以一种方式创建的，使得新变量之间不相关，并且初始变量中的大部分信息都压缩到了第一个成分中（这就是为什么我们称之为
    *压缩*）。因此，例如，当 10 维数据给出 10 个主成分时，PCA 尝试将尽可能多的信息放入第一个成分，剩余的最大信息放入第二个成分，依此类推。当你以这种方式组织主成分信息时，你可以在不丢失太多关键信息的情况下降低维度。
- en: Let’s consider a dataset with *N* training samples {*x*[1], *x*[2], ..., *x*[N]},
    where each sample has *M* features (*x*[i] ∈ ℝ^M). PCA performs something called
    *spectral decomposition of the center-adjusted scatter matrix* ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)
    such that
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个包含 *N* 个训练样本的 *x*[1], *x*[2], ..., *x*[N]，其中每个样本有 *M* 个特征（*x*[i] ∈ ℝ^M）的数据集。PCA
    执行一种称为 *中心调整散布矩阵的谱分解* ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)
    的操作，使得
- en: '![CH09_F01_zhuang-ch9-eqs-5x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-5x.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-5x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-5x.png)'
- en: where μ is the mean vector, and Λ = diag(*λ*[1], *λ*[2], ..., *λ*[M]) is a diagonal
    matrix of eigenvalues with eigenvalues arranged in a monotonically decreasing
    order (i.e., *λ*[1] *≥* *λ*[2] *≥* ⋯ *≥* *λ*[M]).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 μ 是均值向量，Λ = diag(*λ*[1], *λ*[2], ..., *λ*[M]) 是一个对角矩阵，其对角线上的特征值按单调递减的顺序排列（即
    *λ*[1] *≥* *λ*[2] *≥* ⋯ *≥* *λ*[M])。
- en: 'Here, the matrix *U* = [*u*[1],*u*[2], ..., *u*[M]] is an *M* × *M* unitary
    matrix where *u*[j] denotes the *j*th eigenvector of the scatter matrix mentioned
    previously. For PCA, we retain the *m* principal components corresponding to the
    *m* highest eigenvalues to obtain the projection matrix *U*[m]. Once the projection
    matrix is found, you can find the reduced dimension feature vector as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，矩阵 *U* = [*u*[1],*u*[2], ..., *u*[M]] 是一个 *M* × *M* 的单位矩阵，其中 *u*[j] 表示之前提到的散布矩阵的第
    *j* 个特征向量。对于 PCA，我们保留与 *m* 个最高特征值相对应的 *m* 个主成分以获得投影矩阵 *U*[m]。一旦找到投影矩阵，就可以找到以下降低维度的特征向量：
- en: '![CH09_F01_zhuang-ch9-eqs-8x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-8x.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-8x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-8x.png)'
- en: As you can see, the parameter *m* determines to what extent the signal power
    is retained after dimensionality reduction.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，参数 *m* 决定了在降维后信号功率保留的程度。
- en: Decomposing eigenvalues
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值分解
- en: 'In linear algebra, eigenvalue decomposition (EVD) is the factorization of a
    matrix into a canonical form, whereby the matrix is represented in terms of its
    eigenvalues and eigenvectors. Basically, it aims to find eigenvalues (called *λ*’s)
    and eigenvectors (called *u*’s) of a matrix *A* that satisfies the equation: *Au*
    = *λu*.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，特征值分解（EVD）是将矩阵分解为标准形式的过程，其中矩阵用其特征值和特征向量表示。基本上，它旨在找到满足方程 *Au* = *λu* 的矩阵
    *A* 的特征值（称为 *λ*）和特征向量（称为 *u*）。
- en: In general, EVD can be performed in a variety of ways. Some methods, such as
    the QR algorithm, find all the eigenvectors and eigenvalues at once. However,
    PCA reduces the dimensions, so not all the eigenvectors are needed. For this reason,
    we can rely on methods that only find a subset of the eigenvalues and eigenvectors
    to avoid the extra computation associated with finding unneeded eigenvectors.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，特征值分解（EVD）可以以多种方式执行。一些方法，如 QR 算法，一次找到所有特征向量和特征值。然而，PCA 减少了维度，因此不需要所有特征向量。因此，我们可以依赖仅找到特征值和特征向量子集的方法，以避免与找到不必要的特征向量相关的额外计算。
- en: To that end, one of the most notable algorithms for EVD is the power iteration
    method. This method finds the dominant eigenvalue (the largest value) with its
    associated eigenvector. A matrix deflation method can be used afterward to remove
    the effect of the already found dominant eigenvalue while leaving the remaining
    eigenvalues unchanged. We can find the required number of eigenvectors by repeatedly
    applying the power iteration method and matrix deflation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，最著名的特征值分解算法之一是幂迭代法。这种方法找到主导特征值（最大的值）及其相关的特征向量。之后可以使用矩阵消去法来消除已找到的主导特征值的影响，同时保持剩余特征值不变。我们可以通过反复应用幂迭代法和矩阵消去法来找到所需的特征向量数量。
- en: 9.2.2 Other dimensionality reduction methods
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 其他降维方法
- en: Now that you know how PCA works, by projecting the data on principal components,
    let’s look at a few other approaches that can be used for different ML classification
    tasks. Because the same dataset could be utilized in different classification
    problems, let’s define a classification problem as *c* which has a unique set
    of labels associated with the corresponding training samples *x*[i]. Without loss
    of generality, the dataset could be utilized for a single utility target *U* and
    a single privacy target *P*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了主成分分析（PCA）的工作原理，通过将数据投影到主成分上，让我们看看一些可以用于不同机器学习分类任务的其他方法。因为相同的数据库可以在不同的分类问题中使用，所以让我们定义一个分类问题为
    *c*，它与相应的训练样本 *x*[i] 相关的唯一标签集相关。不失一般性，该数据库可以用于单个效用目标 *U* 和单个隐私目标 *P*。
- en: For example, suppose an ML algorithm is trained with a dataset of face images.
    The utility target is to identify the faces, whereas the privacy target is to
    identify the person. In this case, each training sample *x*[i] has two labels
    ∈ {1, 2, ..., *L*^u} and ∈ {1, 2, ..., *L*^p}. *L*^u and *L*^p are the numbers
    of classes of the utility target and the privacy target, respectively.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个机器学习算法使用人脸图像数据集进行训练。效用目标是识别人脸，而隐私目标是识别个人。在这种情况下，每个训练样本 *x*[i] 有两个标签 ∈
    {1, 2, ..., *L*^u} 和 ∈ {1, 2, ..., *L*^p}。*L*^u 和 *L*^p 分别是效用目标和隐私目标的类别数量。
- en: Based on Fisher’s linear discriminant analysis [1], [2], given a classification
    problem, the within-class scatter matrix of its training samples contains most
    of the *noise information*, while the between-class scatter matrix of its training
    samples contains most of the *signal information*. We can define the within-class
    scatter matrix and the between-class scatter matrix for the utility target as
    follows,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于费舍尔线性判别分析 [1]，[2]，给定一个分类问题，其训练样本的类内散布矩阵包含大部分 *噪声信息*，而其训练样本的类间散布矩阵包含大部分 *信号信息*。我们可以按以下方式定义效用目标的类内散布矩阵和类间散布矩阵，
- en: '![CH09_F01_zhuang-ch9-eqs-12x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-12x.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-12x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-12x.png)'
- en: '![CH09_F01_zhuang-ch9-eqs-13x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-13x.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-13x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-13x.png)'
- en: where ![CH09_F01_zhuang-ch9-eqs-14x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-14x.png)
    is the mean vector of all training samples belonging to class *l*, and N^u[l]
    is the number of training samples belonging to class *l* of the utility target.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![CH09_F01_zhuang-ch9-eqs-14x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-14x.png)
    是属于类别 *l* 的所有训练样本的均值向量，而 N^u[l] 是属于类别 *l* 的效用目标训练样本的数量。
- en: Similarly, for the privacy target, the within-class scatter matrix and the between-class
    scatter matrix are defined as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于隐私目标，类内散布矩阵和类间散布矩阵被定义为
- en: '![CH09_F01_zhuang-ch9-eqs-16x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-16x.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-16x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-16x.png)'
- en: '![CH09_F01_zhuang-ch9-eqs-17x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-17x.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-17x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-17x.png)'
- en: 'Suppose we let *W* be a *K* × *M* projection matrix, in which *K* < M. Given
    a testing sample *x*, ![CH09_F01_zhuang-ch9-eqs-18x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-18x.png)∙
    *W* is its subspace projection. The framework that we are going to explore here
    combines the advantages of two eigenvalue decomposition-based dimensionality reduction
    (DR) techniques: DCA (utility-driven projection) [3] and MDR (privacy emphasized
    projection) [4]. Let’s quickly look into those:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们让 *W* 是一个 *K* × *M* 投影矩阵，其中 *K* < M。给定一个测试样本 *x*，![CH09_F01_zhuang-ch9-eqs-18x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-18x.png)∙
    *W* 是其在子空间中的投影。我们将要探讨的框架结合了两种基于特征值分解的降维（DR）技术的优点：DCA（效用驱动投影）[3] 和 MDR（隐私强调投影）[4]。让我们快速了解一下这些：
- en: '*Discriminant component analysis (DCA)*—DCA involves searching for the projection
    matrix *W* ∈ *R*^(M×K),'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*判别成分分析（DCA）*——DCA 涉及搜索投影矩阵 *W* ∈ *R*^(M×K),'
- en: '![CH09_F01_zhuang-ch9-eqs-20x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-20x.png)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-20x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-20x.png)'
- en: where *det*(.) is the determinant operator, *ρI* is a small regularization term
    added for numerical stability, and ![CH09_F01_zhuang-ch9-eqs-21x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-21x.png).
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 *det*(.) 是行列式算子，*ρI* 是为了数值稳定性而添加的小正则化项，![CH09_F01_zhuang-ch9-eqs-21x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-21x.png)。
- en: The optimal solution to this problem can be derived from the first *K* principal
    generalized eigenvectors of the matrix pencil ![CH09_F01_zhuang-ch9-eqs-22x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-22x.png).
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该问题的最优解可以从矩阵 pencil 的前 *K* 个主广义特征向量 ![CH09_F01_zhuang-ch9-eqs-22x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-22x.png)
    中导出。
- en: '*Multiclass discriminant ratio* (MDR)—MDR considers both the utility target
    and the privacy target, which is defined as'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多类判别比*（MDR）——MDR 考虑了效用目标和隐私目标，定义为'
- en: '![CH09_F01_zhuang-ch9-eqs-23x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-23x.png)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F01_zhuang-ch9-eqs-23x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-23x.png)'
- en: where ρI is a small regularization term added for numerical stability. The optimal
    solution to MDR can be derived from the first *K* principal generalized eigenvectors
    of the matrix pencil ![CH09_F01_zhuang-ch9-eqs-24x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-24x.png).
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 ρI 是为了数值稳定性而添加的小正则化项。MDR 的最优解可以从矩阵 pencil 的前 *K* 个主广义特征向量 ![CH09_F01_zhuang-ch9-eqs-24x](../../OEBPS/Images/CH09_F01_zhuang-ch9-eqs-24x.png)
    中导出。
- en: With those basics and mathematical formulations, let’s look at how we can implement
    CP techniques in Python.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基本原理和数学公式的基础上，让我们看看如何在 Python 中实现 CP 技术。
- en: 9.3 Using compressive privacy for ML applications
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 使用压缩隐私进行机器学习应用
- en: So far we’ve discussed the theoretical background of different CP mechanisms.
    Let’s implement these techniques in a real-world face-recognition application.
    Face recognition has been a problem of interest in machine learning (ML) and signal
    processing for over a decade due to its various use cases ranging from simple
    online image searches to surveillance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了不同 CP 机制的理论背景。让我们将这些技术应用于实际的实时人脸识别应用中。由于其在从简单的在线图像搜索到监控等各个领域的广泛应用，人脸识别一直是机器学习（ML）和信号处理领域的一个研究热点。
- en: 'With the current privacy requirements, a real-world face-recognition application
    has to ensure that there won’t be any privacy leaks from the data itself. Let’s
    investigate a couple of different CP methods to see how we can ensure both the
    utility of the face recognition application and the privacy of the reconstructed
    image. We’ll need to compress the face image before submitting it to the facial-recognition
    application such that the application can still identify the face. However, someone
    else should not be able to distinguish the image simply by looking at it or be
    able to reconstruct the original image. For our experiments, we will employ three
    different face datasets. The source code for these hands-on experiments is available
    for download at the book’s GitHub repository: [https://github.com/nogrady/PPML/tree/main/Ch9](https://github.com/nogrady/PPML/tree/main/Ch9).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的隐私要求下，一个现实世界的人脸识别应用必须确保数据本身不会出现任何隐私泄露。让我们研究几种不同的 CP 方法，看看我们如何确保人脸识别应用的效用和重建图像的隐私。在提交给人脸识别应用之前，我们需要压缩人脸图像，以便应用仍然能够识别人脸。然而，其他人不应该仅通过观察就能区分图像，或者能够重建原始图像。在我们的实验中，我们将使用三个不同的面部数据集。这些动手实验的源代码可以在本书的
    GitHub 仓库中下载：[https://github.com/nogrady/PPML/tree/main/Ch9](https://github.com/nogrady/PPML/tree/main/Ch9)。
- en: '*Yale Face Database*—The Yale Face Database contains 165 grayscale images of
    15 individuals and is publicly available from [http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.xhtml](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.xhtml).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*耶鲁人脸数据库*——耶鲁人脸数据库包含 15 个个体的 165 张灰度图像，并可以从 [http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.xhtml](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.xhtml)
    公开获取。'
- en: '*Olivetti faces dataset*—This dataset contains 400 grayscale face images of
    40 different people taken between April 1992 and April 1994 at AT&T Laboratories
    Cambridge. The dataset can be downloaded from [https://scikit-learn.org/0.19/datasets/olivetti_faces.xhtml](https://scikit-learn.org/0.19/datasets/olivetti_faces.xhtml).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*奥利维蒂人脸数据集*—这个数据集包含 1992 年 4 月至 1994 年 4 月在 AT&T 实验室剑桥拍摄的 40 位不同人物的 400 张灰度人脸图像。数据集可以从
    [https://scikit-learn.org/0.19/datasets/olivetti_faces.xhtml](https://scikit-learn.org/0.19/datasets/olivetti_faces.xhtml)
    下载。'
- en: '*Glasses dataset*—We derived this dataset from a combination of the Yale and
    Olivetti datasets by selectively choosing the subjects who are wearing glasses.
    In this case, the dataset contains 300 images. Half of the images are of people
    with glasses, and the other half are people without glasses.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*眼镜数据集*—我们通过选择性地从耶鲁和奥利维蒂数据集中选择戴眼镜的受试者来推导出这个数据集。在这种情况下，数据集包含 300 张图像。其中一半的图像是戴眼镜的人，另一半是不戴眼镜的人。'
- en: For the Yale and Olivetti datasets, the task of recognizing individuals from
    the face images will be our utility (our target application), whereas reconstructing
    the image will be our privacy target. One use case scenario would be an “entity”
    that wants to build a face recognition algorithm using sensitive face images provided
    by users for training. But in this scenario, people are usually hesitant to share
    their face images unless they are changed so that no one can recognize the person
    in the image.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于耶鲁和奥利维蒂数据集，从人脸图像中识别个人的任务将是我们的应用（我们的目标应用），而图像的重构将是我们的隐私目标。一个用例场景是一个“实体”想要使用用户提供的用于训练的敏感人脸图像构建人脸识别算法。但在这种场景中，人们通常不愿意分享他们的面部图像，除非它们被改变到没有人能识别图像中的人。
- en: For simplicity, we can assume that the entity wishing to build the face recognition
    classifier is a service operator, but this operator could be malicious and could
    be trying to reconstruct the original images from the training data received from
    users.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们可以假设想要构建人脸识别分类器的实体是一个服务运营商，但这个运营商可能是恶意的，并试图从从用户那里接收到的训练数据中重建原始图像。
- en: For the glasses dataset, we have two different classes (with and without glasses).
    The utility of our application will be to identify whether the person is wearing
    a pair of glasses or not; the privacy target will again be the reconstruction
    of the image.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于眼镜数据集，我们有两种不同的类别（戴眼镜和不戴眼镜）。我们应用的价值将在于识别某人是否佩戴了一副眼镜；隐私目标将是图像的重构。
- en: 9.3.1 Implementing compressive privacy
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 实现压缩隐私
- en: Now let’s get to work! We’ll use the Yale dataset and see how we can use CP
    techniques in our facial recognition application. First, you’ll need to load some
    modules and the dataset. Note that discriminant_analysis.py is a module that we
    developed for PCA and DCA methods. Refer to the source code for more information.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始工作！我们将使用耶鲁数据集，看看我们如何在人脸识别应用中使用CP技术。首先，你需要加载一些模块和数据集。请注意，discriminant_analysis.py
    是我们为 PCA 和 DCA 方法开发的模块。更多信息请参考源代码。
- en: 'Note You can just use the cleaned version of the Yale dataset, which is available
    in the code repository: [https://github.com/nogrady/PPML/tree/main/Ch9](https://github.com/nogrady/PPML/tree/main/Ch9).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可以直接使用耶鲁数据集的清洁版本，它可在代码仓库中找到：[https://github.com/nogrady/PPML/tree/main/Ch9](https://github.com/nogrady/PPML/tree/main/Ch9)。
- en: Listing 9.1 Loading modules and the dataset
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1 加载模块和数据集
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Load the dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据集。
- en: 'You can run the following command to see what the dataset looks like:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行以下命令来查看数据集的外观：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see from the output, the dataset contains 165 images, where each
    image is 64 x 64 (which is why we get 4,096).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从输出中看到的，数据集包含 165 张图像，每张图像为 64 x 64（这就是为什么我们得到 4,096）。
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now let’s review a couple of images in the dataset. Because the dataset contains
    165 different images, you can run the code in the following listing to pick four
    different images randomly. We are using the randrange function to randomly choose
    images. To show the images in the output, we are using the displayImage routine
    and subplot with four columns.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾数据集中的几幅图像。由于数据集包含 165 张不同的图像，你可以运行以下列表中的代码来随机选择四幅不同的图像。我们使用 randrange
    函数随机选择图像。为了在输出中显示图像，我们使用 displayImage 例程和四列的 subplot。
- en: Listing 9.2 Loading a few images from the dataset
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.2 从数据集中加载一些图像
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Define a function to show the images.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个显示图像的函数。
- en: ❷ Randomly select four different images from the dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从数据集中随机选择四幅不同的图像。
- en: The output will look something like figure 9.2, though you will get different
    random face images.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于图9.2，尽管你将得到不同的随机人脸图像。
- en: '![CH09_F02_Zhuang](../../OEBPS/Images/CH09_F02_Zhuang.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F02_Zhuang](../../OEBPS/Images/CH09_F02_Zhuang.png)'
- en: Figure 9.2 A set of sample images from Yale dataset
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 来自耶鲁数据集的一组样本图像
- en: Now that you know what kind of data we are dealing with, let’s implement different
    CP technologies with this dataset. We are particularly looking at implementing
    principal component analysis (PCA) and discriminant component analysis (DCA) with
    this dataset. We have developed and wrapped the core functionality of PCA and
    DCA into the discriminant_analysis.py class, so you can simply call it and initialize
    the methods.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了我们正在处理的数据类型，让我们用这个数据集实现不同的CP技术。我们特别关注在这个数据集上实现主成分分析（PCA）和判别成分分析（DCA）。我们已经开发和封装了PCA和DCA的核心功能到discriminant_analysis.py类中，所以你可以简单地调用它并初始化方法。
- en: 'Note Discriminant_analysis.py is a class file that we developed to cover the
    PCA and DCA methods. You can refer to the source code of the file for more information:
    [https://github.com/nogrady/PPML/blob/main/Ch9/discriminant_analysis.py](https://github.com/nogrady/PPML/blob/main/Ch9/discriminant_analysis.py).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Discriminant_analysis.py是我们开发的一个类文件，用于涵盖PCA和DCA方法。你可以参考该文件的源代码以获取更多信息：[https://github.com/nogrady/PPML/blob/main/Ch9/discriminant_analysis.py](https://github.com/nogrady/PPML/blob/main/Ch9/discriminant_analysis.py)。
- en: 'The DCA object is initialized with two parameters: rho and rho_p. You’ll recall
    that we discussed these parameters (ρ and updated ρ) in section 9.2.2\. The code
    we use will first define and initialize these values along with a set of dimensions
    that we’ll need to project the image data in order to see the results.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DCA对象使用两个参数初始化：rho和rho_p。你将记得我们在第9.2.2节中讨论了这些参数（ρ和更新的ρ）。我们使用的代码将首先定义和初始化这些值，以及一组我们将需要用于投影图像数据以查看结果的维度。
- en: We’ll start by setting rho = 10 and rho_p = -0.05, but later in this chapter
    you’ll learn about the important properties of these parameters and how different
    values will affect privacy. For now we’ll just focus on the training and testing
    part of the algorithm.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先设置rho = 10和rho_p = -0.05，但在此章的后面部分，你将了解到这些参数的重要属性以及不同值如何影响隐私。现在我们只关注算法的训练和测试部分。
- en: Setting ntests = 10 means that we will do the same experiment 10 times to average
    the final result. You can set this value to any number, but the higher, the better.
    The dims array defines the number of dimensions that we will use. As you can see,
    we will be starting with few dimensions, such as 2, and then move to more dimensions,
    such as 4,096\. Again, you can try setting your own values for this.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 设置ntests = 10意味着我们将进行相同的实验10次以平均最终结果。你可以将此值设置为任何数字，但越高越好。dims数组定义了我们将使用的维度数。正如你所见，我们将从少数维度开始，例如2，然后移动到更多维度，例如4096。再次提醒，你可以尝试为这个值设置自己的值。
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once these values are defined, the mypca and mydca objects are trained with
    the following code, after the dataset is split into train and test sets. Xtr is
    the training data matrix (training set), while ytr is the training label vector.
    The fit command learns some quantities from the data, particularly the principal
    components and the explained variance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了这些值，mypca和mydca对象将在数据集分为训练集和测试集之后，使用以下代码进行训练。Xtr是训练数据矩阵（训练集），而ytr是训练标签向量。fit命令从数据中学习一些量，特别是主成分和解释方差。
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Thereafter, the projections of the data can be obtained as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，可以按以下方式获得数据的投影：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For each dimension that we are interested in (2, 5, 8, 10, 14, 39, 1000, etc.),
    the projection matrix D can be found, and then the image data can be reconstructed
    as Xrec:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们感兴趣的每个维度（2, 5, 8, 10, 14, 39, 1000等），可以找到投影矩阵D，然后可以将图像数据重构为Xrec：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Reconstruct the data using PCA technique.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用PCA技术重构数据。
- en: ❷ Reconstruct the data using DCA technique.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用DCA技术重构数据。
- en: When we put all this together, the complete code looks like the following listing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有这些放在一起时，完整的代码如下所示。
- en: Listing 9.3 The complete code to reconstruct an image and calculate the accuracy
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 重构图像和计算准确性的完整代码
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Precompute all the components.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预计算所有组件。
- en: ❷ Test the accuracy of PCA.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 测试PCA的准确性。
- en: ❸ Test the reconstruction error of PCA.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 测试PCA的重构误差。
- en: ❹ Test the accuracy of DCA.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 测试DCA的准确性。
- en: ❺ Test the reconstruction error of DCA.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 测试DCA的重构误差。
- en: ❻ Show the reconstructed image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 显示重构的图像。
- en: ❼ Save the accuracy and reconstruction error of each cycle as a text file.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将每个周期的准确性和重建误差保存为文本文件。
- en: As you can see in listing 9.3, we are running the PCA and DCA reconstructions
    10 times (remember we set ntests = 10), and for each case it randomly splits the
    dataset for training and testing. In the end, we are calculating the accuracy
    and reconstruction error for each dimension for both PCA and DCA. That will let
    us evaluate how accurately the compressed image can be reconstructed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表9.3所示，我们正在运行PCA和DCA重建10次（记住我们设置了ntests = 10），并且对于每种情况，它都会随机分割数据集进行训练和测试。最后，我们计算每个维度的PCA和DCA的准确性和重建误差。这将使我们能够评估压缩图像重建的准确性。
- en: When you run the code, the results may look like the following. The complete
    output is too long to include here—we’ve just included the first few lines of
    the output and the reconstructed images of the first two runs in the loop.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行代码时，结果可能看起来像以下这样。完整的输出太长，无法在此处包含——我们只包含了输出前几行和循环中前两次运行的重构图像。
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![CH09_F03_UN02_Zhuang](../../OEBPS/Images/CH09_F03_UN02_Zhuang.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F03_UN02_Zhuang](../../OEBPS/Images/CH09_F03_UN02_Zhuang.png)'
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you look closely at the reproduced images in the output, compared with the
    originals, you’ll see that when the reduced dimensions are low, it is difficult
    to identify the person. For instance, if you compare the original image and the
    version with 5 (either PCA or DCA), it is pretty hard to identify the person from
    the compressed image. On the other hand, when dims = 160, you can see that the
    reconstructed image is getting better. That means we are preserving the privacy
    of sensitive data by reducing the dimensions. As you can see, in this case, DCA
    works much better than PCA for the dimensions 5 to 50, making it close to the
    original but still hard to recognize.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看输出中的重现图像，与原始图像相比，你会看到当降低维度时，很难识别这个人。例如，如果你比较原始图像和维度为5（无论是PCA还是DCA）的版本，从压缩图像中识别这个人相当困难。另一方面，当dims
    = 160时，你可以看到重建图像正在变得更好。这意味着我们通过减少维度来保护敏感数据的隐私。正如你所看到的，在这种情况下，DCA对于5到50的维度比PCA表现得更好，使其接近原始图像，但仍难以识别。
- en: 9.3.2 The accuracy of the utility task
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 效用任务的准确性
- en: You now know that when the dimensions are reduced, privacy is improved. But
    what about the accuracy of the utility task? If we cannot get considerable accuracy
    for the utility task, there is no point in using CP techniques in this scenario.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道，当维度减少时，隐私性会提高。但效用任务的准确性如何呢？如果我们无法为效用任务获得相当高的准确性，那么在这种情况下使用CP技术就没有意义了。
- en: To examine how the accuracy of the utility task changes along with the reduced
    dimensions, we can simply increase the value of the ntests variable to the maximum
    value (which is 165 because we have 165 records in the dataset) and average the
    utility accuracy results for each dimension.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查效用任务的准确性如何随着维度的减少而变化，我们可以简单地增加ntests变量的值到最大值（因为数据集中有165条记录，所以最大值是165），并对每个维度的效用准确性结果进行平均。
- en: Figure 9.3 summarizes the accuracy results. If you look carefully at the DCA
    results, you will notice that after the dimension becomes 14, its accuracy saturates
    somewhere around 91%. Now look at the face image where the dimension is 14 for
    DCA. You will notice that it is hard to identify the person but the graph shows
    that the accuracy of the utility task (the facial recognition application in this
    case) is still high. That is how CP works, ensuring the balance between privacy
    and utility.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3总结了准确性结果。如果你仔细查看DCA的结果，你会注意到，当维度变为14时，其准确性在大约91%左右饱和。现在看看维度为14的DCA的脸部图像。你会注意到，很难识别这个人，但图表显示，效用任务（在这种情况下是面部识别应用）的准确性仍然很高。这就是CP如何工作，确保隐私和效用之间的平衡。
- en: '![CH09_F03_Zhuang](../../OEBPS/Images/CH09_F03_Zhuang.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F03_Zhuang](../../OEBPS/Images/CH09_F03_Zhuang.png)'
- en: Figure 9.3 A comparison of the accuracy of the reconstructed images for different
    dimension settings
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 不同维度设置下重建图像准确性的比较
- en: Now that we’ve investigated the accuracy of the utility, let’s see how hard
    it is for someone else to reconstruct the compressed image. This can be measured
    by the reconstruction error. To determine that, we need to go through the whole
    dataset, not just a couple of records. Thus we’ll modify the code a little bit,
    as shown in the following listing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经研究了效用函数的准确性，让我们看看其他人重建压缩图像有多困难。这可以通过重构误差来衡量。为了确定这一点，我们需要遍历整个数据集，而不仅仅是几条记录。因此，我们将稍微修改一下代码，如下所示。
- en: Listing 9.4 Calculating the average reconstruction error
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 计算平均重构误差
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Precompute all the components.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预计算所有组件。
- en: ❷ Test the reconstruction error of PCA.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 测试 PCA 的重构误差。
- en: ❸ Test the reconstruction error of DCA.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 测试 DCA 的重构误差。
- en: ❹ Save the results to a text file.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将结果保存到文本文件中。
- en: Figure 9.4 shows the results of listing 9.4 plotted against different dimensions.
    As you can see, when the dimensions are lower, it is hard for someone to reconstruct
    the image accurately. For example, the DCA datapoint at dimension 14 has a very
    high value for the reconstruction error, making it very hard for someone to reconstruct
    the image. However, as you saw before, it still provides good accuracy for the
    face recognition task. This is the whole idea of using CP technologies for ML
    applications.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 显示了列表 9.4 的结果与不同维度对比。如图所示，当维度较低时，其他人很难准确重建图像。例如，在维度 14 的 DCA 数据点具有很高的重构误差值，这使得其他人很难重建图像。然而，正如你之前看到的，它仍然为面部识别任务提供了良好的准确性。这就是使用
    CP 技术应用于机器学习应用的整体思路。
- en: '![CH09_F04_Zhuang](../../OEBPS/Images/CH09_F04_Zhuang.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F04_Zhuang](../../OEBPS/Images/CH09_F04_Zhuang.png)'
- en: Figure 9.4 Comparing the average reconstruction error for different dimension
    settings
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 比较不同维度设置的平均重构误差
- en: 9.3.3 The effect of ρ' in DCA for privacy and utility
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 DCA 中 ρ' 对隐私和效用的影响
- en: We have experimented with how dimension reduction can help determine privacy.
    When the number of dimensions is lower, more information is lost, yielding better
    privacy. But in DCA we have another parameter, *ρ*'. So far we have only changed
    the number of dimensions and kept *ρ*' as rho_p = -0.05. Now we will look at the
    importance of this parameter in determining the level of privacy by keeping the
    number of dimensions fixed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实验了如何通过降维来确定隐私。当维度数量较低时，损失的信息更多，从而提供更好的隐私。但在 DCA 中，我们还有一个参数 *ρ*。到目前为止，我们只改变了维度的数量，并将
    *ρ* 保持为 rho_p = -0.05。现在我们将通过保持维度数量不变来观察这个参数在确定隐私水平方面的重要性。
- en: 'The following listing is almost the same as listing 9.3, except that instead
    of going through several dimensions, now we’ll change the rho_p parameter to different
    values: [-0.05, -0.01, -0.001, 0.0, 0.001, 0.01, 0.05]. We’ll fix the dimensions
    at 160 (you can also try this with a different value).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表几乎与列表 9.3 相同，只是现在我们将改变 rho_p 参数为不同的值：[-0.05, -0.01, -0.001, 0.0, 0.001,
    0.01, 0.05]。我们将维度固定在 160（你也可以尝试不同的值）。
- en: Listing 9.5 Changing *ρ*' in DCA and determining the effect on privacy
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5 在 DCA 中改变 *ρ* 并确定其对隐私的影响
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The results are shown in figure 9.5\. The top-left image is the original, and
    the rest of the images are produced with different rho_p values. As you can see,
    when the rho_p value changes from -0.05 to 0.05, the resultant image changes significantly,
    making it hard to identify. Thus, we can deduce that *ρ*' is also an important
    parameter in determining the level of privacy when DCA is used. As you can see,
    positive values of *ρ*' provide a much greater level of privacy than negative
    values.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在图 9.5 中。左上角的图像是原始图像，其余图像是用不同的 rho_p 值生成的。如图所示，当 rho_p 值从 -0.05 变化到 0.05
    时，生成的图像发生了显著变化，难以识别。因此，我们可以推断出 *ρ* 也是在 DCA 使用时确定隐私水平的一个重要参数。如图所示，*ρ* 的正值比负值提供了更高的隐私水平。
- en: '![CH09_F05_Zhuang](../../OEBPS/Images/CH09_F05_Zhuang.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F05_Zhuang](../../OEBPS/Images/CH09_F05_Zhuang.png)'
- en: Figure 9.5 Effect of *ρ*' on privacy with dimension fixed at 160 on the Yale
    dataset
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 在耶鲁数据集上固定维度为 160 时 *ρ* 对隐私的影响
- en: We have now explored the possibility of integrating CP technologies into ML
    applications. You’ve seen that with a minimum number of dimensions, maximum face
    recognition accuracy is obtainable, and the privacy of the reconstructed image
    is still preserved. In the next section we’ll extend the discussion further with
    a case study on privacy preservation on horizontally partitioned data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经探讨了将CP技术集成到机器学习应用中的可能性。您已经看到，在最小数量的维度下，可以获得最大的面部识别准确率，同时重建图像的隐私性仍然得到保护。在下一节中，我们将通过一个关于水平分区数据隐私保护案例研究来进一步扩展讨论。
- en: First, though, here are a few exercises you can explore to see the effect of
    *ρ*' on improving accuracy and to play with the different datasets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这里有一些练习您可以探索，以了解*ρ*对提高准确性的影响，并尝试不同的数据集。
- en: Exercise 1
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1
- en: 'You now know that positive values of *ρ*'' help to improve privacy. How about
    the accuracy of the face recognition task? Do positive values of *ρ*'' help accuracy
    as well? Change the code in listing 9.3 and observe the utility accuracy and the
    average reconstruction error for each different *ρ*'' value. (Hint: You can do
    this by adding another for loop that goes through the different rho_p values.)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在知道*ρ*的正值有助于提高隐私。那么，面部识别任务的准确性呢？*ρ*的正值是否也有助于提高准确性？更改列表9.3中的代码，观察每个不同的*ρ*值的有效性和平均重建误差。（提示：您可以通过添加另一个循环来实现，该循环遍历不同的rho_p值。）
- en: 'Solution: In terms of the accuracy of the face recognition task, the value
    of *ρ*'' has no significant effect. Try to plot *ρ*'' against the accuracy, and
    you will see this clearly.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：在面部识别任务的准确性方面，*ρ*的值没有显著影响。尝试绘制*ρ*与准确性的关系图，您将清楚地看到这一点。
- en: Exercise 2
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2
- en: 'All the experiments we have explored so far were based on the Yale dataset.
    Switch to the Olivetti dataset and rerun all the experiments to see whether the
    observations and patterns are similar. (Hint: This is quite straightforward. You
    just need to change the dataset name and the location.)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止探索的所有实验都是基于耶鲁数据集的。切换到奥利维蒂数据集，重新运行所有实验，看看观察到的模式和现象是否相似。（提示：这相当直接。您只需更改数据集名称和位置。）
- en: 'Solution: This solution is provided in the book’s source code repository.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：此解决方案在本书的源代码库中提供。
- en: Exercise 3
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3
- en: Now change the dataset to the glasses dataset. In this case, remember that the
    utility of the application will be identifying the person whether they wear a
    pair of glasses or not. The privacy target is still the reconstruction of the
    image. Change the code and see how PCA and DCA help here.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将数据集更改为眼镜数据集。在这种情况下，请记住，应用程序的效用是确定一个人是否戴眼镜。隐私目标是图像的重建。更改代码，看看PCA和DCA在这里如何帮助。
- en: 'Solution: The solution is provided in the book’s source code repository.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：解决方案在本书的源代码库中提供。
- en: '9.4 Case study: Privacy-preserving PCA and DCA on horizontally partitioned
    data'
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 案例研究：水平分区数据上的隐私保护PCA和DCA
- en: 'As you already know, machine learning (ML) can be classified into two different
    categories or tasks: supervised tasks like regression and classification, and
    unsupervised tasks like clustering. In practice, these techniques are widely used
    in many different applications such as identity and access management, detecting
    fraudulent credit card transactions, building clinical decision support systems,
    and so on. These applications often use personal data, such as healthcare records,
    financial data, and the like, during the training and testing phases of ML.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，机器学习（ML）可以分为两类不同的任务或类别：监督任务，如回归和分类，以及无监督任务，如聚类。在实践中，这些技术被广泛应用于许多不同的应用中，例如身份和访问管理、检测欺诈信用卡交易、构建临床决策支持系统等。这些应用在机器学习的训练和测试阶段通常会使用个人数据，例如医疗记录、财务数据等。
- en: 'While there are many different approaches, dimensionality reduction (DR) is
    an important ML tool that can be used to overcome different types of problems:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多不同的方法，但降维（DR）是机器学习中的一个重要工具，可以用来克服不同类型的问题：
- en: Overfitting when the feature dimensions far exceed the number of training samples
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征维度远超过训练样本数量时的过拟合
- en: Performance degradation due to suboptimal search
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于搜索不优导致的性能下降
- en: Higher computational costs and power consumption resulting from high dimensionality
    in the feature space
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于特征空间中的高维性导致的高计算成本和功耗
- en: 'We’ve discussed a couple of DR methods in this chapter already: PCA and DCA.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中已经讨论了几种DR方法：PCA和DCA。
- en: As you’ll recall, PCA aims to project the data on the principal components with
    the highest variance, thus preserving most of the information while reducing the
    data dimensions. As you can see in figure 9.6 (a), most of the variability happens
    along with the first principal component (shown as PC-1). Hence, projecting all
    the points on that new axis could reduce the dimensions without sacrificing much
    data variability.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所回忆的那样，主成分分析（PCA）旨在将数据投影到具有最高方差的主成分上，从而在降低数据维度的同时保留大部分信息。如图9.6（a）所示，大部分变异性都沿着第一个主成分（表示为PC-1）发生。因此，将所有点投影到这个新轴上可以在不牺牲太多数据变异性的情况下降低维度。
- en: Compared to PCA, DCA is designed for supervised learning applications. The objective
    of DCA is not recoverability (for reducing the reconstruction error like PCA)
    but rather is improving the discriminant power of the learned classifiers so that
    they can effectively discriminate between different classes. Figure 9.6 (b) shows
    an example of a supervised learning problem where PCA would choose PC-1 as the
    principal component on which the data would be projected, because PC-1 is the
    direction of most variability. In contrast, DCA would choose PC-2, since it provides
    the highest discriminant power.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与PCA相比，DCA是为监督学习应用设计的。DCA的目标不是可恢复性（如PCA用于减少重建误差），而是提高学习分类器的判别能力，以便它们可以有效地区分不同的类别。图9.6（b）显示了一个监督学习问题的例子，其中PCA会选择PC-1作为数据投影的主成分，因为PC-1是最大变异性方向。相比之下，DCA会选择PC-2，因为它提供了最高的判别能力。
- en: '![CH09_F06_Zhuang](../../OEBPS/Images/CH09_F06_Zhuang.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F06_Zhuang](../../OEBPS/Images/CH09_F06_Zhuang.png)'
- en: Figure 9.6 How dimensionality reduction works
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 主成分分析（PCA）如何工作
- en: Traditionally, PCA and DCA have been performed by gathering data in a centralized
    location, but in many applications (such as continuous authentication [5]), the
    data is distributed across multiple data owners. In such cases, collaborative
    learning is done on a joint dataset formed of samples held by different data owners,
    where each sample contains the same attributes (features). Such data is described
    as *horizontally partitioned*, since the data is represented as rows of similar
    features (columns), and each data owner holds a different set of rows in the joint
    data matrix.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，PCA和DCA是通过在集中位置收集数据来执行的，但在许多应用中（如连续认证[5]），数据分布在多个数据所有者之间。在这种情况下，协作学习是在由不同数据所有者持有的样本组成的联合数据集上进行的，其中每个样本包含相同的属性（特征）。这种数据被称为*水平分割*，因为数据以类似特征的行（列）表示，并且每个数据所有者在联合数据矩阵中持有不同的一组行。
- en: In this situation, suppose a central entity (the data user) wants to compute
    a PCA (or DCA) projection matrix using the data distributed across multiple data
    owners in a privacy-preserving way. The data owners could then use the projection
    matrix to reduce the dimensions of their data. Such reduced dimensional data could
    then be used as the input to specific privacy-preserving ML algorithms that perform
    classification or regression.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，假设一个中心实体（数据用户）想要以隐私保护的方式使用分布在多个数据所有者之间的数据来计算PCA（或DCA）投影矩阵。然后，数据所有者可以使用投影矩阵来降低其数据的维度。这种降低维度的数据可以随后用作输入到特定的隐私保护机器学习算法中，这些算法执行分类或回归。
- en: In the case study in this section, we will be looking at the problem of a data
    user computing PCA and DCA projection matrices without compromising the privacy
    of the data owners. When it comes to distributed ML with PCA, privacy was not
    considered at all in early approaches. Later on, different privacy-preserving
    methods were proposed for PCA [6], but when it came to the implementation, they
    required all the data owners to remain online throughout the execution of the
    protocol, which was not very practical. In this case study, we’ll provide a solution
    to this problem by proposing and implementing a practical DR method using PCA
    and DCA in a privacy-preserving way.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的研究案例中，我们将探讨数据用户在计算PCA和DCA投影矩阵时不会损害数据所有者隐私的问题。在早期关于PCA的分布式机器学习研究中，根本未考虑隐私问题。后来，为PCA提出了不同的隐私保护方法[6]，但在实施时，它们要求所有数据所有者在协议执行期间保持在线，这并不太实用。在本案例研究中，我们将通过提出并实现一种使用PCA和DCA的隐私保护方法来解决这个问题。
- en: In contrast to earlier approaches, the protocol that we are about to explore
    does not reveal any intermediate results (such as scatter matrices) and does not
    require data owners to interact with each other or remain online after submitting
    their individual encrypted shares. This newer approach can be utilized as a privacy-preserving
    data preprocessing stage before applying other privacy-preserving ML algorithms
    for classification or regression. Thus, it ensures both privacy and utility while
    applying ML algorithms to private data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期方法相比，我们即将探讨的协议不会泄露任何中间结果（例如散布矩阵），并且不需要数据所有者相互交互或提交他们的个人加密份额后保持在线。这种新的方法可以作为在应用其他隐私保护机器学习算法进行分类或回归之前，隐私保护数据预处理阶段的利用。因此，它确保在将机器学习算法应用于私有数据时既保护隐私又保持效用。
- en: 9.4.1 Achieving privacy preservation on horizontally partitioned data
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 在水平分区数据上实现隐私保护
- en: Before we get into the details, let’s quickly walk through the key things that
    we’ll look at in this case study. Computing projection matrices to facilitate
    distributed privacy-preserving PCA (or DCA) requires computing the scatter matrices
    in a distributed fashion. We will use additive homomorphic encryption to compute
    the scatter matrices.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解细节之前，让我们快速浏览一下在这个案例研究中我们将关注的要点。为了便于分布式隐私保护主成分分析（或DCA），需要以分布式方式计算散布矩阵。我们将使用加法同态加密来计算散布矩阵。
- en: In addition to the data owners (who hold the data) and the data user (who aims
    to compute the projection matrix), we will assume the existence of a third entity,
    a crypto service provider (CSP), that will not collude with the data user. When
    data owners send the data, they need to compute their individual shares, encrypt
    them using homomorphic encryption with the CSP’s public key, and, finally, send
    these shares to the data user, who will aggregate these shares. After that, the
    CSP can build a garbled circuit that performs Eigen-decomposition on the scatter
    matrices computed from the aggregated shares (see the sidebar for a discussion
    of garbled circuits). Neither the data user nor the CSP can see the aggregated
    shares in their cleartext form, so data exchange protocols in this solution do
    not reveal any intermediate values, such as the user shares or scatter matrices.
    Moreover, this approach does not burden data owners with interacting with other
    data owners, and it does not require them to remain online after sending their
    encrypted shares, making these protocols practical.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 除了数据所有者（持有数据）和数据用户（旨在计算投影矩阵）之外，我们还将假设存在一个第三方实体，即加密服务提供商（CSP），它不会与数据用户串通。当数据所有者发送数据时，他们需要计算他们的个人份额，使用CSP的公钥进行同态加密，最后将这些份额发送给数据用户，数据用户将汇总这些份额。之后，CSP可以构建一个执行从汇总份额中计算的散布矩阵的混淆电路（参见侧边栏中对混淆电路的讨论）。数据用户和CSP都无法以明文形式看到汇总份额，因此此解决方案中的数据交换协议不会泄露任何中间值，例如用户份额或散布矩阵。此外，这种方法不会给数据所有者带来与其他数据所有者互动的负担，并且不需要他们在发送加密份额后保持在线，这使得这些协议变得实用。
- en: What is a garbled circuit?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是混淆电路？
- en: A garbled circuit (introduced by Andrew Yao) is a way to encrypt a computation
    such that it reveals the output of the computation but reveals nothing about the
    inputs or any intermediate states or values. The idea is to enable two mistrusting
    parties to communicate securely and execute a computation over their private inputs
    without the presence of a trusted third party.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆电路（由安德鲁·姚提出）是一种加密计算的方法，使得它揭示了计算的输出，但不会揭示关于输入或任何中间状态或值的任何信息。其想法是使两个不信任的当事人能够安全地通信，并在他们的私有输入上执行计算，而不需要可信的第三方。
- en: 'Let’s explore this in a simple example. Suppose that Alice has a private input
    *x* and Bob has a private input *y*. They agree on some computation function *f*
    (called the circuit) and both want to learn *f*(*x*, *y*) but do not want the
    other party to learn anything more than that, such as intermediate values or the
    other’s input. This is what they will do:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来探讨这个问题。假设爱丽丝有一个私有输入 *x*，鲍勃有一个私有输入 *y*。他们同意某个计算函数 *f*（称为电路），并且双方都想学习
    *f*(*x*, *y*)，但不希望对方了解任何更多内容，例如中间值或对方的输入。他们将这样做：
- en: Both parties need to agree on how to express *f*. Then Alice converts the circuit
    *f* to its garbled version and sends that to Bob with the garbled version of her
    input x̂.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双方需要就如何表达 *f* 达成一致。然后爱丽丝将电路 *f* 转换为其混淆版本，并将该混淆版本及其输入 x̂ 的混淆版本一起发送给鲍勃。
- en: Bob wants to create the garbled version of his input ŷ without Alice learning
    what the original value *y* was. To do that, they use oblivious transfer (OT)
    techniques. The oblivious transfer is a two-party protocol between a sender and
    a receiver by which the sender transfers some information to the receiver, but
    the sender remains oblivious to what information the receiver actually obtains.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bob想要创建其输入ŷ的混乱版本，而不让Alice知道原始值*y*是什么。为此，他们使用了无意识传输（OT）技术。无意识传输是一种两方协议，发送者和接收者之间通过发送者向接收者传输一些信息，但发送者对接收者实际获得的信息保持无意识。
- en: Now that Bob has the garbled circuit and the garbled inputs x̂ and ŷ for that
    circuit, he can evaluate the garbled circuit and learn *f*(*x*, *y*) and reveal
    it to Alice. Therefore, the protocol reveals no more than *f*(*x*, *y*).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在Bob已经拥有了混乱的电路以及该电路的混乱输入x̂和ŷ，他可以评估混乱电路并学习*f*(*x*, *y*)，并将其向Alice揭示。因此，该协议揭示的不会超过*f*(*x*,
    *y*)。
- en: 9.4.2 Recapping dimensionality reduction approaches
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 回顾降维方法
- en: Later in this chapter we’ll use the dimensionality reduction (DR) techniques
    that we discussed earlier and other privacy-enhancing techniques to achieve privacy
    preservation for PCA and DCA. Let’s quickly recap the DR methods that we discussed.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将使用我们之前讨论的降维（DR）技术以及其他增强隐私的技术，以实现PCA和DCA的隐私保护。让我们快速回顾一下我们讨论过的DR方法。
- en: PCA is a DR method that is often used to reduce the dimensionality of large
    datasets. The idea is to transform a large set of variables into a smaller set
    in a way that still contains most of the information in the large set. Reducing
    the number of variables in a dataset naturally comes at the expense of accuracy.
    However, DR techniques trade a little accuracy for the simplicity of the approach.
    Since smaller datasets are easier to explore and visualize, and because they make
    analyzing data much easier and faster for ML algorithms, these DR techniques play
    a vital role.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种常用于降低大型数据集维度的DR方法。其想法是将大量变量转换成较小的集合，同时仍然包含大部分大集合中的信息。在数据集中减少变量的数量自然会以精度为代价。然而，DR技术以牺牲一点精度为代价，换取方法的简单性。由于较小的数据集更容易探索和可视化，并且因为它们使机器学习算法分析数据变得更容易更快，这些DR技术在其中发挥着至关重要的作用。
- en: While PCA is an unsupervised DR technique (it does not utilize data labels),
    DCA is a supervised method of DR. DCA selects a projection hyperplane that can
    effectively discriminate between different classes. At a high level, the key difference
    between these two techniques is that PCA assumes a linear relationship to the
    gradients, while DCA assumes a unimodal relationship.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PCA是一种无监督的DR技术（它不利用数据标签），但DCA是一种监督的DR方法。DCA选择一个可以有效地区分不同类别的投影超平面。从高层次来看，这两种技术之间的关键区别在于PCA假设梯度之间存在线性关系，而DCA假设存在单峰关系。
- en: 'In addition to PCA and DCA, many other DR techniques rely on two different
    steps discussed at the beginning of this chapter: computing a scatter matrix and
    computing eigenvalues. In general, these DR methods can be distinguished in terms
    of which scatter matrices are needed and which eigenvalue problems need to be
    solved. For instance, linear discriminant analysis (LDA) is a method similar to
    DCA (both are utility driven). We will not be going into the details here, but
    the main difference is that LDA solves eigenvalues utilizing a within-class scatter
    matrix, whereas DCA uses a between-class scatter matrix.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除了PCA和DCA之外，许多其他DR技术依赖于本章开头讨论的两个不同步骤：计算散布矩阵和计算特征值。一般来说，这些DR方法可以根据所需的散布矩阵和需要解决的特征值问题来区分。例如，线性判别分析（LDA）是一种类似于DCA的方法（两者都是基于效用）。我们不会在这里深入细节，但主要区别在于LDA利用类内散布矩阵来求解特征值，而DCA使用类间散布矩阵。
- en: 'While DCA and LDA are utility-driven DR techniques, another class of DR approaches
    concentrates on utility-privacy optimization problems such as the multiclass discriminant
    ratio (MDR) [4]. We looked into the details of MDR at the beginning of this chapter.
    At a high level, MDR aims to maximize the separability of the utility classification
    problem (the intended task), while minimizing the separability of the privacy-sensitive
    classification problem (the sensitive task). It assumes that each data sample
    has two labels: a utility label (for the intended purpose of the data) and a privacy
    label (for specifying the type of sensitive information). Hence, we can obtain
    two between-class scatter matrices: one that is based on the utility labels and
    a second that is computed using the privacy labels.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DCA 和 LDA 是以效用驱动的DR技术，但另一类DR方法集中在效用-隐私优化问题上，如多类判别比（MDR）[4]。我们在本章开头探讨了MDR的细节。在高层上，MDR旨在最大化效用分类问题（预期任务）的分离性，同时最小化隐私敏感分类问题（敏感任务）的分离性。它假设每个数据样本有两个标签：一个效用标签（用于数据的预期目的）和一个隐私标签（用于指定敏感信息的类型）。因此，我们可以获得两个类间散布矩阵：一个基于效用标签，另一个使用隐私标签计算。
- en: 9.4.3 Using additive homomorphic encryption
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.3 使用加法同态加密
- en: To facilitate the crypto service provider’s (CSP’s) function, additive homomorphic
    encryption will be an essential building block in our protocols. In section 3.4.1
    we discussed homomorphic encryption. As you know, there are multiple semantically
    secure additive homomorphic encryption schemes. We will look at how the Paillier
    cryptosystem [7] can be used as an example of such encryption schemes. Let’s say
    function *E*[pk][∙] is an encryption operation indexed by a public key *pk*, and
    suppose *D[sk]*[∙] is a decryption operation indexed by a secret key *sk*. The
    additive homomorphic encryption can be represented as *E*[pk] [*a* + *b*] = *E*[pk][*a*]
    ⊗ *E*[pk] [*b*], where ⊗ is the modulo multiplication operator in the encrypted
    domain. In addition, scalar multiplication can be achieved by *E*[pk][*a*]^b =
    *E*[pk][*a* ∙ *b*]. If you are interested in more details of the Paillier cryptosystem,
    you can refer to the original paper. You’ll need to understand the basic functionality
    of the homomorphic encryption scheme because we’ll use it in the next section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于加密服务提供商（CSP）的功能，加法同态加密将成为我们协议中的基本构建块。在第3.4.1节中，我们讨论了同态加密。正如你所知，存在多种语义安全的加法同态加密方案。我们将探讨Paillier密码系统[7]如何作为此类加密方案的一个示例。假设函数*E*[pk][∙]是一个由公钥*pk*索引的加密操作，并且假设*D[sk]*[∙]是一个由私钥*sk*索引的解密操作。加法同态加密可以表示为*E*[pk][*a*
    + *b*] = *E*[pk][*a*] ⊗ *E*[pk][*b*]，其中⊗是加密域中的模乘运算符。此外，通过*E*[pk][*a*]^b = *E*[pk][*a*
    ∙ *b*]可以实现标量乘法。如果你对Paillier密码系统的更多细节感兴趣，可以参考原始论文。你需要理解同态加密方案的基本功能，因为我们将在下一节中使用它。
- en: Such encryption schemes only accept integers as plain text, but in most use
    cases, you’ll be dealing with real values when you are implementing ML applications.
    When dealing with ML, the typical approach is to discretize the data (the feature
    values) to obtain integer values.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加密方案仅接受整数作为明文，但在大多数用例中，当你实现机器学习（ML）应用时，你会处理实数值。在处理ML时，典型的方法是将数据（特征值）离散化以获得整数值。
- en: 9.4.4 Overview of the proposed approach
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.4 提出方法概述
- en: In this study, we’ll consider the case of horizontally partitioned data across
    multiple data owners. The system architecture is shown in figure 9.7\. Suppose
    there are *N* data owners where each data owner *n* holds a set of feature vectors
    *Xi*^i ∈ ℝ^(k×M) with *M* number of features (dimensions) and *i* = 1, ..., *k*.
    Here, *k* is the total number of feature vectors. Thus, each data owner *n* will
    have a data matrix such that *X*^n ∈ ℝ^(k×M). In addition, in supervised learning,
    each sample *x* has a label *y* indicating that it belongs to one of the classes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们将考虑跨多个数据所有者的水平分区数据的情况。系统架构如图9.7所示。假设有*N*个数据所有者，其中每个数据所有者*n*持有一组特征向量*Xi*^i
    ∈ ℝ^(k×M)，其中*M*是特征数（维度）且*i* = 1, ..., *k*。在这里，*k*是特征向量的总数。因此，每个数据所有者*n*将有一个数据矩阵，使得*X*^n
    ∈ ℝ^(k×M)。此外，在监督学习中，每个样本*x*都有一个标签*y*，表示它属于某一类。
- en: '![CH09_F07_Zhuang](../../OEBPS/Images/CH09_F07_Zhuang.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07_Zhuang](../../OEBPS/Images/CH09_F07_Zhuang.png)'
- en: Figure 9.7 The system architecture and how it works
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 系统架构及其工作原理
- en: In this system, data users want to utilize the data and compute the PCA (or
    DCA) projection matrix from the data, which is horizontally partitioned among
    multiple data owners. There are many practical applications that fall under this
    data-sharing model. For example, with continuous authentication (CA) applications,
    a CA server needs to build authentication profiles for a group of registered users
    using ML algorithms, including DR techniques like PCA. Hence, the CA server (the
    data user) would need to compute PCA (or DCA) projection matrices using data that’s
    distributed (horizontally) across all registered users (the data owners).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个系统中，数据用户希望利用数据并从数据中计算PCA（或DCA）投影矩阵，这些数据在多个数据所有者之间水平划分。有许多实际应用属于这种数据共享模型。例如，在持续认证（CA）应用中，CA服务器需要使用机器学习算法（包括PCA等DR技术）为注册用户群体构建认证配置文件。因此，CA服务器（数据用户）需要使用分布（水平）在所有注册用户（数据所有者）之间的数据进行PCA（或DCA）投影矩阵的计算。
- en: We will look into the details of each step later, but let’s say there are two
    data owners, Alice and Bob, who would like to share their private data for a collaborative
    ML task. In this case, the ML task will be the data user. From a privacy standpoint,
    the most important concern is that any computation party, such as a data user
    or CSP, should not learn any of Alice’s or Bob’s input data or any intermediate
    values in cleartext format. That’s why we need to encrypt the data first. But
    when the data is encrypted, how can the ML algorithm learn anything? That’s where
    the homomorphic properties come into play, allowing you to still perform some
    calculations, like addition or subtraction, even with encrypted data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后详细了解每个步骤的细节，但让我们假设有两个数据所有者，Alice和Bob，他们希望为了协作机器学习任务共享他们的私有数据。在这种情况下，机器学习任务将是数据用户。从隐私的角度来看，最重要的关注点是任何计算方，如数据用户或CSP，都不应了解Alice或Bob的任何输入数据或任何中间值的明文格式。这就是为什么我们需要首先加密数据的原因。但是，当数据被加密时，机器学习算法如何学习任何东西呢？这就是同态性质发挥作用的地方，它允许你在加密数据的情况下仍然执行某些计算，如加法或减法。
- en: Traditionally, PCA and DCA were only used in a centralized location on joint
    datasets (formed of data contributed by multiple data owners), but computing the
    projection matrix *U* requires the data owners to reveal their data. In this case
    study, we will modify the computation of the projection matrix to make it distributed
    and privacy-preserving. To facilitate this privacy-preserving computation, we’ll
    utilize a third party (the crypto service provider), which will engage in a relatively
    short one-round information exchange with the data user to compute the projection
    matrix. The data owners can then use the projection matrix to reduce the dimensions
    of their data. This reduced dimensional data can later be used as input to certain
    privacy-preserving ML algorithms that perform classification or regression.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，主成分分析（PCA）和分解浓度分析（DCA）仅用于联合数据集（由多个数据所有者贡献的数据）的集中位置，但计算投影矩阵 *U* 需要数据所有者透露他们的数据。在本案例研究中，我们将修改投影矩阵的计算方法，使其分布式且保护隐私。为了便于这种隐私保护计算，我们将利用第三方（加密服务提供商），该第三方将与数据用户进行相对简短的单一回合信息交换，以计算投影矩阵。然后，数据所有者可以使用投影矩阵来降低他们数据的维度。这种降低维度的数据可以后来用作输入，供某些执行分类或回归的隐私保护机器学习（ML）算法使用。
- en: 'When designing a security architecture, it is important to identify the threats
    that could be potentially harmful or vulnerable to the solution we are developing.
    Therefore, let’s quickly review the types of threats that we may face, so we can
    mitigate them in our solution. The main privacy requirement of our protocols is
    enabling data owners to preserve the privacy of their data. We’ll consider the
    adversaries to be the computation parties: the data user and the crypto service
    provider (CSP). Neither of these parties should have access to any of the data
    owner’s input data or any intermediate values (such as data owner shares or scatter
    matrices) in cleartext format. The data user should only learn the output of the
    PCA or DCA projection matrix and the eigenvalues.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计安全架构时，识别可能对我们正在开发的解决方案造成潜在危害或脆弱性的威胁非常重要。因此，让我们快速回顾一下我们可能面临的威胁类型，以便我们可以在我们的解决方案中减轻它们。我们协议的主要隐私要求是使数据所有者能够保护其数据的隐私。我们将考虑对手为计算方：数据用户和加密服务提供商（CSP）。这些方中的任何一方都不应能够访问任何数据所有者的输入数据或任何中间值（例如数据所有者份额或散布矩阵）的明文格式。数据用户应仅了解PCA或DCA投影矩阵的输出和特征值。
- en: The CSP’s primary role is to facilitate the scatter matrices’ privacy-preserving
    computation. To do that, the CSP is assumed to be an independent party, not colluding
    with the data user. For instance, the data user and the CSP could be different
    corporations that would not collude, if only to maintain their reputation and
    customer base. In addition, we’ll assume all participants are honest but curious
    (this is the semi-honest adversarial model). This means that all parties will
    correctly follow the protocol specification, but they could be curious and try
    to use the protocol transcripts to extract new information. Hence, both the data
    user and the CSP are considered semi-honest, non-colluding, but otherwise untrusted
    servers. We will also discuss extensions that could account for a data user colluding
    with a subset of the data owners to glean private information pertaining to a
    single data owner.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: CSP的主要角色是促进散布矩阵的隐私保护计算。为此，假设CSP是一个独立的第三方，不与数据用户勾结。例如，数据用户和CSP可能是不同的公司，它们不会勾结，至少为了维护它们的声誉和客户群。此外，我们假设所有参与者都是诚实但好奇的（这是半诚实对抗模型）。这意味着所有方都将正确遵循协议规范，但他们可能会好奇并试图使用协议记录来提取新信息。因此，数据用户和CSP都被视为半诚实、非勾结，但其他方面不受信任的服务器。我们还将讨论可以解释数据用户与数据所有者子集勾结以获取有关单个数据所有者的私人信息的扩展。
- en: All communication between the data owners, the data user, and the CSP are assumed
    to be carried out on secure channels using well-known methods like SSL/TLS, digital
    certificates, and signatures.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据所有者、数据用户和CSP之间的所有通信都是通过使用SSL/TLS、数字证书和签名等已知方法在安全通道上进行的。
- en: 9.4.5 How privacy-preserving computation works
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.5 隐私保护计算的工作原理
- en: Now that we’ve covered all the necessary background information, let’s proceed
    with the privacy-preserving computation of the scatter matrices and the PCA or
    DCA projection matrix. We’ll suppose that *N* data owners are willing to cooperate
    with a certain data user to compute the scatter matrices.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了所有必要的背景信息，让我们继续进行散布矩阵和PCA或DCA投影矩阵的隐私保护计算。我们假设有*N*个数据所有者愿意与某个数据用户合作来计算散布矩阵。
- en: Privacy-preserving pca
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护PCA
- en: 'The first step in PCA is to compute the total scatter matrix in a distributed
    way. Let’s say the total scatter matrix is ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)
    and that it can be computed in an iterative fashion. We have *N* data owners,
    and each data owner *n* carries a set of training samples, which we’ll denote
    as *P*^n. Each data owner computes a set of values, which we’ll call *local shares*,
    as shown in the following equations:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的第一步是以分布式方式计算散布矩阵的总和。假设总散布矩阵是![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)，并且它可以以迭代方式计算。我们有*N*个数据所有者，每个数据所有者*n*携带一组训练样本，我们将这些样本表示为*P*^n。每个数据所有者计算一组值，我们将这些值称为*本地份额*，如下面的方程所示：
- en: '![CH09_F07zhuang-ch9-eqs-32x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-32x.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-32x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-32x.png)'
- en: '![CH09_F07zhuang-ch9-eqs-33x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-33x.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-33x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-33x.png)'
- en: '![CH09_F07zhuang-ch9-eqs-34x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-34x.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-34x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-34x.png)'
- en: The total scatter matrix can be found by summing the partial contributions from
    each party as follows,
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 散布矩阵的总和可以通过将每个参与方的部分贡献相加得到，如下所示，
- en: '![CH09_F07zhuang-ch9-eqs-35x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-35x.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-35x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-35x.png)'
- en: where
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![CH09_F07zhuang-ch9-eqs-36x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-36x.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-36x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-36x.png)'
- en: It is important that the data owners should not send their local shares to the
    data user in cleartext, since they include statistical summaries of their data.
    The local shares can be encrypted using an additively homomorphic encryption scheme
    (such as Paillier’s cryptosystem) where the CSP provides the public key.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，数据所有者不应该以明文形式将他们的本地份额发送给数据用户，因为这些份额包含了他们数据的统计摘要。本地份额可以使用加法同态加密方案（如Paillier密码系统）进行加密，其中CSP提供公钥。
- en: Upon receiving these encrypted shares, the data user can aggregate them to compute
    the encrypted intermediate values (namely *R*, *V*, and *Q*) and send them to
    the CSP for decryption after blinding the values. The data user can then use these
    aggregate values to compute the scatter matrix and the PCA projection matrix by
    using garbled circuits and oblivious transfer. Here, the *blinding* refers to
    adding random numbers to the encrypted values to prevent the CSP from learning
    anything about the data even in its aggregated form. (The values are blinded using
    an equivalent of a one-time pad.)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到这些加密份额后，数据使用者可以将它们聚合起来计算加密的中间值（即 *R*、*V* 和 *Q*），在盲化这些值后发送给CSP进行解密。然后，数据使用者可以使用这些聚合值通过使用混淆电路和无意识传输来计算散点矩阵和PCA投影矩阵。在这里，“盲化”指的是向加密值添加随机数，以防止CSP在聚合形式中了解任何关于数据的信息。（这些值使用一次性密码的等效物进行盲化。）
- en: 'Let’s look at how this protocol works step by step to ensure privacy while
    preserving PCA:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地看看这个协议是如何工作的，以确保在保持PCA的同时保护隐私：
- en: '*Setting up the users*—During setup, the CSP sends its public key *pk* for
    Paillier’s cryptosystem to the data owners and the data user based on their requests.
    This step could also include the official registration of the data owners with
    the CSP.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*设置用户*—在设置过程中，CSP根据数据所有者和数据使用者的请求，向数据所有者和数据使用者发送Paillier密码系统的公钥 *pk*。这一步也可能包括数据所有者向CSP的官方注册。'
- en: '*Computing local shares*—Each data owner *n* will compute its own share *DS*^n
    = {*R*^n, *V*^n, *Q*^n} using the equations we discussed earlier. After discretizing
    all the values to obtain integer values, the data owners will encrypt *R*^n, *V*^n,
    and *Q*^n using the CSP’s public key to obtain *E*[pk][*DS*^n] = {*E*[pk][*R*^n],
    *E*[pk][*V*^n], *E*[pk][*Q*^n]}. Finally, each data owner sends *E*[pk][*DS*^n]
    to the data user.'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计算本地份额*—每个数据所有者 *n* 将使用我们之前讨论的方程计算其自己的份额 *DS*^n = {*R*^n, *V*^n, *Q*^n}。在将所有值离散化以获得整数值后，数据所有者将使用CSP的公钥加密
    *R*^n、*V*^n 和 *Q*^n，以获得 *E*[pk][*DS*^n] = {*E*[pk][*R*^n], *E*[pk][*V*^n], *E*[pk][*Q*^n]}。最后，每个数据所有者将
    *E*[pk][*DS*^n] 发送给数据使用者。'
- en: '*Aggregating the local shares*—The data user receives *E*[pk][*DS*^n] = {*E*[pk][*R*^n],
    *E*[pk][*V*^n], *E*[pk][*Q*^n]} from each data owner and proceeds to compute the
    encryption of *R*, *V*, and *Q*. More explicitly, the data user is capable of
    computing *E*[pk][*R*], *E*[pk][*V*], and *E*[pk][*Q*] from the encrypted data
    owner shares as follows:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*聚合本地份额*—数据使用者从每个数据所有者那里接收 *E*[pk][*DS*^n] = {*E*[pk][*R*^n], *E*[pk][*V*^n],
    *E*[pk][*Q*^n]}，并继续计算 *R*、*V* 和 *Q* 的加密。更具体地说，数据使用者能够从加密的数据所有者份额中计算出 *E*[pk][*R*]，*E*[pk][*V*]，和
    *E*[pk][*Q*]，如下所示：'
- en: '![CH09_F07zhuang-ch9-eqs-37x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-37x.png)'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-37x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-37x.png)'
- en: '![CH09_F07zhuang-ch9-eqs-38x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-38x.png)'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-38x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-38x.png)'
- en: '![CH09_F07zhuang-ch9-eqs-39x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-39x.png)'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-39x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-39x.png)'
- en: The data user adds some random integers to these aggregated values to mask them
    from the CSP, thus obtaining the blinded aggregated shares *E*[pk][*R'*], *E*[pk][*V'*],
    and *E*[pk][*Q'*], which can be sent to the CSP for decryption.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据使用者将这些聚合值添加一些随机整数以掩盖它们，从而获得盲化的聚合份额 *E*[pk][*R'*]，*E*[pk][*V'*]，和 *E*[pk][*Q'*]，这些可以发送给CSP进行解密。
- en: '*Using garbled circuits for eigenvalue decomposition* *(EVD)*:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用混淆电路进行特征值分解（EVD）*：'
- en: Once the blinded aggregated shares are received from the data user, the CSP
    will use its private key to decrypt them, *E*[pk][*R'*], *E*[pk][*V'*], and *E*[pk][*Q'*].
    Since CSP does not know the random values added by the data user, the CSP cannot
    learn the original aggregated values.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦从数据使用者那里收到盲化的聚合份额，CSP将使用其私钥解密它们，*E*[pk][*R'*]，*E*[pk][*V'*]，和 *E*[pk][*Q'*]。由于CSP不知道数据使用者添加的随机值，因此CSP无法了解原始的聚合值。
- en: The CSP will then construct a garbled circuit to perform EVD on the scatter
    matrix computed from the aggregated shares. The input to this garbled circuit
    is the garbled version of the blinded aggregate shares that the CSP decrypted
    and the blinding values which are generated and held by the data user.
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: CSP将构建一个混淆电路，对从聚合份额计算出的散点矩阵执行特征值分解（EVD）。这个混淆电路的输入是CSP解密后的盲化聚合份额的混淆版本以及由数据使用者生成并持有的盲化值。
- en: 'Since the CSP constructs the garbled circuit, it can obtain the garbled version
    of its input by itself. However, the data user needs to interact with the CSP
    using the oblivious transfer to obtain the garbled version of its input: the blinding
    values. Using oblivious transfer, we can guarantee that the CSP will not learn
    the blinding values held by the data user.'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于CSP构建了混淆电路，它可以自己获得其输入的混淆版本。然而，数据用户需要通过不可知传输与CSP交互，以获得其输入的混淆版本：盲化值。使用不可知传输，我们可以保证CSP不会学习到数据用户持有的盲化值。
- en: The garbled circuit constructed by the CSP takes the two garbled inputs and
    computes the scatter matrix from the shares *R'*, *V'*, and *Q'* after subtracting
    the data user’s blinding values added in the previous steps. The CSP follows that
    by performing Eigen-decomposition on the scatter matrix to obtain the PCA projection
    matrix.
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: CSP构建的混淆电路接收两个混淆输入，并从份额 *R'*, *V'*, 和 *Q'* 中减去数据用户在之前步骤中添加的盲化值，从而计算出散布矩阵。CSP随后通过在散布矩阵上执行特征分解来获得PCA投影矩阵。
- en: The data user will receive the garbled circuit as its evaluator. This garbled
    circuit already has the CSP’s garbled input (decrypted and blinded aggregate shares),
    and the data user obtains the garbled version of the blinding values using oblivious
    transfer.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据用户将接收到的混淆电路作为其评估器。这个混淆电路已经包含了CSP的混淆输入（解密和盲化的聚合份额），并且数据用户通过不可知传输获得了盲化值的混淆版本。
- en: Finally, the data user executes the garbled circuit and obtains the projection
    matrix and eigenvalues as output.
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，数据用户执行混淆电路，并获取投影矩阵和特征值作为输出。
- en: That is how privacy-preserving PCA can be implemented and computed in practical
    applications. We will look at the efficiency of this method later in the chapter.
    Now let’s look at how privacy-preserving DCA works.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是隐私保护PCA如何在实际应用中实现和计算的方法。我们将在本章后面讨论这种方法的有效性。现在让我们看看隐私保护DCA是如何工作的。
- en: Privacy-preserving dca
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私保护dca
- en: As we discussed in section 9.2, DCA computation requires the computation of
    the total scatter matrix ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)
    and the signal matrix *S*[B]. Furthermore, some other DCA formulations also use
    something called the *noise* matrix *S*[W]. We will first quickly outline the
    distributed computation of *S*[B] and *S*[W], and then we’ll move on to the protocol
    implementation details.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第9.2节中讨论的那样，DCA计算需要计算总散布矩阵 ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)
    和信号矩阵 *S*[B]。此外，一些其他的DCA公式还使用称为 *噪声* 矩阵 *S*[W]。我们首先简要概述 *S*[B] 和 *S*[W] 的分布式计算，然后我们将继续介绍协议实现细节。
- en: The computation of *S*[B] and *S*[W] could be different depending on whether
    the data owner has data that belongs to a single class or multiple classes. For
    instance, consider a spam email detection application where each data owner has
    spam and non-spam emails. This case represents two classes (spam and non-spam),
    and it is an example of a multiple-class data owner (MCDO). On the other hand,
    sometimes each data owner represents one class, as in continuous authentication
    systems. In this case, all their data would have the same label. This is an example
    of a single-class data owner (SCDO).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*S*[B] 和 *S*[W] 的计算可能因数据所有者是否拥有属于单个类别的数据而不同。例如，考虑一个垃圾邮件检测应用，其中每个数据所有者都有垃圾邮件和非垃圾邮件。这种情况代表两个类别（垃圾邮件和非垃圾邮件），它是多类别数据所有者（MCDO）的一个例子。另一方面，有时每个数据所有者代表一个类别，如在连续认证系统中。在这种情况下，所有他们的数据都会有相同的标签。这是一个单类别数据所有者（SCDO）的例子。'
- en: We will first review the equations for computing the scatter matrices *S*[B]
    and *S*[W] in a distributed way, and then we’ll present the protocol that performs
    the DCA computation in a privacy-preserving way. Let’s say each sample *X*[i]
    has a label *y*[i] indicating it belongs to one of the *K* classes *C*[1], *C*[2],
    ... , *C*[k]. We’ll let *μ* denote the mean vector of the training samples in
    class *C*[k], and *M*[k] denote the number of samples in class *C*[k]. With that,
    the noise matrix *S*[W] can be calculated as follows,
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先回顾计算散布矩阵 *S*[B] 和 *S*[W] 的方程，然后我们将介绍执行隐私保护DCA计算的协议。假设每个样本 *X*[i] 有一个标签
    *y*[i]，表示它属于 *K* 个类别 *C*[1]，*C*[2]，...，*C*[k] 中的一个。我们将用 *μ* 表示类别 *C*[k] 中训练样本的均值向量，用
    *M*[k] 表示类别 *C*[k] 中的样本数量。有了这些，噪声矩阵 *S*[W] 可以按以下方式计算，
- en: '![CH09_F07zhuang-ch9-eqs-40x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-40x.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-40x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-40x.png)'
- en: where the matrix *O*[k] represents the share that pertains to class *k*. Since
    each data owner *n* maps to a single class *k* (in the SCDO case), we can write
    *O*[k] = *O*^n and have ![CH09_F07zhuang-ch9-eqs-41x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-41x.png).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其中矩阵 *O*[k] 代表与类别 *k* 相关的份额。由于每个数据所有者 *n* 映射到单个类别 *k*（在 SCDO 情况下），我们可以写出 *O*[k]
    = *O*^n 并有 ![CH09_F07zhuang-ch9-eqs-41x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-41x.png)。
- en: As for the case of multiple-class data owners (MCDO), each data owner *n* will
    hold data belonging to different classes. We’ll denote P^n[k] as the set of training
    samples held by data owner *n* that belong to class *k*. For each class *k*, a
    data owner can locally compute
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类别数据所有者（MCDO）的情况，每个数据所有者 *n* 将持有属于不同类别的数据。我们将 P^n[k] 表示为数据所有者 *n* 持有的属于类别
    *k* 的训练样本集合。对于每个类别 *k*，数据所有者可以在本地计算
- en: '![CH09_F07zhuang-ch9-eqs-43x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-43x.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-43x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-43x.png)'
- en: In addition, M^n[k] can be computed as M^n[k] = |P^n[k]|, and the data owner
    would also compute the *R*^n similar to PCA, as it is not restricted to a certain
    class. Now the *S*[W] can be arranged as follows in terms of the partial contributions
    from data owners,
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，M^n[k] 可以计算为 M^n[k] = |P^n[k]|，数据所有者也会像 PCA 一样计算 *R*^n，因为它不受特定类别的限制。现在，*S*[W]
    可以根据数据所有者的部分贡献安排如下，
- en: '![CH09_F07zhuang-ch9-eqs-46x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-46x.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-46x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-46x.png)'
- en: where
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![CH09_F07zhuang-ch9-eqs-47x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-47x.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-47x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-47x.png)'
- en: 'Now let’s look at how we can compute the signal matrix *S*[B]. Both the single
    class (SCDO) and multiple classes (MCDO) are computed similarly. Moreover, the
    signal matrix can be computed directly from the aggregate data (*V*[k], *M*[k],
    *V*, *M*) described in previous equations as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何计算信号矩阵 *S*[B]。单类别（SCDO）和多类别（MCDO）的计算方式相同。此外，信号矩阵可以直接从前面方程中描述的聚合数据（*V*[k]，*M*[k]，*V*，*M*）计算如下：
- en: '![CH09_F07zhuang-ch9-eqs-48x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-48x.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-48x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-48x.png)'
- en: Unlike the scatter matrix, both the noise and signal matrices use class mean
    values in their computations. This will require the data owners to send per-class
    shares to the data user. If a data owner only sends the shares related to the
    classes the data owner has, this will leak knowledge of which classes belong to
    which data owner. Hence, it is recommended that all data owners send shares representing
    all classes, and when a data owner does not own certain class data, they can set
    that class share to all zeros.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 与散点矩阵不同，噪声矩阵和信号矩阵在计算中都使用类别平均值。这将需要数据所有者向数据使用者发送每个类别的份额。如果一个数据所有者只发送与数据所有者拥有的类别相关的份额，这将泄露哪些类别属于哪个数据所有者的知识。因此，建议所有数据所有者发送代表所有类别的份额，当一个数据所有者不拥有某些类别的数据时，他们可以将该类份额设置为全零。
- en: 'Let’s look at how the privacy-preserving DCA protocol works step by step:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地看看隐私保护 DCA 协议是如何工作的：
- en: '*Setting up the users*—This step is very similar to what we did for PCA. The
    CSP sends its public key *pk* for Paillier’s cryptosystem to the data owners and
    the data user based on their requests.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*设置用户*—这一步与我们为 PCA 所做的工作非常相似。CSP 根据数据所有者和数据使用者的请求，向他们发送 Paillier 密码系统的公钥 *pk*。'
- en: '*Computing local shares*—Each data owner *n* computes *R*^n from its data,
    and for each class labeled *k*, the data owner computes V^n[k] and M^n[k]. As
    noted earlier, if a data owner does not have samples pertaining to class *k*,
    they still generate V^n[k] and M^n[k] but set them to zeros. After discretization
    and obtaining integer values, the data owner will encrypt *R*^n using the CSP’s
    public key to obtain *E*[pk][*R*^n], and will also encrypt the class-based shares
    (V^n[k] and M^n[k]) to obtain {*k*, *E*[pk][V^n[k]], *E*[pk][M^n[k]]} for each
    class *k*. Finally, the data owner will send their own encrypted share *DS* n
    to the data user.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计算局部份额*—每个数据所有者 *n* 从其数据中计算 *R*^n，对于每个标记为 *k* 的类别，数据所有者计算 V^n[k] 和 M^n[k]。如前所述，如果一个数据所有者没有与类别
    *k* 相关的样本，他们仍然会生成 V^n[k] 和 M^n[k]，但将它们设置为零。在离散化和获得整数值后，数据所有者将使用 CSP 的公钥加密 *R*^n
    以获得 *E*[pk][*R*^n]，并且还会加密基于类别的份额（V^n[k] 和 M^n[k]），为每个类别 *k* 获得如下：{*k*，*E*[pk][V^n[k]]，*E*[pk][M^n[k]]}。最后，数据所有者将发送他们自己的加密份额
    *DS* n 到数据使用者。'
- en: '*Aggregating the local shares*—From each data owner, the data user receives
    *DS*^n, which includes *E*[pk][*R*^n], and for each class *k*, it also includes
    {*k*, *E*[pk][V^n[k]], *E*[pk][M^n[k]]}. The data user then proceeds to reconstruct
    the encryption of *R*, the values of *V*[k] , and the values of *M*[k] as follows:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*聚合本地份额*——从每个数据所有者那里，数据用户接收 *DS*^n，其中包含 *E*[pk][*R*^n]，并且对于每个类别 *k*，它还包括 {*k*,
    *E*[pk][V^n[k]], *E*[pk][M^n[k]]}。数据用户随后继续重建 *R* 的加密、*V*[k] 的值和 *M*[k] 的值，如下所示：'
- en: '![CH09_F07zhuang-ch9-eqs-37xNEW](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-37xNEW.png)'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-37xNEW](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-37xNEW.png)'
- en: And for each class *k* ∈ 1, ..., *K*, the data user computes
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个类别 *k* ∈ 1, ..., *K*，数据用户计算
- en: '![CH09_F07zhuang-ch9-eqs-52x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-52x.png)'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-52x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-52x.png)'
- en: '![CH09_F07zhuang-ch9-eqs-53x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-53x.png)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH09_F07zhuang-ch9-eqs-53x](../../OEBPS/Images/CH09_F07zhuang-ch9-eqs-53x.png)'
- en: Thereafter, the data user adds some random integers to these aggregated values
    to mask them from the CSP, thus obtaining the blinded shares *E*[pk][*R'*] in
    addition to the values of *E*[pk] and *E*[pk]. These blinded values are then sent
    to the CSP.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此后，数据用户将这些聚合值添加一些随机整数以掩盖它们，从而获得盲化份额 *E*[pk][*R'*] 以及 *E*[pk] 和 *E*[pk] 的值。然后，这些盲值被发送到CSP。
- en: '*Performing the generalized eigenvalue decomposition (GEVD)*:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*执行广义特征值分解（GEVD）*：'
- en: The CSP will use its private key to decrypt the blinded shares *E*[pk][*R'*]
    and the values of *E*[pk][*V’*[k]] and *E*[pk][*M’*[k]] received from the data
    user. Without knowing the random values that were added by the data user, the
    CSP cannot learn the aggregated values.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: CSP将使用其私钥解密从数据用户那里接收到的盲化份额 *E*[pk][*R'*] 以及 *E*[pk][*V’*[k]] 和 *E*[pk][*M’*[k]]
    的值。在不了解数据用户添加的随机值的情况下，CSP无法学习到聚合值。
- en: The CSP will then construct a garbled circuit to perform GEVD on the signal
    and the total scatter matrices computed from the aggregated shares. The input
    to this garbled circuit is the garbled version of the blinded aggregate shares
    that the CSP decrypted and the blinding values that were generated and held by
    the data user.
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，CSP将构建一个混淆电路，对从聚合份额计算出的信号和总散布矩阵执行GEVD。这个混淆电路的输入是CSP解密后的盲化聚合份额的混淆版本以及数据用户生成并持有的盲值。
- en: 'As we discussed in the PCA case, since the CSP constructs the garbled circuit,
    it can obtain the garbled version of its input by itself. However, the data user
    needs to interact with the CSP using the oblivious transfer to obtain the garbled
    version of its input: the blinding values. This oblivious transfer guarantees
    that the CSP will not learn the blinding values held by the data user.'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在PCA案例中讨论的那样，由于CSP构建了混淆电路，它可以自己获得其输入的混淆版本。然而，数据用户需要通过盲传输与CSP交互，以获得其输入的混淆版本：盲值。这种盲传输保证CSP不会学习到数据用户持有的盲值。
- en: The garbled circuit constructed by the CSP takes the two garbled inputs and
    removes the blinding values from the aggregated shares and uses the shares *V'*[k]
    and *M'*[k] to compute *V* and *M*. It then computes the total scatter matrix
    ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png),
    and the signal matrix *S*[B], Finally, it performs the generalized eigenvalue
    decomposition to compute the DCA projection matrix.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: CSP构建的混淆电路接受两个混淆输入，并从聚合份额中移除盲值，并使用份额 *V'*[k] 和 *M'*[k] 来计算 *V* 和 *M*。然后，它计算总散布矩阵
    ![CH03_F12_zhuang-ch3-eqs-30x](../../OEBPS/Images/CH03_F12_zhuang-ch3-eqs-30x.png)，以及信号矩阵
    *S*[B]，最后，它执行广义特征值分解来计算DCA投影矩阵。
- en: The data user will receive the garbled circuit as its evaluator. We know that
    this garbled circuit already has the CSP’s garbled input (decrypted and blinded
    aggregate shares), so they obtain the garbled version of the blinding values using
    oblivious transfer.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据用户将接收到混淆电路作为其评估器。我们知道这个混淆电路已经包含了CSP的混淆输入（解密并盲化的聚合份额），因此他们通过盲传输获得盲值的混淆版本。
- en: Finally, the data user executes the garbled circuit and obtains the projection
    matrix and eigenvalues as output.
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，数据用户执行混淆电路，并获得投影矩阵和特征值作为输出。
- en: This is how privacy-preserving DCA can be implemented in your applications and
    use cases.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何在您的应用程序和用例中实现隐私保护DCA。
- en: What is generalized eigenvalue decomposition (GEVD)?
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是广义特征值分解（GEVD）？
- en: Given the matrices *A* and *B*, the GEVD aims to find the eigenvalues (called
    *λ*’s) and eigenvectors (called *u*’s) that satisfy the equation *Au* = *λBu*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 给定矩阵 *A* 和 *B*，GEVD 的目标是找到满足方程 *Au* = *λBu* 的特征值（称为 *λ*）和特征向量（称为 *u*）。
- en: You might think of reducing this problem to the regular EVD by computing the
    inverse of *B* and attempting to solve *B*^((-1))*Au* = *λu*. However, *B*^((-1))*A*
    is not always guaranteed to be a symmetric matrix. An important property of the
    scatter matrices is that they are symmetric. Eigenvalues of a symmetric matrix
    are always real, thus enabling us to have a simpler implementation of the power
    method that does not involve complex values.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会考虑将这个问题简化为常规的EVD，通过计算 *B* 的逆并尝试解 *B*^((-1))*Au* = *λu*。然而，*B*^((-1))*A*
    并不总是保证是一个对称矩阵。散布矩阵的一个重要特性是它们是对称的。对称矩阵的特征值总是实数，这使得我们可以有一个更简单的幂方法实现，该方法不涉及复数。
- en: You have now gone through the equations showing how to transform data using
    encrypted shares and how to execute eigenvalue decomposition to work with PCA
    and DCA in meaningful ML tasks while preserving privacy. In the next section we
    will evaluate the efficiency and accuracy of these different approaches.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经了解了如何使用加密份额转换数据以及如何执行特征值分解以在具有隐私性的ML任务中与PCA和DCA一起工作的方程。在下一节中，我们将评估这些不同方法的效率和准确性。
- en: 9.4.6 Evaluating the efficiency and accuracy of the privacy-preserving PCA and
    DCA
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.6 评估隐私保护PCA和DCA的效率和准确性
- en: So far in this chapter, we’ve discussed the theoretical background and different
    methods of implementing CP (particularly PCA and DCA) in real-world ML applications.
    Here we’ll evaluate efficiency and accuracy of the implementations discussed in
    the previous sections of this case study (particularly in section 9.4.4). As we
    already discussed, when we integrate privacy into our ML tasks, it is important
    to maintain a balance between privacy and utility. Thus, we’ll evaluate this to
    see how effective the proposed approaches are for practical ML applications.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经讨论了理论背景以及在不同现实世界的ML应用中实现CP（尤其是PCA和DCA）的不同方法。在这里，我们将评估本案例研究前几节（尤其是9.4.4节）中讨论的实现方法的效率和准确性。正如我们已经讨论过的，当我们将隐私整合到我们的ML任务中时，保持隐私和效用之间的平衡是很重要的。因此，我们将评估这一点，看看所提出的方案对于实际的ML应用有多有效。
- en: 'In this case study, the datasets we used for the experiments are from the UCI
    machine learning repository [8]. While PCA and DCA work for any type of ML algorithm,
    we chose classification, since the number of datasets available for classification,
    which was 255, far outnumbered those available for clustering or regression (around
    55 each). Moreover, as the efficiency of the proposed protocols depends on the
    data dimensions (the number of features), we chose datasets with varying numbers
    of features: 8-50\. Tables 9.1 and 9.2 summarize the number of features and classes
    for each dataset. The SVM is used as the classification task for these evaluations,
    and the number of data owners is set to 10 in all cases.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们用于实验的数据集来自UCI机器学习仓库[8]。虽然PCA和DCA适用于任何类型的ML算法，但我们选择了分类，因为可用的分类数据集数量（255个）远远超过了聚类或回归（每个大约55个）。此外，由于所提出的协议的效率取决于数据维度（特征的数量），我们选择了具有不同特征数量的数据集：8-50个。表9.1和9.2总结了每个数据集的特征和类别数量。SVM被用作这些评估的分类任务，并且所有情况下数据所有者的数量都设置为10。
- en: Analyzing the efficiency
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 分析效率
- en: Tables 9.1 and 9.2 show the timing data for performing privacy-preserving PCA
    and DCA on different datasets using Paillier’s key length of 1,024 bits. You can
    try this with different key sizes to see how it will affect the performance—you’ll
    find that when the key length is longer, performance is affected adversely.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1和9.2显示了使用Paillier密钥长度为1,024位在不同数据集上执行隐私保护PCA和DCA的计时数据。您可以尝试使用不同的密钥长度来查看它将如何影响性能——您会发现当密钥长度更长时，性能会受到不利影响。
- en: The average time cost for the data owner refers to the total time it took the
    data owner to compute the individual shares and encrypt them. The average time
    cost for data users represents the time needed to collect each share from each
    data owner and add it to the current sum of these shares (in the encrypted domain).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据所有者，平均时间成本指的是数据所有者计算各个份额并对其进行加密所花费的总时间。对于数据使用者，平均时间成本代表从每个数据所有者收集每个份额并添加到当前这些份额总和（在加密域中）所需的时间。
- en: The tables also show the time it took the CSP to decrypt the blinded aggregated
    values received from the data user. Finally, they show the time needed to run
    the eigenvalue decomposition using garbled circuits to compute the PCA or DCA
    projection matrices. The values in brackets refer to the number of principal components
    generated using the garbled circuit. Naturally, reducing this number would decrease
    the computation time for a given dataset. As we will be discussing in the next
    section, even 15 principal components for the SensIT Vehicle (Acoustic) dataset
    is enough to achieve adequate accuracy.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 表格还显示了CSP解密从数据用户接收到的盲化聚合值所需的时间。最后，它们显示了使用混淆电路运行特征值分解以计算PCA或DCA投影矩阵所需的时间。括号中的值指的是使用混淆电路生成的主成分数量。当然，减少这个数量将减少给定数据集的计算时间。正如我们将在下一节讨论的，即使是SensIT
    Vehicle（声学）数据集的15个主成分也足以达到足够的准确性。
- en: Finally, you’ll notice that increasing the dimension of the data will increase
    the computation time for all stages of the protocols, especially the eigenvalue
    decomposition.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你会注意到增加数据的维度将增加协议所有阶段的计算时间，尤其是特征值分解。
- en: Table 9.1 Efficiency of the distributed PCA
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 分布式PCA的效率
- en: '| Dataset | Features | Classes | Avg. time cost for data owner (sec) | Avg.
    time cost for data user (ms) | CSP decryption time (sec) | Eigenvalue decomposition
    using garbled circuits |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 特征 | 类别 | 数据所有者平均时间成本（秒） | 数据用户平均时间成本（毫秒） | CSP解密时间（秒） | 使用混淆电路进行特征值分解
    |'
- en: '| Diabetes | 8 | 2 | 0.63 | 10 | 0.67 | 13.8 sec (8) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 糖尿病 | 8 | 2 | 0.63 | 10 | 0.67 | 13.8 sec (8) |'
- en: '| Breast cancer | 10 | 2 | 0.93 | 11 | 1.0 | 20.7 sec (8) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 乳腺癌 | 10 | 2 | 0.93 | 11 | 1.0 | 20.7 sec (8) |'
- en: '| Australian | 14 | 2 | 1.7 | 12 | 1.8 | 37.4 sec (8) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 14 | 2 | 1.7 | 12 | 1.8 | 37.4 sec (8) |'
- en: '| German | 24 | 2 | 5.0 | 17 | 5.0 | 3.28 min (15) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 德国 | 24 | 2 | 5.0 | 17 | 5.0 | 3.28 min (15) |'
- en: '| Ionosphere | 34 | 2 | 9.8 | 24 | 9.9 | 6.44 min (15) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 电离层 | 34 | 2 | 9.8 | 24 | 9.9 | 6.44 min (15) |'
- en: '| SensIT Vehicle (Acoustic) | 50 | 3 | 22.5 | 40 | 22.7 | 13.8 min (15) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| SensIT Vehicle（声学） | 50 | 3 | 22.5 | 40 | 22.7 | 13.8 min (15) |'
- en: Table 9.2 Efficiency of the distributed DCA
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.2 分布式DCA的效率
- en: '| Dataset | Features | Classes | Avg. time cost for data owner (sec) | Avg.
    time cost for data user (ms) | CSP decryption time (sec) | Eigenvalue decomposition
    using garbled circuits |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 特征 | 类别 | 数据所有者平均时间成本（秒） | 数据用户平均时间成本（毫秒） | CSP解密时间（秒） | 使用混淆电路进行特征值分解
    |'
- en: '| Diabetes | 8 | 2 | 0.7 | 12 | 0.8 | 4.0 sec (1) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 糖尿病 | 8 | 2 | 0.7 | 12 | 0.8 | 4.0 sec (1) |'
- en: '| Breast cancer | 10 | 2 | 1.2 | 13 | 1.3 | 5.8 sec (1) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 乳腺癌 | 10 | 2 | 1.2 | 13 | 1.3 | 5.8 sec (1) |'
- en: '| Australian | 14 | 2 | 2.1 | 15 | 2.2 | 12.1 sec (1) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 14 | 2 | 2.1 | 15 | 2.2 | 12.1 sec (1) |'
- en: '| German | 24 | 2 | 5.6 | 22 | 5.8 | 46.6 sec (1) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 德国 | 24 | 2 | 5.6 | 22 | 5.8 | 46.6 sec (1) |'
- en: '| Ionosphere | 34 | 2 | 11.2 | 29 | 11.6 | 1.9 min (1) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 电离层 | 34 | 2 | 11.2 | 29 | 11.6 | 1.9 min (1) |'
- en: '| SensIT Vehicle (Acoustic) | 50 | 3 | 26.2 | 48 | 26.9 | 6.7 min (2) |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| SensIT Vehicle（声学） | 50 | 3 | 26.2 | 48 | 26.9 | 6.7 min (2) |'
- en: Analyzing the accuracy of the ml task
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 分析ml任务的准确性
- en: Figure 9.8 and table 9.3 summarize the results of performing experiments to
    test the accuracy of the classification tasks after using our privacy-preserving
    protocols compared to results obtained using the Python library NumPy.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8和表9.3总结了执行实验以测试在隐私保护协议使用后分类任务准确性的结果，与使用Python库NumPy获得的结果进行了比较。
- en: '![CH09_F08_Zhuang](../../OEBPS/Images/CH09_F08_Zhuang.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_Zhuang](../../OEBPS/Images/CH09_F08_Zhuang.png)'
- en: Figure 9.8 Privacy-preserving PCA accuracy
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 隐私保护PCA准确性
- en: Here we are using the weighted F1 score to test the accuracy of the classifiers.
    The F1 score can be thought of as a weighted average of the precision and recall
    scores; a classifier is at its best when its F1 score is 1 and at its worst when
    the score is 0\. The F1 score is one of the primary metrics utilized in ML applications
    to compare the performance of two classifiers. For more on how the F1 score works,
    refer back to section 3.4.3.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用加权F1分数来测试分类器的准确性。F1分数可以被视为精确度和召回率分数的加权平均值；当F1分数为1时，分类器处于最佳状态，而当分数为0时，处于最差状态。F1分数是ML应用中用于比较两个分类器性能的主要指标之一。有关F1分数如何工作的更多信息，请参阅第3.4.3节。
- en: 'As a reminder, here is how you can calculate the F1 score:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，以下是计算F1分数的方法：
- en: '![CH09_F08_zhuang-ch9-eqs-59x](../../OEBPS/Images/CH09_F08_zhuang-ch9-eqs-59x.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_zhuang-ch9-eqs-59x](../../OEBPS/Images/CH09_F08_zhuang-ch9-eqs-59x.png)'
- en: '![CH09_F08_zhuang-ch9-eqs-60x](../../OEBPS/Images/CH09_F08_zhuang-ch9-eqs-60x.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_zhuang-ch9-eqs-60x](../../OEBPS/Images/CH09_F08_zhuang-ch9-eqs-60x.png)'
- en: '![CH09_F08_zhuang-ch9-eqs-61x](../../OEBPS/Images/CH09_F08_zhuang-ch9-eqs-61x.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![CH09_F08_zhuang-ch9-eqs-61x](../../OEBPS/Images/CH09_F08_zhuang-ch9-eqs-61x.png)'
- en: The DCA results are shown in table 9.3 with one value each, since DCA projects
    the data to *K* - 1 dimensions (where *K* is the number of class labels), whereas
    PCA projects the data to a variable number of dimensions. As you can see, the
    protocols are correct and their results are equivalent to those obtained using
    NumPy. It should be noted that the slight fluctuation in the weighted F1 score
    is primarily due to the SVM parameter selection. However, the accuracy of both
    methods is almost the same.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: DCA的结果显示在表9.3中，每个值只有一个，因为DCA将数据投影到*K* - 1个维度（其中*K*是类别标签的数量），而PCA将数据投影到可变数量的维度。正如您所看到的，这些协议是正确的，并且它们的结果与使用NumPy获得的结果相同。需要注意的是，加权F1分数的轻微波动主要是由于SVM参数选择造成的。然而，两种方法的准确性几乎相同。
- en: Table 9.3 Privacy-preserving DCA accuracy
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.3 隐私保护DCA准确性
- en: '| Dataset | F1 score %(with proposed implementation) | F1 score %(using the
    NumPy library) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | F1分数 %(使用所提出的方法) | F1分数 %(使用NumPy库) |'
- en: '| Diabetes | 76.5 | 76.4 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 糖尿病 | 76.5 | 76.4 |'
- en: '| Breast cancer | 96.9 | 96.8 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 乳腺癌 | 96.9 | 96.8 |'
- en: '| Australian | 86.1 | 85.5 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 澳大利亚 | 86.1 | 85.5 |'
- en: '| German | 72.7 | 73.8 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 德国 | 72.7 | 73.8 |'
- en: '| Ionosphere | 84.3 | 83.4 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 电离层 | 84.3 | 83.4 |'
- en: '| SensIT Vehicle (Acoustic) | 67.8 | 68.4 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| SensIT Vehicle (Acoustic) | 67.8 | 68.4 |'
- en: In this case study, we discussed how to implement privacy-preserving protocols
    for ML (particularly PCA and DCA) on horizontally partitioned data distributed
    across multiple data owners. From the results, it can be clearly seen that the
    approach is efficient, and it maintains the utility for data users while still
    preserving the privacy of the data owner’s data.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们讨论了如何在水平划分的数据上实现机器学习（尤其是PCA和DCA）的隐私保护协议，这些数据分布在多个数据所有者之间。从结果来看，这种方法是高效的，它既保持了数据用户的效用，又保护了数据所有者数据的隐私。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The main problem with DP-based privacy preservation techniques is that they
    usually add excessive noise to the private data, resulting in a somewhat inevitable
    utility drop.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于DP的隐私保护技术的主要问题是，它们通常会给私有数据添加过多的噪声，导致不可避免地降低效用。
- en: Compressive privacy is an alternative approach (compared to DP) that can be
    utilized in many practical applications for privacy preservation without significant
    loss in the utility task.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩隐私是一种替代方法（与DP相比），可以在许多实际应用中用于隐私保护，而不会在效用任务中造成重大损失。
- en: Compressive privacy essentially perturbs the data by projecting it to a lower-dimensional
    hyperplane via compression and DR techniques such that the data can be recovered
    without affecting the privacy.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩隐私本质上通过压缩和DR技术将数据投影到低维超平面，从而扰动数据，使得数据可以在不影响隐私的情况下被恢复。
- en: Different compressive privacy mechanisms serve different applications, but for
    machine learning and data mining tasks, PCA, DCA, and MDR are a few popular approaches.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的压缩隐私机制适用于不同的应用，但对于机器学习和数据挖掘任务，PCA、DCA和MDR是一些流行的方法。
- en: Compressive privacy technologies like PCA and DCA can also be used in the distributed
    setting to implement privacy-preserving protocols for machine learning.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩隐私技术，如PCA和DCA，也可以在分布式环境中使用，以实现机器学习的隐私保护协议。
