- en: Appendix B. Automating the web with scraping
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B. 使用爬取自动化网络
- en: '*This appendix covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本附录涵盖*'
- en: Creating structured data from web pages
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网页创建结构化数据
- en: Performing basic web scraping with cheerio
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 cheerio 进行基本的网络爬取
- en: Handling dynamic content with jsdom
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 jsdom 处理动态内容
- en: Parsing and outputting structured data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析和输出结构化数据
- en: In the preceding chapter, you learned some general Node programming techniques,
    but now we’re going to start focusing on web development. Scraping the web is
    an ideal way to do this, because it requires a combination of server and client-side
    programming skills. Scraping is all about using programming techniques to make
    sense of web pages and transform them into structured data. Imagine you’re tasked
    with creating a new version of a book publisher’s website that’s currently just
    a set of old-fashioned, static HTML pages. You want to download the pages and
    analyze them to extract the titles, descriptions, authors, and prices for all
    the books. You don’t want to do this by hand, so you write a Node program to do
    it. This is *web scraping*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了某些通用的 Node 编程技术，但现在我们将开始专注于网络开发。网络爬取是做这件事的理想方式，因为它需要服务器端和客户端编程技能的结合。爬取就是使用编程技术理解网页并将它们转换为结构化数据。想象一下，你被分配了一个任务，要创建一个目前只是由一组过时的静态
    HTML 页面组成的图书出版商网站的新版本。你想要下载这些页面并分析它们以提取所有书籍的标题、描述、作者和价格。你不想手动做这件事，所以你编写了一个 Node
    程序来完成它。这就是*网络爬取*。
- en: Node is great at scraping because it strikes a perfect balance between browser-based
    technology and the power of general-purpose scripting languages. In this chapter,
    you’ll learn how to use HTML parsing libraries to extract useful data based on
    CSS selectors, and even to run dynamic web pages in a Node process.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Node 在爬取方面表现卓越，因为它在基于浏览器的技术和通用脚本语言的力量之间取得了完美的平衡。在本章中，你将学习如何使用 HTML 解析库根据 CSS
    选择器提取有用的数据，甚至可以在 Node 进程中运行动态网页。
- en: B.1\. Understanding web scraping
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1\. 理解网络爬取
- en: '*Web scraping* is the process of extracting useful information from websites.
    This usually involves downloading the required pages, parsing them, and then querying
    the raw HTML by using CSS or XPath selectors. The results of the queries are then
    exported as CSV files or saved to a database. [Figure B.1](#app02fig01) shows
    how scraping works from start to finish.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*网络爬取*是从网站中提取有用信息的过程。这通常涉及下载所需的页面，解析它们，然后使用 CSS 或 XPath 选择器查询原始 HTML。查询的结果随后作为
    CSV 文件导出或保存到数据库中。[图 B.1](#app02fig01) 展示了从开始到结束的爬取过程。'
- en: Figure B.1\. Steps for scraping and storing content
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 B.1\. 爬取和存储内容的步骤
- en: '![](Images/bfig01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/bfig01.jpg)'
- en: Web scraping may be against the terms of use of some websites, because of its
    cost or because of resource limitations. If thousands of scrapers hit a single
    site that runs on an old and slow server, the server could be knocked offline.
    Before you scrape any content, you should ensure that you have permission to access
    and duplicate the content. You can technically check the site’s robots.txt ([www.robotstxt.org](http://www.robotstxt.org))
    file for this information, but you should contact the site’s owners first. In
    some cases, the site’s owners may have invited you to index its information—perhaps
    as part of a larger web development contract.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于成本或资源限制，网络爬取可能违反某些网站的使用条款。如果成千上万的爬虫同时访问一个运行在老旧且缓慢服务器上的网站，服务器可能会被关闭。在爬取任何内容之前，你应该确保你有权访问和复制该内容。你可以技术上检查网站的
    robots.txt ([www.robotstxt.org](http://www.robotstxt.org)) 文件以获取此信息，但你应该首先联系网站的所有者。在某些情况下，网站的所有者可能邀请你索引其信息——可能是作为更大规模网络开发合同的一部分。
- en: In this section, you’ll learn how people use scrapers for real sites, and then
    you’ll look at the required tools that allow Node to become a web-scraping powerhouse.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解人们如何使用爬虫处理真实网站，然后你将查看允许 Node 成为网络爬取强者的所需工具。
- en: B.1.1\. Uses of web scraping
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.1\. 网络爬取的用途
- en: A great example of web scraping is the vertical search engine Octopart ([https://octopart.com/](https://octopart.com/)).
    Octopart, shown in [figure B.2](#app02fig02), indexes electronics distributors
    and manufacturers to make it easier for people to find electronics. For example,
    you can search for resistors based on resistance, tolerance, power rating, and
    case type. A site like this uses web crawlers to download content, scrapers to
    make sense of the content and extract interesting values (for example, the tolerance
    of a resistor), and an internal database to store the processed information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Web scraping的一个很好的例子是垂直搜索引擎Octopart ([https://octopart.com/](https://octopart.com/))。如图B.2所示，Octopart索引了电子分销商和制造商，以便人们更容易找到电子产品。例如，你可以根据电阻、公差、功率额定值和外壳类型搜索电阻。这样的网站使用网络爬虫下载内容，使用抓取工具理解内容并提取有趣的价值（例如，电阻的公差），并使用内部数据库存储处理后的信息。
- en: Figure B.2\. Octopart allows users to search for electronic parts.
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图B.2\. Octopart允许用户搜索电子元件。
- en: '![](Images/bfig02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/bfig02.jpg)'
- en: Web scraping isn’t used for only search engines, however. It’s also used in
    the growing fields of data science and data journalism. Data journalists use databases
    to produce stories, but because there’s so much data that isn’t stored in easily
    accessible formats, they may use tools such as web scraping to automate the collection
    and processing of data. This allows journalists to present information in new
    ways, through data--visualization techniques including infographics and interactive
    graphics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Web scraping不仅仅用于搜索引擎，它还被应用于日益增长的数据科学和数据新闻领域。数据记者使用数据库来制作故事，但由于有大量数据存储在不易访问的格式中，他们可能会使用诸如web
    scraping之类的工具来自动收集和处理数据。这使得记者能够以新的方式呈现信息，通过数据可视化技术，包括信息图表和交互式图表。
- en: B.1.2\. Required tools
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1.2\. 必需的工具
- en: 'To get down to business, you need a couple of easily accessible tools: a web
    browser and Node. Browsers are one of the most useful scraping tools—if you can
    right-click and select Inspect Element, you’re already partway to making sense
    of websites and converting them into raw data. The next step is to parse the pages
    with Node. In this chapter, you’ll learn about two types of parser:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进入正题，你需要一些易于访问的工具：一个网络浏览器和Node。浏览器是其中最实用的抓取工具之一——如果你可以右键点击并选择“检查元素”，你就已经迈出了理解网站并将其转换为原始数据的一半。下一步是使用Node解析页面。在本章中，你将了解两种类型的解析器：
- en: 'Lightweight and forgiving: cheerio'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻量级且宽容：cheerio
- en: 'A web-standards-aware, Document Object Model (DOM) simulator: jsdom'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关注Web标准的文档对象模型（DOM）模拟器：jsdom
- en: Both of these libraries are installed with npm. You may need to parse loosely
    structured human-readable data formats such as dates as well. We’ll briefly look
    at Java-Script’s `Date.parse` and Moment.js.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个库都是通过npm安装的。你可能还需要解析松散结构的人读数据格式，如日期。我们将简要介绍Java-Script的`Date.parse`和Moment.js。
- en: The first example uses cheerio, which is a fast way to parse most static web
    pages.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个例子使用了cheerio，这是一种快速解析大多数静态网页的方法。
- en: B.2\. Performing basic web scraping with cheerio
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. 使用cheerio进行基本的Web抓取
- en: 'The cheerio library ([www.npmjs.com/package/cheerio](http://www.npmjs.com/package/cheerio)),
    by Felix Böhm, is perfect for scraping because it combines two key features: fast
    HTML parsing, and a jQuery-like API for querying and manipulating the HTML.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: cheerio库（[www.npmjs.com/package/cheerio](http://www.npmjs.com/package/cheerio)），由Felix
    Böhm编写，非常适合抓取，因为它结合了两个关键特性：快速的HTML解析，以及类似jQuery的API用于查询和操作HTML。
- en: Imagine you need to extract information about books from a publisher’s website.
    The publisher doesn’t yet have an API that exposes book details, so you need to
    download pages from its website and turn them into usable JSON output that includes
    the author name and book title. [Figure B.3](#app02fig03) shows how scraping with
    cheerio works.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要从一个出版社网站上提取有关书籍的信息。该出版社还没有公开书籍详细信息的API，因此你需要下载其网站上的页面，并将它们转换为包含作者姓名和书名的可用的JSON输出。图B.3显示了使用cheerio进行抓取的工作原理。
- en: Figure B.3\. Scraping with cheerio
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图B.3\. 使用cheerio进行抓取
- en: '![](Images/bfig03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/bfig03.jpg)'
- en: The following listing contains a small scraper that uses cheerio. Sample HTML
    has been included, so you don’t need to worry about how to download the page itself
    yet.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表包含一个使用cheerio的小型抓取器。已经包括了示例HTML，所以你不必担心如何下载页面本身。
- en: Listing B.1\. Extracting a book’s details
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表B.1\. 提取书籍的详细信息
- en: '![](Images/blis01_alt.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/blis01_alt.jpg)'
- en: '[Listing B.1](#app02ex01) uses cheerio to parse a hardcoded HTML document by
    using the `cheerio.load()` method and CSS selectors. In a simple example like
    this, the CSS selectors are simple and clear, but often real-world HTML is far
    messier. Unfortunately, poorly structured HTML is unavoidable, and your skill
    as a web scraper is defined by coming up with clever ways to pull out the values
    you need.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表B.1](#app02ex01)使用cheerio通过`cheerio.load()`方法和CSS选择器解析硬编码的HTML文档。在这个简单的例子中，CSS选择器简单明了，但现实世界的HTML通常要混乱得多。不幸的是，结构不良的HTML是不可避免的，您的网络爬虫技能体现在想出巧妙的方法来提取您需要的值。'
- en: Making sense of bad HTML requires two steps. The first is to visualize the document,
    and the second is to define the selectors that target the elements you’re interested
    in. You use cheerio’s features to define the selector in just the right way.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 理解糟糕的HTML需要两个步骤。第一步是可视化文档，第二步是定义针对您感兴趣元素的选择器。您使用cheerio的功能来恰当地定义选择器。
- en: 'Fortunately, modern browsers offer a point-and-click solution for finding selectors:
    if your browser has development tools, you can usually right-click and select
    Inspect Element. Not only will you see the underlying HTML, but the browser should
    also show a representation of the selector that targets the element.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，现代浏览器提供了一个点选解决方案来查找选择器：如果您的浏览器有开发工具，您通常可以右键单击并选择“检查元素”。不仅您会看到底层的HTML，浏览器还应显示一个针对元素的选择器表示。
- en: 'Let’s say you’re trying to extract book information from a quirky site that
    uses tables without any handy CSS classes. The HTML might look like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在尝试从一个使用表格但没有方便CSS类的古怪网站中提取书籍信息。HTML可能看起来像这样：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you open that in Chrome and right-click the title, you’ll see something like
    [figure B.4](#app02fig04).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Chrome中打开它并右键单击标题，您会看到类似[图B.4](#app02fig04)的内容。
- en: Figure B.4\. Viewing HTML in Chrome
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图B.4\. 在Chrome中查看HTML
- en: '![](Images/bfig04_alt.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bfig04_alt.jpg)'
- en: The white bar under the HTML shows “html body table tbody tr td a”—this is close
    to the selector that you need. But it’s not quite right, because the real HTML
    doesn’t have a `tbody`. Chrome has inserted this element. When you’re using browsers
    to visualize documents, you should be prepared to adjust what you discover based
    on the true underlying HTML. This example shows that you need to search for a
    link inside a table cell to get the title, and the next table cell is the corresponding
    author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: HTML下方的白色条形显示“html body table tbody tr td a”——这几乎就是您需要的选择器。但并不完全正确，因为真正的HTML没有`tbody`。Chrome插入了这个元素。当您使用浏览器可视化文档时，您应该准备好根据真正的底层HTML调整您发现的内容。这个例子表明，您需要在一个表格单元格内搜索链接以获取标题，下一个表格单元格是相应的作者。
- en: Assuming the preceding HTML is in a file called messy_html_example.html, the
    following listing will extract the title, link, and author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设前面的HTML存储在一个名为messy_html_example.html的文件中，以下列表将提取标题、链接和作者。
- en: Listing B.2\. Dealing with messy HTML
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表B.2\. 处理糟糕的HTML
- en: '![](Images/blis02_alt.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/blis02_alt.jpg)'
- en: You use the fs module to load the HTML; that’s so you don’t have to keep printing
    HTML in the example. In reality, your data source might be a live website, but
    the data could also be from a file or a database. After the document has been
    parsed, you use `first()` to get the first table cell with an anchor. To get the
    anchor’s URL, you use cheerio’s `attr()` method; it returns a specific attribute
    from an element, just like jQuery. The `eq()` method is also useful; in this listing,
    it’s used to skip the first td, because the second contains the author’s text.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您使用fs模块来加载HTML；这样您就不必在示例中不断打印HTML。实际上，您的数据源可能是一个实时网站，但数据也可能来自文件或数据库。在文档被解析后，您使用`first()`来获取带有锚点的第一个表格单元格。要获取锚点的URL，您使用cheerio的`attr()`方法；它从元素中返回一个特定属性，就像jQuery一样。`eq()`方法也很有用；在这个列表中，它被用来跳过第一个td，因为第二个包含作者的文本。
- en: '|  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Web-parsing dangers**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络解析风险**'
- en: Using a module such as cheerio is a quick and dirty way of interpreting web
    documents. But be careful of the type of content that you attempt to parse with
    it. It may throw an exception with binary data, for example, so using it in a
    web application could crash the Node process. This would be dangerous if your
    scraper is embedded in the same process that serves your web application.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用cheerio这样的模块是一种快速且简单的方法来解析网络文档。但请注意您尝试解析的内容类型。例如，它可能会在二进制数据上抛出异常，因此在使用Node.js网络应用程序时可能会崩溃。如果您的爬虫嵌入在同一个进程中，这将是危险的。
- en: It’s best to check the content type before passing it through a parser, and
    you may want to consider running your web scrapers in their own Node processes
    to reduce the impact of any serious crashes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过解析器传递内容之前最好检查内容类型，你可能还希望考虑在你的 Node 进程中运行你的网络爬虫以减少任何严重崩溃的影响。
- en: '|  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: One of cheerio’s limitations is that it allows you to work only with a static
    version of a document; it’s used for working with pure HTML documents rather than
    dynamic pages that use client-side JavaScript. In the next section, you’ll learn
    how to use jsdom to create a browser-like environment in your Node applications,
    so client-side Java-Script will be executed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: cheerio 的一个限制是它只允许你与文档的静态版本一起工作；它用于处理纯 HTML 文档，而不是使用客户端 JavaScript 的动态页面。在下一节中，你将学习如何使用
    jsdom 在你的 Node 应用程序中创建类似浏览器的环境，以便执行客户端 JavaScript。
- en: B.3\. Handling dynamic content with jsdom
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3\. 使用 jsdom 处理动态内容
- en: '*jsdom* is the web scraper’s dream tool: it downloads HTML, interprets it according
    to the DOM as found in a typical browser, and runs client-side JavaScript. You
    can specify the client-side JavaScript that you want to run, which typically means
    including jQuery. That means you can inject jQuery (or your own custom debugging
    scripts) into any pages. [Figure B.5](#app02fig05) shows how jsdom combines HTML
    and JavaScript to make otherwise unscrapeable content accessible.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*jsdom* 是网络爬虫的梦想工具：它下载 HTML，根据在典型浏览器中找到的 DOM 解释它，并运行客户端 JavaScript。你可以指定要运行的客户端
    JavaScript，这通常意味着包括 jQuery。这意味着你可以将 jQuery（或你自己的自定义调试脚本）注入到任何页面中。[图 B.5](#app02fig05)
    展示了 jsdom 如何结合 HTML 和 JavaScript 使其他难以爬取的内容变得可访问。'
- en: Figure B.5\. Scraping with jsdom
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 B.5\. 使用 jsdom 进行爬取
- en: '![](Images/bfig05_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/bfig05_alt.jpg)'
- en: jsdom does have a downside. It’s not a perfect simulation of a browser, it’s
    slower than cheerio, and the HTML parser is strict, so it may fail for pages with
    poorly written markup. Some sites don’t make sense without client-side JavaScript
    support, however, so it’s an indispensible tool for some scraping tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: jsdom 确实有一些缺点。它并不是一个完美的浏览器模拟，它的速度比 cheerio 慢，HTML 解析器很严格，所以它可能无法处理编写糟糕的标记的页面。然而，一些网站没有客户端
    JavaScript 支持就没什么意义，因此对于某些爬取任务来说，它是一个不可或缺的工具。
- en: The basic usage of jsdom is through the `jsdom.env` method. The following listing
    shows how jsdom can be used to scrape a page by injecting jQuery and pulling out
    useful values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: jsdom 的基本用法是通过 `jsdom.env` 方法。以下列表展示了如何使用 jsdom 通过注入 jQuery 并提取有用值来爬取一个页面。
- en: Listing B.3\. Scraping with jsdom
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 B.3\. 使用 jsdom 进行爬取
- en: '![](Images/blis03_alt.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/blis03_alt.jpg)'
- en: To run [listing B.3](#app02ex03), you need to save jQuery locally and install
    jsdom.^([[1](#app02fn01)]) You can install both with npm. The modules are called
    jsdom ([www.npmjs.com/package/jsdom](http://www.npmjs.com/package/jsdom)) and
    jQuery ([www.npmjs.com/package/jquery](http://www.npmjs.com/package/jquery)),
    respectively. After everything is set up, this code should print out the title,
    author, and description of the HTML fragment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 [列表 B.3](#app02ex03)，你需要将 jQuery 本地保存并安装 jsdom.^([[1](#app02fn01)]) 你可以使用
    npm 安装这两个模块。这些模块分别称为 jsdom ([www.npmjs.com/package/jsdom](http://www.npmjs.com/package/jsdom))
    和 jQuery ([www.npmjs.com/package/jquery](http://www.npmjs.com/package/jquery))。一切设置完成后，此代码应打印出
    HTML 片段的标题、作者和描述。
- en: ¹
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: jsdom 6.3.0 is the current version at the time of writing.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: jsdom 6.3.0 是撰写本文时的当前版本。
- en: The `jsdom.env` method is used to parse the document and inject jQuery. jQuery
    is injected by downloading it from npm, but you could supply the URL to jQuery
    on a content delivery network (CDN) or your filesystem; jsdom will know what to
    do. The `jsdom.env` method is asynchronous and requires a callback to work. The
    callback receives error and window objects; the window object is how you access
    the document. Here the window’s jQuery object has been aliased so it can be easily
    accessed with `$`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`jsdom.env` 方法用于解析文档并注入 jQuery。jQuery 通过从 npm 下载它来注入，但你也可以提供内容分发网络 (CDN) 或你的文件系统上的
    jQuery URL；jsdom 会知道该怎么做。`jsdom.env` 方法是异步的，需要回调来工作。回调接收错误和窗口对象；窗口对象是你访问文档的方式。在这里，窗口的
    jQuery 对象已经被别名化，因此可以很容易地用 `$` 访问。'
- en: A selector is used with jQuery’s `.each` method to iterate over each book. This
    example has only one book, but it demonstrates that jQuery’s traversal methods
    are indeed available. Each value from the book is accessed by using jQuery’s traversal
    methods as well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 jQuery 的 `.each` 方法与选择器一起使用，以遍历每一本书。这个例子只有一本书，但它证明了 jQuery 的遍历方法确实是可用的。通过使用
    jQuery 的遍历方法也可以访问每本书的每个值。
- en: '[Listing B.3](#app02ex03) is similar to the earlier cheerio example in [listing
    B.1](#app02ex01), but the main difference is that jQuery has been parsed and run
    by Node, within the current process. [Listing B.1](#app02ex01) used cheerio to
    provide similar functionality, but cheerio provides its own jQuery-like layer.
    Here you’re running code intended for a browser as if it’s really running in a
    browser.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 B.3](#app02ex03) 与之前的 cheerio 示例 [列表 B.1](#app02ex01) 类似，但主要区别在于 jQuery
    已经由 Node 在当前进程中解析和运行。[列表 B.1](#app02ex01) 使用 cheerio 提供类似的功能，但 cheerio 提供了自己的类似
    jQuery 的层。在这里，您正在运行旨在在浏览器中运行的代码，就像它真的在浏览器中运行一样。'
- en: The `jsdom.env` method is useful only for working with static pages. To parse
    pages that use client-side JavaScript, you need to use `jsdom.jsdom` instead.
    This synchronous method returns a window object that you can manipulate with other
    jsdom utilities. The following listing uses jsdom to parse a document with a `script`
    tag, and `jsdom.jQueryify` to make scraping it easier.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`jsdom.env` 方法仅适用于处理静态页面。要解析使用客户端 JavaScript 的页面，您需要使用 `jsdom.jsdom`。这个同步方法返回一个可以与其他
    jsdom 工具一起操作的窗口对象。以下列表使用 jsdom 解析带有 `script` 标签的文档，并使用 `jsdom.jQueryify` 使抓取更加容易。'
- en: Listing B.4\. Parsing dynamic HTML with jsdom
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 B.4\. 使用 jsdom 解析动态 HTML
- en: '![](Images/blis04_alt.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Images/blis04_alt.jpg]'
- en: '[Listing B.4](#app02ex04) requires jQuery to be installed, so if you’re creating
    this listing by hand, you need to set up a new project with `npm init` and `npm
    install --save jquery jsdom`. It uses a simple HTML document in which the useful
    values that you’re looking for are dynamically inserted. They’re inserted using
    client-side JavaScript found in a `script` tag.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 B.4](#app02ex04) 需要安装 jQuery，因此如果您手动创建此列表，您需要使用 `npm init` 和 `npm install
    --save jquery jsdom` 设置一个新的项目。它使用一个简单的 HTML 文档，其中您正在寻找的有用值是使用 `script` 标签中的客户端
    JavaScript 动态插入的。'
- en: 'This time, `jsdom.jsdom` is used instead of `jsdom.env`. It’s synchronous because
    the document object is created in memory, but won’t do much until you attempt
    to query or manipulate it. To do this, you use `jsdom.jQueryify` to insert your
    specific version of jQuery into the document. After jQuery has been loaded and
    run, the callback is run, which queries the document for the values you’re interested
    in and prints them to the console. The output is shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，使用的是 `jsdom.jsdom` 而不是 `jsdom.env`。它是同步的，因为文档对象是在内存中创建的，但在您尝试查询或操作它之前不会做很多事情。为此，您使用
    `jsdom.jQueryify` 将您的特定版本的 jQuery 插入文档中。在 jQuery 加载并运行后，回调函数将被执行，它会查询文档以获取您感兴趣的数据，并将它们打印到控制台。输出结果如下所示：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This proves that jsdom has invoked the necessary client-side JavaScript. Now
    imagine this is a real web page and you’ll see why jsdom is so powerful: even
    websites made with very little static HTML and dynamic technologies such as Angular
    and React can be scraped.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了 jsdom 已经调用了必要的客户端 JavaScript。现在想象这是一个真正的网页，您将看到为什么 jsdom 如此强大：即使是使用很少的静态
    HTML 和像 Angular 和 React 这样的动态技术构建的网站也可以被抓取。
- en: B.4\. Making sense of raw data
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4\. 理解原始数据
- en: After you finally get useful data from a page, you need to process it so it’s
    suitable for saving to a database or for an export format such as CSV. Your scraped
    data will either be unstructured plain text or encoded using microformats.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在您最终从页面获取有用的数据后，您需要对其进行处理，以便将其保存到数据库或用于 CSV 等导出格式。您抓取的数据将是未结构化的纯文本或使用微格式编码。
- en: '*Microformats* are lightweight, markup-based data formats that are used for
    things like addresses, calendars and events, and tags or keywords. You can find
    established microformats at microformats.org. Here’s an example of a name represented
    as a microformat:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*微格式* 是一种轻量级的基于标记的数据格式，用于地址、日历和事件以及标签或关键词等。您可以在 microformats.org 找到已建立的微格式。以下是一个表示为微格式的名称示例：'
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Microformats are relatively easy to parse; with cheerio or jsdom, a simple expression
    such as `$('.h-card').text()` is sufficient to extract *Joseph Heller*. But plain
    text requires more work. In this section, you’ll see how to parse dates and then
    convert them into more database-friendly formats.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 微格式相对容易解析；使用 cheerio 或 jsdom，一个简单的表达式如 `$('.h-card').text()` 就足以提取 *约瑟夫·海勒*。但纯文本需要更多的工作。在本节中，您将了解如何解析日期，然后将它们转换为更友好的数据库格式。
- en: Most web pages don’t use microformats. One area where this is problematic but
    potentially manageable is date values. Dates can appear in many formats, but they’re
    usually consistent on a given website. After you’ve identified the format, you
    can parse and then format the date.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数网页不使用微格式。在日期值方面，这是一个问题但可能可以管理的领域。日期可以以许多格式出现，但通常在给定网站上是一致的。在确定格式后，您可以解析并格式化日期。
- en: 'JavaScript has a built-in date parser: if you run `new Date(''2016 01 01'')`,
    an instance of `Date` will be returned that corresponds to the first of January,
    2016\. The supported input formats are determined by `Date.parse`, which is based
    on RFC 2822 ([http://tools.ietf.org/html/rfc2822#page-14](http://tools.ietf.org/html/rfc2822#page-14))
    or ISO 8601 ([www.w3.org/TR/NOTE-datetime](http://www.w3.org/TR/NOTE-datetime)).
    Other formats may work and are often worth trying out with your source data to
    see what happens.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript有一个内置的日期解析器：如果您运行`new Date('2016 01 01')`，将返回一个`Date`实例，对应于2016年1月1日。支持的输入格式由`Date.parse`确定，它基于RFC
    2822 ([http://tools.ietf.org/html/rfc2822#page-14](http://tools.ietf.org/html/rfc2822#page-14))或ISO
    8601 ([www.w3.org/TR/NOTE-datetime](http://www.w3.org/TR/NOTE-datetime))。其他格式可能有效，并且通常值得尝试使用源数据查看会发生什么。
- en: 'The other approach is to match values in the source data with a regular expression,
    and then use `Date`’s constructor to make new `Date` objects. The signature for
    the constructor is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用正则表达式匹配源数据中的值，然后使用`Date`构造函数创建新的`Date`对象。构造函数的签名如下：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Date parsing in JavaScript is usually good enough to handle many cases, but
    it falls down in reformatting dates. A great solution to this is Moment.js ([http://momentjs.com](http://momentjs.com)),
    a date-parsing, validation, and formatting library. It has a fluent API, so calls
    can be chained like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: JavaScript中的日期解析通常足够处理许多情况，但在日期重新格式化方面会失败。解决这个问题的一个很好的方法是Moment.js ([http://momentjs.com](http://momentjs.com))，这是一个日期解析、验证和格式化库。它有一个流畅的API，因此可以像这样链式调用：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is convenient for turning scraped data into CSV files that work well with
    programs such as Microsoft Excel. Imagine you have a web page with books that
    include title and published date. You want to save the values to a database, but
    your database requires dates to be formatted as YYYY-MM-DD. The following listing
    shows how to use Moment with cheerio to do this.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于将抓取数据转换为与Microsoft Excel等程序兼容的CSV文件非常有用。想象一下，您有一个包含书籍标题和出版日期的网页。您想将这些值保存到数据库中，但您的数据库要求日期格式为YYYY-MM-DD。以下列表显示了如何使用Moment与cheerio来完成此操作。
- en: Listing B.5\. Parsing dates and generating CSV
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表B.5\. 解析日期并生成CSV
- en: '![](Images/app02ex05-0.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/app02ex05-0.jpg)'
- en: '![](Images/app02ex05-1.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/app02ex05-1.jpg)'
- en: '[Listing B.5](#app02ex05) requires that you install cheerio, Moment, and books.
    It takes as input HTML (from input.html) and then outputs CSV. The HTML should
    have dates in `h4` elements, like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表B.5](#app02ex05) 需要安装cheerio、Moment和books。它以HTML（来自input.html）作为输入，然后输出CSV。HTML中的日期应位于`h4`元素中，如下所示：'
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After the scraper has loaded the input file, it loads up Moment, and then maps
    each book to a simple JavaScript object by using cheerio’s `.map` and `.get` methods.
    The `.map` method iterates over each book, and the callback extracts each element
    that you’re interested in by using the `.find` selector traversal method. To get
    the resulting text values as an array, `.get` is used.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在刮削器加载输入文件后，它加载了Moment，然后通过使用cheerio的`.map`和`.get`方法将每本书映射到一个简单的JavaScript对象。`.map`方法遍历每一本书，回调函数通过使用`.find`选择器遍历方法提取你感兴趣的所有元素。为了将结果文本值作为数组获取，使用`.get`。
- en: '[Listing B.5](#app02ex05) outputs CSV by using `console.log`. The header is
    printed, and then each row is logged in a loop that iterates over each book. The
    dates are converted to a format compatible with MySQL by using Moment; first the
    date is parsed using `new Date`, and then it’s formatted using Moment.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表B.5](#app02ex05) 通过使用`console.log`输出CSV。首先打印标题，然后通过遍历每一本书的循环记录每一行。日期通过使用Moment转换为与MySQL兼容的格式；首先使用`new
    Date`解析日期，然后使用Moment进行格式化。'
- en: After you’ve become used to parsing and formatting dates, you can apply similar
    techniques to other data formats. For example, currency and distance measurements
    can be captured with regular expressions, and then formatted by using a more generic
    number-formatting library such as Numeral ([www.npmjs.com/package/numeral](http://www.npmjs.com/package/numeral)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在你习惯了解析和格式化日期之后，你可以将类似的技术应用到其他数据格式上。例如，货币和距离测量可以通过正则表达式捕获，然后使用更通用的数字格式化库（如Numeral）进行格式化（[www.npmjs.com/package/numeral](http://www.npmjs.com/package/numeral)）。
- en: B.5\. Summary
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5. 摘要
- en: Web scraping is the automated transformation of sometimes badly structured web
    pages into computer-friendly formats such as CSV or databases.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络抓取是将有时结构不良的网页自动转换为计算机友好的格式（如CSV或数据库）的过程。
- en: Web scraping is used for vertical search engines but also for data journalism.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络抓取不仅用于垂直搜索引擎，也用于数据新闻学。
- en: If you're going to scrape a site, you should get permission first. You can do
    this by checking the site’s robots.txt file and contacting the site’s owner.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你打算抓取一个网站，你应该先获得许可。你可以通过检查网站的robots.txt文件和联系网站所有者来实现这一点。
- en: The main tools are static HTML parsers (cheerio) and parsers capable of running
    JavaScript (jsdom), but also browser developer tools for finding the right CSS
    selector for the elements you’re interested in.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要工具包括静态HTML解析器（cheerio）和能够运行JavaScript的解析器（jsdom），以及用于找到你感兴趣的元素的正确CSS选择器的浏览器开发者工具。
- en: Sometimes the data itself is not well formatted, so you may need to parse things
    such as dates or currencies to make them work with databases.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时数据本身格式不佳，因此你可能需要解析日期或货币等事物，以便它们能与数据库兼容。
