- en: 8 Generative adversarial networks (GANs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 生成对抗网络 (GANs)
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: 'Understanding the basic components of GANs: generative and discriminative models'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 GANs 的基本组件：生成模型和判别模型
- en: Evaluating generative models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估生成模型
- en: Learning about popular vision applications of GANs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 GANs 在流行视觉应用中的使用
- en: Building a GAN model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 GAN 模型
- en: Generative adversarial networks (GANs) are a new type of neural architecture
    introduced by Ian Goodfellow and other researchers at the University of Montreal,
    including Yoshua Bengio, in 2014.[1](#pgfId-1184812) GANs have been called “the
    most interesting idea in the last 10 years in ML” by Yann LeCun, Facebook’s AI
    research director. The excitement is well justified. The most notable feature
    of GANs is their capacity to create hyperrealistic images, videos, music, and
    text. For example, except for the far-right column, none of the faces shown on
    the right side of figure 8.1 belong to real humans; they are all fake. The same
    is true for the handwritten digits on the left side of the figure. This shows
    a GAN’s ability to learn features from the training images and imagine its own
    new images using the patterns it has learned.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络 (GANs) 是由蒙特利尔大学的 Ian Goodfellow 及其他研究人员，包括 Yoshua Bengio，于 2014 年引入的一种新型神经网络架构。[1](#pgfId-1184812)
    Facebook 的 AI 研究总监 Yann LeCun 将 GANs 称为“过去 10 年机器学习中最有趣的想法”。这种兴奋是有充分理由的。GANs 最显著的特点是它们能够创建超逼真的图像、视频、音乐和文本。例如，除了最右侧的列外，图
    8.1 右侧显示的所有面部都不是真实人类的；它们都是伪造的。图左侧的手写数字也是如此。这表明 GAN 能够从训练图像中学习特征，并使用其学到的模式想象出新的图像。
- en: '![](../Images/8-1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-1.png)'
- en: 'Figure 8.1 Illustration of GANs’ abilities by Goodfellow and co-authors. These
    are samples generated by GANs after training on two datasets: MNIST and the Toronto
    Faces Dataset (TFD). In both cases, the right-most column contains true data.
    This shows that the produced data is really generated and not only memorized by
    the network. (Source: Goodfellow et al., 2014.)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 Goodfellow 及其合作者展示的 GANs 能力的说明。这些是在对两个数据集（MNIST 和多伦多面部数据集 [TFD]）进行训练后由
    GANs 生成的样本。在两种情况下，最右侧的列包含真实数据。这表明生成数据确实是生成的，而不仅仅是网络记忆的。 (来源：Goodfellow 等，2014
    年。)
- en: 'We’ve learned in the past chapters how deep neural networks can be used to
    understand image features and perform deterministic tasks on them like object
    classification and detection. In this part of the book, we will talk about a different
    type of application for deep learning in the computer vision world: generative
    models. These are neural network models that are able to imagine and produce new
    content that hasn’t been created before. They can imagine new worlds, new people,
    and new realities in a seemingly magical way. We train generative models by providing
    a training dataset in a specific domain; their job is to create images that have
    new objects from the same domain that look like the real data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的章节中，我们学习了如何使用深度神经网络来理解图像特征并在其上执行确定性任务，如对象分类和检测。在这本书的这一部分，我们将讨论计算机视觉世界中深度学习的另一种类型的应用：生成模型。这些是能够想象并产生以前未曾创造过的新内容的神经网络模型。它们能够以看似神奇的方式想象出新的世界、新的人和新的现实。我们通过提供特定领域的训练数据集来训练生成模型；它们的任务是创建看起来像真实数据的新对象图像。
- en: 'For a long time, humans have had an advantage over computers: the ability to
    imagine and create. Computers have excelled in solving problems like regression,
    classification, and clustering. But with the introduction of generative networks,
    researchers can make computers generate content of the same or higher quality
    compared to that created by their human counterparts. By learning to mimic any
    distribution of data, computers can be taught to create worlds that are similar
    to our own in any domain: images, music, speech, prose. They are robot artists,
    in a sense, and their output is impressive. GANs are also seen as an important
    stepping stone toward achieving artificial general intelligence (AGI), an artificial
    system capable of matching human cognitive capacity to acquire expertise in virtually
    any domain--from images, to language, to creative skills needed to compose sonnets.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 很长一段时间以来，人类在计算机上拥有优势：想象和创造的能力。计算机在解决回归、分类和聚类等问题上表现出色。但随着生成网络的引入，研究人员可以让计算机生成与人类对手相同或更高质量的内容。通过学习模仿任何数据分布，计算机可以被教会创造与我们自己的世界相似的内容，无论在哪个领域：图像、音乐、语音、散文。从某种意义上说，它们是机器人艺术家，它们的产出令人印象深刻。GAN也被视为实现通用人工智能（AGI）的重要垫脚石，这是一种能够匹配人类认知能力，在几乎任何领域获得专业知识的人工系统——从图像到语言，再到创作十四行诗所需的创造性技能。
- en: Naturally, this ability to generate new content makes GANs look a little bit
    like magic, at least at first sight. In this chapter, we will only attempt to
    scratch the surface of what is possible with GANs. We will overcome the apparent
    magic of GANs in order to dive into the architectural ideas and math behind these
    models in order to provide the necessary theoretical knowledge and practical skills
    to continue exploring any facet of this field that you find most interesting.
    Not only will we discuss the fundamental notions that GANs rely on, but we will
    also implement and train an end-to-end GAN and go through it step by step. Let’s
    get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，这种生成新内容的能力使得GAN看起来有点像魔法，至少一开始是这样。在本章中，我们只尝试揭开GAN可能性的冰山一角。我们将克服GAN的表面魔法，深入探讨这些模型背后的架构思想和数学，以便提供必要的理论知识与实践技能，继续探索这个领域中最吸引你的任何方面。我们不仅将讨论GAN所依赖的基本概念，还将实现和训练一个端到端GAN，并逐步进行。让我们开始吧！
- en: 8.1 GAN architecture
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 GAN架构
- en: 'GANs are based on the idea of adversarial training. The GAN architecture basically
    consists of two neural networks that compete against each other:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GAN基于对抗性训练的理念。GAN架构基本上由两个相互竞争的神经网络组成：
- en: The generator tries to convert random noise into observations that look as if
    they have been sampled from the original dataset.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器试图将随机噪声转换为看起来像是来自原始数据集的观察值。
- en: The discriminator tries to predict whether an observation comes from the original
    dataset or is one of the generator’s forgeries.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器试图预测一个观察值是否来自原始数据集或生成器的伪造品。
- en: 'This competitiveness helps them to mimic any distribution of data. I like to
    think of the GAN architecture as two boxers fighting (figure 8.2): in their quest
    to win the bout, both are learning each others’ moves and techniques. They start
    with less knowledge about their opponent, and as the match goes on, they learn
    and become better.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种竞争性帮助他们模仿任何数据分布。我喜欢将GAN架构想象成两个拳击手在打架（图8.2）：在争取赢得比赛的过程中，他们都在学习对方的动作和技术。他们开始时对对手的了解较少，随着比赛的进行，他们不断学习和变得更好。
- en: '![](../Images/8-2.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图8-2](../Images/8-2.png)'
- en: 'Figure 8.2 A fight between two adversarial networks: generative and discriminative'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 两个对抗性网络之间的战斗：生成性和判别性
- en: 'Another analogy will help drive home the idea: think of a GAN as the opposition
    of a counterfeiter and a cop in a game of cat and mouse, where the counterfeiter
    is learning to pass false notes, and the cop is learning to detect them (figure
    8.3). Both are dynamic: as the counterfeiter learns to perfect creating false
    notes, the cop is in training and getting better at detecting the fakes. Each
    side learns the other’s methods in a constant escalation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个类比将有助于阐明这个观点：将GAN想象成猫捉老鼠游戏中伪造者和警察的对立面，其中伪造者正在学习传递假钞，而警察正在学习检测它们（图8.3）。双方都是动态的：随着伪造者学会完美地制作假钞，警察正在训练并提高检测假币的能力。双方都在不断学习对方的方法，形成一种持续的升级。
- en: '![](../Images/8-3.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图8-3](../Images/8-3.png)'
- en: Figure 8.3 The GAN’s generator and discriminator models are like a counterfeiter
    and a police officer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 GAN的生成器和判别器模型就像是一个伪造者和一个警察。
- en: 'As you can see in the architecture diagram in figure 8.4, a GAN takes the following
    steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在图8.4的架构图中所见，GAN执行以下步骤：
- en: The generator takes in random numbers and returns an image.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器接收随机数字并返回一个图像。
- en: This generated image is fed into the discriminator alongside a stream of images
    taken from the actual, ground-truth dataset.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个生成的图像被输入到判别器中，同时还有从实际的真实数据集中提取的图像流。
- en: 'The discriminator takes in both real and fake images and returns probabilities:
    numbers between 0 and 1, with 1 representing a prediction of authenticity and
    0 representing a prediction of fake.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判别器接收真实和虚假的图像，并返回概率：介于0和1之间的数字，其中1代表对真实性的预测，0代表对虚假的预测。
- en: '![](../Images/8-4.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-4.png)'
- en: 'Figure 8.4 The GAN architecture is composed of generator and discriminator
    networks. Note that the discriminator network is a typical CNN where the convolutional
    layers reduce in size until they get to the flattened layer. The generator network,
    on the other hand, is an inverted CNN that starts with the flattened vector: the
    convolutional layers increase in size until they form the dimension of the input
    images.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 GAN架构由生成器和判别器网络组成。请注意，判别器网络是一个典型的卷积神经网络（CNN），其中卷积层的大小逐渐减小，直到达到展平层。另一方面，生成器网络是一个倒置的CNN，它从展平的向量开始：卷积层的大小逐渐增加，直到形成输入图像的维度。
- en: If you take a close look at the generator and discriminator networks, you will
    notice that the generator network is an inverted ConvNet that starts with the
    flattened vector. The images are upscaled until they are similar in size to the
    images in the training dataset. We will dive deeper into the generator architecture
    later in this chapter--I just wanted you to notice this phenomenon now.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察生成器和判别器网络，你会注意到生成器网络是一个倒置的卷积神经网络（ConvNet），它从展平的向量开始。图像被上采样，直到它们的大小与训练数据集中的图像相似。我们将在本章后面更深入地探讨生成器架构——我只是想让你现在注意到这个现象。
- en: 8.1.1 Deep convolutional GANs (DCGANs)
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 深度卷积GANs (DCGANs)
- en: In the original GAN paper in 2014, multi-layer perceptron (MLP) networks were
    used to build the generator and discriminator networks. However, since then, it
    has been proven that convolutional layers give greater predictive power to the
    discriminator, which in turn enhances the accuracy of the generator and the overall
    model. This type of GAN is called a deep convolutional GAN (DCGAN) and was developed
    by Alec Radford et al. in 2016.[2](#pgfId-1184867) Now, all GAN architectures
    contain convolutional layers, so the “DC” is implied when we talk about GANs;
    so, for the rest of this chapter, we refer to DCGANs as both GANs and DCGANs.
    You can also go back to chapters 2 and 3 to learn more about the differences between
    MLP and CNN networks and why CNN is preferred for image problems. Next, let’s
    dive deeper into the architecture of the discriminator and generator networks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年的原始GAN论文中，多层感知器（MLP）网络被用来构建生成器和判别器网络。然而，从那时起，已经证明卷积层为判别器提供了更大的预测能力，这反过来又提高了生成器和整体模型的准确性。这种类型的GAN被称为深度卷积GAN（DCGAN），由Alec
    Radford等人于2016年开发。[2](#pgfId-1184867) 现在，所有GAN架构都包含卷积层，因此当我们谈论GAN时，“DC”是隐含的；因此，在本章的其余部分，我们将DCGAN称为GAN和DCGAN。你还可以回顾第2章和第3章，了解更多关于MLP和CNN网络之间的差异以及为什么CNN更适合图像问题。接下来，让我们更深入地探讨判别器和生成器网络的架构。
- en: 8.1.2 The discriminator model
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 判别器模型
- en: 'As explained earlier, the goal of the discriminator is to predict whether an
    image is real or fake. This is a typical supervised classification problem, so
    we can use the traditional classifier network that we learned about in the previous
    chapters. The network consists of stacked convolutional layers, followed by a
    dense output layer with a sigmoid activation function. We use a sigmoid activation
    function because this is a binary classification problem: the goal of the network
    is to output prediction probabilities values that range between 0 and 1, where
    0 means the image generated by the generator is fake and 1 means it is 100% real.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，判别器的目标是预测图像是真实的还是虚假的。这是一个典型的监督分类问题，因此我们可以使用我们在前几章中学到的传统分类器网络。该网络由堆叠的卷积层组成，后面跟着一个具有sigmoid激活函数的密集输出层。我们使用sigmoid激活函数，因为这是一个二元分类问题：网络的目的是输出介于0和1之间的预测概率值，其中0表示生成器生成的图像是虚假的，1表示它是100%真实的。
- en: 'The discriminator is a normal, well understood classification model. As you
    can see in figure 8.5, training the discriminator is pretty straightforward. We
    feed the discriminator labeled images: fake (or generated) and real images. The
    real images come from the training dataset, and the fake images are the output
    of the generator model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 权衡器是一个正常、易于理解的分类模型。如图8.5所示，训练权衡器相当直接。我们向权衡器提供标记图像：伪造（或生成）的图像和真实图像。真实图像来自训练数据集，伪造图像是生成器模型的输出。
- en: '![](../Images/8-5.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-5.png)'
- en: Figure 8.5 The discriminator for the GAN
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 GAN的权衡器
- en: 'Now, let’s implement the discriminator network in Keras. At the end of this
    chapter, we will compile all the code snippets together to build an end-to-end
    GAN. We will first implement a `discriminator_model` function. In this code snippet,
    the shape of the image input is 28 × 28; you can change it as needed for your
    problem:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Keras中实现权衡器网络。在本章末尾，我们将把所有代码片段组合起来构建一个端到端的GAN。我们首先实现一个`discriminator_model`函数。在这个代码片段中，图像输入的形状是28
    × 28；你可以根据你的问题需要更改它：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Instantiates a sequential model and names it discriminator
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化了一个序列模型，并将其命名为discriminator
- en: ❷ Adds a convolutional layer to the discriminator model
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 向权衡器模型添加了一个卷积层
- en: ❸ Adds a leaky ReLU activation function
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加了漏斗ReLU激活函数
- en: ❹ Adds a dropout layer with a 25% dropout probability
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加了一个dropout层，dropout概率为25%
- en: ❺ Adds a second convolutional layer with zero padding
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加了一个带有零填充的第二卷积层
- en: ❻ Adds a batch normalization layer for faster learning and higher accuracy
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 添加了一个批量归一化层以实现更快的学习和更高的精度
- en: ❼ Adds a third convolutional layer with batch normalization, leaky ReLU, and
    a dropout
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 添加了第三个卷积层，包含批量归一化、漏斗ReLU和dropout
- en: ❽ Adds the fourth convolutional layer with batch normalization, leaky ReLU,
    and a dropout
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 添加了第四个卷积层，包含批量归一化、漏斗ReLU和dropout
- en: ❾ Flattens the network and adds the output dense layer with sigmoid activation
    function
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 将网络展平，并添加了具有sigmoid激活函数的输出密集层
- en: ❿ Prints the model summary
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 打印模型摘要
- en: ⓫ Sets the input image shape
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 设置了输入图像的形状
- en: ⓬ Runs the discriminator model to get the output probability
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 运行权衡器模型以获取输出概率
- en: ⓭ Returns a model that takes the image as input and produces the probability
    output
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 返回一个以图像为输入并产生概率输出的模型
- en: 'The output summary of the discriminator model is shown in figure 8.6\. As you
    might have noticed, there is nothing new: the discriminator model follows the
    regular pattern of the traditional CNN networks that we learned about in chapters
    3, 4, and 5\. We stack convolutional, batch normalization, activation, and dropout
    layers to create our model. All of these layers have hyperparameters that we tune
    when we are training the network. For your own implementation, you can tune these
    hyperparameters and add or remove layers as you see fit. Tuning CNN hyperparameters
    is explained in detail in chapters 3 and 4.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 权衡器模型的输出摘要如图8.6所示。正如你可能已经注意到的，没有什么新东西：权衡器模型遵循我们在第3、4和5章中学到的传统CNN网络的常规模式。我们堆叠卷积、批量归一化、激活和dropout层来创建我们的模型。所有这些层都有超参数，我们在训练网络时调整这些超参数。对于你的实现，你可以调整这些超参数，根据需要添加或删除层。CNN超参数的调整在第3和第4章中有详细解释。
- en: '![](../Images/8-6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-6.png)'
- en: Figure 8.6 The output summary for the discriminator model
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 权衡器模型的输出摘要
- en: In the output summary in figure 8.6, note that the width and height of the output
    feature maps decrease in size, whereas the depth increases in size. This is the
    expected behavior for traditional CNN networks as we’ve seen in previous chapters.
    Let’s see what happens to the feature maps’ size in the generator network in the
    next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.6的输出摘要中，请注意输出特征图的宽度和高度在减小，而深度在增加。这是我们之前章节中看到的传统CNN网络的预期行为。让我们在下一节中看看生成器网络中的特征图大小会发生什么变化。
- en: 8.1.3 The generator model
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 生成器模型
- en: The generator takes in some random data and tries to mimic the training dataset
    to generate fake images. Its goal is to trick the discriminator by trying to generate
    images that are perfect replicas of the training dataset. As it is trained, it
    gets better and better after each iteration. But the discriminator is being trained
    at the same time, so the generator has to keep improving as the discriminator
    learns its tricks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接收一些随机数据，并试图模仿训练数据集以生成假图像。它的目标是试图生成与训练数据集完美匹配的图像来欺骗判别器。随着训练的进行，它在每次迭代后都会变得越来越好。但是判别器也在同时进行训练，因此生成器必须随着判别器学习其技巧而不断改进。
- en: '![](../Images/8-7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-7.png)'
- en: Figure 8.7 The generator model of the GAN
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 GAN的生成器模型
- en: As you can see in figure 8.7, the generator model looks like an inverted ConvNet.
    The generator takes a vector input with some random noise data and reshapes it
    into a cube volume that has a width, height, and depth. This volume is meant to
    be treated as a feature map that will be fed to several convolutional layers that
    will create the final image.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如图8.7所示，生成器模型看起来像一个倒置的卷积神经网络。生成器接收一个包含一些随机噪声数据的向量输入，并将其重塑为一个具有宽度、高度和深度的立方体体积。这个体积被用来作为特征图，将被输入到几个卷积层中，以创建最终的图像。
- en: Upsampling to scale feature maps
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上采样以缩放特征图
- en: Traditional convolutional neural networks use pooling layers to downsample input
    images. In order to scale the feature maps, we use upsampling layers that scale
    the image dimensions by repeating each row and column of the input pixels.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 传统卷积神经网络使用池化层对输入图像进行下采样。为了缩放特征图，我们使用上采样层，通过重复输入像素的每一行和每一列来缩放图像维度。
- en: 'Keras has an upsampling layer (`Upsampling2D`) that scales the image dimensions
    by taking a scaling factor (`size`) as an argument:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Keras有一个上采样层（`Upsampling2D`），它通过将一个缩放因子（`size`）作为参数来缩放图像维度：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This line of code repeats every row and column of the image matrix two times,
    because the size of the scaling factor is set to (2, 2); see figure 8.8\. If the
    scaling factor is (3, 3), the upsampling layer repeats each row and column of
    the input matrix three times, as shown in figure 8.9.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码重复图像矩阵的每一行和每一列两次，因为缩放因子的尺寸设置为(2, 2)；参见图8.8。如果缩放因子是(3, 3)，上采样层将输入矩阵的每一行和每一列重复三次，如图8.9所示。
- en: '![](../Images/8-8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-8.png)'
- en: Figure 8.8 Upsampling example when the scaling size is (2, 2)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 当缩放大小为(2, 2)时的上采样示例
- en: '![](../Images/8-9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-9.png)'
- en: Figure 8.9 Upsampling example when scaling size is (3, 3)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 当缩放大小为(3, 3)时的上采样示例
- en: When we build the generator model, we keep adding upsampling layers until the
    size of the feature maps is similar to the training dataset. You will see how
    this is implemented in Keras in the next section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建生成器模型时，我们会不断添加上采样层，直到特征图的大小与训练数据集相似。你将在下一节中看到如何在Keras中实现这一点。
- en: 'Now, let’s build the `generator_model` function that builds the generator network:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建`generator_model`函数，该函数构建生成器网络：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Instantiates a sequential model and names it generator
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化一个序列模型并将其命名为generator
- en: ❷ Adds a dense layer that has a number of neurons = 128 × 7 × 7
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加一个具有128 × 7 × 7个神经元的密集层
- en: ❸ Reshapes the image dimensions to 7 × 7 × 128
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将图像维度重塑为7 × 7 × 128
- en: ❹ Upsampling layer to double the size of the image dimensions to 14 × 14
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 上采样层将图像维度的大小加倍到14 × 14
- en: ❺ Adds a convolutional layer to run the convolutional process and batch normalization
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加一个卷积层来运行卷积过程和批量归一化
- en: ❻ Upsamples the image dimensions to 28 × 28
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将图像维度上采样到28 × 28
- en: ❼ We don’t add upsampling here because the image size of 28 × 28 is equal to
    the image size in the MNIST dataset. You can adjust this for your own problem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 我们在这里不添加上采样，因为28 × 28的图像大小与MNIST数据集中的图像大小相同。你可以根据你自己的问题进行调整。
- en: ❽ Prints the model summary
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 打印模型摘要
- en: ❾ Generates the input noise vector of length = 100\. We use 100 here to create
    a simple network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 生成长度为100的输入噪声向量。我们在这里使用100来创建一个简单的网络。
- en: ❿ Runs the generator model to create the fake image
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 运行生成器模型以创建假图像
- en: ⓫ Returns a model that takes the noise vector as input and outputs the fake
    image
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 返回一个模型，该模型以噪声向量为输入，并输出假图像
- en: The output summary of the generator model is shown in figure 8.10\. In the code
    snippet, the only new component is the `Upsampling` layer to double its input
    dimensions by repeating pixels. Similar to the discriminator, we stack convolutional
    layers on top of each other and add other optimization layers like `BatchNormalization`.
    The key difference in the generator model is that it starts with the flattened
    vector; images are upsampled until they have dimensions similar to the training
    dataset. All of these layers have hyperparameters that we tune when we are training
    the network. For your own implementation, you can tune these hyperparameters and
    add or remove layers as you see fit.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器模型的输出总结如图 8.10 所示。在代码片段中，唯一的新组件是 `Upsampling` 层，通过重复像素来加倍其输入维度。与判别器类似，我们在每一层卷积层之上堆叠其他优化层，如
    `BatchNormalization`。生成器模型的关键区别在于它从扁平化的向量开始；图像被上采样，直到它们的尺寸与训练数据集相似。所有这些层都有超参数，我们在训练网络时调整这些超参数。对于你的实现，你可以根据需要调整这些超参数，添加或删除层。
- en: '![](../Images/8-10.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 8-10](../Images/8-10.png)'
- en: Figure 8.10 The output summary of the generator model
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 生成器模型的输出总结
- en: Notice the change in the output shape after each layer. It starts from a 1D
    vector of 6,272 neurons. We reshaped it to a 7 × 7 × 128 volume, and then the
    width and height were upsampled twice to 14 × 14 followed by 28 × 28\. The depth
    decreased from 128 to 64 to 1 because this network is built to deal with the grayscale
    MNIST dataset project that we will implement later in this chapter. If you are
    building a generator model to generate color images, then you should set the filters
    in the last convolutional layer to 3.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每一层输出形状的变化。它从一个包含 6,272 个神经元的 1D 向量开始。我们将其重塑为一个 7 × 7 × 128 的体积，然后宽度高度被上采样两次，变为
    14 × 14，接着是 28 × 28。深度从 128 减少到 64，再减少到 1，因为此网络是为了处理我们将在本章后面实现的灰度 MNIST 数据集项目而构建的。如果你正在构建一个用于生成彩色图像的生成器模型，那么你应该将最后一层卷积层的过滤器设置为
    3。
- en: 8.1.4 Training the GAN
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 训练 GAN
- en: 'Now that we’ve learned the discriminator and generator models separately, let’s
    put them together to train an end-to-end generative adversarial network. The discriminator
    is being trained to become a better classifier to maximize the probability of
    assigning the correct label to both training examples (real) and images generated
    by the generator (fake): for example, the police officer becomes better at differentiating
    between fakes and real currency. The generator, on the other hand, is being trained
    to become a better forger, to maximize its chances of fooling the discriminator.
    Both networks are getting better at what they do.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经分别学习了判别器和生成器模型，让我们将它们组合起来训练一个端到端的生成对抗网络。判别器正在被训练成为一个更好的分类器，以最大化将正确标签分配给训练示例（真实）和生成器生成的图像（伪造）的概率：例如，警察将更好地区分假币和真币。另一方面，生成器正在被训练成为一个更好的伪造者，以最大化欺骗判别器的机会。这两个网络都在它们所做的事情上变得更擅长。
- en: 'The process of training GAN models involves two processes:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 GAN 模型的过程涉及两个过程：
- en: Train the discriminator. This is a straightforward supervised training process.
    The network is given labeled images coming from the generator (fake) and the training
    data (real), and it learns to classify between real and fake images with a sigmoid
    prediction output. Nothing new here.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练判别器。这是一个直接的监督训练过程。网络被提供了来自生成器（伪造）和训练数据（真实）的标记图像，并学习通过 sigmoid 预测输出区分真实和伪造图像。这里没有什么新内容。
- en: Train the generator. This process is a little tricky. The generator model cannot
    be trained alone like the discriminator. It needs the discriminator model to tell
    it whether it did a good job of faking images. So, we create a combined network
    to train the generator, composed of both discriminator and generator models.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器。这个过程有点棘手。生成器模型不能像判别器那样单独训练。它需要判别器模型来告诉它是否成功地伪造了图像。因此，我们创建了一个由判别器和生成器模型组成的组合网络来训练生成器。
- en: Think of the training processes as two parallel lanes. One lane trains the discriminator
    alone, and the other lane is the combined model that trains the generator. The
    GAN training process is illustrated in figure 8.11.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练过程想象成两条平行的车道。一条车道单独训练判别器，另一条车道是训练生成器的组合模型。GAN 训练过程如图 8.11 所示。
- en: '![](../Images/8-11.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 8-11](../Images/8-11.png)'
- en: Figure 8.11 The process flow to train GANs
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 训练 GAN 的流程图
- en: 'As you can see in figure 8.11, when training the combined model, we freeze
    the weights of the discriminator because this model focuses only on training the
    generator. We will discuss the intuition behind this idea when we explain the
    generator training proces. For now, just know that we need to build and train
    two models: one for the discriminator alone and the other for both discriminator
    and generator models.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图8.11中看到的，在训练组合模型时，我们冻结了判别器的权重，因为这个模型只专注于训练生成器。当我们解释生成器训练过程时，我们将讨论这个想法背后的直觉。现在，只需知道我们需要构建和训练两个模型：一个用于单独的判别器，另一个用于判别器和生成器模型。
- en: Both processes follow the traditional neural network training process explained
    in chapter 2\. It starts with the feedforward process and then makes predictions
    and calculates and backpropagates the error. When training the discriminator,
    the error is backpropagated back to the discriminator model to update its weights;
    in the combined model, the error is backpropagated back to the generator to update
    its weights.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个过程都遵循第2章中解释的传统神经网络训练过程。它从正向传播过程开始，然后进行预测，计算并反向传播错误。在训练判别器时，错误被反向传播回判别器模型以更新其权重；在组合模型中，错误被反向传播回生成器以更新其权重。
- en: During the training iterations, we follow the same neural network training procedure
    to observe the network’s performance and tune its hyperparameters until we see
    that the generator is achieving satisfying results for our problem. This is when
    we can stop the training and deploy the generator model. Now, let’s see how we
    compile the discriminator and the combined networks to train the GAN model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练迭代过程中，我们遵循与第2章中解释的传统神经网络训练过程相同的步骤，以观察网络的性能并调整其超参数，直到我们看到生成器在我们的问题上达到了令人满意的结果。这时，我们可以停止训练并部署生成器模型。现在，让我们看看我们如何编译判别器和组合网络来训练GAN模型。
- en: Training the discriminator
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练判别器
- en: As we said before, this is a straightforward process. First, we build the model
    from the `discriminator_model` method that we created earlier in this chapter.
    Then we compile the model and use the `binary_crossentropy` loss function and
    an `optimizer` of your choice (we use `Adam` in this example).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所述，这是一个简单直接的过程。首先，我们从本章早期创建的`discriminator_model`方法构建模型。然后我们编译模型，并使用你选择的`optimizer`（在这个例子中我们使用`Adam`）以及`binary_crossentropy`损失函数。
- en: 'Let’s see the Keras implementation that builds and compiles the generator.
    Please note that this code snippet is not meant to be compilable on its own--it
    is here for illustration. At the end of this chapter, you can find the full code
    of this project:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Keras实现构建和编译生成器的代码。请注意，这个代码片段本身并不是为了独立编译而设计的——它在这里是为了说明。在本章末尾，你可以找到这个项目的完整代码：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can train the model by creating random training batches using Keras’ `train_on`
    `_batch` method to run a single gradient update on a single batch of data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用Keras的`train_on_batch`方法创建随机训练批次来训练模型，对单个数据批次运行单个梯度更新：
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Sample noise
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 采样噪声
- en: ❷ Generates a batch of new images
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成一批新的图像
- en: Training the generator (combined model)
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练生成器（组合模型）
- en: 'Here is the one tricky part in training GANs: training the generator. While
    the discriminator can be trained in isolation from the generator model, the generator
    needs the discriminator in order to be trained. For this, we build a combined
    model that contains both the generator and the discriminator, as shown in figure
    8.12.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练GANs时，有一个棘手的部分：训练生成器。虽然判别器可以在不与生成器模型分离的情况下进行训练，但生成器需要判别器才能进行训练。为此，我们构建了一个包含生成器和判别器的组合模型，如图8.12所示。
- en: '![](../Images/8-12.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-12.png)'
- en: Figure 8.12 Illustration of the combined model that contains both the generator
    and discriminator models
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12展示了包含生成器和判别器模型的组合模型示意图
- en: When we want to train the generator, we freeze the weights of the discriminator
    model because the generator and discriminator have different loss functions pulling
    in different directions. If we don’t freeze the discriminator weights, it will
    be pulled in the same direction the generator is learning so it will be more likely
    to predict generated images as real, which is not the desired outcome. Freezing
    the weights of the discriminator model doesn’t affect the existing discriminator
    model that we compiled earlier when we were training the discriminator. Think
    of it as having two discriminator models--this is not the case, but it is easier
    to imagine.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要训练生成器时，我们冻结判别器模型的权重，因为生成器和判别器有不同的损失函数，将它们拉向不同的方向。如果我们不冻结判别器权重，它将被拉向生成器学习的同一方向，因此它更有可能将生成的图像预测为真实，这不是我们想要的结果。冻结判别器模型的权重不会影响我们在训练判别器时编译的现有判别器模型。将其想象为有两个判别器模型——这并不是实际情况，但这样更容易想象。
- en: 'Now, let’s build the combined model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建组合模型：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Builds the generator
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建生成器
- en: ❷ The generator takes noise as input and generates an image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成器将噪声作为输入并生成一个图像。
- en: ❸ Freezes the weights of the discriminator model
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 冻结判别器模型的权重
- en: ❹ The discriminator takes generated images as input and determines their validity.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 判别器将生成的图像作为输入并确定其有效性。
- en: ❺ The combined model (stacked generator and discriminator) trains the generator
    to fool the discriminator.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 组合模型（堆叠的生成器和判别器）训练生成器以欺骗判别器。
- en: 'Now that we have built the combined model, we can proceed with the training
    process as normal. We compile the combined model with a `binary_crossentropy`
    loss function and an Adam optimizer:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了组合模型，我们可以像往常一样进行训练过程。我们使用`binary_crossentropy`损失函数和Adam优化器编译组合模型：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Trains the generator (wants the discriminator to mistake images for being
    real)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练生成器（希望判别器将图像误认为是真实的）
- en: Training epochs
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练epoch
- en: In the project at the end of the chapter, you will see that the previous code
    snippet is put inside a loop function to perform the training for a certain number
    of epochs. For each epoch, the two compiled models (discriminator and combined)
    are trained simultaneously. During the training process, both the generator and
    discriminator improve. You can observe the performance of your GAN by printing
    out the results after each epoch (or a set of epochs) to see how the generator
    is doing at generating synthetic images. Figure 8.13 shows an example of the evolution
    of the generator’s performance throughout its training process on the MNIST dataset.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾的项目中，您将看到之前的代码片段被放入循环函数中，以执行一定数量的epoch的训练。对于每个epoch，两个编译好的模型（判别器和组合）同时训练。在训练过程中，生成器和判别器都会得到提升。您可以通过在每个epoch（或一组epoch）后打印出结果来观察您的GAN的性能，看看生成器在生成合成图像方面的表现。图8.13展示了生成器在其训练过程中在MNIST数据集上性能演变的一个示例。
- en: '![](../Images/8-13.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-13.png)'
- en: Figure 8.13 The generator gets better at mimicking the handwritten digits of
    the MNIST dataset throughout its training from epoch 0 to epoch 9,500.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 在其训练过程中，从第0个epoch到第9,500个epoch，生成器在模仿MNIST数据集的手写数字方面变得越来越擅长。
- en: In the example, epoch 0 starts with random noise data that doesn’t yet represent
    the features in the training dataset. As the GAN model goes through the training,
    its generator gets better and better at creating high-quality imitations of the
    training dataset that can fool the discriminator. Manually observing the generator’s
    performance is a good way to evaluate system performance to decide on the number
    of epochs and when to stop training. We’ll look more at GAN evaluation techniques
    in section 8.2.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，第0个epoch以随机噪声数据开始，这些数据尚未代表训练数据集中的特征。随着GAN模型经过训练，其生成器在创建高质量的训练数据集仿制品方面变得越来越擅长，这些仿制品可以欺骗判别器。手动观察生成器的性能是评估系统性能、决定epoch数量和何时停止训练的好方法。我们将在第8.2节中更详细地探讨GAN评估技术。
- en: 8.1.5 GAN minimax function
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.5 GAN最小-最大函数
- en: GAN training is more of a zero-sum game than an optimization problem. In zero-sum
    games, the total utility score is divided among the players. An increase in one
    player’s score results in a decrease in another player’s score. In AI, this is
    called minimax game theory. Minimax is a decision-making algorithm, typically
    used in turn-based, two-player games. The goal of the algorithm is to find the
    optimal next move. One player, called the maximizer, works to get the maximum
    possible score; the other player, called the minimizer, tries to get the lowest
    score by counter-moving against the maximizer.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: GAN训练更像是一场零和游戏，而不是一个优化问题。在零和游戏中，总效用分数在玩家之间分配。一个玩家分数的增加会导致另一个玩家分数的减少。在人工智能中，这被称为最小-最大博弈论。最小-最大是一种决策算法，通常用于回合制、两人游戏。该算法的目标是找到最佳下一步。一个被称为最大化者的玩家努力获得最高分数；另一个被称为最小化者的玩家通过对抗最大化者来尝试获得最低分数。
- en: 'GANs play a minimax game where the entire network attempts to optimize the
    function *V*(*D,G*) in the following equation:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: GAN在以下方程中玩一个最小-最大游戏，其中整个网络试图优化 *V*(*D,G*) 函数：
- en: '![](../Images/Equation-3.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/Equation-3.png)'
- en: The goal of the discriminator (*D*) is to maximize the probability of getting
    the correct label of the image. The generator’s *(G)* goal, on the other hand,
    is to minimize the chances of getting caught. So, we train *D* to maximize the
    probability of assigning the correct label to both training examples and samples
    from *G*. We simultaneously train *G* to minimize log(1 - *D*(*G*(*z*))). In other
    words, *D* and *G* play a two-player minimax game with the value function *V*(*D,G*).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器(*D*)的目标是最大化获取图像正确标签的概率。另一方面，生成器*(G)*的目标是尽量减少被抓住的机会。因此，我们训练*D*来最大化将正确标签分配给训练示例和来自*G*的样本的概率。我们同时训练*G*来最小化
    log(1 - *D*(*G*(*z*))).换句话说，*D*和*G*通过价值函数 *V*(*D,G*) 进行两人最小-最大游戏。
- en: Minimax game theory
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大博弈论
- en: In a two-person, zero-sum game, a person can win only if the other player loses.
    No cooperation is possible. This game theory is widely used in games such as tic-tac-toe,
    backgammon, mancala, chess, and so on. The maximizer player tries to get the highest
    score possible, while the minimizer player tries to do the opposite and get the
    lowest score possible.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在两人零和游戏中，一个人只有在另一个玩家失败的情况下才能获胜。没有合作的可能性。这种博弈论在诸如井字棋、国际象棋、曼卡拉、象棋等游戏中被广泛使用。最大化玩家试图获得尽可能高的分数，而最小化玩家则试图做相反的事情，以获得尽可能低的分数。
- en: In a given game state, if the maximizer has the upper hand, then the score will
    tend to be a positive value. If the minimizer has the upper hand in that state,
    then the score will tend to be a negative value. The values are calculated by
    heuristics that are unique for every type of game.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的游戏状态下，如果最大化者处于优势，那么分数将倾向于正值。如果最小化者在该状态下处于优势，那么分数将倾向于负值。这些值是通过为每种游戏类型独特的启发式算法计算得出的。
- en: Like any other mathematical equation, the preceding one looks terrifying to
    anyone who isn’t well versed in the math behind it, but the idea it represents
    is simple yet powerful. It’s just a mathematical representation of the two competing
    objectives of the discriminator and the generator models. Let’s go through the
    symbols first (table 8.1) and then explain it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他数学方程一样，前面的方程对于不熟悉其背后数学的人来说可能看起来令人恐惧，但它所代表的思想简单而强大。它只是判别器和生成器模型两个竞争目标的数学表示。让我们首先通过（表8.1）了解符号，然后进行解释。
- en: Table 8.1 Symbols used in the minimax equation
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 最小-最大方程中使用的符号
- en: '| Symbol | Explanation |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 说明 |'
- en: '| G | Generator. |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| G | 生成器。 |'
- en: '| D | Discriminator. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| D | 判别器。 |'
- en: '| z | Random noise fed to the generator (G). |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| z | 供给生成器(G)的随机噪声。 |'
- en: '| *G*(*z*) | The generator takes the random noise data (*z*) and tries to reconstruct
    the real images. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| *G*(*z*) | 生成器接收随机噪声数据(*z*)并尝试重建真实图像。 |'
- en: '| *D*(*G*(*z*)) | The discriminator (*D*) output from the generator. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| *D*(*G*(*z*)) | 来自生成器的判别器(*D*)输出。 |'
- en: '| log*D*(*x*) | The discriminator’s probability output for real data. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| log*D*(*x*) | 判别器对真实数据的概率输出。 |'
- en: 'The discriminator takes its input from two sources:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器从两个来源获取输入：
- en: Data from the generator, *G*(*z*)--This is fake data (*z*). The discriminator
    output from the generator is denoted as *D*(*G*(*z*)).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器的数据，*G*(*z*)--这是假数据(*z*)。生成器输出的判别器表示为 *D*(*G*(*z*)).
- en: Real input from the real training data (*x*)--The discriminator output from
    the real data is denoted as log D(*x*).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自真实训练数据(*x*)的真实输入--来自真实数据的判别器输出表示为 log D(*x*).
- en: 'To simplify the minimax equation, the best way to look at it is to break it
    down into two components: the discriminator training function and the generator
    training (combined model) function. During the training process, we created two
    training flows, and each has its own error function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化最小-最大方程，最好的方法是将其分解为两个部分：判别器训练函数和生成器训练（联合模型）函数。在训练过程中，我们创建了两个训练流程，每个流程都有自己的错误函数：
- en: 'One for the discriminator alone, represented by the following function that
    aims to maximize the minimax function by making the predictions as close as possible
    to 1:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个是单独的判别器，以下函数旨在通过使预测尽可能接近 1 来最大化最小-最大函数：
- en: '*E[x ~p[data]]* [log*D*(*x*)]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E[x ~p[data]]* [log*D*(*x*)]'
- en: 'One for the combined model to train the generator represented by the following
    function, which aims to minimize the minimax function by making the predictions
    as close as possible to 0:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个是用于训练生成器的联合模型，以下函数旨在通过使预测尽可能接近 0 来最小化最小-最大函数：
- en: '*E[z ~P z]*(*z*) [log(1 - *D*(*G*(*z*)))]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E[z ~P z]*(*z*) [log(1 - *D*(*G*(*z*)))]'
- en: 'Now that we understand the equation symbols and have a better understanding
    of how the minimax function works, let’s look at the function again:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了方程符号，并对最小-最大函数的工作原理有了更好的理解，让我们再次看看这个函数：
- en: '![](../Images/Equation-4.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/Equation-4.png)'
- en: The goal of the minimax objective function *V*(*D, G* ) is to maximize *D*(*x*)
    from the true data distribution and minimize *D*(*G*(*z*)) from the fake data
    distribution. To achieve this, we use the log-likelihood of *D*(*x*) and 1 - *D*(*z*)
    in the objective function. The log of a vvalue just makes sure that the closer
    we are to an incorrect value, the more we are penalized.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大目标函数 *V*(*D, G*) 的目标是最大化从真实数据分布中的 *D*(*x*)，并最小化从伪造数据分布中的 *D*(*G*(*z*))。为了实现这一点，我们在目标函数中使用
    *D*(*x*) 的对数似然和 1 - *D*(*z*)。一个值的对数只是确保我们越接近一个错误值，我们受到的惩罚就越大。
- en: Early in the GAN training process, the discriminator will reject fake data from
    the generator with high confidence, because the fake images are very different
    from the real training data--the generator hasn’t learned yet. As we train the
    discriminator to maximize the probability of assigning the correct labels to both
    real examples and fake images from the generator, we simultaneously train the
    generator to minimize the discriminator classification error for the generated
    fake data. The discriminator wants to maximize objectives such that *D*(*x*) is
    close to 1 for real data and *D*(*G*(*z*)) is close to 0 for fake data. On the
    other hand, the generator wants to minimize objectives such that *D*(*G*(*z*))
    is close to 1 so that the discriminator is fooled into thinking the generated
    *G*(*z*) is real. We stop the training when the fake data generated by the generator
    is recognized as real data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GAN 训练过程的早期，判别器会以高置信度拒绝生成器产生的伪造数据，因为伪造的图像与真实训练数据非常不同——生成器还没有学会。随着我们训练判别器以最大化将正确标签分配给真实示例和生成器产生的伪造图像的概率，我们同时训练生成器以最小化生成伪造数据时的判别器分类错误。判别器希望最大化目标，使得对于真实数据
    *D*(*x*) 接近 1，对于伪造数据 *D*(*G*(*z*)) 接近 0。另一方面，生成器希望最小化目标，使得 *D*(*G*(*z*)) 接近 1，这样判别器就会上当，认为生成的
    *G*(*z*) 是真实的。当生成器产生的伪造数据被识别为真实数据时，我们停止训练。
- en: 8.2 Evaluating GAN models
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 评估 GAN 模型
- en: Deep learning neural network models that are used for classification and detection
    problems are trained with a loss function until convergence. A GAN generator model,
    on the other hand, is trained using a discriminator that learns to classify images
    as real or generated. As we learned in the previous section, both the generator
    and discriminator models are trained together to maintain an equilibrium. As such,
    no objective loss function is used to train the GAN generator models, and there
    is no way to objectively assess the progress of the training and the relative
    or absolute quality of the model from loss alone. This means models must be evaluated
    using the quality of the generated synthetic images and by manually inspecting
    the generated images.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类和检测问题的深度学习神经网络模型通过损失函数进行训练，直到收敛。另一方面，GAN 生成器模型使用一个学习将图像分类为真实或生成的判别器进行训练。正如我们在上一节中学到的，生成器和判别器模型一起训练以保持平衡。因此，没有使用目标损失函数来训练
    GAN 生成器模型，也没有办法仅从损失中客观地评估训练的进度和模型的相对或绝对质量。这意味着必须使用生成的合成图像的质量以及通过手动检查生成的图像来评估模型。
- en: A good way to identify evaluation techniques is to review research papers and
    the techniques the authors used to evaluate their GANs. Tim Salimans et al. (2016)
    evaluated their GAN performance by having human annotators manually judge the
    visual quality of the synthesized samples.[3](#pgfId-1185219) They created a web
    interface and hired annotators on Amazon Mechanical Turk (MTurk) to distinguish
    between generated data and real data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 识别评估技术的一个好方法是回顾研究论文和作者用来评估他们 GAN 的技术。Tim Salimans 等人（2016）通过让人类标注员手动判断合成样本的视觉质量来评估他们的
    GAN 性能。[3](#pgfId-1185219) 他们创建了一个网络界面，并在 Amazon Mechanical Turk (MTurk) 上雇佣标注员来区分生成数据和真实数据。
- en: 'One downside of using human annotators is that the metric varies depending
    on the setup of the task and the motivation of the annotators. The team also found
    that results changed drastically when they gave annotators feedback about their
    mistakes: by learning from such feedback, annotators are better able to point
    out the flaws in generated images, giving a more pessimistic quality assessment.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人类标注员的一个缺点是，该指标取决于任务的设置和标注员的动机。该团队还发现，当他们对标注员关于其错误的反馈时，结果发生了巨大变化：通过从这种反馈中学习，标注员能够更好地指出生成图像中的缺陷，从而给出更悲观的品质评估。
- en: 'Other non-manual approaches were used by Salimans et al. and by other researchers
    we will discuss in this section. In general, there is no consensus about a correct
    way to evaluate a given GAN generator model. This makes it challenging for researchers
    and practitioners to do the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Salimans 等人和本节中我们将讨论的其他研究人员使用了其他非手动方法。一般来说，关于如何评估给定的 GAN 生成器模型还没有共识。这使得研究人员和实践者难以进行以下操作：
- en: Select the best GAN generator model during a training run--in other words, decide
    when to stop training.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练运行中选择最佳的 GAN 生成器模型--换句话说，决定何时停止训练。
- en: Choose generated images to demonstrate the capability of a GAN generator model.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择生成的图像来展示 GAN 生成器模型的性能。
- en: Compare and benchmark GAN model architectures.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较和基准测试 GAN 模型架构。
- en: Tune the model hyperparameters and configuration and compare results.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型超参数和配置，并比较结果。
- en: Finding quantifiable ways to understand a GAN’s progress and output quality
    is still an active area of research. A suite of qualitative and quantitative techniques
    has been developed to assess the performance of a GAN model based on the quality
    and diversity of the generated synthetic images. Two commonly used evaluation
    metrics for image quality and diversity are the inception score and the Fréchet
    inception distance (FID ). In this section, you will discover techniques for evaluating
    GAN models based on generated synthetic images.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找可量化的方法来理解 GAN 的进度和输出质量仍然是研究的一个活跃领域。已经开发了一套定性和定量技术，用于根据生成的合成图像的质量和多样性来评估 GAN
    模型的性能。用于图像质量和多样性的两个常用评估指标是 Inception 分数和 Fréchet Inception 距离 (FID)。在本节中，你将了解基于生成的合成图像评估
    GAN 模型的技术。
- en: 8.2.1 Inception score
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 Inception 分数
- en: 'The inception score is based on a heuristic that realistic samples should be
    able to be classified when passed through a pretrained network such as Inception
    on ImageNet (hence the name inception score). The idea is really simple. The heuristic
    relies on two values:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 分数基于一个启发式方法，即现实样本应该能够在通过预训练网络（如 ImageNet 上的 Inception）时被分类（因此得名 Inception
    分数）。这个想法真的很简单。启发式依赖于两个值：
- en: High predictability of the generated image --We apply a pretrained inception
    classifier model to every generated image and get its softmax prediction. If the
    generated image is good enough, then it should give us a high predictability score.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的图像高可预测性 -- 我们将预训练的 Inception 分类器模型应用于每个生成的图像，并获取其 softmax 预测。如果生成的图像足够好，那么它应该给出一个高可预测性分数。
- en: Diverse generated samples --No classes should dominate the distribution of the
    generated images.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成样本的多样性 -- 不应有任何类别主导生成的图像分布。
- en: A large number of generated images are classified using the model. Specifically,
    the probability of the image belonging to each class is predicted. The probabilities
    are then summarized in the score to capture both how much each image looks like
    a known class and how diverse the set of images is across the known classes. If
    both these traits are satisfied, there should be a large inception score. A higher
    inception score indicates better-quality generated images.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型对大量生成的图像进行分类。具体来说，预测图像属于每个类的概率。然后将这些概率总结到分数中，以捕捉每个图像看起来像已知类别的程度以及图像集合在已知类别之间的多样性。如果这两个特性都满足，应该有一个较大的
    inception score。较高的 inception score 表示生成的图像质量更好。
- en: 8.2.2 Fréchet inception distance (FID)
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 Fréchet inception 距离 (FID)
- en: The FID score was proposed and used by Martin Heusel et al. in 2017.[4](#pgfId-1185255)
    The score was proposed as an improvement over the existing inception score.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: FID 分数是由 Martin Heusel 等人在 2017 年提出并使用的。[4](#pgfId-1185255) 该分数被提出作为对现有 inception
    score 的改进。
- en: Like the inception score, the FID score uses the Inception model to capture
    specific features of an input image. These activations are calculated for a collection
    of real and generated images. The activations for each real and generated image
    are summarized as a multivariate Gaussian, and the distance between these two
    distributions is then calculated using the Fréchet distance, also called the Wasserstein-2
    distance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与 inception score 类似，FID 分数使用 Inception 模型来捕捉输入图像的特定特征。这些激活值是针对一组真实和生成的图像计算的。每个真实和生成的图像的激活值被总结为一个多元高斯分布，然后使用
    Fréchet 距离（也称为 Wasserstein-2 距离）来计算这两个分布之间的距离。
- en: An important note is that the FID needs a decent sample size to give good results
    (the suggested size is 50,000 samples). If you use too few samples, you will end
    up overestimating your actual FID, and the estimates will have a large variance.
    A lower FID score indicates more realistic images that match the statistical properties
    of real images.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的一点是，FID 需要一个合理的样本量才能给出良好的结果（建议的大小是 50,000 个样本）。如果你使用太多的样本，你最终会高估你的实际 FID，并且估计值会有很大的方差。较低的
    FID 分数表示更符合真实图像统计特性的真实图像。
- en: 8.2.3 Which evaluation scheme to use
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 要使用哪种评估方案
- en: Both measures (inception score and FID) are easy to implement and calculate
    on batches of generated images. As such, the practice of systematically generating
    images and saving models during training can and should continue to be used to
    allow post hoc model selection. Diving deep into the inception score and FID is
    out of the scope of this book. As mentioned earlier, this is an active area of
    research, and there is no consensus in the industry as of the time of writing
    about the one best approach to evaluate GAN performance. Different scores assess
    various aspects of the image-generation process, and it is unlikely that a single
    score can cover all aspects. The goal of this section is to expose you to some
    techniques that have been developed in recent years to automate the GAN evaluation
    process, but manual evaluation is still widely used.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种度量（inception score 和 FID）都很容易在生成的图像批次上实现和计算。因此，在训练过程中系统地生成图像并保存模型的做法可以，并且应该继续使用，以便进行事后模型选择。深入探讨
    inception score 和 FID 超出了本书的范围。如前所述，这是一个活跃的研究领域，截至写作时，行业内还没有关于评估 GAN 性能的最佳方法的共识。不同的分数评估图像生成过程的各个方面，而且不太可能有一个单一的分数可以涵盖所有方面。本节的目标是让你了解近年来开发的一些自动化
    GAN 评估过程的技巧，但手动评估仍然被广泛使用。
- en: When you are getting started, it is a good idea to begin with manual inspection
    of generated images in order to evaluate and select generator models. Developing
    GAN models is complex enough for both beginners and experts; manual inspection
    can get you a long way while refining your model implementation and testing model
    configurations.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当你刚开始时，从手动检查生成的图像开始以评估和选择生成器模型是一个好主意。对于初学者和专家来说，开发 GAN 模型本身就足够复杂了；手动检查可以在改进你的模型实现和测试模型配置方面让你走得很远。
- en: Other researchers are taking different approaches by using domain-specific evaluation
    metrics. For example, Konstantin Shmelkov and his team (2018) used two measures
    based on image classification, GAN-train and GAN-test, which approximated the
    recall (diversity) and precision (quality of the image) of GANs, respectively.[5](#pgfId-1185280)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究人员通过使用特定领域的评估指标采取了不同的方法。例如，Konstantin Shmelkov及其团队（2018年）使用了基于图像分类的两个度量，GAN-train和GAN-test，分别近似GAN的召回率（多样性）和精确度（图像质量）。[5](#pgfId-1185280)
- en: 8.3 Popular GAN applications
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 流行GAN应用
- en: Generative modeling has come a long way in the last five years. The field has
    developed to the point where it is expected that the next generation of generative
    models will be more comfortable creating art than humans. GANs now have the power
    to solve the problems of industries like healthcare, automotive, fine arts, and
    many others. In this section, we will learn about some of the use cases of adversarial
    networks and which GAN architecture is used for that application. The goal of
    this section is not to implement the variations of the GAN network, but to provide
    some exposure to potential applications of GAN models and resources for further
    reading.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的五年里，生成模型已经取得了长足的进步。该领域已经发展到预期下一代生成模型在创作艺术方面将比人类更加得心应手。现在，GANs（生成对抗网络）已经拥有了解决医疗保健、汽车、美术等多个行业问题的能力。在本节中，我们将了解一些对抗网络的用例以及用于该应用的GAN架构。本节的目标不是实现GAN网络的变体，而是提供一些关于GAN模型潜在应用的曝光以及进一步阅读的资源。
- en: 8.3.1 Text-to-photo synthesis
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 文本到照片合成
- en: Synthesis of high-quality images from text descriptions is a challenging problem
    in CV. Samples generated by existing text-to-image approaches can roughly reflect
    the meaning of the given descriptions, but they fail to contain necessary details
    and vivid object parts.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本描述合成高质量图像是计算机视觉中的一个挑战性问题。现有文本到图像方法生成的样本可以大致反映给定描述的意义，但它们缺乏必要的细节和生动的物体部分。
- en: The GAN network that was built for this application is the stacked generative
    adversarial network (StackGAN).[6](#pgfId-1185294) Zhang et al. were able to generate
    256 × 256 photorealistic images conditioned on text descriptions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为此应用构建的GAN网络是堆叠生成对抗网络（StackGAN）。[6](#pgfId-1185294) 张等人能够根据文本描述生成256 × 256的逼真图像。
- en: 'StackGANs work in two stages (figure 8.14):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: StackGANs在两个阶段工作（图8.14）：
- en: 'Stage-I : StackGAN sketches the primitive shape and colors of the object based
    on the given text description, yielding low-resolution images.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一阶段：StackGAN根据给定的文本描述绘制对象的原始形状和颜色，生成低分辨率图像。
- en: 'Stage-II : StackGAN takes the output of stage-I and a text description as input
    and generates high-resolution images with photorealistic details. It is able to
    rectify defects in the images created in stage-I and add compelling details with
    the refinement process.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二阶段：StackGAN以第一阶段的结果和文本描述作为输入，生成具有逼真细节的高分辨率图像。它能够纠正第一阶段创建的图像中的缺陷，并通过细化过程添加引人注目的细节。
- en: '![](../Images/8-14.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-14.png)'
- en: 'Figure 8.14 (*a*) Stage-I: Given text descriptions, StackGAN sketches rough
    shapes and basic colors of objects, yielding low-resolution images. (*b*) Stage-II
    takes Stage-I results and text descriptions as inputs, and generates high-resolution
    images with photorealistic details. (Source: Zhang et al., 2016.)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 (*a*) 第一阶段：给定文本描述，StackGAN绘制对象的粗糙形状和基本颜色，生成低分辨率图像。(*b*) 第二阶段以第一阶段的结果和文本描述作为输入，生成具有逼真细节的高分辨率图像。（来源：张等，2016年。）
- en: 8.3.2 Image-to-image translation (Pix2Pix GAN)
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 图像到图像翻译（Pix2Pix GAN）
- en: 'Image-to-image translation is defined as translating one representation of
    a scene into another, given sufficient training data. It is inspired by the language
    translation analogy: just as an idea can be expressed by many different languages,
    a scene may be rendered by a grayscale image, RGB image, semantic label maps,
    edge sketches, and so on. In figure 8.15, image-to-image translation tasks are
    demonstrated on a range of applications such as converting street scene segmentation
    labels to real images, grayscale to color images, sketches of products to product
    photographs, and day photographs to night ones.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像的转换被定义为在足够的训练数据下将场景的一种表示转换为另一种表示。它受到语言翻译类比的影响：正如一个想法可以用许多不同的语言表达一样，一个场景可以通过灰度图像、RGB图像、语义标签图、边缘草图等来呈现。在图8.15中，图像到图像的转换任务在一系列应用中得到了演示，例如将街景分割标签转换为真实图像、灰度转换为彩色图像、产品草图转换为产品照片以及白天照片转换为夜晚照片。
- en: 'Pix2Pix is a member of the GAN family designed by Phillip Isola et al. in 2016
    for general-purpose image-to-image translation.[7](#pgfId-1185322) The Pix2Pix
    network architecture is similar to the GAN concept: it consists of a generator
    model for outputting new synthetic images that look realistic, and a discriminator
    model that classifies images as real (from the dataset) or fake (generated). The
    training process is also similar to that used for GANs: the discriminator model
    is updated directly, whereas the generator model is updated via the discriminator
    model. As such, the two models are trained simultaneously in an adversarial process
    where the generator seeks to better fool the discriminator and the discriminator
    seeks to better identify the counterfeit images.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2Pix是由Phillip Isola等人于2016年设计的GAN家族成员，用于通用图像到图像的转换。[7](#pgfId-1185322) Pix2Pix网络架构类似于GAN的概念：它包括一个生成器模型，用于输出看起来逼真的新合成图像，以及一个判别器模型，用于将图像分类为真实（来自数据集）或伪造（生成）。训练过程也与GAN类似：判别器模型直接更新，而生成器模型通过判别器模型更新。因此，这两个模型在对抗过程中同时训练，其中生成器试图更好地欺骗判别器，而判别器试图更好地识别伪造图像。
- en: '![](../Images/8-15.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-15.png)'
- en: Figure 8.15 Examples of Pix2Pix applications taken from the original paper.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 从原始论文中摘取的Pix2Pix应用示例。
- en: The novel idea of Pix2Pix networks is that they learn a loss function adapted
    to the task and data at hand, which makes them applicable in a wide variety of
    settings. They are a type of conditional GAN (cGAN) where the generation of the
    output image is conditional on an input source image. The discriminator is provided
    with both a source image and the target image and must determine whether the target
    is a plausible transformation of the source image.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2Pix网络的新颖之处在于，它们学习一个适应当前任务和数据的损失函数，这使得它们可以在各种环境中应用。它们是一种条件生成对抗网络（cGAN），其中输出图像的生成取决于输入源图像。判别器被提供了源图像和目标图像，并必须确定目标是否是源图像的合理变换。
- en: The results of the Pix2Pix network are really promising for many image-to-image
    translation tasks. Visit [https://affinelayer.com/pixsrv](https://affinelayer.com/pixsrv)
    to play more with the Pix2Pix network; this site has an interactive demo created
    by Isola and team in which you can convert sketch edges of cats or products to
    photos and façades to real images.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2Pix网络在许多图像到图像的转换任务中取得了非常令人鼓舞的结果。访问[https://affinelayer.com/pixsrv](https://affinelayer.com/pixsrv)来更多体验Pix2Pix网络；该网站有一个由Isola及其团队创建的交互式演示，您可以在其中将猫或产品的草图边缘转换为照片和立面转换为真实图像。
- en: 8.3.3 Image super-resolution GAN (SRGAN)
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 图像超分辨率GAN（SRGAN）
- en: A certain type of GAN models can be used to convert low-resolution images into
    high-resolution images. This type is called a super-resolution generative adversarial
    networks (SRGAN) and was introduced by Christian Ledig et al. in 2016.[8](#pgfId-1185339)
    Figure 8.16 shows how SRGAN was able to create a very high-resolution image.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特定的GAN模型可以用来将低分辨率图像转换为高分辨率图像。这种类型被称为超分辨率生成对抗网络（SRGAN），由Christian Ledig等人于2016年引入。[8](#pgfId-1185339)
    图8.16展示了SRGAN如何创建一个非常高分辨率的图像。
- en: '![](../Images/8-16.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-16.png)'
- en: 'Figure 8.16 SRGAN converting a low-resolution image to a high-resolution image.
    (Source: Ledig et al., 2016.)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 SRGAN将低分辨率图像转换为高分辨率图像。（来源：Ledig等人，2016年。）
- en: 8.3.4 Ready to get your hands dirty?
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 准备好动手实践了吗？
- en: GAN models have huge potential for creating and imagining new realities that
    have never existed before. The applications mentioned in this chapter are just
    a few examples to give you an idea of what GANs can do today. Such applications
    come out every few weeks and are worth trying. If you are interested in getting
    your hands dirty with more GAN applications, visit the amazing Keras-GAN repository
    at [https://github.com/ eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN),
    maintained by Erik Linder-Norén. It includes many GAN models created using Keras
    and is an excellent resource for Keras examples. Much of the code in this chapter
    was inspired by and adapted from this repository.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: GAN模型在创造和想象从未存在过的新现实方面具有巨大的潜力。本章中提到的应用只是几个例子，以给你一个关于GAN今天能做什么的印象。这样的应用每隔几周就会出现，值得一试。如果你对探索更多GAN应用感兴趣，请访问由Erik
    Linder-Norén维护的惊人的Keras-GAN存储库[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)，它包括许多使用Keras创建的GAN模型，是Keras示例的极好资源。本章中的大部分代码都受到了这个存储库的启发和改编。
- en: '8.4 Project: Building your own GAN'
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 项目：构建自己的GAN
- en: In this project, you’ll build a GAN using convolutional layers in the generator
    and discriminator. This is called a deep convolutional GAN (DCGAN) for short.
    The DCGAN architecture was first explored by Alec Radford et al. (2016), as discussed
    in section 8.1.1, and has seen impressive results in generating new images. You
    can follow along with the implementation in this chapter or run code in the project
    notebook available with this book’s downloadable code.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将使用生成器和判别器中的卷积层构建一个GAN。这被称为深度卷积GAN（DCGAN）。DCGAN架构最初由Alec Radford等人（2016年）探索，如第8.1.1节所述，并在生成新图像方面取得了令人印象深刻的成果。你可以跟随本章中的实现或运行本书可下载代码中的项目笔记本中的代码。
- en: In this project, you’ll be training DCGAN on the Fashion-MNIST dataset ([https://
    github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)).
    Fashion-MNIST consists of 60,000 grayscale images for training and a test set
    of 10,000 images (figure 8.17). Each 28 × 28 grayscale image is associated with
    a label from 10 classes. Fashion-MNIST is intended to serve as a direct replacement
    for the original MNIST dataset for benchmarking machine learning algorithms. I
    chose grayscale images for this project because it requires less computational
    power to train convolutional networks on one-channel grayscale images compared
    to three-channel colored images, which makes it easier for you to train on a personal
    computer without a GPU.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，你将在Fashion-MNIST数据集上训练DCGAN（[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)）。Fashion-MNIST包含60,000个用于训练的灰度图像和10,000个图像的测试集（图8.17）。每个28
    × 28的灰度图像都与10个类别中的一个标签相关联。Fashion-MNIST旨在作为原始MNIST数据集的直接替代品，用于基准测试机器学习算法。我选择灰度图像进行这个项目，因为与三通道彩色图像相比，在单通道灰度图像上训练卷积网络所需的计算能力更少，这使得在没有GPU的个人计算机上训练更容易。
- en: '![](../Images/8-17.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-17.png)'
- en: Figure 8.17 Fashion-MNIST dataset examples
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 Fashion-MNIST数据集示例
- en: 'The dataset is broken into 10 fashion categories. The class labels are as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被划分为10个时尚类别。类别标签如下：
- en: '| Label | Description |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 描述 |'
- en: '| 0 | T-shirt/top |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 0 | T恤/上衣 |'
- en: '| 1 | Trouser |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 裤子 |'
- en: '| 2 | Pullover |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 针织衫 |'
- en: '| 3 | Dress |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 晚礼服 |'
- en: '| 4 | Coat |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 外套 |'
- en: '| 5 | Sandal |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 凉鞋 |'
- en: '| 6 | Shirt |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 衬衫 |'
- en: '| 7 | Sneaker |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 运动鞋 |'
- en: '| 8 | Bag |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 背包 |'
- en: '| 9 | Ankle boot |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 踝靴 |'
- en: 'Step 1: Import libraries'
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：导入库
- en: 'As always, the first thing to do is to import all the libraries we use in this
    project:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，首先要做的是导入我们在项目中使用的所有库：
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Imports the fashion_mnist dataset from Keras
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从Keras导入fashion_mnist数据集
- en: ❷ Imports Keras layers and models
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入Keras层和模型
- en: ❸ Imports numpy and matplotlib
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 导入numpy和matplotlib
- en: 'Step 2: Download and visualize the dataset'
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：下载并可视化数据集
- en: 'Keras makes the Fashion-MNIST dataset available for us to download with just
    one command: `fashion_mnist.load_data()`. Here, we download the dataset and rescale
    the training set to the range -1 to 1 to allow the model to converge faster (see
    the “Data normalization” section in chapter 4 for more details on image scaling):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Keras使我们能够通过一条命令：`fashion_mnist.load_data()`下载Fashion-MNIST数据集。在这里，我们下载了数据集并将训练集重新缩放到-1到1的范围，以便模型更快地收敛（有关图像缩放的更多详细信息，请参阅第4章中的“数据归一化”部分）：
- en: '[PRE8]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Loads the dataset
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载数据集
- en: ❷ Rescales the training data to scale -1 to 1
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将训练数据重新缩放到-1到1的范围
- en: 'Just for the fun of it, let’s visualize the image matrix (figure 8.18):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了乐趣，让我们可视化图像矩阵（图8.18）：
- en: '[PRE9]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/8-18.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-18.png)'
- en: Figure 8.18 A visualized example of the Fashion-MNIST dataset
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 Fashion-MNIST数据集的可视化示例
- en: 'Step 3: Build the generator'
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步：构建生成器
- en: Now, let’s build the generator model. The input will be our noise vector (*z*)
    as explained in section 8.1.5\. The generator architecture is shown in figure
    8.19.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建生成器模型。输入将是我们在第8.1.5节中解释的噪声向量（*z*）。生成器架构如图8.19所示。
- en: '![](../Images/8-19.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-19.png)'
- en: Figure 8.19 Architecture of the generator model
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 生成器模型的架构
- en: 'The first layer is a fully connected layer that is then reshaped into a deep,
    narrow layer, something like 7 × 7 × 128 (in the original DCGAN paper, the team
    reshaped the input to 4 × 4 × 1024). Then we use the upsampling layer to double
    the feature map dimensions from 7 × 7 to 14 × 14 and then again to 28 × 28\. In
    this network, we use three convolutional layers. We also use batch normalization
    and a ReLU activation. For each of these layers, the general scheme is convolution
    ⇒ batch normalization ⇒ ReLU. We keep stacking up layers like this until we get
    the final transposed convolution layer with shape 28 × 28 × 1:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是一个全连接层，然后将其重塑为一个深而窄的层，类似于7 × 7 × 128（在原始DCGAN论文中，团队将输入重塑为4 × 4 × 1024）。然后我们使用上采样层将特征图维度从7
    × 7加倍到14 × 14，然后再加倍到28 × 28。在这个网络中，我们使用三个卷积层。我们还使用了批量归一化和ReLU激活。对于这些层中的每一层，一般的方案是卷积⇒批量归一化⇒ReLU。现在，让我们堆叠这些层，直到我们得到形状为28
    × 28 × 1的最终转置卷积层：
- en: '[PRE10]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Instantiates a sequential model and names it generator
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化一个序列模型并将其命名为生成器
- en: ❷ Adds the dense layer that has a number of neurons = 128 × 7 × 7
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加一个密集层，其神经元数量为128 × 7 × 7
- en: ❸ Reshapes the image dimensions to 7 × 7 × 128
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将图像尺寸重塑为7 × 7 × 128
- en: ❹ Upsampling layer to double the size of the image dimensions to 14 × 14
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 上采样层将图像尺寸加倍到14 × 14
- en: ❺ Adds a convolutional layer to run the convolutional process and batch normalization
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加一个卷积层以运行卷积过程和批量归一化
- en: ❻ Upsamples the image dimensions to 28 × 28
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将图像尺寸上采样到28 × 28
- en: ❼ We don’t add upsampling here because the image size of 28 × 28 is equal to
    the image size in the MNIST dataset. You can adjust this for your own problem.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 我们在这里不添加上采样，因为28 × 28的图像大小与MNIST数据集中的图像大小相同。你可以根据你自己的问题进行调整。
- en: ❽ Prints the model summary
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 打印模型摘要
- en: ❾ Generates the input noise vector of length = 100\. We chose 100 here to create
    a simple network.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 生成长度为100的输入噪声向量。我们在这里选择100是为了创建一个简单的网络。
- en: ❿ Runs the generator model to create the fake image
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 运行生成器模型以创建假图像
- en: ⓫ Returns a model that takes the noise vector as an input and outputs the fake
    image
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 返回一个模型，该模型以噪声向量作为输入并输出假图像
- en: 'Step 4: Build the discriminator'
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：构建判别器
- en: The discriminator is just a convolutional classifier like what we have built
    before (figure 8.20). The inputs to the discriminator are 28 × 28 × 1 images.
    We want a few convolutional layers and then a fully connected layer for the output.
    As before, we want a sigmoid output, and we need to return the logits as well.
    For the depths of the convolutional layers, I suggest starting with 32 or 64 filters
    in the first layer, and then double the depth as you add layers. In this implementation,
    we start with 64 layers, then 128, and then 256\. For downsampling, we do not
    use pooling layers. Instead, we use only strided convolutional layers for downsampling,
    similar to Radford et al.’s implementation.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器只是一个卷积分类器，就像我们之前构建的那样（图8.20）。判别器的输入是28 × 28 × 1的图像。我们想要几个卷积层，然后是一个全连接层作为输出。和之前一样，我们想要一个sigmoid输出，并且需要返回logits。对于卷积层的深度，我建议从第一层的32或64个过滤器开始，然后随着层数的增加，深度加倍。在这个实现中，我们开始使用64层，然后是128层，然后是256层。对于下采样，我们不使用池化层。相反，我们只使用步长卷积层进行下采样，类似于Radford等人实现的方式。
- en: '![](../Images/8-20.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-20.png)'
- en: Figure 8.20 Architecture of the discriminator model
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 判别器模型的架构
- en: 'We also use batch normalization and dropout to optimize training, as we learned
    in chapter 4\. For each of the four convolutional layers, the general scheme is
    convolution ⇒ batch normalization ⇒ leaky ReLU. Now, let’s build the `build_discriminator`
    function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了批量归一化和dropout来优化训练，正如我们在第4章中学到的。对于四个卷积层中的每一个，一般的方案是卷积⇒批量归一化⇒漏ReLU。现在，让我们构建`build_discriminator`函数：
- en: '[PRE11]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Instantiates a sequential model and names it discriminator
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化一个序列模型并将其命名为判别器
- en: ❷ Adds a convolutional layer to the discriminator model
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将卷积层添加到判别器模型中
- en: ❸ Adds a leaky ReLU activation function
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加一个带有泄漏 ReLU 激活函数的泄漏 ReLU 激活函数
- en: ❹ Adds a dropout layer with a 25% dropout probability
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个具有 25% dropout 概率的 dropout 层
- en: ❺ Adds a second convolutional layer with zero padding
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 添加一个具有零填充的第二卷积层
- en: ❻ Adds a zero-padding layer to change the dimension from 7 × 7 to 8 × 8
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 添加一个零填充层以将维度从 7 × 7 更改为 8 × 8
- en: ❼ Adds a batch normalization layer for faster learning and higher accuracy
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 添加一个批量归一化层以加快学习速度并提高准确性
- en: ❽ Adds a third convolutional layer with batch normalization, leaky ReLU, and
    a dropout
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 添加具有批量归一化、泄漏 ReLU 和 dropout 的第三卷积层
- en: ❾ Adds the fourth convolutional layer with batch normalization, leaky ReLU,
    and a dropout
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 添加具有批量归一化、泄漏 ReLU 和 dropout 的第四卷积层
- en: ❿ Flattens the network and adds the output dense layer with sigmoid activation
    function
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 将网络展平并添加具有 sigmoid 激活函数的输出密集层
- en: ⓫ Sets the input image shape
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 设置输入图像形状
- en: ⓬ Runs the discriminator model to get the output probability
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 运行判别器模型以获取输出概率
- en: ⓭ Returns a model that takes the image as input and produces the probability
    output
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 返回一个以图像作为输入并产生概率输出的模型
- en: 'Step 5: Build the combined model'
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 5 步：构建联合模型
- en: As explained in section 8.1.3, to train the generator, we need to build a combined
    network that contains both the generator and the discriminator (figure 8.21).
    The combined model takes the noise signal as input (*z*) and outputs the discriminator’s
    prediction output as fake or real.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 8.1.3 节所述，为了训练生成器，我们需要构建一个包含生成器和判别器的联合网络（图 8.21）。联合模型将噪声信号（*z*）作为输入并输出判别器的预测输出，作为伪造或真实。
- en: '![](../Images/8-21.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-21.png)'
- en: Figure 8.21 Architecture of the combined model
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21 联合模型架构
- en: 'Remember that we want to disable discriminator training for the combined model,
    as explained in detail in section 8.1.3\. When training the generator, we don’t
    want the discriminator to update weights as well, but we still want to include
    the discriminator model in the generator training. So, we create a combined network
    that includes both models but freeze the weights of the discriminator model in
    the combined network:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们希望禁用联合模型中的判别器训练，如第 8.1.3 节中详细解释的那样。在训练生成器时，我们不希望判别器更新权重，但仍然希望将判别器模型包含在生成器训练中。因此，我们创建了一个包含两个模型的联合网络，但在联合网络中冻结了判别器模型的权重：
- en: '[PRE12]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Defines the optimizer
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义优化器
- en: ❷ Builds and compiles the discriminator
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建并编译判别器
- en: ❸ Freezes the discriminator weights because we don’t want to train it during
    generator training
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 冻结判别器权重，因为我们不想在生成器训练期间训练它
- en: ❹ Builds the generator
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 构建生成器
- en: ❺ The generator takes noise as input with latent_dim = 100 and generates images.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 生成器以 latent_dim = 100 的噪声作为输入并生成图像。
- en: ❻ The discriminator takes generated images as input and determines their validity.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 判别器将生成的图像作为输入并确定其有效性。
- en: ❼ The combined model (stacked generator and discriminator) trains the generator
    to fool the discriminator.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 联合模型（堆叠的生成器和判别器）训练生成器以欺骗判别器。
- en: 'Step 6: Build the training function'
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 6 步：构建训练函数
- en: 'When training the GAN model, we train two networks: the discriminator and the
    combined network that we created in the previous section. Let’s build the `train`
    function, which takes the following arguments:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 GAN 模型时，我们训练两个网络：判别器和我们在上一节中创建的联合网络。让我们构建一个 `train` 函数，它接受以下参数：
- en: The number of epochs
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的轮数
- en: The batch size
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小
- en: '`save_interval` to state how often we want to save the results'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save_interval` 来指定我们希望保存结果的频率'
- en: '[PRE13]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Adversarial ground truths
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对抗性真实值
- en: ❷ Selects a random half of images
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择随机的一半图像
- en: ❸ Sample noise, and generates a batch of new images
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 样本噪声，并生成一批新图像
- en: ❹ Trains the discriminator (real classified as 1s and generated as 0s)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练判别器（将真实分类为 1s，将生成分类为 0s）
- en: ❺ Trains the generator (wants the discriminator to mistake images for real ones)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 训练生成器（希望判别器将图像误认为是真实的）
- en: ❻ Prints progress
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印进度
- en: ❼ Saves generated image samples if at save_interval
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 如果在 save_interval 时保存生成的图像样本
- en: 'Before you run the `train()` function, you need to define the following `plot_generated`
    `_images()` function:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 `train()` 函数之前，您需要定义以下 `plot_generated` `_images()` 函数：
- en: '[PRE14]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 7: Train and observe results'
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 7 步：训练并观察结果
- en: 'Now that the code implementation is complete, we are ready to start the DCGAN
    training. To train the model, run the following code snippet:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在代码实现已经完成，我们准备开始DCGAN的训练。要训练模型，运行以下代码片段：
- en: '[PRE15]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This will run the training for 1,000 epochs and saves images every 50 epochs.
    When you run the `train()` function, the training progress prints as shown in
    figure 8.22.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行1,000个时代的训练，并且每50个时代保存一次图像。当您运行`train()`函数时，训练进度会像图8.22所示那样打印出来。
- en: '![](../Images/8-22.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-22.png)'
- en: Figure 8.22 Training progress for the first 16 epochs
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22前16个时代的训练进度
- en: I ran this training myself for 10,000 epochs. Figure 8.23 shows my results after
    0, 50, 1,000, and 10,000 epochs.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我自己运行了10,000个时代的训练。图8.23显示了0、50、1,000和10,000个时代后的结果。
- en: '![](../Images/8-23.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-23.png)'
- en: Figure 8.23 Output of the GAN generator after 0, 50, 1,000, and 10,000 epochs
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23 GAN生成器在0、50、1,000和10,000个时代后的输出
- en: 'As you can see in figure 8.23, at epoch 0, the images are just random noise--no
    patterns or meaningful data. At epoch 50, patterns have started to form. One very
    apparent pattern is the bright pixels beginning to form at the center of the image,
    and the surroundings’ darker pixels. This happens because in the training data,
    all of the shapes are located at the center of the image. Later in the training
    process, at epoch 1,000, you can see clear shapes and can probably guess the type
    of training data fed to the GAN model. Fast-forward to epoch 10,000, and you can
    see that the generator has become very good at re-creating new images not present
    in the training dataset. For example, pick any of the objects created at this
    epoch: let’s say the top-left image (dress). This is a totally new dress design
    that is not present in the training dataset. The GAN model created a completely
    new dress design after learning the dress patterns from the training set. You
    can run the training longer or make the generator network even deeper to get more
    refined results.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在图8.23中看到的，在0个时代，图像只是随机噪声——没有模式或有意义的数据。在50个时代，模式开始形成。一个非常明显的模式是图像中心开始形成的亮像素，以及周围的较暗像素。这是因为训练数据中，所有形状都位于图像的中心。在训练过程的后期，在1,000个时代，您可以清楚地看到形状，并且可以猜测出提供给GAN模型的训练数据类型。快进到10,000个时代，您可以看到生成器已经非常擅长重新创建训练数据集中不存在的新的图像。例如，选择在这个时代创建的任何对象：比如说左上角的图像（连衣裙）。这是一个完全新的连衣裙设计，在训练数据集中不存在。GAN模型在从训练集中学习连衣裙模式后，创建了一个全新的连衣裙设计。您可以运行更长时间的训练，或者使生成器网络更深，以获得更精细的结果。
- en: In closing
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在结束之前
- en: 'For this project, I used the Fashion-MNIST dataset because the images are very
    small and are in grayscale (one-channel), which makes it computationally inexpensive
    for you to train on your local computer with no GPU. Fashion-MNIST is also very
    clean data: all of the images are centered and have less noise so they don’t require
    much preprocessing before you kick off your GAN training. This makes it a good
    toy dataset to jumpstart your first GAN project.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我使用了Fashion-MNIST数据集，因为图像非常小，是灰度图（单通道），这使得您在本地计算机上没有GPU的情况下进行训练时计算成本很低。Fashion-MNIST数据集也非常干净：所有图像都居中，噪声较少，因此在启动GAN训练之前不需要太多预处理。这使得它成为一个很好的玩具数据集，可以用来启动您的第一个GAN项目。
- en: If you are excited to get your hands dirty with more advanced datasets, you
    can try CIFAR as your next step ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    or Google’s Quick, Draw! dataset ([https://quickdraw.withgoogle.com](https://quickdraw.withgoogle.com)),
    which is considered the world’s largest doodle dataset at the time of writing.
    Another, more serious, dataset is Stanford’s Cars Dataset ([https://ai.stanford.edu/~jkrause/cars/car_dataset.html](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)),
    which contains more than 16,000 images of 196 classes of cars. You can try to
    train your GAN model to design a completely new design for your dream car!
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想尝试更高级的数据集，您可以将CIFAR作为您的下一步（[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)）或Google的Quick,
    Draw!数据集（[https://quickdraw.withgoogle.com](https://quickdraw.withgoogle.com)），这是在撰写本文时被认为是世界上最大的涂鸦数据集。另一个更严肃的数据集是斯坦福的Cars数据集（[https://ai.stanford.edu/~jkrause/cars/car_dataset.html](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)），它包含超过16,000张196类汽车的图像。您可以尝试训练您的GAN模型为您的梦想汽车设计一个全新的设计！
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: GANs learn patterns from the training dataset and create new images that have
    a similar distribution of the training set.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN从训练数据集中学习模式，并创建具有与训练集相似分布的新图像。
- en: The GAN architecture consists of two deep neural networks that compete with
    each other.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN架构由两个相互竞争的深度神经网络组成。
- en: The generator tries to convert random noise into observations that look as if
    they have been sampled from the original dataset.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器试图将随机噪声转换为看起来像是从原始数据集中采样的观察值。
- en: The discriminator tries to predict whether an observation comes from the original
    dataset or is one of the generator’s forgeries.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器试图预测一个观察值是否来自原始数据集或生成器伪造的之一。
- en: The discriminator’s model is a typical classification neural network that aims
    to classify images generated by the generator as real or fake.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器的模型是一个典型的分类神经网络，旨在将生成器生成的图像分类为真实或伪造。
- en: The generator’s architecture looks like an inverted CNN that starts with a narrow
    input and is upsampled a few times until it reaches the desired size.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器的架构看起来像是一个倒置的CNN，它以狭窄的输入开始，经过几次上采样，直到达到所需的大小。
- en: The upsampling layer scales the image dimensions by repeating each row and column
    of its input pixels.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上采样层通过重复其输入像素的每一行和每一列来调整图像尺寸。
- en: 'To train the GAN, we train the network in batches through two parallel networks:
    the discriminator and a combined network where we freeze the weights of the discriminator
    and update only the generator’s weights.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了训练GAN，我们通过两个并行网络批量训练网络：判别器和冻结判别器权重并仅更新生成器权重的组合网络。
- en: To evaluate the GAN, we mostly rely on our observation of the quality of images
    created by the generator. Other evaluation metrics are the inception score and
    Fréchet inception distance (FID).
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了评估GAN，我们主要依赖于我们对生成器创建的图像质量的观察。其他评估指标包括Inception得分和Fréchet Inception距离（FID）。
- en: In addition to generating new images, GANs can be used in applications such
    as text-to-photo synthesis, image-to-image translation, image super-resolution,
    and many other applications.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了生成新图像外，GAN还可以用于文本到照片合成、图像到图像翻译、图像超分辨率和其他许多应用。
- en: '* * *'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '***'
- en: 1.Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio, “Generative Adversarial Networks,”
    2014, [http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 1.伊恩·J·古德费尔、让·普吉特-阿巴迪、梅赫迪·米尔扎、丁旭、大卫·沃德-法雷利、谢吉尔·奥齐尔、阿隆·库维尔和约书亚·本吉奥，“生成对抗网络，”2014年，[http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661).
- en: 2.Alec Radford, Luke Metz, and Soumith Chintala, “Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks,” 2016, [http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 2.亚历克·拉德福德、卢克·梅茨和索乌米特·钦塔拉，“使用深度卷积生成对抗网络的无监督表示学习，”2016年，[http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434).
- en: 3.Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    and *x[i]* Chen. “Improved Techniques for Training GANs,” 2016, [http://arxiv.org/abs/1606.03498](http://arxiv.org/abs/1606.03498).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 3.蒂姆·萨利曼斯、伊恩·古德费尔、沃伊切赫·扎伦巴、维基·张、亚历克·拉德福德和*x[i]*陈。“改进GAN的训练技术，”2016年，[http://arxiv.org/abs/1606.03498](http://arxiv.org/abs/1606.03498).
- en: 4.Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and
    Sepp Hochreiter, “GANs Trained by a Two Time-Scale Update Rule Converge to a Local
    Nash Equilibrium,” 2017, [http://arxiv.org/ abs/1706.08500](http://arxiv.org/abs/1706.08500).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 4.马丁·海塞尔、胡伯特·拉姆萨乌尔、托马斯·乌特纳、伯纳德·内斯勒和塞普·霍克赖特，“通过双时间尺度更新规则训练的GAN收敛到局部纳什均衡，”2017年，[http://arxiv.org/abs/1706.08500](http://arxiv.org/abs/1706.08500).
- en: 5.Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari, “How Good Is My
    GAN?” 2018, [http://arxiv .org/abs/1807.09499](http://arxiv.org/abs/1807.09499).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 5.康斯坦丁·施梅尔科夫、科德利亚·施密德和卡尔特克·阿拉哈里，“我的GAN有多好？”2018年，[http://arxiv.org/abs/1807.09499](http://arxiv.org/abs/1807.09499).
- en: '6.Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang,
    and Dimitris Metaxas, “StackGAN: Text to Photo-Realistic Image Synthesis with
    Stacked Generative Adversarial Networks,” 2016, [http://arxiv.org/abs/1612.03242](http://arxiv.org/abs/1612.03242).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 6.韩张、徐涛、李洪升、张少亭、王晓刚、黄晓雷和迪米特里斯·梅塔克萨斯，“StackGAN：使用堆叠生成对抗网络进行文本到逼真图像合成，”2016年，[http://arxiv.org/abs/1612.03242](http://arxiv.org/abs/1612.03242).
- en: 7.Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros, “Image-to-Image
    Translation with Conditional Adversarial Networks,” 2016, [http://arxiv.org/abs/1611.07004](http://arxiv.org/abs/1611.07004).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 7.菲利普·伊索拉、朱俊彦、周廷辉和亚历克谢·A·埃夫罗，“条件对抗网络进行图像到图像翻译，”2016年，[http://arxiv.org/abs/1611.07004](http://arxiv.org/abs/1611.07004).
- en: 8.Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham,
    Alejandro Acosta, Andrew Aitken, et al., “Photo-Realistic Single Image Super-Resolution
    Using a Generative Adversarial Network,” 2016, [http://arxiv.org/abs/1609.04802](http://arxiv.org/abs/1609.04802).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 克里斯蒂安·莱迪格，卢卡斯·泰斯，费伦茨·胡萨，何塞·卡巴列罗，安德鲁·坎宁安，亚历杭德罗·阿科斯塔，安德鲁·艾肯等，《使用生成对抗网络进行逼真单图像超分辨率》，2016年，[http://arxiv.org/abs/1609.04802](http://arxiv.org/abs/1609.04802).
