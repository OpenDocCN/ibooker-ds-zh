- en: 10 Clustering data into groups
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 将数据聚类成组
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Clustering data by centrality
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按中心性聚类数据
- en: Clustering data by density
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按密度聚类数据
- en: Trade-offs between clustering algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类算法之间的权衡
- en: Executing clustering using the scikit-learn library
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn库执行聚类
- en: Iterating over clusters using Pandas
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Pandas迭代簇
- en: '*Clustering* is the process of organizing data points into conceptually meaningful
    groups. What makes a given group “conceptually meaningful”? There is no easy answer
    to that question. The usefulness of any clustered output is dependent on the task
    we’ve been assigned.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚类*是将数据点组织成概念上有意义的组的过程。什么使得一个给定的组“概念上有意义”？对这个问题的答案并不简单。任何聚类输出的有用性取决于我们被分配的任务。'
- en: 'Imagine that we’re asked to cluster a collection of pet photos. Do we cluster
    fish and lizards in one group and fluffy pets (such as hamsters, cats, and dogs)
    in another? Or should hamsters, cats, and dogs be assigned three separate clusters
    of their own? If so, perhaps we should consider clustering pets by breed. Thus,
    Chihuahuas and Great Danes fall into diverging clusters. Differentiating between
    dog breeds will not be easy. However, we can easily distinguish between Chihuahuas
    and Great Danes based on breed size. Maybe we should compromise: we’ll cluster
    on both fluffiness and size, thus bypassing the distinction between the Cairn
    Terrier and the similar-looking Norwich Terrier.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们被要求聚类一组宠物照片。我们是将鱼和蜥蜴聚类到一个组中，将蓬松的宠物（如仓鼠、猫和狗）聚类到另一个组中？还是仓鼠、猫和狗应该被分配到它们自己的三个单独的簇中？如果是这样，也许我们应该考虑按品种聚类宠物。这样，吉娃娃和伟大的丹斯就会落入不同的簇中。区分狗品种将不会容易。然而，我们可以很容易地根据品种大小区分吉娃娃和伟大的丹斯。也许我们应该妥协：我们将同时根据蓬松度和大小进行聚类，从而绕过凯恩犬和外观相似的诺里奇犬之间的区别。
- en: Is the compromise worth it? It depends on our data science task. Suppose we
    work for a pet food company, and our aim is to estimate demand for dog food, cat
    food, and lizard food. Under these conditions, we must distinguish between fluffy
    dogs, fluffy cats, and scaly lizards. However, we won’t need to resolve differences
    between separate dog breeds. Alternatively, imagine an analyst at a vet’s office
    who’s trying to group pet patients by their breed. This second task requires a
    much more granular level of group resolution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个妥协是否值得？这取决于我们的数据科学任务。假设我们为一家宠物食品公司工作，我们的目标是估计狗粮、猫粮和蜥蜴粮的需求。在这种情况下，我们必须区分蓬松的狗、蓬松的猫和鳞片的蜥蜴。然而，我们不需要解决不同狗品种之间的差异。或者，想象一下兽医办公室的一名分析师，他试图根据品种对宠物病人进行分组。这个第二个任务需要更细粒度的组分辨率。
- en: Different situations require different clustering techniques. As data scientists,
    we must choose the correct clustering solution. Over the course of our careers,
    we will cluster thousands (if not tens of thousands) of datasets using a variety
    of clustering techniques. The most commonly used algorithms rely on some notion
    of centrality to distinguish between clusters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的情境需要不同的聚类技术。作为数据科学家，我们必须选择正确的聚类解决方案。在我们的职业生涯中，我们将使用各种聚类技术聚类数千（如果不是数万）个数据集。最常用的算法依赖于某种中心性的概念来区分簇。
- en: 10.1 Using centrality to discover clusters
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 使用中心性发现簇
- en: In section 5, we learned how the centrality of data can be represented using
    the mean. Later, in section 7, we computed the mean length of a single group of
    fish. Eventually, we compared two separate sets of fish by analyzing the difference
    between their means. We utilized that difference to determine whether all the
    fish belonged to the same group. Intuitively, all data points in a single group
    should cluster around one central value. Meanwhile, the measurements in two divergent
    groups should cluster around two different means. Thus, we can utilize centrality
    to distinguish between two divergent groups. Let’s explore this notion in concrete
    detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5节中，我们学习了如何使用平均值表示数据的中心性。后来，在第7节中，我们计算了一个鱼群的平均长度。最终，我们通过分析两组鱼的平均值差异来比较两组鱼。我们利用这个差异来确定所有鱼是否属于同一组。直观上，一个组中的所有数据点都应该围绕一个中心值聚类。同时，两个不同组中的测量值应该围绕两个不同的平均值聚类。因此，我们可以利用中心性来区分两个不同的组。让我们具体探讨这个概念。
- en: Suppose we take a field trip to a lively local pub and see two dartboards hanging
    side by side. Each of the dartboards is covered in darts, and darts also protrude
    from the walls. The tipsy players in the pub aim for the bull’s-eye of one board
    or the other. Frequently, they miss, which leads to the observed scattering of
    darts centered around the two bull’s-eyes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们参观了一个热闹的当地酒吧，并看到两个飞镖盘并排挂着。每个飞镖盘上都插满了飞镖，墙壁上也凸出了飞镖。酒吧里醉醺醺的玩家瞄准一个或另一个飞镖盘的靶心。他们经常失手，这导致了围绕两个靶心的飞镖散布的观察结果。
- en: 'Let’s simulate the scattering numerically. We’ll treat each bull’s-eye location
    as a 2D coordinate. Darts are randomly flung at that coordinate. Consequently,
    the 2D positions of the darts are randomly distributed. The most appropriate distribution
    for modeling dart positions is the normal distribution, for the following reasons:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过数值模拟散布。我们将每个靶心位置视为一个二维坐标。飞镖随机投向该坐标。因此，飞镖的二维位置是随机分布的。建模飞镖位置的最合适的分布是正态分布，原因如下：
- en: A typical dart thrower aims at the bull’s-eye, not at the edge of the dartboard.
    Thus, each dart is more likely to strike close to the center of the board. This
    behavior is consistent with random normal samples, in which values closer to the
    mean occur more frequently than other, more distant values.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个典型的飞镖投掷者瞄准的是靶心，而不是靶边缘。因此，每个飞镖更有可能击中靶板的中心附近。这种行为与随机正态样本一致，其中接近均值的值比其他更远的值出现得更频繁。
- en: We expect the darts to strike the board symmetrically relative to the center.
    Darts will strike 3 inches left of center and 3 inches right of center with equal
    frequency. This symmetry is captured by the bell-shaped normal curve.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们期望飞镖相对于中心对称地击中靶板。飞镖将以相等的频率击中中心左侧3英寸和中心右侧3英寸。这种对称性由钟形正态曲线捕捉。
- en: Suppose the first bull’s-eye is located at coordinate `[0, 0]`. A dart is thrown
    at that coordinate. We’ll model the x and y positions of the dart using two normal
    distributions. These distributions share a mean of 0, and we also assume that
    they share a variance of 2\. The following code generates the random coordinates
    of the dart.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设第一个靶心位于坐标 `[0, 0]`。在这个坐标上投掷飞镖。我们将使用两个正态分布来模拟飞镖的x和y位置。这些分布共享均值为0，我们还假设它们共享方差为2。以下代码生成了飞镖的随机坐标。
- en: Listing 10.1 Modeling dart coordinates using two normal distributions
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1 使用两个正态分布建模飞镖坐标
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note We can more efficiently model dart positions using the `np.random .multivariate_normal`
    method. This method selects a single random point from a *multivariate normal
    distribution.* The multivariate normal curve is simply a normal curve that is
    extended to more than one dimension. Our 2D multivariate normal distribution will
    resemble a round hill whose summit is positioned at `[0, 0]`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们可以使用 `np.random.multivariate_normal` 方法更有效地模拟飞镖的位置。此方法从 *多元正态分布* 中选择一个随机点。多元正态曲线只是扩展到多个维度的正态曲线。我们的二维多元正态分布将类似于一个山顶位于
    `[0, 0]` 的圆形山丘。
- en: Let’s simulate 5,000 random darts tossed at the bull’s-eye positioned at `[0,
    0]`. We also simulate 5,000 random darts tossed at a second bull’s-eye, positioned
    at `[0, 6]`. Then we generate a scatter plot of all the random dart coordinates
    (figure 10.1).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们模拟在 `[0, 0]` 位置放置的靶心投掷的5,000个随机飞镖。我们还模拟在 `[0, 6]` 位置放置的第二个靶心投掷的5,000个随机飞镖。然后我们生成所有随机飞镖坐标的散点图（图10.1）。
- en: Listing 10.2 Simulating randomly thrown darts
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.2 模拟随机投掷的飞镖
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/10-01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-01.png)'
- en: Figure 10.1 A simulation of darts randomly scattered around two bull’s-eye targets
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 随机散布在两个靶心目标周围的飞镖模拟
- en: 'Note Listing 10.2 includes a nested five-line `for` loop beginning with `for
    _ in range(5000)`. It’s possible to use NumPy to execute this loop in just one
    line of code: running `x_coordinates, y_coordinates = np.random.multivariate_normal(bulls_eye,
    np.diag(2 * [variance]), 5000).T` returns 5,000 x and y coordinates sampled from
    the multivariate normal distribution.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表10.2包含一个以 `for _ in range(5000)` 开头的嵌套五行 `for` 循环。可以使用NumPy在仅一行代码中执行此循环：运行
    `x_coordinates, y_coordinates = np.random.multivariate_normal(bulls_eye, np.diag(2
    * [variance]), 5000).T` 返回从多元正态分布中采样的5,000个x和y坐标。
- en: Two overlapping dart groups appear in the plot. The two groups represent 10,000
    darts. Half the darts were aimed at the bull’s-eye on the left, and the rest were
    aimed toward the right. Each dart has an intended target, which we can estimate
    by looking at the plot. Darts closer to `[0, 0]` were probably aimed at the bull’s-eye
    on the left. We’ll incorporate this assumption into our dart plot.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图中出现了两个重叠的飞镖组。这两个组代表10,000个飞镖。其中一半的飞镖瞄准了左侧的靶心，其余的则瞄准了右侧。每个飞镖都有一个目标，我们可以通过观察图表来估计它。靠近
    `[0, 0]` 的飞镖可能瞄准了左侧的靶心。我们将把这个假设纳入我们的飞镖图表中。
- en: Let’s assign each dart to its nearest bull’s-eye. We start by defining a `nearest_
    bulls_eye` function that takes as input a `dart` list holding a dart’s x and y
    positions. The function returns the index of the bull’s-eye that is most proximate
    to `dart`. We measure dart proximity using *Euclidean distance*, which is the
    standard straight-line distance between two points.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将每个飞镖分配给其最近的靶心。我们首先定义一个 `nearest_bulls_eye` 函数，该函数接受一个包含飞镖x和y位置的 `dart` 列表作为输入。该函数返回最接近
    `dart` 的靶心的索引。我们使用 *欧几里得距离* 来衡量飞镖的接近程度，这是两点之间的标准直线距离。
- en: Note Euclidean distance arises from the Pythagorean theorem. Suppose we examine
    a dart at position `[x_dart, y_dart]` relative to a bull’s-eye at position `[x_bull,
    y_bull]`. According to the Pythagorean theorem, `distance² = (x_dart - x_bull)²
    + (y_dart - y_bull)²`. We can solve for distance using a custom Euclidean function.
    Alternatively, we can use the `scipy.spatial.distance.euclidean` function provided
    by SciPy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，欧几里得距离源于勾股定理。假设我们检查一个位于 `[x_dart, y_dart]` 位置的飞镖相对于位于 `[x_bull, y_bull]`
    位置的靶心。根据勾股定理，`distance² = (x_dart - x_bull)² + (y_dart - y_bull)²`。我们可以使用自定义的欧几里得函数来求解距离。或者，我们可以使用SciPy提供的
    `scipy.spatial.distance.euclidean` 函数。
- en: The following code defines `nearest_bulls_eye` and applies it to darts `[0,
    1]` and `[6, 1]`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了 `nearest_bulls_eye` 并将其应用于 `[0, 1]` 和 `[6, 1]` 的飞镖。
- en: Listing 10.3 Assigning darts to the nearest bull’s-eye
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.3 将飞镖分配给最近的靶心
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Obtains the Euclidean distance between the dart and each bull’s-eye using
    the euclidean function imported from SciPy
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用从SciPy导入的euclidean函数获取飞镖与每个靶心之间的欧几里得距离
- en: ❷ Returns the index matching the shortest bull’s-eye distance in the array
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回与数组中最短靶心距离匹配的索引
- en: Now we apply `nearest_bulls_eye` to all our computed dart coordinates. Each
    dart point is plotted using one of two colors to distinguish between the two bull’s-eye
    assignments (figure 10.2).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将 `nearest_bulls_eye` 应用于所有计算出的飞镖坐标。每个飞镖点使用两种颜色之一来区分两个靶心分配（图10.2）。
- en: Listing 10.4 Coloring darts based on the nearest bull’s-eye
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4 根据最近的靶心着色飞镖
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Helper function that plots the colored elements of an inputted darts list.
    Each dart in darts serves as input for nearest_bulls_eye.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 辅助函数，用于绘制输入飞镖列表的彩色元素。列表中的每个飞镖都是nearest_bulls_eye的输入。
- en: ❷ Selects the darts most proximate to bulls_eyes[bs_index]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择最接近bulls_eyes[bs_index]的飞镖
- en: ❸ Separates the x and y coordinates of each dart by transposing an array of
    selected darts. As discussed in section 8, the transpose swaps the row and column
    positions within a 2D data structure.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过转置所选飞镖的数组来分离每个飞镖的x和y坐标。如第8节所述，转置交换了2D数据结构中的行和列位置。
- en: ❹ Combines the separate coordinates of each dart into a single list of x and
    y coordinates.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将每个飞镖的单独坐标合并成一个包含x和y坐标的单个列表。
- en: '![](../Images/10-02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-02.png)'
- en: Figure 10.2 Darts colored based on proximity to the nearest bull’s-eye. Cluster
    A represents all points closest to the left bull’s-eye, and cluster B represents
    all points closest to the right bull’s-eye.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 根据飞镖与最近靶心的距离着色。簇A代表所有最接近左侧靶心的点，簇B代表所有最接近右侧靶心的点。
- en: The colored darts split sensibly into two even clusters. How would we identify
    such clusters if no central coordinates were provided? Well, one primitive strategy
    is to simply guess the location of the bull’s-eyes. We can pick two random darts
    and hope these darts are somehow relatively close to each of the bull’s-eyes,
    although the likelihood of that happening is incredibly low. In most cases, coloring
    darts based on two randomly chosen centers will not yield good results (figure
    10.3).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色飞镖合理地分为两个均匀的簇。如果没有提供中心坐标，我们如何识别这样的簇呢？好吧，一种原始的策略是简单地猜测靶心的位置。我们可以选择两个随机的飞镖，并希望这些飞镖以某种方式相对接近每个靶心，尽管这种情况发生的可能性极低。在大多数情况下，基于两个随机选择的中心着色飞镖不会产生好的结果（图10.3）。
- en: Listing 10.5 Assigning darts to randomly chosen centers
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.5 基于随机选择的中心分配飞镖
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Randomly selects the first two darts to be our representative bull’s-eyes
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 随机选择前两个飞镖作为我们的代表靶心
- en: '![](../Images/10-03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-03.png)'
- en: Figure 10.3 Darts colored based on proximity to randomly selected centers. Cluster
    B is stretched too far to the left.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 根据随机选择的中心接近度着色的飞镖。聚类 B 向左延伸得太远。
- en: 'Our indiscriminately chosen centers feel wrong qualitatively. For instance,
    cluster B on the right seems to be stretching way too far to the left. The arbitrary
    center we’ve assigned doesn’t appear to match its actual bull’s-eye point. But
    there’s a way to remedy our error: we can compute the mean coordinates of all
    the points in the stretched right clustered group and then utilize these coordinates
    to adjust our estimation of the group’s center. After assigning the cluster’s
    mean coordinates to the bull’s-eye, we can reapply our distance-based grouping
    technique to adjust the rightmost cluster’s boundaries. In fact, for maximum effectiveness,
    we will also reset the leftmost cluster’s center to its mean prior to rerunning
    our centrality-based clustering (figure 10.4).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随意选择的中心在定性上感觉不正确。例如，右边的聚类 B 似乎向左延伸得太远。我们分配的任意中心似乎与其实际的靶心点不匹配。但有一种方法可以纠正我们的错误：我们可以计算拉伸的右聚类组中所有点的平均坐标，然后利用这些坐标来调整我们对组中心的估计。在将聚类组的平均坐标分配给靶心后，我们可以重新应用基于距离的分组技术来调整最右侧聚类的边界。实际上，为了达到最大效果，我们还将最左侧聚类的中心重置为其平均值，然后再重新运行基于中心性的聚类（图
    10.4）。
- en: Note When we compute the mean of a 1D array, we return a single value. We are
    now extending that definition to encompass multiple dimensions. When we compute
    the mean of a 2D array, we return the mean of all x coordinates and also the mean
    of all y coordinates. The final output is a 2D array containing means across the
    x-axis and y-axis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当我们计算一维数组的平均值时，我们返回一个单一值。我们现在扩展这个定义以涵盖多个维度。当我们计算二维数组的平均值时，我们返回所有 x 坐标的平均值以及所有
    y 坐标的平均值。最终输出是一个包含沿 x 轴和 y 轴平均值的二维数组。
- en: Listing 10.6 Assigning darts to centers based on means
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.6 基于平均值分配飞镖到中心
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Takes the mean of the x and y coordinates for all the darts assigned to a
    given bull’s-eye. These average coordinates are then used to update our estimated
    bull’s-eye position. We can more efficiently run this calculation by executing
    np.mean(selected_darts, axis=0).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对分配给特定靶心的所有飞镖的 x 和 y 坐标取平均值。然后使用这些平均坐标来更新我们估计的靶心位置。我们可以通过执行 np.mean(selected_darts,
    axis=0) 来更高效地运行这个计算。
- en: '![](../Images/10-04.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-04.png)'
- en: Figure 10.4 Darts colored based on proximity to recomputed centers. The two
    clusters now appear to be more even.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 根据重新计算的中心的接近度着色的飞镖。现在两个聚类看起来更均匀。
- en: The results are already looking better, although they’re not quite as effective
    as they could be. The cluster’s centers still appear a little off. Let’s remedy
    the results by repeating the mean-based centrality adjustment over 10 additional
    iterations (figure 10.5).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来已经更好了，尽管它们还没有达到可能的效果。聚类的中心仍然有点偏。让我们通过重复基于平均值的中心性调整 10 次额外的迭代来纠正结果（图 10.5）。
- en: Listing 10.7 Adjusting bull’s-eye positions over 10 iterations
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.7 在 10 次迭代中调整靶心位置
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/10-05.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-05.png)'
- en: Figure 10.5 Darts colored based on proximity to iteratively recomputed centers
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 根据迭代重新计算的中心的接近度着色的飞镖
- en: The two sets of darts are now perfectly clustered! We have essentially replicated
    the *K-means* clustering algorithm, which organizes data using centrality.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这两组飞镖现在完美地聚簇在一起了！我们本质上复制了 *K-均值* 聚类算法，该算法使用中心性来组织数据。
- en: '10.2 K-means: A clustering algorithm for grouping data into K central groups'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 K-均值：一种将数据分组到 K 个中心组的聚类算法
- en: The K-means algorithm assumes that inputted data points swirl around *K* different
    centers. Each central coordinate is like a hidden bull’s-eye surrounded by scattered
    data points. The purpose of the algorithm is to uncover these hidden central coordinates.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: K-均值算法假设输入的数据点围绕 *K* 个不同的中心旋转。每个中心坐标就像一个被散点数据包围的隐藏靶心。算法的目的是揭示这些隐藏的中心坐标。
- en: We initialize K-means by first selecting *K*, which is the number of central
    coordinates we will search for. In our dartboard analysis, *K* was set to 2, although
    generally *K* can equal any whole number. The algorithm chooses *K* data points
    at random. These data points are treated as though they are true centers. Then
    the algorithm iterates by updating the chosen central locations, which data scientists
    call *centroids*. During a single iteration, every data point is assigned to its
    closest center, leading to the formation of *K* groups. Next, the center of each
    group is updated. The new center equals the mean of the group’s coordinates. If
    we repeat the process long enough, the group means will converge to *K* representative
    centers (figure 10.6). The convergence is mathematically guaranteed. However,
    we cannot know in advance the number of iterations required for the convergence
    to take place. A common trick is to halt the iterations when none of the newly
    computed centers deviate significantly from their predecessors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先选择 *K* 来初始化 K-means，*K* 是我们将要寻找的中心坐标的数量。在我们的飞镖盘分析中，*K* 被设置为 2，尽管通常 *K* 可以等于任何整数。算法随机选择
    *K* 个数据点。这些数据点被视为真正的中心。然后，算法通过更新所选的中心位置（数据科学家称之为 *centroids*）进行迭代。在单次迭代中，每个数据点都被分配到其最近中心，从而形成
    *K* 个组。接下来，每个组的中心被更新。新的中心等于该组坐标的平均值。如果我们重复这个过程足够长的时间，组平均值将收敛到 *K* 个代表性中心（图 10.6）。收敛是数学上保证的。然而，我们无法预先知道收敛所需的迭代次数。一个常见的技巧是在新计算的中心与先前的中心没有显著差异时停止迭代。
- en: '![](../Images/10-06.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-06.png)'
- en: Figure 10.6 The K-means algorithm iteratively converging from two randomly selected
    centroids to the actual bull’s-eye centroids
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 K-means 算法从两个随机选择的中心迭代收敛到实际的靶心中心
- en: 'K-means is not without its limitations. The algorithm is predicated on our
    knowledge of *K*: the number of clusters to look for. Frequently, such knowledge
    is not available. Also, while K-means commonly finds reasonable centers, it’s
    not mathematically guaranteed to find the best possible centers in the data. Occasionally,
    K-means returns unintuitive or suboptimal groups due to poor selection of random
    centroids at the initialization step of the algorithm. Finally, K-means presupposes
    that the clusters in the data actually swirl around *K* central locations. But
    as we learn later in the section, this supposition does not always hold.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 并非没有局限性。该算法基于我们对 *K* 的了解：要寻找的聚类数量。通常，这种知识是不可得的。此外，尽管 K-means 通常可以找到合理的中心，但它并不保证在数据中找到最佳可能的中心。有时，由于算法初始化步骤中随机中心的选择不佳，K-means
    会返回不直观或次优的组。最后，K-means 假设数据中的聚类实际上围绕 *K* 个中心位置旋转。但正如我们在本节后面所学到的，这个假设并不总是成立。
- en: 10.2.1 K-means clustering using scikit-learn
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 使用 scikit-learn 进行 K-means 聚类
- en: The K-means algorithm runs in a reasonable time if it is implemented efficiently.
    A speedy implementation of the algorithm is available through the external scikit-learn
    library. Scikit-learn is an extremely popular machine learning toolkit built on
    top of NumPy and SciPy. It features a variety of core classification, regression,
    and clustering algorithms—including, of course, K-means. Let’s install the library.
    Then we import scikit-learn’s `KMeans` clustering class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 K-means 算法被高效实现，它将在合理的时间内运行。通过外部 scikit-learn 库可以获得算法的快速实现。Scikit-learn 是一个基于
    NumPy 和 SciPy 的非常流行的机器学习工具包，它包含各种核心分类、回归和聚类算法——当然包括 K-means。让我们安装这个库。然后我们导入 scikit-learn
    的 `KMeans` 聚类类。
- en: Note Call `pip install scikit-learn` from the command line terminal to install
    the scikit-learn library.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：从命令行终端调用 `pip install scikit-learn` 以安装 scikit-learn 库。
- en: Listing 10.8 Importing `KMeans` from scikit-learn
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.8 从 scikit-learn 导入 `KMeans`
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Applying `KMeans` to our `darts` data is easy. First, we need to run `KMeans(n_clusters=2)`,
    which will create a `cluster_model` object capable of finding two bull’s-eye centers.
    Then, we can execute K-means by running `cluster_model.fit_predict (darts)`. That
    method call will return an `assigned_bulls_eyes` array that stores the bull’s-eye
    index of each dart.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `KMeans` 应用于我们的 `darts` 数据非常简单。首先，我们需要运行 `KMeans(n_clusters=2)`，这将创建一个 `cluster_model`
    对象，该对象能够找到两个靶心中心。然后，我们可以通过运行 `cluster_model.fit_predict (darts)` 来执行 K-means。这个方法调用将返回一个
    `assigned_bulls_eyes` 数组，该数组存储每个飞镖的靶心索引。
- en: Listing 10.9 K-means clustering using scikit-learn
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.9 使用 scikit-learn 进行 K-means 聚类
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Creates a cluster_model object in which the number of centers is set to 2
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个中心数为 2 的 cluster_model 对象
- en: ❷ Optimizes two centers using the K-means algorithm and returns the assigned
    cluster for each dart
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 K-means 算法优化两个中心并返回每个 dart 分配的簇
- en: Let’s color our darts based on their clustering assignments to verify the results
    (figure 10.7).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据它们的聚类分配给 dart 上色，以验证结果（图 10.7）。
- en: Listing 10.10 Plotting K-means cluster assignments
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.10 绘制 K-means 簇分配图
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/10-07.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片 10-07](../Images/10-07.png)'
- en: Figure 10.7 The K-means clustering results returned by scikit-learn are consistent
    with our expectations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 scikit-learn 返回的 K-means 聚类结果与我们的预期一致。
- en: Our clustering model has located the centroids in the data. Now we can reuse
    these centroids to analyze new data points that the model has not seen before.
    Executing `cluster_model.predict([x, y])` assigns a centroid to a data point defined
    by `x` and `y`. We use the `predict` method to cluster two new data points.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们聚类模型已经定位了数据中的质心。现在我们可以重用这些质心来分析模型之前未见过的新的数据点。执行 `cluster_model.predict([x,
    y])` 将一个质心分配给由 `x` 和 `y` 定义的点。我们使用 `predict` 方法来聚类两个新的数据点。
- en: Listing 10.11 Using `cluster_model` to cluster new data
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.11 使用 `cluster_model` 聚类新数据
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 10.2.2 Selecting the optimal K using the elbow method
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 使用肘部方法选择最优的 K 值
- en: K-means relies on an inputted *K*. This can be a serious hindrance when the
    number of authentic clusters in the data isn’t known in advance. We can, however,
    estimate an appropriate value for *K* using a technique known as the *elbow method*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 依赖于输入的 *K*。当数据中真实的簇的数量事先未知时，这可能是一个严重的阻碍。然而，我们可以使用称为 *肘部方法* 的技术来估计 *K*
    的适当值。
- en: The elbow method depends on a calculated value called *inertia*, which is the
    sum of the squared distances between each point and its closest K-means center.
    If *K* is 1, then the inertia equals the sum of all squared distances to the dataset’s
    mean. This value, as discussed in section 5, is directly proportional to the variance.
    Variance, in turn, is a measure of dispersion. Thus, if *K* is 1, the inertia
    is an estimate of dispersion. This property holds true even if *K* is greater
    than 1\. Basically, inertia estimates total dispersion around our *K* computed
    means.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部方法依赖于一个称为 *惯性* 的计算值，它是每个点到其最近的 K-means 中心的平方距离之和。如果 *K* 为 1，则惯性等于所有点到数据集平均值的平方距离之和。正如第
    5 节所讨论的，这个值与方差成正比。方差反过来是分散度的度量。因此，如果 *K* 为 1，惯性是分散度的估计。即使 *K* 大于 1，这个属性也是正确的。基本上，惯性估计了围绕我们计算的
    *K* 个均值周围的总体分散度。
- en: By estimating dispersion, we can determine whether our *K* value is too high
    or too low. For example, imagine that we set *K* to 1\. Potentially, many of our
    data points will be positioned too far from one center. Our dispersion will be
    large, and our inertia will be large. As we increase *K* toward a more sensible
    number, the additional centers will cause the inertia to decrease. Eventually,
    if we go overboard and set *K* equal to the total number of points, each data
    point will fall into its own private cluster. Dispersion will be eliminated, and
    inertia will drop to zero (figure 10.8).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过估计分散度，我们可以确定我们的 *K* 值是否过高或过低。例如，假设我们将 *K* 设置为 1。可能，我们的大量数据点将位于一个中心太远的位置。我们的分散度将会很大，惯性也会很大。随着我们将
    *K* 增加到一个更合理的数字，额外的中心将导致惯性减少。最终，如果我们过分追求，将 *K* 设置为点的总数，每个数据点将落入它自己的私有簇中。分散度将消除，惯性将降至零（图
    10.8）。
- en: '![](../Images/10-08.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片 10-08](../Images/10-08.png)'
- en: Figure 10.8 Six points, numbered 1 through 6, are plotted in 2D space. The centers,
    marked by stars, are computed across various values of *K*. A line is drawn from
    every point to its nearest center. Inertia is computed by summing the squared
    lengths of the six lines. (A) *K* = 1\. All six lines stretch out from a single
    center. The inertia is quite large. (B) *K* = 2\. Points 5 and 6 are very close
    to a second center. The inertia is reduced. (C) *K* = 3\. Points 1 and 3 are substantially
    closer to a newly formed center. Points 2 and 4 are also substantially closer
    to a newly formed center. The inertia has radically decreased. (D) *K* = 4\. Points
    1 and 3 now overlap with their centers. Their contribution to the inertia has
    shifted from a very low value to zero. The distances between the remaining four
    points and their associated centers remain unchanged. Thus, increasing *K* from
    3 to 4 caused a very small decrease in inertia.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 在 2D 空间中绘制了编号为 1 到 6 的六个点。中心，用星号标记，是在各种 *K* 值下计算的。从每个点到其最近中心画一条线。惯性是通过求六条线的长度的平方和来计算的。（A）*K*
    = 1。所有六条线都从单个中心延伸出来。惯性相当大。（B）*K* = 2。点 5 和 6 非常接近第二个中心。惯性减少。（C）*K* = 3。点 1 和 3
    与新形成的中心显著更近。点 2 和 4 也与新形成的中心显著更近。惯性大幅下降。（D）*K* = 4。点 1 和 3 现在与其中心重叠。它们对惯性的贡献已从非常低的值变为零。剩余四个点及其相关中心之间的距离保持不变。因此，将
    *K* 从 3 增加到 4 导致惯性非常小的下降。
- en: Some inertia values are too large. Others are too low. Somewhere in between
    might lie a value that’s just right. How do we find it?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一些惯性值太大。其他值太低。在两者之间可能存在一个恰到好处的值。我们如何找到它？
- en: Let’s work out a solution. We begin by plotting the inertia of our dartboard
    dataset over a large range of *K* values (figure 10.9). Inertia is automatically
    computed for each scikit-learn `KMeans` object. We can access this stored value
    through the model’s `inertia_` attribute.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出一个解决方案。我们首先在广泛的 *K* 值范围内绘制我们的飞镖靶数据集的惯性（图 10.9）。惯性会自动为每个 scikit-learn `KMeans`
    对象计算。我们可以通过模型的 `inertia_` 属性访问这个存储的值。
- en: '![](../Images/10-09.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-09.png)'
- en: Figure 10.9 An inertia plot for a dartboard simulation containing two bull’s-eye
    targets. The plot resembles an arm bent at the elbow. The elbow points directly
    to a *K* of 2.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 包含两个靶心的飞镖靶模拟的惯性图。该图类似于肘部弯曲的手臂。肘部直接指向 *K* 值为 2。
- en: Listing 10.12 Plotting the K-means inertia
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.12 绘制 K-means 惯性
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The generated plot resembles an arm bent at the elbow, and the elbow points
    at a *K* value of 2\. As we already know, this *K* accurately captures the two
    centers we have preprogrammed into the dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图类似于肘部弯曲的手臂，肘部指向 *K* 值为 2。正如我们已经知道的，这个 *K* 准确地捕捉了我们预先编程到数据集中的两个中心。
- en: Will the approach still hold if the number of present centers is increased?
    We can find out by adding an additional bull’s-eye to our dart-throwing simulation.
    After we increase the cluster count to 3, we regenerate our inertia plot (figure
    10.10).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现有中心的数量增加，这种方法是否仍然适用？我们可以通过向我们的飞镖投掷模拟中添加一个额外的靶心来找出答案。在我们将簇计数增加到 3 后，我们重新生成惯性图（图
    10.10）。
- en: Listing 10.13 Plotting inertia for a 3-dartboard simulation
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.13 3 个飞镖靶模拟的惯性绘图
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/10-10.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-10.png)'
- en: Figure 10.10 An inertia plot for a dartboard simulation containing three bull’s-eye
    targets. The plot resembles an arm bent at the elbow. The lowermost portion of
    the elbow points to a *K* of 3.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 包含三个靶心的飞镖靶模拟的惯性图。该图类似于肘部弯曲的手臂。肘部的最低部分指向 *K* 值为 3。
- en: Adding a third center leads to a new elbow whose lowermost inclination points
    to a *K* value of 3\. Essentially, our elbow plot traces the dispersion captured
    by each incremental *K*. A rapid decrease in inertia between consecutive *K* values
    implies that scattered data points have been assigned to a tighter cluster. The
    reduction in inertia incrementally loses its impact as the inertia curve flattens
    out. This transition from a vertical drop to a gentler angle leads to the presence
    of an elbow shape in our plot. We can use the position of the elbow to select
    a proper *K* in the K-means algorithm.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 添加第三个中心导致出现一个新的肘部，其最低的倾斜度指向 *K* 值为 3。本质上，我们的肘部图追踪了每个增量 *K* 捕获的分散度。连续 *K* 值之间惯性的快速下降意味着散点数据点已被分配到一个更紧密的簇中。随着惯性曲线变平，惯性减少的影响逐渐减弱。这种从垂直下降到更平缓角度的转变导致我们的图中出现肘部形状。我们可以使用肘部的位置来在
    K-means 算法中选择合适的 *K* 值。
- en: The elbow method selection criterion is a useful heuristic, but it is not guaranteed
    to work in every case. Under certain conditions, the elbow levels off slowly over
    multiple *K* values, making it difficult to choose a single valid cluster count.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部方法选择标准是一个有用的启发式方法，但它并不能保证在所有情况下都有效。在特定条件下，肘部水平在多个 *K* 值上缓慢平缓，这使得选择一个有效的单一聚类数量变得困难。
- en: Note There exist more powerful *K*-selection methodologies, such as the *silhouette
    score*, which captures the distance of each point to neighboring clusters. A thorough
    discussion of the silhouette score is beyond the scope of this book. However,
    you’re encouraged to explore the score on your own, using the `sklearn.metrics.silhouette_score`
    method.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：存在更强大的 *K*-选择方法，例如 *轮廓分数*，它捕捉每个点到邻近聚类的距离。对轮廓分数的详细讨论超出了本书的范围。然而，鼓励你自己探索这个分数，使用
    `sklearn.metrics.silhouette_score` 方法。
- en: K-means clustering methods
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类方法
- en: '`k_means_model = KMeans(n_clusters=K)`—Creates a K-means model to search for
    *K* different centroids. We need to fit these centroids to inputted data.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k_means_model = KMeans(n_clusters=K)`—创建一个 K-means 模型来寻找 *K* 个不同的质心。我们需要将这些质心拟合到输入数据中。'
- en: '`clusters = k_means_model.fit_predict(data)`—Executes K-means on inputted data
    using an initialized `KMeans` object. The returned `clusters` array contains cluster
    IDs ranging from 0 to *K*. The cluster ID of `data[i]` is equal to `clusters[i]`.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters = k_means_model.fit_predict(data)`—使用初始化的 `KMeans` 对象在输入数据上执行 K-means。返回的
    `clusters` 数组包含从 0 到 *K* 的聚类 ID。`data[i]` 的聚类 ID 等于 `clusters[i]`。'
- en: '`clusters = KMeans(n_clusters=K).fit_predict(data)`—Executes K-means in a single
    line of code, and returns the resulting clusters.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters = KMeans(n_clusters=K).fit_predict(data)`—通过一行代码执行 K-means，并返回结果聚类。'
- en: '`new_clusters = k_means_model.predict(new_data)`—Finds the nearest centroids
    to previously unseen data using the existing centroids in a data-optimized `KMeans`
    object.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`new_clusters = k_means_model.predict(new_data)`—使用数据优化的 `KMeans` 对象中的现有质心找到先前未见数据最近的质心。'
- en: '`inertia = k_means_model.inertia_`—Returns the inertia associated with a data-optimized
    `KMeans` object.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inertia = k_means_model.inertia_`—返回与数据优化的 `KMeans` 对象相关的惯性。'
- en: '`inertia = KMeans(n_clusters=K).fit(data).inertia_`—Executes K-means in a single
    line of code, and returns the resulting inertia.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inertia = KMeans(n_clusters=K).fit(data).inertia_`—通过一行代码执行 K-means，并返回结果惯性。'
- en: The elbow method isn’t perfect, but it performs reasonably well if the data
    is centered on *K* distinct means. Of course, this assumes that our data clusters
    differ due to centrality. However, in many instances, data clusters differ due
    to the density of the data points in space. Let’s explore the concept of density-driven
    clusters, which are not dependent on centrality.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部方法并不完美，但如果数据集中在 *K* 个不同的均值上，它表现相当好。当然，这假设我们的数据聚类由于中心性而不同。然而，在许多情况下，数据聚类由于空间中数据点的密度而不同。让我们探讨密度驱动聚类的概念，这些聚类不依赖于中心性。
- en: 10.3 Using density to discover clusters
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 使用密度发现聚类
- en: Suppose an astronomer discovers a new planet at the far-flung edge of the solar
    system. The planet, much like Saturn, has multiple rings spinning in constant
    orbits around its center. Each ring is formed from thousands of rocks. We’ll model
    these rocks as individual points defined by x and y coordinates. Let’s generate
    three rock rings composed of many rocks, using scikit-learn’s `make_circles` function
    (figure 10.11).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一位天文学家在太阳系边缘发现了一个新的行星。这个行星，就像土星一样，在其中心周围有多个环以恒定的轨道旋转。每个环都是由数千块岩石组成的。我们将这些岩石建模为由
    x 和 y 坐标定义的单独点。让我们使用 scikit-learn 的 `make_circles` 函数生成由许多岩石组成的三个岩石环（图 10.11）。
- en: Listing 10.14 Simulating rings around a planet
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.14 模拟行星周围的环
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ The make_circles function creates two concentric circles in 2D. The scale
    of the smaller circle’s radius relative to the larger circle is determined by
    the factor parameter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `make_circles` 函数在 2D 中创建两个同心圆。小圆半径相对于大圆的比例由因子参数确定。
- en: '![](../Images/10-11.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-11.png)'
- en: Figure 10.11 A simulation of three rock rings positioned around a central point
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 围绕中心点位置的三块岩石环的模拟
- en: Three ring groups are clearly present in the plot. Let’s search for these three
    clusters using K-means by setting *K* to 3 (figure 10.12).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中明显存在三个环组。让我们通过将 *K* 设置为 3 来使用 K-means 寻找这三个聚类（图 10.12）。
- en: Listing 10.15 Using K-means to cluster rings
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.15 使用 K-means 对环进行聚类
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/10-12.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-12.png)'
- en: Figure 10.12 K-means clustering fails to properly identify the three distinct
    rock rings.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 K-means 聚类未能正确识别三个不同的岩石环。
- en: The output is an utter failure! K-means dissects the data into three symmetric
    segments, and each segment spans multiple rings. The solution doesn’t align with
    our intuitive expectation that each ring should fall into its own distinct group.
    What went wrong? Well, K-means assumed that the three clusters were defined by
    three unique centers, but the actual rings spin around a single central point.
    The difference between clusters is driven not by centrality, but by density. Each
    ring is constructed from a dense collection of points, with empty areas of sparsely
    populated space serving as the boundaries between rings.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果完全失败！K-means 将数据分割成三个对称的段，每个段跨越多个环。这个解决方案不符合我们直观的预期，即每个环都应该属于它自己的独立组。出了什么问题？嗯，K-means
    假设三个簇由三个独特的中心定义，但实际上这些环是围绕一个单一的中心点旋转的。簇之间的差异不是由中心性驱动的，而是由密度驱动的。每个环都是由密集的点集构成的，而空旷的区域则是人口稀少的空地，作为环之间的边界。
- en: 'We need to design an algorithm that clusters data in dense regions of space.
    Doing so requires that we define whether a given region is dense or sparse. One
    simple definition of *density* is as follows: a point is in a dense region only
    if it’s located within a distance *X* of *Y* other points. We’ll refer to *X*
    and *Y* as `epsilon` and `min_points`, respectively. The following code sets `epsilon`
    to 0.1 and `min_points` to 10\. Thus, our rocks are present in a dense region
    of space if they’re within a 0.1 radius of at least 10 other rocks.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设计一个算法，该算法可以在空间密集区域对数据进行聚类。这样做需要我们定义一个给定的区域是密集的还是稀疏的。*密度*的一个简单定义如下：一个点只有在它位于其他
    *Y* 个点距离 *X* 的范围内时，才被认为是密集区域的一部分。我们将 *X* 和 *Y* 分别称为 `epsilon` 和 `min_points`。以下代码将
    `epsilon` 设置为 0.1，将 `min_points` 设置为 10。因此，如果我们的岩石在至少 10 块其他岩石的 0.1 半径范围内，它们就存在于空间的密集区域。
- en: Listing 10.16 Specifying density parameters
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.16 指定密度参数
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s analyze the density of the first rock in our `rocks` list. We begin by
    searching for all other rocks within `epsilon` units of `rocks[0]`. We store the
    indices of these neighboring rocks in a `neighbor_indices` list.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析 `rocks` 列表中的第一块岩石的密度。我们首先在 `epsilon` 单位内搜索所有其他岩石。我们将这些邻近岩石的索引存储在一个 `neighbor_indices`
    列表中。
- en: Listing 10.17 Finding the neighbors of `rocks[0]`
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.17 查找 `rocks[0]` 的邻居
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we compare the number of neighbors to `min_points` to determine whether
    `rocks[0]` lies in a dense region of space.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们比较邻居的数量与 `min_points`，以确定 `rocks[0]` 是否位于空间的密集区域。
- en: Listing 10.18 Checking the density of `rocks[0]`
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.18 检查 `rocks[0]` 的密度
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The rock at index 0 lies in a dense region of space. Do the neighbors of `rocks[0]`
    also share that dense region of space? This is a tricky question to answer. After
    all, it’s possible that every neighbor has fewer than `min_points` neighbors of
    its own. Under our rigorous density definition, we wouldn’t consider these neighbors
    to be dense points. However, this would lead to a ludicrous situation in which
    the dense region is composed of just a single point: `rocks[0]`. We can avoid
    such absurd outcomes by updating our density definition. Let’s formally define
    *density* as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 索引为 0 的岩石位于空间的密集区域。`rocks[0]` 的邻居是否也共享那个密集区域的空间？这是一个棘手的问题。毕竟，每个邻居可能都有少于 `min_points`
    个自己的邻居。根据我们严格的密度定义，我们不会将这些邻居视为密集点。然而，这会导致一个荒谬的情况，即密集区域只由一个点组成：`rocks[0]`。我们可以通过更新我们的密度定义来避免这种荒谬的结果。让我们正式定义
    *密度* 如下：
- en: If a point is located within `epsilon` distance of `min_points` neighbors, then
    that point is in a dense region of space.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个点位于 `epsilon` 距离内 `min_points` 个邻居的范围内，那么该点位于空间的密集区域。
- en: Every neighbor of a point in a dense region of space also clusters in that space.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空间中密集区域中一个点的每个邻居也在该空间中聚类。
- en: Based on our updated definition, we can combine `rocks[0]` and its neighbors
    into a single dense cluster.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们更新的定义，我们可以将 `rocks[0]` 和它的邻居合并成一个单一的密集簇。
- en: Listing 10.19 Creating a dense cluster
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.19 创建密集簇
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The rock at index 0 and its neighbors form a single 41-element dense cluster.
    Do any neighbors of the neighbors belong to a dense region of space? If so, then
    by our updated definition, these rocks also belong to the dense cluster. Thus,
    by analyzing additional neighboring points, we can expand the size of `dense_region_cluster`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 索引为0的岩石及其邻居形成一个包含41个元素的单一密集聚类。邻居的邻居是否属于空间密集区域？如果是这样，根据我们更新的定义，这些岩石也属于密集聚类。因此，通过分析额外的邻近点，我们可以扩大`dense_region_cluster`的大小。
- en: Listing 10.20 Expanding a dense cluster
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.20 扩展密集聚类
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Converts dense_region_indices into a set. This allows us to update the set
    with additional indices without worrying about duplicates.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将`dense_region_indices`转换为集合。这允许我们更新集合，添加额外的索引而不必担心重复。
- en: We’ve iterated over neighbors of neighbors and expanded our dense cluster nearly
    twentyfold. Why stop there? We can expand our cluster even further by analyzing
    the density of newly encountered neighbors. Iteratively repeating our analysis
    will increase the breadth of our cluster boundary. Eventually, the boundary will
    spread to completely encompass one of our rock rings. Then, with no new neighbors
    to absorb, we can repeat the iterative analysis on a `rocks` element that has
    not been analyzed thus far. The repetition will lead to the clustering of additional
    dense rings.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经迭代了邻居的邻居，并将我们的密集聚类几乎扩大了二十倍。为什么停止在这里呢？我们可以通过分析新遇到的邻居的密度来进一步扩大我们的聚类。迭代重复我们的分析将增加聚类边界的广度。最终，边界将扩展到完全包围我们的一个岩石环。然后，没有新的邻居可以吸收，我们可以在尚未分析的`rocks`元素上重复迭代分析。重复将导致更多密集环的聚类。
- en: The procedure just described is known as DBSCAN. The DBSCAN algorithm organizes
    data based on its spatial distribution.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的流程被称为DBSCAN。DBSCAN算法根据数据的空间分布组织数据。
- en: '10.4 DBSCAN: A clustering algorithm for grouping data based on spatial density'
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 DBSCAN：基于空间密度的聚类算法
- en: 'DBSCAN is an acronym that stands for *density-based spatial clustering of applications
    with noise*. This is a ridiculously long name for what essentially is a very simple
    technique:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN是“基于密度的空间聚类应用噪声”的缩写。这是一个非常长的名字，实际上是一个非常简单的技术：
- en: Select a random `point` coordinate from a `data` list.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个`data`列表中随机选择一个`point`坐标。
- en: Obtain all neighbors within `epsilon` distance of that `point`.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取该`point`点周围距离为`epsilon`的所有邻居。
- en: If fewer than `min_points` neighbors are discovered, repeat step 1 using a different
    random point. Otherwise, group `point` and its neighbors into a single cluster.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果发现的邻居少于`min_points`，则使用不同的随机点重复步骤1。否则，将`point`及其邻居分组到一个单独的聚类中。
- en: Iteratively repeat steps 2 and 3 across all newly discovered neighbors. All
    neighboring dense points are merged into the cluster. Iterations terminate after
    the cluster stops expanding.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有新发现的邻居上迭代重复步骤2和3。所有相邻的密集点都被合并到聚类中。当聚类停止扩展后，迭代终止。
- en: After extracting the entire cluster, repeat steps 1-4 on all data points whose
    density hasn’t yet been analyzed.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提取整个聚类后，对所有尚未分析密度的数据点重复步骤1-4。
- en: 'The DBSCAN procedure can be programmed in less than 20 lines of code. However,
    any basic implementation will run very slowly on our `rocks` list. Programming
    a fast implementation requires some very nuanced optimizations that improve neighbor
    traversal speed and are beyond the scope of this book. Fortunately, there’s no
    need for us to rebuild the algorithm from scratch: scikit-learn provides a speedy
    `DBSCAN` class, which we can import from `sklearn.cluster`. Let’s import and initialize
    the class by assigning `epsilon` and `min_points` using the `eps` and `min_samples`
    parameters. Then we utilize `DBSCAN` to cluster our three rings (figure 10.13).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN过程可以用不到20行代码编程。然而，任何基本实现都会在我们的`rocks`列表上运行得非常慢。编写快速实现需要一些非常微妙的优化，这些优化可以提高邻居遍历速度，但超出了本书的范围。幸运的是，我们不需要从头开始重建算法：scikit-learn提供了一个快速的`DBSCAN`类，我们可以从`sklearn.cluster`导入它。让我们通过使用`eps`和`min_samples`参数分配`epsilon`和`min_points`来导入并初始化这个类。然后我们利用`DBSCAN`来聚类我们的三个环（图10.13）。
- en: Listing 10.21 Using `DBSCAN` to cluster rings
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.21 使用`DBSCAN`聚类环
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Creates a cluster_model object to carry out density clustering. An epsilon
    value of 0.1 is passed in using the eps parameter. A min_points value of 10 is
    passed in using the min_samples parameter.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个`cluster_model`对象以执行密度聚类。使用`eps`参数传入一个epsilon值为0.1，使用`min_samples`参数传入一个min_points值为10。
- en: ❷ Clusters the rock rings based on density, and returns the assigned cluster
    for each rock
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据密度对岩石环进行聚类，并为每个岩石返回分配的聚类
- en: '![](../Images/10-13.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-13.png)'
- en: Figure 10.13 DBSCAN clustering accurately identifies the three distinct rock
    rings.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13 DBSCAN聚类准确识别出三个不同的岩石环。
- en: DBSCAN has successfully identified the three rock rings. The algorithm succeeded
    where K-means failed.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN成功识别了三个岩石环。该算法在K-means失败的地方取得了成功。
- en: 10.4.1 Comparing DBSCAN and K-means
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 比较DBSCAN和K-means
- en: DBSCAN is an advantageous algorithm for clustering data composed of curving
    and dense shapes. Also, unlike K-means, the algorithm doesn’t require an approximation
    of the cluster count before execution. Additionally, DBSCAN can filter random
    outliers located in sparse regions of space. For example, if we add an outlier
    located beyond the boundary of the rings, DBSCAN will assign it a cluster ID of
    –1\. The negative value indicates that the outlier cannot be clustered with the
    rest of the dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN是一种适用于由曲线和密集形状组成的数据聚类的有利算法。此外，与K-means不同，该算法在执行前不需要对聚类数量进行近似。此外，DBSCAN还可以过滤位于空间稀疏区域的随机异常值。例如，如果我们添加一个位于环边界之外的异常值，DBSCAN将分配给它一个聚类ID为-1。负值表示该异常值无法与数据集的其余部分聚类。
- en: Note Unlike K-means, a fitted DBSCAN model cannot be reapplied to brand-new
    data. Instead, we need to combine new and old data and execute the clustering
    from scratch. This is because computed K-means centers can easily be compared
    to additional data points. However, the additional data points could influence
    the density distribution of previously seen data, which forces DBSCAN to recompute
    all clusters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 与K-means不同，拟合的DBSCAN模型不能重新应用于全新的数据。相反，我们需要将新数据和旧数据结合起来，并从头开始执行聚类。这是因为计算出的K-means中心可以很容易地与额外的数据点进行比较。然而，额外的数据点可能会影响先前看到的数据的密度分布，这迫使DBSCAN重新计算所有聚类。
- en: Listing 10.22 Finding outliers using DBSCAN
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.22 使用DBSCAN查找异常值
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Another advantage of the DBSCAN technique is that it does not depend on the
    mean. Meanwhile, the K-means algorithm requires us to compute the mean coordinates
    of grouped points. As we discussed in section 5, these mean coordinates minimize
    the sum of squared distances to the center. The minimization property holds only
    if the squared distances are Euclidean. Thus, if our coordinates are not Euclidean,
    the mean is not very useful, and the K-means algorithm should not be applied.
    However, the Euclidean distance is not the only metric for gauging separation
    between points—infinite metrics exist for defining distance. We explore a few
    of them in the subsequent subsection. In the process, we learn how to integrate
    these metrics into our DBSCAN clustering output.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN技术的另一个优点是它不依赖于均值。与此同时，K-means算法要求我们计算分组点的均值坐标。正如我们在第5节中讨论的，这些均值坐标最小化了到中心的平方距离之和。最小化属性仅在平方距离是欧几里得的情况下成立。因此，如果我们的坐标不是欧几里得坐标，均值就不是很有用，不应该应用K-means算法。然而，欧几里得距离并不是衡量点之间分离的唯一度量标准——存在无限多的度量标准来定义距离。我们在后续小节中探索其中的一些。在这个过程中，我们学习如何将这些度量标准集成到我们的DBSCAN聚类输出中。
- en: 10.4.2 Clustering based on non-Euclidean distance
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 基于非欧几里得距离的聚类
- en: Suppose that we are visiting Manhattan and wish to know the walking distance
    from the Empire State Building to Columbus Circle. The Empire State Building is
    located at the intersection of 34th Street and Fifth Avenue. Meanwhile, Columbus
    Circle is located at the intersection of 57th Street and Eighth Avenue. The streets
    and avenues in Manhattan are always perpendicular to each other. This lets us
    represent Manhattan as a 2D coordinate system, where streets are positioned on
    the x-axis and avenues are positioned on the y-axis. Under this representation,
    the Empire State Building is located at coordinate (34, 5), and Columbus Circle
    is located at coordinate (57, 8). We can easily calculate a straight-line Euclidean
    distance between the two coordinate points. However, that final length would be
    impassable because towering steel buildings occupy the area outlined by every
    city block. A more correct solution is limited to a path across the perpendicular
    sidewalks that form the city’s grid. Such a route requires us to walk 3 blocks
    between Fifth Avenue and Third Avenue and then 23 blocks between 34th Street and
    57th Street, for a total distance of 26 blocks. Manhattan’s average block length
    is 0.17 miles, so we can estimate the walking distance as 4.42 miles. Let’s compute
    that walking distance directly using a generalized `manhattan_distance` function.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在参观曼哈顿，并希望知道从帝国大厦到哥伦布圆环的步行距离。帝国大厦位于34街和第五大道的交汇处。同时，哥伦布圆环位于57街和第八大道的交汇处。曼哈顿的街道和大道总是相互垂直。这使得我们可以将曼哈顿表示为一个二维坐标系，其中街道位于x轴上，大道位于y轴上。在这种表示下，帝国大厦位于坐标(34,
    5)，哥伦布圆环位于坐标(57, 8)。我们可以轻松地计算两个坐标点之间的直线欧几里得距离。然而，那个最终长度是无法通行的，因为高耸的钢铁建筑占据了每个城市街区所围成的区域。一个更正确的解决方案仅限于穿过城市网格形成的垂直人行道的路径。这样的路线要求我们在第五大道和第三大道之间走3个街区，然后在34街和57街之间走23个街区，总距离为26个街区。曼哈顿的平均街区长度为0.17英里，因此我们可以估计步行距离为4.42英里。让我们直接使用一个通用的`manhattan_distance`函数来计算这个步行距离。
- en: Listing 10.23 Computing the Manhattan distance
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.23 计算曼哈顿距离
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ We can also generate this output by importing cityblock from scipy.spatial.distance
    and then running 0.17 * cityblock(x, y).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们也可以通过从scipy.spatial.distance导入cityblock然后运行0.17 * cityblock(x, y)来生成这个输出。
- en: Now, suppose we wish to cluster more than two Manhattan locations. We’ll assume
    each cluster holds a point that is within a one-mile walk of three other clustered
    points. This assumption lets us apply DBSCAN clustering using scikit-learn’s `DBSCAN`
    class. We set `eps` to 1 and `min_samples` to 3 during DBSCAN’s initialization.
    Furthermore, we pass `metric= manhattan_distance` into the initialization method.
    The `metric` parameter swaps Euclidean distance for our custom distance metric,
    so the clustering distance correctly reflects the grid-based constraints within
    the city. The following code clusters Manhattan coordinates and plots them on
    a grid along with their cluster designations (figure 10.14).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们希望聚类超过两个曼哈顿位置。我们将假设每个聚类包含一个点，该点距离其他三个聚类点步行一英里以内。这个假设使我们能够使用scikit-learn的`DBSCAN`类应用DBSCAN聚类。我们在DBSCAN初始化时将`eps`设置为1，将`min_samples`设置为3。此外，我们将`metric=
    manhattan_distance`传递到初始化方法中。`metric`参数将欧几里得距离替换为我们自定义的距离度量，因此聚类距离正确地反映了城市内基于网格的约束。以下代码聚类曼哈顿坐标，并在网格上绘制它们及其聚类标识（图10.14）。
- en: Listing 10.24 Clustering using Manhattan distance
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.24 使用曼哈顿距离进行聚类
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ The manhattan_distance function is passed into DBSCAN through the metric parameter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将`manhattan_distance`函数通过`metric`参数传递给DBSCAN。
- en: ❷ Outliers are plotted using x-shaped markers.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 异常值使用X形标记进行绘制。
- en: ❸ The grid method displays the rectangular grid across which we compute Manhattan
    distance.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 网格法显示了我们计算曼哈顿距离的矩形网格。
- en: '![](../Images/10-14.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-14.png)'
- en: Figure 10.14 Five points in a rectangular grid have been clustered using the
    Manhattan distance. The three points in the lower-left corner of the grid fall
    within a single cluster. The remaining two points are outliers, marked by an x.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14 使用曼哈顿距离在矩形网格中对五个点进行聚类。网格左下角的三点位于单个聚类中。其余两个点是异常值，用x标记。
- en: 'The first three locations fall within a single cluster, and the remaining points
    are outliers. Could we have detected that cluster using the K-means algorithm?
    Perhaps. After all, our Manhattan block coordinates can be averaged out, making
    them compatible with a K-means implementation. What if we swap Manhattan distance
    for a different metric where average coordinates are not so easily obtained? Let’s
    define a nonlinear distance metric with the following properties: two points are
    0 units apart if all their elements are negative, 2 units apart if all their elements
    are non-negative, and 10 units apart otherwise. Given this ridiculous measure
    of distance, can we compute the mean of any two arbitrary points? We can’t, and
    K-means cannot be applied. A weakness of the algorithm is that it depends on the
    existence of an average distance. Unlike K-means, the DBSCAN algorithm does not
    require our distance function to be linearly divisible. Thus, we can easily run
    DBSCAN clustering using our ridiculous distance metric.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 前三个位置位于单个聚类中，其余点为异常值。我们能否使用 K-means 算法检测到该聚类？或许可以。毕竟，我们的曼哈顿街区坐标可以平均化，使其与 K-means
    实现兼容。如果我们用不同的度量替换曼哈顿距离，其中平均坐标不容易获得，会怎样？让我们定义一个具有以下特性的非线性距离度量：如果所有元素都是负数，两个点之间距离为
    0 个单位，如果所有元素都是非负数，则距离为 2 个单位，否则为 10 个单位。给定这种荒谬的距离度量，我们能否计算任意两个点的平均值？我们不能，K-means
    也不能应用。该算法的一个弱点是它依赖于平均距离的存在。与 K-means 不同，DBSCAN 算法不需要我们的距离函数是线性可分的。因此，我们可以轻松地使用我们的荒谬距离度量运行
    DBSCAN 聚类。
- en: Listing 10.25 Clustering using a ridiculous measure of distance
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.25 使用荒谬的距离度量进行聚类
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Returns a Boolean array where is_negative_a[i] is True if point_a[i] < 0
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回一个布尔数组，其中 is_negative_a[i] 为 True 如果 point_a[i] < 0
- en: ❷ All elements of point_a and point_b are negative.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ point_a 和 point_b 的所有元素都是负数。
- en: ❸ A negative element exists, but not all elements are negative.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 存在一个负元素，但并非所有元素都是负数。
- en: ❹ All elements are non-negative.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 所有元素都是非负数。
- en: Running DBSCAN with our `ridiculous_measure` metric leads to the clustering
    of negative coordinates into a single group. All other coordinates are treated
    as outliers. These results are not conceptually practical, but the flexibility
    with regard to metric selection is much appreciated. We are not constrained in
    our metric choice! We could, for instance, set the metric to compute traversal
    distance based on the curvature of the Earth. Such a metric would be particularly
    useful for clustering geographic locations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的 `ridiculous_measure` 度量运行 DBSCAN 导致负坐标聚类成一个单独的组。所有其他坐标都被视为异常值。这些结果在概念上并不实用，但关于度量选择的灵活性非常受欢迎。我们在度量选择上不受限制！例如，我们可以将度量设置为基于地球曲率的遍历距离。这种度量对于聚类地理位置特别有用。
- en: DBSCAN clustering methods
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 聚类方法
- en: '`dbscan_model = DBSCAN(eps=epsilon, min_samples=min_points)`—Creates a DBSCAN
    model to cluster by density. A dense point is defined as having at least `min_points`
    neighbors within a distance of `epsilon`. The neighbors are considered to be part
    of the same cluster as the point.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dbscan_model = DBSCAN(eps=epsilon, min_samples=min_points)`—创建一个 DBSCAN 模型，通过密度进行聚类。密集点定义为在距离
    `epsilon` 内至少有 `min_points` 个邻居的点。邻居被认为与点属于同一个聚类。'
- en: '`clusters = dbscan_model.fit_predict(data)`—Executes DBSCAN on inputted data
    using an initialized `DBSCAN` object. The `clusters` array contains cluster IDs.
    The cluster ID of `data[i]` is equal to `clusters[i]`. Unclustered outlier points
    are assigned an ID of –1.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters = dbscan_model.fit_predict(data)`—使用初始化的 `DBSCAN` 对象在输入数据上执行 DBSCAN。`clusters`
    数组包含聚类 ID。`data[i]` 的聚类 ID 等于 `clusters[i]`。未聚类的异常值点被分配一个 ID 为 –1。'
- en: '`clusters = DBSCAN(eps=epsilon, min_samples=min_points).fit_predict (data)`—Executes
    DBSCAN in a single line of code, and returns the resulting clusters.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clusters = DBSCAN(eps=epsilon, min_samples=min_points).fit_predict (data)`—使用单行代码执行
    DBSCAN，并返回结果聚类。'
- en: '`dbscan_model = DBSCAN(eps=epsilon, min_samples=min_points, metric =metric_function)`—Creates
    a DBSCAN model where the distance metric is defined by a custom metric function.
    The `metric_function` distance metric does not need to be Euclidean.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dbscan_model = DBSCAN(eps=epsilon, min_samples=min_points, metric =metric_function)`—创建一个
    DBSCAN 模型，其中距离度量由自定义度量函数定义。`metric_function` 距离度量不需要是欧几里得距离。'
- en: 'DBSCAN does have certain drawbacks. The algorithm is intended to detect clusters
    with similar point-density distributions. However, real-world data varies in density.
    For instance, pizza shops in Manhattan are distributed more densely than pizza
    shops in Orange County, California. Thus, we might have trouble choosing density
    parameters that will let us cluster shops in both locations. This highlights another
    limitation of the algorithm: DBSCAN requires meaningful values for the `eps` and
    `min_samples` parameters. In particular, varying `eps` inputs will greatly impact
    the quality of clustering. Unfortunately, there is no one reliable procedure for
    estimating the appropriate `eps`. While certain heuristics are occasionally mentioned
    in the literature, their benefit is minimal. Most of the time, we must rely on
    our gut-level understanding of the problem to assign practical inputs to the two
    DBSCAN parameters. For example, if we were to cluster a set of geographic locations,
    our `eps` and `min_samples` values would depend on whether the locations are spread
    out across the entire globe or whether they are constrained to a single geographic
    region. In each instance, our understanding of density and distance would vary.
    Generally speaking, if we are clustering random cities spread out across the Earth,
    we can set the `min_samples` and `eps` parameters to equal three cities and 250
    miles, respectively. This assumes each cluster holds a city within 250 miles of
    at least three other clustered cities. For a more regional location distribution,
    a lower `eps` value is required.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 算法确实存在某些缺点。该算法旨在检测具有相似点密度分布的簇。然而，现实世界的数据在密度上有所不同。例如，曼哈顿的披萨店比加利福尼亚橙县的披萨店分布得更密集。因此，我们可能难以选择能够让我们在两个地点聚类商店的密度参数。这突出了算法的另一个局限性：DBSCAN
    需要 `eps` 和 `min_samples` 参数的有意义值。特别是，`eps` 输入的变化将极大地影响聚类的质量。不幸的是，没有一种可靠的程序可以用来估计适当的
    `eps`。虽然文献中偶尔提到了某些启发式方法，但它们的益处很小。大多数时候，我们必须依靠我们对问题的直观理解来为这两个 DBSCAN 参数分配实际输入。例如，如果我们要对一组地理位置进行聚类，我们的
    `eps` 和 `min_samples` 值将取决于这些位置是否分布在整个地球或是否仅限于单个地理区域。在每种情况下，我们对密度和距离的理解都会有所不同。一般来说，如果我们正在聚类散布在地球上的随机城市，我们可以将
    `min_samples` 和 `eps` 参数分别设置为三个城市和 250 英里。这假设每个簇都包含至少三个其他聚类城市的城市，距离在 250 英里以内。对于更区域性的位置分布，需要更低的
    `eps` 值。
- en: 10.5 Analyzing clusters using Pandas
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 使用 Pandas 分析簇
- en: So far, we have kept our data inputs and clustering outputs separate. For instance,
    in our rock ring analysis, the input data is in the `rocks` list and the clustering
    output is in a `rock_clusters` array. Tracking both the coordinates and the clusters
    requires us to map indices between the input list and the output array. Thus,
    if we wish to extract all the rocks in cluster 0, we must obtain all instances
    of `rocks[i`] where `rock_ clusters[i] == 0`. This index analysis is convoluted.
    We can more intuitively analyze clustered rocks by combining the coordinates and
    the clusters together in a single Pandas table.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直将数据输入和聚类输出分开。例如，在我们的岩石环分析中，输入数据在 `rocks` 列表中，聚类输出在 `rock_clusters`
    数组中。跟踪坐标和簇需要我们在输入列表和输出数组之间映射索引。因此，如果我们想提取簇 0 中的所有岩石，我们必须获取所有 `rocks[i]` 的实例，其中
    `rock_clusters[i] == 0`。这种索引分析很复杂。我们可以通过将坐标和簇结合到一个单一的 Pandas 表格中，更直观地分析聚类岩石。
- en: 'The following code creates a Pandas table with three columns: `X`, `Y`, and
    `Cluster`. Each *i*th row in the table holds the x coordinate, the y coordinate,
    and the cluster of the rock located at `rocks[i]`.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建了一个包含三个列的 Pandas 表格：`X`、`Y` 和 `Cluster`。表格中的第 *i* 行包含位于 `rocks[i]` 的岩石的
    x 坐标、y 坐标和簇。
- en: Listing 10.26 Storing clustered coordinates in a table
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.26 在表中存储聚类坐标
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Our Pandas table lets us easily access the rocks in any cluster. Let’s plot
    the rocks that fall into cluster 0, using techniques described in section 8 (figure
    10.15).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Pandas 表格使我们能够轻松访问任何簇中的岩石。让我们绘制落入簇 0 的岩石，使用第 8 节（图 10.15）中描述的技术。
- en: Listing 10.27 Plotting a single cluster using Pandas
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.27 使用 Pandas 绘制单个簇
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Select just those rows where the Cluster column equals 0
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅选择那些簇列等于 0 的行
- en: ❷ Plots the X and Y columns of the selected rows. Note that we can also execute
    the scatter plot by running df_cluster.plot.scatter(x='X', y='Y').
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制所选行的 X 和 Y 列。请注意，我们也可以通过运行 df_cluster.plot.scatter(x='X', y='Y') 来执行散点图。
- en: '![](../Images/10-15.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-15.png)'
- en: Figure 10.15 Rocks that fall into cluster 0
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15 落入簇 0 的岩石
- en: 'Pandas allows us to obtain a table containing elements from any single cluster.
    Alternatively, we might want to obtain multiple tables, where each table maps
    to a cluster ID. In Pandas, this is done by calling `df.groupby(''Cluster'')`.
    The `groupby` method will create three tables: one for each cluster. It will return
    an iterable over the mappings between cluster IDs and tables. Let’s use the `groupby`
    method to iterate over our three clusters. We’ll subsequently plot the rocks in
    cluster 1 and cluster 2, but not the rocks in cluster 0 (figure 10.16).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 允许我们从任何单个簇中获取包含元素的表格。或者，我们可能想要获取多个表格，其中每个表格映射到一个簇 ID。在 Pandas 中，这是通过调用
    `df.groupby('Cluster')` 来实现的。`groupby` 方法将创建三个表格：一个对应每个簇。它将返回一个可迭代的簇 ID 和表格之间的映射。让我们使用
    `groupby` 方法遍历我们的三个簇。我们将随后绘制簇 1 和簇 2 中的岩石，但不会绘制簇 0 中的岩石（图 10.16）。
- en: 'Note Calling `df.groupby(''Cluster'')` returns more than just an iterable:
    it returns a `DataFrameGroupBy` object, which provides additional methods for
    cluster filtering and analysis.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：调用 `df.groupby('Cluster')` 返回的不仅仅是可迭代的对象：它返回一个 `DataFrameGroupBy` 对象，该对象提供了用于簇过滤和分析的额外方法。
- en: Listing 10.28 Iterating over clusters using Pandas
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.28 使用 Pandas 遍历簇
- en: '[PRE27]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Each element of the iterable returned by df.groupby('Cluster') is a tuple.
    The first element of the tuple is the cluster ID obtained from df.Cluster. The
    second element is a table composed of all rows where df.Cluster equals the cluster
    ID.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ df.groupby('Cluster') 返回的可迭代对象的每个元素都是一个元组。元组的第一个元素是从 df.Cluster 获得的簇 ID。第二个元素是由
    df.Cluster 等于簇 ID 的所有行组成的表格。
- en: '![](../Images/10-16.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片 10-16](../Images/10-16.png)'
- en: Figure 10.16 Rocks that fall into clusters 1 and 2
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 落入簇 1 和簇 2 的岩石
- en: The Pandas `groupby` method lets us iteratively examine different clusters.
    This could prove useful in our case study 3 analysis.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 的 `groupby` 方法让我们可以迭代地检查不同的簇。这在我们的案例研究 3 分析中可能很有用。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The *K-means* algorithm clusters inputted data by searching for *K centroids*.
    These centroids represent the mean coordinates of the discovered data groups.
    K-means is initialized by selecting *K* random centroids. Each data point is then
    clustered based on its nearest centroid, and the centroids are iteratively recomputed
    until they converge on stable locations.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K-means* 算法通过寻找 *K 个质心* 来聚类输入数据。这些质心代表发现的数据组的平均坐标。K-means 通过选择 *K* 个随机质心来初始化。然后，每个数据点根据其最近的质心进行聚类，并且质心被迭代地重新计算，直到它们收敛到稳定的位置。'
- en: K-means is guaranteed to converge to a solution. However, that solution may
    not be optimal.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means 保证收敛到解决方案。然而，该解决方案可能不是最优的。
- en: K-means requires Euclidean distances to distinguish between points. The algorithm
    is not intended to cluster non-Euclidean coordinates.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means 需要欧几里得距离来区分点。该算法不适用于聚类非欧几里得坐标。
- en: After executing K-means clustering, we can compute the *inertia* of the result.
    Inertia equals the sum of the squared distances between each data point and its
    closest center.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行 K-means 聚类后，我们可以计算结果的 *惯性*。惯性等于每个数据点与其最近中心之间的平方距离之和。
- en: Plotting the inertia across a range of *K* values generates an *elbow plot*.
    The elbow component in the elbow-shaped plot should point downward to a reasonable
    *K* value. Using the elbow plot, we can heuristically select a meaningful *K*
    input for K-means.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一系列 *K* 值范围内绘制惯性将生成一个 *肘部图*。肘部图中的肘部成分应指向一个合理的 *K* 值。使用肘部图，我们可以启发式地选择 K-means
    的一个有意义的 *K* 输入。
- en: The *DBSCAN* algorithm clusters data based on density. Density is defined using
    the `epsilon` and `min_points` parameters. If a point is located within `epsilon`
    distance of `min_points` neighbors, then that point is in a dense region of space.
    Every neighbor of a point in a dense region of space also clusters in that space.
    DBSCAN iteratively expands the boundaries of a dense region of space until a complete
    cluster is detected.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DBSCAN* 算法根据密度聚类数据。密度是通过 `epsilon` 和 `min_points` 参数定义的。如果一个点位于 `epsilon`
    距离内的 `min_points` 个邻居中，那么该点位于空间的一个密集区域。空间中一个点的每个邻居也在该空间中聚类。DBSCAN 通过迭代扩展密集区域的空间边界，直到检测到一个完整的簇。'
- en: Points in non-dense regions are not clustered by the DBSCAN algorithm. They
    are treated as outliers.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN 算法不会聚类非密集区域中的点。它们被视为异常值。
- en: DBSCAN is an advantageous algorithm for clustering data composed of curving
    and dense shapes.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN 是一种适用于由曲线和密集形状组成的数据聚类的有利算法。
- en: DBSCAN can cluster using arbitrary, non-Euclidean distances.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN可以使用任意、非欧几里得距离进行聚类。
- en: There is no reliable heuristic for choosing appropriate `epsilon` and `min_points`
    parameters. However, if we wish to cluster global cities, we can set the two parameters
    to 250 miles and three cities, respectively.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有可靠的启发式方法来选择合适的`epsilon`和`min_points`参数。然而，如果我们希望对全球城市进行聚类，可以将这两个参数分别设置为250英里和三个城市。
- en: Storing clustered data in a Pandas table allows us to intuitively iterate over
    clusters with the `groupby` method.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将聚类数据存储在Pandas表中，使我们能够通过`groupby`方法直观地遍历聚类。
