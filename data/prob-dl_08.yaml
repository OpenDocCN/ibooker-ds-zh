- en: 6 Probabilistic deep learning models in the wild
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 野外的概率深度学习模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Probabilistic deep learning in state-of-the-art models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最先进模型中的概率深度学习
- en: Flexible distributions in modern architectures
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代架构中的灵活分布
- en: Mixtures of probability distributions for flexible CPDs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于灵活CPD的概率分布混合
- en: Normalizing flows to generate complex data like facial images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 归一化流用于生成如面部图像等复杂数据
- en: '![](../Images/6-unnumb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-unnumb.png)'
- en: Many real-world data like sound samples or images come from complex and high-dimensional
    distributions. In this chapter, you’ll learn how to define complex probability
    distributions that can be used to model real-world data. In the last two chapters,
    you learned to set up models that work with easy-to-handle distributions. You
    worked with linear regression models with a Gaussian conditional probability distribution
    (CPD) or a Poisson model with its distribution as a CPD. (Maybe you find yourself
    in the figure at the top of this chapter, where the ranger stands in a protected
    area with some domestic animals, but the animals out in the world are more wilder
    than the ones you’ve worked with up to now.) You also learned enough about different
    kinds of domestic probabilistic models to join us and journey into the wild to
    state-of-the-art models that handle complex CPDs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的数据，如声音样本或图像，来自复杂和高维分布。在本章中，你将学习如何定义复杂的概率分布，这些分布可以用来模拟现实世界的数据。在前两章中，你学习了如何设置与易于处理的分布一起工作的模型。你使用具有高斯条件概率分布（CPD）的线性回归模型或具有作为CPD的分布的泊松模型进行了工作。（也许你发现自己处于本章顶部的图中，那里有
    Ranger 站在一个有家畜的保护区内，但世界上的动物比你到目前为止所工作的动物更野。）你还学习了足够多的关于不同类型的家畜概率模型的知识，以便加入我们，进入处理复杂CPD的最先进模型的世界。
- en: One way to model complex distributions are mixtures of simple distributions
    such as Normal, Poisson, or logistic distributions, which you know from the previous
    chapters. Mixture models are used in state-of-the-art networks like Google’s parallel
    WaveNet or OpenAI’s PixelCNN++ to model the output.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟复杂分布的一种方式是简单分布的混合，例如正态分布、泊松分布或逻辑分布，这些分布你在前几章中已经了解过。混合模型被用于最先进的网络中，如谷歌的并行WaveNet或OpenAI的PixelCNN++，以模拟输出。
- en: WaveNet generates realistic sounding speech from text.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WaveNet从文本生成听起来逼真的语音。
- en: PixelCNN++ generates realistic looking images.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PixelCNN++生成看起来逼真的图像。
- en: 'In a case study in this chapter, we give you the chance to set up your own
    mixture models and use these to outperform a recent publicly described prediction
    model. You also learn another way to model these complex distributions: the so-called
    normalizing flows. Normalizing flows(NFs) allow you to learn a transformation
    from a simple distribution to a complicated distribution. In simple cases, this
    can be done with a statistical method called the change of variable method. You’ll
    learn how to apply this method in section 6.3.2, and you’ll see that TensorFlow
    Probability (TFP) supports this method with so-called bijectors.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的一个案例研究中，我们给你机会设置自己的混合模型，并使用这些模型超越最近公开描述的预测模型。你还学习了一种模拟这些复杂分布的另一种方法：所谓的归一化流。归一化流（NFs）允许你学习从简单分布到复杂分布的转换。在简单的情况下，这可以通过一种称为变量变换的统计方法来完成。你将在第6.3.2节中学习如何应用这种方法，你将看到TensorFlow
    Probability（TFP）通过所谓的双射器支持这种方法。
- en: By combining the change of variable method with DL, you can learn quite complicated
    and high-dimensional distributions as encountered in real-world applications.
    For example, sensor readings from complex machines are high-dimensional data.
    If you have a machine that is working correctly, you can learn the corresponding
    “machine OK” distribution. After you learn this distribution, you can continuously
    check if the sensor data the machine produces is still from the “machine OK” distribution.
    If the probability that the sensor data comes from the “machine OK” distribution
    is low, you might want to check the machine. This application is called novelty
    detection. But you can also do more fun applications, such as modeling the distributions
    of images of faces and then sampling from this distribution to create realistic-looking
    faces of people who don’t exist. You can imagine that such a facial image distribution
    is quite complicated. You’ll do other fun stuff with this distribution too, like
    giving Leonardo DiCaprio a goatee or morphing between different people. Sound
    complicated? Well, it’s a bit complicated, but the good thing is that it works
    with the same principle that you’ve used so far (and will continue to use for
    the rest of the book)--the principle of maximum likelihood (MaxLike).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将变量变换方法与深度学习相结合，您可以学习到在现实世界应用中遇到的相当复杂和高维的分布。例如，复杂机器的传感器读数是高维数据。如果您有一台正在正常工作的机器，您可以学习相应的“机器正常”分布。学习了这个分布之后，您可以持续检查机器产生的传感器数据是否仍然来自“机器正常”分布。如果传感器数据来自“机器正常”分布的概率低，您可能需要检查这台机器。这种应用被称为新颖性检测。但您还可以进行更多有趣的应用，例如建模人脸图像的分布，然后从这个分布中采样以创建不存在的人的逼真人脸。您可以想象这样的面部图像分布相当复杂。您还会用这个分布做其他有趣的事情，比如给莱昂纳多·迪卡普里奥画一个络腮胡或者在不同人之间进行变形。听起来很复杂吗？好吧，它确实有点复杂，但好消息是它使用的是您迄今为止（以及本书剩余部分将继续使用）所使用的相同原理——最大似然原理（MaxLike）。
- en: 6.1 Flexible probability distributions in state-of-the-art DL models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 先进深度学习模型中的灵活概率分布
- en: In this section, you’ll see how to use flexible probability distributions for
    state-of-the-art models in DL. Up to now, you’ve encountered different probability
    distributions such as the Normal or uniform distribution for a continuous variable
    (the blood pressure in the American women data), a multinomial distribution for
    a categorical variable (the ten digits in the MNIST data), or the Poisson and
    zero-inflated Poisson (*z*IP) for count data (the number of fish caught in the
    camper data).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解如何使用灵活的概率分布来构建深度学习中的先进模型。到目前为止，您已经遇到了不同的概率分布，例如连续变量（美国女性数据中的血压）的正态分布或均匀分布，分类变量（MNIST数据中的十个数字）的多项分布，或者是计数数据（露营者数据中捕获的鱼的数量）的泊松分布和零膨胀泊松分布（*z*IP）。
- en: The number of parameters defining the distribution is often an indicator of
    the flexibility of the distribution. The Poisson distribution, for example, has
    only one parameter (often called rate).The ZIP distribution has two parameters
    (rate and the mixing proportion), and in chapter 5, you saw that you could achieve
    a better model for the camper data when using the ZIP distribution instead of
    the Poisson distribution as the CPD. According to this criterion, the multinomial
    distribution is especially flexible because it has as many parameters as possible
    values (or, actually, one parameter less because probabilities need to sum up
    to one). In the MNIST example, you used an image as input to predict a multinomial
    CPD for the categorical outcome. The predicted multinomial CPD has ten (or more
    correctly, nine) parameters, giving us the probabilities of ten possible classes
    (see figure 6.1).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 定义分布的参数数量通常是分布灵活性的一个指标。例如，泊松分布只有一个参数（通常称为速率）。ZIP分布有两个参数（速率和混合比例），在第五章中，您看到当使用ZIP分布而不是泊松分布作为CPD时，您可以为露营者数据实现更好的模型。根据这一标准，多项分布特别灵活，因为它具有尽可能多的参数值（或者更准确地说，少一个参数，因为概率需要加起来等于一）。在MNIST示例中，您使用图像作为输入来预测分类结果的多元CPD。预测的多元CPD有十个（或者更准确地说，九个）参数，为我们提供了十个可能类别的概率（见图6.1）。
- en: '![](../Images/6-1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-1.png)'
- en: 'Figure 6.1 Multinomial distribution with ten classes: *MN*(*p*[0] , *p*[1]
    , *p*[2] , *p*[3] , *p*[4] , , *p*[5] , *p*[6] , *p*[7] , *p*[8] , *p*[9])'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 具有十个类别的多项分布：*MN*(*p*[0] , *p*[1] , *p*[2] , *p*[3] , *p*[4] , , *p*[5]
    , *p*[6] , *p*[7] , *p*[8] , *p*[9])
- en: Indeed, using the multinomial distribution in a digit classification by convolutional
    neural networks (CNNs) became the first and most heavily used real-world applications
    for DL models. In 1998, Yann LeCun, who was then working at AT&T Bell Laboratory,
    implemented a CNN for ZIP code recognition. This is known as LeNet-5.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在卷积神经网络（CNNs）进行数字分类中使用多项分布已成为深度学习模型在现实世界中的第一个且最广泛使用的应用。1998年，当时在AT&T贝尔实验室工作的Yann
    LeCun实现了一个用于ZIP代码识别的CNN。这被称为LeNet-5。
- en: 6.1.1 Multinomial distribution as a flexible distribution
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 多项分布作为一种灵活的分布
- en: In 2016, an example of a real-world task requiring flexible distribution was
    Google’s WaveNet. This model generates astonishingly real-sounding artificial
    speech from text. Go to [https://cloud.google.com/text-to-speech/](https://cloud.google.com/text-to-speech/)
    for a demonstration on the text of your choice. The architecture is based on 1D
    causal convolutions, like the ones you saw in section 2.3.3, and their specialization
    called dilated convolutions, which are shown in the notebook [http://mng.bz/8pVZ](http://mng.bz/8pVZ)
    . If you’re interested in the architecture, you might also want to read the blog
    post [http://mng.bz/EdJo](http://mng.bz/EdJo) .
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，一个需要灵活分布的现实世界任务示例是谷歌的WaveNet。该模型可以从文本生成听起来非常逼真的合成语音。前往[https://cloud.google.com/text-to-speech/](https://cloud.google.com/text-to-speech/)查看您选择的文本的演示。其架构基于1D因果卷积，就像你在2.3.3节中看到的那样，以及它们的特殊化形式——扩张卷积，这在笔记本[http://mng.bz/8pVZ](http://mng.bz/8pVZ)中有展示。如果你对架构感兴趣，你可能还想阅读博客文章[http://mng.bz/EdJo](http://mng.bz/EdJo)。
- en: 'WaveNet works directly on raw audio, usually using a sampling rate of 16 kHz
    (16 kiloHertz), which is 16,000 samples per second. But you can also use higher
    sampling rates. The audio signal for each time point t is then discretized (typically
    one uses 16-bit for this). For example, the audio signal at time *t* , *x**[t]*
    takes discrete values from 0 to 2^(16) − 1 = 65,535 . But the interesting piece
    for this chapter is the probability part (stuff from the probability shelf on
    the right side of figure 5.1 in chapter 5). In WaveNet, we assume that xt depends
    only on the audio signals of samples earlier in time. This gives us:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet直接在原始音频上工作，通常使用16 kHz（16千赫兹）的采样率，即每秒16,000个样本。但你也可以使用更高的采样率。对于每个时间点t的音频信号，然后进行离散化（通常使用16位进行此操作）。例如，时间
    *t* 的音频信号 *x**[t]* 取离散值从0到2^(16) − 1 = 65,535。但本章有趣的部分是概率部分（来自第5章中图5.1右侧的概率架上的内容）。在WaveNet中，我们假设xt只依赖于时间较早的样本的音频信号。这给我们：
- en: '*P*(*x**[t]*) = *P*(*x**[t]*|*x**[t]*[−1] , *x**[t]*[−2] ,... *x*[0])'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*x**[t]*) = *P*(*x**[t]*|*x**[t]*[−1] , *x**[t]*[−2] ,... *x*[0])'
- en: 'You can sample values xt from previous values as shown in figure 6.2 and then
    determine the probability distribution of future values. Such models are called
    autoregressive models. Note that you look at a probabilistic model where you can
    predict a whole distribution of possible outcomes: *P*(*x**[t]*) This lets you
    determine the likelihood or probability of the observed value *x**[t]* under the
    predicted distribution. Welcome home! You can use the good old MaxLike principle
    to fit this kind of model.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从如图6.2所示的前一个值中采样xt值，然后确定未来值的概率分布。这类模型被称为自回归模型。请注意，你查看的是一个概率模型，你可以预测整个可能结果的分布：*P*(*x**[t]*)
    这让你能够确定在预测分布下观察到的值 *x**[t]* 的似然或概率。欢迎回家！你可以使用古老的MaxLike原理来拟合这类模型。
- en: '![](../Images/6-2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-2.png)'
- en: Figure 6.2 The WaveNet principle. The discrete values of the sound xt at time
    t (on the top) are predicted from earlier values in time (on the bottom). Go to
    [http://mng.bz/NKJN](http://mng.bz/NKJN) for an animated version of this figure
    showing how WaveNet can create samples from the future by successively applying
    equation 6.1.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 WaveNet原理。在顶部，时间t的离散声音值xt被预测为来自时间较早的值（在底部）。前往[http://mng.bz/NKJN](http://mng.bz/NKJN)查看此图的动画版本，展示WaveNet如何通过连续应用方程6.1来创建未来的样本。
- en: But which type of distribution do you pick for *P*(*x**[t]*)? The outcome *x*
    can take all discrete values from 0 to 2^(16) − 1 = 65,535 . You don’t really
    know how the distribution of these values will look. It probably won’t look like
    a Normal distribution; that would suggest that there’s a typical value and that
    the probability of the outcome values is decreasing quickly with distance to this
    typical value. You need a more flexible type of distribution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但你选择哪种分布来表示 *P*(*x**[t]*)？结果 *x* 可以取从0到2^(16) − 1 = 65,535的所有离散值。你并不真正知道这些值的分布看起来会是什么样子。它可能不会像正态分布那样；这会暗示有一个典型值，并且结果值的概率会随着距离这个典型值的增加而迅速下降。你需要一种更灵活的分布类型。
- en: In principle, you can model the 65,536 different values with a multinomial distribution,
    where you estimate a probability for each possible value. This ignores the ordering
    of the values (0 < 1 < 2 < . . . < 65,535), but it is indeed flexible because
    you can estimate for each of the 65,536 possible values a probability. The only
    restriction is that these predicted probabilities need to add up to 1, which can
    be easily achieved by a softmax layer. Oord et al., the authors of the WaveNet
    paper, chose to go down this road, but first, they reduced the depth of the signal
    from 16-bit (encoding 65,536 different values) to 8-bit (encoding 256 different
    values) after performing a non-linear transformation on the original sound values.
    All together, the Deep Mind people trained a dilated causal 1D convolutional NN
    with a softmax output, predicting the multinomial CPD with 256 classes and called
    it WaveNet.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在原则上，你可以用多项式分布来表示65,536个不同的值，其中你为每个可能的值估计一个概率。这忽略了值的顺序（0 < 1 < 2 < . . . < 65,535），但它确实是灵活的，因为你可以为65,536个可能的每个值估计一个概率。唯一的限制是这些预测概率需要加起来等于1，这可以通过softmax层轻松实现。WaveNet论文的作者Oord等人选择了这条路，但在他们这样做之前，他们在对原始声音值进行非线性变换后，将信号的深度从16位（编码65,536个不同的值）降低到8位（编码256个不同的值）。总的来说，Deep
    Mind的人训练了一个具有softmax输出的膨胀因果1D卷积神经网络，预测了256个类别的多项式CPD，并将其称为WaveNet。
- en: 'Now you can draw new samples from the learned distribution. To do so, you provide
    a start sequence of audio values, *x*[0] , *x*[1] , . . ., *x**[t]*[−1] , to the
    trained WaveNet, which will then predict a multinomial CPD: *P*(*x**[t]*) = *P*(*x**[t]*|*x**[t]*[−1]
    , *x**[t]*[−2] ,... *x*[0]). From that you then sample the next audio value *x**[t]*
    . You can proceed by providing *x*[1] , *x*[2] , . . ., *x**[t]*[−1] and then
    sample from the resulting CPD the next value *x**[t]*[+1] , and so on.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以从学习到的分布中抽取新的样本。为此，你向训练好的WaveNet提供一个音频值的起始序列，*x*[0] ， *x*[1] ， . . .， *x**[t]*[−1]
    ，然后WaveNet将预测一个多项式CPD：*P*(*x**[t]*) = *P*(*x**[t]*|*x**[t]*[−1] ， *x**[t]*[−2]
    ，... *x*[0])。然后，从这个多项式CPD中采样下一个音频值 *x**[t]* 。你可以通过提供 *x*[1] ， *x*[2] ， . . .，
    *x**[t]*[−1] 并从结果CPD中采样下一个值 *x**[t]*[+1] 来继续这个过程。
- en: 'Let’s take a look at another prominent autoregressive model: OpenAI’s PixelCNN.
    This is a network that can predict a pixel based on “previous” pixels. While for
    WaveNet, the ordering of the audio values is simply time, for images, there’s
    no natural way to order the pixels. You could, for example, order them like the
    characters in text and read them from left to right and from top to button, like
    you do when reading this text. Then, these models can sample a pixel xt for a
    certain color based on all previous pixels xt'' with t'' < t. You again have the
    same structure as in equation 6.1, where xt is now a pixel value.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看另一个突出的自回归模型：OpenAI的PixelCNN。这是一个可以根据“之前”的像素预测像素的网络。对于WaveNet来说，音频值的顺序仅仅是时间，而对于图像，没有自然的方式来对像素进行排序。例如，你可以像阅读文本一样按文本中的字符顺序排序，从左到右，从上到下读取。然后，这些模型可以根据所有之前的像素xt'（t'
    < t）来采样一个特定颜色的像素xt。你再次拥有与方程6.1中相同的结构，其中xt现在是一个像素值。
- en: How do you train the models? One can take the same approach as in WaveNet and
    encode the pixel values with 8-bit, which limits the output to 256 possible values,
    and use a softmax on an output of 256 one-hot encoded categorical variables. This
    was indeed done in PixelCNN.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何训练这些模型？可以采取与WaveNet相同的方法，用8位对像素值进行编码，这限制了输出到256个可能值，并在256个单热编码的分类变量输出上使用softmax。这确实是在PixelCNN中做到的。
- en: 'A year later, in early 2017, the engineers from OpenAI improved PixelCNN, which
    is reported in the paper called “PixelCNN++: Improving the PixelCNN with Discretized
    Logistic Mixture Likelihood and other Modifications” (see [https://arxiv.org/
    abs/1701.05517](https://arxiv.org/abs/1701.05517)). What! You don’t know what
    “Discretized Logistic Mixture Likelihood” means? No worries. You’ll learn about
    that soon. For the moment, let’s just appreciate that with this new kind of CPD,
    OpenAI improved the prediction performance quantified by a test NLL of 2.92 compared
    to an NLL of 3.14 that was achieved by the original PixelCNN. After that paper
    on PixelCNN++, the Google engineers also enhanced WaveNet to something called
    parallel WaveNet(see [https://arxiv.org/abs/ 1711.10433](https://arxiv.org/abs/1711.10433)).
    Among other enhancements, they switched from a multinomial CPD to a discretized
    logistic mixture distribution as CPD. (you’ll see what this means later.) When
    the parallel WaveNet model was set up, this was quite some work, but now with
    TensorFlow Probability, it’s quite easy as you’ll see in the next section.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一年后，在2017年初，OpenAI的工程师改进了PixelCNN，这在名为“PixelCNN++：通过离散化逻辑混合似然和其他修改改进PixelCNN”（见[https://arxiv.org/abs/1701.05517](https://arxiv.org/abs/1701.05517)）的论文中有报道。什么！你不知道“离散化逻辑混合似然”是什么意思？不用担心。你很快就会了解到。现在，让我们仅仅欣赏一下，有了这种新的CPD，OpenAI通过测试NLL为2.92，比原始PixelCNN的NLL
    3.14有了改进。在那篇关于PixelCNN++的论文之后，谷歌工程师也将WaveNet增强为并行WaveNet（见[https://arxiv.org/abs/1711.10433](https://arxiv.org/abs/1711.10433)）。在众多改进中，他们从多项式CPD切换到离散化逻辑混合分布作为CPD。（你稍后会看到这意味着什么。）当设置并行WaveNet模型时，这是一项相当多的工作，但现在有了TensorFlow
    Probability，它相当简单，你将在下一节中看到。
- en: 6.1.2 Making sense of discretized logistic mixture
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 理解离散化的逻辑混合
- en: In both applications, WaveNet and PixelCNN, one has to predict discrete values
    from 0 to an upper value (typically, 255 or 65,535). This is like count data but
    with a maximum value. Why not take a count distribution like a Poissonian and
    clamp the maximal values? This would be fine, in principle, but it turns out the
    distributions need to be more complex. Therefore, in the papers, a mixture of
    distributions was used. The distributions used for the mixture in the PixelCNN++
    paper were discretized logistic functions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个应用中，WaveNet和PixelCNN，都需要从0预测到上限值（通常是255或65,535）。这就像计数数据，但有最大值。为什么不采用像泊松分布这样的计数分布，并钳位最大值呢？在原则上这应该是可以的，但结果证明分布需要更复杂。因此，在论文中使用了分布的混合。PixelCNN++论文中用于混合的分布是离散化的逻辑函数。
- en: Let’s unfold the discretized logistic mixture. You know that the density of
    a Normal distribution is bell-shaped, and the density of a logistic distribution
    looks, indeed, quite similar. Figure 6.3 shows the densities of the logistic functions
    with different values for the `scale` parameter on the left and the corresponding
    cumulative distribution function(CDF) on the right. The logistic CDF is, in fact,
    the same as the sigmoid activation function used in chapter 2\. Have a look at
    the following optional notebook to learn more about the logistic functions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展开离散化的逻辑混合。你知道正态分布的密度是钟形，逻辑分布的密度实际上看起来非常相似。图6.3展示了左侧具有不同`scale`参数值的逻辑函数密度，以及右侧相应的累积分布函数（CDF）。实际上，逻辑CDF与第2章中使用的sigmoid激活函数相同。查看以下可选笔记本，了解更多关于逻辑函数的信息。
- en: '![](../Images/6-3.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-3.png)'
- en: Figure 6.3 Three logistic functions created using tfd.Logistic(loc=1, scale=scale)
    with values of 0.25, 1.0, and 2.0 for the scale parameter. On the left is the
    probability density function (PDF) and on the right, the cumulative probability
    density function (CDF).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 使用tfd.Logistic(loc=1, scale=scale)创建的三个逻辑函数，`scale`参数的值为0.25、1.0和2.0。左侧是概率密度函数（PDF），右侧是累积概率密度函数（CDF）。
- en: '| ![](../Images/computer-icon.png) | Optional exercise Open [http://mng.bz/D2Jn](http://mng.bz/D2Jn)
    . The notebook shows the code for figures 6.3, 6.4, and 6.5, and for listing 6.2.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![电脑图标](../Images/computer-icon.png) 可选练习 打开 [http://mng.bz/D2Jn](http://mng.bz/D2Jn)
    。笔记本展示了第6.3、6.4和6.5图以及列表6.2的代码。'
- en: Read it in parallel with this text.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与本文并行阅读。
- en: Change the parameters of the distributions and see how the curves change.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变分布的参数，看看曲线如何变化。
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In the WaveNet and PixelCNN models, the outcome is discrete. An appropriate
    CDF should, therefore, model discrete (and not continuous) values. But the logistic
    distribution is for continuous values without lower and upper limits. Therefore,
    we discretize the logistic distribution and clamp the values to the possible range.
    In TFP, this can be done using the `QuantizedDistribution` function. `QuantizedDistribution`
    takes a probability distribution (called inner distribution in figure 6.4) and
    creates a quantized version of it. The optional exercise in the accompanying notebook
    elaborates on the details of using `QuantizedDistribution` .
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在WaveNet和PixelCNN模型中，结果是离散的。因此，适当的CDF应该模拟离散（而不是连续）值。但是，逻辑分布是用于没有上下限的连续值。因此，我们需要将逻辑分布离散化，并将值夹在可能的范围内。在TFP中，可以使用`QuantizedDistribution`函数来完成此操作。`QuantizedDistribution`函数接受一个概率分布（如图6.4中的内部分布）并创建其量化版本。附带的笔记本中的可选练习详细说明了使用`QuantizedDistribution`的细节。
- en: '![](../Images/6-4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-4.png)'
- en: Figure 6.4 A quantized version of a logistic function with the parameters `loc=1`
    and `scale=0.25`
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 参数`loc=1`和`scale=0.25`的逻辑函数的量化版本
- en: To handle more flexible distributions, we mix several quantized logistic distributions
    (see figure 6.5). For the mixing, one can use a categorical distribution that
    determines the weights (mixture proportions) of the different distributions that
    are mixed. The following listing shows an example.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理更灵活的分布，我们混合了几个量化逻辑分布（见图6.5）。对于混合，可以使用一个类别分布来确定混合的不同分布的权重（混合比例）。以下列表显示了一个示例。
- en: Listing 6.1 Mixing two quantized distributions
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1 混合两个量化分布
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Centers of the two base distributions at 4 and 10
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将两个基本分布的中心放在4和10
- en: ❷ Spread of the two base distributions
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 两个基本分布的扩散
- en: ❸ Mixes 80% of the first (at 4.0) and 20% of the second
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将第一个分布（在4.0处）的80%和第二个分布的20%混合
- en: ❹ Two independent distributions
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 两个独立分布
- en: ❺ The quantized versions of the two independent distributions
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 两个独立分布的量化版本
- en: ❻ A mixture of both distributions
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 两个分布的混合
- en: ❼ The mixture is done with a categorical distribution with 80% and 20%.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用80%和20%的类别分布进行混合。
- en: Figure 6.5 shows the resulting distribution. This distribution is appropriate
    for data like pixel values (in the case of PixelCNN) and sound amplitudes (in
    the case of WaveNet). You can easily construct more and more flexible outcome
    distributions if you don’t mix only two but, for example, four or ten distributions
    together.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5显示了得到的分布。这个分布适用于像素值（在PixelCNN的情况下）和声音振幅（在WaveNet的情况下）等数据。如果你不混合两个分布，而是混合四个或十个分布，你可以轻松地构建更多更灵活的结果分布。
- en: '![](../Images/6-5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-5.png)'
- en: Figure 6.5 The resulting discrete distribution when mixing two logistic distributions
    (see listing 6.2 for the code that produces these plots)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 混合两个逻辑分布时得到的离散分布（代码见列表6.2，用于生成这些图表）
- en: If you want to use this distribution instead of, say, a Poissonian for your
    own network, you can copy and paste the function `quant_mixture_logistic` from
    the end of listing 6.2\. It’s taken from the TensorFlow documentation of `QuantizedDistribution`
    .
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要在自己的网络中使用这个分布而不是，比如说，泊松分布，你可以从列表6.2的末尾复制并粘贴函数`quant_mixture_logistic`。这个函数来自`QuantizedDistribution`的TensorFlow文档。
- en: 'For each mixture component, the NN needs to estimate three parameters: the
    location and spread of the component, and how much the component is weighted.
    If you work with num logistic distribution components in the mixture, then the
    output of the NN needs to have 3 · num output nodes: three for each component,
    controlling location, spread, and weight. Note that the function `quant_mixture_logistic`
    expects an output without activation (as it is by default in Keras). The following
    listing shows how to use this function for a mixture with two components. In this
    case, the network has six outputs.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个混合成分，神经网络需要估计三个参数：成分的位置和扩散以及成分的权重。如果你在混合中使用num个逻辑分布成分，那么神经网络的输出需要具有3 · num个输出节点：每个成分三个，分别控制位置、扩散和权重。请注意，函数`quant_mixture_logistic`期望一个没有激活的输出（在Keras中默认如此）。以下列表显示了如何使用此函数进行具有两个成分的混合。在这种情况下，网络有六个输出。
- en: Listing 6.2 Using `quant_mixture_logistic()` as distribution
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.2 将`quant_mixture_logistic()`用作分布
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Splits the output into chunks of size 3
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将输出分成大小为3的块
- en: ❷ Transforms into positive values as needed for the scale
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 转换为正数值，以适应缩放
- en: ❸ Shifts the distribution by 0.5
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将分布向右移动0.5
- en: ❹ Using logits, no need for normalizing the probabilities
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用logits，无需对概率进行归一化
- en: '❺ The last layer of the network. Controls the parameters of the mixture model:
    three for each component (here 2 · 3). Stay with the default linear activation
    and don’t restrict the value range. The transformation ensuring positive values
    is done by the softplus function above.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 网络的最后一层。控制混合模型的参数：每个组件三个（这里为2·3）。保持默认的线性激活，不要限制值范围。确保正值变换的是上面的softplus函数。
- en: '6.2 Case study: Bavarian roadkills'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 案例研究：巴伐利亚道路伤亡
- en: Let’s apply what we learned about mixtures in the last section to a case study
    demonstrating the advantages of using an appropriate flexible probability distribution
    as a conditional outcome distribution. Because it takes quite some computational
    resources to train NNs like PixelCNN, here we use a medium-sized data set. The
    data set describes deer-related car accidents in the years 2002 through 2011 on
    roads in Bavaria, Germany. It counts the number of deer killed during 30-minute
    periods anywhere in Bavaria. We previously used this data set in other studies
    for the analysis of count data. It’s originally from [https://zenodo.org/record/17179](https://zenodo.org/record/17179)
    . Table 6.1 contains some rows of the data set after some preprocessing.[1](#pgfId-1079922)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将上一节关于混合的知识应用到案例研究中，以展示使用适当的灵活概率分布作为条件结果分布的优势。由于训练像PixelCNN这样的神经网络需要相当多的计算资源，所以我们这里使用一个中等大小的数据集。该数据集描述了德国巴伐利亚州2002年至2011年在道路上的鹿相关交通事故。它统计了在任何30分钟期间巴伐利亚境内被杀的鹿的数量。我们之前曾使用这个数据集在其他研究中分析计数数据。它最初来自[https://zenodo.org/record/17179](https://zenodo.org/record/17179)
    。表6.1包含了一些预处理后的数据集行。[1](#pgfId-1079922)
- en: Table 6.1 Some rows of deer-related car accidents in Bavaria
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 巴伐利亚一些与鹿有关的交通事故行
- en: '| Wild | Year | Time | Daytime | Weekday |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 野生 | 年份 | 时间 | 白天 | 星期 |'
- en: '| 0 | 2002.0 | 0.000000 | night.am | Sunday |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2002.0 | 0.000000 | 夜间上午 | 星期日 |'
- en: '| 0 | 2002.0 | 0.020833 | night.am | Sunday |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2002.0 | 0.020833 | 夜间上午 | 星期日 |'
- en: '| . . . | . . . | . . . | . . . | . . . |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| . . . | . . . | . . . | . . . | . . . |'
- en: '| 1 | 2002.0 | 0.208333 | night.am | Sunday |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2002.0 | 0.208333 | 夜间上午 | 星期日 |'
- en: '| 0 | 2002.0 | 0.229167 | pre.sunrise.am | Sunday |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2002.0 | 0.229167 | 日出前上午 | 星期日 |'
- en: '| 0 | 2002.0 | 0.270833 | pre.sunrise.am | Sunday |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2002.0 | 0.270833 | 日出前上午 | 星期日 |'
- en: 'The columns have the following meanings:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列的含义如下：
- en: Wild --The number of deer killed in road accidents in Bavaria.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 野生 --在巴伐利亚交通事故中遇害的鹿的数量。
- en: Year --The year (from 2002 to 2009 in the training set and from 2010 to 2011
    for the test set).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份 --年份（训练集为2002年至2009年，测试集为2010年至2011年）。
- en: Time --The number of days to the event (starting with January 1, 2002, as zero).
    These numbers are measured in fractions of a day. The time resolution, 30 min,
    corresponds to a fraction of 1/48 = 0.020833 (see the second row).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间 --事件发生的天数（从2002年1月1日开始，以0计）。这些数字以一天的分数来衡量。时间分辨率，30分钟，对应于1/48的分数，即0.020833（见第二行）。
- en: 'Daytime --The time during the day with respect to sunset and sunrise. The following
    levels are included in the data set: night.am, pre.sunrise.am, post.sunrise.am,
    day.am, day.pm, pre.sunset.pm, post.sunset.pm, and night.pm, corresponding to
    the times night, before sunrise, after sunrise, morning, afternoon, and so on.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白天 --白天相对于日落和日出的时间。数据集中包含以下级别：夜间上午、日出前上午、日出后上午、上午、下午、日落前下午、日落后下午和夜间下午，分别对应夜间、日出前、日出后、早晨、下午等时间。
- en: Weekday --The weekday from Sunday to Saturday; holidays are coded as Sundays.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 星期 --从星期日到星期六的星期几；假日编码为星期日。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/B2O0](http://mng.bz/B2O0)
    . The notebook contains all you need to load the data set for the deer accident
    case study.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![电脑图标](../Images/computer-icon.png) 实践时间 打开 [http://mng.bz/B2O0](http://mng.bz/B2O0)
    。笔记本包含加载鹿交通事故案例研究数据集所需的所有内容。'
- en: Use all you learned in this section to develop a probabilistic DL model for
    the target variable (wil*D*). You should get an NLL of lower than 1.8 on the test
    set.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用本节所学的一切来开发针对目标变量（wil*D*）的概率深度学习模型。你应该在测试集上获得低于1.8的NLL。
- en: A real challenge is an NLL lower than 1.6599, which is a value obtained with
    sophisticated statistical modeling (see the works from Sandra Siegfried and Torsten
    Hothorn at [http://mng.bz/dygN](http://mng.bz/dygN)).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个真正的挑战是NLL低于1.6599，这是一个通过复杂的统计模型获得的价值（参见Sandra Siegfried和Torsten Hothorn的作品，见[http://mng.bz/dygN](http://mng.bz/dygN)）。
- en: A solution is given in the notebook (try to do better than what’s given). Compare
    your results with the solution.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本中给出了一个解决方案（尽量做得更好）。比较你的结果与解决方案。
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Good hunting! If you get an NLL significantly below 1.65 on the test set, drop
    us a line, and we might do a paper together.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你打猎愉快！如果你在测试集上得到一个低于1.65的NLL，给我们发个信息，我们可能会一起写篇论文。
- en: '6.3 Go with the flow: Introduction to normalizing flows (NFs)'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 随波逐流：归一化流（NFs）简介
- en: In section 6.1, you saw a flexible way to model complex distributions by providing
    a mixture of simple base distributions. This method works great when your distribution
    is in a low-dimensional space. In the case of PixelCNN++ and parallel WaveNet,
    the application tasks are regression problems, and the conditional outcome distribution
    is, therefore, one dimensional.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6.1节中，你看到了一种灵活的方式来通过提供简单基分布的混合来建模复杂分布。当你的分布处于低维空间时，这种方法效果很好。在PixelCNN++和平行WaveNet的情况下，应用任务是回归问题，因此条件结果分布是一维的。
- en: But how does one set up and fit a flexible high-dimensional distribution? Think,
    for example, of color images with 256 × 256 × 3 = 195,840 pixels defining a 195,840-dimensional
    space where each image can be represented by one point. If you pick a random point
    in this space, then you’d most probably get an image that looks like noise. This
    means the distribution of realistic images like facial images only covers a subregion,
    which might not be easy to define. How can you learn the 195,840-dimensional distribution
    from which facial images can be drawn? Use NFs! In a nutshell, an NF learns a
    transformation (flow) from a simple high-dimensional distribution to a complex
    one. In a valid distribution, probabilities need to sum up to 1 in the discrete
    case or the integral needs to be 1 in the continuous case, and these need to be
    normalized. The flows in NFs keep this normalizing property intact. Hence, the
    name normalizing flow or NF for short.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 但如何设置和拟合一个灵活的高维分布呢？例如，考虑256 × 256 × 3 = 195,840像素的颜色图像，它定义了一个195,840维的空间，其中每个图像都可以用一个点来表示。如果你在这个空间中随机选择一个点，那么你很可能会得到一个看起来像噪声的图像。这意味着像面部图像这样的真实图像的分布只覆盖了一个子区域，这可能不容易定义。你如何从可以从中抽取面部图像的195,840维分布中学习？使用NFs！简而言之，NF通过从简单的高维分布到复杂的分布学习一个转换（流）。在一个有效的分布中，在离散情况下概率需要加起来等于1，或者在连续情况下积分需要等于1，并且这些需要被归一化。NF中的流保持了这种归一化属性。因此，得名归一化流或简称NF。
- en: In this section, we explain how NFs work. You’ll see that NFs are probabilistic
    models that you can fit with the same MaxLike approach that you’ve used throughout
    the last couple of chapters. You’ll also be able to use a fitted distribution
    to generate realistic looking faces of people who don’t even exist or to morph
    an image of your face with the image of Brad Pitt, for example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们解释NFs是如何工作的。你会发现NFs是概率模型，你可以使用与上一章中相同的MaxLike方法来拟合。你还将能够使用拟合的分布来生成不存在的人的逼真面孔，或者将你的面部图像与布拉德·皮特的图像进行变形，例如。
- en: NFs are especially useful in high-dimensional spaces. Because it’s hard to imagine
    a space with more than three dimensions, we explain NFs in low dimensions. But
    don’t worry, we get to the high-dimensional distribution of facial images at the
    end of this section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: NFs在高维空间中特别有用。因为很难想象一个超过三个维度的空间，所以我们用低维来解释NFs。但不用担心，我们会在本节末尾讨论高维面部图像的分布。
- en: What are NFs and what are they good for? The basic idea is that an NF can fit
    a complex distribution (like the one in figure 6.6) without picking in advance
    an appropriate distribution family or setting up a mixture of several distributions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: NFs是什么？它们有什么好处？基本思想是，NF可以在事先没有选择合适的分布族或设置多个分布的混合的情况下拟合一个复杂的分布（如图6.6所示）。
- en: '![](../Images/6-6.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-6.png)'
- en: Figure 6.6 Sketch of a parametric probability density estimation. Each point
    (x1, x2) is assigned a probability density. We chose the parameter θ to match
    the data points (Dots).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6参数概率密度估计的草图。每个点(x1, x2)被分配一个概率密度。我们选择了参数θ来匹配数据点（点）。
- en: A probability density allows you to sample from that distribution. In the case
    of the facial image distribution, you can generate facial images from the distribution.
    The generated faces aren’t the ones from the training data (or to be more precise,
    the chance to draw a training sample from the learned distribution is small).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 概率密度允许你从这个分布中进行采样。在面部图像分布的情况下，你可以从这个分布中生成面部图像。生成的面孔不是来自训练数据（或者更准确地说，从学习到的分布中抽取训练样本的机会很小）。
- en: NFs, therefore, fall under the class of generative models. Other well-known
    generative models are generative adversarial networks(GANs) and variational autoencoders(VAEs).
    GANs can generate quite impressive results when it comes to creating images of
    faces that don’t exist. Visit [http://mng.bz/rrNB](http://mng.bz/rrNB) to see
    such a generated image. If you want to learn more, the book GANs in Action by
    Jakub Langr and Vladimir Bok (Manning, 2019) gives an accessible and comprehensive
    introduction to GANs (see [http://mng.bz/VgZP](http://mng.bz/VgZP)). But as you’ll
    see later, NFs can also produce real-looking images.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，NFs 属于生成模型类别。其他著名的生成模型包括生成对抗网络（GANs）和变分自编码器（VAEs）。GANs 在创建不存在的人脸图像方面可以生成相当令人印象深刻的结果。访问
    [http://mng.bz/rrNB](http://mng.bz/rrNB) 查看这样的生成图像。如果你想了解更多，Jakub Langr 和 Vladimir
    Bok 合著的《GANs in Action》（Manning, 2019）提供了对 GANs 的易于理解和全面的介绍（见 [http://mng.bz/VgZP](http://mng.bz/VgZP)）。但正如你稍后将会看到的，NFs
    也可以生成看起来很真实的图像。
- en: In contrast to GANs and VAEs, NFs are probabilistic models that really learn
    the probability distribution and allow for each sample to determine the corresponding
    probability (likelihoo*D*). Say you’ve used NF to learn the distribution of facial
    images, and you have an image *x*, then you can ask the NF via *p*(*x*) what’s
    the probability of that image? This has quite useful applications, like novelty
    detection.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GANs 和 VAEs 相比，NFs 是概率模型，它们真正学习了概率分布，并允许每个样本确定相应的概率（似然度 *D*）。假设你已经使用 NF 学习了面部图像的分布，并且你有一个图像
    *x*，然后你可以通过 *p*(*x*) 向 NF 询问该图像的概率是多少？这有相当有用的应用，比如新颖性检测。
- en: In novelty detection, you want to find out if a data point is from a certain
    distribution or if it’s an original (novel) data point. For example, you’ve recorded
    data from a machine (let’s say, a jet engine) under normal conditions. This can
    be quite high-dimensional data, like a vibrational spectra. You then train an
    NF for the “machine OK” distribution. While the machine is operating, you constantly
    check the probability of the data coming from the “machine OK” distribution. If
    this probability is low, you have an indication that the machine isn’t working
    correctly and something is wrong. But before we come to high-dimensional data,
    let’s start our journey into NFs with low-dimensional data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在新颖性检测中，你想要确定一个数据点是否来自某个分布，或者它是否是一个原始（新颖）的数据点。例如，你记录了机器（比如说，喷气发动机）在正常条件下的数据。这可以是非常高维的数据，比如振动光谱。然后你训练一个
    NF 来表示“机器正常”的分布。当机器运行时，你不断检查数据来自“机器正常”分布的概率。如果这个概率很低，这表明机器可能工作不正常，出了问题。但在我们进入高维数据之前，让我们从低维数据开始我们的
    NFs 之旅。
- en: 6.3.1 The principle idea of NFs
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 NFs 的基本原理
- en: In the left panel in figure 6.7, you see a one-dimensional data set. The data
    is a quite famous data set in statistics. It holds 272 waiting times between two
    eruptions of Old Faithful geyser in Yellowstone National Park. In the right panel
    of figure 6.7, you see a two-dimensional artificial data set. Imagine your statistics
    teacher asks you from which distribution does this data come. What would be your
    answer? Is it a Gaussian, a Weibull, a log normal? Even for the one-dimensional
    case on the left, none of the listed distributions fit. But because you’re a good
    reader, you remember section 6.1 and come up with a mixture of, for example, two
    Gaussians. That works for quite simple distributions, such as the one shown in
    the left panel of figure 6.7, but for really high dimensional and complex distributions,
    this approach breaks down.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 6.7 的左面板中，你看到一个一维数据集。这个数据是统计学中一个非常著名的数据集。它包含了黄石国家公园老忠实喷泉两次喷发之间的 272 个等待时间。在图
    6.7 的右面板中，你看到一个二维的人工数据集。想象一下，你的统计学老师问你这些数据来自哪个分布。你会怎么回答？是高斯分布、威布尔分布还是对数正态分布？即使在左边的单维情况下，上述任何分布都不适合。但因为你是一个优秀的读者，你记得第
    6.1 节，并提出了例如两个高斯分布的混合。这对于相当简单的分布，如图 6.7 左面板中所示，是有效的，但对于真正高维和复杂的分布，这种方法就失效了。
- en: '![](../Images/6-7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7](../Images/6-7.png)'
- en: 'Figure 6.7 Two data sets: a real one on the left in 1D (waiting times between
    two geyser eruptions) and an artificial one on the right. Do you know a probability
    distribution that produces this kind of data? We don’t.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 两个数据集：左边是 1D 中的一个真实数据集（两个间歇泉喷发之间的等待时间），右边是人工数据集。你知道产生这种数据的概率分布吗？我们不知道。
- en: What to do? Remember the old saying, “If the mountain won’t come to Mohammed,
    Mohammed must go to the mountain”? Figure 6.8 shows the main idea of an NF. Take
    data coming from a Gaussian and transform it so that at the end, the data looks
    like it’s coming from a complicated distribution. This is done by a transformation
    function *g*(*z*). On the other hand, the complicated function describing the
    data in *x* is transformed via the function *g*^(−1)(*x*) to *z* .
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该怎么办呢？记住那句老话，“如果山不来佛祖，佛祖就去见山”？图 6.8 展示了 NF 的主要思想。将来自高斯的数据进行变换，使得最终数据看起来像来自复杂的分布。这是通过变换函数
    *g*(*z*) 实现的。另一方面，描述 *x* 中数据的复杂函数通过函数 *g*^(−1)(*x*) 转换为 *z*。
- en: 'The main task of the NFs is to find these transformations: *g*(*z*) and *g*^(−1)(*x*).
    We assume for a moment that we’ve found such a function pair: *g* and *g*^(−1)
    . We want two things from it. First, it should enable us to sample from the complicated
    function *P**[x]*(*x*), allowing for applications to generate new, realistic-looking
    images of faces. Second, it should allow us to calculate the probability *P**[x]*(*x*)
    for a given *x*, allowing for applications like novelty detection.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: NF 的主要任务是找到这些变换：*g*(*z*) 和 *g*^(−1)(*x*)。我们暂时假设我们已经找到了这样的函数对：*g* 和 *g*^(−1)
    。我们希望从它得到两件事。首先，它应该使我们能够从复杂的函数 *P**[x]*(*x*) 中采样，允许应用于生成新的、看起来逼真的面部图像。其次，它应该允许我们计算给定
    *x* 的概率 *P**[x]*(*x*)，允许应用于新颖性检测等应用。
- en: '![](../Images/6-8.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8](../Images/6-8.png)'
- en: Figure 6.8 The NF principle. The complicated PDF *p**^x*(*x*) of the data *x*
    transformed to an easy Gaussian with the PDF *P**[z]*(*z*) = *N*(*z*; 0, 1). The
    transformation function *x* = *g*(*z*) transfers between the easy Gaussian in
    *z* and the complicated function in *x*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 NF 原理。数据 *x* 的复杂 PDF *p**^x*(*x*) 通过变换函数 *x* = *g*(*z*) 转换为容易的高斯分布，其 PDF
    为 *P**[z]*(*z*) = *N*(*z*; 0, 1)。
- en: Let’s start with the first task and find out how we can use g to do the sampling
    of a new example *x*. Remember, you can’t directly sample *x* from *P**[x]*(*x*)
    because you don’t know *P**[x]*(*x*). But for the simple distribution *P**[z]*(*z*),
    you know how to draw a sample. That’s easy! In case of a Gaussian as the simple
    distribution, you can do it with TFP using `z=fd.Normal(0,1).sample()` . Then
    you apply the transformation function g to get the corresponding sample *x* =
    *g*(*z*). So, the first task is solved.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个任务开始，看看我们如何使用 g 来对新示例 *x* 进行采样。记住，你不能直接从 *P**[x]*(*x*) 中采样 *x*，因为你不知道
    *P**[x]*(*x*)。但对于简单的分布 *P**[z]*(*z*)，你知道如何抽取样本。这很简单！如果简单分布是高斯分布，你可以使用 TFP 通过 `z=fd.Normal(0,1).sample()`
    来实现。然后你应用变换函数 g 来得到相应的样本 *x* = *g*(*z*)。所以，第一个任务就解决了。
- en: 'What about the second task? How probable is a certain sample *x* ? You can’t
    calculate *p**^x*(*x*) directly, but you can transform *x* back to *z* via *z*
    = *g*^(−1)(*x*) for which you know the probability *P**[z]*(*z*). With *P**[z]*(*z*),
    you can calculate the probability of *x*. In the case of a Gaussian as the simple
    distribution *P**[z]*(*z*), determining the probability of a number *z* is easy:
    use `tfd.Normal(0,1).prob(*z*)`.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 那第二个任务呢？某个样本 *x* 的概率有多大？你不能直接计算 *p**^x*(*x*)，但你可以通过 *z* = *g*^(−1)(*x*) 将 *x*
    转换回 *z*，对于这个 *z*，你知道概率 *P**[z]*(*z*)。有了 *P**[z]*(*z*)，你可以计算 *x* 的概率。在简单分布 *P**[z]*(*z*)
    是高斯分布的情况下，确定数字 *z* 的概率很容易：使用 `tfd.Normal(0,1).prob(*z*)`。
- en: Can we take any transformation function g? Here, that isn’t the case. To find
    out the required properties of the transformation, let’s consider that we go in
    a loop from *z* to *x* and back. Let’s take an example. What happens if we start
    with a fixed value of, say, *z* = 4? Then we’d use g to get the corresponding
    *x* value, *x* = *g*(4), and go back from *x* to *z* again with *z* = *g*^(−1)(*x*).
    You should again end up with the value *z* = *g*^(−1)(*x*) = *g*^(−1)(*g*(4))
    = 4 . This must apply for all values of z. That’s why we call *g*^(−1) the inverse
    of g.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否使用任何变换函数 g？在这里，情况并非如此。为了找出变换所需属性，让我们考虑从 *z* 到 *x* 再返回的循环。让我们举一个例子。如果我们从一个固定的值开始，比如说
    *z* = 4，那么我们会使用 g 来得到相应的 *x* 值，*x* = *g*(4)，然后再次从 *x* 返回到 *z*，*z* = *g*^(−1)(*x*)。你应该再次得到
    *z* = *g*^(−1)(*x*) = *g*^(−1)(*g*(4)) = 4 的值。这必须适用于所有 z 的值。这就是为什么我们称 *g*^(−1)
    为 g 的逆。
- en: It’s not possible for all functions g to find an inverse function *g*^(−1) .
    If a function g has an inverse function *g*^(−1) , then g is called bijective.
    Some functions such as *g*(*z*) = *z*² are only bijective on a limited range of
    data; here, for example, for positive values. (Can you tell what’s the inverse
    function?) g has to be bijective. Additionally, we want the flows implemented
    efficiently. In the next sections, you learn about the mathematical details and
    their implications for an efficient implementation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是所有函数 g 都能找到一个逆函数 *g*^(−1) 。如果一个函数 g 有逆函数 *g*^(−1) ，那么 g 被称为双射。一些函数，例如 *g*(*z*)
    = *z*²，仅在有限的数据范围内是双射；例如，在这里，对于正值。 (你能告诉我逆函数是什么吗?) g 必须是双射。此外，我们希望实现高效的流程。在接下来的章节中，你将了解数学细节及其对高效实现的影响。
- en: 6.3.2 The change of variable technique for probabilities
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 概率的变量变换技术
- en: In this section, you first learn how to use the NF method in one dimension.
    This is what statisticians call the change of variable technique, which is used
    to properly transform distributions. It’s the core method of all NFs, where (usually)
    several such transformation layers are stacked to a deep NF model. To explain
    what’s going on in a single layer of an NF model, we start with the transformation
    of a one-dimensional distribution. Later, in section 6.3.5, we’ll generalize the
    findings from this one-dimensional problem to higher dimensions. To code such
    NF models, we use TFP and especially the TFP `bijector` package (for an example,
    see listing 6.3). All TFP `bijector` classes are about bijective transformations,
    which apply the change of variables technique to correctly transform probability
    distributions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你首先学习如何在一维中使用 NF 方法。这是统计学家所说的变量变换技术，用于正确地变换分布。这是所有 NF 的核心方法，其中（通常）将多个这样的变换层堆叠成深度
    NF 模型。为了解释 NF 模型单层中的情况，我们首先从一维分布的变换开始。稍后，在第 6.3.5 节中，我们将从一维问题推广到高维。为了编写这样的 NF
    模型，我们使用 TFP 和特别地使用 TFP 的 `bijector` 包（例如，见列表 6.3）。所有 TFP `bijector` 类都是关于双射变换的，它们将变量变换技术应用于正确变换概率分布。
- en: Let’s start simple. Consider the transformation *x* = *g*(*z*) = *z*² and choose
    *z* to be uniformly distributed between 0 and 2\. (We refer to this as the simple
    example in this section.) The function *g*^(−1)(*x*) = √ *x* satisfies *g*^(−1)(*g*(*x*))
    = √ *z*²) = *z* for the picked range of z. By the way, that wouldn’t be possible
    if *z* was chosen uniformly from -1 to 1 (a positive range is require*D*). But
    now, if we work with the uniformly distributed *z* between 0 and 2, how does the
    distribution of *x* = *g*(*z*) = *z*² look? See if you can guess first.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单开始。考虑变换 *x* = *g*(*z*) = *z*² 并选择 *z* 在 0 和 2 之间均匀分布。（我们称这个例子为这一节中的简单例子。）函数
    *g*^(−1)(*x*) = √*x* 满足 *g*^(−1)(*g*(*x*)) = √(*z*²) = *z* 对于选择的 z 范围。顺便说一句，如果
    *z* 从 -1 到 1（一个正范围）均匀选择，那就不会是可能的（需要正范围）。但现在，如果我们使用在 0 和 2 之间均匀分布的 *z*，那么 *x* =
    *g*(*z*) = *z*² 的分布看起来如何？看看你是否能猜出来。
- en: A way of checking something in statistics is to always try it out with a simulation.
    To simulate 100,000 data points from a uniform distribution, you might use `tdf.Uniform
    (0,2).sample(1000)` . Take these values, square them, and plot a histogram (`plt.hist`).
    The solution is given in the following notebook. But try it first to see if you
    can do it on your own.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中检查某事的一种方法是在模拟中始终尝试它。为了从均匀分布中模拟 100,000 个数据点，你可能使用 `tdf.Uniform (0,2).sample(1000)`
    。取这些值，将它们平方，并绘制直方图 (`plt.hist`)。解决方案在下面的笔记本中给出。但先试试看你是否能自己完成。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/xWVW](http://mng.bz/xWVW)
    . This notebook contains companion code for the change of variables/`TFP.bijectors`
    exercise in this chapter. Follow it while reading this section. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ![电脑图标](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/xWVW](http://mng.bz/xWVW)
    。这个笔记本包含本章变量变换/`TFP.bijectors` 练习的配套代码。在阅读本节时跟随它。'
- en: Probably this result is a bit contrary to your first intuition. Let’s check
    what happens when you apply the square transformation to uniformly distributed
    samples. In figure 6.9, you see a plot of the square transformation function (the
    solid thick curve) and, on the horizontal axis, 100 samples (Depicted by the ticks)
    drawn from a uniform distribution between 0 and 2\. The corresponding histogram
    is shown above the plot. To square each sample (tick), you can go from the tick
    vertically up to the square function and where you hit it, then go horizontally
    left. The transformed values are the ticks drawn on the vertical axis. If you
    do that with all the *z* samples, you get a distribution of the ticks on the vertical
    axis. Note that the ticks are denser in the region around 0 than those that are
    at around 4\. The corresponding histogram is shown on the right. With this procedure,
    it becomes clear that a transformation function can squeeze samples together in
    regions where the transformation function is flat and move samples apart in regions
    where it’s steep.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可能这个结果与你的第一直觉有点相反。让我们检查一下当将平方变换应用于均匀分布样本时会发生什么。在图 6.9 中，你可以看到一个平方变换函数（实线粗曲线）的图表，以及在水平轴上的
    100 个样本（由刻度表示），这些样本是从 0 到 2 之间的均匀分布中抽取的。相应的直方图显示在图表上方。要平方每个样本（刻度），你可以从刻度垂直向上到平方函数，然后击中它，然后水平向左移动。变换后的值是垂直轴上的刻度。如果你对所有的
    *z* 样本都这样做，你将得到垂直轴上刻度的分布。请注意，刻度在 0 附近的区域比在 4 附近的区域密集。相应的直方图显示在右侧。通过这个程序，可以清楚地看出，变换函数可以在变换函数平坦的区域将样本挤压在一起，并在变换函数陡峭的区域将样本分开。
- en: '![](../Images/6-9.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-9.png)'
- en: Figure 6.9 A square transformation function (solid thick curve) applied to 100
    *z* samples drawn from a uniform distribution (ticks on the horizontal axis) yields
    transformed 100 *x* samples (ticks on the vertical axis). The histograms above
    and on the right of the plot show the distribution of the *z* and *x* samples,
    respectively.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 将一个平方变换函数（实线粗曲线）应用于从均匀分布（水平轴上的刻度）中抽取的 100 个 *z* 样本，得到变换后的 100 个 *x* 样本（垂直轴上的刻度）。图上方的直方图和右侧的直方图分别显示了
    *z* 和 *x* 样本的分布。
- en: This intuition also implies that linear functions (with constant steepness but
    different offsets) don’t change the shape of the distribution, only the values.
    Therefore, you need a non-linear transformation function if you want to go from
    a simple distribution to a distribution with a more complex shape.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这种直觉还意味着线性函数（具有恒定的斜率但不同的偏移量）不会改变分布的形状，只会改变值。因此，如果你想从一个简单的分布转换到一个具有更复杂形状的分布，你需要一个非线性变换函数。
- en: Another important property of the transformation function g is that it needs
    to be monotone in order to be bijective.[2](#pgfId-1080089) This implies that
    the samples stay in the same order (no overtaking). For a transformation function
    that’s monotone, increasing from *z*[1] < *z*[2] always follows *x*[1] < *x*[2](see
    figure 6.9 for an example of a monotone increasing transformation). If the transformation
    function is monotone decreasing, then *z*[1] < *z*[2] always implies *x*[1] <
    *x*[2] . This property also indicates that you always have the same number of
    samples between *x*[1] and *x*[2] as between *z*[1] = *g*(*x*[1]) and *z*[2] =
    *g*(*x*[2]) .
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 变换函数 g 的另一个重要特性是它需要是单调的，以便是双射的。[2](#pgfId-1080089) 这意味着样本保持相同的顺序（没有超越）。对于单调递增的变换函数，从
    *z*[1] < *z*[2] 总是跟随 *x*[1] < *x*[2](参见图 6.9 中单调递增变换的示例)。如果变换函数是单调递减的，那么 *z*[1]
    < *z*[2] 总是意味着 *x*[1] < *x*[2] 。这一特性还表明，在 *x*[1] 和 *x*[2] 之间，你总是有与在 *z*[1] = *g*(*x*[1])
    和 *z*[2] = *g*(*x*[2]) 之间相同的样本数量。
- en: For the NF, we need a formula to describe the transformation. Now that we’ve
    built our intuitive model of the transformation, let’s do the final step needed
    and go from samples and histograms to probability densities. Instead of the number
    of samples in a certain interval, now the probability in a certain interval is
    preserved (see figure 6.10). Strictly speaking (and we are quite sloppy in this
    book), *P**[z]*(*z*) is a probability density. All probability densities are normalized,
    meaning the area under the density is 1\. When using a transformation to go from
    one distribution to another distribution, this normalization is preserved; hence,
    the name “normalizing flow.” You don’t lose probability; it’s like a conservation
    of mass principle. And this preserving property not only holds for the whole area
    under the density curve, but also for smaller intervals.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NF，我们需要一个公式来描述变换。现在我们已经建立了变换的直观模型，让我们完成最后一步，从样本和直方图到概率密度的转换。现在，我们保留的是某个区间的概率，而不是某个区间的样本数量（见图6.10）。严格来说（在这本书中我们相当粗心），*P*[z]*(*z*)是一个概率密度。所有概率密度都是归一化的，这意味着密度下的面积是1。当使用变换从一个分布转换到另一个分布时，这种归一化是保留的；因此，称为“归一化流”。你不会丢失概率；这就像质量守恒原理。这种保留属性不仅适用于密度曲线下的整个区域，也适用于更小的区间。
- en: '![](../Images/6-10.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-10.png)'
- en: Figure 6.10 Understanding transformations. The area *P**[z]*(*z*)|dz| = *p**^x*(*x*)|dx|
    (shaded in the figure) needs to be preserved. An animated version is available
    at [https://youtu.be/fJ8YL2MaFHw](https://youtu.be/fJ8YL2MaFHw) . Note that strictly
    speaking, *dz* and *dx* should be infinitesimally small.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 理解变换。需要保留的面积 *P*[z]*(*z*)|dz| = *p*[x]*(*x*)|dx|（图中阴影部分）。
- en: 'To come from a probability density value *p**[x]*(*x*) to a real probability
    for values close to *x*, we have to look at an area under the density curve *p**[x]*(*x*)
    within a small interval with the length *dx* . We get such a probability by multiplying
    *p**[x]*(*x*) with *dx* : *p**[x]*(*x*)*dx* . The same is true for z, where *p**[z]*(*z*)*dz*
    is a probability. These two probabilities need to be the same. In figure 6.10,
    you see the transformation. The shaded areas under the curve need to be the same.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '要从概率密度值 *p*[x]*(*x*) 到接近 *x* 的值的真实概率，我们必须查看密度曲线 *p*[x]*(*x*) 在长度为 *dx* 的小区间下的面积。我们通过将
    *p*[x]*(*x*) 与 *dx* 相乘得到这样一个概率：*p*[x]*(*x*)*dx*。对于 *z* 也是如此，其中 *p*[z]*(*z*)*dz*
    是一个概率。这两个概率需要相同。在图6.10中，你可以看到变换。曲线下的阴影区域需要相同。 '
- en: From this we get the equation:[3](#pgfId-1080099)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方程我们得到：[3](#pgfId-1080099)
- en: '*p**[z]*(*z*) ⋅ |*dz*| = *p**[x]*(*x*) ⋅ |*dx*|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[z]*(*z*) ⋅ |*dz*| = *p*[x]*(*x*) ⋅ |*dx*|'
- en: 'This equation ensures that no probability is lost during the transformation
    (the mass is conserve*D*). We can solve the equation to:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程确保在变换过程中没有概率丢失（质量是守恒的）。我们可以解这个方程得到：
- en: '*p**[x]*(*x*) = *p**[z]*(*z*) ⋅ |*dz* /*dx*|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[x]*(*x*) = *p*[z]*(*z*) ⋅ |*dz* / *dx*|'
- en: '*p**[x]*(*x*) = *p**[z]*(*z*) ⋅ |*dx* /*dz*|^(−1)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[x]*(*x*) = *p*[z]*(*z*) ⋅ |*dx* / *dz*|^(−1)'
- en: Here we swapped the numerator dz and the denominator dx. It’s OK and backed
    by stricter math.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们交换了分子*dz*和分母*dx*。这是可以的，并且有更严格的数学支持。
- en: '*p**[x]*(*x*) = *p**[z]*(*z*) ⋅ |*dg*(*z*)/*dz*|^(−1) where *x* = *g*(*z*)
    .'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[x]*(*x*) = *p*[z]*(*z*) ⋅ |*dg*(*z*) / *dz*|^(−1) 其中 *x* = *g*(*z*) .'
- en: '*p**[x]*(*x*) = *p**[z]*(*z*) ⋅ |*g''*(*z*)|^(−1)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[x]*(*x*) = *p*[z]*(*z*) ⋅ |*g''*(*z*)|^(−1)'
- en: '*p**[x]*(*x*) = *p**[z]*(*g*^(−1)(*x*)) ⋅ |*g''*(*g*^(−1)(*x*))|^(−1) where
    *z* = *g*^(−1)(*x*)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*[x]*(*x*) = *p*[z]*(*g*^(−1)(*x*)) ⋅ |*g''*(*g*^(−1)(*x*))|^(−1) 其中 *z*
    = *g*^(−1)(*x*)'
- en: 'Equation 6.2 is quite famous and has its own name: it’s called the change of
    variable formula. The change of variable formula determines the probability density
    *p**[x]* of a transformed variable *x* = *g*(*z*). You need to determine the derivative
    *dg*(*z*)/*dz* and the inverse transformation function, and then you can use equation
    6.2 to determine *p**^x*(*x*). The term |*dz* /*dx*| describes the change of a
    length (the length of the interval on the horizontal axis in figure 6.10) when
    going from *z* to *x*. This ensures that the shaded area in figure 6.10 stays
    constant. We need the absolute value to cover cases where the transformation function
    is decreasing. In this case, |*dz* /*dx*| would be negative. When going from *x*
    to z, the length scales the opposite way:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6.2 非常著名，并且有自己的名字：它被称为变量变换公式。变量变换公式确定了变换后的变量 *x* = *g*(*z*) 的概率密度 *p**[x]*。你需要确定导数
    *dg*(*z*)/*dz* 和逆变换函数，然后你可以使用方程 6.2 来确定 *p**^x*(*x*)。项 |*dz* /*dx*| 描述了从 *z* 到
    *x* 时长度的变化（如图 6.10 中水平轴上区间的长度）。这确保了图 6.10 中的阴影区域保持不变。我们需要绝对值来覆盖变换函数递减的情况。在这种情况下，|*dz*
    /*dx*| 将是负数。当从 *x* 到 *z* 时，长度按相反的方式缩放：
- en: '|*dz*/*dx*| = 1 / |*dx*/*dz*|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|*dz*/*dx*| = 1 / |*dx*/*dz*|'
- en: Let’s take a moment to recap what you’ve learned so far. If we have an invertible
    transformation *g*(*z*) going from *z* to *x*, and the inverse function *g*^(−1)(*x*)
    going from *x* to z, equation 6.2 tells us how the probability distribution changes
    under the transformation. Knowing the transformation *g*(*z*) along with its derivative
    g'(*z*) and *g*^(−1)(*x*), we can apply the NF. We’ll tackle the question of how
    to learn these flows, g and g -1, in the next section. But first, let’s apply
    the formula to the initial example and see how this can be done quite elegantly
    with TFP’s `Bijector` class.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间回顾一下你到目前为止学到的内容。如果我们有一个从 *z* 到 *x* 的可逆变换 *g*(*z*)，以及从 *x* 到 *z* 的逆函数
    *g*^(−1)(*x*)，方程 6.2 告诉我们在变换下概率分布如何变化。知道变换 *g*(*z*) 以及其导数 g'(*z*) 和 *g*^(−1)(*x*)，我们可以应用
    NF。我们将在下一节中解决如何学习这些流，g 和 g -1 的问题。但首先，让我们将公式应用于初始示例，看看如何使用 TFP 的 `Bijector` 类非常优雅地完成这项工作。
- en: In the initial example, we assumed that *z* is uniformly distributed between
    0 and 2 in this interval, *p**[z]*(*z*) = 1/2 , so that the distribution is normalized.
    Let’s do the math for this example where *x* = *g*(*z*) = *z*² . With *z* = *g*^(−1)(*x*)
    = √*x* and *g'*(*g*^(−1)(*x*)) = 2 ⋅ *g*^(−1)(*x*) , equation 6.2 becomes
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始示例中，我们假设 *z* 在这个区间内均匀分布在 0 和 2 之间，*p**[z]*(*z*) = 1/2，因此分布是归一化的。让我们来计算这个例子中的数学问题，其中
    *x* = *g*(*z*) = *z*²。当 *z* = *g*^(−1)(*x*) = √*x* 和 *g'*(*g*^(−1)(*x*)) = 2 ⋅
    *g*^(−1)(*x*) 时，方程 6.2 变为
- en: '*p**[x]*(*x*) = *p**[z]*(*g*^(−1)(*x*)) ⋅ |*g''*(*g*^(−1)(*x*))|^(−1)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*p**[x]*(*x*) = *p**[z]*(*g*^(−1)(*x*)) ⋅ |*g''*(*g*^(−1)(*x*))|^(−1)'
- en: '*p**[x]*(*x*) = 1/2 ⋅ |2 ⋅ √*x*|^(−1) = 1/4 ⋅ √*x*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*p**[x]*(*x*) = 1/2 ⋅ |2 ⋅ √*x*|^(−1) = 1/4 ⋅ √*x*'
- en: This looks the same as the simulation (see figure 6.11).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来与模拟相同（参见图 6.11）。
- en: '![](../Images/6-11.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-11.png)'
- en: Figure 6.11 Comparing the densities of *x* = z2 resulting from the simulation
    (histogram) and analytical derivation using equation 6.2 (curved solid line) when
    assuming a uniformly distributed z
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 比较了模拟得到的 *x* = z² 的密度（直方图）和假设 z 均匀分布时使用方程 6.2 的解析推导（曲线实线）
- en: It turns out that TFP has good support for transforming variables. At the heart
    of variable transformation is a bijective transformation function g. The package
    `tfp.bijector` is all about bijectors, which we introduced earlier in this section.
    Let’s have a first look at a bijector in TFP (see the following listing and also
    the accompanying notebook [http://mng.bz/xWVW](http://mng.bz/xWVW)).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，TFP 对变量变换有很好的支持。变量变换的核心是一个双射变换函数 g。`tfp.bijector` 包含所有关于双射的内容，我们之前在本节中介绍了它。让我们先看看
    TFP 中的双射（参见下面的列表和相关的笔记本 [http://mng.bz/xWVW](http://mng.bz/xWVW)）。
- en: Listing 6.3 A first bijector
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 一个初等双射
- en: '[PRE2]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ This is a simple bijector, going from *z* ® z**2.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是一个简单的双射，从 *z* 变换到 z²。
- en: ❷ Yields 4
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 产生 4
- en: ❸ Yields 2
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 产生 2
- en: In the listing, a bijector `g` transforms one distribution to another. The first
    (usually the simple) distribution is called the base distribution or the source
    distribution on which the bijector `g` is applied. The resulting distribution
    is called the transformed distribution or the target distribution. The next listing
    shows how our simple example can be implemented in TFP.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表中，一个双射函数`g`将一个分布转换成另一个。第一个（通常是简单的）分布被称为基础分布或源分布，在这个源分布上应用双射函数`g`。得到的分布被称为转换分布或目标分布。接下来的列表展示了我们的简单示例如何在TFP中实现。
- en: Listing 6.4 The simple example in TFP
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.4 TFP中的简单示例
- en: '[PRE3]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The bijector; here a square function
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 双射函数；这里是一个平方函数
- en: ❷ The base distribution; here a uniform distribution
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 基础分布；这里是一个均匀分布
- en: ❸ Combining a base distribution and a bijector into a new distribution
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将基础分布和双射函数组合成一个新的分布
- en: ❹ TransformedDistribution behaves like a usual distribution.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ TransformedDistribution表现得像通常的分布。
- en: Note that we didn’t need to implement the change of variable formula ourselves.
    TFP is doing the work for us! If we’d like to create our own bijector, we’d need
    to implement the change of variable formula.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不需要自己实现变量变换公式。TFP为我们完成了这项工作！如果我们想创建自己的双射函数，我们需要实现变量变换公式。
- en: 6.3.3 Fitting an NF to data
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 将NF拟合到数据
- en: In this section, you learn the first step to use an NF for modeling complicated
    distributions and that this is quite easy using TFP bijectors. We restrict ourselves
    first to one-dimensional distributions. We do this by using only one flow, g.
    In the following section, we go deeper and chain several of those flows to allow
    for more flexibility to model complex distributions. Then in section 6.3.5, we’ll
    use flows for higher dimensional distributions. In this section, you’ll learn
    these flows, which are given by a parametric bijective function g.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习使用NF建模复杂分布的第一步，并且使用TFP双射函数这相当简单。我们首先限制自己使用一维分布。我们通过只使用一个流`g`来实现这一点。在下一节中，我们将深入探讨并链式连接几个这样的流，以允许更灵活地建模复杂分布。然后在第6.3.5节中，我们将使用流来处理高维分布。在本节中，您将学习这些流，这些流由一个参数化的双射函数`g`给出。
- en: Spoiler alert You determine the parameters of a flow via the good old MaxLike
    principle.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 揭秘警告：您通过古老的MaxLike原则确定流的参数。
- en: How do we model a distribution via NFs? If your data *x* has the complicated
    unknown distribution *p**[x]*(*x*) , then use the bijective transformation function
    g to get *x* by transforming a variable *z* with a simple base distribution *x*
    = *g*(*z*). If you know what transformation g to use, you’re fine. You could apply
    it with the methods you just learned in section 6.3.2\. The likelihood of *p**^x*(*x*)
    for each sample *x* is then given by the likelihood of the transformed value *p**[x]*(*x**[i]*)
    = *p**[z]*(*g*^(−1)(*x**[i]*)) ⋅ |*g*'(*g*^(−1)(*x**[i]*))|^(−1) . But how do
    you know which bijective transformation g to use?
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何通过NF建模分布？如果您的数据*x*具有复杂的未知分布*p**[x]*(*x*)，那么使用双射变换函数`g`通过将变量*z*通过一个简单的基础分布*x*
    = *g*(*z*)转换来得到*x*。如果您知道要使用什么变换`g`，那么您就没事了。您可以使用您在第6.3.2节中学到的方 法来应用它。对于每个样本*x*，*p**^x*(*x*)的似然由转换值*p**[x]*(*x**[i]*)
    = *p**[z]*(*g*^(−1)(*x**[i]*)) ⋅ |*g*'(*g*^(−1)(*x**[i]*))|^(−1)给出。但您如何知道要使用哪个双射变换`g`呢？
- en: 'Solution number one: ask old-fashioned statisticians. They’ll fire up EMACS
    and, in a first step, fit a simple model like a Gaussian to the data. Of course,
    a Gaussian isn’t enough to fit a complicated distribution. The experienced statistician
    then stares at the difference between the model and the data and does some other
    magic only they and their priesthood understand in full detail. Finally, they
    mumble something like, “Apply a log transformation on your data, kid; then you
    can fit your data with a Gaussian.” So off you go and implement a flow with `tfb.Exp()`
    . Meditate a second on why to use the exponential before you read on.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案之一：询问老式的统计学家。他们会启动EMACS，并在第一步中，将一个简单的模型（如高斯分布）拟合到数据上。当然，高斯分布不足以拟合复杂的分布。经验丰富的统计学家然后会盯着模型与数据之间的差异，做一些只有他们和他们的祭司阶层完全理解的魔法。最后，他们会咕哝着说：“孩子，在你的数据上应用对数变换；然后你可以用高斯分布拟合你的数据。”于是你就可以去实现一个使用`tfb.Exp()`的流了。在继续阅读之前，请思考一下为什么要在使用指数之前进行冥想。
- en: The answer is the statistician gave you the transformation on how to go from
    the complicated distribution to the simple Gauss distribution, *z* = *ɡ*^(−1)(*x*)
    = log(*x*). Therefore, the flow g that goes from the simple to the complicated
    distribution is given by the inverse of the logarithm, which is the exponential
    *x* = *ɡ*(*z*) = exp(*z*).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是统计学家给了你如何从复杂分布到简单高斯分布的变换，*z* = *ɡ*^(−1)(*x*) = log(*x*)。因此，从简单分布到复杂分布的流程
    g 由对数的逆，即指数 *x* = *ɡ*(*z*) = exp(*z*) 给出。
- en: 'Solution number two: you realize that we live in the 21st century and have
    the computer power to find in a data-driven way the bijective transformation *ɡ*
    that transforms a variable *z* with a simple base distribution *p**[z]*(*z*) of
    your choice to the variable *x* = *ɡ*(*z*) of interest. Knowing the flow, g allows
    you to determine the complicated distribution *p**[x]*(*x*) = *p**[z]*(*z*) ⋅
    |*ɡ* ''(*z*)| ^(−1) = *p**[z]*(*ɡ*^(−1)(*x*)) ⋅ |*ɡ* ''(*ɡ*^(−1)(*x*))| ^(−1)(see
    equation 6.2).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种解决方案：你意识到我们生活在 21 世纪，拥有计算机能力，可以通过数据驱动的方式找到双射变换 *ɡ*，它将一个变量 *z* 转换为你选择的简单基分布
    *p**[z]*(*z*)，到感兴趣的变量 *x* = *ɡ*(*z*)。知道了流程，g 允许你确定复杂的分布 *p**[x]*(*x*) = *p**[z]*(*z*)
    ⋅ |*ɡ* '(*z*)| ^(−1) = *p**[z]*(*ɡ*^(−1)(*x*)) ⋅ |*ɡ* '(*ɡ*^(−1)(*x*))| ^(−1)（见方程
    6.2）。
- en: The key idea for the data-driven approach is that you set up a flexible bijective
    transformation function g, which has learnable parameters θ. How to determine
    the values of these parameters? The usual way--with the MaxLike approach. You
    have training data *x**[i]* , and you can calculate the likelihood of a single
    training sample i by calculating *p**[x]*(*x**[i]* ) = *p**[z]*(*g*^(−1)(*x**[i]*
    )) ⋅ |*g*'(*g*^(−1)(*x**[i]* ))|^(−1) and the joint likelihood of all data points
    by multiplying all individual likelihood contributions ![](../Images/6-11_E01.png) .
    In practice, you minimize the  ![](../Images/6-11_E02.png) in your training data.
    That’s it!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动方法的关键思想是，你设置一个灵活的双射变换函数 g，它具有可学习的参数 θ。如何确定这些参数的值？通常的方式——使用最大似然法。你有训练数据 *x**[i]*，你可以通过计算单个训练样本
    i 的似然度 *p**[x]*(*x**[i]* ) = *p**[z]*(*g*^(−1)(*x**[i]* )) ⋅ |*g*'(*g*^(−1)(*x**[i]*
    ))|^(−1) 来计算，以及通过乘以所有单个似然贡献来计算所有数据点的联合似然度！[](../Images/6-11_E01.png)。在实践中，你在训练数据中最小化！[](../Images/6-11_E02.png)。就是这样！
- en: 'Let’s start with an extremely easy example. Our first learnable flow is linear
    and involves only two parameters: *a* and *b* *g*(*x*) = *a* ⋅ *z* + *b* . In
    listing 6.5, you can see that we use an affine bijector. Just to get the term
    straight, an affine function *g*(*x*) = *a* ⋅ *z* + *b* is the linear function
    *g*(*x*) = *a* ⋅ *z* plus an offset b. In this book, we’re a bit relaxed; we’ll
    often say “linear” when we mean “affine.” Of course, with such an easy flow, you
    can’t do too much fancy stuff.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常简单的例子开始。我们的第一个可学习流程是线性的，只涉及两个参数：*a* 和 *b* *g*(*x*) = *a* ⋅ *z* + *b*。在列表
    6.5 中，你可以看到我们使用了一个仿射双射。只是为了明确这个术语，一个仿射函数 *g*(*x*) = *a* ⋅ *z* + *b* 是线性函数 *g*(*x*)
    = *a* ⋅ *z* 加上一个偏移量 b。在这本书中，我们有点放松；当我们说“线性”时，我们通常是指“仿射”。当然，对于如此简单的流程，你无法做太多复杂的事情。
- en: In the discussion of figure 6.9, we already pointed out that the shape of the
    distribution stays the same when using a linear transformation function. Now we
    want to learn the transformation from *z* ∼ *N* (0, 1) to *x* ∼ *N* (5, 0.2) .
    Because both distributions are bell-shaped, an (affine) linear transformation
    can do the trick. The following listing shows the complete code, and it’s also
    in the accompanying notebook.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在 6.9 图的讨论中，我们已指出，当使用线性变换函数时，分布的形状保持不变。现在我们想要学习从 *z* ∼ *N* (0, 1) 到 *x* ∼ *N*
    (5, 0.2) 的变换。因为这两个分布都是钟形，一个（仿射）线性变换就能做到这一点。下面的列表显示了完整的代码，它也在配套的笔记本中。
- en: Listing 6.5 A simple example in TFP
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5 TFP 中的一个简单示例
- en: '[PRE4]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Defines the variables
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义变量
- en: ❷ Sets up the flow using an affine transformation defined by two variables
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用由两个变量定义的仿射变换来设置流程
- en: ❸ The NLL of the data
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据的 NLL
- en: ❹ Calculates the gradients for the trainable variables
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算可训练变量的梯度
- en: ❺ Applies the gradients to update the variables
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将梯度应用于更新变量
- en: Training for a few epochs results in *a ≈ 0.2* and *b ≈ 5* and so transforms
    the *N* (0, 1) distributed variable into an *N* (5, 0.2) distributed variable
    (see the notebook [http:// mng.bz/xWVW](http://mng.bz/xWVW) for the result). Of
    course, such a simple (affine) linear transformation is much too simple to transform
    a Gaussian into more complex distributions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 训练几个epoch后，结果为 *a ≈ 0.2* 和 *b ≈ 5*，并将 *N* (0, 1) 分布的变量转换为 *N* (5, 0.2) 分布的变量（参见笔记本
    [http:// mng.bz/xWVW](http://mng.bz/xWVW) 中的结果）。当然，这样的简单（仿射）线性变换对于将高斯转换为更复杂的分布来说太简单了。
- en: 6.3.4 Going deeper by chaining flows
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 通过链式流进行更深入的建模
- en: You saw in section 6.3.3 that a linear flow can only shift and stretch the base
    distribution, but it can’t change the shape of the distribution. Therefore, the
    result of linearly transforming a Gaussian is, again, a Gaussian(with changed
    parameters). In this section, you learn a way to model a target distribution that
    has a very different shape compared to the base distribution. This lets you model
    complex real-world distributions such as the waiting time between two eruptions
    of Old Faithful geyser. You’ll see that this is quite easy with TFP.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你在6.3.3节中看到，线性流只能改变基本分布的形状和拉伸，但它不能改变分布的形状。因此，线性变换高斯的结果，再次，是一个高斯（参数已改变）。在本节中，你将学习一种建模目标分布的方法，该分布与基本分布的形状非常不同。这使得你可以模拟复杂现实世界的分布，例如老忠实喷泉两次喷发之间的等待时间。你会发现这使用TFP相当简单。
- en: 'How do we create flows that can change the shape of a distribution? Remember
    the fool’s rule of DL--stack more layers (as discussed in section 2.1.2). Also
    remember that between the layers in an NN, you use a non-linear activation function;
    otherwise, a deep stack of layers could be replaced by one layer. With respect
    to an NF, this rule tells you not to use just one flow but a series of flows (the
    non-linearities in between are important, and we come back to this point later).
    You start from *z* and go along a chain of k transformations to x: *z* = *z*[o]
    → *z*[1] → *z*[2] ⋯→*z**[k]* = *x* . Figure 6.12 shows this transformation.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何创建可以改变分布形状的流？记住深度学习中的“傻瓜规则”——堆叠更多层（如第2.1.2节所述）。还要记住，在神经网络中的层之间，你使用非线性激活函数；否则，深层堆叠的层可以被一层替代。对于NF来说，这个规则告诉你不要只使用一个流，而是一系列流（层之间的非线性很重要，我们稍后会回到这一点）。你从
    *z* 开始，沿着k个变换的链到 x：*z* = *z*[o] → *z*[1] → *z*[2] ⋯→*z**[k]* = *x*。图6.12展示了这个变换。
- en: '![](../Images/6-12.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-12.png)'
- en: Figure 6.12 A chain of simple transformations makes it possible to create complex
    transformations needed to model complex distributions. From right to left, starting
    from a standard Gaussian distribution z0 ~ *N*(0,1) changes via successive transformations
    to a complex distribution with a bimodal shape (on the left).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 通过一系列简单的变换，可以创建出用于模拟复杂分布所需的复杂变换。从右到左，从标准高斯分布 z0 ~ *N*(0,1) 通过连续变换变为具有双峰形状的复杂分布（在左侧）。
- en: Let’s look at a chain of two transformations from *z*[o] → *z*[1] → *z*[2] to
    understand the general formula. You know the probability distribution *p**[z[0]]*(*z*[0]),
    but how can you determine the probability distribution *p**[z[2]]*(*z*[2])? Let’s
    do it step by step and, in each step, use the change of variable formula (equation
    6.2).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从 *z*[o] → *z*[1] → *z*[2] 的两个变换链，以理解一般公式。你知道概率分布 *p**[z[0]]*(*z*[0])，但你如何确定概率分布
    *p**[z[2]]*(*z*[2])？让我们一步一步来做，并在每一步中使用变量变换公式（方程6.2）。
- en: 'First determine the probability distribution *p**[z[1]]*(*z*[1]). You get *z*
    1 by transforming *z*[0] , *z*[1] = *g*[1](*z*[0]). To use the change of variable
    formula, you need to determine the derivative g 1'' and the inverted function
    *g* 1-1\. The distribution *p**[z[1]]*(*z*[1]). can then be determined by equation
    6.2 as *p**[z[1]]*(*z*[1]) = *p**[z[0]]*(*z*[0]) ⋅ | *ɡ*[1]''(*z*[0])|^(−1) .
    Proceeding like that, you can determine the probability density pz 2 of the transformed
    variable *p**[z[2]]*(*z*[2]) = *p**[z[1]]*(*z*[1]) ⋅ |*ɡ*[2] ''(*z*[1])|^(−1)
    , where you can plug in the former formula for *p**[z[1]]*(*z*[1]), yielding the
    chained flow:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 首先确定概率分布 *p**[z[1]]*(*z*[1])。通过变换 *z*[0] 得到 *z* 1，*z*[1] = *g*[1](*z*[0])。为了使用变量变换公式，你需要确定导数
    g 1' 和逆函数 *g* 1-1。然后，可以通过方程6.2确定分布 *p**[z[1]]*(*z*[1])，即 *p**[z[1]]*(*z*[1]) =
    *p**[z[0]]*(*z*[0]) ⋅ | *ɡ*[1]'(*z*[0])|^(−1) 。按照这种方式，你可以确定变换变量的概率密度 pz 2，即 *p**[z[2]]*(*z*[2])
    = *p**[z[1]]*(*z*[1]) ⋅ |*ɡ*[2] '(*z*[1])|^(−1) ，其中你可以将前面的公式代入 *p**[z[1]]*(*z*[1])，得到链式流：
- en: '*p**[z[2]]*(*z*[2]) = *p**[z[0]]*(*z*[0]) ⋅ |*ɡ*[1] ''(*z*[0])|^(−1) ⋅ |*ɡ*[2]
    '' (*z*[1])|^(−1)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*p**[z[2]]*(*z*[2]) = *p**[z[0]]*(*z*[0]) ⋅ |*ɡ*[1] ''(*z*[0])|^(−1) ⋅ |*ɡ*[2]
    '' (*z*[1])|^(−1)'
- en: 'Often, it’s more convenient to operate on log probabilities instead of probabilities.
    Taking the log (and using the log rule log(*a**^p*) = *p* ⋅ log(*a*) with *a*
    = *ɡ**[i]* '' (*z**[i]* −1) and *p* = −1) of the previous formula yields this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，操作对数概率比操作概率更方便。对前面的公式取对数（并使用对数规则 log(*a**^p*) = *p* ⋅ log(*a*)，其中 *a* = *ɡ**[i]*
    ' (*z**[i]* −1) 和 *p* = −1）得到以下结果：
- en: log( *p**[z[2]]*(*z*[2])) = log( *p**[z[0]]*(*z*[0])) − log(| *ɡ*[1] '( *z*[0])|)
    − log(| *ɡ*[2] '( *z*[1])|)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: log( *p**[z[2]]*(*z*[2])) = log( *p**[z[0]]*(*z*[0])) − log(| *ɡ*[1] '( *z*[0])|)
    − log(| *ɡ*[2] '( *z*[1])|)
- en: 'For a complete flow (with *x* = zk), this formula generalizes to:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于完整的流程（*x* = zk），此公式推广到：
- en: '![](../Images/6-12_E01.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-12_E01.png)'
- en: To calculate the probability *p**^x*(*x*), simply backup in the chain in figure
    6.12 from *x* = *z**[k]* → *z**[k]*[−1] ... → *z*[o] and sum the −log(|*ɡ**[i]*
    '(*z**[i]* [−1])| terms. Let’s build such a chain in TFP.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算概率 *p**^x*(*x*)，只需在图 6.12 的链中从 *x* = *z**[k]* → *z**[k]*[−1] ... → *z*[o]
    追溯，并求和 −log(|*ɡ**[i]* '(*z**[i]* [−1])| 项。让我们在 TFP 中构建这样的链。
- en: 'It’s quite convenient to create a chain of bijectors in TFP: simply use the
    class `Chain(bs)` from the `tfp.bijectors` package with a list of `Bijectors bs`
    . The result is again, a bijector. So, do we simply need to chain a few affine
    scalar bijectors or are we done? We’re not quite done yet. An affine scalar transformation
    only shifts and scales a distribution. One way to think of this is that such an
    affine transformation is a straight line in figure 6.9\. There isn’t a possibility
    to change the shape of the distribution.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TFP 中创建双射算子链非常方便：只需使用 `tfp.bijectors` 包中的 `Chain(bs)` 类，并传递一个 `Bijectors bs`
    列表。结果仍然是一个双射算子。所以，我们只需要简单地链式连接几个仿射标量双射算子，或者我们就完成了？我们还没有完全完成。仿射标量变换只会平移和缩放一个分布。一种思考方式是，这种仿射变换在图
    6.9 中是一条直线。没有改变分布形状的可能性。
- en: What do you need to do if you want to change the shape of the distribution?
    You could introduce some non-linear bijectors between the stacked linear flows,
    or you could use non-linear bijectors instead of the linear bijectors. Let’s go
    for the first option.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想改变分布的形状，你需要做什么？你可以在堆叠的线性流程之间引入一些非线性双射算子，或者你可以使用非线性双射算子代替线性双射算子。让我们选择第一种方法。
- en: 'You need to pick a non-linear parametric transformation function for which
    you can then find the parameter values via the MaxLike approach. There are many
    possible transformation functions (bijectors) to do so. Have a look at [http://mng.bz/AApz](http://mng.bz/AApz)
    . Many of the bijectors have either no parameters, like `softplus` , or limit
    the allowed range of *z* or *x*. The `SinhArcsinh` bijector has a complicated
    name but looks quite promising: it has two parameters, `skewness` and `tailweight`
    , and if `tailweight>0` , there are no restrictions on *x* and z. Figure 6.13
    shows that bijector for some parameters. For `tailweight=1` and `skewness=1` it
    looks quite non-linear, and with these parameters, we do not need to restrict
    the range of *x* and *y*. We, therefore, use it to fit the Old Faithful data (see
    listing 6.6). Note that there might be other bijectors in the TFP package that
    fulfill the requirements as well.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要选择一个非线性参数化变换函数，然后可以通过最大似然方法找到参数值。有许多可能的变换函数（双射算子）可以这样做。查看 [http://mng.bz/AApz](http://mng.bz/AApz)
    。许多双射算子要么没有参数，如 `softplus`，要么限制 *z* 或 *x* 的允许范围。`SinhArcsinh` 双射算子有一个复杂的名字，但看起来很有希望：它有两个参数，`skewness`
    和 `tailweight`，如果 `tailweight>0`，则对 *x* 和 *z* 没有约束。图 6.13 显示了某些参数下的该双射算子。对于 `tailweight=1`
    和 `skewness=1`，它看起来相当非线性，并且使用这些参数，我们不需要限制 *x* 和 *y* 的范围。因此，我们使用它来拟合 Old Faithful
    数据（见列表 6.6）。请注意，TFP 包中可能有其他满足要求的双射算子。
- en: '![](../Images/6-13.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-13.png)'
- en: Figure 6.13 The bijector `SinhArcsinh` for different parameter values
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 不同参数值下的双射算子 `SinhArcsinh`
- en: Let’s construct a chain and add `SinhArcsinh` bijectors between the `AffineScalar`
    bijectors. This is done in the following listing.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个链并添加 `AffineScalar` 双射算子之间的 `SinhArcsinh` 双射算子。这将在以下列表中完成。
- en: Listing 6.6 The simple Old Faithful bijector example in TFP
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.6 TFP 中的简单 Old Faithful 双射算子示例
- en: '[PRE5]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Number of layers
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 层数数量
- en: ❷ The AffineScalar transformation
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仿射标量变换
- en: ❸ The SinhArcsinh acting as non-linearity
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ SinhArcsinh 作为非线性
- en: ❹ Creates the chain of bijectors from the list of bijectors
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从双射算子列表创建双射算子链
- en: Visit the notebook [http://mng.bz/xWVW](http://mng.bz/xWVW) to see this chain
    of bijectors used for the Old Faithful geyser waiting times, which yields the
    histogram in figure 6.14\. By the way, in figure 6.12, you see some of the steps
    from *N*(0,1) to the distribution of the Old Faithful’s waiting times.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 访问笔记本[http://mng.bz/xWVW](http://mng.bz/xWVW)以查看用于Old Faithful喷泉等待时间的这一系列双射算子，它产生了图6.14中的直方图。顺便说一句，在图6.12中，你可以看到从*N*(0,1)到Old
    Faithful等待时间分布的一些步骤。
- en: '![](../Images/6-14.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-14.png)'
- en: Figure 6.14 The histogram of the Old Faithful geyser waiting times (filled bars),
    along with the fitted density distribution (solid line). The histogram doesn’t
    show the shape of a simple distribution like a Gaussian. A flow of five layers
    captures the characteristics of the data well (solid line).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 Old Faithful喷泉等待时间的直方图（填充柱状图），以及拟合的密度分布（实线）。直方图没有显示出像高斯这样的简单分布的形状。五层流动很好地捕捉了数据的特征（实线）。
- en: So far, we’ve considered one-dimensional data, but does this method also work
    for higher dimensional data? Imagine, for example, image data where each image
    has the dimension 256 × 256 × 3 (height × width × channels). Again, we’re interested
    in learning the distribution of this image data so that we can sample from it.
    It turns out that this flow method also works for higher dimensions. The only
    principle difference is that the bijectors are no longer one-dimensional functions
    but have as many dimensions as your data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们考虑了一维数据，但这种方法是否也适用于高维数据？想象一下，例如，图像数据，其中每个图像的维度为256 × 256 × 3（高度 × 宽度
    × 通道）。同样，我们感兴趣的是学习这种图像数据的分布，以便从中采样。结果证明，这种流动方法也适用于高维。唯一的原理区别是，双射算子不再是单维函数，而是具有与你的数据一样多的维度。
- en: In the next two sections, we extend the NF method to higher dimensions. If you’re
    more interested in the application of this method than in the mathematical twists,
    you can skip those sections and go directly to section 6.3.7\. But if you want
    to know the details, keep reading!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两节中，我们将NF方法扩展到高维。如果你对这种方法的应用比数学上的变化更感兴趣，你可以跳过这些章节，直接跳到6.3.7节。但如果你想知道细节，请继续阅读！
- en: 6.3.5 Transformation between higher dimensional spaces*
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5 高维空间之间的转换*
- en: 'Let’s formulate the task of modeling the distributions of high-dimensional
    data like images so that we can use the flow method to fit the distribution. First,
    we flatten the image data to receive (for each image) the vectors *x* with 196,608
    entries. The resulting vectors live in a 196,608 space and have an unknown distribution
    of *p**[x]*(*x*), which is probably complex. You can now pick a simple base distribution,
    *p**[z]*(*z*), for a variable z, such as a Gaussian, for example. The task is
    to find a transformation *g*(*z*) that transforms the vector *z* to the vector
    *x*. We need a bijective transformation g and, therefore, the dimensionality of
    *x* and *z* have to be the same. Let’s see how such a transformation looks for
    a three-dimensional space, which means we deal with the data points *x* = (*x*[1]
    , *x*[2] , *x*[3]) and *z* = ( *z*[1] , *z*[1] , *z*[3]). The transformation *x*
    = *g*(*z*) looks like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们制定一个任务，对高维数据的分布（如图像）进行建模，这样我们就可以使用流动方法来拟合分布。首先，我们将图像数据展平，以接收（对于每张图像）具有196,608个条目的向量*x*。这些向量生活在196,608维空间中，并且具有未知的分布*p**[x]*(*x*)，这可能是复杂的。现在，你可以选择一个简单的基分布*p**[z]*(*z*)，例如高斯分布。任务是找到一个变换*g*(*z*)，将向量*z*转换为向量*x*。我们需要一个双射变换g，因此*x*和*z*的维度必须相同。让我们看看这种变换在三维空间中的样子，这意味着我们处理的数据点是*x*
    = (*x*[1] , *x*[2] , *x*[3])和*z* = ( *z*[1] , *z*[1] , *z*[3])。变换*x* = *g*(*z*)看起来是这样的：
- en: '![](../Images/6-14_E01.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-14_E01.png)'
- en: For the one-dimensional flow, the main formula was
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一维流动，主要公式是
- en: '![](../Images/6-14_E02.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-14_E02.png)'
- en: and the term |*dg*(*z*) / *dz*| was identified as a change in length when going
    from *z* to *x*. In three dimensions, we need to take into account the change
    of a volume as well. For four and more dimensions, the change of the volume is
    now the change of a hypervolume of the transformation g. From now on we just call
    it a volume, regardless if we have a length or an area.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 并且术语|*dg*(*z*) / *dz*|被识别为从*z*到*x*时长度的变化。在三维中，我们需要考虑体积的变化。对于四维及以上，体积的变化现在是变换g的超体积的变化。从现在起，我们只称之为体积，无论我们是否有长度或面积。
- en: 'The scalar derivative |*dg*(*z*) / *dz*| in the one-dimensional formula (equation
    6.3) is replaced by a matrix of partial derivatives, which is called the Jacobi
    matrix. To understand the Jacobi matrix, we first recall what a partial derivative
    is. The partial derivative of a function *ɡ*(*z*[1] , *z*[2] , *z*[3]) of three
    variables *z*[1], *z*[2], *z*[3] with respect to (w.r.t.) *z*[2] is written as
    *∂* *ɡ*(*z*[1] , *z*[2] , *z*[3]) / *∂z*[2] . To make an easy example, what is
    *∂* *ɡ*(*z*[1] , *z*[2] , *z*[3]) / *∂z*[2] when *ɡ* ( *z*[1] , *z*[2] , *z*[3]
    ) = 42 ⋅ *z*[2] + sinh(exp(*z*[1] /*z*[3]))? Well you are lucky, it’s 42\. That
    was easy! The partial derivative w.r.t. z1 and z3 would have been much more complicated.
    Instead of *ɡ*(*z*[1] , *z*[2] , *z*[3]) just returning a single number, we consider
    the case in which g returns a vector, let’s say, of three components. If you need
    an example, this function g could be a fully connected network (fcNN) with three
    input and output neurons. For this example, the Jacobi matrix looks as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在一维公式（方程6.3）中，标量导数 |*dg*(*z*) / *dz*| 被替换为一个偏导数矩阵，这被称为雅可比矩阵。为了理解雅可比矩阵，我们首先回顾一下偏导数的定义。一个关于三个变量
    *z*[1], *z*[2], *z*[3] 的函数 *ɡ*(*z*[1] , *z*[2] , *z*[3]) 对 *z*[2] 的偏导数表示为 *∂*
    *ɡ*(*z*[1] , *z*[2] , *z*[3]) / *∂z*[2] 。为了举一个简单的例子，当 *ɡ* ( *z*[1] , *z*[2] ,
    *z*[3] ) = 42 ⋅ *z*[2] + sinh(exp(*z*[1] /*z*[3])) 时，*∂* *ɡ*(*z*[1] , *z*[2] ,
    *z*[3]) / *∂z*[2] 是多少呢？幸运的是，答案是42。这很简单！关于 z1 和 z3 的偏导数将会更复杂。如果函数 *ɡ*(*z*[1] ,
    *z*[2] , *z*[3]) 只返回一个数字，我们考虑 g 返回一个向量，比如说，有三个分量。如果你需要一个例子，这个函数 g 可以是一个具有三个输入和输出神经元的全连接网络（fcNN）。对于这个例子，雅可比矩阵如下所示：
- en: '![](../Images/6-14_E03.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-14_E03.png)'
- en: 'In the one-dimensional case, you had to determine the absolute value of the
    derivative |dg(*z*)/dz| in the change of variable formula (equation 6.3). In the
    higher dimensional case, it turns out that you have to replace this term by the
    absolute value of the determinant of the Jacobi matrix. The change for the variable
    formula for high-dimensional data looks like this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在一维情况下，你需要在变量变换公式（方程6.3）中确定导数 |dg(*z*)/dz| 的绝对值。在更高维的情况下，实际上你需要用雅可比矩阵行列式的绝对值来替换这个项。高维数据的变量变换公式看起来如下所示：
- en: '![](../Images/6-14_E04.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-14_E04.png)'
- en: You don’t know what’s a determinant or forgot about it? Don’t worry. The only
    thing you need to know is that for a triangular matrix (like the one shown in
    equation 6.5), you can compute the determinant as a product of the diagonal elements.
    This is, of course, also true if some (or all) of the off-diagonal elements in
    the lower part of the triangular matrix are also zero. Anyway, you don’t have
    to compute the determinant yourself.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你不知道什么是行列式或者忘记了它？不用担心。你需要知道的是，对于三角矩阵（如方程6.5中所示），你可以通过计算对角元素的乘积来计算行列式。当然，如果三角矩阵下方的某些（或所有）非对角元素也为零，这也是正确的。无论如何，你不必自己计算行列式。
- en: Each TFP bijector implements the method `log_det_jacobian(*z*)` , and the flow
    or a chain of flows can be calculated as described previously. The calculation
    of a determinant is quite time-consuming. There’s a nice trick, however, to speed
    up the calculation. If a matrix is a so-called triangular matrix, then the determinant
    is the product of the diagonal elements. How to get zeros in a Jacobi matrix?
    A partial derivative of a function gk w.r.t. a variable zi gets zero if the function
    gk doesn’t depend on variable zi. If we build the flows so that g 1(*z*1, z2,
    z3) is independent of z2, z3 and g2(*z*1, z2,z3) is independent of z3, then the
    previous matrix becomes
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 每个TFP双射实现方法 `log_det_jacobian(*z*)`，流或流的链可以按照之前描述的方法计算。计算行列式相当耗时。然而，有一个很好的技巧可以加快计算速度。如果一个矩阵是所谓的三角矩阵，那么行列式是对角元素的乘积。如何在雅可比矩阵中得到零？如果函数
    gk 不依赖于变量 zi，那么函数 gk 对 zi 的偏导数将得到零。如果我们构建的流使得 g 1(*z*1, z2, z3) 与 z2, z3 无关，g2(*z*1,
    z2,z3) 与 z3 无关，那么之前的矩阵如下所示：
- en: '![](../Images/6-14_E05.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-14_E05.png)'
- en: This matrix is a triangular matrix and, hence, you can obtain the determinant
    by the product of the diagonal elements. A nice property of a triangular Jacobi
    matrix is that you don’t have to calculate the off-diagonal terms (shown in gray
    in equation 6.5) to determine the determinant. These off-diagonal terms play a
    role in the first term, *p**[z]*(*z*) = *p**[z]*(*ɡ*^(−1)(*x*)) (equation 6.4),
    but not for the second term, |det(*∂g*(*z*) / *∂z*)|^(−1) . To model complex distributions,
    it might be necessary to use complex functions for these off-diagonal terms. Fortunately,
    it’s not a problem at all if these expressions are complicated because you don’t
    have to calculate the derivatives of them. To get such a nice triangular Jacobi
    matrix, one simply has to ensure that *ɡ**[i]*(*z*) is independent of *z**[j]*
    with j > i.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵是一个三角矩阵，因此可以通过对角元素的乘积来获得行列式。三角雅可比矩阵的一个好性质是，您不需要计算非对角项（在方程 6.5 中以灰色显示）来确定行列式。这些非对角项在第一个项
    *p**[z]*(*z*) = *p**[z]*(*ɡ*^(−1)(*x*))（方程 6.4）中起作用，但不是第二个项，|det(*∂g*(*z*) / *∂z*)|^(−1)
    。为了模拟复杂分布，可能需要使用复杂函数来模拟这些非对角项。幸运的是，如果这些表达式很复杂，这根本不是问题，因为您不需要计算它们的导数。要获得这样一个好的三角雅可比矩阵，只需确保
    *ɡ**[i]*(*z*) 与 *z**[j]* 独立，其中 j > i。
- en: We’ve seen that bijectors that lead to a triangular Jacobi matrix are convenient
    to handle. But are these also flexible enough to model all kinds of complex distributions?
    Fortunately, the answer is yes! In 2005, Bogachev and colleagues showed that for
    any D-dimensional distribution pair (a complicated distribution for *x* and a
    simple base distribution for z), you can find triangular bijectors that transform
    one distribution into the other.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，导致三角雅可比矩阵的双射函数便于处理。但它们是否足够灵活以模拟所有类型的复杂分布？幸运的是，答案是肯定的！在 2005 年，博加切夫和他的同事们证明了对于任何
    D 维分布对（x 的复杂分布和 z 的简单基分布），您都可以找到将一个分布转换成另一个分布的三角双射函数。
- en: 6.3.6 Using networks to control flows
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.6 使用网络控制流
- en: 'Now you’re going to see the powerful combination of networks and NFs. The basic
    idea is to use NNs to model the components gi of the D-dimensional bijector function
    #'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '现在您将看到网络和 NFs 的强大组合。基本思想是使用神经网络来模拟 D 维双射函数的 gi 组分 #'
- en: '*ɡ*(*z*) = (*ɡ*[1](*z*[1] ,... *z**[D]*), *ɡ*[2](*z*[1] ,... *z**[D]*),...,
    *ɡ**[D]*(*z*[1] ,... *z**[D]*)). The discussion in the last section gives us some
    guidelines on how to design the NNs used to model the different components gi
    of the bijector g:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*ɡ*(*z*) = (*ɡ*[1](*z*[1] ,... *z**[D]*), *ɡ*[2](*z*[1] ,... *z**[D]*),...,
    *ɡ**[D]*(*z*[1] ,... *z**[D]*))。上一节的讨论为我们提供了一些关于如何设计用于模拟双射函数 g 的不同分量 gi 的神经网络的指导：'
- en: We want the bijector to have a triangular Jacobi matrix, which ensures that
    gi is independent of zj (where j > i), i.e., *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[D]*)
    = *ɡ**[i]*(*z*[1] ,... *z**[i]* ).
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望双射函数具有三角雅可比矩阵，这确保了 gi 与 zj 独立（其中 j > i），即 *ɡ**[i]*(*z*[1] , *z*[2] ,...,
    *z**[D]*) = *ɡ**[i]*(*z*[1] ,... *z**[i]* )。
- en: 'We want the diagonal elements of the Jacobi matrix to be easy to compute:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望雅可比矩阵的对角元素易于计算：
- en: '*∂* *ɡ**[i]*(*z*[1] ,... *z**[i]* ) /*∂* *ɡ**[i]* .'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*∂* *ɡ**[i]*(*z*[1] ,... *z**[i]* ) /*∂* *ɡ**[i]* .'
- en: For the off-diagonal elements in the lower triangular of the Jacobi matrix,
    there’s no need to compute a partial derivative of these functions. They can be
    quite complicated.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于雅可比矩阵下三角中的非对角元素，不需要计算这些函数的偏导数。它们可以相当复杂。
- en: Last, but not least, we need an invertible transformation.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，但同样重要的是，我们需要一个可逆变换。
- en: 'Let’s focus on the first item in the list and write the components of a triangular
    bijector function *g*(*z*):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注列表中的第一项，并写出三角双射函数 *g*(*z*) 的分量：
- en: '*x*[1] = *ɡ*[1](*z*[1] , *z*[2] ... *z**[D]*) = *ɡ*[1](*z*[1])'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[1] = *ɡ*[1](*z*[1] , *z*[2] ... *z**[D]*) = *ɡ*[1](*z*[1])'
- en: '*x*[2] = *ɡ*[2](*z*[1] , *z*[2] ... *z**[D]*) = *ɡ*[2](*z*[1] , *z*[2])'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[2] = *ɡ*[2](*z*[1] , *z*[2] ... *z**[D]*) = *ɡ*[2](*z*[1] , *z*[2])'
- en: '....'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '....'
- en: '*x**[D]* = *ɡ**[D]*(*z*[1] , *z*[2] ... *z**[D]*) = *ɡ**[D]*(*z*[1] ,... *z**[D]*)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*x**[D]* = *ɡ**[D]*(*z*[1] , *z*[2] ... *z**[D]*) = *ɡ**[D]*(*z*[1] ,... *z**[D]*)'
- en: 'The next question is which parametric functions do we use for the component
    gi ? Here the second and third guidelines in the preceding list come into play.
    You should design gi such that the partial derivative *∂* *ɡ**[i]*(*z*[1] ,...,
    *z**[i]* ) /*∂* *ɡ**[i]* , corresponding to a diagonal element of the Jacobi matrix,
    is easy to compute. For a linear function, the derivative is easy to compute.
    Let’s choose gi to be linear in *z**[i]* :'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是我们应该使用哪些参数函数来表示 gi？在这里，前面列表中的第二和第三条准则发挥作用。你应该设计 gi，使得与雅可比矩阵对角元素相对应的偏导数
    *∂* *ɡ**[i]*(*z*[1] ,..., *z**[i]* ) /*∂* *ɡ**[i]* 容易计算。对于线性函数，导数是容易计算的。让我们选择
    gi 在 *z**[i]* 上是线性的：
- en: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b* + *a* ⋅ *z**[i]*'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b* + *a* ⋅ *z**[i]*'
- en: 'Note that gi can be non-linear in *z*[1] , *z*[2] ,..., *z**[i]* [−1] This
    means that the intercept *b* and the slope a can be complex functions of these
    *z* components: *b**[i]* = *b**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) and *a**[i]*
    = *a**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]). This yields'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，gi 在 *z*[1] , *z*[2] ,..., *z**[i]* [−1] 上可以是非线性的。这意味着截距 *b* 和斜率 a 可以是这些
    *z* 元素的复杂函数：*b**[i]* = *b**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) 和 *a**[i]*
    = *a**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1])。这导致
- en: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b**[i]*(*z*[1] , *z*[2]
    ,..., *z**[i]* [−1]) + *a**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) ⋅ *z**[i]*'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b**[i]*(*z*[1] , *z*[2]
    ,..., *z**[i]* [−1]) + *a**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) ⋅ *z**[i]*'
- en: 'Because *b**[i]* = *b**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) and *a**[i]*
    = *a**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) can be complex functions, you
    can use NNs to model these. It’s a known fact that NNs with at least one hidden
    layer are flexible enough to fit every function, and so a and *b* can depend in
    a complex manner on the provided *z* components. In one-dimensional cases, you
    need a monotone increasing or decreasing function to ensure bijectivity. This
    can be guaranteed by ensuring that the slope isn’t zero. In multidimensional cases,
    instead of the slope, you now need to ensure that the determinant of the Jacobi
    matrix isn’t zero. We do this by making sure that all entries of the diagonal
    are larger than zero. For this, you can use the same trick as in chapter 4 when
    modeling a positive standard deviation: you don’t directly use the output *α**[i]*(*z*[1]
    , *z*[2] ,..., *z**[i]* [−1]) of the NN as a slope, but you first pipe it through
    an exponential function. This yields *α**[i]* = exp(*α**[i]*(*z*[1] , *z*[2] ,...,
    *z**[i]* [−1])) and, in this case, equation 6.6 becomes'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 *b**[i]* = *b**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) 和 *a**[i]* = *a**[i]*(*z*[1]
    , *z*[2] ,..., *z**[i]* [−1]) 可以是复杂函数，你可以使用神经网络来模拟这些。这是一个已知的事实，具有至少一个隐藏层的神经网络足够灵活，可以适应任何函数，因此
    a 和 *b* 可以以复杂的方式依赖于提供的 *z* 元素。在一维情况下，你需要一个单调递增或递减的函数来确保双射性。这可以通过确保斜率不为零来保证。在多维情况下，而不是斜率，你现在需要确保雅可比矩阵的行列式不为零。我们通过确保对角线上的所有项都大于零来实现这一点。为此，你可以使用与第
    4 章中建模正标准差相同的技巧：你不会直接使用神经网络输出的 *α**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]) 作为斜率，而是首先将其通过指数函数。这得到
    *α**[i]* = exp(*α**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1]))，在这种情况下，方程式 6.6 变为
- en: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b**[i]*(*z*[1] , *z*[2]
    ,..., *z**[i]* [−1]) + exp(*α**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1])) ⋅ *z**[i]*
    Equation 6.7'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b**[i]*(*z*[1] , *z*[2]
    ,..., *z**[i]* [−1]) + exp(*α**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1])) ⋅ *z**[i]*
    方程式 6.7'
- en: 'Computing the determinant of the Jacobi matrix is easy. Just compute the product
    of the partial derivatives of gi w.r.t. zi:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 计算雅可比矩阵的行列式是容易的。只需计算 gi 对 zi 的偏导数的乘积：
- en: '![](../Images/6-14_E06.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-14_E06.png)'
- en: As you can see, the matrix is easy to calculate! The determinant is given by
    the product of positive terms and, thus, is also positive.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，矩阵是容易计算的！行列式是正项的乘积，因此也是正的。
- en: As discussed, models like the one in equation 6.6 allow for an efficient implementation
    of NF models. In the literature, these kinds of models are sometimes also called
    inverse autoregressive models. The name “autoregressive” indicates that the input
    of the regression model for the variable *x**[i]* depends only on previous observations
    of *x*[1] ,..., *x**[j]*[−1] from the same variable (hence the name, “auto”).
    You saw examples of the WaveNet and PixelCNN autoregressive models in section
    6.1.1\. But a flow model isn’t autoregressive per se because *x**[i]* = *ɡ**[i]*(*z*[1]
    , *z*[2] ,..., *z**[i]* ) is determined by the former (and current) values of
    *z* and not *x*. Still, there’s a connection that gives rise to the name inverse
    autoregressive models. If you’re interested in the details, you might want to
    look at the blog post [http://mng.bz/Z26P](http://mng.bz/Z26P) .
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，方程式 6.6 中的模型允许高效实现 NF 模型。在文献中，这类模型有时也被称为逆自回归模型。名称“自回归”表明，回归模型对于变量 *x**[i]*
    的输入仅依赖于同一变量的先前观察值 *x*[1] ,..., *x**[j]*[−1]（因此得名“auto”）。你在第 6.1.1 节中看到了 WaveNet
    和 PixelCNN 自回归模型的例子。但流模型本身并不是自回归的，因为 *x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,...,
    *z**[i]* ) 是由前一个（和当前）的 z 值决定的，而不是由 x 决定的。尽管如此，还存在一种联系，使得这些模型被称为逆自回归模型。如果你对细节感兴趣，你可能想看看这篇博客文章
    [http://mng.bz/Z26P](http://mng.bz/Z26P)。
- en: To realize such an NF model with an fcNN, you need D different networks, each
    calculating ai and bi (see equation 6.7) from different inputs:*z*[1] , *z*[2]
    ,..., *z**[i]* for *i* ∈ {1 , 2 ,...,*D* }. Having D networks would require many
    parameters, and further, the sampling from the D networks would also take quite
    some time. Why not take a single network that takes all *z*[1] , *z*[2] ,...,
    *z**[D]* inputs and then outputs all ai and bi values from which you can calculate
    all *z*[1] , *z*[2] ,..., *z**[D]* values in one go? Take a second to come up
    with an answer?
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 fcNN 实现这样的 NF 模型，你需要 D 个不同的网络，每个网络从不同的输入计算 ai 和 bi（见方程式 6.7）：*z*[1] , *z*[2]
    ,..., *z**[i]* 对于 *i* ∈ {1 , 2 ,...,*D* }。拥有 D 个网络将需要许多参数，而且，从 D 个网络中进行采样也会花费相当多的时间。为什么不使用一个接受所有
    *z*[1] , *z*[2] ,..., *z**[D]* 输入并一次性输出所有 ai 和 bi 值的网络，从而可以一次性计算出所有 *z*[1] , *z*[2]
    ,..., *z**[D]* 值呢？花点时间来想一个答案？
- en: 'The answer is that an fcNN violates the requirement that gi is independent
    of zi with j > i: that’s *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[D]*) = *ɡ*(*z*[1]
    ,..., *z**[i]* ) and thus doesn’t yield a triangular Jacobian. But there’s a solution.
    There are special networks, called autoregressive networks, that mask parts of
    the connections to ensure that the output nodes ai don’t depend on the input nodes
    zi with j > i. Luckily, you can use TFP `tfp.bijectors. AutoregressiveNetwork`
    , which guarantees that property. This network was first described in a paper
    called “Masked Autoencoder for Distribution Estimation(MADE)” (see [https://arxiv
    .org/abs/1502.03509](https://arxiv.org/abs/1502.03509)).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，全连接神经网络（fcNN）违反了 gi 与 zi 在 j > i 时相互独立的条件：即 *ɡ**[i]*(*z*[1] , *z*[2] ,...,
    *z**[D]*) = *ɡ*(*z*[1] ,..., *z**[i]* )，因此不能产生三角雅可比矩阵。但有一个解决方案。存在一种特殊的网络，称为自回归网络，它隐藏了部分连接以确保输出节点
    ai 不依赖于输入节点 zi 在 j > i 的情况下。幸运的是，你可以使用 TFP 的 `tfp.bijectors.AutoregressiveNetwork`，这保证了该属性。这种网络首次在名为“Masked
    Autoencoder for Distribution Estimation(MADE)”的论文中描述（见 [https://arxiv.org/abs/1502.03509](https://arxiv.org/abs/1502.03509)）。
- en: Let’s look at the training of such a network in D = 4 dimensions. In the training,
    we go from the observed four-dimensional *x* to a four-dimensional z, where we
    get the likelihood *p**[x]*(*x*) = *p**[z]*(*z*) = *p**[z]*(*ɡ*^(−1)(*x*)). For
    this, we rely on
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在 D = 4 维度下这种网络的训练过程。在训练过程中，我们从观察到的四维 *x* 转换到四维 z，其中我们得到似然 *p**[x]*(*x*)
    = *p**[z]*(*z*) = *p**[z]*(*ɡ*^(−1)(*x*))。为此，我们依赖于
- en: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b**[i]*(*z*[1] , *z*[2]
    ,..., *z**[i]* [−1]) + exp(*α**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1])) ⋅ *z**[i]*
    Equation 6.7 (repeate*D*)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* ) = *b**[i]*(*z*[1] , *z*[2]
    ,..., *z**[i]* [−1]) + exp(*α**[i]*(*z*[1] , *z*[2] ,..., *z**[i]* [−1])) ⋅ *z**[i]*
    方程式 6.7 (重复*D*)'
- en: 'We then need to solve equation 6.7 for zi , yielding:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要解方程式 6.7 来得到 zi ，得到：
- en: '![](../Images/6-14_E07.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-14_E07.png)'
- en: This is a sequential process, and thus, training can’t be parallelized and is
    rather slow. However, during the test phase, it’s fast.[4](#pgfId-1080452)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序过程，因此训练不能并行化，速度相对较慢。然而，在测试阶段，它很快。[4](#pgfId-1080452)
- en: Laurent Dinh, et al., introduced a somewhat different approach to build an invertible
    flow in a paper called “Density Estimation using Real NVP,” which is available
    at [https://arxiv.org/abs/1605.08803](https://arxiv.org/abs/1605.08803) . In this
    paper, they proposed a flow called a real non-volume preserving flow or Real NVP.
    The name non-volume preserving states that this method (like the triangular flows)
    can have a Jacobian determinant unequal to one and can thus change the volume.
    Compared to the design in equation 6.6, their Real NVP design is much simpler
    (shown in figure 6.15). When comparing figure 6.15 to equation 6.6, you can see
    that the Real NVP architecture is a simplified and sparse version of the triangular
    flow. If you use a triangular flow and set the first d dimensions to *b* = 0 and
    *a* = 0 and then let the remaining dimensions a and *b* only depend on the first
    d components of z, then you end up with a Real NVP model. The Real NVP architecture
    isn’t as flexible as a fully triangular bijector, but it allows for fast computations.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Laurent Dinh等人，在名为“使用真实NVP进行密度估计”的论文中介绍了一种构建可逆流的不同方法，该论文可在[https://arxiv.org/abs/1605.08803](https://arxiv.org/abs/1605.08803)找到。在这篇论文中，他们提出了一种称为真实非体积保持流或Real
    NVP的流。名称非体积保持表示这种方法（如三角形流）可以具有不等于一的雅可比行列式，因此可以改变体积。与方程6.6中的设计相比，他们的Real NVP设计要简单得多（如图6.15所示）。当将图6.15与方程6.6进行比较时，你可以看到Real
    NVP架构是三角形流的简化且稀疏版本。如果你使用三角形流，并将前d个维度设置为*b* = 0和*a* = 0，然后让剩余的维度a和*b*仅依赖于z的前d个分量，那么你最终会得到一个Real
    NVP模型。Real NVP架构不如完全三角形双射器灵活，但它允许快速计算。
- en: '![](../Images/6-15.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-15.png)'
- en: Figure 6.15 The architecture of a Real NVP model. The first d components, *z*[1]
    , *z*[2] ,..., *z**[d]* , stay untransformed, yielding *x*[1] = *z*[1] , *x*[2]
    = *z*[2] ,..., *x**[d]* = *z**[d]* . The remaining components of *x*, *x**[d+1]*
    ,... ,*x*, depend only on the first d components of *z*(*z*[1] , *z*[2] ,...,
    *z**[d]*), and are transformed as *x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[d]*)
    = *b**[i]*(*z*[1] , *z*[2] ,..., *z**[d]*) + exp(*α**[i]*(*z*[1] , *z*[2] ,...,
    *z**[d]*)) ⋅ *z**[i]* for *i* = *d* + 1 ,...,*D* ; this multiplication is indicated
    by ⊙ .
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 真实NVP模型的架构。前d个分量*z*[1] , *z*[2] ,..., *z**[d]*保持未变换，得到*x*[1] = *z*[1]
    , *x*[2] = *z*[2] ,..., *x**[d]* = *z**[d]*。*x*的其余分量*x**[d+1]* ,... ,*x*仅依赖于*z*的前d个分量(*z*[1]
    , *z*[2] ,..., *z**[d]*)，并按以下方式变换：*x**[i]* = *ɡ**[i]*(*z*[1] , *z*[2] ,..., *z**[d]*)
    = *b**[i]*(*z*[1] , *z*[2] ,..., *z**[d]*) + exp(*α**[i]*(*z*[1] , *z*[2] ,...,
    *z**[d]*)) ⋅ *z**[i]*，对于*i* = *d* + 1 ,...,*D*；这种乘法用⊙表示。
- en: Figure 6.15 In a Real NVP model, the first d components are passed through directly
    from *z* to *x*(see figure 6.15 and also the following example) and further used
    for training an NN, which outputs ai and bi for the remaining coordinates *x**[i]*
    for *i* = *d* + 1 ,...,*D* .
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 在一个真实NVP模型中，前d个分量直接从*z*传递到*x*（参见图6.15以及以下示例），并进一步用于训练一个神经网络，该网络为剩余坐标*x**[i]*（对于*i*
    = *d* + 1 ,...,*D*）输出ai和bi。
- en: 'The idea in Real NVP is to first choose a d between 1 and the dimensionality
    of your problem D (Dimensionality of *z* and *x*). To make the discussion simpler,
    let’s choose D = 5 and d = 2\. But, of course, the results are also valid for
    a general case. The flow is going from *z*(following a simple distribution) to
    *x*(following a complex distribution). The first d components (here d = 2) are
    passed through directly from *z* to *x*(see the first two lines of equation 6.8).
    Now these d (here two), z1 and z2, are the input to an NN computing ai and bi
    (see equation 6.7), yielding the slope *a**[i]* = exp(*α**[i]* ) and the shift
    *b* of the linear transformation *x**[i]* = *b**[i]* + *a**[i]* ⋅ *z**[i]* for
    *i* ∈ {3, 4, 5 } (see lines 3-5 in equation 6.8). The NN has two heads as an outcome.
    Both have D - d (here, 5 - 2 = 3) nodes. One head is *b*[1](*z*[1] , *z*[2]),
    *b*[2](*z*[1] , *z*[2]), *b*[3](*z*[1] , *z*[2]) and the other head is *a*[1](*z*[1]
    , *z*[2]), *a*[2](*z*[1] , *z*[2]), *a*[3](*z*[1] , *z*[2]). The next three transformed
    variables are determined using the network via:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Real NVP 中，首先选择一个 d，其范围在 1 和你问题的维度 D（z 和 x 的维度）之间。为了简化讨论，让我们选择 D = 5 和 d =
    2。但是，当然，这些结果也适用于一般情况。流程是从 *z*（遵循简单的分布）到 *x*（遵循复杂的分布）。前 d 个分量（这里 d = 2）直接从 *z*
    传递到 *x*（参见方程 6.8 的前两行）。现在这些 d（这里两个），z1 和 z2，是输入到一个计算 ai 和 bi 的神经网络（参见方程 6.7），得到斜率
    *a**[i]* = exp(*α**[i]* ) 和线性变换 *x**[i]* = *b**[i]* + *a**[i]* ⋅ *z**[i]* 的平移
    *b*，其中 *i* ∈ {3, 4, 5 }（参见方程 6.8 的第 3-5 行）。神经网络有两个头部作为结果。两者都有 D - d（这里，5 - 2 =
    3）个节点。一个头部是 *b*[1](*z*[1] , *z*[2]), *b*[2](*z*[1] , *z*[2]), *b*[3](*z*[1] ,
    *z*[2])，另一个头部是 *a*[1](*z*[1] , *z*[2]), *a*[2](*z*[1] , *z*[2]), *a*[3](*z*[1]
    , *z*[2])。接下来的三个变换变量是通过以下方式确定的：
- en: '*x*[1] = *ɡ*[1](*z*[1]) = *z*[1]'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[1] = *ɡ*[1](*z*[1]) = *z*[1]'
- en: '*x*[2] = *ɡ*[2](*z*[2]) = *z*[2]'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[2] = *ɡ*[2](*z*[2]) = *z*[2]'
- en: '*x*[3] = *ɡ*[3](*z*[1] , *z*[2] , *z*[3]) = *b*[3](*z*[1] , *z*[2]) + exp(*α*[3](*z*[1]
    , *z*[2])) ⋅ *z*[3]'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[3] = *ɡ*[3](*z*[1] , *z*[2] , *z*[3]) = *b*[3](*z*[1] , *z*[2]) + exp(*α*[3](*z*[1]
    , *z*[2])) ⋅ *z*[3]'
- en: '*x*[4] = *ɡ*[4](*z*[1] , *z*[2] , *z*[4]) = *b*[4](*z*[1] , *z*[2]) + exp(*α*[4](*z*[1]
    , *z*[2])) ⋅ *z*[4]'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[4] = *ɡ*[4](*z*[1] , *z*[2] , *z*[4]) = *b*[4](*z*[1] , *z*[2]) + exp(*α*[4](*z*[1]
    , *z*[2])) ⋅ *z*[4]'
- en: '*x*[5] = *ɡ*[5](*z*[1] , *z*[2] , *z*[5]) = *b*[5](*z*[1] , *z*[2]) + exp(*α*[5](*z*[1]
    , *z*[2])) ⋅ *z*[5]'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*[5] = *ɡ*[5](*z*[1] , *z*[2] , *z*[5]) = *b*[5](*z*[1] , *z*[2]) + exp(*α*[5](*z*[1]
    , *z*[2])) ⋅ *z*[5]'
- en: It’s an affine transformation like the one we used before, and the scale and
    shift terms are controlled by an NN, but this time, the NN only gets the first
    d = 2 components of *z* as input. Does this network fulfill the requirements to
    be bijective and triangular? Let’s invert the flow and go from *x* to z. This
    yields
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种类似于我们之前使用的仿射变换，缩放和平移项由一个神经网络控制，但这次，神经网络只获取 *z* 的前 d = 2 个分量作为输入。这个网络满足作为双射和三角形的条件吗？让我们逆流而行，从
    *x* 到 z。这得到：
- en: '![](../Images/6-15_E01.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6-15_E01.png](../Images/6-15_E01.png)'
- en: Also, the Jacobi matrix has the desired triangular form, even if all off-diagonal
    elements in columns > d are zero. You only need to compute the diagonal elements
    to determine the determinant, which is required for the NF method.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，雅可比矩阵具有所需的三角形形式，即使列 > d 中的所有非对角元素为零。你只需要计算对角元素来确定行列式，这对于 NF 方法是必需的。
- en: '![](../Images/6-15_E02.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6-15_E02.png](../Images/6-15_E02.png)'
- en: The non-zero, off-diagonal elements are gray-shaded in the equation because
    we don’t need them. This part of the flow is called a coupling layer.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中的非零、非对角元素以灰色阴影表示，因为我们不需要它们。这一部分的流程称为耦合层。
- en: It’s a bit strange in a Real NVP that the first d dimensions aren’t affected
    by the flow. But we can bring them into play in additional layers. Because we
    want to stack more layers anyway, let’s reshuffle the zi components before we
    get to the next layer. Reshuffling is invertible, and the determinant of the Jacobi
    matrix is 1\. We can use the TFP bijector `tfb.Permute()` to reshuffle. In listing
    6.7, which shows the relevant code, we use five pairs of coupling layers and permutations
    (see also the following notebook).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Real NVP 中，第一个 d 维度不受流的影响，这有点奇怪。但我们可以通过额外的层来发挥它们的作用。因为我们无论如何都想堆叠更多的层，所以在到达下一层之前，让我们重新排列
    zi 分量。重新排列是可逆的，雅可比矩阵的行列式为 1。我们可以使用 TFP bijector `tfb.Permute()` 来重新排列。在列出 6.7
    的相关代码中，我们使用了五对耦合层和排列（也请参阅以下笔记本）。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/RArK](http://mng.bz/RArK)
    . The notebook contains the code to show how to use a Real NVP flow of a banana-shaped,
    2D distribution on a toy data set.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![计算机图标](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/RArK](http://mng.bz/RArK)
    。该笔记本包含代码，展示了如何在一个玩具数据集上使用香蕉形状的 2D 分布的 Real NVP 流。'
- en: Execute the code and try to understand it
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行代码并尝试理解它
- en: Play with the number of hidden layers
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩转隐藏层的数量
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Listing 6.7 The simple example of a Real NVP TFP
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 实际NVP TFP的简单示例
- en: '[PRE6]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Adds num_blocks of coupling permutations to the list of bijectors
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将num_blocks个耦合排列添加到bijectors列表中
- en: ❷ Number of hidden layers in the NF model
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ NF模型中的隐藏层数量
- en: ❸ Size of the hidden layers
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 隐藏层的大小
- en: ❹ Defines the network
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义网络
- en: ❺ A shift and flow with parameters from the network
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用网络参数进行平移和流
- en: ❻ Permutation of coordinates
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 坐标排列
- en: ❼ Distribution of *z* with two independent Gaussians
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用两个独立的高斯分布的*z*分布
- en: So now you’ve seen how to construct flows using networks. The trick is to keep
    everything invertible and to construct the flows in a way that the determinant
    of the Jacobi matrix can be easily calculated. Finally, let’s have a look at the
    Glow architecture and have some fun sampling real-looking facial images from a
    normalizing flow.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你现在已经看到了如何使用网络构建流。技巧是保持一切可逆，并以一种方式构建流，使得雅可比矩阵的行列式可以轻松计算。最后，让我们看看Glow架构，并尝试从归一化流中采样逼真的面部图像。
- en: '6.3.7 Fun with flows: Sampling faces'
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.7 与流一起玩乐：采样人脸
- en: Now you come to the fun part. OpenAI did some great work in developing an NF
    model that they call the Glow model. You can use it to create realistic-looking
    faces and other images. The Glow model is similar to the Real NVP model with some
    tweaks. The main change is that the permutation is replaced by a 1 × 1 convolution.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你来到了有趣的部分。OpenAI在开发NF模型方面做了很多出色的工作，他们称之为Glow模型。你可以用它来创建看起来逼真的面孔和其他图像。Glow模型与Real
    NVP模型类似，但有一些调整。主要变化是将排列替换为1 × 1卷积。
- en: In this section, we now work with image data. In the examples up to section
    6.3.5, we worked with 1D scalar data. In section 6.3.5 and 6.3.6, we used D-dimensional
    data (for both *z* and *x*), but still the data were simple vectors. If we want
    to operate on images, we need to work with tensors to take their 2D structure
    into account. Therefore, we now have to operate on tensors *x* and *z* of shape
    (h, w, d ), which define the height (h), width (w), and number of color channels
    (*D*) instead of vectors.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们现在处理图像数据。在6.3.5节之前的例子中，我们处理了1D标量数据。在6.3.5节和6.3.6节中，我们使用了D维数据（对于*z*和*x*），但数据仍然是简单的向量。如果我们想对图像进行操作，我们需要处理张量以考虑其2D结构。因此，我们现在必须操作形状为(h,
    w, d)的张量*x*和*z*，它们定义了高度(h)、宽度(w)和颜色通道数(*D*)，而不是向量。
- en: How do we apply a Real NVP-like flow on the tensors? Recall the Real NVP architecture
    for vectors (see figure 6.15 and equation 6.8). In the case of tensors, the first
    d channels (now d two-dimensional slices) aren’t affected by the transformation
    but serve as input to a CNN. That CNN defines the transformations of the remaining
    channels of the input.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在张量上应用类似Real NVP的流？回想一下向量的Real NVP架构（见图6.15和方程6.8）。在张量的情况下，前d个通道（现在是d个二维切片）不受变换的影响，但作为CNN的输入。这个CNN定义了输入剩余通道的变换。
- en: As in a regular CNN architecture, the height and width are reduced, and the
    number of channels is increased when going deeper into the network. The idea behind
    this is to find more abstract representations. But in an NF model, the input and
    output need to have the same dimensions. The number of channels is increased,
    therefore, by a factor of four if height and width are reduced by a factor of
    two.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 正如常规CNN架构一样，随着深入网络，高度和宽度减小，通道数增加。背后的想法是找到更抽象的表示。但在NF模型中，输入和输出需要具有相同的维度。因此，如果高度和宽度减半，通道数将增加四倍。
- en: 'In the output layer, the height and width are one, and the depth is given by
    the number of values of the input *h* ⋅ *w* ⋅ *d* . For more details, see the
    paper “Glow: Generative Flow with Invertible 1x1 Convolutions” by Kingma and Dhariwal,
    which you can find at [https://arxiv.org/abs/1807.03039](https://arxiv.org/abs/1807.03039)
    or have a look at the official GitHub repository at [https://github.com/openai/glow](https://github.com/openai/glow)
    .'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '在输出层，高度和宽度为1，深度由输入*h* ⋅ *w* ⋅ *d*的值数给出。更多细节，请参阅Kingma和Dhariwal撰写的论文“Glow: Generative
    Flow with Invertible 1x1 Convolutions”，您可以在[https://arxiv.org/abs/1807.03039](https://arxiv.org/abs/1807.03039)找到或查看官方GitHub仓库[https://github.com/openai/glow](https://github.com/openai/glow)。'
- en: The bottom line is that an image *z* with dimensions (h, w, d ), typically (256,256,3),
    is transformed into a vector of length *h* ⋅ *w* ⋅ *d* , typically 196,608, where
    each dimension comes from an independent *N*(0,1) distributed Gaussian. This vector
    can again be reshaped into a color image *x* of dimensions (256 × 256 × 3).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 底线是，一个具有 (h, w, d) 维度的图像 *z*，通常是 (256,256,3)，被转换成一个长度为 *h* ⋅ *w* ⋅ *d* 的向量，通常是196,608，其中每个维度来自一个独立的
    *N*(0,1) 分布的高斯。这个向量又可以重塑成一个具有 (256 × 256 × 3) 维度的彩色图像 *x*。
- en: The network has been trained on 30,000 images of celebrities. The training took
    quite some time, but fortunately one can download the pre-trained weights. Let’s
    play with this. Open the following notebook and follow it while reading the text
    in this section.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络是在30,000张名人图像上训练的。训练花费了一些时间，但幸运的是，可以下载预训练的权重。让我们来玩玩这个。打开以下笔记本，在阅读本节文本的同时跟随它。
- en: '| ![](../Images/computer-icon.png) | Hands-on time Open [http://mng.bz/2XR0](http://mng.bz/2XR0)
    . The notebook contains code to download the weights of a pre-trained Glow model.
    It’s highly recommended to use the Colab version because the weights are approximately
    1 GB. Further, because the weights are stored in TensorFlow 1, we use a TF 1 version
    of Colab. With the notebook opened:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![](../Images/computer-icon.png) | 实践时间 打开 [http://mng.bz/2XR0](http://mng.bz/2XR0)
    。该笔记本包含下载预训练Glow模型权重的代码。强烈建议使用Colab版本，因为权重大约有1GB。此外，由于权重存储在TensorFlow 1中，我们使用TF
    1版本的Colab。笔记本打开后：'
- en: Sample random faces
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本随机人脸
- en: Manipulate a face
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作人脸
- en: Morph between two faces
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在两个面部之间进行变形
- en: Make Leonardo smile
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让莱昂纳多微笑
- en: '|'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: First, sample a random face from the learned distribution of facial images.
    You can do this by sampling a vector *z* containing 196,608 independent Gaussian
    distributions and then transform this to a vector *x* = *g*(*z*) that can be reshaped
    to a facial image. Doing so, one usually finds artifacts. To avoid this and get
    more realistic “normal” face images, one doesn’t draw from *N*(0,1) but from a
    Gaussian with reduced variance such as *N*(0,0.7) to get closer to the center.
    This reduces the risk of getting unusual looking facial images.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从学习到的面部图像分布中采样一个随机人脸。你可以通过采样一个包含196,608个独立高斯分布的向量 *z* 来完成这个操作，然后将这个向量 *x*
    = *g*(*z*) 转换成一个可以重塑为面部图像的向量。这样做，通常会发现一些伪影。为了避免这种情况并获得更逼真的“正常”人脸图像，人们不会从 *N*(0,1)
    中抽取，而是从方差较小的高斯分布，如 *N*(0,0.7) 中抽取，以更接近中心。这降低了得到不寻常外观面部图像的风险。
- en: 'Another interesting application is mixing faces. Figure 6.17 shows the basic
    idea. You start with the first image, say an image *x*[1] from Beyoncé. You then
    use the flow to calculate the corresponding vector *z*[1] = *ɡ*^(−1)(*x*[1]) .
    Then take a second image, say Leonardo DiCaprio, and calculate the corresponding
    z2\. Now let’s mix the two vectors. We have a variable c in the range 0 to 1 that
    describes the DiCaprio content. For c = 1, it’s DiCaprio; for c = 0, it’s Beyoncé.
    In the *z* space, the mixture is given by *z**[c]* = c ⋅ *z*[2] + (1− *c*) *z*[1]
    . An alternative view on the formula is to rearrange it: *z**[c]* = c ⋅ *z*[2]
    + (1− *c*)*z*[1] = *z*[1] + c(*z*[2] − *z*[1]) = *z*[1] + *c* Δ . Then Δ is the
    difference between *z*[2] and *z*[1] . See figure 6.16 for this interpretation.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的应用是混合人脸。图6.17展示了基本思想。你从一个图像开始，比如说贝昂丝的图像 *x*[1]。然后使用流来计算相应的向量 *z*[1] =
    *ɡ*^(−1)(*x*[1])。然后取第二个图像，比如说莱昂纳多·迪卡普里奥，并计算相应的 z2。现在让我们混合这两个向量。我们有一个变量 c，其范围在0到1之间，描述了迪卡普里奥的内容。对于
    c = 1，它是迪卡普里奥；对于 c = 0，它是贝昂丝。在 *z* 空间中，混合由 *z**[c]* = c ⋅ *z*[2] + (1− *c*) *z*[1]
    给出。对公式的另一种看法是重新排列它：*z**[c]* = c ⋅ *z*[2] + (1− *c*)*z*[1] = *z*[1] + c(*z*[2]
    − *z*[1]) = *z*[1] + *c* Δ。然后 Δ 是 *z*[2] 和 *z*[1] 之间的差异。参见图6.16以了解这种解释。
- en: '![](../Images/6-16.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-16.png)'
- en: Figure 6.16 Schematic sketch of the mixture in the space z. Note that the *z*
    space is high-dimensional and not 2D as in the figure. We move in a linear interpolation
    from Beyoncé in the direction Δ = z2 − z1 toward DiCaprio.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 z空间中混合的示意图。注意，*z* 空间是高维的，而不是如图所示的2D。我们在Δ = z2 − z1指向迪卡普里奥的方向上从贝昂丝进行线性插值移动。
- en: We start from c = 0 (Beyoncé) and move from there in the direction Δ to DiCaprio.
    You then use the NF *x**[c]* = *ɡ*(*z**[c]*) to go from the *z* space to the *x*
    space. For some values of c, the resulting images, xc, are shown in figure 6.17.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 c = 0（贝昂丝）开始，沿着 Δ 的方向移动到迪卡普里奥。然后你使用 NF *x**[c]* = *ɡ*(*z**[c]*) 从 *z* 空间移动到
    *x* 空间。对于某些 c 的值，结果图像 xc 在图6.17中显示。
- en: '![](../Images/6-17.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-17.png)'
- en: Figure 6.17 Morphing from Beyoncé to Leonardo DiCaprio. The values are from
    left to right, c = 0 (100% Beyoncé), c = 0.25, c = 0.5, c = 0.75, and c = 1 (100%
    DiCaprio). An animated version is available at [https://youtu.be/JTtW_nhjIYA](https://youtu.be/JTtW_nhjIYA)
    .
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17从碧昂丝到莱昂纳多·迪卡普里奥的变形。值从左到右依次为，c = 0（100%碧昂丝），c = 0.25，c = 0.5，c = 0.75，和c
    = 1（100%迪卡普里奥）。动画版本可在[https://youtu.be/JTtW_nhjIYA](https://youtu.be/JTtW_nhjIYA)找到。
- en: What’s nice about figure 6.17 is that for all the intermediate xc’s, the faces
    do somewhat look realistic. The question is can we find other interesting directions
    in the high-dimensional space? It turns out, yes.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17的优点是，对于所有中间的xc值，面部看起来多少有些逼真。问题是我们在高维空间中能否找到其他有趣的方向？结果证明，是的。
- en: The CelebA data set is annotated with 40 categories, such as having a goatee,
    a big nose, a double chin, smiling, and so on. Could we use these to find a goatee
    direction? We take the average position of all images flagged as goatee and call
    it z1, then we take the average of all images flagged as no goatee and call it
    z2\. Now let’s hope that △ = *z*[1] - *z*[2] is indeed a direction having a goatee.
    These directions have been calculated by OpenAI, and we can use them in the notebook.
    Let’s give it a try and grow DiCaprio a goatee. The result is shown in figure
    6.18.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: CelebA数据集标注了40个类别，例如有胡须、大鼻子、双下巴、微笑等。我们能用这些来找到胡须方向吗？我们取所有标记为胡须的图像的平均位置，称之为z1，然后取所有标记为无胡须的图像的平均值，称之为z2。现在让我们希望△
    = *z*[1] - *z*[2]确实是一个有胡须的方向。这些方向已经被OpenAI计算出来，我们可以在笔记本中使用它们。让我们试一试，给迪卡普里奥长出胡须。结果如图6.18所示。
- en: '![](../Images/6-18.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-18.png)'
- en: Figure 6.18 Growing Leonardo DiCaprio a goatee. The values are from left to
    right, c = 0 (original, no goatee), c = 0.25, c = 0.5, c = 0.75, and c = 1\. You
    can find an animated version at [https://youtu.be/OwMRY9MdCMc](https://youtu.be/OwMRY9MdCMc)
    .
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18给莱昂纳多·迪卡普里奥长胡须。值从左到右依次为，c = 0（原始，无胡须），c = 0.25，c = 0.5，c = 0.75，和c = 1。您可以在[https://youtu.be/OwMRY9MdCMc](https://youtu.be/OwMRY9MdCMc)找到动画版本。
- en: That’s quite fascinating because the goatee information hasn’t been used during
    the training of the flow. Only after the training happened was the direction of
    the goatee found in the latent space. Let’s try to understand why moving in the
    latent *z* space produces valid images in the *x* space. Look at the example of
    morphing from Beyoncé to Leonardo DiCaprio. There are two points in the 196,608-dimensional
    *x* space. For a better understanding, let’s take a look at the 2D example discussed
    in section 6.3.5\. Feel free to open the notebook [http://mng.bz/RArK](http://mng.bz/RArK)
    again and scroll down to the cell, Understanding the Mixture. The *z* distribution
    in the 2D example is produced by two independent Gaussians (see the left side
    of figure 6.19), and the *x* distribution looks like a boomerang (see the right
    side of figure 6.19).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常令人着迷，因为胡须信息在流的训练过程中没有被使用。只有在训练完成后，在潜在空间中才找到了胡须的方向。让我们尝试理解为什么在潜在*z*空间中的移动会在*x*空间中产生有效的图像。看看从碧昂丝到莱昂纳多·迪卡普里奥的变形示例。在196,608维*x*空间中有两个点。为了更好地理解，让我们再次打开第6.3.5节中讨论的2D示例。请随意打开笔记本[http://mng.bz/RArK](http://mng.bz/RArK)并向下滚动到单元格，理解混合。2D示例中的*z*分布是由两个独立的高斯分布产生的（见图6.19的左侧），而*x*分布看起来像一把飞镖（见图6.19的右侧）。
- en: '![](../Images/6-19.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-19.png)'
- en: Figure 6.19 A synthetic 2D example of a complex *x* distribution (shown on the
    right) and the latent *z* distribution (on the left). A learned Real NVP flow
    transforms from latent space to *x* space. The straight line in the *z* space
    corresponds to moving along the curved line in the *x* space.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19展示了复杂*x*分布（右侧所示）和潜在*z*分布（左侧）。学习到的Real NVP流将潜在空间转换为*x*空间。*z*空间中的直线对应于*x*空间中曲线的移动。
- en: 'We start with two points in the *x* space: in the high-dimensional example,
    that’s Beyoncé and DiCaprio. In our 2D examples, these are the points (0.6, 0.25)
    and (0.6, -0.25), labeled with stars in figure 6.19\. We then use the inverse
    flow to determine the corresponding points z1 and z2 in the *z* space, labeled
    by stars on the left side of figure 6.19\. In the *z* space, we then move along
    a straight line from z1 to z2\. (This is what you saw in figure 6.16.) In figure
    6.19, you can see that the line on the left is completely in the distribution.
    We don’t move into regions where there’s no training data (gray points). Now,
    we transform the line back to the *x* space of the real data. You can see on the
    right side of figure 6.19 that the resulting line is now curved and also stays
    in regions where there’s data. The same happens in the high-dimensional space,
    which is the reason that all the points between Beyoncé and DiCaprio look like
    real faces.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*x*空间中的两个点开始：在高维示例中，那是碧昂丝和迪卡普里奥。在我们的2D示例中，这些点是（0.6，0.25）和（0.6，-0.25），在图6.19中标有星号。然后我们使用逆流来确定*z*空间中相应的z1和z2点，在图6.19的左侧标有星号。在*z*空间中，然后我们沿着直线从z1移动到z2。（这就是你在图6.16中看到的内容。）在图6.19中，你可以看到左侧的线完全在分布内。我们不移动到没有训练数据的区域（灰色点）。现在，我们将线转换回真实数据的*x*空间。你可以在图6.19的右侧看到，结果线现在是弯曲的，并且仍然停留在有数据的区域。在高度空间中也会发生同样的情况，这就是为什么贝昂丝和迪卡普里奥之间的所有点看起来都像真实人脸的原因。
- en: What would happen if we connect the two points in the *x* space directly? We’d
    leave the region of known points (see the dashed line in figure 6.19). The same
    would happen in the high-dimensional space, producing images that wouldn’t look
    like real images. How about the goatee? Again, we move along a straight direction
    in the latent space *z* without leaving the distribution. We start from a valid
    point (DiCaprio) and move along a certain direction (in our case, the goatee)
    without leaving the *z* distribution.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们直接连接*x*空间中的两个点会发生什么？我们会离开已知点的区域（见图6.19中的虚线）。在高维空间中也会发生同样的情况，产生的图像不会像真实图像。那么山羊胡呢？再次，我们在潜在空间*z*中沿着直线移动，而不离开分布。我们从有效点（迪卡普里奥）开始，沿着某个方向（在我们的例子中，是山羊胡）移动，而不离开*z*分布。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Real-world data needs complex distributions.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际世界的数据需要复杂分布。
- en: For categorical data, the multinomial distribution offers maximal flexibility
    with a drawback of having many parameters.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类数据，多项分布提供了最大的灵活性，但缺点是参数很多。
- en: For discrete data with many possible values (like count data), multinomial distributions
    are ineffective.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有许多可能值的离散数据（如计数数据），多项分布是无效的。
- en: For simple count data, Poisson distributions are fine.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于简单的计数数据，泊松分布是合适的。
- en: For complex discrete data including count data, mixtures of discretized logistic
    distributions are successfully used in the wild, like in PixelCNN++ and parallel
    WaveNet.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于包括计数数据在内的复杂离散数据，混合离散逻辑分布已在野外成功应用，如PixelCNN++和parallel WaveNet。
- en: Normalizing flows (NF) are an alternative approach to model complex distributions.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正态流（NF）是建模复杂分布的另一种方法。
- en: NFs are based on learning a transformation function that leads from simple base
    distributions to the complex real-world distribution of interest.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFs基于学习一个转换函数，该函数从简单的基分布到感兴趣的复杂真实世界分布。
- en: A powerful NF can be realized with an NN.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过NN实现强大的NF。
- en: You can use an NN-powered NF to model high-dimensional complex distributions
    like faces.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用基于NN的NF来模拟高维复杂分布，如人脸。
- en: You can also use NF models to sample data from the learned distribution.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还可以使用NF模型从学习到的分布中采样数据。
- en: TFP offers the `bijector` package that’s centered around an NF.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFP提供了以NF为中心的`bijector`包。
- en: As in chapters 4 and 5, the maximum likelihood (MaxLike) principle does the
    trick in learning NFs.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如第4章和第5章所述，在NFs的学习中，最大似然（MaxLike）原理起到了作用。
- en: 1.In case you’re interested, the preprocessing was done using the statistics
    software R; the script is available at [http://mng.bz/lGg6](http://mng.bz/lGg6)
    . We’d like to thank Sandra Siegfried and Torsten Hothorn from the University
    of Zurich for providing us with help and the initial version of the R script.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 1.如果你感兴趣，预处理是使用统计软件R完成的；脚本可在[http://mng.bz/lGg6](http://mng.bz/lGg6)找到。我们想感谢苏黎世大学的Sandra
    Siegfried和Torsten Hothorn为我们提供帮助和R脚本的初始版本。
- en: 2.More precisely, it needs to be strictly monotone.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 2.更精确地说，它需要严格单调。
- en: 3.We take the absolute values (|dz| and |dx|) because dz and dx could be negative.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 我们取绝对值（|dz| 和 |dx|），因为 dz 和 dx 可能是负数。
- en: 4.In fact, autoregressive flows also exist. With the different trade-offs, these
    nets are fast in training but slow in prediction. It turns out that WaveNet is
    such an autoregressive flow.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 事实上，自回归流也存在。由于不同的权衡，这些网络在训练时速度快，但在预测时速度慢。结果发现 WaveNet 就是一种自回归流。
