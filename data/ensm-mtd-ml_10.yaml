- en: 7 Learning with continuous and count labels
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用连续和计数标签进行学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Regression in machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的回归
- en: Loss and likelihood functions for regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归的损失和似然函数
- en: When to use different loss and likelihood functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用不同的损失和似然函数
- en: Adapting parallel and sequential ensembles for regression problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将并行和顺序集成应用于回归问题
- en: Using ensembles for regression in practical settings
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实际设置中使用集成进行回归
- en: Many real-world modeling, prediction, and forecasting problems are best framed
    and solved as regression problems. Regression has a rich history predating the
    advent of machine learning and has long been a part of the standard statistician’s
    toolkit.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的建模、预测和预测问题最好以回归问题的形式提出和解决。回归有着悠久的历史，早于机器学习的出现，并且长期以来一直是标准统计学家工具箱的一部分。
- en: 'Regression techniques have been developed and widely applied in many areas.
    Here are just a few examples:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 回归技术在许多领域得到了开发和应用。以下只是几个例子：
- en: '*Weather forecasting*—To predict the precipitation tomorrow using data from
    today, including temperature, humidity, cloud cover, wind, and more'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*天气预报*——使用今天的数据预测明天的降水量，包括温度、湿度、云量、风速等'
- en: '*Insurance analytics*—To predict the number of automobile insurance claims
    over a period of time, given various vehicle and driver attributes'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*保险分析*——根据各种车辆和驾驶员属性，预测一段时间内的汽车保险索赔数量'
- en: '*Financial forecasting*—To predict stock prices using historical stock data
    and trends'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*金融预测*——使用历史股票数据和趋势预测股票价格'
- en: '*Demand forecasting*—To predict the residential energy load for the next three
    months using historical, demographic, and weather data'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*需求预测*——使用历史、人口统计和天气数据预测未来三个月的住宅能源负荷'
- en: Whereas chapters 2-6 introduced ensembling techniques for classification problems,
    in this chapter, we’ll see how to adapt ensembling techniques to regression problems.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 而第2-6章介绍了用于分类问题的集成技术，在本章中，我们将了解如何将集成技术应用于回归问题。
- en: 'Consider the task of detecting fraudulent credit card transactions. This is
    a *classification problem* because we’re aiming to distinguish between two types
    of transactions: fraudulent (e.g., with class label 1) and not fraudulent (e.g.,
    with class label 0). The labels (or targets) we want to predict in classification
    are *categorical* (0, 1, . . .) and represent different categories.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑检测欺诈信用卡交易的任务。这是一个*分类问题*，因为我们旨在区分两种类型的交易：欺诈（例如，具有类别标签1）和非欺诈（例如，具有类别标签0）。在分类中，我们想要预测的标签（或目标）是*分类的*（0，1，……）并代表不同的类别。
- en: On the other hand, consider the task of predicting a cardholder’s monthly credit
    card balance. This is an instance of a *regression* task. Unlike classification,
    the labels (or targets) we want to predict are *continuous* values (e.g., $650.35).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，考虑预测持卡人每月信用卡余额的任务。这是一个*回归*任务的例子。与分类不同，我们想要预测的标签（或目标）是*连续*值（例如，$650.35）。
- en: Consider yet another task of predicting the number of times a cardholder uses
    their card every week. This is also an instance of a regression task, though with
    a subtle difference. The labels, or targets, we want to predict are *counts*.
    We typically distinguish between *continuous regression* and *count regression*
    because it doesn’t always make sense to model counts as continuous values (e.g.,
    what does it even mean to predict that a cardholder will use their card 7.62 times?).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑另一个预测持卡人每周使用其卡次数的任务。这也是一个回归任务的例子，尽管存在细微的差别。我们想要预测的标签或目标是*计数*。我们通常区分*连续回归*和*计数回归*，因为将计数建模为连续值并不总是有意义的（例如，预测持卡人将使用其卡7.62次是什么意思呢？）。
- en: In this chapter, we’ll learn about these types of problems and others that can
    be modeled with regression, as well as how we can train regression ensembles.
    Section 7.1 introduces regression formally, shows some commonly used regression
    models, and explains how regression can be used to model continuous and count-valued
    labels (and even categorical labels) under a single framework called the generalized
    linear model (GLM). Sections 7.2 and 7.3 show how we can adapt ensemble methods
    to regression problems. Section 7.3 introduces loss and likelihood functions for
    continuous and count-valued targets, along with guidelines on when and how to
    use them. We conclude with a case study in section 7.4, this time from the realm
    of demand forecasting.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解这些类型的问题以及其他可以用回归建模的问题，以及我们如何训练回归集成。7.1 节正式介绍了回归，展示了常用的回归模型，并解释了如何在广义线性模型（GLM）的单一框架下使用回归来建模连续和计数值标签（甚至分类标签）。7.2
    和 7.3 节展示了如何将集成方法应用于回归问题。7.3 节介绍了连续和计数值目标的损失和似然函数，以及何时以及如何使用它们的指南。我们以第 7.4 节的案例研究结束，这次来自需求预测领域。
- en: 7.1 A brief review of regression
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 回归的简要回顾
- en: This section reviews the terminology and background material for regression.
    We begin with the more familiar and traditional framing of regression as learning
    with continuous labels. We’ll then discuss Poisson regression, an important technique
    for learning with count labels, and logistic regression, another important technique
    for learning with categorical labels.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了回归的术语和背景材料。我们首先从更熟悉和传统的回归框架开始，即连续标签的学习。然后我们将讨论泊松回归，这是一种重要的学习计数标签的技术，以及逻辑回归，这是另一种重要的学习分类标签的技术。
- en: In particular, we’ll see that linear, Poisson, and logistic regression are all
    individual variations within the GLM framework. We’ll also briefly review two
    important nonlinear regression methods—decision-tree regression and artificial
    neural networks (ANNs)—as they are both often used as base estimators or meta-estimators
    in ensemble methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们会看到线性、泊松和逻辑回归都是 GLM 框架内的个别变体。我们还将简要回顾两种重要的非线性回归方法——决策树回归和人工神经网络（ANNs），因为它们通常被用作集成方法中的基础估计器或元估计器。
- en: 7.1.1 Linear regression for continuous labels
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 连续标签的线性回归
- en: 'The most fundamental regression method is *linear regression*, where the model
    to be trained is a linear, weighted combination of the input features:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的回归方法是 *线性回归*，其中要训练的模型是输入特征的线性、加权组合。
- en: '![00-kunapuli-ch7-eqs-0xa](../Images/00-kunapuli-ch7-eqs-0xa.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![00-kunapuli-ch7-eqs-0xa](../Images/00-kunapuli-ch7-eqs-0xa.png)'
- en: The linear regression model *f*(*x*) takes an example x as input and is parameterized
    by feature weights, w, and the intercept (also known as the bias) *w*[0]. This
    model is trained by identifying the weights that minimize the *mean squared error*
    (MSE) between the true labels (*y*[i]) and predicted labels (*f*(*x*[i])) over
    all *n* training examples where
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型 *f*(*x*) 以一个示例 x 作为输入，并由特征权重 w 和截距（也称为偏差）*w*[0] 参数化。该模型通过识别最小化所有 *n*
    个训练示例中真实标签 (*y*[i]) 和预测标签 (*f*(*x*[i])) 之间 *均方误差* (MSE) 的权重进行训练。
- en: '![00-kunapuli-ch7-eqs-1x](../Images/00-kunapuli-ch7-eqs-1x.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![00-kunapuli-ch7-eqs-1x](../Images/00-kunapuli-ch7-eqs-1x.png)'
- en: 'The MSE is nothing but the (mean) squared loss. Since we minimize the loss
    function to learn the model, linear regression is also known by another name that
    may be familiar to you: ordinary least squares (OLS) regression.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差（MSE）不过是（平均）平方损失。由于我们通过最小化损失函数来学习模型，线性回归也被称为你可能熟悉的其他名称：普通最小二乘（OLS）回归。
- en: 'Recall from chapter 6, section 6.2 (and also from chapter 1), that most machine-learning
    problems can be cast as combinations of regularization and loss functions, where
    the regularization function controls model complexity, and the loss function controls
    model fit:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第 6 章，第 6.2 节（以及第 1 章），大多数机器学习问题都可以表示为正则化函数和损失函数的组合，其中正则化函数控制模型复杂度，损失函数控制模型拟合度：
- en: '![00-kunapuli-ch7-eqs-3x](../Images/00-kunapuli-ch7-eqs-3x.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![00-kunapuli-ch7-eqs-3x](../Images/00-kunapuli-ch7-eqs-3x.png)'
- en: '*α*, of course, is the regularization parameter that trades off between fit
    and complexity. It must be determined and set by the user, typically through practices
    such as cross validation (CV).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*α* 当然是权衡拟合和复杂度的正则化参数。它必须由用户确定并设置，通常通过交叉验证（CV）等实践来完成。'
- en: 'Optimizing (specifically, minimizing) this learning objective essentially amounts
    to training a model. From this perspective, ordinary least squares regression
    can be framed as an *unregularized learning problem* where only the squared-loss
    function is optimized:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 优化（特别是，最小化）这个学习目标本质上等同于训练一个模型。从这个角度来看，普通最小二乘回归可以被视为一个 *未正则化的学习问题*，其中只优化了平方损失函数：
- en: '![00-kunapuli-ch7-eqs-4x](../Images/00-kunapuli-ch7-eqs-4x.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![00-kunapuli-ch7-eqs-4x](../Images/00-kunapuli-ch7-eqs-4x.png)'
- en: Is it possible to use different regularization functions to come up with other
    linear regression methods? Absolutely, and this is precisely what the statistics
    community has been up to for the better part of the past century.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 是否可以使用不同的正则化函数来提出其他线性回归方法？绝对可以，这正是统计界在过去几十年中一直在做的事情。
- en: Common linear regression methods
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的线性回归方法
- en: 'Let’s see some common linear regression methods in practice through scikit-learn’s
    linear_model subpackage, which implements several linear regression models. We’ll
    use a synthetic data set, where the true underlying function is given by *f*(*x*)
    = -2.5*x* + 3.2\. This is a univariate function, or a function of one variable
    (for our purposes, one feature). In practice, we’ll often not know the true underlying
    function, of course. The following code snippet generates a small, noisy data
    set of 100 training examples:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过scikit-learn的linear_model子包中实现的几个线性回归模型，看看一些常见的线性回归方法在实际中的应用。我们将使用一个合成数据集，其中真实的基础函数由
    *f*(*x*) = -2.5*x* + 3.2 给出。这是一个一元函数，或一个变量的函数（对我们来说，一个特征）。在实践中，我们当然通常不知道真实的基础函数。以下代码片段生成一个包含100个训练示例的小型、噪声数据集：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Creates a seeded random number generator in NumPy
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在NumPy中创建一个有种子（seeded）的随机数生成器
- en: ❷ Generates noisy labels according to this (linear) function
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据此（线性）函数生成噪声标签
- en: We can visualize this data set in figure 7.1.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图7.1中可视化这个数据集。
- en: '![CH07_F01_Kunapuli](../Images/CH07_F01_Kunapuli.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F01_Kunapuli](../Images/CH07_F01_Kunapuli.png)'
- en: Figure 7.1 Data for a synthetic regression problem to which we fit several linear
    regression models, generated by the univariate (1D), noisy function *f*(*x*) =
    -2.5*x* + 3.2 shown by the line overlaid on the data points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 我们拟合了几个线性回归模型，以拟合一个合成回归问题的数据，这些数据由一维（1D）、噪声函数 *f*(*x*) = -2.5*x* + 3.2
    生成，该函数由覆盖在数据点上的线表示。
- en: Different regularization methods serve diverse modeling needs and can handle
    different types of data problems. The most common data problem that linear regression
    models must contend with is that of *multicollinearity*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的正则化方法服务于不同的建模需求，可以处理不同类型的数据问题。线性回归模型必须应对的最常见数据问题是 *多重共线性*。
- en: Multicollinearity in data arises when one feature depends on others, that is,
    when the features are *correlated with each other*. For example, in medical data,
    patient weight and blood pressure are often highly correlated. In practical terms,
    this means that both features convey nearly the *same* *information,* and it should
    be possible to train a less complex model by selecting and using only one of them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的多重共线性出现时，一个特征依赖于其他特征，即特征之间 *相关*。例如，在医疗数据中，患者体重和血压通常高度相关。在实践中，这意味着这两个特征几乎传达了
    *相同* 的 *信息*，并且应该可以通过选择和使用其中之一来训练一个更简单的模型。
- en: 'To understand the effect of different regularization methods, we’ll explicitly
    create a data set with multicollinearity using our recently generated univariate
    data. Specifically, we’ll create a data set with two features, where one feature
    is dependent on the other:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解不同正则化方法的影响，我们将使用我们最近生成的单变量数据显式创建一个具有多重共线性（multicollinearity）的数据集。具体来说，我们将创建一个包含两个特征的数据集，其中一个特征依赖于另一个特征：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This produces a data set of two features, where the second feature is three
    times the first (with some added random noise to keep it more realistic). We now
    have a 2D data set, where the second feature is highly correlated with the first.
    As before, we’ll split the data set into training (75%) and test (25%) sets:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了一个包含两个特征的数据集，其中第二个特征是第一个特征的3倍（添加了一些随机噪声以使其更真实）。现在我们有一个二维数据集，其中第二个特征与第一个特征高度相关。与之前一样，我们将数据集分为训练集（75%）和测试集（25%）：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now train four commonly used linear regression models:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们训练四种常用的线性回归模型：
- en: OLS regression, with no regularization
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OLS回归，无正则化
- en: Ridge regression, which uses L2 regularization
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 岭回归，使用L2正则化
- en: Least Absolute Shrinkage and Selection Operator (LASSO), which uses L1 regularization
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小绝对收缩和选择算子（LASSO），使用L1正则化
- en: Elastic net, which uses a combination of L1 and L2 regularization
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性网络，它结合了 L1 和 L2 正则化
- en: The following listing initializes and trains all 4 models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表初始化并训练所有 4 个模型。
- en: Listing 7.1 Linear regression models
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 线性回归模型
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Initializes four common linear regression models
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化四个常见的线性回归模型
- en: ❷ Trains the regression model
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练回归模型
- en: ❸ Gets predictions on the test set
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在测试集上获取预测
- en: ❹ Computes the test error using MSE and MAD
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 MSE 和 MAD 计算测试误差
- en: ❺ Prints the regression weights
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印回归权重
- en: 'The unregularized OLS model will serve as our baseline for comparing the others:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 未正则化的 OLS 模型将作为我们比较其他模型的基准：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We’ll use two metrics to evaluate the performance of each model: mean squared
    error (MSE) and mean absolute deviation (MAD). This model has an MSE of 2.786
    and a MAD of 1.3\. The next linear regression model, ridge regression, uses L2
    regularization, which is just the sum of squares of the weights, that is,'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两个指标来评估每个模型的表现：均方误差（MSE）和平均绝对偏差（MAD）。该模型具有 MSE 2.786 和 MAD 1.3。下一个线性回归模型，岭回归，使用
    L2 正则化，即权重的平方和，
- en: '![CH07_F01_Kunapuli-eqs-5x](../Images/CH07_F01_Kunapuli-eqs-5x.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F01_Kunapuli-eqs-5x](../Images/CH07_F01_Kunapuli-eqs-5x.png)'
- en: So, what does L2 regularization do? Learning involves minimizing the learning
    objective; when the regularization term, or sum of squares, is minimized, it pushes
    individual weights to zero. This is known as *shrinkage* of the model weights,
    which reduces model complexity.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，L2 正则化做什么呢？学习涉及最小化学习目标；当正则化项或平方和最小化时，它将单个权重推到零。这被称为模型权重的 *收缩*，它减少了模型复杂度。
- en: 'The squared loss term in the objective is critical because, without it, we
    would train a degenerate model with all zero weights. Thus, a ridge regression
    model trades off complexity for fit, the balance of which is controlled by appropriately
    setting the parameter *α* > 0\. Listing 7.1 produces the following ridge regression
    model (with *α* > 0.5):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数中的平方损失项至关重要，因为没有它，我们将训练一个所有权重都为零的退化模型。因此，岭回归模型在复杂性和拟合度之间进行权衡，这种平衡由适当地设置参数
    *α* > 0\. 列表 7.1 生成以下岭回归模型（*α* > 0.5）：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The effect of regularization and the resultant shrinkage is immediately evident
    when we compare the weights learned by L2-regularized ridge regression, [-0.34,
    -0.7], to those learned by unregularized OLS regression, [-1.46, -0.322].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 L2 正则化的岭回归学习到的权重 [-0.34, -0.7] 与未正则化的 OLS 回归学习到的权重 [-1.46, -0.322] 进行比较时，正则化和由此产生的收缩效应立即显现。
- en: As mentioned earlier, another popular linear regression method is Least Absolute
    Shrinkage and Selection Operator (LASSO), which is rather similar to ridge regression
    except that it uses L1 regularization to control model complexity. That is, the
    learning objective with L1 regression becomes
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，另一种流行的线性回归方法是最小绝对收缩和选择算子（LASSO），它与岭回归相当相似，只是它使用 L1 正则化来控制模型复杂度。也就是说，L1
    回归的学习目标变为
- en: '![CH07_F01_Kunapuli-eqs-6x](../Images/CH07_F01_Kunapuli-eqs-6x.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F01_Kunapuli-eqs-6x](../Images/CH07_F01_Kunapuli-eqs-6x.png)'
- en: L1 regularization is the sum of absolute values of the weights, rather than
    the sum of squares in L2 regularization. The effect, overall, is similar to L2
    regularization, except that L1 regularization shrinks the weights for less-predictive
    features. In contrast, L2 regularization shrinks the weights for all the features
    uniformly.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化是权重绝对值的总和，而不是 L2 正则化中的平方和。总体效果与 L2 正则化相似，但 L1 正则化会缩小预测能力较弱的特征的权重。相比之下，L2
    正则化会均匀地缩小所有特征的权重。
- en: Put another way, L1 regularization pushes the weights of less-informative features
    down to zero, which makes it well suited for feature selection. L2 regularization
    pushes the weights of all features down together, which makes it well suited for
    handling correlated and covariant features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，L1 正则化将信息量较小的特征的权重推到零，这使得它非常适合特征选择。L2 正则化将所有特征的权重一起推到零，这使得它非常适合处理相关和协变的特征。
- en: 'Listing 7.1 produces the following LASSO model (with α > 0.5):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 生成以下 LASSO 模型（α > 0.5）：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Contrast the LASSO model’s weights, [0, -0.798], to those learned by ridge
    regression, [-0.34, -0.7]: LASSO has actually learned a zero weight for the first
    feature!'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将 LASSO 模型的权重 [0, -0.798] 与岭回归学习到的权重 [-0.34, -0.7] 进行对比：LASSO 实际上为第一个特征学习了零权重！
- en: We can see that L1 regularization induces *model sparsity*. That is, LASSO performs
    implicit feature selection during learning to identify a small set of features
    needed to build a less-complex model, while maintaining or even improving performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，L1正则化诱导了*模型稀疏性*。也就是说，LASSO在学习过程中进行隐式特征选择，以识别构建更简单模型所需的一小组特征，同时保持或提高性能。
- en: Put another way, this LASSO model only depends on one feature, while the OLS
    model requires two. This makes the LASSO model less complex than the OLS model.
    While this may not mean much for this toy data set, this has significant scalability
    implications when deployed for a data set that has thousands of features.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这个LASSO模型只依赖于一个特征，而OLS模型需要两个特征。这使得LASSO模型比OLS模型更简单。虽然这对于这个玩具数据集可能意义不大，但当应用于具有数千个特征的数据库时，这具有显著的扩展性影响。
- en: Recall that our synthetic data set was carefully constructed to have two highly
    correlated features. LASSO has identified this, determined that it doesn’t require
    both, and hence learned a zero weight for one, effectively zeroing out its contribution
    to the final model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的合成数据集是精心构建的，具有两个高度相关的特征。LASSO已经识别出这一点，确定不需要两者，因此学习了一个零权重，有效地消除了其对最终模型贡献。
- en: 'The final linear regression model we’ll look at is called elastic net, a celebrated,
    widely used, and well-studied model. Elastic net regression uses a combination
    of both L1 and L2 regularization:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要研究的最后一个线性回归模型称为弹性网络，这是一个著名、广泛使用且研究深入的模型。弹性网络回归使用L1和L2正则化的组合：
- en: '![CH07_F01_Kunapuli-eqs-7x](../Images/CH07_F01_Kunapuli-eqs-7x.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F01_Kunapuli-eqs-7x](../Images/CH07_F01_Kunapuli-eqs-7x.png)'
- en: The proportions of L1 and L2 regularizers in the overall regularization are
    controlled by the L1 ratio, 0 ≤ *ρ* ≤ 1, while the parameter *α* > 0 still controls
    the tradeoff between the overall regularization and the loss function.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化在整体正则化中的比例由L1比率控制，0 ≤ *ρ* ≤ 1，而参数*α* > 0仍然控制整体正则化和损失函数之间的权衡。
- en: The L1 ratio allows us to tune the contribution of L1 and L2 objectives. For
    example, if *ρ* = 0, the elastic net objective becomes a ridge regression objective.
    Alternately, if *ρ* = 1, the elastic net objective becomes a LASSO objective.
    For all other values between 0 and 1, the elastic net objective is some combination
    of ridge regression and LASSO.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: L1比率允许我们调整L1和L2目标函数的贡献。例如，如果*ρ* = 0，弹性网络目标函数变为岭回归目标函数。相反，如果*ρ* = 1，弹性网络目标函数变为LASSO目标函数。对于0和1之间的所有其他值，弹性网络目标函数是岭回归和LASSO的某种组合。
- en: 'Listing 7.1 produces the following elastic net model, with *α* = 0.5, *ρ* =
    0.5, and elastic net’s test set performance as MSE = 2.824, MAD = 1.304:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1生成了以下弹性网络模型，其中*α* = 0.5，*ρ* = 0.5，弹性网络测试集性能为MSE = 2.824，MAD = 1.304：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we see from the results, the elastic net model still has the sparsity-inducing
    characteristics of LASSO (observe that the first learned weight is zero), while
    incorporating the robustness of ridge regression to correlations in the data (compare
    the test set performances of ridge regression and elastic net).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中我们可以看到，弹性网络模型仍然具有LASSO的稀疏性诱导特征（注意第一个学习的权重为零），同时结合了岭回归对数据相关性的鲁棒性（比较岭回归和弹性网络在测试集上的性能）。
- en: Table 7.1 summarizes several common linear regression models, all of which can
    be cast into the squared loss + regularization framework discussed previously.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1总结了几个常见的线性回归模型，所有这些模型都可以转换为之前讨论的平方损失+正则化框架。
- en: Table 7.1 Four popular linear regression methods that all use the squared-loss
    function but use different regularization approaches to contribute to model robustness
    and sparsity
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1：四种流行的线性回归方法，它们都使用平方损失函数，但采用不同的正则化方法来提高模型的鲁棒性和稀疏性
- en: '| Model | Loss function | Regularization | Comment |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 损失函数 | 正则化 | 备注 |'
- en: '| OLS regression | Squared loss(*y* - *f*(***x***))² | None | Classical linear
    regression; becomes unstable with highly correlated features |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| OLS回归 | 平方损失(*y* - *f*(***x***))² | 无 | 经典线性回归；在高度相关特征下变得不稳定 |'
- en: '| Ridge regression | Squared loss(*y* - *f*(***x***))² | L2 penalty1/2(*w*[1]²+
    ⋅⋅⋅ + *w*[d]²) | Shrinks the weights to control model complexity, and encourages
    robustness to highly correlated features |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 岭回归 | 平方损失(*y* - *f*(***x***))² | L2惩罚1/2(*w*[1]²+ ⋅⋅⋅ + *w*[d]²) | 缩小权重以控制模型复杂度，并鼓励对高度相关特征的鲁棒性
    |'
- en: '| LASSO | Squared loss(*y* - *f*(***x***))² | L1 penalty&#124;*w*[1]&#124;
    + ⋅⋅⋅ + &#124;*w*[d]&#124; | Shrinks the weights even more, encourages sparse
    models, performs implicit feature selection |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| LASSO | 平方损失(*y* - *f*(***x***))² | L1惩罚&#124;*w*[1]&#124; + ⋅⋅⋅ + &#124;*w*[d]&#124;
    | 进一步缩小权重，鼓励稀疏模型，执行隐式特征选择 |'
- en: '| Elastic net | Squared loss(*y* - *f*(***x***))² | *ρ*L1 + (1 - *ρ*)L20 ≤
    *ρ* ≤ 1 | Weighted combination of both regularizers to balance between sparsity
    and robustness |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 弹性网络 | 平方损失(*y* - *f*(***x***))² | *ρ*L1 + (1 - *ρ*)L20 ≤ *ρ* ≤ 1 | 两种正则化器的加权组合，以平衡稀疏性和鲁棒性
    |'
- en: During model training, these regularized loss functions are often optimized
    through gradient descent, Newton’s descent, or their variants, as discussed in
    chapter 5, section 5.1, and chapter 6, section 6.1.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，这些正则化损失函数通常通过梯度下降、牛顿下降或其变体进行优化，如第5章第5.1节和第6章第6.1节所述。
- en: All the linear regression methods in table 7.1 use the squared loss. Other regression
    methods can be derived using different loss functions. We’ll see examples in section
    7.3, and again in the case study in section 7.4.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1中的所有线性回归方法都使用平方损失。其他回归方法可以使用不同的损失函数推导出来。我们将在第7.3节中看到示例，并在第7.4节的案例研究中再次看到。
- en: 7.1.2 Poisson regression for count labels
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 计数标签的泊松回归
- en: The previous section introduced regression as a machine-learning approach suited
    for modeling problems with continuous-valued targets (labels). There are often
    situations, however, where we have to develop models in which the labels are counts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了回归作为一种适合于建模具有连续值目标（标签）问题的机器学习方法。然而，常常存在这样的情况，我们必须开发标签为计数的模型。
- en: In health informatics, for instance, we may want to build a model to predict
    the number (essentially, the count) of doctor visits given specific patient data.
    In insurance pricing, a common problem is that of modeling claim frequency to
    predict the count of how many insurance claims we can expect for different types
    of insurance policies. Urban planning is another example in which we may want
    to model different count variables for census regions, such as household size,
    number of crimes, number of births and deaths, and many more. In all of these
    problems, we’re still interested in building a regression model of the form *y*
    = *f*(*x*); however, the target label *y* is no longer a continuous value, but
    a count.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在健康信息学中，我们可能想要构建一个模型来预测给定特定患者数据的医生就诊次数（本质上，是计数）。在保险定价中，一个常见问题是建模索赔频率，以预测不同类型保险政策的预期索赔次数。城市规划是另一个例子，我们可能想要为人口普查区域建模不同的计数变量，例如家庭规模、犯罪数量、出生和死亡数量等。在所有这些问题中，我们仍然感兴趣的是构建形式为*y*
    = *f*(*x*)的回归模型；然而，目标标签*y*不再是连续值，而是一个计数。
- en: Assumptions of continuous-valued regression models
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 连续值回归模型的假设
- en: One approach is to simply treat counts as continuous values, but this doesn’t
    always work. For one, continuous-valued predictions of count variables can’t always
    be interpreted meaningfully. If we were predicting the number of doctor visits
    per patient, for example, a prediction of 2.41 visits isn’t really helpful because
    it’s not clear whether it’s two visits or three. What’s worse, a continuous-valued
    predictor may even predict negative values that may be completely meaningless.
    What does -4.7 visits to a doctor even mean? This discussion shows that continuous
    and count-valued targets mean completely different things and should be treated
    differently.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是将计数简单地视为连续值，但这并不总是有效。首先，计数变量的连续值预测并不总是可以有意义地解释。例如，如果我们预测每位患者的就诊次数，那么预测2.41次就诊并不是很有帮助，因为它不清楚是两次还是三次。更糟糕的是，连续值预测器甚至可能预测负值，这可能完全没有意义。医生就诊-4.7次是什么意思？这个讨论表明，连续值和计数值目标意味着完全不同的事情，应该被不同地对待。
- en: First, let’s look at how linear regression fits continuous-valued targets. Figure
    7.2 (left) shows a (noisy) univariate data set, where the continuous-valued label
    (*y*) depends on a single feature (*x*).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看线性回归如何拟合连续值目标。图7.2（左）显示了一个（有噪声的）一元数据集，其中连续值标签(*y*)依赖于单个特征(*x*)。
- en: A linear regression model assumes that for an input *x*, the prediction errors
    or residuals *y* = *f*(*x*) are distributed according to the normal distribution.
    In figure 7.2 (left), we overlay several such normal distributions on the data,
    labels, and linear regression model (the dotted line).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型假设对于一个输入*x*，预测误差或残差*y* = *f*(*x*)是按照正态分布分布的。在图7.2（左）中，我们在数据、标签和线性回归模型（虚线）上叠加了几个这样的正态分布。
- en: To put it simply, linear regression tries to fit a linear model so that the
    residuals have a normal distribution. The normal distribution, also called the
    Gaussian distribution, is a *probability distribution*, or a mathematical description
    of the spread and shape of the possible values a (random) variable can take. As
    we can see in figure 7.2 (right), the normal distribution is a continuous-valued
    distribution and a reasonable choice for continuous-valued labels.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，线性回归试图拟合一个线性模型，使得残差具有正态分布。正态分布，也称为高斯分布，是一种**概率分布**，或者是对一个（随机）变量可能取的值的分布和形状的数学描述。如图7.2（右）所示，正态分布是一个连续值分布，对于连续值标签来说是一个合理的选择。
- en: '![CH07_F02_Kunapuli](../Images/CH07_F02_Kunapuli.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F02_Kunapuli](../Images/CH07_F02_Kunapuli.png)'
- en: Figure 7.2 Linear regression (left) fits continuous-valued targets by assuming
    that the spread of the targets can be modeled by the continuous-valued normal
    distribution (right). More precisely, linear regression assumes that the predictions
    *f*(*x*) for an example x are distributed according to the normal distribution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2（左）的线性回归通过假设目标的分布可以通过连续值正态分布（右）来建模，从而拟合连续值目标。更确切地说，线性回归假设对于示例x的预测*f*(*x*)是按照正态分布分布的。
- en: But what of count data? In figure 7.3, we visualize the difference between our
    data set from figure 7.2 with continuous-valued targets (left) and a second data
    set with count-valued targets (right).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于计数数据呢？在图7.3中，我们可视化了图7.2中连续值目标的数据集（左）和具有计数值目标的数据集（右）之间的差异。
- en: '![CH07_F03_Kunapuli](../Images/CH07_F03_Kunapuli.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F03_Kunapuli](../Images/CH07_F03_Kunapuli.png)'
- en: Figure 7.3 Visualizing the differences between continuous-valued targets (left)
    and count-valued targets (right) shows us that linear regression won’t work well
    because the distribution (spread and shape) of the count labels is quite different
    from that of continuous labels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3（左）和（右）展示了连续值目标（左）和计数值目标（右）之间的差异可视化，这表明线性回归不会很好地工作，因为计数标签的分布（分布和形状）与连续标签的分布相当不同。
- en: We begin to see some rather stark differences between continuous-valued and
    count-valued labels. Intuitively, a regression model designed for continuous targets
    would struggle to build a viable model with count-valued targets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始看到连续值标签和计数值标签之间的一些相当明显的差异。直观上，为连续目标设计的回归模型在构建计数值目标的有效模型时会遇到困难。
- en: 'This is because regression models for continuous targets assume that the residuals
    have a certain shape: the normal distribution. As we’ll see, count-valued targets
    are *not* normally distributed, but, in fact, often follow a Poisson distribution.
    Because count-valued labels are fundamentally different from continuous-valued
    labels, a regression approach designed for continuous-valued labels won’t generally
    work well on count-valued labels.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为针对连续目标的回归模型假设残差具有某种形状：正态分布。正如我们将要看到的，计数值目标并不是正态分布的，但实际上通常遵循泊松分布。由于计数值标签与连续值标签在本质上不同，因此为连续值标签设计的回归方法通常不会很好地适用于计数值标签。
- en: New assumptions for count-valued regression models
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 计数值回归模型的新假设
- en: 'Can we keep the general framework of linear regression, then, but extend it
    to be able to handle count-valued data? We can indeed, with some modeling changes:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否保持线性回归的一般框架，然后将其扩展以能够处理计数值数据？实际上我们可以，通过一些建模变化：
- en: 'We’ll have to change how we link the label (prediction target) to the input
    features. Linear regression relates labels to features through a linear function:
    *y* = *β*[0] + *β*''*x*. For count labels, we’ll introduce a link function *g*(*y*)
    into model *g*(*y*) = *β*[0] + *β*''*x*; in particular, we’ll use the log-link
    function—log(*y*) = *β*[0] + *β*''*x*—or equivalently, by inverting the log as
    *y* = *e*^(*β*[0]+*β*''x). Link functions are often chosen based on two key factors:
    (1) the underlying probability distribution that we think is best suited for the
    data and how it behaves, and (2) task- and application-dependent considerations.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将不得不改变我们将标签（预测目标）与输入特征相连接的方式。线性回归通过线性函数将标签与特征相关联：*y* = *β*[0] + *β*'*x*。对于计数标签，我们将引入链接函数
    *g*(*y*) 到模型 *g*(*y*) = *β*[0] + *β*'*x*；特别是，我们将使用对数链接函数——log(*y*) = *β*[0] +
    *β*'*x*——或者等价地，通过取对数的逆作为 *y* = *e*^(*β*[0]+*β*'x)。链接函数通常基于两个关键因素来选择：（1）我们认为最适合数据及其行为的潜在概率分布，以及（2）任务和应用相关的考虑。
- en: We’ll have to change our assumptions on how we think the predictions *f*(*x*)
    are distributed. Linear regression assumes the normal distribution for continuous-valued
    labels. For count-valued labels, we’ll need the Poisson distribution, which is
    one of several distributions that can be used to model counts.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将不得不改变我们对预测 *f*(*x*) 分布的假设。线性回归假设连续值标签服从正态分布。对于计数值标签，我们需要泊松分布，这是可以用来模拟计数的几种分布之一。
- en: The Poisson distribution is a discrete probability distribution, so it’s well
    suited to handle discrete count-valued labels and expresses the probability of
    how many events can occur in a fixed interval of time. In this case, the log-link
    function is a natural fit for the Poisson distribution and other distributions
    that have an exponential form.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泊松分布是一个离散概率分布，因此非常适合处理离散的计数值标签，并表达了在固定时间间隔内可能发生多少事件的概率。在这种情况下，对数链接函数是泊松分布和其他具有指数形式的分布的自然选择。
- en: 'Figure 7.4 illustrates the need for a log-link function as well as the Poisson
    distribution for developing regression models for count-valued data:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4说明了在开发计数值数据的回归模型时需要对数链接函数以及泊松分布：
- en: Observe the mean (average) trend of the count labels (*y*) in relation to the
    regression data (*x*) in figure 7.4 (left), illustrated by the dashed line. Intuitively,
    this is a gentle exponential trend and shows how the features (*x*) can be linked
    to the labels (*y*).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察图7.4（左）中计数标签（*y*）相对于回归数据（*x*）的平均（平均）趋势，由虚线表示。直观上看，这是一个温和的指数趋势，展示了特征（*x*）如何与标签（*y*）相联系。
- en: Observe how the Poisson distributions overlaid on the visualization model the
    nature of counts (discrete) as well as their spread far better than the normal
    distribution.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察泊松分布在可视化模型上如何更好地模拟计数（离散）的性质以及它们的分布，比正态分布要好得多。
- en: A regression model with these changes allows us to model count-valued targets
    and is appropriately called *Poisson regression*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化后的回归模型使我们能够模拟计数值目标，并适当地称为*泊松回归*。
- en: To recap, Poisson regression still uses a linear model to capture the effect
    of the various input features from the examples. However, it introduces a log-link
    function and the Poisson distribution assumption to effectively model count-labeled
    data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，泊松回归仍然使用线性模型来捕捉示例中的各种输入特征的效果。然而，它引入了对数链接函数和泊松分布假设，以有效地对计数标签数据进行建模。
- en: The Poisson regression approach just described is an extension of ordinary linear
    regression, meaning that it has no regularization. Unsurprisingly, however, we
    can add different regularization terms to induce robustness or sparsity, as we
    saw in section 7.1.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的泊松回归方法是对普通线性回归的扩展，这意味着它没有正则化。然而，不出所料，我们可以添加不同的正则化项以诱导鲁棒性或稀疏性，正如我们在第7.1节中看到的。
- en: '![CH07_F04_Kunapuli](../Images/CH07_F04_Kunapuli.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F04_Kunapuli](../Images/CH07_F04_Kunapuli.png)'
- en: Figure 7.4 Poisson regression (left) fits count-valued targets by assuming that
    the spread of the targets can be modeled by the discrete-valued Poisson distribution
    (right). More precisely, Poisson regression assumes that the predictions *f*(*x*)
    for an example *x* are distributed according to the Poisson distribution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4泊松回归（左）通过假设目标的分布可以通过离散值的泊松分布（右）来模拟，来拟合计数值目标。更精确地说，泊松回归假设示例 *x* 的预测 *f*(*x*)
    是按照泊松分布分布的。
- en: scikit-learn’s implementation of Poisson regression is part of the sklearn.linear_
    model subpackage. It implements Poisson regression with L2 regularization, where
    the effect of regularization can be controlled through the argument alpha.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn对泊松回归的实现是sklearn.linear_模型子包的一部分。它实现了带有L2正则化的泊松回归，其中正则化的效果可以通过alpha参数来控制。
- en: Thus, the hyperparameter alpha is the regularization parameter, analogous to
    the regularization parameter in ridge regression. Setting alpha=0 causes the model
    to learn an unregularized Poisson regressor, which, as with unregularized linear
    regression, can’t handle feature correlations as effectively.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，超参数alpha是正则化参数，类似于岭回归中的正则化参数。设置alpha=0会导致模型学习一个未正则化的泊松回归器，就像未正则化的线性回归一样，不能有效地处理特征相关性。
- en: 'In the following example, we call Poisson regression with alpha=0.01, which
    trains a regression model for count labels and is *also* robust to feature correlations
    in the data:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用alpha=0.01调用泊松回归，这训练了一个用于计数标签的回归模型，并且对数据中的特征相关性具有鲁棒性：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This snippet, executed on the data in figure 7.4 (see the companion Python
    code to generate this data), results in the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.4中的数据上执行此代码片段（请参阅生成此数据的配套Python代码），结果如下：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can train a ridge regression model on this synthetic data set with count-valued
    features. Remember that ridge regression uses the MSE as the loss function, which
    is unsuited for count variables, as shown here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在具有计数特征的这个合成数据集上训练岭回归模型。记住，岭回归使用MSE作为损失函数，这对于计数变量来说是不合适的，如下所示：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 7.1.3 Logistic regression for classification labels
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 逻辑回归用于分类标签
- en: In the previous section, we saw that it’s possible to extend linear regression
    to count-valued labels with an appropriate choice of link function and target
    distribution. What other label types can we handle? Can this idea (of adding link
    functions and introducing other types of distributions) be extended to categorical
    labels? Categorical (or class) labels are used to describe classes in binary classification
    problems (0 or 1) or multiclass classification problems (0, 1, 2).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了可以通过适当选择链接函数和目标分布来将线性回归扩展到计数标签。我们还能处理哪些标签类型？这个想法（添加链接函数和引入其他类型的分布）能否扩展到分类标签？分类（或类别）标签用于描述二元分类问题（0或1）或多类分类问题（0，1，2）中的类别。
- en: 'The question, then, is can we apply a regression framework to a classification
    problem? Amazingly, yes! For simplicity, let’s focus on binary classification,
    where labels can take only two values, 0 or 1:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，问题来了，我们能否将回归框架应用于分类问题？令人惊讶的是，是的！为了简单起见，让我们关注二元分类，其中标签只能取两个值，0或1：
- en: We’ll have to change how we *link* the target label to the input features. For
    class/categorical labels, we use the logit link function *g*(*y*) = ln(*y*/1 –
    *y*). Thus, the model we’ll learn will be ln(*y*/(1 – *y*)) = *β*[0] + *β*’*x*.This
    may seem like a rather arbitrary choice at first, but a slightly deeper look demystifies
    this choice.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将不得不改变如何将目标标签与输入特征*链接*。对于类别/分类标签，我们使用logit链接函数 *g*(*y*) = ln(*y*/1 – *y*)。因此，我们将学习的模型将是
    ln(*y*/(1 – *y*)) = *β*[0] + *β*’*x*。这种选择可能一开始看起来相当随意，但稍微深入一点就可以消除这种选择的神秘性。
- en: First, by inverting the logit function, we have the equivalent link *y* = 1/(1+*e*^(-(*β*[0]+*β*’*x*)))
    between the labels *y* and the data *x*. That is, *y* is modeled with the sigmoid
    function, also known as the *logistic function*! Thus, using the logit link function
    in a regression model turns it into logistic regression, a well-known classification
    algorithm!
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，通过反转logit函数，我们得到了标签*y*与数据*x*之间的等效链接*y* = 1/(1+*e*^(-(*β*[0]+*β*’*x*)))。也就是说，*y*是通过sigmoid函数建模的，也称为*逻辑函数*！因此，在回归模型中使用logit链接函数将其转换为逻辑回归，这是一种众所周知的分类算法！
- en: 'Second, we can think of *y*/(1 – *y*) as a ratio of *y* : (1 - *y*), which
    we interpret as the odds of y being Class 0 to being Class 1\. These odds are
    exactly the same as the odds offered in gambling and betting. The logit link function
    is simply the logarithm of the odds, or *log-odds*. This link function, essentially,
    is providing a measure of likelihood of the class being 0 or 1.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '其次，我们可以将*y*/(1 – *y*)视为*y* : (1 - *y*)的比率，我们将其解释为y属于类别0相对于属于类别1的概率。这些概率与赌博和投注中提供的概率完全相同。logit链接函数仅仅是概率的对数，或称为*对数概率*。这个链接函数本质上提供了类别为0或1的似然度度量。'
- en: Linear regression assumed the normal distribution for continuous-valued labels,
    and Poisson regression assumed the Poisson distribution for count-valued labels.
    Logistic regression assumes the Bernoulli distribution for binary class labels.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归假设连续值标签服从正态分布，泊松回归假设计数值标签服从泊松分布。逻辑回归假设二元类别标签服从伯努利分布。
- en: The Bernoulli, like the Poisson distribution, is another discrete probability
    distribution. However, rather than describing counts of events, the Bernoulli
    distribution models the outcomes of yes/no questions. This is ideally suited for
    the binary classification case, where we ask the question, “Does this example
    belong to Class 0 or Class 1?”
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伯努利分布，就像泊松分布一样，是另一种离散概率分布。然而，伯努利分布并不是描述事件的数量，而是模型化是/否问题的结果。这非常适合二分类情况，其中我们提出问题：“这个例子属于类别0还是类别1？”
- en: Putting all of this together, we visualize logistic regression analogously to
    linear regression or Poisson regression in figure 7.5.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们在图7.5中将逻辑回归类比为线性回归或泊松回归。
- en: Figure 7.5 (left) shows a binary classification data set, where the data has
    only one feature and the targets belong to one of two categories. In this case,
    the binary labels follow the Bernoulli distribution, and the sigmoid link function
    (dotted line) allows us to relate the data (*x*) to the labels (*y*) nicely. Figure
    7.5 (right) shows us a closer look at the Bernoulli distribution.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5（左）显示了一个二元分类数据集，其中数据只有一个特征，目标属于两个类别之一。在这种情况下，二元标签遵循伯努利分布，而Sigmoid链接函数（虚线）使我们能够很好地将数据（*x*）与标签（*y*）联系起来。图7.5（右）展示了伯努利分布的更详细视图。
- en: '![CH07_F05_Kunapuli](../Images/CH07_F05_Kunapuli.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F05_Kunapuli](../Images/CH07_F05_Kunapuli.png)'
- en: Figure 7.5 Logistic regression (left) fits 0/1-valued targets by assuming that
    the spread of the targets can be modeled by the discrete-valued Bernoulli distribution
    (right). Observe how the prediction probabilities (the heights of the bars) of
    Class 0 and Class 1 change with the data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5（左）的逻辑回归通过假设目标值的分布可以由离散值的伯努利分布（右）来建模，拟合0/1值的目标。观察类别0和类别1的预测概率（条形的高度）如何随着数据的变化而变化。
- en: Logistic regression, of course, is one of many different classification algorithms,
    though one with a close connection to regression. This segue into classification
    problems is only intended to highlight the various types of problems the general
    regression framework can handle.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归当然是许多不同的分类算法之一，尽管它与回归有密切的联系。这种过渡到分类问题只是为了强调通用回归框架可以处理的各种类型的问题。
- en: 7.1.4 Generalized linear models
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 广义线性模型
- en: 'The generalized linear model (GLM) framework includes different combinations
    of link functions and probability distributions (and many other models) to create
    problem-specific regression variants. Linear regression, Poisson regression, logistic
    regression, and many other models are all different GLM variants. A (regularized)
    GLM regression model has four components:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 广义线性模型（GLM）框架包括不同组合的链接函数和概率分布（以及许多其他模型），以创建特定问题的回归变体。线性回归、泊松回归、逻辑回归以及许多其他模型都是不同的GLM变体。一个（正则化的）GLM回归模型有四个组成部分：
- en: Probability distribution (formally, from the exponential family of distributions)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率分布（形式上，来自指数分布族）
- en: Linear model *η* = *β*[0] + *β*'*x*
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型 *η* = *β*[0] + *β*'*x*
- en: Link function *g*(*y*) = *η*
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 链接函数 *g*(*y*) = *η*
- en: Regularization function *R*(*β*)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化函数 *R*(*β*)
- en: Why do we care about GLMs? First, they’re obviously a cool modeling approach
    that allows us to handle several different types of regression problems in one
    unified framework. Second, and more importantly, GLMs are often used as weak learners
    in sequential models, especially in many gradient-boosting packages such as XGBoost.
    Third, and most important, GLMs allow us to think about problems in a principled
    manner; in practice, this means that during data set analysis, as we begin to
    get a good sense of the labels and their distribution, we can see which GLM variant
    best suits the problem at hand.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么关心GLM？首先，它们显然是一种酷的建模方法，允许我们在一个统一的框架中处理几种不同类型的回归问题。其次，更重要的是，GLM通常被用作序列模型中的弱学习器，特别是在许多梯度提升包中，如XGBoost。第三，最重要的是，GLM允许我们以原则性的方式思考问题；在实践中，这意味着在数据集分析过程中，当我们开始对标签及其分布有一个良好的感觉时，我们可以看到哪个GLM变体最适合当前的问题。
- en: Table 7.2 shows different GLM variants, link function-distribution combinations,
    and types of labels they’re best suited for. Some of these approaches, such as
    Tweedie regression, may be new to you, and we’ll get into them more in sections
    7.3 and 7.4.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2展示了不同的GLM变体、链接函数-分布组合以及它们最适合的标签类型。其中一些方法，如Tweedie回归，可能对你来说是新的，我们将在第7.3节和第7.4节中更详细地介绍它们。
- en: Table 7.2 GLMs for different types of labels
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2不同类型标签的GLMs
- en: '| Model | Link function | Distribution | Type of label |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 链接函数 | 分布 | 标签类型 |'
- en: '| Linear regression | Identity*g*(*y*) = *y* | Normal | Real-valued |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | Identity*g*(*y*) = *y* | 正态 | 实数值 |'
- en: '| Gamma regression | Negative inverse *g*(*y*) = -(1/*y*) | Gamma | Positive
    real-valued |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Gamma回归 | 负逆 *g*(*y*) = -(1/*y*) | Gamma | 正实数值 |'
- en: '| Poisson regression | Log *g*(*y*) = log(*y*) | Poisson | Counts/occurrences;
    integer-valued |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Poisson回归 | Log *g*(*y*) = log(*y*) | 泊松 | 计数/发生次数；整数值 |'
- en: '| Logistic regression | Logit *g*(*y*) = (*y*/(1-*y*)) | Bernoulli | 0-1; binary
    class labels; yes/no outcomes |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | Logit *g*(*y*) = (*y*/(1-*y*)) | 伯努利 | 0-1；二进制类标签；是/否结果 |'
- en: '| Multiclass logistic regression | Multiclass logit *g*(*y*) = (*y*/(*K*-*y*))
    | Binomial | 0-K; multiclass labels; multichoice outcomes |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 多类逻辑回归 | 多类logit *g*(*y*) = (*y*/(*K*-*y*)) | 二项式 | 0-K；多类标签；多选结果 |'
- en: '| Tweedie regression | Log *g*(*y*) = ln(*y*) | Tweedie | Labels with many
    zeros, right-skewed targets |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Tweedie回归 | Log *g*(*y*) = ln(*y*) | Tweedie | 标签中有很多零，目标向右偏斜 |'
- en: The last method, Tweedie regression, is a particularly important GLM variant
    that is widely used for regression modeling in agriculture, insurance, weather,
    and many other areas.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种方法，Tweedie回归，是一种特别重要的GLM变体，在农业、保险、天气和许多其他领域的回归建模中得到广泛应用。
- en: 7.1.5 Nonlinear regression
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.5 非线性回归
- en: 'Unlike linear regression, where the model to be learned is cast as a weighted
    sum of the features, *f*(*w*) = *w*[0] + *w*[1]*x*[1] + ⋅⋅⋅ + *w*[d]*x*[d], the
    model to be learned in nonlinear regression can be made up of any combination
    of features and functions of features. For example, a polynomial regression model
    of three features can be constructed from weighted combinations of all possible
    feature interactions:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归不同，线性回归中的模型被表示为特征的加权求和，即 *f*(*w*) = *w*[0] + *w*[1]*x*[1] + ⋅⋅⋅ + *w*[d]*x*[d]，而非线性回归中要学习的模型可以由任何特征和特征函数的组合构成。例如，一个由三个特征构成的多项式回归模型可以通过所有可能的特征交互的加权组合来构建：
- en: '![CH07_F05_Kunapuli-eqs-16x](../Images/CH07_F05_Kunapuli-eqs-16x.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F05_Kunapuli-eqs-16x](../Images/CH07_F05_Kunapuli-eqs-16x.png)'
- en: 'From a modeling perspective, nonlinear regression poses two challenges:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从建模的角度来看，非线性回归提出了两个挑战：
- en: '*Which feature combinations should we use?* In the preceding example, with
    three features, we have 2³ = 8 feature combinations, each with its own weight.
    In general, with *d* features, we would have 2^d feature combinations to consider
    and as many weights to learn. Doing this exhaustively can be extremely computationally
    expensive, and even more so since the example doesn’t include any higher-order
    terms (e.g., *x*²[2]*x*[3]), which are often also included to build nonlinear
    models!'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们应该使用哪些特征组合？* 在前面的例子中，有三个特征，我们有2³ = 8种特征组合，每种组合都有自己的权重。一般来说，有*d*个特征，我们将有2^d个特征组合要考虑，以及同样数量的权重要学习。这样做可能会非常耗费计算资源，尤其是在例子中没有包括任何高阶项（例如，*x*²[2]*x*[3]），这些高阶项通常也被包括在内以构建非线性模型！'
- en: '*Which nonlinear functions should we use?* All sorts of functions and combinations
    beyond polynomials are admissible: trigonometric, exponential, logarithmic, and
    many others, as well as many more combinations. Searching through this space of
    functions exhaustively is simply computationally infeasible.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们应该使用哪些非线性函数？* 除了多项式之外，所有类型的函数和组合都是可接受的：三角函数、指数函数、对数函数以及许多其他函数，以及更多的组合。在这个函数空间中全面搜索在计算上是不可行的。'
- en: 'While many different nonlinear regression techniques have been proposed, studied,
    and used, two approaches are especially relevant in the modern context: decision
    trees and neural networks. We’ll discuss them both briefly, though we’ll focus
    more on decision trees as they are the building blocks of most ensemble methods.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经提出了许多不同的非线性回归技术，进行了研究和应用，但在现代背景下，两种方法尤其相关：决策树和神经网络。我们将简要讨论这两种方法，尽管我们将更多地关注决策树，因为它们是大多数集成方法的基础。
- en: Tree-based methods use decision trees to define the space of nonlinear functions
    to explore. During learning, decision trees are grown using the same loss functions
    as described previously, such as the squared loss. Each time a new decision node
    is added, it introduces a new feature interaction/combination into the tree.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的方法使用决策树来定义非线性函数的空间以进行探索。在学习过程中，决策树使用与之前描述相同的损失函数进行生长，例如平方损失。每次添加一个新的决策节点时，都会将一个新的特征交互/组合引入树中。
- en: 'Thus, decision trees induce feature combinations greedily and recursively during
    learning via the loss function as a scoring metric. As the tree grows, its nonlinearity
    (or complexity) also increases. The learning objective of decision trees, then,
    can be written as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，决策树通过损失函数作为评分指标，在学习的贪婪和递归过程中诱导特征组合。随着树的生长，其非线性（或复杂性）也增加。决策树的学习目标可以写成以下形式：
- en: '![CH07_F05_Kunapuli-eqs-18x](../Images/CH07_F05_Kunapuli-eqs-18x.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F05_Kunapuli-eqs-18x](../Images/CH07_F05_Kunapuli-eqs-18x.png)'
- en: 'On the other hand, ANNs use layers of neurons to successively induce increasingly
    complex feature combinations at each layer. The nonlinearity of a neural network
    increases with network depth, which directly influences the number of network
    weights that must be learned:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，ANN使用神经元层来逐层诱导越来越复杂的特征组合。神经网络的非线性随着网络深度的增加而增加，这直接影响必须学习的网络权重的数量：
- en: '![CH07_F05_Kunapuli-eqs-19x](../Images/CH07_F05_Kunapuli-eqs-19x.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F05_Kunapuli-eqs-19x](../Images/CH07_F05_Kunapuli-eqs-19x.png)'
- en: The scikit-learn package provides many nonlinear regression approaches. Let’s
    take a quick look at how we can train decision tree and neural network regressors
    for a simple problem.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn包提供了许多非线性回归方法。让我们快速看一下我们如何为简单问题训练决策树和神经网络回归器。
- en: 'As before, let’s generate a simple, univariate data set to visualize these
    two regression approaches. The data is generated with *f*(*x*) = *e*^(-0.5x)sin
    (1.25π*x* - 1.414), which is the true underlying nonlinear relationship between
    the data *x* and the continuous labels *y*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，让我们生成一个简单的一元数据集来可视化这两种回归方法。数据是通过 *f*(*x*) = *e*^(-0.5x)sin (1.25π*x* -
    1.414) 生成的，这是数据 *x* 和连续标签 *y* 之间真实的潜在非线性关系：
- en: '[PRE11]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Split into train and test sets:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集：
- en: '[PRE12]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, train a decision-tree regressor of maximum depth 5:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练一个最大深度为5的决策树回归器：
- en: '[PRE13]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The learned decision tree function is shown in figure 7.6 (right). A decision
    tree with a univariate (single-variable) split function learns axis-parallel fits,
    which is reflected in the decision-tree model in the figure: the model is made
    up of segments that are parallel to the x- or y-axes.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的决策树函数如图7.6（右）所示。具有一元（单变量）分割函数的决策树学习到轴平行的拟合，这在图中的决策树模型中得到了反映：模型由与x轴或y轴平行的段组成。
- en: '![CH07_F06_Kunapuli](../Images/CH07_F06_Kunapuli.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F06_Kunapuli](../Images/CH07_F06_Kunapuli.png)'
- en: 'Figure 7.6 Left: The true function relating labels to data (solid curve) and
    the generated data samples. Right: Two nonlinear regression models fit to this
    synthetic data set, decision tree and neural network regressors.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 左：与数据相关的真实函数（实线）和生成数据样本。右：拟合到这个合成数据集的两个非线性回归模型，决策树和神经网络回归器。
- en: 'In similar fashion, we can train an ANN for regression, also known as a multilayer
    perceptron (MLP) regressor:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们可以训练一个用于回归的ANN，也称为多层感知器（MLP）回归器：
- en: '[PRE14]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This neural network is made up of three hidden layers, each containing 50 neurons,
    which are specified during network initialization through hidden_layer _sizes=(50,
    50, 50).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络由三个隐藏层组成，每个隐藏层包含50个神经元，这些神经元在通过hidden_layer_sizes=(50, 50, 50)指定网络初始化时被指定。
- en: 'MLPRegressor uses the piecewise-linear rectifier function (*relu*(*x*) = *max*(*x*,0))
    as the activation for each neuron. The regression function learned by the neural
    network is in figure 7.6 (right). Since the neural network activation functions
    were piecewise linear, the final learned neural network model is nonlinear, though
    made up of several linear components (hence, piecewise). Comparing the performance
    of both networks, we see that they are quite similar:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: MLPRegressor使用分段线性整流函数（*relu*(*x*) = *max*(*x*,0)）作为每个神经元的激活函数。神经网络学习的回归函数如图7.6（右）所示。由于神经网络激活函数是分段线性的，因此最终学习的神经网络模型是非线性的，尽管由多个线性组件组成（因此是分段的）。比较两个网络的性能，我们发现它们相当相似：
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Finally, ensemble methods for regression are typically trained nonlinear regression
    models (except with specific choices of base estimators), much like the ones discussed
    in this subsection.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，回归的集成方法通常训练非线性回归模型（除非有特定的基础估计器选择），这与本小节讨论的类似。
- en: 7.2 Parallel ensembles for regression
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 平行回归集成
- en: In this section, we revisit parallel ensembles, both homogeneous (chapter 2)
    and heterogeneous (chapter 3), and see how they can be applied to regression problems.
    Before we dive into how, let’s refresh ourselves on how parallel ensembles work.
    Figure 7.7 illustrates a generic parallel ensemble, where base estimators are
    regressors.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了平行集成，包括同质（第2章）和异质（第3章）集成，并探讨它们如何应用于回归问题。在我们深入探讨之前，让我们先了解一下平行集成的工作原理。图7.7说明了通用平行集成，其中基础估计器是回归器。
- en: '![CH07_F07_Kunapuli](../Images/CH07_F07_Kunapuli.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F07_Kunapuli](../Images/CH07_F07_Kunapuli.png)'
- en: Figure 7.7 Parallel ensembles train multiple base estimators independently of
    each other and then combine their predictions into a joint ensemble prediction.
    Parallel regression ensembles simply use regression algorithms such as decision-tree
    regression as base-learning algorithms.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7展示了平行集成独立于彼此训练多个基础估计器，然后将它们的预测组合成一个联合集成预测。平行回归集成简单地使用回归算法，如决策树回归作为基础学习算法。
- en: Parallel ensemble methods train each component estimator independently of the
    others, which means that they can be trained in parallel. Parallel ensembles typically
    use strong learners, or high-complexity, high-fit learners as base learners. This
    is in contrast to sequential ensembles, which typically use weak learners, or
    low-complexity, low-fit learners as base learners.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 平行集成方法独立于其他组件估计器训练每个组件估计器，这意味着它们可以并行训练。平行集成通常使用强学习器，或高复杂度、高拟合度学习器作为基础学习器。这与通常使用弱学习器，或低复杂度、低拟合度学习器作为基础学习器的顺序集成形成对比。
- en: 'As with all ensemble methods, ensemble diversity among the component base estimators
    is the key. Parallel ensembles achieve this in two ways:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有集成方法一样，组件基础估计器之间的集成多样性是关键。平行集成通过两种方式实现这一点：
- en: '*Homogeneous ensembles*—The base-learning algorithm is fixed, but the training
    data is randomly subsampled to induce ensemble diversity. In section 7.2.1, we
    look at two such approaches: random forest and Extra Trees.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同质集成*——基础学习算法保持不变，但训练数据被随机子采样以诱导集成多样性。在第7.2.1节中，我们探讨了两种这样的方法：随机森林和Extra Trees。'
- en: '*Heterogeneous ensembles*—The base-learning algorithm is changed for diversity,
    while the training data is fixed. In sections 7.2.2 and 7.2.3, we look at two
    such approaches: fusing base estimator predictions with a combining function (or
    aggregator) and stacking base estimator predictions by learning a second-level
    estimator (or meta-estimator).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异质集成*——为了多样性而改变基础学习算法，而训练数据保持不变。在第7.2.2节和7.2.3节中，我们探讨了两种这样的方法：将基础估计器的预测与组合函数（或聚合器）融合，以及通过学习二级估计器（或元估计器）来堆叠基础估计器的预测。'
- en: We focus on a problem with continuous-valued labels called AutoMPG, which is
    a popular regression data set that is often used as a benchmark to evaluate regression
    methods. The regression task is to predict the fuel efficiency of various car
    models or miles per gallon (MPG). The features consist of various engine-related
    attributes such as number of cylinders, displacement, horsepower, weight, and
    acceleration. The data set is available from the UCI Machine Learning Repository
    ([http://mng.bz/Y6Yo](http://mng.bz/Y6Yo)), as well as with the source code for
    this book.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注一个具有连续值标签的问题，称为AutoMPG，这是一个流行的回归数据集，常被用作评估回归方法的基准。回归任务是预测各种汽车模型的燃油效率或每加仑英里数（MPG）。特征包括各种与发动机相关的属性，如气缸数、排量、马力、重量和加速度。数据集可以从UCI机器学习仓库（[http://mng.bz/Y6Yo](http://mng.bz/Y6Yo)）以及本书的源代码中获得。
- en: Listing 7.2 shows how to load the data and split it into training and test sets.
    The listing also includes a preprocessing step, where the data is centered and
    rescaled so that each feature has a mean of 0 and standard deviation of 1\. This
    step, called normalization or standardization, ensures that all the features are
    in the same range of values and improves the performance of downstream learning
    algorithms.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.2展示了如何加载数据并将其分为训练集和测试集。列表还包括一个预处理步骤，其中数据被居中和缩放，使得每个特征的平均值为0，标准差为1。这一步称为归一化或标准化，确保所有特征都在相同的数值范围内，并提高了下游学习算法的性能。
- en: Listing 7.2 Loading and preprocessing the AutoMPG data set
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列出7.2 加载和预处理AutoMPG数据集
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Loads the data set using pandas
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用pandas加载数据集
- en: ❷ Gets column indices for labels and features
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取标签和特征的列索引
- en: ❸ Splits the data set into train and test sets
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据集分为训练集和测试集
- en: '❹ Data preprocessing: normalize training and test data and labels.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 数据预处理：归一化训练和测试数据以及标签。
- en: ❺ Further split train and test data into Xtrn, Xtst (features) and ytrn, ytst
    (labels).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 进一步将训练和测试数据分为Xtrn, Xtst（特征）和ytrn, ytst（标签）。
- en: We’ll be using this data set as a running example for this and the next section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用此数据集作为本节和下一节的运行示例。
- en: 7.2.1 Random forests and Extra Trees
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 随机森林和Extra Trees
- en: 'Homogeneous parallel ensembles are some of the oldest ensemble methods and
    are generally variants of *bagging*. Chapter 2 introduced homogeneous ensemble
    methods in the context of classification. To recap, each base estimator in a parallel
    ensemble method such as bagging can be trained *independently* using the following
    steps:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 同质并行集成是一些最古老的集成方法，通常是*袋装*的变体。第二章在分类的背景下介绍了同质集成方法。为了回顾，并行集成方法（如袋装）中的每个基估计器可以独立使用以下步骤进行训练：
- en: Generate a bootstrap sample (by sampling with replacement, which means an example
    can be sampled multiple times) from the original data set.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始数据集中生成一个自助样本（通过有放回地采样，这意味着一个示例可以被多次采样）。
- en: Fit a base estimator to the bootstrap sample; since each bootstrap sample will
    be different, the base estimators will be diverse.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将基估计器拟合到自助样本；由于每个自助样本都将不同，基估计器将是多样化的。
- en: We can follow the same for regression ensembles. The only difference is in how
    the individual base estimator predictions are aggregated. For classification,
    we use majority voting; for regression, we use the mean (essentially, the average
    prediction), though others (e.g., the median) can also be used.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对回归集合采取同样的方法。唯一的区别在于如何聚合单个基估计器的预测。对于分类，我们使用多数投票；对于回归，我们使用平均值（本质上，是平均预测），尽管也可以使用其他方法（例如，中位数）。
- en: NOTE Each base estimator in bagging is a fully trained strong estimator; therefore,
    if the bagging ensemble contains 10 base regressors, it will take 10 times as
    long to train. Of course, this training procedure can be parallelized over multiple
    CPU cores; however, the overall computational resources needed for full-blown
    bagging is often prohibitive.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：袋装中的每个基估计器都是一个完全训练的强估计器；因此，如果袋装集成包含10个基回归器，则训练时间将是10倍。当然，此训练过程可以并行化到多个CPU核心；然而，全功能袋装所需的总体计算资源通常具有威慑力。
- en: 'As bagging can be rather computationally expensive to train, two important
    tree-based and randomized variants are used:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于袋装方法在训练时可能相当计算密集，因此使用了两种重要的基于树和随机化的变体：
- en: '*Random forest*—This is essentially bagging with *randomized decision trees*
    as base estimators. In other words, random forests perform bootstrap sampling
    to generate a training subset (exactly like bagging), and then use randomized
    decision trees as base estimators.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机森林*—这本质上是以*随机决策树*作为基估计器的袋装。换句话说，随机森林执行自助采样以生成训练子集（就像袋装一样），然后使用随机决策树作为基估计器。'
- en: Randomized decision trees are trained using a modified decision-tree learning
    algorithm, which introduces randomness when growing trees. Specifically, instead
    of considering all the features to identify the best split, a *r*andom subset
    of features is evaluated to identify the best feature to split on.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用修改后的决策树学习算法训练随机决策树，该算法在生长树时引入随机性。具体来说，不是考虑所有特征来识别最佳分裂，而是评估一个*随机*特征子集以识别最佳的分裂特征。
- en: '*Extra Trees (extremely randomized trees)*—These randomized trees take the
    idea of randomized decision trees to the extreme by selecting not just the splitting
    variable from a random subset of features but also the splitting threshold. This
    extreme randomization is so effective, in fact, that we can construct an ensemble
    of extremely randomized trees directly from the original data set without bootstrap
    sampling!'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Extra Trees（极端随机树）*—这些随机树将随机决策树的想法推向了极致，不仅从特征随机子集中选择分裂变量，还选择分裂阈值。这种极端随机化实际上非常有效，以至于我们可以直接从原始数据集构建极端随机树集合，而无需进行自助采样！'
- en: Randomization has two important and beneficial consequences. One, as we expect,
    is that it improves training efficiency and reduces the computational requirements.
    The other is that it improves ensemble diversity! Random forests and Extra Trees
    can be adapted to regression by modifying the underlying learning algorithm to
    train regression trees to make continuous-valued predictions rather than classification
    trees.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 随机化有两个重要且有益的后果。一方面，正如我们所期望的，它提高了训练效率并减少了计算需求。另一方面，它提高了集成多样性！随机森林和Extra Trees可以通过修改底层学习算法来适应回归，使其训练回归树进行连续值预测而不是分类树。
- en: Regression trees use different splitting criteria during training compared to
    classification trees. In principle, any loss function for regression can be used
    as the splitting criterion. However, two commonly implemented splitting criteria
    are mean squared error (MSE) and mean absolute error (MAE). We’ll look at other
    loss functions for regression in section 7.3.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类树相比，回归树在训练过程中使用不同的分割标准。原则上，任何回归的损失函数都可以用作分割标准。然而，两种常见的分割标准是均方误差（MSE）和平均绝对误差（MAE）。我们将在第7.3节中查看其他回归的损失函数。
- en: 'Listing 7.3 shows how we can use scikit-learn’s RandomForestRegressor and ExtraTreesRegressor
    to train regression ensembles for the AutoMPG data set. Two versions of each method
    are trained: one using MSE and one using MAE as the training criteria.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3展示了我们如何使用scikit-learn的RandomForestRegressor和ExtraTreesRegressor来训练AutoMPG数据集的回归集成。每种方法都训练了两个版本：一个使用MSE作为训练标准，另一个使用MAE作为训练标准。
- en: Listing 7.3 Random forest and Extra Trees for regression
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.3 随机森林和Extra Trees回归
- en: '[PRE17]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Initializes ensembles
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化集成
- en: ❷ Creates data structures to store model predictions and evaluation results
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建数据结构以存储模型预测和评估结果
- en: ❸ Trains the ensemble
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练集成
- en: ❹ Gets ensemble predictions on both train and test sets
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取训练集和测试集上的集成预测
- en: ❺ Evaluates train and test set performance with MAE and MSE
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用MAE和MSE评估训练集和测试集的性能
- en: ❻ Saves results
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 保存结果
- en: 'All models are also evaluated using MSE and MAE as the evaluation criteria.
    These evaluation metrics are added to the results variable:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型也使用MSE和MAE作为评估标准进行评估。这些评估指标被添加到结果变量中：
- en: '[PRE18]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding example, we used the default parameter settings for both RandomForestRegressor
    and ExtraTreesRegressor. For instance, each trained ensemble is of size 100 as
    n_estimators=100, by default.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用了RandomForestRegressor和ExtraTreesRegressor的默认参数设置。例如，每个训练的集成大小默认为100，因为n_estimators默认为100。
- en: As with any other machine-learning algorithm, we have to identify the best model
    hyperparameters (e.g., n_estimators) through a grid search or randomized search.
    There are several examples of this in the case study in section 7.4.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他机器学习算法一样，我们必须通过网格搜索或随机搜索来识别最佳模型超参数（例如，n_estimators）。在第7.4节中的案例研究中有几个这样的例子。
- en: 7.2.2 Combining regression models
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 结合回归模型
- en: Another classic ensembling approach, especially when we have different types
    of models, is to simply combine their predictions. This is essentially one of
    the simplest heterogeneous parallel ensembling approaches.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种经典的集成方法，尤其是在我们拥有不同类型的模型时，就是简单地结合它们的预测。这本质上是最简单的异构并行集成方法之一。
- en: Why combine regression models? It’s quite common, during the data exploration
    phase, to experiment with different machine-learning algorithms. This means that
    we often have several different models available to us for ensembling. For example,
    in section 7.2.1, we trained four different regression models. Because we have
    the predictions of four different models, we can happily combine them into one
    ensemble prediction—but what combination functions should we use?
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要结合回归模型？在数据探索阶段，尝试不同的机器学习算法是很常见的。这意味着我们通常有多个不同的模型可供集成。例如，在第7.2.1节中，我们训练了四个不同的回归模型。因为我们有四个不同模型的预测，我们可以愉快地将它们组合成一个集成预测——但是我们应该使用什么组合函数呢？
- en: '*For continuous-valued targets*—Use combining functions/aggregators such as
    weighted mean, median, min, or max. In particular, the median is especially effective
    when combining heterogenous predictions where the models are in greater disagreement.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于连续值目标*——使用加权平均、中位数、最小值或最大值等组合函数/聚合器。特别是，当结合异构预测且模型之间差异较大时，中位数特别有效。'
- en: For example, if we have five models in the ensemble predicting [0.29, 0.3, 0.32,
    0.35, 0.85], then most of the models agree, though there is one outlier with 0.85\.
    The mean of these predictions is 0.42, while the median is 0.32\. Thus, the median
    tends to discard the influence of the outliers (and behaves similarly to majority
    voting), while the mean tends to include them. This is because the median is simply
    (and literally) the middle value, while the mean is the averaged value.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们有五个模型组成的集成预测值为[0.29, 0.3, 0.32, 0.35, 0.85]，那么大多数模型意见一致，尽管有一个异常值0.85。这些预测的平均值为0.42，而中位数为0.32。因此，中位数倾向于忽略异常值的影响（并且行为类似于多数投票），而平均值则倾向于包含它们。这是因为中位数仅仅是（字面上）中间的值，而平均值是平均值。
- en: '*For count-valued targets*—Use combining functions/aggregators such as mode
    and median. We can think of the mode, in particular, as the generalization of
    majority voting to counts. The mode is simply the most common answer.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对于计数值目标*——使用如众数和中位数之类的组合函数/聚合器。我们可以特别将众数视为将多数投票推广到计数。众数仅仅是出现最频繁的答案。'
- en: For example, if we have five models in the ensemble predicting [12, 15, 15,
    15, 16], the mode is 15\. If there are conflicts, with equal counts we can use
    random selection to break ties.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们有五个模型组成的集成预测值为[12, 15, 15, 15, 16]，众数是15。如果有冲突，在计数相等的情况下，我们可以使用随机选择来打破平局。
- en: 'Listing 7.4 illustrates the use of four simple aggregators for continuous-valued
    data. In this listing, we use the four regressors trained in listing 7.3 as the
    (heterogeneous) base estimators whose values we’ll combine: RandomForestRegressor
    and ExtraTreesRegressor, each trained with MSE and MAE as the loss function/split
    criteria.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4说明了使用四个简单的聚合器对连续值数据进行处理的用法。在这个列表中，我们使用列表7.3中训练的四个回归器作为（异构的）基础估计器，我们将组合它们的值：RandomForestRegressor和ExtraTreesRegressor，每个都使用MSE和MAE作为损失函数/分割标准。
- en: Listing 7.4 Aggregators for continuous-valued labels
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4 连续值标签的聚合器
- en: '[PRE19]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Different combining functions for continuous-valued predictions
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 连续值预测的不同组合函数
- en: ❷ Data structure model predictions and evaluation results
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 数据结构模型预测和评估结果
- en: ❸ Collects predictions of the four ensembles trained in listing 7.3
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 收集列表7.3中训练的四个集成模型的预测结果
- en: ❹ Aggregates predictions of the four ensembles trained in listing 7.3
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 聚合列表7.3中训练的四个集成模型的预测结果
- en: ❺ Collects and saves results
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 收集并保存结果
- en: 'Again, all models are also evaluated using MSE and MAE as the evaluation criteria.
    These evaluation metrics are added to the results variable:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，所有模型也使用MSE和MAE作为评估标准进行评估。这些评估指标被添加到结果变量中：
- en: '[PRE20]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 7.2.3 Stacking regression models
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 堆叠回归模型
- en: Another way to combine the predictions of different (heterogeneous) regressors
    is through stacking or meta-learning. Instead of making up a function ourselves
    (e.g., the mean or median), we train a second-level model to learn how to combine
    the predictions of the base estimators. This second-level regressor is known as
    the meta-learner or the meta-estimator.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 结合不同（异构）回归器预测的另一种方式是通过堆叠或元学习。我们不是自己编写一个函数（例如，平均值或中位数），而是训练一个二级模型来学习如何组合基础估计器的预测。这个二级回归器被称为元学习器或元估计器。
- en: The meta-estimator is often a nonlinear model that can effectively combine the
    predictions of the base estimators in a nonlinear manner. The price we pay for
    this added complexity is that stacking can often overfit, especially in the presence
    of noisy data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 元估计器通常是一个非线性模型，可以有效地以非线性方式组合基础估计器的预测。我们为此增加的复杂性所付出的代价是，堆叠往往容易过拟合，尤其是在存在噪声数据的情况下。
- en: To guard against overfitting, stacking is often combined with k-fold CV such
    that each base estimator isn’t trained on the exact same data set. This often
    leads to more diversity and robustness, while decreasing the chances of overfitting.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过拟合，堆叠通常与k折交叉验证结合使用，这样每个基础估计器就不会在完全相同的数据集上训练。这通常会导致更多样化和鲁棒性，同时降低过拟合的可能性。
- en: In chapter 3, listing 3.1, we implemented a stacking model for classification
    from scratch. An alternate implementation uses scikit-learn’s StackingClassifier
    and StackingRegressor. This is illustrated for regression problems in listing
    7.5.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章，列表3.1中，我们从零开始实现了一个用于分类的堆叠模型。另一种实现方式是使用scikit-learn的StackingClassifier和StackingRegressor。这在列表7.5中的回归问题中得到了说明。
- en: 'Here, we train four nonlinear regressors: kernel ridge regression (a nonlinear
    extension of ridge regression), support vector regression, k-nearest neighbor
    regression, and Extra Trees. We use an ANN as a meta-learner, which allows us
    to combine predictions of various heterogeneous regression models in a learnable
    and highly nonlinear fashion.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们训练了四种非线性回归器：核岭回归（岭回归的非线性扩展）、支持向量回归、k-最近邻回归和Extra Trees。我们使用一个人工神经网络（ANN）作为元学习器，这使得我们能够以可学习和高度非线性的方式结合各种异构回归模型的预测。
- en: Listing 7.5 Stacking regression models
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 堆叠回归模型
- en: '[PRE21]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Initializes first-level (base) regressors
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化第一级（基础）回归器
- en: ❷ Initializes second-level (meta) regressor
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化第二级（元）回归器
- en: ❸ Trains a stacking regressor with 3-fold CV
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用3折交叉验证训练堆叠回归器
- en: ❹ Computes train and test errors
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算训练和测试误差
- en: 'The stacking regression produces the following output:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠回归产生以下输出：
- en: '[PRE22]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It should be noted here that default parameters were used with the individual
    base regressors. The performance of this stacking ensemble can further be improved
    with effective hyperparameter tuning of the base estimator models, which improves
    the performance of each ensemble component and hence the ensemble overall.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意的是，这里使用了各个基础回归器的默认参数。通过有效调整基础估计器模型的超参数，可以进一步提高这种堆叠集成的方法的性能，从而提高每个集成组件以及整个集成的性能。
- en: 7.3 Sequential ensembles for regression
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 用于回归的顺序集成
- en: In this section, we revisit sequential ensembles, specifically gradient boosting
    (with LightGBM; refer to chapter 5) and Newton boosting (with XGBoost; refer to
    chapter 6), and see how they can be adapted to regression problems.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾顺序集成，特别是梯度提升（使用LightGBM；参见第5章）和牛顿提升（使用XGBoost；参见第6章），并探讨它们如何适应回归问题。
- en: Both of these approaches are very general in that they can be trained on a wide
    variety of loss functions. This means they can easily be adapted to different
    types of problem settings, allowing for problem-specific modeling of continuous-valued
    and count-valued labels. Before we dive into how, let’s refresh ourselves on how
    sequential ensembles work. Figure 7.8 illustrates a generic sequential ensemble
    where base estimators are regressors. Unlike parallel ensembles, sequential ensembles
    grow the ensemble one estimator at a time, where successive estimators aim to
    improve on the predictions of the previous ones.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都非常通用，因为它们可以在广泛的损失函数上训练。这意味着它们可以很容易地适应不同类型的问题设置，允许对连续值和计数值标签进行特定问题的建模。在我们深入探讨如何之前，让我们先了解一下顺序集成的工作原理。图7.8说明了具有回归器作为基础估计器的通用顺序集成。与并行集成不同，顺序集成一次只增长一个估计器，连续的估计器旨在改进前一个估计器的预测。
- en: '![CH07_F08_Kunapuli](../Images/CH07_F08_Kunapuli.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F08_Kunapuli](../Images/CH07_F08_Kunapuli.png)'
- en: Figure 7.8 Unlike parallel ensembles that train base estimators *independently*
    of each other, sequential ensembles, such as boosting, train successive base estimators
    stagewise to identify and minimize the errors made by the previous base estimator.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 与并行集成不同，并行集成在训练基础估计器时是 *独立* 的，而顺序集成，如提升（boosting），是分阶段训练连续的基础估计器，以识别和最小化前一个基础估计器所犯的错误。
- en: Each successive base estimator uses the *residual* as a means of identifying
    which training examples need attention in the current iteration. In regression
    problems, the residual tells the base estimator how much the model is underestimating
    or overestimating the prediction (see figure 7.9).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 每个连续的基础估计器使用 *残差* 作为识别当前迭代中需要关注的训练示例的手段。在回归问题中，残差告诉基础估计器模型低估或高估预测的程度（参见图7.9）。
- en: '![CH07_F09_Kunapuli](../Images/CH07_F09_Kunapuli.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F09_Kunapuli](../Images/CH07_F09_Kunapuli.png)'
- en: Figure 7.9 A linear regression model and its predictions (squares) fit to a
    data set (circles). The residuals are a measure of the *error* between the true
    label (*y*[i]) and predicted label *f*(*x*[i]). The size of the residual of each
    training example indicates the extent of the error in fitting, while the sign
    of the residual indicates whether the model is underestimating or overestimating.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 一个线性回归模型及其预测（正方形）拟合到数据集（圆形）。残差是真实标签（*y*[i]）和预测标签 *f*(*x*[i]）之间 *误差* 的度量。每个训练示例的残差大小表示拟合误差的程度，而残差的符号表示模型是低估还是高估。
- en: 'More concretely, regression residuals convey two important pieces of information
    to the base learners. For each training example, the magnitude of the residual
    can be interpreted in a straightforward manner: bigger residuals mean more errors.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，回归残差向基学习器传达了两个重要的信息。对于每个训练示例，残差的幅度可以以直接的方式解释：更大的残差意味着更多的错误。
- en: The sign of the residual also conveys important information. A positive residual
    suggests that the current model’s prediction is *underestimating* the true value;
    that is, the model has to increase its prediction. A negative residual suggests
    that the current model’s prediction is *overestimating* the true value; that is,
    the model has to decrease its prediction.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 残差的符号也传达了重要信息。正残差表明当前模型的预测是*低估*真实值；也就是说，模型必须增加其预测。负残差表明当前模型的预测是*高估*真实值；也就是说，模型必须减少其预测。
- en: The loss function and, more importantly, its derivatives allow us to measure
    the residual between the current model’s prediction and the true label. By changing
    the loss function, we’re essentially changing how we prioritize different examples.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数及其导数允许我们测量当前模型预测与真实标签之间的残差。通过改变损失函数，我们实际上是在改变我们优先考虑不同示例的方式。
- en: 'Both gradient boosting and Newton boosting use shallow regression trees as
    weak base learners. Weak learners (contrast with bagging and its variants, which
    use strong learners) are essentially low-complexity, low-fit models. By training
    a sequence of weak learners to fix the mistakes of the previously learned weak
    learners, both methods boost the performance of the ensemble in stages:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升和牛顿提升都使用浅层回归树作为弱基学习器。弱学习器（与使用强学习器的bagging及其变体相对比）本质上是非常低复杂度、低拟合度的模型。通过训练一系列弱学习器来纠正先前学习到的弱学习器的错误，这两种方法都在各个阶段提升了集成性能。
- en: '*Gradient boosting*—Uses the negative gradient of the loss function as the
    residual to identify training examples to focus on.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*梯度提升*——使用损失函数的负梯度作为残差来识别需要关注的训练示例。'
- en: '*Newton boosting*—Uses Hessian-weighted gradients of the loss function as the
    residual to identify training examples to focus on. The Hessians (second derivatives)
    of the loss functions incorporate local “curvature” information to increase the
    weight on training examples with higher loss values.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*牛顿提升*——使用损失函数的Hessian加权梯度作为残差来识别需要关注的训练示例。损失函数的Hessian（二阶导数）包含了局部“曲率”信息，以增加具有更高损失值的训练示例的权重。'
- en: Loss functions, then, are a key ingredient in developing effective sequential
    ensembles.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，损失函数是开发有效序列集成的一个关键组成部分。
- en: 7.3.1 Loss and likelihood functions for regression
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 回归的损失和似然函数
- en: 'In this section, we’ll take a look at some common (and uncommon) loss functions
    for different types of labels: continuous-valued, continuous-valued but positive,
    and count-valued. Each of these loss functions penalizes errors differently and
    will result in learning models with different properties, much like how different
    regularization functions produced models with different properties (in section
    7.1).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨不同类型标签（连续值、连续值但为正数和计数值）的一些常见（和不常见）的损失函数。每个损失函数对错误的惩罚方式不同，将导致具有不同特性的学习模型，这与不同正则化函数产生具有不同特性的模型（在7.1节中）类似。
- en: Many loss functions are ultimately derived from how we assume the residuals
    are distributed. We’ve already seen this in section 7.1, where we assume that
    the residuals of continuous-valued targets can be modeled using the Gaussian distribution,
    count-valued targets can be modeled using the Poisson distribution, and so on.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 许多损失函数最终是从我们如何假设残差分布得来的。我们已经在7.1节中看到了这一点，其中我们假设连续值目标的残差可以用高斯分布来建模，计数值目标可以用泊松分布来建模，等等。
- en: Here, we formalize that notion. Note that some loss functions don’t have a closed-form
    expression. In such cases, it’s useful to visualize the *negative log of the underlying
    distribution.* This term, called the *negative log-likelihood*, is sometimes optimized
    instead of the loss function and ultimately has the same effect in the final model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正式化这个概念。请注意，一些损失函数没有封闭形式的表达式。在这种情况下，可视化*底层分布的负对数*是有用的。这个术语称为*负对数似然*，有时它被优化而不是损失函数，最终在最终模型中具有相同的效果。
- en: We consider three types of labels and their corresponding loss functions. These
    are visualized in figure 7.10.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑三种类型的标签及其相应的损失函数。这些在图7.10中进行了可视化。
- en: '![CH07_F10_Kunapuli](../Images/CH07_F10_Kunapuli.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F10_Kunapuli](../Images/CH07_F10_Kunapuli.png)'
- en: 'Figure 7.10 Loss and log-likelihood functions for three different types of
    targets: continuous-valued (left), positive continuous-valued (center), and count-valued
    (right)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 三种不同类型目标的损失和log-似然函数：连续值（左），正连续值（中），和计数值（右）
- en: Continuous-valued labels
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 连续值标签
- en: 'There are several well-known loss functions for continuous-valued targets.
    Two of the most common are as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续值目标，存在几个著名的损失函数。以下是最常见的两个：
- en: Squared error (SE), 1/2⋅(*y* – *f*(*x*))²,—Directly corresponds to assuming
    a Gaussian distribution over the residuals
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平方误差（SE），1/2⋅(*y* – *f*(*x*))²——直接对应于假设残差上的高斯分布
- en: Absolute error (AE), |*y* - *f*(*x*)|—Corresponds to assuming the Laplacian
    distribution over the residuals
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对误差（AE），|*y* - *f*(*x*)|——对应于假设残差上的拉普拉斯分布
- en: The SE penalizes errors far more heavily than the AE, as is evident from the
    loss values at the extremes in figure 7.10\. This makes the SE highly sensitive
    to outliers. The SE is also a doubly differentiable loss function, which means
    that we can compute both the first and second derivatives. Thus, we can use it
    for both gradient boosting (which uses residuals) and Newton boosting (which uses
    Hessian-boosted residuals). The AE isn’t doubly differentiable, meaning it can’t
    be used in Newton boosting.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: SE对错误的惩罚远比AE严重，如图7.10中的损失值在极端情况所示。这使得SE对异常值非常敏感。SE也是一个双可微的损失函数，这意味着我们可以计算一阶和二阶导数。因此，我们可以将其用于梯度提升（使用残差）和牛顿提升（使用Hessian提升的残差）。AE不是双可微的，这意味着它不能用于牛顿提升。
- en: 'The Huber loss is a hybrid of the SE and the AE and switches its behavior between
    the two at some user-specified threshold τ:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: Huber损失是SE和AE的混合体，并在某个用户指定的阈值τ之间切换其行为：
- en: '![CH07_F10_Kunapuli-eqs-21x](../Images/CH07_F10_Kunapuli-eqs-21x.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F10_Kunapuli-eqs-21x](../Images/CH07_F10_Kunapuli-eqs-21x.png)'
- en: For residuals smaller than τ, the Huber loss behaves like the SE, and beyond
    the threshold, it behaves like the scaled AE (refer to figure 7.10). This makes
    the Huber loss ideal in situations where we desire to limit the influence of outliers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小于τ的残差，Huber损失的行为类似于SE，超过阈值，它表现为缩放后的AE（参见图7.10）。这使得Huber损失在希望限制异常值影响的情况下非常理想。
- en: 'Note that the Huber loss can’t be directly used with Newton boosting as it
    contains the AE as one of its components. For this reason, Newton-boosting implementations
    use a smooth approximation called the *pseudo-Huber loss*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于Huber损失包含AE作为其组成部分之一，因此不能直接与牛顿提升法一起使用。因此，牛顿提升法的实现使用了一个平滑近似，称为*伪Huber损失*：
- en: '![CH07_F10_Kunapuli-eqs-22x](../Images/CH07_F10_Kunapuli-eqs-22x.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F10_Kunapuli-eqs-22x](../Images/CH07_F10_Kunapuli-eqs-22x.png)'
- en: The pseudo-Huber loss behaves like the Huber loss, though it’s an approximate
    version that outputs 1/2⋅(*y* – *f*(*x*))² for residuals (*y* - *f*(*x*)) close
    to zero.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 伪Huber损失的行为类似于Huber损失，尽管它是一个近似版本，对于接近零的残差（*y* - *f*(*x*)），它输出1/2⋅(*y* – *f*(*x*))²。
- en: Continuous-valued positive labels
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 连续值正标签
- en: In some domains, such as insurance claims analytics, the target labels that
    we want to predict only take positive values. For example, the claim amount is
    continuous-valued but can only be positive.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些领域，例如保险索赔分析，我们想要预测的目标标签只取正值。例如，索赔金额是连续值，但只能为正。
- en: In such situations, where the Gaussian distribution isn’t appropriate, we can
    use the *gamma distribution*. The gamma distribution is a highly flexible distribution
    that can fit many target distribution shapes. This makes it ideally suited for
    modeling problems where the target distributions have long tails—that is, outliers
    that can’t be ignored.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当高斯分布不合适时，我们可以使用*伽马分布*。伽马分布是一个高度灵活的分布，可以拟合许多目标分布形状。这使得它非常适合建模问题，其中目标分布具有长尾——即不能忽略的异常值。
- en: The gamma distribution doesn’t correspond to a closed-form loss function. Shown
    earlier in figure 7.10 (center), we plot the negative log-likelihood instead,
    which functions as a surrogate loss function.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 伽马分布不对应于一个封闭形式的损失函数。如图7.10（中心）所示，我们之前绘制的是负对数似然，它充当了一个代理损失函数。
- en: First, observe that the loss function is only defined for positive real values
    (x-axis). Next, observe how the log-likelihood function only gently penalizes
    errors to the further right. This allows the underlying models to fit to right-skewed
    data.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，观察损失函数仅对正实数值（x 轴）有定义。接下来，观察对数似然函数如何仅对更右侧的错误进行轻微惩罚。这允许基础模型拟合右偏斜的数据。
- en: Count-valued labels
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 计数值标签
- en: Beyond continuous-valued labels, some regression problems require us to fit
    count-valued targets. We’ve already seen examples of this in section 7.1, where
    we learned that counts, which are discrete-valued, can be modeled using the Poisson
    distribution.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 除了连续值标签之外，一些回归问题需要我们拟合计数值目标。我们已经在第 7.1 节中看到了这样的例子，我们了解到计数（离散值）可以使用泊松分布进行建模。
- en: Like the gamma distribution, the Poisson distribution also doesn’t correspond
    to a closed-form loss function. Figure 7.10 (right) illustrates the negative log-likelihood
    of the Poisson distribution, which can be used to build regression models (called
    Poisson regression).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与伽马分布一样，泊松分布也不对应于闭式损失函数。图 7.10（右）说明了泊松分布的负对数似然，它可以用于构建回归模型（称为泊松回归）。
- en: Hybrid labels
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 混合标签
- en: In some problems, the underlying labels can’t be modeled by a single distribution.
    For example, in weather analytics, if we want to model rainfall, we can expect
    that (1) on most days, we’ll have no rain at all; (2) on some days, we’ll have
    varying degrees of rainfall; and (3) on some rare occasions, we’ll have very heavy
    rainfall.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些问题中，基础标签不能由单个分布来建模。例如，在天气分析中，如果我们想建模降雨，我们可以预期（1）在大多数日子里，我们根本不会下雨；（2）在某些日子里，会有不同程度的降雨；（3）在少数情况下，会有非常严重的降雨。
- en: Figure 7.11 shows the distribution of rainfall data, where we have a big “point
    mass” or spike at 0 (corresponding to most days that receive no rainfall). In
    addition, this distribution is also right skewed as there are a small number of
    days with very high rainfall.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 显示了降雨数据的分布，其中在 0 处有一个大的“点质量”或尖峰（对应于大多数无雨的日子）。此外，这个分布也是右偏斜的，因为有少数几天降雨量非常高。
- en: '![CH07_F11_Kunapuli](../Images/CH07_F11_Kunapuli.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F11_Kunapuli](../Images/CH07_F11_Kunapuli.png)'
- en: Figure 7.11 Modeling some types of labels effectively requires combinations
    of distributions, called compound distributions. One such compound distribution
    is the Tweedie distribution.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 有效地建模某些类型的标签需要分布的组合，称为复合分布。其中一种复合分布是 Tweedie 分布。
- en: 'To model this problem, we need a loss function corresponding to a hybrid distribution,
    specifically a Poisson-gamma distribution: the Poisson distribution to model the
    big point mass at 0, and the gamma distribution to model the right-skewed, positive,
    continuous data. For such labels, we can use a powerful family of probability
    distributions called the Tweedie distributions, which are parameterized by the
    Tweedie power parameter *p.* Different values of *p* give rise to different distributions:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建模这个问题，我们需要一个与混合分布相对应的损失函数，具体来说是一个泊松-伽马分布：泊松分布用于建模 0 处的大点质量，伽马分布用于建模右偏斜、正的连续数据。对于这样的标签，我们可以使用一个强大的概率分布族，称为
    Tweedie 分布，它由 Tweedie 力参数 *p* 参数化。不同的 *p* 值会产生不同的分布：
- en: '*p* = 0: Gaussian (normal) distribution'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* = 0: 高斯（正态）分布'
- en: '*p =* 1: Poisson distribution'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p =* 1: 泊松分布'
- en: '1 < *p* < 2: Poisson-gamma distributions for different *p*'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1 < *p* < 2: 不同 *p* 的泊松-伽马分布'
- en: '*p* = 2: Gamma distribution'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* = 2: 伽马分布'
- en: '*p* = 3: Inverse Gaussian distribution'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* = 3: 反高斯分布'
- en: Other choices of p produce many other distributions. For our purposes, we’re
    mostly interested in using 1 < *p* < 2, to create hybrid Poisson-gamma loss functions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 *p* 的选择会产生许多其他分布。对我们来说，我们主要对使用 1 < *p* < 2 的值感兴趣，以创建混合泊松-伽马损失函数。
- en: Both LightGBM and XGBoost come with support for the Tweedie distribution, which
    has led to their widespread adoption in domains such as weather analytics, insurance
    analytics, and health informatics. We’ll see how to use this in our case study
    in section 7.4.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 和 XGBoost 都支持 Tweedie 分布，这导致了它们在天气分析、保险分析和健康信息学等领域的广泛应用。我们将在第 7.4 节中看到如何在我们的案例研究中使用它。
- en: 7.3.2 Gradient boosting with LightGBM and XGBoost
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 使用 LightGBM 和 XGBoost 进行梯度提升
- en: Now, armed with the knowledge of various loss functions, let’s see how we can
    apply gradient-boosting regressors to the AutoMPG data set.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，掌握了各种损失函数的知识，让我们看看如何将梯度提升回归器应用于 AutoMPG 数据集。
- en: Gradient boosting with LightGBM
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LightGBM 进行梯度提升
- en: 'First, let’s apply standard gradient boosting, that is, LightGBM’s LGBMRegressor
    with the Huber loss function. There are several LightGBM hyperparameters that
    we also have to select. These parameters control various components of LightGBM:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们应用标准的梯度提升，即LightGBM的LGBMRegressor与Huber损失函数。我们还需要选择几个XGBoost的超参数。这些参数控制LightGBM的各个组件：
- en: '*Loss function parameters*—alpha is the Huber loss parameter, the threshold
    where it switches from behaving like the MSE to behaving like the MAE loss.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数参数*—`alpha`是Huber损失参数，是它从行为类似于MSE转换为行为类似于MAE损失的阈值。'
- en: '*Learning control parameters*—learning_rate is used to control the rate at
    which the model learns so that it doesn’t rapidly fit and then overfit the training
    data; and subsample is used to randomly sample a smaller fraction of the data
    during training to induce additional ensemble diversity and improve training efficiency.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习控制参数*—`learning_rate`用于控制模型学习的速率，以便它不会快速拟合并过度拟合训练数据；`subsample`用于在训练过程中随机采样数据的一个较小部分，以诱导额外的集成多样性和提高训练效率。'
- en: '*Regularization parameters*—lambda_l1 and lambda_l2 are the weights on the
    L1 and L2 regularizations functions, respectively; these correspond to *a* and
    *b* in the elastic net objective (refer to table 7.1).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正则化参数*—`lambda_l1`和`lambda_l2`分别是L1和L2正则化函数的权重；它们对应于弹性网目标函数中的`a`和`b`（参见表7.1）。'
- en: '*Tree learning parameters*—max_depth limits the maximum depth of each weak
    tree in the ensemble.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*树学习参数*—`max_depth`限制了集成中每个弱树的深度最大值。'
- en: There are other hyperparameters in each category that also allow for finer-grained
    control over training. We select hyperparameters using a combination of randomized
    search (since an exhaustive grid search would be too slow) and CV. Listing 7.6
    shows an example of this with LightGBM.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类别中还有其他超参数，也可以提供对训练的更精细控制。我们通过组合随机搜索（因为穷举网格搜索会太慢）和交叉验证来选择超参数。列表7.6展示了使用LightGBM的示例。
- en: In addition to hyperparameter selection, the listing also implements early stopping,
    where training is terminated if no performance improvement is observed on an evaluation
    set.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 除了超参数选择之外，列表还实现了早停，如果在评估集上没有观察到性能改进，则终止训练。
- en: Listing 7.6 LightGBM with Huber loss
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 使用Huber损失的LightGBM
- en: '[PRE23]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Ranges of hyperparameters that we want to search over
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们想要搜索的超参数范围
- en: ❷ Initializes a LightGBM regressor
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化一个LightGBM回归器
- en: ❸ Since GridSearchCV will be slow, searches more than 20 random parameter combinations
    with 5-fold CV
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于GridSearchCV会较慢，因此使用5折交叉验证搜索超过20种随机参数组合
- en: ❹ Fits the regressor with early stopping
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用早停法拟合回归器
- en: ❺ Computes train and test errors
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算训练和测试误差
- en: 'This produces the following output:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE24]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The LightGBM (gradient boosting) model trained with the Huber loss achieves
    test MSE of 0.0951, highlighted in bold in the preceding code snippet.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Huber损失训练的LightGBM（梯度提升）模型在测试中达到MSE为0.0951，在前面代码片段中用粗体突出显示。
- en: Newton boosting with XGBoost
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 使用XGBoost的牛顿提升
- en: 'We can repeat this training and evaluation with XGBoost’s XGBRegressor. Since
    Newton boosting requires second derivatives, which can’t be computed for the Huber
    loss, XGBoost doesn’t provide this loss directly. Instead, XGBoost provides a
    pseudo-Huber loss, the differentiable approximation of the Huber loss introduced
    in section 7.3.1\. Again, as with LightGBM, we have to set several different hyperparameters.
    Many of XGBoost’s parameters correspond exactly to LightGBM’s parameters, though
    they have different names:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用XGBoost的XGBRegressor重复此训练和评估。由于牛顿提升需要二阶导数，而Huber损失无法计算，XGBoost不直接提供此损失。相反，XGBoost提供了一个伪Huber损失，这是在第7.3.1节中引入的Huber损失的微分近似。同样，与LightGBM一样，我们还需要设置几个不同的超参数。XGBoost的许多参数与LightGBM的参数完全对应，尽管它们的名称不同：
- en: '*Learning control parameters*—learning_rate is used to control the rate at
    which the model learns so that it doesn’t rapidly fit and then overfit the training
    data; and colsample_bytree is used to randomly sample a smaller fraction of the
    features (similar to random forests) during training to induce additional ensemble
    diversity and improve training efficiency.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习控制参数*—`learning_rate`用于控制模型学习的速率，以便它不会快速拟合并过度拟合训练数据；`colsample_bytree`用于在训练过程中随机采样特征的一个较小部分（类似于随机森林），以诱导额外的集成多样性和提高训练效率。'
- en: '*Regularization parameters*—reg_alpha and reg_lambda are the weights on the
    L1 and L2 regularizations functions, respectively; these correspond to *a* and
    *b* in the elastic net objective (refer to table 7.1).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正则化参数*——reg_alpha和reg_lambda分别是L1和L2正则化函数的权重；这些对应于弹性网络目标函数中的*a*和*b*（参见表7.1）。'
- en: '*Tree learning parameters*—max_depth limits the maximum depth of each weak
    tree in the ensemble.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*树学习参数*——max_depth限制了集成中每个弱树的深度。'
- en: The following listing shows how we can train an XGBRegressor, including a randomized
    hyperparameter search.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了如何训练一个XGBRegressor，包括随机超参数搜索。
- en: Listing 7.7 Using XGBoost with pseudo-Huber loss
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 使用XGBoost和伪Huber损失
- en: '[PRE25]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Ranges of hyperparameters that we want to search over
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们想要搜索的超参数范围
- en: ❷ Initializes an XGBoost regressor
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化XGBoost回归器
- en: ❸ Since GridSearchCV will be slow, searches more than 20 random parameter combinations
    with 5-fold CV
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于GridSearchCV会较慢，因此使用5折交叉验证搜索超过20种随机参数组合
- en: ❹ Fits the regressor with early stopping
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用早停法拟合回归器
- en: ❺ Computes train and test errors
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算训练和测试误差
- en: 'This produces the following output:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下输出：
- en: '[PRE26]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The XGBoost model trained with the pseudo-Huber loss achieves a test MSE of
    0.0947 (highlighted in bold in the output of listing 7.7). This is similar to
    the performance of the LightGBM model, which achieved a test MSE of 0.0951 (see
    output produced by listing 7.6).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用伪Huber损失训练的XGBoost模型在测试中实现了0.0947的均方误差（在列表7.7的输出中被加粗）。这与实现了0.0951测试均方误差的LightGBM模型的性能相似（参见列表7.6产生的输出）。
- en: This illustrates that the pseudo-Huber loss is a reasonable substitute for the
    Huber loss when the situation calls for it. We’ll shortly see how we can use LightGBM
    and XGBoost with other loss functions discussed in this section on the task of
    bike demand prediction in the chapter case study.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明当需要时，伪Huber损失是Huber损失的合理替代。我们很快就会看到如何使用LightGBM和XGBoost在本章案例研究中讨论的其他损失函数来预测自行车需求。
- en: '7.4 Case study: Demand forecasting'
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 案例研究：需求预测
- en: 'Demand forecasting is an important problem that arises in many business contexts
    when the goal is to predict the demand for a certain product or commodity. Accurately
    predicting demand is critical for downstream supply chain management and optimization:
    to ensure that there is enough supply to meet needs and not too much that there
    is waste.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 需求预测是在许多商业环境中出现的一个重要问题，当目标是预测某种产品或商品的需求时。准确预测需求对于下游供应链管理和优化至关重要：确保有足够的供应来满足需求，而又不会过多导致浪费。
- en: Demand forecasting is often cast as a regression problem of using historical
    data and trends to build a model to predict future demand. The target labels can
    be continuous or count-valued.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 需求预测通常被表述为使用历史数据和趋势构建模型来预测未来需求的回归问题。目标标签可以是连续的或计数的。
- en: For example, in energy demand forecasting, the label to predict (energy demand
    in gigawatt hours) is continuous valued. Alternately, in product demand forecasting,
    the label to predict (number of items to be shipped) is count-valued.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在能源需求预测中，要预测的标签（以吉瓦时计的能源需求）是连续值。或者，在产品需求预测中，要预测的标签（要运输的项目数量）是计数值。
- en: In this section, we study the problem of bike rental forecasting. As we see
    in this section, the nature of the problem (and, especially, the targets/labels)
    is quite similar to those arising in the areas of weather prediction and analytics,
    insurance and risk analytics, health informatics, energy demand forecasting, business
    intelligence, and many others.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究自行车租赁预测问题。正如我们在这个节中所看到的，问题的性质（尤其是目标/标签）与气象预测和分析、保险和风险分析、健康信息学、能源需求预测、商业智能等领域中出现的性质相当相似。
- en: We analyze the data set and then build progressively more complex models, beginning
    with single linear models, then moving on to ensemble nonlinear models. At each
    stage, we’ll perform hyperparameter tuning to select the best hyperparameter combinations.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析数据集，然后构建越来越复杂的模型，从单个线性模型开始，然后转向集成非线性模型。在每一个阶段，我们将执行超参数调整以选择最佳的超参数组合。
- en: 7.4.1 The UCI Bike Sharing data set
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 UCI自行车共享数据集
- en: The Bike Sharing data set[¹](#pgfId-1153567) was the first of several similar
    publicly available data sets that tracks the usage of bicycle-sharing services
    in major metropolitan areas. These data sets are made publicly available through
    the UCI Machine Learning Repository ([http://mng.bz/GRrM](http://mng.bz/GRrM)).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bike Sharing数据集][¹](#pgfId-1153567)是第一个跟踪主要大都市区自行车共享服务使用情况的几个类似公开数据集之一。这些数据集通过UCI机器学习库公开提供（[http://mng.bz/GRrM](http://mng.bz/GRrM)）。'
- en: This data set, first made available in 2013, tracks hourly and daily bicycle
    rentals of casual riders and registered member riders of Capital Bike Sharing
    in Washington, DC. In addition, the data set also contains several features describing
    the weather as well as the time of day and day of the year.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集首次于2013年提供，追踪了华盛顿特区Capital Bike Sharing的休闲骑行者和注册会员的每小时和每日自行车租赁情况。此外，数据集还包含描述天气以及一天中时间和年份的几个特征。
- en: The overall goal of the problem in this case study is to predict the bike rental
    demand of casual riders depending on the time of day, the season, and the weather.
    The demand is measured in total number of users—a count!
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究中的问题总体目标是根据一天中的时间、季节和天气预测休闲骑行者的租车需求。需求以总用户数衡量——一个计数！
- en: Why only model casual riders? The number of registered users appears to be fairly
    consistent across the year, since these are users who presumably use bike sharing
    as a regular transportation option rather than a recreational activity. This is
    akin to commuters who have a monthly/annual bus pass for their daily commutes
    as opposed to tourists who only buy bus tickets as needed.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只针对休闲骑行者建模？注册用户数量似乎在全年中相当稳定，因为这些用户可能将自行车共享作为常规交通选择，而不是娱乐活动。这类似于有月度/年度公交通行证的通勤者，而不是只按需购买公交票的游客。
- en: 'Keeping this in mind, we construct a derived data set for our case study that
    can be used to build a model to forecast the rental bike demand of casual users.
    The (modified) data set for this case study is available with the book’s code
    and can be loaded thus:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，我们为案例研究构建了一个派生数据集，该数据集可用于构建模型来预测休闲用户的租车需求。本案例研究的（修改后）数据集与本书的代码一起提供，可以按以下方式加载：
- en: '[PRE27]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can look at the statistics of the data set with the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方式查看数据集的统计信息：
- en: '[PRE28]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This will compute various statistics of all the features in the data set, as
    shown in figure 7.12, which is helpful in getting a sense of the various features
    and how their values are distributed at a high level.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这将计算数据集中所有特征的各项统计信息，如图7.12所示，这有助于了解各种特征及其值在高级别上的分布情况。
- en: '![CH07_F12_Kunapuli](../Images/CH07_F12_Kunapuli.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F12_Kunapuli](../Images/CH07_F12_Kunapuli.png)'
- en: Figure 7.12 Statistics of the Bike Sharing data set. The “casual” column is
    the prediction target (label).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 Bike Sharing数据集的统计信息。其中“casual”列是预测目标（标签）。
- en: 'The data set contains several continuous weather features: temp (normalized
    temperature), atemp (normalized “feels like” temperature), hum (humidity), and
    windspeed. The categorical feature weathersit describes the type of weather seen
    at that time with four categories:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含几个连续的天气特征：temp（归一化温度）、atemp（归一化“体感”温度）、hum（湿度）和windspeed。分类特征weathersit描述了当时看到的天气类型，有四个类别：
- en: '1: Clear, Few Clouds, Partly Cloudy'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1：晴朗，少量云，部分多云
- en: '2: Mist + Cloudy, Mist + Broken Clouds, Mist + Few Clouds, Mist'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2：雾+多云，雾+破碎云，雾+少量云，雾
- en: '3: Light Snow, Light Rain + Thunderstorm + Scattered Clouds, Rain + Scattered
    Clouds'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3：小雪，小雨+雷暴+散云，雨+散云
- en: '4: Heavy Rain + Ice Pellets + Thunderstorm + Mist, Snow + Fog'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4：大雨+冰雹+雷暴+雾，雪+雾
- en: 'The data set also contains discrete features: season (1: winter, 2: spring,
    3: summer, 4: fall), mnth (1 to 12 for January through December), and hr (hour
    from 0 to 23) to describe the time. In addition, the binary features holiday,
    weekday, and workingday encode whether the day in question is a holiday, weekday,
    or workday.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集还包含离散特征：season（1：冬季，2：春季，3：夏季，4：秋季）、mnth（1到12代表1月到12月）和hr（从0到23的小时）来描述时间。此外，二元特征holiday、weekday和workingday编码了问题中的天是假日、工作日还是工作日。
- en: Preprocessing the features
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 特征预处理
- en: Let’s preprocess this data set by normalizing the features, that is, ensuring
    each feature is zero mean, unit standard deviation. Normalization isn’t always
    the best approach to deal with discrete features. For now, though, let’s use this
    simple preprocessing and keep our focus on ensembles for regression. In chapter
    8, we delve more into preprocessing strategies for these types of features.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过归一化特征来预处理这个数据集，也就是说，确保每个特征具有零均值和单位标准差。归一化并不总是处理离散特征的最好方法。不过，现在我们先使用这种简单的预处理，并专注于回归的集成方法。在第
    8 章中，我们将更深入地探讨这些类型特征的预处理策略。
- en: 'Listing 7.8 shows our preprocessing steps: it splits the data into training
    (80% of the data) and test sets (remaining 20% of the data) and applies normalization
    to the features. As always, we’ll hold out the test set from the training process
    so that we can evaluate the performance of each of our trained models on the test
    set.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 展示了我们的预处理步骤：它将数据分为训练集（数据量的 80%）和测试集（剩余的 20% 数据），并对特征应用归一化。一如既往，我们将测试集从训练过程中排除，以便我们可以在测试集上评估每个训练模型的性能。
- en: Listing 7.8 Preprocessing the Bike Sharing data set
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 预处理共享单车数据集
- en: '[PRE29]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Gets the column index for the label
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取标签的列索引
- en: ❷ Gets the column indices for the features
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取特征的列索引
- en: ❸ Splits into train and test sets
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 分割为训练集和测试集
- en: ❹ Preprocesses features by normalizing
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过归一化预处理特征
- en: Count-valued targets
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 计数型目标
- en: The target label we want to predict is casual, that is, the number of casual
    users, which is count-valued, ranging from 0 to 367\. We plot the histogram of
    these targets in figure 7.13 (left). This data set has a large point mass at 0,
    indicating that, on many days, there are no casual users. Further, we can see
    that this distribution has a *long tail*, which makes it *right skewed*.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要预测的目标标签是偶然的，即偶然用户数量，这是一个计数型值，范围从 0 到 367。我们在图 7.13（左）中绘制了这些目标的直方图。这个数据集在
    0 处有一个大的点质量，表明在许多日子里，没有偶然用户。此外，我们可以看到这个分布有一个 *长尾*，这使得它 *右偏斜*。
- en: We can further analyze these labels by applying a log transformation, that is,
    transform each count label *y* to *log*(1 + *y*), where we add 1 to avoid taking
    the logarithm of zero count data. This is shown in figure 7.13 (right).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过应用对数变换进一步分析这些标签，即把每个计数标签 *y* 转换为 *log*(1 + *y*)，其中我们加 1 以避免对零计数数据进行对数运算。这如图
    7.13（右）所示。
- en: '![CH07_F13_Kunapuli](../Images/CH07_F13_Kunapuli.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F13_Kunapuli](../Images/CH07_F13_Kunapuli.png)'
- en: Figure 7.13 Histogram of count-valued targets, the number of casual users (left);
    histogram of the count targets after log transformation (right)
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 计数型目标直方图，偶然用户数量（左）；对数变换后的计数目标直方图（右）
- en: 'This gives us two great insights regarding how we might want to model the problem:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了两个关于我们如何建模问题的深刻见解：
- en: '*Use Tweedie distribution*—The distribution of the log-transformed count target
    looks very similar to the histogram of rainfall shown earlier in figure 7.11,
    which suggests that a Tweedie distribution might be appropriate for modeling this
    problem. Recall that a Tweedie distribution with parameter 1 *< p <* 2 can model
    a compound Poisson-gamma distribution: the Poisson distribution to model the big
    point mass at 0, and the gamma distribution to model the right-skewed, positive
    continuous data.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 Tweedie 分布*——对数变换后的计数目标分布看起来与之前图 7.11 中显示的降雨量直方图非常相似，这表明 Tweedie 分布可能适合建模这个问题。回想一下，参数
    1 *< p <* 2 的 Tweedie 分布可以模拟复合泊松-伽马分布：泊松分布用于模拟 0 处的大点质量，伽马分布用于模拟右偏斜的正连续数据。'
- en: '*Use GLM*—The log transformation itself suggests a connection between the target
    and the features. If we were to model this regression task as a GLM, we would
    have to use the log-link function. We would like to extend this notion to ensemble
    methods (which are usually nonlinear).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用 GLM*——对数变换本身表明目标与特征之间存在联系。如果我们把这个回归任务建模为 GLM，我们就必须使用对数连接函数。我们希望将这个概念扩展到集成方法（通常是非线性）。'
- en: As we’ll see shortly, LightGBM and XGBoost provide support for modeling both
    the log link (and other link functions) and distributions such as Poisson, gamma,
    and Tweedie. This allows them to emulate the intuition of GLMs to capture the
    nuances of the data set, while going beyond the restriction of GLMs to learning
    only linear models.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们很快将看到的，LightGBM 和 XGBoost 支持建模对数连接（以及其他连接函数）和泊松、伽马和 Tweedie 等分布。这使得它们能够模仿
    GLM 的直觉来捕捉数据集的细微差别，同时超越 GLM 只能学习线性模型的限制。
- en: 7.4.2 GLMs and stacking
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 GLM 和堆叠
- en: 'Let’s first train individual general linear regression models that capture
    the intuitions gleaned previously. In addition, we’ll also stack these individual
    models to combine their predictions. We’ll train three individual regressors:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先训练个别的一般线性回归模型，以捕捉之前获得的直觉。此外，我们还将堆叠这些个别模型以结合它们的预测。我们将训练三个个别回归器：
- en: '*Tweedie regression with the log-link function*—Uses the Tweedie distribution
    to model the positive, right-skewed targets. We use scikit-learn’s Tweedie Regressor,
    which requires that we choose two parameters: alpha, parameter for the L2 regularization
    term, and power, which should be between 1 and 2.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*带有对数连接函数的Tweedie回归*—使用Tweedie分布来模拟正偏斜的目标。我们使用scikit-learn的Tweedie回归器，它要求我们选择两个参数：alpha，L2正则化项的参数，以及power，它应该在1到2之间。'
- en: '*Poisson regression with the log-link function*—Uses the Poisson distribution
    to model count variables. We use scikit-learn’s PoissonRegressor, which requires
    that we choose only one parameter: alpha, parameter for the L2 regularization
    term. It should be noted that setting power=1 in TweedieRegressor is equivalent
    to using PoissonRegressor.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*带有对数连接函数的泊松回归*—使用泊松分布来模拟计数变量。我们使用scikit-learn的PoissonRegressor，它要求我们选择仅一个参数：alpha，L2正则化项的参数。需要注意的是，在TweedieRegressor中将power设置为1相当于使用PoissonRegressor。'
- en: '*Ridge regression*—Uses the normal distribution to model continuous variables.
    This, in general, isn’t well suited for this data and is included as a baseline,
    as it’s one of the most common methods we’ll encounter in the wild.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*岭回归*—使用正态分布来模拟连续变量。一般来说，这种方法并不适合这些数据，但它被包括作为基线，因为它是我们将在野外遇到的最常见方法之一。'
- en: The following listing demonstrates how we can train these regressors with the
    hyperparameter search through the exhaustive grid search and combined with CV.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了我们如何通过穷举网格搜索和结合交叉验证来训练这些回归器。
- en: Listing 7.9 Training GLMs for bike rental prediction
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.9：为自行车租赁预测训练GLM
- en: '[PRE30]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Ranges of hyperparameters for ridge, Poisson, and Tweedie regressors
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 岭回归、泊松回归和Tweedie回归的超参数范围
- en: '❷ Tweedie regression has an additional parameter: power.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Tweedie回归有一个额外的参数：power。
- en: ❸ Initializes GLMs
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化GLM
- en: ❹ Saves individual GLMs after CV
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在交叉验证（CV）后保存单个广义线性模型（GLM）
- en: ❺ Performs grid search for each GLM with 5-fold CV
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对每个GLM进行网格搜索，使用5折交叉验证
- en: ❻ Gets the final refit GLM and computes train and test predictions
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取最终的重新拟合GLM并计算训练和测试预测
- en: '❼ Computes and saves three metrics for each GLM: MAE, MSE, and *R* ² score'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 为每个GLM计算并保存三个指标：MAE、MSE和*R*²分数
- en: 'If we use print(results), we’ll see what the three models have learned. We
    evaluate the train and test set performance using these metrics: MSE, MAE, and
    *R*² score. Recall that the *R*² score (or the coefficient of determination) is
    the proportion of the target variance that is explainable from the data.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用print(results)，我们将看到三个模型学到了什么。我们使用这些指标来评估训练集和测试集的性能：MSE、MAE和*R*²分数。回想一下，*R*²分数（或决定系数）是可从数据中解释的目标方差的比例。
- en: '*R*² scores range from negative infinity to 1, with higher scores indicating
    better performance. MSE and MAE range from 0 to infinity, with lower errors indicating
    better performance:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*² 分数范围从负无穷大到1，分数越高表示性能越好。均方误差（MSE）和平均绝对误差（MAE）的范围从0到无穷大，误差越低表示性能越好：'
- en: '[PRE31]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The test set performance immediately confirms one of our intuitions: classical
    regression approaches, which assume a normal distribution over the data, fare
    the worst. Poisson or Tweedie distributions, however, show promise.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的性能立即证实了我们的一种直觉：假设数据具有正态分布的经典回归方法表现最差。泊松或Tweedie分布则显示出希望。
- en: 'We’ve now have trained our first three machine-learning models: let’s ensemble
    them by stacking them. The following listing shows how to do this using ANN regression.
    While the GLMs we trained are linear, this stacked model will be nonlinear!'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经训练了三个机器学习模型：让我们通过堆叠它们来集成它们。以下列表显示了如何使用ANN回归来完成此操作。虽然我们训练的GLM是线性的，但这个堆叠模型将是非线性的！
- en: Listing 7.10 Stacking GLMs for bike rental prediction
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.10：为自行车租赁预测堆叠GLM
- en: '[PRE32]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ GLMs with the best parameter settings from listing 7.9 are base estimators.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列表7.9中最佳参数设置的GLM是基估计器。
- en: ❷ Three-layer neural network is the meta estimator.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 三层神经网络是元估计器。
- en: ❸ Trains the stacking ensemble
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练堆叠集成
- en: ❹ Makes train and test predictions
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 进行训练和测试预测
- en: '❺ Computes and saves three metrics for this model: MAE, MSE, and *R*² score'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为此模型计算并保存三个指标：MAE、MSE和*R*²分数
- en: Now, we can compare the results of stacking with the individual models
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以比较堆叠与单个模型的结果
- en: '[PRE33]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The stacked GLM ensemble already improves test set performance noticeably, indicating
    that nonlinear models are the way to go.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的GLM集成已经明显提高了测试集的性能，这表明非线性模型是可行的方向。
- en: 7.4.3 Random forest and Extra Trees
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 随机森林和Extra Trees
- en: 'Now, let’s train some more parallel ensembles for the bike rental prediction
    task using scikit-learn’s RandomForestRegressor and ExtraTreesRegressor. Both
    modules support MSE, MAE, and Poisson as the loss function. However, unlike GLMs,
    random forest and Extra Trees don’t use a log-link function. We’ll train two different
    ensembles: one for each MSE and Poisson loss functions, and with similar hyperparameter
    sweeps for each.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用scikit-learn的RandomForestRegressor和ExtraTreesRegressor来训练更多并行集成，用于自行车租赁预测任务。这两个模块都支持MSE、MAE和Poisson作为损失函数。然而，与GLMs不同，随机森林和Extra
    Trees不使用对数连接函数。我们将训练两个不同的集成：一个用于MSE损失函数，另一个用于Poisson损失函数，并且每个都进行类似的超参数搜索。
- en: 'For both methods, we’re looking to identify the best choice of two hyperparameters:
    ensemble size (n_estimators) and the maximum depth of each base estimator (max_depth).
    We can set the loss functions through the criterion argument for each method as
    ''squared_error'' or ''poisson''.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种方法，我们都在寻找两个超参数的最佳选择：集成大小（n_estimators）和每个基础估计器的最大深度（max_depth）。我们可以通过每个方法的criterion参数设置损失函数为'squared_error'或'poisson'。
- en: The following listing demonstrates how we can train these regressors with hyperparameter
    search through exhaustive grid search and combined with CV—similar to what we
    did for GLMs.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了我们如何通过穷举网格搜索和结合CV来训练这些回归器，这与我们为GLMs所做的方法类似。
- en: Listing 7.11 Random forest and Extra Trees for bike rental prediction
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.11 随机森林和Extra Trees用于自行车租赁预测
- en: '[PRE34]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Ranges of hyperparameters for both random forest and Extra Trees
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 随机森林和Extra Trees的超参数范围
- en: ❷ Both ensembles use MSE as the training criterion.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 两个集成都使用MSE作为训练标准。
- en: ❸ Hyperparameter tuning with grid search and 5-fold CV
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用网格搜索和5折交叉验证进行超参数调整
- en: ❹ Gets train and test predictions for each ensemble
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取每个集成的训练和测试预测
- en: '❺ Computes and saves three metrics for each ensemble: MAE, MSE, and *R*² score'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 为每个集成计算并保存三个指标：MAE、MSE和*R*²得分
- en: 'Compare the results of these parallel ensemble models with stacking and individual
    GLMs. In particular, observe the sharp improvement in performance compared to
    single models, which demonstrates the power of ensemble methods, even when trained
    on suboptimal loss functions:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些并行集成模型的成果与堆叠和单个GLMs进行比较。特别是，观察与单个模型相比性能的显著提升，这证明了集成方法的力量，即使是在次优损失函数上训练也是如此：
- en: '[PRE35]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Can we get similar or better performance with gradient and Newton-boosting methods?
    Let’s find out.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否通过梯度提升和牛顿提升方法获得相似或更好的性能？让我们来看看。
- en: 7.4.4 XGBoost and LightGBM
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 XGBoost和LightGBM
- en: 'Finally, let’s train sequential ensembles using both XGBoost and LightGBM on
    this data set. Both packages have support for a wide variety of loss functions:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用XGBoost和LightGBM在这个数据集上训练序列集成。这两个包都支持广泛的损失函数：
- en: Some of the loss and likelihood functions that XGBoost supports include the
    MSE, pseudo-Huber, Poisson, and Tweedie losses with the log-link function. Note
    again that XGBoost implements Newton boosting, which requires computing second
    derivatives; this means that XGBoost can’t implement the MAE or Huber losses directly.
    Instead, XGBoost provides support for the pseudo-Huber loss.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost支持的损失和似然函数包括MSE、伪Huber、Poisson和Tweedie损失，带有对数连接函数。请注意，XGBoost实现了牛顿提升，这需要计算二阶导数；这意味着XGBoost不能直接实现MAE或Huber损失。相反，XGBoost提供了对伪Huber损失的支持。
- en: Like XGBoost, LightGBM supports the MSE, Poisson, and Tweedie losses with the
    log-link function. However, since it implements gradient boosting, which only
    requires first derivatives, it directly supports the MAE and the Huber loss.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与XGBoost类似，LightGBM支持MSE、Poisson和Tweedie损失，带有对数连接函数。然而，由于它实现了梯度提升，这只需要一阶导数，它直接支持MAE和Huber损失。
- en: For both models, we’ll need to tune for several hyperparameters that control
    various aspects of ensembling (e.g., learning rate and early stopping), regularization
    (e.g., weights on the L1 and L2 regularizations), and tree learning (e.g., maximum
    tree depth).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个模型，我们需要调整控制集成各个方面（例如，学习率和早期停止）的几个超参数，正则化（例如，L1和L2正则化的权重），以及树学习（例如，最大树深度）。
- en: Many of the previous models we trained only required tuning of a small number
    of hyperparameters, which allowed us to identify them through a grid search procedure.
    Grid search is time consuming, and the computational expense becomes prohibitive,
    so it should be avoided in instances such as this. Instead of an exhaustive grid
    search, randomized search can be an efficient alternative.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前训练的许多模型只需要调整少量超参数，这使得我们能够通过网格搜索过程识别它们。网格搜索耗时，计算成本变得过高，因此在这种情况下应避免使用。与其进行详尽的网格搜索，随机搜索可以是一个有效的替代方案。
- en: In a randomized hyperparameter search, we sample a smaller number of random
    hyperparameter combinations from the full list. If necessary, we can perform more
    fine-tuning once we’ve identified a good combination to attempt to refine and
    improve our results further.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机超参数搜索中，我们从完整列表中采样较少的随机超参数组合。一旦我们确定了一个好的组合，我们就可以进行更精细的调整，以进一步优化我们的结果。
- en: The following listing shows the steps for randomized parameter search and ensemble
    training for XGBoost with different loss functions.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了使用不同损失函数的XGBoost进行随机参数搜索和集成训练的步骤。
- en: Listing 7.12 XGBoost for bike rental prediction
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.12 XGBoost用于自行车租赁预测
- en: '[PRE36]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Ranges of hyperparameters for all XGBoost loss functions
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所有XGBoost损失函数的超参数范围
- en: ❷ Initializes XGBoost models, each with a different loss function
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化具有不同损失函数的XGBoost模型
- en: '❸ For the Tweedie loss, we have an additional hyperparameter: power.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 对于Tweedie损失，我们还有一个额外的超参数：power。
- en: ❹ Hyperparameter tuning using randomized search with 5-fold CV
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用5折交叉验证进行随机搜索的超参数调整
- en: ❺ Selects the best model using negative Poisson log-likelihood
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用负泊松对数似然函数选择最佳模型
- en: ❻ Gets train and test predictions for each ensemble
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 获取每个集成模型的训练和测试预测
- en: '❼ Computes and saves three metrics for each XGBoost ensemble: MAE, MSE, and
    *R*² score'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 为每个XGBoost集成计算并保存三个指标：MAE、MSE和*R*²分数
- en: NOTE Listing 7.12 uses early stopping to terminate training early if there is
    no noticeable performance improvement on an evaluation set. When we last employed
    early stopping on the AutoMPG data set (refer to listing 7.6), we used the MSE
    as the evaluation metric to track performance improvement. Here, we use the negative
    Poisson log-likelihood (eval_metric='poisson-nloglik'). Recall from our discussion
    in section 7.3.1 that negative log-likelihood is often used as a surrogate for
    loss functions without a closed form. In this case, because we’re modeling count
    targets (which follow a Poisson distribution), it may be more appropriate to measure
    model performance with negative Poisson log-likelihood. It would’ve also been
    appropriate to compare test set performances of different models with this metric
    alongside MSE, MAE, and *R*², as we’ve been doing. However, this metric isn’t
    always available or exposed in most packages.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表7.12使用提前停止来提前终止训练，如果评估集上没有明显的性能提升。当我们最后一次在AutoMPG数据集上使用提前停止（参考列表7.6）时，我们使用MSE作为评估指标来跟踪性能提升。在这里，我们使用负泊松对数似然（eval_metric='poisson-nloglik'）。回想一下我们在7.3.1节中的讨论，负对数似然通常用作没有闭式形式的损失函数的替代。在这种情况下，因为我们正在对计数目标（遵循泊松分布）进行建模，所以使用负泊松对数似然来衡量模型性能可能更合适。当然，也可以将这个指标与MSE、MAE和*R*²一起比较，就像我们一直在做的那样。然而，这个指标并不总是可用或公开在大多数包中。
- en: 'The performance of XGBoost with different loss functions is shown here:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了XGBoost在不同损失函数下的性能：
- en: '[PRE37]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: These results are dramatically improved, with XGBoost trained with Poisson and
    Tweedie losses performing the best.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果显著改善，使用泊松和Tweedie损失训练的XGBoost表现最佳。
- en: 'We can repeat a similar experiment with LightGBM. The implementation for this
    (which can be found in the companion code) is quite similar to how we trained
    LightGBM models for the AutoMPG data set in listing 7.6 and XGBoost models for
    the Bike Sharing data set in listing 7.11\. The performance of LightGBM with MSE,
    MAE, Huber, Poisson, and Tweedie losses are shown here:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用类似的方法对LightGBM进行实验。这个实现（可以在配套代码中找到）与我们训练LightGBM模型的方法非常相似，如列表7.6中的AutoMPG数据集和列表7.11中的Bike
    Sharing数据集。这里展示了LightGBM在MSE、MAE、Huber、泊松和Tweedie损失下的性能：
- en: '[PRE38]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: LightGBM’s performance is similar to that of XGBoost, with Poisson and Tweedie
    losses, again, performing the best, and XGBoost edging LightGBM out slightly.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM的性能与XGBoost相似，在泊松和Tweedie损失下，再次表现出最佳性能，而XGBoost略优于LightGBM。
- en: 'Figure 7.14 summarizes the test set performance (with *R*² score) of all the
    models we’ve trained for the bike rental demand prediction tasks. We note the
    following:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14总结了我们对自行车租赁需求预测任务训练的所有模型的测试集性能（使用R²分数）。我们注意到以下几点：
- en: Individual GLMs perform far worse than any ensemble method. This is unsurprising
    since ensemble methods combine the power of many individual models into a final
    prediction. Furthermore, many of the ensemble regressors are nonlinear and fit
    the data better, while all GLMs are linear and limited.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个广义线性模型（GLM）的性能远低于任何集成方法。这并不令人惊讶，因为集成方法将多个单个模型的力量结合成一个最终的预测。此外，许多集成回归器是非线性的，并且更好地拟合数据，而所有GLM都是线性的并且有限。
- en: The appropriate choice of loss functions is critical to training a good model.
    In this case, LightGBM and XGBoost models trained with Tweedie fit and generalize
    best. This is because the Tweedie loss captures the distribution of the bike demand,
    which is a count-valued target.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的损失函数对于训练一个好的模型至关重要。在这种情况下，使用Tweedie拟合的LightGBM和XGBoost模型训练效果最佳。这是因为Tweedie损失函数捕捉了自行车需求分布，它是一个计数值目标。
- en: Packages such as LightGBM and XGBoost provide loss functions such as the Tweedie,
    while scikit-learn’s ensemble method implementations (random forest, Extra Trees)
    only support MSE and MAE losses (at the time of this writing). It’s possible to
    push the performance of these methods up further by adopting losses such as the
    Tweedie, but this would require custom loss implementations.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如LightGBM和XGBoost之类的包提供了Tweedie等损失函数，而scikit-learn的集成方法实现（随机森林、Extra Trees）仅支持MSE和MAE损失（在撰写本文时）。通过采用Tweedie等损失函数，可以进一步提高这些方法的性能，但这需要自定义损失实现。
- en: '![CH07_F14_Kunapuli](../Images/CH07_F14_Kunapuli.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F14_Kunapuli](../Images/CH07_F14_Kunapuli.png)'
- en: Figure 7.14 The test set performance (with the *R*² score metric) of the various
    ensemble methods for regression as we progressed through our analysis and modeling.
    Gradient-boosting (LightGBM) and Newton-boosting (XGBoost) ensembles are the current
    state of the art. Among these methods, performance can further be improved through
    a judicious choice of loss function and systematic parameter selection.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 在我们的分析和建模过程中，回归的各种集成方法的测试集性能（使用R²分数指标）。梯度提升（LightGBM）和牛顿提升（XGBoost）集成是目前最先进的技术。在这些方法中，通过谨慎选择损失函数和系统性地选择参数，性能可以进一步提高。
- en: Summary
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Regression can be used to model continuous-valued, count-valued, and even discrete-valued
    targets.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归可以用来建模连续值、计数值，甚至离散值的目标。
- en: Classical linear models such as ordinary least squares (OLS), ridge regression,
    Least Absolute Shrinkage and Selection (LASSO), and elastic net all use the squared
    loss function, but they use different regularization functions.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的线性模型，如普通最小二乘法（OLS）、岭回归、最小绝对收缩和选择（LASSO）和弹性网络，都使用平方损失函数，但它们使用不同的正则化函数。
- en: Poisson regression uses a linear model with a log-link function and the Poisson
    distribution assumption on the targets to effectively model count-labeled data.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泊松回归使用带有对数连接函数的线性模型，并在目标上假设泊松分布，以有效地建模计数标签数据。
- en: Gamma regression uses a linear model with a log-link function and the gamma
    distribution assumption on the targets to effectively model continuous, but positively
    valued and right-skewed data.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伽马回归使用带有对数连接函数的线性模型，并在目标上假设伽马分布，以有效地建模连续、正值且右偏的数据。
- en: Tweedie regression uses a linear model with a log-link function and the Tweedie
    distribution assumption to model compound distributions on data arising in many
    practical applications, such as insurance, weather, and health analytics.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tweedie回归使用带有对数连接函数的线性模型，并假设Tweedie分布来对在许多实际应用中出现的数据建模复合分布，例如保险、天气和健康分析。
- en: Classic mean squared regression, Poisson regression, gamma regression, Tweedie
    regression, and even logistic regression are all different variants of generalized
    linear models (GLMs).
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典的均方回归、泊松回归、伽马回归、Tweedie回归，甚至逻辑回归都是广义线性模型（GLM）的不同变体。
- en: Random forests and Extra Trees use randomized regression tree learning to induce
    ensemble diversity.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林和Extra Trees使用随机回归树学习来诱导集成多样性。
- en: Common statistical measures such as mean and median can be used to combine the
    predictions of continuous targets and mode and median to combine predictions of
    count targets.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的统计量，如平均值和中位数，可以用来结合连续目标的预测，而众数和中位数可以用来结合计数目标的预测。
- en: Artificial neural network (ANN) regressors are good choices for meta-estimators
    when learning stacking ensembles.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在学习堆叠集成时，人工神经网络（ANN）回归器是元估计器的良好选择。
- en: Loss functions such as mean squared error (MSE), mean average deviation (MAD),
    and Huber loss are well suited for continuous-valued labels.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数，如均方误差（MSE）、平均绝对偏差（MAD）和Huber损失，非常适合连续值标签。
- en: The gamma likelihood function is well suited for continuous-valued but positive
    labels (i.e., they don't take negative values).
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伽马似然函数非常适合连续值但为正的标签（即它们不取负值）。
- en: The Poisson likelihood function is well suited for count-valued labels.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泊松似然函数非常适合计数值标签。
- en: Some problems contain a mix of these labels and can be modeled with a Tweedie
    likelihood function.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些问题包含这些标签的混合，可以用Tweedie似然函数进行建模。
- en: LightGBM and XGBoost provide support for modeling both the log link (and other
    link functions) and distributions such as Poisson, gamma, and Tweedie.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM和XGBoost提供了对建模对数链接（以及其他链接函数）以及泊松、伽马和Tweedie等分布的支持。
- en: Hyperparameter selection, either through exhaustive grid search (slow, but thorough)
    or randomized search (fast, but approximate), is essential for good ensemble development
    in practice.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中，超参数选择，无论是通过穷举网格搜索（慢但彻底）还是随机搜索（快但近似），对于良好的集成开发至关重要。
- en: '* * *'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) “Event labeling combining ensemble detectors and background knowledge,”
    by H. Fanaee-T and J. Gama. *Progress in Artificial Intelligence 2*, 113-127 (2014).
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) “结合集成检测器和背景知识的活动标注”，作者为H. Fanaee-T和J. Gama。*人工智能进展2*，第113-127页（2014年）。
