- en: Chapter 56\. Blatantly Discriminatory Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第56章。公然歧视性的算法
- en: Eric Siegel
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 埃里克·西格尔
- en: '![](Images/Eric_Siegel.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Eric_Siegel.png)'
- en: Founder, Predictive Analytics World
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 创始人，预测分析世界
- en: Imagine sitting across from a person being evaluated for a job, a loan, or even
    parole. When they ask how the decision process works, you inform them, “For one
    thing, our algorithm penalized your score by seven points because you’re black.”
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你坐在一个正在接受工作、贷款或假释评估的人的对面。当他们问到决策过程是如何运作的，你告诉他们：“其中一件事是，我们的算法因为你是黑人而将你的分数降低了七分。”
- en: We are headed in that direction. Distinguished experts are now campaigning for
    discriminatory algorithms in law enforcement and beyond. They argue that computers
    should be authorized to make life-altering decisions based directly on race and
    other protected classes. This would mean that computers could explicitly penalize
    black defendants for being black.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正朝着这个方向前进。杰出的专家们现在正在为执法和其他领域的歧视性算法进行宣传。他们认为，计算机应该被授权直接基于种族和其他受保护群体做出改变生活的决策。这将意味着计算机可以明确因为黑人而惩罚黑人的被告。
- en: In most cases, data scientists intentionally design algorithms to be blind to
    protected classes. This is accomplished by prohibiting predictive models from
    inputting such factors. Doing so does not eliminate *machine bias*, the well-known
    phenomenon wherein models falsely flag one group more than another via “surrogate”
    variables (discussed in my article in [Part VII](part07.xhtml#case_studies) of
    this book, “[*To Fight Bias in Predictive Policing, Justice Can’t Be Color-Blind*](ch89.xhtml#to_fight_bias_in_predictive_policingcom)”).
    But suppressing such model inputs is a fundamental first step, without which models
    are *discriminatory*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，数据科学家故意设计算法，使其对受保护群体保持盲目。这是通过禁止预测模型输入这些因素来实现的。这样做并不会消除*机器偏见*，这种众所周知的现象，其中模型通过“替代”变量（在我在本书的[第七部分](part07.xhtml#case_studies)中讨论的文章，“[*在预测执法中抗击偏见，正义不能是色盲*](ch89.xhtml#to_fight_bias_in_predictive_policingcom)”）错误地标记一个群体多于另一个群体）。但抑制此类模型输入是一个基本的第一步，若没有它，模型就是*歧视性的*。
- en: I use the term “discriminatory” for decisions that are based in part on a protected
    class, such as when profiling by race or religion to determine police searches.
    An exception holds when decisions are intended to benefit a protected group, such
    as for affirmative action, or when determining whether someone qualifies for a
    grant designated for a minority group.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用“歧视性”一词是指部分基于受保护群体的决策，例如通过种族或宗教进行剖析以确定警察搜查的情况。当决策旨在使受保护群体受益时，例如为了平权行动，或确定某人是否符合指定少数群体的补助资格时，则存在例外。
- en: Discriminatory algorithms meet the very definition of inequality. For example,
    for informing pretrial release, parole, and sentencing decisions, models calculate
    the probability of future criminal convictions. If the data links race to convictions—showing
    that black defendants have more convictions than white defendants—then the resulting
    model would penalize the score of each black defendant just for being black. There
    couldn’t be a more blatant case of criminalizing blackness.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 歧视性算法恰恰符合不平等的定义。例如，在通知预审释放、假释和判决决策时，模型计算未来犯罪定罪的概率。如果数据将种族与定罪联系起来——显示黑人被告的定罪次数多于白人被告——那么生成的模型会因为黑人的肤色而惩罚每个黑人的分数。这不可能有比将黑色犯罪化更公然的案例了。
- en: Support for discriminatory policies and decision making paves the way for discriminatory
    algorithms. Thirty-six percent of Americans would support a religion-based policy
    to ban Muslims from entering the US. The US bans transgender individuals from
    serving in the military. Three-fourths of Americans support increased airport
    security checks based in part on ethnicity, and 25% of Americans support the use
    of racial profiling by police. Résumés with “white sounding names” receive 50%
    more responses than those with “African American sounding names.”^([1](ch56.xhtml#ch01fn43))
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 支持歧视性政策和决策为歧视性算法铺平了道路。三十六％的美国人支持基于宗教的政策，禁止穆斯林进入美国。美国禁止跨性别人士服役于军队。四分之三的美国人支持基于种族部分增加机场安全检查，25％的美国人支持警方使用种族
    profiling。拥有“白人听起来的名字”的简历收到的回复比拥有“非裔美国人听起来的名字”的简历多50％。^([1](ch56.xhtml#ch01fn43))
- en: Expert support for discriminatory algorithms signals an emerging threat. A paper
    cowritten by Stanford University assistant professor Sharad Goel criticizes the
    standard that algorithms not be discriminatory. The paper recommends discriminatory
    models “when...protected traits add predictive value.” In one lecture, this professor
    said, “We can pretend like we don’t have the information, but it’s there....It’s
    actually good to include race in your algorithm.”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 支持歧视性算法的专家表示了一种新兴威胁。一篇与斯坦福大学助理教授Sharad Goel共同撰写的论文批评了算法不应该歧视的标准。该论文建议在“受保护的特征增加预测价值时”采用歧视性模型。在一堂讲座中，这位教授说：“我们可以假装我们没有这些信息，但事实上我们有...
    把种族包含在你的算法中实际上是件好事。”
- en: University of Pennsylvania criminology professor Richard Berk—who has been commissioned
    by parole departments to build predictive models—also calls for race-based prediction,
    in a paper on the application of machine learning to predict which convicts will
    kill or be killed. Berk writes, “One can employ the best model, which for these
    data happens to include race as a predictor. This is the most technically defensible
    position.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚大学犯罪学教授Richard Berk——曾被假释部门委托构建预测模型——还呼吁基于种族进行预测，在一篇关于应用机器学习预测哪些罪犯会杀人或被杀的论文中。Berk写道：“可以采用最佳模型，这些数据恰好包括种族作为预测因子。这是最技术上合理的立场。”
- en: Data compels these experts to endorse discriminatory algorithms. To them, it
    serves as a rationale for prejudice. It’s as if the data is saying, “Be racist.”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数据促使这些专家支持歧视性算法。对他们来说，这是一种偏见的理由。就像数据在说：“要种族歧视一样。”
- en: But “obeying” the data and generating discriminatory algorithms violates the
    most essential notions of fairness and civil rights. Even if it’s true that my
    group commits more crime, it would violate my rights to be held accountable for
    others, to have my classification count against me.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，“服从”数据并生成歧视性算法违反了公平和公民权利的最基本概念。即使我的群体确实犯更多的罪，也会侵犯我的权利，让我为他人负责，让我的分类对我不利。
- en: Discriminatory computers wreak more havoc than humans enacting discriminatory
    policies. Once it is crystallized as an algorithm, a discriminatory process executes
    automatically, coldly, and on a more significant scale, affecting greater numbers
    of people. Formalized and deployed mechanically, it takes on a concrete, accepted
    status. It becomes the system. More than any human, the computer is “the Man.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 歧视性计算机造成的破坏比实施歧视政策的人类更为严重。一旦作为算法晶化，歧视过程将自动执行，冷酷地在更广泛的范围内影响更多的人群。形式化和机械化地部署，它具有具体和被接受的地位。它成为了系统的一部分。比起任何人类，计算机更像是“那个人”。
- en: So get more data. Just as we human decision makers would strive to see as much
    beyond race as we can when considering a job candidate or criminal suspect, making
    an analogous effort—on a larger scale—to widen the database would enable our computer
    to transcend discrimination as well. Resistance to investing in this effort would
    reveal a willingness to compromise this nation’s freedoms, the very freedoms we
    were trying to protect with law enforcement and immigration policies in the first
    place.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，需要更多数据。就像我们人类决策者在考虑职位候选人或犯罪嫌疑人时会努力尽可能超越种族因素一样，进行类似的努力——在更大的范围内扩展数据库，将使我们的计算机也能超越歧视。抵制投资这一努力将表明愿意妥协本国的自由，这些自由正是我们通过执法和移民政策来保护的。
- en: ^([1](ch56.xhtml#ch01fn43-marker)) References for many specifics in this article
    can be found in Eric Siegel, “When Machines and Data Promote Blatant Discrimination,”
    *San Francisco Chronicle*, September 21, 2018, [*https://oreil.ly/8YzGp*](https://oreil.ly/8YzGp).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch56.xhtml#ch01fn43-marker)) 本文中许多具体内容的参考资料可以在Eric Siegel的《当机器和数据促进公然歧视时》一文中找到，*旧金山纪事报*，2018年9月21日，[*https://oreil.ly/8YzGp*](https://oreil.ly/8YzGp)。
