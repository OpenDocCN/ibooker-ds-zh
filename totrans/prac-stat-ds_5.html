<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Classification"><div class="chapter" id="Classification">
<h1><span class="label">Chapter 5. </span>Classification</h1>


<p>Data scientists are often tasked with automating decisions for business problems.<a data-type="indexterm" data-primary="classification" id="ix_clss"/> Is an email an attempt at phishing? Is a customer likely to churn? Is the web user likely to click on an advertisement? These are all <em>classification</em> problems, a form of <em>supervised learning</em> in which we first train a model on data where the outcome is known and then apply the model to data where the outcome is not known.<a data-type="indexterm" data-primary="supervised learning" id="idm46522850403512"/><a data-type="indexterm" data-primary="prediction" data-seealso="classification" id="idm46522850402808"/>  Classification is perhaps the most important form of prediction: the goal is to predict whether a record is a 1 or a 0 (phishing/not-phishing, click/don’t click, churn/don’t churn), or in some cases, one of several categories (for example, Gmail’s filtering of your inbox into “primary,” “social,” “promotional,” or “forums”).</p>

<p>Often, we need more than a simple binary classification: we want to know the predicted probability that a case belongs to a class.
Rather than having a model simply assign a binary classification, most algorithms can return a probability score (propensity) of belonging to the class of interest.<a data-type="indexterm" data-primary="propensity" data-seealso="probability" id="idm46522850421928"/> In fact, with logistic regression, the default output from <em>R</em> is on the log-odds scale, and this must be transformed to a propensity.<a data-type="indexterm" data-primary="probability" id="idm46522850381032"/> In <em>Python</em>’s <code>scikit-learn</code>, logistic regression, like most classification methods, provides two prediction methods: <code>predict</code> (which returns the class) and <code>predict_proba</code> (which returns probabilities for each class). A sliding cutoff can then be used to convert the propensity score to a decision. The general approach is as follows:</p>
<ol>
<li>
<p>Establish a cutoff probability for the class of interest, above which we consider a record as belonging to that class.</p>
</li>
<li>
<p>Estimate (with any model) the probability that a record belongs to the class of interest.</p>
</li>
<li>
<p>If that probability is above the cutoff probability, assign the new record to the class of interest.</p>
</li>

</ol>

<p>The higher the cutoff, the fewer the records predicted as 1—that is, as belonging to the class of interest. The lower the cutoff, the more the records predicted as 1.</p>

<p>This chapter covers several key techniques for classification and estimating propensities; additional methods that can be used both for classification and for numerical prediction are described in the next chapter.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522850432712">
<h5>More Than Two Categories?</h5>
<p>The vast majority of problems involve a binary response.
Some classification problems, however, involve a response with more than two possible outcomes.<a data-type="indexterm" data-primary="classification" data-secondary="predicting more than two classes" id="idm46522850400872"/>
For example, at the anniversary of a customer’s subscription contract, there might be three outcomes: the customer leaves or “churns” (<em>Y</em> = 2), goes on a month-to-month contract (<em>Y</em> = 1), or signs a new long-term contract (<em>Y</em> = 0).
The goal is to predict <em>Y = j</em> for <em>j =</em> 0, 1, or 2.
Most of the classification methods in this chapter can be applied, either directly or with modest adaptations, to responses that have more than two outcomes.
Even in the case of more than two outcomes, the problem can often be recast into a series of binary problems using conditional probabilities.
For example, to predict the outcome of the contract,
you can solve two binary prediction problems:</p>

<ul>
<li>
<p>Predict whether <em>Y</em> = 0 or <em>Y</em> &gt; 0.</p>
</li>
<li>
<p>Given that <em>Y</em> &gt; 0, predict whether <em>Y</em> = 1 or <em>Y</em> = 2.</p>
</li>
</ul>

<p>In this case, it makes sense to break up the problem into two cases: (1) whether the customer
churns; and (2) if they don’t churn, what type of contract they will choose.
From a model-fitting viewpoint, it is often advantageous to convert the multiclass problem to a series of binary problems.
This is particularly true when one category is much more common than the other categories.</p>
</div></aside>






<section data-type="sect1" data-pdf-bookmark="Naive Bayes"><div class="sect1" id="NaiveBayes">
<h1>Naive Bayes</h1>

<p>The naive Bayes algorithm uses the probability<a data-type="indexterm" data-primary="naive Bayes algorithm" id="ix_naBayes"/><a data-type="indexterm" data-primary="classification" data-secondary="naive Bayes algorithm" id="ix_clsssnBa"/> of observing predictor values, given an outcome, to estimate what is really of interest: the probability of observing outcome <em>Y = i</em>, given a set of predictor values.<sup><a data-type="noteref" id="idm46522850239256-marker" href="ch05.xhtml#idm46522850239256">1</a></sup></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522850238424">
<h5>Key Terms for Naive Bayes</h5><dl>
<dt class="horizontal"><strong><em>Conditional probability</em></strong></dt>
<dd>
<p>The probability of observing some event (say, <em>X = i</em>) given some other event (say, <em>Y = i</em>), written as <math alttext="upper P left-parenthesis upper X Subscript i Baseline vertical-bar upper Y Subscript i Baseline right-parenthesis">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi> </msub>
    <mo>|</mo>
    <msub><mi>Y</mi> <mi>i</mi> </msub>
    <mo>)</mo>
  </mrow>
</math>.<a data-type="indexterm" data-primary="conditional probability" id="idm46522850229256"/><a data-type="indexterm" data-primary="posterior probability" id="idm46522850228520"/></p>
</dd>
<dt class="horizontal"><strong><em>Posterior probability</em></strong></dt>
<dd>
<p>The probability of an outcome after the predictor information has been incorporated (in contrast to the <em>prior probability</em> of outcomes, not taking predictor information into account).</p>
</dd>
</dl>
</div></aside>

<p>To understand naive Bayesian classification, we can start out by imagining complete or exact Bayesian classification.<a data-type="indexterm" data-primary="Bayesian classification" data-seealso="naive Bayes algorithm" id="idm46522850224568"/> For each record to be classified:</p>
<ol>
<li>
<p>Find all the other records with the same predictor profile (i.e., where the predictor values are the same).</p>
</li>
<li>
<p>Determine what classes those records belong to and which class is most prevalent (i.e., probable).</p>
</li>
<li>
<p>Assign that class to the new record.</p>
</li>

</ol>

<p>The preceding approach amounts to finding all the records in the sample that are exactly like the new record to be classified in the sense that all the predictor values are identical.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Predictor variables must be categorical (factor) variables in the standard naive Bayes algorithm.<a data-type="indexterm" data-primary="categorical variables" data-secondary="required for naive Bayes algorithm" id="idm46522850218392"/><a data-type="indexterm" data-primary="factor variables" data-secondary="in naive Bayes algorithm" id="idm46522850217352"/>  See <a data-type="xref" href="#NumericPredictors">“Numeric Predictor Variables”</a> for two workarounds for using continuous variables.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Why Exact Bayesian Classification Is Impractical"><div class="sect2" id="idm46522850215176">
<h2>Why Exact Bayesian Classification Is Impractical</h2>

<p>When the number of predictor variables exceeds a handful, many of the records to be classified will be without exact matches.<a data-type="indexterm" data-primary="naive Bayes algorithm" data-secondary="why exact Bayesian classification is impractical" id="idm46522850213768"/>
Consider a model to predict voting on the basis of demographic variables.
Even a sizable sample may not contain even a single match for a new record who is a male Hispanic with high income from the US Midwest who voted in the last election, did not vote in the prior election, has three daughters and one son, and is divorced.
And this is with just eight variables, a small number for most classification problems.
The addition of just a single new variable with five equally frequent categories reduces the probability of a match by a factor <span class="keep-together">of 5</span>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Naive Solution"><div class="sect2" id="idm46522850211224">
<h2>The Naive Solution</h2>

<p>In the naive Bayes solution, we no longer restrict the probability calculation to those records that match the record to be classified.<a data-type="indexterm" data-primary="naive Bayes algorithm" data-secondary="solution" id="ix_naBayessol"/>
Instead, we use the entire data set.
The naive Bayes modification is as follows:</p>
<ol>
<li>
<p>For a binary response <em>Y = i</em> (<em>i</em> = 0 or 1), estimate the individual conditional probabilities for each predictor <math alttext="upper P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <msub><mi>X</mi> <mi>j</mi> </msub>
    <mo>|</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mo>)</mo>
  </mrow>
</math>; these are the probabilities that the predictor value is in the record when we observe <em>Y = i</em>.
This probability is estimated by the proportion of <em>X<sub>j</sub></em> values among the <em>Y = i</em> records in the training set.</p>
</li>
<li>
<p>Multiply these probabilities by each other, and then by the proportion of records belonging to <em>Y = i</em>.</p>
</li>
<li>
<p>Repeat steps 1 and 2 for all the classes.</p>
</li>
<li>
<p>Estimate a probability for outcome <em>i</em> by taking the value calculated in step 2 for class <em>i</em> and dividing it by the sum of such values for all classes.</p>
</li>
<li>
<p>Assign the record to the class with the highest probability
for this set of predictor values.</p>
</li>

</ol>

<p>This naive Bayes algorithm can also be stated as an equation for the probability of observing outcome <em>Y = i</em>, given a set of predictor values <math alttext="upper X 1 comma ellipsis comma upper X Subscript p Baseline">
  <mrow>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi> </msub>
  </mrow>
</math>:</p>
<div data-type="equation">
<math display="block">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mi>Y</mi>
  <mo>=</mo>
  <mi>i</mi>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>X</mi>
    <mn>1</mn>
  </msub>
  <mo>,</mo>
  <msub>
    <mi>X</mi>
    <mn>2</mn>
  </msub>
  <mo>,</mo>
  <mo>…<!-- … --></mo>
  <mo>,</mo>
  <msub>
    <mi>X</mi>
    <mi>p</mi>
  </msub>
  <mo stretchy="false">)</mo>
</math>
</div>

<p>Here is the full formula for calculating class probabilities using exact Bayes <span class="keep-together">classification</span>:</p>
<div data-type="equation">
<math display="block">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mi>Y</mi>
  <mo>=</mo>
  <mi>i</mi>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>X</mi>
    <mn>1</mn>
  </msub>
  <mo>,</mo>
  <msub>
    <mi>X</mi>
    <mn>2</mn>
  </msub>
  <mo>,</mo>
  <mo>…<!-- … --></mo>
  <mo>,</mo>
  <msub>
    <mi>X</mi>
    <mi>p</mi>
  </msub>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mo>…<!-- … --></mo>
      <mo>,</mo>
      <msub>
        <mi>X</mi>
        <mi>p</mi>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mo stretchy="false">)</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mo>…<!-- … --></mo>
      <mo>,</mo>
      <msub>
        <mi>X</mi>
        <mi>p</mi>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mo stretchy="false">)</mo>
      <mo>+</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mo>…<!-- … --></mo>
      <mo>,</mo>
      <msub>
        <mi>X</mi>
        <mi>p</mi>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
</math>
</div>

<p>Under the naive Bayes assumption of conditional independence, this equation changes into:</p>
<div data-type="equation">
<math display="block">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mi>Y</mi>
  <mo>=</mo>
  <mi>i</mi>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>X</mi>
    <mn>1</mn>
  </msub>
  <mo>,</mo>
  <msub>
    <mi>X</mi>
    <mn>2</mn>
  </msub>
  <mo>,</mo>
  <mo>…<!-- … --></mo>
  <mo>,</mo>
  <msub>
    <mi>X</mi>
    <mi>p</mi>
  </msub>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mn>1</mn>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
      <mo>…<!-- … --></mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mi>p</mi>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mo stretchy="false">)</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mn>1</mn>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mo stretchy="false">)</mo>
      <mo>…<!-- … --></mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mi>p</mi>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>0</mn>
      <mo stretchy="false">)</mo>
      <mo>+</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mn>1</mn>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
      <mo>…<!-- … --></mo>
      <mi>P</mi>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>X</mi>
        <mi>p</mi>
      </msub>
      <mrow class="MJX-TeXAtom-ORD">
        <mo stretchy="false">|</mo>
      </mrow>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
</math>
</div>

<p>Why is this formula called “naive”?
We have made a simplifying assumption that the <em>exact conditional probability</em> of a vector of predictor values, given observing an <span class="keep-together">outcome</span>, is sufficiently well estimated by the product of the individual conditional probabilities <math alttext="upper P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <msub><mi>X</mi> <mi>j</mi> </msub>
    <mo>|</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mo>)</mo>
  </mrow>
</math>.
In other words, in estimating <math alttext="upper P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <msub><mi>X</mi> <mi>j</mi> </msub>
    <mo>|</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mo>)</mo>
  </mrow>
</math> instead of <math alttext="upper P left-parenthesis upper X 1 comma upper X 2 comma ellipsis upper X Subscript p Baseline vertical-bar upper Y equals i right-parenthesis">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>X</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <msub><mi>X</mi> <mi>p</mi> </msub>
    <mo>|</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mo>)</mo>
  </mrow>
</math>, we are assuming <math alttext="upper X Subscript j">
  <msub><mi>X</mi> <mi>j</mi> </msub>
</math> is <em>independent</em> of all the other predictor variables <math alttext="upper X Subscript k">
  <msub><mi>X</mi> <mi>k</mi> </msub>
</math> for <math alttext="k not-equals j">
  <mrow>
    <mi>k</mi>
    <mo>≠</mo>
    <mi>j</mi>
  </mrow>
</math>.</p>

<p>Several packages in <em>R</em> can be used to estimate a naive Bayes model.
The following fits a model to the loan payment data using the <code>klaR</code> package:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">klaR</code><code class="p">)</code>
<code class="n">naive_model</code> <code class="o">&lt;-</code> <code class="nf">NaiveBayes</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">purpose_</code> <code class="o">+</code> <code class="n">home_</code> <code class="o">+</code> <code class="n">emp_len_</code><code class="p">,</code>
                          <code class="n">data</code> <code class="o">=</code> <code class="nf">na.omit</code><code class="p">(</code><code class="n">loan_data</code><code class="p">))</code>
<code class="n">naive_model</code><code class="o">$</code><code class="n">table</code>
<code class="o">$</code><code class="n">purpose_</code>
          <code class="n">var</code>
<code class="n">grouping</code>   <code class="n">credit_card</code> <code class="n">debt_consolidation</code> <code class="n">home_improvement</code> <code class="n">major_purchase</code>
  <code class="n">paid</code> <code class="n">off</code>  <code class="m">0.18759649</code>         <code class="m">0.55215915</code>       <code class="m">0.07150104</code>     <code class="m">0.05359270</code>
  <code class="n">default</code>   <code class="m">0.15151515</code>         <code class="m">0.57571347</code>       <code class="m">0.05981209</code>     <code class="m">0.03727229</code>
          <code class="n">var</code>
<code class="n">grouping</code>      <code class="n">medical</code>      <code class="n">other</code> <code class="n">small_business</code>
  <code class="n">paid</code> <code class="n">off</code> <code class="m">0.01424728</code> <code class="m">0.09990737</code>     <code class="m">0.02099599</code>
  <code class="n">default</code>  <code class="m">0.01433549</code> <code class="m">0.11561025</code>     <code class="m">0.04574126</code>

<code class="o">$</code><code class="n">home_</code>
          <code class="n">var</code>
<code class="n">grouping</code>    <code class="n">MORTGAGE</code>       <code class="n">OWN</code>      <code class="n">RENT</code>
  <code class="n">paid</code> <code class="n">off</code> <code class="m">0.4894800</code> <code class="m">0.0808963</code> <code class="m">0.4296237</code>
  <code class="n">default</code>  <code class="m">0.4313440</code> <code class="m">0.0832782</code> <code class="m">0.4853778</code>

<code class="o">$</code><code class="n">emp_len_</code>
          <code class="n">var</code>
<code class="n">grouping</code>     <code class="o">&lt;</code> <code class="m">1</code> <code class="n">Year</code>   <code class="o">&gt;</code> <code class="m">1</code> <code class="n">Year</code>
  <code class="n">paid</code> <code class="n">off</code> <code class="m">0.03105289</code> <code class="m">0.96894711</code>
  <code class="n">default</code>  <code class="m">0.04728508</code> <code class="m">0.95271492</code></pre>

<p>The output from the model is the conditional probabilities <math alttext="upper P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <msub><mi>X</mi> <mi>j</mi> </msub>
    <mo>|</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mo>)</mo>
  </mrow>
</math>.</p>

<p>In <em>Python</em> we can use <code>sklearn.naive_bayes.MultinomialNB</code> from <code>scikit-learn</code>. We need to convert the categorical features to dummy variables before we fit the model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'purpose_'</code><code class="p">,</code> <code class="s1">'home_'</code><code class="p">,</code> <code class="s1">'emp_len_'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">loan_data</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">prefix</code><code class="o">=</code><code class="s1">''</code><code class="p">,</code> <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">''</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">naive_model</code> <code class="o">=</code> <code class="n">MultinomialNB</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">fit_prior</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">naive_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>It is possible to derive the conditional probabilities from the fitted model using the property <code>feature_log_prob_</code>.</p>

<p>The model can be used to predict the outcome of a new loan. We use the last value of the data set for testing:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">new_loan</code> <code class="o">&lt;-</code> <code class="n">loan_data</code><code class="p">[</code><code class="m">147</code><code class="p">,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'purpose_'</code><code class="p">,</code> <code class="s">'home_'</code><code class="p">,</code> <code class="s">'emp_len_'</code><code class="p">)]</code>
<code class="nf">row.names</code><code class="p">(</code><code class="n">new_loan</code><code class="p">)</code> <code class="o">&lt;-</code> <code class="kc">NULL</code>
<code class="n">new_loan</code>
       	 <code class="n">purpose_</code>    <code class="n">home_</code>  <code class="n">emp_len_</code>
	<code class="m">1</code> <code class="n">small_business</code> <code class="n">MORTGAGE</code>  <code class="o">&gt;</code> <code class="m">1</code> <code class="n">Year</code></pre>

<p>In <em>Python</em>, we get this value as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">new_loan</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">146</code><code class="p">:</code><code class="mi">146</code><code class="p">,</code> <code class="p">:]</code></pre>

<p>In this case, the model predicts a default (<em>R</em>):</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">predict</code><code class="p">(</code><code class="n">naive_model</code><code class="p">,</code> <code class="n">new_loan</code><code class="p">)</code>
<code class="o">$</code><code class="n">class</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="n">default</code>
<code class="n">Levels</code><code class="o">:</code> <code class="n">paid</code> <code class="n">off</code> <code class="n">default</code>

<code class="o">$</code><code class="n">posterior</code>
      <code class="n">paid</code> <code class="n">off</code>   <code class="n">default</code>
<code class="p">[</code><code class="m">1</code><code class="p">,]</code> <code class="m">0.3463013</code> <code class="m">0.6536987</code></pre>

<p>As we discussed, <code>scikit-learn</code>’s classification models have two methods—<code>predict</code>, which returns the predicted class, and <code>predict_proba</code>, which returns the class <span class="keep-together">probabilities</span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s1">'predicted class: '</code><code class="p">,</code> <code class="n">naive_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">new_loan</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code>

<code class="n">probabilities</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">naive_model</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">new_loan</code><code class="p">),</code>
                             <code class="n">columns</code><code class="o">=</code><code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code><code class="o">.</code><code class="n">cat</code><code class="o">.</code><code class="n">categories</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'predicted probabilities'</code><code class="p">,</code> <code class="n">probabilities</code><code class="p">)</code>
<code class="o">--</code>
<code class="n">predicted</code> <code class="n">class</code><code class="p">:</code>  <code class="n">default</code>
<code class="n">predicted</code> <code class="n">probabilities</code>
    <code class="n">default</code>  <code class="n">paid</code> <code class="n">off</code>
<code class="mi">0</code>  <code class="mf">0.653696</code>  <code class="mf">0.346304</code></pre>

<p>The prediction also returns a <code>posterior</code> estimate of the probability of default.<a data-type="indexterm" data-primary="posterior probability" id="idm46522849581000"/><a data-type="indexterm" data-primary="bias" data-secondary="biased estimates from naive Bayes classifier" id="idm46522849580392"/>
The naive Bayesian classifier is known to produce <em>biased</em> estimates.  However, where the goal is to <em>rank</em> records according to the probability that <em>Y</em> = 1, unbiased estimates of probability are not needed, and naive Bayes produces good results.<a data-type="indexterm" data-primary="ranking records, naive Bayes algorithm" id="idm46522849577800"/><a data-type="indexterm" data-primary="naive Bayes algorithm" data-secondary="solution" data-startref="ix_naBayessol" id="idm46522849577128"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Numeric Predictor Variables"><div class="sect2" id="NumericPredictors">
<h2>Numeric Predictor Variables</h2>

<p>The Bayesian classifier works only with
categorical predictors (e.g., with spam classification, where the presence or absence of words, phrases, characters, and so on lies at the heart of the predictive task).<a data-type="indexterm" data-primary="naive Bayes algorithm" data-secondary="numeric predictor variables with" id="idm46522849574392"/><a data-type="indexterm" data-primary="classification" data-secondary="naive Bayes algorithm" data-tertiary="applying to numeric predictor variables" id="idm46522849573400"/><a data-type="indexterm" data-primary="predictor variables" data-secondary="numeric, applying naive Bayes algorithm to" id="idm46522849572168"/>
To apply naive Bayes to numerical predictors, one of two approaches must be taken:</p>

<ul>
<li>
<p>Bin and convert the numerical predictors to categorical predictors and apply the algorithm of the previous section.</p>
</li>
<li>
<p>Use a probability model—for example, the normal distribution (see <a data-type="xref" href="ch02.xhtml#NormalDist">“Normal Distribution”</a>)—to estimate the conditional probability <math alttext="upper P left-parenthesis upper X Subscript j Baseline vertical-bar upper Y equals i right-parenthesis">
  <mrow>
    <mi>P</mi>
    <mo>(</mo>
    <msub><mi>X</mi> <mi>j</mi> </msub>
    <mo>|</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mi>i</mi>
    <mo>)</mo>
  </mrow>
</math>.</p>
</li>
</ul>
<div data-type="caution"><h6>Caution</h6>
<p>When a predictor category is absent in the training data, the algorithm assigns <em>zero probability</em> to the outcome variable in new data, rather than simply ignoring this variable and using the information from other variables, as other methods might.<a data-type="indexterm" data-primary="smoothing parameter, use with naive Bayes algorithm" id="idm46522849560680"/>
Most implementations of Naive Bayes use a smoothing parameter (Laplace Smoothing) to prevent this.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522849559384">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Naive Bayes works with categorical (factor) predictors and outcomes.</p>
</li>
<li>
<p>It asks, “Within each outcome category, which predictor categories are most probable?”</p>
</li>
<li>
<p>That information is then inverted to estimate probabilities of outcome categories, given predictor values.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522849554968">
<h2>Further Reading</h2>

<ul>
<li>
<p><em>The Elements of Statistical Learning</em>, 2nd ed., by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (Springer, 2009).</p>
</li>
<li>
<p>There is a full chapter on naive Bayes in <em>Data Mining for Business Analytics</em> by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions for <em>R</em>, <em>Python</em>, Excel, and JMP).<a data-type="indexterm" data-primary="classification" data-secondary="naive Bayes algorithm" data-startref="ix_clsssnBa" id="idm46522849550440"/><a data-type="indexterm" data-primary="naive Bayes algorithm" data-startref="ix_naBayes" id="idm46522849549160"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Discriminant Analysis"><div class="sect1" id="DiscriminantAnalysis">
<h1>Discriminant Analysis</h1>

<p><em>Discriminant analysis</em> is the earliest statistical classifier; it was introduced by R. A. Fisher<a data-type="indexterm" data-primary="classification" data-secondary="discriminant analysis" id="ix_clssdisc"/><a data-type="indexterm" data-primary="Fisher, R.A." id="idm46522849544328"/><a data-type="indexterm" data-primary="discriminant analysis" id="ix_discan"/> in 1936 in an article published in the <em>Annals of Eugenics</em> journal.<sup><a data-type="noteref" id="idm46522849542200-marker" href="ch05.xhtml#idm46522849542200">2</a></sup></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522849540632">
<h5>Key Terms for Discriminant Analysis</h5><dl>
<dt class="horizontal"><strong>Covariance</strong></dt>
<dd>
<p>A measure of the extent to which one variable varies in concert with another (i.e., similar magnitude and direction).</p>
</dd>
<dt class="horizontal"><strong>Discriminant function</strong></dt>
<dd>
<p>The function that, when applied to the predictor variables, maximizes the separation of the classes.<a data-type="indexterm" data-primary="discriminant function" id="idm46522849535816"/></p>
</dd>
<dt class="horizontal"><strong>Discriminant weights</strong></dt>
<dd>
<p>The scores that result from the application of the discriminant function and are used to estimate probabilities of belonging to one class or another.<a data-type="indexterm" data-primary="discriminant weights" id="idm46522849533272"/></p>
</dd>
</dl>
</div></aside>

<p>While discriminant analysis encompasses several techniques, the most commonly used is <em>linear discriminant analysis</em>, or <em>LDA</em>.<a data-type="indexterm" data-primary="linear discriminant analysis (LDA)" data-seealso="discriminant analysis" id="idm46522849531048"/><a data-type="indexterm" data-primary="discriminant analysis" data-secondary="linear discriminant analysis (LDA)" id="idm46522849495976"/>
The original method proposed by Fisher was actually slightly different from LDA, but the mechanics are essentially the same.
LDA is now less widely used with the advent of more sophisticated techniques, such as tree models and logistic regression.</p>

<p>However, you may still encounter LDA in some applications, and it has links to other more widely used methods (such as principal components analysis; see <a data-type="xref" href="ch07.xhtml#PCA">“Principal Components Analysis”</a>).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Linear discriminant analysis should not be confused with Latent Dirichlet Allocation, also referred to as LDA.<a data-type="indexterm" data-primary="Latent Dirichlet Allocation" id="idm46522849492088"/>
Latent Dirichlet Allocation is used in text and natural language processing and is unrelated to linear discriminant analysis.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Covariance Matrix"><div class="sect2" id="Covariance">
<h2>Covariance Matrix</h2>

<p>To understand discriminant analysis, it is first necessary to introduce the concept of <em>covariance</em> between two or more variables.<a data-type="indexterm" data-primary="classification" data-secondary="discriminant analysis" data-tertiary="covariance matrix" id="idm46522849488232"/><a data-type="indexterm" data-primary="discriminant analysis" data-secondary="covariance matrix" id="idm46522849486984"/><a data-type="indexterm" data-primary="covariance" id="idm46522849486040"/><a data-type="indexterm" data-primary="variables" data-secondary="covariance between" id="idm46522849485368"/>
The covariance measures the relationship between two variables <math alttext="x">
  <mi>x</mi>
</math> and <math alttext="z">
  <mi>z</mi>
</math>.
Denote the mean for each variable by <math alttext="x overbar">
  <mover accent="true"><mi>x</mi> <mo>¯</mo></mover>
</math> and <math alttext="z overbar">
  <mover accent="true"><mi>z</mi> <mo>¯</mo></mover>
</math> (see <a data-type="xref" href="ch01.xhtml#Mean">“Mean”</a>).
The covariance <math alttext="s Subscript x comma z">
  <msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow> </msub>
</math> between <math alttext="x">
  <mi>x</mi>
</math> and <math alttext="z">
  <mi>z</mi>
</math> is given by:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow> </msub>
    <mo>=</mo>
    <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>z</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>z</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow> <mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></mfrac>
  </mrow>
</math>
</div>

<p>where <em>n</em> is the number of records
(note that we divide by <em>n</em> – 1 instead of <em>n</em>; see <a data-type="xref" href="ch01.xhtml#Nminus1">“Degrees of Freedom, and <em>n</em> or <em>n</em> – 1?”</a>).</p>

<p>As with the correlation coefficient (see <a data-type="xref" href="ch01.xhtml#Correlations">“Correlation”</a>), positive values indicate a positive relationship and negative values indicate a negative relationship.
Correlation, however, is constrained to be between –1 and 1, whereas covariance scale depends on the scale of the variables <math alttext="x">
  <mi>x</mi>
</math> and <math alttext="z">
  <mi>z</mi>
</math>.<a data-type="indexterm" data-primary="covariance matrix" id="idm46522849450248"/>
The <em>covariance matrix</em> <math alttext="normal upper Sigma">
  <mi>Σ</mi>
</math> for <math alttext="x">
  <mi>x</mi>
</math> and <math alttext="z">
  <mi>z</mi>
</math> consists of the individual variable variances, <math alttext="s Subscript x Superscript 2">
  <msubsup><mi>s</mi> <mi>x</mi> <mn>2</mn> </msubsup>
</math> and <math alttext="s Subscript z Superscript 2">
  <msubsup><mi>s</mi> <mi>z</mi> <mn>2</mn> </msubsup>
</math>, on the diagonal (where row and column are the same variable) and the covariances between variable pairs on the off-diagonals:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mover accent="true"><mi>Σ</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <mfenced open="[" close="]">
      <mtable>
        <mtr>
          <mtd>
            <msubsup><mi>s</mi> <mi>x</mi> <mn>2</mn> </msubsup>
          </mtd>
          <mtd>
            <msub><mi>s</mi> <mrow><mi>x</mi><mo>,</mo><mi>z</mi></mrow> </msub>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msub><mi>s</mi> <mrow><mi>z</mi><mo>,</mo><mi>x</mi></mrow> </msub>
          </mtd>
          <mtd>
            <msubsup><mi>s</mi> <mi>z</mi> <mn>2</mn> </msubsup>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math>
</div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Recall that the standard deviation is used to normalize a variable to a <em>z</em>-score; the covariance matrix is used in a multivariate extension of this standardization process.<a data-type="indexterm" data-primary="Mahalanobis distance" id="idm46522849423640"/><a data-type="indexterm" data-primary="standard deviation" data-secondary="covariance matrix and" id="idm46522849422936"/><a data-type="indexterm" data-primary="z-scores" id="idm46522849421992"/>
This is known as Mahalanobis distance (see <a data-type="xref" class="pagenum" href="ch06.xhtml#Mahalanobis">“Other Distance Metrics”</a>) and is related to the LDA function.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Fisher’s Linear Discriminant"><div class="sect2" id="idm46522849419672">
<h2>Fisher’s Linear Discriminant</h2>

<p>For simplicity, let’s focus on a classification problem in which we want to predict a binary outcome <em>y</em> using just two continuous numeric variables <math alttext="left-parenthesis x comma z right-parenthesis">
  <mrow>
    <mo>(</mo>
    <mi>x</mi>
    <mo>,</mo>
    <mi>z</mi>
    <mo>)</mo>
  </mrow>
</math>.<a data-type="indexterm" data-primary="Fisher's linear discriminant" id="idm46522849413672"/><a data-type="indexterm" data-primary="discriminant analysis" data-secondary="Fisher's linear discriminant" id="idm46522849412872"/><a data-type="indexterm" data-primary="classification" data-secondary="discriminant analysis" data-tertiary="Fisher's linear discriminant" id="idm46522849411912"/>
Technically, discriminant analysis assumes the predictor variables are normally distributed continuous variables, but, in practice, the method works well even for nonextreme departures from normality, and for binary predictors.
Fisher’s linear discriminant distinguishes
variation <em>between</em> groups, on the one hand, from variation <em>within</em> groups on the other.
Specifically, seeking to divide the records into two groups, linear discriminant analysis (LDA) focuses on maximizing the “between” sum of squares <math alttext="normal upper S normal upper S Subscript normal b normal e normal t normal w normal e normal e normal n">
  <msub><mi> SS </mi> <mi> between </mi> </msub>
</math> (measuring the variation between the two groups)
relative to the “within” sum of squares <math alttext="normal upper S normal upper S Subscript normal w normal i normal t normal h normal i normal n">
  <msub><mi> SS </mi> <mi> within </mi> </msub>
</math> (measuring the within-group variation).
In this case, the two groups correspond to the records <math alttext="left-parenthesis x 0 comma z 0 right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>x</mi> <mn>0</mn> </msub>
    <mo>,</mo>
    <msub><mi>z</mi> <mn>0</mn> </msub>
    <mo>)</mo>
  </mrow>
</math> for which <em>y</em> = 0 and the records
<math alttext="left-parenthesis x 1 comma z 1 right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>z</mi> <mn>1</mn> </msub>
    <mo>)</mo>
  </mrow>
</math> for which <em>y</em> = 1.
The method finds the linear combination <math alttext="w Subscript x Baseline x plus w Subscript z Baseline z">
  <mrow>
    <msub><mi>w</mi> <mi>x</mi> </msub>
    <mi>x</mi>
    <mo>+</mo>
    <msub><mi>w</mi> <mi>z</mi> </msub>
    <mi>z</mi>
  </mrow>
</math> that maximizes that sum of squares ratio:</p>
<div data-type="equation">
<math display="block">
  <mfrac><msub><mi> SS </mi> <mi> between </mi> </msub> <msub><mi> SS </mi> <mi> within </mi> </msub></mfrac>
</math>
</div>

<p>The between  sum of squares is the squared distance between the two group means, and the within sum of squares is the spread around the means within each group, weighted by the covariance matrix.
Intuitively, by maximizing the between sum of squares and minimizing the within  sum of squares, this method yields the greatest separation between the two groups.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="A Simple Example"><div class="sect2" id="idm46522849383976">
<h2>A Simple Example</h2>

<p>The <code>MASS</code> package, associated with the book <em>Modern Applied Statistics with S</em> by W. N. Venables and B. D. Ripley (Springer, 1994), provides a function for LDA with <em>R</em>.<a data-type="indexterm" data-primary="classification" data-secondary="discriminant analysis" data-tertiary="simple example" id="ix_clssdiscex"/><a data-type="indexterm" data-primary="discriminant analysis" data-secondary="simple example" id="ix_discanex"/>
The following applies this function to a sample of loan data using two predictor variables, <code>borrower_score</code> and <code>payment_inc_ratio</code>, and prints out the estimated linear discriminator weights:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">MASS</code><code class="p">)</code>
<code class="n">loan_lda</code> <code class="o">&lt;-</code> <code class="nf">lda</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">borrower_score</code> <code class="o">+</code> <code class="n">payment_inc_ratio</code><code class="p">,</code>
                     <code class="n">data</code><code class="o">=</code><code class="n">loan3000</code><code class="p">)</code>
<code class="n">loan_lda</code><code class="o">$</code><code class="n">scaling</code>
                          <code class="n">LD1</code>
<code class="n">borrower_score</code> 	   <code class="m">7.17583880</code>
<code class="n">payment_inc_ratio</code> <code class="m">-0.09967559</code></pre>

<p>In <em>Python</em>, we can use <code>LinearDiscriminantAnalysis</code> from <code>sklearn.discriminant_analysis</code>. The <code>scalings_</code> property gives the estimated weights:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">loan3000</code><code class="o">.</code><code class="n">outcome</code> <code class="o">=</code> <code class="n">loan3000</code><code class="o">.</code><code class="n">outcome</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="s1">'category'</code><code class="p">)</code>

<code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="s1">'payment_inc_ratio'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">loan_lda</code> <code class="o">=</code> <code class="n">LinearDiscriminantAnalysis</code><code class="p">()</code>
<code class="n">loan_lda</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">loan_lda</code><code class="o">.</code><code class="n">scalings_</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h1>Using Discriminant Analysis for Feature Selection</h1>
<p>If the predictor<a data-type="indexterm" data-primary="feature selection" data-secondary="using discriminant analysis for" id="idm46522849350776"/> variables are normalized prior to running LDA, the discriminator weights are measures of variable importance, thus providing a computationally efficient method of feature selection.</p>
</div>

<p>The <code>lda</code> function can predict the <a data-type="indexterm" data-primary="linear discriminant function" id="idm46522849256152"/>probability of “default” versus “paid off”:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">loan_lda</code><code class="p">)</code>
<code class="nf">head</code><code class="p">(</code><code class="n">pred</code><code class="o">$</code><code class="n">posterior</code><code class="p">)</code>
   <code class="n">paid</code> <code class="n">off</code>   <code class="n">default</code>
<code class="m">1</code> <code class="m">0.4464563</code> <code class="m">0.5535437</code>
<code class="m">2</code> <code class="m">0.4410466</code> <code class="m">0.5589534</code>
<code class="m">3</code> <code class="m">0.7273038</code> <code class="m">0.2726962</code>
<code class="m">4</code> <code class="m">0.4937462</code> <code class="m">0.5062538</code>
<code class="m">5</code> <code class="m">0.3900475</code> <code class="m">0.6099525</code>
<code class="m">6</code> <code class="m">0.5892594</code> <code class="m">0.4107406</code></pre>

<p>The <code>predict_proba</code> method of the fitted model returns the probabilities for the “default” and “paid off” outcomes:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pred</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">loan_lda</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">loan3000</code><code class="p">[</code><code class="n">predictors</code><code class="p">]),</code>
                    <code class="n">columns</code><code class="o">=</code><code class="n">loan_lda</code><code class="o">.</code><code class="n">classes_</code><code class="p">)</code>
<code class="n">pred</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p>A plot of the predictions helps illustrate how LDA works.
Using the output from the <code>predict</code> function, a plot of the estimated probability of default is produced as follows:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">center</code> <code class="o">&lt;-</code> <code class="m">0.5</code> <code class="o">*</code> <code class="p">(</code><code class="n">loan_lda</code><code class="o">$</code><code class="n">mean</code><code class="p">[</code><code class="m">1</code><code class="p">,</code> <code class="p">]</code> <code class="o">+</code> <code class="n">loan_lda</code><code class="o">$</code><code class="n">mean</code><code class="p">[</code><code class="m">2</code><code class="p">,</code> <code class="p">])</code>
<code class="n">slope</code> <code class="o">&lt;-</code> <code class="o">-</code><code class="n">loan_lda</code><code class="o">$</code><code class="n">scaling</code><code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="o">/</code> <code class="n">loan_lda</code><code class="o">$</code><code class="n">scaling</code><code class="p">[</code><code class="m">2</code><code class="p">]</code>
<code class="n">intercept</code> <code class="o">&lt;-</code> <code class="n">center</code><code class="p">[</code><code class="m">2</code><code class="p">]</code> <code class="o">-</code> <code class="n">center</code><code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="o">*</code> <code class="n">slope</code>

<code class="nf">ggplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">lda_df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">borrower_score</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">payment_inc_ratio</code><code class="p">,</code>
                        <code class="n">color</code><code class="o">=</code><code class="n">prob_default</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="m">.6</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">scale_color_gradientn</code><code class="p">(</code><code class="n">colors</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="s">'#ca0020'</code><code class="p">,</code> <code class="s">'#f7f7f7'</code><code class="p">,</code> <code class="s">'#0571b0'</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">scale_x_continuous</code><code class="p">(</code><code class="n">expand</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code><code class="m">0</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">scale_y_continuous</code><code class="p">(</code><code class="n">expand</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code><code class="m">0</code><code class="p">),</code> <code class="n">lim</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">20</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_abline</code><code class="p">(</code><code class="n">slope</code><code class="o">=</code><code class="n">slope</code><code class="p">,</code> <code class="n">intercept</code><code class="o">=</code><code class="n">intercept</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s">'darkgreen'</code><code class="p">)</code></pre>

<p>A similar graph is created in Python using this code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Use scalings and center of means to determine decision boundary</code>
<code class="n">center</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">loan_lda</code><code class="o">.</code><code class="n">means_</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">slope</code> <code class="o">=</code> <code class="o">-</code> <code class="n">loan_lda</code><code class="o">.</code><code class="n">scalings_</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="n">loan_lda</code><code class="o">.</code><code class="n">scalings_</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
<code class="n">intercept</code> <code class="o">=</code> <code class="n">center</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">-</code> <code class="n">center</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="n">slope</code>

<code class="c1"># payment_inc_ratio for borrower_score of 0 and 20</code>
<code class="n">x_0</code> <code class="o">=</code> <code class="p">(</code><code class="mi">0</code> <code class="o">-</code> <code class="n">intercept</code><code class="p">)</code> <code class="o">/</code> <code class="n">slope</code>
<code class="n">x_20</code> <code class="o">=</code> <code class="p">(</code><code class="mi">20</code> <code class="o">-</code> <code class="n">intercept</code><code class="p">)</code> <code class="o">/</code> <code class="n">slope</code>

<code class="n">lda_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">loan3000</code><code class="p">,</code> <code class="n">pred</code><code class="p">[</code><code class="s1">'default'</code><code class="p">]],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">lda_df</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">g</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">scatterplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code>
                    <code class="n">hue</code><code class="o">=</code><code class="s1">'default'</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">lda_df</code><code class="p">,</code>
                    <code class="n">palette</code><code class="o">=</code><code class="n">sns</code><code class="o">.</code><code class="n">diverging_palette</code><code class="p">(</code><code class="mi">240</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">n</code><code class="o">=</code><code class="mi">9</code><code class="p">,</code> <code class="n">as_cmap</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
                    <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">legend</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>

<code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">20</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="mf">0.15</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">((</code><code class="n">x_0</code><code class="p">,</code> <code class="n">x_20</code><code class="p">),</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">20</code><code class="p">),</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="o">*</code><code class="n">loan_lda</code><code class="o">.</code><code class="n">means_</code><code class="o">.</code><code class="n">transpose</code><code class="p">())</code></pre>

<p class="pagebreak-before">The resulting plot is shown in <a data-type="xref" href="#LoanLDA">Figure 5-1</a>. Data points on the left of the diagonal line are predicted to default (probability greater than 0.5).</p>

<figure><div id="LoanLDA" class="figure">
<img src="Images/psd2_0501.png" alt="LDA prediction of loan default using two variables: a score of the borrower's creditworthiness and the payment-to-income ratio. Data points on the left of the diagonal line are predicted to default (probability greater than 0.5)." width="1132" height="869"/>
<h6><span class="label">Figure 5-1. </span>LDA prediction of loan default using two variables: a score of the borrower’s creditworthiness and the payment-to-income ratio</h6>
</div></figure>

<p>Using the discriminant function weights, LDA splits the predictor space into two regions, as shown by the solid line.
The predictions farther away from the line in both directions have a higher level of confidence (i.e., a probability further away from 0.5).</p>
<div data-type="note" epub:type="note"><h1>Extensions of Discriminant Analysis</h1>
<p>More predictor variables: while the text and example in this section used just two predictor variables, LDA works just as well with more than two predictor variables.<a data-type="indexterm" data-primary="predictor variables" data-secondary="using more than two in linear discriminant analysis" id="idm46522848727112"/> The only limiting factor is the number of records (estimating the covariance matrix requires a sufficient number of records per variable, which is typically not an issue in data science applications).<a data-type="indexterm" data-primary="classification" data-secondary="discriminant analysis" data-tertiary="simple example" data-startref="ix_clssdiscex" id="idm46522848725816"/><a data-type="indexterm" data-primary="discriminant analysis" data-secondary="simple example" data-startref="ix_discanex" id="idm46522848724328"/></p>

<p>There are other variants of discriminant analysis.<a data-type="indexterm" data-primary="discriminant analysis" data-secondary="variants of" id="idm46522848722728"/>
The best<a data-type="indexterm" data-primary="quadratic discriminant analysis (QDA)" id="idm46522848721624"/> known is quadratic discriminant analysis (QDA).
Despite its name, QDA is still a linear discriminant function.
The main difference is that in LDA, the covariance matrix is assumed to be the same for the two groups corresponding to <em>Y</em> = 0 and <em>Y</em> = 1.
In QDA, the covariance matrix is allowed to be different for the two groups.
In practice, the difference in most applications is not critical.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522848719320">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Discriminant analysis works with continuous or categorical predictors, as well as with categorical outcomes.</p>
</li>
<li>
<p>Using the covariance matrix, it calculates a <em>linear discriminant function</em>, which is used to distinguish records belonging to one class from those belonging to another.<a data-type="indexterm" data-primary="linear discriminant function" id="idm46522848715768"/></p>
</li>
<li>
<p>This function is applied to the records to derive weights, or scores, for each record (one weight for each possible class), which determines its estimated class.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522849383352">
<h2>Further Reading</h2>

<ul>
<li>
<p>Both <em>The Elements of Statistical Learning</em>, 2nd ed., by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, <em>An Introduction to Statistical Learning</em> by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (Springer, 2013), have a section on discriminant analysis.</p>
</li>
<li>
<p><em>Data Mining for Business Analytics</em> by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions for <em>R</em>, <em>Python</em>, Excel, and JMP) has a full chapter on discriminant <span class="keep-together">analysis</span>.</p>
</li>
<li>
<p>For historical interest, Fisher’s original article on the topic, “The Use of Multiple Measurements in Taxonomic Problems,” as published in 1936 in <em>Annals of Eugenics</em> (now called <em>Annals of Genetics</em>), can be found <a href="https://oreil.ly/_TCR8">online</a>.<a data-type="indexterm" data-primary="classification" data-secondary="discriminant analysis" data-startref="ix_clssdisc" id="idm46522848705096"/><a data-type="indexterm" data-primary="discriminant analysis" data-startref="ix_discan" id="idm46522848703816"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Logistic Regression"><div class="sect1" id="LogisticRegression">
<h1>Logistic Regression</h1>

<p>Logistic regression is analogous to multiple linear regression (see <a data-type="xref" href="ch04.xhtml#Regression">Chapter 4</a>), except the outcome is binary.<a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" id="ix_clsslogr"/><a data-type="indexterm" data-primary="regression" data-secondary="logistic regression" id="ix_regrlog"/><a data-type="indexterm" data-primary="logistic regression" id="ix_logreg"/>
Various transformations are employed to convert the problem to one in which a linear model can be fit.
Like discriminant analysis, and unlike <em>K</em>-Nearest Neighbor and naive Bayes, logistic regression is a structured model approach rather than a data-centric approach.
Due to its fast computational speed and its output of a model that lends itself to rapid scoring of new data, it is a popular method.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522848695192">
<h5>Key Terms for Logistic Regression</h5><dl>
<dt class="horizontal"><strong><em>Logit</em></strong></dt>
<dd>
<p>The function that maps class membership probability to a range from ± ∞ (instead of 0 to 1).</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Log odds (see below)</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Odds</em></strong></dt>
<dd>
<p>The ratio of “success” (1) to “not success” (0).</p>
</dd>
<dt class="horizontal"><strong><em>Log odds</em></strong></dt>
<dd>
<p>The response in the transformed model (now linear), which gets mapped back to a probability.</p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Logistic Response Function and Logit"><div class="sect2" id="idm46522848685960">
<h2>Logistic Response Function and Logit</h2>

<p>The key ingredients for logistic regression are the <em>logistic response function</em> and the <em>logit</em>, in which we map a probability (which is on a 0–1 scale) to a more expansive scale suitable for linear modeling.<a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="logistic response function and logit" id="idm46522848683208"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="logistic response function and logit" id="idm46522848681896"/><a data-type="indexterm" data-primary="logit function" id="idm46522848680936"/></p>

<p>The first step is to think of the outcome variable not as a binary label but as the probability <em>p</em> that the label is a “1.”
Naively, we might be tempted to model <em>p</em> as a linear function of the predictor variables:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>p</mi>
    <mo>=</mo>
    <msub><mi>β</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>β</mi> <mn>1</mn> </msub>
    <msub><mi>x</mi> <mn>1</mn> </msub>
    <mo>+</mo>
    <msub><mi>β</mi> <mn>2</mn> </msub>
    <msub><mi>x</mi> <mn>2</mn> </msub>
    <mo>+</mo>
    <mo>⋯</mo>
    <mo>+</mo>
    <msub><mi>β</mi> <mi>q</mi> </msub>
    <msub><mi>x</mi> <mi>q</mi> </msub>
  </mrow>
</math>
</div>

<p>However, fitting this model does not ensure that <em>p</em> will end up between 0 and 1, as a probability must.</p>

<p>Instead, we model <em>p</em> by applying a  <em>logistic response</em> or <em>inverse logit</em> function <a data-type="indexterm" data-primary="logistic response function" id="idm46522848662488"/><a data-type="indexterm" data-primary="inverse logit function" id="idm46522848661784"/>to the <span class="keep-together">predictors</span>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>p</mi>
    <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn> </msub><mo>+</mo><msub><mi>β</mi> <mn>1</mn> </msub><msub><mi>x</mi> <mn>1</mn> </msub><mo>+</mo><msub><mi>β</mi> <mn>2</mn> </msub><msub><mi>x</mi> <mn>2</mn> </msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi> <mi>q</mi> </msub><msub><mi>x</mi> <mi>q</mi> </msub><mo>)</mo></mrow> </msup></mrow></mfrac>
  </mrow>
</math>
</div>

<p>This transform ensures that the <em>p</em> stays between 0 and 1.</p>

<p>To get the exponential expression out of the denominator, we consider <em>odds</em> instead of probabilities.<a data-type="indexterm" data-primary="odds" id="idm46522848643928"/>
Odds, familiar to bettors everywhere, are the ratio of “successes” (1) to “nonsuccesses” (0).  In terms of probabilities, odds are the probability of an event divided by the probability that the event will not occur.  For example, if the probability that a horse will win is 0.5, the probability of “won’t win” is (1 – 0.5) = 0.5, and the odds are 1.0:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi> Odds </mi>
    <mrow>
      <mo>(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac>
  </mrow>
</math>
</div>

<p>We can obtain the probability<a data-type="indexterm" data-primary="odds" data-secondary="obtaining probability from" id="idm46522848634872"/> from the odds<a data-type="indexterm" data-primary="inverse odds function" id="idm46522848633704"/> using the inverse odds function:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>p</mi>
    <mo>=</mo>
    <mfrac><mi> Odds </mi> <mrow><mn>1</mn><mo>+</mo><mi> Odds </mi></mrow></mfrac>
  </mrow>
</math>
</div>

<p>We combine this with the<a data-type="indexterm" data-primary="logistic response function" id="idm46522848627816"/> logistic response function, shown earlier, to get:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi> Odds </mi>
    <mrow>
      <mo>(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msup><mi>e</mi> <mrow><msub><mi>β</mi> <mn>0</mn> </msub><mo>+</mo><msub><mi>β</mi> <mn>1</mn> </msub><msub><mi>x</mi> <mn>1</mn> </msub><mo>+</mo><msub><mi>β</mi> <mn>2</mn> </msub><msub><mi>x</mi> <mn>2</mn> </msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>β</mi> <mi>q</mi> </msub><msub><mi>x</mi> <mi>q</mi> </msub></mrow> </msup>
  </mrow>
</math>
</div>

<p>Finally, taking the logarithm of both sides, we get an expression that involves a linear function of the predictors:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mo form="prefix">log</mo>
    <mrow>
      <mo>(</mo>
      <mi> Odds </mi>
      <mrow>
        <mo>(</mo>
        <mi>Y</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msub><mi>β</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>β</mi> <mn>1</mn> </msub>
    <msub><mi>x</mi> <mn>1</mn> </msub>
    <mo>+</mo>
    <msub><mi>β</mi> <mn>2</mn> </msub>
    <msub><mi>x</mi> <mn>2</mn> </msub>
    <mo>+</mo>
    <mo>⋯</mo>
    <mo>+</mo>
    <msub><mi>β</mi> <mi>q</mi> </msub>
    <msub><mi>x</mi> <mi>q</mi> </msub>
  </mrow>
</math>
</div>

<p>The <em>log-odds</em> function,<a data-type="indexterm" data-primary="logit function" id="idm46522848591544"/> also known as the <em>logit</em> function, maps the probability <em>p</em>  from <math alttext="left-parenthesis 0 comma 1 right-parenthesis">
  <mrow>
    <mo>(</mo>
    <mn>0</mn>
    <mo>,</mo>
    <mn>1</mn>
    <mo>)</mo>
  </mrow>
</math> to any value <math alttext="left-parenthesis negative normal infinity comma plus normal infinity right-parenthesis">
  <mrow>
    <mo>(</mo>
    <mo>-</mo>
    <mi>∞</mi>
    <mo>,</mo>
    <mo>+</mo>
    <mi>∞</mi>
    <mo>)</mo>
  </mrow>
</math>—see <a data-type="xref" href="#LogitFun">Figure 5-2</a>.
The transformation circle is complete; we have used a linear model to predict a probability, which we can in turn map to a class label by applying a cutoff rule—any record with a probability greater than the cutoff is classified as a 1.<a data-type="indexterm" data-primary="log-odds function" data-see="logit function" id="idm46522848580696"/></p>

<figure><div id="LogitFun" class="figure">
<img src="Images/psd2_0502.png" alt="Graph of the logit function that maps a probability to a scale suitable for a linear model" width="1457" height="1163"/>
<h6><span class="label">Figure 5-2. </span>Graph of the logit function that maps a probability to a scale suitable for a linear model</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Logistic Regression and the GLM"><div class="sect2" id="GLM">
<h2>Logistic Regression and the GLM</h2>

<p>The response in the logistic regression formula is the log odds of a binary outcome of 1.<a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="generalized linear models" id="ix_clsslogrGLM"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="and the generalized linear model" data-secondary-sortas="generalized" id="ix_logregGLM"/><a data-type="indexterm" data-primary="generalized linear models (GLMs)" id="ix_GLMs"/>
We observe only the binary outcome, not the log odds, so special statistical methods are needed to fit the equation.
Logistic regression is a special instance of a <em>generalized linear model</em> (GLM) developed to extend linear regression to other settings.<a data-type="indexterm" data-primary="linear regression" data-secondary="generalized linear model (GLM)" id="idm46522848501096"/></p>

<p>In <em>R</em>, to fit a logistic regression, the <code>glm</code> function is used with the <code>family</code> parameter set to <code>binomial</code>.
The following code fits a logistic regression to the personal loan data introduced in <a data-type="xref" href="ch06.xhtml#KNN">“K-Nearest Neighbors”</a>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">logistic_model</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">payment_inc_ratio</code> <code class="o">+</code> <code class="n">purpose_</code> <code class="o">+</code>
                        <code class="n">home_</code> <code class="o">+</code> <code class="n">emp_len_</code> <code class="o">+</code> <code class="n">borrower_score</code><code class="p">,</code>
                      <code class="n">data</code><code class="o">=</code><code class="n">loan_data</code><code class="p">,</code> <code class="n">family</code><code class="o">=</code><code class="s">'binomial'</code><code class="p">)</code>
<code class="n">logistic_model</code>

<code class="n">Call</code><code class="o">:</code>  <code class="nf">glm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">outcome</code> <code class="o">~</code> <code class="n">payment_inc_ratio</code> <code class="o">+</code> <code class="n">purpose_</code> <code class="o">+</code> <code class="n">home_</code> <code class="o">+</code>
    <code class="n">emp_len_</code> <code class="o">+</code> <code class="n">borrower_score</code><code class="p">,</code> <code class="n">family</code> <code class="o">=</code> <code class="s">"binomial"</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
               <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>           <code class="n">payment_inc_ratio</code>
                   <code class="m">1.63809</code>                     <code class="m">0.07974</code>
<code class="n">purpose_debt_consolidation</code>    <code class="n">purpose_home_improvement</code>
                   <code class="m">0.24937</code>                     <code class="m">0.40774</code>
    <code class="n">purpose_major_purchase</code>             <code class="n">purpose_medical</code>
                   <code class="m">0.22963</code>                     <code class="m">0.51048</code>
             <code class="n">purpose_other</code>      <code class="n">purpose_small_business</code>
                   <code class="m">0.62066</code>                     <code class="m">1.21526</code>
                  <code class="n">home_OWN</code>                   <code class="n">home_RENT</code>
                   <code class="m">0.04833</code>                     <code class="m">0.15732</code>
         <code class="n">emp_len_</code> <code class="o">&gt;</code> <code class="m">1</code> <code class="n">Year</code>              <code class="n">borrower_score</code>
                  <code class="m">-0.35673</code>                    <code class="m">-4.61264</code>

<code class="n">Degrees</code> <code class="n">of</code> <code class="n">Freedom</code><code class="o">:</code> <code class="m">45341</code> <code class="nf">Total </code><code class="p">(</code><code class="n">i.e.</code> <code class="n">Null</code><code class="p">);</code>  <code class="m">45330</code> <code class="n">Residual</code>
<code class="n">Null</code> <code class="n">Deviance</code><code class="o">:</code>	    <code class="m">62860</code>
<code class="n">Residual</code> <code class="n">Deviance</code><code class="o">:</code> <code class="m">57510</code> 	<code class="n">AIC</code><code class="o">:</code> <code class="m">57540</code></pre>

<p>The response is <code>outcome</code>, which takes a 0 if the loan is paid off and a 1 if the loan defaults.
<code>purpose_</code> and <code>home_</code> are factor variables representing the purpose of the loan and the home ownership status.
As in linear regression, a factor variable with <em>P</em> levels is represented with <em>P</em> – 1 columns.
By default in <em>R</em>, the <em>reference</em> coding is used, and the levels are all compared <a data-type="indexterm" data-primary="reference coding" id="idm46522848380648"/>to the reference level (see <a data-type="xref" href="ch04.xhtml#FactorsRegression">“Factor Variables in Regression”</a>).
The reference levels for these factors are <code>credit_card</code> and <code>MORTGAGE</code>, respectively.
The variable <code>borrower_score</code> is a score from 0 to 1 representing the creditworthiness of the borrower (from poor to excellent).
This variable was created from several other variables using <em>K</em>-Nearest Neighbor—see <a data-type="xref" href="ch06.xhtml#KnnFeatureEngine">“KNN as a Feature Engine”</a>.</p>

<p>In <em>Python</em>, we use the <code>scikit-learn</code> class <code>LogisticRegression</code> from <code>sklearn.linear_model</code>. The arguments <code>penalty</code> and <code>C</code> are used to prevent overfitting by L1 or L2 regularization. Regularization is switched on by default. In order to fit without regularization, we set <code>C</code> to a very large value. The <code>solver</code> argument selects the used minimizer; the method <code>liblinear</code> is the default:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="s1">'purpose_'</code><code class="p">,</code> <code class="s1">'home_'</code><code class="p">,</code> <code class="s1">'emp_len_'</code><code class="p">,</code>
              <code class="s1">'borrower_score'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">loan_data</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">prefix</code><code class="o">=</code><code class="s1">''</code><code class="p">,</code> <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">''</code><code class="p">,</code>
                   <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">logit_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">penalty</code><code class="o">=</code><code class="s1">'l2'</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">1e42</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>
<code class="n">logit_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>In contrast to <em>R</em>, <code>scikit-learn</code> derives the classes from the unique values in <code>y</code> (<em>paid off</em> and <em>default</em>). Internally, the classes are ordered alphabetically. As this is the reverse order from the factors used in <em>R</em>, you will see that the coefficients are reversed. The <code>predict</code> method returns the class label and <code>predict_proba</code> returns the probabilities in the order available from the attribute <code>logit_reg.classes_</code>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Generalized Linear Models"><div class="sect2" id="idm46522848240024">
<h2>Generalized Linear Models</h2>

<p>Generalized linear models (GLMs) are characterized<a data-type="indexterm" data-primary="generalized linear models (GLMs)" data-secondary="characteristics and applications of" id="idm46522848238632"/> by two main components:</p>

<ul>
<li>
<p>A probability distribution or family (binomial in the case of logistic regression)</p>
</li>
<li>
<p>A link function—i.e., a transformation function that maps the response to the predictors (logit in the case of logistic regression)</p>
</li>
</ul>

<p>Logistic regression is by far the most common form of GLM.
A data scientist will encounter other types of GLMs.
Sometimes a log link function is used instead of the logit;
in practice, use of a log link is unlikely to lead to very different results for most applications.
The Poisson distribution is<a data-type="indexterm" data-primary="Poisson distributions" id="idm46522848234568"/> commonly used to model count data (e.g., the number of times a user visits a web page in a certain amount of time).
Other families include negative binomial and gamma, often used to model elapsed time (e.g., time to failure).
In contrast to logistic regression, application of GLMs with these models is more nuanced and involves greater care.
These are best avoided unless you are familiar with and understand the utility and pitfalls of these methods.<a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="generalized linear models" data-startref="ix_clsslogrGLM" id="idm46522848233272"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="and the generalized linear model" data-secondary-sortas="generalized" data-startref="ix_logregGLM" id="idm46522848231768"/><a data-type="indexterm" data-primary="generalized linear models (GLMs)" data-startref="ix_GLMs" id="idm46522848230264"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Predicted Values from Logistic Regression"><div class="sect2" id="LogisticPrediction">
<h2>Predicted Values from Logistic Regression</h2>

<p>The predicted value from logistic regression is in terms of the log odds: <math alttext="ModifyingAbove upper Y With caret equals log left-parenthesis normal upper O normal d normal d normal s left-parenthesis upper Y equals 1 right-parenthesis right-parenthesis">
  <mrow>
    <mover accent="true"><mi>Y</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <mo form="prefix">log</mo>
    <mrow>
      <mo>(</mo>
      <mi> Odds </mi>
      <mrow>
        <mo>(</mo>
        <mi>Y</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>)</mo>
      </mrow>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>.<a data-type="indexterm" data-primary="prediction" data-secondary="predicted values from logistic regression" id="idm46522848218648"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="predicted values from" id="idm46522848217608"/><a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="predicted values from" id="idm46522848216664"/>
The predicted probability is given by the logistic response function:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mover accent="true"><mi>p</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mover accent="true"><mi>Y</mi> <mo>^</mo></mover></mrow> </msup></mrow></mfrac>
  </mrow>
</math>
</div>

<p>For example, look at the predictions from the model <code>logistic_model</code> in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">logistic_model</code><code class="p">)</code>
<code class="nf">summary</code><code class="p">(</code><code class="n">pred</code><code class="p">)</code>
     <code class="n">Min.</code>   <code class="m">1</code><code class="n">st</code> <code class="n">Qu.</code>    <code class="n">Median</code>      <code class="n">Mean</code>   <code class="m">3</code><code class="n">rd</code> <code class="n">Qu.</code>      <code class="n">Max.</code>
<code class="m">-2.704774</code> <code class="m">-0.518825</code> <code class="m">-0.008539</code>  <code class="m">0.002564</code>  <code class="m">0.505061</code>  <code class="m">3.509606</code></pre>

<p>In <em>Python</em>, we can convert the probabilities into a data frame and use the <code>describe</code> method to get these
 characteristics of the distribution:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pred</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">logit_reg</code><code class="o">.</code><code class="n">predict_log_proba</code><code class="p">(</code><code class="n">X</code><code class="p">),</code>
                    <code class="n">columns</code><code class="o">=</code><code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code><code class="o">.</code><code class="n">cat</code><code class="o">.</code><code class="n">categories</code><code class="p">)</code>
<code class="n">pred</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code></pre>

<p class="pagebreak-before">Converting these values to probabilities is a simple transform:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">prob</code> <code class="o">&lt;-</code> <code class="m">1</code><code class="o">/</code><code class="p">(</code><code class="m">1</code> <code class="o">+</code> <code class="nf">exp</code><code class="p">(</code><code class="o">-</code><code class="n">pred</code><code class="p">))</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="n">prob</code><code class="p">)</code>
   <code class="n">Min.</code> <code class="m">1</code><code class="n">st</code> <code class="n">Qu.</code>  <code class="n">Median</code>    <code class="n">Mean</code> <code class="m">3</code><code class="n">rd</code> <code class="n">Qu.</code>    <code class="n">Max.</code>
<code class="m">0.06269</code> <code class="m">0.37313</code> <code class="m">0.49787</code> <code class="m">0.50000</code> <code class="m">0.62365</code> <code class="m">0.97096</code></pre>

<p>The probabilities are directly available using the <code>predict_proba</code> methods in <code>scikit-learn</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pred</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">logit_reg</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">),</code>
                    <code class="n">columns</code><code class="o">=</code><code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code><code class="o">.</code><code class="n">cat</code><code class="o">.</code><code class="n">categories</code><code class="p">)</code>
<code class="n">pred</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code></pre>

<p>These are on a scale from 0 to 1 and don’t yet declare whether the predicted value is default or paid off.
We could declare any value greater than 0.5 as default.
In practice, a lower cutoff is often appropriate if the goal is to identify members of a rare class (see <a data-type="xref" href="#RareClassProblem">“The Rare Class Problem”</a>).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Interpreting the Coefficients and Odds Ratios"><div class="sect2" id="OddsRatio">
<h2>Interpreting the Coefficients and Odds Ratios</h2>

<p>One advantage of logistic regression is that it produces a model that can be scored to <a data-type="indexterm" data-primary="logistic regression" data-secondary="interpreting coefficients and odds ratio" id="idm46522848029768"/><a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="interpreting coefficients and odds ratio" id="idm46522848028760"/>new data rapidly, without recomputation.  Another is the relative ease of interpretation of the model, as compared with other classification methods.<a data-type="indexterm" data-primary="coefficients" data-secondary="in logistic regression" id="idm46522848027224"/>
The key conceptual idea is understanding an <em>odds ratio</em>.<a data-type="indexterm" data-primary="odds ratio" id="idm46522848025768"/>
The odds ratio is <a data-type="indexterm" data-primary="factor variables" data-secondary="binary, odds ratio for" id="idm46522848024904"/>easiest to understand for a binary factor variable <em>X</em>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>odds</mtext>
    <mspace width="4.pt"/>
    <mtext>ratio</mtext>
    <mo>=</mo>
    <mfrac><mrow><mi> Odds </mi><mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>1</mn><mo>)</mo></mrow> <mrow><mi> Odds </mi><mo>(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mo>|</mo><mi>X</mi><mo>=</mo><mn>0</mn><mo>)</mo></mrow></mfrac>
  </mrow>
</math>
</div>

<p>This is interpreted as the odds that <em>Y</em> = 1 when <em>X</em> = 1 versus the odds that  <em>Y</em> = 1 when <em>X</em> = 0.
If the odds ratio is 2,
then the odds that <em>Y</em> = 1 are two times higher when <em>X</em> = 1 versus when <em>X</em> = 0.</p>

<p>Why bother with an odds ratio rather than probabilities?
We work with odds because the coefficient <math alttext="beta Subscript j">
  <msub><mi>β</mi> <mi>j</mi> </msub>
</math> in the logistic regression is the log of the odds ratio for <math alttext="upper X Subscript j">
  <msub><mi>X</mi> <mi>j</mi> </msub>
</math>.</p>

<p>An example will make this more explicit.
For the model fit in <a data-type="xref" href="#GLM">“Logistic Regression and the GLM”</a>,
the regression coefficient for <code>purpose_small_business</code> is 1.21526.
This means that a loan to a small business compared to a loan to pay off credit card debt reduces the odds of defaulting versus being paid off by <math>
  <mrow>
    <mi>e</mi>
    <mi>x</mi>
    <mi>p</mi>
    <mo>(</mo>
    <mn>1</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>21526</mn>
    <mo>)</mo>
    <mo>≈</mo>
    <mn>3</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>4</mn>
  </mrow>
</math>.
Clearly, loans for the purpose of creating or expanding a small business are considerably riskier than other types of loans.</p>

<p><a data-type="xref" href="#LogOddsRatio">Figure 5-3</a> shows the relationship between the odds ratio and the log-odds ratio for odds ratios greater than 1.
Because the coefficients are on the log scale, an increase of 1 in the coefficient results in an increase of <math>
  <mrow>
    <mi>e</mi>
    <mi>x</mi>
    <mi>p</mi>
    <mo>(</mo>
    <mn>1</mn>
    <mo>)</mo>
    <mo>≈</mo>
    <mn>2</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>72</mn>
  </mrow>
</math> in the odds ratio.</p>

<figure><div id="LogOddsRatio" class="figure">
<img src="Images/psd2_0503.png" alt="The relationship between the odds ratio and the log-odds ratio" width="1456" height="1155"/>
<h6><span class="label">Figure 5-3. </span>The relationship between the odds ratio and the log-odds ratio</h6>
</div></figure>

<p>Odds ratios<a data-type="indexterm" data-primary="odds ratio" data-secondary="relationship with log-odds ratio" id="idm46522847906392"/><a data-type="indexterm" data-primary="log-odds ratio and odds ratio" id="idm46522847905416"/> for numeric variables <em>X</em> can be interpreted similarly: they measure the change in the odds ratio for a unit change in <em>X</em>.
For example, the effect of increasing the payment-to-income ratio from, say, 5 to 6 increases the odds of the loan defaulting by a factor of <math>
  <mrow>
    <mi>e</mi>
    <mi>x</mi>
    <mi>p</mi>
    <mo>(</mo>
    <mn>0</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>08244</mn>
    <mo>)</mo>
    <mo>≈</mo>
    <mn>1</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>09</mn>
  </mrow>
</math>.
The variable <code>borrower_score</code> is a score on the borrowers’ creditworthiness and ranges from 0 (low) to 1 (high).
The odds of the best borrowers relative to the worst borrowers defaulting on their loans is smaller by a <span class="keep-together">factor</span> of <math>
  <mrow>
    <mi>e</mi>
    <mi>x</mi>
    <mi>p</mi>
    <mo>(</mo>
    <mo>-</mo>
    <mn>4</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>61264</mn>
    <mo>)</mo>
    <mo>≈</mo>
    <mn>0</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>01</mn>
  </mrow>
</math>.
In other words, the default risk from the borrowers with the poorest creditworthiness is 100 times greater than that of the best borrowers!</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Linear and Logistic Regression: Similarities and Differences"><div class="sect2" id="LinearLogistic">
<h2>Linear and Logistic Regression: Similarities and Differences</h2>

<p>Linear regression and logistic regression share many commonalities.
Both assume a parametric linear form relating the predictors with the response.<a data-type="indexterm" data-primary="logistic regression" data-secondary="comparison to linear regression" id="ix_logregcmp"/><a data-type="indexterm" data-primary="linear regression" data-secondary="comparison to logistic regression" id="ix_linregcmp"/><a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="comparison to linear regression" id="ix_clsslogrcmp"/><a data-type="indexterm" data-primary="regression" data-secondary="logistic regression" data-tertiary="comparison to linear regression" id="ix_regrlogcmp"/>
Exploring and finding the best model are done in very similar ways.
Extensions to the linear model, like the use of a spline transform of a predictor (see <a data-type="xref" href="ch04.xhtml#Splines">“Splines”</a>), are equally applicable in the logistic regression setting. Logistic regression differs in two fundamental ways:</p>

<ul>
<li>
<p>The way the model is fit (least squares is not applicable)</p>
</li>
<li>
<p>The nature and analysis of the residuals from the model</p>
</li>
</ul>










<section data-type="sect3" data-pdf-bookmark="Fitting the model"><div class="sect3" id="idm46522847874776">
<h3>Fitting the model</h3>

<p>Linear regression is fit using least squares, and the quality of the fit is evaluated using RMSE and R-squared statistics.<a data-type="indexterm" data-primary="fitting the model" data-secondary="linear versus logistic regression" id="idm46522847872840"/>  In logistic regression (unlike in linear regression), there is no closed-form solution, and the model must be fit using  <em>maximum likelihood estimation</em> (MLE).<a data-type="indexterm" data-primary="maximum likelihood estimation (MLE)" id="idm46522847871240"/>    Maximum likelihood estimation is a process that tries to find the model that is most likely to have produced the data we see.
In the logistic regression equation, the response is not 0 or 1 but rather an estimate of the log odds that the response is 1.
The MLE finds the solution such that the estimated log odds best describes the observed outcome.
The mechanics of the algorithm involve a quasi-Newton optimization that iterates between <a data-type="indexterm" data-primary="Fisher's scoring" id="idm46522847869880"/>a scoring step (<em>Fisher’s scoring</em>), based on the current parameters, and an update to the parameters to improve the fit.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="MLE">
<h5>Maximum Likelihood Estimation</h5>
<p>Here is a bit more detail, if you like statistical symbols:  start with a set of data <math alttext="left-parenthesis upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>X</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>n</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> and a probability model <math alttext="upper P Subscript theta Baseline left-parenthesis upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n Baseline right-parenthesis">
  <mrow>
    <msub><mi>P</mi> <mi>θ</mi> </msub>
    <mrow>
      <mo>(</mo>
      <msub><mi>X</mi> <mn>1</mn> </msub>
      <mo>,</mo>
      <msub><mi>X</mi> <mn>2</mn> </msub>
      <mo>,</mo>
      <mo>⋯</mo>
      <mo>,</mo>
      <msub><mi>X</mi> <mi>n</mi> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math> that depends on a set of parameters <math alttext="theta">
  <mi>θ</mi>
</math>.
The goal of MLE is to find the set of parameters <math alttext="ModifyingAbove theta With caret">
  <mover accent="true"><mi>θ</mi> <mo>^</mo></mover>
</math> that maximizes the value of <math alttext="upper P Subscript theta Baseline left-parenthesis upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n Baseline right-parenthesis">
  <mrow>
    <msub><mi>P</mi> <mi>θ</mi> </msub>
    <mrow>
      <mo>(</mo>
      <msub><mi>X</mi> <mn>1</mn> </msub>
      <mo>,</mo>
      <msub><mi>X</mi> <mn>2</mn> </msub>
      <mo>,</mo>
      <mo>⋯</mo>
      <mo>,</mo>
      <msub><mi>X</mi> <mi>n</mi> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>;
that is, it maximizes the probability of observing <math alttext="left-parenthesis upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript n Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>X</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>n</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> given the model <math alttext="upper P">
  <mi>P</mi>
</math>.
In the fitting process, the<a data-type="indexterm" data-primary="deviance" id="idm46522847827848"/> model is evaluated using a metric called <em>deviance</em>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mi> deviance </mi>
    <mo>=</mo>
    <mo>-</mo>
    <mn>2</mn>
    <mo form="prefix">log</mo>
    <mfenced separators="" open="(" close=")">
      <msub><mi>P</mi> <mover accent="true"><mi>θ</mi> <mo>^</mo></mover> </msub>
      <mrow>
        <mo>(</mo>
        <msub><mi>X</mi> <mn mathvariant="script">1</mn> </msub>
        <mo>,</mo>
        <msub><mi>X</mi> <mn mathvariant="script">2</mn> </msub>
        <mo>,</mo>
        <mo>⋯</mo>
        <mo>,</mo>
        <msub><mi>X</mi> <mi>n</mi> </msub>
        <mo>)</mo>
      </mrow>
    </mfenced>
  </mrow>
</math>
</div>

<p>Lower deviance corresponds to a better fit.</p>
</div></aside>

<p>Fortunately, most practitioners don’t need to concern themselves with the details of the fitting algorithm since this is handled by the software.
Most data scientists will not need to worry about the fitting method, other than understanding that it is a way to find a good model under certain assumptions.</p>
<div data-type="caution"><h1>Handling Factor Variables</h1>
<p>In logistic regression, factor variables should be coded as in linear regression; see <a data-type="xref" href="ch04.xhtml#FactorsRegression">“Factor Variables in Regression”</a>.
In <em>R</em> and other software, this is normally handled automatically, and generally reference encoding is used.<a data-type="indexterm" data-primary="factor variables" data-secondary="coding in logistic regression" id="idm46522847805576"/>
All of the other classification methods covered in this chapter typically use the one hot encoder representation (see <a data-type="xref" href="ch06.xhtml#OneHotEncoder">“One Hot Encoder”</a>).
In <em>Python</em>’s <code>scikit-learn</code>, it is easiest to use one hot encoding, which means that only <em>n – 1</em> of the resulting dummies can be used in the <span class="keep-together">regression</span>.<a data-type="indexterm" data-primary="regression" data-secondary="logistic regression" data-tertiary="comparison to linear regression" data-startref="ix_regrlogcmp" id="idm46522847801496"/><a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="comparison to linear regression" data-startref="ix_clsslogrcmp" id="idm46522847799928"/><a data-type="indexterm" data-primary="linear regression" data-secondary="comparison to logistic regression" data-startref="ix_linregcmp" id="idm46522847798424"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="comparison to linear regression" data-startref="ix_logregcmp" id="idm46522847797192"/></p>
</div>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Assessing the Model"><div class="sect2" id="idm46522847795960">
<h2>Assessing the Model</h2>

<p>Like other classification methods, logistic regression is assessed by how accurately<a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="assessing the model" data-startref="ix_logregmod" id="idm46522847794264"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="assessing the model" id="ix_logregmod"/> the model classifies new data (see <a data-type="xref" href="#EvaluatingModels">“Evaluating Classification Models”</a>).
As with linear regression, some additional standard statistical tools are available to examine and improve the model.
Along with the estimated coefficients, <em>R</em> reports the standard error of the coefficients (SE), a <em>z</em>-value, and a p-value:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">summary</code><code class="p">(</code><code class="n">logistic_model</code><code class="p">)</code>

<code class="n">Call</code><code class="o">:</code>
<code class="nf">glm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">outcome</code> <code class="o">~</code> <code class="n">payment_inc_ratio</code> <code class="o">+</code> <code class="n">purpose_</code> <code class="o">+</code> <code class="n">home_</code> <code class="o">+</code>
    <code class="n">emp_len_</code> <code class="o">+</code> <code class="n">borrower_score</code><code class="p">,</code> <code class="n">family</code> <code class="o">=</code> <code class="s">"binomial"</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">)</code>

<code class="n">Deviance</code> <code class="n">Residuals</code><code class="o">:</code>
     <code class="n">Min</code>        <code class="m">1</code><code class="n">Q</code>    <code class="n">Median</code>        <code class="m">3</code><code class="n">Q</code>       <code class="n">Max</code>
<code class="m">-2.51951</code>  <code class="m">-1.06908</code>  <code class="m">-0.05853</code>   <code class="m">1.07421</code>   <code class="m">2.15528</code>

<code class="n">Coefficients</code><code class="o">:</code>
                            <code class="n">Estimate</code> <code class="n">Std.</code> <code class="n">Error</code> <code class="n">z</code> <code class="n">value</code> <code class="nf">Pr</code><code class="p">(</code><code class="o">&gt;|</code><code class="n">z</code><code class="o">|</code><code class="p">)</code>
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>                 <code class="m">1.638092</code>   <code class="m">0.073708</code>  <code class="m">22.224</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">payment_inc_ratio</code>           <code class="m">0.079737</code>   <code class="m">0.002487</code>  <code class="m">32.058</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">purpose_debt_consolidation</code>  <code class="m">0.249373</code>   <code class="m">0.027615</code>   <code class="m">9.030</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">purpose_home_improvement</code>    <code class="m">0.407743</code>   <code class="m">0.046615</code>   <code class="m">8.747</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">purpose_major_purchase</code>      <code class="m">0.229628</code>   <code class="m">0.053683</code>   <code class="m">4.277</code> <code class="m">1.89e-05</code> <code class="o">***</code>
<code class="n">purpose_medical</code>             <code class="m">0.510479</code>   <code class="m">0.086780</code>   <code class="m">5.882</code> <code class="m">4.04e-09</code> <code class="o">***</code>
<code class="n">purpose_other</code>               <code class="m">0.620663</code>   <code class="m">0.039436</code>  <code class="m">15.738</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">purpose_small_business</code>      <code class="m">1.215261</code>   <code class="m">0.063320</code>  <code class="m">19.192</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">home_OWN</code>                    <code class="m">0.048330</code>   <code class="m">0.038036</code>   <code class="m">1.271</code>    <code class="m">0.204</code>
<code class="n">home_RENT</code>                   <code class="m">0.157320</code>   <code class="m">0.021203</code>   <code class="m">7.420</code> <code class="m">1.17e-13</code> <code class="o">***</code>
<code class="n">emp_len_</code> <code class="o">&gt;</code> <code class="m">1</code> <code class="n">Year</code>          <code class="m">-0.356731</code>   <code class="m">0.052622</code>  <code class="m">-6.779</code> <code class="m">1.21e-11</code> <code class="o">***</code>
<code class="n">borrower_score</code>             <code class="m">-4.612638</code>   <code class="m">0.083558</code> <code class="m">-55.203</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="o">---</code>
<code class="n">Signif.</code> <code class="n">codes</code><code class="o">:</code>  <code class="m">0</code> ‘<code class="o">***</code>’ <code class="m">0.001</code> ‘<code class="o">**</code>’ <code class="m">0.01</code> ‘<code class="o">*</code>’ <code class="m">0.05</code> ‘<code class="n">.’</code> <code class="m">0.1</code> ‘ ’ <code class="m">1</code>

<code class="p">(</code><code class="n">Dispersion</code> <code class="n">parameter</code> <code class="n">for</code> <code class="n">binomial</code> <code class="n">family</code> <code class="n">taken</code> <code class="n">to</code> <code class="n">be</code> <code class="m">1</code><code class="p">)</code>

    <code class="n">Null</code> <code class="n">deviance</code><code class="o">:</code> <code class="m">62857</code>  <code class="n">on</code> <code class="m">45341</code>  <code class="n">degrees</code> <code class="n">of</code> <code class="n">freedom</code>
<code class="n">Residual</code> <code class="n">deviance</code><code class="o">:</code> <code class="m">57515</code>  <code class="n">on</code> <code class="m">45330</code>  <code class="n">degrees</code> <code class="n">of</code> <code class="n">freedom</code>
<code class="n">AIC</code><code class="o">:</code> <code class="m">57539</code>

<code class="n">Number</code> <code class="n">of</code> <code class="n">Fisher</code> <code class="n">Scoring</code> <code class="n">iterations</code><code class="o">:</code> <code class="m">4</code></pre>

<p>The package <code>statsmodels</code> has an implementation for generalized linear model (<code>GLM</code>) that provides similarly detailed information:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y_numbers</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code> <code class="k">if</code> <code class="n">yi</code> <code class="o">==</code> <code class="s1">'default'</code> <code class="k">else</code> <code class="mi">0</code> <code class="k">for</code> <code class="n">yi</code> <code class="ow">in</code> <code class="n">y</code><code class="p">]</code>
<code class="n">logit_reg_sm</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">GLM</code><code class="p">(</code><code class="n">y_numbers</code><code class="p">,</code> <code class="n">X</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">const</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>
                      <code class="n">family</code><code class="o">=</code><code class="n">sm</code><code class="o">.</code><code class="n">families</code><code class="o">.</code><code class="n">Binomial</code><code class="p">())</code>
<code class="n">logit_result</code> <code class="o">=</code> <code class="n">logit_reg_sm</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
<code class="n">logit_result</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code></pre>

<p>Interpretation of the p-value comes with the same caveat as in regression and should be viewed more as a relative indicator of variable importance (see <a data-type="xref" href="ch04.xhtml#RMSE">“Assessing the Model”</a>) than as a formal measure of statistical significance.
A logistic regression model, which has a binary response, does not have an associated RMSE or R-squared.
Instead, a logistic regression model is typically evaluated using  more general metrics for classification; see <a data-type="xref" href="#EvaluatingModels">“Evaluating Classification Models”</a>.</p>

<p>Many other concepts for linear regression carry over to the logistic regression setting (and other GLMs).
For example, you can use stepwise regression, fit interaction terms, or include spline terms.
The same concerns regarding confounding and correlated variables apply to logistic regression (see <a data-type="xref" href="ch04.xhtml#InterpretingRegression">“Interpreting the Regression Equation”</a>).
You can fit generalized additive models (see <a data-type="xref" href="ch04.xhtml#GAMS">“Generalized Additive Models”</a>) using the <code>mgcv</code> package in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">logistic_gam</code> <code class="o">&lt;-</code> <code class="nf">gam</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="nf">s</code><code class="p">(</code><code class="n">payment_inc_ratio</code><code class="p">)</code> <code class="o">+</code> <code class="n">purpose_</code> <code class="o">+</code>
                    <code class="n">home_</code> <code class="o">+</code> <code class="n">emp_len_</code> <code class="o">+</code> <code class="nf">s</code><code class="p">(</code><code class="n">borrower_score</code><code class="p">),</code>
                    <code class="n">data</code><code class="o">=</code><code class="n">loan_data</code><code class="p">,</code> <code class="n">family</code><code class="o">=</code><code class="s">'binomial'</code><code class="p">)</code></pre>

<p>The formula interface of <code>statsmodels</code> also supports these extensions in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">statsmodels.formula.api</code> <code class="kn">as</code> <code class="nn">smf</code>
<code class="n">formula</code> <code class="o">=</code> <code class="p">(</code><code class="s1">'outcome ~ bs(payment_inc_ratio, df=4) + purpose_ + '</code> <code class="o">+</code>
           <code class="s1">'home_ + emp_len_ + bs(borrower_score, df=4)'</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">smf</code><code class="o">.</code><code class="n">glm</code><code class="p">(</code><code class="n">formula</code><code class="o">=</code><code class="n">formula</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">loan_data</code><code class="p">,</code> <code class="n">family</code><code class="o">=</code><code class="n">sm</code><code class="o">.</code><code class="n">families</code><code class="o">.</code><code class="n">Binomial</code><code class="p">())</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code></pre>










<section data-type="sect3" data-pdf-bookmark="Analysis of residuals"><div class="sect3" id="idm46522847259224">
<h3>Analysis of residuals</h3>

<p>One area where logistic regression differs from linear regression is in the analysis of the residuals.<a data-type="indexterm" data-primary="residuals" data-secondary="analysis of, in logistic regression" id="idm46522847302056"/>
As in linear regression (see <a data-type="xref" href="ch04.xhtml#HousePartialResid">Figure 4-9</a>), it is straightforward to compute <a data-type="indexterm" data-primary="partial residual plots" data-secondary="in logistic regression" id="idm46522847300184"/>partial residuals in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">terms</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">logistic_gam</code><code class="p">,</code> <code class="n">type</code><code class="o">=</code><code class="s">'terms'</code><code class="p">)</code>
<code class="n">partial_resid</code> <code class="o">&lt;-</code> <code class="nf">resid</code><code class="p">(</code><code class="n">logistic_model</code><code class="p">)</code> <code class="o">+</code> <code class="n">terms</code>
<code class="n">df</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">payment_inc_ratio</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">[,</code> <code class="s">'payment_inc_ratio'</code><code class="p">],</code>
                 <code class="n">terms</code> <code class="o">=</code> <code class="n">terms</code><code class="p">[,</code> <code class="s">'s(payment_inc_ratio)'</code><code class="p">],</code>
                 <code class="n">partial_resid</code> <code class="o">=</code> <code class="n">partial_resid</code><code class="p">[,</code> <code class="s">'s(payment_inc_ratio)'</code><code class="p">])</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">payment_inc_ratio</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">partial_resid</code><code class="p">,</code> <code class="n">solid</code> <code class="o">=</code> <code class="kc">FALSE</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="m">46</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="m">0.4</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">geom_line</code><code class="p">(</code><code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">payment_inc_ratio</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">terms</code><code class="p">),</code>
            <code class="n">color</code><code class="o">=</code><code class="s">'red'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="m">0.5</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="m">1.5</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">labs</code><code class="p">(</code><code class="n">y</code><code class="o">=</code><code class="s">'Partial Residual'</code><code class="p">)</code></pre>

<p>The resulting plot is displayed in <a data-type="xref" href="#LogisticPartialResidual">Figure 5-4</a>.
The estimated fit, shown by the line, goes between two sets of point clouds.
The top cloud corresponds to a response of 1 (defaulted loans), and the bottom cloud corresponds to a response of 0 (loans paid off).
This is very typical of residuals from a logistic regression since the output is binary. The prediction is measured as the logit (log of the odds ratio), which will always be some finite value. The actual value, an absolute 0 or 1, corresponds to an infinite logit, either positive or negative, so the residuals (which get added to the fitted value) will never equal 0. Hence the plotted points lie in clouds either above or below the fitted line in the partial residual plot.
Partial residuals in logistic regression, while less valuable than in regression, are still useful to confirm nonlinear behavior and identify highly influential records.</p>

<p>There is currently no implementation of partial residuals in any of the major <em>Python</em> packages. We provide <em>Python</em> code to create the partial residual plot in the accompanying source code repository.</p>

<figure class="width-75"><div id="LogisticPartialResidual" class="figure">
<img src="Images/psd2_0504.png" alt="Partial residuals from logistic regression" width="1456" height="1155"/>
<h6><span class="label">Figure 5-4. </span>Partial residuals from logistic regression</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Some of the output from the <code>summary</code> function can effectively be ignored.
The dispersion parameter does not apply to logistic regression and is there for other types of GLMs.
The residual deviance and the number of scoring iterations are related to the maximum likelihood <a data-type="indexterm" data-primary="logistic regression" data-secondary="assessing the model" data-startref="ix_logregmod" id="idm46522847143880"/><a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-tertiary="assessing the model" data-startref="ix_clsslogrmod" id="idm46522847142632"/><a data-type="indexterm" data-primary="maximum likelihood estimation (MLE)" id="idm46522847141144"/>fitting method; see <a data-type="xref" href="#MLE">“Maximum Likelihood Estimation”</a>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522847139288">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Logistic regression is like linear regression, except that the outcome is a binary variable.</p>
</li>
<li>
<p>Several transformations are needed to get the model into a form that can be fit as a linear model, with the log of the odds ratio as the response variable.</p>
</li>
<li>
<p>After the linear model is fit (by an iterative process), the log odds is mapped back to a probability.</p>
</li>
<li>
<p>Logistic regression is popular because it is computationally fast and produces a model that can be scored to new data with only a few arithmetic operations.</p>
</li>
</ul>
</div></aside>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522847295624">
<h2>Further Reading</h2>

<ul>
<li>
<p>The standard reference on logistic regression is <em>Applied Logistic Regression</em>, 3rd ed., by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant (Wiley, 2013).</p>
</li>
<li>
<p>Also popular are two books by Joseph Hilbe: <em>Logistic Regression Models</em> (very comprehensive, 2017) and <em>Practical Guide to Logistic Regression</em> (compact, 2015), both from Chapman &amp; Hall/CRC Press.</p>
</li>
<li>
<p>Both <em>The Elements of Statistical Learning</em>, 2nd ed., by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (Springer, 2009), and its shorter cousin, <em>An Introduction to Statistical Learning</em> by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (Springer, 2013), have a section on logistic regression.</p>
</li>
<li>
<p><em>Data Mining for Business Analytics</em> by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions for <em>R</em>, <em>Python</em>, Excel, and JMP) has a full chapter on logistic regression.<a data-type="indexterm" data-primary="classification" data-secondary="logistic regression" data-startref="ix_clsslogr" id="idm46522847125672"/><a data-type="indexterm" data-primary="regression" data-secondary="logistic regression" data-startref="ix_regrlog" id="idm46522847124424"/><a data-type="indexterm" data-primary="logistic regression" data-startref="ix_logreg" id="idm46522847123208"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Evaluating Classification Models"><div class="sect1" id="EvaluatingModels">
<h1>Evaluating Classification Models</h1>

<p>It is common in predictive modeling to train a number of different models, apply each to a holdout sample, and assess their performance.<a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" id="ix_clssmod"/> Sometimes, after a number of models have been evaluated and tuned, and if there are enough data, a third holdout sample, not used previously, is used to estimate how the chosen model will perform with completely new data.  Different disciplines and practitioners will also use the terms <em>validation</em> and <em>test</em> to refer to the holdout sample(s). Fundamentally, the assessment process attempts to learn which model produces the most accurate and useful predictions.<a data-type="indexterm" data-primary="accuracy" id="idm46522847117048"/><a data-type="indexterm" data-primary="confusion matrix" id="idm46522847116344"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522847115544">
<h5>Key Terms for Evaluating Classification Models</h5><dl>
<dt class="horizontal"><strong><em>Accuracy</em></strong></dt>
<dd>
<p>The percent (or proportion) of cases classified correctly.</p>
</dd>
<dt class="horizontal"><strong><em>Confusion matrix</em></strong></dt>
<dd>
<p>A tabular display (2×2 in the binary case) of the record counts by their predicted and actual classification status.</p>
</dd>
<dt class="horizontal"><strong><em>Sensitivity</em></strong></dt>
<dd>
<p>The percent (or proportion) of all 1s that are correctly classified as 1s.<a data-type="indexterm" data-primary="sensitivity" id="idm46522847108664"/></p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Recall</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Specificity</em></strong></dt>
<dd>
<p>The percent (or proportion) of all 0s that are correctly classified as 0s.<a data-type="indexterm" data-primary="specificity" id="idm46522847104136"/></p>
</dd>
<dt class="horizontal"><strong><em>Precision</em></strong></dt>
<dd>
<p>The percent (proportion) of predicted 1s that are actually 1s.<a data-type="indexterm" data-primary="precision" id="idm46522847101624"/><a data-type="indexterm" data-primary="ROC curve" id="idm46522847100920"/><a data-type="indexterm" data-primary="lift" id="idm46522847100248"/></p>
</dd>
<dt class="horizontal"><strong><em>ROC curve</em></strong></dt>
<dd>
<p>A plot of sensitivity versus specificity.</p>
</dd>
<dt class="horizontal"><strong><em>Lift</em></strong></dt>
<dd>
<p>A measure of how effective the model is at identifying (comparatively rare) 1s at different probability cutoffs.</p>
</dd>
</dl>
</div></aside>

<p>A simple way to measure classification performance is to count the proportion of predictions that are correct, i.e., measure the <em>accuracy</em>.
Accuracy  is simply a measure of total error:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>accuracy</mtext>
    <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><mrow><mi> True </mi><mi> Positive </mi></mrow><mo>+</mo><mo>∑</mo><mrow><mi> True </mi><mi> Negative </mi></mrow></mrow> <mrow><mi> Sample </mi><mi> Size </mi></mrow></mfrac>
  </mrow>
</math>
</div>

<p>In most classification algorithms, each case is assigned an “estimated probability of being a 1.”<sup><a data-type="noteref" id="idm46522847087352-marker" href="ch05.xhtml#idm46522847087352">3</a></sup>  The default decision point, or cutoff, is typically 0.50 or 50%. If the probability is above 0.5, the classification is “1”; otherwise it is “0.” An alternative default cutoff is the prevalent probability of 1s in the data.</p>








<section data-type="sect2" data-pdf-bookmark="Confusion Matrix"><div class="sect2" id="idm46522847086008">
<h2>Confusion Matrix</h2>

<p>At the heart of classification metrics is the <em>confusion matrix</em>.<a data-type="indexterm" data-primary="confusion matrix" id="ix_confmat"/><a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="confusion matrix" id="ix_clssmodCM"/>
The confusion matrix is a table showing the number of correct and incorrect predictions categorized by type of response.
Several packages are available in <em>R</em> and <em>Python</em> to compute a confusion matrix, but in the binary case, it is simple to compute one by hand.</p>

<p>To illustrate the confusion matrix,
consider the <code>logistic_gam</code> model that was trained on a balanced data set with an equal number of defaulted and paid-off loans  (see <a data-type="xref" href="#LogisticPartialResidual">Figure 5-4</a>).
Following the usual conventions, <em>Y</em> = 1 corresponds to the event of interest (e.g., default), and <em>Y</em> = 0 corresponds to a negative (or usual) event (e.g., paid off).
The following computes the confusion matrix for the <code>logistic_gam</code> model applied to the entire (unbalanced) training set in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">logistic_gam</code><code class="p">,</code> <code class="n">newdata</code><code class="o">=</code><code class="n">train_set</code><code class="p">)</code>
<code class="n">pred_y</code> <code class="o">&lt;-</code> <code class="nf">as.numeric</code><code class="p">(</code><code class="n">pred</code> <code class="o">&gt;</code> <code class="m">0</code><code class="p">)</code>
<code class="n">true_y</code> <code class="o">&lt;-</code> <code class="nf">as.numeric</code><code class="p">(</code><code class="n">train_set</code><code class="o">$</code><code class="n">outcome</code><code class="o">==</code><code class="s">'default'</code><code class="p">)</code>
<code class="n">true_pos</code> <code class="o">&lt;-</code> <code class="p">(</code><code class="n">true_y</code><code class="o">==</code><code class="m">1</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">pred_y</code><code class="o">==</code><code class="m">1</code><code class="p">)</code>
<code class="n">true_neg</code> <code class="o">&lt;-</code> <code class="p">(</code><code class="n">true_y</code><code class="o">==</code><code class="m">0</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">pred_y</code><code class="o">==</code><code class="m">0</code><code class="p">)</code>
<code class="n">false_pos</code> <code class="o">&lt;-</code> <code class="p">(</code><code class="n">true_y</code><code class="o">==</code><code class="m">0</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">pred_y</code><code class="o">==</code><code class="m">1</code><code class="p">)</code>
<code class="n">false_neg</code> <code class="o">&lt;-</code> <code class="p">(</code><code class="n">true_y</code><code class="o">==</code><code class="m">1</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="n">pred_y</code><code class="o">==</code><code class="m">0</code><code class="p">)</code>
<code class="n">conf_mat</code> <code class="o">&lt;-</code> <code class="nf">matrix</code><code class="p">(</code><code class="nf">c</code><code class="p">(</code><code class="nf">sum</code><code class="p">(</code><code class="n">true_pos</code><code class="p">),</code> <code class="nf">sum</code><code class="p">(</code><code class="n">false_pos</code><code class="p">),</code>
                     <code class="nf">sum</code><code class="p">(</code><code class="n">false_neg</code><code class="p">),</code> <code class="nf">sum</code><code class="p">(</code><code class="n">true_neg</code><code class="p">)),</code> <code class="m">2</code><code class="p">,</code> <code class="m">2</code><code class="p">)</code>
<code class="nf">colnames</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">)</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code><code class="s">'Yhat = 1'</code><code class="p">,</code> <code class="s">'Yhat = 0'</code><code class="p">)</code>
<code class="nf">rownames</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">)</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code><code class="s">'Y = 1'</code><code class="p">,</code> <code class="s">'Y = 0'</code><code class="p">)</code>
<code class="n">conf_mat</code>
      <code class="n">Yhat</code> <code class="o">=</code> <code class="m">1</code> <code class="n">Yhat</code> <code class="o">=</code> <code class="m">0</code>
<code class="n">Y</code> <code class="o">=</code> <code class="m">1</code> <code class="m">14295</code>    <code class="m">8376</code>
<code class="n">Y</code> <code class="o">=</code> <code class="m">0</code> <code class="m">8052</code>     <code class="m">14619</code></pre>

<p>In <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pred</code> <code class="o">=</code> <code class="n">logit_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">pred_y</code> <code class="o">=</code> <code class="n">logit_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code> <code class="o">==</code> <code class="s1">'default'</code>
<code class="n">true_y</code> <code class="o">=</code> <code class="n">y</code> <code class="o">==</code> <code class="s1">'default'</code>
<code class="n">true_pos</code> <code class="o">=</code> <code class="n">true_y</code> <code class="o">&amp;</code> <code class="n">pred_y</code>
<code class="n">true_neg</code> <code class="o">=</code> <code class="o">~</code><code class="n">true_y</code> <code class="o">&amp;</code> <code class="o">~</code><code class="n">pred_y</code>
<code class="n">false_pos</code> <code class="o">=</code> <code class="o">~</code><code class="n">true_y</code> <code class="o">&amp;</code> <code class="n">pred_y</code>
<code class="n">false_neg</code> <code class="o">=</code> <code class="n">true_y</code> <code class="o">&amp;</code> <code class="o">~</code><code class="n">pred_y</code>

<code class="n">conf_mat</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">([[</code><code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">true_pos</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">false_neg</code><code class="p">)],</code>
                         <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">false_pos</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">true_neg</code><code class="p">)]],</code>
                       <code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="s1">'Y = default'</code><code class="p">,</code> <code class="s1">'Y = paid off'</code><code class="p">],</code>
                       <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'Yhat = default'</code><code class="p">,</code> <code class="s1">'Yhat = paid off'</code><code class="p">])</code>
<code class="n">conf_mat</code></pre>

<p>The predicted outcomes are columns and the true outcomes are the rows.
The diagonal elements of the matrix show the number of correct predictions, and the off-diagonal elements show the number of incorrect predictions. For example, 14,295 defaulted loans were correctly predicted as a default, but 8,376 defaulted loans were incorrectly predicted as paid off.</p>

<p><a data-type="xref" href="#ConfusionGraphic">Figure 5-5</a> shows the relationship between the confusion matrix for a binary response <em>Y</em> and different metrics (see <a data-type="xref" href="#PrecisonRecallSpecificity">“Precision, Recall, and Specificity”</a> for more on the metrics).
As with the example for the loan data, the actual response is along the rows and the predicted response is along the columns.
The diagonal boxes (upper left, lower right) show when the predictions <math alttext="ModifyingAbove upper Y With caret">
  <mover accent="true"><mi>Y</mi> <mo>^</mo></mover>
</math> correctly predict the response.
One important metric not explicitly called out is the false positive <em>rate</em> (the mirror image of precision).<a data-type="indexterm" data-primary="false positive rate" id="idm46522846865336"/>
When 1s are rare, the ratio of false positives to all predicted positives can be high, leading to the unintuitive situation in which a predicted 1 is most likely a 0.<a data-type="indexterm" data-primary="medical screening tests, false positives and" id="idm46522846864328"/>
This problem plagues medical screening tests (e.g., mammograms) that are widely applied: due to the relative rarity of the condition, positive test results most likely do not mean breast cancer.
This leads to much confusion in the public.</p>

<figure><div id="ConfusionGraphic" class="figure">
<img src="Images/psd2_0505.png" alt="images/confusion-matrix-terms.png" width="923" height="476"/>
<h6><span class="label">Figure 5-5. </span>Confusion matrix for a binary response and various metrics</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Here, we present the actual response along the rows and the predicted response along the columns, but it is not uncommon to see this reversed. A notable example is the popular <code>caret</code> package in <em>R</em>.<a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="confusion matrix" data-startref="ix_clssmodCM" id="idm46522846859288"/><a data-type="indexterm" data-primary="confusion matrix" data-startref="ix_confmat" id="idm46522846857736"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Rare Class Problem"><div class="sect2" id="RareClassProblem">
<h2>The Rare Class Problem</h2>

<p>In many cases, there<a data-type="indexterm" data-primary="rare class problem" id="idm46522846855096"/><a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="rare class problem" id="idm46522846854360"/> is an imbalance in the classes to be predicted, with one class much more prevalent than the other—for example, legitimate insurance claims versus fraudulent ones, or browsers versus purchasers at a website.
The rare class (e.g., the fraudulent claims) is usually the class of more interest and is typically designated 1, in contrast to the more prevalent 0s.
In the typical scenario, the 1s are the more important case, in the sense that misclassifying them as 0s is costlier than misclassifying 0s as 1s.  For example, correctly identifying a fraudulent insurance claim may save thousands of dollars.
On the other hand, correctly identifying a nonfraudulent claim merely saves you the cost and effort of going through the claim by hand with a more careful review (which is what you would do if the claim were tagged as “fraudulent”).</p>

<p>In such cases, unless the classes are easily separable, the most accurate classification model may be one that simply classifies everything as a 0.
For example, if only 0.1% of the browsers at a web store end up purchasing, a model that predicts that each browser will leave without purchasing will be 99.9% accurate.  However, it will be useless.  Instead, we would be happy with a model that is less accurate overall but is good at picking out the purchasers, even if it misclassifies some nonpurchasers along the way.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Precision, Recall, and Specificity"><div class="sect2" id="PrecisonRecallSpecificity">
<h2>Precision, Recall, and Specificity</h2>

<p>Metrics other than pure accuracy—metrics that are more nuanced—are commonly used in evaluating classification models.<a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="precision, recall, and specificity" id="idm46522846848840"/>
Several of these have a long history in statistics—especially biostatistics, where they are used to describe the expected performance of diagnostic tests.
The <em>precision</em>  measures the accuracy<a data-type="indexterm" data-primary="precision" id="idm46522846846840"/> of a predicted positive outcome (see <a data-type="xref" href="#ConfusionGraphic">Figure 5-5</a>):</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>precision</mtext>
    <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><mrow><mi> True </mi><mi> Positive </mi></mrow></mrow> <mrow><mo>∑</mo><mrow><mi> True </mi><mi> Positive </mi></mrow><mo>+</mo><mo>∑</mo><mrow><mi> False </mi><mi> Positive </mi></mrow></mrow></mfrac>
  </mrow>
</math>
</div>

<p>The <em>recall</em>, also known as <em>sensitivity</em>,   measures<a data-type="indexterm" data-primary="recall" data-seealso="sensitivity" id="idm46522846836840"/><a data-type="indexterm" data-primary="sensitivity" id="idm46522846835832"/> the strength of the model to predict a positive outcome—the proportion of the 1s that it correctly identifies (see <a data-type="xref" href="#ConfusionGraphic">Figure 5-5</a>).
The term <em>sensitivity</em> is used a lot in biostatistics and medical diagnostics, whereas <em>recall</em> is used more in the machine learning community.
The definition of recall is:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>recall</mtext>
    <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><mrow><mi> True </mi><mi> Positive </mi></mrow></mrow> <mrow><mo>∑</mo><mrow><mi> True </mi><mi> Positive </mi></mrow><mo>+</mo><mo>∑</mo><mrow><mi> False </mi><mi> Negative </mi></mrow></mrow></mfrac>
  </mrow>
</math>
</div>

<p class="pagebreak-before">Another metric<a data-type="indexterm" data-primary="specificity" id="idm46522846825912"/> used is <em>specificity</em>,  which measures
a model’s ability to predict a negative outcome:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>specificity</mtext>
    <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><mrow><mi> True </mi><mi> Negative </mi></mrow></mrow> <mrow><mo>∑</mo><mrow><mi> True </mi><mi> Negative </mi></mrow><mo>+</mo><mo>∑</mo><mrow><mi> False </mi><mi> Positive </mi></mrow></mrow></mfrac>
  </mrow>
</math>
</div>

<p>We can calculate the three metrics from <code>conf_mat</code> in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="c1"># precision</code>
<code class="n">conf_mat</code><code class="p">[</code><code class="m">1</code><code class="p">,</code> <code class="m">1</code><code class="p">]</code> <code class="o">/</code> <code class="nf">sum</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">[,</code><code class="m">1</code><code class="p">])</code>
<code class="c1"># recall</code>
<code class="n">conf_mat</code><code class="p">[</code><code class="m">1</code><code class="p">,</code> <code class="m">1</code><code class="p">]</code> <code class="o">/</code> <code class="nf">sum</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">[</code><code class="m">1</code><code class="p">,])</code>
<code class="c1"># specificity</code>
<code class="n">conf_mat</code><code class="p">[</code><code class="m">2</code><code class="p">,</code> <code class="m">2</code><code class="p">]</code> <code class="o">/</code> <code class="nf">sum</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">[</code><code class="m">2</code><code class="p">,])</code></pre>

<p>Here is the equivalent code to calculate the metrics in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">conf_mat</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">y</code><code class="p">,</code> <code class="n">logit_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Precision'</code><code class="p">,</code> <code class="n">conf_mat</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="nb">sum</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]))</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Recall'</code><code class="p">,</code> <code class="n">conf_mat</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="nb">sum</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="p">:]))</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Specificity'</code><code class="p">,</code> <code class="n">conf_mat</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code> <code class="o">/</code> <code class="nb">sum</code><code class="p">(</code><code class="n">conf_mat</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="p">:]))</code>

<code class="n">precision_recall_fscore_support</code><code class="p">(</code><code class="n">y</code><code class="p">,</code> <code class="n">logit_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">),</code>
                                <code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="s1">'default'</code><code class="p">,</code> <code class="s1">'paid off'</code><code class="p">])</code></pre>

<p><code>scikit-learn</code> has a custom method <code>precision_recall_fscore_support</code> that calculates precision and recall/specificity all at once.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="ROC Curve"><div class="sect2" id="ROCCurve">
<h2>ROC Curve</h2>

<p>You can see that there is a trade-off between recall and specificity.
Capturing more 1s generally means misclassifying more 0s as 1s.<a data-type="indexterm" data-primary="recall" data-secondary="trade-off with specificity" id="idm46522846492152"/><a data-type="indexterm" data-primary="specificity" data-secondary="trade-off with recall" id="idm46522846491112"/><a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="ROC curve" id="ix_clssmodROC"/><a data-type="indexterm" data-primary="ROC curve" id="ix_ROC"/>
The ideal classifier would do an excellent job of classifying the 1s, without misclassifying more 0s as 1s.<a data-type="indexterm" data-primary="Receiver Operating Characteristics curve" data-see="ROC curve" id="idm46522846487480"/></p>

<p>The metric that captures this trade-off is the “Receiver Operating Characteristics” curve, usually referred to as the <em>ROC curve</em>.
The ROC curve plots recall (sensitivity) on the y-axis against specificity on the x-axis.<sup><a data-type="noteref" id="idm46522846485448-marker" href="ch05.xhtml#idm46522846485448">4</a></sup>
The ROC curve shows the trade-off between recall and specificity as you change the cutoff to determine how to classify a record.
Sensitivity (recall) is plotted on the y-axis, and you may encounter two forms in which the x-axis is labeled:</p>

<ul class="pagebreak-before">
<li>
<p>Specificity plotted on the x-axis, with 1 on the left and 0 on the right</p>
</li>
<li>
<p>1-Specificity plotted on the x-axis, with 0 on the left and 1 on the right</p>
</li>
</ul>

<p>The curve looks identical whichever way it is done.
The process to compute the ROC curve is:</p>
<ol>
<li>
<p>Sort the records by the predicted probability of being a 1, starting with the most probable and ending with the least probable.</p>
</li>
<li>
<p>Compute the cumulative specificity and recall based on the sorted records.</p>
</li>

</ol>

<p>Computing the ROC curve in <em>R</em> is straightforward.
The following code computes ROC for the loan data:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">idx</code> <code class="o">&lt;-</code> <code class="nf">order</code><code class="p">(</code><code class="o">-</code><code class="n">pred</code><code class="p">)</code>
<code class="n">recall</code> <code class="o">&lt;-</code> <code class="nf">cumsum</code><code class="p">(</code><code class="n">true_y</code><code class="p">[</code><code class="n">idx</code><code class="p">]</code> <code class="o">==</code> <code class="m">1</code><code class="p">)</code> <code class="o">/</code> <code class="nf">sum</code><code class="p">(</code><code class="n">true_y</code> <code class="o">==</code> <code class="m">1</code><code class="p">)</code>
<code class="n">specificity</code> <code class="o">&lt;-</code> <code class="p">(</code><code class="nf">sum</code><code class="p">(</code><code class="n">true_y</code> <code class="o">==</code> <code class="m">0</code><code class="p">)</code> <code class="o">-</code> <code class="nf">cumsum</code><code class="p">(</code><code class="n">true_y</code><code class="p">[</code><code class="n">idx</code><code class="p">]</code> <code class="o">==</code> <code class="m">0</code><code class="p">))</code> <code class="o">/</code> <code class="nf">sum</code><code class="p">(</code><code class="n">true_y</code> <code class="o">==</code> <code class="m">0</code><code class="p">)</code>
<code class="n">roc_df</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">recall</code> <code class="o">=</code> <code class="n">recall</code><code class="p">,</code> <code class="n">specificity</code> <code class="o">=</code> <code class="n">specificity</code><code class="p">)</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">roc_df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">specificity</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">recall</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_line</code><code class="p">(</code><code class="n">color</code><code class="o">=</code><code class="s">'blue'</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">scale_x_reverse</code><code class="p">(</code><code class="n">expand</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">0</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">scale_y_continuous</code><code class="p">(</code><code class="n">expand</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">0</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_line</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="nf">data.frame</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="p">(</code><code class="m">0</code><code class="o">:</code><code class="m">100</code><code class="p">)</code> <code class="o">/</code> <code class="m">100</code><code class="p">),</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="m">1</code><code class="o">-</code><code class="n">x</code><code class="p">),</code>
            <code class="n">linetype</code><code class="o">=</code><code class="s">'dotted'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s">'red'</code><code class="p">)</code></pre>

<p>In <em>Python</em>, we can use the <code>scikit-learn</code> function <code>sklearn.metrics.roc_curve</code> to calculate the required information for the ROC curve. You can find similar packages for <em>R</em>, e.g., <code>ROCR</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fpr</code><code class="p">,</code> <code class="n">tpr</code><code class="p">,</code> <code class="n">thresholds</code> <code class="o">=</code> <code class="n">roc_curve</code><code class="p">(</code><code class="n">y</code><code class="p">,</code> <code class="n">logit_reg</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">)[:,</code><code class="mi">0</code><code class="p">],</code>
                                 <code class="n">pos_label</code><code class="o">=</code><code class="s1">'default'</code><code class="p">)</code>
<code class="n">roc_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s1">'recall'</code><code class="p">:</code> <code class="n">tpr</code><code class="p">,</code> <code class="s1">'specificity'</code><code class="p">:</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">fpr</code><code class="p">})</code>

<code class="n">ax</code> <code class="o">=</code> <code class="n">roc_df</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'specificity'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'recall'</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code> <code class="n">legend</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'specificity'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'recall'</code><code class="p">)</code></pre>

<p>The result is shown in <a data-type="xref" href="#ROCLoan">Figure 5-6</a>.
The dotted diagonal line corresponds to a classifier no better than random chance.
An extremely effective classifier (or, in medical situations, an extremely effective diagnostic test) will have an ROC that hugs the upper-left corner—it will correctly identify lots of 1s without misclassifying lots of 0s as 1s.
For this model, if we want a classifier with a specificity of at least 50%, then the recall is about 75%.</p>

<figure class="width-75"><div id="ROCLoan" class="figure">
<img src="Images/psd2_0506.png" alt="ROC curve for the loan data" width="1170" height="1169"/>
<h6><span class="label">Figure 5-6. </span>ROC curve for the loan data</h6>
</div></figure>
<div data-type="note" epub:type="note"><h1>Precision-Recall Curve</h1>
<p>In addition to ROC curves, it can be illuminating to examine the <a href="https://oreil.ly/_89Pr">precision-recall (PR) curve</a>.<a data-type="indexterm" data-primary="precision-recall (PR) curve" id="idm46522846098984"/>
PR curves are computed in a similar way except that the data is ordered from least to most probable and cumulative precision and recall statistics are computed.
PR curves are especially useful in evaluating data with highly unbalanced outcomes.<a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="ROC curve" data-startref="ix_clssmodROC" id="idm46522846097880"/><a data-type="indexterm" data-primary="ROC curve" data-startref="ix_ROC" id="idm46522846096392"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="AUC"><div class="sect2" id="AUC">
<h2>AUC</h2>

<p>The ROC curve is a valuable graphical tool, but by itself doesn’t constitute a single measure for the performance of a classifier.<a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="AUC metric" id="ix_clssmodAUC"/><a data-type="indexterm" data-primary="ROC curve" data-secondary="AUC metric" id="ix_ROCAUC"/><a data-type="indexterm" data-primary="AUC (area under the ROC curve)" id="ix_AUC"/>
The ROC curve can be used, however, to produce the area underneath the curve (AUC) metric.
AUC is simply the total area under the ROC curve.
The larger the value of AUC, the more effective the classifier.
An AUC of 1 indicates a perfect classifier: it gets all the 1s correctly classified, and it doesn’t misclassify any 0s as 1s.</p>

<p>A completely ineffective classifier—the diagonal line—will have an AUC of 0.5.</p>

<p><a data-type="xref" href="#AUCLoan">Figure 5-7</a> shows the area under the ROC curve for the loan model.
The value of AUC can be computed by a numerical integration in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">sum</code><code class="p">(</code><code class="n">roc_df</code><code class="o">$</code><code class="n">recall</code><code class="p">[</code><code class="m">-1</code><code class="p">]</code> <code class="o">*</code> <code class="nf">diff</code><code class="p">(</code><code class="m">1</code> <code class="o">-</code> <code class="n">roc_df</code><code class="o">$</code><code class="n">specificity</code><code class="p">))</code>
    <code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.6926172</code></pre>

<p>In <em>Python</em>, we can either calculate the accuracy as shown for <em>R</em> or use <code>scikit-learn</code>’s function <code>sklearn.metrics.roc_auc_score</code>. You will need to provide the expected value as 0 or 1:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">roc_df</code><code class="o">.</code><code class="n">recall</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">diff</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">roc_df</code><code class="o">.</code><code class="n">specificity</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="n">roc_auc_score</code><code class="p">([</code><code class="mi">1</code> <code class="k">if</code> <code class="n">yi</code> <code class="o">==</code> <code class="s1">'default'</code> <code class="k">else</code> <code class="mi">0</code> <code class="k">for</code> <code class="n">yi</code> <code class="ow">in</code> <code class="n">y</code><code class="p">],</code>
                    <code class="n">logit_reg</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">)[:,</code> <code class="mi">0</code><code class="p">]))</code></pre>

<p>The model has an AUC of about 0.69, corresponding to a relatively weak classifier.</p>

<figure class="width-75"><div id="AUCLoan" class="figure">
<img src="Images/psd2_0507.png" alt="Area under the ROC curve for the loan data" width="1170" height="1169"/>
<h6><span class="label">Figure 5-7. </span>Area under the ROC curve for the loan data</h6>
</div></figure>
<div data-type="caution"><h1>False Positive Rate Confusion</h1>
<p>False positive/negative rates are often confused or conflated with specificity or sensitivity (even in publications and software!).  Sometimes the false positive rate is defined as the proportion of true negatives that test positive. In many cases (such as network intrusion detection), the term is used to refer to the proportion of positive signals that are true negatives.<a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="AUC metric" data-startref="ix_clssmodAUC" id="idm46522845969976"/><a data-type="indexterm" data-primary="ROC curve" data-secondary="AUC metric" data-startref="ix_ROCAUC" id="idm46522845968456"/><a data-type="indexterm" data-primary="AUC (area under the ROC curve)" data-startref="ix_AUC" id="idm46522845967240"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Lift"><div class="sect2" id="idm46522846095064">
<h2>Lift</h2>

<p>Using the AUC as a metric to evaluate a model is an improvement over simple accuracy, as it can assess how well a classifier handles the trade-off between overall accuracy and the need to identify the more important 1s.<a data-type="indexterm" data-primary="lift" id="idm46522845964424"/><a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-tertiary="lift" id="idm46522845963720"/>  But it does not completely address the rare-case problem, where you need to lower the model’s probability cutoff below 0.5 to avoid having all records classified as 0.  In such cases, for a record to be classified as a 1, it might be sufficient to have a probability of 0.4, 0.3, or lower.  In effect, we end up overidentifying 1s, reflecting their greater importance.</p>

<p>Changing this cutoff will improve your chances of catching the 1s (at the cost of misclassifying more 0s as 1s).
But what is the optimum cutoff?</p>

<p>The concept of lift lets you defer answering that question. Instead, you consider the records in order of their predicted probability of being 1s. Say, of the top 10% classified as 1s, how much better did the algorithm do, compared to the benchmark of simply picking blindly?
If you can get 0.3% response in this top decile instead of the 0.1% you get overall by picking randomly, the algorithm is said to have a <em>lift</em> (also called <em>gains</em>) of 3 in the top decile.<a data-type="indexterm" data-primary="gains" data-seealso="lift" id="idm46522845959736"/>
A lift chart (gains chart) quantifies this over the range of the data.  It can be produced decile by decile, or continuously over the range of the data.</p>

<p>To compute a lift chart, you first produce a <em>cumulative gains chart</em> that shows the recall on the y-axis and the total number of records on the x-axis.
The <em>lift curve</em> is the ratio of the cumulative gains to the diagonal line corresponding to random selection.<a data-type="indexterm" data-primary="lift" data-secondary="lift curve" id="idm46522845956792"/><a data-type="indexterm" data-primary="cumulative gains chart" id="idm46522845955816"/>
<em>Decile gains charts</em> are one of the oldest techniques in predictive modeling, dating from the days before internet commerce.<a data-type="indexterm" data-primary="decile gains charts" id="idm46522845954472"/>
They were particularly popular among direct mail professionals.
Direct mail is an expensive method of advertising if applied indiscriminately, and advertisers used predictive models (quite simple ones, in the early days) to identify the potential customers with the likeliest prospect of payoff.</p>
<div data-type="caution"><h1>Uplift</h1>
<p>Sometimes the term <em>uplift</em> is used to mean the same thing as lift.<a data-type="indexterm" data-primary="uplift" data-seealso="lift" id="idm46522845951672"/><a data-type="indexterm" data-primary="lift" data-secondary="uplift and" id="idm46522845950696"/>  An alternate meaning is used in a more restrictive setting, when an A/B test has been conducted and the treatment (A or B) is then used as a predictor variable in a predictive model.  The uplift is the improvement in response predicted <em>for an individual case</em> with treatment A versus treatment B.  This is determined by scoring the individual case first with the predictor set to A, and then again with the predictor toggled to B.  Marketers and political campaign consultants use this method to determine which of two messaging treatments should be used with which customers or voters.</p>
</div>

<p>A lift curve lets you look at the consequences of setting different probability cutoffs for classifying records as 1s.
It can be an intermediate step in settling on an appropriate cutoff level.
For example, a tax authority might have only a certain amount of resources that it can spend on tax audits, and it wants to spend them on the likeliest tax cheats.
With its resource constraint in mind, the authority would use a lift chart to estimate where to draw the line between tax returns selected for audit and those left alone.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522845947432">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Accuracy (the percent of predicted classifications that are correct) is but a first step in evaluating a model.</p>
</li>
<li>
<p>Other metrics (recall, specificity, precision) focus on more specific performance characteristics (e.g., recall measures how good a model is at correctly identifying 1s).</p>
</li>
<li>
<p>AUC (area under the ROC curve) is a common metric for the ability of a model to distinguish 1s from 0s.</p>
</li>
<li>
<p>Similarly, lift measures how effective a model is in identifying the 1s, and it is often calculated decile by decile, starting with the most probable 1s.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522845941752">
<h2>Further Reading</h2>

<p>Evaluation and assessment are typically covered in the context of a particular model (e.g., <em>K</em>-Nearest Neighbors or decision trees); three books that handle the subject in its own chapter are:</p>

<ul>
<li>
<p><em>Data Mining</em>, 3rd ed., by Ian Whitten, Eibe Frank, and Mark Hall (Morgan Kaufmann, 2011).</p>
</li>
<li>
<p><em>Modern Data Science with R</em> by Benjamin Baumer, Daniel Kaplan, and Nicholas Horton (Chapman &amp; Hall/CRC Press, 2017).</p>
</li>
<li>
<p><em>Data Mining for Business Analytics</em> by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions for <em>R</em>, <em>Python</em>, Excel, and JMP).<a data-type="indexterm" data-primary="classification" data-secondary="evaluating models" data-startref="ix_clssmod" id="idm46522845935080"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Strategies for Imbalanced Data"><div class="sect1" id="ImbalancedData">
<h1>Strategies for Imbalanced Data</h1>

<p>The previous section dealt with evaluation of classification models using metrics that go beyond simple accuracy and are suitable for imbalanced data—data in which the outcome of interest (purchase on a website, insurance fraud, etc.) is rare.<a data-type="indexterm" data-primary="imbalanced data strategies for classification models" id="ix_imbal"/><a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" id="ix_clssmodimb"/>
In this section, we look at additional strategies that can improve predictive modeling performance with imbalanced data.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522845928584">
<h5>Key Terms for Imbalanced Data</h5><dl>
<dt class="horizontal"><strong><em>Undersample</em></strong></dt>
<dd>
<p>Use fewer of the prevalent class records in the classification model.</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Downsample</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Oversample</em></strong></dt>
<dd>
<p>Use more of the rare class records in the classification model, bootstrapping if necessary.<a data-type="indexterm" data-primary="oversampling" id="idm46522845889752"/></p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Upsample</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Up weight or down weight</em></strong></dt>
<dd>
<p>Attach more (or less) weight to the rare (or prevalent) class in the model.<a data-type="indexterm" data-primary="up/down weighting" id="idm46522845885352"/></p>
</dd>
<dt class="horizontal"><strong><em>Data generation</em></strong></dt>
<dd>
<p>Like bootstrapping, except each new bootstrapped record is slightly different from its source.<a data-type="indexterm" data-primary="data generation" id="idm46522845882728"/><a data-type="indexterm" data-primary="z-scores" id="idm46522845882024"/></p>
</dd>
<dt class="horizontal"><strong><em>z-score</em></strong></dt>
<dd>
<p>The value that results after standardization.</p>
</dd>
<dt class="horizontal"><strong><em>K</em></strong></dt>
<dd>
<p>The number of neighbors considered in the nearest neighbor calculation.<a data-type="indexterm" data-primary="K (in K-Nearest Neighbors)" id="idm46522845877640"/></p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Undersampling"><div class="sect2" id="Undersampling">
<h2>Undersampling</h2>

<p>If you have enough data, as is the case with the loan data,
one solution is to <em>undersample</em> (or downsample) the prevalent class, so the data to be modeled is more balanced between 0s and 1s.<a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" data-tertiary="undersampling" id="idm46522845874120"/><a data-type="indexterm" data-primary="undersampling" id="idm46522845872856"/>
The basic idea in undersampling is that the data for the dominant class has many redundant records.
Dealing with a smaller, more balanced data set yields benefits in model performance, and it makes it easier to prepare the data and to explore and pilot models.<a data-type="indexterm" data-primary="downsampling" data-seealso="undersampling" id="idm46522845871784"/></p>

<p>How much data is enough?
It depends on the application, but in general, having tens of thousands of records for the less dominant class is enough.  The more easily distinguishable the 1s are from the 0s, the less data needed.</p>

<p>The loan data analyzed in <a data-type="xref" href="#LogisticRegression">“Logistic Regression”</a> was based on a balanced training set:
half of the loans were paid off, and the other half were in default.
The predicted values were similar: half of the probabilities were less than 0.5, and half were greater than 0.5.
In the full data set, only about 19% of the loans were in default, as shown in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">mean</code><code class="p">(</code><code class="n">full_train_set</code><code class="o">$</code><code class="n">outcome</code><code class="o">==</code><code class="s">'default'</code><code class="p">)</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.1889455</code></pre>

<p>In <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s1">'percentage of loans in default: '</code><code class="p">,</code>
      <code class="mi">100</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">full_train_set</code><code class="o">.</code><code class="n">outcome</code> <code class="o">==</code> <code class="s1">'default'</code><code class="p">))</code></pre>

<p>What happens if we use the full data set to train the model? Let’s see what this looks like in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">full_model</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">payment_inc_ratio</code> <code class="o">+</code> <code class="n">purpose_</code> <code class="o">+</code> <code class="n">home_</code> <code class="o">+</code>
                            <code class="n">emp_len_</code><code class="o">+</code> <code class="n">dti</code> <code class="o">+</code> <code class="n">revol_bal</code> <code class="o">+</code> <code class="n">revol_util</code><code class="p">,</code>
                 <code class="n">data</code><code class="o">=</code><code class="n">full_train_set</code><code class="p">,</code> <code class="n">family</code><code class="o">=</code><code class="s">'binomial'</code><code class="p">)</code>
<code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">full_model</code><code class="p">)</code>
<code class="nf">mean</code><code class="p">(</code><code class="n">pred</code> <code class="o">&gt;</code> <code class="m">0</code><code class="p">)</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.003942094</code></pre>

<p>And in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="s1">'purpose_'</code><code class="p">,</code> <code class="s1">'home_'</code><code class="p">,</code> <code class="s1">'emp_len_'</code><code class="p">,</code>
              <code class="s1">'dti'</code><code class="p">,</code> <code class="s1">'revol_bal'</code><code class="p">,</code> <code class="s1">'revol_util'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">full_train_set</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">prefix</code><code class="o">=</code><code class="s1">''</code><code class="p">,</code> <code class="n">prefix_sep</code><code class="o">=</code><code class="s1">''</code><code class="p">,</code>
                   <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">full_train_set</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">full_model</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">penalty</code><code class="o">=</code><code class="s1">'l2'</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">1e42</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>
<code class="n">full_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'percentage of loans predicted to default: '</code><code class="p">,</code>
      <code class="mi">100</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">full_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code> <code class="o">==</code> <code class="s1">'default'</code><code class="p">))</code></pre>

<p>Only 0.39% of the loans are predicted to be in default, or less than 1/47 of the expected number.<sup><a data-type="noteref" id="idm46522845748808-marker" href="ch05.xhtml#idm46522845748808">5</a></sup>
The loans that were paid off overwhelm the loans in default because the model is trained using all the data equally.
Thinking about it intuitively, the presence of so many nondefaulting loans, coupled with the inevitable variability in predictor data, means that, even for a defaulting loan, the model is likely to find some nondefaulting loans that it is similar to, by chance.
When a balanced sample was used, roughly 50% of the loans were predicted to be in default.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Oversampling and Up/Down Weighting"><div class="sect2" id="UpDownWeighting">
<h2>Oversampling and Up/Down Weighting</h2>

<p>One criticism of the undersampling method is that it throws away data and is not using all the information at hand.<a data-type="indexterm" data-primary="imbalanced data strategies for classification models" data-secondary="oversampling and up/down weighting" id="idm46522845618840"/><a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" data-tertiary="oversampling and up/down weighting" id="idm46522845617880"/><a data-type="indexterm" data-primary="oversampling" data-secondary="and up/down weighting" id="idm46522845616632"/>
If you have a relatively small data set, and the rarer class contains a few hundred or a few thousand records,
then undersampling the dominant class has the risk of throwing out useful information.
In this case, instead of downsampling the dominant case, you should oversample (upsample) the rarer class by drawing additional rows with replacement (bootstrapping).</p>

<p>You can achieve a similar effect by weighting the data.<a data-type="indexterm" data-primary="up/down weighting" id="idm46522845614792"/><a data-type="indexterm" data-primary="weighting" data-secondary="up weight and down weight" id="idm46522845614088"/>  Many classification algorithms take a weight argument that will allow you to up/down weight the data.
For example, apply a weight vector to the loan data using the <code>weight</code> argument to <code>glm</code> in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">wt</code> <code class="o">&lt;-</code> <code class="nf">ifelse</code><code class="p">(</code><code class="n">full_train_set</code><code class="o">$</code><code class="n">outcome</code><code class="o">==</code><code class="s">'default'</code><code class="p">,</code>
             <code class="m">1</code> <code class="o">/</code> <code class="nf">mean</code><code class="p">(</code><code class="n">full_train_set</code><code class="o">$</code><code class="n">outcome</code> <code class="o">==</code> <code class="s">'default'</code><code class="p">),</code> <code class="m">1</code><code class="p">)</code>
<code class="n">full_model</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">payment_inc_ratio</code> <code class="o">+</code> <code class="n">purpose_</code> <code class="o">+</code> <code class="n">home_</code> <code class="o">+</code>
                            <code class="n">emp_len_</code><code class="o">+</code> <code class="n">dti</code> <code class="o">+</code> <code class="n">revol_bal</code> <code class="o">+</code> <code class="n">revol_util</code><code class="p">,</code>
                  <code class="n">data</code><code class="o">=</code><code class="n">full_train_set</code><code class="p">,</code> <code class="n">weight</code><code class="o">=</code><code class="n">wt</code><code class="p">,</code> <code class="n">family</code><code class="o">=</code><code class="s">'quasibinomial'</code><code class="p">)</code>
<code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">full_model</code><code class="p">)</code>
<code class="nf">mean</code><code class="p">(</code><code class="n">pred</code> <code class="o">&gt;</code> <code class="m">0</code><code class="p">)</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.5767208</code></pre>

<p>Most <code>scikit-learn</code> methods allow specifying weights in the <code>fit</code> function using the keyword argument <code>sample_weight</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">default_wt</code> <code class="o">=</code> <code class="mi">1</code> <code class="o">/</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">full_train_set</code><code class="o">.</code><code class="n">outcome</code> <code class="o">==</code> <code class="s1">'default'</code><code class="p">)</code>
<code class="n">wt</code> <code class="o">=</code> <code class="p">[</code><code class="n">default_wt</code> <code class="k">if</code> <code class="n">outcome</code> <code class="o">==</code> <code class="s1">'default'</code> <code class="k">else</code> <code class="mi">1</code>
      <code class="k">for</code> <code class="n">outcome</code> <code class="ow">in</code> <code class="n">full_train_set</code><code class="o">.</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">full_model</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">penalty</code><code class="o">=</code><code class="s2">"l2"</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">1e42</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s1">'liblinear'</code><code class="p">)</code>
<code class="n">full_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">sample_weight</code><code class="o">=</code><code class="n">wt</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'percentage of loans predicted to default (weighting): '</code><code class="p">,</code>
      <code class="mi">100</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">full_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code> <code class="o">==</code> <code class="s1">'default'</code><code class="p">))</code></pre>

<p>The weights for loans that default are set to <math alttext="StartFraction 1 Over p EndFraction">
  <mfrac><mn>1</mn> <mi>p</mi></mfrac>
</math>, where <em>p</em> is the probability of default.
The nondefaulting loans have a weight of 1.
The sums of the weights for the defaulting loans and nondefaulting loans are roughly equal.
The mean of the predicted values is now about 58% instead of 0.39%.</p>

<p>Note that weighting provides an alternative to both upsampling the rarer class and downsampling the dominant class.</p>
<div data-type="note" epub:type="note"><h1>Adapting the Loss Function</h1>
<p>Many classification and regression algorithms optimize a certain criteria or <em>loss function</em>.<a data-type="indexterm" data-primary="loss function" id="idm46522845363928"/><a data-type="indexterm" data-primary="deviance" data-secondary="attempt to minimize in logistic regression" id="idm46522845363192"/>
For example, logistic regression attempts to minimize the deviance.
In the literature, some propose to modify the loss function in order to avoid the problems caused by a rare class.
In practice, this is hard to do: classification algorithms can be complex and difficult to modify.<a data-type="indexterm" data-primary="weighting" data-secondary="using to change loss function in classification" id="idm46522845361720"/>
Weighting is an easy way to change the loss function, discounting errors for records with low weights in favor of records with higher weights.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data Generation"><div class="sect2" id="idm46522845620312">
<h2>Data Generation</h2>

<p>A variation of upsampling via bootstrapping (see <a data-type="xref" href="#UpDownWeighting">“Oversampling and Up/Down Weighting”</a>) is <em>data generation</em> by perturbing existing records to create new records.<a data-type="indexterm" data-primary="data generation" id="idm46522845357320"/><a data-type="indexterm" data-primary="imbalanced data strategies for classification models" data-secondary="data generation" id="idm46522845356616"/><a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" data-tertiary="data generation" id="idm46522845355576"/>
The intuition behind this idea is that since we observe only a limited set of instances, the algorithm doesn’t have a rich set of information to build classification “rules.”
By creating new records that are similar but not identical to existing records, the algorithm has a chance to learn a more robust set of rules. This notion is similar in spirit to ensemble statistical models such as boosting and bagging (see <a data-type="xref" href="ch06.xhtml#StatisticalML">Chapter 6</a>).</p>

<p>The idea gained<a data-type="indexterm" data-primary="SMOTE algorithm" id="idm46522845352728"/> traction with the publication of the <em>SMOTE</em>  algorithm, which stands for “Synthetic Minority Oversampling Technique.”
The SMOTE algorithm finds a record that is similar to the record being upsampled (see <a data-type="xref" href="ch06.xhtml#KNN">“K-Nearest Neighbors”</a>) and creates a synthetic record that is a randomly weighted average of the original record and the neighboring record, where the weight is generated separately for each predictor.
The number of synthetic oversampled records created depends on the oversampling ratio required to bring the data set into approximate balance with respect to outcome classes.<a data-type="indexterm" data-primary="Synthetic Minority Oversampling Technique" data-see="SMOTE algorithm" id="idm46522845350104"/></p>

<p>There are several implementations of SMOTE in <em>R</em>.
The most comprehensive package for handling unbalanced data is <code>unbalanced</code>.
It offers a variety of techniques, including a “Racing” algorithm to select the best method.
However, the SMOTE algorithm is simple enough that it can be implemented directly in <em>R</em> using the <code>FNN</code> package.</p>

<p>The <em>Python</em> package <code>imbalanced-learn</code> implements a variety of methods with an API that is compatible with <code>scikit-learn</code>. It provides various methods for over- and undersampling and support for using these techniques with boosting and bagging classifiers.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Cost-Based Classification"><div class="sect2" id="idm46522845344808">
<h2>Cost-Based Classification</h2>

<p>In practice, accuracy and AUC are a poor man’s way to choose a classification rule.<a data-type="indexterm" data-primary="cost-based classification" id="idm46522845343512"/><a data-type="indexterm" data-primary="imbalanced data strategies for classification models" data-secondary="cost-based classification" id="idm46522845342792"/><a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" data-tertiary="cost-based classification" id="idm46522845341736"/>
Often, an estimated cost can be assigned to false positives versus false negatives, and it is more appropriate to incorporate these costs to determine the best cutoff when classifying 1s and 0s.
For example, suppose the expected cost of a default of a new loan is <math alttext="upper C">
  <mi>C</mi>
</math> and the expected return from a paid-off loan is <math alttext="upper R">
  <mi>R</mi>
</math>.
Then the expected return for that loan is:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>expected</mtext>
    <mspace width="4.pt"/>
    <mtext>return</mtext>
    <mo>=</mo>
    <mi>P</mi>
    <mo>(</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mn>0</mn>
    <mo>)</mo>
    <mo>×</mo>
    <mi>R</mi>
    <mo>+</mo>
    <mi>P</mi>
    <mo>(</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>)</mo>
    <mo>×</mo>
    <mi>C</mi>
  </mrow>
</math>
</div>

<p>Instead of simply labeling a loan as default or paid off, or determining the probability of default,
it makes more sense to determine if the loan has a positive expected return.
Predicted probability of default is an intermediate step, and it must be combined with the loan’s total value to determine expected profit, which is the ultimate planning metric of business.
For example, a smaller value loan might be passed over in favor of a larger one with a slightly higher predicted default probability.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Exploring the Predictions"><div class="sect2" id="idm46522845324840">
<h2>Exploring the Predictions</h2>

<p>A single metric, such as AUC, cannot evaluate all aspects of the suitability of a model for a situation.<a data-type="indexterm" data-primary="prediction" data-secondary="exploring predictions from classification models" id="idm46522845322936"/><a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="for predictions from classification models" id="idm46522845321928"/><a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" data-tertiary="exploring the predictions" id="ix_clsssimbpre"/><a data-type="indexterm" data-primary="imbalanced data strategies for classification models" data-secondary="exploring the predictions" id="ix_imbalexppre"/>
<a data-type="xref" href="#ComparisonOfRules">Figure 5-8</a> displays the decision rules for four different models fit to the loan data using just two predictor variables: <code>borrower_score</code> and <code>payment_inc_ratio</code>.
The models are linear discriminant analysis (LDA), logistic linear regression, logistic regression fit using a generalized additive model (GAM), and a tree model (see <a data-type="xref" href="ch06.xhtml#TreeModels">“Tree Models”</a>).<a data-type="indexterm" data-primary="linear discriminant analysis (LDA)" id="idm46522845315208"/><a data-type="indexterm" data-primary="tree models" id="idm46522845314488"/><a data-type="indexterm" data-primary="logistic linear regression" id="idm46522845313816"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="fit using generalized additive model" id="idm46522845313128"/><a data-type="indexterm" data-primary="generalized additive models (GAM)" data-secondary="logistic regression fit with" id="idm46522845312168"/>
The region to the upper left of the lines corresponds to a predicted default.
LDA and logistic linear regression give nearly identical results in this case.
The tree model produces the least regular rule, with two steps.
Finally, the GAM fit of the logistic regression represents a compromise between the tree model and the linear model.</p>

<figure><div id="ComparisonOfRules" class="figure">
<img src="Images/psd2_0508.png" alt="Comparison of the classification rules for four different methods. Logistic and LDA produce nearly identical, overlapping linear classifiers." width="1732" height="1156"/>
<h6><span class="label">Figure 5-8. </span>Comparison of the classification rules for four different methods</h6>
</div></figure>

<p>It is not easy to visualize the prediction rules in higher dimensions
or, in the case of the GAM and the tree model, even to generate the regions for such rules.</p>

<p>In any case, exploratory analysis of predicted values is always warranted.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522845307640">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Highly imbalanced data (i.e., where the interesting outcomes, the 1s, are rare) are problematic for classification algorithms.</p>
</li>
<li>
<p>One strategy for working with imbalanced data is to balance the training data via undersampling the abundant case (or oversampling the rare case).</p>
</li>
<li>
<p>If using all the 1s still leaves you with too few 1s, you can bootstrap the rare cases, or use SMOTE to create synthetic data similar to existing rare cases.</p>
</li>
<li>
<p>Imbalanced data usually indicates that correctly classifying one class (the 1s) has higher value, and that value ratio should be built into the assessment metric.<a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" data-tertiary="exploring the predictions" data-startref="ix_clsssimbpre" id="idm46522845302424"/><a data-type="indexterm" data-primary="imbalanced data strategies for classification models" data-secondary="exploring the predictions" data-startref="ix_imbalexppre" id="idm46522845300920"/></p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522845299144">
<h2>Further Reading</h2>

<ul>
<li>
<p>Tom Fawcett, author of <em>Data Science for Business</em>, has a <a href="https://oreil.ly/us2rd">good article on imbalanced classes</a>.</p>
</li>
<li>
<p>For more on SMOTE, see Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer, <a href="https://oreil.ly/bwaIQ">“SMOTE: Synthetic Minority Over-sampling Technique,”</a> <em>Journal of Artificial Intelligence Research</em> 16 (2002): 321–357.</p>
</li>
<li>
<p>Also see the Analytics Vidhya Content Team’s <a href="https://oreil.ly/gZUDs">“Practical Guide to Deal with Imbalanced Classification Problems in <em>R</em>,”</a> March 28, 2016.</p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46522845932984">
<h1>Summary</h1>

<p>Classification, the <a data-type="indexterm" data-primary="imbalanced data strategies for classification models" data-startref="ix_imbal" id="idm46522845259800"/><a data-type="indexterm" data-primary="classification" data-secondary="strategies for imbalanced data" data-startref="ix_clssmodimb" id="idm46522845258696"/>process of predicting which of two or more categories a record belongs to, is a fundamental tool of predictive analytics.  Will a loan default (yes or no)?  Will it prepay?  Will a web visitor click on a link?  Will they purchase something? Is an insurance claim fraudulent? Often in classification problems, one class is of primary interest (e.g., the fraudulent insurance claim), and in binary classification, this class is designated as a 1, with the other, more prevalent class being a 0. Often, a key part of the process is estimating a <em>propensity score</em>, a probability of belonging to the class of interest.  A common scenario is one in which the class of interest is relatively rare.  When evaluating a classifier, there are a variety of model assessment metrics that go beyond simple accuracy; these are important in the rare-class situation, when classifying all records as 0s can yield high accuracy.<a data-type="indexterm" data-primary="classification" data-startref="ix_clss" id="idm46522845256072"/></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46522850239256"><sup><a href="ch05.xhtml#idm46522850239256-marker">1</a></sup> This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck; used with permission.</p><p data-type="footnote" id="idm46522849542200"><sup><a href="ch05.xhtml#idm46522849542200-marker">2</a></sup> It is certainly surprising that the first article on statistical classification was published in a journal devoted to eugenics. Indeed, there is a <a href="https://oreil.ly/eUJvR">disconcerting connection between the early development of statistics and eugenics</a>.</p><p data-type="footnote" id="idm46522847087352"><sup><a href="ch05.xhtml#idm46522847087352-marker">3</a></sup> Not all methods provide unbiased estimates of probability. In most cases, it is sufficient that the method provide a ranking equivalent to the rankings that would result from an unbiased probability estimate; the cutoff method is then functionally equivalent.</p><p data-type="footnote" id="idm46522846485448"><sup><a href="ch05.xhtml#idm46522846485448-marker">4</a></sup> The ROC curve was first used during World War II to describe the performance of radar receiving stations, whose job was to correctly identify (classify) reflected radar signals and alert defense forces to incoming <span class="keep-together">aircraft</span>.</p><p data-type="footnote" id="idm46522845748808"><sup><a href="ch05.xhtml#idm46522845748808-marker">5</a></sup> Due to differences in implementation, results in <em>Python</em> differ slightly: 1%, or about 1/18 of the expected <span class="keep-together">number</span>.</p></div></div></section></div>



  </body></html>