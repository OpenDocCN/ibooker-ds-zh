- en: Chapter 2\. Introduction to Data Analysis with PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。使用 PySpark 进行数据分析介绍
- en: Python is the most widely used language for data science tasks. The prospect
    of being able to do statistical computing and web programming using the same language
    contributed to its rise in popularity in the early 2010s. This has led to a thriving
    ecosystem of tools and a helpful community for data analysis, often referred to
    as the PyData ecosystem. This is a big reason for PySpark’s popularity. Being
    able to leverage distributed computing via Spark in Python helps data science
    practitioners be more productive because of familiarity with the programming language
    and presence of a wide community. For that same reason, we have opted to write
    our examples in PySpark.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是数据科学任务中最广泛使用的语言。能够使用同一种语言进行统计计算和 Web 编程的前景，促使它在2010年代初期的流行。这导致了一个繁荣的工具生态系统和一个为数据分析提供帮助的社区，通常被称为
    PyData 生态系统。这是 PySpark 受欢迎的一个重要原因。能够通过 Python 中的 Spark 进行分布式计算帮助数据科学从业者更加高效，因为他们熟悉这种编程语言并且有一个广泛的社区支持。出于同样的原因，我们选择在
    PySpark 中编写我们的示例。
- en: It’s difficult to express how transformative it is to do all of your data munging
    and analysis in a single environment, regardless of where the data itself is stored
    and processed. It’s the sort of thing that you have to experience to understand,
    and we wanted to be sure that our examples captured some of that magic feeling
    we experienced when we first started using PySpark. For example, PySpark provides
    interoperability with pandas, which is one of the most popular PyData tools. We
    will explore this feature further in the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 很难用言语表达，在一个环境中进行所有数据整理和分析，而不管数据本身存储在何处或如何处理，这种转变有多么具有变革性。这是一种你必须亲自体验才能理解的事情，我们希望我们的示例能捕捉到我们初次使用
    PySpark 时所体验到的一些魔力感觉。例如，PySpark 提供了与 pandas 的互操作性，后者是最流行的 PyData 工具之一。我们将在本章进一步探索这个特性。
- en: In this chapter, we will explore PySpark’s powerful DataFrame API via a data
    cleansing exercise. In PySpark, the DataFrame is an abstraction for datasets that
    have a regular structure in which each record is a row made up of a set of columns,
    and each column has a well-defined data type. You can think of a dataframe as
    the Spark analogue of a table in a relational database. Even though the naming
    convention might make you think of a `pandas.DataFrame` object, Spark’s DataFrames
    are a different beast. This is because they represent distributed datasets on
    a cluster, not local data where every row in the data is stored on the same machine.
    Although there are similarities in how you use DataFrames and the role they play
    inside the Spark ecosystem, there are some things you may be used to doing when
    working with dataframes in pandas or R that do not apply to Spark, so it’s best
    to think of them as their own distinct entity and try to approach them with an
    open mind.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过一个数据清洗的练习来探索 PySpark 强大的 DataFrame API。在 PySpark 中，DataFrame 是一个抽象概念，用于描述具有规则结构的数据集，其中每条记录是由一组列构成的行，并且每列具有明确定义的数据类型。你可以将
    DataFrame 想象成 Spark 生态系统中的表格的类比。尽管命名约定可能让你以为它类似于 `pandas.DataFrame` 对象，但 Spark
    的 DataFrames 是一种不同的存在。这是因为它们代表了集群上的分布式数据集，而不是本地数据，其中每一行数据都存储在同一台机器上。尽管在使用 DataFrames
    和它们在 Spark 生态系统中扮演的角色方面存在相似之处，但在使用 pandas 或 R 中处理 DataFrame 时习惯的一些事情并不适用于 Spark，因此最好将它们视为独特的实体，并尝试以开放的心态来接近它们。
- en: As for data cleansing, it is the first step in any data science project, and
    often the most important. Many clever analyses have been undone because the data
    analyzed had fundamental quality problems or underlying artifacts that biased
    the analysis or led the data scientist to see things that weren’t really there.
    Hence, what better way to introduce you to working with data using PySpark and
    DataFrames than a data cleansing exercise?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 至于数据清洗，这是任何数据科学项目中的第一步，通常也是最重要的一步。许多精彩的分析因为所分析的数据存在根本的质量问题或基础性的人为瑕疵，导致分析师产生偏见或看到实际并不存在的东西而功亏一篑。因此，没有比通过一个数据清洗的练习更好的方式来介绍使用
    PySpark 和 DataFrame 处理数据。
- en: First, we will introduce PySpark’s fundamentals and practice them using a sample
    dataset from the University of California, Irvine, Machine Learning Repository.
    We’ll reiterate why PySpark is a good choice for data science and introduce its
    programming model. Then we’ll set up PySpark on our system or cluster and analyze
    our dataset using PySpark’s DataFrame API. Most of your time using PySpark for
    data analysis will center around the DataFrame API, so get ready to become intimately
    familiar with it. This will prepare us for the following chapters where we delve
    into various machine learning algorithms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍 PySpark 的基础知识，并使用加利福尼亚大学欧文分校的机器学习库中的示例数据集进行练习。我们将重申为什么 PySpark 是进行数据科学的好选择，并介绍其编程模型。然后，我们将在我们的系统或集群上设置
    PySpark 并使用 PySpark 的 DataFrame API 分析我们的数据集。您在使用 PySpark 进行数据分析时，大部分时间都将围绕着 DataFrame
    API，因此准备好与它密切了解。这将为我们做好准备，以便在接下来的章节中深入探讨各种机器学习算法。
- en: You don’t need to deeply understand how Spark works under the hood for performing
    data science tasks. However, understanding basic concepts about Spark’s architecture
    will make it easier to work with PySpark and make better decisions when writing
    code. That is what we will cover in the next section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于执行数据科学任务而言，您无需深入了解 Spark 如何在底层工作。然而，理解 Spark 架构的基本概念将使您更容易使用 PySpark 并在编写代码时做出更好的决策。这将在下一节中介绍。
- en: When using the DataFrame API, your PySpark code should provide comparable performance
    with Scala. If you’re using a UDF or RDDs, you will have a performance impact.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 DataFrame API 时，您的 PySpark 代码应该具有与 Scala 相当的性能。如果您使用 UDF 或 RDD，则会产生性能影响。
- en: Spark Architecture
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 架构
- en: '![aaps 0201](assets/aaps_0201.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0201](assets/aaps_0201.png)'
- en: Figure 2-1\. Spark architecture diagram
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. Spark 架构图
- en: '[Figure 2-1](#spark_architecture_diagram) depicts the Spark architecture through
    high-level components. Spark applications run as independent sets of processes
    on a cluster or locally. At a high level, a Spark application is comprised of
    a driver process, a cluster manager, and a set of executor processes. The driver
    program is the central component and responsible for distributing tasks across
    executor processes. There will always be just one driver process. When we talk
    about scaling, we mean increasing the number of executors. The cluster manager
    simply manages resources.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-1](#spark_architecture_diagram) 通过高级组件展示了 Spark 架构。Spark 应用程序作为集群或本地独立的进程集运行。在高层次上，Spark
    应用程序由驱动程序、集群管理器和一组执行程序进程组成。驱动程序是中心组件，负责在执行程序进程之间分发任务。将始终存在一个驱动程序进程。当我们谈论扩展性时，我们指的是增加执行程序的数量。集群管理器简单地管理资源。'
- en: Spark is a distributed, data-parallel compute engine. In the data-parallel model,
    more data partitions equals more parallelism. Partitioning allows for efficient
    parallelism. A distributed scheme of breaking up data into chunks or partitions
    allows Spark executors to process only data that is close to them, minimizing
    network bandwidth. That is, each executor’s core is assigned its own data partition
    to work on. Remember this whenever a choice related to partitioning comes up.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个分布式的、数据并行的计算引擎。在数据并行模型中，更多的数据分区意味着更多的并行性。分区允许有效的并行处理。将数据分割成块或分区的分布式方案允许
    Spark 执行程序仅处理靠近它们的数据，从而最小化网络带宽。也就是说，每个执行程序的核心被分配了自己的数据分区来处理。每当涉及到分区的选择时，请记住这一点。
- en: 'Spark programming starts with a dataset, usually residing in some form of distributed,
    persistent storage like the Hadoop distributed file system (HDFS) or a cloud-based
    solution such as AWS S3 and in a format like Parquet. Writing a Spark program
    typically consists of a few steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 编程始于一个数据集，通常驻留在某种形式的分布式持久存储中，例如 Hadoop 分布式文件系统（HDFS）或像 AWS S3 这样的云解决方案，并且格式为
    Parquet。编写 Spark 程序通常包括以下几个步骤：
- en: Define a set of transformations on the input dataset.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入数据集定义一组转换。
- en: Invoke actions that output the transformed datasets to persistent storage or
    return results to the driver’s local memory. These actions will ideally be performed
    by the worker nodes, as depicted on the right in [Figure 2-1](#spark_architecture_diagram).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用操作，将转换后的数据集输出到持久存储或将结果返回到驱动程序的本地内存。这些操作理想情况下将由工作节点执行，如[图 2-1](#spark_architecture_diagram)
    中右侧所示。
- en: Run local computations that operate on the results computed in a distributed
    fashion. These can help you decide what transformations and actions to undertake
    next.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地运行计算，其操作基于分布式计算的结果。这些计算可以帮助您决定接下来要进行的转换和操作。
- en: 'It’s important to remember that all of PySpark’s higher-level abstractions
    still rely on the same philosophy that has been present in Spark since the very
    beginning: the interplay between storage and execution. Understanding these principles
    will help you make better use of Spark for data analysis.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，PySpark 的所有高级抽象仍然依赖于自从 Spark 最初推出以来一直存在的哲学：存储和执行的相互作用。了解这些原则将帮助您更好地利用
    Spark 进行数据分析。
- en: Next, we will install and set up PySpark on our machine so that we can start
    performing data analysis. This is a one-time exercise that will help us run the
    code examples from this and following chapters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在我们的机器上安装并设置 PySpark，以便开始进行数据分析。这是一个一次性的练习，将帮助我们运行本章和以下章节的代码示例。
- en: Installing PySpark
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 PySpark
- en: The examples and code in this book assume you have Spark 3.1.1 available. For
    the purpose of following the code examples, install PySpark from the [PyPI repository](https://oreil.ly/t0WBZ).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例和代码假设您有 Spark 3.1.1 可用。为了跟随代码示例，从 [PyPI 仓库](https://oreil.ly/t0WBZ) 安装
    PySpark。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that PySpark requires Java 8 or later to be installed. If you want SQL,
    ML, and/or MLlib as extra dependencies, that’s an option too. We will need these
    later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PySpark 需要安装 Java 8 或更新版本。如果您想要 SQL、ML 和/或 MLlib 作为额外的依赖项，这也是一个选择。我们稍后会需要这些。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Installing from PyPI skips the libraries required to run Scala, Java, or R.
    Full releases can be obtained from the [Spark project site](https://oreil.ly/pK2Wi).
    Refer to the [Spark documentation](https://oreil.ly/FLh4U) for instructions on
    setting up a Spark environment, whether on a cluster or simply on your local machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PyPI 安装跳过了运行 Scala、Java 或 R 所需的库。您可以从 [Spark 项目站点](https://oreil.ly/pK2Wi)
    获取完整的发布版。请参阅 [Spark 文档](https://oreil.ly/FLh4U)，了解如何在集群或本地环境中设置 Spark 环境的说明。
- en: 'Now we’re ready to launch the `pyspark-shell`, which is an REPL for the Python
    language that also has some Spark-specific extensions. This is similar to the
    Python or IPython shell that you may have used. If you’re just running these examples
    on your personal computer, you can launch a local Spark cluster by specifying
    `local[N]`, where `N` is the number of threads to run, or `*` to match the number
    of cores available on your machine. For example, to launch a local cluster that
    uses eight threads on an eight-core machine:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备启动 `pyspark-shell`，这是 Python 语言的 REPL，还具有一些特定于 Spark 的扩展功能。这类似于您可能使用过的
    Python 或 IPython shell。如果您只是在个人电脑上运行这些示例，可以通过指定 `local[N]`（其中 `N` 是要运行的线程数）或 `*`（匹配机器上可用的核心数）来启动本地
    Spark 集群。例如，在八核机器上启动一个使用八个线程的本地集群：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A Spark application itself is often referred to as a Spark *cluster*. That is
    a logical abstraction and is different from a physical cluster (multiple machines).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 应用本身通常被称为 Spark *集群*。这是一个逻辑抽象，不同于物理集群（多台机器）。
- en: 'If you have a Hadoop cluster that runs a version of Hadoop that supports YARN,
    you can launch the Spark jobs on the cluster by using the value of `yarn` for
    the Spark master:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个运行支持 YARN 的 Hadoop 集群，可以使用 `yarn` 作为 Spark 主机值，在集群上启动 Spark 作业：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The rest of the examples in this book will not show a `--master` argument to
    `spark-shell`, but you will typically need to specify this argument as appropriate
    for your environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本书其余示例中不会显示 `--master` 参数给 `spark-shell`，但通常需要根据您的环境指定此参数。
- en: You may need to specify additional arguments to make the Spark shell fully utilize
    your resources. A list of arguments can be found by executing `pyspark --help`.
    For example, when running Spark with a local master, you can use `--driver-memory
    2g` to let the single local process use 2 GB of memory. YARN memory configuration
    is more complex, and relevant options like `--executor-memory` are explained in
    the [Spark on YARN documentation](https://oreil.ly/3bRjy).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 若要使 Spark shell 充分利用您的资源，您可能需要指定额外的参数。您可以通过执行 `pyspark --help` 查找参数列表。例如，在使用本地主机运行
    Spark 时，您可以使用 `--driver-memory 2g` 让单个本地进程使用 2 GB 内存。YARN 内存配置更为复杂，类似 `--executor-memory`
    的相关选项在 [Spark on YARN 文档](https://oreil.ly/3bRjy) 中有详细解释。
- en: 'The Spark framework officially supports four cluster deployment modes: standalone,
    YARN, Kubernetes, and Mesos. More details can be found in the [Deploying Spark
    documentation](https://oreil.ly/hG2a5).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 框架正式支持四种集群部署模式：独立模式、YARN、Kubernetes 和 Mesos。更多细节可以在 [部署 Spark 文档](https://oreil.ly/hG2a5)
    中找到。
- en: 'After running one of these commands, you will see a lot of log messages from
    Spark as it initializes itself, but you should also see a bit of ASCII art, followed
    by some additional log messages and a prompt:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些命令之后，你将看到 Spark 初始化自身的大量日志消息，但你还应该看到一些 ASCII 艺术，然后是一些额外的日志消息和一个提示符：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can run the `:help` command in the shell. This will prompt you to either
    start an interactive help mode or ask for help about specific Python objects.
    In addition to the note about `:help`, the Spark log messages indicated “SparkSession
    available as *spark*.” This is a reference to the `SparkSession`, which acts as
    an entry point to all Spark operations and data. Go ahead and type `spark` at
    the command line:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 shell 中运行`:help`命令。这将提示你启动交互式帮助模式或者请求关于特定 Python 对象的帮助。除了关于`:help`的说明之外，Spark
    日志消息还指示“SparkSession 可以作为 *spark* 使用。”这是对`SparkSession`的引用，它充当了所有 Spark 操作和数据的入口点。继续在命令行中输入`spark`：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The REPL will print the string form of the object. For the `SparkSession` object,
    this is simply its name plus the hexadecimal address of the object in memory.
    (`DEADBEEF` is a placeholder; the exact value you see here will vary from run
    to run.) In an interactive Spark shell, the Spark driver instantiates a SparkSession
    for you, while in a Spark application, you create a SparkSession object yourself.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: REPL 将打印对象的字符串形式。对于`SparkSession`对象，这只是它的名称加上对象在内存中的十六进制地址。(`DEADBEEF`是一个占位符；你看到的确切值会因运行而异。)
    在交互式 Spark shell 中，Spark 驱动程序会为你实例化一个 SparkSession，而在 Spark 应用中，你自己创建一个 SparkSession
    对象。
- en: In Spark 2.0, the SparkSession became a unified entry point to all Spark operations
    and data. Previously used entry points such as SparkContext, SQLContext, HiveContext,
    SparkConf, and StreamingContext can be accessed through it too.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 2.0 中，SparkSession 成为了所有 Spark 操作和数据的统一入口点。之前使用的入口点如 SparkContext、SQLContext、HiveContext、SparkConf
    和 StreamingContext 也可以通过它来访问。
- en: 'What exactly do we do with the `spark` variable? `SparkSession` is an object,
    so it has methods associated with it. We can see what those methods are in the
    PySpark shell by typing the name of a variable, followed by a period, followed
    by tab:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何处理`spark`变量？`SparkSession`是一个对象，因此它有相关的方法。我们可以在 PySpark shell 中输入变量名后跟一个点和制表符来查看这些方法：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Out of all the methods provided by SparkSession, the ones that we’re going to
    use most often allow us to create DataFrames. Now that we have set up PySpark,
    we can set up our dataset of interest and start using PySpark’s DataFrame API
    to interact with it. That’s what we will do in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SparkSession 提供的所有方法中，我们将最常用的那些方法用于创建 DataFrame。现在我们已经设置好了 PySpark，我们可以设置我们感兴趣的数据集，并开始使用
    PySpark 的 DataFrame API 与其交互。这就是我们将在下一节中要做的事情。
- en: Setting Up Our Data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置我们的数据
- en: The UC Irvine Machine Learning Repository is a fantastic source for interesting
    (and free) datasets for research and education. The dataset we’ll analyze was
    curated from a record linkage study performed at a German hospital in 2010, and
    it contains several million pairs of patient records that were matched according
    to several different criteria, such as the patient’s name (first and last), address,
    and birthday. Each matching field was assigned a numerical score from 0.0 to 1.0
    based on how similar the strings were, and the data was then hand-labeled to identify
    which pairs represented the same person and which did not. The underlying values
    of the fields that were used to create the dataset were removed to protect the
    privacy of the patients. Numerical identifiers, the match scores for the fields,
    and the label for each pair (match versus nonmatch) were published for use in
    record linkage research.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: UC Irvine 机器学习库是一个提供有趣（且免费）数据集用于研究和教育的绝佳资源。我们将要分析的数据集是从 2010 年在德国一家医院进行的记录链接研究中精选出来的，它包含了数百万对病人记录，这些记录是根据多种不同的标准进行匹配的，比如病人的姓名（名和姓）、地址和生日。每个匹配字段都被分配了一个从
    0.0 到 1.0 的数值分数，这个分数是根据字符串的相似程度来确定的，然后对数据进行了手工标记，以确定哪些对代表同一个人，哪些不代表。用于创建数据集的字段的基本值已被移除，以保护病人的隐私。数字标识符、字段的匹配分数以及每个对的标签（匹配与不匹配）都已发布，供记录链接研究使用。
- en: 'From the shell, let’s pull the data from the repository:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从 shell 中，让我们从库中提取数据：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you have a Hadoop cluster handy, you can create a directory for the block
    data in HDFS and copy the files from the dataset there:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个 Hadoop 集群可用，你可以在 HDFS 中为块数据创建一个目录，并将数据集中的文件复制到那里：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To create a dataframe for our record linkage dataset, we’re going to use the
    `S⁠p⁠a⁠r⁠k​S⁠e⁠s⁠s⁠i⁠o⁠n` object. Specifically, we will use the `csv` method on
    its Reader API:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要为我们的记录链接数据集创建一个数据框架，我们将使用`S⁠p⁠a⁠r⁠k​S⁠e⁠s⁠s⁠i⁠o⁠n`对象。具体来说，我们将使用其Reader API上的`csv`方法：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'By default, every column in a CSV file is treated as a `string` type, and the
    column names default to `_c0`, `_c1`, `_c2`, and so on. We can look at the head
    of a dataframe in the shell by calling its `show` method:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，CSV文件中的每列都被视为`string`类型，并且列名默认为`_c0`、`_c1`、`_c2`等。我们可以通过调用其`show`方法在shell中查看数据框架的头部：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that the first row of the DataFrame is the name of the header columns,
    as we expected, and that the CSV file has been cleanly split up into its individual
    columns. We can also see the presence of the `?` strings in some of the columns;
    we will need to handle these as missing values. In addition to naming each column
    correctly, it would be ideal if Spark could properly infer the data type of each
    of the columns for us.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到DataFrame的第一行是标题列的名称，正如我们预期的那样，并且CSV文件已经被干净地分割成其各个列。我们还可以看到某些列中存在`?`字符串；我们需要将这些处理为缺失值。除了正确命名每个列外，如果Spark能够正确推断每列的数据类型，那将是理想的。
- en: 'Fortunately, Spark’s CSV reader provides all of this functionality for us via
    options that we can set on the Reader API. You can see the full list of options
    that the API takes in the [`pyspark` documentation](https://oreil.ly/xiLj1). For
    now, we’ll read and parse the linkage data like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Spark的CSV阅读器通过我们可以在Reader API上设置的选项为我们提供了所有这些功能。您可以在[`pyspark`文档](https://oreil.ly/xiLj1)中看到API接受的完整选项列表。目前，我们将像这样读取和解析链接数据：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When we call `show` on the `parsed` data, we see that the column names are
    set correctly and the `?` strings have been replaced by `null` values. To see
    the inferred type for each column, we can print the schema of the `parsed` DataFrame
    like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对`parsed`数据调用`show`时，我们可以看到列名已正确设置，而`?`字符串已被替换为`null`值。要查看每列的推断类型，我们可以像这样打印`parsed`
    DataFrame的模式：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Each `Column` instance contains the name of the column, the most specific data
    type that could handle the type of data contained in each record, and a boolean
    field that indicates whether or not the column may contain null values, which
    is true by default. In order to perform the schema inference, Spark must do *two*
    passes over the dataset: one pass to figure out the type of each column, and a
    second pass to do the actual parsing. The first pass can work on a sample if desired.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`Column`实例包含列的名称、可以处理每条记录中包含的数据类型的最具体数据类型以及一个布尔字段，指示列是否可能包含null值，默认情况下为true。为了执行模式推断，Spark必须对数据集进行*两次*扫描：一次用于确定每列的类型，另一次用于实际解析。如果需要，第一次可以对样本进行处理。
- en: If you know the schema that you want to use for a file ahead of time, you can
    create an instance of the `pyspark.sql.types.StructType` class and pass it to
    the Reader API via the `schema` function. This can have a significant performance
    benefit when the dataset is very large, since Spark will not need to perform an
    extra pass over the data to figure out the data type of each column.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您预先知道要为文件使用的模式，可以创建`pyspark.sql.types.StructType`类的实例，并通过`schema`函数将其传递给Reader
    API。当数据集非常大时，这可能会带来显著的性能优势，因为Spark不需要再次扫描数据以确定每列的数据类型。
- en: 'Here is an example of defining a schema using `StructType` and `StructField`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`StructType`和`StructField`定义模式的示例：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Another way to define the schema is using DDL (data definition language) statements:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种定义模式的方法是使用DDL（数据定义语言）语句：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'DataFrames have a number of methods that enable us to read data from the cluster
    into the PySpark REPL on our client machine. Perhaps the simplest of these is
    `first`, which returns the first element of the DataFrame into the client:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames有许多方法，使我们能够从群集中的数据读取到客户端机器上的PySpark REPL中。其中最简单的方法之一是`first`，它将DataFrame的第一个元素返回到客户端：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `first` method can be useful for sanity checking a dataset, but we’re generally
    interested in bringing back larger samples of a DataFrame into the client for
    analysis. When we know that a DataFrame contains only a small number of records,
    we can use the `toPandas` or `collect` method to return all the contents of a
    DataFrame to the client as an array. For extremely large DataFrames, using these
    methods can be dangerous and cause an out-of-memory exception. Because we don’t
    know how big the linkage dataset is just yet, we’ll hold off on doing this right
    now.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`first` 方法对于检查数据集的合理性很有用，但是我们通常对将 DataFrame 的较大样本带回客户端进行分析更感兴趣。当我们知道 DataFrame
    只包含少量记录时，我们可以使用 `toPandas` 或 `collect` 方法将 DataFrame 的所有内容作为数组返回给客户端。对于非常大的 DataFrame，使用这些方法可能会导致内存不足异常。因为我们目前还不知道链接数据集有多大，所以我们暂时不做这个操作。'
- en: In the next several sections, we’ll use a mix of local development and testing
    and cluster computation to perform more munging and analysis of the record linkage
    data, but if you need to take a moment to drink in the new world of awesome that
    you have just entered, we certainly understand.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将使用本地开发和测试以及集群计算来执行更多的数据清洗和记录链接数据的分析，但是如果您需要花点时间来享受您刚刚进入的这个令人惊叹的新世界，我们当然理解。
- en: Analyzing Data with the DataFrame API
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DataFrame API 进行数据分析
- en: The DataFrame API comes with a powerful set of tools that will likely be familiar
    to data scientists who are used to Python and SQL. In this section, we will begin
    to explore these tools and how to apply them to the record linkage data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame API 提供了一套强大的工具，这些工具对于习惯于 Python 和 SQL 的数据科学家可能会很熟悉。在本节中，我们将开始探索这些工具以及如何将它们应用于记录链接数据。
- en: 'If we look at the schema of the `parsed` DataFrame and the first few rows of
    data, we see this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 `parsed` DataFrame 的模式和前几行数据，我们会看到这样：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first two fields are integer IDs that represent the patients who were matched
    in the record.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个字段是整数 ID，表示在记录中匹配的病人。
- en: The next nine fields are (possibly missing) numeric values (either doubles or
    ints) that represent match scores on different fields of the patient records,
    such as their names, birthdays, and locations. The fields are stored as integers
    when the only possible values are match (1) or no-match (0), and doubles whenever
    partial matches are possible.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的九个字段是（可能缺失的）数值（可以是双精度或整数），表示病人记录不同字段（如姓名、生日和位置）上的匹配分数。当只可能的值是匹配（1）或不匹配（0）时，这些字段存储为整数；当可能存在部分匹配时，存储为双精度数值。
- en: The last field is a boolean value (`true` or `false`) indicating whether or
    not the pair of patient records represented by the line was a match.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个字段是布尔值（`true` 或 `false`），指示表示由该行表示的病人记录对是否匹配。
- en: 'Our goal is to come up with a simple classifier that allows us to predict whether
    a record will be a match based on the values of the match scores for the patient
    records. Let’s start by getting an idea of the number of records we’re dealing
    with via the `count` method:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是设计一个简单的分类器，以便根据病人记录的匹配分数的值预测记录是否匹配。让我们首先通过 `count` 方法了解我们要处理的记录数量：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is a relatively small dataset—certainly small enough to fit in memory on
    one of the nodes in a cluster or even on your local machine if you don’t have
    a cluster available. Thus far, every time we’ve processed the data, Spark has
    reopened the file, reparsed the rows, and then performed the action requested,
    like showing the first few rows of the data or counting the number of records.
    When we ask another question, Spark will do these same operations, again and again,
    even if we have filtered the data down to a small number of records or are working
    with an aggregated version of the original dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对较小的数据集——足够小，以至于可以放入集群中的一个节点的内存中，甚至是在您的本地计算机上（如果您没有集群可用的话）。到目前为止，每当我们处理数据时，Spark
    都会重新打开文件，重新解析行，然后执行请求的操作，比如显示数据的前几行或者计算记录的数量。当我们提出另一个问题时，Spark 将再次执行这些操作，即使我们已将数据过滤到少量记录或正在使用原始数据集的聚合版本。
- en: 'This isn’t an optimal use of our compute resources. After the data has been
    parsed once, we’d like to save the data in its parsed form on the cluster so that
    we don’t have to reparse it every time we want to ask a new question. Spark supports
    this use case by allowing us to signal that a given DataFrame should be cached
    in memory after it is generated by calling the `cache` method on the instance.
    Let’s do that now for the `parsed` DataFrame:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我们计算资源的最佳利用方式。数据解析后，我们希望将数据保存在集群上以其解析形式，这样每次想要提出新问题时就无需重新解析数据。Spark通过允许我们调用实例上的`cache`方法来信号化给定的DataFrame在生成后应缓存在内存中来支持这种用例。现在让我们对`parsed`
    DataFrame这样做：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once our data has been cached, the next thing we want to know is the relative
    fraction of records that were matches versus those that were nonmatches:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的数据已被缓存，我们想知道的下一件事是记录匹配与非匹配的相对比例：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Instead of writing a function to extract the `is_match` column, we simply pass
    its name to the `groupBy` method on the DataFrame, call the `count` method to,
    well, count the number of records inside each grouping, sort the resulting data
    in descending order based on the `count` column, and then cleanly render the result
    of the computation in the REPL with `show`. Under the covers, the Spark engine
    determines the most efficient way to perform the aggregation and return the results.
    This illustrates the clean, fast, and expressive way to do data analysis that
    Spark provides.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 不是编写一个函数来提取`is_match`列，而是直接将其名称传递给DataFrame上的`groupBy`方法，调用`count`方法来统计每个分组内的记录数，基于`count`列按降序排序，然后使用`show`在REPL中清晰地呈现计算结果。在底层，Spark引擎确定执行聚合并返回结果的最有效方式。这展示了使用Spark进行数据分析的清晰、快速和表达力强的方式。
- en: 'Note that there are two ways we can reference the names of the columns in the
    DataFrame: either as literal strings, like in `groupBy("is_match")`, or as `Column`
    objects by using the `col` function that we used on the `count` column. Either
    approach is valid in most cases, but we needed to use the `col` function to call
    the `desc` method on the resulting `count` column object.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们可以以两种方式引用DataFrame中列的名称：作为文字字符串，如在`groupBy("is_match")`中，或者作为`Column`对象，通过在`count`列上使用的`col`函数。在大多数情况下，任何一种方法都是有效的，但我们需要使用`col`函数来调用所得到的`count`列对象上的`desc`方法。
- en: 'You may have noticed that the functions in the DataFrame API are similar to
    the components of a SQL query. This isn’t a coincidence, and in fact we have the
    option to treat any DataFrame we create as if it were a database table and to
    express our questions using familiar and powerful SQL syntax. First, we need to
    tell the Spark SQL execution engine the name it should associate with the `parsed`
    DataFrame, since the name of the variable itself (“parsed”) isn’t available to
    Spark:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，DataFrame API中的函数类似于SQL查询的组件。这不是巧合，事实上，我们可以将我们创建的任何DataFrame视为数据库表，并使用熟悉和强大的SQL语法来表达我们的问题。首先，我们需要告诉Spark
    SQL执行引擎应将`parsed` DataFrame关联的名称，因为变量本身的名称（“parsed”）对于Spark不可用：
- en: '[PRE20]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Because the `parsed` DataFrame is available only during the length of this PySpark
    REPL session, it is a *temporary* table. Spark SQL may also be used to query persistent
    tables in HDFS if we configure Spark to connect to an Apache Hive metastore that
    tracks the schemas and locations of structured datasets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`parsed` DataFrame仅在此PySpark REPL会话期间可用，它是一个临时表。如果我们配置Spark连接到跟踪结构化数据集架构和位置的Apache
    Hive元数据存储，则Spark SQL也可以用于查询HDFS中的持久表。
- en: 'Once our temporary table is registered with the Spark SQL engine, we can query
    it like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的临时表已在Spark SQL引擎中注册，我们可以像这样查询它：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You have the option of running Spark either by using an ANSI 2003-compliant
    version of Spark SQL (the default) or in HiveQL mode by calling the `enableHiveSupport`
    method when you create a `SparkSession` instance via its Builder API.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择通过调用`enableHiveSupport`方法，在创建`SparkSession`实例时使用符合ANSI 2003标准的Spark SQL（默认方式），或者以HiveQL模式运行Spark。
- en: 'Should you use Spark SQL or the DataFrame API to do your analysis in PySpark?
    There are pros and cons to each: SQL has the benefit of being broadly familiar
    and expressive for simple queries. It also lets you query data using JDBC/ODBC
    connectors from databases such as PostgreSQL or tools such as Tableau. The downside
    of SQL is that it can be difficult to express complex, multistage analyses in
    a dynamic, readable, and testable way—all areas where the DataFrame API shines.
    Throughout the rest of the book, we use both Spark SQL and the DataFrame API,
    and we leave it as an exercise for the reader to examine the choices we made and
    translate our computations from one interface to the other.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在PySpark中进行分析时，应该使用Spark SQL还是DataFrame API？各有利弊：SQL具有广泛的熟悉性和表达能力，适用于简单查询。它还允许您使用像PostgreSQL这样的数据库或像Tableau这样的工具通过JDBC/ODBC连接器查询数据。SQL的缺点在于，它可能难以以动态、可读和可测试的方式表达复杂的多阶段分析——而在这些方面，DataFrame
    API表现出色。在本书的其余部分，我们将同时使用Spark SQL和DataFrame API，并留给读者来审视我们所做选择，并将我们的计算从一种接口转换到另一种接口。
- en: We can apply functions one by one to our DataFrame to obtain statistics such
    as count and mean. However, PySpark offers a better way to obtain summary statistics
    for DataFrames, and that’s what we will cover in the next section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐个在DataFrame上应用函数来获得统计数据，例如计数和均值。然而，PySpark提供了一种更好的方法来获取数据框的汇总统计信息，这正是我们将在下一节中介绍的内容。
- en: Fast Summary Statistics for DataFrames
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速数据框汇总统计信息
- en: 'Although there are many kinds of analyses that may be expressed equally well
    in SQL or with the DataFrame API, there are certain common things that we want
    to be able to do with dataframes that can be tedious to express in SQL. One such
    analysis that is especially helpful is computing the min, max, mean, and standard
    deviation of all the non-null values in the numerical columns of a dataframe.
    In PySpark, this function has the same name that it does in pandas, `describe`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多分析可以同样适用SQL或DataFrame API来表达，但是有一些常见的数据框操作，在SQL中表达起来可能会显得繁琐。其中一种特别有帮助的分析是计算数据框中所有数值列非空值的最小值、最大值、均值和标准差。在PySpark中，这个函数与pandas中的名称相同，即`describe`：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `summary` DataFrame has one column for each variable in the `parsed` DataFrame,
    along with another column (also named `summary`) that indicates which metric—`count`,
    `mean`, `stddev`, `min`, or `max`—is present in the rest of the columns in the
    row. We can use the `select` method to choose a subset of the columns to make
    the summary statistics easier to read and compare:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary`数据框中每个变量都有一列与`parsed`数据框中的其他列（也称为`summary`）相关的度量标准——`count`、`mean`、`stddev`、`min`或`max`。我们可以使用`select`方法选择列的子集，以使汇总统计信息更易于阅读和比较：'
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note the difference in the value of the `count` variable between `cmp_fname_c1`
    and `cmp_fname_c2`. While almost every record has a non-null value for `cmp_fname_c1`,
    less than 2% of the records have a non-null value for `cmp_fname_c2`. To create
    a useful classifier, we need to rely on variables that are almost always present
    in the data—unless the fact that they are missing indicates something meaningful
    about whether the record matches.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`count`变量在`cmp_fname_c1`和`cmp_fname_c2`之间的差异。几乎每条记录都有`cmp_fname_c1`的非空值，但不到2%的记录有`cmp_fname_c2`的非空值。要创建一个有用的分类器，我们需要依赖那些数据中几乎总是存在的变量，除非它们的缺失说明了记录是否匹配的某些有意义的信息。
- en: 'Once we have an overall feel for the distribution of the variables in our data,
    we want to understand how the values of those variables are correlated with the
    value of the `is_match` column. Therefore, our next step is to compute those same
    summary statistics for just the subsets of the `parsed` DataFrame that correspond
    to matches and nonmatches. We can filter DataFrames using either SQL-style `where`
    syntax or with `Column` objects using the DataFrame API and then use `describe`
    on the resulting DataFrames:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对数据中变量的分布有了整体感觉，我们希望了解这些变量的值如何与`is_match`列的值相关联。因此，我们的下一步是仅计算与匹配和非匹配对应的`parsed`
    DataFrame子集的相同汇总统计信息。我们可以使用类似SQL的`where`语法或使用DataFrame API中的`Column`对象来过滤数据框，然后对结果数据框使用`describe`：
- en: '[PRE24]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The logic inside the string we pass to the `where` function can include statements
    that would be valid inside a `WHERE` clause in Spark SQL. For the filtering condition
    that uses the DataFrame API, we use the `==` operator on the `is_match` column
    object to check for equality with the boolean object `False`, because that is
    just Python, not SQL. Note that the `where` function is an alias for the `filter`
    function; we could have reversed the `where` and `filter` calls in the above snippet
    and everything would have worked the same way.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在传递给`where`函数的字符串中使用的逻辑可以包含在 Spark SQL 的`WHERE`子句中有效的语句。对于使用 DataFrame API
    的过滤条件，我们在`is_match`列对象上使用`==`运算符，检查是否等于布尔对象`False`，因为这只是 Python，而不是 SQL。请注意，`where`函数是`filter`函数的别名；我们可以在上面的片段中颠倒`where`和`filter`调用，结果仍然会相同。
- en: We can now start to compare our `match_summary` and `miss_summary` DataFrames
    to see how the distribution of the variables changes depending on whether the
    record is a match or a miss. Although this is a relatively small dataset, doing
    this comparison is still somewhat tedious—what we really want is to transpose
    the `match_summary` and `miss_summary` DataFrames so that the rows and columns
    are swapped, which would allow us to join the transposed DataFrames together by
    variable and analyze the summary statistics, a practice that most data scientists
    know as “pivoting” or “reshaping” a dataset. In the next section, we’ll show you
    how to perform these transformations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始比较`match_summary`和`miss_summary` DataFrames，以查看变量的分布如何随记录是匹配还是未匹配而变化。尽管这是一个相对较小的数据集，但进行这种比较仍然有些繁琐——我们真正想要的是对`match_summary`和`miss_summary`
    DataFrames进行转置，使行和列互换，这样我们可以通过变量联合转置后的 DataFrames 并分析汇总统计数据，这是大多数数据科学家所知的“数据透视”或“重塑”数据集的做法。在下一节中，我们将展示如何执行这些转换。
- en: Pivoting and Reshaping DataFrames
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据透视和重塑 DataFrames
- en: We can transpose the DataFrames entirely using functions provided by PySpark.
    However, there is another way to perform this task. PySpark allows conversion
    between Spark and pandas DataFrames. We will convert the DataFrames in question
    into pandas DataFrames, reshape them, and convert them back to Spark DataFrames.
    We can safely do this because of the small size of the `summary`, `match_summary`,
    and `miss_summary` DataFrames since pandas DataFrames reside in memory. In upcoming
    chapters, we will rely on Spark operations for such transformations on larger
    datasets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 PySpark 提供的函数完全转置 DataFrames。然而，还有另一种执行此任务的方法。PySpark 允许在 Spark 和 pandas
    DataFrames 之间进行转换。我们将会把相关的 DataFrames 转换为 pandas DataFrames，重新整理它们，然后再转换回 Spark
    DataFrames。由于`summary`、`match_summary`和`miss_summary` DataFrames 的规模较小，我们可以安全地这样做，因为
    pandas DataFrames 存储在内存中。在接下来的章节中，对于更大的数据集，我们将依赖于 Spark 操作来进行这些转换。
- en: Conversion to/from pandas DataFrames is possible because of the Apache Arrow
    project, which allows efficient data transfer between JVM and Python processes.
    The PyArrow library was installed as a dependency of the Spark SQL module when
    we installed `pyspark[sql]` using pip.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Apache Arrow 项目的支持，我们可以进行 pandas DataFrames 与 Spark DataFrames 之间的转换，它允许在
    JVM 和 Python 进程之间进行高效的数据传输。当我们使用 pip 安装`pyspark[sql]`时，PyArrow 库已作为 Spark SQL
    模块的依赖安装。
- en: 'Let’s convert `summary` into a pandas DataFrame:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将`summary`转换为 pandas DataFrame：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now use pandas functions on the `summary_p` DataFrame:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在`summary_p` DataFrame 上使用 pandas 函数：
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now perform a transpose operation to swap rows and columns using familiar
    pandas methods on the DataFrame:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以执行一个转置操作，使用熟悉的 pandas 方法在 DataFrame 上交换行和列：
- en: '[PRE27]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We have successfully transposed the `summary_p` pandas DataFrame. Convert it
    into a Spark DataFrame using SparkSession’s `createDataFrame` method:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功转置了`summary_p` pandas DataFrame。使用 SparkSession 的`createDataFrame`方法将其转换为
    Spark DataFrame：
- en: '[PRE28]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We are not done yet. Print the schema of the `summaryT` DataFrame:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成。打印`summaryT` DataFrame 的模式：
- en: '[PRE29]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the summary schema, as obtained from the `describe` method, every field
    is treated as a string. Since we want to analyze the summary statistics as numbers,
    we’ll need to convert the values from strings to doubles:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在从`describe`方法获取的 summary 架构中，每个字段都被视为字符串。由于我们希望将汇总统计数据分析为数字，我们需要将值从字符串转换为双精度数：
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that we have figured out how to transpose a summary DataFrame, let’s implement
    our logic into a function that we can reuse on the `match_summary` and `m⁠i⁠s⁠s⁠_​s⁠u⁠m⁠m⁠a⁠r⁠y`
    DataFrames:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经弄清楚如何转置汇总 DataFrame，让我们把我们的逻辑实现为一个函数，以便在`match_summary`和`m⁠i⁠s⁠s⁠_​s⁠u⁠m⁠m⁠a⁠r⁠y`
    DataFrames 上重复使用：
- en: '[PRE31]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now in your Spark shell, use the `pivot_summary` function on the `match_summary`
    and `miss_summary` DataFrames:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在你的 Spark shell 中，对 `match_summary` 和 `miss_summary` DataFrames 使用 `pivot_summary`
    函数：
- en: '[PRE32]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now that we have successfully transposed the summary DataFrames, we can join
    and compare them. That’s what we will do in the next section. Further, we will
    also select desirable features for building our model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们成功地转置了摘要 DataFrame，接下来我们将它们联接和比较。这将在下一节中进行。此外，我们还将选择用于构建模型的理想特征。
- en: Joining DataFrames and Selecting Features
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接 DataFrames 和选择特征
- en: 'So far, we have used Spark SQL and the DataFrame API only to filter and aggregate
    the records from a dataset, but we can also use these tools to perform joins (inner,
    left outer, right outer, or full outer) on DataFrames. Although the DataFrame
    API includes a `join` function, it’s often easier to express these joins using
    Spark SQL, especially when the tables we are joining have a large number of column
    names in common and we want to be able to clearly indicate which column we are
    referring to in our select expressions. Let’s create temporary views for the `match_summaryT`
    and `miss_summaryT` DataFrames, join them on the `field` column, and compute some
    simple summary statistics on the resulting rows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅使用 Spark SQL 和 DataFrame API 来过滤和聚合数据集中的记录，但我们也可以使用这些工具在 DataFrame
    上执行连接操作（内连接、左连接、右连接或全连接）。虽然 DataFrame API 包括一个 `join` 函数，但是在要连接的表具有许多列名相同时，并且我们想要能够清楚地指示我们在选择表达式中正在引用的列时，使用
    Spark SQL 表达这些连接通常更容易。让我们为 `match_summaryT` 和 `miss_summaryT` DataFrames 创建临时视图，在
    `field` 列上对它们进行连接，并对结果行进行一些简单的摘要统计：
- en: '[PRE33]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A good feature has two properties: it tends to have significantly different
    values for matches and nonmatches (so the difference between the means will be
    large), and it occurs often enough in the data that we can rely on it to be regularly
    available for any pair of records. By this measure, `cmp_fname_c2` isn’t very
    useful because it’s missing a lot of the time, and the difference in the mean
    value for matches and nonmatches is relatively small—0.09, for a score that ranges
    from 0 to 1\. The `cmp_sex` feature also isn’t particularly helpful because even
    though it’s available for any pair of records, the difference in means is just
    0.03.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的特征具有两个属性：它倾向于在匹配和非匹配中有显著不同的值（因此均值之间的差异会很大），并且在数据中经常出现，我们可以依赖它定期为任何一对记录提供。按此标准，`cmp_fname_c2`
    并不是非常有用，因为它经常缺失，而且在匹配和非匹配的均值之间的差异相对较小——为了一个从 0 到 1 的得分来说，差异只有 0.09。`cmp_sex` 特征也并不特别有帮助，因为即使它对于任何一对记录都是可用的，均值之间的差异只有
    0.03。
- en: 'Features `cmp_plz` and `cmp_by`, on the other hand, are excellent. They almost
    always occur for any pair of records, and there is a very large difference in
    the mean values (more than 0.77 for both features). Features `cmp_bd`, `cmp_lname_c1`,
    and `cmp_bm` also seem beneficial: they are generally available in the dataset,
    and the difference in mean values for matches and nonmatches is substantial.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 `cmp_plz` 和 `cmp_by` 非常出色。几乎每对记录都会出现它们，并且它们的均值差异非常大（两个特征均超过 0.77）。特征 `cmp_bd`、`cmp_lname_c1`
    和 `cmp_bm` 也看起来有益：它们通常在数据集中出现，并且在匹配和非匹配之间的均值差异也很显著。
- en: 'Features `cmp_fname_c1` and `cmp_lname_c2` are more of a mixed bag: `cmp_fname_c1`
    doesn’t discriminate all that well (the difference in the means is only 0.28)
    even though it’s usually available for a pair of records, whereas `cmp_lname_c2`
    has a large difference in the means, but it’s almost always missing. It’s not
    quite obvious under what circumstances we should include these features in our
    model based on this data.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 `cmp_fname_c1` 和 `cmp_lname_c2` 的情况更加复杂：`cmp_fname_c1` 的区分度并不太好（均值之间的差异仅为
    0.28），尽管它通常对于一对记录来说是可用的，而 `cmp_lname_c2` 的均值差异很大，但几乎总是缺失。基于这些数据，我们不太明确在什么情况下应该将这些特征包含在我们的模型中。
- en: 'For now, we’re going to use a simple scoring model that ranks the similarity
    of pairs of records based on the sums of the values of the obviously good features:
    `cmp_plz`, `cmp_by`, `cmp_bd`, `cmp_lname_c1`, and `cmp_bm`. For the few records
    where the values of these features are missing, we’ll use 0 in place of the `null`
    value in our sum. We can get a rough feel for the performance of our simple model
    by creating a dataframe of the computed scores and the value of the `is_match`
    column and evaluating how well the score discriminates between matches and nonmatches
    at various thresholds.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用一个简单的评分模型，根据明显好特征的值之和对记录对的相似性进行排名：`cmp_plz`、`cmp_by`、`cmp_bd`、`cmp_lname_c1`
    和 `cmp_bm`。对于这些特征值缺失的少数记录，我们将在我们的汇总中使用 0 替代 `null` 值。我们可以通过创建计算得分和 `is_match`
    列的数据框架来大致了解我们简单模型的性能，并评估该分数在各种阈值下如何区分匹配和非匹配。
- en: Scoring and Model Evaluation
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打分与模型评估
- en: For our scoring function, we are going to sum up the value of five fields (`cmp_lname_c1`,
    `cmp_plz`, `cmp_by`, `cmp_bd`, and `cmp_bm`). We will use `expr` from `pyspark.sql.functions`
    for doing this. The `expr` function parses an input expression string into the
    column that it represents. This string can even involve multiple columns.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的评分函数，我们将汇总五个字段的值（`cmp_lname_c1`、`cmp_plz`、`cmp_by`、`cmp_bd` 和 `cmp_bm`）。我们将使用来自
    `pyspark.sql.functions` 的 `expr` 来执行这些操作。`expr` 函数将输入表达式字符串解析为其表示的列。该字符串甚至可以涉及多个列。
- en: 'Let’s create the required expression string:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建所需的表达式字符串：
- en: '[PRE34]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can now use the `sum_expression` string for calculating the score. When
    summing up the values, we will account for and replace null values with 0 using
    DataFrame’s `fillna` method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 `sum_expression` 字符串来计算分数。在汇总值时，我们将使用 DataFrame 的 `fillna` 方法来处理和替换空值为
    0：
- en: '[PRE35]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The final step in creating our scoring function is to decide what threshold
    the score must exceed in order for us to predict that the two records represent
    a match. If we set the threshold too high, then we will incorrectly mark a matching
    record as a miss (called the *false-negative* rate), whereas if we set the threshold
    too low, we will incorrectly label misses as matches (the *false-positive* rate).
    For any nontrivial problem, we always have to trade some false positives for some
    false negatives, and the question of what the threshold value should be usually
    comes down to the relative cost of the two kinds of errors in the situation to
    which the model is being applied.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们评分函数的最后一步是决定分数必须超过哪个阈值，以便我们预测两条记录代表匹配。如果设置的阈值过高，则会错误地将匹配记录标记为未命中（称为*假阴性*率），而如果设置的阈值过低，则会错误地将未命中标记为匹配（*假阳性*率）。对于任何非平凡问题，我们总是需要在两种错误类型之间进行权衡，并且模型适用于的情况通常取决于这两种错误的相对成本。
- en: 'To help us choose a threshold, it’s helpful to create a *contingency table*
    (which is sometimes called a *cross tabulation*, or *crosstab*) that counts the
    number of records whose scores fall above/below the threshold value crossed with
    the number of records in each of those categories that were/were not matches.
    Since we don’t know what threshold value we’re going to use yet, let’s write a
    function that takes the `scored` DataFrame and the choice of threshold as parameters
    and computes the crosstabs using the DataFrame API:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们选择阈值，创建一个*列联表*（有时称为*交叉制表*或*交叉表*），计算得分高于/低于阈值的记录数，交叉与每个类别中的记录数是否匹配。由于我们还不知道要使用哪个阈值，让我们编写一个函数，它以
    `scored` DataFrame 和阈值选择为参数，并使用 DataFrame API 计算交叉制表：
- en: '[PRE36]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note that we are including the `selectExpr` method of the DataFrame API to dynamically
    determine the value of the field named `above` based on the value of the `t` argument
    using Python’s f-string formatting syntax, which allows us to substitute variables
    by name if we preface the string literal with the letter `f` (yet another handy
    bit of Scala implicit magic). Once the `above` field is defined, we create the
    crosstab with a standard combination of the `groupBy`, `pivot`, and `count` methods
    that we used before.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们包括 DataFrame API 的 `selectExpr` 方法，通过 Python 的 f-string 格式化语法动态确定基于 `t`
    参数的字段 `above` 的值，该语法允许我们按名称替换变量，如果我们用字母 `f` 开头的字符串文字（这是另一个有用的 Scala 隐式魔法）。一旦定义了
    `above` 字段，我们就使用我们之前使用的 `groupBy`、`pivot` 和 `count` 方法的标准组合创建交叉制表。
- en: 'By applying a high threshold value of 4.0, meaning that the average of the
    five features is 0.8, we can filter out almost all of the nonmatches while keeping
    over 90% of the matches:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用高阈值值 4.0，意味着五个特征的平均值为 0.8，我们可以过滤掉几乎所有的非匹配项，同时保留超过 90% 的匹配项：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By applying a lower threshold of 2.0, we can ensure that we capture *all* of
    the known matching records, but at a substantial cost in terms of false positive
    (top-right cell):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用较低的阈值 2.0，我们可以确保捕获*所有*已知的匹配记录，但在假阳性方面要付出相当大的代价（右上角的单元格）：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Even though the number of false positives is higher than we want, this more
    generous filter still removes 90% of the nonmatching records from our consideration
    while including every positive match. Even though this is pretty good, it’s possible
    to do even better; see if you can find a way to use some of the other values from
    `MatchData` (both missing and not) to come up with a scoring function that successfully
    identifies every `true` match at the cost of less than 100 false positives.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管假阳性的数量高于我们的期望，这种更慷慨的过滤器仍然将 90% 的非匹配记录从我们的考虑中移除，同时包括每一个正匹配项。尽管这已经相当不错了，但可能还有更好的方法；看看你能否找到一种利用
    `MatchData` 中的其他值（包括缺失的和不缺失的）来设计一个评分函数，成功识别每一个真正的匹配项，并且假阳性少于 100 个。
- en: Where to Go from Here
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步该怎么办
- en: If this chapter was your first time carrying out data preparation and analysis
    with PySpark, we hope that you got a feel for what a powerful foundation these
    tools provide. If you have been using Python and Spark for a while, we hope that
    you will pass this chapter along to your friends and colleagues as a way of introducing
    them to that power as well.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一章是您第一次使用 PySpark 进行数据准备和分析，希望您能感受到这些工具提供的强大基础。如果您已经使用 Python 和 Spark 一段时间了，希望您将本章介绍给您的朋友和同事，让他们也体验一下这种强大的力量。
- en: Our goal for this chapter was to provide you with enough knowledge to be able
    to understand and complete the rest of the examples in this book. If you are the
    kind of person who learns best through practical examples, your next step is to
    continue on to the next set of chapters, where we will introduce you to MLlib,
    the machine learning library designed for Spark.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是为您提供足够的知识，以便能够理解并完成本书中其余示例的学习。如果你是那种通过实际示例学习最好的人，那么您的下一步是继续学习下一组章节，我们将向您介绍为
    Spark 设计的机器学习库 MLlib。
