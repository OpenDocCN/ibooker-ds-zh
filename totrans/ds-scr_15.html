<html><head></head><body><section data-pdf-bookmark="Chapter 14. Simple Linear Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="simple_linear_regression">&#13;
<h1><span class="label">Chapter 14. </span>Simple Linear Regression</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>Art, like morality, consists in drawing the line somewhere.</p>&#13;
    <p data-type="attribution">G. K. Chesterton</p>&#13;
</blockquote>&#13;
&#13;
<p>In <a data-type="xref" href="ch05.html#statistics">Chapter 5</a>, we<a data-primary="predictive models" data-secondary="simple linear regression" data-type="indexterm" id="PMsimple14"/> used the <code>correlation</code> function to measure the strength of the linear relationship between two variables. For most applications, knowing that such a linear relationship exists isn’t enough.  We’ll want to understand the nature of the relationship. This is where we’ll use simple linear regression.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Model" data-type="sect1"><div class="sect1" id="idm45635739385688">&#13;
<h1>The Model</h1>&#13;
&#13;
<p>Recall<a data-primary="simple linear regression" data-secondary="model for" data-type="indexterm" id="idm45635739383640"/> that we were investigating the relationship between a DataSciencester user’s number of friends and the amount of time the user spends on the site each day.  Let’s assume that you’ve convinced yourself that having more friends <em>causes</em> people to spend more time on the site, rather than one of the alternative explanations we discussed.</p>&#13;
&#13;
<p>The VP of Engagement asks you to build a model describing this relationship.  Since you found a pretty strong linear relationship, a natural place to start is a linear model.</p>&#13;
&#13;
<p>In particular, you hypothesize that there are constants <em>α</em> (alpha) and <em>β</em> (beta) such that:</p>&#13;
<div data-type="equation">&#13;
<math alttext="y Subscript i Baseline equals beta x Subscript i Baseline plus alpha plus epsilon Subscript i" display="block">&#13;
  <mrow>&#13;
    <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>β</mi>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mo>+</mo>&#13;
    <mi>α</mi>&#13;
    <mo>+</mo>&#13;
    <msub><mi>ε</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>where <math>&#13;
  <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
</math> is the number of minutes user <em>i</em> spends on the site daily, <math>&#13;
  <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
</math> is the number of friends user <em>i</em> has, and <em>ε</em> is a (hopefully small) error term representing the fact that there are other factors not accounted for by this simple model.</p>&#13;
&#13;
<p>Assuming we’ve determined such an <code>alpha</code> and <code>beta</code>, then we make predictions simply with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">x_i</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">beta</code> <code class="o">*</code> <code class="n">x_i</code> <code class="o">+</code> <code class="n">alpha</code></pre>&#13;
&#13;
<p>How do we choose <code>alpha</code> and <code>beta</code>?  Well, any choice of <code>alpha</code> and <code>beta</code>&#13;
gives us a predicted output for each input <code>x_i</code>.&#13;
Since we know the actual output <code>y_i</code>, we can compute the error for each pair:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">error</code><code class="p">(</code><code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">x_i</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">y_i</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    The error from predicting beta * x_i + alpha</code>&#13;
<code class="sd">    when the actual value is y_i</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="n">predict</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code> <code class="n">x_i</code><code class="p">)</code> <code class="o">-</code> <code class="n">y_i</code></pre>&#13;
&#13;
<p>What we’d really like to know is the total error&#13;
over the entire dataset.&#13;
But we don’t want to just add the errors—if the prediction for <code>x_1</code> is too high&#13;
and the prediction for <code>x_2</code> is too low, the errors may just cancel out.</p>&#13;
&#13;
<p>So instead we add up the <em>squared</em> errors:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">Vector</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sum_of_sqerrors</code><code class="p">(</code><code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="n">error</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code> <code class="n">x_i</code><code class="p">,</code> <code class="n">y_i</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>&#13;
               <code class="k">for</code> <code class="n">x_i</code><code class="p">,</code> <code class="n">y_i</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">))</code></pre>&#13;
&#13;
<p>The <em>least squares solution</em> is<a data-primary="least squares solution" data-type="indexterm" id="idm45635739147448"/> to&#13;
choose the <code>alpha</code> and <code>beta</code> that make <code>sum_of_sqerrors</code>&#13;
as small as possible.</p>&#13;
&#13;
<p>Using calculus (or tedious algebra), the error-minimizing <code>alpha</code> and <code>beta</code> are given by:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Tuple</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">Vector</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.statistics</code> <code class="kn">import</code> <code class="n">correlation</code><code class="p">,</code> <code class="n">standard_deviation</code><code class="p">,</code> <code class="n">mean</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">least_squares_fit</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">float</code><code class="p">,</code> <code class="nb">float</code><code class="p">]:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Given two vectors x and y,</code>&#13;
<code class="sd">    find the least-squares values of alpha and beta</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="n">beta</code> <code class="o">=</code> <code class="n">correlation</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="o">*</code> <code class="n">standard_deviation</code><code class="p">(</code><code class="n">y</code><code class="p">)</code> <code class="o">/</code> <code class="n">standard_deviation</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
    <code class="n">alpha</code> <code class="o">=</code> <code class="n">mean</code><code class="p">(</code><code class="n">y</code><code class="p">)</code> <code class="o">-</code> <code class="n">beta</code> <code class="o">*</code> <code class="n">mean</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code></pre>&#13;
&#13;
<p>Without going through the exact mathematics, let’s think about why this might be a reasonable solution.  The choice of <code>alpha</code> simply says that when we see the average value of the independent variable <code>x</code>, we predict the average value of the dependent variable <code>y</code>.</p>&#13;
&#13;
<p>The choice of <code>beta</code> means that when the input value increases by <code>standard_deviation(x)</code>, the prediction then increases by <code>correlation(x, y) * standard_deviation(y)</code>.  In the case where <code>x</code> and <code>y</code> are perfectly correlated, a one-standard-deviation increase in <code>x</code> results in a one-standard-deviation-of-<code>y</code> increase in the prediction.  When they’re perfectly anticorrelated, the increase in <code>x</code> results in a <em>decrease</em> in the prediction.  And when the correlation is 0, <code>beta</code> is 0, which means that changes in <code>x</code> don’t affect the prediction at all.</p>&#13;
<!--the quote after +y+ is not rendering right-->&#13;
&#13;
<p>As usual, let’s write a quick test for this:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">x</code> <code class="o">=</code> <code class="p">[</code><code class="n">i</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="o">-</code><code class="mi">100</code><code class="p">,</code> <code class="mi">110</code><code class="p">,</code> <code class="mi">10</code><code class="p">)]</code>&#13;
<code class="n">y</code> <code class="o">=</code> <code class="p">[</code><code class="mi">3</code> <code class="o">*</code> <code class="n">i</code> <code class="o">-</code> <code class="mi">5</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">x</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># Should find that y = 3x - 5</code>&#13;
<code class="k">assert</code> <code class="n">least_squares_fit</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="o">==</code> <code class="p">(</code><code class="o">-</code><code class="mi">5</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre>&#13;
&#13;
<p>Now it’s easy to apply this to the outlierless data from <a data-type="xref" href="ch05.html#statistics">Chapter 5</a>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.statistics</code> <code class="kn">import</code> <code class="n">num_friends_good</code><code class="p">,</code> <code class="n">daily_minutes_good</code>&#13;
&#13;
<code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code> <code class="o">=</code> <code class="n">least_squares_fit</code><code class="p">(</code><code class="n">num_friends_good</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">)</code>&#13;
<code class="k">assert</code> <code class="mf">22.9</code> <code class="o">&lt;</code> <code class="n">alpha</code> <code class="o">&lt;</code> <code class="mf">23.0</code>&#13;
<code class="k">assert</code> <code class="mf">0.9</code> <code class="o">&lt;</code> <code class="n">beta</code> <code class="o">&lt;</code> <code class="mf">0.905</code></pre>&#13;
&#13;
<p>This gives values of <code>alpha</code> = 22.95 and <code>beta</code> = 0.903.  So our model says that we expect a user with <em>n</em> friends to spend 22.95 + <em>n</em> * 0.903 minutes on the site each day.  That is, we predict that a user with no friends on DataSciencester would still spend about 23 minutes a day on the site. And for each additional friend, we expect a user to spend almost a minute more on the site each day.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#simple_linear_regression_image">Figure 14-1</a>, we plot the prediction line&#13;
to get a sense of how well the model fits the observed data.</p>&#13;
&#13;
<figure><div class="figure" id="simple_linear_regression_image">&#13;
<img alt="Simple Linear Regression." src="assets/dsf2_1401.png"/>&#13;
<h6><span class="label">Figure 14-1. </span>Our simple linear model</h6>&#13;
</div></figure>&#13;
&#13;
<p>Of course, we need a better way to&#13;
figure out how well we’ve fit the data than staring at the graph.&#13;
A<a data-primary="coefficient of determination" data-type="indexterm" id="idm45635738956376"/><a data-primary="R-squared" data-type="indexterm" id="idm45635738859944"/> common measure is the <em>coefficient of determination</em> (or <em>R-squared</em>), which measures the fraction of the total variation in the dependent variable that is captured by the model:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.statistics</code> <code class="kn">import</code> <code class="n">de_mean</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">total_sum_of_squares</code><code class="p">(</code><code class="n">y</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""the total squared variation of y_i's from their mean"""</code>&#13;
    <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="n">v</code> <code class="o">**</code> <code class="mi">2</code> <code class="k">for</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">de_mean</code><code class="p">(</code><code class="n">y</code><code class="p">))</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">r_squared</code><code class="p">(</code><code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    the fraction of variation in y captured by the model, which equals</code>&#13;
<code class="sd">    1 - the fraction of variation in y not captured by the model</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="mf">1.0</code> <code class="o">-</code> <code class="p">(</code><code class="n">sum_of_sqerrors</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="o">/</code>&#13;
                  <code class="n">total_sum_of_squares</code><code class="p">(</code><code class="n">y</code><code class="p">))</code>&#13;
&#13;
<code class="n">rsq</code> <code class="o">=</code> <code class="n">r_squared</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code> <code class="n">num_friends_good</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">)</code>&#13;
<code class="k">assert</code> <code class="mf">0.328</code> <code class="o">&lt;</code> <code class="n">rsq</code> <code class="o">&lt;</code> <code class="mf">0.330</code></pre>&#13;
&#13;
<p>Recall that we chose the <code>alpha</code> and <code>beta</code> that minimized the sum of the squared prediction errors. A linear model we could have chosen is “always predict <code>mean(y)</code>” (corresponding to <code>alpha</code> = mean(y) and <code>beta</code> = 0), whose sum of squared errors exactly equals its total sum of squares. This means an R-squared of 0, which indicates a model that (obviously, in this case) performs no better than just predicting the mean.</p>&#13;
<!--again a bad quote after mean(y)+-->&#13;
&#13;
<p>Clearly, the least squares model must be at least as good as that one, which means that the sum of the squared errors is <em>at most</em> the total sum of squares, which means that the R-squared must be at least 0. And the sum of squared errors must be at least 0, which means that the R-squared can be at most 1.</p>&#13;
&#13;
<p>The higher the number, the better our model fits the data. Here we calculate an R-squared of 0.329, which tells us that our model is only sort of okay at fitting the data, and that clearly there are other factors at play.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Gradient Descent" data-type="sect1"><div class="sect1" id="idm45635739384744">&#13;
<h1>Using Gradient Descent</h1>&#13;
&#13;
<p>If<a data-primary="simple linear regression" data-secondary="using gradient descent" data-type="indexterm" id="idm45635738710744"/><a data-primary="gradient descent" data-secondary="simple linear regression using" data-type="indexterm" id="idm45635738709768"/> we write <code>theta = [alpha, beta]</code>, we can also solve this using gradient descent:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">import</code> <code class="nn">tqdm</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.gradient_descent</code> <code class="kn">import</code> <code class="n">gradient_step</code>&#13;
&#13;
<code class="n">num_epochs</code> <code class="o">=</code> <code class="mi">10000</code>&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">guess</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">(),</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()]</code>  <code class="c1"># choose random value to start</code>&#13;
&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.00001</code>&#13;
&#13;
<code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="n">num_epochs</code><code class="p">)</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
        <code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code> <code class="o">=</code> <code class="n">guess</code>&#13;
&#13;
        <code class="c1"># Partial derivative of loss with respect to alpha</code>&#13;
        <code class="n">grad_a</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="n">error</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code> <code class="n">x_i</code><code class="p">,</code> <code class="n">y_i</code><code class="p">)</code>&#13;
                     <code class="k">for</code> <code class="n">x_i</code><code class="p">,</code> <code class="n">y_i</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">num_friends_good</code><code class="p">,</code>&#13;
                                         <code class="n">daily_minutes_good</code><code class="p">))</code>&#13;
&#13;
        <code class="c1"># Partial derivative of loss with respect to beta</code>&#13;
        <code class="n">grad_b</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="mi">2</code> <code class="o">*</code> <code class="n">error</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code> <code class="n">x_i</code><code class="p">,</code> <code class="n">y_i</code><code class="p">)</code> <code class="o">*</code> <code class="n">x_i</code>&#13;
                     <code class="k">for</code> <code class="n">x_i</code><code class="p">,</code> <code class="n">y_i</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">num_friends_good</code><code class="p">,</code>&#13;
                                         <code class="n">daily_minutes_good</code><code class="p">))</code>&#13;
&#13;
        <code class="c1"># Compute loss to stick in the tqdm description</code>&#13;
        <code class="n">loss</code> <code class="o">=</code> <code class="n">sum_of_sqerrors</code><code class="p">(</code><code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code>&#13;
                               <code class="n">num_friends_good</code><code class="p">,</code> <code class="n">daily_minutes_good</code><code class="p">)</code>&#13;
        <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"loss: {loss:.3f}"</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># Finally, update the guess</code>&#13;
        <code class="n">guess</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">guess</code><code class="p">,</code> <code class="p">[</code><code class="n">grad_a</code><code class="p">,</code> <code class="n">grad_b</code><code class="p">],</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># We should get pretty much the same results:</code>&#13;
<code class="n">alpha</code><code class="p">,</code> <code class="n">beta</code> <code class="o">=</code> <code class="n">guess</code>&#13;
<code class="k">assert</code> <code class="mf">22.9</code> <code class="o">&lt;</code> <code class="n">alpha</code> <code class="o">&lt;</code> <code class="mf">23.0</code>&#13;
<code class="k">assert</code> <code class="mf">0.9</code> <code class="o">&lt;</code> <code class="n">beta</code> <code class="o">&lt;</code> <code class="mf">0.905</code></pre>&#13;
&#13;
<p>If you run this you’ll get the same values for <code>alpha</code> and <code>beta</code>&#13;
as we did using the exact formula.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Maximum Likelihood Estimation" data-type="sect1"><div class="sect1" id="maximum_likelihood_estimation">&#13;
<h1>Maximum Likelihood Estimation</h1>&#13;
&#13;
<p>Why<a data-primary="simple linear regression" data-secondary="maximum likelihood estimation" data-type="indexterm" id="idm45635738512568"/><a data-primary="maximum likelihood estimation" data-type="indexterm" id="idm45635738511480"/> choose least squares?  One justification involves <em>maximum likelihood estimation</em>. Imagine that we have a sample of data <math>&#13;
  <mrow>&#13;
    <msub><mi>v</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>v</mi> <mi>n</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
that comes from a distribution that depends on some unknown parameter <em>θ</em> (theta):</p>&#13;
<div data-type="equation">&#13;
<math alttext="p left-parenthesis v 1 comma ellipsis comma v Subscript n Baseline vertical-bar theta right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>p</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>v</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>v</mi> <mi>n</mi> </msub>&#13;
    <mo>|</mo>&#13;
    <mi>θ</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>If we didn’t know <em>θ</em>, we could turn around and think of this quantity as the <em>likelihood</em> of <em>θ</em> given the sample:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper L left-parenthesis theta vertical-bar v 1 comma ellipsis comma v Subscript n Baseline right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>L</mi>&#13;
    <mo>(</mo>&#13;
    <mi>θ</mi>&#13;
    <mo>|</mo>&#13;
    <msub><mi>v</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>v</mi> <mi>n</mi> </msub>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Under this approach, the most likely <em>θ</em> is the value that maximizes this likelihood function—that is, the value that makes the observed data the most probable. In the case of a continuous distribution, in which we have a probability distribution function rather than a probability mass function, we can do the same thing.</p>&#13;
&#13;
<p>Back to regression. One assumption that’s often made about the simple regression model is that the regression errors are normally distributed with mean 0 and some (known) standard deviation <em>σ</em>.  If that’s the case, then the likelihood based on seeing a pair <code>(x_i, y_i)</code> is:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper L left-parenthesis alpha comma beta vertical-bar x Subscript i Baseline comma y Subscript i Baseline comma sigma right-parenthesis equals StartFraction 1 Over StartRoot 2 pi EndRoot sigma EndFraction exp left-parenthesis minus left-parenthesis y Subscript i Baseline minus alpha minus beta x Subscript i Baseline right-parenthesis squared slash 2 sigma squared right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>L</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>α</mi>&#13;
      <mo>,</mo>&#13;
      <mi>β</mi>&#13;
      <mo>|</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <mi>σ</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>1</mn> <mrow><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt><mi>σ</mi></mrow></mfrac>&#13;
    <mo form="prefix">exp</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mo>-</mo>&#13;
      <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><mi>α</mi><mo>-</mo><mi>β</mi><msub><mi>x</mi> <mi>i</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>&#13;
      <mo>/</mo>&#13;
      <mn>2</mn>&#13;
      <msup><mi>σ</mi> <mn>2</mn> </msup>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The likelihood based on the entire dataset is the product of the individual likelihoods, which&#13;
is largest precisely when <code>alpha</code> and <code>beta</code> are chosen to minimize the sum of squared errors. That is, in this case (with these assumptions), minimizing the sum of squared errors is equivalent to maximizing the likelihood of the observed data.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635738703976">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<p>Continue reading about multiple regression in <a data-type="xref" href="ch15.html#multiple_regression">Chapter 15</a>!<a data-primary="" data-startref="PMsimple14" data-type="indexterm" id="idm45635738458504"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>