- en: 'Chapter 21\. Case Study: Detecting Fake News'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 21 章\. 案例研究：检测假新闻
- en: '*Fake news*—false information created in order to deceive others—is an important
    issue because it can harm people. For example, the social media post in [Figure 21-1](#fig-dont-use-sanitizer)
    confidently stated that hand sanitizer doesn’t work on coronaviruses. Though factually
    incorrect, it spread through social media anyway: it was shared nearly 100,000
    times and was likely seen by millions of people.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*假新闻*——为了欺骗他人而创造的虚假信息——是一个重要问题，因为它可能会伤害人们。例如，社交媒体上的帖子在 [图 21-1](#fig-dont-use-sanitizer)
    中自信地宣称手部消毒剂对冠状病毒无效。尽管事实不确，但它还是通过社交媒体传播开来：被分享了近 10 万次，很可能被数百万人看到。'
- en: '![](assets/leds_2101.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_2101.png)'
- en: Figure 21-1\. A popular post on Twitter from March 2020 falsely claimed that
    sanitizer doesn’t kill coronaviruses
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 21-1\. 2020 年 3 月推特上流行的一条帖子错误地声称消毒剂不能杀死冠状病毒。
- en: We might wonder whether we can automatically detect fake news without having
    to read the stories. For this case study, we go through the steps of the data
    science lifecycle. We start by refining our research question and obtaining a
    dataset of news articles and labels. Then we wrangle and transform the data. Next,
    we explore the data to understand its content and devise features to use for modeling.
    Finally, we build models using logistic regression to predict whether news articles
    are real or fake, and evaluate their performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想知道是否可以在不阅读故事的情况下自动检测假新闻。对于这个案例研究，我们遵循数据科学生命周期的步骤。我们首先细化我们的研究问题并获取新闻文章和标签的数据集。然后我们清理和转换数据。接下来，我们探索数据以理解其内容，并设计用于建模的特征。最后，我们使用逻辑回归构建模型，预测新闻文章是否真实或虚假，并评估其性能。
- en: We’ve included this case study because it lets us reiterate several important
    ideas in data science. First, natural language data appear often, and even basic
    techniques can enable useful analyses. Second, model selection is an important
    part of data analysis, and in this case study we apply what we’ve learned about
    cross-validation, the bias-variance trade-off, and regularization. Finally, even
    models that perform well on the test set might have inherent limitations when
    we try to use them in practice, as we will soon see.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包括这个案例研究是因为它让我们重申数据科学中的几个重要概念。首先，自然语言数据经常出现，即使是基本技术也能进行有用的分析。其次，模型选择是数据分析的重要部分，在这个案例研究中，我们应用了交叉验证、偏差-方差权衡和正则化的学习成果。最后，即使在测试集上表现良好的模型，在实际应用中也可能存在固有限制，我们很快就会看到。
- en: Let’s start by refining our research question and understanding the scope of
    our data.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先细化我们的研究问题，并理解我们数据的范围。
- en: Question and Scope
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题和范围
- en: 'Our initial research question is: can we automatically detect fake news? To
    refine this question, we consider the kind of information that we might use to
    build a model for detecting fake news. If we have hand-classified news stories
    where people have read each story and determined whether it is fake or not, then
    our question becomes: can we build a model to accurately predict whether a news
    story is fake based on its content?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初的研究问题是：我们能自动检测假新闻吗？为了细化这个问题，我们考虑了用于建立检测假新闻模型的信息类型。如果我们手动分类了新闻故事，人们已阅读每个故事并确定其真假，那么我们的问题变成了：我们能否建立一个模型来准确预测新闻故事是否为假的，基于其内容？
- en: To address this question, we can use the FakeNewsNet data repository as described
    in [Shu et al](https://arxiv.org/abs/1809.01286). This repository contains content
    from news and social media websites, as well as metadata like user engagement
    metrics. For simplicity, we only look at the dataset’s political news articles.
    This subset of the data includes only articles that were fact-checked by [Politifact](https://www.politifact.com),
    a nonpartisan organization with a good reputation. Each article in the dataset
    has a “real” or “fake” label based on Politifact’s evaluation, which we use as
    the ground truth.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以使用 FakeNewsNet 数据库，如 [Shu et al](https://arxiv.org/abs/1809.01286)
    所述。该数据库包含来自新闻和社交媒体网站的内容，以及用户参与度等元数据。为简单起见，我们只查看数据集的政治新闻文章。该数据子集仅包括由 [Politifact](https://www.politifact.com)
    进行事实检查的文章，Politifact 是一个声誉良好的非党派组织。数据集中的每篇文章都有基于 Politifact 评估的“真实”或“虚假”标签，我们将其用作基准真实性。
- en: Politifact uses a nonrandom sampling method to select articles to fact-check.
    According to its website, Politifact’s journalists select the “most newsworthy
    and significant” claims each day. Politifact started in 2007 and the repository
    was published in 2020, so most of the articles were published between 2007 and
    2020.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Politifact使用非随机抽样方法选择文章进行事实核查。根据其网站，Politifact的记者每天选择“最有新闻价值和重要性”的主张。Politifact始于2007年，存储库发布于2020年，因此大多数文章发布于2007年到2020年之间。
- en: 'Summarizing this information, we determine that the target population consists
    of all political news stories published online in the time period from 2007 to
    2020 (we would also want to list the sources of the stories). The access frame
    is determined by Politifact’s identification of the most newsworthy claims of
    the day. So the main sources of bias for this data include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这些信息，我们确定目标人群包括所有在线发布的政治新闻故事，时间跨度从2007年到2020年（我们也想列出这些故事的来源）。访问框架由Politifact确定，标识出当天最有新闻价值的主张。因此，这些数据的主要偏见来源包括：
- en: Coverage bias
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖偏见
- en: The news outlets are limited to those that Politifact monitored, which may miss
    arcane or short-lived sites.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻媒体仅限于Politifact监控的那些，这可能会忽略奥秘或短暂存在的网站。
- en: Selection bias
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 选择偏见
- en: The data are limited to articles Politifact decided were interesting enough
    to fact-check, which means that articles might skew toward ones that are both
    widely shared and controversial.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仅限于Politifact认为足够有趣以进行事实核查的文章，这意味着文章可能偏向于广泛分享和具有争议性的文章。
- en: Measurement bias
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 测量偏见
- en: Whether a story should be labeled “fake” or “real” is determined by one organization
    (Politifact) and reflects the biases, unintentional or otherwise, that the organization
    has in its fact-checking methodology.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 故事是否应标记为“假”或“真”由一个组织（Politifact）决定，并反映了该组织在其事实核查方法中存在的偏见，无论是有意还是无意。
- en: Drift
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 漂移
- en: Since we only have articles published between 2007 and 2020, there is likely
    to be drift in the content. Topics are popularized and faked in rapidly evolving
    news trends.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只有2007年到2020年间发布的文章，内容可能会有些漂移。话题在快速发展的新闻趋势中被推广和伪造。
- en: We will keep these limitations of the data in mind as we begin to wrangle the
    data into a form that we can analyze.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开始整理数据之前，会牢记这些数据的限制，以便将其整理成可分析的形式。
- en: Obtaining and Wrangling the Data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取和整理数据
- en: 'Let’s get the data into Python using the [GitHub page for FakeNewsNet](https://oreil.ly/0DOHd).
    Reading over the repository description and code, we find that the repository
    doesn’t actually store the news articles itself. Instead, running the repository
    code will scrape news articles from online web pages directly (using techniques
    we covered in [Chapter 14](ch14.html#ch-web)). This presents a challenge: if an
    article is no longer available online, it likely will be missing from our dataset.
    Noting this, let’s proceed with downloading the data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用[FakeNewsNet的GitHub页面](https://oreil.ly/0DOHd)将数据导入Python。阅读存储库描述和代码后，我们发现该存储库实际上并不存储新闻文章本身。相反，运行存储库代码将直接从在线网页上抓取新闻文章（使用我们在[第14章](ch14.html#ch-web)中介绍的技术）。这带来了一个挑战：如果一篇文章不再在网上可用，那么它很可能会在我们的数据集中丢失。注意到这一点后，让我们继续下载数据。
- en: Note
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The FakeNewsNet code highlights one challenge in reproducible research—online
    datasets change over time, but it can be difficult (or even illegal) to store
    and share copies of this data. For example, other parts of the FakeNewsNet dataset
    use Twitter posts, but the dataset creators would violate Twitter’s terms and
    services if they stored copies of the posts in their repository. When working
    with data gathered from the web, we suggest documenting the date the data were
    gathered and reading the terms and services of the data sources carefully.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: FakeNewsNet代码突显了可重复研究中的一个挑战——在线数据集随时间变化，但如果在存储库中存储和共享这些数据可能会面临困难（甚至违法）。例如，FakeNewsNet数据集的其他部分使用Twitter帖子，但如果创建者在其存储库中存储帖子副本则会违反Twitter的条款和服务。在处理从网络收集的数据时，建议记录数据收集日期并仔细阅读数据来源的条款和服务。
- en: 'Running the script to download the Politifact data takes about an hour. After
    that, we place the datafiles into the *data/politifact* folder. The articles that
    Politifact labeled as fake and real are in *data/politifact/fake* and *data/politifact/real*.
    Let’s take a look at one of the articles labeled “real”:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本下载Politifact数据大约需要一个小时。之后，我们将数据文件放入*data/politifact*文件夹中。Politifact标记为假和真的文章分别位于*data/politifact/fake*和*data/politifact/real*文件夹中。让我们看一看其中一个标记为“真实”的文章：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each article’s data is stored in a JSON file named *news content.json*. Let’s
    load the JSON for one article into a Python dictionary (see [Chapter 14](ch14.html#ch-web)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每篇文章的数据存储在名为 *news content.json* 的 JSON 文件中。让我们将一篇文章的 JSON 加载到 Python 字典中（参见
    [第 14 章](ch14.html#ch-web)）：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, we’ve displayed the keys and values in `article_json` as a table:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将 `article_json` 中的键和值显示为表格：
- en: '|   | value |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|   | value |'
- en: '| --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| key |   |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| key |   |'
- en: '| --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **url** | http://www.senate.gov/legislative/LIS/roll_cal... |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **url** | http://www.senate.gov/legislative/LIS/roll_cal... |'
- en: '| **text** | Roll Call Vote 111th Congress - 1st Session\n\... |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **text** | Roll Call Vote 111th Congress - 1st Session\n\... |'
- en: '| **images** | [http://statse.webtrendslive.com/dcs222dj3ow9j... |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **images** | [http://statse.webtrendslive.com/dcs222dj3ow9j... |'
- en: '| **top_img** | http://www.senate.gov/resources/images/us_sen.ico |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **top_img** | http://www.senate.gov/resources/images/us_sen.ico |'
- en: '| **keywords** | [] |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **keywords** | [] |'
- en: '| **authors** | [] |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **authors** | [] |'
- en: '| **canonical_link** |   |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **canonical_link** |   |'
- en: '| **title** | U.S. Senate: U.S. Senate Roll Call Votes 111th... |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **title** | U.S. Senate: U.S. Senate Roll Call Votes 111th... |'
- en: '| **meta_data** | {''viewport’: ‘width=device-width, initial-scal... |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **meta_data** | {''viewport’: ‘width=device-width, initial-scal... |'
- en: '| **movies** | [] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **movies** | [] |'
- en: '| **publish_date** | None |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **publish_date** | None |'
- en: '| **source** | http://www.senate.gov |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **source** | http://www.senate.gov |'
- en: '| **summary** |   |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **summary** |   |'
- en: 'There are many fields in the JSON file, but for this analysis we only look
    at a few that are primarily related to the content of the article: the article’s
    title, text content, URL, and publication date. We create a dataframe where each
    row represents one article (the granularity in a news story). To do this, we load
    in each available JSON file as a Python dictionary, and then extract the fields
    of interest to store as a `pandas` `DataFrame` named `df_raw`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 文件中有很多字段，但是对于这个分析，我们只关注几个与文章内容主要相关的字段：文章的标题、文本内容、URL 和发布日期。我们创建一个数据框，其中每一行代表一篇文章（新闻报道的粒度）。为此，我们将每个可用的
    JSON 文件加载为 Python 字典，然后提取感兴趣的字段以存储为 `pandas` 的 `DataFrame`，命名为 `df_raw`：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|   | url | text | title | publish_date | label |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|   | url | text | title | publish_date | label |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | dailybuzzlive.com/cannibals-arrested-florida/ | Police in Vernal Heights,
    Florida, arrested 3-... | Cannibals Arrested in Florida Claim Eating Hum... |
    1.62e+09 | fake |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 0 | dailybuzzlive.com/cannibals-arrested-florida/ | Police in Vernal Heights,
    Florida, arrested 3-... | Cannibals Arrested in Florida Claim Eating Hum... |
    1.62e+09 | fake |'
- en: '| 1 | https://web.archive.org/web/20171228192703/htt... | WASHINGTON — Rod
    Jay Rosenstein, Deputy Attorn... | BREAKING: Trump fires Deputy Attorney General
    ... | 1.45e+09 | fake |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 1 | https://web.archive.org/web/20171228192703/htt... | WASHINGTON — Rod
    Jay Rosenstein, Deputy Attorn... | BREAKING: Trump fires Deputy Attorney General
    ... | 1.45e+09 | fake |'
- en: 'Exploring this dataframe reveals some issues we’d like to address before we
    begin the analysis. For example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 探索这个数据框会揭示一些在开始分析之前我们想要解决的问题。例如：
- en: Some articles couldn’t be downloaded. When this happened, the `url` column contains
    `NaN`.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些文章无法下载。当出现这种情况时，`url` 列包含 `NaN`。
- en: Some articles don’t have text (such as a web page with only video content).
    We drop these articles from our dataframe.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些文章没有文本（例如只有视频内容的网页）。我们从数据框中删除这些文章。
- en: The `publish_date` column stores timestamps in Unix format (seconds since the
    Unix epoch), so we need to convert them to `pandas.Timestamp` objects.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`publish_date` 列以 Unix 格式（自 Unix 纪元以来的秒数）存储时间戳，因此我们需要将它们转换为 `pandas.Timestamp`
    对象。'
- en: We are interested in the base URL of a web page. However, the `source` field
    in the JSON file has many missing values compared to the `url` column, so we must
    extract the base URL using the full URL in the `url` column. For example, from
    *dailybuzzlive.com/cannibals-arrested-florida/* we get *dailybuzzlive.com*.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对网页的基本 URL 感兴趣。然而，JSON 文件中的 `source` 字段与 `url` 列相比有许多缺失值，所以我们必须使用 `url` 列中的完整
    URL 提取基本 URL。例如，从 *dailybuzzlive.com/cannibals-arrested-florida/* 我们得到 *dailybuzzlive.com*。
- en: Some articles were downloaded from an archival website (`web.archive.org`).
    When this happens, we want to extract the actual base URL from the original by
    removing the `web.archive.org` prefix.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些文章是从存档网站（`web.archive.org`）下载的。当这种情况发生时，我们希望从原始的 URL 中提取实际的基本 URL，通过移除 `web.archive.org`
    前缀。
- en: We want to concatenate the `title` and `text` columns into a single `content`
    column that contains all of the text content of the article.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望将 `title` 和 `text` 列连接成一个名为 `content` 的单一列，其中包含文章的所有文本内容。
- en: 'We can tackle these data issues using a combination of `pandas` functions and
    regular expressions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `pandas` 函数和正则表达式来解决这些数据问题：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After data wrangling, we end up with the following dataframe named `df`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理后，我们得到名为`df`的以下数据框架：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|   | timestamp | baseurl | content | label |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|   | timestamp | baseurl | content | label |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **0** | 2021-04-05 16:39:51 | dailybuzzlive.com | Cannibals Arrested in Florida
    Claim Eating Hum... | fake |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 2021-04-05 16:39:51 | dailybuzzlive.com | 佛罗里达州被捕的食人族声称吃... | 假 |'
- en: '| **1** | 2016-01-01 23:17:43 | houstonchronicle-tv.com | BREAKING: Trump fires
    Deputy Attorney General ... | fake |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 2016-01-01 23:17:43 | houstonchronicle-tv.com | 突发新闻：特朗普解雇副检察...
    | 假 |'
- en: Now that we’ve loaded and cleaned the data, we can proceed to exploratory data
    analysis.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已加载并清理了数据，可以进行探索性数据分析。
- en: Exploring the Data
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: The dataset of news articles we’re exploring is just one part of the larger
    FakeNewsNet dataset. As such, the original paper doesn’t provide detailed information
    about our subset of data. So, to better understand the data, we must explore it
    ourselves.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在探索的新闻文章数据集只是更大的FakeNewsNet数据集的一部分。因此，原始论文并未提供有关我们数据子集的详细信息。因此，为了更好地理解数据，我们必须自己进行探索。
- en: 'Before starting exploratory data analysis, we apply our standard practice of
    splitting the data into training and test sets. We perform EDA using only the
    train set:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始探索性数据分析之前，我们遵循标准做法，将数据分割为训练集和测试集。我们只使用训练集进行EDA：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|   | timestamp | baseurl | content |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|   | timestamp | baseurl | content |'
- en: '| --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **164** | 2019-01-04 19:25:46 | worldnewsdailyreport.com | Chinese lunar
    rover finds no evidence of Ameri... |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **164** | 2019-01-04 19:25:46 | worldnewsdailyreport.com | 中国月球车未发现美国...
    |'
- en: '| **28** | 2016-01-12 21:02:28 | occupydemocrats.com | Virginia Republican
    Wants Schools To Check Chi... |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **28** | 2016-01-12 21:02:28 | occupydemocrats.com | 弗吉尼亚州共和党人要求学校检查... |'
- en: 'Let’s count the number of real and fake articles in the train set:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们统计训练集中真假文章的数量：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Our train set has 584 articles, and there are about 60 more articles labeled
    `real` than `fake`. Next, we check for missing values in the three fields:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练集有584篇文章，实际文章比虚假文章多约60篇。接下来，我们检查这三个字段中是否存在缺失值：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Nearly half of the timestamps are null. This feature will limit the dataset
    if we use it in the analysis. Let’s take a closer look at the `baseurl`, which
    represents the website that published the original article.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳几乎一半为空。如果我们在分析中使用它，这个特征将限制数据集。让我们仔细查看`baseurl`，它表示发布原始文章的网站。
- en: Exploring the Publishers
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索出版商
- en: 'To understand the `baseurl` column, we start by counting the number of articles
    from each website:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解`baseurl`列，我们首先统计每个网站的文章数：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our train set has 584 rows, and we have found that there are 337 unique publishing
    websites. This means that the dataset includes many publications with only a few
    articles. A histogram of the number of articles published by each website confirms
    this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练集有584行，我们发现有337个独特的发布网站。这意味着数据集包含许多只有少数文章的出版物。每个网站发布的文章数量的直方图证实了这一点：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](assets/leds_21in01.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in01.png)'
- en: 'This histogram shows that the vast majority (261 out of 337) of websites have
    only one article in the train set, and only a few websites have more than five
    articles in the train set. Nonetheless, it can be informative to identify the
    websites that published the most fake or real articles. First, we find the websites
    that published the most fake articles:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此直方图显示，绝大多数网站（337中的261个）在训练集中只有一篇文章，只有少数网站在训练集中有超过五篇文章。尽管如此，识别发布最多假或真文章的网站可能具有信息量。首先，我们找出发布最多假文章的网站：
- en: '![](assets/leds_21in02.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in02.png)'
- en: 'Next, we list the websites that published the greatest number of real articles:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们列出发布最多真实文章的网站：
- en: '![](assets/leds_21in03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in03.png)'
- en: Only `cnn.com` and `washingtonpost.com` appear on both lists. Even without knowing
    the total number of articles for these sites, we might expect that an article
    from `yournewswire.com` is more likely to be labeled as `fake`, while an article
    from `whitehouse.gov` is more likely to be labeled as `real`. That said, we don’t
    expect that using the publishing website to predict article truthfulness would
    work very well; there are simply too few articles from most of the websites in
    the dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`cnn.com` 和 `washingtonpost.com` 出现在两个列表中。即使我们不知道这些网站的文章总数，我们可能会预期来自 `yournewswire.com`
    的文章更有可能被标记为“假”，而来自 `whitehouse.gov` 的文章更有可能被标记为“真”。尽管如此，我们并不指望使用发布网站来预测文章的真实性会非常有效；数据集中大多数网站的文章数量实在太少。'
- en: Next, let’s explore the `timestamp` column, which records the publication date
    of the news articles.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索`timestamp`列，记录新闻文章的发布日期。
- en: Exploring Publication Date
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索发布日期
- en: 'Plotting the timestamps on a histogram shows that most articles were published
    after 2000, although there seems to be at least one article published before 1940:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将时间戳绘制在直方图上显示，大多数文章是在 2000 年之后发布的，尽管至少有一篇文章是在 1940 年之前发布的：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](assets/leds_21in04.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in04.png)'
- en: 'When we take a closer look at the news articles published prior to 2000, we
    find that the timestamps don’t match the actual publication date of the article.
    These date issues are most likely related to the web scraper collecting inaccurate
    information from the web pages. We can zoom into the region of the histogram after
    2000:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们更仔细地查看发布于 2000 年之前的新闻文章时，我们发现时间戳与文章的实际发布日期不符。这些日期问题很可能与网络爬虫从网页中收集到的不准确信息有关。我们可以放大直方图中
    2000 年之后的区域：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](assets/leds_21in05.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in05.png)'
- en: As expected, most of the articles were published between 2007 (the year Politifact
    was founded) and 2020 (the year the FakeNewsNet repository was published). But
    we also find that the timestamps are concentrated on the years 2016 to 2018—the
    year of the controversial 2016 US presidential election and the two years following.
    This insight is a further caution on the limitation of our analysis to carry over
    to nonelection years.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，大多数文章是在 2007 年（Politifact 成立的年份）至 2020 年（FakeNewsNet 仓库发布的年份）之间发布的。但我们还发现，时间戳主要集中在
    2016 年至 2018 年之间——这是有争议的 2016 年美国总统选举年及其后两年。这一发现进一步提示我们的分析局限性可能不适用于非选举年。
- en: Our main aim is to use the text content for classification. We explore some
    word frequencies next.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标是使用文本内容进行分类。接下来我们探索一些词频。
- en: Exploring Words in Articles
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索文章中的单词
- en: 'We’d like to see whether there’s a relationship between the words used in the
    articles and whether the article was labeled as `fake`. One simple way to do this
    is to look at individual words like *military*, then count how many articles that
    mentioned “military” were labeled `fake`. For *military* to be useful, the articles
    that mention it should have a much higher or much lower fraction of fake articles
    than 45% (the proportion of fake articles in the dataset: 264/584).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想看看文章中使用的单词与文章是否被标记为“假”之间是否存在关系。一个简单的方法是查看像 *军事* 这样的单词，然后统计提到“军事”的文章中有多少被标记为“假”。对于
    *军事* 来说，文章提到它的比例应该远高于或远低于数据集中的假文章比例 45%（数据集中假文章的比例：264/584）。
- en: 'We can use our domain knowledge of political topics to pick out a few candidate
    words to explore:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用我们对政治话题的领域知识来挑选一些候选单词进行探索：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we define a function that creates a new feature for each word, where the
    feature contains `True` if the word appeared in the article and `False` if not:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个函数，为每个单词创建一个新特征，如果文章中出现该单词，则特征为`True`，否则为`False`：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This is like one-hot encoding for the presence of a word (see [Chapter 15](ch15.html#ch-linear)).
    We can use this function to further wrangle our data and create a new dataframe
    with a feature for each of our chosen words:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是单词存在的一种独热编码（参见[第 15 章](ch15.html#ch-linear)）。我们可以使用这个函数进一步处理我们的数据，并创建一个包含每个选择的单词特征的新数据框架：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|   | trump | clinton | state | vote | ... | swamp | cnn | the | label |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|   | trump | clinton | state | vote | ... | swamp | cnn | the | label |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **164** | False | False | True | False | ... | False | False | True | 1 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **164** | False | False | True | False | ... | False | False | True | 1 |'
- en: '| **28** | False | False | False | False | ... | False | False | True | 1 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **28** | False | False | False | False | ... | False | False | True | 1 |'
- en: '| **708** | False | False | True | True | ... | False | False | True | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **708** | False | False | True | True | ... | False | False | True | 0 |'
- en: '| **193** | False | False | False | False | ... | False | False | True | 1
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **193** | False | False | False | False | ... | False | False | True | 1
    |'
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we can find the proportion of these articles that were labeled `fake`.
    We visualize these calculations in the following plots. In the left plot, we mark
    the proportion of `fake` articles in the entire train set using a dotted line,
    which helps us understand how informative each word feature is—a highly informative
    word will have a point that lies far away from the line:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以找出这些文章中被标记为`fake`的比例。我们在以下图表中可视化了这些计算结果。在左图中，我们用虚线标记了整个训练集中`fake`文章的比例，这有助于我们理解每个单词特征的信息量—一个高信息量的单词将使得其点远离该线：
- en: '![](assets/leds_21in06.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in06.png)'
- en: This plot reveals a few interesting considerations for modeling. For example,
    notice that the word *antifa* is highly predictive—all articles that mention the
    word *antifa* are labeled `fake`. However, *antifa* only appears in a few articles.
    On the other hand, the word *the* appears in nearly every article, but is uninformative
    for distinguishing between `real` and `fake` articles because the proportion of
    articles with *the* that are fake matches the proportion of fake articles overall.
    We might instead do better with a word like *vote*, which is predictive and appears
    in many news articles.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表揭示了建模的一些有趣的考虑因素。例如，注意到单词*antifa*具有很高的预测性—所有提到单词*antifa*的文章都标记为`fake`。然而，*antifa*只出现在少数文章中。另一方面，单词*the*几乎出现在每篇文章中，但对于区分`real`和`fake`文章没有信息量，因为含有*the*的文章的比例与总体fake文章的比例相匹配。我们可能会更喜欢像*vote*这样的单词，它既有预测能力又出现在许多新闻文章中。
- en: This exploratory analysis brought us understanding of the time frame that our
    news articles were published in, the broad range of publishing websites captured
    in the data, and candidate words to use for prediction. Next, we fit models for
    predicting whether articles are fake or real.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个探索性分析使我们了解了我们的新闻文章发表的时间框架，数据中涵盖的广泛发布网站以及用于预测的候选词。接下来，我们为预测文章是真实还是虚假拟合模型。
- en: Modeling
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模
- en: Now that we’ve obtained, cleaned, and explored our data, let’s fit models to
    predict whether articles are real or fake. In this section, we use logistic regression
    because we have a binary classification problem. We fit three different models
    that increase in complexity. First, we fit a model that just uses the presence
    of a single handpicked word in the document as an explanatory feature. Then we
    fit a model that uses multiple handpicked words. Finally, we fit a model that
    uses all the words in the train set, vectorized using the tf-idf transform (introduced
    in [Chapter 13](ch13.html#ch-text)). Let’s start with the simple single-word model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得、清洗并探索了我们的数据，让我们拟合模型来预测文章是真实还是虚假。在本节中，我们使用逻辑回归因为我们面临二元分类问题。我们拟合了三种不同复杂度的模型。首先，我们拟合了一个仅使用单个手动选择的单词作为解释特征的模型。然后我们拟合了一个使用多个手动选择的单词的模型。最后，我们拟合了一个使用所有在训练集中的单词，并使用
    tf-idf 转换向量化的模型（介绍见[第13章](ch13.html#ch-text)）。让我们从简单的单词模型开始。
- en: A Single-Word Model
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词模型
- en: 'Our EDA showed that the word *vote* is related to whether an article is labeled
    `real` or `fake`. To test this, we fit a logistic regression model using a single
    binary feature: `1` if the word *vote* appears in the article and `0` if not.
    We start by defining a function to lowercase the article content:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索性数据分析表明，单词*vote*与文章被标记为`real`或`fake`相关。为了验证这一点，我们使用一个二元特征拟合了逻辑回归模型：如果文章中出现单词*vote*则为`1`，否则为`0`。我们首先定义一个函数将文章内容转为小写：
- en: '[PRE27]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For our first classifier, we only use the word *vote*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个分类器，我们只使用单词*vote*：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can chain the `lowercase` function and the function `make_word_features`
    from our EDA into a `scikit-learn` pipeline. This provides a convenient way to
    transform and fit data all at once:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`lowercase`函数和来自我们的探索性数据分析的`make_word_features`函数链在一起成为一个`scikit-learn`管道。这提供了一种方便的方式来一次性地转换和拟合数据：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When used, the preceding pipeline converts the characters in the article content
    to lowercase, creates a dataframe with a binary feature for each word of interest,
    and fits a logistic regression model on the data using <math><msub><mi>L</mi>
    <mn>2</mn></msub></math> regularization. Additionally, the `LogisticRegressionCV`
    function uses cross-validation (fivefold by default) to select the best regularization
    parameter. (See [Chapter 16](ch16.html#ch-risk) for more on regularization and
    cross-validation.)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用时，前面的流水线将文章内容中的字符转换为小写，为每个感兴趣的单词创建一个二元特征的数据框，并使用<math><msub><mi>L</mi> <mn>2</mn></msub></math>正则化在数据上拟合逻辑回归模型。另外，`LogisticRegressionCV`函数使用交叉验证（默认为五折）来选择最佳的正则化参数。（有关正则化和交叉验证的更多信息，请参见[第
    16 章](ch16.html#ch-risk)。）
- en: 'Let’s use the pipeline to fit the training data:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用管道来拟合训练数据：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Overall, the single-word classifier only classifies 65% of articles correctly.
    We plot the confusion matrix of the classifier on the train set to see what kinds
    of mistakes it makes:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，单词分类器只能正确分类 65% 的文章。我们在训练集上绘制分类器的混淆矩阵，以查看它所犯的错误类型：
- en: '![](assets/leds_21in07.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in07.png)'
- en: 'Our model often misclassifies real articles (0) as fake (1). Since this model
    is simple, we can take a look at the probabilities for the two cases: the word
    *vote* is in the article or is not:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型经常将真实文章（0）错误地分类为虚假（1）。由于这个模型很简单，我们可以看一下这两种情况的概率：文章中是否包含*vote*这个词：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'When an article contains the word *vote*, the model gives a high probability
    of the article being real, and when *vote* is absent, the probability leans slightly
    toward the article being fake. We encourage readers to verify this for themselves
    using the definition of the logistic regression model and the fitted coefficients:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当文章包含*vote*这个词时，模型会给出文章为真实的高概率，而当*vote*缺失时，概率略微倾向于文章为虚假。我们鼓励读者使用逻辑回归模型的定义和拟合系数来验证这一点：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As we saw in [Chapter 19](ch19.html#ch-logistic), the coefficient indicates
    the size of the change in the odds with a change in the explanatory variable.
    With a 0-1 variable like the presence or absence of a word in an article, this
    has a particularly intuitive meaning. For an article with *vote* in it, the odds
    of being fake decrease by a factor of <math><mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mrow><mi>v</mi> <mi>o</mi> <mi>t</mi> <mi>e</mi></mrow></msub>
    <mo stretchy="false">)</mo></math> , which is:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第 19 章](ch19.html#ch-logistic)中看到的那样，系数表示随着解释变量的变化而发生的几率变化的大小。对于像文章中的一个
    0-1 变量这样的变量，这有着特别直观的含义。对于一个包含*vote*的文章，其为虚假的几率会减少一个因子<math><mi>exp</mi> <mo>⁡</mo>
    <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mrow><mi>v</mi> <mi>o</mi> <mi>t</mi>
    <mi>e</mi></mrow></msub> <mo stretchy="false">)</mo></math>，即：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that in this modeling scenario, a label of `0` corresponds to a real
    article and a label of `1` corresponds to a fake article. This might seem a bit
    counterintuitive—we’re saying that a “true positive” is when a model correctly
    predicts a fake article as fake. In binary classification, we typically say a
    “positive” result is the one with the presence of something unusual. For example,
    a person who tests positive for an illness would expect to have the illness.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这种建模场景中，标签`0`表示真实文章，标签`1`表示虚假文章。这可能看起来有点反直觉—我们说“真正的正例”是当模型正确预测一篇虚假文章为虚假时。在二元分类中，我们通常说“正面”的结果是指存在某种不寻常情况的结果。例如，测试结果为阳性的人可能会有这种疾病。
- en: Let’s make our model a bit more sophisticated by introducing additional word
    features.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过引入额外的单词特征来使我们的模型变得更加复杂一些。
- en: Multiple-Word Model
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多单词模型
- en: 'We create a model that uses all of the words we examined in our EDA of the
    train set, except for *the*. Let’s fit a model using these 15 features:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个模型，该模型使用了我们在训练集的探索性数据分析（EDA）中检查过的所有单词，除了*the*。让我们使用这 15 个特征来拟合一个模型：
- en: '[PRE38]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This model is about 10 percentage points more accurate than the one-word model.
    It may seem a bit surprising that going from a one-word model to a 15-word model
    only gains 10 percentage points. The confusion matrix is helpful in teasing out
    the kinds of errors made:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型比单词模型准确率高约 10 个百分点。从一个单词模型转换为一个 15 个单词模型仅获得 10 个百分点可能会有点令人惊讶。混淆矩阵有助于揭示出所犯错误的类型：
- en: '![](assets/leds_21in08.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in08.png)'
- en: 'We can see that this classifier does a better job of classifying real articles
    accurately. However, it makes more mistakes than the simple one-word model when
    classifying fake article—59 of the fake articles were classified as real. In this
    scenario, we might be more concerned about misclassifying an article as fake when
    it is real. So we wish to have a high precision—the ratio of fake articles correctly
    predicted as fake to articles predicted as fake:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这个分类器在准确分类真实文章方面做得更好。然而，在将虚假文章分类时，它会犯更多错误——有 59 篇虚假文章被错误分类为真实文章。在这种情况下，我们可能更关注将一篇文章误分类为虚假而实际上它是真实的。因此，我们希望有很高的精确度——正确预测为虚假文章的虚假文章比例：
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The precision in our larger model is improved, but about 30% of the articles
    labeled as fake are actually real. Let’s take a look at the model’s coefficients:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更大的模型中的精确度有所提高，但约有 30% 的被标记为虚假的文章实际上是真实的。让我们来看一下模型的系数：
- en: '![](assets/leds_21in09.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in09.png)'
- en: We can make a quick interpretation of the coefficients by looking at their signs.
    The large positive values on *trump* and *investig* indicate that the model predicts
    that new articles containing these words have a higher probability of being fake.
    The reverse is true for words like *congress* and *vote*, which have negative
    weights. We can use these coefficients to compare the log odds when an article
    does or does not contain a particular word.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察它们的符号，我们可以快速解释系数。*trump* 和 *investig* 上的大正值表明模型预测包含这些词的新文章更有可能是虚假的。对于像 *congress*
    和 *vote* 这样具有负权重的词来说情况相反。我们可以使用这些系数来比较文章是否包含特定词时的对数几率。
- en: Although this larger model performs better than the simple one-word model, we
    had to handpick the word features using our knowledge of the news. What if we
    missed the words that are highly predictive? To address this, we can incorporate
    all the words in the articles using the tf-idf transform.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个更大的模型的表现比简单的单词模型更好，但我们不得不使用我们对新闻的知识手动挑选单词特征。如果我们漏掉了高度预测性的词怎么办？为了解决这个问题，我们可以使用
    tf-idf 转换将所有文章中的所有单词合并起来。
- en: Predicting with the tf-idf Transform
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 tf-idf 转换进行预测
- en: 'For the third and final model, we use the term frequency-inverse document frequency
    (tf-idf) transform from [Chapter 13](ch13.html#ch-text) to vectorize the entire
    text of all articles in the train set. Recall that with this transform, an article
    is converted into a vector with one element for each word that appears in any
    of the 564 articles. The vector consists of normalized counts of the number of
    times the word appears in the article normalized by the rareness of the word.
    The tf-idf puts more weight on words that only appear in a few documents. This
    means that our classifier uses all the words in the train set’s news articles
    for prediction. As we’ve done previously when we introduced tf-idf, first we remove
    stopwords, then we tokenize the words, and then we use the `TfidfVectorizer` from
    `scikit-learn`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第三个也是最后一个模型，我们使用了第 13 章中的 term frequency-inverse document frequency (tf-idf)
    转换来向量化训练集中所有文章的整个文本。回想一下，使用此转换，一篇文章被转换为一个向量，其中每个词的出现次数在任何 564 篇文章中都会出现。该向量由词在文章中出现的次数的归一化计数组成，除以该词的稀有度。tf-idf
    对仅出现在少数文档中的词赋予更高的权重。这意味着我们的分类器用于预测的是训练集新闻文章中的所有单词。正如我们之前介绍 tf-idf 时所做的那样，首先我们移除停用词，然后对单词进行标记化，最后我们使用
    `scikit-learn` 中的 `TfidfVectorizer`：
- en: '[PRE43]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We find that this model achieves 100% accuracy on the train set. We can take
    a look at the tf-idf transformer to better understand the model. Let’s start by
    finding out how many unique tokens the classifier uses:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现这个模型在训练集上实现了 100% 的准确率。我们可以查看 tf-idf 转换器来更好地理解模型。让我们首先找出分类器使用的唯一标记的数量：
- en: '[PRE47]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This means that our classifier has 23,812 features, a large increase from our
    previous model, which only had 15\. Since we can’t display that many model weights,
    we display the 10 most negative and 10 most positive weights:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的分类器有 23,812 个特征，比我们之前的模型大幅增加，之前的模型只有 15 个。由于我们无法显示那么多模型权重，我们显示了 10 个最负和
    10 个最正的权重：
- en: '![](assets/leds_21in10.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_21in10.png)'
- en: These coefficients show a few quirks about this model. We see that several influential
    features correspond to punctuation in the original text. It’s unclear whether
    we should clean out the punctuation in the model. On the one hand, punctuation
    doesn’t seem to convey as much meaning as words do. On the other, it seems plausible
    that, for example, lots of exclamation points in an article could help a model
    decide whether the article is real or fake. In this case, we’ve decided to keep
    punctuation, but curious readers can repeat this analysis after stripping the
    punctuation out to see how the resulting model is affected.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系数展示了该模型的一些怪癖。我们看到一些有影响力的特征对应于原始文本中的标点符号。目前尚不清楚我们是否应该清除模型中的标点符号。一方面，标点符号似乎没有单词传达的意义那么多。另一方面，似乎合理的是，例如，一篇文章中有很多感叹号可能有助于模型决定文章是真实还是假的。在这种情况下，我们决定保留标点符号，但是好奇的读者可以在去除标点符号后重复此分析，以查看生成的模型受到的影响。
- en: 'We conclude by displaying the test set error for all three models:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们显示了所有三个模型的测试集误差：
- en: '|   | test set error |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|   | 测试集误差 |'
- en: '| --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **model1** | 0.61 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **模型1** | 0.61 |'
- en: '| **model2** | 0.70 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| **模型2** | 0.70 |'
- en: '| **model3** | 0.88 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **模型3** | 0.88 |'
- en: 'As we might expect, the models became more accurate as we introduced more features.
    The model that used tf-idf performed much better than the models with binary handpicked
    word features, but it did not meet the 100% accuracy obtained on the train set.
    This illustrates a common trade-off in modeling: given enough data, more complex
    models can often outperform simpler ones, especially in situations like this case
    study where simpler models have too much model bias to perform well. However,
    complex models can be more difficult to interpret. For example, our tf-idf model
    had over 20,000 features, which makes it basically impossible to explain how our
    model makes its decisions. In addition, the tf-idf model takes much longer to
    make predictions—it’s over 100 times slower compared to model 2\. All of these
    factors need to be considered when deciding which model to use in practice.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预料的那样，随着我们引入更多的特征，模型变得更加准确。使用 tf-idf 的模型比使用二进制手工选择的词特征的模型表现要好得多，但是它没有达到在训练集上获得的
    100% 准确率。这说明了建模中的一种常见权衡：在给定足够的数据的情况下，更复杂的模型通常可以胜过更简单的模型，特别是在这种情况研究中，更简单的模型有太多的模型偏差而表现不佳的情况下。但是，复杂的模型可能更难解释。例如，我们的
    tf-idf 模型有超过 20,000 个特征，这使得基本上不可能解释我们的模型如何做出决策。此外，与模型 2 相比，tf-idf 模型需要更长时间进行预测——它的速度慢了
    100 倍。在决定使用哪种模型时，所有这些因素都需要考虑在内。
- en: In addition, we need to be careful about what our models are useful for. In
    this case, our models use the content of the news articles for prediction, making
    them highly dependent on the words that appear in the train set. However, our
    models will likely not perform as well on future news articles that use words
    that didn’t appear in the train set. For example, our models use the US election
    candidates’ names in 2016 for prediction, but they won’t know to incorporate the
    names of the candidates in 2020 or 2024\. To use our models in the longer term,
    we would need to address this issue of *drift*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们需要注意我们的模型适用于什么。在这种情况下，我们的模型使用新闻文章的内容进行预测，这使得它们高度依赖于出现在训练集中的单词。然而，我们的模型可能不会在未来的新闻文章上表现得像在训练集中没有出现的单词那样好。例如，我们的模型使用
    2016 年美国选举候选人的名字进行预测，但是它们不会知道要在 2020 或 2024 年纳入候选人的名字。为了在较长时间内使用我们的模型，我们需要解决这个*漂移*问题。
- en: 'That said, it’s surprising that a logistic regression model can perform well
    with a relatively small amount of feature engineering (tf-idf). We’ve addressed
    our original research question: our tf-idf model appears effective for detecting
    fake news in our dataset, and it could plausibly generalize to other news published
    in the same time period covered in the training data.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，令人惊讶的是，一个逻辑回归模型在相对较少的特征工程（tf-idf）下也能表现良好。我们已经回答了我们最初的研究问题：我们的 tf-idf 模型在检测我们数据集中的假新闻方面表现出色，而且可能可以推广到训练数据覆盖的同一时间段内发布的其他新闻。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We’re quickly approaching the end of the chapter and thus the end of the book.
    We started this book by talking about the data science lifecycle. Let’s take another
    look at the lifecycle, in [Figure 21-2](#fig-ds-lifecycle-conclusion), to appreciate
    everything that you’ve learned.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就要结束本章，从而结束这本书。我们从讨论数据科学生命周期开始这本书。让我们再次看看生命周期，在 [图 21-2](#fig-ds-lifecycle-conclusion)
    中，以欣赏您所学到的一切。
- en: '![](assets/leds_2102.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_2102.png)'
- en: Figure 21-2\. The four high-level steps of the data science lifecycle, each
    of which we dove into throughout this book
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 21-2。数据科学生命周期的四个高级步骤，本书中我们详细探讨了每个步骤。
- en: 'This case study stepped through each stage of the data science lifecycle:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究逐步介绍了数据科学生命周期的每个阶段：
- en: Many data analyses begin with a research question. The case study we presented
    in this chapter started by asking whether we can create models to automatically
    detect fake news.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 许多数据分析从一个研究问题开始。本章中我们呈现的案例研究从询问我们是否可以创建自动检测假新闻模型开始。
- en: We obtained data by using code found online that scrapes web pages into JSON
    files. Since the data description was relatively minimal, we needed to clean the
    data to understand it. This included creating new features to indicate the presence
    or absence of certain words in the articles.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用在线找到的代码将网页抓取到JSON文件中来获取数据。由于数据描述相对较少，我们需要清理数据以理解它。这包括创建新的特征来指示文章中特定词语的存在或缺失。
- en: Our initial explorations identified possible words that might be useful for
    prediction. After fitting simple models and exploring their precision and accuracy,
    we further transformed the articles using tf-idf to convert each news article
    into a normalized word vector.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的初步探索确定了可能对预测有用的单词。在拟合简单模型并探索它们的精确度和准确度后，我们进一步使用tf-idf转换文章，将每篇新闻文章转换为归一化的词向量。
- en: We used the vectorized text as features in a logistic model, and we fitted the
    final model using regularization and cross-validation. Finally, we found the accuracy
    and precision of the fitted model on the test set.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将向量化文本作为逻辑模型中的特征，并使用正则化和交叉验证拟合最终模型。最后，在测试集上找到拟合模型的准确度和精确度。
- en: When we write out the steps in the lifecycle like this, the steps seem to flow
    smoothly into each other. But reality is messy—as the diagram illustrates, real
    data analyses jump forward and backward between steps. For example, at the end
    of our case study, we discovered data cleaning questions that might motivate us
    to revisit earlier stages of the lifecycle. Although our model was quite accurate,
    the majority of the training data came from the 2016–2018 time period, so we have
    to carefully evaluate the model’s performance if we want to use it on articles
    published outside that time frame.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们像这样详细列出生命周期中的步骤时，步骤之间似乎流畅连接在一起。但现实是混乱的——正如图表所示，真实数据分析在各个步骤之间来回跳跃。例如，在我们的案例研究结束时，我们发现了可能促使我们重新访问生命周期早期阶段的数据清理问题。尽管我们的模型非常准确，但大部分训练数据来自2016年至2018年的时期，因此如果我们想要在该时间段之外的文章上使用它，就必须仔细评估模型的性能。
- en: 'In essence, it’s important to keep the entire lifecycle in mind at each stage
    of a data analysis. As a data scientist, you will be asked to justify your decisions,
    which means that you need to deeply understand your research question and data.
    To develop this understanding, the principles and techniques in this book equip
    you with a foundational set of skills. Going forward into your data science journey,
    we recommend that you continue to expand your skills by:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，重要的是在数据分析的每个阶段牢记整个生命周期。作为数据科学家，你将被要求证明你的决策，这意味着你需要深入理解你的研究问题和数据。本书中的原则和技术将为你提供一套基础技能。在你的数据科学旅程中继续前进，我们建议你通过以下方式继续扩展你的技能：
- en: Revisiting a case study from this book. Start by replicating our analysis, then
    dive deeper into questions that you have about the data.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新审视本书的案例研究。首先复制我们的分析，然后深入探讨你对数据的疑问。
- en: Conducting an independent data analysis. Pose a research question you’re interested
    in, find relevant data from the web, and analyze the data to see how well the
    data matched your expectations. Doing this will give you firsthand experience
    with the entire data science lifecycle.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行独立的数据分析。提出你感兴趣的研究问题，从网络中找到相关数据，并分析数据，看看数据与你的期望有多大匹配。这样做将使你对整个数据科学生命周期有第一手经验。
- en: Taking a deep dive into a topic. We’ve provided many in-depth resources in the
    [Additional Material](bibliography01.html#ax-extra-reading) appendix. Take the
    resource that seems most interesting to you and learn more about it.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入研究一个主题。我们在[附加材料](bibliography01.html#ax-extra-reading)附录中提供了许多深入资源。选择你最感兴趣的资源，并深入了解。
- en: The world needs people like you who can use data to make conclusions, so we
    sincerely hope that you’ll use these skills to help others make effective strategies,
    better products, and informed decisions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 世界需要像你这样能够利用数据得出结论的人，因此我们真诚地希望你能利用这些技能帮助他人制定有效的战略、打造更好的产品，并做出明智的决策。
