- en: 'Chapter 21\. Case Study: Detecting Fake News'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Fake news*—false information created in order to deceive others—is an important
    issue because it can harm people. For example, the social media post in [Figure 21-1](#fig-dont-use-sanitizer)
    confidently stated that hand sanitizer doesn’t work on coronaviruses. Though factually
    incorrect, it spread through social media anyway: it was shared nearly 100,000
    times and was likely seen by millions of people.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_2101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21-1\. A popular post on Twitter from March 2020 falsely claimed that
    sanitizer doesn’t kill coronaviruses
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We might wonder whether we can automatically detect fake news without having
    to read the stories. For this case study, we go through the steps of the data
    science lifecycle. We start by refining our research question and obtaining a
    dataset of news articles and labels. Then we wrangle and transform the data. Next,
    we explore the data to understand its content and devise features to use for modeling.
    Finally, we build models using logistic regression to predict whether news articles
    are real or fake, and evaluate their performance.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve included this case study because it lets us reiterate several important
    ideas in data science. First, natural language data appear often, and even basic
    techniques can enable useful analyses. Second, model selection is an important
    part of data analysis, and in this case study we apply what we’ve learned about
    cross-validation, the bias-variance trade-off, and regularization. Finally, even
    models that perform well on the test set might have inherent limitations when
    we try to use them in practice, as we will soon see.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by refining our research question and understanding the scope of
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: Question and Scope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our initial research question is: can we automatically detect fake news? To
    refine this question, we consider the kind of information that we might use to
    build a model for detecting fake news. If we have hand-classified news stories
    where people have read each story and determined whether it is fake or not, then
    our question becomes: can we build a model to accurately predict whether a news
    story is fake based on its content?'
  prefs: []
  type: TYPE_NORMAL
- en: To address this question, we can use the FakeNewsNet data repository as described
    in [Shu et al](https://arxiv.org/abs/1809.01286). This repository contains content
    from news and social media websites, as well as metadata like user engagement
    metrics. For simplicity, we only look at the dataset’s political news articles.
    This subset of the data includes only articles that were fact-checked by [Politifact](https://www.politifact.com),
    a nonpartisan organization with a good reputation. Each article in the dataset
    has a “real” or “fake” label based on Politifact’s evaluation, which we use as
    the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: Politifact uses a nonrandom sampling method to select articles to fact-check.
    According to its website, Politifact’s journalists select the “most newsworthy
    and significant” claims each day. Politifact started in 2007 and the repository
    was published in 2020, so most of the articles were published between 2007 and
    2020.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summarizing this information, we determine that the target population consists
    of all political news stories published online in the time period from 2007 to
    2020 (we would also want to list the sources of the stories). The access frame
    is determined by Politifact’s identification of the most newsworthy claims of
    the day. So the main sources of bias for this data include:'
  prefs: []
  type: TYPE_NORMAL
- en: Coverage bias
  prefs: []
  type: TYPE_NORMAL
- en: The news outlets are limited to those that Politifact monitored, which may miss
    arcane or short-lived sites.
  prefs: []
  type: TYPE_NORMAL
- en: Selection bias
  prefs: []
  type: TYPE_NORMAL
- en: The data are limited to articles Politifact decided were interesting enough
    to fact-check, which means that articles might skew toward ones that are both
    widely shared and controversial.
  prefs: []
  type: TYPE_NORMAL
- en: Measurement bias
  prefs: []
  type: TYPE_NORMAL
- en: Whether a story should be labeled “fake” or “real” is determined by one organization
    (Politifact) and reflects the biases, unintentional or otherwise, that the organization
    has in its fact-checking methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Drift
  prefs: []
  type: TYPE_NORMAL
- en: Since we only have articles published between 2007 and 2020, there is likely
    to be drift in the content. Topics are popularized and faked in rapidly evolving
    news trends.
  prefs: []
  type: TYPE_NORMAL
- en: We will keep these limitations of the data in mind as we begin to wrangle the
    data into a form that we can analyze.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining and Wrangling the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s get the data into Python using the [GitHub page for FakeNewsNet](https://oreil.ly/0DOHd).
    Reading over the repository description and code, we find that the repository
    doesn’t actually store the news articles itself. Instead, running the repository
    code will scrape news articles from online web pages directly (using techniques
    we covered in [Chapter 14](ch14.html#ch-web)). This presents a challenge: if an
    article is no longer available online, it likely will be missing from our dataset.
    Noting this, let’s proceed with downloading the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The FakeNewsNet code highlights one challenge in reproducible research—online
    datasets change over time, but it can be difficult (or even illegal) to store
    and share copies of this data. For example, other parts of the FakeNewsNet dataset
    use Twitter posts, but the dataset creators would violate Twitter’s terms and
    services if they stored copies of the posts in their repository. When working
    with data gathered from the web, we suggest documenting the date the data were
    gathered and reading the terms and services of the data sources carefully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the script to download the Politifact data takes about an hour. After
    that, we place the datafiles into the *data/politifact* folder. The articles that
    Politifact labeled as fake and real are in *data/politifact/fake* and *data/politifact/real*.
    Let’s take a look at one of the articles labeled “real”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Each article’s data is stored in a JSON file named *news content.json*. Let’s
    load the JSON for one article into a Python dictionary (see [Chapter 14](ch14.html#ch-web)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we’ve displayed the keys and values in `article_json` as a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| key |   |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **url** | http://www.senate.gov/legislative/LIS/roll_cal... |'
  prefs: []
  type: TYPE_TB
- en: '| **text** | Roll Call Vote 111th Congress - 1st Session\n\... |'
  prefs: []
  type: TYPE_TB
- en: '| **images** | [http://statse.webtrendslive.com/dcs222dj3ow9j... |'
  prefs: []
  type: TYPE_TB
- en: '| **top_img** | http://www.senate.gov/resources/images/us_sen.ico |'
  prefs: []
  type: TYPE_TB
- en: '| **keywords** | [] |'
  prefs: []
  type: TYPE_TB
- en: '| **authors** | [] |'
  prefs: []
  type: TYPE_TB
- en: '| **canonical_link** |   |'
  prefs: []
  type: TYPE_TB
- en: '| **title** | U.S. Senate: U.S. Senate Roll Call Votes 111th... |'
  prefs: []
  type: TYPE_TB
- en: '| **meta_data** | {''viewport’: ‘width=device-width, initial-scal... |'
  prefs: []
  type: TYPE_TB
- en: '| **movies** | [] |'
  prefs: []
  type: TYPE_TB
- en: '| **publish_date** | None |'
  prefs: []
  type: TYPE_TB
- en: '| **source** | http://www.senate.gov |'
  prefs: []
  type: TYPE_TB
- en: '| **summary** |   |'
  prefs: []
  type: TYPE_TB
- en: 'There are many fields in the JSON file, but for this analysis we only look
    at a few that are primarily related to the content of the article: the article’s
    title, text content, URL, and publication date. We create a dataframe where each
    row represents one article (the granularity in a news story). To do this, we load
    in each available JSON file as a Python dictionary, and then extract the fields
    of interest to store as a `pandas` `DataFrame` named `df_raw`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|   | url | text | title | publish_date | label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | dailybuzzlive.com/cannibals-arrested-florida/ | Police in Vernal Heights,
    Florida, arrested 3-... | Cannibals Arrested in Florida Claim Eating Hum... |
    1.62e+09 | fake |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | https://web.archive.org/web/20171228192703/htt... | WASHINGTON — Rod
    Jay Rosenstein, Deputy Attorn... | BREAKING: Trump fires Deputy Attorney General
    ... | 1.45e+09 | fake |'
  prefs: []
  type: TYPE_TB
- en: 'Exploring this dataframe reveals some issues we’d like to address before we
    begin the analysis. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Some articles couldn’t be downloaded. When this happened, the `url` column contains
    `NaN`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some articles don’t have text (such as a web page with only video content).
    We drop these articles from our dataframe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `publish_date` column stores timestamps in Unix format (seconds since the
    Unix epoch), so we need to convert them to `pandas.Timestamp` objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are interested in the base URL of a web page. However, the `source` field
    in the JSON file has many missing values compared to the `url` column, so we must
    extract the base URL using the full URL in the `url` column. For example, from
    *dailybuzzlive.com/cannibals-arrested-florida/* we get *dailybuzzlive.com*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some articles were downloaded from an archival website (`web.archive.org`).
    When this happens, we want to extract the actual base URL from the original by
    removing the `web.archive.org` prefix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to concatenate the `title` and `text` columns into a single `content`
    column that contains all of the text content of the article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can tackle these data issues using a combination of `pandas` functions and
    regular expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After data wrangling, we end up with the following dataframe named `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|   | timestamp | baseurl | content | label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 2021-04-05 16:39:51 | dailybuzzlive.com | Cannibals Arrested in Florida
    Claim Eating Hum... | fake |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2016-01-01 23:17:43 | houstonchronicle-tv.com | BREAKING: Trump fires
    Deputy Attorney General ... | fake |'
  prefs: []
  type: TYPE_TB
- en: Now that we’ve loaded and cleaned the data, we can proceed to exploratory data
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset of news articles we’re exploring is just one part of the larger
    FakeNewsNet dataset. As such, the original paper doesn’t provide detailed information
    about our subset of data. So, to better understand the data, we must explore it
    ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before starting exploratory data analysis, we apply our standard practice of
    splitting the data into training and test sets. We perform EDA using only the
    train set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|   | timestamp | baseurl | content |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **164** | 2019-01-04 19:25:46 | worldnewsdailyreport.com | Chinese lunar
    rover finds no evidence of Ameri... |'
  prefs: []
  type: TYPE_TB
- en: '| **28** | 2016-01-12 21:02:28 | occupydemocrats.com | Virginia Republican
    Wants Schools To Check Chi... |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s count the number of real and fake articles in the train set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Our train set has 584 articles, and there are about 60 more articles labeled
    `real` than `fake`. Next, we check for missing values in the three fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Nearly half of the timestamps are null. This feature will limit the dataset
    if we use it in the analysis. Let’s take a closer look at the `baseurl`, which
    represents the website that published the original article.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Publishers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand the `baseurl` column, we start by counting the number of articles
    from each website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Our train set has 584 rows, and we have found that there are 337 unique publishing
    websites. This means that the dataset includes many publications with only a few
    articles. A histogram of the number of articles published by each website confirms
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_21in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This histogram shows that the vast majority (261 out of 337) of websites have
    only one article in the train set, and only a few websites have more than five
    articles in the train set. Nonetheless, it can be informative to identify the
    websites that published the most fake or real articles. First, we find the websites
    that published the most fake articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_21in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we list the websites that published the greatest number of real articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_21in03.png)'
  prefs: []
  type: TYPE_IMG
- en: Only `cnn.com` and `washingtonpost.com` appear on both lists. Even without knowing
    the total number of articles for these sites, we might expect that an article
    from `yournewswire.com` is more likely to be labeled as `fake`, while an article
    from `whitehouse.gov` is more likely to be labeled as `real`. That said, we don’t
    expect that using the publishing website to predict article truthfulness would
    work very well; there are simply too few articles from most of the websites in
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the `timestamp` column, which records the publication date
    of the news articles.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Publication Date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Plotting the timestamps on a histogram shows that most articles were published
    after 2000, although there seems to be at least one article published before 1940:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_21in04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we take a closer look at the news articles published prior to 2000, we
    find that the timestamps don’t match the actual publication date of the article.
    These date issues are most likely related to the web scraper collecting inaccurate
    information from the web pages. We can zoom into the region of the histogram after
    2000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_21in05.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, most of the articles were published between 2007 (the year Politifact
    was founded) and 2020 (the year the FakeNewsNet repository was published). But
    we also find that the timestamps are concentrated on the years 2016 to 2018—the
    year of the controversial 2016 US presidential election and the two years following.
    This insight is a further caution on the limitation of our analysis to carry over
    to nonelection years.
  prefs: []
  type: TYPE_NORMAL
- en: Our main aim is to use the text content for classification. We explore some
    word frequencies next.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Words in Articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’d like to see whether there’s a relationship between the words used in the
    articles and whether the article was labeled as `fake`. One simple way to do this
    is to look at individual words like *military*, then count how many articles that
    mentioned “military” were labeled `fake`. For *military* to be useful, the articles
    that mention it should have a much higher or much lower fraction of fake articles
    than 45% (the proportion of fake articles in the dataset: 264/584).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use our domain knowledge of political topics to pick out a few candidate
    words to explore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define a function that creates a new feature for each word, where the
    feature contains `True` if the word appeared in the article and `False` if not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This is like one-hot encoding for the presence of a word (see [Chapter 15](ch15.html#ch-linear)).
    We can use this function to further wrangle our data and create a new dataframe
    with a feature for each of our chosen words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '|   | trump | clinton | state | vote | ... | swamp | cnn | the | label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **164** | False | False | True | False | ... | False | False | True | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **28** | False | False | False | False | ... | False | False | True | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **708** | False | False | True | True | ... | False | False | True | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **193** | False | False | False | False | ... | False | False | True | 1
    |'
  prefs: []
  type: TYPE_TB
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can find the proportion of these articles that were labeled `fake`.
    We visualize these calculations in the following plots. In the left plot, we mark
    the proportion of `fake` articles in the entire train set using a dotted line,
    which helps us understand how informative each word feature is—a highly informative
    word will have a point that lies far away from the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_21in06.png)'
  prefs: []
  type: TYPE_IMG
- en: This plot reveals a few interesting considerations for modeling. For example,
    notice that the word *antifa* is highly predictive—all articles that mention the
    word *antifa* are labeled `fake`. However, *antifa* only appears in a few articles.
    On the other hand, the word *the* appears in nearly every article, but is uninformative
    for distinguishing between `real` and `fake` articles because the proportion of
    articles with *the* that are fake matches the proportion of fake articles overall.
    We might instead do better with a word like *vote*, which is predictive and appears
    in many news articles.
  prefs: []
  type: TYPE_NORMAL
- en: This exploratory analysis brought us understanding of the time frame that our
    news articles were published in, the broad range of publishing websites captured
    in the data, and candidate words to use for prediction. Next, we fit models for
    predicting whether articles are fake or real.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve obtained, cleaned, and explored our data, let’s fit models to
    predict whether articles are real or fake. In this section, we use logistic regression
    because we have a binary classification problem. We fit three different models
    that increase in complexity. First, we fit a model that just uses the presence
    of a single handpicked word in the document as an explanatory feature. Then we
    fit a model that uses multiple handpicked words. Finally, we fit a model that
    uses all the words in the train set, vectorized using the tf-idf transform (introduced
    in [Chapter 13](ch13.html#ch-text)). Let’s start with the simple single-word model.
  prefs: []
  type: TYPE_NORMAL
- en: A Single-Word Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our EDA showed that the word *vote* is related to whether an article is labeled
    `real` or `fake`. To test this, we fit a logistic regression model using a single
    binary feature: `1` if the word *vote* appears in the article and `0` if not.
    We start by defining a function to lowercase the article content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For our first classifier, we only use the word *vote*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can chain the `lowercase` function and the function `make_word_features`
    from our EDA into a `scikit-learn` pipeline. This provides a convenient way to
    transform and fit data all at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: When used, the preceding pipeline converts the characters in the article content
    to lowercase, creates a dataframe with a binary feature for each word of interest,
    and fits a logistic regression model on the data using <math><msub><mi>L</mi>
    <mn>2</mn></msub></math> regularization. Additionally, the `LogisticRegressionCV`
    function uses cross-validation (fivefold by default) to select the best regularization
    parameter. (See [Chapter 16](ch16.html#ch-risk) for more on regularization and
    cross-validation.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the pipeline to fit the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Overall, the single-word classifier only classifies 65% of articles correctly.
    We plot the confusion matrix of the classifier on the train set to see what kinds
    of mistakes it makes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_21in07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our model often misclassifies real articles (0) as fake (1). Since this model
    is simple, we can take a look at the probabilities for the two cases: the word
    *vote* is in the article or is not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'When an article contains the word *vote*, the model gives a high probability
    of the article being real, and when *vote* is absent, the probability leans slightly
    toward the article being fake. We encourage readers to verify this for themselves
    using the definition of the logistic regression model and the fitted coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in [Chapter 19](ch19.html#ch-logistic), the coefficient indicates
    the size of the change in the odds with a change in the explanatory variable.
    With a 0-1 variable like the presence or absence of a word in an article, this
    has a particularly intuitive meaning. For an article with *vote* in it, the odds
    of being fake decrease by a factor of <math><mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mrow><mi>v</mi> <mi>o</mi> <mi>t</mi> <mi>e</mi></mrow></msub>
    <mo stretchy="false">)</mo></math> , which is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that in this modeling scenario, a label of `0` corresponds to a real
    article and a label of `1` corresponds to a fake article. This might seem a bit
    counterintuitive—we’re saying that a “true positive” is when a model correctly
    predicts a fake article as fake. In binary classification, we typically say a
    “positive” result is the one with the presence of something unusual. For example,
    a person who tests positive for an illness would expect to have the illness.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make our model a bit more sophisticated by introducing additional word
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple-Word Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We create a model that uses all of the words we examined in our EDA of the
    train set, except for *the*. Let’s fit a model using these 15 features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This model is about 10 percentage points more accurate than the one-word model.
    It may seem a bit surprising that going from a one-word model to a 15-word model
    only gains 10 percentage points. The confusion matrix is helpful in teasing out
    the kinds of errors made:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_21in08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that this classifier does a better job of classifying real articles
    accurately. However, it makes more mistakes than the simple one-word model when
    classifying fake article—59 of the fake articles were classified as real. In this
    scenario, we might be more concerned about misclassifying an article as fake when
    it is real. So we wish to have a high precision—the ratio of fake articles correctly
    predicted as fake to articles predicted as fake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The precision in our larger model is improved, but about 30% of the articles
    labeled as fake are actually real. Let’s take a look at the model’s coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_21in09.png)'
  prefs: []
  type: TYPE_IMG
- en: We can make a quick interpretation of the coefficients by looking at their signs.
    The large positive values on *trump* and *investig* indicate that the model predicts
    that new articles containing these words have a higher probability of being fake.
    The reverse is true for words like *congress* and *vote*, which have negative
    weights. We can use these coefficients to compare the log odds when an article
    does or does not contain a particular word.
  prefs: []
  type: TYPE_NORMAL
- en: Although this larger model performs better than the simple one-word model, we
    had to handpick the word features using our knowledge of the news. What if we
    missed the words that are highly predictive? To address this, we can incorporate
    all the words in the articles using the tf-idf transform.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting with the tf-idf Transform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the third and final model, we use the term frequency-inverse document frequency
    (tf-idf) transform from [Chapter 13](ch13.html#ch-text) to vectorize the entire
    text of all articles in the train set. Recall that with this transform, an article
    is converted into a vector with one element for each word that appears in any
    of the 564 articles. The vector consists of normalized counts of the number of
    times the word appears in the article normalized by the rareness of the word.
    The tf-idf puts more weight on words that only appear in a few documents. This
    means that our classifier uses all the words in the train set’s news articles
    for prediction. As we’ve done previously when we introduced tf-idf, first we remove
    stopwords, then we tokenize the words, and then we use the `TfidfVectorizer` from
    `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We find that this model achieves 100% accuracy on the train set. We can take
    a look at the tf-idf transformer to better understand the model. Let’s start by
    finding out how many unique tokens the classifier uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that our classifier has 23,812 features, a large increase from our
    previous model, which only had 15\. Since we can’t display that many model weights,
    we display the 10 most negative and 10 most positive weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_21in10.png)'
  prefs: []
  type: TYPE_IMG
- en: These coefficients show a few quirks about this model. We see that several influential
    features correspond to punctuation in the original text. It’s unclear whether
    we should clean out the punctuation in the model. On the one hand, punctuation
    doesn’t seem to convey as much meaning as words do. On the other, it seems plausible
    that, for example, lots of exclamation points in an article could help a model
    decide whether the article is real or fake. In this case, we’ve decided to keep
    punctuation, but curious readers can repeat this analysis after stripping the
    punctuation out to see how the resulting model is affected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We conclude by displaying the test set error for all three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | test set error |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **model1** | 0.61 |'
  prefs: []
  type: TYPE_TB
- en: '| **model2** | 0.70 |'
  prefs: []
  type: TYPE_TB
- en: '| **model3** | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: 'As we might expect, the models became more accurate as we introduced more features.
    The model that used tf-idf performed much better than the models with binary handpicked
    word features, but it did not meet the 100% accuracy obtained on the train set.
    This illustrates a common trade-off in modeling: given enough data, more complex
    models can often outperform simpler ones, especially in situations like this case
    study where simpler models have too much model bias to perform well. However,
    complex models can be more difficult to interpret. For example, our tf-idf model
    had over 20,000 features, which makes it basically impossible to explain how our
    model makes its decisions. In addition, the tf-idf model takes much longer to
    make predictions—it’s over 100 times slower compared to model 2\. All of these
    factors need to be considered when deciding which model to use in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we need to be careful about what our models are useful for. In
    this case, our models use the content of the news articles for prediction, making
    them highly dependent on the words that appear in the train set. However, our
    models will likely not perform as well on future news articles that use words
    that didn’t appear in the train set. For example, our models use the US election
    candidates’ names in 2016 for prediction, but they won’t know to incorporate the
    names of the candidates in 2020 or 2024\. To use our models in the longer term,
    we would need to address this issue of *drift*.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, it’s surprising that a logistic regression model can perform well
    with a relatively small amount of feature engineering (tf-idf). We’ve addressed
    our original research question: our tf-idf model appears effective for detecting
    fake news in our dataset, and it could plausibly generalize to other news published
    in the same time period covered in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re quickly approaching the end of the chapter and thus the end of the book.
    We started this book by talking about the data science lifecycle. Let’s take another
    look at the lifecycle, in [Figure 21-2](#fig-ds-lifecycle-conclusion), to appreciate
    everything that you’ve learned.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_2102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21-2\. The four high-level steps of the data science lifecycle, each
    of which we dove into throughout this book
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This case study stepped through each stage of the data science lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: Many data analyses begin with a research question. The case study we presented
    in this chapter started by asking whether we can create models to automatically
    detect fake news.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We obtained data by using code found online that scrapes web pages into JSON
    files. Since the data description was relatively minimal, we needed to clean the
    data to understand it. This included creating new features to indicate the presence
    or absence of certain words in the articles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our initial explorations identified possible words that might be useful for
    prediction. After fitting simple models and exploring their precision and accuracy,
    we further transformed the articles using tf-idf to convert each news article
    into a normalized word vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We used the vectorized text as features in a logistic model, and we fitted the
    final model using regularization and cross-validation. Finally, we found the accuracy
    and precision of the fitted model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we write out the steps in the lifecycle like this, the steps seem to flow
    smoothly into each other. But reality is messy—as the diagram illustrates, real
    data analyses jump forward and backward between steps. For example, at the end
    of our case study, we discovered data cleaning questions that might motivate us
    to revisit earlier stages of the lifecycle. Although our model was quite accurate,
    the majority of the training data came from the 2016–2018 time period, so we have
    to carefully evaluate the model’s performance if we want to use it on articles
    published outside that time frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, it’s important to keep the entire lifecycle in mind at each stage
    of a data analysis. As a data scientist, you will be asked to justify your decisions,
    which means that you need to deeply understand your research question and data.
    To develop this understanding, the principles and techniques in this book equip
    you with a foundational set of skills. Going forward into your data science journey,
    we recommend that you continue to expand your skills by:'
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting a case study from this book. Start by replicating our analysis, then
    dive deeper into questions that you have about the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting an independent data analysis. Pose a research question you’re interested
    in, find relevant data from the web, and analyze the data to see how well the
    data matched your expectations. Doing this will give you firsthand experience
    with the entire data science lifecycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a deep dive into a topic. We’ve provided many in-depth resources in the
    [Additional Material](bibliography01.html#ax-extra-reading) appendix. Take the
    resource that seems most interesting to you and learn more about it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The world needs people like you who can use data to make conclusions, so we
    sincerely hope that you’ll use these skills to help others make effective strategies,
    better products, and informed decisions.
  prefs: []
  type: TYPE_NORMAL
