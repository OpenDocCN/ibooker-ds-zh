<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Text Classification Algorithms"><div class="chapter" id="ch-classification">
<h1><span class="label">Chapter 6. </span>Text Classification Algorithms</h1>

<p>The internet is often referred to as the great enabler: it allows us to accomplish a lot in our daily lives with the help of online tools and platforms. On the other hand, it can also be a source of information overload and endless search. Whether it is communicating with colleagues and customers, partners, or vendors, emails and other messaging tools are an inherent part of our daily work lives. Brands interact with customers and get valuable feedback on their products through social media channels like Facebook and Twitter. Software developers and product managers communicate using ticketing applications like <a href="https://trello.com">Trello</a> to track development tasks, while open source communities use <a href="https://github.com">GitHub</a> issues and <a href="https://bugzilla.org">Bugzilla</a> to track software bugs that need to be fixed or new functionality that needs to be added.</p>

<p>While these tools are useful for getting work done, they can also become overwhelming and quickly turn into a deluge of information. A lot of emails contain promotional content, spam, and marketing newsletters that are often a distraction. Similarly, software developers can easily get buried under a mountain of bug reports and feature requests that take away their productivity. In order to make the best use of these tools, we must also <a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="about" id="idm45634195661800"/>use techniques to categorize, filter, and prioritize the more important information from the less relevant pieces, and text classification is one such technique that can help us achieve this.</p>

<p>The most <a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="use cases for" id="idm45634195659704"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for text classification" data-secondary-sortas="text classification" id="idm45634195658328"/>common example of this is spam detection that is provided by email providers. In this application of text classification, every incoming email is analyzed to determine whether it contains meaningful and useful content or irrelevant information that is not useful. This allows the email application to show only the relevant and important emails and take away the deluge of less useful information. Another application is the classification of incoming customer service requests or software bug reports. If we are able to classify and assign them to the right person or department, then they will be resolved faster. There are several applications of text classification, and in this chapter we will develop a blueprint that can be applied across several of them.</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634195655640">
	<h1>What You’ll Learn and What We’ll Build</h1>


<p>In this chapter, we will build a blueprint for text classification using a supervised learning technique. We will use a dataset containing bug reports of a software application and use the blueprint to predict the priority of these bugs and the specific module that a particular bug belongs to. After studying this chapter, you will understand how to apply supervised learning techniques, splitting the data into train and test parts, validating model performance using accuracy measures, and applying cross-validation techniques. You will also learn about different types of text classification such as binary and multiclass classifications.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Introducing the Java Development Tools Bug Dataset"><div class="sect1" id="idm45634195653432">
<h1>Introducing the Java Development Tools Bug Dataset</h1>

<p>Software technology products are often complex and consist of several interacting components. For example, let’s say you are part of a team developing an Android application that plays podcasts. Apart from the player itself, there can be separate components such as the library manager, search and discover, and so on. If a user reports that they are unable to play any podcasts, then it’s important to recognize that this is a critical bug that needs immediate attention. Another user might report an issue with their favorite podcast not showing up. This may not be as critical, but it’s important to determine whether this needs to be looked at by the library manager team or if it’s actually a problem for the search and discover team. To ensure fast response times, it’s important to classify issues accurately and assign them to the right team. Bugs are an inevitable part of any software product, but a quick response will ensure that customers will be happy and continue to use your product.</p>

<p>In this chapter, we will use <a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="with JDT bugs dataset" data-secondary-sortas="JDT bugs dataset" id="ch6_term3"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="JDT (Java Development Tools) bugs dataset" id="ch6_term43"/>blueprints to classify bugs and issues raised during the development of the Java Development Tools (JDT) <a href="https://eclipse.org/jdt">open source project</a>. The JDT project is a part of the Eclipse foundation, which develops the Eclipse integrated development environment (IDE). JDT provides all the functionality needed by software developers to write code using Java in the Eclipse IDE. Users of JDT report bugs and track issues with the tool Bugzilla, a popular open source bug tracking software. Bugzilla is also used by other open source projects like Firefox and the Eclipse Platform. A dataset containing the bugs for all these projects can be found on <a href="https://oreil.ly/giRWx">GitHub</a>, and we will use the bugs dataset of the JDT project.</p>

<p>The following section loads a <em>CSV</em> file that contains the JDT bugs dataset. This dataset contains 45,296 bugs and some of the available characteristics for each bug. We print a list of all the features reported for a bug and look at some of them in more detail to see what the bug reports look like:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'eclipse_jdt.csv'</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">df</code><code class="p">[[</code><code class="s1">'Issue_id'</code><code class="p">,</code><code class="s1">'Priority'</code><code class="p">,</code><code class="s1">'Component'</code><code class="p">,</code><code class="s1">'Title'</code><code class="p">,</code><code class="s1">'Description'</code><code class="p">]]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Index(['Issue_id', 'Priority', 'Component', 'Duplicated_issue', 'Title',
       'Description', 'Status', 'Resolution', 'Version', 'Created_time',
       'Resolved_time'],
      dtype='object')
</pre>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>Issue_id</th>
			<th>Priority</th>
			<th>Component</th>
			<th>Title</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>38438</th>
			<td>239715</td>
			<td>P3</td>
			<td>UI</td>
			<td>No property tester for TestCaseElement for property projectNature</td>
			<td>I20080613-2000; ; Not sure if this belongs to JDT/Debug or Platform/Debug.; ; I saw this error message several times today in my error log but Im not yet sure how to reproduce it.; ; -- Error Deta...</td>
		</tr>
		<tr>
			<th>44129</th>
			<td>395007</td>
			<td>P3</td>
			<td>UI</td>
			<td>[package explorer] Refresh action not available on Java package folders</td>
			<td>M3.; ; F5 (Refresh) is available as a context menu entry for ordinary source folders but not for Java package folders in the e4 Java Package explorer.; ; Please restore the 3.x functionality.</td>
		</tr>
	</tbody>
</table>

<p>Based on the details shown in the previous table, we can see that each bug report contains the following important features:</p>

<dl>
<dt>Issue_id</dt>
  <dd>The primary key for the issue used to track the bug.</dd>
<dt>Priority</dt>
  <dd>This varies from P1 (most critical) to P5 (least critical) and defines the severity of the bug (a categorical field).</dd>
<dt>Component</dt>
  <dd>This refers to the specific architectural part of the project where the bug occurs. This could be the UI, the APT, etc. (a categorical field).</dd>
<dt>Title</dt>
  <dd>This is a short summary entered by the user that briefly describes the bug (a full text field).</dd>
<dt>Description</dt>
  <dd>This is a more detailed description of the software behavior that produces the bug and its impact on usage (a full text field).</dd>
</dl>

<p>While creating the bug reports, users follow the guidelines mentioned on the JDT Bugzilla website. This describes what information the user needs to provide while raising a bug so that the developer can find a quick resolution. The website also <span class="keep-together">includes</span> <span class="keep-together">guidelines</span> that help the user identify what priority should be given for a particular bug. Our blueprint will use these bug reports to develop a supervised learning algorithm that can be used to automatically assign a priority to any bug that is raised in the future.</p>

<p>In the previous section, we got a high-level understanding of the dataset and the various features for each bug report. Let’s now explore a single bug report in more detail. We randomly sample a single bug (you can choose a different value for <code>random_state</code> to see a different bug) and transpose the results so that the results can be displayed with more detail. If we do not transpose, the Description feature would be shown in a truncated manner, whereas now we can see all the contents:</p>

<!-- pd.set_option('display.max_colwidth', -1) -->
<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>
</pre>

<p><code>Out:</code></p>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>11811</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>Issue_id</th>
			<td>33113</td>
		</tr>
		<tr>
			<th>Priority</th>
			<td>P3</td>
		</tr>
		<tr>
			<th>Component</th>
			<td>Debug</td>
		</tr>
		<tr>
			<th>Title</th>
			<td>Evaluating for loop suspends in URLClassLoader</td>
		</tr>
		<tr>
			<th>Description</th>
			<td>Debug to a breakpoint in some HelloWorld program. In the DisplayView; highlight and ; Display the following code snippet:; ; for (int i = 0; i &lt; 10; i++) {; System.out.println(i);; }; ; Instead of just reporting No explicit return value; the debugger suspends in the ; URLClassLoader; apparently trying to load the class int. You have hit Resume several ; more times before the evaluation completes. The DebugView does not indicate why it ; has stopped (the thread is just labeled Evaluating). This behavior does not happen if ; you turn off the Suspend on uncaught exceptions preference.</td>
		</tr>
		<tr>
			<th>Status</th>
			<td>VERIFIED</td>
		</tr>
		<tr>
			<th>Resolution</th>
			<td>FIXED</td>
		</tr>
		<tr>
			<th>Version</th>
			<td>2.1</td>
		</tr>
		<tr>
			<th>Created_time</th>
			<td>2003-02-25 15:40:00 -0500</td>
		</tr>
		<tr>
			<th>Resolved_time</th>
			<td>2003-03-05 17:11:17 -0500</td>
		</tr>
	</tbody>
</table>

<p>We can see from the previous table that this bug was raised in the Debug component where the program would crash while evaluating a <code>for</code> loop. We can also see that the user has assigned a medium priority (P3) and that this bug was fixed in a week’s time. We can see that the reporter of this bug has followed the guidelines and provided a lot of information that also helps the software developer understand and identify the problem and provide a fix. Most software users are aware that the more information they provide, the easier it would be for a developer to understand the issue and provide a fix. Therefore, we can assume that most bug reports contain enough information for us to create a supervised learning model.</p>

<p>The output graph describes the distribution of bug reports across different priorities. We can see that most bugs have been assigned a level of P3. While this might be because Bugzilla assigns P3 as the default option, it is more likely that this reflects the natural tendency of users to pick a medium level for their bug reports. They believe that the bug does not have a high priority (P1) and at the same time do not want their bug to not be looked at all by choosing a P5. This is reflected in a lot of real-world phenomena and is generally referred to as the normal distribution, where a lot of observations are found at the center or mean with fewer observations at the ends. This could be also visualized as a bell curve.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code><code class="o">.</code><code class="n">sort_index</code><code class="p">()</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'bar'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<figure><div class="figure"><img src="Images/btap_06in01.jpg" width="1330" height="847"/>
<h6/>
</div></figure>

<p>The vast difference between the number of bugs with priority P3 versus other priorities is a problem for building a supervised learning model and is referred <a contenteditable="false" data-type="indexterm" data-primary="class imbalance" id="idm45634195460216"/>to as <em>class imbalance</em>. Because the class P3 has an order of magnitude greater number of observations than the other classes, the text classification algorithm will have much more information on P3 bugs than the other priorities: P1, P2, P4, and P5. We will see how the class imbalance of the Priority feature impacts our solution and also attempt to overcome it later in the blueprint. This is similar to learning something as a human. If you have seen more examples of one outcome, you will “predict” more of the same.</p>

<p>In the following snippet, we can see how many bugs are reported against each component of the JDT. The UI and Core components have a much greater number of bugs than the Doc or APT components. This is expected since some components of a software system are larger and more important than others. The Doc component, <span class="keep-together">for example,</span> consists of the documentation section of the software and is used by <span class="keep-together">software</span> developers to understand the functionality but is probably not a working <span class="keep-together">component</span>. The Core component, on the other hand, is an important functional component of JDT and therefore has many more bugs <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term3" id="idm45634195455080"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term43" id="idm45634195453704"/>assigned to it:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'Component'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
UI       17479
Core     13669
Debug    7542
Text     5901
APT      406
Doc      299
Name: Component, dtype: int64
</pre>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Building a Text Classification System"><div class="sect1" id="idm45634195652488">
<h1>Blueprint: Building a Text Classification System</h1>

<p>We will take a step-by-step approach to building a <a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="for text classification" data-secondary-sortas="text classification" id="ch6_term13"/><a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="blueprint for" id="ch6_term14"/>text classification system and then combine all of these steps to present a unified blueprint. This <em>text classification</em> system falls under the <a contenteditable="false" data-type="indexterm" data-primary="supervised learning models" id="ch6_term9"/>broader category of <em>supervised learning</em> models. <em>Supervised learning</em> refers to a domain of <a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="data for" id="ch6_term10"/><a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="in supervised learning" data-secondary-sortas="supervised learning" id="ch6_term11"/>machine learning algorithms that uses labeled data points as training data to learn the relationship between independent variables and the target variable. The <a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="training of" id="idm45634195409592"/><a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="of machine learning models" data-secondary-sortas="machine learning models" id="idm45634195408216"/>process of learning the relationship is also referred to as <em>training a machine learning model</em>. If the target variable is a continuous numeric variable like distance, sales units, or transaction amounts, we would train a <em>regression</em> model. However, in our case, the target variable (Priority) is a categorical variable like the priority or component, and we will choose a <em>classification</em> method to train a supervised learning model. This model will use independent variables such as title or description to predict the priority or component of the bug. A supervised machine learning method aims to learn the mapping function from input to output variable(s), defined mathematically as follows:</p>

<div data-type="equation">
  <p><math alttext="y equals f left-parenthesis upper X right-parenthesis">
  <mrow>
    <mi>y</mi>
    <mo>=</mo>
    <mi>f</mi>
    <mo>(</mo>
    <mi>X</mi>
    <mo>)</mo>
  </mrow>
</math></p>
</div>

<p>In the preceding equation, <math alttext="y">
  <mi>y</mi>
</math> is the output or target variable, <math alttext="f">
  <mi>f</mi>
</math> is the mapping function, and <math alttext="upper X">
  <mi>X</mi>
</math> is the input variable or set of variables.</p>

<p>Since we are using data that contains the labeled target variable, this is referred to as <em>supervised learning</em>. <a data-type="xref" href="#fig-supervised-learning">Figure 6-1</a> illustrates the workflow of a supervised learning model. There are two phases of the workflow: the training phase and the predicting phase. The training phase starts with the training data that includes the training observations (which could be text data like bug reports) and the associated labels (which is what we would want to predict like priority or software component). While many features of the training observations could be used as is, this alone may not be enough to learn the mapping function, and we would like to add domain knowledge to help the model understand the relationship better. For example, we could add a feature that shows on which day of the week the bug was reported since bugs are likely to be fixed sooner if they are reported earlier in the week. This <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="about" id="idm45634195372152"/>step is referred to as <em>feature engineering</em>, and the <a contenteditable="false" data-type="indexterm" data-primary="feature vectors" id="idm45634195370168"/>result is a set of <em>feature vectors</em> for each document. The training step of a supervised learning model accepts as input the feature vectors and their associated labels and tries to learn the mapping function. At the end of the training step, we have the mapping function, which is also called the trained model and can be used to generate predictions.</p>

<p>During the prediction phase, the model receives a new input observation (for example, a bug report) and transforms the documents in the same way as applied during the training phase to produce the feature vectors. The new feature vectors are fed into the trained model to generate the prediction (for example, a bug priority). In this manner we have achieved an automated way of predicting a label.</p>

<figure><div id="fig-supervised-learning" class="figure"><img src="Images/btap_0601.jpg" width="1401" height="680"/>
<h6><span class="label">Figure 6-1. </span>Workflow of a supervised learning algorithm used for classification.</h6>
</div></figure>

<p>Text classification <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term9" id="idm45634195365464"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term10" id="idm45634195364056"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term11" id="idm45634195362680"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with text classification" data-secondary-sortas="text classification" id="idm45634195361304"/>is an example of a supervised learning algorithm where we use text data and NLP techniques such as text vectorization to assign a categorical target variable to a given document. Classification algorithms can be characterized into the following categories:</p>

<dl>
  <dt>Binary classification</dt>
  <dd>This is <a contenteditable="false" data-type="indexterm" data-primary="binary classification" id="idm45634195357864"/>actually a special case of multiclass classification where an observation can have any one of two values (binary). For example, a given email can be marked as spam or not spam. But each observation will have only one label.</dd>

  <dt>Multiclass classification</dt>
  <dd>In this type of <a contenteditable="false" data-type="indexterm" data-primary="multiclass classification" id="idm45634195355528"/>classification algorithm, each observation is associated with one label. For example, a bug report can have a single value of priority from any of the five categories P1, P2, P3, P4, or P5. Similarly, when attempting to identify the software component that a bug is reported in, each bug can be in one of six categories (UI, Core, Debug, Text, APT, or Doc).</dd>

  <dt>Multilabel classification</dt>
  <dd>In this type of <a contenteditable="false" data-type="indexterm" data-primary="multilabel classification" id="idm45634195353048"/>classification algorithm each observation can be assigned to multiple labels. For example, a single news article could be tagged with multiple labels, such as Security, Tech, and Blockchain. Several strategies can be used to solve a multilabel classification problem, including the use of multiple binary classification models to generate the final result, but we will not cover this in our blueprint.</dd>
</dl>

<section data-type="sect2" data-pdf-bookmark="Step 1: Data Preparation"><div class="sect2" id="idm45634195351176">
<h2>Step 1: Data Preparation</h2>

<p>Before proceeding to <a contenteditable="false" data-type="indexterm" data-primary="data preprocessing" data-secondary="for text classification" data-secondary-sortas="text classification" id="idm45634195349640"/>build the text classification model, we must perform some necessary preprocessing steps to clean the data and format it in a manner that is suitable for the application of machine learning algorithms. Since our objective is to identify the priority of a bug report given its title and description, we select only those columns that are relevant for the text classification model. We also remove any rows that contain empty values using the <code>dropna</code> function. Finally, we combine the title and description columns to create a single text value and apply the text cleaning blueprint from <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a> to remove special characters. After removing the special characters, we filter out those observations that have fewer than 50 characters in the text field. These bug reports have not been filled out correctly and contain very little description of the problem and are not helpful in training the model:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s1">'Title'</code><code class="p">,</code><code class="s1">'Description'</code><code class="p">,</code><code class="s1">'Priority'</code><code class="p">]]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>
<code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'Title'</code><code class="p">]</code> <code class="o">+</code> <code class="s1">' '</code> <code class="o">+</code> <code class="n">df</code><code class="p">[</code><code class="s1">'Description'</code><code class="p">]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'Title'</code><code class="p">,</code><code class="s1">'Description'</code><code class="p">])</code>
<code class="n">df</code><code class="o">.</code><code class="n">columns</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting" data-code-language="python">
	<code class="n">Index</code><code class="p">([</code><code class="s1">'Priority'</code><code class="p">,</code> <code class="s1">'text'</code><code class="p">],</code> <code class="n">dtype</code><code class="o">=</code><code class="s1">'object'</code><code class="p">)</code>
</pre>

Then:

<pre data-code-language="python" data-type="programlisting">
	<code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">clean</code><code class="p">)</code>
	<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">len</code><code class="p">()</code> <code class="o">&gt;</code> <code class="mi">50</code><code class="p">]</code>
	<code class="n">df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>


<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>Priority</th>
			<th>text</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>28311</th>
			<td>P3</td>
			<td>Need to re-run APT on anti-dependencies when files are generated If a generated file satisfies a missing type in another file we should rerun APT on the file which would be fixed by the new type. Currently java compilation does the correct thing but APT does not. Need to keep track of files with missing types and recompile at the end of the round if new types are generated. For good perf need to track the names and only compile those missing types that were generated</td>
		</tr>
		<tr>
			<th>25026</th>
			<td>P2</td>
			<td>Externalize String wizard: usability improvements M6 Test pass Since most of the Java developers will not be faces with the Eclipses mode I would move the check box down to the area of the Accessor class. Furthermore the wizard shouldnt provide the option if org.eclipse.osgi.util.NLS isnt present in the workspace. This will avoid that normal Java developers are faces with the option at all</td>
		</tr>
	</tbody>
</table>

<p>We can see from the preceding summary of the text feature for two bug reports that our cleaning steps have removed a lot of special characters; we still have retained a lot of the code structure and statements that form part of the description. This is useful information that the model can use to understand the bug and will have an impact on whether it belongs to a higher priority.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Step 2: Train-Test Split"><div class="sect2" id="idm45634195180952">
<h2>Step 2: Train-Test Split</h2>

<p>During the <a contenteditable="false" data-type="indexterm" data-primary="supervised learning models" id="idm45634195179256"/><a contenteditable="false" data-type="indexterm" data-primary="train-test split" id="ch6_term16"/>process of training a supervised learning model, we are attempting to learn a function that most closely resembles the real-world behavior. We use the information available in the training data to learn this function. Afterward, it is important to evaluate how close our learned function is to the real world, and we split our entire data into train and test splits to achieve this. We split the data, typically using a percentage, with the larger share assigned to the train split. For example, if we have a dataset with 100 observations and apply a train-test split in the ratio of 80-20, then 80 observations will become part of the train split and 20 observations will become part of the test split. The model is now trained on the train split, which uses only the 80 observations to learn the function. We will use the <a contenteditable="false" data-type="indexterm" data-primary="test split" id="idm45634195175800"/>test split of 20 observations to evaluate the learned function. An illustration of this is shown in <a data-type="xref" href="#fig-train-test-split">Figure 6-2</a>.</p>

<p>During training phase:</p>

<div data-type="equation">
  <p><math alttext="y Subscript t r a i n Baseline equals upper F left-parenthesis upper X Subscript t r a i n Baseline right-parenthesis">
  <mrow>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow> </msub>
    <mo>=</mo>
    <mi>F</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>X</mi> <mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></p>
</div>

<p>During evaluation:</p>

<div data-type="equation">
  <p><math alttext="y Subscript p r e d i c t i o n Baseline equals upper F left-parenthesis upper X Subscript t e s t Baseline right-parenthesis">
  <mrow>
    <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow> </msub>
    <mo>=</mo>
    <mi>F</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>X</mi> <mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math></p>
</div>

<figure><div id="fig-train-test-split" class="figure"><img src="Images/btap_0602.jpg" width="1346" height="597"/>
	<h6><span class="label">Figure 6-2. </span>A train-test split in the ratio 80-20.</h6>
	</div></figure>

<p>The model has seen only the 80 observations in the train split, and the learned function is now applied on a completely independent and unseen test split to generate the predictions. We know the real values of the target variable in the test split, and comparing these with the predictions will give us a true measure of how well the learned function performs and how close it is to real-world behavior:</p>

<div data-type="equation">
  <p><math alttext="a c c u r a c y equals e r r o r normal bar m e t r i c left-parenthesis y Subscript p r e d i c t i o n Baseline comma y Subscript t r u e Baseline right-parenthesis">
  <mrow>
    <mi>a</mi>
    <mi>c</mi>
    <mi>c</mi>
    <mi>u</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>c</mi>
    <mi>y</mi>
    <mo>=</mo>
    <mi>e</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mo>_</mo>
    <mi>m</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>c</mi>
    <mo>(</mo>
    <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow> </msub>
    <mo>,</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> </msub>
    <mo>)</mo>
  </mrow>
</math></p>
</div>

<p>Evaluating the learned model on the test split provides an unbiased estimate of the error of the text classification model since the observations in the test split have been randomly sampled from the training observations and are not part of the learning process. The test split will be used during model evaluation, and there are several metrics that can be used to measure this error, which will be discussed in <a data-type="xref" href="#ch06step4modeleval">“Step 4: Model Evaluation”</a>.</p>

<p>We use the <code>sklearn.model_selection.train_test_split</code> function to implement the train-test split, and we provide 0.2 as the argument for the <code>test_size</code> (denoting 20% of our data as our test split). In addition, we must also specify our independent and target variables, and the method returns to us a list of four elements; the first two elements are the independent variables split into train and test splits, and the next two elements are the target variable splits. One important argument of the function to note is the <code>random_state</code>. This number influences how the rows are sampled and therefore which set of observations goes to the train split and which set of observations goes to the test split. If you provide a different number, the 80-20 split will remain the same, but a different selection of observations will go to the train and test splits. It’s important to remember that to reproduce the same results you must choose the same value of the <code>random_state</code>. For example, if you want to check what happens to the model on adding a new independent variable, you must be able to compare the accuracy before and after adding the new variable. Therefore, you must use the same <code>random_state</code> so that you can determine whether a change occurred. The last <a contenteditable="false" data-type="indexterm" data-primary="stratify parameter" id="idm45634195105672"/>parameter to take note of is <code>stratify</code>, which ensures that the distribution of the target variable is maintained in the train and test splits. If this is not maintained, then the training split can have a much higher number of observations of a certain class, which does not reflect the distribution in the training data and leads to the model learning an <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term16" id="idm45634195103688"/>unrealistic function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">],</code>
                                                    <code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">],</code>
                                                    <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>
                                                    <code class="n">stratify</code><code class="o">=</code><code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">])</code>

<code class="k">print</code><code class="p">(</code><code class="s1">'Size of Training Data '</code><code class="p">,</code> <code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Size of Test Data '</code><code class="p">,</code> <code class="n">X_test</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Size of Training Data  36024
Size of Test Data  9006
</pre>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Step 3: Training the Machine Learning Model"><div class="sect2" id="idm45634195180328">
<h2>Step 3: Training the Machine Learning Model</h2>

<p>Our next step in creating the <a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="training of" id="ch6_term17"/><a contenteditable="false" data-type="indexterm" data-primary="SVM (support vector machine) algorithm" id="ch6_term18"/><a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="of machine learning models" data-secondary-sortas="machine learning models" id="ch6_term20"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with text classification" data-secondary-sortas="text classification" id="ch6_term21"/>text classification blueprint is to train a supervised machine learning model using a suitable algorithm. SVM is one of the popular algorithms used when working with text classification, and we will first provide an introduction to the method and then illustrate why it’s well-suited to our task.</p>

<p>Consider a set of points in the X-Y plane with each point belonging to one of two classes: cross or circle, as represented in <a data-type="xref" href="#fig-svm-illustration">Figure 6-3</a>. The SVM works by choosing a line that clearly separates the two classes. Of course, there could be several such lines (shown by the dotted options), and the algorithm chooses the line that provides the maximum separation between the closest cross and circle points (identified with a box around them). These <a contenteditable="false" data-type="indexterm" data-primary="support vectors" id="idm45634195035816"/>closest cross and circle points are referred to as <em>support vectors</em>. In the illustration, we are able to identify a hyperplane that clearly separates the cross and circle points, but in reality, it might be difficult to achieve this. For example, there may be a few circle points that lie on the extreme left, and it would be impossible to then generate a hyperplane. The algorithm manages this with the tolerance parameter <code>tol</code> that allows for some flexibility and accepts an error in the form of misclassified points when deciding a hyperplane.</p>

<figure><div id="fig-svm-illustration" class="figure"><img src="Images/btap_0603.jpg" width="1362" height="711"/>
<h6><span class="label">Figure 6-3. </span>Hyperplane and support vectors in a simple two-dimensional classification example.</h6>
</div></figure>

<p>Before proceeding to run the SVM model, we must prepare our text data in a suitable format that can be used by the algorithm. This means that we must find a way to represent text data in a numeric format. The simplest way is to count the number of times each word occurs in a bug report and combine the counts of all words to create a numeric representation for each observation. This technique has the disadvantage that commonly occurring words will have large values and could be understood as important features when this is not true. Therefore, we use the <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="vectorization with" id="idm45634195030664"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with TF-IDF weighting" data-secondary-sortas="TF-IDF weighting" id="idm45634195029320"/>preferred option of representing the text using a Term-Frequency Inverse Document Frequency (TF-IDF) vectorization, which is explained in more detail in <a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">min_df</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code> <code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="n">stop_words</code><code class="o">=</code><code class="s2">"english"</code><code class="p">)</code>
<code class="n">X_train_tf</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
</pre>

<p>The TF-IDF vectorization performed in the previous step results in a sparse matrix. The SVM algorithm is preferred when working with text data because it is more suited to work with sparse data compared to other algorithms like <a href="https://oreil.ly/uFkYZ">Random Forest</a>. They are also better suited to work with input features that are purely numeric (as in our case), while other algorithms are capable of handling a mixture of numeric and categorical input features. For <a contenteditable="false" data-type="indexterm" data-primary="sklearn.svm.LinearSVC module " id="idm45634194964808"/>our text classification model we will use the <code>sklearn.svm.LinearSVC</code> module that is provided by the scikit-learn library. SVMs can actually be initialized with different kernel functions, and the linear kernel is recommended for use with text data as there are a large number of features that can be considered linearly separable. It is also faster to fit since it has fewer parameters to optimize. The scikit-learn package provides different implementations of a linear SVM, and if you are interested, you can learn the differences between them as described in
<!-- Uncomment if these end up on the same page after page breaking-->
<a data-type="xref" href="#svcvslinearsvcvssgdclassif">“SVC Versus LinearSVC Versus SGDClassifier”</a>.</p>
<!-- <a href="#svcvslinearsvcvssgdclassif" data-type='xref' data-xrefstyle='select:nopage'>#svcvslinearsvcvssgdclassif</a>.</p> -->

<p>In the following code, we initialize the model with a certain <code>random_state</code> and specify a tolerance value of 0.00001. The arguments are specific to the type of model we use, and we will show later in this chapter how we can arrive at the optimal parameter values for these arguments. For now we start by specifying some default values and then call the <code>fit</code> method, making sure to use the vectorized independent variables that we created in the previous step:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model1</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="mf">1e-5</code><code class="p">)</code>
<code class="n">model1</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_tf</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting" data-code-language="python">
<code class="n">LinearSVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">class_weight</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">dual</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">fit_intercept</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
          <code class="n">intercept_scaling</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">loss</code><code class="o">=</code><code class="s1">'squared_hinge'</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code>
          <code class="n">multi_class</code><code class="o">=</code><code class="s1">'ovr'</code><code class="p">,</code> <code class="n">penalty</code><code class="o">=</code><code class="s1">'l2'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="mf">1e-05</code><code class="p">,</code>
          <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
</pre>

<p>On executing the preceding code, we fit a model using the training data, and the result shows us the various parameters of the model that was generated. Most of these are the default values since we <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term17" id="idm45634194862312"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term18" id="idm45634194841224"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term20" id="idm45634194839848"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term21" id="idm45634194838472"/>specified only the <code>random_state</code> and tolerance.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="svcvslinearsvcvssgdclassif">
<h5>SVC Versus LinearSVC Versus SGDClassifier</h5>

<p><code>sklearn.svm.SVC</code> is the <a contenteditable="false" data-type="indexterm" data-primary="sklearn.svm.SVC model" id="idm45634194834312"/>generic implementation of the support vector machine algorithm provided by scikit-learn. This can be used to build models with different kernel functions, including linear, polynomial, and radial basis functions. The <code>sklearn.svm.LinearSVC</code> is a specific implementation of a <a contenteditable="false" data-type="indexterm" data-primary="LinearSVC model training" id="idm45634194832392"/>linear SVM. Ideally, it should produce the same results as an SVC with linear kernel. However, the key difference is that LinearSVC uses the <a href="https://oreil.ly/5UzQ8">liblinear implementation</a>, while SVC is based on the <a href="https://oreil.ly/IR1Ji">libsvm</a> implementation. Both of them are popular open source libraries in <span class="keep-together">C++</span> that implement the SVM algorithm but use different approaches. LinearSVC is much faster, whereas SVC is more generic and supports multiple kernels. <code>sklearn.linear_model.SGDClassifier</code> is <a contenteditable="false" data-type="indexterm" data-primary="SGDClassifier" id="idm45634194828376"/>actually an optimization algorithm called <em>stochastic gradient descent</em> (SGD), and is used to optimize a given objective function. When we specify the loss of an SGDClassifier to “hinge,” this equates to a linear SVM and should arrive at the same result. Again, the approach is different, and therefore the results may not be the same. To summarize, all three methods can be used to implement an SVM with a linear kernel, but LinearSVC is normally the fastest, whereas the other two methods are more generic.</p>
</div></aside>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Step 4: Model Evaluation"><div class="sect2" id="ch06step4modeleval">
<h2>Step 4: Model Evaluation</h2>

<p>We now have a <a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="evaluation of" id="ch6_term23"/>model that can be used to predict the target variable for all the observations in the test split. For these observations, we also know the real target variable, and therefore we can calculate the performance of our model. There are many <span class="keep-together">metrics</span> that can be used to quantify the accuracy of our model, and we will introduce three of them in this section.</p>

<p>The simplest <a contenteditable="false" data-type="indexterm" data-primary="accuracy metrics" id="idm45634194820728"/>way to validate our text classification model is accuracy: the ratio of the number of predictions that the model got right to the total number of observations. This can be expressed mathematically as follows:</p>

<div data-type="equation">
  <p><math alttext="upper A c c u r a c y equals StartFraction upper N u m b e r o f c o r r e c t p r e d i c t i o n s Over upper T o t a l n u m b e r o f p r e d i c t i o n s m a d e EndFraction">
  <mrow>
    <mi>A</mi>
    <mi>c</mi>
    <mi>c</mi>
    <mi>u</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>c</mi>
    <mi>y</mi>
    <mo>=</mo>
    <mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mspace width="4pt"/><mi>o</mi><mi>f</mi><mspace width="4pt"/><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mspace width="4pt"/><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow> <mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mspace width="4pt"/><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mspace width="4pt"/><mi>o</mi><mi>f</mi><mspace width="4pt"/><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mspace width="4pt"/><mi>m</mi><mi>a</mi><mi>d</mi><mi>e</mi></mrow></mfrac>
  </mrow>
</math></p>
</div>

<p>To measure the accuracy of the model, we use the trained model to generate predictions and compare with the real values. To generate the predictions, we must apply the same vectorization to the test split of the independent variable and then call the predict method of the trained model. Once we have the predictions, we can use the <code>accuracy_score</code> method shown next that automatically generates this metric by comparing the true values and the model predictions of the test split:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">X_test_tf</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>

<code class="n">Y_pred</code> <code class="o">=</code> <code class="n">model1</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_tf</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Accuracy Score - '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Accuracy Score -  0.8748612036420165
</pre>

<p>As you can see, we have achieved a high accuracy score of 87.5%, which indicates that we have a good model that is able to predict the priority of bugs accurately. Please note that if you initialized the model with a different <code>random_state</code>, you might not get the same score, but it would be similar. It is always a good idea to compare the performance of a trained model with a simple baseline approach that could be based on simple rules of thumb or business knowledge. The objective is to check whether the trained model performs better than the baseline and therefore adds value. We can <a contenteditable="false" data-type="indexterm" data-primary="sklearn.svm.DummyClassifier module" id="idm45634194730584"/>use the <code>sklearn.svm.DummyClassifier</code> module, which provides simple strategies like <code>most_frequent</code>, where the baseline model always predicts the class with highest frequency, or which is <code>stratified</code>, which generates predictions that respect the training data distribution:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">clf</code> <code class="o">=</code> <code class="n">DummyClassifier</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'most_frequent'</code><code class="p">)</code>
<code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="n">Y_pred_baseline</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Accuracy Score - '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred_baseline</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Accuracy Score -  0.8769709082833667
</pre>

<p>We can clearly see that our trained model is not adding any value since it performs just as well as a baseline that always chooses the class P3. Another aspect that we must dig deeper to investigate is how well the model is performing for the different priority levels. Is it better at predicting priority P1 or P5? To analyze this, we can use <a contenteditable="false" data-type="indexterm" data-primary="confusion matrix" id="idm45634194665368"/>another evaluation tool known as the <em>confusion matrix</em>. The confusion matrix is a grid that compares the predicted values with the actual values for all the classified observations. The most common representation of a confusion matrix is for a binary classification problem with only two labels.</p>

<p>We can modify our multiclass classification problem to suit this representation by considering one class as P3 and the other class as all of the rest. Let’s look at <a data-type="xref" href="#fig-confusion-matrix">Figure 6-4</a>, a sample representation of the confusion matrix that predicts only whether a particular bug has a priority P3 or not.</p>

<figure><div id="fig-confusion-matrix" class="figure"><img src="Images/btap_0604.jpg" width="982" height="524"/>
<h6><span class="label">Figure 6-4. </span>Confusion matrix for priority P3 and not P3.</h6>
</div></figure>

<p>The rows depict the predictions, and the columns depict the actual values. Each slot in the matrix is the count of observations falling in that slot:</p>

<dl>
<dt>True Positive</dt>
	<dd>The count of those observations that were predicted to be positive and are indeed positive.</dd>
<dt>True Negative</dt>
	<dd>The count of those observations that were predicted to be negative and are indeed negative.</dd>
<dt>False Positive</dt>
	<dd>The count of those observations that were predicted to be positive but are actually negative.</dd>
<dt>False Negative</dt>
	<dd>The count of those observations that were predicted to be negative but are actually positive.</dd>
</dl>

<p class="pagebreak-before">Based on this list, we can automatically derive the accuracy measure using the following equation:</p>

<div data-type="equation">
  <p><math alttext="upper A c c u r a c y equals StartFraction left-parenthesis upper T r u e upper P o s i t i v e plus upper T r u e upper N e g a t i v e right-parenthesis Over left-parenthesis upper T r u e upper P o s i t i v e plus upper T r u e upper N e g a t i v e plus upper F a l s e upper P o s i t i v e plus upper F a l s e upper N e g a t i v e right-parenthesis EndFraction">
  <mrow>
    <mi>A</mi>
    <mi>c</mi>
    <mi>c</mi>
    <mi>u</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>c</mi>
    <mi>y</mi>
    <mo>=</mo>
    <mfrac><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mspace width="4pt"/><mo>+</mo><mspace width="4pt"/><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow> <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mspace width="4pt"/><mo>+</mo><mspace width="4pt"/><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mspace width="4pt"/><mo>+</mo><mspace width="4pt"/><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mspace width="4pt"/><mo>+</mo><mspace width="4pt"/><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mspace width="4pt"/><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac>
  </mrow>
</math></p>
</div>

<p>This is nothing but a ratio of all the predictions that were correct and the total number of predictions.</p>

<section data-type="sect3" data-pdf-bookmark="Precision and recall"><div class="sect3" id="idm45634194591416">
<h3>Precision and recall</h3>

<p>The real value of using the confusion matrix is in other <a contenteditable="false" data-type="indexterm" data-primary="precision and recall" id="ch6_term25"/><a contenteditable="false" data-type="indexterm" data-primary="recall" id="ch6_term26"/>measures like Precision and Recall, which give us more insight into how the model performs for different classes.</p>

<p>Let’s take the positive (P3) class and consider the Precision:</p>

<div data-type="equation">
  <p><math alttext="upper P r e c i s i o n equals StartFraction upper T r u e upper P o s i t i v e Over left-parenthesis upper T r u e upper P o s i t i v e plus upper F a l s e upper P o s i t i v e right-parenthesis EndFraction">
  <mrow>
    <mi>P</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mspace width="4pt"/><mo>+</mo><mspace width="4pt"/><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac>
  </mrow>
</math></p>
</div>

<p>This metric tells us what proportion of predicted positives is actually positive, or how accurate our model is at predicting the positive class. If we want to be sure of our positive predictions, then this is a metric we must maximize. For example, if we are classifying emails as spam (positive), then we must be  accurate at this; otherwise, a good email might accidentally be sent to the spam folder.</p>

<p>Another metric that is derived from the confusion matrix is Recall:</p>

<div data-type="equation">
  <p><math alttext="upper R e c a l l equals StartFraction upper T r u e upper P o s i t i v e Over left-parenthesis upper T r u e upper P o s i t i v e plus upper F a l s e upper N e g a t i v e right-parenthesis EndFraction">
  <mrow>
    <mi>R</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>l</mi>
    <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mspace width="4pt"/><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mspace width="4pt"/><mo>+</mo><mspace width="4pt"/><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mspace width="4pt"/><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac>
  </mrow>
</math></p>
</div>

<p>This metric tells us what proportion of real positive values is actually identified by our model. A high recall means that our model is able to capture most of the positive classifications in reality. This is especially important when the cost of not identifying a positive case is very high, for example, if a patient has cancer but our model does not identify it.</p>

<p>From the previous discussion, we can conclude that both precision and recall are important metrics depending on the application of the model. <a contenteditable="false" data-type="indexterm" data-primary="F1 score" id="idm45634194542648"/>The <em>F1 score</em> is a metric that creates a harmonic mean of both of these measures and can also be used as a proxy to evaluate the overall accuracy of the model:</p>

<div data-type="equation">
  <p><math alttext="upper F Baseline 1 upper S c o r e equals StartFraction 2 asterisk left-parenthesis upper P r e c i s i o n asterisk upper R e c a l l right-parenthesis Over left-parenthesis upper P r e c i s i o n plus upper R e c a l l right-parenthesis EndFraction">
  <mrow>
    <mi>F</mi>
    <mn>1</mn>
    <mspace width="4pt"/>
    <mi>S</mi>
    <mi>c</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mo>=</mo>
    <mfrac><mrow><mn>2</mn><mspace width="4pt"/><mo>*</mo><mspace width="4pt"/><mo>(</mo><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mspace width="4pt"/><mo>*</mo><mspace width="4pt"/><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>)</mo></mrow> <mrow><mo>(</mo><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mspace width="4pt"/><mo>+</mo><mspace width="4pt"/><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>)</mo></mrow></mfrac>
  </mrow>
</math></p>
</div>

<p>Now that we have developed an understanding of the confusion matrix, let’s come back to our blueprint and add the step to evaluate the confusion matrix of the trained model. Note that the earlier representation was simplified as a binary classification, whereas our model is actually a multiclass classification problem, and therefore the confusion matrix will change accordingly. For example, the confusion matrix for our model can be generated with the function <code>confusion_matrix</code>, as shown here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">Y_pred</code> <code class="o">=</code> <code class="n">model1</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_tf</code><code class="p">)</code>
<code class="n">confusion_matrix</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
array([[  17,    6,  195,    5,    0],
       [   7,   14,  579,    7,    0],
       [  21,   43, 7821,   13,    0],
       [   0,    7,  194,   27,    0],
       [   0,    0,   50,    0,    0]])
</pre>

<p>This can also be <a contenteditable="false" data-type="indexterm" data-primary="heatmaps" id="idm45634194506936"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with heatmaps" data-secondary-sortas="heatmaps" id="idm45634194498936"/>visualized in the form of a heatmap by using the <code>plot_confusion_matrix</code> function as shown here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">plot_confusion_matrix</code><code class="p">(</code><code class="n">model1</code><code class="p">,</code><code class="n">X_test_tf</code><code class="p">,</code>
                      <code class="n">Y_test</code><code class="p">,</code> <code class="n">values_format</code><code class="o">=</code><code class="s1">'d'</code><code class="p">,</code>
                      <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">Blues</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
</pre>

<figure><div class="figure"><img src="Images/btap_06in02.jpg" class="width-60" width="322" height="253"/>
<h6/>
</div></figure>

<p class="">We can define the precision and recall for each category using the same methodology as described earlier but will now include the count of observations that were incorrectly classified into other categories as well.</p>
<p class="pagebreak-before">For example, the precision of the category P3 can be calculated as the ratio of correctly predicted P3 values (7,821) and all predicted P3 values (195 + 579 + 7,821 + 194 + 50), resulting in the following:</p>

<div data-type="equation">
  <p><em>Precision (P3)</em> = 7,821 / 8,839 = 0.88</p>
</div>

<p>Similarly, the recall for P3 can be calculated as the ratio of correctly predicted P3 values and all actual P3 values (21 + 43 + 7,821 + 13 + 0), resulting in the following:</p>

<div data-type="equation">
  <p><em>Recall (P2)</em> = 7,821 / 7,898 = 0.99</p>
</div>

<p>An easier way to determine these measures directly is to use the <code>classification_report</code> function from scikit-learn that automatically calculates these values <span class="keep-together">for us</span>:</p><a contenteditable="false" data-type="indexterm" data-primary="classification_report function" id="idm45634194442024"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="classification_report function  of" id="idm45634194440952"/>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
              precision    recall  f1-score   support

          P1       0.38      0.08      0.13       223
          P2       0.20      0.02      0.04       607
          P3       0.88      0.99      0.93      7898
          P4       0.52      0.12      0.19       228
          P5       0.00      0.00      0.00        50

    accuracy                           0.87      9006
   macro avg       0.40      0.24      0.26      9006
weighted avg       0.81      0.87      0.83      9006
</pre>

<p>Based on our calculations and the previous classification report, one issue becomes glaringly obvious: while the recall and precision values for the class P3 are quite high, these values for the other classes are low and even 0 in some cases (P5). The overall accuracy of the model is 88%, but if we hard-coded our prediction to always be P3, this would also be correct 88% of the time. This makes it clear that our model has not learned much of significance and is merely predicting the majority class. This highlights the fact that during model evaluation we must analyze several metrics and not rely on the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term25" id="idm45634194431080"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term26" id="idm45634194429736"/>accuracy alone.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Class imbalance"><div class="sect3" id="idm45634194590504">
<h3>Class imbalance</h3>

<p>The reason <a contenteditable="false" data-type="indexterm" data-primary="class imbalance" id="ch6_term28"/>for the model to behave in this manner is due to the <em>class imbalance</em> in the priority classes that we observed earlier. While there were close to 36,000 bugs with a priority of P3, the number of bugs with other priority classes was only about 4,000 and even fewer in other cases. This means that when we trained our model, it was able to learn the characteristics of the P3 class alone.</p>

<p>There are <a contenteditable="false" data-type="indexterm" data-primary="downsampling techniques" id="idm45634194381704"/><a contenteditable="false" data-type="indexterm" data-primary="upsampling techniques" id="idm45634194380568"/>several techniques we can use to overcome the issue of class imbalance. They belong to two categories of upsampling and downsampling techniques. Upsampling techniques refer to methods used to artificially increase the number of observations of the minority class (non-P3 classes in our example). These techniques can vary from simply adding multiple copies to generating new observations using a method like SMOTE.<sup><a data-type="noteref" id="idm45634194378904-marker" href="ch06.xhtml#idm45634194378904">1</a></sup> Downsampling techniques refer to methods that are used to reduce the number of observations of the majority class (P3 in our example). We will choose to randomly downsample the P3 class to have a similar number of observations as the other classes:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Filter bug reports with priority P3 and sample 4000 rows from it</code>
<code class="n">df_sampleP3</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'P3'</code><code class="p">]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="mi">4000</code><code class="p">)</code>

<code class="c1"># Create a separate DataFrame containing all other bug reports</code>
<code class="n">df_sampleRest</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">]</code> <code class="o">!=</code> <code class="s1">'P3'</code><code class="p">]</code>

<code class="c1"># Concatenate the two DataFrame to create the new balanced bug reports dataset</code>
<code class="n">df_balanced</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">df_sampleRest</code><code class="p">,</code> <code class="n">df_sampleP3</code><code class="p">])</code>

<code class="c1"># Check the status of the class imbalance</code>
<code class="n">df_balanced</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
P3    4000
P2    3036
P4    1138
P1    1117
P5    252
Name: Priority, dtype: int64
</pre>

<p>Please note that in performing the downsampling, we are losing information, and this is not generally a good idea. However, whenever we come across a class imbalance problem, this prevents our model from learning the right information. We try to overcome this by using upsampling and downsampling techniques, but this will always involve a compromise with regard to data quality. While we have chosen a simplistic approach, please see the following sidebar to understand various ways to deal with the situation.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634194318264">
<h5>Dealing with Class Imbalance</h5>

<p>One of the straightforward ways of dealing with class imbalance is by randomly upsampling or downsampling. However, there are several more creative ways to deal with this situation, the <a contenteditable="false" data-type="indexterm" data-primary="synthetic-minority oversampling technique (SMOTE)" id="idm45634194316792"/><a contenteditable="false" data-type="indexterm" data-primary="SMOTE (synthetic-minority oversampling technique)" id="idm45634194315592"/>first of which is the <em>synthetic-minority oversampling technique</em> (SMOTE). Using this method we don’t just create copies of the same observation but instead synthetically generate new observations that are similar to the minority class. From the <a href="https://oreil.ly/Se_qS">paper</a>, we have the following simple description:</p>

<blockquote>
  <p>Take the difference between the <a contenteditable="false" data-type="indexterm" data-primary="feature vectors" id="idm45634194312232"/>feature vector (sample) under consideration and its nearest neighbor. Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two specific features.</p>
</blockquote>

<p>This creates new samples that lie close to existing minority samples but are not exactly the same. The <a contenteditable="false" data-type="indexterm" data-primary="imbalanced-learn package (Python)" id="idm45634194310040"/>most useful Python package for dealing with class imbalance is <span class="keep-together"><a href="https://oreil.ly/tuzV3"><em>imbalanced-learn</em></a></span>, which is also compatible with scikit-learn. In addition to the methods discussed, it also provides additional sampling techniques such as <em>NearMiss</em> (for undersampling using <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term23" id="idm45634194306920"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term28" id="idm45634194305576"/>nearest neighbors).</p>
</div></aside>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Final Blueprint for Text Classification"><div class="sect1" id="idm45634195416472">
<h1>Final Blueprint for Text Classification</h1>

<p>We will now combine all the steps we have listed so far to create our blueprint for text classification:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Loading the balanced DataFrame</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">df_balanced</code><code class="p">[[</code><code class="s1">'text'</code><code class="p">,</code> <code class="s1">'Priority'</code><code class="p">]]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>

<code class="c1"># Step 1 - Data Preparation</code>

<code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">clean</code><code class="p">)</code>

<code class="c1"># Step 2 - Train-Test Split</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">],</code>
                                                    <code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">],</code>
                                                    <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>
                                                    <code class="n">stratify</code><code class="o">=</code><code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Size of Training Data '</code><code class="p">,</code> <code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Size of Test Data '</code><code class="p">,</code> <code class="n">X_test</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>

<code class="c1"># Step 3 - Training the Machine Learning model</code>

<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">min_df</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">stop_words</code><code class="o">=</code><code class="s2">"english"</code><code class="p">)</code>
<code class="n">X_train_tf</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>

<code class="n">model1</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="mf">1e-5</code><code class="p">)</code>
<code class="n">model1</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_tf</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>

<code class="c1"># Step 4 - Model Evaluation</code>

<code class="n">X_test_tf</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="n">Y_pred</code> <code class="o">=</code> <code class="n">model1</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test_tf</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Accuracy Score - '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Size of Training Data  7634
Size of Test Data  1909
Accuracy Score -  0.4903090623363017
              precision    recall  f1-score   support

          P1       0.45      0.29      0.35       224
          P2       0.42      0.47      0.44       607
          P3       0.56      0.65      0.61       800
          P4       0.39      0.29      0.33       228
          P5       0.00      0.00      0.00        50

    accuracy                           0.49      1909
   macro avg       0.37      0.34      0.35      1909
weighted avg       0.47      0.49      0.48      1909
</pre>

<p>Based on the results, we can see that our accuracy is now at 49%, which is not good. Analyzing further, we can see that <a contenteditable="false" data-type="indexterm" data-primary="precision and recall" id="idm45634194297800"/><a contenteditable="false" data-type="indexterm" data-primary="recall" id="idm45634194104200"/>precision and recall values have improved for priority P1 and P2, indicating that we are able to better predict bugs with this priority. However, it’s also obvious that for bugs with priority P5, this model does not offer anything. We see that this model does perform better than a simple baseline using a <em>stratified</em> strategy, as shown next. Even though the earlier model had a higher accuracy, it wasn’t actually a good model because it was ineffective. This model is also not good but at least presents a true picture and informs us that we must not use it for generating predictions:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">clf</code> <code class="o">=</code> <code class="n">DummyClassifier</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'stratified'</code><code class="p">)</code>
<code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="n">Y_pred_baseline</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Accuracy Score - '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred_baseline</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Accuracy Score -  0.30434782608695654
</pre>

<p class="pagebreak-before">The following are some examples of where our model predictions for these priorities are accurate:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Create a DataFrame combining the Title and Description,</code>
<code class="c1"># Actual and Predicted values that we can explore</code>
<code class="n">frame</code> <code class="o">=</code> <code class="p">{</code> <code class="s1">'text'</code><code class="p">:</code> <code class="n">X_test</code><code class="p">,</code> <code class="s1">'actual'</code><code class="p">:</code> <code class="n">Y_test</code><code class="p">,</code> <code class="s1">'predicted'</code><code class="p">:</code> <code class="n">Y_pred</code> <code class="p">}</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">frame</code><code class="p">)</code>

<code class="n">result</code><code class="p">[((</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'P1'</code><code class="p">)</code> <code class="o">|</code> <code class="p">(</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'P2'</code><code class="p">))</code> <code class="o">&amp;</code>
       <code class="p">(</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">==</code> <code class="n">result</code><code class="p">[</code><code class="s1">'predicted'</code><code class="p">])]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>text</th>
			<th>actual</th>
			<th>predicted</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>64</th>
			<td>Java launcher: Dont prompt for element to launch if theres only one I went to debug a CU by selecting it and clicking the debug tool item. I was prompted to select a launcher and I also had to select the only available class on the second page. The second step shouldnt be necessary. The next button on the first page should be disabled. NOTES: DW The first time you launch something in your workspace you must go through this pain...This is due to the debugger being pluggable for different lauguages. In this case the launcher selection is generic debug support and the choosing of a class to launch is java specific debug support. To promote lazy plugin loading and to avoid launchers doing exhaustive searching for launchable targets the launcher selection page does not poll the pluggable launch page to see if it can finish with the current selection. Once you have selected a defualt launcher for a project the launcher selection page will not bother you again. Moved to inactive for post-June consideratio</td>
			<td>P2</td>
			<td>P2</td>
		</tr>
		<tr>
			<th>5298</th>
			<td>Rapid stepping toString When you do rapid stepping and have an object selected displaying details we get exceptions in the log. This is because the toString attempts an evaluation while a step is in progress. We have to allow stepping during evaluations so this is a tricky timing issue. &lt;/log-entr</td>
			<td>P1</td>
			<td>P1</td>
		</tr>
	</tbody>
</table>

<p>Here are some cases where the model prediction is inaccurate:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">result</code><code class="p">[((</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'P1'</code><code class="p">)</code> <code class="o">|</code> <code class="p">(</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'P2'</code><code class="p">))</code> <code class="o">&amp;</code>
       <code class="p">(</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">!=</code> <code class="n">result</code><code class="p">[</code><code class="s1">'predicted'</code><code class="p">])]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>text</th>
			<th>actual</th>
			<th>predicted</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>4707</th>
			<td>Javadoc wizard: Problems with default package 20020328 1. empty project. create A.java in default package 2. Start export wizard select the default package press Finish 3. Creation fails javadoc: No source files for package A Loading source files for package A... 1 error Dont know if this is a general javadoc probl</td>
			<td>P1</td>
			<td>P2</td>
		</tr>
		<tr>
			<th>16976</th>
			<td>Breakpoint condition compiler should not matter about NON-NLS strings Ive a project in which Ive set compiler option usage of non-externalized strings to Warning. When I want to set a condition on a breakpoint which contains a string object.equals for example I break all the time at this point due to a compilation error... Then Im obliged to write my condition as: boolean cond = object.equals //$NON-NLS-1$ return cond to avoid this problem. Wont it be possible that debugger uses a specific compiler which would ignore current project/workspace compiler options but only uses default one</td>
			<td>P2</td>
			<td>P3</td>
		</tr>
	</tbody>
</table>

<p>Our model is not accurate, and from observing the predictions, it is not clear whether a relationship between description and priority exists. To improve the accuracy of our model, we have to perform additional data cleaning steps and perform steps such as lemmatization, removing noisy tokens, modifying <code>min_df</code> and <code>max_df</code>, including trigrams, and so on. We recommend that you modify the current <code>clean</code> function provided in <a data-type="xref" href="ch04.xhtml#ch04largedatasetfeatureextract">“Feature Extraction on a Large Dataset”</a> and check the performance. Another option is also to determine the right hyperparameters for the selected model, and in the next section, we will introduce the cross-validation and grid search techniques, which can help us better understand model performance and arrive at an <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term13" id="idm45634193812136"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term14" id="idm45634193810792"/>optimized model.</p>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Using Cross-Validation to Estimate Realistic Accuracy Metrics"><div class="sect1" id="idm45634194303688">
<h1>Blueprint: Using Cross-Validation to Estimate Realistic Accuracy Metrics</h1>

<p>Before training the model, we created a <a contenteditable="false" data-type="indexterm" data-primary="train-test split" id="idm45634193807448"/>train-test split so that we can accurately evaluate our model. Based on the test split, we got an accuracy of 48.7%. However, it is desirable to improve this accuracy. Some of the techniques that we could use include adding additional features such as trigrams, adding additional text cleaning steps, choosing different model parameters, and then checking performance on the <a contenteditable="false" data-type="indexterm" data-primary="test split" id="idm45634193805832"/>test split. Our result is always based on a single hold-out dataset that we created using the train-test split. If we go back and change the <code>random_state</code> or <code>shuffle</code> our data, then we might get a different test split, which might have different accuracy for the same model. Therefore, we rely heavily on a given test split to determine the accuracy of our model.</p>

<p><em>Cross-validation</em> is <a contenteditable="false" data-type="indexterm" data-primary="cross-validation for accuracy metrics" id="ch6_term30"/><a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="with cross-validation for accuracy metrics" data-secondary-sortas="cross-validation for accuracy metrics" id="ch6_term31"/>a technique that allows us to train on different splits of data and validate also on different splits of data in a repetitive manner so that the final model that is trained achieves the right balance <a contenteditable="false" data-type="indexterm" data-primary="underfitting of trained model" id="idm45634193798760"/>between <em>underfitting</em> and <em>overfitting</em>. Underfitting is the phenomenon where our trained model does not learn the underlying relationship well and makes similar predictions for every observation that are far away from the real value. This is because the chosen model is not complex enough to model the phenomenon (wrong choice of model) or there are insufficient observations from which to learn the relationship. <a contenteditable="false" data-type="indexterm" data-primary="overfitting of trained model" id="idm45634193796216"/>Overfitting is the phenomenon where the chosen model is complex and has fit the underlying pattern very well during training but produces significant deviations on the test data. This indicates that the trained model does not generalize well to unseen data. By using a cross-validation technique, we become aware of these drawbacks by training and testing on multiple splits of the data and can arrive at a more realistic performance of our model.</p>

<p>There are many variants of cross-validation, and the most widely used is K-fold cross-validation<a contenteditable="false" data-type="indexterm" data-primary="K-fold cross-validation" id="idm45634193794104"/>. <a data-type="xref" href="#fig-cross-validation">Figure 6-5</a> demonstrates a K-fold strategy, where we first divide the entire training dataset into K folds. In each iteration, a model is trained on a different set of K-1 folds of the data, and validation is performed on the held-out Kth fold. The overall performance is taken to be the average of the performance on all hold-out K folds. In this way we are not basing our model accuracy on just one test split but multiple such splits, and similarly we are also training the model on multiple splits of the training data. This allows us to use all the observations for training our model as we do not need to have a separate hold-out test split.</p>

<figure><div id="fig-cross-validation" class="figure"><img src="Images/btap_0605.jpg" width="1354" height="890"/>
<h6><span class="label">Figure 6-5. </span>A K-fold cross-validation strategy where a different hold-out set (shaded) is chosen each time the model is trained. The rest of the sets form part of the training data.</h6>
</div></figure>

<p>To perform cross-validation, we <a contenteditable="false" data-type="indexterm" data-primary="cross_val_score " id="idm45634193767304"/>will use the <code>cross_val_score</code> method from scikit-learn. This takes as arguments the model that needs to be fit, the training dataset, and the number of folds that we want to use. In this case, we use a five-fold cross-validation strategy, and, depending on the number of training observations and availability of computing infrastructure, this can vary between 5 and 10. The method returns the validation score for each iteration of the cross-validation, and we can calculate the mean value obtained across all validation folds. From the results, we can see that the validation score varies from 36% up to 47%. This indicates that the model accuracy we reported earlier on the test dataset was optimistic and an artifact of the specific way in which the train-test split occurred. A more realistic accuracy that we can expect from this model is actually the average score of 44% derived from cross-validation. It’s important to perform this exercise to understand the true potential of any model. We perform the <a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with text classification" data-secondary-sortas="text classification" id="ch6_term32"/>vectorization step again because we are going to use the entire dataset and not just the train split:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Vectorization</code>

<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">min_df</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code> <code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="n">stop_words</code><code class="o">=</code><code class="s2">"english"</code><code class="p">)</code>
<code class="n">df_tf</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">])</code><code class="o">.</code><code class="n">toarray</code><code class="p">()</code>

<code class="c1"># Cross Validation with 5 folds</code>

<code class="n">scores</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">model1</code><code class="p">,</code>
                         <code class="n">X</code><code class="o">=</code><code class="n">df_tf</code><code class="p">,</code>
                         <code class="n">y</code><code class="o">=</code><code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">],</code>
                         <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>

<code class="k">print</code> <code class="p">(</code><code class="s2">"Validation scores from each iteration of the cross validation "</code><code class="p">,</code> <code class="n">scores</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s2">"Mean value across of validation scores "</code><code class="p">,</code> <code class="n">scores</code><code class="o">.</code><code class="n">mean</code><code class="p">())</code>
<code class="k">print</code> <code class="p">(</code><code class="s2">"Standard deviation of validation scores "</code><code class="p">,</code> <code class="n">scores</code><code class="o">.</code><code class="n">std</code><code class="p">())</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Validation scores from each iteration of the cross validation
[0.47773704 0.47302252 0.45468832 0.44054479 0.3677318 ]
Mean value across of validation scores  0.44274489261393396
Standard deviation of validation scores  0.03978852971586144
</pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Using a cross-validation technique allows us to use all observations, and we <a contenteditable="false" data-type="indexterm" data-primary="test split" id="idm45634193681400"/>do not need to create a separate hold-out test split. This gives the model more data to <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term30" id="idm45634193680056"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term31" id="idm45634193678680"/>learn from.</p>
</div>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Performing Hyperparameter Tuning with Grid Search"><div class="sect1" id="idm45634193808824">
<h1>Blueprint: Performing Hyperparameter Tuning with Grid Search</h1>

<p><em>Grid search</em> is a useful <a contenteditable="false" data-type="indexterm" data-primary="grid search for hyperparameter tuning" id="ch6_term35"/><a contenteditable="false" data-type="indexterm" data-primary="hyperparameters" id="ch6_term36"/><a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="with grid search for hyperparameter tuning" data-secondary-sortas="grid search for hyperparameter tuning" id="ch6_term37"/>technique to improve the accuracy of the model by evaluating different parameters that are used as arguments for the model. It does so by trying different combinations of hyperparameters that can maximize a given metric (e.g., accuracy) for the machine learning model. For example, if <a contenteditable="false" data-type="indexterm" data-primary="sklearn.svm.SVC model" id="idm45634193669496"/>we use the <code>sklearn.svm.SVC</code> model, it has a parameter named <a href="https://oreil.ly/30Xsq"><code>kernel</code></a> that can take several values: <code>linear</code>, <code>rbf</code> (radial basis function), <code>poly</code> (polynomial), and so on. Furthermore, by setting up a preprocessing pipeline, we could also test with different values of <code>ngram_range</code> for the TF-IDF vectorization. When we do a grid search, we provide the set of parameter values that we would like to evaluate, and combined with the cross-validation method of training a model, this identifies the set of hyperparameters that maximizes model accuracy. The biggest drawback of this technique is that it is CPU- and time-intensive; in a way we, are testing many possible combinations of hyperparameters to arrive at the set of values that perform best.</p>

<p>To test the right choice of hyperparameters for our model, we first create a <code>training_pipeline</code> where we define the steps that we would like to run. In this case, we <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="vectorization with" id="idm45634193663496"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with TF-IDF weighting" data-secondary-sortas="TF-IDF weighting" id="idm45634193662152"/>specify the TF-IDF vectorization and the <a contenteditable="false" data-type="indexterm" data-primary="LinearSVC model training" id="idm45634193660408"/>LinearSVC model training. We then define a set of parameters that we would like to test using the variable <code>grid_param</code>. Since a parameter value is specific to a certain step in the pipeline, we use the name of the step as the prefix when specifying the <code>grid_param</code>. For example, <code>min_df</code> is a parameter used by the vectorization step and is therefore referred to as <code>tfidf__min_df</code>. Finally, <a contenteditable="false" data-type="indexterm" data-primary="GridSearchCV method" id="idm45634193657192"/>we use the <code>GridSearchCV</code> method, which provides the functionality to test multiple versions of the entire pipeline with different sets of hyperparameters and produces the cross-validation scores from which we pick the best-performing <span class="keep-together">version</span>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">training_pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">(</code>
    <code class="n">steps</code><code class="o">=</code><code class="p">[(</code><code class="s1">'tfidf'</code><code class="p">,</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="s2">"english"</code><code class="p">)),</code>
            <code class="p">(</code><code class="s1">'model'</code><code class="p">,</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="mf">1e-5</code><code class="p">))])</code>

<code class="n">grid_param</code> <code class="o">=</code> <code class="p">[{</code>
    <code class="s1">'tfidf__min_df'</code><code class="p">:</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">],</code>
    <code class="s1">'tfidf__ngram_range'</code><code class="p">:</code> <code class="p">[(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">6</code><code class="p">)],</code>
    <code class="s1">'model__penalty'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'l2'</code><code class="p">],</code>
    <code class="s1">'model__loss'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'hinge'</code><code class="p">],</code>
    <code class="s1">'model__max_iter'</code><code class="p">:</code> <code class="p">[</code><code class="mi">10000</code><code class="p">]</code>
<code class="p">},</code> <code class="p">{</code>
    <code class="s1">'tfidf__min_df'</code><code class="p">:</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">],</code>
    <code class="s1">'tfidf__ngram_range'</code><code class="p">:</code> <code class="p">[(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">6</code><code class="p">)],</code>
    <code class="s1">'model__C'</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">10</code><code class="p">],</code>
    <code class="s1">'model__tol'</code><code class="p">:</code> <code class="p">[</code><code class="mf">1e-2</code><code class="p">,</code> <code class="mf">1e-3</code><code class="p">]</code>
<code class="p">}]</code>

<code class="n">gridSearchProcessor</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">training_pipeline</code><code class="p">,</code>
                                   <code class="n">param_grid</code><code class="o">=</code><code class="n">grid_param</code><code class="p">,</code>
                                   <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">],</code> <code class="n">df</code><code class="p">[</code><code class="s1">'Priority'</code><code class="p">])</code>

<code class="n">best_params</code> <code class="o">=</code> <code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">best_params_</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Best alpha parameter identified by grid search "</code><code class="p">,</code> <code class="n">best_params</code><code class="p">)</code>

<code class="n">best_result</code> <code class="o">=</code> <code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">best_score_</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Best result identified by grid search "</code><code class="p">,</code> <code class="n">best_result</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Best alpha parameter identified by grid search  {'model__loss': 'hinge',
'model__max_iter': 10000, 'model__penalty': 'l2', 'tfidf__min_df': 10,
'tfidf__ngram_range': (1, 6)}
Best result identified by grid search  0.46390780513357777
</pre>

<p>We have evaluated two values of <code>min_df</code> and <code>ngram_range</code> with two different sets of model parameters. In the first set, we tried with the l2 <code>model_penalty</code> and hinge <code>model_loss</code> with a maximum of 1,000 iterations. In the second set, we tried to vary the value of the regularization parameter <code>C</code> and <code>tolerance</code> values of the model. While we saw the parameters of the best model earlier, we can also check the performance of all other models that were generated to see how the parameter values interact. You can see the top five models and their parameter values as in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term32" id="idm45634193461480"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term35" id="idm45634193460104"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term36" id="idm45634193458728"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term37" id="idm45634193457352"/>following:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">gridsearch_results</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">)</code>
<code class="n">gridsearch_results</code><code class="p">[[</code><code class="s1">'rank_test_score'</code><code class="p">,</code> <code class="s1">'mean_test_score'</code><code class="p">,</code>
                    <code class="s1">'params'</code><code class="p">]]</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s1">'rank_test_score'</code><code class="p">])[:</code><code class="mi">5</code><code class="p">]</code>
</pre>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>rank_test_score</th>
			<th>mean_test_score</th>
			<th>params</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>3</th>
			<td>1</td>
			<td>0.46</td>
			<td>{'model__loss’: ‘hinge', ‘model__max_iter’: 10000, ‘model__penalty’: ‘l2', ‘tfidf__min_df’: 10, ‘tfidf__ngram_range’: (1, 6)}</td>
		</tr>
		<tr>
			<th>2</th>
			<td>2</td>
			<td>0.46</td>
			<td>{'model__loss’: ‘hinge', ‘model__max_iter’: 10000, ‘model__penalty’: ‘l2', ‘tfidf__min_df’: 10, ‘tfidf__ngram_range’: (1, 3)}</td>
		</tr>
		<tr>
			<th>0</th>
			<td>3</td>
			<td>0.46</td>
			<td>{'model__loss’: ‘hinge', ‘model__max_iter’: 10000, ‘model__penalty’: ‘l2', ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’: (1, 3)}</td>
		</tr>
		<tr>
			<th>1</th>
			<td>4</td>
			<td>0.46</td>
			<td>{'model__loss’: ‘hinge', ‘model__max_iter’: 10000, ‘model__penalty’: ‘l2', ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’: (1, 6)}</td>
		</tr>
		<tr>
			<th>5</th>
			<td>5</td>
			<td>0.45</td>
			<td>{'model__C’: 1, ‘model__tol’: 0.01, ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’: (1, 6)}</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Blueprint Recap and Conclusion"><div class="sect1" id="idm45634193676616">
<h1>Blueprint Recap and Conclusion</h1>

<p>Let’s recap the <a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="blueprint for" id="ch6_term41"/>steps of the blueprint for text classification by applying this to a different classification task. If you recall, we mentioned at the beginning of the chapter that to enable quick bug fixes, we must identify the priority of the bug and also assign it to the right team. The assignment can be done automatically by identifying which part of the software the bug belongs to. We have seen that the bug reports have a feature named <code>Component</code> with values including <code>Core</code>, <code>UI</code>, and <code>Doc</code>. This can be helpful in assigning the bug to the right team or individual, leading to a faster resolution. This task is similar to identifying the bug priority and will help us understand how the blueprint can be applied to any other application.
</p>
<p class="pagebreak-before">We update the blueprint with the following changes:</p>
<ul>
	<li>Additional step to include grid search to identify the best hyperparameters and limit the number of options tested to increase runtime</li>
	<li>Additional option <a contenteditable="false" data-type="indexterm" data-primary="sklearn.svm.SVC model" id="idm45634193424136"/>to use the <code>sklearn.svm.SVC</code> function to compare performance and try nonlinear kernels</li>
</ul>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Flag that determines the choice of SVC and LinearSVC</code>
<code class="n">runSVC</code> <code class="o">=</code> <code class="bp">True</code>

<code class="c1"># Loading the DataFrame</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'eclipse_jdt.csv'</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[[</code><code class="s1">'Title'</code><code class="p">,</code> <code class="s1">'Description'</code><code class="p">,</code> <code class="s1">'Component'</code><code class="p">]]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">dropna</code><code class="p">()</code>
<code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'Title'</code><code class="p">]</code> <code class="o">+</code> <code class="n">df</code><code class="p">[</code><code class="s1">'Description'</code><code class="p">]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'Title'</code><code class="p">,</code> <code class="s1">'Description'</code><code class="p">])</code>

<code class="c1"># Step 1 - Data Preparation</code>
<code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">clean</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">len</code><code class="p">()</code> <code class="o">&gt;</code> <code class="mi">50</code><code class="p">]</code>

<code class="k">if</code> <code class="p">(</code><code class="n">runSVC</code><code class="p">):</code>
    <code class="c1"># Sample the data when running SVC to ensure reasonable run-times</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'Component'</code><code class="p">,</code> <code class="n">as_index</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">sample</code><code class="p">,</code>
                                                       <code class="n">random_state</code><code class="o">=</code><code class="mi">21</code><code class="p">,</code>
                                                       <code class="n">frac</code><code class="o">=.</code><code class="mi">2</code><code class="p">)</code>

<code class="c1"># Step 2 - Train-Test Split</code>
<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">,</code> <code class="n">Y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">],</code>
                                                    <code class="n">df</code><code class="p">[</code><code class="s1">'Component'</code><code class="p">],</code>
                                                    <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code>
                                                    <code class="n">stratify</code><code class="o">=</code><code class="n">df</code><code class="p">[</code><code class="s1">'Component'</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Size of Training Data '</code><code class="p">,</code> <code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Size of Test Data '</code><code class="p">,</code> <code class="n">X_test</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>

<code class="c1"># Step 3 - Training the Machine Learning model</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="s2">"english"</code><code class="p">)</code>

<code class="k">if</code> <code class="p">(</code><code class="n">runSVC</code><code class="p">):</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">SVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">probability</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
    <code class="n">grid_param</code> <code class="o">=</code> <code class="p">[{</code>
        <code class="s1">'tfidf__min_df'</code><code class="p">:</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">],</code>
        <code class="s1">'tfidf__ngram_range'</code><code class="p">:</code> <code class="p">[(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">6</code><code class="p">)],</code>
        <code class="s1">'model__C'</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">100</code><code class="p">],</code>
        <code class="s1">'model__kernel'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'linear'</code><code class="p">]</code>
    <code class="p">}]</code>
<code class="k">else</code><code class="p">:</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="mf">1e-5</code><code class="p">)</code>
    <code class="n">grid_param</code> <code class="o">=</code> <code class="p">{</code>
        <code class="s1">'tfidf__min_df'</code><code class="p">:</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">],</code>
        <code class="s1">'tfidf__ngram_range'</code><code class="p">:</code> <code class="p">[(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">6</code><code class="p">)],</code>
        <code class="s1">'model__C'</code><code class="p">:</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">100</code><code class="p">],</code>
        <code class="s1">'model__loss'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'hinge'</code><code class="p">]</code>
    <code class="p">}</code>

<code class="n">training_pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">(</code>
    <code class="n">steps</code><code class="o">=</code><code class="p">[(</code><code class="s1">'tfidf'</code><code class="p">,</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="s2">"english"</code><code class="p">)),</code> <code class="p">(</code><code class="s1">'model'</code><code class="p">,</code> <code class="n">model</code><code class="p">)])</code>

<code class="n">gridSearchProcessor</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">estimator</code><code class="o">=</code><code class="n">training_pipeline</code><code class="p">,</code>
                                   <code class="n">param_grid</code><code class="o">=</code><code class="n">grid_param</code><code class="p">,</code>
                                   <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>

<code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>

<code class="n">best_params</code> <code class="o">=</code> <code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">best_params_</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Best alpha parameter identified by grid search "</code><code class="p">,</code> <code class="n">best_params</code><code class="p">)</code>

<code class="n">best_result</code> <code class="o">=</code> <code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">best_score_</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Best result identified by grid search "</code><code class="p">,</code> <code class="n">best_result</code><code class="p">)</code>

<code class="n">best_model</code> <code class="o">=</code> <code class="n">gridSearchProcessor</code><code class="o">.</code><code class="n">best_estimator_</code>

<code class="c1"># Step 4 - Model Evaluation</code>

<code class="n">Y_pred</code> <code class="o">=</code> <code class="n">best_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Accuracy Score - '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="n">classification_report</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Size of Training Data  7204
Size of Test Data  1801
Best alpha parameter identified by grid search  {'model__C': 1,
'model__kernel': 'linear', 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 6)}
Best result identified by grid search  0.739867279666898
Accuracy Score -  0.7368128817323709
              precision    recall  f1-score   support

         APT       1.00      0.25      0.40        16
        Core       0.74      0.77      0.75       544
       Debug       0.89      0.77      0.82       300
         Doc       0.50      0.17      0.25        12
        Text       0.61      0.45      0.52       235
          UI       0.71      0.81      0.76       694

    accuracy                           0.74      1801
   macro avg       0.74      0.54      0.58      1801
weighted avg       0.74      0.74      0.73      1801
</pre>

<p>Based on the accuracy and classification report, we have achieved an accuracy of 73%, and we can conclude that this model is able to predict the software component that a bug is referring to more accurately than the priority. While some of the improvement is due to the <a contenteditable="false" data-type="indexterm" data-primary="grid search for hyperparameter tuning" id="idm45634193411624"/><a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="with cross-validation for accuracy metrics" data-secondary-sortas="cross-validation for accuracy metrics" id="idm45634193410552"/><a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="with grid search for hyperparameter tuning" data-secondary-sortas="grid search for hyperparameter tuning" id="idm45634192944392"/>additional steps of grid search and cross-validation, most of it is simply because there is a relationship that the model could identify between the bug description and the Component it refers to. The Component feature <a contenteditable="false" data-type="indexterm" data-primary="class imbalance" id="idm45634192942344"/>does not show the same level of the class imbalance problem that we noticed earlier. However, even within Component, we can see the poor results for the software component Doc, which has few observations compared to the other components. Also, comparing with the baseline, we can see that this model improves in performance. We can try to balance our data, or we can make an informed business decision that it’s more important for the model to predict those software components that have a larger number of bugs:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">clf</code> <code class="o">=</code> <code class="n">DummyClassifier</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'most_frequent'</code><code class="p">)</code>
<code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">Y_train</code><code class="p">)</code>
<code class="n">Y_pred_baseline</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Accuracy Score - '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">Y_pred_baseline</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Accuracy Score -  0.38534147695724597
</pre>

<p>Let’s also attempt to understand how this model tries to make its predictions by looking at where it works well and where it fails. We will first sample two observations where the predictions were accurate:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># Create a DataFrame combining the Title and Description,</code>
<code class="c1"># Actual and Predicted values that we can explore</code>
<code class="n">frame</code> <code class="o">=</code> <code class="p">{</code> <code class="s1">'text'</code><code class="p">:</code> <code class="n">X_test</code><code class="p">,</code> <code class="s1">'actual'</code><code class="p">:</code> <code class="n">Y_test</code><code class="p">,</code> <code class="s1">'predicted'</code><code class="p">:</code> <code class="n">Y_pred</code> <code class="p">}</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">frame</code><code class="p">)</code>

<code class="n">result</code><code class="p">[</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">==</code> <code class="n">result</code><code class="p">[</code><code class="s1">'predicted'</code><code class="p">]]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>


<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>text</th>
			<th>actual</th>
			<th>predicted</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>28225</th>
			<td>Move static initializer lacks atomic undo.When a method is moved the move can be atomically undone with a single Undo command. But when a static initializer is moved it can only be undone with an Undo command issued in both the source and destination files</td>
			<td>UI</td>
			<td>UI</td>
		</tr>
		<tr>
			<th>30592</th>
			<td>Debug view steals focus when breakpoint hitM5 - I20060217-1115 When you debug a program that has breakpoints when the debugger hits a breakpoint pressing Ctrl+Sht+B does not remove the breakpoint even though the line looks like it has the focus. To actually remove the breakpoint one has to click in the editor on the proper line and repress the keys</td>
			<td>Debug</td>
			<td>Debug</td>
		</tr>
	</tbody>
</table>

<p>We can see that when a bug is classified as belonging to the Debug component the description makes use of terms like <em>debugger</em> and <em>breakpoint</em>, whereas when a bug is classified in UI, we see an indication of <em>Undo</em> and <em>movement</em>. This seems to indicate that the trained model is able to learn associations between words in the description and the corresponding software component. Let’s also look at some observations where the predictions were incorrect:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">result</code><code class="p">[</code><code class="n">result</code><code class="p">[</code><code class="s1">'actual'</code><code class="p">]</code> <code class="o">!=</code> <code class="n">result</code><code class="p">[</code><code class="s1">'predicted'</code><code class="p">]]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>


<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>text</th>
			<th>actual</th>
			<th>predicted</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>16138</th>
			<td>Line wrapping on @see tags creates a new warning Invalid parameters declarationIn Eclipce 3.0M5 with the javadoc checking enabled linewrapping will cause a warning Javadoc: Invalid parameters declaration This will cause the warning: /** * @see com.xyz.util.monitoring.MonitoringObserver#monitorSetValue */ where this will not : /** * @see com.xyz.util.monitoring.MonitoringObserver#monitorSetValue *</td>
			<td>Text</td>
			<td>Core</td>
		</tr>
		<tr>
			<th>32903</th>
			<td>After a String array is created eclipse fails to recognize methods for an object.Type these lines in any program. String abc = new String {a b c} System. After System. eclipse wont list all the available methods</td>
			<td>Core</td>
			<td>UI</td>
		</tr>
	</tbody>
</table>

<p>Here, it becomes more difficult to identify reasons for an incorrect classification, but we must analyze further if we want to improve the accuracy of our model. After we build a model, we must investigate our predictions and understand why the model made these predictions. There are several <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch6_term41" id="idm45634192829272"/>techniques that we can use to explain model predictions, and this will be covered in more detail in <a data-type="xref" href="ch07.xhtml#ch-explain">Chapter 7</a>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Closing Remarks"><div class="sect1" id="idm45634193431064">
<h1>Closing Remarks</h1>

<p>In this chapter, we presented a blueprint for performing the different steps in building a supervised text classification model. It starts with the data preparation steps, including the balancing of classes, if required. We then showed the steps for creating train and test splits, including the use of cross-validation as the preferred technique to arrive at an accurate measure of model accuracy. We then presented grid search as one of the techniques to validate different settings of hyperparameters to find the most optimal combination. <a contenteditable="false" data-type="indexterm" data-primary="supervised learning models" data-seealso="text classification algorithms; text classification results, explaining" id="idm45634192825112"/><a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="about" id="idm45634192823800"/>Supervised machine learning is a broad area with multiple applications like loan default prediction, ad-click prediction, etc. This blueprint presents an end-to-end technique for building a supervised machine learning model and can be extended to problems outside of text <a contenteditable="false" data-type="indexterm" data-primary="text classification algorithms" data-secondary="further reading on" id="idm45634192822040"/>classification as well.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Further Reading"><div class="sect1" id="idm45634192820312">
<h1>Further Reading</h1>

<ul class="author-date-bib">
  <li>Bergstra, James, and Yoshua Bengio. “Random Search for Hyper-Parameter Optimization.” 2012. <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"><em>http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf</em></a>.</li>
  <li>Berwick, R. “An Idiot’s guide to Support Vector Machines.” <a href="http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf"><em>http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</em></a>.</li>
  <li>Kohavi, Ron. “A Study of CrossValidation and Bootstrap for Accuracy Estimation and Model Selection.” <a href="http://ai.stanford.edu/~ronnyk/accEst.pdf"><em>http://ai.stanford.edu/~ronnyk/accEst.pdf</em></a>.</li>
  <li>Raschka, Sebastian. “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning.” 2018. <a href="https://arxiv.org/pdf/1811.12808.pdf"><em>https://arxiv.org/pdf/1811.12808.pdf</em></a>.</li>
</ul>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45634194378904"><sup><a href="ch06.xhtml#idm45634194378904-marker">1</a></sup> Nitesh Chawla et al. “Synthetic Minority Over-Sampling Technique.” <em>Journal of Artificial Intelligence Research</em> 16 (June 2002). <a href="https://arxiv.org/pdf/1106.1813.pdf"><em>https://arxiv.org/pdf/1106.1813.pdf</em></a>.</p></div></div></section></div>



  </body></html>