<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 4. Making Predictions with Decision Trees &#10;and Decision Forests" data-type="chapter" epub:type="chapter"><div class="chapter" id="making_predictions_with_decision_trees_and_decision_forests">
<h1><span class="label">Chapter 4. </span>Making Predictions with Decision Trees 
<span class="keep-together">and Decision Forests</span></h1>
<p>Classification and regression are the oldest and<a data-primary="classification" data-secondary="supervised learning via" data-type="indexterm" id="idm46507982479840"/><a data-primary="regression" data-secondary="supervised learning via" data-type="indexterm" id="idm46507982478864"/><a data-primary="supervised learning with classification and regression" data-type="indexterm" id="idm46507982477920"/><a data-primary="statistics" data-secondary="classification and regression" data-type="indexterm" id="idm46507982477280"/><a data-primary="data analysis" data-secondary="classification and regression" data-type="indexterm" id="idm46507982476368"/> most well-studied types of predictive analytics. Most algorithms you will likely encounter in analytics packages and libraries are classification or regression techniques, like support vector machines, logistic regression, neural networks, and deep learning. <a data-primary="predictions" data-secondary="classification and regression algorithms" data-seealso="decision trees" data-type="indexterm" id="idm46507982475328"/>The common thread linking regression and classification is that both involve predicting one (or more) values given one (or more) other values. To do so, both require a body of inputs and outputs to learn from. They need to be fed both questions and known answers. For this reason, they are known as types of supervised learning.</p>
<p>PySpark MLlib offers implementations of a<a data-primary="MLlib component of Spark" data-secondary="classification and regression algorithms" data-type="indexterm" id="idm46507982473632"/> number of classification and regression algorithms. These include decision trees, naïve Bayes, logistic regression, and linear regression. The exciting thing about these algorithms is that they can help predict the future—or at least, predict the things we don’t yet know for sure, like the likelihood you will buy a car based on your online behavior, whether an email is spam given the words it contains, or which acres of land are likely to grow the most crops given their location and soil chemistry.</p>
<p>In this chapter, we will focus on a popular and flexible type of algorithm for<a data-primary="classification" data-secondary="decision trees" data-seealso="decision trees" data-type="indexterm" id="idm46507982472208"/><a data-primary="regression" data-secondary="decision trees" data-seealso="decision trees" data-type="indexterm" id="idm46507982470960"/> both classification and regression (decision trees) and the algorithm’s extension (random decision forests). First, we will understand the basics of decision trees and forests and introduce the former’s PySpark implementation. The PySpark implementation of decision trees supports binary and multiclass classification, and regression. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances. This will be followed by preparation of our dataset and creating our first decision tree. Then we’ll tune our decision tree model. We’ll finish up by training a random forest model on our processed dataset and making predictions.</p>
<p>Although PySpark’s decision tree implementation is easy to get started with, it is helpful to understand the fundamentals of decision tree and random forest algorithms. That is what we’ll cover in the next section.</p>
<section data-pdf-bookmark="Decision Trees and Forests" data-type="sect1"><div class="sect1" id="idm46507982469104">
<h1>Decision Trees and Forests</h1>
<p><em>Decision trees</em> are a family of algorithms<a data-primary="decision trees" data-secondary="about" data-type="indexterm" id="ch04-tree"/><a data-primary="forest-covered land decision tree" data-secondary="about decision trees" data-type="indexterm" id="ch04-tree2"/><a data-primary="categorical data" data-secondary="decision trees for" data-type="indexterm" id="idm46507982464592"/><a data-primary="numeric data" data-secondary="decision trees for" data-type="indexterm" id="idm46507982463648"/> that can naturally handle both categorical and numeric features. Building a single tree can be done using parallel computing, and many trees can be built in parallel at once. <a data-primary="outliers in data and decision trees" data-type="indexterm" id="idm46507982462576"/>They are robust to outliers in the data, meaning that a few extreme and possibly erroneous data points might not affect predictions at all. They can consume data of different types and on different scales without the need for preprocessing or normalization.</p>
<p>Decision tree–based algorithms have the advantage of being comparatively intuitive to understand and reason about. In fact, we all probably use the same reasoning embodied in decision trees, implicitly, in everyday life. For example, I sit down to have morning coffee with milk. Before I commit to that milk and add it to my brew, I want to predict: is the milk spoiled? I don’t know for sure. I might check if the use-by date has passed. If not, I predict no, it’s not spoiled. If the date has passed, but it was three or fewer days ago, I take my chances and predict no, it’s not spoiled. Otherwise, I sniff the milk. If it smells funny, I predict yes, and otherwise no.</p>
<p>This series of yes/no decisions that leads to a prediction are what decision trees embody. Each decision leads to one of two results, which is either a prediction or another decision, as shown in <a data-type="xref" href="#MilkDecisionTree">Figure 4-1</a>. In this sense, it is natural to think of the process as a tree of decisions, where each internal node in the tree is a decision, and each leaf node is a final answer.</p>
<p>That is a simplistic decision tree and was not built with any rigor. To elaborate, consider another example.<a data-primary="decision trees" data-secondary="pet store example" data-type="indexterm" id="ch04-pet"/><a data-primary="pet store decision tree" data-type="indexterm" id="ch04-pet2"/> A robot has taken a job in an exotic pet store. It wants to learn, before the shop opens, which animals in the shop would make a good pet for a child. The owner lists nine pets that would and wouldn’t be suitable before hurrying off. The robot compiles the information found in <a data-type="xref" href="#Pet_Stats">Table 4-1</a> from examining the animals.</p>
<figure><div class="figure" id="MilkDecisionTree">
<img alt="aaps 0401" height="750" src="assets/aaps_0401.png" width="1180"/>
<h6><span class="label">Figure 4-1. </span>Decision tree: is milk spoiled?</h6>
</div></figure>
<table id="Pet_Stats">
<caption><span class="label">Table 4-1. </span>Exotic pet store “feature vectors”</caption>
<thead>
<tr>
<th>Name</th>
<th>Weight (kg)</th>
<th># Legs</th>
<th>Color</th>
<th>Good pet?</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Fido</p></td>
<td><p>20.5</p></td>
<td><p>4</p></td>
<td><p>Brown</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Mr. Slither</p></td>
<td><p>3.1</p></td>
<td><p>0</p></td>
<td><p>Green</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p>Nemo</p></td>
<td><p>0.2</p></td>
<td><p>0</p></td>
<td><p>Tan</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Dumbo</p></td>
<td><p>1390.8</p></td>
<td><p>4</p></td>
<td><p>Gray</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p>Kitty</p></td>
<td><p>12.1</p></td>
<td><p>4</p></td>
<td><p>Gray</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Jim</p></td>
<td><p>150.9</p></td>
<td><p>2</p></td>
<td><p>Tan</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p>Millie</p></td>
<td><p>0.1</p></td>
<td><p>100</p></td>
<td><p>Brown</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p>McPigeon</p></td>
<td><p>1.0</p></td>
<td><p>2</p></td>
<td><p>Gray</p></td>
<td><p>No</p></td>
</tr>
<tr>
<td><p>Spot</p></td>
<td><p>10.0</p></td>
<td><p>4</p></td>
<td><p>Brown</p></td>
<td><p>Yes</p></td>
</tr>
</tbody>
</table>
<p>The robot can make a decision for the nine listed pets. There are many more pets available in the store. It still needs a methodology for deciding which animals among the rest will be suitable as pets for kids. We can assume that the characteristics of all animals are available. Using the decision data provided by the store owner and a decision tree, we can help the robot learn what a good pet for a kid looks like.</p>
<p>Although a name is given, it will not be included as a feature in our decision tree model. There is little reason to believe the name alone is predictive; “Felix” could name a cat or a poisonous tarantula, for all the robot knows. So, there are two numeric features (weight, number of legs) and one categorical feature (color) predicting a categorical target (is/is not a good pet for a child.</p>
<p>The way a decision tree works is by making one or more decisions in sequence based on provided features. To start off, the robot might try to fit a simple decision tree to this training data, consisting of a single decision based
on weight, as shown in <a data-type="xref" href="#PetStoreDecisionTree1">Figure 4-2</a>.</p>
<figure><div class="figure" id="PetStoreDecisionTree1">
<img alt="aaps 0402" height="360" src="assets/aaps_0402.png" width="712"/>
<h6><span class="label">Figure 4-2. </span>Robot’s first decision tree</h6>
</div></figure>
<p>The logic of the decision tree is easy to read and make sense of: 500kg animals certainly sound unsuitable as pets. This rule predicts the correct value in five of nine cases. A quick glance suggests that we could improve the rule by lowering the weight threshold to 100kg. This gets six of nine examples correct. The heavy animals are now predicted correctly; the lighter animals are only partly correct.</p>
<p>So, a second decision can be constructed to further refine the prediction for examples with weights less than 100kg. It would be good to pick a feature that changes some of the incorrect Yes predictions to No. For example, there is one small green animal, sounding suspiciously like a snake, that will be classified by our current model as a suitable pet candidate. The robot could predict correctly by adding a decision based on color, as shown in <a data-type="xref" href="#PetStoreDecisionTree2">Figure 4-3</a>.</p>
<figure><div class="figure" id="PetStoreDecisionTree2">
<img alt="aaps 0403" height="589" src="assets/aaps_0403.png" width="972"/>
<h6><span class="label">Figure 4-3. </span>Robot’s next decision tree</h6>
</div></figure>
<p class="pagebreak-before">Now, seven of nine examples are correct. Of course, decision rules could be added until all nine were correctly predicted. The logic embodied in the resulting decision tree would probably sound implausible when translated into common speech: “If the animal’s weight is less than 100kg, its color is brown instead of green, and it has fewer than 10 legs, then yes, it is a suitable pet.” While perfectly fitting the given examples, a decision tree like this would fail to predict that a small, brown, four-legged wolverine is not a suitable pet. <a data-primary="decision trees" data-secondary="overfitting" data-type="indexterm" id="idm46507982410272"/><a data-primary="overfitting" data-type="indexterm" id="idm46507982409296"/>Some balance is needed to avoid this phenomenon, known as <em>overfitting</em>.<a data-startref="ch04-pet" data-type="indexterm" id="idm46507982408112"/><a data-startref="ch04-pet2" data-type="indexterm" id="idm46507982407376"/></p>
<p>Decision trees generalize into a more<a data-primary="random forests" data-type="indexterm" id="idm46507982406272"/><a data-primary="decision trees" data-secondary="random forests" data-type="indexterm" id="idm46507982405568"/><a data-primary="overfitting" data-secondary="random forests reducing" data-type="indexterm" id="idm46507982404624"/><a data-primary="decision trees" data-secondary="overfitting" data-tertiary="random forests reducing" data-type="indexterm" id="idm46507982403680"/> powerful algorithm, called <em>random forests</em>. Random forests combine many decision trees to reduce the risk of overfitting and train the decision trees separately. The algorithm injects randomness into the training process so that each decision tree is a bit different. Combining the predictions reduces the variance of the predictions, makes the resulting model more generalizable, and improves performance on test data.</p>
<p>This is enough of an introduction to decision trees and random forests for us to begin using them with PySpark. In the next section, we will introduce the dataset that we’ll work with and prepare it for use in PySpark.<a data-startref="ch04-tree" data-type="indexterm" id="idm46507982401488"/><a data-startref="ch04-tree2" data-type="indexterm" id="idm46507982400784"/></p>
</div></section>
<section data-pdf-bookmark="Preparing the Data" data-type="sect1"><div class="sect1" id="idm46507982468544">
<h1>Preparing the Data</h1>
<p>The dataset used in this chapter is the<a data-primary="decision trees" data-secondary="Covtype dataset" data-tertiary="preparing the data" data-type="indexterm" id="ch04-prep"/><a data-primary="Covtype dataset" data-type="indexterm" id="ch04-prep2"/><a data-primary="datasets" data-secondary="forest-covered land parcels" data-type="indexterm" id="idm46507981960976"/><a data-primary="forest-covered land decision tree" data-secondary="Covtype dataset" data-tertiary="preparing the data" data-type="indexterm" id="ch04-prep4"/><a data-primary="Colorado forest-covered land parcels dataset" data-type="indexterm" id="ch04-prep5"/><a data-primary="datasets" data-secondary="Covtype" data-type="indexterm" id="idm46507981957952"/> well-known Covtype dataset, available <a href="https://oreil.ly/spUWl">online</a> as a compressed CSV-format data file, <em>covtype.data.gz</em>, and accompanying info file, <em>covtype.info</em>.</p>
<p>The dataset records the types of forest-covered parcels of land in Colorado, USA. It’s only a coincidence that the dataset concerns real-world forests! Each data record contains several features describing each parcel of land—like its elevation, slope, distance to water, shade, and soil type—along with the known forest type covering the land. The forest cover type is to be predicted from the rest of the features, of which there are 54 in total.</p>
<p>This dataset has been used in research and even<a data-primary="Kaggle competition forest cover dataset" data-type="indexterm" id="idm46507981954944"/> a <a href="https://oreil.ly/LpjgW">Kaggle competition</a>. It is an interesting dataset to explore in this chapter because it contains both categorical and numeric features. There are 581,012 examples in the dataset, which does not exactly qualify as big data but is large enough to be manageable as an example and still highlight some issues of scale.</p>
<p>Thankfully, the data is already in a simple CSV format and does not require much cleansing or other preparation to be used with PySpark MLlib. The <em>covtype.data</em> file should be extracted and copied into your local or cloud storage (such as AWS S3).</p>
<p>Start <code>pyspark-shell</code>. You may find it helpful to give the<a data-primary="memory" data-secondary="driver-memory for large local memory" data-type="indexterm" id="idm46507981951824"/><a data-primary="installing PySpark API" data-secondary="driver-memory for large local memory" data-type="indexterm" id="idm46507981950976"/><a data-primary="PySpark API" data-secondary="driver-memory for large local memory" data-type="indexterm" id="idm46507981950128"/><a data-primary="Python" data-secondary="PySpark API" data-tertiary="driver-memory for large local memory" data-type="indexterm" id="idm46507981949280"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="driver-memory for large local memory" data-type="indexterm" id="idm46507981948192"/><a data-primary="decision trees" data-secondary="creating" data-tertiary="memory intensive" data-type="indexterm" id="idm46507981947104"/> shell a healthy amount of memory to work with, as building decision forests can be resource intensive. If you have the memory, specify <code>--driver-memory 8g</code> or similar.</p>
<p>CSV files contain fundamentally tabular data,<a data-primary="csv method" data-secondary="dataframe created" data-type="indexterm" id="idm46507981945088"/><a data-primary="Reader API" data-secondary="csv method" data-type="indexterm" id="idm46507981944240"/> organized into rows of columns. Sometimes these columns are given names in a header line, although that’s not the case here. The column names are given in the companion file, <em>covtype.info</em>. Conceptually, each column of a CSV file has a type as well—a number, a string—but a CSV file doesn’t specify this.</p>
<p>It’s natural to parse this data as a dataframe because this is PySpark’s abstraction for tabular data, with a defined column schema, including column names and types. PySpark has built-in support for reading CSV data. Let’s read our dataset as a DataFrame using the built-in CSV reader:<a data-primary="schemas of dataframes" data-secondary="forest-covered land parcels" data-type="indexterm" id="idm46507981942192"/><a data-primary="dataframes" data-secondary="schemas" data-tertiary="forest-covered land parcels" data-type="indexterm" id="idm46507981941344"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">data_without_header</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">option</code><code class="p">(</code><code class="s2">"inferSchema"</code><code class="p">,</code> <code class="kc">True</code><code class="p">)</code>\
                      <code class="o">.</code><code class="n">option</code><code class="p">(</code><code class="s2">"header"</code><code class="p">,</code> <code class="kc">False</code><code class="p">)</code><code class="o">.</code><code class="n">csv</code><code class="p">(</code><code class="s2">"data/covtype.data"</code><code class="p">)</code>
<code class="n">data_without_header</code><code class="o">.</code><code class="n">printSchema</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">root</code>
 <code class="o">|--</code> <code class="n">_c0</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">_c1</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">_c2</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">_c3</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">_c4</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">_c5</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">...</code></pre>
<p>This code reads the input as CSV and does not attempt to parse the first line as a header of column names. It also requests that the type of each column be inferred by examining the data. It correctly infers that all of the columns are numbers, and, more specifically, integers. Unfortunately, it can name the columns only <code>_c0</code> and so on.</p>
<p>We can look at the <em>covtype.info</em> file for the column names.</p>
<pre data-code-language="shell" data-type="programlisting">$ cat data/covtype.info

...
<code class="o">[</code>...<code class="o">]</code>
<code class="m">7</code>.	Attribute information:

Given is the attribute name, attribute type, the measurement unit and
a brief description.  The forest cover <code class="nb">type</code> is the classification
problem.  The order of this listing corresponds to the order of
numerals along the rows of the database.

Name                                    Data Type
Elevation                               quantitative
Aspect                                  quantitative
Slope                                   quantitative
Horizontal_Distance_To_Hydrology        quantitative
Vertical_Distance_To_Hydrology          quantitative
Horizontal_Distance_To_Roadways         quantitative
Hillshade_9am                           quantitative
Hillshade_Noon                          quantitative
Hillshade_3pm                           quantitative
Horizontal_Distance_To_Fire_Points      quantitative
Wilderness_Area <code class="o">(</code><code class="m">4</code> binary columns<code class="o">)</code>      qualitative
Soil_Type <code class="o">(</code><code class="m">40</code> binary columns<code class="o">)</code>           qualitative
Cover_Type <code class="o">(</code><code class="m">7</code> types<code class="o">)</code>                    integer

Measurement                  Description

meters                       Elevation <code class="k">in</code> meters
azimuth                      Aspect <code class="k">in</code> degrees azimuth
degrees                      Slope <code class="k">in</code> degrees
meters                       Horz Dist to nearest surface water features
meters                       Vert Dist to nearest surface water features
meters                       Horz Dist to nearest roadway
<code class="m">0</code> to <code class="m">255</code> index               Hillshade index at 9am, summer solstice
<code class="m">0</code> to <code class="m">255</code> index               Hillshade index at noon, summer soltice
<code class="m">0</code> to <code class="m">255</code> index               Hillshade index at 3pm, summer solstice
meters                       Horz Dist to nearest wildfire ignition point
<code class="m">0</code> <code class="o">(</code>absence<code class="o">)</code> or <code class="m">1</code> <code class="o">(</code>presence<code class="o">)</code>  Wilderness area designation
<code class="m">0</code> <code class="o">(</code>absence<code class="o">)</code> or <code class="m">1</code> <code class="o">(</code>presence<code class="o">)</code>  Soil Type designation
<code class="m">1</code> to <code class="m">7</code>                       Forest Cover Type designation
...</pre>
<p>Looking at the column information, it’s clear that some features are indeed numeric. <code>Elevation</code> is an elevation in meters; <code>Slope</code> is measured in degrees. However, <code>Wilderness_Area</code> is something different, because it is said to span four columns, each of which is a 0 or 1. In reality, <code>Wilderness_Area</code> is a categorical value, not a numeric one.</p>
<p>These four columns are actually<a data-primary="one-hot encoding" data-type="indexterm" id="idm46507981741744"/><a data-primary="1-of-N encoding" data-see="one-hot encoding" data-type="indexterm" id="idm46507981741040"/><a data-primary="1-of-N encoding" data-primary-sortas="one-of-N" data-see="one-hot encoding" data-type="indexterm" id="idm46507981740096"/><a data-primary="categorical data" data-secondary="one-hot encoding" data-type="indexterm" id="idm46507981738880"/> a one-hot or 1-of-N encoding. When this form of encoding is performed on a categorical feature, one categorical feature that takes on <em>N</em> distinct values becomes <em>N</em> numeric features, each taking on the value 0 or 1. Exactly one of the <em>N</em> values has value 1, and the others are 0. For example, a categorical feature for weather that can be <code>cloudy</code>, <code>rainy</code>, or <code>clear</code> would become three numeric features, where <code>cloudy</code> is represented by <code>1,0,0</code>; <code>rainy</code> by <code>0,1,0</code>; and so on. These three numeric features might be thought of as <code>is_cloudy</code>, <code>is_rainy</code>, and <code>is_clear</code> features. Likewise, 40 of the columns are really one <code>Soil_Type</code> categorical feature.</p>
<p>This isn’t the only possible way to encode a categorical feature<a data-primary="categorical data" data-secondary="distinct numeric values" data-type="indexterm" id="idm46507981795296"/><a data-primary="numeric data" data-secondary="categorical data coded as" data-type="indexterm" id="idm46507981794288"/> as a number. Another possible encoding simply assigns a distinct numeric value to each possible value of the categorical feature. For example, <code>cloudy</code> may become 1.0, <code>rainy</code> 2.0, and so on. The target itself, <code>Cover_Type</code>, is a categorical value encoded as a value 1 to 7.</p>
<div data-type="note" epub:type="note">
<p>Be careful when encoding a categorical feature as a single numeric feature. The original categorical values have no ordering, but when encoded as a number, they appear to. Treating the encoded feature as numeric leads to meaningless results because the algorithm is effectively pretending that <code>rainy</code> is somehow greater than, and two times larger than, <code>cloudy</code>. It’s OK as long as the encoding’s numeric value is not used as a number.</p>
</div>
<p>We have seen both types of encodings of categorical features. It would have, perhaps, been simpler and more straightforward to not encode such features (and in two ways, no less) and to instead simply include their values directly, like “Rawah Wilderness Area.” This may be an artifact of history; the dataset was released in 1998. For performance reasons or to match the format expected by libraries of the day, which were built more for regression problems, datasets often contain data encoded in these ways.</p>
<p>In any event, before proceeding, it is useful to add column names to this DataFrame to make it easier to work with:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">sql</code><code class="nn">.</code><code class="nn">types</code><code> </code><code class="kn">import</code><code> </code><code class="n">DoubleType</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">sql</code><code class="nn">.</code><code class="nn">functions</code><code> </code><code class="kn">import</code><code> </code><code class="n">col</code><code>
</code><code>
</code><code class="n">colnames</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s2">"</code><code class="s2">Elevation</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">Aspect</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">Slope</code><code class="s2">"</code><code class="p">,</code><code> </code><code>\
</code><code>            </code><code class="s2">"</code><code class="s2">Horizontal_Distance_To_Hydrology</code><code class="s2">"</code><code class="p">,</code><code> </code><code>\
</code><code>            </code><code class="s2">"</code><code class="s2">Vertical_Distance_To_Hydrology</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">Horizontal_Distance_To_Roadways</code><code class="s2">"</code><code class="p">,</code><code> </code><code>\
</code><code>            </code><code class="s2">"</code><code class="s2">Hillshade_9am</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">Hillshade_Noon</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">Hillshade_3pm</code><code class="s2">"</code><code class="p">,</code><code> </code><code>\
</code><code>            </code><code class="s2">"</code><code class="s2">Horizontal_Distance_To_Fire_Points</code><code class="s2">"</code><code class="p">]</code><code> </code><code class="o">+</code><code> </code><code>\</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code class="p">[</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Wilderness_Area_</code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s2">"</code><code> </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code><code class="p">]</code><code> </code><code class="o">+</code><code> </code><code>\
</code><code class="p">[</code><code class="sa">f</code><code class="s2">"</code><code class="s2">Soil_Type_</code><code class="si">{</code><code class="n">i</code><code class="si">}</code><code class="s2">"</code><code> </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">40</code><code class="p">)</code><code class="p">]</code><code> </code><code class="o">+</code><code> </code><code>\
</code><code class="p">[</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">]</code><code>
</code><code>
</code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">data_without_header</code><code class="o">.</code><code class="n">toDF</code><code class="p">(</code><code class="o">*</code><code class="n">colnames</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                          </code><code class="n">withColumn</code><code class="p">(</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>                                    </code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">DoubleType</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code class="n">data</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="p">)</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="n">Row</code><code class="p">(</code><code class="n">Elevation</code><code class="o">=</code><code class="mi">2596</code><code class="p">,</code><code class="n">Aspect</code><code class="o">=</code><code class="mi">51</code><code class="p">,</code><code class="n">Slope</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code><code class="n">Horizontal_Distance_To_Hydrology</code><code class="o">=</code><code class="mi">258</code><code class="p">,</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>+ concatenates collections.</p></dd>
</dl>
<p>The wilderness- and soil-related columns are named <code>Wilderness_Area_0</code>, <code>Soil_Type_0</code>, etc., and a bit of Python can generate these 44 names without having to type them all out. Finally, the target <code>Cover_Type</code> column is cast to a <code>double</code> value up front, because it will actually be necessary to consume it as a <code>double</code> rather than <code>int</code> in all PySpark MLlib APIs. This will become apparent later.</p>
<p>You can call <code>data.show</code> to see some rows of the dataset, but the display is so wide that it will be difficult to read it all. <code>data.head</code> displays it as a raw <code>Row</code> object, which will be more readable in this case.</p>
<p>Now that we’re familiar with our dataset and have processed it, we can train a decision tree model.<a data-startref="ch04-prep" data-type="indexterm" id="idm46507981512640"/><a data-startref="ch04-prep2" data-type="indexterm" id="idm46507981511936"/><a data-startref="ch04-prep4" data-type="indexterm" id="idm46507981511264"/><a data-startref="ch04-prep5" data-type="indexterm" id="idm46507981510592"/></p>
</div></section>
<section data-pdf-bookmark="Our First Decision Tree" data-type="sect1"><div class="sect1" id="idm46507981964544">
<h1>Our First Decision Tree</h1>
<p>In <a data-type="xref" href="ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set">Chapter 3</a>, we built a recommender model right away<a data-primary="decision trees" data-secondary="creating" data-type="indexterm" id="ch04-create"/><a data-primary="forest-covered land decision tree" data-secondary="creating decision tree" data-type="indexterm" id="ch04-create2"/> on all of the available data. This created a recommender that could be sense-checked by anyone with some knowledge of music: looking at a user’s listening habits and recommendations, we got some sense that it was producing good results. Here, that is not possible. We would have no idea how to make up a 54-feature description of a new parcel of land in Colorado, or what kind of forest cover to expect from such a parcel.</p>
<p>Instead, we must jump straight to<a data-primary="decision trees" data-secondary="creating" data-tertiary="holding out data for" data-type="indexterm" id="idm46507981467280"/><a data-primary="models" data-secondary="scoring and model evaluation" data-tertiary="holding out data for" data-type="indexterm" id="idm46507981466192"/><a data-primary="forest-covered land decision tree" data-secondary="creating decision tree" data-tertiary="holding out data for" data-type="indexterm" id="idm46507981465104"/><a data-primary="data subsets for machine learning" data-secondary="holding out data for" data-type="indexterm" id="idm46507981464016"/> holding out some data for purposes of evaluating the resulting model. Before, the AUC metric was used to assess the agreement between held-out listening data and predictions from recommendations. <a data-primary="AUC (area under the curve) metric" data-type="indexterm" id="idm46507981463040"/><a data-primary="accuracy evaluation metric" data-type="indexterm" id="idm46507981462432"/>AUC may be viewed as the probability that a randomly chosen good recommendation ranks above a randomly chosen bad recommendation. The principle is the same here, although the evaluation metric will be different: <em>accuracy</em>. The majority—90%—of the data will again be used for training, and, later, we’ll see that a subset of this training set will be held out for cross-validation (the CV set).
The other 10% held out here is actually a third subset, a proper test set.</p>
<pre data-code-language="python" data-type="programlisting"><code class="p">(</code><code class="n">train_data</code><code class="p">,</code> <code class="n">test_data</code><code class="p">)</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">randomSplit</code><code class="p">([</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">])</code>
<code class="n">train_data</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code>
<code class="n">test_data</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code></pre>
<p>The data needs a little more preparation to be used<a data-primary="decision trees" data-secondary="Covtype dataset" data-tertiary="VectorAssembler of feature vector" data-type="indexterm" id="idm46507981437120"/><a data-primary="forest-covered land decision tree" data-secondary="Covtype dataset" data-tertiary="VectorAssembler of feature vector" data-type="indexterm" id="idm46507981436032"/><a data-primary="vectors" data-secondary="feature vectors" data-tertiary="VectorAssembler of feature vector" data-type="indexterm" id="idm46507981434912"/><a data-primary="feature vectors" data-secondary="VectorAssembler of feature vector" data-type="indexterm" id="idm46507981433728"/> with a classifier in MLlib. The input DataFrame contains many columns, each holding one feature that could be used to predict the target column. MLlib requires all of the inputs to be collected into <em>one</em> column, whose value is a vector. PySpark’s <code>VectorAssembler</code> class is an abstraction for vectors in the linear algebra sense and contains only numbers. For most intents and purposes, they work like a simple array of <code>double</code> values (floating-point numbers). Of course, some of the input features are conceptually categorical, even if they’re all represented with numbers in the input.</p>
<p>Fortunately, the <code>VectorAssembler</code> class can do this work:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">ml</code><code class="nn">.</code><code class="nn">feature</code><code> </code><code class="kn">import</code><code> </code><code class="n">VectorAssembler</code><code>
</code><code>
</code><code class="n">input_cols</code><code> </code><code class="o">=</code><code> </code><code class="n">colnames</code><code class="p">[</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code class="n">vector_assembler</code><code> </code><code class="o">=</code><code> </code><code class="n">VectorAssembler</code><code class="p">(</code><code class="n">inputCols</code><code class="o">=</code><code class="n">input_cols</code><code class="p">,</code><code>
</code><code>                                    </code><code class="n">outputCol</code><code class="o">=</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code class="n">assembled_train_data</code><code> </code><code class="o">=</code><code> </code><code class="n">vector_assembler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code><code>
</code><code>
</code><code class="n">assembled_train_data</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="n">truncate</code><code> </code><code class="o">=</code><code> </code><code class="kc">False</code><code class="p">)</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code> </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">|</code><code class="n">featureVector</code><code>                                                       </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code> </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">|</code><code class="p">(</code><code class="mi">54</code><code class="p">,</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">5</code><code class="p">,</code><code class="mi">6</code><code class="p">,</code><code class="mi">7</code><code class="p">,</code><code class="mi">8</code><code class="p">,</code><code class="mi">9</code><code class="p">,</code><code class="mi">13</code><code class="p">,</code><code class="mi">18</code><code class="p">]</code><code class="p">,</code><code class="p">[</code><code class="mf">1874.0</code><code class="p">,</code><code class="mf">18.0</code><code class="p">,</code><code class="mf">14.0</code><code class="p">,</code><code class="mf">90.0</code><code class="p">,</code><code class="mf">208.0</code><code class="p">,</code><code class="mf">209.0</code><code class="p">,</code><code> </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">|</code><code class="p">(</code><code class="mi">54</code><code class="p">,</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">,</code><code class="mi">3</code><code class="p">,</code><code class="mi">4</code><code class="p">,</code><code class="mi">5</code><code class="p">,</code><code class="mi">6</code><code class="p">,</code><code class="mi">7</code><code class="p">,</code><code class="mi">8</code><code class="p">,</code><code class="mi">9</code><code class="p">,</code><code class="mi">13</code><code class="p">,</code><code class="mi">18</code><code class="p">]</code><code class="p">,</code><code class="p">[</code><code class="mf">1879.0</code><code class="p">,</code><code class="mf">28.0</code><code class="p">,</code><code class="mf">19.0</code><code class="p">,</code><code class="mf">30.0</code><code class="p">,</code><code class="mf">12.0</code><code class="p">,</code><code class="mf">95.0</code><code class="p">,</code><code> </code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Excludes the label, Cover_Type</p></dd>
</dl>
<p>The key parameters of <code>VectorAssembler</code> are the columns to combine into the feature vector, and the name of the new column containing the feature vector. Here, all columns—<em>except</em> the target, of course—are included as input features. The resulting DataFrame has a new <code>featureVector</code> column, as shown.</p>
<p>The output doesn’t look exactly like a sequence of numbers, but that’s because this shows a raw representation of the vector, represented as a <code>SparseVector</code> instance to save storage. Because most of the 54 values are 0, it stores only nonzero values and their indices. This detail won’t matter in classification.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507981381232">
<h5>Feature Vector</h5>
<p>Consider predicting tomorrow’s high temperature<a data-primary="predictions" data-secondary="tomorrow’s high from today’s temperature" data-type="indexterm" id="idm46507981379200"/><a data-primary="temperature predictions" data-type="indexterm" id="idm46507981378256"/><a data-primary="feature vectors" data-type="indexterm" id="idm46507981376368"/><a data-primary="weather predictions" data-type="indexterm" id="idm46507981375696"/><a data-primary="numeric data" data-secondary="feature vectors" data-type="indexterm" id="idm46507981375024"/><a data-primary="predictions" data-secondary="feature vectors" data-type="indexterm" id="idm46507981374080"/><a data-primary="vectors" data-secondary="feature vectors" data-type="indexterm" id="idm46507981373136"/> given today’s weather. There is nothing wrong with this idea, but “today’s weather” is a casual concept that requires structuring before it can be fed into a learning algorithm. Really, it is certain <em>features</em> of today’s weather that may predict tomorrow’s temperature, such as high temperature, low temperature, average humidity, whether it’s cloudy, rainy, or clear today, and the number of weather forecasters predicting a cold snap tomorrow.</p>
<p>These features are also sometimes<a data-primary="dimensions (features)" data-type="indexterm" id="idm46507981370944"/><a data-primary="predictors (features)" data-type="indexterm" id="idm46507981337696"/><a data-primary="variables (features)" data-type="indexterm" id="idm46507981337024"/> called <em>dimensions</em>, <em>predictors</em>, or just <em>variables</em>. Each of these features can be quantified. For example, high and low temperatures are measured in degrees Celsius, humidity can be measured as a fraction between 0 and 1, and weather type can be labeled <code>cloudy</code>, <code>rainy</code>, or <code>clear</code>. The number of forecasters is, of course, an integer count. Today’s weather might therefore be reduced to a list of values like <code>13.1,19.0,0.73,cloudy,1</code>.</p>
<p>These five features together, in order, are known as a <em>feature vector</em> and can describe any day’s weather. This usage bears
some resemblance to use of the term <em>vector</em> in linear algebra, except that a vector in this sense can conceptually contain nonnumeric values, and even lack some values. These features are not all of the same type. The first two features are measured in degrees Celsius, but the third is unitless, a fraction. The fourth is not a number at all, and the fifth is a number that is always a nonnegative integer.</p>
<p>A learning algorithm needs to train on data in order to make predictions. Feature vectors provide an organized way to describe input to a learning algorithm (here: <code>12.5,15.5,0.10,clear,0</code>). The output, or <em>target</em>, of the prediction can also be thought of as a feature. Here, it is a numeric feature: <code>17.2</code>.</p>
<p>It’s not uncommon to simply include the target as another feature in the feature vector. <a data-primary="training set" data-type="indexterm" id="idm46507980833600"/>The entire training example might be thought of as <code>12.5,15.5,0.10,clear,0,17.2</code>. The collection of all of these examples is known as the <em>training set</em>.</p>
</div></aside>
<p><code>VectorAssembler</code> is an example of <code>Transformer</code> within<a data-primary="MLlib component of Spark" data-secondary="Pipeline" data-tertiary="VectorAssembler" data-type="indexterm" id="idm46507980830464"/><a data-primary="Transformer class" data-type="indexterm" id="idm46507980829216"/><a data-primary="VectorAssembler" data-type="indexterm" id="idm46507980828544"/><a data-primary="transformations of distributed data" data-secondary="VectorAssembler" data-type="indexterm" id="idm46507980827872"/><a data-primary="Pipeline" data-secondary="VectorAssembler" data-type="indexterm" id="idm46507980826960"/> the current MLlib Pipelines API. It transforms the input DataFrame into another DataFrame based on some logic, and is composable with other transformations into a pipeline. Later in this chapter, these transformations will be connected into an actual <code>Pipeline</code>. Here, the transformation is just invoked directly, which is sufficient to build a first decision tree classifier model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.classification</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>

<code class="n">classifier</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">seed</code> <code class="o">=</code> <code class="mi">1234</code><code class="p">,</code> <code class="n">labelCol</code><code class="o">=</code><code class="s2">"Cover_Type"</code><code class="p">,</code>
                                    <code class="n">featuresCol</code><code class="o">=</code><code class="s2">"featureVector"</code><code class="p">,</code>
                                    <code class="n">predictionCol</code><code class="o">=</code><code class="s2">"prediction"</code><code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">classifier</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">assembled_train_data</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">toDebugString</code><code class="p">)</code>
<code class="o">...</code>
<code class="n">DecisionTreeClassificationModel</code><code class="p">:</code> <code class="n">uid</code><code class="o">=</code><code class="n">DecisionTreeClassifier_da03f8ab5e28</code><code class="p">,</code> <code class="o">...</code>
  <code class="n">If</code> <code class="p">(</code><code class="n">feature</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="mf">3036.5</code><code class="p">)</code>
   <code class="n">If</code> <code class="p">(</code><code class="n">feature</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="mf">2546.5</code><code class="p">)</code>
    <code class="n">If</code> <code class="p">(</code><code class="n">feature</code> <code class="mi">10</code> <code class="o">&lt;=</code> <code class="mf">0.5</code><code class="p">)</code>
     <code class="n">If</code> <code class="p">(</code><code class="n">feature</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="mf">2412.5</code><code class="p">)</code>
      <code class="n">If</code> <code class="p">(</code><code class="n">feature</code> <code class="mi">3</code> <code class="o">&lt;=</code> <code class="mf">15.0</code><code class="p">)</code>
       <code class="n">Predict</code><code class="p">:</code> <code class="mf">4.0</code>
      <code class="n">Else</code> <code class="p">(</code><code class="n">feature</code> <code class="mi">3</code> <code class="o">&gt;</code> <code class="mf">15.0</code><code class="p">)</code>
       <code class="n">Predict</code><code class="p">:</code> <code class="mf">3.0</code>
     <code class="n">Else</code> <code class="p">(</code><code class="n">feature</code> <code class="mi">0</code> <code class="o">&gt;</code> <code class="mf">2412.5</code><code class="p">)</code>
       <code class="o">...</code></pre>
<p>Again, the essential configuration for the classifier consists of column names: the column containing the input feature vectors and the column containing the target value to predict. Because the model will later be used to predict new values of the target, it is given the name of a column to store predictions.</p>
<p>Printing a representation of the model shows some of its tree structure. It consists of a series of nested decisions about features, comparing feature values to thresholds. (Here, for historical reasons, the features are referred to only by number, not name, unfortunately.)</p>
<p>Decision trees are able to assess<a data-primary="decision trees" data-secondary="feature importance assessment" data-type="indexterm" id="idm46507981315856"/><a data-primary="forest-covered land decision tree" data-secondary="feature importance assessment" data-type="indexterm" id="idm46507981314960"/> the importance of input features as part of their building process. That is, they can estimate how much each input feature contributes to making correct predictions. This information is simple to access from the model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">featureImportances</code><code class="o">.</code><code class="n">toArray</code><code class="p">(),</code>
            <code class="n">index</code><code class="o">=</code><code class="n">input_cols</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'importance'</code><code class="p">])</code><code class="o">.</code>\
            <code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s2">"importance"</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
<code class="o">...</code>
                                  <code class="n">importance</code>
<code class="n">Elevation</code>                         <code class="mf">0.826854</code>
<code class="n">Hillshade_Noon</code>                    <code class="mf">0.029087</code>
<code class="n">Soil_Type_1</code>                       <code class="mf">0.028647</code>
<code class="n">Soil_Type_3</code>                       <code class="mf">0.026447</code>
<code class="n">Wilderness_Area_0</code>                 <code class="mf">0.024917</code>
<code class="n">Horizontal_Distance_To_Hydrology</code>  <code class="mf">0.024862</code>
<code class="n">Soil_Type_31</code>                      <code class="mf">0.018573</code>
<code class="n">Wilderness_Area_2</code>                 <code class="mf">0.012458</code>
<code class="n">Horizontal_Distance_To_Roadways</code>   <code class="mf">0.003608</code>
<code class="n">Hillshade_9am</code>                     <code class="mf">0.002840</code>
<code class="o">...</code></pre>
<p>This pairs importance values (higher is better) with column names and prints them in order from most to least important. Elevation seems to dominate as the most important feature; most features are estimated to have virtually no importance when predicting the cover type!</p>
<p>The resulting <code>DecisionTreeClassificationModel</code> is itself<a data-primary="transformations of distributed data" data-secondary="decision trees" data-type="indexterm" id="idm46507981126320"/><a data-primary="predictions" data-secondary="decision trees" data-tertiary="transformer" data-type="indexterm" id="idm46507981125552"/> a transformer because it can transform a dataframe containing feature vectors into a dataframe also containing predictions.</p>
<p>For example, it might be interesting to see what the model predicts on the training data and compare its prediction with the known correct cover type:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">assembled_train_data</code><code class="p">)</code>
<code class="n">predictions</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"Cover_Type"</code><code class="p">,</code> <code class="s2">"prediction"</code><code class="p">,</code> <code class="s2">"probability"</code><code class="p">)</code><code class="o">.</code>\
            <code class="n">show</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="n">truncate</code> <code class="o">=</code> <code class="kc">False</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+----------+----------+------------------------------------------------</code> <code class="o">...</code>
<code class="o">|</code><code class="n">Cover_Type</code><code class="o">|</code><code class="n">prediction</code><code class="o">|</code><code class="n">probability</code>                                      <code class="o">...</code>
<code class="o">+----------+----------+------------------------------------------------</code> <code class="o">...</code>
<code class="o">|</code><code class="mf">6.0</code>       <code class="o">|</code><code class="mf">4.0</code>       <code class="o">|</code><code class="p">[</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.028372324539571926</code><code class="p">,</code><code class="mf">0.2936784469885515</code><code class="p">,</code> <code class="o">...</code>
<code class="o">|</code><code class="mf">6.0</code>       <code class="o">|</code><code class="mf">3.0</code>       <code class="o">|</code><code class="p">[</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.024558587479935796</code><code class="p">,</code><code class="mf">0.6454654895666132</code><code class="p">,</code> <code class="o">...</code>
<code class="o">|</code><code class="mf">6.0</code>       <code class="o">|</code><code class="mf">3.0</code>       <code class="o">|</code><code class="p">[</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.024558587479935796</code><code class="p">,</code><code class="mf">0.6454654895666132</code><code class="p">,</code> <code class="o">...</code>
<code class="o">|</code><code class="mf">6.0</code>       <code class="o">|</code><code class="mf">3.0</code>       <code class="o">|</code><code class="p">[</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.0</code><code class="p">,</code><code class="mf">0.024558587479935796</code><code class="p">,</code><code class="mf">0.6454654895666132</code><code class="p">,</code> <code class="o">...</code>
<code class="o">...</code></pre>
<p>Interestingly, the output also contains<a data-primary="probability vectors of decision tree" data-type="indexterm" id="idm46507980740864"/> a <code>probability</code> column that gives the model’s estimate of how likely it is that each possible outcome is correct. This shows that in these instances, it’s fairly sure the answer is 3 in several cases and quite sure the answer isn’t 1.</p>
<p>Eagle-eyed readers might note that the<a data-primary="vectors" data-secondary="probability vector 0 value" data-type="indexterm" id="idm46507980697504"/><a data-primary="decision trees" data-secondary="creating" data-tertiary="probability vector 0 value" data-type="indexterm" id="idm46507980696592"/> probability vectors actually have eight values even though there are only seven possible outcomes. The vector’s values at indices 1 to 7 do contain the probability of outcomes 1 to 7. However, there is also a value at index 0, which always shows as probability 0.0. This can be ignored, as 0 isn’t even a valid outcome, as this says. It’s a quirk of representing this information as a vector that’s worth being aware of.</p>
<p>Based on the above snippet, it looks like the model could use some work. Its predictions look like they are often wrong.
As with the ALS implementation in <a data-type="xref" href="ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set">Chapter 3</a>, the<a data-primary="decision trees" data-secondary="hyperparameters" data-type="indexterm" id="idm46507980694096"/><a data-primary="hyperparameters" data-secondary="decision trees" data-type="indexterm" id="idm46507980693120"/><a data-primary="forest-covered land decision tree" data-secondary="decision tree hyperparameters" data-type="indexterm" id="idm46507980692176"/> <code>DecisionTreeClassifier</code> implementation has several hyperparameters
for which a value must be chosen, and they’ve all been left to defaults here. Here, the test set can be used to produce an unbiased evaluation of the expected accuracy of a model built with these default hyperparameters.</p>
<p>We will now use <code>MulticlassClassificationEvaluator</code> to compute<a data-primary="MLlib component of Spark" data-secondary="MulticlassClassificationEvaluator" data-type="indexterm" id="idm46507980689248"/><a data-primary="MulticlassClassificationEvaluator metric" data-type="indexterm" id="idm46507980600512"/><a data-primary="dataframes" data-secondary="assessing quality of output" data-type="indexterm" id="idm46507980599872"/><a data-primary="decision trees" data-secondary="creating" data-tertiary="assessing quality of output" data-type="indexterm" id="idm46507980598912"/><a data-primary="forest-covered land decision tree" data-secondary="creating decision tree" data-tertiary="assessing quality of output" data-type="indexterm" id="idm46507980597680"/><a data-primary="predictions" data-secondary="decision trees" data-tertiary="assessing quality of predictions" data-type="indexterm" id="idm46507980596432"/><a data-primary="classifiers" data-secondary="MulticlassClassificationEvaluator" data-type="indexterm" id="idm46507980595200"/> accuracy and other metrics that evaluate the quality of the model’s predictions. It’s an example of an evaluator in MLlib, which is responsible for assessing the quality of an output DataFrame in some way:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.evaluation</code> <code class="kn">import</code> <code class="n">MulticlassClassificationEvaluator</code>

<code class="n">evaluator</code> <code class="o">=</code> <code class="n">MulticlassClassificationEvaluator</code><code class="p">(</code><code class="n">labelCol</code><code class="o">=</code><code class="s2">"Cover_Type"</code><code class="p">,</code>
                                        <code class="n">predictionCol</code><code class="o">=</code><code class="s2">"prediction"</code><code class="p">)</code>

<code class="n">evaluator</code><code class="o">.</code><code class="n">setMetricName</code><code class="p">(</code><code class="s2">"accuracy"</code><code class="p">)</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">predictions</code><code class="p">)</code>
<code class="n">evaluator</code><code class="o">.</code><code class="n">setMetricName</code><code class="p">(</code><code class="s2">"f1"</code><code class="p">)</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">predictions</code><code class="p">)</code>

<code class="o">...</code>
<code class="mf">0.6989423087953562</code>
<code class="mf">0.6821216079701136</code></pre>
<p>After being given the column containing the “label” (target, or known correct output value) and the name of the column containing the prediction, it finds that the two match about 70% of the time. This is the accuracy of this classifier. It can compute other related measures, like the F1 score. For our purposes here, accuracy will be used to evaluate classifiers.</p>
<p>This single number gives a good summary of the quality of the classifier’s output. <a data-primary="confusion matrix" data-type="indexterm" id="idm46507980526240"/><a data-primary="decision trees" data-secondary="creating" data-tertiary="confusion matrix" data-type="indexterm" id="idm46507980555408"/><a data-primary="predictions" data-secondary="decision trees" data-tertiary="confusion matrix" data-type="indexterm" id="idm46507980554224"/>Sometimes, however, it can be useful to look at the <em>confusion matrix</em>. This is a table with a row and a column for every possible value of the target. Because there are seven target category values, this is a 7×7 matrix, where each row corresponds to an actual correct value, and each column to a predicted value, in order. The entry at row <em>i</em> and column <em>j</em> counts the number of times an example with true category <em>i</em> was predicted as category <em>j</em>. So, the correct predictions are the counts along the diagonal, and the predictions are everything else.</p>
<p>It’s possible to calculate a<a data-primary="dataframes" data-secondary="DataFrameWriter API" data-tertiary="confusion matrix" data-type="indexterm" id="idm46507980550096"/> confusion matrix directly with the DataFrame API, using its more general operators.</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">confusion_matrix</code><code> </code><code class="o">=</code><code> </code><code class="n">predictions</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>  </code><code class="n">pivot</code><code class="p">(</code><code class="s2">"</code><code class="s2">prediction</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">8</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>  </code><code class="n">na</code><code class="o">.</code><code class="n">fill</code><code class="p">(</code><code class="mf">0.0</code><code class="p">)</code><code class="o">.</code><code>\</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>  </code><code class="n">orderBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code class="n">confusion_matrix</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code>
</code><code class="o">|</code><code class="n">Cover_Type</code><code class="o">|</code><code>     </code><code class="mi">1</code><code class="o">|</code><code>     </code><code class="mi">2</code><code class="o">|</code><code>    </code><code class="mi">3</code><code class="o">|</code><code>  </code><code class="mi">4</code><code class="o">|</code><code>  </code><code class="mi">5</code><code class="o">|</code><code>  </code><code class="mi">6</code><code class="o">|</code><code>    </code><code class="mi">7</code><code class="o">|</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code>
</code><code class="o">|</code><code>       </code><code class="mf">1.0</code><code class="o">|</code><code class="mi">133792</code><code class="o">|</code><code> </code><code class="mi">51547</code><code class="o">|</code><code>  </code><code class="mi">109</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code> </code><code class="mi">5223</code><code class="o">|</code><code>
</code><code class="o">|</code><code>       </code><code class="mf">2.0</code><code class="o">|</code><code> </code><code class="mi">57026</code><code class="o">|</code><code class="mi">192260</code><code class="o">|</code><code> </code><code class="mi">4888</code><code class="o">|</code><code> </code><code class="mi">57</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">750</code><code class="o">|</code><code>
</code><code class="o">|</code><code>       </code><code class="mf">3.0</code><code class="o">|</code><code>     </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">3368</code><code class="o">|</code><code class="mi">28238</code><code class="o">|</code><code class="mi">590</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>    </code><code class="mi">0</code><code class="o">|</code><code>
</code><code class="o">|</code><code>       </code><code class="mf">4.0</code><code class="o">|</code><code>     </code><code class="mi">0</code><code class="o">|</code><code>     </code><code class="mi">0</code><code class="o">|</code><code> </code><code class="mi">1493</code><code class="o">|</code><code class="mi">956</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>    </code><code class="mi">0</code><code class="o">|</code><code>
</code><code class="o">|</code><code>       </code><code class="mf">5.0</code><code class="o">|</code><code>     </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">8282</code><code class="o">|</code><code>  </code><code class="mi">283</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>    </code><code class="mi">0</code><code class="o">|</code><code>
</code><code class="o">|</code><code>       </code><code class="mf">6.0</code><code class="o">|</code><code>     </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">3371</code><code class="o">|</code><code class="mi">11872</code><code class="o">|</code><code class="mi">406</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>    </code><code class="mi">0</code><code class="o">|</code><code>
</code><code class="o">|</code><code>       </code><code class="mf">7.0</code><code class="o">|</code><code>  </code><code class="mi">8122</code><code class="o">|</code><code>    </code><code class="mi">74</code><code class="o">|</code><code>    </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code>  </code><code class="mi">0</code><code class="o">|</code><code class="mi">10319</code><code class="o">|</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Replace null with 0.</p></dd>
</dl>
<p>Spreadsheet users may have recognized the problem as just like that of computing a pivot table.<a data-primary="PIVOT function" data-type="indexterm" id="idm46507980376896"/><a data-primary="Spark SQL module" data-secondary="PIVOT function" data-type="indexterm" id="idm46507980376192"/><a data-primary="SQL for data analysis" data-secondary="PIVOT function" data-type="indexterm" id="idm46507980375248"/> A pivot table groups values by two dimensions, whose values become rows and columns of the output, and computes some aggregation within those groupings, like a count here. This is also available as a PIVOT function in several databases and is supported by Spark SQL. It’s arguably more elegant and powerful to compute it this way.</p>
<p>Although 70% accuracy sounds decent, it’s not immediately clear whether it is outstanding or poor. How well would a simplistic approach do to establish a baseline? Just as a broken clock is correct twice a day, randomly guessing a classification for each example would also occasionally produce the correct answer.</p>
<p>We could construct such a random “classifier” by picking a class at random in proportion to its prevalence in the training set. For example, if 30% of the training set were cover type 1, then the random classifier would guess “1” 30% of the time. Each classification would be correct in proportion to its prevalence in the test set. If 40% of the test set were cover type 1, then guessing “1” would be correct 40% of the time. Cover type 1 would then be guessed correctly 30% x 40% = 12% of the time and contribute 12% to overall accuracy. Therefore, we can evaluate the accuracy by summing these products of probabilities:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">sql</code><code> </code><code class="kn">import</code><code> </code><code class="n">DataFrame</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">class_probabilities</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">total</code><code> </code><code class="o">=</code><code> </code><code class="n">data</code><code class="o">.</code><code class="n">count</code><code class="p">(</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">data</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code>\</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>    </code><code class="n">orderBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>    </code><code class="n">select</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">count</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">DoubleType</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>    </code><code class="n">withColumn</code><code class="p">(</code><code class="s2">"</code><code class="s2">count_proportion</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">count</code><code class="s2">"</code><code class="p">)</code><code class="o">/</code><code class="n">total</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>    </code><code class="n">select</code><code class="p">(</code><code class="s2">"</code><code class="s2">count_proportion</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">collect</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code>
</code><code class="n">train_prior_probabilities</code><code> </code><code class="o">=</code><code> </code><code class="n">class_probabilities</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code><code>
</code><code class="n">test_prior_probabilities</code><code> </code><code class="o">=</code><code> </code><code class="n">class_probabilities</code><code class="p">(</code><code class="n">test_data</code><code class="p">)</code><code>
</code><code>
</code><code class="n">train_prior_probabilities</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code>
</code><code class="p">[</code><code class="n">Row</code><code class="p">(</code><code class="n">count_proportion</code><code class="o">=</code><code class="mf">0.36455357859838705</code><code class="p">)</code><code class="p">,</code><code>
</code><code> </code><code class="n">Row</code><code class="p">(</code><code class="n">count_proportion</code><code class="o">=</code><code class="mf">0.4875111371136425</code><code class="p">)</code><code class="p">,</code><code>
</code><code> </code><code class="n">Row</code><code class="p">(</code><code class="n">count_proportion</code><code class="o">=</code><code class="mf">0.06155716924206445</code><code class="p">)</code><code class="p">,</code><code>
</code><code> </code><code class="n">Row</code><code class="p">(</code><code class="n">count_proportion</code><code class="o">=</code><code class="mf">0.00468236760696409</code><code class="p">)</code><code class="p">,</code><code>
</code><code> </code><code class="n">Row</code><code class="p">(</code><code class="n">count_proportion</code><code class="o">=</code><code class="mf">0.016375858943914835</code><code class="p">)</code><code class="p">,</code><code>
</code><code> </code><code class="n">Row</code><code class="p">(</code><code class="n">count_proportion</code><code class="o">=</code><code class="mf">0.029920118693908142</code><code class="p">)</code><code class="p">,</code><code>
</code><code> </code><code class="n">Row</code><code class="p">(</code><code class="n">count_proportion</code><code class="o">=</code><code class="mf">0.03539976980111887</code><code class="p">)</code><code class="p">]</code><code>
</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code>
</code><code class="n">train_prior_probabilities</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">p</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="k">for</code><code> </code><code class="n">p</code><code> </code><code class="ow">in</code><code> </code><code class="n">train_prior_probabilities</code><code class="p">]</code><code>
</code><code class="n">test_prior_probabilities</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">p</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="k">for</code><code> </code><code class="n">p</code><code> </code><code class="ow">in</code><code> </code><code class="n">test_prior_probabilities</code><code class="p">]</code><code>
</code><code>
</code><code class="nb">sum</code><code class="p">(</code><code class="p">[</code><code class="n">train_p</code><code> </code><code class="o">*</code><code> </code><code class="n">cv_p</code><code> </code><code class="k">for</code><code> </code><code class="n">train_p</code><code class="p">,</code><code> </code><code class="n">cv_p</code><code> </code><code class="ow">in</code><code> </code><code class="nb">zip</code><code class="p">(</code><code class="n">train_prior_probabilities</code><code class="p">,</code><code>
</code><code>                                              </code><code class="n">test_prior_probabilities</code><code class="p">)</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code>
</code><code class="mf">0.37735294664034547</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Count by category</p></dd>
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Order counts by category</p></dd>
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Sum products of pairs in training and test sets</p></dd>
</dl>
<p>Random guessing achieves 37% accuracy then, which makes 70% seem like a good result after all. But the latter result was achieved with default hyperparameters. We can do even better by exploring what the hyperparameters actually mean for the tree-building process. That is what we will do in the next section.<a data-startref="ch04-create" data-type="indexterm" id="idm46507980063904"/><a data-startref="ch04-create2" data-type="indexterm" id="idm46507980385936"/></p>
</div></section>
<section data-pdf-bookmark="Decision Tree Hyperparameters" data-type="sect1"><div class="sect1" id="idm46507980385008">
<h1>Decision Tree Hyperparameters</h1>
<p>In <a data-type="xref" href="ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set">Chapter 3</a>, the ALS algorithm<a data-primary="decision trees" data-secondary="hyperparameters" data-type="indexterm" id="idm46507980332320"/><a data-primary="hyperparameters" data-secondary="decision trees" data-type="indexterm" id="idm46507980331344"/><a data-primary="forest-covered land decision tree" data-secondary="decision tree hyperparameters" data-type="indexterm" id="idm46507980330400"/> exposed several hyperparameters whose values we had to choose by building models with various combinations of values and then assessing the quality of each result using some metric. The process is the same here, although the metric is now multiclass accuracy instead of AUC. The hyperparameters controlling how the tree’s decisions are chosen will be quite different as well: maximum depth, maximum bins, impurity measure, and minimum information gain.</p>
<p><em>Maximum depth</em> simply limits the number<a data-primary="overfitting" data-secondary="hyperparameter selection and" data-type="indexterm" id="idm46507980224928"/><a data-primary="hyperparameters" data-secondary="overfitting" data-tertiary="hyperparameter selection and" data-type="indexterm" id="idm46507980223888"/><a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="maximum depth" data-type="indexterm" id="idm46507980222656"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="maximum depth" data-type="indexterm" id="idm46507980221440"/><a data-primary="decision trees" data-secondary="overfitting" data-tertiary="hyperparameter selection and" data-type="indexterm" id="idm46507980312832"/> of levels in the decision tree. It is the maximum number of chained decisions that the classifier will make to classify an example. It is useful to limit this to avoid overfitting the training data, as illustrated previously in the pet store example.</p>
<p>The decision tree algorithm is responsible for coming up with potential decision rules to try at each level, like the <code>weight &gt;= 100</code> or <code>weight &gt;= 500</code> decisions in the pet store example. Decisions are always of the same form: for numeric features, decisions are of the form <code>feature &gt;= value</code>; and for categorical features, they are of the form <code>feature in (value1, value2, …)</code>. So, the set of decision rules to try is really a set of values to plug in to the decision rule. <a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="bins" data-type="indexterm" id="idm46507980232496"/><a data-primary="bins for decision rules" data-type="indexterm" id="idm46507980231280"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="bins" data-type="indexterm" id="idm46507980230608"/>These are referred to as <em>bins</em> in the PySpark MLlib implementation. A larger number of bins requires more processing time but might lead to finding a more optimal decision rule.</p>
<p>What makes a decision rule good? <a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="good decision rules" data-type="indexterm" id="idm46507980228592"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="good decision rules" data-type="indexterm" id="idm46507980143648"/>Intuitively, a good rule would meaningfully distinguish examples by target category value. For example, a rule that divides the Covtype dataset into examples with only categories 1–3 on the one hand and 4–7 on the other would be excellent because it clearly separates some categories from others. A rule that resulted in about the same mix of all categories as are found in the whole dataset doesn’t seem helpful. Following either branch of such a decision leads to about the same distribution of possible target values and so doesn’t really make progress toward a confident classification.</p>
<p>Put another way, good rules divide the training data’s target values into relatively homogeneous, or “pure,” subsets. Picking a best rule means minimizing the impurity of the two subsets it induces. There are two commonly used measures of impurity: Gini impurity and entropy.</p>
<p><em>Gini impurity</em> is directly related to the accuracy<a data-primary="classifiers" data-secondary="random guess classifier hyperparameter" data-type="indexterm" id="idm46507980140688"/><a data-primary="random guess classifier hyperparameter" data-type="indexterm" id="idm46507980139648"/><a data-primary="Gini impurity hyperparameter" data-type="indexterm" id="idm46507980217456"/><a data-primary="impurity hyperparameter" data-type="indexterm" id="idm46507980216768"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="Gini impurity" data-type="indexterm" id="idm46507980216096"/><a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="Gini impurity" data-type="indexterm" id="idm46507980214880"/> of the random guess classifier. Within a subset, it is the probability that a randomly chosen classification of a randomly chosen example (both according to the distribution of classes in the subset) is
<em>incorrect</em>. To calculate this value, we first multiply each class with its respective proportion among all classes. Then we subtract the sum of all the values from 1. If a subset has <em>N</em> classes and <em>p</em><sub><em>i</em></sub> is the proportion of examples of class <em>i</em>,
then its Gini impurity is given in the Gini impurity equation:</p>
<div class="fifty-percent" data-type="equation">
<math alttext="upper I Subscript upper G Baseline left-parenthesis p right-parenthesis equals 1 minus sigma-summation Underscript i equals 1 Overscript upper N Endscripts p Subscript i Superscript 2" display="block">
<mrow>
<msub><mi>I</mi> <mi>G</mi> </msub>
<mrow>
<mo>(</mo>
<mi>p</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mn>1</mn>
<mo>-</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi> </munderover>
<msubsup><mi>p</mi> <mi>i</mi> <mn>2</mn> </msubsup>
</mrow>
</math>
</div>
<p>If the subset contains only one class, this value is 0 because it is completely “pure.” When there
are <em>N</em> classes in the subset, this value is larger than 0 and is largest when the classes occur
the same number of times—maximally impure.</p>
<p><em>Entropy</em> is another measure of impurity,<a data-primary="entropy as impurity metric" data-type="indexterm" id="idm46507979911632"/><a data-primary="impurity hyperparameter" data-secondary="entropy metric" data-type="indexterm" id="idm46507979910960"/><a data-primary="Gini impurity hyperparameter" data-secondary="entropy metric" data-type="indexterm" id="idm46507979910016"/><a data-primary="uncertainty" data-secondary="entropy metric" data-type="indexterm" id="idm46507979909056"/><a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="entropy" data-type="indexterm" id="idm46507979908112"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="entropy" data-type="indexterm" id="idm46507980322720"/> borrowed from information theory. Its nature is more
difficult to explain, but it captures how much uncertainty the collection of
target values in the subset implies about predictions for data that falls in that subset.
A subset containing one class suggests that the outcome for the subset is completely certain and has 0 entropy—no uncertainty. A subset containing one of each possible class, on the other hand,
suggests a lot of uncertainty about predictions for that subset because data has been observed with all kinds
of target values. This has high entropy. Hence, low entropy, like low Gini impurity, is a good thing. Entropy is defined
by the entropy equation:</p>
<div class="fifty-percent" data-type="equation">
<math alttext="upper I Subscript upper E Baseline left-parenthesis p right-parenthesis equals sigma-summation Underscript i equals 1 Overscript upper N Endscripts p Subscript i Baseline log left-parenthesis StartFraction 1 Over p Subscript i Baseline EndFraction right-parenthesis equals minus sigma-summation Underscript i equals 1 Overscript upper N Endscripts p Subscript i Baseline log left-parenthesis p Subscript i Baseline right-parenthesis" display="block">
<mrow>
<msub><mi>I</mi> <mi>E</mi> </msub>
<mrow>
<mo>(</mo>
<mi>p</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi> </munderover>
<msub><mi>p</mi> <mi>i</mi> </msub>
<mo form="prefix">log</mo>
<mrow>
<mo>(</mo>
<mfrac><mn>1</mn> <msub><mi>p</mi> <mi>i</mi> </msub></mfrac>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mo>-</mo>
<munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi> </munderover>
<msub><mi>p</mi> <mi>i</mi> </msub>
<mo form="prefix">log</mo>
<mrow>
<mo>(</mo>
<msub><mi>p</mi> <mi>i</mi> </msub>
<mo>)</mo>
</mrow>
</mrow>
</math>
</div>
<div data-type="note" epub:type="note">
<p>Interestingly, uncertainty has units.<a data-primary="uncertainty" data-secondary="units of" data-type="indexterm" id="idm46507980036480"/><a data-primary="nats (uncertainty unit)" data-type="indexterm" id="idm46507980035504"/><a data-primary="bits (uncertainty unit)" data-type="indexterm" id="idm46507980034832"/><a data-primary="information gain of a decision rule" data-type="indexterm" id="idm46507980034160"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="information gain" data-type="indexterm" id="idm46507980033472"/><a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="information gain" data-type="indexterm" id="idm46507980032256"/> Because the logarithm is the natural log (base <em>e</em>), the units are <em>nats</em>, the base <em>e</em> counterpart to more familiar <em>bits</em> (which we can obtain by using log base 2 instead). It really is measuring information, so it’s also common to talk about the <em>information gain</em> of a decision rule when using entropy with decision trees.</p>
</div>
<p>One or the other measure may be a better metric for picking decision rules in a given dataset. They are, in a way, similar. Both involve a weighted average: a sum over values weighted by <em>p</em><sub><em>i</em></sub>. The default in PySpark’s implementation is Gini impurity.</p>
<p>Finally, <em>minimum information gain</em> is a hyperparameter<a data-primary="information gain of a decision rule" data-secondary="minimum information gain hyperparameter" data-type="indexterm" id="idm46507979659888"/><a data-primary="minimum information gain hyperparameter" data-type="indexterm" id="idm46507979658832"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="minimum information gain" data-type="indexterm" id="idm46507979658144"/><a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="minimum information gain" data-type="indexterm" id="idm46507979656912"/> that imposes a minimum information gain, or decrease in impurity,
for candidate decision rules. Rules that do not improve the subsets’ impurity enough are rejected. Like a lower maximum depth, this can help the model resist overfitting because decisions that barely help divide the training input may in fact not helpfully divide future data at all.</p>
<p>Now that we understand the relevant hyperparameters of a decision tree algorithm, we will tune our model in the next section to improve its performance.</p>
</div></section>
<section data-pdf-bookmark="Tuning Decision Trees" data-type="sect1"><div class="sect1" id="idm46507980384416">
<h1>Tuning Decision Trees</h1>
<p>It’s not obvious from looking at the data<a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="tuning" data-type="indexterm" id="ch04-tun"/><a data-primary="tuning decision trees" data-type="indexterm" id="ch04-tun2"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="tuning" data-type="indexterm" id="ch04-tun3"/><a data-primary="forest-covered land decision tree" data-secondary="decision tree hyperparameters" data-tertiary="tuning" data-type="indexterm" id="ch04-tun4"/> which impurity measure leads to better accuracy or what maximum depth or number of bins is enough without being excessive. Fortunately, as in <a data-type="xref" href="ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set">Chapter 3</a>, it’s simple to let PySpark try a number of combinations of these values and report the results.</p>
<p>First, it’s necessary to set up a pipeline<a data-primary="Pipeline" data-secondary="VectorAssembler plus DecisionTreeClassifier" data-type="indexterm" id="idm46507979645824"/><a data-primary="MLlib component of Spark" data-secondary="Pipeline" data-tertiary="VectorAssembler plus DecisionTreeClassifier" data-type="indexterm" id="idm46507979644880"/> encapsulating the two steps we performed in previous sections—creating a feature vector and using it to create a decision tree model. Creating the <code>VectorAssembler</code> and <code>DecisionTreeClassifier</code> and chaining these two <code>Transformer</code>s together results in a single <code>Pipeline</code> object that represents
these two operations together as one operation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="n">assembler</code> <code class="o">=</code> <code class="n">VectorAssembler</code><code class="p">(</code><code class="n">inputCols</code><code class="o">=</code><code class="n">input_cols</code><code class="p">,</code> <code class="n">outputCol</code><code class="o">=</code><code class="s2">"featureVector"</code><code class="p">)</code>
<code class="n">classifier</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">1234</code><code class="p">,</code> <code class="n">labelCol</code><code class="o">=</code><code class="s2">"Cover_Type"</code><code class="p">,</code>
                                    <code class="n">featuresCol</code><code class="o">=</code><code class="s2">"featureVector"</code><code class="p">,</code>
                                    <code class="n">predictionCol</code><code class="o">=</code><code class="s2">"prediction"</code><code class="p">)</code>

<code class="n">pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">(</code><code class="n">stages</code><code class="o">=</code><code class="p">[</code><code class="n">assembler</code><code class="p">,</code> <code class="n">classifier</code><code class="p">])</code></pre>
<p>Naturally, pipelines can be much longer and more complex. This is about as simple as it gets. <a data-primary="hyperparameters" data-secondary="ParamGridBuilder for testing" data-type="indexterm" id="idm46507979585088"/><a data-primary="ML component of Spark" data-secondary="ParamGridBuilder for hyperparameters" data-type="indexterm" id="idm46507979584240"/><a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="ParamGridBuilder for testing" data-type="indexterm" id="idm46507979583392"/>Now we can also define the combinations of hyperparameters that should be tested using the PySpark ML API’s built-in support, <code>ParamGridBuilder</code>. <a data-primary="classifiers" data-secondary="MulticlassClassificationEvaluator" data-type="indexterm" id="idm46507979581792"/><a data-primary="predictions" data-secondary="decision trees" data-tertiary="assessing quality of predictions" data-type="indexterm" id="idm46507979580944"/><a data-primary="decision trees" data-secondary="creating" data-tertiary="assessing quality of output" data-type="indexterm" id="idm46507979579856"/><a data-primary="forest-covered land decision tree" data-secondary="creating decision tree" data-tertiary="assessing quality of output" data-type="indexterm" id="idm46507979578768"/><a data-primary="dataframes" data-secondary="assessing quality of output" data-type="indexterm" id="idm46507979577680"/><a data-primary="MLlib component of Spark" data-secondary="MulticlassClassificationEvaluator" data-type="indexterm" id="idm46507979576832"/><a data-primary="MulticlassClassificationEvaluator metric" data-type="indexterm" id="idm46507979575984"/>It’s also time to define the evaluation metric that will be used to pick the “best” hyperparameters, and that is again <code>MulticlassClassificationEvaluator</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.tuning</code> <code class="kn">import</code> <code class="n">ParamGridBuilder</code>

<code class="n">paramGrid</code> <code class="o">=</code> <code class="n">ParamGridBuilder</code><code class="p">()</code><code class="o">.</code> \
  <code class="n">addGrid</code><code class="p">(</code><code class="n">classifier</code><code class="o">.</code><code class="n">impurity</code><code class="p">,</code> <code class="p">[</code><code class="s2">"gini"</code><code class="p">,</code> <code class="s2">"entropy"</code><code class="p">])</code><code class="o">.</code> \
  <code class="n">addGrid</code><code class="p">(</code><code class="n">classifier</code><code class="o">.</code><code class="n">maxDepth</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">20</code><code class="p">])</code><code class="o">.</code> \
  <code class="n">addGrid</code><code class="p">(</code><code class="n">classifier</code><code class="o">.</code><code class="n">maxBins</code><code class="p">,</code> <code class="p">[</code><code class="mi">40</code><code class="p">,</code> <code class="mi">300</code><code class="p">])</code><code class="o">.</code> \
  <code class="n">addGrid</code><code class="p">(</code><code class="n">classifier</code><code class="o">.</code><code class="n">minInfoGain</code><code class="p">,</code> <code class="p">[</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.05</code><code class="p">])</code><code class="o">.</code> \
  <code class="n">build</code><code class="p">()</code>

<code class="n">multiclassEval</code> <code class="o">=</code> <code class="n">MulticlassClassificationEvaluator</code><code class="p">()</code><code class="o">.</code> \
  <code class="n">setLabelCol</code><code class="p">(</code><code class="s2">"Cover_Type"</code><code class="p">)</code><code class="o">.</code> \
  <code class="n">setPredictionCol</code><code class="p">(</code><code class="s2">"prediction"</code><code class="p">)</code><code class="o">.</code> \
  <code class="n">setMetricName</code><code class="p">(</code><code class="s2">"accuracy"</code><code class="p">)</code></pre>
<p>This means that a model will be built and evaluated for two values of four hyperparameters. That’s 16 models. They’ll be evaluated by multiclass accuracy. <a data-primary="TrainValidationSplit" data-type="indexterm" id="idm46507979478816"/>Finally, <code>TrainValidationSplit</code> brings these components together—the pipeline that makes models, model evaluation metrics, and hyperparameters to try—and can run the evaluation on the training data. It’s worth noting that <code>CrossValidator</code> could be used here as well to perform full k-fold cross-validation, but it is <em>k</em> times more expensive and doesn’t add as much value in the presence of big data. So, <code>TrainValidationSplit</code> is used here:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.tuning</code> <code class="kn">import</code> <code class="n">TrainValidationSplit</code>

<code class="n">validator</code> <code class="o">=</code> <code class="n">TrainValidationSplit</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">1234</code><code class="p">,</code>
  <code class="n">estimator</code><code class="o">=</code><code class="n">pipeline</code><code class="p">,</code>
  <code class="n">evaluator</code><code class="o">=</code><code class="n">multiclassEval</code><code class="p">,</code>
  <code class="n">estimatorParamMaps</code><code class="o">=</code><code class="n">paramGrid</code><code class="p">,</code>
  <code class="n">trainRatio</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>

<code class="n">validator_model</code> <code class="o">=</code> <code class="n">validator</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code></pre>
<p>This will take several minutes or more, depending on your hardware, because it’s building and evaluating many models. <a data-primary="decision trees" data-secondary="creating" data-tertiary="holding out data for" data-type="indexterm" id="idm46507979434608"/><a data-primary="forest-covered land decision tree" data-secondary="creating decision tree" data-tertiary="holding out data for" data-type="indexterm" id="idm46507979389328"/><a data-primary="models" data-secondary="scoring and model evaluation" data-tertiary="holding out data for" data-type="indexterm" id="idm46507979388240"/><a data-primary="data subsets for machine learning" data-secondary="holding out data for" data-type="indexterm" id="idm46507979387152"/>Note the train ratio parameter is set to 0.9. This means that the training data is actually further subdivided by <code>TrainValidationSplit</code> into 90%/10% subsets. The former is used for training each model. The remaining 10% of the input is held out as a cross-validation
set to evaluate the model. If it’s already holding out some data for evaluation, then why did we hold out 10% of the
original data as a test set?</p>
<p>If the purpose of the CV set was to evaluate <em>parameters</em> that fit to the <em>training</em> set, then the purpose of the test set is to evaluate <em>hyperparameters</em> that were “fit” to the CV set. That is, the test set ensures an unbiased estimate of the accuracy of the final, chosen model and its hyperparameters.</p>
<p>Say that the best model chosen by this process exhibits 90% accuracy on the CV set. It seems reasonable to expect it will exhibit 90% accuracy on future data. However, there’s an element of randomness in how these models are built. <a data-primary="overfitting" data-secondary="hyperparameters can overfit" data-type="indexterm" id="idm46507979383744"/><a data-primary="hyperparameters" data-secondary="overfitting" data-tertiary="hyperparameters can overfit" data-type="indexterm" id="idm46507979382896"/><a data-primary="decision trees" data-secondary="overfitting" data-tertiary="hyperparameters can overfit" data-type="indexterm" id="idm46507979381808"/>By chance, this
model and evaluation could have turned out unusually well. The top model and evaluation result could have benefited from a bit of luck, so its accuracy estimate is likely to be slightly optimistic. Put another way, hyperparameters can overfit too.</p>
<p>To really assess how well this best model is likely to perform on future examples, we need to evaluate it on examples that were not used to train it. But we also need to avoid examples in the CV set that were used to evaluate it. That is why a third subset, the test set, was held out.</p>
<p>The result of the validator contains the best model it found. This itself is a representation of the best overall <em>pipeline</em> it found, because we provided an instance of a pipeline to run. To query the parameters chosen by <code>DecisionTreeClassifier</code>, it’s necessary to manually extract <code>DecisionTreeClassificationModel</code> from the resulting <code>PipelineModel</code>, which is the final stage in the pipeline:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pprint</code> <code class="kn">import</code> <code class="n">pprint</code>

<code class="n">best_model</code> <code class="o">=</code> <code class="n">validator_model</code><code class="o">.</code><code class="n">bestModel</code>
<code class="n">pprint</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">stages</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">extractParamMap</code><code class="p">())</code>

<code class="o">...</code>
<code class="p">{</code><code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'predictionCol'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'prediction column name.'</code><code class="p">):</code> <code class="s1">'prediction'</code><code class="p">,</code>
 <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'probabilityCol'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'...'</code><code class="p">):</code> <code class="s1">'probability'</code><code class="p">,</code>
 <code class="p">[</code><code class="o">...</code><code class="p">]</code>
 <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'impurity'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'...'</code><code class="p">):</code> <code class="s1">'entropy'</code><code class="p">,</code>
 <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'maxDepth'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'...'</code><code class="p">):</code> <code class="mi">20</code><code class="p">,</code>
 <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'minInfoGain'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'...'</code><code class="p">):</code> <code class="mf">0.0</code><code class="p">,</code>
 <code class="p">[</code><code class="o">...</code><code class="p">]</code>
 <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'featuresCol'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'features column name.'</code><code class="p">):</code> <code class="s1">'featureVector'</code><code class="p">,</code>
 <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'maxBins'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'...'</code><code class="p">):</code> <code class="mi">40</code><code class="p">,</code>
 <code class="p">[</code><code class="o">...</code><code class="p">]</code>
 <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'labelCol'</code><code class="p">,</code> <code class="n">doc</code><code class="o">=</code><code class="s1">'label column name.'</code><code class="p">):</code> <code class="s1">'Cover_Type'</code><code class="p">}</code>
 <code class="o">...</code>
<code class="p">}</code></pre>
<p>This output contains a lot of information about the fitted model, but it also tells us that entropy apparently worked best as the impurity measure and that a max depth of 20 was not surprisingly better than 1. It might be surprising that the best model was fit with just 40 bins, but this is probably a sign that 40 was “plenty” rather than “better” than 300. Lastly, no minimum information gain was better than a small minimum, which could imply that the model is more prone to underfit than overfit.</p>
<p>You may wonder if it is possible to see the accuracy that each of the models achieved for each combination of hyperparameters.<a data-primary="getEstimatorParamMaps" data-type="indexterm" id="idm46507979374608"/><a data-primary="validationMetrics" data-type="indexterm" id="idm46507979374000"/><a data-primary="hyperparameters" data-secondary="decision trees" data-tertiary="accuracy and" data-type="indexterm" id="idm46507979373360"/><a data-primary="decision trees" data-secondary="hyperparameters" data-tertiary="accuracy and" data-type="indexterm" id="idm46507979295264"/> The hyperparameters and the evaluations are exposed by <code>getEstimatorParamMaps</code> and <code>validationMetrics</code>, respectively. They can be combined to display all of the parameter combinations sorted by metric value:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">validator_model</code> <code class="o">=</code> <code class="n">validator</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code>

<code class="n">metrics</code> <code class="o">=</code> <code class="n">validator_model</code><code class="o">.</code><code class="n">validationMetrics</code>
<code class="n">params</code> <code class="o">=</code> <code class="n">validator_model</code><code class="o">.</code><code class="n">getEstimatorParamMaps</code><code class="p">()</code>
<code class="n">metrics_and_params</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">metrics</code><code class="p">,</code> <code class="n">params</code><code class="p">))</code>

<code class="n">metrics_and_params</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">metrics_and_params</code>

<code class="o">...</code>
<code class="p">[(</code><code class="mf">0.9130409881445563</code><code class="p">,</code>
  <code class="p">{</code><code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'minInfoGain'</code> <code class="o">...</code><code class="p">):</code> <code class="mf">0.0</code><code class="p">,</code>
   <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'maxDepth'</code><code class="o">...</code><code class="p">):</code> <code class="mi">20</code><code class="p">,</code>
   <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'maxBins'</code> <code class="o">...</code><code class="p">):</code> <code class="mi">40</code><code class="p">,</code>
   <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'impurity'</code><code class="o">...</code><code class="p">):</code> <code class="s1">'entropy'</code><code class="p">}),</code>
 <code class="p">(</code><code class="mf">0.9112655352131498</code><code class="p">,</code>
  <code class="p">{</code><code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'minInfoGain'</code><code class="p">,</code><code class="o">...</code><code class="p">):</code> <code class="mf">0.0</code><code class="p">,</code>
   <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'maxDepth'</code> <code class="o">...</code><code class="p">):</code> <code class="mi">20</code><code class="p">,</code>
   <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'maxBins'</code><code class="o">...</code><code class="p">):</code> <code class="mi">300</code><code class="p">,</code>
   <code class="n">Param</code><code class="p">(</code><code class="o">...</code><code class="n">name</code><code class="o">=</code><code class="s1">'impurity'</code><code class="o">...</code><code class="p">:</code> <code class="s1">'entropy'</code><code class="p">}),</code>
<code class="o">...</code></pre>
<p>What was the accuracy that this model achieved on the CV set? And, finally, what accuracy
does the model achieve on the test set?</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">metrics</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code>
</code><code class="nb">print</code><code class="p">(</code><code class="n">metrics</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">)</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code>
</code><code class="mf">0.9130409881445563</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code>
</code><code class="n">multiclassEval</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">best_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">test_data</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="mf">0.9138921373048084</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p><code>best_Model</code> is a complete pipeline.</p></dd>
</dl>
<p>The results are both about 91%. It happens that the estimate from the CV set was pretty fine to begin with. In fact, it is not usual for the test set to show a very different result.</p>
<p>This is an interesting point at which to revisit<a data-primary="decision trees" data-secondary="overfitting" data-type="indexterm" id="idm46507978949024"/><a data-primary="overfitting" data-type="indexterm" id="idm46507978948144"/> the issue of overfitting. As discussed previously, it’s possible to build a decision tree so deep and elaborate that it fits the given training examples very well or perfectly but fails to generalize to other examples because it has fit the idiosyncrasies and noise of the training data too closely. This is a problem common to most machine learning algorithms, not just decision trees.</p>
<p>When a decision tree has overfit, it will exhibit high accuracy when run on the same training data that it fit the model to, but low accuracy on other examples. Here, the final model’s accuracy was about 91% on other, new examples. Accuracy can just as easily be evaluated over the same data that the model was trained on, <code>trainData</code>. This gives an accuracy of about 95%. <a data-primary="decision trees" data-secondary="overfitting" data-tertiary="hyperparameter selection and" data-type="indexterm" id="idm46507978946544"/><a data-primary="overfitting" data-secondary="hyperparameter selection and" data-type="indexterm" id="idm46507978945328"/>The difference is not large but suggests that the decision tree has overfit the training data to some extent. A lower maximum depth might be a better choice.</p>
<p>So far, we’ve implicitly treated all input features, including categoricals, as if they’re numeric. Can we improve our model’s performance further by treating categorical features as exactly that? We will explore this next.<a data-startref="ch04-tun" data-type="indexterm" id="idm46507978943904"/><a data-startref="ch04-tun2" data-type="indexterm" id="idm46507978910368"/><a data-startref="ch04-tun3" data-type="indexterm" id="idm46507978909760"/><a data-startref="ch04-tun4" data-type="indexterm" id="idm46507978909152"/></p>
</div></section>
<section data-pdf-bookmark="Categorical Features Revisited" data-type="sect1"><div class="sect1" id="idm46507979654256">
<h1>Categorical Features Revisited</h1>
<p>The categorical features in our dataset<a data-primary="decision trees" data-secondary="Covtype dataset" data-tertiary="categorical features revisited" data-type="indexterm" id="ch04-cat"/><a data-primary="categorical data" data-secondary="numeric treatment" data-type="indexterm" id="ch04-cat2"/><a data-primary="categorical data" data-secondary="one-hot encoding" data-tertiary="numeric treatment of" data-type="indexterm" id="idm46507978904656"/><a data-primary="one-hot encoding" data-secondary="numeric treatment of" data-type="indexterm" id="idm46507978903568"/><a data-primary="numeric data" data-secondary="categorical data treated as" data-type="indexterm" id="ch04-cat3"/><a data-primary="forest-covered land decision tree" data-secondary="Covtype dataset" data-tertiary="categorical features revisited" data-type="indexterm" id="ch04-cat4"/> are one-hot encoded as several binary 0/1 values. Treating these individual features as numeric turns out to be fine, because any decision rule on the “numeric” features will choose thresholds between 0 and 1, and all are equivalent since all values are 0 or 1.</p>
<p>Of course, this encoding forces the decision tree algorithm to consider the values of the underlying categorical features individually. Because features like soil type are broken down into many features and because decision trees treat features individually, it is harder to relate information about related soil types.</p>
<p>For example, nine different soil types are actually part of the Leighton family, and they may be related in ways that the decision tree can exploit. If soil type were encoded as a single categorical feature with 40 soil values, then the tree could express rules like “if the soil type is one of the nine Leighton family types” directly. However, when encoded as 40 features, the tree would have to learn a sequence of nine decisions on soil type to do the same, this expressiveness may lead to better decisions and more efficient trees.</p>
<p>However, having 40 numeric features represent one 40-valued categorical feature increases memory  usage and slows things down.</p>
<p>What about undoing the one-hot encoding? This would replace, for example, the four columns encoding wilderness type with one column that encodes the wilderness type as a number between 0 and 3, like <code>Cover_Type</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code><code> </code><code class="nf">unencode_one_hot</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">wilderness_cols</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">Wilderness_Area_</code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">)</code><code> </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code><code class="p">]</code><code>
</code><code>    </code><code class="n">wilderness_assembler</code><code> </code><code class="o">=</code><code> </code><code class="n">VectorAssembler</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                            </code><code class="n">setInputCols</code><code class="p">(</code><code class="n">wilderness_cols</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                            </code><code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">wilderness</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">unhot_udf</code><code> </code><code class="o">=</code><code> </code><code class="n">udf</code><code class="p">(</code><code class="k">lambda</code><code> </code><code class="n">v</code><code class="p">:</code><code> </code><code class="n">v</code><code class="o">.</code><code class="n">toArray</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">tolist</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">index</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code>    </code><code class="n">with_wilderness</code><code> </code><code class="o">=</code><code> </code><code class="n">wilderness_assembler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>      </code><code class="n">drop</code><code class="p">(</code><code class="o">*</code><code class="n">wilderness_cols</code><code class="p">)</code><code class="o">.</code><code>\</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>      </code><code class="n">withColumn</code><code class="p">(</code><code class="s2">"</code><code class="s2">wilderness</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">unhot_udf</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">wilderness</code><code class="s2">"</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">soil_cols</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">Soil_Type_</code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">)</code><code> </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">40</code><code class="p">)</code><code class="p">]</code><code>
</code><code>    </code><code class="n">soil_assembler</code><code> </code><code class="o">=</code><code> </code><code class="n">VectorAssembler</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                      </code><code class="n">setInputCols</code><code class="p">(</code><code class="n">soil_cols</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                      </code><code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">soil</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">with_soil</code><code> </code><code class="o">=</code><code> </code><code class="n">soil_assembler</code><code class="o">.</code><code>\
</code><code>                </code><code class="n">transform</code><code class="p">(</code><code class="n">with_wilderness</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                </code><code class="n">drop</code><code class="p">(</code><code class="o">*</code><code class="n">soil_cols</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                </code><code class="n">withColumn</code><code class="p">(</code><code class="s2">"</code><code class="s2">soil</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">unhot_udf</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">soil</code><code class="s2">"</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">with_soil</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Note UDF definition</p></dd>
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Drop one-hot columns; no longer needed</p></dd>
</dl>
<p>Here <code>VectorAssembler</code> is deployed to combine the 4 and 40 wilderness and soil type columns into two <code>Vector</code> columns. The values in these <code>Vector</code>s are all 0, except for one location that has a 1. There’s no simple DataFrame function for this, so we have to define our own UDF that can be used to operate on columns. This turns these two new columns into numbers of just the type we need.</p>
<p>We can now transform our dataset by removing one-hot encoding using our function defined above:<a data-primary="schemas of dataframes" data-secondary="forest-covered land parcels" data-type="indexterm" id="idm46507978682160"/><a data-primary="dataframes" data-secondary="schemas" data-tertiary="forest-covered land parcels" data-type="indexterm" id="idm46507978681216"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">unenc_train_data</code> <code class="o">=</code> <code class="n">unencode_one_hot</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code>
<code class="n">unenc_train_data</code><code class="o">.</code><code class="n">printSchema</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">root</code>
 <code class="o">|--</code> <code class="n">Elevation</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Aspect</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Slope</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Horizontal_Distance_To_Hydrology</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Vertical_Distance_To_Hydrology</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Horizontal_Distance_To_Roadways</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Hillshade_9am</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Hillshade_Noon</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Hillshade_3pm</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Horizontal_Distance_To_Fire_Points</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">Cover_Type</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">wilderness</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">soil</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
<code class="o">...</code>

<code class="n">unenc_train_data</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s1">'wilderness'</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>

<code class="o">+----------+------+</code>
<code class="o">|</code><code class="n">wilderness</code><code class="o">|</code> <code class="n">count</code><code class="o">|</code>
<code class="o">+----------+------+</code>
<code class="o">|</code>         <code class="mi">3</code><code class="o">|</code> <code class="mi">33271</code><code class="o">|</code>
<code class="o">|</code>         <code class="mi">0</code><code class="o">|</code><code class="mi">234532</code><code class="o">|</code>
<code class="o">|</code>         <code class="mi">1</code><code class="o">|</code> <code class="mi">26917</code><code class="o">|</code>
<code class="o">|</code>         <code class="mi">2</code><code class="o">|</code><code class="mi">228144</code><code class="o">|</code>
<code class="o">+----------+------+</code></pre>
<p>From here, nearly the same process as above can be used to tune the hyperparameters of a decision tree model built on this data and to choose and evaluate a best model. There’s one important difference, however. The two new numeric columns have nothing about them that indicates they’re actually an encoding of categorical values. To treat them as numbers is not correct, as their ordering is meaningless. The model will still get built but because of some information in these features not being available, the accuracy may suffer.</p>
<p>Internally MLlib can store additional metadata about each column. <a data-primary="metadata about categorial feature columns" data-type="indexterm" id="idm46507978595168"/><a data-primary="categorical data" data-secondary="metadata about" data-type="indexterm" id="idm46507978594560"/><a data-primary="decision trees" data-secondary="Covtype dataset" data-tertiary="metadata on categorial features" data-type="indexterm" id="idm46507978414528"/><a data-primary="forest-covered land decision tree" data-secondary="Covtype dataset" data-tertiary="metadata on categorial features" data-type="indexterm" id="idm46507978413344"/><a data-primary="VectorIndexer" data-type="indexterm" id="idm46507978412144"/>The details of this data are generally hidden from the caller but include information such as whether the column encodes a categorical value and how many distinct values it takes on. To add this metadata, it’s necessary to put the data through <code>VectorIndexer</code>. Its job is to turn input into properly labeled categorical feature columns. Although we did much of the work already to turn the categorical features into 0-indexed values, <code>VectorIndexer</code> will take care of the metadata.</p>
<p>We need to add this stage to the <code>Pipeline</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">ml</code><code class="nn">.</code><code class="nn">feature</code><code> </code><code class="kn">import</code><code> </code><code class="n">VectorIndexer</code><code>
</code><code>
</code><code class="n">cols</code><code> </code><code class="o">=</code><code> </code><code class="n">unenc_train_data</code><code class="o">.</code><code class="n">columns</code><code>
</code><code class="n">inputCols</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">c</code><code> </code><code class="k">for</code><code> </code><code class="n">c</code><code> </code><code class="ow">in</code><code> </code><code class="n">cols</code><code> </code><code class="k">if</code><code> </code><code class="n">c</code><code class="o">!=</code><code class="s1">'</code><code class="s1">Cover_Type</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>
</code><code class="n">assembler</code><code> </code><code class="o">=</code><code> </code><code class="n">VectorAssembler</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setInputCols</code><code class="p">(</code><code class="n">inputCols</code><code class="p">)</code><code class="o">.</code><code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code class="n">indexer</code><code> </code><code class="o">=</code><code> </code><code class="n">VectorIndexer</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>  </code><code class="n">setMaxCategories</code><code class="p">(</code><code class="mi">40</code><code class="p">)</code><code class="o">.</code><code>\</code><code> </code><a class="co" href="#callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1" id="co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>  </code><code class="n">setInputCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">indexedVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code class="n">classifier</code><code> </code><code class="o">=</code><code> </code><code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setLabelCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">Cover_Type</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                      </code><code class="n">setFeaturesCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">indexedVector</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                      </code><code class="n">setPredictionCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">prediction</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code class="n">pipeline</code><code> </code><code class="o">=</code><code> </code><code class="n">Pipeline</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setStages</code><code class="p">(</code><code class="p">[</code><code class="n">assembler</code><code class="p">,</code><code> </code><code class="n">indexer</code><code class="p">,</code><code> </code><code class="n">classifier</code><code class="p">]</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1" id="callout_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>&gt;= 40 because soil has 40 values</p></dd>
</dl>
<p>The approach assumes that the training set contains<a data-primary="missing data values" data-secondary="training on categorical features" data-type="indexterm" id="idm46507978247984"/> all possible values of each of the categorical features at least once. That is, it works correctly only if all 4 soil values and all 40 wilderness values appear in the training set so that all possible values get a mapping. Here, that happens to be true, but may not be for small training sets of data in which some labels appear very infrequently. In those cases, it could be necessary to manually create and add a <code>VectorIndexerModel</code> with the complete value mapping supplied manually.</p>
<p>Aside from that, the process is the same as before. You should find that it chose a similar best model but that accuracy on the test set is about 93%. By treating categorical features as actual categorical features in the previous sections, the classifier improved its accuracy by almost 2%.</p>
<p>We have trained and tuned a decision tree. Now, we will move on to random forests, a more powerful algorithm. As we will see in the next section, implementing them using PySpark will be surprisingly straightforward at this point.<a data-startref="ch04-cat" data-type="indexterm" id="idm46507978269712"/><a data-startref="ch04-cat2" data-type="indexterm" id="idm46507978269008"/><a data-startref="ch04-cat3" data-type="indexterm" id="idm46507978268336"/><a data-startref="ch04-cat4" data-type="indexterm" id="idm46507978267664"/></p>
</div></section>
<section data-pdf-bookmark="Random Forests" data-type="sect1"><div class="sect1" id="RandomDecisionForests">
<h1>Random Forests</h1>
<p>If you have been following along with the code examples,<a data-primary="decision trees" data-secondary="random forests" data-type="indexterm" id="ch04-ranfor"/><a data-primary="random forests" data-type="indexterm" id="ch04-ranfor2"/><a data-primary="decision trees" data-secondary="about" data-tertiary="randomness" data-type="indexterm" id="idm46507978211440"/><a data-primary="forest-covered land decision tree" data-secondary="about decision trees" data-tertiary="randomness" data-type="indexterm" id="idm46507978210352"/> you may have noticed that your results differ slightly from those presented in the code listings in the book. That is because there is an element of randomness in building decision trees, and the randomness comes into play when you’re deciding what data to use and what decision rules to explore.</p>
<p>The algorithm does not consider every possible decision rule at every level. To do so would take an incredible amount of time. For a categorical feature over <em>N</em> values, there are 2<sup><em>N</em></sup>–2 possible decision rules (every subset except the empty set and entire set). For an even moderately large <em>N</em>, this would create billions of candidate decision rules.</p>
<p>Instead, decision trees use several heuristics to determine which few rules to actually consider. The process of picking rules also involves some randomness; only a few features picked at random are looked at each time, and only values from a random subset of the training data. This trades a bit of accuracy for a lot of speed, but it also means that the decision tree algorithm won’t build the same tree every time. This is a good thing.</p>
<p>It’s good for the same reason that the “wisdom of the crowds” usually beats individual predictions. To illustrate, take this quick quiz: how many black taxis operate in London?</p>
<p>Don’t peek at the answer; guess first.</p>
<p>I guessed 10,000, which is well off the correct answer<a data-primary="regression" data-secondary="regression to the mean" data-type="indexterm" id="idm46507978205808"/> of about 19,000. Because I guessed low, you’re a bit more likely to have guessed higher than I did, and so the average of our answers will tend to be more accurate. There’s that regression to the mean again. The average guess from an informal poll of 13 people in the office was indeed closer: 11,170.</p>
<p>A key to this effect is that the guesses were independent and didn’t influence one another. (You didn’t peek, did you?) The exercise would be useless if we had all agreed on and used the same methodology to make a guess, because the guesses would have been the same answer—the same potentially quite wrong answer. It would even have been different and worse if I’d merely influenced you by stating my guess up front.</p>
<p>It would be great to have not one tree but many trees, each producing reasonable but different and independent estimations of the right target value. Their collective average prediction should fall close to the true answer, more than any individual tree’s does. It’s the <em>randomness</em> in the process of building that helps create this independence. This is the key to <em>random forests</em>.</p>
<p>Randomness is injected by building many trees, each of which sees a different random subset of data—and even of features. This makes the forest as a whole less prone to overfitting. If a particular feature contains noisy data or is deceptively predictive only in the <em>training</em> set, then most trees will not consider this problem feature most of the time. Most trees will not fit the noise and will tend to “outvote” the trees that have fit the noise in the forest.</p>
<p>The prediction of a random forest is simply a weighted average of the trees’ predictions. For a categorical target, this can be a majority vote or the most probable value based on the average of probabilities produced by the trees. Random forests, like decision trees, also support regression, and the forest’s prediction in this case is the average of the number predicted by each tree.</p>
<p>While random forests are a more powerful and complex classification technique, the good news is that it’s virtually no different to use it in the pipeline that has been developed in this chapter. Simply drop in a <code>RandomForestClassifier</code> in place of <code>DecisionTreeClassifier</code> and proceed as before. There’s really no more code or API to understand in order to use it:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.classification</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>

<code class="n">classifier</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">1234</code><code class="p">,</code> <code class="n">labelCol</code><code class="o">=</code><code class="s2">"Cover_Type"</code><code class="p">,</code>
                                    <code class="n">featuresCol</code><code class="o">=</code><code class="s2">"indexedVector"</code><code class="p">,</code>
                                    <code class="n">predictionCol</code><code class="o">=</code><code class="s2">"prediction"</code><code class="p">)</code></pre>
<p>Note that this classifier has another hyperparameter: the number of trees to build. Like the max bins hyperparameter, higher values should give better results up to a point. The cost, however, is that building many trees of course takes many times longer than building one.</p>
<p>The accuracy of the best random forest model produced from a similar tuning process is 95% off the bat—about 2% better already, although viewed another way, that’s a 28% reduction in the error rate over the best decision tree built previously,
from 7% down to 5%. You may do better with further tuning.</p>
<p>Incidentally, at this point we have a more reliable picture of feature importance:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">forest_model</code> <code class="o">=</code> <code class="n">best_model</code><code class="o">.</code><code class="n">stages</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>

<code class="n">feature_importance_list</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">input_cols</code><code class="p">,</code>
                                  <code class="n">forest_model</code><code class="o">.</code><code class="n">featureImportances</code><code class="o">.</code><code class="n">toArray</code><code class="p">()))</code>
<code class="n">feature_importance_list</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

<code class="n">pprint</code><code class="p">(</code><code class="n">feature_importance_list</code><code class="p">)</code>
<code class="o">...</code>
<code class="p">(</code><code class="mf">0.28877055118903183</code><code class="p">,</code><code class="n">Elevation</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.17288279582959612</code><code class="p">,</code><code class="n">soil</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.12105056811661499</code><code class="p">,</code><code class="n">Horizontal_Distance_To_Roadways</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.1121550648692802</code><code class="p">,</code><code class="n">Horizontal_Distance_To_Fire_Points</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.08805270405239551</code><code class="p">,</code><code class="n">wilderness</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.04467393191338021</code><code class="p">,</code><code class="n">Vertical_Distance_To_Hydrology</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.04293099150373547</code><code class="p">,</code><code class="n">Horizontal_Distance_To_Hydrology</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.03149644050848614</code><code class="p">,</code><code class="n">Hillshade_Noon</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.028408483578137605</code><code class="p">,</code><code class="n">Hillshade_9am</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.027185325937200706</code><code class="p">,</code><code class="n">Aspect</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.027075578474331806</code><code class="p">,</code><code class="n">Hillshade_3pm</code><code class="p">)</code>
<code class="p">(</code><code class="mf">0.015317564027809389</code><code class="p">,</code><code class="n">Slope</code><code class="p">)</code></pre>
<p>Random forests are appealing in the context of big data because trees are supposed to be built independently,<a data-primary="data-parallel model" data-type="indexterm" id="idm46507978091824"/><a data-primary="parallelizing" data-secondary="decision trees" data-type="indexterm" id="idm46507978091216"/> and big data technologies like Spark and MapReduce inherently need <em>data-parallel</em> problems, where parts of the overall solution can be computed independently on parts of the data. The fact that trees can, and should, train on only a subset of features or
input data makes it trivial to parallelize building the trees.<a data-startref="ch04-ranfor" data-type="indexterm" id="idm46507978019712"/><a data-startref="ch04-ranfor2" data-type="indexterm" id="idm46507978019008"/></p>
</div></section>
<section data-pdf-bookmark="Making Predictions" data-type="sect1"><div class="sect1" id="idm46507978266144">
<h1>Making Predictions</h1>
<p>Building a classifier, while an interesting and nuanced process, is not the end goal. <a data-primary="decision trees" data-secondary="predictions" data-type="indexterm" id="idm46507978016688"/><a data-primary="predictions" data-secondary="decision trees" data-type="indexterm" id="idm46507978015712"/>The goal is to make predictions. This is the payoff, and it is comparatively quite easy.</p>
<p>The resulting “best model” is actually a whole pipeline of operations. It encapsulates how input is transformed for use with the model and includes the model itself, which can make predictions. It can operate on a dataframe of new input. The only difference from the <code>data</code> DataFrame we started with is that it lacks the <code>Cover_Type</code> column. When we’re making predictions—especially about the future, says Mr. Bohr—the output is of course not known.</p>
<p>To prove it, try dropping the <code>Cover_Type</code> from the test data input and obtaining a prediction:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">unenc_test_data</code> <code class="o">=</code> <code class="n">unencode_one_hot</code><code class="p">(</code><code class="n">test_data</code><code class="p">)</code>
<code class="n">bestModel</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">unenc_test_data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"Cover_Type"</code><code class="p">))</code><code class="o">.</code>\
                    <code class="n">select</code><code class="p">(</code><code class="s2">"prediction"</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>

<code class="o">...</code>
<code class="o">+----------+</code>
<code class="o">|</code><code class="n">prediction</code><code class="o">|</code>
<code class="o">+----------+</code>
<code class="o">|</code>       <code class="mf">6.0</code><code class="o">|</code>
<code class="o">+----------+</code></pre>
<p>The result should be 6.0, which corresponds to class 7 (the original feature was 1-indexed) in the original Covtype dataset. The predicted cover type for the land described in this example is Krummholz.</p>
</div></section>
<section data-pdf-bookmark="Where to Go from Here" data-type="sect1"><div class="sect1" id="idm46507977970416">
<h1>Where to Go from Here</h1>
<p>This chapter introduced two related and important types of machine learning, classification and regression, along with some foundational concepts in building and tuning models: features, vectors, training, and cross-validation. It demonstrated how to predict a type of forest cover from things like location and soil type using the Covtype dataset, with decision trees and forests implemented in PySpark.</p>
<p>As with recommenders in <a data-type="xref" href="ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set">Chapter 3</a>, it could be useful to continue exploring the effect of hyperparameters on accuracy. Most decision tree hyperparameters trade time for accuracy: more bins and trees generally produce better accuracy but hit a point of diminishing returns.</p>
<p>The classifier here turned out to be very accurate. It’s unusual to achieve more than 95% accuracy. <a data-primary="iteration in data analysis" data-secondary="classifier model improvement" data-type="indexterm" id="idm46507977949632"/>In general, you will achieve further improvements in accuracy by including more features or transforming existing features into a more
predictive form. This is a common, repeated step in iteratively improving a classifier model. For example, for this dataset, the two features encoding horizontal and vertical distance-to-surface-water features could produce a third feature:  straight-line distance-to-surface-water features. This might turn out to be more useful than either original feature. Or, if it were possible to collect more data, we might try adding new information like soil moisture to improve classification.</p>
<p>Of course, not all prediction problems in the real world are exactly like the Covtype dataset. For example, some problems require predicting a continuous numeric value, not a categorical value. Much of the same analysis and code applies to this type of <em>regression</em> problem; the <code>RandomForestRegressor</code> class will be of use in this case.</p>
<p>Furthermore, decision trees and forests are not the only classification or regression algorithms, and not the only ones implemented in PySpark. Each algorithm operates quite differently from decision trees and forests. However, many elements are the same: they plug into a <code>Pipeline</code> and operate on columns in a dataframe, and have hyperparameters that you must select using training, cross-validation, and test subsets of the input data. The same general principles, with these other algorithms, can also be deployed to model classification and regression problems.</p>
<p>These have been examples of supervised learning. What happens when some, or all, of the target values are unknown? The following chapter will explore what can be done in this situation.</p>
</div></section>
</div></section></div></body></html>