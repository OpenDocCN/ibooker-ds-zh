- en: Chapter 3\. Fitting Functions to Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章。将函数拟合到数据
- en: '*Today it fits. Tomorrow?*'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*今天适合。明天呢？*'
- en: 'In this chapter, we introduce the core mathematical ideas lying at the heart
    of many AI applications, including the mathematical engines of neural networks.
    Our goal is to internalize the following structure of the machine learning part
    of an AI problem:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了许多人工智能应用的核心数学思想，包括神经网络的数学引擎。我们的目标是内化人工智能问题的机器学习部分的以下结构：
- en: 'Identify the problem, depending on the specific use case: Classify images,
    classify documents, predict house prices, detect fraud or anomalies, recommend
    the next product, predict the likelihood of a criminal re-offending, predict the
    internal structure of a building given external images, convert speech to text,
    generate audio, generate images, generate video, *etc*.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据具体用例识别问题：分类图像，分类文档，预测房价，检测欺诈或异常，推荐下一个产品，预测犯罪分子再次犯罪的可能性，根据外部图像预测建筑的内部结构，将语音转换为文本，生成音频，生成图像，生成视频，*等等*。
- en: Acquire the appropriate data, in order to *train* our models to do the right
    thing. We say that our models *learn* from the data. Make sure this data is clean,
    complete, and if necessary, depending on the specific model we are implementing,
    transformed (normalized, standarized, some features aggregated, *etc.*). This
    step is usually way more time consuming than implementing and training the machine
    learning models.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取适当的数据，以便*训练*我们的模型做正确的事情。我们说我们的模型从数据中*学习*。确保这些数据是干净的、完整的，并且如果必要的话，根据我们正在实施的具体模型进行转换（归一化、标准化、一些特征聚合，*等等*）。这一步通常比实施和训练机器学习模型耗费更多时间。
- en: Create a *hypothesis function*. We use the terms hypothesis function, *learning
    function*, *prediction function*, *training function*, and *model* interchangeably.
    Our main assumption is that this input/output mathematical function explains the
    observed data, and it can be used later to make predictions on new data. We give
    our model features, like a person’s daily habits, and it returns a prediction,
    like this person’s likelihood to pay back a loan. In this chapter, we will give
    our model the length measurements of a fish, and it will return its weight.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个*假设函数*。我们可以互换使用假设函数、*学习函数*、*预测函数*、*训练函数*和*模型*这些术语。我们的主要假设是，这个输入/输出的数学函数解释了观察到的数据，并且可以在以后用于对新数据进行预测。我们给我们的模型特征，比如一个人的日常习惯，它返回一个预测，比如这个人偿还贷款的可能性。在本章中，我们将给我们的模型鱼的长度测量，并且它将返回它的重量。
- en: We will encounter many models (including neural networks) where our training
    function has unknown parameters called *weights*. The goal is to find the numerical
    values of these weights using the data. After we find these weight values, we
    can use the *trained* function to make predictions, by plugging the features of
    a new data point into the formula of the trained function.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将遇到许多模型（包括神经网络），其中我们的训练函数具有称为*权重*的未知参数。目标是使用数据找到这些权重的数值。在找到这些权重值之后，我们可以使用*训练*函数进行预测，将新数据点的特征插入到训练函数的公式中。
- en: In order to find the values of the unknown weights, we create *another function*
    called the *error function*, the *cost function*, the *objective function*, or
    the *loss function* (everything in the AI field has three or more names). This
    function has to measure some sort of distance between the ground truth and our
    predictions. Naturally, we want our predictions to be as close to ground truths
    as possible, so we search for weight values that minimize our loss function. Mathematically,
    we solve a minimization problem. The field of *mathematical optimization* is essential
    to AI.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了找到未知权重的值，我们创建*另一个函数*，称为*误差函数*、*成本函数*、*目标函数*或*损失函数*（人工智能领域的一切都有三个或更多的名称）。这个函数必须衡量地面真相和我们的预测之间的某种距离。自然地，我们希望我们的预测尽可能接近地面真相，因此我们寻找最小化我们损失函数的权重值。从数学上讲，我们解决了一个最小化问题。*数学优化*领域对人工智能至关重要。
- en: Throughout this process, we are the engineers, so it is us who get to decide
    on the mathematical formulas for training functions, loss functions, optimization
    methods, and computer implementations. Different engineers decide on different
    processes, with different performance results, and that is okay. The judge, in
    the end, is the performance of the deployed model, and contrary to popular belief,
    mathematical models are flexible and can be tweaked and altered when needed. It
    is crucial to monitor performance after deployment.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个过程中，我们是工程师，所以我们决定训练函数、损失函数、优化方法和计算机实现的数学公式。不同的工程师决定不同的流程，产生不同的性能结果，这是可以接受的。最终的评判是部署模型的性能，与普遍看法相反，数学模型是灵活的，可以在需要时进行调整和修改。部署后监控性能至关重要。
- en: 'Since our goal is to find the weight values that minimize the error between
    our predictions and ground truths, we need to find an efficient mathematical way
    to search for these *minimizers*: Those special weight values that produce the
    least error. The *gradient descent* method plays a key role here. This powerful
    yet simple method involves calculating *one derivative* of our error function.
    This is one reason we spent half of our calculus classes calculating derivatives
    (and the gradient: this is one derivative in higher dimensions). There are other
    methods that require computing two derivatives. We will encounter them and comment
    on the benefits *vs.* the downsides of using higher order methods.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们的目标是找到最小化预测和实际值之间误差的权重数值，我们需要找到一种高效的数学方法来搜索这些*最小化者*：那些产生最小误差的特殊权重数值。*梯度下降*方法在这里起着关键作用。这种强大而简单的方法涉及计算我们误差函数的*一个导数*。这就是我们花了一半的微积分课程计算导数的原因之一（以及梯度：这是高维度中的一个导数）。还有其他需要计算两个导数的方法。我们将遇到它们，并评论使用高阶方法的利弊。
- en: When data sets are enormous and our model happens to be a layered neural network,
    we need an efficient way to calculate this one derivative. The *backpropagation
    algorithm* steps in at this point. We will walk through gradient descent and backpropagation
    in the next chapter.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据集非常庞大，而我们的模型恰好是一个分层神经网络时，我们需要一种高效的方法来计算这个导数。*反向传播算法*就是在这一点上发挥作用。我们将在下一章中讨论梯度下降和反向传播。
- en: If our learning function fits the given data too well, then it will not perform
    well on new data. The reason is that a function with too good of a fit with the
    data means that it picks up on the noise in the data as well as the signal (for
    example, the function on the left of [Figure 3-1](#Fig_noise_fit_regular_fit)).
    We do not want to pick up on noise. This is where *regularization* helps. There
    are multiple mathematical ways to regularize a function, which means make it smoother
    and less oscillatory and erratic. In general, a function that follows the noise
    in the data oscillates too much. We desire more regular functions. We visit regularization
    techniques in the next chapter.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的学习函数对给定数据拟合得太好，那么它在新数据上表现不佳。原因是对数据拟合得太好的函数意味着它既捕捉到了数据中的噪音，也捕捉到了信号（例如，[图3-1](#Fig_noise_fit_regular_fit)左侧的函数）。我们不希望捕捉到噪音。这就是*正则化*发挥作用的地方。有多种数学方法可以使函数正则化，这意味着使其更加平滑和不那么振荡和不规则。一般来说，跟随数据中的噪音的函数振荡太多。我们希望更规则的函数。我们将在下一章中介绍正则化技术。
- en: '![280](assets/emai_0301.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0301.png)'
- en: 'Figure 3-1\. Left: The fitting function fits the data perfectly, however, it
    is not a good prediction function since it fits the noise in the data istead of
    the main signal. Right: A more regular function fitting the same data set. Using
    this function will give better predictions than the function in the left subplot,
    even though the function in the left subplot matches the data points better.'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。左：拟合函数完美地拟合了数据，但它不是一个很好的预测函数，因为它拟合了数据中的噪音而不是主要信号。右：一个更规则的函数拟合了相同的数据集。使用这个函数将比左侧子图中的函数给出更好的预测，即使左侧子图中的函数更好地匹配了数据点。
- en: In the following sections, we explore the above structure of an AI problem with
    real, but simple, data sets. We will see in the next chapters how the same concepts
    generalize to much more involved tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将用真实但简单的数据集探索AI问题的上述结构。我们将在接下来的章节中看到相同的概念如何推广到更复杂的任务。
- en: Traditional And Very Useful Machine Learning Models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统且非常有用的机器学习模型
- en: All the data used in this chapter is *labeled* with ground truths, and the goal
    of our models is to *predict* the labels of new (unseen) and unlabeled data. This
    is *supervised* learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的所有数据都带有地面真相标签，我们模型的目标是*预测*新的（未见过的）和未标记的数据的标签。这是*监督*学习。
- en: 'In the next few sections, we fit training functions into our labeled data using
    the following popular machine learning models. While you may hear so much about
    the latest and greatest developments in AI, you are probably better off in a typical
    business setting starting with these more traditional models:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将使用以下流行的机器学习模型将训练函数拟合到我们的标记数据中。虽然您可能会听到关于AI最新和最伟大发展的许多消息，但在典型的商业环境中，您可能最好从这些更传统的模型开始：
- en: '**Linear Regression**: Predict a numerical value.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线性回归**：预测数值。'
- en: '**Logistic Regression**: Classify into two classes (binary classification).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：分类到两个类别（二元分类）。'
- en: '**Softmax Regression**: Classify into multiple classes.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Softmax回归**：分类到多个类别。'
- en: '**Support Vector Machines**: Classify into two classes, or regression (predict
    a numerical value).'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**支持向量机**：分类到两个类别，或回归（预测数值）。'
- en: '**Decision Trees**: Classify into any number of classes, or regression (predict
    a numerical value).'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**决策树**：分类到任意数量的类别，或回归（预测数值）。'
- en: '**Random Forests**: Classify into any number of classes, or regression (predict
    a numerical value).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机森林**：分类到任意数量的类别，或回归（预测数值）。'
- en: '**Ensembles of models**: Bundle up the results of many models, by averaging
    the prediction values, voting for the most popular class, or some other bundling
    mechanism.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型集成**：通过平均预测值、投票最受欢迎的类别或其他捆绑机制来捆绑许多模型的结果。'
- en: We try multiple models on the same data sets in order to compare performance.
    In the real world, it is rare that any model ever gets deployed without having
    been compared with many other models. This is the nature of the computation heavy
    AI industry, and it is why we need parallel computing, which ebables us to train
    multiple models at once (except for models that build and improve on the results
    of other models, like in the case of *stacking*. For those we cannot use parallel
    computing).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在相同的数据集上尝试多个模型以进行性能比较。在现实世界中，很少有任何模型在没有与许多其他模型进行比较的情况下部署。这是计算密集型AI行业的特性，这也是为什么我们需要并行计算的原因，它使我们能够同时训练多个模型（除了那些构建和改进其他模型结果的模型，比如*堆叠*的情况，我们不能使用并行计算）。
- en: 'Before we dive into any machine learning models, it is extremely important
    to note that it has been reported again and again that only about five percent
    of a data scientist’s time, and/or an AI researcher’s time, is spent on training
    machine learning models. The majority of the time is consumed by aquiring data,
    cleaning data, organizing data, creating appropriate pipelines for data, *etc.*,
    *before* feeding it data into machine learning models. So machine learning is
    only one step in the production process, and it is an easy step once the data
    is ready to train the model. We will discover how these machine learning models
    work: Most of the mathematics we need resides in these models. AI researchers
    are always trying to enhance machine learning models, and automatically fit them
    into production pipelines. It is therefore important for us to eventually learn
    about the whole pipeline, from raw data (including its storage, hardware, query
    protocols, *etc.*) to deployment to monitoring. Learning machine learning is only
    one piece of a bigger and more interesting story.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究任何机器学习模型之前，非常重要的一点是，一再有报道称，数据科学家和/或人工智能研究人员只有大约百分之五的时间用于训练机器学习模型。大部分时间都用于获取数据、清理数据、组织数据、为数据创建适当的管道，*等等*，*在*将数据输入机器学习模型之前。因此，机器学习只是生产过程中的一步，一旦数据准备好训练模型就变得很容易。我们将发现这些机器学习模型是如何工作的：我们需要的大部分数学知识都存在于这些模型中。人工智能研究人员一直在努力改进机器学习模型，并将它们自动适应到生产管道中。因此，对我们来说，最终学习整个流程，从原始数据（包括其存储、硬件、查询协议，*等等*）到部署和监控，是非常重要的。学习机器学习只是更大更有趣故事的一部分。
- en: We must start with *regression* since the ideas of regression are so fundamental
    for most of the AI models and applications that will follow. Only for *linear
    regression*, we find our minimizing weights using an *analytical* method, giving
    an explicit formula for the desired weights directly in terms of the training
    data set and its the target labels. It is the simplicity of the linear regression
    model that allows for this explicit analytical solution. Most other models do
    not have such explicit solutions and we have to find their minimizers using numerical
    methods, among which the gradient descent is extremely popular.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须从*回归*开始，因为回归的思想对于接下来的大多数人工智能模型和应用都是如此基础。只有对于*线性回归*，我们才会使用*解析*方法找到最小化权重，直接给出所需权重的显式公式，以训练数据集及其目标标签为参数。正是线性回归模型的简单性使得这种显式解析解成为可能。大多数其他模型没有这样的显式解，我们必须使用数值方法找到它们的最小值，其中梯度下降方法非常受欢迎。
- en: 'In regression and many other upcoming models, including the neural networks
    of the next few chapters, watch for the following progression in the modeling
    process:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归和许多其他即将出现的模型中，包括接下来几章的神经网络，要注意建模过程中的以下进展：
- en: The Training Function
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练函数
- en: The Loss Function
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数
- en: Optimization
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化
- en: Numerical Solutions *vs.* Analytical Solutions
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值解*vs.*解析解
- en: 'It is important to be aware of the difference between numerical solutions and
    analytical solutions of mathematical problems. A mathematical problem can be anything,
    such as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 了解数学问题的数值解和解析解之间的区别非常重要。数学问题可以是任何东西，比如：
- en: Find the minimizer of some function.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到某个函数的最小值。
- en: Find the best way to go from destination *A* to destination *B*, with a constrained
    budget.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到从目的地*A*到目的地*B*的最佳方式，预算受限。
- en: Find the best way to design and query a data warehouse.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到设计和查询数据仓库的最佳方式。
- en: Find the solution of a mathematical equation (where a left hand side with math
    stuff *equals* a right hand side with math stuff). These equations could be algebraic
    equations, ordinary differential equations, partial differential equations, integro-differential
    equations, systems of equations, or any sort of mathematical equations. Their
    solutions could be static or evolving in time. They could model anything from
    the physical, biological, socioeconomical or natural worlds.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找数学方程的解（其中左手边有数学内容*等于*右手边有数学内容）。这些方程可以是代数方程、常微分方程、偏微分方程、积分微分方程、方程组，或者任何类型的数学方程。它们的解可以是静态的，也可以随时间演变。它们可以模拟物理、生物、社会经济或自然世界的任何事物。
- en: 'Here is the vocabulary:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是词汇表：
- en: '*Numerical*: has to do with numbers.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数值*：与数字有关。'
- en: '*Analytical*: has to do with analysis.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解析*：与分析有关。'
- en: As a rule of thumb, numerical solutions are much easier to obtain and much more
    accessible than analytical solutions, provided that we have enough computational
    power to simulate and compute these solutions. All we usually need to do is discretize
    some continuous spaces and/or functions, albeit sometimes in very clever ways,
    and evaluate functions on these discrete quantities. The only problem with numerical
    solutions is that they are only approximate solutions. Unless they are backed
    by estimates on how far off they are from the true analytical solutions and how
    fast they converge to these true solutions, which in turn require mathematical
    backgroud and analysis, numerical solutions are not exact. They do however provide
    incredibly useful insights about the true solutions. In many cases, numerical
    solutions are the only ones available, and many scientific and engineering fields
    would not have advanced at all had they not relied on numerical solutions of complex
    problems. If those fields waited for analytical solutions and proofs to happen,
    or in other words for mathematical theory to *catch up*, they would’ve had very
    slow progress.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，数值解比分析解容易获得得多，也更容易获得，只要我们有足够的计算能力来模拟和计算这些解。我们通常只需要离散化一些连续空间和/或函数，尽管有时需要非常巧妙的方法，并在这些离散量上评估函数。数值解的唯一问题是它们只是近似解。除非它们有估计值表明它们与真实分析解的差距有多大，以及它们收敛到这些真实解的速度有多快，而这又需要数学背景和分析，数值解并不是精确的。然而，它们确实提供了关于真实解的非常有用的见解。在许多情况下，数值解是唯一可用的解，许多科学和工程领域如果不依赖于复杂问题的数值解，就不会有任何进展。如果这些领域等待分析解和证明发生，或者换句话说，等待数学理论“赶上”，它们的进展将会非常缓慢。
- en: Analytical solutions, on the other hand, are exact, robust, and have a whole
    mathematical theory backing them up. They come accompanied with theorems and proofs.
    When analytical solutions are available they are very powerful. They are, however,
    not easily accessible, sometimes impossible to obtain, and they do require deep
    knowledge and domain expertise in fields such as calculus, mathematical analysis,
    algebra, theory of differential equations, *etc*. Analytical methods, however,
    are extremely valuable for describing important properties of solutions (even
    when explicit solutions are not available), guiding numerical techniques, and
    providing ground truths to compare approximate numerical methods against (in the
    lucky cases when these analytical solutions are available).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，分析解是精确的、稳健的，并且有整个数学理论支持它们。它们伴随着定理和证明。当分析解可用时，它们非常强大。然而，它们并不容易获取，有时甚至是不可能的，并且确实需要在微积分、数学分析、代数、微分方程理论等领域具有深厚的知识和专业知识。然而，分析方法对于描述解的重要性质（即使明确的解不可用）、指导数值技术，并提供基本事实以比较近似数值方法（在这些分析解可用的幸运情况下）是极其有价值的。
- en: Some researchers are purely analytical and theoretical, others are purely numerical
    and computational, and the best place to exist is somewhere near the intersection,
    where we have a descent understanding of the analytical and the numerical aspects
    of our mathematical problems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员纯粹是分析和理论的，另一些则是纯粹数值和计算的，而最好的存在位置是在接近交集的地方，我们对数学问题的分析和数值方面有一个良好的理解。
- en: 'Regression: Predict A Numerical Value'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归：预测一个数值
- en: 'A quick search on [Kaggle website](https://www.kaggle.com) for data sets for
    regression returns many excellent data sets and related notebooks. I randomly
    chose a simple [Fish Market](https://www.kaggle.com/aungpyaeap/fish-market) data
    set which we will use to explain our upcoming mathematics. Our goal is to build
    a model that predicts the weight of a fish given its five different length measurements,
    or features, labeled in the data set as: Length1, Length2, Length3, Height, and
    Width (see [Figure 3-2](#Fig_fish_data)). For the sake of simplicity, we choose
    not to incorporate the categorical feature, Species, into this model, even though
    we could (and that would give us better predictions, since a fish type is a good
    predictor of its weight). If we choose to include the Species feature then we
    would have to convert its values into numerical values, using *one hot coding*,
    which means exactly as it sounds: Assign a code for each fish made up of ones
    and zeros based on its category (type). Our Species feature has seven categories:
    Perch, Bream, Roach, Pike, Smelt, Parkki, and Whitefish. So if our fish is Pike
    the we would code its species as (0,0,0,1,0,0,0) and if it is Bream we would code
    its species as (0,1,0,0,0,0,0). Of course this adds seven more dimensions to our
    feature space and seven more weights to train.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Kaggle网站](https://www.kaggle.com)上快速搜索回归数据集会返回许多优秀的数据集和相关笔记本。我随机选择了一个简单的[Fish
    Market](https://www.kaggle.com/aungpyaeap/fish-market)数据集，我们将用它来解释我们即将介绍的数学。我们的目标是构建一个模型，根据鱼的五种不同长度测量或特征来预测鱼的重量，这些特征在数据集中标记为：Length1、Length2、Length3、Height和Width（见[图3-2](#Fig_fish_data)）。为简单起见，我们选择不将分类特征Species纳入此模型，尽管我们可以（这样会给我们更好的预测，因为鱼的类型是其重量的良好预测因子）。如果我们选择包括Species特征，那么我们将不得不将其值转换为数值值，使用*one
    hot coding*，这意味着确切地说：根据其类别（类型）为每条鱼分配由一和零组成的代码。我们的Species特征有七个类别：鲈鱼、鲷鱼、鲫鱼、梭子鱼、胖鱼、Parkki和白鱼。因此，如果我们的鱼是梭子鱼，那么我们将把它的种类编码为（0,0,0,1,0,0,0），如果它是鲷鱼，我们将把它的种类编码为（0,1,0,0,0,0,0）。当然，这会给我们的特征空间增加七个维度，并增加七个权重来训练。
- en: '![280](assets/emai_0302.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0302.png)'
- en: Figure 3-2\. The first five rows of the fish data set downloaded from Kaggle’s
    [Fish Market](https://www.kaggle.com/aungpyaeap/fish-market). The Weight column
    is the target feature, and our goal is to build a model that predicts the weight
    of a new fish given its length measurements.
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2. 从Kaggle的[Fish Market](https://www.kaggle.com/aungpyaeap/fish-market)下载的鱼数据集的前五行。重量列是目标特征，我们的目标是构建一个模型，根据鱼的长度测量来预测新鱼的重量。
- en: 'Let’s save ink space and relabel our five features as: <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> , <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    , <math alttext="x 3"><msub><mi>x</mi> <mn>3</mn></msub></math> , <math alttext="x
    4"><msub><mi>x</mi> <mn>4</mn></msub></math> , and <math alttext="x 5"><msub><mi>x</mi>
    <mn>5</mn></msub></math> , then write the fish weight as a function of these five
    features <math alttext="y equals f left-parenthesis x 1 comma x 2 comma x 3 comma
    x 4 comma x 5 right-parenthesis"><mrow><mi>y</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>4</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>5</mn></msub> <mo>)</mo></mrow></math> . This
    way, once we settle on an acceptable formula for the this function, all we have
    to do is input the feature values for a certain fish and our function will output
    the predicted weight of that fish.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们节省墨水空间，并将我们的五个特征重新标记为：<math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>
    , <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math> , <math alttext="x
    3"><msub><mi>x</mi> <mn>3</mn></msub></math> , <math alttext="x 4"><msub><mi>x</mi>
    <mn>4</mn></msub></math> , 和 <math alttext="x 5"><msub><mi>x</mi> <mn>5</mn></msub></math>
    , 然后将鱼的重量写成这五个特征的函数 <math alttext="y equals f left-parenthesis x 1 comma x 2 comma
    x 3 comma x 4 comma x 5 right-parenthesis"><mrow><mi>y</mi> <mo>=</mo> <mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>4</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>5</mn></msub> <mo>)</mo></mrow></math> . 这样，一旦我们确定了这个函数的一个可接受的公式，我们只需要输入某条鱼的特征值，我们的函数就会输出该鱼的预测重量。
- en: 'This section builds a foundation for everything to come, so it is important
    to first see how it is organized:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本节为即将到来的一切构建了基础，因此首先看看它是如何组织的是很重要的：
- en: '**Training Function**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练函数**'
- en: Parametric models *vs.* non-parametric models
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数模型 *vs.* 非参数模型
- en: '**Loss function**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**'
- en: The predicted value *vs.* the true value
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测值 *vs.* 真实值
- en: The absolute value distance *vs.* the squared distance
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对值距离 *vs.* 平方距离
- en: Functions with singularities (pointy points)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有奇点（尖点）的函数
- en: For linear regression, the loss function is the Mean Squared Error
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于线性回归，损失函数是均方误差
- en: Vectors in this book are always column vectors
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书中的向量始终是列向量
- en: The Training, Validation and Test Subsets
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、验证和测试子集
- en: When the training data has highly correlated features
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练数据具有高度相关的特征时
- en: '**Optimization**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化**'
- en: Convex landscapes *vs.* non-convex lanscapes
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凸景观 *vs.* 非凸景观
- en: How do we locate minimizers of functions?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何找到函数的最小值点？
- en: Calculus in a nutshell
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微积分简介
- en: A one-dimensional optimization example
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一维优化示例
- en: Derivatives of linear algebra expressions that we use all the time
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们一直在使用的线性代数表达式的导数
- en: Minimizing the mean squared error loss function
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化均方误差损失函数
- en: 'Caution: Multiplying large matrices by each other is very expensive. Multiply
    matrices by vectors instead.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 警告：将大矩阵相乘是非常昂贵的。应该将矩阵乘以向量。
- en: 'Caution: We never want to fit the training data too well'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 警告：我们永远不希望训练数据拟合得太好
- en: Training Function
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练函数
- en: A quick exploration of the data, as in plotting the weight against the various
    length features, allows us to assume a linear model (even though a nonlinear one
    could be better in this case). That is, we assume that the weight depends linearly
    on the length features (see [Figure 3-3](#Fig_weight_lengths_scatterplots)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据的快速探索，例如绘制重量与各种长度特征的关系，使我们可以假设一个线性模型（即使在这种情况下非线性模型可能更好）。也就是说，我们假设重量线性地依赖于长度特征（参见[图3-3](#Fig_weight_lengths_scatterplots)）。
- en: '![280](assets/emai_0303.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0303.png)'
- en: Figure 3-3\. Scatterplots of the Fish Market numerical features. For more details,
    check out the attached Jupyter Notebook, or some of the public [notebooks on Kaggle](https://www.kaggle.com/aungpyaeap/fish-market/code)
    associated with this data set.
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3\. 鱼市场数值特征的散点图。有关更多详细信息，请查看附加的Jupyter笔记本，或者与此数据集相关的一些公开的[Kaggle笔记本](https://www.kaggle.com/aungpyaeap/fish-market/code)。
- en: 'This means that the weight of a fish, y, can be computed using a *linear combination*
    of its five different length measurements, plus a bias term <math alttext="omega
    0"><msub><mi>ω</mi> <mn>0</mn></msub></math> , giving the following *training
    function*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着鱼的重量y可以使用其五个不同长度测量的*线性组合*来计算，再加上一个偏置项 <math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math> ，得到以下*训练函数*：
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>
- en: After our major decision in the modeling process to use a linear training function
    <math alttext="f left-parenthesis x 1 comma x 2 comma x 3 comma x 4 comma x 5
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>4</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>5</mn></msub>
    <mo>)</mo></mrow></math> , all we have to do is to find the appropriate values
    of the parameters <math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math>
    , <math alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math> , <math alttext="omega
    2"><msub><mi>ω</mi> <mn>2</mn></msub></math> , <math alttext="omega 3"><msub><mi>ω</mi>
    <mn>3</mn></msub></math> , <math alttext="omega 4"><msub><mi>ω</mi> <mn>4</mn></msub></math>
    , and <math alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math> . We will
    *learn* the best values for our <math alttext="omega"><mi>ω</mi></math> ’s from
    the data. The process of using the data in order to find the appropriate <math
    alttext="omega"><mi>ω</mi></math> ’s is called *training* the model. A *trained*
    model is then a model where the <math alttext="omega"><mi>ω</mi></math> values
    have been decided on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在建模过程中做出的主要决定是使用线性训练函数f(x1, x2, x3, x4, x5)，我们所要做的就是找到参数ω0，ω1，ω2，ω3，ω4和ω5的适当值。我们将从数据中*学习*出ω的最佳值。利用数据找到适当的ω的过程称为*训练*模型。*训练*好的模型是指ω的值已经确定的模型。
- en: In general, training functions, whether linear or nonlinear, including those
    representing neural networks, have unknown parameters <math alttext="omega"><mi>ω</mi></math>
    ’s that we need to learn from the given data. For linear models, each parameter
    gives each feature a certain weight in the prediction process. So if the value
    of <math alttext="omega 2"><msub><mi>ω</mi> <mn>2</mn></msub></math> is larger
    than the value of <math alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math>
    , then the second feature plays a more important role than the fifth feature in
    our prediction, assuming that the second and fifth features have comparable scales.
    This is one of the reasons it is good to scale or normalize the data before training
    the model. If on the other hand the value <math alttext="omega 3"><msub><mi>ω</mi>
    <mn>3</mn></msub></math> associated with the third feature dies, meaning becomes
    zero or negligible, then the third feature can be omitted from the data set as
    it plays no role in our predictions. Therefore, learning our <math alttext="omega"><mi>ω</mi></math>
    ’s from the data allows us to mathematically compute the contribution of each
    feature to our predictions (or the importance of feature combinations if some
    features were combined during the data preparation stage, before training). In
    other words, the models learn how the data features interact and how strong these
    interactions are. The moral is that through a trained learning function, we can
    quantify how features come together in order to produce both observed and yet-to-be
    observed results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，无论是线性还是非线性的训练函数，包括表示神经网络的函数，都有我们需要从给定数据中学习的未知参数ω。对于线性模型，每个参数在预测过程中给每个特征赋予一定的权重。因此，如果ω2的值大于ω5的值，那么第二个特征在我们的预测中起到比第五个特征更重要的作用，假设第二个和第五个特征具有可比较的规模。这是在训练模型之前对数据进行缩放或归一化的好处之一。另一方面，如果与第三个特征相关联的ω3值消失，即变为零或可以忽略，那么第三个特征可以从数据集中省略，因为它在我们的预测中没有作用。因此，从数据中学习我们的ω允许我们在数学上计算每个特征对我们的预测的贡献（或者在数据准备阶段合并一些特征时，特征组合的重要性）。换句话说，模型学习数据特征如何相互作用以及这些相互作用的强度。结论是，通过训练学习函数，我们可以量化特征如何相互作用以产生已观察到的和尚未观察到的结果。
- en: 'Note: Parametric models vs non-parametric models'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意：参数模型与非参数模型
- en: 'A model that has parameters (we are calling them weights) pre-built into its
    formula, such as the <math alttext="omega"><mi>ω</mi></math> ’s in our current
    linear regression model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有预先内置参数（我们称之为权重）的模型，比如我们当前的线性回归模型中的ω：
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数学公式：y = ω0 + ω1x1 + ω2x2 + ω3x3 + ω4x4 + ω5x5
- en: (and later the <math alttext="omega"><mi>ω</mi></math> ’s of neural networks)
    is called a *parametric model*. This means that we fix the formula of the training
    function ahead of the actual training, and all the training does is solve for
    the parameters that are involved in the formula. Fixing the formula ahead of time
    is analogous to specifying the *family* that a training function belongs to, and
    finding the parameter values specifies the exact member of that family that best
    explains the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: （以及神经网络的<math alttext="omega"><mi>ω</mi></math>）被称为*参数模型*。这意味着我们在实际训练之前固定训练函数的公式，所有训练所做的就是解决公式中涉及的参数。提前固定公式类似于指定训练函数所属的*族*，找到参数值指定了最能解释数据的确切成员。
- en: '*Non-parametric* models, such as decision trees and random forests that we
    will discuss later in this chapter, do not specify the formula for the training
    function with its parameters ahead of time. So when we train a non-parametric
    model, we do not know how many parameters the trained model will end up having.
    The model *adapts* to the data and determines the required amount of parameters
    depending on the data. Careful here, the bells of over-fitting are ringing! Recall
    that we don’t want our models to adapt to the data too much, because they might
    not generalize well to unseen data. These models are usually accompanied with
    techniques that help them avoid overfitting.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*非参数模型*，例如我们将在本章后面讨论的决策树和随机森林，不会提前指定训练函数及其参数的公式。因此，当我们训练非参数模型时，我们不知道训练模型最终会有多少参数。模型会根据数据*自适应*并确定所需的参数数量。在这里要小心，过度拟合的警钟正在响！请记住，我们不希望我们的模型过度适应数据，因为它们可能无法很好地推广到未见过的数据。这些模型通常配有帮助它们避免过拟合的技术。'
- en: Both parametric and non-parametric models have *other parameters* called *hyperparameters*
    that also need to be tuned during the training process. These however are not
    built into the formula of the training function (and don’t end up in the formula
    of a non-parametric’s model either). We will encounter plenty of hyperparameters
    throughout the book.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化和非参数化模型都有*其他参数*称为*超参数*，在训练过程中也需要进行调整。然而，这些参数并没有内置到训练函数的公式中（非参数化模型的公式中也没有）。我们将在本书中遇到许多超参数。
- en: Loss Function
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: We have convinced ourselves that the next logical step is finding suitable values
    for the <math alttext="omega"><mi>ω</mi></math> ’s that appear in the training
    function (of our linear parametric model), using the data that we have. In order
    to do that, we need to *optimize an appropriate loss function*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确信，下一个逻辑步骤是找到适合训练函数（我们的线性参数模型）中出现的<math alttext="omega"><mi>ω</mi></math>的合适值，使用我们拥有的数据。为了做到这一点，我们需要*优化适当的损失函数*。
- en: The predicted value *vs*. the true value
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测值与真实值
- en: 'Suppose that we assign some random numerical values for each of our unknown
    <math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math> , <math alttext="omega
    1"><msub><mi>ω</mi> <mn>1</mn></msub></math> , <math alttext="omega 2"><msub><mi>ω</mi>
    <mn>2</mn></msub></math> , <math alttext="omega 3"><msub><mi>ω</mi> <mn>3</mn></msub></math>
    , <math alttext="omega 4"><msub><mi>ω</mi> <mn>4</mn></msub></math> , and <math
    alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math> , say for example
    <math alttext="omega 0 equals negative 3"><mrow><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>=</mo> <mo>-</mo> <mn>3</mn></mrow></math> , <math alttext="omega 1 equals
    4"><mrow><msub><mi>ω</mi> <mn>1</mn></msub> <mo>=</mo> <mn>4</mn></mrow></math>
    , <math alttext="omega 2 equals 0.2"><mrow><msub><mi>ω</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn></mrow></math> , <math alttext="omega
    3 equals 0.03"><mrow><msub><mi>ω</mi> <mn>3</mn></msub> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>03</mn></mrow></math> , <math alttext="omega 4 equals 0.4"><mrow><msub><mi>ω</mi>
    <mn>4</mn></msub> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn></mrow></math> ,
    and <math alttext="omega 5 equals 0.5"><mrow><msub><mi>ω</mi> <mn>5</mn></msub>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math> . Then the formula for
    the linear training function <math alttext="y equals omega 0 plus omega 1 x 1
    plus omega 2 x 2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub></mrow></math> becomes:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们为我们每个未知的<math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math>，<math
    alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>，<math alttext="omega
    2"><msub><mi>ω</mi> <mn>2</mn></msub></math>，<math alttext="omega 3"><msub><mi>ω</mi>
    <mn>3</mn></msub></math>，<math alttext="omega 4"><msub><mi>ω</mi> <mn>4</mn></msub></math>和<math
    alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math>分配一些随机数值，例如<math alttext="omega
    0等于负3"><mrow><msub><mi>ω</mi> <mn>0</mn></msub> <mo>=</mo> <mo>-</mo> <mn>3</mn></mrow></math>，<math
    alttext="omega 1等于4"><mrow><msub><mi>ω</mi> <mn>1</mn></msub> <mo>=</mo> <mn>4</mn></mrow></math>，<math
    alttext="omega 2等于0.2"><mrow><msub><mi>ω</mi> <mn>2</mn></msub> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>2</mn></mrow></math>，<math alttext="omega 3等于0.03"><mrow><msub><mi>ω</mi>
    <mn>3</mn></msub> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>03</mn></mrow></math>，<math
    alttext="omega 4等于0.4"><mrow><msub><mi>ω</mi> <mn>4</mn></msub> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>4</mn></mrow></math>，和<math alttext="omega 5等于0.5"><mrow><msub><mi>ω</mi>
    <mn>5</mn></msub> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>。然后线性训练函数的公式<math
    alttext="y等于omega 0加omega 1 x 1加omega 2 x 2加omega 3 x 3加omega 4 x 4加omega 5 x
    5"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>4</mn></msub> <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>5</mn></msub> <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>变为：
- en: <math alttext="dollar-sign y equals negative 3 plus 4 x 1 plus 0.2 x 2 plus
    0.03 x 3 plus 0.4 x 4 plus 0.5 x 5 dollar-sign"><mrow><mi>y</mi> <mo>=</mo> <mo>-</mo>
    <mn>3</mn> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>2</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>03</mn> <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>4</mn> <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>5</mn> <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'and is ready to make predictions: Plug in numerical values for the length features
    of the <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    fish, then obtain a predicted value for the weight of this fish. For example,
    the first fish in our data set is a bream and has length measurements <math alttext="x
    1 Superscript 1 Baseline equals 23.2"><mrow><msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>23</mn> <mo>.</mo> <mn>2</mn></mrow></math> , <math alttext="x
    2 Superscript 1 Baseline equals 25.4"><mrow><msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>25</mn> <mo>.</mo> <mn>4</mn></mrow></math> , <math alttext="x
    3 Superscript 1 Baseline equals 30"><mrow><msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>30</mn></mrow></math> , <math alttext="x 4 Superscript 1 Baseline
    equals 11.52"><mrow><msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup> <mo>=</mo>
    <mn>11</mn> <mo>.</mo> <mn>52</mn></mrow></math> , and <math alttext="x 5 Superscript
    1 Baseline equals 4.02"><mrow><msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>4</mn> <mo>.</mo> <mn>02</mn></mrow></math> . Plugging these into
    the training function, we get the prediction for the weight of this fish:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column y Subscript p r e
    d i c t Superscript 1 2nd Column equals omega 0 plus omega 1 x 1 Superscript 1
    Baseline plus omega 2 x 2 Superscript 1 Baseline plus omega 3 x 3 Superscript
    1 Baseline plus omega 4 x 4 Superscript 1 Baseline plus omega 5 x 5 Superscript
    1 Baseline 2nd Row 1st Column Blank 2nd Column equals negative 3 plus 4 left-parenthesis
    23.2 right-parenthesis plus 0.2 left-parenthesis 25.4 right-parenthesis plus 0.03
    left-parenthesis 30 right-parenthesis plus 0.4 left-parenthesis 11.52 right-parenthesis
    plus 0.5 left-parenthesis 4.02 right-parenthesis 3rd Row 1st Column Blank 2nd
    Column equals 102.398 grams period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mn>3</mn> <mo>+</mo>
    <mn>4</mn> <mo>(</mo> <mn>23</mn> <mo>.</mo> <mn>2</mn> <mo>)</mo> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>2</mn> <mo>(</mo> <mn>25</mn> <mo>.</mo> <mn>4</mn>
    <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>03</mn> <mo>(</mo> <mn>30</mn>
    <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>(</mo> <mn>11</mn>
    <mo>.</mo> <mn>52</mn> <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn>
    <mo>(</mo> <mn>4</mn> <mo>.</mo> <mn>02</mn> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mn>102</mn> <mo>.</mo> <mn>398</mn> <mtext>grams.</mtext></mrow></mtd></mtr></mtable></math>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column y Subscript p r e
    d i c t Superscript 1 2nd Column equals omega 0 plus omega 1 x 1 Superscript 1
    Baseline plus omega 2 x 2 Superscript 1 Baseline plus omega 3 x 3 Superscript
    1 Baseline plus omega 4 x 4 Superscript 1 Baseline plus omega 5 x 5 Superscript
    1 Baseline 2nd Row 1st Column Blank 2nd Column equals negative 3 plus 4 left-parenthesis
    23.2 right-parenthesis plus 0.2 left-parenthesis 25.4 right-parenthesis plus 0.03
    left-parenthesis 30 right-parenthesis plus 0.4 left-parenthesis 11.52 right-parenthesis
    plus 0.5 left-parenthesis 4.02 right-parenthesis 3rd Row 1st Column Blank 2nd
    Column equals 102.398 grams period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mn>3</mn> <mo>+</mo>
    <mn>4</mn> <mo>(</mo> <mn>23</mn> <mo>.</mo> <mn>2</mn> <mo>)</mo> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>2</mn> <mo>(</mo> <mn>25</mn> <mo>.</mo> <mn>4</mn>
    <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>03</mn> <mo>(</mo> <mn>30</mn>
    <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>(</mo> <mn>11</mn>
    <mo>.</mo> <mn>52</mn> <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn>
    <mo>(</mo> <mn>4</mn> <mo>.</mo> <mn>02</mn> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mn>102</mn> <mo>.</mo> <mn>398</mn> <mtext>grams.</mtext></mrow></mtd></mtr></mtable></math>
- en: 'In general, for the <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    fish, we have:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于第i条鱼，我们有：
- en: <math alttext="dollar-sign y Subscript p r e d i c t Superscript i Baseline
    equals omega 0 plus omega 1 x 1 Superscript i Baseline plus omega 2 x 2 Superscript
    i Baseline plus omega 3 x 3 Superscript i Baseline plus omega 4 x 4 Superscript
    i Baseline plus omega 5 x 5 Superscript i dollar-sign"><mrow><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>i</mi></msubsup> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mi>i</mi></msubsup></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y Subscript p r e d i c t Superscript i Baseline
    equals omega 0 plus omega 1 x 1 Superscript i Baseline plus omega 2 x 2 Superscript
    i Baseline plus omega 3 x 3 Superscript i Baseline plus omega 4 x 4 Superscript
    i Baseline plus omega 5 x 5 Superscript i dollar-sign"><mrow><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>i</mi></msubsup> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mi>i</mi></msubsup></mrow></math>
- en: The fish under consideration, however, has a certain *true* weight, <math alttext="y
    Subscript t r u e Superscript i"><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup></math> , which is its label if it belongs in the labeled
    data set. For the first fish in our data set, the true weight is <math alttext="y
    Subscript t r u e Superscript 1 Baseline equals 242"><mrow><msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mn>1</mn></msubsup> <mo>=</mo>
    <mn>242</mn></mrow></math> *grams*. Our linear model with randomly chosen <math
    alttext="omega"><mi>ω</mi></math> values predicted 102.398 *grams*. This is of
    course pretty far off, since we did not calibrate the <math alttext="omega"><mi>ω</mi></math>
    values at all. In any case, we can measure the *error* between the weight predicted
    by our model and the true weight, then find ways to do better in terms of our
    choices for the <math alttext="omega"><mi>ω</mi></math> ’s.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑的鱼有一个*真实*重量，<math alttext="y Subscript t r u e Superscript i"><msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>i</mi></msubsup></math>，如果它属于标记数据集，则为其标签。对于我们数据集中的第一条鱼，真实重量是<math
    alttext="y Subscript t r u e Superscript 1 Baseline equals 242"><mrow><msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mn>1</mn></msubsup> <mo>=</mo>
    <mn>242</mn></mrow></math> *克*。我们随机选择的线性模型预测了102.398 *克*。这当然相差甚远，因为我们根本没有校准<math
    alttext="omega"><mi>ω</mi></math>值。无论如何，我们可以测量我们的模型预测的重量与真实重量之间的*误差*，然后找到更好的方法来选择<math
    alttext="omega"><mi>ω</mi></math>。
- en: The absolute value distance *vs* the squared distance
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绝对值距离*vs*平方距离
- en: 'One of the nice things about mathematics is that it has multiple ways to measure
    how far off things are from each other, using different distance metrics. For
    example, we can naively measure the distance between two quantities as being one
    if they are different and zero if they are the same, encoding the words: different-1,
    similar-0\. Of course, using such a naive metric, we lose a ton of information,
    since the distance between quantities such as two and ten will be equal the distance
    between two and a million, namely 1.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数学的一大优点是它有多种方法来衡量事物之间的差距，使用不同的距离度量。例如，我们可以天真地将两个量之间的距离测量为如果它们不同则为1，如果它们相同则为0，编码为：不同-1，相似-0。当然，使用这样一个天真的度量，我们会失去大量信息，因为两和十之间的距离将等于两和一百万之间的距离，即1。
- en: 'There are some distance metrics that are popular in machine learning. We first
    introduce the two most commonly used:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中有一些流行的距离度量。我们首先介绍两种最常用的：
- en: 'The absolute value distance: <math alttext="StartAbsoluteValue y Subscript
    p r e d i c t Baseline minus y Subscript t r u e Baseline EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mrow><mo>|</mo></mrow></mrow></math> , stemming from the calculus function <math
    alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math>
    .'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对值距离：<math alttext="StartAbsoluteValue y Subscript p r e d i c t Baseline minus
    y Subscript t r u e Baseline EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow> <msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mrow><mo>|</mo></mrow></mrow></math>，源自微积分函数<math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math>。
- en: 'The squared distance: <math alttext="StartAbsoluteValue y Subscript p r e d
    i c t Baseline minus y Subscript t r u e Baseline EndAbsoluteValue squared"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup></mrow></math> , stemming from
    the calculus function <math alttext="StartAbsoluteValue x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> (which is the same as <math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math> for scalar quantities). Of course, this will square the
    units as well.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平方距离：<math alttext="StartAbsoluteValue y Subscript p r e d i c t Baseline minus
    y Subscript t r u e Baseline EndAbsoluteValue squared"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup></mrow></math>，源自微积分函数<math alttext="StartAbsoluteValue
    x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math>（对于标量量来说，这与<math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math>是相同的）。当然，这也会平方单位。
- en: 'Inspecting the graphs of the functions <math alttext="StartAbsoluteValue x
    EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> and <math
    alttext="x squared"><msup><mi>x</mi> <mn>2</mn></msup></math> in [Figure 3-4](#Fig_abs_x_square_x),
    we notice a great difference in function smoothness at the point (0,0). The function
    <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi>
    <mo>|</mo></mrow></math> has a corner at that point, rendering it undifferentiable
    at x=0\. This *singularity* of <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math> at x=0 turns many practitioners (and mathematicians!)
    away from incorporating this function, or functions with similar singularities,
    into their models. However, let’s engrave the following into our brains:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 检查函数图像<math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math>和<math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math>在[图3-4](#Fig_abs_x_square_x)中，我们注意到在点(0,0)处函数的平滑度有很大的差异。函数<math
    alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math>在该点有一个拐角，使得它在x=0处不可微。这种在x=0处的<math
    alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math>的*奇点*使得许多从业者（包括数学家！）不愿将这个函数或具有类似奇点的函数纳入他们的模型中。然而，让我们铭记以下事实：
- en: '*Mathematical models are flexible*. When we encounter a hurdle we dig deeper,
    understand what’s going on, then we work around the hurdle.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*数学模型是灵活的*。当我们遇到障碍时，我们会深入挖掘，了解发生了什么，然后我们会克服障碍。'
- en: '![275](assets/emai_0304.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![275](assets/emai_0304.png)'
- en: 'Figure 3-4\. Left: Graph of <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math> has a corner at <math alttext="x equals 0"><mrow><mi>x</mi>
    <mo>=</mo> <mn>0</mn></mrow></math> rendering its derivative undefined at that
    point. Right: Graph of <math alttext="StartAbsoluteValue x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> is smooth at <math alttext="x equals 0"><mrow><mi>x</mi>
    <mo>=</mo> <mn>0</mn></mrow></math> so its derivative has no problems there.'
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4。左：图形<math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math>在<math alttext="x等于0"><mrow><mi>x</mi> <mo>=</mo>
    <mn>0</mn></mrow></math>处有一个转角，使得其在该点的导数未定义。右：图形<math alttext="StartAbsoluteValue
    x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math>在<math alttext="x等于0"><mrow><mi>x</mi> <mo>=</mo> <mn>0</mn></mrow></math>处平滑，因此其导数在那里没有问题。
- en: 'Other than the difference in the *regularity* of the functions <math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> and <math
    alttext="StartAbsoluteValue x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> (meaning whether they have derivatives at all points
    or not), there is one more point that we need to pay attention to before deciding
    to incorporate either function into our error formula: *If a number is large,
    then its square is even larger*. This simple observation means that if we decide
    to measure the error using squared distances between true values and predicted
    values, then our method will be *more sensitive to the outliers* in the data.
    One messed up outlier might skew our whole prediction function towards it, and
    hence away from the more prevelant patterns in the data. Ideally, we would’ve
    taken care of outliers and decided whether we should keep them or not during the
    data preparation step, before feeding the data into any machine learning model.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 除了函数<math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi>
    <mo>|</mo></mrow></math>和<math alttext="StartAbsoluteValue x EndAbsoluteValue
    squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow> <mn>2</mn></msup></math>的*规则性*之外（即它们在所有点是否都有导数），在决定是否将任一函数纳入我们的误差公式之前，我们还需要注意另一个问题：*如果一个数很大，那么它的平方就更大*。这个简单的观察意味着，如果我们决定使用真实值和预测值之间的平方距离来衡量误差，那么我们的方法将对数据中的异常值*更敏感*。一个混乱的异常值可能会使我们整个预测函数偏向它，因此远离数据中更普遍的模式。理想情况下，我们应该在数据准备步骤中处理异常值，并决定是否应该在将数据输入任何机器学习模型之前保留它们。
- en: 'One last difference between <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math> (and similar piecewise linear functions) and
    <math alttext="x squared"><msup><mi>x</mi> <mn>2</mn></msup></math> (and similar
    nonlinear but differentiable functions) is that the derivative of <math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> is very
    easy:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi>
    <mo>|</mo></mrow></math>（以及类似的分段线性函数）和<math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math>（以及类似的非线性但可微函数）之间的最后一个区别是，<math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math>的导数非常简单：
- en: 1 if x>0, -1 if x<0 (and undefined if x=0).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 1 如果x>0，-1 如果x<0（如果x=0，则未定义）。
- en: In a model that involves billions of computational steps, this property where
    there is *no need to evaluate anything* when using the derivative of <math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> proves
    extremely valuable. Derivatives of functions that are neither linear nor piecewise
    linear usually involve evaluations (because they also have *x*’s in their formulas
    and not only constants like in the piecewise linear case) which can be expensive
    in big data settings.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及数十亿次计算步骤的模型中，当使用<math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math>的导数时，*无需评估任何内容*，这一特性被证明非常有价值。通常情况下，既不是线性的也不是分段线性的函数的导数需要进行评估（因为它们的公式中不仅有常数，还有*x*），这在大数据环境中可能会很昂贵。
- en: Functions With Singularities
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具有奇点的函数
- en: In general, graphs of *differentiable* functions do not have cusps, kinks, corners,
    or anything pointy. If they do have such *singularities*, then the function has
    no derivative at these points. The reason is that at a pointy point, you can draw
    two different tangent lines to the graph of the function, depending on whether
    you decide to draw the tangent line to the left or to the right of the point (see
    [Figure 3-5](#Fig_singularity_tangents)). Recall that the derivative of a function
    at a point is the slope of the tangent line to the graph of the function at that
    point. If there are two *different* slopes then we cannot define the derivative
    at the point.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，*可微*函数的图形没有尖点、转角、角或任何尖锐的地方。如果它们有这样的*奇点*，那么这些点处的函数就没有导数。原因是在尖锐的点上，你可以画出两条不同的切线，取决于你决定是在点的左边还是右边画切线（见[图3-5](#Fig_singularity_tangents)）。回想一下，函数在某一点的导数是函数图形在该点的切线的斜率。如果有两个*不同*的斜率，那么我们就无法定义该点的导数。
- en: '![275](assets/emai_0305.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![275](assets/emai_0305.png)'
- en: Figure 3-5\. At singular points, the derivative does not exist. There are more
    than one possible slope of tangent at such points.
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5。在奇点处，导数不存在。在这些点上，切线可能有多个可能的斜率。
- en: 'This *discontinuity* in the slope of the tangent creates a problem for methods
    that rely on evaluating the derivative of the function, such as the Gradient Descent
    method. The problem here is two fold:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 切线斜率的*不连续*在依赖于评估函数的导数的方法（如梯度下降法）中造成了问题。问题在于：
- en: If you happen to land at a quirky pointy point, then the method doesn’t know
    what to do, since there is no defined derivative there. Some people assign a value
    for the derivative that at point (called the *subgradient* or the *subdifferential*)
    and move on. In reality, what are the odds that we will be unlucky enough to land
    exactly at that one horrible point? Unless the landscape of the function looks
    like the rough terrains of the Alps (actually many do), the numerical method might
    manage to avoid them.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你碰巧落在一个古怪的尖点上，那么方法就不知道该怎么办，因为那里没有定义的导数。有些人会为那一点分配一个导数值（称为*次梯度*或*次微分*）然后继续前进。实际上，我们会不幸地正好落在那一个可怕的点上的几率有多大呢？除非函数的景观看起来像阿尔卑斯山的崎岖地形（实际上很多函数确实如此），数值方法可能会设法避开它们。
- en: The other problem is instability. Since the value of the derivative jumps so
    abruptly as you traverse the landscape of the function across this point, a method
    using this derivative will abrupltly change value as well, creating instabilities
    if you are trying to converge somewhere. Imagine you are hiking down the Swiss
    Alps [Figure 3-6](#Fig_Swiss_Alps) (the landscape of the loss function) and your
    destination is that pretty little town that you can see down the valley (the place
    with the lowest error value). Then *suddenly* you get carried by some alien (the
    alien is the mathematical search method relying on this abruptly changing derivative)
    to the *other* side of the mountain, where you cannot see your destination anymore.
    In fact, now all you can see down the valley is some ugly shrubs, and an extremely
    narrow canyon that can trap you if your method carries you there. Your convergence
    to your original destination is now unstable, if not totally lost.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个问题是不稳定性。由于导数的值在函数的景观中跳跃得如此突然，使用这个导数的方法也会突然改变数值，如果你试图收敛到某个地方，就会产生不稳定性。想象一下你正在瑞士阿尔卑斯山徒步旅行[图3-6](#Fig_Swiss_Alps)（损失函数的景观），你的目的地是山谷下面那个漂亮的小镇（误差值最低的地方）。然后*突然*你被某个外星人带到了山的*另一边*，你再也看不到你的目的地了。事实上，现在你在山谷下看到的只有一些丑陋的灌木丛，还有一个非常狭窄的峡谷，如果你的方法把你带到那里，你就会被困住。你原来的目的地的收敛现在是不稳定的，甚至完全丢失了。
- en: '![275](assets/emai_0306.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![275](assets/emai_0306.png)'
- en: 'Figure 3-6\. Swiss Alps: Optimization is similar to hiking the landscape of
    a function.'
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6\. 瑞士阿尔卑斯山：优化类似于徒步穿越函数的景观。
- en: Nevertheless, functions with such singularities are used all the time in machine
    learning. We will encounter them in the formulas of some neural network training
    functions (Rectified Linear Unit function- who names these?), some loss functions
    (absolute value distance), and in some regularizing terms (Lasso regression- who
    names these too?).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，具有这种奇点的函数在机器学习中经常被使用。我们将在一些神经网络训练函数的公式（修正线性单元函数-谁起这些名字？）、一些损失函数（绝对值距离）和一些正则化项（Lasso回归-这些也是谁起的名字？）中遇到它们。
- en: For linear regression, the loss function is the Mean Squared Error
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对于线性回归，损失函数是均方误差
- en: 'Back to the main goal for this section: Constructing an error function, also
    called the *loss function*, which encodes how much error our model commits when
    making its predictions, and must be made small.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 回到本节的主要目标：构建一个误差函数，也称为*损失函数*，它编码了我们的模型在进行预测时产生了多少误差，并且必须尽量小。
- en: 'For linear regression, we use the *mean squared error function*. This function
    averages over the squared distance errors between the prediction and the true
    value for *m* data points (we will mention which data points to include here shortly):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性回归，我们使用*均方误差函数*。该函数对*m*个数据点的预测与真实值之间的平方距离误差进行平均（我们很快会提到包括哪些数据点）：
- en: <math alttext="dollar-sign Mean Squared Error equals StartFraction 1 Over m
    EndFraction left-parenthesis StartAbsoluteValue y Subscript p r e d i c t Superscript
    1 Baseline minus y Subscript t r u e Superscript 1 Baseline EndAbsoluteValue squared
    plus StartAbsoluteValue y Subscript p r e d i c t Superscript 2 Baseline minus
    y Subscript t r u e Superscript 2 Baseline EndAbsoluteValue squared plus ellipsis
    plus StartAbsoluteValue y Subscript p r e d i c t Superscript m Baseline minus
    y Subscript t r u e Superscript m Baseline EndAbsoluteValue squared right-parenthesis
    dollar-sign"><mrow><mtext>Mean</mtext> <mtext>Squared</mtext> <mtext>Error</mtext>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mrow><mo>|</mo></mrow>
    <msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mn>1</mn></msubsup> <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mrow><mo>+</mo>
    <mo>|</mo></mrow> <msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mn>2</mn></msubsup> <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mo>+</mo>
    <mo>⋯</mo> <mo>+</mo> <msup><mrow><mo>|</mo><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup> <mo>-</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>m</mi></msubsup> <mo>|</mo></mrow> <mn>2</mn></msup></mfenced></mrow></math>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差等于1/m（|y^predict^1 - y^true^1|^2 + |y^predict^2 - y^true^2|^2 + ... + |y^predict^m
    - y^true^m|^2）
- en: 'Let’s write the above expression more compactly using the sum notation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用求和符号更紧凑地写出上面的表达式：
- en: <math alttext="dollar-sign Mean Squared Error equals StartFraction 1 Over m
    EndFraction sigma-summation Underscript i equals 1 Overscript m Endscripts StartAbsoluteValue
    y Subscript p r e d i c t Superscript i Baseline minus y Subscript t r u e Superscript
    i Baseline EndAbsoluteValue squared dollar-sign"><mrow><mtext>Mean</mtext> <mtext>Squared</mtext>
    <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup> <msup><mrow><mo>|</mo><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差等于1/m的sigma-求和（从i=1到m）|y^predict^i - y^true^i|^2
- en: Now we get into the great habit of using the even more compact linear algebra
    notation of vectors and matrices. This habit proves extremely handy in the field,
    as we don’t want to drown while trying to keep track of indices. Indices can sneak
    up into our rosy dreams of understanding everything and quickly transform them
    into very scary nightmares. Another very important reason to use the compact linear
    algebra notation is that both the software and the hardware built for machine
    learning models are optimized for matrix and *tensor* (think of an object made
    of layered matrices, like a three dimensional box instead of a flat square) computations.
    Moreover, the beautiful field of numerical linear algebra has worked through many
    potential problems and paved the way for us to enjoy the fast methods to perform
    all kinds of matrix computations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们养成了使用更紧凑的线性代数符号的好习惯。这种习惯在这个领域非常方便，因为我们不想在试图跟踪索引的同时淹没。索引可能潜入我们对理解一切的美好梦想中，并迅速将它们转变成非常可怕的噩梦。使用紧凑的线性代数符号的另一个非常重要的原因是，为机器学习模型构建的软件和硬件都针对矩阵和*张量*（想象一个由分层矩阵组成的对象，就像一个三维盒子而不是一个平面正方形）计算进行了优化。此外，美丽的数值线性代数领域已经解决了许多潜在问题，并为我们提供了快速执行各种矩阵计算的方法。
- en: 'Using linear algebra notation, we can write the mean squared error as:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性代数符号，我们可以将均方误差写成：
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差是预测基准减去真实值的修改后的y箭头预测基准减去修改后的y箭头真实基准的t次方左括号修改后的y箭头预测基准减去修改后的y箭头真实基准右括号等于1除以m平行于修改后的y箭头预测基准减去修改后的y箭头真实基准平行于句号
- en: The last equality introduces the <math alttext="l squared"><msup><mi>l</mi>
    <mn>2</mn></msup></math> *norm* of a vector, which by definition is just the <math
    alttext="StartRoot sum of squares of its components EndRoot"><msqrt><mrow><mtext>sum</mtext>
    <mtext>of</mtext> <mtext>squares</mtext> <mtext>of</mtext> <mtext>its</mtext>
    <mtext>components</mtext></mrow></msqrt></math> .
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个等式引入了向量的l平方范数，根据定义，这只是其分量的平方和的平方根。
- en: 'Take home idea: *The loss function that we constructed encodes the difference
    between the predictions and the ground truths for the data points involved the
    training process, measured in some norm: a mathematical entity that acts as a
    distance*. There are many other norms that we could’ve used, but the <math alttext="l
    squared"><msup><mi>l</mi> <mn>2</mn></msup></math> *norm* is pretty popular.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 主要观点：*我们构建的损失函数编码了训练过程中涉及的数据点的预测和地面真相之间的差异，用某种范数来衡量：作为距离的数学实体*。我们可以使用许多其他范数，但l平方范数非常受欢迎。
- en: 'Notation: Vectors In This Book Are Always Column Vectors'
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 符号：本书中的向量始终是列向量
- en: To be consistent in notation throughout the book, *all* vectors are column vectors.
    So if a vector <math alttext="ModifyingAbove v With right-arrow"><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover></math> has four components, the symbol <math alttext="ModifyingAbove
    v With right-arrow"><mover accent="true"><mi>v</mi> <mo>→</mo></mover></math>
    stands for <math alttext="Start 4 By 1 Matrix 1st Row  v 1 2nd Row  v 2 3rd Row  v
    3 4th Row  v 4 EndMatrix"><mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>v</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced></math> .
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在整本书中符号一致，*所有*向量都是列向量。因此，如果一个向量修改后的v箭头有四个分量，符号修改后的v箭头代表的是修改后的v箭头代表的是4乘1矩阵第一行v1第二行v2第三行v3第四行v4。
- en: The transpose of a vector <math alttext="ModifyingAbove v With right-arrow"><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover></math> is then always a row vector.
    The transpose of the above vector with four components is <math alttext="ModifyingAbove
    v With right-arrow Superscript t Baseline equals Start 1 By 4 Matrix 1st Row 1st
    Column v 1 2nd Column v 2 3rd Column v 3 4th Column v 4 EndMatrix"><mrow><msup><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><msub><mi>v</mi> <mn>1</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>3</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>4</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
    .
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 向量<math alttext="ModifyingAbove v With right-arrow"><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover></math>的转置始终是一个行向量。具有四个分量的上述向量的转置是<math alttext="ModifyingAbove
    v With right-arrow Superscript t Baseline equals Start 1 By 4 Matrix 1st Row 1st
    Column v 1 2nd Column v 2 3rd Column v 3 4th Column v 4 EndMatrix"><mrow><msup><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><msub><mi>v</mi> <mn>1</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>3</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>4</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>。
- en: 'We will never use the dot product notation (also called the scalar product
    because we *multiply* two vectors but our answer is a scalar number). Instead
    of writing the dot product of two vectors <math alttext="ModifyingAbove a With
    right-arrow period ModifyingAbove b With right-arrow"><mrow><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>.</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    , we will write <math alttext="ModifyingAbove a With right-arrow Superscript t
    Baseline ModifyingAbove b With right-arrow"><mrow><msup><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    , which is the same thing, but in essence thinks of a column vector as a matrix
    of shape: *length of the vector by 1*, and its transpose as a matrix of shape:
    *1 by length of the vector*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们永远不会使用点积符号（也称为数量积，因为我们*乘以*两个向量，但我们的答案是一个标量）。而不是写两个向量的点积<math alttext="ModifyingAbove
    a With right-arrow period ModifyingAbove b With right-arrow"><mrow><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>.</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>，我们将写成<math
    alttext="ModifyingAbove a With right-arrow Superscript t Baseline ModifyingAbove
    b With right-arrow"><mrow><msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>，这是一样的，但本质上将列向量视为形状为：*向量的长度乘以1*的矩阵，其转置为形状为：*1乘以向量的长度*的矩阵。
- en: Suppose now that <math alttext="ModifyingAbove a With right-arrow"><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></math> and <math alttext="ModifyingAbove b With right-arrow"><mover
    accent="true"><mi>b</mi> <mo>→</mo></mover></math> have four components then
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 假设现在<math alttext="ModifyingAbove a With right-arrow"><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></math>和<math alttext="ModifyingAbove b With right-arrow"><mover
    accent="true"><mi>b</mi> <mo>→</mo></mover></math>有四个分量，那么
- en: <math alttext="dollar-sign ModifyingAbove a With right-arrow Superscript t Baseline
    ModifyingAbove b With right-arrow equals Start 1 By 4 Matrix 1st Row 1st Column
    a 1 2nd Column a 2 3rd Column a 3 4th Column a 4 EndMatrix Start 4 By 1 Matrix
    1st Row  b 1 2nd Row  b 2 3rd Row  b 3 4th Row  b 4 EndMatrix equals a 1 b 1 plus
    a 2 b 2 plus a 3 b 3 plus a 4 b 4 equals sigma-summation Underscript i equals
    1 Overscript 4 Endscripts a Subscript i Baseline b Subscript i Baseline period
    dollar-sign"><mrow><msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>b</mi> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msub><mi>a</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>2</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>3</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>b</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>b</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>b</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>b</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced> <mo>=</mo> <msub><mi>a</mi> <mn>1</mn></msub>
    <msub><mi>b</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>b</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>3</mn></msub>
    <msub><mi>b</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>4</mn></msub>
    <msub><mi>b</mi> <mn>4</mn></msub> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mn>4</mn></msubsup> <msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>.</mo></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove a With right-arrow Superscript t Baseline
    ModifyingAbove b With right-arrow equals Start 1 By 4 Matrix 1st Row 1st Column
    a 1 2nd Column a 2 3rd Column a 3 4th Column a 4 EndMatrix Start 4 By 1 Matrix
    1st Row  b 1 2nd Row  b 2 3rd Row  b 3 4th Row  b 4 EndMatrix equals a 1 b 1 plus
    a 2 b 2 plus a 3 b 3 plus a 4 b 4 equals sigma-summation Underscript i equals
    1 Overscript 4 Endscripts a Subscript i Baseline b Subscript i Baseline period
    dollar-sign"><mrow><msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>b</mi> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msub><mi>a</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>2</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>3</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>b</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>b</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>b</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>b</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced> <mo>=</mo> <msub><mi>a</mi> <mn>1</mn></msub>
    <msub><mi>b</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>b</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>3</mn></msub>
    <msub><mi>b</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>4</mn></msub>
    <msub><mi>b</mi> <mn>4</mn></msub> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mn>4</mn></msubsup> <msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>.</mo></mrow></math>
- en: Moreover,
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，
- en: <math alttext="dollar-sign parallel-to ModifyingAbove a With right-arrow parallel-to
    equals ModifyingAbove a With right-arrow Superscript t Baseline ModifyingAbove
    a With right-arrow equals a 1 squared plus a 2 squared plus a 3 squared plus a
    4 squared period dollar-sign"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow> <msup><mi>l</mi> <mn>2</mn></msup>
    <mn>2</mn></msubsup> <mo>=</mo> <msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>=</mo>
    <msubsup><mi>a</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>a</mi>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>a</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msubsup><mi>a</mi> <mn>4</mn> <mn>2</mn></msubsup> <mo>.</mo></mrow></math>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign parallel-to ModifyingAbove a With right-arrow parallel-to
    equals ModifyingAbove a With right-arrow Superscript t Baseline ModifyingAbove
    a With right-arrow equals a 1 squared plus a 2 squared plus a 3 squared plus a
    4 squared period dollar-sign"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow> <msup><mi>l</mi> <mn>2</mn></msup>
    <mn>2</mn></msubsup> <mo>=</mo> <msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>=</mo>
    <msubsup><mi>a</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>a</mi>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>a</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msubsup><mi>a</mi> <mn>4</mn> <mn>2</mn></msubsup> <mo>.</mo></mrow></math>
- en: Similarly,
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，
- en: <math alttext="dollar-sign parallel-to ModifyingAbove b With right-arrow parallel-to
    equals ModifyingAbove b With right-arrow Superscript t Baseline ModifyingAbove
    b With right-arrow equals b 1 squared plus b 2 squared plus b 3 squared plus b
    4 squared period dollar-sign"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>b</mi>
    <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow> <msup><mi>l</mi> <mn>2</mn></msup>
    <mn>2</mn></msubsup> <mo>=</mo> <msup><mover accent="true"><mi>b</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover> <mo>=</mo>
    <msubsup><mi>b</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>b</mi>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>b</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msubsup><mi>b</mi> <mn>4</mn> <mn>2</mn></msubsup> <mo>.</mo></mrow></math>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign parallel-to ModifyingAbove b With right-arrow parallel-to
    equals ModifyingAbove b With right-arrow Superscript t Baseline ModifyingAbove
    b With right-arrow equals b 1 squared plus b 2 squared plus b 3 squared plus b
    4 squared period dollar-sign"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>b</mi>
    <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow> <msup><mi>l</mi> <mn>2</mn></msup>
    <mn>2</mn></msubsup> <mo>=</mo> <msup><mover accent="true"><mi>b</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover> <mo>=</mo>
    <msubsup><mi>b</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>b</mi>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>b</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msubsup><mi>b</mi> <mn>4</mn> <mn>2</mn></msubsup> <mo>.</mo></mrow></math>
- en: This way, we use matrix notation throughout, and we only put an arrow above
    a letter to indicate that we are dealing with a column vector.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们在整个过程中都使用矩阵表示，并且只在字母上方加上箭头，以表明我们正在处理一个列向量。
- en: The Training, Validation and Test Subsets
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练、验证和测试子集
- en: Which data points do we include in our loss function? Do we include the whole
    data set, some small batches of it, or even only one point? Are we measuring this
    mean squared error for the data points in the *training subset*, the *validation
    subset*, or the *test subset*? And what are these subsets anyway?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在损失函数中包含哪些数据点？我们包括整个数据集，其中的一些小批次，甚至只有一个点吗？我们是针对*训练子集*、*验证子集*还是*测试子集*来测量均方误差？这些子集到底是什么？
- en: 'In practice, we split a data set into three subsets:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们将数据集分成三个子集：
- en: 'The *training* subset: This is the subset of the data that we use to fit our
    training function. This means that the data points in this subset are the ones
    that get incorporated into our loss function (by plugging their feature values
    and label into the <math alttext="y Subscript p r e d i c t"><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>
    and the <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    of the loss function).'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练子集：这是我们用来拟合训练函数的数据子集。这意味着这个子集中的数据点被纳入我们的损失函数中（通过将它们的特征值和标签插入到损失函数的<math alttext="y
    Subscript p r e d i c t"><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>和<math
    alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>中）。
- en: 'The *validation* subset: The data points in this subset are used in multiple
    ways:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证子集：这个子集中的数据点被多种方式使用：
- en: 'The common description is that we use this subset in order to *tune the hyperparameters
    of the machine learning model*. The hyperparameters are any parameters in the
    machine learning model that *are not* the <math alttext="omega"><mi>ω</mi></math>
    ’s of the training function that we are trying to solve for. In machine learning,
    there are many of these, and their values affect the results and the performance
    of the model. Examples of hyperparameters include (you don’t have to know what
    these are yet): The learning rate that appears in the gradient descent method,
    the hyperparameter that determines the width of the margin in support vector machine
    methods, the percentage of original data split into training, validation and test
    subsets, the batch size when doing randomized batch gradient descent, weight decay
    hyperparameters such as those used in Ridge, LASSO and Elastic Net regression,
    hyperparameters that come with momentum methods such as gradient descent with
    momentum and ADAM (these have terms that accelerate the convergence of the method
    towards the minimum and these terms are multiplied by hyperparameters that need
    to be tuned before testing and deployment), the architecture of a neural network
    such as the number of layers, the width of each layer, *etc.*, and the number
    of *epochs* during the optimization process (the number of passes over the entire
    training subset that the optimizer has seen).'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的描述是，我们使用这个子集来*调整机器学习模型的超参数*。 超参数是机器学习模型中*不是*我们试图解决的训练函数的<math alttext="omega"><mi>ω</mi></math>的任何参数。
    在机器学习中，有许多这样的参数，它们的值会影响模型的结果和性能。 超参数的示例包括（您现在不必知道这些是什么）：出现在梯度下降方法中的学习率，决定支持向量机方法中边缘宽度的超参数，原始数据分成训练、验证和测试子集的百分比，随机批量梯度下降时的批量大小，权重衰减超参数，例如Ridge、LASSO和Elastic
    Net回归中使用的超参数，带有动量方法的超参数，例如带有动量的梯度下降和ADAM（这些方法加速了方法向最小值的收敛，这些项乘以需要在测试和部署之前进行调整的超参数），神经网络的架构，例如层数、每层的宽度，*等*，以及优化过程中的*epochs*的数量（优化器已经看到整个训练子集的传递次数）。
- en: The validation subset also helps us know when to *stop* optimizing *before*
    overfitting our training subset.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证子集还帮助我们知道何时*停止*优化*之前*过度拟合我们的训练子集。
- en: It also serves as a test set to compare the performance of *different* machine
    learning models on the same data set, for example, comparing the performance of
    a linear regression model to a random forest to a neural network.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还作为一个测试集，用来比较*不同*机器学习模型在同一数据集上的性能，例如，比较线性回归模型、随机森林和神经网络的性能。
- en: 'The *test* subset: After deciding on the best model to use (or averaging or
    aggregating the results of multiple models) and training the model, we use this
    untouched subset of the data as a last stage test for our model before deployment
    into the real world. Since the model has not seen any of the data points in this
    subset before (which means it has not included any of them in the optimization
    process), it can be considered as the closest analogue to a real world situation.
    This allows us to judge the performance of our model before we start applying
    it on completely new real world data.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*测试*子集：在决定使用最佳模型（或对多个模型的结果进行平均或聚合）并训练模型之后，我们使用这个未触及的数据子集作为模型在部署到真实世界之前的最后阶段测试。由于模型之前没有见过这个子集中的任何数据点（这意味着它没有在优化过程中包含其中任何数据点），因此它可以被视为最接近真实世界情况的类比。这使我们能够在开始将其应用于全新的真实世界数据之前评估我们模型的性能。'
- en: Recap
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: Let’s recap a little before moving forward.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们简要回顾一下。
- en: Our current machine learning model is called *linear regression*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们当前的机器学习模型称为*线性回归*。
- en: 'Our training function is linear with formula:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的训练函数是线性的，公式为：
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 period dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 period dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
- en: The <math alttext="x"><mi>x</mi></math> ’s are the features, and the <math alttext="omega"><mi>ω</mi></math>
    ’s are the unknown weights or parameters.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="x"><mi>x</mi></math>是特征，<math alttext="omega"><mi>ω</mi></math>是未知的权重或参数。
- en: 'If we plug in the feature values of a particular data point, for example the
    tenth data point, into the formula of the training function, we get our model’s
    prediction for this point:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们将特定数据点的特征值（例如第十个数据点）代入训练函数的公式中，我们就可以得到我们模型对该点的预测：
- en: <math alttext="dollar-sign y Subscript p r e d i c t Superscript 10 Baseline
    equals omega 0 plus omega 1 x 1 Superscript 10 Baseline plus omega 2 x 2 Superscript
    10 Baseline plus omega 3 x 3 Superscript 10 Baseline plus omega 4 x 4 Superscript
    10 Baseline plus omega 5 x 5 Superscript 10 Baseline period dollar-sign"><mrow><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>10</mn></msubsup> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>10</mn></msubsup>
    <mo>.</mo></mrow></math>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y Subscript p r e d i c t Superscript 10 Baseline
    equals omega 0 plus omega 1 x 1 Superscript 10 Baseline plus omega 2 x 2 Superscript
    10 Baseline plus omega 3 x 3 Superscript 10 Baseline plus omega 4 x 4 Superscript
    10 Baseline plus omega 5 x 5 Superscript 10 Baseline period dollar-sign"><mrow><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>10</mn></msubsup> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>10</mn></msubsup>
    <mo>.</mo></mrow></math>
- en: The superscript 10 indicates that these are values corresponding to the tenth
    data point.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上标10表示这些值对应于第十个数据点。
- en: 'Our loss function is the mean squared error function with formula:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的损失函数是带有以下公式的均方误差函数：
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
- en: We want to find the values of the <math alttext="omega"><mi>ω</mi></math> ’s
    that minimize this loss function. So the next step must be solving a minimization
    (optimization) problem.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要找到最小化这个损失函数的<math alttext="omega"><mi>ω</mi></math>的值。因此，下一步必须是解决一个最小化（优化）问题。
- en: In order to make our optimization life much easier, we will once again employ
    the convenient notation of linear algebra (vectors and matrices). This allows
    us to include the entire training subset of the data as a matrix in the formula
    of the loss function, and do our computations immediately on the training subset,
    as opposed to computing on each data point separately. This little notation maneuver
    saves us from a lot of mistakes, pain, and from tedious calculations with many
    components that are difficult to keep track of on very large data sets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的优化工作更加轻松，我们将再次使用线性代数（向量和矩阵）的便捷符号。这使我们能够将整个训练数据子集作为损失函数公式中的矩阵，并立即在训练子集上进行计算，而不是在每个数据点上分别计算。这种小小的符号操作可以避免我们在非常大的数据集上出现许多难以跟踪的组件的错误、痛苦和繁琐计算。
- en: 'First, write the prediction of our model corresponding to each data point of
    the training subset:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，写出我们模型对训练子集的每个数据点的预测：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column y Subscript p r e
    d i c t Superscript 1 2nd Column equals 1 omega 0 plus omega 1 x 1 Superscript
    1 Baseline plus omega 2 x 2 Superscript 1 Baseline plus omega 3 x 3 Superscript
    1 Baseline plus omega 4 x 4 Superscript 1 Baseline plus omega 5 x 5 Superscript
    1 Baseline 2nd Row 1st Column y Subscript p r e d i c t Superscript 2 2nd Column
    equals 1 omega 0 plus omega 1 x 1 squared plus omega 2 x 2 squared plus omega
    3 x 3 squared plus omega 4 x 4 squared plus omega 5 x 5 squared 3rd Row 1st Column  ellipsis
    4th Row 1st Column y Subscript p r e d i c t Superscript m 2nd Column equals 1
    omega 0 plus omega 1 x 1 Superscript m Baseline plus omega 2 x 2 Superscript m
    Baseline plus omega 3 x 3 Superscript m Baseline plus omega 4 x 4 Superscript
    m Baseline plus omega 5 x 5 Superscript m EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr> <mtr><mtd columnalign="right"><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mi>m</mi></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mi>m</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mi>m</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mrow></mtd></mtr></mtable></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column y Subscript p r e
    d i c t Superscript 1 2nd Column equals 1 omega 0 plus omega 1 x 1 Superscript
    1 Baseline plus omega 2 x 2 Superscript 1 Baseline plus omega 3 x 3 Superscript
    1 Baseline plus omega 4 x 4 Superscript 1 Baseline plus omega 5 x 5 Superscript
    1 Baseline 2nd Row 1st Column y Subscript p r e d i c t Superscript 2 2nd Column
    equals 1 omega 0 plus omega 1 x 1 squared plus omega 2 x 2 squared plus omega
    3 x 3 squared plus omega 4 x 4 squared plus omega 5 x 5 squared 3rd Row 1st Column  ellipsis
    4th Row 1st Column y Subscript p r e d i c t Superscript m 2nd Column equals 1
    omega 0 plus omega 1 x 1 Superscript m Baseline plus omega 2 x 2 Superscript m
    Baseline plus omega 3 x 3 Superscript m Baseline plus omega 4 x 4 Superscript
    m Baseline plus omega 5 x 5 Superscript m EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></sub> <msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr> <mtr><mtd columnalign="right"><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mi>m</mi></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mi>m</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mi>m</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mrow></mtd></mtr></mtable></math>
- en: 'We can easily arrange the above system as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地将上述系统安排如下：
- en: <math alttext="dollar-sign Start 4 By 1 Matrix 1st Row  y Subscript p r e d
    i c t Superscript 1 Baseline 2nd Row  y Subscript p r e d i c t Superscript 2
    Baseline 3rd Row   ellipsis 4th Row  y Subscript p r e d i c t Superscript m Baseline
    EndMatrix equals Start 4 By 1 Matrix 1st Row  1 2nd Row  1 3rd Row   ellipsis
    4th Row  1 EndMatrix omega 0 plus Start 4 By 1 Matrix 1st Row  x 1 Superscript
    1 Baseline 2nd Row  x 1 squared 3rd Row   ellipsis 4th Row  x 1 Superscript m
    Baseline EndMatrix omega 1 plus Start 4 By 1 Matrix 1st Row  x 1 Superscript 1
    Baseline 2nd Row  x 2 squared 3rd Row   ellipsis 4th Row  x 2 Superscript m Baseline
    EndMatrix omega 2 plus Start 4 By 1 Matrix 1st Row  x 3 Superscript 1 Baseline
    2nd Row  x 3 squared 3rd Row   ellipsis 4th Row  x 3 Superscript m Baseline EndMatrix
    omega 3 plus Start 4 By 1 Matrix 1st Row  x 4 Superscript 1 Baseline 2nd Row  x
    4 squared 3rd Row   ellipsis 4th Row  x 4 Superscript m Baseline EndMatrix omega
    4 plus Start 4 By 1 Matrix 1st Row  x 5 Superscript 1 Baseline 2nd Row  x 5 squared
    3rd Row   ellipsis 4th Row  x 5 Superscript m Baseline EndMatrix omega 5 comma
    dollar-sign"><mrow><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>1</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>1</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>2</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>2</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>3</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>3</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>4</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>4</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>5</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>5</mn></msub> <mo>,</mo></mrow></math>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="美元符号 开始 4x1 矩阵 第一行  y 下标 p r e d i c t 上标 1 2nd Row  y 下标 p r
    e d i c t 上标 2 3rd Row   省略 4th Row  y 下标 p r e d i c t 上标 m 等于 开始 4x1 矩阵 第一行  1
    2nd Row  1 3rd Row   省略 4th Row  1 EndMatrix omega 0 加 开始 4x1 矩阵 第一行  x 1 上标 1
    2nd Row  x 1 平方 3rd Row   省略 4th Row  x 1 上标 m EndMatrix omega 1 加 开始 4x1 矩阵 第一行  x
    1 上标 1 2nd Row  x 2 平方 3rd Row   省略 4th Row  x 2 上标 m EndMatrix omega 2 加 开始 4x1
    矩阵 第一行  x 3 上标 1 2nd Row  x 3 平方 3rd Row   省略 4th Row  x 3 上标 m EndMatrix omega
    3 加 开始 4x1 矩阵 第一行  x 4 上标 1 2nd Row  x 4 平方 3rd Row   省略 4th Row  x 4 上标 m EndMatrix
    omega 4 加 开始 4x1 矩阵 第一行  x 5 上标 1 2nd Row  x 5 平方 3rd Row   省略 4th Row  x 5 上标
    m EndMatrix omega 5 逗号 美元符号"><mrow><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>1</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>1</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>2</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>2</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>3</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>3</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>4</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>4</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>5</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>5</mn></msub> <mo>,</mo></mrow></math>
- en: 'or even better:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更好：
- en: <math alttext="dollar-sign Start 4 By 1 Matrix 1st Row  y Subscript p r e d
    i c t Superscript 1 Baseline 2nd Row  y Subscript p r e d i c t Superscript 2
    Baseline 3rd Row   ellipsis 4th Row  y Subscript p r e d i c t Superscript m Baseline
    EndMatrix equals Start 4 By 6 Matrix 1st Row 1st Column 1 2nd Column x 1 Superscript
    1 Baseline 3rd Column x 2 Superscript 1 Baseline 4th Column x 3 Superscript 1
    Baseline 5th Column x 4 Superscript 1 Baseline 6th Column x 5 Superscript 1 Baseline
    2nd Row 1st Column 1 2nd Column x 1 squared 3rd Column x 2 squared 4th Column
    x 3 squared 5th Column x 4 squared 6th Column x 5 squared 3rd Row 1st Column  ellipsis
    4th Row 1st Column 1 2nd Column x 1 Superscript m Baseline 3rd Column x 2 Superscript
    m Baseline 4th Column x 3 Superscript m Baseline 5th Column x 4 Superscript m
    Baseline 6th Column x 5 Superscript m Baseline EndMatrix Start 6 By 1 Matrix 1st
    Row  omega 0 2nd Row  omega 1 3rd Row  omega 2 4th Row  omega 3 5th Row  omega
    4 6th Row  omega 5 EndMatrix period dollar-sign"><mrow><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi> <mn>1</mn>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>4</mn> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi> <mn>1</mn> <mi>m</mi></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>4</mn> <mi>m</mi></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced>
    <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>ω</mi> <mn>0</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi>
    <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi> <mn>3</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>4</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi>
    <mn>5</mn></msub></mtd></mtr></mtable></mfenced> <mo>.</mo></mrow></math>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector on the left hand side of the above equation is <math alttext="ModifyingAbove
    y With right-arrow Subscript p r e d i c t"><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>
    , the matrix on the right hand side is the training subset *X* augmented with
    the vector of ones, and the last vector on the right hand side has all the unknown
    weights packed neatly into it. Call this vector <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    , then write <math alttext="ModifyingAbove y With right-arrow Subscript p r e
    d i c t"><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>
    compactly in terms of the training subset and <math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程左侧的向量是<math alttext="ModifyingAbove y With right-arrow Subscript p r e d
    i c t"><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>，右侧的矩阵是训练子集*X*与包含1的向量增广，右侧的最后一个向量包含了所有未知的权重。将这个向量称为<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>，然后用训练子集和<math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math>简洁地写成<math alttext="ModifyingAbove
    y With right-arrow Subscript p r e d i c t"><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>：
- en: <math alttext="dollar-sign ModifyingAbove y With right-arrow Subscript p r e
    d i c t Baseline equals upper X ModifyingAbove omega With right-arrow period dollar-sign"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>.</mo></mrow></math>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove y With right-arrow Subscript p r e
    d i c t Baseline equals upper X ModifyingAbove omega With right-arrow period dollar-sign"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>.</mo></mrow></math>
- en: 'Now the formula of the mean squared error loss function, which we wrote before
    as:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们之前写过的均方误差损失函数的公式如下：
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup></mrow></math>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup></mrow></math>
- en: 'becomes:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 变成：
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis upper X ModifyingAbove omega With right-arrow minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis Superscript t
    Baseline left-parenthesis upper X ModifyingAbove omega With right-arrow minus
    ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    equals StartFraction 1 Over m EndFraction parallel-to upper X ModifyingAbove omega
    With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u e Baseline
    parallel-to period dollar-sign"><mrow><mtext>Mean</mtext> <mtext>Squared</mtext>
    <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced
    close=")" open="(" separators=""><mi>X</mi><mover accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis upper X ModifyingAbove omega With right-arrow minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis Superscript t
    Baseline left-parenthesis upper X ModifyingAbove omega With right-arrow minus
    ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    equals StartFraction 1 Over m EndFraction parallel-to upper X ModifyingAbove omega
    With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u e Baseline
    parallel-to period dollar-sign"><mrow><mtext>均方误差</mtext> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <msup><mfenced close=")" open="(" separators=""><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
- en: We are now ready to find the <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that minimizes the neatly written
    loss function. For that, we have to visit the rich and beautiful mathematical
    field of optimization.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备找到最小化精心编写的损失函数的<math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math>。为此，我们必须访问优化的丰富而美丽的数学领域。
- en: When The Training Data Has Highly Correlated Features
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当训练数据具有高度相关的特征时
- en: Inspecting the training matrix (augmented with the vector of ones)
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 检查训练矩阵（附加了一个包含1的向量）
- en: <math alttext="dollar-sign upper X equals Start 4 By 6 Matrix 1st Row 1st Column
    1 2nd Column x 1 Superscript 1 Baseline 3rd Column x 2 Superscript 1 Baseline
    4th Column x 3 Superscript 1 Baseline 5th Column x 4 Superscript 1 Baseline 6th
    Column x 5 Superscript 1 Baseline 2nd Row 1st Column 1 2nd Column x 1 squared
    3rd Column x 2 squared 4th Column x 3 squared 5th Column x 4 squared 6th Column
    x 5 squared 3rd Row 1st Column  ellipsis 4th Row 1st Column 1 2nd Column x 1 Superscript
    m Baseline 3rd Column x 2 Superscript m Baseline 4th Column x 3 Superscript m
    Baseline 5th Column x 4 Superscript m Baseline 6th Column x 5 Superscript m EndMatrix
    dollar-sign"><mrow><mi>X</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>2</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper X equals Start 4 By 6 Matrix 1st Row 1st Column
    1 2nd Column x 1 Superscript 1 Baseline 3rd Column x 2 Superscript 1 Baseline
    4th Column x 3 Superscript 1 Baseline 5th Column x 4 Superscript 1 Baseline 6th
    Column x 5 Superscript 1 Baseline 2nd Row 1st Column 1 2nd Column x 1 squared
    3rd Column x 2 squared 4th Column x 3 squared 5th Column x 4 squared 6th Column
    x 5 squared 3rd Row 1st Column  ellipsis 4th Row 1st Column 1 2nd Column x 1 Superscript
    m Baseline 3rd Column x 2 Superscript m Baseline 4th Column x 3 Superscript m
    Baseline 5th Column x 4 Superscript m Baseline 6th Column x 5 Superscript m EndMatrix
    dollar-sign"><mrow><mi>X</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>2</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'that appears in the vector <math alttext="ModifyingAbove y With right-arrow
    Subscript p r e d i c t Baseline equals upper X ModifyingAbove omega With right-arrow"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math>
    , the formula of the mean squared error loss function, and later in the formula
    that determines the unknown <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> (also called *the normal equation*):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 出现在向量<math alttext="ModifyingAbove y With right-arrow Subscript p r e d i c
    t Baseline equals upper X ModifyingAbove omega With right-arrow"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math>中，均方误差损失函数的公式，以及后来确定未知<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>（也称为*正规方程*）的公式。
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis Superscript negative
    1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline comma dollar-sign"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>,</mo></mrow></math>
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis Superscript negative
    1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline comma dollar-sign"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></row></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>,</mo></mrow></math>
- en: 'we can see how our model might have a problem if two or more features (*x*
    columns) of the data are highly correlated: This means that there is a strong
    linear relationship between the features, so one of these features can be determined
    (or nearly determined) using a linear combination of the others. Thus, the corresponding
    feature columns are *not linearly independent* (or close to not being linearly
    independent). For matrices, this is a problem, since it indicates that the matrix
    either cannot be inverted or is *ill conditioned*. Ill conditioned matrices produce
    large instabilities in computations, since slight variations in the training data
    (which must be assumed) produces large variations in the model parameters and
    hence renders its predictions unreliable.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果数据的两个或更多特征（*x*列）高度相关，我们的模型可能会出现问题：这意味着特征之间存在强烈的线性关系，因此其中一个特征可以通过其他特征的线性组合来确定（或几乎确定）。因此，相应的特征列是*不线性独立*的（或接近不是线性独立的）。对于矩阵来说，这是一个问题，因为它表明矩阵要么不能被反转，要么*病态*。病态的矩阵在计算中产生大的不稳定性，因为训练数据的轻微变化（必须假设）会导致模型参数的大变化，从而使其预测不可靠。
- en: We desire well conditioned matrices in our computations, so we must get rid
    of the sources of ill conditioning. When we have highly correlated features, one
    possible avenue is to include only one of them in our model, as the others do
    not add much information. Another solution is to apply dimension reduction techniques
    such as Principal Component Analysis, which we will encounter in [Chapter 11](ch11.xhtml#ch11).
    The Fish Market data set has highly correlated features, and the accompanying
    Jupyter Notebook addresses those.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在计算中希望得到条件良好的矩阵，因此必须消除病态条件的来源。当我们有高度相关的特征时，一个可能的途径是只在我们的模型中包含其中一个，因为其他特征并不添加太多信息。另一个解决方案是应用主成分分析等降维技术，我们将在[第11章](ch11.xhtml#ch11)中遇到。鱼市数据集具有高度相关的特征，附带的Jupyter
    Notebook解决了这些问题。
- en: That said, it is important to note that some machine learning models, such as
    decision trees and random forests (discussed below) are not affected by correlated
    featured, while others, such as the current linear regression model, and the next
    logistic regression and support vector machines models are negatively affected
    by them. As for neural network models, even though they can *learn* the correlations
    involved in the data features during training, they perform better when these
    redundancies are taken care of ahead of time, in addition to saving computation
    cost and time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，重要的是要注意，一些机器学习模型，如决策树和随机森林（下文讨论），不受相关特征的影响，而其他一些模型，如当前的线性回归模型，以及接下来的逻辑回归和支持向量机模型受到了负面影响。至于神经网络模型，即使它们可以在训练过程中*学习*数据特征中的相关性，但当这些冗余在时间之前得到处理时，它们的表现更好，除了节省计算成本和时间。
- en: Optimization
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化
- en: Optimization means find the optimal, best, maximal, minimal, or extreme solution.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 优化意味着寻找最佳、最大、最小或极端解决方案。
- en: We wrote a linear training function
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了一个线性训练函数
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 period dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 period dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
- en: and we left the values of its six parameters <math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math> , <math alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>
    , <math alttext="omega 2"><msub><mi>ω</mi> <mn>2</mn></msub></math> , <math alttext="omega
    3"><msub><mi>ω</mi> <mn>3</mn></msub></math> , <math alttext="omega 4"><msub><mi>ω</mi>
    <mn>4</mn></msub></math> , and <math alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math>
    unknown. The goal is to find the values that make our training function *best
    fit the training data subset*, where the word *best* is quantified using the loss
    function. This function provides a measure of how far the prediction made by the
    model’s training function is from the ground truth. We want this loss function
    to be small so we solve a minimization problem.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们留下了它的六个参数<math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math>，<math
    alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>，<math alttext="omega
    2"><msub><mi>ω</mi> <mn>2</mn></msub></math>，<math alttext="omega 3"><msub><mi>ω</mi>
    <mn>3</mn></msub></math>，<math alttext="omega 4"><msub><mi>ω</mi> <mn>4</mn></msub></math>和<math
    alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math>的值未知。目标是找到使我们的训练函数*最适合训练数据子集*的值，其中*最适合*一词是使用损失函数量化的。该函数提供了模型训练函数所做预测与真实情况的偏差程度的度量。我们希望这个损失函数很小，因此我们解决了一个最小化问题。
- en: We are not going to sit there and try out every possible <math alttext="omega"><mi>ω</mi></math>
    value until we find the combination that gives the least loss. Even if we did,
    we wouldn’t know when to stop, since we wouldn’t know whether there are other
    *better* values. We must have prior knowledge about the landscape of the loss
    function and take advantage of its mathematical properties. The analogy is hiking
    down the Swiss Alps with a blindfold *vs.* hiking with no blindfold and a detailed
    map ([Figure 3-7](#Fig_Swiss_Alps2) shows the rough terrain of the Swiss Alps).
    Instead of searching the landscape of the loss function for minimizers with a
    blindfold, we tap into the field of *optimization*. Optimization is a beautiful
    branch of mathematics that provides various methods to efficiently search for
    and locate optimizers of functions and their corresponding optimal values.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会坐在那里尝试每个可能的<math alttext="omega"><mi>ω</mi></math>值，直到找到使损失最小的组合。即使我们这样做了，我们也不会知道何时停止，因为我们不知道是否还有其他*更好*的值。我们必须对损失函数的地形有先验知识，并利用其数学特性。类比是盲目徒步穿越瑞士阿尔卑斯山*vs.*带着详细地图徒步穿越（[图3-7](#Fig_Swiss_Alps2)显示了瑞士阿尔卑斯山的崎岖地形）。我们不是盲目地搜索损失函数的地形以寻找最小值，而是利用*优化*领域。优化是数学的一个美丽分支，提供了各种方法来高效地搜索函数的最优解及其对应的最优值。
- en: '![275](assets/emai_0307.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![275](assets/emai_0307.png)'
- en: 'Figure 3-7\. Swiss Alps: Optimization is similar to hiking the landscape of
    a function. The destination is the bottom of the lowest valley (minimization)
    or the top of the highest peak (maximization). We need two things: The coordinates
    of the minimizing or maximizing points, and the height of the landscape at those
    points.'
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7. 瑞士阿尔卑斯山：优化类似于徒步穿越函数的地形。目的地是最低谷的底部（最小化）或最高峰的顶部（最大化）。我们需要两样东西：最小化或最大化点的坐标，以及这些点的地形高度。
- en: 'The optimization problem in this chapter and in the next few looks like:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和接下来几章的优化问题如下：
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts Loss Function period dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mtext>Loss</mtext>
    <mtext>Function</mtext> <mo>.</mo></mrow></math>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts Loss Function period dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mtext>Loss</mtext>
    <mtext>Function</mtext> <mo>.</mo></mrow></math>
- en: 'For the current linear regression model, this is:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前的线性回归模型，这是：
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts StartFraction 1 Over m EndFraction left-parenthesis upper X ModifyingAbove
    omega With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis Superscript t Baseline left-parenthesis upper X ModifyingAbove
    omega With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals min Underscript ModifyingAbove omega With
    right-arrow Endscripts StartFraction 1 Over m EndFraction parallel-to upper X
    ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><msub><mo form="prefix"
    movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced close=")" open="(" separators=""><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>=</mo> <msub><mo form="prefix" movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></msub> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts StartFraction 1 Over m EndFraction left-parenthesis upper X ModifyingAbove
    omega With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis Superscript t Baseline left-parenthesis upper X ModifyingAbove
    omega With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals min Underscript ModifyingAbove omega With
    right-arrow Endscripts StartFraction 1 Over m EndFraction parallel-to upper X
    ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><msub><mo form="prefix"
    movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced close=")" open="(" separators=""><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>=</mo> <msub><mo form="prefix" movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></msub> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
- en: 'When we do math, we should never lose track of what it is that we know and
    what it is that we are looking for. Otherwise we would run risk of getting trapped
    in a circular logic. In the above formula, we know:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行数学运算时，我们绝不能忘记我们知道什么，我们正在寻找什么。否则我们可能会陷入循环逻辑的陷阱。在上述公式中，我们知道：
- en: '*m* (the number of instances in the training subset),'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*（训练子集中的实例数），'
- en: '*X* (the training subset augmented with a vector of ones),'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*（训练子集增加了一个全为1的向量），'
- en: <math alttext="ModifyingAbove y With right-arrow Subscript t r u e"><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    (the vector of labels corresponding to the training subset).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练子集对应的标签向量中的最小损失函数值。
- en: 'And we are looking for:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在寻找：
- en: 'The minimizing <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> : We must locate it.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化的<math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>：我们必须找到它。
- en: The minimum value of the loss function at the minimizing <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    .
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最小化的损失函数值处的<math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>。
- en: Convex landscapes *vs.* non-convex lanscapes
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 凸景观 *vs.* 非凸景观
- en: '*The easiest functions* to deal with and the easiest equations to solve are
    linear. Unfortunately, most of the functions (and equations) that we deal with
    are nonlinear. At the same time, this is not too unfortunate since linear life
    is flat, boring, unstimulating, and uneventful. When the function we have at our
    hands is full-blown nonlinear, we sometimes *linearize* it near certain points
    that we care for. The idea here is that even though the full picture of the function
    might be nonlinear, we may be able to approximate it by a linear function in the
    locality that we focus on. In other words, in a very small neighborhood, the nonlinear
    function might look and behave linearly, albeit the said neighborhood might be
    infinitesimally small. For an analogy, think about how Earth looks (and behaves
    in terms of calculating distances *etc.*) flatly from our own locality, and how
    we can only see its nonlinear shape from high up. When we want to linearize a
    function near a point, we approximate it by its tangent space near that point
    (this is its tangent line if it is a function of one variable, tangent plane if
    it is a function of two variables, and tangent hyperplane if it is a function
    of three or more variables). For this, we need to calculate one derivative of
    the function with respect to all of its variables, since this gives us the slope
    (which measures the inclination) of the approximating flat space.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*最容易处理*和最容易解决的方程是线性的。不幸的是，我们处理的大多数函数（和方程）都是非线性的。与此同时，这并不太不幸，因为线性的生活是平淡、无聊、乏味和无趣的。当我们手头的函数是完全非线性的时候，有时我们会在我们关心的某些点附近*线性化*它。这里的想法是，即使函数的整体图像可能是非线性的，我们可能能够在我们关注的局部用线性函数来近似它。换句话说，在一个非常小的邻域内，非线性函数可能看起来和行为上是线性的，尽管该邻域可能是无限小的。打个比方，想想地球从我们自己的地方看起来是平的，从高处我们只能看到它的非线性形状。当我们想要在某一点附近线性化一个函数时，我们通过计算函数对所有变量的一个导数来近似它，因为这给我们提供了近似平坦空间的斜率（它测量了近似平坦空间的倾斜度）。'
- en: 'The sad news is that linearizing near one point may not be enough, and we may
    want to use linear approximations at multiple locations. Thankfully that is doable,
    since we all we have to do computationally is to evaluate one derivative at several
    points. This leads us to *the next easiest functions* to deal with (after linear
    functions): Piecewise linear functions, which are linear but only in piecewise
    structures, or linear except at isolated points or locations. The field of *linear
    programming* deals with such functions, where the functions to optimize are linear,
    and the boundaries of the domains where the optimization happens are piecewise
    linear (they are intersections of half spaces).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 令人沮丧的消息是，在一个点附近进行线性化可能不够，我们可能希望在多个位置使用线性近似。幸运的是，这是可行的，因为我们在计算上所要做的就是在几个点上评估一个导数。这将我们带到了（在线性函数之后）*最容易处理的函数*：分段线性函数，它们在结构上是线性的，或者在孤立点或位置上是线性的。*线性规划*领域处理这样的函数，其中要优化的函数是线性的，而优化发生的域的边界是分段线性的（它们是半空间的交集）。
- en: When our goal is optimization, the best functions to deal with are either linear
    (where the field of linear programming helps us) or convex (where we do not worry
    about getting stuck at local minima, and where we have good inequalities that
    help us with the analysis).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的目标是优化时，最好处理的函数要么是线性的（线性规划领域帮助我们），要么是凸的（我们不用担心陷入局部最小值，而且我们有很好的不等式帮助我们进行分析）。
- en: 'One important type of functions to keep in mind, which appears in machine learning,
    is a function which is the maximum of two or more convex functions. These functions
    are always convex. Recall that linear functions are flat so they are at the same
    time convex and concave. This is useful since some functions are defined as the
    maxima of linear functions: Those are not guaranteed to be linear (they are piecewise
    linear), but are guaranteed to be convex. That is, even though we lose linearity
    when we take the maximum of linear functions, we are compensated with convexity.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中要记住的一个重要类型的函数是两个或多个凸函数的最大值。这些函数总是凸的。线性函数是平的，因此它们同时是凸的和凹的。这很有用，因为有些函数被定义为线性函数的最大值：这些函数不一定是线性的（它们是分段线性的），但是保证是凸的。也就是说，即使我们在取线性函数的最大值时失去了线性性，但我们得到了凸性的补偿。
- en: 'The Rectified Linear Unit function (ReLU) that is used as a nonlinear activation
    function in neural networks is an example of a function defined as the maximum
    of two linear functions: <math alttext="upper R e upper L upper U left-parenthesis
    x right-parenthesis equals m a x left-parenthesis 0 comma x right-parenthesis"><mrow><mi>R</mi>
    <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>m</mi>
    <mi>a</mi> <mi>x</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    . Another example is the Hinge Loss function used for support vector machines:
    <math alttext="upper H left-parenthesis x right-parenthesis equals m a x left-parenthesis
    0 comma 1 minus t x right-parenthesis"><mrow><mi>H</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mn>1</mn> <mo>-</mo> <mi>t</mi> <mi>x</mi> <mo>)</mo></mrow></math> where *t*
    is either 1 or -1.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中作为非线性激活函数使用的修正线性单元函数（ReLU）是一个被定义为两个线性函数的最大值的例子：<math alttext="upper R
    e upper L upper U left-parenthesis x right-parenthesis equals m a x left-parenthesis
    0 comma x right-parenthesis"><mrow><mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></math>。另一个例子是支持向量机中使用的铰链损失函数：<math
    alttext="upper H left-parenthesis x right-parenthesis equals m a x left-parenthesis
    0 comma 1 minus t x right-parenthesis"><mrow><mi>H</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mn>1</mn> <mo>-</mo> <mi>t</mi> <mi>x</mi> <mo>)</mo></mrow></math>，其中*t*是1或-1。
- en: Note that the minimum of a family of convex functions is not guaranteed to be
    convex, it can have a double well. However, their maximum is definitely convex.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一组凸函数的最小值不能保证是凸的，它可能有双井。然而，它们的最大值肯定是凸的。
- en: 'There is one more relationship between linearity and convexity: If we have
    a convex function (nonlinear since linear would be trivial), then the maximum
    of all the linear functions that stay below our function is exactly equal to it.
    In other words, convexity replaces linearity, in the sense that when linearity
    is not available, but convexity is, we can replace our convex function with the
    maximum of all the linear functions whose graph lies below our function’s graph
    (see [Figure 3-8](#Fig_convex_above_tangents)). Recall that the graph of a convex
    function lies above the graph of its tangent at any point, and that the tangents
    are linear. This gives us a direct path to exploiting the simplicity of linear
    functions when we have convex functions. We have equality when we consider the
    maximum of *all* the tangents, and only approximation when we consider the maximum
    of the tangents at few points.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 线性和凸性之间还有另一个关系：如果我们有一个凸函数（非线性，因为线性将是平凡的），那么所有保持在函数下方的线性函数的最大值恰好等于它。换句话说，凸性取代了线性性，意思是当线性性不可用时，但凸性可用时，我们可以用所有图形位于函数图形下方的线性函数的最大值来替换我们的凸函数（见[图3-8](#Fig_convex_above_tangents)）。请记住，凸函数的图形位于任意点的切线图形上方，而切线是线性的。这为我们提供了一条直接利用线性函数简单性的路径，当我们有凸函数时。当我们考虑*所有*切线的最大值时，我们有相等，当我们考虑少数点的切线的最大值时，我们只有近似。
- en: '![300](assets/emai_0308.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0308.png)'
- en: Figure 3-8\. A convex function is equal to the maximum of all of its tangents.
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8。凸函数等于其所有切线的最大值。
- en: '[Figure 3-9](#Fig_convex_landscape) and [Figure 3-10](#Fig_nonconvex_landscape)
    show the general landscapes of nonlinear convex and nonconvex functions repectively.
    Overall, the landscape of a convex function is good for minimization problems.
    We have no fear of getting stuck at local minima since any local minimum is also
    a global minimum for a convex function. The landscape of a non-convex function
    has peaks, valleys, and saddle points. A minimization problem on such a landscape
    runs the risk of getting stuck at local minima and never finding the global minima.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-9](#Fig_convex_landscape)和[图3-10](#Fig_nonconvex_landscape)分别展示了非线性凸函数和非凸函数的一般景观。总的来说，凸函数的景观对于最小化问题是有利的。我们不必担心被困在局部最小值，因为对于凸函数来说，任何局部最小值也是全局最小值。非凸函数的景观有峰值、谷底和鞍点。在这样的景观上进行最小化问题会有被困在局部最小值而无法找到全局最小值的风险。'
- en: '![280](assets/emai_0309.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0309.png)'
- en: Figure 3-9\. The landscape of a convex function is good for minimization problems.
    We have no fear of getting stuck at local minima since any local minimum is also
    a global minimum for a convex function.
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9。凸函数的景观对于最小化问题是有利的。我们不必担心被困在局部最小值，因为对于凸函数来说，任何局部最小值也是全局最小值。
- en: '![280](assets/emai_0310.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![280](assets/emai_0310.png)'
- en: Figure 3-10\. The landscape of a non-convex function has peaks, valleys, and
    saddle points. A minimization problem on such a landscape runs the risk of getting
    stuck at local minima and never finding the global minima.
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10。非凸函数的景观有峰值、谷底和鞍点。在这样的景观上进行最小化问题会有被困在局部最小值而无法找到全局最小值的风险。
- en: Finally, make sure you know the distinction between a convex function, a convex
    set, and a convex optimization problem, which optimizes a convex function over
    a convex set.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请确保你知道凸函数、凸集和凸优化问题之间的区别，凸优化问题是在凸集上优化凸函数。
- en: How do we locate minimizers of functions?
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们如何找到函数的最小值？
- en: 'In general, there are two approaches to locating minimizers (and/or maximizers)
    of functions. The tradeoff is usually between:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，有两种方法来找到函数的最小值（和/或最大值）。通常的权衡是：
- en: Calculating only one derivative and converging to the minimum slowly (though
    there are acceleration methods to speed the convergence up). These are called
    *gradient* methods. The gradient is one derivative of a function of several variables.
    For example, our loss function is a function of several <math alttext="omega"><mi>ω</mi></math>
    ’s (or of one vector <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> ).
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只计算一个导数并缓慢收敛到最小值（尽管有加速方法可以加快收敛速度）。这些被称为*梯度*方法。梯度是多个变量的函数的一个导数。例如，我们的损失函数是多个<math
    alttext="omega"><mi>ω</mi></math>的函数（或一个向量<math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>）。
- en: Calculating two derivatives (computationally much more expensive, which is a
    big turn off, especially when we have thousands of parameters) and converging
    to the minimum faster. Computation costs can be saved a little by approximating
    the second derivative instead of computing it exactly. Second derivative methods
    are called *Newton*’s methods. The *Hessian* (the matrix of second derivatives)
    or an approximation of the Hessian appear in these methods.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两个导数（在计算上更昂贵，尤其是当我们有成千上万个参数时，这是一个大的缺点），并更快地收敛到最小值。可以通过近似第二导数来节省一些计算成本，而不是精确计算它。第二导数方法被称为*牛顿*方法。*Hessian*（二阶导数的矩阵）或Hessian的近似出现在这些方法中。
- en: We never need to go beyond calculating two derivatives.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从不需要计算超过两个导数。
- en: But why are the first and second derivatives of a function so important for
    locating its optimizers? The concise answer is that the first derivative contains
    information on how fast a function increases or decreases at a point (so if you
    follow its direction you might ascend to the maximum or descend to the minimum),
    and the second derivative contains information on the shape of the *bowl* of the
    function, if it curves up or curves down.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么函数的一阶和二阶导数对于找到其最优解如此重要呢？简洁的答案是，一阶导数包含了函数在某一点上增加或减少的速度信息（因此如果你按照它的方向，你可能会上升到最大值或下降到最小值），而二阶导数包含了函数的“碗”的形状信息，它是向上弯曲还是向下弯曲。
- en: 'One key idea from calculus remains fundamental: Minimizers (and/or maximizers)
    happen at *critical points* (defined as the points where one derivative of our
    function is either equal to zero or does not exist) or at boundary points. So
    in order to locate these optimizers, we must search through *both* the boundary
    points (if our search space has a boundary) *and* the interior critical points.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分中的一个关键思想仍然是基本的：极小值（和/或极大值）发生在*临界点*（定义为函数的一个导数为零或不存在的点）或边界点。因此，为了找到这些最优解，我们必须搜索*边界点*（如果我们的搜索空间有边界）*和*内部临界点。
- en: How do we locate the critical points in the interior of our search space?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何找到搜索空间内部的临界点？
- en: Approach 1
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 方法1
- en: 'We follow these steps:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照这些步骤进行。
- en: Find one derivative of our function (not too bad, we all did it in calculus),
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到我们函数的一个导数（不太糟糕，我们在微积分中都做过），
- en: then set it equal to zero (we can all write the symbols equal and zero),
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将其设置为零（我们都可以写出等于和零的符号），
- en: and solve for the <math alttext="omega"><mi>ω</mi></math> ’s that make our derivative
    zero (this is the bad step!).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并解出使我们的导数为零的<math alttext="omega"><mi>ω</mi></math>（这是一个糟糕的步骤！）。
- en: For functions whose derivatives are linear, such as our mean squared error loss
    function, it is sort of easy to solve for these <math alttext="omega"><mi>ω</mi></math>
    ’s. The field of linear algebra was especially built to help solve linear systems
    of equations. The field of numerical linear algebra was built to help solve realistic
    and large systems of linear equations where ill conditioning is prevalent. We
    have many tools at our disposal (and software packages) when our systems are linear.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其导数是线性的函数，比如我们的均方误差损失函数，解这些<math alttext="omega"><mi>ω</mi></math>是相对容易的。线性代数领域特别是为了帮助解线性方程组而建立的。数值线性代数领域是为了帮助解决现实和大型的线性方程组而建立的，其中病态条件很普遍。当我们的系统是线性的时，我们有很多可用的工具（和软件包）。
- en: 'On the other hand, when our equations are nonlinear, finding solutions is an
    entirely different story. It becomes a hit or miss game, with mostly miss! Here’s
    a short example that illustrates the difference between solving a linear and a
    nonlinear equation:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当我们的方程是非线性的时，找到解是完全不同的故事。这成为了一个碰运气的游戏，大多数情况下都是运气不佳！以下是一个简短的例子，说明了解线性和非线性方程之间的差异：
- en: Solving a linear equation
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 解线性方程
- en: Find <math alttext="omega"><mi>ω</mi></math> such that <math alttext="0.002
    omega minus 5 equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>002</mn> <mi>ω</mi> <mo>-</mo>
    <mn>5</mn> <mo>=</mo> <mn>0</mn></mrow></math> .
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 找到<math alttext="omega"><mi>ω</mi></math>，使得<math alttext="0.002 omega minus
    5 equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>002</mn> <mi>ω</mi> <mo>-</mo> <mn>5</mn>
    <mo>=</mo> <mn>0</mn></mrow></math>。
- en: '*Solution*: Moving the 5 over to the other side then dividing by 0.002, we
    get <math alttext="omega equals 5 slash 0.002 equals 2500"><mrow><mi>ω</mi> <mo>=</mo>
    <mn>5</mn> <mo>/</mo> <mn>0</mn> <mo>.</mo> <mn>002</mn> <mo>=</mo> <mn>2500</mn></mrow></math>
    . Done.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案*：将5移到另一边，然后除以0.002，我们得到<math alttext="omega equals 5 slash 0.002 equals
    2500"><mrow><mi>ω</mi> <mo>=</mo> <mn>5</mn> <mo>/</mo> <mn>0</mn> <mo>.</mo>
    <mn>002</mn> <mo>=</mo> <mn>2500</mn></mrow></math>。完成。'
- en: Solving a nonlinear equation
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 解非线性方程
- en: Find <math alttext="omega"><mi>ω</mi></math> such that <math alttext="0.002
    sine left-parenthesis omega right-parenthesis minus 5 omega squared plus e Superscript
    omega Baseline equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>002</mn> <mo form="prefix">sin</mo>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>-</mo> <mn>5</mn> <msup><mi>ω</mi>
    <mn>2</mn></msup> <mo>+</mo> <msup><mi>e</mi> <mi>ω</mi></msup> <mo>=</mo> <mn>0</mn></mrow></math>
    .
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 找到<math alttext="omega"><mi>ω</mi></math>，使得<math alttext="0.002 sine left-parenthesis
    omega right-parenthesis minus 5 omega squared plus e Superscript omega Baseline
    equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>002</mn> <mo form="prefix">sin</mo>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>-</mo> <mn>5</mn> <msup><mi>ω</mi>
    <mn>2</mn></msup> <mo>+</mo> <msup><mi>e</mi> <mi>ω</mi></msup> <mo>=</mo> <mn>0</mn></mrow></math>。
- en: '*Solution*: Yes, I am out of here. We need a numerical method! (See [Figure 3-11](#Fig_roots_f_nonlinear)
    for a graphical approximation of the solution of this nonlinear equation).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*解决方案*：是的，我要离开了。我们需要一个数值方法！（见[图3-11](#Fig_roots_f_nonlinear)以图形逼近解这个非线性方程的解）。'
- en: '![275](assets/emai_0311.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![275](assets/emai_0311.png)'
- en: Figure 3-11\. It is difficult to solve nonlinear equations. Here, we plot <math
    alttext="f left-parenthesis omega right-parenthesis equals 0.002 sine left-parenthesis
    omega right-parenthesis minus 5 omega squared plus e Superscript omega"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>002</mn> <mo form="prefix">sin</mo> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mn>5</mn> <msup><mi>ω</mi> <mn>2</mn></msup> <mo>+</mo> <msup><mi>e</mi>
    <mi>ω</mi></msup></mrow></math> and approximate its three roots (points where
    <math alttext="f left-parenthesis omega right-parenthesis equals 0"><mrow><mi>f</mi>
    <mo>(</mo> <mi>ω</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math> ) on the
    graph.
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-11。解非线性方程很困难。在这里，我们绘制<math alttext="f left-parenthesis omega right-parenthesis
    equals 0.002 sine left-parenthesis omega right-parenthesis minus 5 omega squared
    plus e Superscript omega"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>002</mn> <mo form="prefix">sin</mo> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow> <mo>-</mo> <mn>5</mn> <msup><mi>ω</mi> <mn>2</mn></msup>
    <mo>+</mo> <msup><mi>e</mi> <mi>ω</mi></msup></mrow></math>并在图上近似其三个根（使<math alttext="f
    left-parenthesis omega right-parenthesis equals 0"><mrow><mi>f</mi> <mo>(</mo>
    <mi>ω</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math>的点）。
- en: There are many numerical techniques devoted solely to finding solutions of nonlinear
    equations (and entire fields devoted to numerically solving nonlinear ordinary
    and partial differential equations). These methods find approximate solutions
    then provide bounds on how far off the numerical solutions are from the exact
    analytical solutions. They usually construct a sequence that converges to the
    anaytical solution under certain conditions. Some methods converge faster than
    others, and are better suited for certain problems but not others.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多数值技术专门用于求解非线性方程的解（以及专门用于数值求解非线性常微分方程和偏微分方程的整个领域）。这些方法找到近似解，然后提供数值解与精确解相差多远的界限。它们通常构造一个在某些条件下收敛到解析解的序列。有些方法收敛速度比其他方法快，并且更适合某些问题而不适合其他问题。
- en: Approach 2
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 方法2
- en: Another option is to follow the gradient direction, in order to descend towards
    the minimum or ascend towards the maximum.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是沿着梯度方向前进，以便向最小值下降或向最大值上升。
- en: To understand these gradient type methods, think of hiking down a mountain (or
    skiing down the mountain if the method is accelerated or has momentum). We start
    at a random point in our search space and that sets us at an initial height level
    on the landscape of the function. Now the method moves us to a new point in the
    search space, and hopefully, at this new location, we end up at a new height level
    that is *lower* than the height level we came from. Hence, we would’ve *descended*.
    We repeat this and ideally, if the terrain of the function cooperates, this sequence
    of points will converge towards the minimizer of the function that we are looking
    for.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这些梯度类型的方法，可以想象下山徒步旅行（或者如果方法加速或具有动量，则滑雪下山）。我们从搜索空间中的一个随机点开始，这将使我们处于函数景观的初始高度水平。现在，该方法将我们移动到搜索空间中的一个新点，希望在这个新位置，我们最终到达的高度水平比我们来自的高度水平*更低*。因此，我们将*下降*。我们重复这个过程，理想情况下，如果函数的地形配合，这些点的序列将收敛到我们正在寻找的函数的最小化器。
- en: Of course, for functions whose landscapes have many peaks and valleys, where
    we start, or in other words, how to initialize, matters, since we could descend
    down an entirely different valley than the one we want to end up at. We might
    end up at a local minimum instead of a global minimum.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，对于函数的景观具有许多峰和谷的情况，我们从哪里开始，或者换句话说，如何初始化，很重要，因为我们可能下降到与我们想要到达的完全不同的山谷。我们可能最终会陷入局部最小值而不是全局最小值。
- en: 'Functions that are convex and bounded below are shaped liked a salad bowl so
    with those we do not worry about getting stuck at local minima and away from global
    minima. There could be another source of worry with convex functions: When the
    shape of the bowl of the function is too narrow, then our method might become
    painfully slow to converge. We will go over this in detail in the next chapter.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 凸函数和下界有界的函数的形状像沙拉碗，因此我们不必担心被困在局部最小值处，远离全局最小值。凸函数可能存在另一个令人担忧的问题：当函数的碗形太窄时，我们的方法可能变得非常缓慢。我们将在下一章中详细讨论这一点。
- en: Both Approach 1 and Approach 2 are useful and popular. Sometimes, we have no
    option to use but one or the other, depending on how fast each method converges
    for our particular setting, how *regular* the function we are trying to optimize
    is (how many well behaved derivatives it has), *etc*. Other times, it is just
    a matter of taste. For linear regression’s mean squared error loss function, both
    types of methods work, so we will use Approach 1, only because we will use gradient
    descent methods for *all* the other loss functions in this book.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 方法1和方法2都很有用且受欢迎。有时，我们别无选择，只能使用其中一种，这取决于每种方法在特定设置下收敛的速度有多快，我们试图优化的函数有多“规则”（它有多少良好的导数），等等。有时，这只是品味的问题。对于线性回归的均方误差损失函数，两种方法都适用，所以我们将使用方法1，只是因为我们将在本书中对*所有*其他损失函数使用梯度下降方法。
- en: We must mention that the hiking down the mountain analogy for descent methods
    is excellent but a tad bit misleading. When we, humans, hike down a mountain,
    we physically belong in the same three dimensional space that our mountain landscape
    exists in, meaning we are at a certain elevation and we are able to descend to
    a location with lower elevation, even with a blindfold, and even when it is too
    foggy and we can only descend one tiny step at a time. We sense the elevation
    then move downhill. Numerical descent methods, on the other hand, do not search
    for the minimum in the same space dimension as the one the landscape of the function
    is embedded in. Instead, they search on the ground level, one dimension *below*
    the landscape (see [Figure 3-12](#Fig_ground_level_search)). This makes descending
    towards the minimum much more difficult, since at ground level we can move from
    any point to any other without knowing what height level exists above us, until
    we evaluate the function itself at the point and find the height. So our method
    might accidentally move us from one ground point with a certain elevation above
    us to another ground point with a *higher* elevation, hence farther away from
    the minimum. This is why it is important to locate, on ground level, a direction
    that quickly *decreases* the function height, and how far we can move in that
    direction on ground level (step size) while *still decsreasing the function height
    above us*. The step size is also called the *learning rate hyperparameter*, which
    we will encounter everytime we use a descent method.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须提到，对于下降方法的登山下山类比是很好的，但有点误导。当我们人类下山时，我们在物理上属于与我们的山脉景观存在的相同三维空间，这意味着我们处于某个海拔高度，我们能够下降到更低海拔的位置，即使我们被蒙住眼睛，即使天气雾蒙蒙，我们只能一步一步地下降。我们感知海拔然后向下移动。另一方面，数值下降方法并不在与函数景观嵌入的相同空间维度中搜索最小值。相反，它们在地面上搜索，即函数景观的一维*下方*（参见[图3-12](#Fig_ground_level_search)）。这使得朝向最小值的下降变得更加困难，因为在地面上，我们可以从任意一点移动到任何其他点，而不知道我们上方存在什么高度水平，直到我们在该点评估函数本身并找到高度。因此，我们的方法可能会意外地将我们从一个具有某个海拔高度的地面点移动到另一个具有*更高*海拔高度的地面点，因此离最小值更远。这就是为什么在地面上定位一个快速*减小*函数高度的方向以及我们在地面上可以移动多远（步长）而*仍然减小*我们上方函数高度的重要性。步长也称为*学习率超参数*，每当我们使用下降方法时都会遇到。
- en: '![250](assets/emai_0312.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0312.png)'
- en: Figure 3-12\. Search for the minimum happens on ground level and not directly
    on the landscape of the function.
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12。搜索最小值发生在地面上，而不是直接在函数的景观上。
- en: 'Back to our main goal: We want to find the best <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    for our training function, so we must minimize our mean squared error loss function,
    by Approach 1: Take one derivative of the loss function and set it equal to zero,
    then solve for the vector <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> . For this, we need to master
    *doing calculus on linear algebra expressions*. Let’s revisit our calculus class
    first.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的主要目标：我们想要找到最佳的<math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math>，以便为我们的训练函数最小化均方误差损失函数，方法1：对损失函数进行一阶导数，并将其设置为零，然后解出向量<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>。为此，我们需要掌握*对线性代数表达式进行微积分*。让我们首先回顾一下我们的微积分课程。
- en: Calculus in a nutshell
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微积分简介
- en: 'In a first course on Calculus, we learn about functions of single variable
    ( <math alttext="f left-parenthesis omega right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow></math> ), their graphs, and evaluating
    them at certain points. Then we learn about the most important operation in mathematical
    analysis: The limit. From the limit concept we define continuity and discontinuity
    of functions, derivative at a point <math alttext="f prime left-parenthesis omega
    right-parenthesis"><mrow><msup><mi>f</mi> <mo>''</mo></msup> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow></mrow></math> (limit of slopes of secants through
    a point) and integral over a domain (limit of sums of mini regions determined
    by the function over the domain). We end the class with the fundamental theorem
    of calculus, relating integration and differentiation as inverse operations. Of
    the key properties of the derivative is that it determines how fast a function
    increases or descreases at a certain point, hence, it plays a crucial role in
    locating a minimum and/or maximum of a function in the interior of its domain
    (boundary points are separate).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在微积分的第一门课程中，我们学习关于单变量函数（<math alttext="f left-parenthesis omega right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow></math>）及其图形，并在特定点进行评估。然后我们学习数学分析中最重要的操作：极限。从极限概念中，我们定义函数的连续性和不连续性，点的导数<math
    alttext="f prime left-parenthesis omega right-parenthesis"><mrow><msup><mi>f</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow></mrow></math>（通过点的切线斜率的极限）和域上的积分（由函数在域上确定的小区域的和的极限）。我们以微积分基本定理结束课程，将积分和微分作为反向操作进行关联。导数的关键特性之一是它确定函数在某一点的增长或减少速度，因此，在其定义域的内部定位函数的最小值和/或最大值中起着至关重要的作用（边界点是分开的）。
- en: In a multivariable calculus course, which is usually a third course in calculus,
    many ideas transfer from single variable calculus, including the importance of
    the derivative, now called the *gradient* because we have several variables, in
    locating any interior minima and/or maxima. The gradient <math alttext="normal
    nabla left-parenthesis f left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis right-parenthesis"><mrow><mi>∇</mi> <mo>(</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
    of <math alttext="f left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    is the derivative of the function with respect to the vector <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    of variables.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在多变量微积分课程中，通常是微积分的第三门课程，许多概念从单变量微积分中转移，包括导数的重要性，现在称为*梯度*，因为我们有几个变量，用于定位任何内部最小值和/或最大值。函数<math
    alttext="f left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>的梯度<math
    alttext="normal nabla left-parenthesis f left-parenthesis ModifyingAbove omega
    With right-arrow right-parenthesis right-parenthesis"><mrow><mi>∇</mi> <mo>(</mo>
    <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math>是函数对于变量向量<math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>的导数。
- en: In deep learning, the unknown weights are organized in matrices, not in vectors,
    so we need to take the derivative of a function <math alttext="f left-parenthesis
    upper W right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>W</mi> <mo>)</mo></mrow></math>
    with repect to a matrix *W* of variables.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，未知的权重是以矩阵而不是向量的形式组织的，因此我们需要对变量矩阵*W*的函数<math alttext="f left-parenthesis
    upper W right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>W</mi> <mo>)</mo></mrow></math>进行导数。
- en: For our purposes in AI, the function whose derivative we need to calculate is
    the loss function, which has the training function built into it. By the *chain
    rule for derivatives*, we would also need to calculate the derivative of the training
    function with respect to the <math alttext="omega"><mi>ω</mi></math> ’s as well.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在AI中的目的，我们需要计算导数的函数是损失函数，其中包含了训练函数。根据导数的链式法则，我们还需要计算对于<math alttext="omega"><mi>ω</mi></math>的训练函数的导数。
- en: Let’s demonstrate using one simple example from single variable calculus then
    immediately transition to taking derivatives of linear algebra expressions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用单变量微积分的一个简单例子来演示，然后立即过渡到对线性代数表达式进行导数运算。
- en: A one-dimensional optimization example
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一维优化示例
- en: '*Find the minimizer(s) and the minimum value (if any) of the function <math
    alttext="f left-parenthesis omega right-parenthesis equals 3 plus left-parenthesis
    0.5 omega minus 2 right-parenthesis squared"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ω</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> on the interval [-1,6].*'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '*找到函数<math alttext="f left-parenthesis omega right-parenthesis equals 3 plus
    left-parenthesis 0.5 omega minus 2 right-parenthesis squared"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo>
    <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ω</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>在区间[-1,6]上的最小值（如果有的话）和最小值。*'
- en: One impossibly long way to go about this is to try out the *infinitely many*
    values of <math alttext="omega"><mi>ω</mi></math> between -1 and 6 and choose
    the <math alttext="omega"><mi>ω</mi></math> ’s that give the lowest *f* value.
    Another way is to use our calculus knowledge that optimizers (minimizers and/or
    maximizers) happen either at critical points (where the derivative is either nonexistent
    or zero) or at boundary points. For reference, see [Figure 3-13](#Fig_minimize_f).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一个不可能的长方法是尝试在-1和6之间尝试*无限多*个<math alttext="omega"><mi>ω</mi></math>的值，并选择给出最低*f*值的<math
    alttext="omega"><mi>ω</mi></math>。另一种方法是使用我们的微积分知识，即优化器（最小化器和/或最大化器）发生在临界点（导数不存在或为零）或边界点。有关参考，请参阅[图3-13](#Fig_minimize_f)。
- en: '![275](assets/emai_0313.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![275](assets/emai_0313.png)'
- en: Figure 3-13\. The minimum value of the function <math alttext="f left-parenthesis
    omega right-parenthesis equals 3 plus left-parenthesis 0.5 omega minus 2 right-parenthesis
    squared"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ω</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> on the interval [-1,6] is 3 and happens at the
    critical point <math alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math>
    . At this critical point, the derivative of the function is zero, meaning that
    if we draw a tangent line, it will be horizontal.
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13。函数的最小值<math alttext="f left-parenthesis omega right-parenthesis equals
    3 plus left-parenthesis 0.5 omega minus 2 right-parenthesis squared"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo>
    <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ω</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math>在区间[-1,6]上是3，并且发生在临界点<math alttext="omega equals
    4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math>。在这个临界点，函数的导数为零，这意味着如果我们画一条切线，它将是水平的。
- en: 'Our boundary points are *-1* and *6*, so we evaluate our function at these
    points first: <math alttext="f left-parenthesis negative 1 right-parenthesis equals
    3 plus left-parenthesis 0.5 left-parenthesis negative 1 right-parenthesis minus
    2 right-parenthesis squared equals 9.25"><mrow><mi>f</mi> <mrow><mo>(</mo> <mo>-</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>9</mn> <mo>.</mo> <mn>25</mn></mrow></math> and
    <math alttext="f left-parenthesis 6 right-parenthesis equals 3 plus left-parenthesis
    0.5 left-parenthesis 6 right-parenthesis minus 2 right-parenthesis squared equals
    4"><mrow><mi>f</mi> <mrow><mo>(</mo> <mn>6</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn>
    <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mn>6</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>4</mn></mrow></math> . Obviously, *-1* is not
    a minimizer since *f(6)<f(-1)*, so this boundary point gets out of the competition
    and now it is only the boundary point *6* competing with interior critical point(s).
    In order to find our critical points, we inspect the derivative of the function
    in the interior of the interval *[-1,6]*: <math alttext="f prime left-parenthesis
    omega right-parenthesis equals 0 plus 2 left-parenthesis 0.5 omega minus 2 right-parenthesis
    asterisk 0.5 equals 0.25 left-parenthesis 0.5 omega minus 2 right-parenthesis"><mrow><msup><mi>f</mi>
    <mo>''</mo></msup> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>2</mn> <mrow><mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mi>ω</mi>
    <mo>-</mo> <mn>2</mn> <mo>)</mo></mrow> <mo>*</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>25</mn> <mrow><mo>(</mo> <mn>0</mn> <mo>.</mo>
    <mn>5</mn> <mi>ω</mi> <mo>-</mo> <mn>2</mn> <mo>)</mo></mrow></mrow></math> .
    Setting this derivative equal to zero we have <math alttext="0.25 left-parenthesis
    0.5 omega minus 2 right-parenthesis equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>25</mn>
    <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mi>ω</mi> <mo>-</mo> <mn>2</mn> <mo>)</mo>
    <mo>=</mo> <mn>0</mn></mrow></math> implying that <math alttext="omega equals
    4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math> . Thus, we only found
    one critical point <math alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo>
    <mn>4</mn></mrow></math> in the interior of the interval *[-1,6]*. At this special
    point, the value of the function is <math alttext="f left-parenthesis 4 right-parenthesis
    equals 3 plus left-parenthesis 0.5 left-parenthesis 4 right-parenthesis minus
    2 right-parenthesis squared equals 3"><mrow><mi>f</mi> <mrow><mo>(</mo> <mn>4</mn>
    <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mn>4</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>3</mn></mrow></math> . Since the value of *f*
    is the lowest here, we have obviously found the winner of our minimization competition,
    namely, <math alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math>
    with minimum *f* value equal to *3*.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的边界点是*-1*和*6*，所以我们首先在这些点上评估我们的函数：<math alttext="f left-parenthesis negative
    1 right-parenthesis equals 3 plus left-parenthesis 0.5 left-parenthesis negative
    1 right-parenthesis minus 2 right-parenthesis squared equals 9.25"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn>
    <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>9</mn> <mo>.</mo> <mn>25</mn></mrow></math> 和
    <math alttext="f left-parenthesis 6 right-parenthesis equals 3 plus left-parenthesis
    0.5 left-parenthesis 6 right-parenthesis minus 2 right-parenthesis squared equals
    4"><mrow><mi>f</mi> <mrow><mo>(</mo> <mn>6</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn>
    <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mn>6</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>4</mn></mrow></math> 。显然，*-1*不是一个最小化器，因为*f(6)<f(-1)*，所以这个边界点退出了竞争，现在只有边界点*6*与内部临界点竞争。为了找到我们的临界点，我们检查区间*[-1,6]*内函数的导数：<math
    alttext="f prime left-parenthesis omega right-parenthesis equals 0 plus 2 left-parenthesis
    0.5 omega minus 2 right-parenthesis asterisk 0.5 equals 0.25 left-parenthesis
    0.5 omega minus 2 right-parenthesis"><mrow><msup><mi>f</mi> <mo>'</mo></msup>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn> <mo>+</mo>
    <mn>2</mn> <mrow><mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mi>ω</mi> <mo>-</mo>
    <mn>2</mn> <mo>)</mo></mrow> <mo>*</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>=</mo>
    <mn>0</mn> <mo>.</mo> <mn>25</mn> <mrow><mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn>
    <mi>ω</mi> <mo>-</mo> <mn>2</mn> <mo>)</mo></mrow></mrow></math> 。将这个导数设为零，我们有<math
    alttext="0.25 left-parenthesis 0.5 omega minus 2 right-parenthesis equals 0"><mrow><mn>0</mn>
    <mo>.</mo> <mn>25</mn> <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mi>ω</mi>
    <mo>-</mo> <mn>2</mn> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math> 意味着<math
    alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math>
    。因此，我们只在区间*[-1,6]*的内部找到了一个临界点<math alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo>
    <mn>4</mn></mrow></math>。在这个特殊点，函数的值是<math alttext="f left-parenthesis 4 right-parenthesis
    equals 3 plus left-parenthesis 0.5 left-parenthesis 4 right-parenthesis minus
    2 right-parenthesis squared equals 3"><mrow><mi>f</mi> <mrow><mo>(</mo> <mn>4</mn>
    <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mn>4</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>3</mn></mrow></math> 。由于*f*的值在这里是最低的，显然我们已经找到了我们最小化竞赛的赢家，即<math
    alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math>，最小的*f*值等于*3*。
- en: Derivatives of linear algebra expressions that we use all the time
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们经常使用的线性代数表达式的导数
- en: 'It is efficient to calculate derivatives directly on expressions involving
    vectors and matrices, without having to resolve them into their components. The
    following two are popular:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在涉及向量和矩阵的表达式上直接计算导数是高效的，而不必将它们分解成它们的分量。以下两种方法很受欢迎：
- en: When *a* and <math alttext="omega"><mi>ω</mi></math> are scalars and *a* is
    constant, the derivative of <math alttext="f left-parenthesis omega right-parenthesis
    equals a omega"><mrow><mi>f</mi> <mo>(</mo> <mi>ω</mi> <mo>)</mo> <mo>=</mo> <mi>a</mi>
    <mi>ω</mi></mrow></math> is <math alttext="f prime left-parenthesis omega right-parenthesis
    equals a"><mrow><msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>ω</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mi>a</mi></mrow></math> . When <math alttext="ModifyingAbove
    a With right-arrow"><mover accent="true"><mi>a</mi> <mo>→</mo></mover></math>
    and <math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math> are vectors (of the same length) and the entries of
    <math alttext="ModifyingAbove a With right-arrow"><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></math> are constant, then the gradient of <math alttext="f
    left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis equals
    ModifyingAbove a With right-arrow Superscript t Baseline ModifyingAbove omega
    With right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <msup><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math>
    is <math alttext="normal nabla f left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis equals ModifyingAbove a With right-arrow"><mrow><mi>∇</mi> <mi>f</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> .
    Similarly, the gradient of <math alttext="f left-parenthesis ModifyingAbove omega
    With right-arrow right-parenthesis equals ModifyingAbove w With right-arrow Superscript
    t Baseline ModifyingAbove a With right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mover accent="true"><mi>w</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> is <math alttext="normal
    nabla f left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    equals ModifyingAbove a With right-arrow"><mrow><mi>∇</mi> <mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> .
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当*a*和<math alttext="omega"><mi>ω</mi></math>是标量且*a*是常数时，<math alttext="f left-parenthesis
    omega right-parenthesis equals a omega"><mrow><mi>f</mi> <mo>(</mo> <mi>ω</mi>
    <mo>)</mo> <mo>=</mo> <mi>a</mi> <mi>ω</mi></mrow></math>的导数是<math alttext="f
    prime left-parenthesis omega right-parenthesis equals a"><mrow><msup><mi>f</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>a</mi></mrow></math>。当<math
    alttext="ModifyingAbove a With right-arrow"><mover accent="true"><mi>a</mi> <mo>→</mo></mover></math>和<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>是向量（长度相同）且<math alttext="ModifyingAbove a With right-arrow"><mover
    accent="true"><mi>a</mi> <mo>→</mo></mover></math>的条目是常数时，<math alttext="f left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis equals ModifyingAbove
    a With right-arrow Superscript t Baseline ModifyingAbove omega With right-arrow"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math>的梯度是<math alttext="normal
    nabla f left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    equals ModifyingAbove a With right-arrow"><mrow><mi>∇</mi> <mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math>。同样，<math alttext="f
    left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis equals
    ModifyingAbove w With right-arrow Superscript t Baseline ModifyingAbove a With
    right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <msup><mover accent="true"><mi>w</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math>的梯度是<math
    alttext="normal nabla f left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis equals ModifyingAbove a With right-arrow"><mrow><mi>∇</mi> <mi>f</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math>。
- en: When *s* is scalar and constant and <math alttext="omega"><mi>ω</mi></math>
    is scalar, then the derivative of the quadratic function <math alttext="f left-parenthesis
    omega right-parenthesis equals s omega squared"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>s</mi> <msup><mi>ω</mi> <mn>2</mn></msup></mrow></math>
    is <math alttext="f prime left-parenthesis omega right-parenthesis equals 2 s
    omega"><mrow><msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>2</mn> <mi>s</mi> <mi>ω</mi></mrow></math> . The analogous high
    dimensional case is when *S* is a symmetric matrix with constant entries, then
    the function <math alttext="f left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis equals ModifyingAbove omega With right-arrow Superscript t Baseline
    upper S ModifyingAbove omega With right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mi>S</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math> is quadratic
    and its gradient is <math alttext="normal nabla f left-parenthesis ModifyingAbove
    omega With right-arrow right-parenthesis equals 2 upper S ModifyingAbove omega
    With right-arrow"><mrow><mi>∇</mi> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mn>2</mn> <mi>S</mi> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></mrow></math> .
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当*s*是标量且常数，<math alttext="omega"><mi>ω</mi></math>是标量时，二次函数<math alttext="f
    left-parenthesis omega right-parenthesis equals s omega squared"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>s</mi> <msup><mi>ω</mi>
    <mn>2</mn></msup></mrow></math>的导数是<math alttext="f prime left-parenthesis omega
    right-parenthesis equals 2 s omega"><mrow><msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>2</mn> <mi>s</mi> <mi>ω</mi></mrow></math>。类似的高维情况是当*S*是具有常数条目的对称矩阵时，函数<math
    alttext="f left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    equals ModifyingAbove omega With right-arrow Superscript t Baseline upper S ModifyingAbove
    omega With right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mi>S</mi> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></mrow></math>是二次的，其梯度是<math alttext="normal nabla f left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis equals 2 upper S ModifyingAbove
    omega With right-arrow"><mrow><mi>∇</mi> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mn>2</mn> <mi>S</mi> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></mrow></math>。
- en: Minimizing the mean squared error loss function
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小化均方误差损失函数
- en: 'We are finally ready to minimize the mean squared error loss function:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好最小化均方误差损失函数了。
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals StartFraction 1 Over m EndFraction left-parenthesis
    upper X ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline right-parenthesis Superscript t Baseline left-parenthesis
    upper X ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline right-parenthesis period dollar-sign"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced close=")" open="("
    separators=""><mi>X</mi><mover accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>.</mo></mrow></math>
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals StartFraction 1 Over m EndFraction left-parenthesis
    upper X ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline right-parenthesis Superscript t Baseline left-parenthesis
    upper X ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline right-parenthesis period dollar-sign"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced close=")" open="("
    separators=""><mi>X</mi><mover accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>.</mo></mrow></math>
- en: 'Let’s open the above expression up before taking its gradient and setting it
    equal to zero:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在将其梯度展开并将其设置为零之前打开上述表达式：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis 2nd Column equals StartFraction
    1 Over m EndFraction left-parenthesis left-parenthesis upper X ModifyingAbove
    omega With right-arrow right-parenthesis Superscript t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Superscript t Baseline right-parenthesis
    left-parenthesis upper X ModifyingAbove omega With right-arrow minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis 2nd Row 1st Column
    Blank 2nd Column equals StartFraction 1 Over m EndFraction left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline upper X Superscript t Baseline minus
    ModifyingAbove y With right-arrow Subscript t r u e Superscript t Baseline right-parenthesis
    left-parenthesis upper X ModifyingAbove omega With right-arrow minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis 3rd Row 1st Column
    Blank 2nd Column equals StartFraction 1 Over m EndFraction left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline upper X Superscript t Baseline upper
    X ModifyingAbove omega With right-arrow minus ModifyingAbove omega With right-arrow
    Superscript t Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow
    Subscript t r u e Baseline minus ModifyingAbove y With right-arrow Subscript t
    r u e Superscript t Baseline upper X ModifyingAbove omega With right-arrow plus
    ModifyingAbove y With right-arrow Subscript t r u e Superscript t Baseline ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis 4th Row 1st Column
    Blank 2nd Column equals StartFraction 1 Over m EndFraction left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline upper S ModifyingAbove omega With
    right-arrow minus ModifyingAbove omega With right-arrow Superscript t Baseline
    ModifyingAbove a With right-arrow minus ModifyingAbove a With right-arrow Superscript
    t Baseline ModifyingAbove omega With right-arrow plus ModifyingAbove y With right-arrow
    Subscript t r u e Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline right-parenthesis comma EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><msup><mrow><mo>(</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>)</mo></mrow> <mi>t</mi></msup>
    <mo>-</mo> <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup></mfenced> <mfenced close=")" open="(" separators=""><mi>X</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <mfenced close=")" open="(" separators=""><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <msup><mi>X</mi> <mi>t</mi></msup> <mo>-</mo>
    <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup></mfenced> <mfenced close=")" open="(" separators=""><mi>X</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <mfenced close=")" open="(" separators=""><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>-</mo> <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>+</mo> <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <mfenced close=")" open="(" separators=""><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mi>S</mi> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>-</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>-</mo>
    <msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>+</mo> <msubsup><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>t</mi></msubsup>
    <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>,</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 数学符号开始布局第一行第一列上方L左括号修改上方omega箭头右括号第二列等于分数1/m左括号左括号上方X修改上方omega箭头右括号上标t减去上方y箭头下标true上标t右括号左括号上方X修改上方omega箭头减去上方y箭头下标true右括号第二行第一列空第二列等于分数1/m左括号上方omega箭头上标t上方X上标t减去上方y箭头下标true上标t右括号左括号上方X修改上方omega箭头减去上方y箭头下标true右括号第三行第一列空第二列等于分数1/m左括号上方omega箭头上标t上方X上标t上方X修改上方omega箭头减去上方omega箭头上标t上方X上标t上方y箭头下标true减去上方y箭头下标true上标t上方X修改上方omega箭头加上上方y箭头下标true上标t上方y箭头下标true右括号第四行第一列空第二列等于分数1/m左括号上方omega箭头上标S上方omega箭头减去上方omega箭头上标t上方a箭头减去上方a箭头上标t上方omega箭头加上上方y箭头下标true上标t上方y箭头下标true右括号，结束布局。
- en: 'where in the last step we set <math alttext="upper X Superscript t Baseline
    upper X equals upper S"><mrow><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi> <mo>=</mo>
    <mi>S</mi></mrow></math> and <math alttext="upper X Superscript t Baseline ModifyingAbove
    y With right-arrow Subscript t r u e Baseline equals ModifyingAbove a With right-arrow"><mrow><msup><mi>X</mi>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> .
    Next, take the gradient of the last expression with respect to <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    and set it equal to zero. When calculating the gradient we use what we just learned
    about differentiating linear algebra expressions in the above subsection:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，我们设置了<math alttext="upper X Superscript t Baseline upper X equals upper
    S"><mrow><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi> <mo>=</mo> <mi>S</mi></mrow></math>和<math
    alttext="upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline equals ModifyingAbove a With right-arrow"><mrow><msup><mi>X</mi>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math>。接下来，对上述表达式关于<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>的梯度进行计算，并将其设置为零。在计算梯度时，我们使用了上面小节中学到的关于线性代数表达式的微分知识：
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column normal nabla upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis 2nd
    Column equals StartFraction 1 Over m EndFraction left-parenthesis 2 upper S ModifyingAbove
    omega With right-arrow minus ModifyingAbove a With right-arrow minus ModifyingAbove
    a With right-arrow plus 0 right-parenthesis 2nd Row 1st Column Blank 2nd Column
    equals ModifyingAbove 0 With right-arrow EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>∇</mi> <mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mn>2</mn>
    <mi>S</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>-</mo> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>+</mo> <mn>0</mn></mfenced></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mn>0</mn> <mo>→</mo></mover></mrow></mtd></mtr></mtable></math>
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartLayout 1st Row 1st Column normal nabla upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis 2nd
    Column equals StartFraction 1 Over m EndFraction left-parenthesis 2 upper S ModifyingAbove
    omega With right-arrow minus ModifyingAbove a With right-arrow minus ModifyingAbove
    a With right-arrow plus 0 right-parenthesis 2nd Row 1st Column Blank 2nd Column
    equals ModifyingAbove 0 With right-arrow EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>∇</mi> <mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mn>2</mn>
    <mi>S</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>-</mo> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>+</mo> <mn>0</mn></mfenced></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mn>0</mn> <mo>→</mo></mover></mrow></mtd></mtr></mtable></math>
- en: 'Now it is easy to solve for <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> :'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很容易解出<math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>：
- en: <math alttext="dollar-sign StartFraction 1 Over m EndFraction left-parenthesis
    2 upper S ModifyingAbove omega With right-arrow minus 2 ModifyingAbove a With
    right-arrow right-parenthesis equals ModifyingAbove 0 With right-arrow dollar-sign"><mrow><mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mn>2</mn> <mi>S</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <mn>2</mn> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover></mfenced> <mo>=</mo> <mover accent="true"><mn>0</mn>
    <mo>→</mo></mover></mrow></math>
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign StartFraction 1 Over m EndFraction left-parenthesis
    2 upper S ModifyingAbove omega With right-arrow minus 2 ModifyingAbove a With
    right-arrow right-parenthesis equals ModifyingAbove 0 With right-arrow dollar-sign"><mrow><mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mn>2</mn> <mi>S</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <mn>2</mn> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover></mfenced> <mo>=</mo> <mover accent="true"><mn>0</mn>
    <mo>→</mo></mover></mrow></math>
- en: so
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 所以
- en: <math alttext="dollar-sign 2 upper S ModifyingAbove omega With right-arrow equals
    2 ModifyingAbove a With right-arrow dollar-sign"><mrow><mn>2</mn> <mi>S</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <mn>2</mn> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></mrow></math>
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign 2 upper S ModifyingAbove omega With right-arrow equals
    2 ModifyingAbove a With right-arrow dollar-sign"><mrow><mn>2</mn> <mi>S</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <mn>2</mn> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></mrow></math>
- en: which gives
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals upper
    S Superscript negative 1 Baseline ModifyingAbove a With right-arrow dollar-sign"><mrow><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mi>S</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math>
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals upper
    S Superscript negative 1 Baseline ModifyingAbove a With right-arrow dollar-sign"><mrow><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mi>S</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math>
- en: 'Now recall that we set <math alttext="upper S equals upper X Superscript t
    Baseline upper X"><mrow><mi>S</mi> <mo>=</mo> <msup><mi>X</mi> <mi>t</mi></msup>
    <mi>X</mi></mrow></math> and <math alttext="ModifyingAbove a With right-arrow
    equals upper X Superscript t Baseline y Subscript t r u e"><mrow><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math> , so
    let’s rewrite our minimizing <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> in terms of the training set
    *X* (augmented with ones) and the corresponding labels vector <math alttext="ModifyingAbove
    y With right-arrow Subscript t r u e"><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> :'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis Superscript negative
    1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline period dollar-sign"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>.</mo></mrow></math>
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Fish Market data set, this ends up being (see the accompanying Jupyter
    notebook):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals Start
    6 By 1 Matrix 1st Row  omega 0 2nd Row  omega 1 3rd Row  omega 2 4th Row  omega
    3 5th Row  omega 4 6th Row  omega 5 EndMatrix equals Start 6 By 1 Matrix 1st Row  negative
    475.19929130109716 2nd Row  82.84970118 3rd Row  negative 28.85952426 4th Row  negative
    28.50769512 5th Row  29.82981435 6th Row  30.97250278 EndMatrix dollar-sign"><mrow><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>ω</mi>
    <mn>0</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi>
    <mn>3</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi> <mn>4</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>5</mn></msub></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mo>-</mo> <mn>475</mn> <mo>.</mo>
    <mn>19929130109716</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>82</mn> <mo>.</mo>
    <mn>84970118</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>28</mn> <mo>.</mo>
    <mn>85952426</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>28</mn> <mo>.</mo>
    <mn>50769512</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>29</mn> <mo>.</mo> <mn>82981435</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>30</mn> <mo>.</mo> <mn>97250278</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Caution: Multiplying large matrices by each other is very expensive. Multiply
    matrices by vectors instead.'
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try to avoid multiplying matrices by each other at all costs, instead, multiply
    your matrices with *vectors*. For example, in the normal equation <math alttext="ModifyingAbove
    omega With right-arrow equals left-parenthesis upper X Superscript t Baseline
    upper X right-parenthesis Superscript negative 1 Baseline upper X Superscript
    t Baseline ModifyingAbove y With right-arrow Subscript t r u e"><mrow><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup>
    <mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi>X</mi>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>
    , compute <math alttext="upper X Superscript t Baseline ModifyingAbove y With
    right-arrow Subscript t r u e"><mrow><msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>
    first, and avoid computing <math alttext="left-parenthesis upper X Superscript
    t Baseline upper X right-parenthesis Superscript negative 1"><msup><mrow><mo>(</mo><msup><mi>X</mi>
    <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    altogether. The way around this is to solve instead the linear system <math alttext="upper
    X ModifyingAbove omega With right-arrow equals ModifyingAbove y With right-arrow
    Subscript t r u e"><mrow><mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>
    using the *pseudo inverse* of *X* (check the accompanying Jupyter notebook). We
    will discuss the the *pseudo inverse* in [Chapter 11](ch11.xhtml#ch11), but for
    now, it allows us to invert (which is equivalent to divide by) matrices that do
    not have an inverse.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽量避免相乘矩阵，而是用*向量*相乘。例如，在正规方程<math alttext="ModifyingAbove omega With right-arrow
    equals left-parenthesis upper X Superscript t Baseline upper X right-parenthesis
    Superscript negative 1 Baseline upper X Superscript t Baseline ModifyingAbove
    y With right-arrow Subscript t r u e"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>，首先计算<math
    alttext="upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e"><mrow><msup><mi>X</mi> <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>，避免计算<math
    alttext="left-parenthesis upper X Superscript t Baseline upper X right-parenthesis
    Superscript negative 1"><msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup>
    <mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>。解决这个问题的方法是使用*X*的*伪逆*来解决线性系统<math
    alttext="upper X ModifyingAbove omega With right-arrow equals ModifyingAbove y
    With right-arrow Subscript t r u e"><mrow><mi>X</mi> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>=</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>（查看附带的Jupyter笔记本）。我们将在[第11章](ch11.xhtml#ch11)中讨论*伪逆*，但现在，它允许我们求解（相当于除以）没有逆的矩阵。
- en: 'We just located the weights vector <math alttext="ModifyingAbove omega With
    right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that gives
    the best fit between our training data and the linear regression training function:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是找到了权重向量<math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>，它能够在我们的训练数据和线性回归训练函数之间提供最佳拟合：
- en: <math alttext="dollar-sign f left-parenthesis ModifyingAbove omega With right-arrow
    semicolon ModifyingAbove x With right-arrow right-parenthesis equals omega 0 plus
    omega 1 x 1 plus omega 2 x 2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x
    5 period dollar-sign"><mrow><mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>4</mn></msub> <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>5</mn></msub> <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign f left-parenthesis ModifyingAbove omega With right-arrow
    semicolon ModifyingAbove x With right-arrow right-parenthesis equals omega 0 plus
    omega 1 x 1 plus omega 2 x 2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x
    5 period dollar-sign"><mrow><mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>4</mn></msub> <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>5</mn></msub> <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
- en: We used an analytical method (compute the gradient of the loss function and
    set it equal to zero) to derive the solution given by the normal equation. This
    is one of the very rare instances where we are able to derive an analytical solution.
    All other methods for finding the minimizing <math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> will
    be numerical.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一种分析方法（计算损失函数的梯度并将其置为零）来推导正规方程给出的解决方案。这是我们能够推导出解析解的非常罕见的情况之一。所有其他找到最小化<math
    alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math>的方法都是数值方法。
- en: 'Caution: We never want to fit the training data too well'
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意：我们永远不希望训练数据拟合得太好。
- en: The <math alttext="ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis Superscript negative
    1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi>
    <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math> that
    we calculated gives the <math alttext="omega"><mi>ω</mi></math> values that make
    the training function *best fit* the training data, but too good of a fit means
    that the training function might also be picking up on the noise and not only
    on the signal in the data. So the above solution, or even the minimization problem
    itself, needs to be modified in order to *not get too good of a fit*. *Regularization*
    or *early stopping* are helpful here. We will spend some time on those in the
    next chapter.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'This was the long way to regression. We had to pass through calculus and linear
    algebra on the way, because we are just starting. Presenting the upcoming machine
    learning models: Logistic regression, support vector machines, decision trees,
    and random forests, will be faster, since all we do is apply the exact same ideas
    to different functions.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic Regression: Classify Into Two Classes'
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic Regression is mainly used for classification tasks. We first explain
    how we can use this model for binary classification tasks (classify into two classes,
    such as cancer/not cancer; safe for children/not safe; likely to pay back a loan/unlikely;
    *etc.*). Then we will generalize the model into classifying into multiple classes
    (for example, classify handwritten images of digits into 0, 1, 2, 3, 4, 5, 6,
    7, 8 or 9). Again, we have the same mathematical set up:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Training function
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss Function
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training function
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to linear regression, the training function for logistic regression
    computes a linear combination of the features and adds a constant bias term, but
    instead of outputting the result as it is, it passes it through the *logistic
    function*, whose graph is plotted in [Figure 3-14](#Fig_logistic), and whose formula
    is:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign sigma left-parenthesis s right-parenthesis equals
    StartFraction 1 Over 1 plus e Superscript negative s Baseline EndFraction dollar-sign"><mrow><mi>σ</mi>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac></mrow></math>![275](assets/emai_0314.png)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3-14\. Graph of the logistic function <math alttext="sigma left-parenthesis
    s right-parenthesis equals StartFraction 1 Over 1 plus e Superscript negative
    s Baseline EndFraction"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac></mrow></math>
    . Note that this function can be evaluated at any *s* and always outputs a number
    between *0* and *1*, hence, its output can be interpreted as a probability.
  id: totrans-299
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is a function that only takes values between *0* and *1*, so its output
    can be interpreted as a probability of a data point belonging to a certain class:
    If the output is less than *0.5* then classify the data point as belonging to
    the first class, and if the output is greater than *0.5* then classify the data
    point in the other class. The number *0.5* is the *threshold* where the decision
    to classify the data point is made.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the training function here ends up being a linear combination of
    features, plus bias, composed first with the logistic function, then finally composed
    with a thresholding function:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals upper T h r e s h left-parenthesis sigma
    left-parenthesis omega 0 plus omega 1 x 1 plus ellipsis plus omega Subscript n
    Baseline x Subscript n Baseline right-parenthesis right-parenthesis dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <mi>T</mi> <mi>h</mi> <mi>r</mi> <mi>e</mi> <mi>s</mi> <mi>h</mi> <mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow></math>
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign y equals upper T h r e s h left-parenthesis sigma
    left-parenthesis omega 0 plus omega 1 x 1 plus ellipsis plus omega Subscript n
    Baseline x Subscript n Baseline right-parenthesis right-parenthesis dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <mi>T</mi> <mi>h</mi> <mi>r</mi> <mi>e</mi> <mi>s</mi> <mi>h</mi> <mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow></math>
- en: Similar to the linear regression case, the <math alttext="omega"><mi>ω</mi></math>
    ’s are the unknowns that we need to optimize our loss function for. Just like
    linear regression, the number of these unknowns is equal to the number of data
    features, plus one for the bias term. For tasks like classifying images, each
    pixel is a feature, so we could have thousands of those.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归情况类似，<math alttext="omega"><mi>ω</mi></math> 是我们需要优化损失函数的未知数。就像线性回归一样，这些未知数的数量等于数据特征的数量，再加上一个偏差项。对于像分类图像这样的任务，每个像素都是一个特征，所以我们可能有成千上万个。
- en: Loss Function
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: Let’s design a good *loss function* for classification. We are the engineers
    and we want to penalize wrongly classified training data points. In our labeled
    data set, if an instance belongs in a class then its <math alttext="y Subscript
    t r u e Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> , and if it doesn’t then its <math alttext="y
    Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> .
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为分类设计一个良好的*损失函数*。我们是工程师，我们希望惩罚错误分类的训练数据点。在我们的标记数据集中，如果一个实例属于一个类，那么它的<math
    alttext="y Subscript t r u e Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math>，如果不属于，则<math alttext="y Subscript t r u e
    Baseline equals 0"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math>。
- en: We want our training function to output <math alttext="y Subscript p r e d i
    c t Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> for training instances that belong in the
    positive class (whose <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    is also 1). Successful <math alttext="omega"><mi>ω</mi></math> values should give
    a high value of *t* (result of the linear combination step) to go into the logistic
    function, hence assigning high probability for positive instances and passing
    the 0.5 threshold to obtain <math alttext="y Subscript p r e d i c t Baseline
    equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, if the linear combination plus
    bias step gives a low *t* value while <math alttext="y Subscript t r u e Baseline
    equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> , penalize it.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的训练函数输出<math alttext="y Subscript p r e d i c t Baseline equals 1"><mrow><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math>，对于属于正类的训练实例（其<math alttext="y Subscript t
    r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>也为1）。成功的<math
    alttext="omega"><mi>ω</mi></math>值应该给出一个较高的*t*值（线性组合步骤的结果），以进入逻辑函数，从而为正实例分配高概率，并通过0.5阈值获得<math
    alttext="y Subscript p r e d i c t Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math>。因此，如果线性组合加偏差步骤给出一个较低的*t*值，而<math alttext="y
    Subscript t r u e Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math>，则对其进行惩罚。
- en: Similarly, successful weight values should give a low *t* value to go into the
    logistic function for training instances that do not belong in the class (their
    true <math alttext="y Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    ). Therefore if the linear combination plus bias step gives a high *t* value while
    <math alttext="y Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> , penalize it.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，成功的权重值应该给出一个较低的*t*值，以进入逻辑函数，用于不属于该类的训练实例（它们真实的<math alttext="y Subscript
    t r u e Baseline equals 0"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math>）。因此，如果线性组合加偏差步骤给出一个较高的*t*值，而<math alttext="y
    Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math>，则对其进行惩罚。
- en: 'So how do we find a loss function that penalizes a wrongly classified training
    data point? Both false positives and false negatives should be penalized. Recall
    that the outputs of this classification model are either *1* or *0*:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何找到一个惩罚错误分类的训练数据点的损失函数呢？假阳性和假阴性都应该受到惩罚。回想一下，这个分类模型的输出要么是*1*，要么是*0*：
- en: 'Think of a calculus function that rewards *1* and penalizes *0*: <math alttext="minus
    log left-parenthesis s right-parenthesis"><mrow><mo>-</mo> <mo form="prefix">log</mo>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> (see [Figure 3-15](#Fig_log_s_log_1_s)).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一下奖励*1*并惩罚*0*的微积分函数：<math alttext="minus log left-parenthesis s right-parenthesis"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math>（见[图3-15](#Fig_log_s_log_1_s)）。
- en: 'Think of a calculus function that penalizes *1* and rewards *0*: <math alttext="minus
    log left-parenthesis 1 minus s right-parenthesis"><mrow><mo>-</mo> <mo form="prefix">log</mo>
    <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>s</mi> <mo>)</mo></mrow></math> (see [Figure 3-15](#Fig_log_s_log_1_s)).'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![275](assets/emai_0315.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-15\. Left: Graph of the function <math alttext="f left-parenthesis
    s right-parenthesis equals minus l o g left-parenthesis s right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo> <mo>=</mo> <mo>-</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> . This function assigns high values
    for numbers near *0* and low values for numbers near *1*. Right: Graph of the
    function <math alttext="f left-parenthesis s right-parenthesis equals minus l
    o g left-parenthesis 1 minus s right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>s</mi> <mo>)</mo> <mo>=</mo> <mo>-</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>s</mi> <mo>)</mo></mrow></math> . This function assigns
    high values for numbers near *1* and low values for numbers near *0*.'
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now focus on the output of the logistic function <math alttext="sigma left-parenthesis
    s right-parenthesis"><mrow><mi>σ</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math>
    for the current choice of <math alttext="omega"><mi>ω</mi></math> ’s:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: If <math alttext="sigma left-parenthesis s right-parenthesis"><mrow><mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> is less than *0.5* (model prediction
    is <math alttext="y Subscript p r e d i c t Baseline equals 0"><mrow><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> ) but the true <math alttext="y Subscript
    t r u e Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> (a false negative) make the model pay by penalizing
    <math alttext="minus log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mo>(</mo> <mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo> <mo>)</mo></mrow></math> . If instead <math alttext="sigma
    left-parenthesis s right-parenthesis greater-than 0.5"><mrow><mi>σ</mi> <mo>(</mo>
    <mi>s</mi> <mo>)</mo> <mo>></mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>
    , so the model prediction is <math alttext="y Subscript p r e d i c t Baseline
    equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> ) (a true positive), <math alttext="minus
    log left-parenthesis sigma left-parenthesis s right-parenthesis right-parenthesis"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mo>(</mo> <mi>σ</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo>
    <mo>)</mo></mrow></math> is small so no high penalty is paid.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, if <math alttext="sigma left-parenthesis s right-parenthesis"><mrow><mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> is more than *0.5* but the the
    true <math alttext="y Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    (a false positive), make the model pay by penalizing <math alttext="minus log
    left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis right-parenthesis"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mo>(</mo>
    <mi>s</mi> <mo>)</mo> <mo>)</mo></mrow></math> . Again, for a true negative no
    high penalty is paid either.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, we can write the cost for misclassifying one training instance <math
    alttext="left-parenthesis x 1 Superscript i Baseline comma x 2 Superscript i Baseline
    comma ellipsis comma x Subscript n Superscript i Baseline semicolon y Subscript
    t r u e Baseline right-parenthesis"><mrow><mo>(</mo> <msubsup><mi>x</mi> <mn>1</mn>
    <mi>i</mi></msubsup> <mo>,</mo> <msubsup><mi>x</mi> <mn>2</mn> <mi>i</mi></msubsup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup>
    <mo>;</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow></math> as:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign c o s t equals Start 2 By 2 Matrix 1st Row 1st Column
    Blank 2nd Column minus log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 1 2nd Row 1st Column
    Blank 2nd Column minus log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 0 EndMatrix equals minus
    y Subscript t r u e Baseline log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis minus left-parenthesis 1 minus y Subscript t r u e Baseline
    right-parenthesis log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis dollar-sign"><mrow><mi>c</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi>
    <mo>=</mo> <mfenced close="}" open="{" separators=""><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the loss function is the average cost over *m* training instances,
    giving us the formula for the popular *cross entropy loss function*:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis equals 2nd Column minus
    StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript
    m Endscripts y Subscript t r u e Superscript i Baseline log left-parenthesis sigma
    left-parenthesis omega 0 plus omega 1 x 1 Superscript i Baseline plus ellipsis
    plus omega Subscript n Baseline x Subscript n Superscript i Baseline right-parenthesis
    right-parenthesis plus 2nd Row 1st Column Blank 2nd Column left-parenthesis 1
    minus y Subscript t r u e Superscript i Baseline right-parenthesis log left-parenthesis
    1 minus sigma left-parenthesis omega 0 plus omega 1 x 1 Superscript i Baseline
    plus ellipsis plus omega Subscript n Baseline x Subscript n Superscript i Baseline
    right-parenthesis right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>ω</mi> <mi>n</mi></msub> <msubsup><mi>x</mi>
    <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the linear regression case, if we decide to minimize the loss function
    by setting <math alttext="normal nabla upper L left-parenthesis omega right-parenthesis
    equals 0"><mrow><mi>∇</mi> <mi>L</mi> <mo>(</mo> <mi>ω</mi> <mo>)</mo> <mo>=</mo>
    <mn>0</mn></mrow></math> there is no closed form solution formula for the <math
    alttext="omega"><mi>ω</mi></math> ’s. The good news is that this function is convex
    so next chapter’s gradient descent (or stochastic or mini-batch gradient descents)
    is guaranteed to find a minimum (if the *learning rate* is not too large and if
    we wait long enough).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Softmax Regression: Classify Into Multiple Classes'
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can easily generalize the logistic regression idea to classify into multiple
    classes. A famous example for such non-binary classification task is classifying
    images of the ten handwritten numerals 0, 1, 2, 3, 4, 5, 6, 7, 8 and 9 using the
    [MNIST data set](http://yann.lecun.com/exdb/mnist/). This data set contains 70,000
    images of handwritten numerals (see [Figure 3-16](#Fig_MNIST) for samples of these
    images), split into 60,000 training subset and 10,000 test subset. Each image
    is labeled with the class it belongs to, which is one of the ten numerals.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![275](assets/emai_0316.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-16\. Sample images from the MNIST data set. ([Image source: Wikipedia](https://en.wikipedia.org/wiki/MNIST_database)).'
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The [link for this data set](http://yann.lecun.com/exdb/mnist/) also contains
    results from many classifying models, including linear classifiers, *k-nearest
    neighbors*, *decision trees*, *support vector machines* with various *kernels*,
    and neural networks with various architectures, along with references for the
    corresponding papers and their years of publication. It is interesting to see
    the progress in performance as the years go by and as the methods evolve.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'Careful: Do not confuse classifying into multiple classes with multi-output
    models'
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Softmax regression predicts one class at a time, so we cannot use it to classify,
    for example, five people in the same image. Instead, we can use it to check whether
    a given Facebook image is a picture of me, my sister, my brother, my husband,
    or my daughter. An image passed into the softmax regression model can have only
    one of the five of us or the model’s classification would be less obvious. This
    means that our classes have to be mutually exclusive. So when Facebook automatically
    tags five people in the same image, they are using a muti-output model, not a
    softmax regression model.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have the features of a data point and we want to use this information
    in order to classify the data point into one of *k* possible classes. The following
    training function, loss function, and optimization process should be clear by
    now.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Note About Features of Image Data
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For greyscale images, each pixel intensity is a feature, so images usually have
    thousands of features. Greyscale images are usually represented as two dimensional
    matrices of numbers, with pixel intensities as the matrix entries. Color images
    come in three channels, red, green, and blue, where each channel is again represented
    as a two dimensional matrix of numbers, and the channels are stacked on top on
    each other, forming three layers of two dimensional matrices. This structure is
    called a tensor. Check out this [linked Jupyter Notebook] that illustrates how
    we can work with greyscale and color images in Python.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Training function
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is always the same: Linearly combine the features and add a
    constant bias term. In logistic regression, when we only had two classes, we passed
    the result into the logistic function, of formula:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign sigma left-parenthesis s right-parenthesis equals
    StartFraction 1 Over 1 plus e Superscript negative s Baseline EndFraction equals
    StartStartFraction 1 OverOver 1 plus StartFraction 1 Over e Superscript s Baseline
    EndFraction EndEndFraction equals StartFraction e Superscript s Baseline Over
    1 plus e Superscript s Baseline EndFraction equals StartFraction e Superscript
    s Baseline Over e Superscript 0 Baseline plus e Superscript s Baseline EndFraction
    comma dollar-sign"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn> <msup><mi>e</mi>
    <mi>s</mi></msup></mfrac></mrow></mfrac> <mo>=</mo> <mfrac><msup><mi>e</mi> <mi>s</mi></msup>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mi>s</mi></msup></mrow></mfrac> <mo>=</mo>
    <mfrac><msup><mi>e</mi> <mi>s</mi></msup> <mrow><msup><mi>e</mi> <mn>0</mn></msup>
    <mo>+</mo><msup><mi>e</mi> <mi>s</mi></msup></mrow></mfrac> <mo>,</mo></mrow></math>
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: which we interpreted as the probability of the data point belonging in the class
    of interest or not. Note that we rewrote the formula for the logistic function
    as <math alttext="sigma left-parenthesis s right-parenthesis equals StartFraction
    e Superscript s Baseline Over e Superscript 0 Baseline plus e Superscript s Baseline
    EndFraction"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><msup><mi>e</mi> <mi>s</mi></msup> <mrow><msup><mi>e</mi> <mn>0</mn></msup>
    <mo>+</mo><msup><mi>e</mi> <mi>s</mi></msup></mrow></mfrac></mrow></math> in order
    to highlight the fact that it captures two probabilities, one for each class.
    In other words, <math alttext="sigma left-parenthesis s right-parenthesis"><mrow><mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> gives the probability that a data
    point is in the class of interest, and <math alttext="1 minus sigma left-parenthesis
    s right-parenthesis equals StartFraction e Superscript 0 Baseline Over e Superscript
    0 Baseline plus e Superscript s Baseline EndFraction"><mrow><mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mn>0</mn></msup> <mrow><msup><mi>e</mi> <mn>0</mn></msup> <mo>+</mo><msup><mi>e</mi>
    <mi>s</mi></msup></mrow></mfrac></mrow></math> gives the probability that the
    data point is not in the class.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have multiple classes instead of only two, then for the same data point,
    we repeat same process multiple times: One time for each class. Each class has
    its own bias and set of weights that linearly combine the features, thus, given
    a data point with feature values <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>
    , <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math> ,…, and <math
    alttext="x Subscript n"><msub><mi>x</mi> <mi>n</mi></msub></math> , we compute
    *k* different linear combinations plus biases:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column s Superscript 1 2nd
    Column equals omega 0 Superscript 1 Baseline plus omega 1 Superscript 1 Baseline
    x 1 plus omega 2 Superscript 1 Baseline x 2 plus ellipsis plus omega Subscript
    n Superscript 1 Baseline x Subscript n Baseline 2nd Row 1st Column s squared 2nd
    Column equals omega 0 squared plus omega 1 squared x 1 plus omega 2 squared x
    2 plus ellipsis plus omega Subscript n Superscript 2 Baseline x Subscript n Baseline
    3rd Row 1st Column Blank 4th Row 1st Column  ellipsis 5th Row 1st Column s Superscript
    k 2nd Column equals omega 0 Superscript k Baseline plus omega 1 Superscript k
    Baseline x 1 plus omega 2 Superscript k Baseline x 2 plus ellipsis plus omega
    Subscript n Superscript k Baseline x Subscript n Baseline period EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><msup><mi>s</mi> <mn>1</mn></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mn>0</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msubsup><mi>ω</mi> <mn>1</mn> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mn>2</mn> <mn>1</mn></msubsup>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi>
    <mi>n</mi> <mn>1</mn></msubsup> <msub><mi>x</mi> <mi>n</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>s</mi> <mn>2</mn></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>2</mn> <mn>2</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mn>2</mn></msubsup>
    <msub><mi>x</mi> <mi>n</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>s</mi> <mi>k</mi></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mi>k</mi></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mi>k</mi></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>2</mn> <mi>k</mi></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mi>k</mi></msubsup>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Get Into Good Habits
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You want to get into the good habit of keeping track of how many unknown <math
    alttext="omega"><mi>ω</mi></math> ’s end up in the formula for your training function.
    Recall that these are the <math alttext="omega"><mi>ω</mi></math> ’s that we find
    via minimizing a loss function. The other good habit is having an efficient and
    consistent way to organize them throughout your model (in a vector, matrix, *etc.*).
    In the softmax case, when we have *k* classes, and *n* features for each data
    point, we end up with <math alttext="k times n"><mrow><mi>k</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    <math alttext="omega"><mi>ω</mi></math> ’s for the linear combinations, then *k*
    biases, for a total of <math alttext="k times n plus k"><mrow><mi>k</mi> <mo>×</mo>
    <mi>n</mi> <mo>+</mo> <mi>k</mi></mrow></math> unknown <math alttext="omega"><mi>ω</mi></math>
    ’s. For example, if we use a softmax regression model to classify images in the
    [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database) of handwritten
    numerals, each image has <math alttext="28 times 28"><mrow><mn>28</mn> <mo>×</mo>
    <mn>28</mn></mrow></math> pixels, meaning 784 features, and we want to classify
    them into 10 classes, so we end up having to optimize for 7850 <math alttext="omega"><mi>ω</mi></math>
    ’s. For both the linear and logistic regression models, we only had <math alttext="n
    plus 1"><mrow><mi>n</mi> <mo>+</mo> <mn>1</mn></mrow></math> unknown <math alttext="omega"><mi>ω</mi></math>
    ’s that we needed to optimize for.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we pass each of these *k* results into function called the *softmax function*,
    that generalizes the logistic function from two to multiple classes, and we also
    interpret it as a probability. The formula for the softmax function looks like:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign sigma left-parenthesis s Superscript j Baseline right-parenthesis
    equals StartFraction e Superscript s Super Superscript j Superscript Baseline
    Over e Superscript s Super Superscript 1 Superscript Baseline plus e Superscript
    s squared Baseline plus ellipsis plus e Superscript s Super Superscript k Superscript
    Baseline EndFraction dollar-sign"><mrow><mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mi>j</mi></msup> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi> <msup><mi>s</mi>
    <mi>j</mi></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>s</mi> <mn>1</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>s</mi> <mn>2</mn></msup></msup> <mo>+</mo><mo>⋯</mo><mo>+</mo><msup><mi>e</mi>
    <msup><mi>s</mi> <mi>k</mi></msup></msup></mrow></mfrac></mrow></math>
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: This way, the same data point will get *k* probability scores, one score corresponding
    to each class. Finally, we classify the data point as belonging to the class where
    it obtained the largest probability score.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Aggregating all of the above, we obtain the final formula of the training function
    that we can now use for classification (that is, after we find the optimal <math
    alttext="omega"><mi>ω</mi></math> values by minimizing an appropriate loss function):'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals j such that sigma left-parenthesis omega
    0 Superscript j Baseline plus omega 1 Superscript j Baseline x 1 plus ellipsis
    plus omega Subscript n Superscript j Baseline x Subscript n Baseline right-parenthesis
    is maximal period dollar-sign"><mrow><mi>y</mi> <mo>=</mo> <mi>j</mi> <mtext>such</mtext>
    <mtext>that</mtext> <mi>σ</mi> <mo>(</mo> <msubsup><mi>ω</mi> <mn>0</mn> <mi>j</mi></msubsup>
    <mo>+</mo> <msubsup><mi>ω</mi> <mn>1</mn> <mi>j</mi></msubsup> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi>
    <mi>j</mi></msubsup> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo> <mtext>is</mtext>
    <mtext>maximal.</mtext></mrow></math>
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that for the above training function, all we have to do is input the data
    features (the *x* values), and it returns one class number: *j*.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We derived the cross entropy loss function in the case of logistic regression:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals minus StartFraction 1 Over m EndFraction
    sigma-summation Underscript i equals 1 Overscript m Endscripts y Subscript t r
    u e Superscript i Baseline log left-parenthesis sigma left-parenthesis omega 0
    plus omega 1 x 1 Superscript i Baseline plus ellipsis plus omega Subscript n Baseline
    x Subscript n Superscript i Baseline right-parenthesis right-parenthesis plus
    left-parenthesis 1 minus y Subscript t r u e Superscript i Baseline right-parenthesis
    log left-parenthesis 1 minus sigma left-parenthesis omega 0 plus omega 1 x 1 Superscript
    i Baseline plus ellipsis plus omega Subscript n Baseline x Subscript n Superscript
    i Baseline right-parenthesis right-parenthesis comma dollar-sign"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup> <msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>i</mi></msubsup> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi>
    <mn>1</mn> <mi>i</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>ω</mi>
    <mi>n</mi></msub> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo></mrow></math>
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: using
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign c o s t equals Start 2 By 2 Matrix 1st Row 1st Column
    Blank 2nd Column minus log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 1 2nd Row 1st Column
    Blank 2nd Column minus log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 0 EndMatrix equals minus
    y Subscript t r u e Baseline log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis minus left-parenthesis 1 minus y Subscript t r u e Baseline
    right-parenthesis log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis dollar-sign"><mrow><mi>c</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi>
    <mo>=</mo> <mfenced close="}" open="{" separators=""><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we generalize the same logic to multiple classes. Let’s use the notation
    that <math alttext="y Subscript t r u e comma i Baseline equals 1"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> if a certain data point belongs in the i’th
    class and is zero otherwise. Then we have the cost associate with misclassifying
    a certain data point as:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign c o s t equals Start 5 By 2 Matrix 1st Row 1st Column
    Blank 2nd Column minus log left-parenthesis sigma left-parenthesis s Superscript
    1 Baseline right-parenthesis right-parenthesis if y Subscript t r u e comma 1
    Baseline equals 1 2nd Row 1st Column Blank 2nd Column minus log left-parenthesis
    sigma left-parenthesis s squared right-parenthesis right-parenthesis if y Subscript
    t r u e comma 2 Baseline equals 1 3rd Row 1st Column Blank 2nd Column minus log
    left-parenthesis sigma left-parenthesis s cubed right-parenthesis right-parenthesis
    if y Subscript t r u e comma 3 Baseline equals 1 4th Row 1st Column  ellipsis
    5th Row 1st Column Blank 2nd Column minus log left-parenthesis sigma left-parenthesis
    s Superscript k Baseline right-parenthesis right-parenthesis if y Subscript t
    r u e comma k Baseline equals 1 EndMatrix equals minus y Subscript t r u e comma
    1 Baseline log left-parenthesis sigma left-parenthesis s Superscript 1 Baseline
    right-parenthesis right-parenthesis minus ellipsis minus y Subscript t r u e comma
    k Baseline log left-parenthesis sigma left-parenthesis s Superscript k Baseline
    right-parenthesis right-parenthesis period dollar-sign"><mrow><mi>c</mi> <mi>o</mi>
    <mi>s</mi> <mi>t</mi> <mo>=</mo> <mfenced close="}" open="{" separators=""><mtable
    displaystyle="true"><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi> <mn>1</mn></msup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>2</mn></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mn>3</mn></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>3</mn></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi> <mi>k</mi></msup> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>k</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mn>1</mn></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mo>⋯</mo> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>k</mi></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mi>k</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Averaging over all the *m* data points in the training set, we obtain the *generalized
    cross entropy loss function*, generalizing the cross entropy loss function from
    the case of only two classes to the case of multiple classes:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals minus StartFraction 1 Over m EndFraction
    sigma-summation Underscript i equals 1 Overscript m Endscripts y Subscript t r
    u e comma 1 Superscript i Baseline log left-parenthesis sigma left-parenthesis
    omega 0 Superscript 1 Baseline plus omega 1 Superscript 1 Baseline x 1 Superscript
    i Baseline plus ellipsis plus omega Subscript n Superscript 1 Baseline x Subscript
    n Superscript i Baseline right-parenthesis right-parenthesis plus y Subscript
    t r u e comma 2 Superscript i Baseline log left-parenthesis sigma left-parenthesis
    omega 0 squared plus omega 1 squared x 1 Superscript i Baseline plus ellipsis
    plus omega Subscript n Superscript 2 Baseline x Subscript n Superscript i Baseline
    right-parenthesis right-parenthesis plus ellipsis plus y Subscript t r u e comma
    k Superscript i Baseline log left-parenthesis sigma left-parenthesis omega 0 Superscript
    k Baseline plus omega 1 Superscript k Baseline x 1 Superscript i Baseline plus
    ellipsis plus omega Subscript n Superscript k Baseline x Subscript n Superscript
    i Baseline right-parenthesis right-parenthesis dollar-sign"><mrow><mi>L</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>1</mn></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>1</mn></msubsup> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mn>1</mn></msubsup>
    <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>2</mn></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mn>2</mn></msubsup>
    <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>k</mi></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mi>k</mi></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mi>k</mi></msubsup> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mi>k</mi></msubsup>
    <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a formula for the loss function, we can search for its minimizing
    <math alttext="omega"><mi>ω</mi></math> ’s. As most of the loss functions that
    we will encounter, there is no explicit formula for the minimizers of this loss
    function in terms of the training set and their target labels, so we settle for
    finding the minimizers using numerical methods, in particular: Next chapter’s
    gradient descent, stochastic gradient descent, or mini-batch gradient descent.
    Again, the generalized cross entropy loss function has its convexity working to
    our advantage in the minimization process, so we are guaranteed to find our sought
    after <math alttext="omega"><mi>ω</mi></math> ’s.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Note About Cross Entropy And Information Theory
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cross entropy concept is borrowed from information theory. We will elaborate
    on this when discussing decision trees later in this chapter. For now, keep the
    following quantity in mind, where *p* is the probability of an event occurring:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign log left-parenthesis StartFraction 1 Over p EndFraction
    right-parenthesis equals minus log left-parenthesis p right-parenthesis dollar-sign"><mrow><mo
    form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn> <mi>p</mi></mfrac> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: The above quantity is large when *p* is small, therefore, it quantifies bigger
    *surprise* for *less probable* events.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Note About The Logistic and The Softmax Functions and Statistical Mechanics
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with statistical mechanics, you might have noticed that
    the logistic and softmax functions calculate probabilities in the same way the
    *partition function* from the field of statistical mechanics calculates the probability
    of finding a system in a certain state.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating The Above Models Into The Last Layer Of A Neural Network
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The linear regression model makes its predictions by appropriately linearly
    combining data features, then adding bias. The logistic regression and the softmax
    regression models make their classifications by appropriately linearly combining
    data features, adding bias, then passing the result into a probability scoring
    function. In these simple models, the features of the data are only linearly combined,
    hence, these models are weak in terms of picking up on potentialy important nonlinear
    interactions between the features of the data. Neural network models incorporate
    nonlinear *activation functions* into their training functions, and do this over
    multiple layers, hence are better equipped to detect nonlinear and more complex
    relationships. The last layer of a neural network is its output layer. The layer
    right before the last layer spits out some higher order features and inputs them
    into the last layer. If we want our network to classify data into multiple classes,
    then we can make our last layer a softmax layer; if we want it to classify into
    two classes, then our last layer can be a logistic regression layer; and if we
    want the network to predict numerical values, then we can make its last layer
    a regression layer. We will see examples of these in [Chapter 5](ch05.xhtml#ch05).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Other Popular Machine Learning Techniques and Ensembles of Techniques
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After regression and logistic regression, it is important to branch out into
    the machine learning community and learn the ideas behind some of the most popular
    techniques for classification and regression tasks. *Support vector machines*,
    *decision trees*, and *random forests* are very powerful and popular, and are
    able to perform both classification and regression tasks. The natural question
    is then, when do we use a specific machine learning method, including linear and
    logistic regression, and later neural networks? How do we know which method to
    use and base our conclusions and predictions on? These are the types of questions
    where the mathematical analysis of the machine learning models helps.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Since the mathematical analysis of each method, including the types of data
    sets it is usually best suited for, is only now gaining serious attention, after
    the recent increase in resource allocation for research in AI, machine learning,
    and data science, the current practice is to try out each method on the same data
    set and use the one with the best results. That is, assuming we have the required
    computational and time resources to try out different machine learning techniques.
    Even better, if you do have the time and resources to train various machine learning
    models (parallel computing is perfect here), then *ensemble methods* combine their
    results, either by averaging or by voting, ironically, yet mathematically sound,
    giving better results than the best individual performers, *and even when the
    best performers are weak performers!*
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'One example of an ensemble is a random forest: It is an ensemble of decision
    trees.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: When basing our predictions on ensembles, industry terms like *bagging* (or
    *bootstrap aggregating*), *pasting*, *boosting* such as *ADA boost* and *Gradient
    boosting*, *stacking*, and *random patches* appear. Bagging and pasting train
    *the same* machine learning model on different random subsets of the training
    set. Bagging samples instances from the training set with replacement and pasting
    samples instances from the training set without replacement. *Random patches*
    sample from the feature space as well, training a machine learning model on a
    random subset of the features at a time. This is very helpful when the data set
    has many many features, such as images (where each pixel is a feature). *Stacking
    learns* the prediction mechanism of the ensemble instead of simple voting or averaging.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Support vector machine is an extremely popular machine learning method able
    to perform classification and regression tasks with both linear (flat) and nonlinear
    (curved) decision boundaries.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: For classification, this method seeks to separate the labeled data using a widest
    possible margin, resulting in an optimal *highway* of separation as opposed to
    a thin line of separation. Let’s explain how support vector machines classify
    labeled data instances in the context of this chapter’s structure of training
    function, loss function and optimization.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Training function
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again we linearly combine the features of a data point with unknown weights
    <math alttext="omega"><mi>ω</mi></math> ’s and add bias <math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math> . We then pass the answer through the *sign* function:
    If the linear combination of features plus bias is a positive number, return 1
    (or classify in the first class), if it is negative, return -1 (or classify in
    the other). So the formula for the training function becomes:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign f left-parenthesis ModifyingAbove omega With right-arrow
    semicolon ModifyingAbove x With right-arrow right-parenthesis equals s i g n left-parenthesis
    ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove x
    With right-arrow plus omega 0 right-parenthesis dollar-sign"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'We must design a loss function that penalizes misclassified points. For logistic
    regression, we used the cross entropy loss function. For support vector machines,
    our loss function is based on a function called the *hinge loss function*:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign max left-parenthesis 0 comma 1 minus y Subscript
    t r u e Baseline left-parenthesis ModifyingAbove omega With right-arrow Superscript
    t Baseline ModifyingAbove x With right-arrow plus omega 0 right-parenthesis right-parenthesis
    period dollar-sign"><mrow><mo form="prefix" movablelimits="true">max</mo> <mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo> <mo>.</mo></mrow></math>
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the hinge loss function penalizes errors in classification. First,
    recall that <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    is either 1 or -1, depending on whether the data point belongs in the positive
    or the negative class.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: If for a certain data point <math alttext="y Subscript t r u e"><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> is 1 but <math
    alttext="ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove
    x With right-arrow plus omega 0 less-than 0"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>0</mn></mrow></math>
    , the training function will misclassify it and give us <math alttext="y Subscript
    p r e d i c t Baseline equals negative 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mo>-</mo> <mn>1</mn></mrow></math> , and the hinge loss function’s
    value will be <math alttext="1 minus left-parenthesis 1 right-parenthesis left-parenthesis
    ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove x
    With right-arrow plus omega 0 right-parenthesis greater-than 1"><mrow><mn>1</mn>
    <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>)</mo></mrow> <mrow><mo>(</mo> <msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow>
    <mo>></mo> <mn>1</mn></mrow></math> which is a high penalty when your goal is
    to minimize.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If on the other hand the <math alttext="y Subscript t r u e"><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> is 1 and <math
    alttext="ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove
    x With right-arrow plus omega 0 less-than 0"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>0</mn></mrow></math>
    , the training function will correctly classify it and give us <math alttext="y
    Subscript p r e d i c t Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> . The hinge loss function, however, is designed
    in such a way that would still penalize us if <math alttext="ModifyingAbove omega
    With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow plus
    omega 0 less-than 1"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>1</mn></mrow></math> , and its
    value will be <math alttext="1 minus left-parenthesis 1 right-parenthesis left-parenthesis
    ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove x
    With right-arrow plus omega 0 right-parenthesis"><mrow><mn>1</mn> <mo>-</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mrow></math>
    which is now less than one but still bigger than zero.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only when <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    is 1 and <math alttext="ModifyingAbove omega With right-arrow Superscript t Baseline
    ModifyingAbove x With right-arrow plus omega 0 less-than 1"><mrow><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>1</mn></mrow></math>
    (the training function will still correctly classify this point and give <math
    alttext="y Subscript p r e d i c t Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> ) the hinge loss function value will be zero
    since it will be the maximum between zero and a negative quantity.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The same logic applies when <math alttext="y Subscript t r u e"><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> is -1: The
    hinge loss function will penalize a lot for a wrong prediction, a little for a
    right prediction but if it doesn’t have far enough *margin* from the *zero divider*
    (a margin bigger than 1), and will return zero only when the prediction is right
    and the point is at a distance larger than 1 from the zero divider.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the zero divider has equation <math alttext="ModifyingAbove omega
    With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow plus
    omega 0 equals 0"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math> , and the
    margin edges have equations <math alttext="ModifyingAbove omega With right-arrow
    Superscript t Baseline ModifyingAbove x With right-arrow plus omega 0 equals negative
    1"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>=</mo> <mo>-</mo> <mn>1</mn></mrow></math> and <math alttext="ModifyingAbove
    omega With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow
    plus omega 0 equals 1"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>=</mo> <mn>1</mn></mrow></math> . The distance
    between the margin edges is easy to calculate as <math alttext="StartFraction
    2 Over parallel-to omega parallel-to EndFraction"><mfrac><mn>2</mn> <msub><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></mfrac></math> . So if we want to increase this margin width,
    we have to decrease <math alttext="parallel-to omega parallel-to"><msub><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></math> , thus, this term must enter the loss function, along
    with the hingle loss function, which penalizes both misclassified points and points
    within the margin boundaries.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now if we average the hinge loss over all the m data points in the training
    set, and add <math alttext="parallel-to omega parallel-to"><msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></math> , we obtain the formula for the loss function
    that is commonly used for support vector machines:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals StartFraction 1 Over m EndFraction sigma-summation
    Underscript i equals 1 Overscript m Endscripts max left-parenthesis 0 comma 1
    minus y Subscript t r u e Superscript i Baseline left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow
    Superscript i Baseline plus omega 0 right-parenthesis right-parenthesis plus lamda
    parallel-to omega parallel-to dollar-sign"><mrow><mi>L</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mo form="prefix" movablelimits="true">max</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow> <mn>2</mn>
    <mn>2</mn></msubsup></mrow></math>
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal now is to search for the <math alttext="ModifyingAbove w With right-arrow"><mover
    accent="true"><mi>w</mi> <mo>→</mo></mover></math> that minimizes the loss function.
    Let’s observe this loss function for a minute:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'It has two terms: <math alttext="StartFraction 1 Over m EndFraction sigma-summation
    Underscript i equals 1 Overscript m Endscripts max left-parenthesis 0 comma 1
    minus y Subscript t r u e Superscript i Baseline left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow
    Superscript i Baseline plus omega 0 right-parenthesis right-parenthesis"><mrow><mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mo form="prefix" movablelimits="true">max</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    and <math alttext="lamda parallel-to ModifyingAbove omega With right-arrow parallel-to"><mrow><mrow><mi>λ</mi>
    <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></math> . Whenever we have more than one
    term in an optimization problem, it is most likely that they are competing terms,
    in the sense that the same <math alttext="omega"><mi>ω</mi></math> values that
    make the first term small and thus happy might make the second term big and thus
    sad. So it is a push and pull game between the two terms as we search for the
    <math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math> that optimizes their sum.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The <math alttext="lamda"><mi>λ</mi></math> that appears with the <math alttext="lamda
    parallel-to ModifyingAbove omega With right-arrow parallel-to"><mrow><mrow><mi>λ</mi>
    <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></math> term is an example of a model hyperparameter
    that we can tune during the validation stage of the training process. Note that
    controlling the value of <math alttext="lamda"><mi>λ</mi></math> helps us control
    the width of the margin this way: If we choose a large <math alttext="lamda"><mi>λ</mi></math>
    value, the optimizer will get busy choosing <math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> with
    very low <math alttext="parallel-to omega parallel-to"><msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></math> , in order to compensate for that large
    <math alttext="lamda"><mi>λ</mi></math> , and the first term of the loss function
    will get less attention. But recall that a smaller <math alttext="parallel-to
    omega parallel-to"><msub><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow> <mn>2</mn></msub></math>
    means a larger margin!'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The <math alttext="lamda parallel-to ModifyingAbove omega With right-arrow parallel-to"><mrow><mrow><mi>λ</mi>
    <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></math> term can also be thought of as a
    *regularization term*, which we will discuss in the next chapter.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This loss function is convex and bounded below by zero so its minimization
    problem is not too bad: We don’t have to worry about getting stuck at local minima.
    The first term has a singularity, but as we mentioned before we can define its
    subgradient at the singular point then apply a descent method.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some optimization problems can be reformulated and instead of solving the original
    *primal* problem, we end up solving its *dual* problem! Usually, one is easier
    to solve than the other. We can think of the dual problem as another optimization
    problem living in a parallel universe of the primal problem. The universes meet
    at the optimizer. Hence solving one problem automatically gives the solution of
    the other. We study duality when we study optimization. Of particular interest
    and huge applications areas, are linear and quadratic optimization, also known
    as linear and quadratic programming. The minimization problem that we currently
    have:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts StartFraction 1 Over m EndFraction sigma-summation Underscript i equals
    1 Overscript m Endscripts max left-parenthesis 0 comma 1 minus y Subscript t r
    u e Superscript i Baseline left-parenthesis ModifyingAbove omega With right-arrow
    Superscript t Baseline ModifyingAbove x With right-arrow Superscript i Baseline
    plus omega 0 right-parenthesis right-parenthesis plus lamda parallel-to omega
    parallel-to dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup>
    <mo form="prefix" movablelimits="true">max</mo> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow> <mn>2</mn>
    <mn>2</mn></msubsup></mrow></math>
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'is an example of quadratic programming, and it has a dual problem formulation
    that turns out to be easier to optimize than the primal (especially when the number
    of features is high):'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign max Underscript ModifyingAbove alpha With right-arrow
    Endscripts sigma-summation Underscript j equals 1 Overscript m Endscripts alpha
    Subscript j minus one-half sigma-summation Underscript j equals 1 Overscript m
    Endscripts sigma-summation Underscript k equals 1 Overscript m Endscripts alpha
    Subscript j Baseline alpha Subscript k Baseline y Subscript t r u e Superscript
    j Baseline y Subscript t r u e Superscript k Baseline left-parenthesis left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis Superscript
    t Baseline ModifyingAbove x With right-arrow Superscript k Baseline right-parenthesis
    dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">max</mo> <mover
    accent="true"><mi>α</mi> <mo>→</mo></mover></msub> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>α</mi> <mi>j</mi></msub> <mo>-</mo> <mfrac><mn>1</mn>
    <mn>2</mn></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>α</mi> <mi>j</mi></msub> <msub><mi>α</mi> <mi>k</mi></msub>
    <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>j</mi></msubsup>
    <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>k</mi></msubsup>
    <mrow><mo>(</mo> <msup><mrow><mo>(</mo><msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mi>j</mi></msup> <mo>)</mo></mrow> <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>k</mi></msup> <mo>)</mo></mrow></mrow></math>
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: subject to the constraints <math alttext="alpha Subscript j Baseline greater-than-or-equal-to
    0"><mrow><msub><mi>α</mi> <mi>j</mi></msub> <mo>≥</mo> <mn>0</mn></mrow></math>
    and <math alttext="sigma-summation Underscript j equals 1 Overscript m Endscripts
    alpha Subscript j Baseline y Subscript t r u e Superscript j Baseline equals 0"><mrow><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup> <msub><mi>α</mi>
    <mi>j</mi></msub> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>j</mi></msubsup> <mo>=</mo> <mn>0</mn></mrow></math> . Writing the above formula
    is usually straight forward when we learn about primal and dual problems, so we
    skip the derivation in favor of not interrupting our flow.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Quadratic programming is a very well developed field and there are many software
    packages that can solve this problem. Once we find the maximizing <math alttext="ModifyingAbove
    alpha With right-arrow"><mover accent="true"><mi>α</mi> <mo>→</mo></mover></math>
    , we can find the vector <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that minimizes the primal problem
    using <math alttext="ModifyingAbove omega With right-arrow equals sigma-summation
    Underscript j equals 1 Overscript m Endscripts alpha Subscript j Baseline y Subscript
    t r u e Superscript i Baseline ModifyingAbove x With right-arrow Superscript j"><mrow><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>α</mi> <mi>j</mi></msub> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mi>j</mi></msup></mrow></math> . Once we have our <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    , we can classify new data points using our now trained function:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column f left-parenthesis
    ModifyingAbove x With right-arrow Subscript n e w Baseline right-parenthesis 2nd
    Column equals s i g n left-parenthesis ModifyingAbove omega With right-arrow Superscript
    t Baseline ModifyingAbove x With right-arrow Subscript n e w Baseline plus omega
    0 right-parenthesis 2nd Row 1st Column Blank 2nd Column equals s i g n left-parenthesis
    sigma-summation Underscript j Endscripts alpha Subscript j Baseline y Superscript
    i Baseline left-parenthesis ModifyingAbove x With right-arrow Superscript j Baseline
    right-parenthesis Superscript t Baseline ModifyingAbove x With right-arrow Subscript
    n e w Baseline plus omega 0 right-parenthesis period EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>f</mi> <mo>(</mo>
    <msub><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>s</mi> <mi>i</mi>
    <mi>g</mi> <mi>n</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi>
    <mo>(</mo> <munder><mo>∑</mo> <mi>j</mi></munder> <msub><mi>α</mi> <mi>j</mi></msub>
    <msup><mi>y</mi> <mi>i</mi></msup> <msup><mrow><mo>(</mo><msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: If you want to avoid quadratic programming, there is another very fast method
    called *coordinate descent* that solves the dual problem and works very well with
    large data sets with high number of features.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kernel Trick: We Can Transition The Same Ideas To Nonlinear Classification'
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One very important note about the dual problem: The data points appear only
    in pairs, more specifically, only in a scalar product, namely, <math alttext="left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis Superscript
    t Baseline ModifyingAbove x With right-arrow Superscript k"><mrow><msup><mrow><mo>(</mo><msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>k</mi></msup></mrow></math>
    . Similarly they only appear as a scalar product in the trained function. This
    simple observation allows for magic:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: If we find a function <math alttext="upper K left-parenthesis ModifyingAbove
    x With right-arrow Superscript j Baseline comma ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis"><mrow><mi>K</mi> <mo>(</mo> <msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo> <msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow></math>
    that can be applied to pairs of data points, and it happens to give us the scalar
    product of pairs of transformed data points into some higher domensional space
    (without knowing what the actual transformation is), then we can solve the same
    exact dual problem in the higher dimensional space, by replacing the scalar product
    in the formula of the dual problem by <math alttext="upper K left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline comma ModifyingAbove
    x With right-arrow Superscript j Baseline right-parenthesis"><mrow><mi>K</mi>
    <mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup>
    <mo>,</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup>
    <mo>)</mo></mrow></math> .
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intuition here is that data that is nonlinearly separable in lower dimensions
    is almost always linearly separable in higher dimensions. So transform all the
    data points to higher dimensions then separate. The kernel trick solves the linear
    classification problem in higher dimensions *without* transforming each point.
    The kernel itself evaluates the dot product of transformed data without transforming
    the data. Pretty cool stuff.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of kernel functions include:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper K left-parenthesis ModifyingAbove x With right-arrow Superscript
    j Baseline comma ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis
    equals left-parenthesis left-parenthesis ModifyingAbove x With right-arrow Superscript
    j Baseline right-parenthesis Superscript t Baseline ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis squared"><mrow><mi>K</mi> <mrow><mo>(</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mrow><mo>(</mo><msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mi>t</mi></msup> <msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> .
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The polynomial kernel: <math alttext="upper K left-parenthesis ModifyingAbove
    x With right-arrow Superscript j Baseline comma ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis equals left-parenthesis 1 plus left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis Superscript
    t Baseline ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis
    Superscript d"><mrow><mi>K</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msup><mrow><mo>(</mo><msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup>
    <mo>)</mo></mrow> <mi>d</mi></msup></mrow></math> .'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Gaussian kernel: <math alttext="upper K left-parenthesis ModifyingAbove
    x With right-arrow Superscript j Baseline comma ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis equals e Superscript minus gamma StartAbsoluteValue
    x Super Subscript j Superscript minus x Super Subscript k Superscript EndAbsoluteValue
    squared"><mrow><mi>K</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>e</mi>
    <mrow><mrow><mo>-</mo><mi>γ</mi><mo>|</mo></mrow><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>-</mo><msub><mi>x</mi> <mi>k</mi></msub> <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup></mrow></msup></mrow></math>
    .'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Staying with our driving theme for this chapter that everything is a function,
    a decision tree, in essence, is a function that takes boolean variables as an
    input (these are variables that can only assume *true* (or 1) or *false* (or 0)
    values) such as: Is the feature>5, is the feature=sunny, is the feature=man, *etc.*,
    and outputs a *decision* such as: approve the loan, classify as covid19, return
    25, *etc*. Instead of adding or multiplying boolean variables we use the logical
    *or*, *and* and *not* operators.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if our features are not given in the original data set as boolean
    variables? Then we must transform them to boolean variables before feeding them
    into the model to make predictions. For example, the decision tree in [Figure 3-17](#Fig_regression_tree)
    was trained on the Fish Market data set. It is a regression tree. The tree takes
    raw data, but the function representing the tree actually operates on new variables,
    which are the original data features transformed into boolean variables:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: a1=(Width <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 5.117)
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a2=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 59.55)
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a3=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 41.1)
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a4=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 34.9)
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a5=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 27.95)
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a6=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 21.25)
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![400](assets/emai_0317.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. A regression decision tree constructed on the Fish Market data
    set. See the accompanying Jupyter notebook for details.
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now the function representing the decision tree in [Figure 3-17](#Fig_regression_tree)
    is:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign f left-parenthesis a Baseline 1 comma a Baseline
    2 comma a Baseline 3 comma a Baseline 4 comma a Baseline 5 comma a Baseline 6
    right-parenthesis equals left-parenthesis a Baseline 1 and a Baseline 5 and a
    Baseline 6 right-parenthesis times 39.584 plus left-parenthesis a Baseline 1 and
    a Baseline 5 and not a Baseline 6 right-parenthesis times 139.968 plus left-parenthesis
    a Baseline 1 and not a Baseline 5 and a Baseline 4 right-parenthesis times 287.278
    plus left-parenthesis a Baseline 1 and not a Baseline 5 and not a Baseline 4 right-parenthesis
    times 422.769 plus left-parenthesis not a Baseline 1 and a Baseline 2 and a Baseline
    3 right-parenthesis times 639.737 plus left-parenthesis not a Baseline 1 and a
    Baseline 2 and not a Baseline 3 right-parenthesis times 824.211 plus left-parenthesis
    not a Baseline 1 and not a Baseline 2 right-parenthesis times 1600 dollar-sign"><mrow><mi>f</mi>
    <mo>(</mo> <mi>a</mi> <mn>1</mn> <mo>,</mo> <mi>a</mi> <mn>2</mn> <mo>,</mo> <mi>a</mi>
    <mn>3</mn> <mo>,</mo> <mi>a</mi> <mn>4</mn> <mo>,</mo> <mi>a</mi> <mn>5</mn> <mo>,</mo>
    <mi>a</mi> <mn>6</mn> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn> <mtext>and</mtext>
    <mi>a</mi> <mn>5</mn> <mtext>and</mtext> <mi>a</mi> <mn>6</mn> <mo>)</mo> <mo>×</mo>
    <mn>39</mn> <mo>.</mo> <mn>584</mn> <mo>+</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn>
    <mtext>and</mtext> <mi>a</mi> <mn>5</mn> <mtext>and</mtext> <mtext>not</mtext>
    <mi>a</mi> <mn>6</mn> <mo>)</mo> <mo>×</mo> <mn>139</mn> <mo>.</mo> <mn>968</mn>
    <mo>+</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn> <mtext>and</mtext> <mtext>not</mtext>
    <mi>a</mi> <mn>5</mn> <mtext>and</mtext> <mi>a</mi> <mn>4</mn> <mo>)</mo> <mo>×</mo>
    <mn>287</mn> <mo>.</mo> <mn>278</mn> <mo>+</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn>
    <mtext>and</mtext> <mtext>not</mtext> <mi>a</mi> <mn>5</mn> <mtext>and</mtext>
    <mtext>not</mtext> <mi>a</mi> <mn>4</mn> <mo>)</mo> <mo>×</mo> <mn>422</mn> <mo>.</mo>
    <mn>769</mn> <mo>+</mo> <mo>(</mo> <mtext>not</mtext> <mi>a</mi> <mn>1</mn> <mtext>and</mtext>
    <mi>a</mi> <mn>2</mn> <mtext>and</mtext> <mi>a</mi> <mn>3</mn> <mo>)</mo> <mo>×</mo>
    <mn>639</mn> <mo>.</mo> <mn>737</mn> <mo>+</mo> <mo>(</mo> <mtext>not</mtext>
    <mi>a</mi> <mn>1</mn> <mtext>and</mtext> <mi>a</mi> <mn>2</mn> <mtext>and</mtext>
    <mtext>not</mtext> <mi>a</mi> <mn>3</mn> <mo>)</mo> <mo>×</mo> <mn>824</mn> <mo>.</mo>
    <mn>211</mn> <mo>+</mo> <mo>(</mo> <mtext>not</mtext> <mi>a</mi> <mn>1</mn> <mtext>and</mtext>
    <mtext>not</mtext> <mi>a</mi> <mn>2</mn> <mo>)</mo> <mo>×</mo> <mn>1600</mn></mrow></math>
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that unlike the training functions that we encountered in this chapter
    so far, the above function has no parameters <math alttext="omega"><mi>ω</mi></math>
    ’s that we need to solve for. This is called a *nonparametric model*, and it doesn’t
    fix the *shape* of the function ahead of time. This gives it the flexibility to
    *grow* with the data, or in other words, adapt to the data. Of course, with this
    high adaptability to the data comes the high risk of overfitting the data. Thankfully
    there are ways around this, which we list some here without any elaboration: pruning
    the tree after growing it, restricting the number of layers, setting a minimum
    number of data instances per node, or using an ensemble of trees instead of one
    tree, called a *random forest*, discussed below.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'One very important observation: The decision tree *decided* to split over only
    two features of the original data set, namely the Width and Length3 features.
    Decision trees are designed in a way that keeps the more imporatnt features (those
    providing the most information that contribute to our prediction) closer to the
    root. Therefore, decision trees can help in feature selection, where we select
    the most important features to contribute to our final model’s predictions.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: It is no wonder the Width and Length3 features ended up being the most important
    for predicting the weight of the fish. The correlation matrix in [Figure 3-18](#Fig_fish_corr_matrix)
    and the scatter plots in [Figure 3-3](#Fig_weight_lengths_scatterplots) show extremely
    strong correlation between all the length features. This means that the information
    they provide is redundant, and including all of them in our prediction models
    will increase computation costs and lower performance.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0318.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. Correlation matrix for the Fish Market data set. There is an extremely
    strong correlation between all the length features.
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note: Feature Selection'
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just introduced the very important topic of feature selection. Real world
    data sets come with many features and some of them may provide redundant information,
    others are not important at all for predicting our target label. Including irrelevant
    and/redundant features in a machine learning model increases computational cost
    and lowers its performance. We just saw that decision trees are one way that helps
    select the important features. Another way is a regularization technique called
    Lasso regression, which we will introduce in [Chapter 4](ch04.xhtml#ch04). There
    are statistical tests that test for feature dependencies on each other. The *F-test*
    tests for linear dependencies (this gives higher scores for correlated features,
    but correlations alone are deceptive), and *Mutual Information* tests for nonlinear
    dependencies. These provide a *measure* of how much a feature contributes to determining
    the target label, and hence aids in feature selection, by keeping the most promising
    features. We can also test for features’ dependencies on each other, as well as
    their correlations and scatterplots. *Variance* thresholding removes features
    with little to no variance, on the premise that if a feature does not vary much
    within itself, it has little predictive power.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we train a decision tree on a data set? What function do we optimize?
    There are two functions that are usually optimized when *growing* decision trees:
    The *entropy* and the *gini impurity*. Using one or the other doesn’t make much
    difference in the resultant trees. We develop these next.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Entropy and Gini Impurity
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we decide to split a node of the tree on the feature that is evaluated
    as the *most important*. *Entropy* and *Gini Impurity* are the two popular ways
    to measure importance of a feature. They are not mathematically equivalent but
    they both work and provide reasonable decision trees. Gini impurity is usually
    less expensive to compute, so it is the default in software packages, but you
    have the option to change from the default setting to entropy. Using Gini impurity
    tends to produce less balanced trees when there are classes with much higher frequency
    than others. These classes end up isolated in their own branches. However, in
    many cases, using either entropy or Gini impurity provides not much difference
    in the resulting decision trees.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: With the entropy approach, we look for the feature split that provides the *maximal
    information gain* (we’ll give its formula shortly). Information gain is borrowed
    from information theory, and it has to do with the concept of *entropy*. Entropy,
    in turn, is borrowed from Thermodynamics and Statistical Physics, and it quantifies
    the amount of disorder in a certain system.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the gini impurity approach, we look for the feature split that provides
    children nodes with lowest average gini impurity (we’ll also give its formula
    shortly).
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To maximize information gain (or minimize gini impurity), the algorithm that
    grows a decision tree has to go over each feature of the training data subset
    and calculate the information gain (or gini impurity) accomplished whether the
    tree uses that particular feature as a node to split on, then choose the feature
    that provides the highest information gain (or children nodes with lowest average
    gini impurity). Moreover, if the feature has real numerical values, the algorithm
    has to decide on *what question to ask at the node*, meaning what feature value
    to split on, for example, is <math alttext="x 5 less-than 0.1"><mrow><msub><mi>x</mi>
    <mn>5</mn></msub> <mo><</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn></mrow></math> ?
    The algorithm has to do this sequentially at each layer of the tree, calculating
    information gain (or gini impurities) over the features of the data instances
    in each node and sometimes on each split value possibility. This is easier to
    understand with examples. But first, we write the formulas for the entropy, information
    gain, and gini impurity.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Entropy and Information Gain
  id: totrans-435
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The easiest way to understand the entropy formula is to rely on the intuition
    that if an event is highly probable, then there is little surprise associated
    with it happening. So when <math alttext="p left-parenthesis e v e n t right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>e</mi> <mi>v</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mo>)</mo></mrow></math>
    is large, its surprise is low. We can mathematically encode this with a function
    that decreases when its probability increases. The calculus function <math alttext="log
    StartFraction 1 Over x EndFraction"><mrow><mo form="prefix">log</mo> <mfrac><mn>1</mn>
    <mi>x</mi></mfrac></mrow></math> works and has the additional property that the
    surprises of independent events add up. Therefore, we can define:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper S u r p r i s e left-parenthesis e v e n t
    right-parenthesis equals log StartFraction 1 Over p left-parenthesis e v e n t
    right-parenthesis EndFraction equals minus log left-parenthesis p left-parenthesis
    e v e n t right-parenthesis right-parenthesis dollar-sign"><mrow><mi>S</mi> <mi>u</mi>
    <mi>r</mi> <mi>p</mi> <mi>r</mi> <mi>i</mi> <mi>s</mi> <mi>e</mi> <mrow><mo>(</mo>
    <mi>e</mi> <mi>v</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo form="prefix">log</mo> <mfrac><mn>1</mn> <mrow><mi>p</mi><mo>(</mo><mi>e</mi><mi>v</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>e</mi> <mi>v</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the entropy of a random variable (which in our case is a particular feature
    in our training data set) is defined as the *expected surprise* associated with
    the random variable, so we must add up the surprises of each possible outcome
    of the random variable (surprise of each value of the feature in question) multiplied
    with their respective probabilities, obtaining:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper E n t r o p y left-parenthesis upper X right-parenthesis
    equals minus p left-parenthesis o u t c o m e 1 right-parenthesis log left-parenthesis
    p left-parenthesis o u t c o m e 1 right-parenthesis right-parenthesis minus p
    left-parenthesis o u t c o m e 2 right-parenthesis log left-parenthesis p left-parenthesis
    o u t c o m e 2 right-parenthesis right-parenthesis minus ellipsis minus p left-parenthesis
    o u t c o m e Subscript n Baseline right-parenthesis log left-parenthesis p left-parenthesis
    o u t c o m e Subscript n Baseline right-parenthesis right-parenthesis period
    dollar-sign"><mrow><mi>E</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>o</mi> <mi>p</mi>
    <mi>y</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi> <mi>o</mi>
    <mi>m</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi>
    <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>-</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi>
    <mi>t</mi> <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mo>⋯</mo> <mo>-</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi> <mi>o</mi>
    <mi>m</mi> <msub><mi>e</mi> <mi>n</mi></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi>
    <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi> <mi>n</mi></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'The the entropy for one feature of our training data that assumes a bunch of
    values is:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper E n t r o p y left-parenthesis upper F e a
    t u r e right-parenthesis equals minus p left-parenthesis v a l u e 1 right-parenthesis
    log left-parenthesis p left-parenthesis v a l u e 1 right-parenthesis right-parenthesis
    minus p left-parenthesis v a l u e 2 right-parenthesis log left-parenthesis p
    left-parenthesis v a l u e 2 right-parenthesis right-parenthesis minus ellipsis
    minus p left-parenthesis v a l u e Subscript n Baseline right-parenthesis log
    left-parenthesis p left-parenthesis v a l u e Subscript n Baseline right-parenthesis
    right-parenthesis dollar-sign"><mrow><mi>E</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi>
    <mi>o</mi> <mi>p</mi> <mi>y</mi> <mrow><mo>(</mo> <mi>F</mi> <mi>e</mi> <mi>a</mi>
    <mi>t</mi> <mi>u</mi> <mi>r</mi> <mi>e</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mo>⋯</mo> <mo>-</mo> <mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our goal is to select to split on a feature that provides large information
    gain about the outcome (the label or the target feature), let’s first calculate
    the entropy of the outcome feature. Assume for simplicity that this is a binary
    classification problem, so the outcome feature only has two values: positive (in
    the class) and negative (not in the class). If we let *p* be the number of positive
    instances in the target feature and *n* be the number of negative ones, then *p+n=m*
    will be the number of instances in the training data subset. Now the probability
    to select a positive instance from that target column will be <math alttext="StartFraction
    p Over m EndFraction equals StartFraction p Over p plus n EndFraction"><mrow><mfrac><mi>p</mi>
    <mi>m</mi></mfrac> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac></mrow></math>
    , and the probability to select a negative instance is similarly <math alttext="StartFraction
    n Over m EndFraction equals StartFraction n Over p plus n EndFraction"><mrow><mfrac><mi>n</mi>
    <mi>m</mi></mfrac> <mo>=</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac></mrow></math>
    . Thus, the entropy of the outcome feature (without leveraging any information
    from the other features) is:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column upper
    E n t r o p y left-parenthesis Outcome Feature right-parenthesis 2nd Row 1st Column
    Blank 2nd Column equals minus p left-parenthesis p o s i t i v e right-parenthesis
    log left-parenthesis p left-parenthesis p o s i t i v e right-parenthesis right-parenthesis
    minus p left-parenthesis n e g a t i v e right-parenthesis log left-parenthesis
    p left-parenthesis n e g a t i v e right-parenthesis right-parenthesis 3rd Row
    1st Column Blank 2nd Column equals minus StartFraction p Over p plus n EndFraction
    log left-parenthesis StartFraction p Over p plus n EndFraction right-parenthesis
    minus StartFraction n Over p plus n EndFraction log left-parenthesis StartFraction
    n Over p plus n EndFraction right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mi>E</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>o</mi>
    <mi>p</mi> <mi>y</mi> <mo>(</mo> <mtext>Outcome</mtext> <mtext>Feature</mtext>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo>
    <mi>p</mi> <mo>(</mo> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mo>)</mo> <mo form="prefix">log</mo> <mo>(</mo> <mi>p</mi>
    <mo>(</mo> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi>
    <mi>e</mi> <mo>)</mo> <mo>)</mo> <mo>-</mo> <mi>p</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi>
    <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>)</mo> <mo
    form="prefix">log</mo> <mo>(</mo> <mi>p</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi>
    <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>)</mo> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: Now we leverage information from one other feature and calculate the difference
    in entropy of the outcome feature, which we expect to decrease as we gain more
    information (more information generally results in less surprise).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we choose feature *A* to split a node of our decision tree on. Suppose
    Feature *A* assumes four values, and has <math alttext="k 1"><msub><mi>k</mi>
    <mn>1</mn></msub></math> instances with <math alttext="v a l u e 1"><mrow><mi>v</mi>
    <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub></mrow></math>
    , of these <math alttext="p 1"><msub><mi>p</mi> <mn>1</mn></msub></math> are labeled
    positive as their target outcome, and <math alttext="n 1"><msub><mi>n</mi> <mn>1</mn></msub></math>
    are labeled negative as their target outcome, so <math alttext="p 1 plus n 1 equals
    k 1"><mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>n</mi> <mn>1</mn></msub>
    <mo>=</mo> <msub><mi>k</mi> <mn>1</mn></msub></mrow></math> . Similarly, Feature
    *A* has <math alttext="k 2"><msub><mi>k</mi> <mn>2</mn></msub></math> instances
    with <math alttext="v a l u e 2"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>2</mn></msub></mrow></math> , of these <math alttext="p 2"><msub><mi>p</mi>
    <mn>2</mn></msub></math> are labeled positive as their target outcome, and <math
    alttext="n 2"><msub><mi>n</mi> <mn>2</mn></msub></math> are labeled negative as
    their target outcome, so <math alttext="p 2 plus n 2 equals k 2"><mrow><msub><mi>p</mi>
    <mn>2</mn></msub> <mo>+</mo> <msub><mi>n</mi> <mn>2</mn></msub> <mo>=</mo> <msub><mi>k</mi>
    <mn>2</mn></msub></mrow></math> . The same applies for <math alttext="v a l u
    e 3"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>
    and <math alttext="v a l u e 4"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>4</mn></msub></mrow></math> of Feature *A*. Note that <math
    alttext="k 1 plus k 2 plus k 3 plus k 4 equals m"><mrow><msub><mi>k</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>k</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>k</mi> <mn>3</mn></msub>
    <mo>+</mo> <msub><mi>k</mi> <mn>4</mn></msub> <mo>=</mo> <mi>m</mi></mrow></math>
    , the total number of instances in the training subset of the data set. Now the
    each value <math alttext="v a l u e Subscript k"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi>
    <mi>u</mi> <msub><mi>e</mi> <mi>k</mi></msub></mrow></math> of Feature *A* can
    be thought of as a random variable in its own respect, with <math alttext="p Subscript
    k"><msub><mi>p</mi> <mi>k</mi></msub></math> positive outcomes and <math alttext="n
    Subscript k"><msub><mi>n</mi> <mi>k</mi></msub></math> negative outcomes, so we
    can calculate its entropy (expected surprise):'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Entropy left-parenthesis
    v a l u e 1 right-parenthesis 2nd Column equals minus StartFraction p 1 Over p
    1 plus n 1 EndFraction log left-parenthesis StartFraction p 1 Over p 1 plus n
    1 EndFraction right-parenthesis minus StartFraction n 1 Over p 1 plus n 1 EndFraction
    log left-parenthesis StartFraction n 1 Over p 1 plus n 1 EndFraction right-parenthesis
    2nd Row 1st Column Entropy left-parenthesis v a l u e 2 right-parenthesis 2nd
    Column equals minus StartFraction p 2 Over p 2 plus n 2 EndFraction log left-parenthesis
    StartFraction p 2 Over p 2 plus n 2 EndFraction right-parenthesis minus StartFraction
    n 2 Over p 2 plus n 2 EndFraction log left-parenthesis StartFraction n 2 Over
    p 2 plus n 2 EndFraction right-parenthesis 3rd Row 1st Column Entropy left-parenthesis
    v a l u e 3 right-parenthesis 2nd Column equals minus StartFraction p 3 Over p
    3 plus n 3 EndFraction log left-parenthesis StartFraction p 3 Over p 3 plus n
    3 EndFraction right-parenthesis minus StartFraction n 3 Over p 3 plus n 3 EndFraction
    log left-parenthesis StartFraction n 3 Over p 3 plus n 3 EndFraction right-parenthesis
    4th Row 1st Column Entropy left-parenthesis v a l u e 4 right-parenthesis 2nd
    Column equals minus StartFraction p 4 Over p 4 plus n 4 EndFraction log left-parenthesis
    StartFraction p 4 Over p 4 plus n 4 EndFraction right-parenthesis minus StartFraction
    n 4 Over p 4 plus n 4 EndFraction log left-parenthesis StartFraction n 4 Over
    p 4 plus n 4 EndFraction right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mtext>Entropy</mtext> <mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi> <mn>1</mn></msub>
    <mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi> <mn>1</mn></msub>
    <mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi> <mn>1</mn></msub> <mrow><msub><mi>p</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi> <mn>1</mn></msub>
    <mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Entropy</mtext>
    <mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mtext>Entropy</mtext> <mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi> <mn>3</mn></msub>
    <mrow><msub><mi>p</mi> <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi> <mn>3</mn></msub>
    <mrow><msub><mi>p</mi> <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi> <mn>3</mn></msub> <mrow><msub><mi>p</mi>
    <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi> <mn>3</mn></msub>
    <mrow><msub><mi>p</mi> <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Entropy</mtext>
    <mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>4</mn></msub>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have this information, we can calculate the *expected entropy*
    after splitting on Feature *A*, so we add up the above four entropies each multiplied
    with its respective probability. Note that <math alttext="p left-parenthesis v
    a l u e 1 right-parenthesis equals StartFraction k 1 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>1</mn></msub>
    <mi>m</mi></mfrac></mrow></math> , <math alttext="p left-parenthesis v a l u e
    2 right-parenthesis equals StartFraction k 2 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>2</mn></msub>
    <mi>m</mi></mfrac></mrow></math> , <math alttext="p left-parenthesis v a l u e
    3 right-parenthesis equals StartFraction k 3 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>3</mn></msub>
    <mi>m</mi></mfrac></mrow></math> , and <math alttext="p left-parenthesis v a l
    u e 4 right-parenthesis equals StartFraction k 4 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>4</mn></msub>
    <mi>m</mi></mfrac></mrow></math> . Therefore, the expected entropy after splitting
    on Feature *A* would be:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column Expected
    Entropy left-parenthesis Feature upper A right-parenthesis 2nd Row 1st Column
    Blank 2nd Column equals p left-parenthesis v a l u e 1 right-parenthesis Entropy
    left-parenthesis v a l u e 1 right-parenthesis plus p left-parenthesis v a l u
    e 2 right-parenthesis Entropy left-parenthesis v a l u e 2 right-parenthesis 3rd
    Row 1st Column Blank 2nd Column plus p left-parenthesis v a l u e 3 right-parenthesis
    Entropy left-parenthesis v a l u e 3 right-parenthesis plus p left-parenthesis
    v a l u e 4 right-parenthesis Entropy left-parenthesis v a l u e 4 right-parenthesis
    4th Row 1st Column Blank 2nd Column equals StartFraction k 1 Over m EndFraction
    Entropy left-parenthesis v a l u e 1 right-parenthesis plus StartFraction k 2
    Over m EndFraction Entropy left-parenthesis v a l u e 2 right-parenthesis plus
    StartFraction k 3 Over m EndFraction Entropy left-parenthesis v a l u e 3 right-parenthesis
    plus StartFraction k 4 Over m EndFraction Entropy left-parenthesis v a l u e 4
    right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mtext>Expected</mtext> <mtext>Entropy</mtext> <mo>(</mo>
    <mtext>Feature</mtext> <mtext>A</mtext> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi>
    <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>+</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi>
    <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>4</mn></msub> <mo>)</mo></mrow>
    <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>4</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mfrac><msub><mi>k</mi> <mn>1</mn></msub>
    <mi>m</mi></mfrac> <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <mfrac><msub><mi>k</mi> <mn>2</mn></msub> <mi>m</mi></mfrac> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mfrac><msub><mi>k</mi> <mn>3</mn></msub>
    <mi>m</mi></mfrac> <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <mfrac><msub><mi>k</mi> <mn>4</mn></msub> <mi>m</mi></mfrac> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: where the entropy of each value of Feature *A* is given in the previous paragraph.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'So what would the information gained from using feature *A* to split on be?
    It would the difference between the entropy of the outcome feature without any
    information from Feature *A* and the expected entropy of Feature *A*. Therefore,
    we have a formula for *information gain* given that we decide to split on Feature
    *A*:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column Information
    Gain equals 2nd Row 1st Column Blank 2nd Column upper E n t r o p y left-parenthesis
    Outcome Feature right-parenthesis minus Expected Entropy left-parenthesis Feature
    upper A right-parenthesis equals 3rd Row 1st Column Blank 2nd Column minus StartFraction
    p Over p plus n EndFraction log left-parenthesis StartFraction p Over p plus n
    EndFraction right-parenthesis minus StartFraction n Over p plus n EndFraction
    log left-parenthesis StartFraction n Over p plus n EndFraction right-parenthesis
    minus Expected Entropy left-parenthesis Feature upper A right-parenthesis period
    EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mtext>Information</mtext>
    <mtext>Gain</mtext> <mo>=</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>E</mi>
    <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>o</mi> <mi>p</mi> <mi>y</mi> <mo>(</mo> <mtext>Outcome</mtext>
    <mtext>Feature</mtext> <mo>)</mo> <mo>-</mo> <mtext>Expected</mtext> <mtext>Entropy</mtext>
    <mo>(</mo> <mtext>Feature</mtext> <mtext>A</mtext> <mo>)</mo> <mo>=</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>-</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mtext>Expected</mtext> <mtext>Entropy</mtext> <mrow><mo>(</mo>
    <mtext>Feature</mtext> <mtext>A</mtext> <mo>)</mo></mrow> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Now it is easy to go through each feature of the training data subset and calculate
    the information gain resulting from using that feature to split on. Ultimately,
    the decision tree algorithm decides to split on the feature with highest information
    gain. The algorithm does this recursively for each node and at each layer of the
    tree, until it runs out of features to split on or of data instances. Therefore,
    we obtain our entropy based decision tree.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not too difficult to generalize the above logic to the case where we
    have a multi-class output, for example, a classification problem with three or
    more target labels. The classical [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris)
    from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)
    is a great example with three target labels. This data set has four features for
    a given Iris flower: Its sepal length and width, and its petal length and width.
    Note that each of these features is a continuous random variable, not discrete.
    So we have to devise a test to split on the values of each feature, *before* applying
    the above logic. This is part of the feature engineering stage of a data science
    project. The engineering step here is: Transform a continuous valued feature into
    a boolean feature, for example, *is the petal length>2.45*? We will not go over
    how to choose the number 2.45, but by now you probably can guess that there is
    an optimization process that should go on here as well.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  id: totrans-454
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each decision tree is characterized by its nodes, branches, and leaves. A node
    is considered *pure* if it only contains data instances from the training data
    subset that have the same target label (this means they belong to the same class).
    Note that a pure node is a desired node, since we know its class. Therefore, an
    algorithm would want to grow a tree in a way that minimizes the *impurity* of
    the nodes: If the data instances in a node do not all belong in the same class,
    then the node is *impure*. *Gini impurity* quantifies this impurity the following
    way:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that our classification problem has three classes, like the [Iris Data
    Set](https://archive.ics.uci.edu/ml/datasets/iris). Suppose also that a certain
    node in a decision tree grown to fit this data set has *n* training instances,
    with <math alttext="n 1"><msub><mi>n</mi> <mn>1</mn></msub></math> of these belonging
    in the first class, <math alttext="n 2"><msub><mi>n</mi> <mn>2</mn></msub></math>
    in the second class, and <math alttext="n 3"><msub><mi>n</mi> <mn>3</mn></msub></math>
    in the third class (so <math alttext="n 1 plus n 2 plus n 3 equals n"><mrow><msub><mi>n</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>n</mi>
    <mn>3</mn></msub> <mo>=</mo> <mi>n</mi></mrow></math> ). Then the Gini impurity
    of this node is given by:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Gini impurity equals 1 minus left-parenthesis StartFraction
    n 1 Over n EndFraction right-parenthesis squared minus left-parenthesis StartFraction
    n 2 Over n EndFraction right-parenthesis squared minus left-parenthesis StartFraction
    n 3 Over n EndFraction right-parenthesis squared dollar-sign"><mrow><mtext>Gini</mtext>
    <mtext>impurity</mtext> <mo>=</mo> <mn>1</mn> <mo>-</mo> <msup><mrow><mo>(</mo><mfrac><msub><mi>n</mi>
    <mn>1</mn></msub> <mi>n</mi></mfrac><mo>)</mo></mrow> <mn>2</mn></msup> <mo>-</mo>
    <msup><mrow><mo>(</mo><mfrac><msub><mi>n</mi> <mn>2</mn></msub> <mi>n</mi></mfrac><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>-</mo> <msup><mrow><mo>(</mo><mfrac><msub><mi>n</mi> <mn>3</mn></msub>
    <mi>n</mi></mfrac><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: So for each node the fraction of the data instances belonging to each class
    is calculated, squared, then the sum of those is subtracted from 1\. Note that
    if all the data instances of a node belong in the same class, then the above formula
    gives a Gini impurity equal to zero.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision tree growing algorithm now looks for the feature and split point
    in each feature that produce children nodes with the lowest Gini impurity, on
    average. This means the children of a node must on average be purer than the parent
    node. Thus, the algorithm tries to minimize a weighted average of the Gini impurities
    of two the children nodes (of a binary tree). Each child’s Gini impurity is weighted
    by its relative size, which is the ratio between its number of instances relative
    to total number of instances in that tree layer (which is the same as the number
    of instances as its parent’s). Thus, we end up having to search for the feature
    and the split point (for each feature) combination that solve the following minimization
    problem:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign min Underscript upper F e a t u r e comma upper F
    e a t u r e upper S p l i t upper V a l u e Endscripts StartFraction n Subscript
    l e f t Baseline Over n EndFraction upper G i n i left-parenthesis Left Node right-parenthesis
    plus StartFraction n Subscript r i g h t Baseline Over n EndFraction upper G i
    n i left-parenthesis Right Node right-parenthesis dollar-sign"><mrow><msub><mo
    form="prefix" movablelimits="true">min</mo> <mrow><mi>F</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>,</mo><mi>F</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>S</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mfrac><msub><mi>n</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub>
    <mi>n</mi></mfrac> <mi>G</mi> <mi>i</mi> <mi>n</mi> <mi>i</mi> <mrow><mo>(</mo>
    <mtext>Left</mtext> <mtext>Node</mtext> <mo>)</mo></mrow> <mo>+</mo> <mfrac><msub><mi>n</mi>
    <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub> <mi>n</mi></mfrac>
    <mi>G</mi> <mi>i</mi> <mi>n</mi> <mi>i</mi> <mrow><mo>(</mo> <mtext>Right</mtext>
    <mtext>Node</mtext> <mo>)</mo></mrow></mrow></math>
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="n Subscript l e f t"><msub><mi>n</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub></math>
    and <math alttext="n Subscript r i g h t"><msub><mi>n</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub></math>
    are the number of data instances that end up being in the left and right children
    nodes, and *n* is the number of data instances that are in the parent node (note
    that <math alttext="n Subscript l e f t"><msub><mi>n</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub></math>
    and <math alttext="n Subscript r i g h t"><msub><mi>n</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub></math>
    must add up to *n*).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Regression Decision Trees
  id: totrans-462
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important to point out that decision trees can be used for both regression
    and classification. A regression decision tree returns a predicted value rather
    than a class, but a similar process to a classification tree applies.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of splitting a node by selecting a feature and a feature value (for
    example, is height>3 feet?) that maximize information gain or minimize Gini impurity,
    we select a feature and a feature value that minimize a mean squared distance
    between the true labels and the average of the labels of all the instances in
    each of the left and right children nodes. That is, the algorithm chooses a feature
    and feature value to split on, then looks at the left and right children nodes
    resulting from that split, and calculates:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: The average value of all the labels of the training data instances in the left
    node. This average will be the left node value <math alttext="y Subscript l e
    f t"><msub><mi>y</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub></math>
    , and it is the value predicted by the decision tree if this node ends up being
    a leaf node.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average value of all the labels of the training data instances in the right
    node. This average will be the right node value <math alttext="y Subscript r i
    g h t"><msub><mi>y</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub></math>
    . Similarly, this is the value predicted by the decision tree if this node ends
    up being a leaf node.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the squared distance between the left node value and the true label
    of of each instance in the left node <math alttext="sigma-summation Underscript
    upper L e f t upper N o d e upper I n s t a n c e s Endscripts StartAbsoluteValue
    y Subscript t r u e Superscript i Baseline minus y Subscript l e f t Baseline
    EndAbsoluteValue squared"><mrow><msub><mo>∑</mo> <mrow><mi>L</mi><mi>e</mi><mi>f</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <msup><mrow><mo>|</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo><msub><mi>y</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub>
    <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math> .
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the squared distance between the right node value and the true label
    of of each instance in the right node <math alttext="sigma-summation Underscript
    upper R i g h t upper N o d e upper I n s t a n c e s Endscripts StartAbsoluteValue
    y Subscript t r u e Superscript i Baseline minus y Subscript r i g h t Baseline
    EndAbsoluteValue squared"><mrow><msub><mo>∑</mo> <mrow><mi>R</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <msup><mrow><mo>|</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo><msub><mi>y</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math> .
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A weighted average of the above two sums, where each node is weighted by its
    size relative to the parent node, just like we did for the Gini impurity:'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartFraction n Subscript l e f t Baseline Over n
    EndFraction sigma-summation Underscript upper L e f t upper N o d e upper I n
    s t a n c e s Endscripts StartAbsoluteValue y Subscript t r u e Superscript i
    Baseline minus y Subscript l e f t Baseline EndAbsoluteValue squared plus StartFraction
    n Subscript r i g h t Baseline Over n EndFraction sigma-summation Underscript
    upper R i g h t upper N o d e upper I n s t a n c e s Endscripts StartAbsoluteValue
    y Subscript t r u e Superscript i Baseline minus y Subscript r i g h t Baseline
    EndAbsoluteValue squared right-parenthesis period dollar-sign"><mrow><mfrac><msub><mi>n</mi>
    <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub> <mi>n</mi></mfrac>
    <msub><mo>∑</mo> <mrow><mi>L</mi><mi>e</mi><mi>f</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <mrow><mo>|</mo></mrow> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mo>+</mo> <mfrac><msub><mi>n</mi>
    <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub> <mi>n</mi></mfrac>
    <msub><mo>∑</mo> <mrow><mi>R</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <mrow><mo>|</mo></mrow> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mrow><msup><mo>|</mo> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: The above algorithm is *greedy* and computation heavy, in the sense that it
    has to do this for *each feature and each possible feature split value*, then
    choose the feature and feature split that provide the smallest weighted squared
    error average between the left and right children nodes.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: The CART (Classification And Regression Tree) algorithm is famous algorithm
    used by software packages, including Python’s Scikit Learn which we use in the
    Jupyter Notebooks supplementing this book. This algorithm produces trees whose
    nodes only have two children (binary trees) where the test at each node only has
    Yes or No answers. Other algorithms such as ID3 can produce trees with nodes that
    have two or more children.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of Decision Trees
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Decision trees are very easy to interpret and are popular for many good reasons:
    They adapt to large data sets, different data types (discrete and continuous features,
    no scaling of data needed), and can perform both regression and classification
    tasks. However, they can be unstable, in the sense that adding just one instance
    to the data set can change the tree at its root and hence result in a very different
    decision tree. They are also sensitive to rotations in the data, since their decision
    boundaries are usually horizontal and vertical (not slanted like support vector
    machines). This is because splits usually happen at specific feature values, so
    the decision boundaries end up parallel to the feature axes. One fix to that is
    to transform the data set to match its *pricipal axes*, using the *singular value
    decomposition method* presented later in [Chapter 11](ch11.xhtml#ch11). Decision
    trees tend to overfit the data so they need pruning. This is usually done using
    statistical tests. The greedy algorithms involved in constructing the trees, where
    the search happens over all features and their values makes them computationally
    expensive and less accurate. Random forests, discussed next, address some of these
    shortcomings.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When I first learned about decision trees, the most perplexing aspects for
    me were:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: How do we start the tree, meaning how do we decide which data feature is the
    root feature?
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At what particular feature value do we decide to split a node?
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When do we stop?
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, how do we grow a tree?
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Note that we answered some of the above questions in the previous subsection.)
    It didn’t make matters any easier that I would surf the internet looking for answers,
    only to encounter declarations that decision trees are so easy to build and understand,
    so it felt like I was the only one deeply confused with decision trees.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: 'My puzzlement instantly disappeared when I learned about *random forests*.
    The amazing thing about random forests is that we can get incredibly good regression
    or classification results, *without* answering any of my bewildering questions.
    Randomizing the whole process, meaning, building many decision trees while answering
    all my questions with two words: *choose randomly*, then aggregating their predictions
    in an ensemble produces very good results, even better than one carefully crafted
    decision tree. It has been said that *randomization often produces reliability*!'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Another very useful property of random forests is that they give a measure of
    *feature importance*, helping us pinpoint which features significantly affect
    our predictions, and aid in feature selection as well.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: k-means Clustering
  id: totrans-484
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common goal of data analysts is to partiton data into *clusters*, each cluster
    highlighting certain common traits. *k-means clustering* is a common machine learning
    method that partitions *n* data points (vectors) into *k* clusters, where each
    data point gets assigned to belongs to the cluster with the nearest mean. The
    mean of each cluster, or its centroid, serves as the prototype of the cluster.
    Overall, k-means clustering minimizes the variance (the squared Euclidean distances
    to the mean) within each cluster.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common algorithm for k-means clustering is iterative:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with an initial set of *k* means. This means that we specify the number
    of clusters ahead of time, which raises the question: How to initialize it? How
    to select the locations of the first *k* centroids? There is literature on that.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign each data point to the cluster with the nearest mean in terms of squared
    Euclidean distance.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recalculate the means of each cluster.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm converges when the data point assignments to each cluster do not
    change anymore.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Performance Measures For Classification Models
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is relatively easy to develop mathematical models that compute things and
    produce outputs. It is a completely different story to develop models that perform
    well for our desired tasks. Furthermore, models that perform well according to
    some metrics behave badly according to some other metrics. We need extra care
    developing performance metrics and deciding which ones to rely on, depending on
    our specific use cases.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance of models that predict numerical values, such as regression
    models, is easier than classificaion models, since we have many ways to compute
    distances between numbers (good predictions and bad predictions). On the other
    hand, when our task is classification (we can use models such as logistic regression,
    softmax regression, support vector machines, decision trees, random forests, or
    neural networks), we have to put some extra thought into evaluating performance.
    Moreover, there are usually tradeoffs. For example, if our task is to classify
    YouTube videos as being safe for kids (positive) or not safe for kids (negative),
    do we tweak our model so as to reduce the number of false positives or false negatives?
    It is obviously more problematic if a video is classified as safe while in reality
    it is unsafe (false positive) than the other way around, so our performance metric
    needs to reflect that.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: The following are the performance meausures commonly used for classification
    models. Do not worry about memorizing their names, as the way they are named do
    not make logical sense. Spend your time instead understanding their meanings.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Percentage of times prediction model got the classification right:'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A c c u r a c y equals StartFraction true positives
    plus true negatives Over all predicted positives plus all predicted negatives
    EndFraction dollar-sign"><mrow><mi>A</mi> <mi>c</mi> <mi>c</mi> <mi>u</mi> <mi>r</mi>
    <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext><mtext>+</mtext><mtext>true</mtext><mtext>negatives</mtext></mrow>
    <mrow><mtext>all</mtext><mtext>predicted</mtext><mtext>positives+</mtext><mtext>all</mtext><mtext>predicted</mtext><mtext>negatives</mtext></mrow></mfrac></mrow></math>
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix**: Counting all true positives, false positives, true negatives,
    and false negatives.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| True Negative | False Positive |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| False Negative | True Positive |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '**Precision score**: Accuracy of positive predictions:'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper P r e c i s i o n equals StartFraction true
    positives Over all predicted positives EndFraction equals StartFraction true positives
    Over true positives plus false positives EndFraction dollar-sign"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>c</mi> <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow> <mrow><mtext>all</mtext><mtext>predicted</mtext><mtext>positives</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow> <mrow><mtext>true</mtext><mtext>positives</mtext><mo>+</mo><mtext>false</mtext><mtext>positives</mtext></mrow></mfrac></mrow></math>
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall score**: Ratio of the positive instances that are correctly classified:'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper R e c a l l equals StartFraction true positives
    Over all positives labels EndFraction equals StartFraction true positives Over
    true positives plus false negatives EndFraction dollar-sign"><mrow><mi>R</mi>
    <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi> <mi>l</mi> <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow>
    <mrow><mtext>all</mtext><mtext>positives</mtext><mtext>labels</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow> <mrow><mtext>true</mtext><mtext>positives</mtext><mo>+</mo><mtext>false</mtext><mtext>negatives</mtext></mrow></mfrac></mrow></math>
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '**Specificity**: Ratio of the negative instances that are correctly classified:'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper S p e c i f i c i t y equals StartFraction
    true negatives Over all negative labels EndFraction equals StartFraction true
    negatives Over true negatives plus false positives EndFraction dollar-sign"><mrow><mi>S</mi>
    <mi>p</mi> <mi>e</mi> <mi>c</mi> <mi>i</mi> <mi>f</mi> <mi>i</mi> <mi>c</mi> <mi>i</mi>
    <mi>t</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>negatives</mtext></mrow>
    <mrow><mtext>all</mtext><mtext>negative</mtext><mtext>labels</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>negatives</mtext></mrow> <mrow><mtext>true</mtext><mtext>negatives</mtext><mo>+</mo><mtext>false</mtext><mtext>positives</mtext></mrow></mfrac></mrow></math>
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: '**<math alttext="upper F 1"><msub><mi>F</mi> <mn>1</mn></msub></math> score**:
    This quantity is only high when both precision and recall scores are high:'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper F 1 equals StartStartFraction 2 OverOver StartFraction
    1 Over p r e c i s i o n EndFraction plus StartFraction 1 Over r e c a l l EndFraction
    EndEndFraction dollar-sign"><mrow><msub><mi>F</mi> <mn>1</mn></msub> <mo>=</mo>
    <mfrac><mn>2</mn> <mrow><mfrac><mn>1</mn> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mfrac></mrow></math>
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'AUC (Area Under the Curve) and ROC (Receiver Operating Characteristics) curves:
    These curves provide a performance measure for a classification model at various
    threshold values. We can use these curves to measure how well a certain variable
    predicts a certain outcome, for example, how well does the GRE subject test score
    predict passing a graduate school’s qualifying exam in the first year?'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrew Ng’s hundred page book: Machine Learning Yearning provides an excellent
    guide for performance metrics’ best practices. Please read carefully before diving
    into real AI applications, since the book’s recipes are based on many trials,
    successes, and failures.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Looking Ahead
  id: totrans-510
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we surveyed some of the most popular machine learning models,
    emphasizing a particular mathematical structure that appears throughout the book:
    training function, loss function and optimization. We discussed linear, logistic,
    and softmax regression, then quickly breezed over support vector machines, decision
    trees, ensembles and random forests.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we made a descent case for studying the following topics from mathematics:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Calculus
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: The minimum and maximum happen at the boundary or at points where one derivative
    is zero or does not exist.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: Linear Algebra
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: 'Linearly combining features: <math alttext="omega 1 x 1 plus omega 2 x 2 plus
    ellipsis plus omega Subscript n Baseline x Subscript n"><mrow><msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
    .'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing various mathematical expressions using matrix and vector notation.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scalar product of two vectors <math alttext="ModifyingAbove a With right-arrow
    Superscript t Baseline ModifyingAbove b With right-arrow"><mrow><msup><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    .
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The <math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math> norm
    of a vector.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid working with ill conditioned matrices. Get rid of linearly dependent features.
    This also has to do with feature selection.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid multiplying matrices by each other, this is too expensive. Multiply matrices
    by vectors instead.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: For convex functions we do not worry about getting stuck at local minima since
    local minima are also global minima. We do worry about narrow valleys (next chapter).
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent methods, they need only one derivative (next chapter).
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newton’s methods, they need two derivatives or an approximation of two derivatives
    (inconvenient for large data).
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quadratic programming, the dual problem, and coordinate descent (all appear
    in support vector machines).
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Correlation matrix and scatterplots.
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F-test and Mutual Information for feature selection.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standarizing the data features (subtracting the mean and dividing by the standard
    deviation).
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More steps we did not and will not go over (yet):'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Validating our models- tune the weight values and the hyper-parameters so as
    not to overfit.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the trained model on the testing subset of the data, which our model had
    not used (or seen) during the training and validation steps (we do this in the
    accompanying Jupyter notebook).
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy and monitor the finalized model.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Never stop thinking on how to improve our models and how to better integrate
    them into the whole production pipeline.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we step into the new and exciting era of neural networks.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
