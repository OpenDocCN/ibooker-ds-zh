- en: Chapter 3\. Fitting Functions to Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Today it fits. Tomorrow?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In this chapter, we introduce the core mathematical ideas lying at the heart
    of many AI applications, including the mathematical engines of neural networks.
    Our goal is to internalize the following structure of the machine learning part
    of an AI problem:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Identify the problem, depending on the specific use case: Classify images,
    classify documents, predict house prices, detect fraud or anomalies, recommend
    the next product, predict the likelihood of a criminal re-offending, predict the
    internal structure of a building given external images, convert speech to text,
    generate audio, generate images, generate video, *etc*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acquire the appropriate data, in order to *train* our models to do the right
    thing. We say that our models *learn* from the data. Make sure this data is clean,
    complete, and if necessary, depending on the specific model we are implementing,
    transformed (normalized, standarized, some features aggregated, *etc.*). This
    step is usually way more time consuming than implementing and training the machine
    learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a *hypothesis function*. We use the terms hypothesis function, *learning
    function*, *prediction function*, *training function*, and *model* interchangeably.
    Our main assumption is that this input/output mathematical function explains the
    observed data, and it can be used later to make predictions on new data. We give
    our model features, like a person’s daily habits, and it returns a prediction,
    like this person’s likelihood to pay back a loan. In this chapter, we will give
    our model the length measurements of a fish, and it will return its weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will encounter many models (including neural networks) where our training
    function has unknown parameters called *weights*. The goal is to find the numerical
    values of these weights using the data. After we find these weight values, we
    can use the *trained* function to make predictions, by plugging the features of
    a new data point into the formula of the trained function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to find the values of the unknown weights, we create *another function*
    called the *error function*, the *cost function*, the *objective function*, or
    the *loss function* (everything in the AI field has three or more names). This
    function has to measure some sort of distance between the ground truth and our
    predictions. Naturally, we want our predictions to be as close to ground truths
    as possible, so we search for weight values that minimize our loss function. Mathematically,
    we solve a minimization problem. The field of *mathematical optimization* is essential
    to AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this process, we are the engineers, so it is us who get to decide
    on the mathematical formulas for training functions, loss functions, optimization
    methods, and computer implementations. Different engineers decide on different
    processes, with different performance results, and that is okay. The judge, in
    the end, is the performance of the deployed model, and contrary to popular belief,
    mathematical models are flexible and can be tweaked and altered when needed. It
    is crucial to monitor performance after deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since our goal is to find the weight values that minimize the error between
    our predictions and ground truths, we need to find an efficient mathematical way
    to search for these *minimizers*: Those special weight values that produce the
    least error. The *gradient descent* method plays a key role here. This powerful
    yet simple method involves calculating *one derivative* of our error function.
    This is one reason we spent half of our calculus classes calculating derivatives
    (and the gradient: this is one derivative in higher dimensions). There are other
    methods that require computing two derivatives. We will encounter them and comment
    on the benefits *vs.* the downsides of using higher order methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When data sets are enormous and our model happens to be a layered neural network,
    we need an efficient way to calculate this one derivative. The *backpropagation
    algorithm* steps in at this point. We will walk through gradient descent and backpropagation
    in the next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If our learning function fits the given data too well, then it will not perform
    well on new data. The reason is that a function with too good of a fit with the
    data means that it picks up on the noise in the data as well as the signal (for
    example, the function on the left of [Figure 3-1](#Fig_noise_fit_regular_fit)).
    We do not want to pick up on noise. This is where *regularization* helps. There
    are multiple mathematical ways to regularize a function, which means make it smoother
    and less oscillatory and erratic. In general, a function that follows the noise
    in the data oscillates too much. We desire more regular functions. We visit regularization
    techniques in the next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![280](assets/emai_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-1\. Left: The fitting function fits the data perfectly, however, it
    is not a good prediction function since it fits the noise in the data istead of
    the main signal. Right: A more regular function fitting the same data set. Using
    this function will give better predictions than the function in the left subplot,
    even though the function in the left subplot matches the data points better.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the following sections, we explore the above structure of an AI problem with
    real, but simple, data sets. We will see in the next chapters how the same concepts
    generalize to much more involved tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional And Very Useful Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the data used in this chapter is *labeled* with ground truths, and the goal
    of our models is to *predict* the labels of new (unseen) and unlabeled data. This
    is *supervised* learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next few sections, we fit training functions into our labeled data using
    the following popular machine learning models. While you may hear so much about
    the latest and greatest developments in AI, you are probably better off in a typical
    business setting starting with these more traditional models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear Regression**: Predict a numerical value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Logistic Regression**: Classify into two classes (binary classification).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Softmax Regression**: Classify into multiple classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Support Vector Machines**: Classify into two classes, or regression (predict
    a numerical value).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decision Trees**: Classify into any number of classes, or regression (predict
    a numerical value).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Random Forests**: Classify into any number of classes, or regression (predict
    a numerical value).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ensembles of models**: Bundle up the results of many models, by averaging
    the prediction values, voting for the most popular class, or some other bundling
    mechanism.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We try multiple models on the same data sets in order to compare performance.
    In the real world, it is rare that any model ever gets deployed without having
    been compared with many other models. This is the nature of the computation heavy
    AI industry, and it is why we need parallel computing, which ebables us to train
    multiple models at once (except for models that build and improve on the results
    of other models, like in the case of *stacking*. For those we cannot use parallel
    computing).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into any machine learning models, it is extremely important
    to note that it has been reported again and again that only about five percent
    of a data scientist’s time, and/or an AI researcher’s time, is spent on training
    machine learning models. The majority of the time is consumed by aquiring data,
    cleaning data, organizing data, creating appropriate pipelines for data, *etc.*,
    *before* feeding it data into machine learning models. So machine learning is
    only one step in the production process, and it is an easy step once the data
    is ready to train the model. We will discover how these machine learning models
    work: Most of the mathematics we need resides in these models. AI researchers
    are always trying to enhance machine learning models, and automatically fit them
    into production pipelines. It is therefore important for us to eventually learn
    about the whole pipeline, from raw data (including its storage, hardware, query
    protocols, *etc.*) to deployment to monitoring. Learning machine learning is only
    one piece of a bigger and more interesting story.'
  prefs: []
  type: TYPE_NORMAL
- en: We must start with *regression* since the ideas of regression are so fundamental
    for most of the AI models and applications that will follow. Only for *linear
    regression*, we find our minimizing weights using an *analytical* method, giving
    an explicit formula for the desired weights directly in terms of the training
    data set and its the target labels. It is the simplicity of the linear regression
    model that allows for this explicit analytical solution. Most other models do
    not have such explicit solutions and we have to find their minimizers using numerical
    methods, among which the gradient descent is extremely popular.
  prefs: []
  type: TYPE_NORMAL
- en: 'In regression and many other upcoming models, including the neural networks
    of the next few chapters, watch for the following progression in the modeling
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: The Training Function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Loss Function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Numerical Solutions *vs.* Analytical Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to be aware of the difference between numerical solutions and
    analytical solutions of mathematical problems. A mathematical problem can be anything,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the minimizer of some function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the best way to go from destination *A* to destination *B*, with a constrained
    budget.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the best way to design and query a data warehouse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the solution of a mathematical equation (where a left hand side with math
    stuff *equals* a right hand side with math stuff). These equations could be algebraic
    equations, ordinary differential equations, partial differential equations, integro-differential
    equations, systems of equations, or any sort of mathematical equations. Their
    solutions could be static or evolving in time. They could model anything from
    the physical, biological, socioeconomical or natural worlds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Numerical*: has to do with numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Analytical*: has to do with analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a rule of thumb, numerical solutions are much easier to obtain and much more
    accessible than analytical solutions, provided that we have enough computational
    power to simulate and compute these solutions. All we usually need to do is discretize
    some continuous spaces and/or functions, albeit sometimes in very clever ways,
    and evaluate functions on these discrete quantities. The only problem with numerical
    solutions is that they are only approximate solutions. Unless they are backed
    by estimates on how far off they are from the true analytical solutions and how
    fast they converge to these true solutions, which in turn require mathematical
    backgroud and analysis, numerical solutions are not exact. They do however provide
    incredibly useful insights about the true solutions. In many cases, numerical
    solutions are the only ones available, and many scientific and engineering fields
    would not have advanced at all had they not relied on numerical solutions of complex
    problems. If those fields waited for analytical solutions and proofs to happen,
    or in other words for mathematical theory to *catch up*, they would’ve had very
    slow progress.
  prefs: []
  type: TYPE_NORMAL
- en: Analytical solutions, on the other hand, are exact, robust, and have a whole
    mathematical theory backing them up. They come accompanied with theorems and proofs.
    When analytical solutions are available they are very powerful. They are, however,
    not easily accessible, sometimes impossible to obtain, and they do require deep
    knowledge and domain expertise in fields such as calculus, mathematical analysis,
    algebra, theory of differential equations, *etc*. Analytical methods, however,
    are extremely valuable for describing important properties of solutions (even
    when explicit solutions are not available), guiding numerical techniques, and
    providing ground truths to compare approximate numerical methods against (in the
    lucky cases when these analytical solutions are available).
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers are purely analytical and theoretical, others are purely numerical
    and computational, and the best place to exist is somewhere near the intersection,
    where we have a descent understanding of the analytical and the numerical aspects
    of our mathematical problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regression: Predict A Numerical Value'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A quick search on [Kaggle website](https://www.kaggle.com) for data sets for
    regression returns many excellent data sets and related notebooks. I randomly
    chose a simple [Fish Market](https://www.kaggle.com/aungpyaeap/fish-market) data
    set which we will use to explain our upcoming mathematics. Our goal is to build
    a model that predicts the weight of a fish given its five different length measurements,
    or features, labeled in the data set as: Length1, Length2, Length3, Height, and
    Width (see [Figure 3-2](#Fig_fish_data)). For the sake of simplicity, we choose
    not to incorporate the categorical feature, Species, into this model, even though
    we could (and that would give us better predictions, since a fish type is a good
    predictor of its weight). If we choose to include the Species feature then we
    would have to convert its values into numerical values, using *one hot coding*,
    which means exactly as it sounds: Assign a code for each fish made up of ones
    and zeros based on its category (type). Our Species feature has seven categories:
    Perch, Bream, Roach, Pike, Smelt, Parkki, and Whitefish. So if our fish is Pike
    the we would code its species as (0,0,0,1,0,0,0) and if it is Bream we would code
    its species as (0,1,0,0,0,0,0). Of course this adds seven more dimensions to our
    feature space and seven more weights to train.'
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. The first five rows of the fish data set downloaded from Kaggle’s
    [Fish Market](https://www.kaggle.com/aungpyaeap/fish-market). The Weight column
    is the target feature, and our goal is to build a model that predicts the weight
    of a new fish given its length measurements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s save ink space and relabel our five features as: <math alttext="x 1"><msub><mi>x</mi>
    <mn>1</mn></msub></math> , <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math>
    , <math alttext="x 3"><msub><mi>x</mi> <mn>3</mn></msub></math> , <math alttext="x
    4"><msub><mi>x</mi> <mn>4</mn></msub></math> , and <math alttext="x 5"><msub><mi>x</mi>
    <mn>5</mn></msub></math> , then write the fish weight as a function of these five
    features <math alttext="y equals f left-parenthesis x 1 comma x 2 comma x 3 comma
    x 4 comma x 5 right-parenthesis"><mrow><mi>y</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>4</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>5</mn></msub> <mo>)</mo></mrow></math> . This
    way, once we settle on an acceptable formula for the this function, all we have
    to do is input the feature values for a certain fish and our function will output
    the predicted weight of that fish.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section builds a foundation for everything to come, so it is important
    to first see how it is organized:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Function**'
  prefs: []
  type: TYPE_NORMAL
- en: Parametric models *vs.* non-parametric models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**'
  prefs: []
  type: TYPE_NORMAL
- en: The predicted value *vs.* the true value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The absolute value distance *vs.* the squared distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions with singularities (pointy points)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For linear regression, the loss function is the Mean Squared Error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectors in this book are always column vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Training, Validation and Test Subsets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the training data has highly correlated features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**'
  prefs: []
  type: TYPE_NORMAL
- en: Convex landscapes *vs.* non-convex lanscapes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we locate minimizers of functions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculus in a nutshell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A one-dimensional optimization example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derivatives of linear algebra expressions that we use all the time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing the mean squared error loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caution: Multiplying large matrices by each other is very expensive. Multiply
    matrices by vectors instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caution: We never want to fit the training data too well'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A quick exploration of the data, as in plotting the weight against the various
    length features, allows us to assume a linear model (even though a nonlinear one
    could be better in this case). That is, we assume that the weight depends linearly
    on the length features (see [Figure 3-3](#Fig_weight_lengths_scatterplots)).
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Scatterplots of the Fish Market numerical features. For more details,
    check out the attached Jupyter Notebook, or some of the public [notebooks on Kaggle](https://www.kaggle.com/aungpyaeap/fish-market/code)
    associated with this data set.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This means that the weight of a fish, y, can be computed using a *linear combination*
    of its five different length measurements, plus a bias term <math alttext="omega
    0"><msub><mi>ω</mi> <mn>0</mn></msub></math> , giving the following *training
    function*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: After our major decision in the modeling process to use a linear training function
    <math alttext="f left-parenthesis x 1 comma x 2 comma x 3 comma x 4 comma x 5
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>4</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>5</mn></msub>
    <mo>)</mo></mrow></math> , all we have to do is to find the appropriate values
    of the parameters <math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math>
    , <math alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math> , <math alttext="omega
    2"><msub><mi>ω</mi> <mn>2</mn></msub></math> , <math alttext="omega 3"><msub><mi>ω</mi>
    <mn>3</mn></msub></math> , <math alttext="omega 4"><msub><mi>ω</mi> <mn>4</mn></msub></math>
    , and <math alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math> . We will
    *learn* the best values for our <math alttext="omega"><mi>ω</mi></math> ’s from
    the data. The process of using the data in order to find the appropriate <math
    alttext="omega"><mi>ω</mi></math> ’s is called *training* the model. A *trained*
    model is then a model where the <math alttext="omega"><mi>ω</mi></math> values
    have been decided on.
  prefs: []
  type: TYPE_NORMAL
- en: In general, training functions, whether linear or nonlinear, including those
    representing neural networks, have unknown parameters <math alttext="omega"><mi>ω</mi></math>
    ’s that we need to learn from the given data. For linear models, each parameter
    gives each feature a certain weight in the prediction process. So if the value
    of <math alttext="omega 2"><msub><mi>ω</mi> <mn>2</mn></msub></math> is larger
    than the value of <math alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math>
    , then the second feature plays a more important role than the fifth feature in
    our prediction, assuming that the second and fifth features have comparable scales.
    This is one of the reasons it is good to scale or normalize the data before training
    the model. If on the other hand the value <math alttext="omega 3"><msub><mi>ω</mi>
    <mn>3</mn></msub></math> associated with the third feature dies, meaning becomes
    zero or negligible, then the third feature can be omitted from the data set as
    it plays no role in our predictions. Therefore, learning our <math alttext="omega"><mi>ω</mi></math>
    ’s from the data allows us to mathematically compute the contribution of each
    feature to our predictions (or the importance of feature combinations if some
    features were combined during the data preparation stage, before training). In
    other words, the models learn how the data features interact and how strong these
    interactions are. The moral is that through a trained learning function, we can
    quantify how features come together in order to produce both observed and yet-to-be
    observed results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Parametric models vs non-parametric models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A model that has parameters (we are calling them weights) pre-built into its
    formula, such as the <math alttext="omega"><mi>ω</mi></math> ’s in our current
    linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: (and later the <math alttext="omega"><mi>ω</mi></math> ’s of neural networks)
    is called a *parametric model*. This means that we fix the formula of the training
    function ahead of the actual training, and all the training does is solve for
    the parameters that are involved in the formula. Fixing the formula ahead of time
    is analogous to specifying the *family* that a training function belongs to, and
    finding the parameter values specifies the exact member of that family that best
    explains the data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Non-parametric* models, such as decision trees and random forests that we
    will discuss later in this chapter, do not specify the formula for the training
    function with its parameters ahead of time. So when we train a non-parametric
    model, we do not know how many parameters the trained model will end up having.
    The model *adapts* to the data and determines the required amount of parameters
    depending on the data. Careful here, the bells of over-fitting are ringing! Recall
    that we don’t want our models to adapt to the data too much, because they might
    not generalize well to unseen data. These models are usually accompanied with
    techniques that help them avoid overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Both parametric and non-parametric models have *other parameters* called *hyperparameters*
    that also need to be tuned during the training process. These however are not
    built into the formula of the training function (and don’t end up in the formula
    of a non-parametric’s model either). We will encounter plenty of hyperparameters
    throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have convinced ourselves that the next logical step is finding suitable values
    for the <math alttext="omega"><mi>ω</mi></math> ’s that appear in the training
    function (of our linear parametric model), using the data that we have. In order
    to do that, we need to *optimize an appropriate loss function*.
  prefs: []
  type: TYPE_NORMAL
- en: The predicted value *vs*. the true value
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose that we assign some random numerical values for each of our unknown
    <math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math> , <math alttext="omega
    1"><msub><mi>ω</mi> <mn>1</mn></msub></math> , <math alttext="omega 2"><msub><mi>ω</mi>
    <mn>2</mn></msub></math> , <math alttext="omega 3"><msub><mi>ω</mi> <mn>3</mn></msub></math>
    , <math alttext="omega 4"><msub><mi>ω</mi> <mn>4</mn></msub></math> , and <math
    alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math> , say for example
    <math alttext="omega 0 equals negative 3"><mrow><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>=</mo> <mo>-</mo> <mn>3</mn></mrow></math> , <math alttext="omega 1 equals
    4"><mrow><msub><mi>ω</mi> <mn>1</mn></msub> <mo>=</mo> <mn>4</mn></mrow></math>
    , <math alttext="omega 2 equals 0.2"><mrow><msub><mi>ω</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn></mrow></math> , <math alttext="omega
    3 equals 0.03"><mrow><msub><mi>ω</mi> <mn>3</mn></msub> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>03</mn></mrow></math> , <math alttext="omega 4 equals 0.4"><mrow><msub><mi>ω</mi>
    <mn>4</mn></msub> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn></mrow></math> ,
    and <math alttext="omega 5 equals 0.5"><mrow><msub><mi>ω</mi> <mn>5</mn></msub>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math> . Then the formula for
    the linear training function <math alttext="y equals omega 0 plus omega 1 x 1
    plus omega 2 x 2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub></mrow></math> becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals negative 3 plus 4 x 1 plus 0.2 x 2 plus
    0.03 x 3 plus 0.4 x 4 plus 0.5 x 5 dollar-sign"><mrow><mi>y</mi> <mo>=</mo> <mo>-</mo>
    <mn>3</mn> <mo>+</mo> <mn>4</mn> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>2</mn> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>03</mn> <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>4</mn> <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>5</mn> <msub><mi>x</mi> <mn>5</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and is ready to make predictions: Plug in numerical values for the length features
    of the <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    fish, then obtain a predicted value for the weight of this fish. For example,
    the first fish in our data set is a bream and has length measurements <math alttext="x
    1 Superscript 1 Baseline equals 23.2"><mrow><msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>23</mn> <mo>.</mo> <mn>2</mn></mrow></math> , <math alttext="x
    2 Superscript 1 Baseline equals 25.4"><mrow><msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>25</mn> <mo>.</mo> <mn>4</mn></mrow></math> , <math alttext="x
    3 Superscript 1 Baseline equals 30"><mrow><msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>30</mn></mrow></math> , <math alttext="x 4 Superscript 1 Baseline
    equals 11.52"><mrow><msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup> <mo>=</mo>
    <mn>11</mn> <mo>.</mo> <mn>52</mn></mrow></math> , and <math alttext="x 5 Superscript
    1 Baseline equals 4.02"><mrow><msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup>
    <mo>=</mo> <mn>4</mn> <mo>.</mo> <mn>02</mn></mrow></math> . Plugging these into
    the training function, we get the prediction for the weight of this fish:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column y Subscript p r e
    d i c t Superscript 1 2nd Column equals omega 0 plus omega 1 x 1 Superscript 1
    Baseline plus omega 2 x 2 Superscript 1 Baseline plus omega 3 x 3 Superscript
    1 Baseline plus omega 4 x 4 Superscript 1 Baseline plus omega 5 x 5 Superscript
    1 Baseline 2nd Row 1st Column Blank 2nd Column equals negative 3 plus 4 left-parenthesis
    23.2 right-parenthesis plus 0.2 left-parenthesis 25.4 right-parenthesis plus 0.03
    left-parenthesis 30 right-parenthesis plus 0.4 left-parenthesis 11.52 right-parenthesis
    plus 0.5 left-parenthesis 4.02 right-parenthesis 3rd Row 1st Column Blank 2nd
    Column equals 102.398 grams period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mn>3</mn> <mo>+</mo>
    <mn>4</mn> <mo>(</mo> <mn>23</mn> <mo>.</mo> <mn>2</mn> <mo>)</mo> <mo>+</mo>
    <mn>0</mn> <mo>.</mo> <mn>2</mn> <mo>(</mo> <mn>25</mn> <mo>.</mo> <mn>4</mn>
    <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>03</mn> <mo>(</mo> <mn>30</mn>
    <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>(</mo> <mn>11</mn>
    <mo>.</mo> <mn>52</mn> <mo>)</mo> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn>
    <mo>(</mo> <mn>4</mn> <mo>.</mo> <mn>02</mn> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mn>102</mn> <mo>.</mo> <mn>398</mn> <mtext>grams.</mtext></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, for the <math alttext="i Superscript t h"><msup><mi>i</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    fish, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y Subscript p r e d i c t Superscript i Baseline
    equals omega 0 plus omega 1 x 1 Superscript i Baseline plus omega 2 x 2 Superscript
    i Baseline plus omega 3 x 3 Superscript i Baseline plus omega 4 x 4 Superscript
    i Baseline plus omega 5 x 5 Superscript i dollar-sign"><mrow><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>i</mi></msubsup> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mi>i</mi></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The fish under consideration, however, has a certain *true* weight, <math alttext="y
    Subscript t r u e Superscript i"><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup></math> , which is its label if it belongs in the labeled
    data set. For the first fish in our data set, the true weight is <math alttext="y
    Subscript t r u e Superscript 1 Baseline equals 242"><mrow><msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mn>1</mn></msubsup> <mo>=</mo>
    <mn>242</mn></mrow></math> *grams*. Our linear model with randomly chosen <math
    alttext="omega"><mi>ω</mi></math> values predicted 102.398 *grams*. This is of
    course pretty far off, since we did not calibrate the <math alttext="omega"><mi>ω</mi></math>
    values at all. In any case, we can measure the *error* between the weight predicted
    by our model and the true weight, then find ways to do better in terms of our
    choices for the <math alttext="omega"><mi>ω</mi></math> ’s.
  prefs: []
  type: TYPE_NORMAL
- en: The absolute value distance *vs* the squared distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the nice things about mathematics is that it has multiple ways to measure
    how far off things are from each other, using different distance metrics. For
    example, we can naively measure the distance between two quantities as being one
    if they are different and zero if they are the same, encoding the words: different-1,
    similar-0\. Of course, using such a naive metric, we lose a ton of information,
    since the distance between quantities such as two and ten will be equal the distance
    between two and a million, namely 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some distance metrics that are popular in machine learning. We first
    introduce the two most commonly used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The absolute value distance: <math alttext="StartAbsoluteValue y Subscript
    p r e d i c t Baseline minus y Subscript t r u e Baseline EndAbsoluteValue"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mrow><mo>|</mo></mrow></mrow></math> , stemming from the calculus function <math
    alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math>
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The squared distance: <math alttext="StartAbsoluteValue y Subscript p r e d
    i c t Baseline minus y Subscript t r u e Baseline EndAbsoluteValue squared"><mrow><mrow><mo>|</mo></mrow>
    <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup></mrow></math> , stemming from
    the calculus function <math alttext="StartAbsoluteValue x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> (which is the same as <math alttext="x squared"><msup><mi>x</mi>
    <mn>2</mn></msup></math> for scalar quantities). Of course, this will square the
    units as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inspecting the graphs of the functions <math alttext="StartAbsoluteValue x
    EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> and <math
    alttext="x squared"><msup><mi>x</mi> <mn>2</mn></msup></math> in [Figure 3-4](#Fig_abs_x_square_x),
    we notice a great difference in function smoothness at the point (0,0). The function
    <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi>
    <mo>|</mo></mrow></math> has a corner at that point, rendering it undifferentiable
    at x=0\. This *singularity* of <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math> at x=0 turns many practitioners (and mathematicians!)
    away from incorporating this function, or functions with similar singularities,
    into their models. However, let’s engrave the following into our brains:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mathematical models are flexible*. When we encounter a hurdle we dig deeper,
    understand what’s going on, then we work around the hurdle.'
  prefs: []
  type: TYPE_NORMAL
- en: '![275](assets/emai_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-4\. Left: Graph of <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math> has a corner at <math alttext="x equals 0"><mrow><mi>x</mi>
    <mo>=</mo> <mn>0</mn></mrow></math> rendering its derivative undefined at that
    point. Right: Graph of <math alttext="StartAbsoluteValue x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> is smooth at <math alttext="x equals 0"><mrow><mi>x</mi>
    <mo>=</mo> <mn>0</mn></mrow></math> so its derivative has no problems there.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Other than the difference in the *regularity* of the functions <math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> and <math
    alttext="StartAbsoluteValue x EndAbsoluteValue squared"><msup><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow>
    <mn>2</mn></msup></math> (meaning whether they have derivatives at all points
    or not), there is one more point that we need to pay attention to before deciding
    to incorporate either function into our error formula: *If a number is large,
    then its square is even larger*. This simple observation means that if we decide
    to measure the error using squared distances between true values and predicted
    values, then our method will be *more sensitive to the outliers* in the data.
    One messed up outlier might skew our whole prediction function towards it, and
    hence away from the more prevelant patterns in the data. Ideally, we would’ve
    taken care of outliers and decided whether we should keep them or not during the
    data preparation step, before feeding the data into any machine learning model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One last difference between <math alttext="StartAbsoluteValue x EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>x</mi> <mo>|</mo></mrow></math> (and similar piecewise linear functions) and
    <math alttext="x squared"><msup><mi>x</mi> <mn>2</mn></msup></math> (and similar
    nonlinear but differentiable functions) is that the derivative of <math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> is very
    easy:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 if x>0, -1 if x<0 (and undefined if x=0).
  prefs: []
  type: TYPE_NORMAL
- en: In a model that involves billions of computational steps, this property where
    there is *no need to evaluate anything* when using the derivative of <math alttext="StartAbsoluteValue
    x EndAbsoluteValue"><mrow><mo>|</mo> <mi>x</mi> <mo>|</mo></mrow></math> proves
    extremely valuable. Derivatives of functions that are neither linear nor piecewise
    linear usually involve evaluations (because they also have *x*’s in their formulas
    and not only constants like in the piecewise linear case) which can be expensive
    in big data settings.
  prefs: []
  type: TYPE_NORMAL
- en: Functions With Singularities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, graphs of *differentiable* functions do not have cusps, kinks, corners,
    or anything pointy. If they do have such *singularities*, then the function has
    no derivative at these points. The reason is that at a pointy point, you can draw
    two different tangent lines to the graph of the function, depending on whether
    you decide to draw the tangent line to the left or to the right of the point (see
    [Figure 3-5](#Fig_singularity_tangents)). Recall that the derivative of a function
    at a point is the slope of the tangent line to the graph of the function at that
    point. If there are two *different* slopes then we cannot define the derivative
    at the point.
  prefs: []
  type: TYPE_NORMAL
- en: '![275](assets/emai_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. At singular points, the derivative does not exist. There are more
    than one possible slope of tangent at such points.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This *discontinuity* in the slope of the tangent creates a problem for methods
    that rely on evaluating the derivative of the function, such as the Gradient Descent
    method. The problem here is two fold:'
  prefs: []
  type: TYPE_NORMAL
- en: If you happen to land at a quirky pointy point, then the method doesn’t know
    what to do, since there is no defined derivative there. Some people assign a value
    for the derivative that at point (called the *subgradient* or the *subdifferential*)
    and move on. In reality, what are the odds that we will be unlucky enough to land
    exactly at that one horrible point? Unless the landscape of the function looks
    like the rough terrains of the Alps (actually many do), the numerical method might
    manage to avoid them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The other problem is instability. Since the value of the derivative jumps so
    abruptly as you traverse the landscape of the function across this point, a method
    using this derivative will abrupltly change value as well, creating instabilities
    if you are trying to converge somewhere. Imagine you are hiking down the Swiss
    Alps [Figure 3-6](#Fig_Swiss_Alps) (the landscape of the loss function) and your
    destination is that pretty little town that you can see down the valley (the place
    with the lowest error value). Then *suddenly* you get carried by some alien (the
    alien is the mathematical search method relying on this abruptly changing derivative)
    to the *other* side of the mountain, where you cannot see your destination anymore.
    In fact, now all you can see down the valley is some ugly shrubs, and an extremely
    narrow canyon that can trap you if your method carries you there. Your convergence
    to your original destination is now unstable, if not totally lost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![275](assets/emai_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-6\. Swiss Alps: Optimization is similar to hiking the landscape of
    a function.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Nevertheless, functions with such singularities are used all the time in machine
    learning. We will encounter them in the formulas of some neural network training
    functions (Rectified Linear Unit function- who names these?), some loss functions
    (absolute value distance), and in some regularizing terms (Lasso regression- who
    names these too?).
  prefs: []
  type: TYPE_NORMAL
- en: For linear regression, the loss function is the Mean Squared Error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Back to the main goal for this section: Constructing an error function, also
    called the *loss function*, which encodes how much error our model commits when
    making its predictions, and must be made small.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For linear regression, we use the *mean squared error function*. This function
    averages over the squared distance errors between the prediction and the true
    value for *m* data points (we will mention which data points to include here shortly):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Mean Squared Error equals StartFraction 1 Over m
    EndFraction left-parenthesis StartAbsoluteValue y Subscript p r e d i c t Superscript
    1 Baseline minus y Subscript t r u e Superscript 1 Baseline EndAbsoluteValue squared
    plus StartAbsoluteValue y Subscript p r e d i c t Superscript 2 Baseline minus
    y Subscript t r u e Superscript 2 Baseline EndAbsoluteValue squared plus ellipsis
    plus StartAbsoluteValue y Subscript p r e d i c t Superscript m Baseline minus
    y Subscript t r u e Superscript m Baseline EndAbsoluteValue squared right-parenthesis
    dollar-sign"><mrow><mtext>Mean</mtext> <mtext>Squared</mtext> <mtext>Error</mtext>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mrow><mo>|</mo></mrow>
    <msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mn>1</mn></msubsup> <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mrow><mo>+</mo>
    <mo>|</mo></mrow> <msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mn>2</mn></msubsup> <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mo>+</mo>
    <mo>⋯</mo> <mo>+</mo> <msup><mrow><mo>|</mo><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup> <mo>-</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>m</mi></msubsup> <mo>|</mo></mrow> <mn>2</mn></msup></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write the above expression more compactly using the sum notation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Mean Squared Error equals StartFraction 1 Over m
    EndFraction sigma-summation Underscript i equals 1 Overscript m Endscripts StartAbsoluteValue
    y Subscript p r e d i c t Superscript i Baseline minus y Subscript t r u e Superscript
    i Baseline EndAbsoluteValue squared dollar-sign"><mrow><mtext>Mean</mtext> <mtext>Squared</mtext>
    <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup> <msup><mrow><mo>|</mo><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Now we get into the great habit of using the even more compact linear algebra
    notation of vectors and matrices. This habit proves extremely handy in the field,
    as we don’t want to drown while trying to keep track of indices. Indices can sneak
    up into our rosy dreams of understanding everything and quickly transform them
    into very scary nightmares. Another very important reason to use the compact linear
    algebra notation is that both the software and the hardware built for machine
    learning models are optimized for matrix and *tensor* (think of an object made
    of layered matrices, like a three dimensional box instead of a flat square) computations.
    Moreover, the beautiful field of numerical linear algebra has worked through many
    potential problems and paved the way for us to enjoy the fast methods to perform
    all kinds of matrix computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using linear algebra notation, we can write the mean squared error as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The last equality introduces the <math alttext="l squared"><msup><mi>l</mi>
    <mn>2</mn></msup></math> *norm* of a vector, which by definition is just the <math
    alttext="StartRoot sum of squares of its components EndRoot"><msqrt><mrow><mtext>sum</mtext>
    <mtext>of</mtext> <mtext>squares</mtext> <mtext>of</mtext> <mtext>its</mtext>
    <mtext>components</mtext></mrow></msqrt></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Take home idea: *The loss function that we constructed encodes the difference
    between the predictions and the ground truths for the data points involved the
    training process, measured in some norm: a mathematical entity that acts as a
    distance*. There are many other norms that we could’ve used, but the <math alttext="l
    squared"><msup><mi>l</mi> <mn>2</mn></msup></math> *norm* is pretty popular.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notation: Vectors In This Book Are Always Column Vectors'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To be consistent in notation throughout the book, *all* vectors are column vectors.
    So if a vector <math alttext="ModifyingAbove v With right-arrow"><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover></math> has four components, the symbol <math alttext="ModifyingAbove
    v With right-arrow"><mover accent="true"><mi>v</mi> <mo>→</mo></mover></math>
    stands for <math alttext="Start 4 By 1 Matrix 1st Row  v 1 2nd Row  v 2 3rd Row  v
    3 4th Row  v 4 EndMatrix"><mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>v</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced></math> .
  prefs: []
  type: TYPE_NORMAL
- en: The transpose of a vector <math alttext="ModifyingAbove v With right-arrow"><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover></math> is then always a row vector.
    The transpose of the above vector with four components is <math alttext="ModifyingAbove
    v With right-arrow Superscript t Baseline equals Start 1 By 4 Matrix 1st Row 1st
    Column v 1 2nd Column v 2 3rd Column v 3 4th Column v 4 EndMatrix"><mrow><msup><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><msub><mi>v</mi> <mn>1</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>2</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>3</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>4</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'We will never use the dot product notation (also called the scalar product
    because we *multiply* two vectors but our answer is a scalar number). Instead
    of writing the dot product of two vectors <math alttext="ModifyingAbove a With
    right-arrow period ModifyingAbove b With right-arrow"><mrow><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>.</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    , we will write <math alttext="ModifyingAbove a With right-arrow Superscript t
    Baseline ModifyingAbove b With right-arrow"><mrow><msup><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    , which is the same thing, but in essence thinks of a column vector as a matrix
    of shape: *length of the vector by 1*, and its transpose as a matrix of shape:
    *1 by length of the vector*.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose now that <math alttext="ModifyingAbove a With right-arrow"><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></math> and <math alttext="ModifyingAbove b With right-arrow"><mover
    accent="true"><mi>b</mi> <mo>→</mo></mover></math> have four components then
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove a With right-arrow Superscript t Baseline
    ModifyingAbove b With right-arrow equals Start 1 By 4 Matrix 1st Row 1st Column
    a 1 2nd Column a 2 3rd Column a 3 4th Column a 4 EndMatrix Start 4 By 1 Matrix
    1st Row  b 1 2nd Row  b 2 3rd Row  b 3 4th Row  b 4 EndMatrix equals a 1 b 1 plus
    a 2 b 2 plus a 3 b 3 plus a 4 b 4 equals sigma-summation Underscript i equals
    1 Overscript 4 Endscripts a Subscript i Baseline b Subscript i Baseline period
    dollar-sign"><mrow><msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>b</mi> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msub><mi>a</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>2</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>3</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>b</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>b</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>b</mi> <mn>3</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>b</mi>
    <mn>4</mn></msub></mtd></mtr></mtable></mfenced> <mo>=</mo> <msub><mi>a</mi> <mn>1</mn></msub>
    <msub><mi>b</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>b</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>3</mn></msub>
    <msub><mi>b</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>4</mn></msub>
    <msub><mi>b</mi> <mn>4</mn></msub> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mn>4</mn></msubsup> <msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Moreover,
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign parallel-to ModifyingAbove a With right-arrow parallel-to
    equals ModifyingAbove a With right-arrow Superscript t Baseline ModifyingAbove
    a With right-arrow equals a 1 squared plus a 2 squared plus a 3 squared plus a
    4 squared period dollar-sign"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow> <msup><mi>l</mi> <mn>2</mn></msup>
    <mn>2</mn></msubsup> <mo>=</mo> <msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>=</mo>
    <msubsup><mi>a</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>a</mi>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>a</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msubsup><mi>a</mi> <mn>4</mn> <mn>2</mn></msubsup> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign parallel-to ModifyingAbove b With right-arrow parallel-to
    equals ModifyingAbove b With right-arrow Superscript t Baseline ModifyingAbove
    b With right-arrow equals b 1 squared plus b 2 squared plus b 3 squared plus b
    4 squared period dollar-sign"><mrow><mrow><mo>∥</mo></mrow> <mover accent="true"><mi>b</mi>
    <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow> <msup><mi>l</mi> <mn>2</mn></msup>
    <mn>2</mn></msubsup> <mo>=</mo> <msup><mover accent="true"><mi>b</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover> <mo>=</mo>
    <msubsup><mi>b</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>b</mi>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>b</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msubsup><mi>b</mi> <mn>4</mn> <mn>2</mn></msubsup> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This way, we use matrix notation throughout, and we only put an arrow above
    a letter to indicate that we are dealing with a column vector.
  prefs: []
  type: TYPE_NORMAL
- en: The Training, Validation and Test Subsets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Which data points do we include in our loss function? Do we include the whole
    data set, some small batches of it, or even only one point? Are we measuring this
    mean squared error for the data points in the *training subset*, the *validation
    subset*, or the *test subset*? And what are these subsets anyway?
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we split a data set into three subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *training* subset: This is the subset of the data that we use to fit our
    training function. This means that the data points in this subset are the ones
    that get incorporated into our loss function (by plugging their feature values
    and label into the <math alttext="y Subscript p r e d i c t"><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>
    and the <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    of the loss function).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The *validation* subset: The data points in this subset are used in multiple
    ways:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The common description is that we use this subset in order to *tune the hyperparameters
    of the machine learning model*. The hyperparameters are any parameters in the
    machine learning model that *are not* the <math alttext="omega"><mi>ω</mi></math>
    ’s of the training function that we are trying to solve for. In machine learning,
    there are many of these, and their values affect the results and the performance
    of the model. Examples of hyperparameters include (you don’t have to know what
    these are yet): The learning rate that appears in the gradient descent method,
    the hyperparameter that determines the width of the margin in support vector machine
    methods, the percentage of original data split into training, validation and test
    subsets, the batch size when doing randomized batch gradient descent, weight decay
    hyperparameters such as those used in Ridge, LASSO and Elastic Net regression,
    hyperparameters that come with momentum methods such as gradient descent with
    momentum and ADAM (these have terms that accelerate the convergence of the method
    towards the minimum and these terms are multiplied by hyperparameters that need
    to be tuned before testing and deployment), the architecture of a neural network
    such as the number of layers, the width of each layer, *etc.*, and the number
    of *epochs* during the optimization process (the number of passes over the entire
    training subset that the optimizer has seen).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The validation subset also helps us know when to *stop* optimizing *before*
    overfitting our training subset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It also serves as a test set to compare the performance of *different* machine
    learning models on the same data set, for example, comparing the performance of
    a linear regression model to a random forest to a neural network.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *test* subset: After deciding on the best model to use (or averaging or
    aggregating the results of multiple models) and training the model, we use this
    untouched subset of the data as a last stage test for our model before deployment
    into the real world. Since the model has not seen any of the data points in this
    subset before (which means it has not included any of them in the optimization
    process), it can be considered as the closest analogue to a real world situation.
    This allows us to judge the performance of our model before we start applying
    it on completely new real world data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s recap a little before moving forward.
  prefs: []
  type: TYPE_NORMAL
- en: Our current machine learning model is called *linear regression*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our training function is linear with formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 period dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The <math alttext="x"><mi>x</mi></math> ’s are the features, and the <math alttext="omega"><mi>ω</mi></math>
    ’s are the unknown weights or parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we plug in the feature values of a particular data point, for example the
    tenth data point, into the formula of the training function, we get our model’s
    prediction for this point:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y Subscript p r e d i c t Superscript 10 Baseline
    equals omega 0 plus omega 1 x 1 Superscript 10 Baseline plus omega 2 x 2 Superscript
    10 Baseline plus omega 3 x 3 Superscript 10 Baseline plus omega 4 x 4 Superscript
    10 Baseline plus omega 5 x 5 Superscript 10 Baseline period dollar-sign"><mrow><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>10</mn></msubsup> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>10</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>10</mn></msubsup>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The superscript 10 indicates that these are values corresponding to the tenth
    data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our loss function is the mean squared error function with formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We want to find the values of the <math alttext="omega"><mi>ω</mi></math> ’s
    that minimize this loss function. So the next step must be solving a minimization
    (optimization) problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to make our optimization life much easier, we will once again employ
    the convenient notation of linear algebra (vectors and matrices). This allows
    us to include the entire training subset of the data as a matrix in the formula
    of the loss function, and do our computations immediately on the training subset,
    as opposed to computing on each data point separately. This little notation maneuver
    saves us from a lot of mistakes, pain, and from tedious calculations with many
    components that are difficult to keep track of on very large data sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, write the prediction of our model corresponding to each data point of
    the training subset:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column y Subscript p r e
    d i c t Superscript 1 2nd Column equals 1 omega 0 plus omega 1 x 1 Superscript
    1 Baseline plus omega 2 x 2 Superscript 1 Baseline plus omega 3 x 3 Superscript
    1 Baseline plus omega 4 x 4 Superscript 1 Baseline plus omega 5 x 5 Superscript
    1 Baseline 2nd Row 1st Column y Subscript p r e d i c t Superscript 2 2nd Column
    equals 1 omega 0 plus omega 1 x 1 squared plus omega 2 x 2 squared plus omega
    3 x 3 squared plus omega 4 x 4 squared plus omega 5 x 5 squared 3rd Row 1st Column  ellipsis
    4th Row 1st Column y Subscript p r e d i c t Superscript m 2nd Column equals 1
    omega 0 plus omega 1 x 1 Superscript m Baseline plus omega 2 x 2 Superscript m
    Baseline plus omega 3 x 3 Superscript m Baseline plus omega 4 x 4 Superscript
    m Baseline plus omega 5 x 5 Superscript m EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr> <mtr><mtd columnalign="right"><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mi>m</mi></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup> <mo>+</mo>
    <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mi>m</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub> <msubsup><mi>x</mi> <mn>4</mn> <mi>m</mi></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub> <msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily arrange the above system as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Start 4 By 1 Matrix 1st Row  y Subscript p r e d
    i c t Superscript 1 Baseline 2nd Row  y Subscript p r e d i c t Superscript 2
    Baseline 3rd Row   ellipsis 4th Row  y Subscript p r e d i c t Superscript m Baseline
    EndMatrix equals Start 4 By 1 Matrix 1st Row  1 2nd Row  1 3rd Row   ellipsis
    4th Row  1 EndMatrix omega 0 plus Start 4 By 1 Matrix 1st Row  x 1 Superscript
    1 Baseline 2nd Row  x 1 squared 3rd Row   ellipsis 4th Row  x 1 Superscript m
    Baseline EndMatrix omega 1 plus Start 4 By 1 Matrix 1st Row  x 1 Superscript 1
    Baseline 2nd Row  x 2 squared 3rd Row   ellipsis 4th Row  x 2 Superscript m Baseline
    EndMatrix omega 2 plus Start 4 By 1 Matrix 1st Row  x 3 Superscript 1 Baseline
    2nd Row  x 3 squared 3rd Row   ellipsis 4th Row  x 3 Superscript m Baseline EndMatrix
    omega 3 plus Start 4 By 1 Matrix 1st Row  x 4 Superscript 1 Baseline 2nd Row  x
    4 squared 3rd Row   ellipsis 4th Row  x 4 Superscript m Baseline EndMatrix omega
    4 plus Start 4 By 1 Matrix 1st Row  x 5 Superscript 1 Baseline 2nd Row  x 5 squared
    3rd Row   ellipsis 4th Row  x 5 Superscript m Baseline EndMatrix omega 5 comma
    dollar-sign"><mrow><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>1</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>1</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>2</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>2</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>3</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>3</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>4</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>4</mn></msub> <mo>+</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi> <mn>5</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <msub><mi>ω</mi>
    <mn>5</mn></msub> <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'or even better:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Start 4 By 1 Matrix 1st Row  y Subscript p r e d
    i c t Superscript 1 Baseline 2nd Row  y Subscript p r e d i c t Superscript 2
    Baseline 3rd Row   ellipsis 4th Row  y Subscript p r e d i c t Superscript m Baseline
    EndMatrix equals Start 4 By 6 Matrix 1st Row 1st Column 1 2nd Column x 1 Superscript
    1 Baseline 3rd Column x 2 Superscript 1 Baseline 4th Column x 3 Superscript 1
    Baseline 5th Column x 4 Superscript 1 Baseline 6th Column x 5 Superscript 1 Baseline
    2nd Row 1st Column 1 2nd Column x 1 squared 3rd Column x 2 squared 4th Column
    x 3 squared 5th Column x 4 squared 6th Column x 5 squared 3rd Row 1st Column  ellipsis
    4th Row 1st Column 1 2nd Column x 1 Superscript m Baseline 3rd Column x 2 Superscript
    m Baseline 4th Column x 3 Superscript m Baseline 5th Column x 4 Superscript m
    Baseline 6th Column x 5 Superscript m Baseline EndMatrix Start 6 By 1 Matrix 1st
    Row  omega 0 2nd Row  omega 1 3rd Row  omega 2 4th Row  omega 3 5th Row  omega
    4 6th Row  omega 5 EndMatrix period dollar-sign"><mrow><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><msubsup><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow>
    <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi> <mn>1</mn>
    <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>4</mn> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi> <mn>1</mn> <mi>m</mi></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>3</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>4</mn> <mi>m</mi></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced>
    <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>ω</mi> <mn>0</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi>
    <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi> <mn>3</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>4</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi>
    <mn>5</mn></msub></mtd></mtr></mtable></mfenced> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector on the left hand side of the above equation is <math alttext="ModifyingAbove
    y With right-arrow Subscript p r e d i c t"><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>
    , the matrix on the right hand side is the training subset *X* augmented with
    the vector of ones, and the last vector on the right hand side has all the unknown
    weights packed neatly into it. Call this vector <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    , then write <math alttext="ModifyingAbove y With right-arrow Subscript p r e
    d i c t"><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub></math>
    compactly in terms of the training subset and <math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove y With right-arrow Subscript p r e
    d i c t Baseline equals upper X ModifyingAbove omega With right-arrow period dollar-sign"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the formula of the mean squared error loss function, which we wrote before
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline
    minus ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    Superscript t Baseline left-parenthesis ModifyingAbove y With right-arrow Subscript
    p r e d i c t Baseline minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals StartFraction 1 Over m EndFraction parallel-to
    ModifyingAbove y With right-arrow Subscript p r e d i c t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline parallel-to dollar-sign"><mrow><mtext>Mean</mtext>
    <mtext>Squared</mtext> <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msup><mrow><mo>(</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mi>t</mi></msup> <mrow><mo>(</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Mean Squared Error StartFraction 1 Over m EndFraction
    left-parenthesis upper X ModifyingAbove omega With right-arrow minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis Superscript t
    Baseline left-parenthesis upper X ModifyingAbove omega With right-arrow minus
    ModifyingAbove y With right-arrow Subscript t r u e Baseline right-parenthesis
    equals StartFraction 1 Over m EndFraction parallel-to upper X ModifyingAbove omega
    With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u e Baseline
    parallel-to period dollar-sign"><mrow><mtext>Mean</mtext> <mtext>Squared</mtext>
    <mtext>Error</mtext> <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced
    close=")" open="(" separators=""><mi>X</mi><mover accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to find the <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that minimizes the neatly written
    loss function. For that, we have to visit the rich and beautiful mathematical
    field of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: When The Training Data Has Highly Correlated Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inspecting the training matrix (augmented with the vector of ones)
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper X equals Start 4 By 6 Matrix 1st Row 1st Column
    1 2nd Column x 1 Superscript 1 Baseline 3rd Column x 2 Superscript 1 Baseline
    4th Column x 3 Superscript 1 Baseline 5th Column x 4 Superscript 1 Baseline 6th
    Column x 5 Superscript 1 Baseline 2nd Row 1st Column 1 2nd Column x 1 squared
    3rd Column x 2 squared 4th Column x 3 squared 5th Column x 4 squared 6th Column
    x 5 squared 3rd Row 1st Column  ellipsis 4th Row 1st Column 1 2nd Column x 1 Superscript
    m Baseline 3rd Column x 2 Superscript m Baseline 4th Column x 3 Superscript m
    Baseline 5th Column x 4 Superscript m Baseline 6th Column x 5 Superscript m EndMatrix
    dollar-sign"><mrow><mi>X</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><msubsup><mi>x</mi> <mn>1</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>2</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>1</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>4</mn> <mn>1</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>5</mn> <mn>1</mn></msubsup></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mn>2</mn></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mn>2</mn></msubsup></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><msubsup><mi>x</mi>
    <mn>1</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn> <mi>m</mi></msubsup></mtd>
    <mtd><msubsup><mi>x</mi> <mn>3</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi>
    <mn>4</mn> <mi>m</mi></msubsup></mtd> <mtd><msubsup><mi>x</mi> <mn>5</mn> <mi>m</mi></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'that appears in the vector <math alttext="ModifyingAbove y With right-arrow
    Subscript p r e d i c t Baseline equals upper X ModifyingAbove omega With right-arrow"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math>
    , the formula of the mean squared error loss function, and later in the formula
    that determines the unknown <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> (also called *the normal equation*):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis Superscript negative
    1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline comma dollar-sign"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'we can see how our model might have a problem if two or more features (*x*
    columns) of the data are highly correlated: This means that there is a strong
    linear relationship between the features, so one of these features can be determined
    (or nearly determined) using a linear combination of the others. Thus, the corresponding
    feature columns are *not linearly independent* (or close to not being linearly
    independent). For matrices, this is a problem, since it indicates that the matrix
    either cannot be inverted or is *ill conditioned*. Ill conditioned matrices produce
    large instabilities in computations, since slight variations in the training data
    (which must be assumed) produces large variations in the model parameters and
    hence renders its predictions unreliable.'
  prefs: []
  type: TYPE_NORMAL
- en: We desire well conditioned matrices in our computations, so we must get rid
    of the sources of ill conditioning. When we have highly correlated features, one
    possible avenue is to include only one of them in our model, as the others do
    not add much information. Another solution is to apply dimension reduction techniques
    such as Principal Component Analysis, which we will encounter in [Chapter 11](ch11.xhtml#ch11).
    The Fish Market data set has highly correlated features, and the accompanying
    Jupyter Notebook addresses those.
  prefs: []
  type: TYPE_NORMAL
- en: That said, it is important to note that some machine learning models, such as
    decision trees and random forests (discussed below) are not affected by correlated
    featured, while others, such as the current linear regression model, and the next
    logistic regression and support vector machines models are negatively affected
    by them. As for neural network models, even though they can *learn* the correlations
    involved in the data features during training, they perform better when these
    redundancies are taken care of ahead of time, in addition to saving computation
    cost and time.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimization means find the optimal, best, maximal, minimal, or extreme solution.
  prefs: []
  type: TYPE_NORMAL
- en: We wrote a linear training function
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals omega 0 plus omega 1 x 1 plus omega 2 x
    2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x 5 period dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>4</mn></msub>
    <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>5</mn></msub>
    <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: and we left the values of its six parameters <math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math> , <math alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>
    , <math alttext="omega 2"><msub><mi>ω</mi> <mn>2</mn></msub></math> , <math alttext="omega
    3"><msub><mi>ω</mi> <mn>3</mn></msub></math> , <math alttext="omega 4"><msub><mi>ω</mi>
    <mn>4</mn></msub></math> , and <math alttext="omega 5"><msub><mi>ω</mi> <mn>5</mn></msub></math>
    unknown. The goal is to find the values that make our training function *best
    fit the training data subset*, where the word *best* is quantified using the loss
    function. This function provides a measure of how far the prediction made by the
    model’s training function is from the ground truth. We want this loss function
    to be small so we solve a minimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: We are not going to sit there and try out every possible <math alttext="omega"><mi>ω</mi></math>
    value until we find the combination that gives the least loss. Even if we did,
    we wouldn’t know when to stop, since we wouldn’t know whether there are other
    *better* values. We must have prior knowledge about the landscape of the loss
    function and take advantage of its mathematical properties. The analogy is hiking
    down the Swiss Alps with a blindfold *vs.* hiking with no blindfold and a detailed
    map ([Figure 3-7](#Fig_Swiss_Alps2) shows the rough terrain of the Swiss Alps).
    Instead of searching the landscape of the loss function for minimizers with a
    blindfold, we tap into the field of *optimization*. Optimization is a beautiful
    branch of mathematics that provides various methods to efficiently search for
    and locate optimizers of functions and their corresponding optimal values.
  prefs: []
  type: TYPE_NORMAL
- en: '![275](assets/emai_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-7\. Swiss Alps: Optimization is similar to hiking the landscape of
    a function. The destination is the bottom of the lowest valley (minimization)
    or the top of the highest peak (maximization). We need two things: The coordinates
    of the minimizing or maximizing points, and the height of the landscape at those
    points.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The optimization problem in this chapter and in the next few looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts Loss Function period dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mtext>Loss</mtext>
    <mtext>Function</mtext> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'For the current linear regression model, this is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts StartFraction 1 Over m EndFraction left-parenthesis upper X ModifyingAbove
    omega With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis Superscript t Baseline left-parenthesis upper X ModifyingAbove
    omega With right-arrow minus ModifyingAbove y With right-arrow Subscript t r u
    e Baseline right-parenthesis equals min Underscript ModifyingAbove omega With
    right-arrow Endscripts StartFraction 1 Over m EndFraction parallel-to upper X
    ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline parallel-to period dollar-sign"><mrow><msub><mo form="prefix"
    movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced close=")" open="(" separators=""><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>=</mo> <msub><mo form="prefix" movablelimits="true">min</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></msub> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mrow><mo>∥</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>∥</mo></mrow> <mrow><msup><mi>l</mi> <mn>2</mn></msup></mrow> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'When we do math, we should never lose track of what it is that we know and
    what it is that we are looking for. Otherwise we would run risk of getting trapped
    in a circular logic. In the above formula, we know:'
  prefs: []
  type: TYPE_NORMAL
- en: '*m* (the number of instances in the training subset),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X* (the training subset augmented with a vector of ones),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="ModifyingAbove y With right-arrow Subscript t r u e"><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    (the vector of labels corresponding to the training subset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And we are looking for:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The minimizing <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> : We must locate it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum value of the loss function at the minimizing <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convex landscapes *vs.* non-convex lanscapes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*The easiest functions* to deal with and the easiest equations to solve are
    linear. Unfortunately, most of the functions (and equations) that we deal with
    are nonlinear. At the same time, this is not too unfortunate since linear life
    is flat, boring, unstimulating, and uneventful. When the function we have at our
    hands is full-blown nonlinear, we sometimes *linearize* it near certain points
    that we care for. The idea here is that even though the full picture of the function
    might be nonlinear, we may be able to approximate it by a linear function in the
    locality that we focus on. In other words, in a very small neighborhood, the nonlinear
    function might look and behave linearly, albeit the said neighborhood might be
    infinitesimally small. For an analogy, think about how Earth looks (and behaves
    in terms of calculating distances *etc.*) flatly from our own locality, and how
    we can only see its nonlinear shape from high up. When we want to linearize a
    function near a point, we approximate it by its tangent space near that point
    (this is its tangent line if it is a function of one variable, tangent plane if
    it is a function of two variables, and tangent hyperplane if it is a function
    of three or more variables). For this, we need to calculate one derivative of
    the function with respect to all of its variables, since this gives us the slope
    (which measures the inclination) of the approximating flat space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sad news is that linearizing near one point may not be enough, and we may
    want to use linear approximations at multiple locations. Thankfully that is doable,
    since we all we have to do computationally is to evaluate one derivative at several
    points. This leads us to *the next easiest functions* to deal with (after linear
    functions): Piecewise linear functions, which are linear but only in piecewise
    structures, or linear except at isolated points or locations. The field of *linear
    programming* deals with such functions, where the functions to optimize are linear,
    and the boundaries of the domains where the optimization happens are piecewise
    linear (they are intersections of half spaces).'
  prefs: []
  type: TYPE_NORMAL
- en: When our goal is optimization, the best functions to deal with are either linear
    (where the field of linear programming helps us) or convex (where we do not worry
    about getting stuck at local minima, and where we have good inequalities that
    help us with the analysis).
  prefs: []
  type: TYPE_NORMAL
- en: 'One important type of functions to keep in mind, which appears in machine learning,
    is a function which is the maximum of two or more convex functions. These functions
    are always convex. Recall that linear functions are flat so they are at the same
    time convex and concave. This is useful since some functions are defined as the
    maxima of linear functions: Those are not guaranteed to be linear (they are piecewise
    linear), but are guaranteed to be convex. That is, even though we lose linearity
    when we take the maximum of linear functions, we are compensated with convexity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Rectified Linear Unit function (ReLU) that is used as a nonlinear activation
    function in neural networks is an example of a function defined as the maximum
    of two linear functions: <math alttext="upper R e upper L upper U left-parenthesis
    x right-parenthesis equals m a x left-parenthesis 0 comma x right-parenthesis"><mrow><mi>R</mi>
    <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>m</mi>
    <mi>a</mi> <mi>x</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    . Another example is the Hinge Loss function used for support vector machines:
    <math alttext="upper H left-parenthesis x right-parenthesis equals m a x left-parenthesis
    0 comma 1 minus t x right-parenthesis"><mrow><mi>H</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>m</mi> <mi>a</mi> <mi>x</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mn>1</mn> <mo>-</mo> <mi>t</mi> <mi>x</mi> <mo>)</mo></mrow></math> where *t*
    is either 1 or -1.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the minimum of a family of convex functions is not guaranteed to be
    convex, it can have a double well. However, their maximum is definitely convex.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one more relationship between linearity and convexity: If we have
    a convex function (nonlinear since linear would be trivial), then the maximum
    of all the linear functions that stay below our function is exactly equal to it.
    In other words, convexity replaces linearity, in the sense that when linearity
    is not available, but convexity is, we can replace our convex function with the
    maximum of all the linear functions whose graph lies below our function’s graph
    (see [Figure 3-8](#Fig_convex_above_tangents)). Recall that the graph of a convex
    function lies above the graph of its tangent at any point, and that the tangents
    are linear. This gives us a direct path to exploiting the simplicity of linear
    functions when we have convex functions. We have equality when we consider the
    maximum of *all* the tangents, and only approximation when we consider the maximum
    of the tangents at few points.'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. A convex function is equal to the maximum of all of its tangents.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 3-9](#Fig_convex_landscape) and [Figure 3-10](#Fig_nonconvex_landscape)
    show the general landscapes of nonlinear convex and nonconvex functions repectively.
    Overall, the landscape of a convex function is good for minimization problems.
    We have no fear of getting stuck at local minima since any local minimum is also
    a global minimum for a convex function. The landscape of a non-convex function
    has peaks, valleys, and saddle points. A minimization problem on such a landscape
    runs the risk of getting stuck at local minima and never finding the global minima.'
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. The landscape of a convex function is good for minimization problems.
    We have no fear of getting stuck at local minima since any local minimum is also
    a global minimum for a convex function.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![280](assets/emai_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. The landscape of a non-convex function has peaks, valleys, and
    saddle points. A minimization problem on such a landscape runs the risk of getting
    stuck at local minima and never finding the global minima.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, make sure you know the distinction between a convex function, a convex
    set, and a convex optimization problem, which optimizes a convex function over
    a convex set.
  prefs: []
  type: TYPE_NORMAL
- en: How do we locate minimizers of functions?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In general, there are two approaches to locating minimizers (and/or maximizers)
    of functions. The tradeoff is usually between:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating only one derivative and converging to the minimum slowly (though
    there are acceleration methods to speed the convergence up). These are called
    *gradient* methods. The gradient is one derivative of a function of several variables.
    For example, our loss function is a function of several <math alttext="omega"><mi>ω</mi></math>
    ’s (or of one vector <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> ).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating two derivatives (computationally much more expensive, which is a
    big turn off, especially when we have thousands of parameters) and converging
    to the minimum faster. Computation costs can be saved a little by approximating
    the second derivative instead of computing it exactly. Second derivative methods
    are called *Newton*’s methods. The *Hessian* (the matrix of second derivatives)
    or an approximation of the Hessian appear in these methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We never need to go beyond calculating two derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: But why are the first and second derivatives of a function so important for
    locating its optimizers? The concise answer is that the first derivative contains
    information on how fast a function increases or decreases at a point (so if you
    follow its direction you might ascend to the maximum or descend to the minimum),
    and the second derivative contains information on the shape of the *bowl* of the
    function, if it curves up or curves down.
  prefs: []
  type: TYPE_NORMAL
- en: 'One key idea from calculus remains fundamental: Minimizers (and/or maximizers)
    happen at *critical points* (defined as the points where one derivative of our
    function is either equal to zero or does not exist) or at boundary points. So
    in order to locate these optimizers, we must search through *both* the boundary
    points (if our search space has a boundary) *and* the interior critical points.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we locate the critical points in the interior of our search space?
  prefs: []
  type: TYPE_NORMAL
- en: Approach 1
  prefs: []
  type: TYPE_NORMAL
- en: 'We follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Find one derivative of our function (not too bad, we all did it in calculus),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: then set it equal to zero (we can all write the symbols equal and zero),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and solve for the <math alttext="omega"><mi>ω</mi></math> ’s that make our derivative
    zero (this is the bad step!).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For functions whose derivatives are linear, such as our mean squared error loss
    function, it is sort of easy to solve for these <math alttext="omega"><mi>ω</mi></math>
    ’s. The field of linear algebra was especially built to help solve linear systems
    of equations. The field of numerical linear algebra was built to help solve realistic
    and large systems of linear equations where ill conditioning is prevalent. We
    have many tools at our disposal (and software packages) when our systems are linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, when our equations are nonlinear, finding solutions is an
    entirely different story. It becomes a hit or miss game, with mostly miss! Here’s
    a short example that illustrates the difference between solving a linear and a
    nonlinear equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Solving a linear equation
  prefs: []
  type: TYPE_NORMAL
- en: Find <math alttext="omega"><mi>ω</mi></math> such that <math alttext="0.002
    omega minus 5 equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>002</mn> <mi>ω</mi> <mo>-</mo>
    <mn>5</mn> <mo>=</mo> <mn>0</mn></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: '*Solution*: Moving the 5 over to the other side then dividing by 0.002, we
    get <math alttext="omega equals 5 slash 0.002 equals 2500"><mrow><mi>ω</mi> <mo>=</mo>
    <mn>5</mn> <mo>/</mo> <mn>0</mn> <mo>.</mo> <mn>002</mn> <mo>=</mo> <mn>2500</mn></mrow></math>
    . Done.'
  prefs: []
  type: TYPE_NORMAL
- en: Solving a nonlinear equation
  prefs: []
  type: TYPE_NORMAL
- en: Find <math alttext="omega"><mi>ω</mi></math> such that <math alttext="0.002
    sine left-parenthesis omega right-parenthesis minus 5 omega squared plus e Superscript
    omega Baseline equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>002</mn> <mo form="prefix">sin</mo>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>-</mo> <mn>5</mn> <msup><mi>ω</mi>
    <mn>2</mn></msup> <mo>+</mo> <msup><mi>e</mi> <mi>ω</mi></msup> <mo>=</mo> <mn>0</mn></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: '*Solution*: Yes, I am out of here. We need a numerical method! (See [Figure 3-11](#Fig_roots_f_nonlinear)
    for a graphical approximation of the solution of this nonlinear equation).'
  prefs: []
  type: TYPE_NORMAL
- en: '![275](assets/emai_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. It is difficult to solve nonlinear equations. Here, we plot <math
    alttext="f left-parenthesis omega right-parenthesis equals 0.002 sine left-parenthesis
    omega right-parenthesis minus 5 omega squared plus e Superscript omega"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>002</mn> <mo form="prefix">sin</mo> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mn>5</mn> <msup><mi>ω</mi> <mn>2</mn></msup> <mo>+</mo> <msup><mi>e</mi>
    <mi>ω</mi></msup></mrow></math> and approximate its three roots (points where
    <math alttext="f left-parenthesis omega right-parenthesis equals 0"><mrow><mi>f</mi>
    <mo>(</mo> <mi>ω</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math> ) on the
    graph.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many numerical techniques devoted solely to finding solutions of nonlinear
    equations (and entire fields devoted to numerically solving nonlinear ordinary
    and partial differential equations). These methods find approximate solutions
    then provide bounds on how far off the numerical solutions are from the exact
    analytical solutions. They usually construct a sequence that converges to the
    anaytical solution under certain conditions. Some methods converge faster than
    others, and are better suited for certain problems but not others.
  prefs: []
  type: TYPE_NORMAL
- en: Approach 2
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to follow the gradient direction, in order to descend towards
    the minimum or ascend towards the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: To understand these gradient type methods, think of hiking down a mountain (or
    skiing down the mountain if the method is accelerated or has momentum). We start
    at a random point in our search space and that sets us at an initial height level
    on the landscape of the function. Now the method moves us to a new point in the
    search space, and hopefully, at this new location, we end up at a new height level
    that is *lower* than the height level we came from. Hence, we would’ve *descended*.
    We repeat this and ideally, if the terrain of the function cooperates, this sequence
    of points will converge towards the minimizer of the function that we are looking
    for.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, for functions whose landscapes have many peaks and valleys, where
    we start, or in other words, how to initialize, matters, since we could descend
    down an entirely different valley than the one we want to end up at. We might
    end up at a local minimum instead of a global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Functions that are convex and bounded below are shaped liked a salad bowl so
    with those we do not worry about getting stuck at local minima and away from global
    minima. There could be another source of worry with convex functions: When the
    shape of the bowl of the function is too narrow, then our method might become
    painfully slow to converge. We will go over this in detail in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Both Approach 1 and Approach 2 are useful and popular. Sometimes, we have no
    option to use but one or the other, depending on how fast each method converges
    for our particular setting, how *regular* the function we are trying to optimize
    is (how many well behaved derivatives it has), *etc*. Other times, it is just
    a matter of taste. For linear regression’s mean squared error loss function, both
    types of methods work, so we will use Approach 1, only because we will use gradient
    descent methods for *all* the other loss functions in this book.
  prefs: []
  type: TYPE_NORMAL
- en: We must mention that the hiking down the mountain analogy for descent methods
    is excellent but a tad bit misleading. When we, humans, hike down a mountain,
    we physically belong in the same three dimensional space that our mountain landscape
    exists in, meaning we are at a certain elevation and we are able to descend to
    a location with lower elevation, even with a blindfold, and even when it is too
    foggy and we can only descend one tiny step at a time. We sense the elevation
    then move downhill. Numerical descent methods, on the other hand, do not search
    for the minimum in the same space dimension as the one the landscape of the function
    is embedded in. Instead, they search on the ground level, one dimension *below*
    the landscape (see [Figure 3-12](#Fig_ground_level_search)). This makes descending
    towards the minimum much more difficult, since at ground level we can move from
    any point to any other without knowing what height level exists above us, until
    we evaluate the function itself at the point and find the height. So our method
    might accidentally move us from one ground point with a certain elevation above
    us to another ground point with a *higher* elevation, hence farther away from
    the minimum. This is why it is important to locate, on ground level, a direction
    that quickly *decreases* the function height, and how far we can move in that
    direction on ground level (step size) while *still decsreasing the function height
    above us*. The step size is also called the *learning rate hyperparameter*, which
    we will encounter everytime we use a descent method.
  prefs: []
  type: TYPE_NORMAL
- en: '![250](assets/emai_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Search for the minimum happens on ground level and not directly
    on the landscape of the function.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Back to our main goal: We want to find the best <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    for our training function, so we must minimize our mean squared error loss function,
    by Approach 1: Take one derivative of the loss function and set it equal to zero,
    then solve for the vector <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> . For this, we need to master
    *doing calculus on linear algebra expressions*. Let’s revisit our calculus class
    first.'
  prefs: []
  type: TYPE_NORMAL
- en: Calculus in a nutshell
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a first course on Calculus, we learn about functions of single variable
    ( <math alttext="f left-parenthesis omega right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow></math> ), their graphs, and evaluating
    them at certain points. Then we learn about the most important operation in mathematical
    analysis: The limit. From the limit concept we define continuity and discontinuity
    of functions, derivative at a point <math alttext="f prime left-parenthesis omega
    right-parenthesis"><mrow><msup><mi>f</mi> <mo>''</mo></msup> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow></mrow></math> (limit of slopes of secants through
    a point) and integral over a domain (limit of sums of mini regions determined
    by the function over the domain). We end the class with the fundamental theorem
    of calculus, relating integration and differentiation as inverse operations. Of
    the key properties of the derivative is that it determines how fast a function
    increases or descreases at a certain point, hence, it plays a crucial role in
    locating a minimum and/or maximum of a function in the interior of its domain
    (boundary points are separate).'
  prefs: []
  type: TYPE_NORMAL
- en: In a multivariable calculus course, which is usually a third course in calculus,
    many ideas transfer from single variable calculus, including the importance of
    the derivative, now called the *gradient* because we have several variables, in
    locating any interior minima and/or maxima. The gradient <math alttext="normal
    nabla left-parenthesis f left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis right-parenthesis"><mrow><mi>∇</mi> <mo>(</mo> <mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
    of <math alttext="f left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow></math>
    is the derivative of the function with respect to the vector <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    of variables.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, the unknown weights are organized in matrices, not in vectors,
    so we need to take the derivative of a function <math alttext="f left-parenthesis
    upper W right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>W</mi> <mo>)</mo></mrow></math>
    with repect to a matrix *W* of variables.
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes in AI, the function whose derivative we need to calculate is
    the loss function, which has the training function built into it. By the *chain
    rule for derivatives*, we would also need to calculate the derivative of the training
    function with respect to the <math alttext="omega"><mi>ω</mi></math> ’s as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s demonstrate using one simple example from single variable calculus then
    immediately transition to taking derivatives of linear algebra expressions.
  prefs: []
  type: TYPE_NORMAL
- en: A one-dimensional optimization example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Find the minimizer(s) and the minimum value (if any) of the function <math
    alttext="f left-parenthesis omega right-parenthesis equals 3 plus left-parenthesis
    0.5 omega minus 2 right-parenthesis squared"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ω</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> on the interval [-1,6].*'
  prefs: []
  type: TYPE_NORMAL
- en: One impossibly long way to go about this is to try out the *infinitely many*
    values of <math alttext="omega"><mi>ω</mi></math> between -1 and 6 and choose
    the <math alttext="omega"><mi>ω</mi></math> ’s that give the lowest *f* value.
    Another way is to use our calculus knowledge that optimizers (minimizers and/or
    maximizers) happen either at critical points (where the derivative is either nonexistent
    or zero) or at boundary points. For reference, see [Figure 3-13](#Fig_minimize_f).
  prefs: []
  type: TYPE_NORMAL
- en: '![275](assets/emai_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. The minimum value of the function <math alttext="f left-parenthesis
    omega right-parenthesis equals 3 plus left-parenthesis 0.5 omega minus 2 right-parenthesis
    squared"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mi>ω</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> on the interval [-1,6] is 3 and happens at the
    critical point <math alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math>
    . At this critical point, the derivative of the function is zero, meaning that
    if we draw a tangent line, it will be horizontal.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Our boundary points are *-1* and *6*, so we evaluate our function at these
    points first: <math alttext="f left-parenthesis negative 1 right-parenthesis equals
    3 plus left-parenthesis 0.5 left-parenthesis negative 1 right-parenthesis minus
    2 right-parenthesis squared equals 9.25"><mrow><mi>f</mi> <mrow><mo>(</mo> <mo>-</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>9</mn> <mo>.</mo> <mn>25</mn></mrow></math> and
    <math alttext="f left-parenthesis 6 right-parenthesis equals 3 plus left-parenthesis
    0.5 left-parenthesis 6 right-parenthesis minus 2 right-parenthesis squared equals
    4"><mrow><mi>f</mi> <mrow><mo>(</mo> <mn>6</mn> <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn>
    <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mn>6</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>4</mn></mrow></math> . Obviously, *-1* is not
    a minimizer since *f(6)<f(-1)*, so this boundary point gets out of the competition
    and now it is only the boundary point *6* competing with interior critical point(s).
    In order to find our critical points, we inspect the derivative of the function
    in the interior of the interval *[-1,6]*: <math alttext="f prime left-parenthesis
    omega right-parenthesis equals 0 plus 2 left-parenthesis 0.5 omega minus 2 right-parenthesis
    asterisk 0.5 equals 0.25 left-parenthesis 0.5 omega minus 2 right-parenthesis"><mrow><msup><mi>f</mi>
    <mo>''</mo></msup> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn>
    <mo>+</mo> <mn>2</mn> <mrow><mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mi>ω</mi>
    <mo>-</mo> <mn>2</mn> <mo>)</mo></mrow> <mo>*</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>25</mn> <mrow><mo>(</mo> <mn>0</mn> <mo>.</mo>
    <mn>5</mn> <mi>ω</mi> <mo>-</mo> <mn>2</mn> <mo>)</mo></mrow></mrow></math> .
    Setting this derivative equal to zero we have <math alttext="0.25 left-parenthesis
    0.5 omega minus 2 right-parenthesis equals 0"><mrow><mn>0</mn> <mo>.</mo> <mn>25</mn>
    <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mi>ω</mi> <mo>-</mo> <mn>2</mn> <mo>)</mo>
    <mo>=</mo> <mn>0</mn></mrow></math> implying that <math alttext="omega equals
    4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math> . Thus, we only found
    one critical point <math alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo>
    <mn>4</mn></mrow></math> in the interior of the interval *[-1,6]*. At this special
    point, the value of the function is <math alttext="f left-parenthesis 4 right-parenthesis
    equals 3 plus left-parenthesis 0.5 left-parenthesis 4 right-parenthesis minus
    2 right-parenthesis squared equals 3"><mrow><mi>f</mi> <mrow><mo>(</mo> <mn>4</mn>
    <mo>)</mo></mrow> <mo>=</mo> <mn>3</mn> <mo>+</mo> <msup><mrow><mo>(</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo>(</mo><mn>4</mn><mo>)</mo></mrow><mo>-</mo><mn>2</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>=</mo> <mn>3</mn></mrow></math> . Since the value of *f*
    is the lowest here, we have obviously found the winner of our minimization competition,
    namely, <math alttext="omega equals 4"><mrow><mi>ω</mi> <mo>=</mo> <mn>4</mn></mrow></math>
    with minimum *f* value equal to *3*.'
  prefs: []
  type: TYPE_NORMAL
- en: Derivatives of linear algebra expressions that we use all the time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is efficient to calculate derivatives directly on expressions involving
    vectors and matrices, without having to resolve them into their components. The
    following two are popular:'
  prefs: []
  type: TYPE_NORMAL
- en: When *a* and <math alttext="omega"><mi>ω</mi></math> are scalars and *a* is
    constant, the derivative of <math alttext="f left-parenthesis omega right-parenthesis
    equals a omega"><mrow><mi>f</mi> <mo>(</mo> <mi>ω</mi> <mo>)</mo> <mo>=</mo> <mi>a</mi>
    <mi>ω</mi></mrow></math> is <math alttext="f prime left-parenthesis omega right-parenthesis
    equals a"><mrow><msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>ω</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mi>a</mi></mrow></math> . When <math alttext="ModifyingAbove
    a With right-arrow"><mover accent="true"><mi>a</mi> <mo>→</mo></mover></math>
    and <math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math> are vectors (of the same length) and the entries of
    <math alttext="ModifyingAbove a With right-arrow"><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></math> are constant, then the gradient of <math alttext="f
    left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis equals
    ModifyingAbove a With right-arrow Superscript t Baseline ModifyingAbove omega
    With right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <msup><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math>
    is <math alttext="normal nabla f left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis equals ModifyingAbove a With right-arrow"><mrow><mi>∇</mi> <mi>f</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> .
    Similarly, the gradient of <math alttext="f left-parenthesis ModifyingAbove omega
    With right-arrow right-parenthesis equals ModifyingAbove w With right-arrow Superscript
    t Baseline ModifyingAbove a With right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mover accent="true"><mi>w</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> is <math alttext="normal
    nabla f left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis
    equals ModifyingAbove a With right-arrow"><mrow><mi>∇</mi> <mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When *s* is scalar and constant and <math alttext="omega"><mi>ω</mi></math>
    is scalar, then the derivative of the quadratic function <math alttext="f left-parenthesis
    omega right-parenthesis equals s omega squared"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>ω</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>s</mi> <msup><mi>ω</mi> <mn>2</mn></msup></mrow></math>
    is <math alttext="f prime left-parenthesis omega right-parenthesis equals 2 s
    omega"><mrow><msup><mi>f</mi> <mo>'</mo></msup> <mrow><mo>(</mo> <mi>ω</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>2</mn> <mi>s</mi> <mi>ω</mi></mrow></math> . The analogous high
    dimensional case is when *S* is a symmetric matrix with constant entries, then
    the function <math alttext="f left-parenthesis ModifyingAbove omega With right-arrow
    right-parenthesis equals ModifyingAbove omega With right-arrow Superscript t Baseline
    upper S ModifyingAbove omega With right-arrow"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mi>S</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></mrow></math> is quadratic
    and its gradient is <math alttext="normal nabla f left-parenthesis ModifyingAbove
    omega With right-arrow right-parenthesis equals 2 upper S ModifyingAbove omega
    With right-arrow"><mrow><mi>∇</mi> <mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mn>2</mn> <mi>S</mi> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimizing the mean squared error loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are finally ready to minimize the mean squared error loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals StartFraction 1 Over m EndFraction left-parenthesis
    upper X ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline right-parenthesis Superscript t Baseline left-parenthesis
    upper X ModifyingAbove omega With right-arrow minus ModifyingAbove y With right-arrow
    Subscript t r u e Baseline right-parenthesis period dollar-sign"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msup><mfenced close=")" open="("
    separators=""><mi>X</mi><mover accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mi>t</mi></msup> <mfenced close=")" open="(" separators=""><mi>X</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s open the above expression up before taking its gradient and setting it
    equal to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis 2nd Column equals StartFraction
    1 Over m EndFraction left-parenthesis left-parenthesis upper X ModifyingAbove
    omega With right-arrow right-parenthesis Superscript t Baseline minus ModifyingAbove
    y With right-arrow Subscript t r u e Superscript t Baseline right-parenthesis
    left-parenthesis upper X ModifyingAbove omega With right-arrow minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis 2nd Row 1st Column
    Blank 2nd Column equals StartFraction 1 Over m EndFraction left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline upper X Superscript t Baseline minus
    ModifyingAbove y With right-arrow Subscript t r u e Superscript t Baseline right-parenthesis
    left-parenthesis upper X ModifyingAbove omega With right-arrow minus ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis 3rd Row 1st Column
    Blank 2nd Column equals StartFraction 1 Over m EndFraction left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline upper X Superscript t Baseline upper
    X ModifyingAbove omega With right-arrow minus ModifyingAbove omega With right-arrow
    Superscript t Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow
    Subscript t r u e Baseline minus ModifyingAbove y With right-arrow Subscript t
    r u e Superscript t Baseline upper X ModifyingAbove omega With right-arrow plus
    ModifyingAbove y With right-arrow Subscript t r u e Superscript t Baseline ModifyingAbove
    y With right-arrow Subscript t r u e Baseline right-parenthesis 4th Row 1st Column
    Blank 2nd Column equals StartFraction 1 Over m EndFraction left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline upper S ModifyingAbove omega With
    right-arrow minus ModifyingAbove omega With right-arrow Superscript t Baseline
    ModifyingAbove a With right-arrow minus ModifyingAbove a With right-arrow Superscript
    t Baseline ModifyingAbove omega With right-arrow plus ModifyingAbove y With right-arrow
    Subscript t r u e Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline right-parenthesis comma EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><msup><mrow><mo>(</mo><mi>X</mi><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover><mo>)</mo></mrow> <mi>t</mi></msup>
    <mo>-</mo> <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup></mfenced> <mfenced close=")" open="(" separators=""><mi>X</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <mfenced close=")" open="(" separators=""><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <msup><mi>X</mi> <mi>t</mi></msup> <mo>-</mo>
    <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup></mfenced> <mfenced close=")" open="(" separators=""><mi>X</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <mfenced close=")" open="(" separators=""><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>-</mo> <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup> <mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>+</mo> <msubsup><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>t</mi></msubsup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <mfenced close=")" open="(" separators=""><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mi>S</mi> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>-</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>-</mo>
    <msup><mover accent="true"><mi>a</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>+</mo> <msubsup><mover accent="true"><mi>y</mi>
    <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>t</mi></msubsup>
    <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mfenced>
    <mo>,</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where in the last step we set <math alttext="upper X Superscript t Baseline
    upper X equals upper S"><mrow><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi> <mo>=</mo>
    <mi>S</mi></mrow></math> and <math alttext="upper X Superscript t Baseline ModifyingAbove
    y With right-arrow Subscript t r u e Baseline equals ModifyingAbove a With right-arrow"><mrow><msup><mi>X</mi>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math> .
    Next, take the gradient of the last expression with respect to <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    and set it equal to zero. When calculating the gradient we use what we just learned
    about differentiating linear algebra expressions in the above subsection:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column normal nabla upper
    L left-parenthesis ModifyingAbove omega With right-arrow right-parenthesis 2nd
    Column equals StartFraction 1 Over m EndFraction left-parenthesis 2 upper S ModifyingAbove
    omega With right-arrow minus ModifyingAbove a With right-arrow minus ModifyingAbove
    a With right-arrow plus 0 right-parenthesis 2nd Row 1st Column Blank 2nd Column
    equals ModifyingAbove 0 With right-arrow EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>∇</mi> <mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mn>2</mn>
    <mi>S</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover> <mo>-</mo> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>+</mo> <mn>0</mn></mfenced></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mover accent="true"><mn>0</mn> <mo>→</mo></mover></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it is easy to solve for <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartFraction 1 Over m EndFraction left-parenthesis
    2 upper S ModifyingAbove omega With right-arrow minus 2 ModifyingAbove a With
    right-arrow right-parenthesis equals ModifyingAbove 0 With right-arrow dollar-sign"><mrow><mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <mfenced close=")" open="(" separators=""><mn>2</mn> <mi>S</mi>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>-</mo> <mn>2</mn> <mover
    accent="true"><mi>a</mi> <mo>→</mo></mover></mfenced> <mo>=</mo> <mover accent="true"><mn>0</mn>
    <mo>→</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: so
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign 2 upper S ModifyingAbove omega With right-arrow equals
    2 ModifyingAbove a With right-arrow dollar-sign"><mrow><mn>2</mn> <mi>S</mi> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <mn>2</mn> <mover accent="true"><mi>a</mi>
    <mo>→</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: which gives
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals upper
    S Superscript negative 1 Baseline ModifyingAbove a With right-arrow dollar-sign"><mrow><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mi>S</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>a</mi> <mo>→</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now recall that we set <math alttext="upper S equals upper X Superscript t
    Baseline upper X"><mrow><mi>S</mi> <mo>=</mo> <msup><mi>X</mi> <mi>t</mi></msup>
    <mi>X</mi></mrow></math> and <math alttext="ModifyingAbove a With right-arrow
    equals upper X Superscript t Baseline y Subscript t r u e"><mrow><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math> , so
    let’s rewrite our minimizing <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> in terms of the training set
    *X* (augmented with ones) and the corresponding labels vector <math alttext="ModifyingAbove
    y With right-arrow Subscript t r u e"><msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis Superscript negative
    1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e Baseline period dollar-sign"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'For the Fish Market data set, this ends up being (see the accompanying Jupyter
    notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove omega With right-arrow equals Start
    6 By 1 Matrix 1st Row  omega 0 2nd Row  omega 1 3rd Row  omega 2 4th Row  omega
    3 5th Row  omega 4 6th Row  omega 5 EndMatrix equals Start 6 By 1 Matrix 1st Row  negative
    475.19929130109716 2nd Row  82.84970118 3rd Row  negative 28.85952426 4th Row  negative
    28.50769512 5th Row  29.82981435 6th Row  30.97250278 EndMatrix dollar-sign"><mrow><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>ω</mi>
    <mn>0</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>2</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi>
    <mn>3</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>ω</mi> <mn>4</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>ω</mi> <mn>5</mn></msub></mtd></mtr></mtable></mfenced> <mo>=</mo>
    <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mo>-</mo> <mn>475</mn> <mo>.</mo>
    <mn>19929130109716</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>82</mn> <mo>.</mo>
    <mn>84970118</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>28</mn> <mo>.</mo>
    <mn>85952426</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>28</mn> <mo>.</mo>
    <mn>50769512</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>29</mn> <mo>.</mo> <mn>82981435</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>30</mn> <mo>.</mo> <mn>97250278</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Caution: Multiplying large matrices by each other is very expensive. Multiply
    matrices by vectors instead.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try to avoid multiplying matrices by each other at all costs, instead, multiply
    your matrices with *vectors*. For example, in the normal equation <math alttext="ModifyingAbove
    omega With right-arrow equals left-parenthesis upper X Superscript t Baseline
    upper X right-parenthesis Superscript negative 1 Baseline upper X Superscript
    t Baseline ModifyingAbove y With right-arrow Subscript t r u e"><mrow><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi> <mi>t</mi></msup>
    <mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <msup><mi>X</mi>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>
    , compute <math alttext="upper X Superscript t Baseline ModifyingAbove y With
    right-arrow Subscript t r u e"><mrow><msup><mi>X</mi> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>
    first, and avoid computing <math alttext="left-parenthesis upper X Superscript
    t Baseline upper X right-parenthesis Superscript negative 1"><msup><mrow><mo>(</mo><msup><mi>X</mi>
    <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    altogether. The way around this is to solve instead the linear system <math alttext="upper
    X ModifyingAbove omega With right-arrow equals ModifyingAbove y With right-arrow
    Subscript t r u e"><mrow><mi>X</mi> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mo>=</mo> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math>
    using the *pseudo inverse* of *X* (check the accompanying Jupyter notebook). We
    will discuss the the *pseudo inverse* in [Chapter 11](ch11.xhtml#ch11), but for
    now, it allows us to invert (which is equivalent to divide by) matrices that do
    not have an inverse.
  prefs: []
  type: TYPE_NORMAL
- en: 'We just located the weights vector <math alttext="ModifyingAbove omega With
    right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that gives
    the best fit between our training data and the linear regression training function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign f left-parenthesis ModifyingAbove omega With right-arrow
    semicolon ModifyingAbove x With right-arrow right-parenthesis equals omega 0 plus
    omega 1 x 1 plus omega 2 x 2 plus omega 3 x 3 plus omega 4 x 4 plus omega 5 x
    5 period dollar-sign"><mrow><mi>f</mi> <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>;</mo> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>4</mn></msub> <msub><mi>x</mi> <mn>4</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>5</mn></msub> <msub><mi>x</mi> <mn>5</mn></msub> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We used an analytical method (compute the gradient of the loss function and
    set it equal to zero) to derive the solution given by the normal equation. This
    is one of the very rare instances where we are able to derive an analytical solution.
    All other methods for finding the minimizing <math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> will
    be numerical.
  prefs: []
  type: TYPE_NORMAL
- en: 'Caution: We never want to fit the training data too well'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The <math alttext="ModifyingAbove omega With right-arrow equals left-parenthesis
    upper X Superscript t Baseline upper X right-parenthesis Superscript negative
    1 Baseline upper X Superscript t Baseline ModifyingAbove y With right-arrow Subscript
    t r u e"><mrow><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi>
    <mi>t</mi></msup> <mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <msup><mi>X</mi> <mi>t</mi></msup> <msub><mover accent="true"><mi>y</mi> <mo>→</mo></mover>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></mrow></math> that
    we calculated gives the <math alttext="omega"><mi>ω</mi></math> values that make
    the training function *best fit* the training data, but too good of a fit means
    that the training function might also be picking up on the noise and not only
    on the signal in the data. So the above solution, or even the minimization problem
    itself, needs to be modified in order to *not get too good of a fit*. *Regularization*
    or *early stopping* are helpful here. We will spend some time on those in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was the long way to regression. We had to pass through calculus and linear
    algebra on the way, because we are just starting. Presenting the upcoming machine
    learning models: Logistic regression, support vector machines, decision trees,
    and random forests, will be faster, since all we do is apply the exact same ideas
    to different functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic Regression: Classify Into Two Classes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic Regression is mainly used for classification tasks. We first explain
    how we can use this model for binary classification tasks (classify into two classes,
    such as cancer/not cancer; safe for children/not safe; likely to pay back a loan/unlikely;
    *etc.*). Then we will generalize the model into classifying into multiple classes
    (for example, classify handwritten images of digits into 0, 1, 2, 3, 4, 5, 6,
    7, 8 or 9). Again, we have the same mathematical set up:'
  prefs: []
  type: TYPE_NORMAL
- en: Training function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to linear regression, the training function for logistic regression
    computes a linear combination of the features and adds a constant bias term, but
    instead of outputting the result as it is, it passes it through the *logistic
    function*, whose graph is plotted in [Figure 3-14](#Fig_logistic), and whose formula
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign sigma left-parenthesis s right-parenthesis equals
    StartFraction 1 Over 1 plus e Superscript negative s Baseline EndFraction dollar-sign"><mrow><mi>σ</mi>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac></mrow></math>![275](assets/emai_0314.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3-14\. Graph of the logistic function <math alttext="sigma left-parenthesis
    s right-parenthesis equals StartFraction 1 Over 1 plus e Superscript negative
    s Baseline EndFraction"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac></mrow></math>
    . Note that this function can be evaluated at any *s* and always outputs a number
    between *0* and *1*, hence, its output can be interpreted as a probability.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is a function that only takes values between *0* and *1*, so its output
    can be interpreted as a probability of a data point belonging to a certain class:
    If the output is less than *0.5* then classify the data point as belonging to
    the first class, and if the output is greater than *0.5* then classify the data
    point in the other class. The number *0.5* is the *threshold* where the decision
    to classify the data point is made.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the training function here ends up being a linear combination of
    features, plus bias, composed first with the logistic function, then finally composed
    with a thresholding function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals upper T h r e s h left-parenthesis sigma
    left-parenthesis omega 0 plus omega 1 x 1 plus ellipsis plus omega Subscript n
    Baseline x Subscript n Baseline right-parenthesis right-parenthesis dollar-sign"><mrow><mi>y</mi>
    <mo>=</mo> <mi>T</mi> <mi>h</mi> <mi>r</mi> <mi>e</mi> <mi>s</mi> <mi>h</mi> <mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the linear regression case, the <math alttext="omega"><mi>ω</mi></math>
    ’s are the unknowns that we need to optimize our loss function for. Just like
    linear regression, the number of these unknowns is equal to the number of data
    features, plus one for the bias term. For tasks like classifying images, each
    pixel is a feature, so we could have thousands of those.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s design a good *loss function* for classification. We are the engineers
    and we want to penalize wrongly classified training data points. In our labeled
    data set, if an instance belongs in a class then its <math alttext="y Subscript
    t r u e Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> , and if it doesn’t then its <math alttext="y
    Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: We want our training function to output <math alttext="y Subscript p r e d i
    c t Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> for training instances that belong in the
    positive class (whose <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    is also 1). Successful <math alttext="omega"><mi>ω</mi></math> values should give
    a high value of *t* (result of the linear combination step) to go into the logistic
    function, hence assigning high probability for positive instances and passing
    the 0.5 threshold to obtain <math alttext="y Subscript p r e d i c t Baseline
    equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> . Therefore, if the linear combination plus
    bias step gives a low *t* value while <math alttext="y Subscript t r u e Baseline
    equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> , penalize it.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, successful weight values should give a low *t* value to go into the
    logistic function for training instances that do not belong in the class (their
    true <math alttext="y Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    ). Therefore if the linear combination plus bias step gives a high *t* value while
    <math alttext="y Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> , penalize it.
  prefs: []
  type: TYPE_NORMAL
- en: 'So how do we find a loss function that penalizes a wrongly classified training
    data point? Both false positives and false negatives should be penalized. Recall
    that the outputs of this classification model are either *1* or *0*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of a calculus function that rewards *1* and penalizes *0*: <math alttext="minus
    log left-parenthesis s right-parenthesis"><mrow><mo>-</mo> <mo form="prefix">log</mo>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> (see [Figure 3-15](#Fig_log_s_log_1_s)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of a calculus function that penalizes *1* and rewards *0*: <math alttext="minus
    log left-parenthesis 1 minus s right-parenthesis"><mrow><mo>-</mo> <mo form="prefix">log</mo>
    <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>s</mi> <mo>)</mo></mrow></math> (see [Figure 3-15](#Fig_log_s_log_1_s)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![275](assets/emai_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-15\. Left: Graph of the function <math alttext="f left-parenthesis
    s right-parenthesis equals minus l o g left-parenthesis s right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo> <mo>=</mo> <mo>-</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> . This function assigns high values
    for numbers near *0* and low values for numbers near *1*. Right: Graph of the
    function <math alttext="f left-parenthesis s right-parenthesis equals minus l
    o g left-parenthesis 1 minus s right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <mi>s</mi> <mo>)</mo> <mo>=</mo> <mo>-</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>s</mi> <mo>)</mo></mrow></math> . This function assigns
    high values for numbers near *1* and low values for numbers near *0*.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now focus on the output of the logistic function <math alttext="sigma left-parenthesis
    s right-parenthesis"><mrow><mi>σ</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math>
    for the current choice of <math alttext="omega"><mi>ω</mi></math> ’s:'
  prefs: []
  type: TYPE_NORMAL
- en: If <math alttext="sigma left-parenthesis s right-parenthesis"><mrow><mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> is less than *0.5* (model prediction
    is <math alttext="y Subscript p r e d i c t Baseline equals 0"><mrow><msub><mi>y</mi>
    <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> ) but the true <math alttext="y Subscript
    t r u e Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> (a false negative) make the model pay by penalizing
    <math alttext="minus log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mo>(</mo> <mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo> <mo>)</mo></mrow></math> . If instead <math alttext="sigma
    left-parenthesis s right-parenthesis greater-than 0.5"><mrow><mi>σ</mi> <mo>(</mo>
    <mi>s</mi> <mo>)</mo> <mo>></mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>
    , so the model prediction is <math alttext="y Subscript p r e d i c t Baseline
    equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> ) (a true positive), <math alttext="minus
    log left-parenthesis sigma left-parenthesis s right-parenthesis right-parenthesis"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mo>(</mo> <mi>σ</mi> <mo>(</mo> <mi>s</mi> <mo>)</mo>
    <mo>)</mo></mrow></math> is small so no high penalty is paid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, if <math alttext="sigma left-parenthesis s right-parenthesis"><mrow><mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> is more than *0.5* but the the
    true <math alttext="y Subscript t r u e Baseline equals 0"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math>
    (a false positive), make the model pay by penalizing <math alttext="minus log
    left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis right-parenthesis"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mo>(</mo>
    <mi>s</mi> <mo>)</mo> <mo>)</mo></mrow></math> . Again, for a true negative no
    high penalty is paid either.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, we can write the cost for misclassifying one training instance <math
    alttext="left-parenthesis x 1 Superscript i Baseline comma x 2 Superscript i Baseline
    comma ellipsis comma x Subscript n Superscript i Baseline semicolon y Subscript
    t r u e Baseline right-parenthesis"><mrow><mo>(</mo> <msubsup><mi>x</mi> <mn>1</mn>
    <mi>i</mi></msubsup> <mo>,</mo> <msubsup><mi>x</mi> <mn>2</mn> <mi>i</mi></msubsup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup>
    <mo>;</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow></math> as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign c o s t equals Start 2 By 2 Matrix 1st Row 1st Column
    Blank 2nd Column minus log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 1 2nd Row 1st Column
    Blank 2nd Column minus log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 0 EndMatrix equals minus
    y Subscript t r u e Baseline log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis minus left-parenthesis 1 minus y Subscript t r u e Baseline
    right-parenthesis log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis dollar-sign"><mrow><mi>c</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi>
    <mo>=</mo> <mfenced close="}" open="{" separators=""><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the loss function is the average cost over *m* training instances,
    giving us the formula for the popular *cross entropy loss function*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column upper L left-parenthesis
    ModifyingAbove omega With right-arrow right-parenthesis equals 2nd Column minus
    StartFraction 1 Over m EndFraction sigma-summation Underscript i equals 1 Overscript
    m Endscripts y Subscript t r u e Superscript i Baseline log left-parenthesis sigma
    left-parenthesis omega 0 plus omega 1 x 1 Superscript i Baseline plus ellipsis
    plus omega Subscript n Baseline x Subscript n Superscript i Baseline right-parenthesis
    right-parenthesis plus 2nd Row 1st Column Blank 2nd Column left-parenthesis 1
    minus y Subscript t r u e Superscript i Baseline right-parenthesis log left-parenthesis
    1 minus sigma left-parenthesis omega 0 plus omega 1 x 1 Superscript i Baseline
    plus ellipsis plus omega Subscript n Baseline x Subscript n Superscript i Baseline
    right-parenthesis right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>L</mi> <mo>(</mo> <mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mo>)</mo> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>-</mo>
    <mfrac><mn>1</mn> <mi>m</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>+</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>ω</mi> <mi>n</mi></msub> <msubsup><mi>x</mi>
    <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the linear regression case, if we decide to minimize the loss function
    by setting <math alttext="normal nabla upper L left-parenthesis omega right-parenthesis
    equals 0"><mrow><mi>∇</mi> <mi>L</mi> <mo>(</mo> <mi>ω</mi> <mo>)</mo> <mo>=</mo>
    <mn>0</mn></mrow></math> there is no closed form solution formula for the <math
    alttext="omega"><mi>ω</mi></math> ’s. The good news is that this function is convex
    so next chapter’s gradient descent (or stochastic or mini-batch gradient descents)
    is guaranteed to find a minimum (if the *learning rate* is not too large and if
    we wait long enough).
  prefs: []
  type: TYPE_NORMAL
- en: 'Softmax Regression: Classify Into Multiple Classes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can easily generalize the logistic regression idea to classify into multiple
    classes. A famous example for such non-binary classification task is classifying
    images of the ten handwritten numerals 0, 1, 2, 3, 4, 5, 6, 7, 8 and 9 using the
    [MNIST data set](http://yann.lecun.com/exdb/mnist/). This data set contains 70,000
    images of handwritten numerals (see [Figure 3-16](#Fig_MNIST) for samples of these
    images), split into 60,000 training subset and 10,000 test subset. Each image
    is labeled with the class it belongs to, which is one of the ten numerals.
  prefs: []
  type: TYPE_NORMAL
- en: '![275](assets/emai_0316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3-16\. Sample images from the MNIST data set. ([Image source: Wikipedia](https://en.wikipedia.org/wiki/MNIST_database)).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The [link for this data set](http://yann.lecun.com/exdb/mnist/) also contains
    results from many classifying models, including linear classifiers, *k-nearest
    neighbors*, *decision trees*, *support vector machines* with various *kernels*,
    and neural networks with various architectures, along with references for the
    corresponding papers and their years of publication. It is interesting to see
    the progress in performance as the years go by and as the methods evolve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Careful: Do not confuse classifying into multiple classes with multi-output
    models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Softmax regression predicts one class at a time, so we cannot use it to classify,
    for example, five people in the same image. Instead, we can use it to check whether
    a given Facebook image is a picture of me, my sister, my brother, my husband,
    or my daughter. An image passed into the softmax regression model can have only
    one of the five of us or the model’s classification would be less obvious. This
    means that our classes have to be mutually exclusive. So when Facebook automatically
    tags five people in the same image, they are using a muti-output model, not a
    softmax regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have the features of a data point and we want to use this information
    in order to classify the data point into one of *k* possible classes. The following
    training function, loss function, and optimization process should be clear by
    now.
  prefs: []
  type: TYPE_NORMAL
- en: Note About Features of Image Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For greyscale images, each pixel intensity is a feature, so images usually have
    thousands of features. Greyscale images are usually represented as two dimensional
    matrices of numbers, with pixel intensities as the matrix entries. Color images
    come in three channels, red, green, and blue, where each channel is again represented
    as a two dimensional matrix of numbers, and the channels are stacked on top on
    each other, forming three layers of two dimensional matrices. This structure is
    called a tensor. Check out this [linked Jupyter Notebook] that illustrates how
    we can work with greyscale and color images in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Training function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is always the same: Linearly combine the features and add a
    constant bias term. In logistic regression, when we only had two classes, we passed
    the result into the logistic function, of formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign sigma left-parenthesis s right-parenthesis equals
    StartFraction 1 Over 1 plus e Superscript negative s Baseline EndFraction equals
    StartStartFraction 1 OverOver 1 plus StartFraction 1 Over e Superscript s Baseline
    EndFraction EndEndFraction equals StartFraction e Superscript s Baseline Over
    1 plus e Superscript s Baseline EndFraction equals StartFraction e Superscript
    s Baseline Over e Superscript 0 Baseline plus e Superscript s Baseline EndFraction
    comma dollar-sign"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>s</mi></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn> <msup><mi>e</mi>
    <mi>s</mi></msup></mfrac></mrow></mfrac> <mo>=</mo> <mfrac><msup><mi>e</mi> <mi>s</mi></msup>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mi>s</mi></msup></mrow></mfrac> <mo>=</mo>
    <mfrac><msup><mi>e</mi> <mi>s</mi></msup> <mrow><msup><mi>e</mi> <mn>0</mn></msup>
    <mo>+</mo><msup><mi>e</mi> <mi>s</mi></msup></mrow></mfrac> <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: which we interpreted as the probability of the data point belonging in the class
    of interest or not. Note that we rewrote the formula for the logistic function
    as <math alttext="sigma left-parenthesis s right-parenthesis equals StartFraction
    e Superscript s Baseline Over e Superscript 0 Baseline plus e Superscript s Baseline
    EndFraction"><mrow><mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><msup><mi>e</mi> <mi>s</mi></msup> <mrow><msup><mi>e</mi> <mn>0</mn></msup>
    <mo>+</mo><msup><mi>e</mi> <mi>s</mi></msup></mrow></mfrac></mrow></math> in order
    to highlight the fact that it captures two probabilities, one for each class.
    In other words, <math alttext="sigma left-parenthesis s right-parenthesis"><mrow><mi>σ</mi>
    <mo>(</mo> <mi>s</mi> <mo>)</mo></mrow></math> gives the probability that a data
    point is in the class of interest, and <math alttext="1 minus sigma left-parenthesis
    s right-parenthesis equals StartFraction e Superscript 0 Baseline Over e Superscript
    0 Baseline plus e Superscript s Baseline EndFraction"><mrow><mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mn>0</mn></msup> <mrow><msup><mi>e</mi> <mn>0</mn></msup> <mo>+</mo><msup><mi>e</mi>
    <mi>s</mi></msup></mrow></mfrac></mrow></math> gives the probability that the
    data point is not in the class.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have multiple classes instead of only two, then for the same data point,
    we repeat same process multiple times: One time for each class. Each class has
    its own bias and set of weights that linearly combine the features, thus, given
    a data point with feature values <math alttext="x 1"><msub><mi>x</mi> <mn>1</mn></msub></math>
    , <math alttext="x 2"><msub><mi>x</mi> <mn>2</mn></msub></math> ,…, and <math
    alttext="x Subscript n"><msub><mi>x</mi> <mi>n</mi></msub></math> , we compute
    *k* different linear combinations plus biases:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column s Superscript 1 2nd
    Column equals omega 0 Superscript 1 Baseline plus omega 1 Superscript 1 Baseline
    x 1 plus omega 2 Superscript 1 Baseline x 2 plus ellipsis plus omega Subscript
    n Superscript 1 Baseline x Subscript n Baseline 2nd Row 1st Column s squared 2nd
    Column equals omega 0 squared plus omega 1 squared x 1 plus omega 2 squared x
    2 plus ellipsis plus omega Subscript n Superscript 2 Baseline x Subscript n Baseline
    3rd Row 1st Column Blank 4th Row 1st Column  ellipsis 5th Row 1st Column s Superscript
    k 2nd Column equals omega 0 Superscript k Baseline plus omega 1 Superscript k
    Baseline x 1 plus omega 2 Superscript k Baseline x 2 plus ellipsis plus omega
    Subscript n Superscript k Baseline x Subscript n Baseline period EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><msup><mi>s</mi> <mn>1</mn></msup></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mi>ω</mi> <mn>0</mn> <mn>1</mn></msubsup>
    <mo>+</mo> <msubsup><mi>ω</mi> <mn>1</mn> <mn>1</mn></msubsup> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <msubsup><mi>ω</mi> <mn>2</mn> <mn>1</mn></msubsup>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi>
    <mi>n</mi> <mn>1</mn></msubsup> <msub><mi>x</mi> <mi>n</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>s</mi> <mn>2</mn></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>2</mn> <mn>2</mn></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mn>2</mn></msubsup>
    <msub><mi>x</mi> <mi>n</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr>
    <mtr><mtd columnalign="right"><msup><mi>s</mi> <mi>k</mi></msup></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mi>k</mi></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mi>k</mi></msubsup> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo>
    <msubsup><mi>ω</mi> <mn>2</mn> <mi>k</mi></msubsup> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mi>k</mi></msubsup>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Get Into Good Habits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You want to get into the good habit of keeping track of how many unknown <math
    alttext="omega"><mi>ω</mi></math> ’s end up in the formula for your training function.
    Recall that these are the <math alttext="omega"><mi>ω</mi></math> ’s that we find
    via minimizing a loss function. The other good habit is having an efficient and
    consistent way to organize them throughout your model (in a vector, matrix, *etc.*).
    In the softmax case, when we have *k* classes, and *n* features for each data
    point, we end up with <math alttext="k times n"><mrow><mi>k</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    <math alttext="omega"><mi>ω</mi></math> ’s for the linear combinations, then *k*
    biases, for a total of <math alttext="k times n plus k"><mrow><mi>k</mi> <mo>×</mo>
    <mi>n</mi> <mo>+</mo> <mi>k</mi></mrow></math> unknown <math alttext="omega"><mi>ω</mi></math>
    ’s. For example, if we use a softmax regression model to classify images in the
    [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database) of handwritten
    numerals, each image has <math alttext="28 times 28"><mrow><mn>28</mn> <mo>×</mo>
    <mn>28</mn></mrow></math> pixels, meaning 784 features, and we want to classify
    them into 10 classes, so we end up having to optimize for 7850 <math alttext="omega"><mi>ω</mi></math>
    ’s. For both the linear and logistic regression models, we only had <math alttext="n
    plus 1"><mrow><mi>n</mi> <mo>+</mo> <mn>1</mn></mrow></math> unknown <math alttext="omega"><mi>ω</mi></math>
    ’s that we needed to optimize for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we pass each of these *k* results into function called the *softmax function*,
    that generalizes the logistic function from two to multiple classes, and we also
    interpret it as a probability. The formula for the softmax function looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign sigma left-parenthesis s Superscript j Baseline right-parenthesis
    equals StartFraction e Superscript s Super Superscript j Superscript Baseline
    Over e Superscript s Super Superscript 1 Superscript Baseline plus e Superscript
    s squared Baseline plus ellipsis plus e Superscript s Super Superscript k Superscript
    Baseline EndFraction dollar-sign"><mrow><mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mi>j</mi></msup> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi> <msup><mi>s</mi>
    <mi>j</mi></msup></msup> <mrow><msup><mi>e</mi> <msup><mi>s</mi> <mn>1</mn></msup></msup>
    <mo>+</mo><msup><mi>e</mi> <msup><mi>s</mi> <mn>2</mn></msup></msup> <mo>+</mo><mo>⋯</mo><mo>+</mo><msup><mi>e</mi>
    <msup><mi>s</mi> <mi>k</mi></msup></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This way, the same data point will get *k* probability scores, one score corresponding
    to each class. Finally, we classify the data point as belonging to the class where
    it obtained the largest probability score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aggregating all of the above, we obtain the final formula of the training function
    that we can now use for classification (that is, after we find the optimal <math
    alttext="omega"><mi>ω</mi></math> values by minimizing an appropriate loss function):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y equals j such that sigma left-parenthesis omega
    0 Superscript j Baseline plus omega 1 Superscript j Baseline x 1 plus ellipsis
    plus omega Subscript n Superscript j Baseline x Subscript n Baseline right-parenthesis
    is maximal period dollar-sign"><mrow><mi>y</mi> <mo>=</mo> <mi>j</mi> <mtext>such</mtext>
    <mtext>that</mtext> <mi>σ</mi> <mo>(</mo> <msubsup><mi>ω</mi> <mn>0</mn> <mi>j</mi></msubsup>
    <mo>+</mo> <msubsup><mi>ω</mi> <mn>1</mn> <mi>j</mi></msubsup> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi>
    <mi>j</mi></msubsup> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo> <mtext>is</mtext>
    <mtext>maximal.</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that for the above training function, all we have to do is input the data
    features (the *x* values), and it returns one class number: *j*.'
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We derived the cross entropy loss function in the case of logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals minus StartFraction 1 Over m EndFraction
    sigma-summation Underscript i equals 1 Overscript m Endscripts y Subscript t r
    u e Superscript i Baseline log left-parenthesis sigma left-parenthesis omega 0
    plus omega 1 x 1 Superscript i Baseline plus ellipsis plus omega Subscript n Baseline
    x Subscript n Superscript i Baseline right-parenthesis right-parenthesis plus
    left-parenthesis 1 minus y Subscript t r u e Superscript i Baseline right-parenthesis
    log left-parenthesis 1 minus sigma left-parenthesis omega 0 plus omega 1 x 1 Superscript
    i Baseline plus ellipsis plus omega Subscript n Baseline x Subscript n Superscript
    i Baseline right-parenthesis right-parenthesis comma dollar-sign"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup> <msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>i</mi></msubsup> <mo
    form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi>
    <mn>1</mn> <mi>i</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>ω</mi>
    <mi>n</mi></msub> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: using
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign c o s t equals Start 2 By 2 Matrix 1st Row 1st Column
    Blank 2nd Column minus log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 1 2nd Row 1st Column
    Blank 2nd Column minus log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis if y Subscript t r u e Baseline equals 0 EndMatrix equals minus
    y Subscript t r u e Baseline log left-parenthesis sigma left-parenthesis s right-parenthesis
    right-parenthesis minus left-parenthesis 1 minus y Subscript t r u e Baseline
    right-parenthesis log left-parenthesis 1 minus sigma left-parenthesis s right-parenthesis
    right-parenthesis dollar-sign"><mrow><mi>c</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi>
    <mo>=</mo> <mfenced close="}" open="{" separators=""><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we generalize the same logic to multiple classes. Let’s use the notation
    that <math alttext="y Subscript t r u e comma i Baseline equals 1"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> if a certain data point belongs in the i’th
    class and is zero otherwise. Then we have the cost associate with misclassifying
    a certain data point as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign c o s t equals Start 5 By 2 Matrix 1st Row 1st Column
    Blank 2nd Column minus log left-parenthesis sigma left-parenthesis s Superscript
    1 Baseline right-parenthesis right-parenthesis if y Subscript t r u e comma 1
    Baseline equals 1 2nd Row 1st Column Blank 2nd Column minus log left-parenthesis
    sigma left-parenthesis s squared right-parenthesis right-parenthesis if y Subscript
    t r u e comma 2 Baseline equals 1 3rd Row 1st Column Blank 2nd Column minus log
    left-parenthesis sigma left-parenthesis s cubed right-parenthesis right-parenthesis
    if y Subscript t r u e comma 3 Baseline equals 1 4th Row 1st Column  ellipsis
    5th Row 1st Column Blank 2nd Column minus log left-parenthesis sigma left-parenthesis
    s Superscript k Baseline right-parenthesis right-parenthesis if y Subscript t
    r u e comma k Baseline equals 1 EndMatrix equals minus y Subscript t r u e comma
    1 Baseline log left-parenthesis sigma left-parenthesis s Superscript 1 Baseline
    right-parenthesis right-parenthesis minus ellipsis minus y Subscript t r u e comma
    k Baseline log left-parenthesis sigma left-parenthesis s Superscript k Baseline
    right-parenthesis right-parenthesis period dollar-sign"><mrow><mi>c</mi> <mi>o</mi>
    <mi>s</mi> <mi>t</mi> <mo>=</mo> <mfenced close="}" open="{" separators=""><mtable
    displaystyle="true"><mtr><mtd columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi> <mn>1</mn></msup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>2</mn></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>-</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mn>3</mn></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>3</mn></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi> <mi>k</mi></msup> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mtext>if</mtext> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>k</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mn>1</mn></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mo>⋯</mo> <mo>-</mo>
    <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>k</mi></mrow></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo> <msup><mi>s</mi>
    <mi>k</mi></msup> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Averaging over all the *m* data points in the training set, we obtain the *generalized
    cross entropy loss function*, generalizing the cross entropy loss function from
    the case of only two classes to the case of multiple classes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals minus StartFraction 1 Over m EndFraction
    sigma-summation Underscript i equals 1 Overscript m Endscripts y Subscript t r
    u e comma 1 Superscript i Baseline log left-parenthesis sigma left-parenthesis
    omega 0 Superscript 1 Baseline plus omega 1 Superscript 1 Baseline x 1 Superscript
    i Baseline plus ellipsis plus omega Subscript n Superscript 1 Baseline x Subscript
    n Superscript i Baseline right-parenthesis right-parenthesis plus y Subscript
    t r u e comma 2 Superscript i Baseline log left-parenthesis sigma left-parenthesis
    omega 0 squared plus omega 1 squared x 1 Superscript i Baseline plus ellipsis
    plus omega Subscript n Superscript 2 Baseline x Subscript n Superscript i Baseline
    right-parenthesis right-parenthesis plus ellipsis plus y Subscript t r u e comma
    k Superscript i Baseline log left-parenthesis sigma left-parenthesis omega 0 Superscript
    k Baseline plus omega 1 Superscript k Baseline x 1 Superscript i Baseline plus
    ellipsis plus omega Subscript n Superscript k Baseline x Subscript n Superscript
    i Baseline right-parenthesis right-parenthesis dollar-sign"><mrow><mi>L</mi> <mrow><mo>(</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <mo>-</mo> <mfrac><mn>1</mn> <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>1</mn></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mn>1</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>1</mn></msubsup> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mn>1</mn></msubsup>
    <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mn>2</mn></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mn>2</mn></msubsup>
    <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mo>,</mo><mi>k</mi></mrow>
    <mi>i</mi></msubsup> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>σ</mi> <mrow><mo>(</mo>
    <msubsup><mi>ω</mi> <mn>0</mn> <mi>k</mi></msubsup> <mo>+</mo> <msubsup><mi>ω</mi>
    <mn>1</mn> <mi>k</mi></msubsup> <msubsup><mi>x</mi> <mn>1</mn> <mi>i</mi></msubsup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msubsup><mi>ω</mi> <mi>n</mi> <mi>k</mi></msubsup>
    <msubsup><mi>x</mi> <mi>n</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a formula for the loss function, we can search for its minimizing
    <math alttext="omega"><mi>ω</mi></math> ’s. As most of the loss functions that
    we will encounter, there is no explicit formula for the minimizers of this loss
    function in terms of the training set and their target labels, so we settle for
    finding the minimizers using numerical methods, in particular: Next chapter’s
    gradient descent, stochastic gradient descent, or mini-batch gradient descent.
    Again, the generalized cross entropy loss function has its convexity working to
    our advantage in the minimization process, so we are guaranteed to find our sought
    after <math alttext="omega"><mi>ω</mi></math> ’s.'
  prefs: []
  type: TYPE_NORMAL
- en: Note About Cross Entropy And Information Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cross entropy concept is borrowed from information theory. We will elaborate
    on this when discussing decision trees later in this chapter. For now, keep the
    following quantity in mind, where *p* is the probability of an event occurring:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign log left-parenthesis StartFraction 1 Over p EndFraction
    right-parenthesis equals minus log left-parenthesis p right-parenthesis dollar-sign"><mrow><mo
    form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn> <mi>p</mi></mfrac> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The above quantity is large when *p* is small, therefore, it quantifies bigger
    *surprise* for *less probable* events.
  prefs: []
  type: TYPE_NORMAL
- en: Note About The Logistic and The Softmax Functions and Statistical Mechanics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with statistical mechanics, you might have noticed that
    the logistic and softmax functions calculate probabilities in the same way the
    *partition function* from the field of statistical mechanics calculates the probability
    of finding a system in a certain state.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating The Above Models Into The Last Layer Of A Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The linear regression model makes its predictions by appropriately linearly
    combining data features, then adding bias. The logistic regression and the softmax
    regression models make their classifications by appropriately linearly combining
    data features, adding bias, then passing the result into a probability scoring
    function. In these simple models, the features of the data are only linearly combined,
    hence, these models are weak in terms of picking up on potentialy important nonlinear
    interactions between the features of the data. Neural network models incorporate
    nonlinear *activation functions* into their training functions, and do this over
    multiple layers, hence are better equipped to detect nonlinear and more complex
    relationships. The last layer of a neural network is its output layer. The layer
    right before the last layer spits out some higher order features and inputs them
    into the last layer. If we want our network to classify data into multiple classes,
    then we can make our last layer a softmax layer; if we want it to classify into
    two classes, then our last layer can be a logistic regression layer; and if we
    want the network to predict numerical values, then we can make its last layer
    a regression layer. We will see examples of these in [Chapter 5](ch05.xhtml#ch05).
  prefs: []
  type: TYPE_NORMAL
- en: Other Popular Machine Learning Techniques and Ensembles of Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After regression and logistic regression, it is important to branch out into
    the machine learning community and learn the ideas behind some of the most popular
    techniques for classification and regression tasks. *Support vector machines*,
    *decision trees*, and *random forests* are very powerful and popular, and are
    able to perform both classification and regression tasks. The natural question
    is then, when do we use a specific machine learning method, including linear and
    logistic regression, and later neural networks? How do we know which method to
    use and base our conclusions and predictions on? These are the types of questions
    where the mathematical analysis of the machine learning models helps.
  prefs: []
  type: TYPE_NORMAL
- en: Since the mathematical analysis of each method, including the types of data
    sets it is usually best suited for, is only now gaining serious attention, after
    the recent increase in resource allocation for research in AI, machine learning,
    and data science, the current practice is to try out each method on the same data
    set and use the one with the best results. That is, assuming we have the required
    computational and time resources to try out different machine learning techniques.
    Even better, if you do have the time and resources to train various machine learning
    models (parallel computing is perfect here), then *ensemble methods* combine their
    results, either by averaging or by voting, ironically, yet mathematically sound,
    giving better results than the best individual performers, *and even when the
    best performers are weak performers!*
  prefs: []
  type: TYPE_NORMAL
- en: 'One example of an ensemble is a random forest: It is an ensemble of decision
    trees.'
  prefs: []
  type: TYPE_NORMAL
- en: When basing our predictions on ensembles, industry terms like *bagging* (or
    *bootstrap aggregating*), *pasting*, *boosting* such as *ADA boost* and *Gradient
    boosting*, *stacking*, and *random patches* appear. Bagging and pasting train
    *the same* machine learning model on different random subsets of the training
    set. Bagging samples instances from the training set with replacement and pasting
    samples instances from the training set without replacement. *Random patches*
    sample from the feature space as well, training a machine learning model on a
    random subset of the features at a time. This is very helpful when the data set
    has many many features, such as images (where each pixel is a feature). *Stacking
    learns* the prediction mechanism of the ensemble instead of simple voting or averaging.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Support vector machine is an extremely popular machine learning method able
    to perform classification and regression tasks with both linear (flat) and nonlinear
    (curved) decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: For classification, this method seeks to separate the labeled data using a widest
    possible margin, resulting in an optimal *highway* of separation as opposed to
    a thin line of separation. Let’s explain how support vector machines classify
    labeled data instances in the context of this chapter’s structure of training
    function, loss function and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Training function
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again we linearly combine the features of a data point with unknown weights
    <math alttext="omega"><mi>ω</mi></math> ’s and add bias <math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math> . We then pass the answer through the *sign* function:
    If the linear combination of features plus bias is a positive number, return 1
    (or classify in the first class), if it is negative, return -1 (or classify in
    the other). So the formula for the training function becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign f left-parenthesis ModifyingAbove omega With right-arrow
    semicolon ModifyingAbove x With right-arrow right-parenthesis equals s i g n left-parenthesis
    ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove x
    With right-arrow plus omega 0 right-parenthesis dollar-sign"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>;</mo>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo>
    <mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Loss Function
  prefs: []
  type: TYPE_NORMAL
- en: 'We must design a loss function that penalizes misclassified points. For logistic
    regression, we used the cross entropy loss function. For support vector machines,
    our loss function is based on a function called the *hinge loss function*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign max left-parenthesis 0 comma 1 minus y Subscript
    t r u e Baseline left-parenthesis ModifyingAbove omega With right-arrow Superscript
    t Baseline ModifyingAbove x With right-arrow plus omega 0 right-parenthesis right-parenthesis
    period dollar-sign"><mrow><mo form="prefix" movablelimits="true">max</mo> <mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the hinge loss function penalizes errors in classification. First,
    recall that <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    is either 1 or -1, depending on whether the data point belongs in the positive
    or the negative class.
  prefs: []
  type: TYPE_NORMAL
- en: If for a certain data point <math alttext="y Subscript t r u e"><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> is 1 but <math
    alttext="ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove
    x With right-arrow plus omega 0 less-than 0"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>0</mn></mrow></math>
    , the training function will misclassify it and give us <math alttext="y Subscript
    p r e d i c t Baseline equals negative 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mo>-</mo> <mn>1</mn></mrow></math> , and the hinge loss function’s
    value will be <math alttext="1 minus left-parenthesis 1 right-parenthesis left-parenthesis
    ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove x
    With right-arrow plus omega 0 right-parenthesis greater-than 1"><mrow><mn>1</mn>
    <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>)</mo></mrow> <mrow><mo>(</mo> <msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow>
    <mo>></mo> <mn>1</mn></mrow></math> which is a high penalty when your goal is
    to minimize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If on the other hand the <math alttext="y Subscript t r u e"><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> is 1 and <math
    alttext="ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove
    x With right-arrow plus omega 0 less-than 0"><mrow><msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>0</mn></mrow></math>
    , the training function will correctly classify it and give us <math alttext="y
    Subscript p r e d i c t Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> . The hinge loss function, however, is designed
    in such a way that would still penalize us if <math alttext="ModifyingAbove omega
    With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow plus
    omega 0 less-than 1"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>1</mn></mrow></math> , and its
    value will be <math alttext="1 minus left-parenthesis 1 right-parenthesis left-parenthesis
    ModifyingAbove omega With right-arrow Superscript t Baseline ModifyingAbove x
    With right-arrow plus omega 0 right-parenthesis"><mrow><mn>1</mn> <mo>-</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>)</mo></mrow> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mrow></math>
    which is now less than one but still bigger than zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only when <math alttext="y Subscript t r u e"><msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math>
    is 1 and <math alttext="ModifyingAbove omega With right-arrow Superscript t Baseline
    ModifyingAbove x With right-arrow plus omega 0 less-than 1"><mrow><msup><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo><</mo> <mn>1</mn></mrow></math>
    (the training function will still correctly classify this point and give <math
    alttext="y Subscript p r e d i c t Baseline equals 1"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> ) the hinge loss function value will be zero
    since it will be the maximum between zero and a negative quantity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The same logic applies when <math alttext="y Subscript t r u e"><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub></math> is -1: The
    hinge loss function will penalize a lot for a wrong prediction, a little for a
    right prediction but if it doesn’t have far enough *margin* from the *zero divider*
    (a margin bigger than 1), and will return zero only when the prediction is right
    and the point is at a distance larger than 1 from the zero divider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the zero divider has equation <math alttext="ModifyingAbove omega
    With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow plus
    omega 0 equals 0"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math> , and the
    margin edges have equations <math alttext="ModifyingAbove omega With right-arrow
    Superscript t Baseline ModifyingAbove x With right-arrow plus omega 0 equals negative
    1"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <mi>t</mi></msup>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>=</mo> <mo>-</mo> <mn>1</mn></mrow></math> and <math alttext="ModifyingAbove
    omega With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow
    plus omega 0 equals 1"><mrow><msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>+</mo>
    <msub><mi>ω</mi> <mn>0</mn></msub> <mo>=</mo> <mn>1</mn></mrow></math> . The distance
    between the margin edges is easy to calculate as <math alttext="StartFraction
    2 Over parallel-to omega parallel-to EndFraction"><mfrac><mn>2</mn> <msub><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></mfrac></math> . So if we want to increase this margin width,
    we have to decrease <math alttext="parallel-to omega parallel-to"><msub><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn></msub></math> , thus, this term must enter the loss function, along
    with the hingle loss function, which penalizes both misclassified points and points
    within the margin boundaries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now if we average the hinge loss over all the m data points in the training
    set, and add <math alttext="parallel-to omega parallel-to"><msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></math> , we obtain the formula for the loss function
    that is commonly used for support vector machines:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L left-parenthesis ModifyingAbove omega With
    right-arrow right-parenthesis equals StartFraction 1 Over m EndFraction sigma-summation
    Underscript i equals 1 Overscript m Endscripts max left-parenthesis 0 comma 1
    minus y Subscript t r u e Superscript i Baseline left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow
    Superscript i Baseline plus omega 0 right-parenthesis right-parenthesis plus lamda
    parallel-to omega parallel-to dollar-sign"><mrow><mi>L</mi> <mrow><mo>(</mo> <mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mo form="prefix" movablelimits="true">max</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow> <mn>2</mn>
    <mn>2</mn></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Optimization
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal now is to search for the <math alttext="ModifyingAbove w With right-arrow"><mover
    accent="true"><mi>w</mi> <mo>→</mo></mover></math> that minimizes the loss function.
    Let’s observe this loss function for a minute:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It has two terms: <math alttext="StartFraction 1 Over m EndFraction sigma-summation
    Underscript i equals 1 Overscript m Endscripts max left-parenthesis 0 comma 1
    minus y Subscript t r u e Superscript i Baseline left-parenthesis ModifyingAbove
    omega With right-arrow Superscript t Baseline ModifyingAbove x With right-arrow
    Superscript i Baseline plus omega 0 right-parenthesis right-parenthesis"><mrow><mfrac><mn>1</mn>
    <mi>m</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mo form="prefix" movablelimits="true">max</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
    and <math alttext="lamda parallel-to ModifyingAbove omega With right-arrow parallel-to"><mrow><mrow><mi>λ</mi>
    <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></math> . Whenever we have more than one
    term in an optimization problem, it is most likely that they are competing terms,
    in the sense that the same <math alttext="omega"><mi>ω</mi></math> values that
    make the first term small and thus happy might make the second term big and thus
    sad. So it is a push and pull game between the two terms as we search for the
    <math alttext="ModifyingAbove omega With right-arrow"><mover accent="true"><mi>ω</mi>
    <mo>→</mo></mover></math> that optimizes their sum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The <math alttext="lamda"><mi>λ</mi></math> that appears with the <math alttext="lamda
    parallel-to ModifyingAbove omega With right-arrow parallel-to"><mrow><mrow><mi>λ</mi>
    <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></math> term is an example of a model hyperparameter
    that we can tune during the validation stage of the training process. Note that
    controlling the value of <math alttext="lamda"><mi>λ</mi></math> helps us control
    the width of the margin this way: If we choose a large <math alttext="lamda"><mi>λ</mi></math>
    value, the optimizer will get busy choosing <math alttext="ModifyingAbove omega
    With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math> with
    very low <math alttext="parallel-to omega parallel-to"><msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></math> , in order to compensate for that large
    <math alttext="lamda"><mi>λ</mi></math> , and the first term of the loss function
    will get less attention. But recall that a smaller <math alttext="parallel-to
    omega parallel-to"><msub><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow> <mn>2</mn></msub></math>
    means a larger margin!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The <math alttext="lamda parallel-to ModifyingAbove omega With right-arrow parallel-to"><mrow><mrow><mi>λ</mi>
    <mo>∥</mo></mrow> <mover accent="true"><mi>ω</mi> <mo>→</mo></mover> <msubsup><mrow><mo>∥</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup></mrow></math> term can also be thought of as a
    *regularization term*, which we will discuss in the next chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This loss function is convex and bounded below by zero so its minimization
    problem is not too bad: We don’t have to worry about getting stuck at local minima.
    The first term has a singularity, but as we mentioned before we can define its
    subgradient at the singular point then apply a descent method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some optimization problems can be reformulated and instead of solving the original
    *primal* problem, we end up solving its *dual* problem! Usually, one is easier
    to solve than the other. We can think of the dual problem as another optimization
    problem living in a parallel universe of the primal problem. The universes meet
    at the optimizer. Hence solving one problem automatically gives the solution of
    the other. We study duality when we study optimization. Of particular interest
    and huge applications areas, are linear and quadratic optimization, also known
    as linear and quadratic programming. The minimization problem that we currently
    have:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign min Underscript ModifyingAbove omega With right-arrow
    Endscripts StartFraction 1 Over m EndFraction sigma-summation Underscript i equals
    1 Overscript m Endscripts max left-parenthesis 0 comma 1 minus y Subscript t r
    u e Superscript i Baseline left-parenthesis ModifyingAbove omega With right-arrow
    Superscript t Baseline ModifyingAbove x With right-arrow Superscript i Baseline
    plus omega 0 right-parenthesis right-parenthesis plus lamda parallel-to omega
    parallel-to dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">min</mo>
    <mover accent="true"><mi>ω</mi> <mo>→</mo></mover></msub> <mfrac><mn>1</mn> <mi>m</mi></mfrac>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup>
    <mo form="prefix" movablelimits="true">max</mo> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo>
    <mn>1</mn> <mo>-</mo> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mrow><mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>i</mi></msup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>+</mo> <mi>λ</mi> <msubsup><mrow><mo>∥</mo><mi>ω</mi><mo>∥</mo></mrow> <mn>2</mn>
    <mn>2</mn></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'is an example of quadratic programming, and it has a dual problem formulation
    that turns out to be easier to optimize than the primal (especially when the number
    of features is high):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign max Underscript ModifyingAbove alpha With right-arrow
    Endscripts sigma-summation Underscript j equals 1 Overscript m Endscripts alpha
    Subscript j minus one-half sigma-summation Underscript j equals 1 Overscript m
    Endscripts sigma-summation Underscript k equals 1 Overscript m Endscripts alpha
    Subscript j Baseline alpha Subscript k Baseline y Subscript t r u e Superscript
    j Baseline y Subscript t r u e Superscript k Baseline left-parenthesis left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis Superscript
    t Baseline ModifyingAbove x With right-arrow Superscript k Baseline right-parenthesis
    dollar-sign"><mrow><msub><mo form="prefix" movablelimits="true">max</mo> <mover
    accent="true"><mi>α</mi> <mo>→</mo></mover></msub> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>α</mi> <mi>j</mi></msub> <mo>-</mo> <mfrac><mn>1</mn>
    <mn>2</mn></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>α</mi> <mi>j</mi></msub> <msub><mi>α</mi> <mi>k</mi></msub>
    <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>j</mi></msubsup>
    <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow> <mi>k</mi></msubsup>
    <mrow><mo>(</mo> <msup><mrow><mo>(</mo><msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mi>j</mi></msup> <mo>)</mo></mrow> <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>k</mi></msup> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: subject to the constraints <math alttext="alpha Subscript j Baseline greater-than-or-equal-to
    0"><mrow><msub><mi>α</mi> <mi>j</mi></msub> <mo>≥</mo> <mn>0</mn></mrow></math>
    and <math alttext="sigma-summation Underscript j equals 1 Overscript m Endscripts
    alpha Subscript j Baseline y Subscript t r u e Superscript j Baseline equals 0"><mrow><msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></msubsup> <msub><mi>α</mi>
    <mi>j</mi></msub> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>j</mi></msubsup> <mo>=</mo> <mn>0</mn></mrow></math> . Writing the above formula
    is usually straight forward when we learn about primal and dual problems, so we
    skip the derivation in favor of not interrupting our flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quadratic programming is a very well developed field and there are many software
    packages that can solve this problem. Once we find the maximizing <math alttext="ModifyingAbove
    alpha With right-arrow"><mover accent="true"><mi>α</mi> <mo>→</mo></mover></math>
    , we can find the vector <math alttext="ModifyingAbove omega With right-arrow"><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover></math> that minimizes the primal problem
    using <math alttext="ModifyingAbove omega With right-arrow equals sigma-summation
    Underscript j equals 1 Overscript m Endscripts alpha Subscript j Baseline y Subscript
    t r u e Superscript i Baseline ModifyingAbove x With right-arrow Superscript j"><mrow><mover
    accent="true"><mi>ω</mi> <mo>→</mo></mover> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <msub><mi>α</mi> <mi>j</mi></msub> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mi>j</mi></msup></mrow></math> . Once we have our <math alttext="ModifyingAbove
    omega With right-arrow"><mover accent="true"><mi>ω</mi> <mo>→</mo></mover></math>
    , we can classify new data points using our now trained function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column f left-parenthesis
    ModifyingAbove x With right-arrow Subscript n e w Baseline right-parenthesis 2nd
    Column equals s i g n left-parenthesis ModifyingAbove omega With right-arrow Superscript
    t Baseline ModifyingAbove x With right-arrow Subscript n e w Baseline plus omega
    0 right-parenthesis 2nd Row 1st Column Blank 2nd Column equals s i g n left-parenthesis
    sigma-summation Underscript j Endscripts alpha Subscript j Baseline y Superscript
    i Baseline left-parenthesis ModifyingAbove x With right-arrow Superscript j Baseline
    right-parenthesis Superscript t Baseline ModifyingAbove x With right-arrow Subscript
    n e w Baseline plus omega 0 right-parenthesis period EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>f</mi> <mo>(</mo>
    <msub><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>s</mi> <mi>i</mi>
    <mi>g</mi> <mi>n</mi> <mo>(</mo> <msup><mover accent="true"><mi>ω</mi> <mo>→</mo></mover>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mi>s</mi> <mi>i</mi> <mi>g</mi> <mi>n</mi>
    <mo>(</mo> <munder><mo>∑</mo> <mi>j</mi></munder> <msub><mi>α</mi> <mi>j</mi></msub>
    <msup><mi>y</mi> <mi>i</mi></msup> <msup><mrow><mo>(</mo><msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mi>t</mi></msup> <msub><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>)</mo> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: If you want to avoid quadratic programming, there is another very fast method
    called *coordinate descent* that solves the dual problem and works very well with
    large data sets with high number of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kernel Trick: We Can Transition The Same Ideas To Nonlinear Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One very important note about the dual problem: The data points appear only
    in pairs, more specifically, only in a scalar product, namely, <math alttext="left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis Superscript
    t Baseline ModifyingAbove x With right-arrow Superscript k"><mrow><msup><mrow><mo>(</mo><msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>k</mi></msup></mrow></math>
    . Similarly they only appear as a scalar product in the trained function. This
    simple observation allows for magic:'
  prefs: []
  type: TYPE_NORMAL
- en: If we find a function <math alttext="upper K left-parenthesis ModifyingAbove
    x With right-arrow Superscript j Baseline comma ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis"><mrow><mi>K</mi> <mo>(</mo> <msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo> <msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow></math>
    that can be applied to pairs of data points, and it happens to give us the scalar
    product of pairs of transformed data points into some higher domensional space
    (without knowing what the actual transformation is), then we can solve the same
    exact dual problem in the higher dimensional space, by replacing the scalar product
    in the formula of the dual problem by <math alttext="upper K left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline comma ModifyingAbove
    x With right-arrow Superscript j Baseline right-parenthesis"><mrow><mi>K</mi>
    <mo>(</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup>
    <mo>,</mo> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup>
    <mo>)</mo></mrow></math> .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The intuition here is that data that is nonlinearly separable in lower dimensions
    is almost always linearly separable in higher dimensions. So transform all the
    data points to higher dimensions then separate. The kernel trick solves the linear
    classification problem in higher dimensions *without* transforming each point.
    The kernel itself evaluates the dot product of transformed data without transforming
    the data. Pretty cool stuff.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of kernel functions include:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper K left-parenthesis ModifyingAbove x With right-arrow Superscript
    j Baseline comma ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis
    equals left-parenthesis left-parenthesis ModifyingAbove x With right-arrow Superscript
    j Baseline right-parenthesis Superscript t Baseline ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis squared"><mrow><mi>K</mi> <mrow><mo>(</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo>
    <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mrow><mo>(</mo><msup><mrow><mo>(</mo><msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mi>t</mi></msup> <msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></math> .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The polynomial kernel: <math alttext="upper K left-parenthesis ModifyingAbove
    x With right-arrow Superscript j Baseline comma ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis equals left-parenthesis 1 plus left-parenthesis
    ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis Superscript
    t Baseline ModifyingAbove x With right-arrow Superscript j Baseline right-parenthesis
    Superscript d"><mrow><mi>K</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>+</mo><msup><mrow><mo>(</mo><msup><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow>
    <mi>t</mi></msup> <msup><mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mi>j</mi></msup>
    <mo>)</mo></mrow> <mi>d</mi></msup></mrow></math> .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Gaussian kernel: <math alttext="upper K left-parenthesis ModifyingAbove
    x With right-arrow Superscript j Baseline comma ModifyingAbove x With right-arrow
    Superscript j Baseline right-parenthesis equals e Superscript minus gamma StartAbsoluteValue
    x Super Subscript j Superscript minus x Super Subscript k Superscript EndAbsoluteValue
    squared"><mrow><mi>K</mi> <mrow><mo>(</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>,</mo> <msup><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mi>j</mi></msup> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>e</mi>
    <mrow><mrow><mo>-</mo><mi>γ</mi><mo>|</mo></mrow><msub><mi>x</mi> <mi>j</mi></msub>
    <mo>-</mo><msub><mi>x</mi> <mi>k</mi></msub> <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup></mrow></msup></mrow></math>
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Staying with our driving theme for this chapter that everything is a function,
    a decision tree, in essence, is a function that takes boolean variables as an
    input (these are variables that can only assume *true* (or 1) or *false* (or 0)
    values) such as: Is the feature>5, is the feature=sunny, is the feature=man, *etc.*,
    and outputs a *decision* such as: approve the loan, classify as covid19, return
    25, *etc*. Instead of adding or multiplying boolean variables we use the logical
    *or*, *and* and *not* operators.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if our features are not given in the original data set as boolean
    variables? Then we must transform them to boolean variables before feeding them
    into the model to make predictions. For example, the decision tree in [Figure 3-17](#Fig_regression_tree)
    was trained on the Fish Market data set. It is a regression tree. The tree takes
    raw data, but the function representing the tree actually operates on new variables,
    which are the original data features transformed into boolean variables:'
  prefs: []
  type: TYPE_NORMAL
- en: a1=(Width <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 5.117)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a2=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 59.55)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a3=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 41.1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a4=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 34.9)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a5=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 27.95)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a6=(Length3 <math alttext="less-than-or-equal-to"><mo>≤</mo></math> 21.25)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![400](assets/emai_0317.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-17\. A regression decision tree constructed on the Fish Market data
    set. See the accompanying Jupyter notebook for details.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now the function representing the decision tree in [Figure 3-17](#Fig_regression_tree)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign f left-parenthesis a Baseline 1 comma a Baseline
    2 comma a Baseline 3 comma a Baseline 4 comma a Baseline 5 comma a Baseline 6
    right-parenthesis equals left-parenthesis a Baseline 1 and a Baseline 5 and a
    Baseline 6 right-parenthesis times 39.584 plus left-parenthesis a Baseline 1 and
    a Baseline 5 and not a Baseline 6 right-parenthesis times 139.968 plus left-parenthesis
    a Baseline 1 and not a Baseline 5 and a Baseline 4 right-parenthesis times 287.278
    plus left-parenthesis a Baseline 1 and not a Baseline 5 and not a Baseline 4 right-parenthesis
    times 422.769 plus left-parenthesis not a Baseline 1 and a Baseline 2 and a Baseline
    3 right-parenthesis times 639.737 plus left-parenthesis not a Baseline 1 and a
    Baseline 2 and not a Baseline 3 right-parenthesis times 824.211 plus left-parenthesis
    not a Baseline 1 and not a Baseline 2 right-parenthesis times 1600 dollar-sign"><mrow><mi>f</mi>
    <mo>(</mo> <mi>a</mi> <mn>1</mn> <mo>,</mo> <mi>a</mi> <mn>2</mn> <mo>,</mo> <mi>a</mi>
    <mn>3</mn> <mo>,</mo> <mi>a</mi> <mn>4</mn> <mo>,</mo> <mi>a</mi> <mn>5</mn> <mo>,</mo>
    <mi>a</mi> <mn>6</mn> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn> <mtext>and</mtext>
    <mi>a</mi> <mn>5</mn> <mtext>and</mtext> <mi>a</mi> <mn>6</mn> <mo>)</mo> <mo>×</mo>
    <mn>39</mn> <mo>.</mo> <mn>584</mn> <mo>+</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn>
    <mtext>and</mtext> <mi>a</mi> <mn>5</mn> <mtext>and</mtext> <mtext>not</mtext>
    <mi>a</mi> <mn>6</mn> <mo>)</mo> <mo>×</mo> <mn>139</mn> <mo>.</mo> <mn>968</mn>
    <mo>+</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn> <mtext>and</mtext> <mtext>not</mtext>
    <mi>a</mi> <mn>5</mn> <mtext>and</mtext> <mi>a</mi> <mn>4</mn> <mo>)</mo> <mo>×</mo>
    <mn>287</mn> <mo>.</mo> <mn>278</mn> <mo>+</mo> <mo>(</mo> <mi>a</mi> <mn>1</mn>
    <mtext>and</mtext> <mtext>not</mtext> <mi>a</mi> <mn>5</mn> <mtext>and</mtext>
    <mtext>not</mtext> <mi>a</mi> <mn>4</mn> <mo>)</mo> <mo>×</mo> <mn>422</mn> <mo>.</mo>
    <mn>769</mn> <mo>+</mo> <mo>(</mo> <mtext>not</mtext> <mi>a</mi> <mn>1</mn> <mtext>and</mtext>
    <mi>a</mi> <mn>2</mn> <mtext>and</mtext> <mi>a</mi> <mn>3</mn> <mo>)</mo> <mo>×</mo>
    <mn>639</mn> <mo>.</mo> <mn>737</mn> <mo>+</mo> <mo>(</mo> <mtext>not</mtext>
    <mi>a</mi> <mn>1</mn> <mtext>and</mtext> <mi>a</mi> <mn>2</mn> <mtext>and</mtext>
    <mtext>not</mtext> <mi>a</mi> <mn>3</mn> <mo>)</mo> <mo>×</mo> <mn>824</mn> <mo>.</mo>
    <mn>211</mn> <mo>+</mo> <mo>(</mo> <mtext>not</mtext> <mi>a</mi> <mn>1</mn> <mtext>and</mtext>
    <mtext>not</mtext> <mi>a</mi> <mn>2</mn> <mo>)</mo> <mo>×</mo> <mn>1600</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that unlike the training functions that we encountered in this chapter
    so far, the above function has no parameters <math alttext="omega"><mi>ω</mi></math>
    ’s that we need to solve for. This is called a *nonparametric model*, and it doesn’t
    fix the *shape* of the function ahead of time. This gives it the flexibility to
    *grow* with the data, or in other words, adapt to the data. Of course, with this
    high adaptability to the data comes the high risk of overfitting the data. Thankfully
    there are ways around this, which we list some here without any elaboration: pruning
    the tree after growing it, restricting the number of layers, setting a minimum
    number of data instances per node, or using an ensemble of trees instead of one
    tree, called a *random forest*, discussed below.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One very important observation: The decision tree *decided* to split over only
    two features of the original data set, namely the Width and Length3 features.
    Decision trees are designed in a way that keeps the more imporatnt features (those
    providing the most information that contribute to our prediction) closer to the
    root. Therefore, decision trees can help in feature selection, where we select
    the most important features to contribute to our final model’s predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: It is no wonder the Width and Length3 features ended up being the most important
    for predicting the weight of the fish. The correlation matrix in [Figure 3-18](#Fig_fish_corr_matrix)
    and the scatter plots in [Figure 3-3](#Fig_weight_lengths_scatterplots) show extremely
    strong correlation between all the length features. This means that the information
    they provide is redundant, and including all of them in our prediction models
    will increase computation costs and lower performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![280](assets/emai_0318.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-18\. Correlation matrix for the Fish Market data set. There is an extremely
    strong correlation between all the length features.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note: Feature Selection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just introduced the very important topic of feature selection. Real world
    data sets come with many features and some of them may provide redundant information,
    others are not important at all for predicting our target label. Including irrelevant
    and/redundant features in a machine learning model increases computational cost
    and lowers its performance. We just saw that decision trees are one way that helps
    select the important features. Another way is a regularization technique called
    Lasso regression, which we will introduce in [Chapter 4](ch04.xhtml#ch04). There
    are statistical tests that test for feature dependencies on each other. The *F-test*
    tests for linear dependencies (this gives higher scores for correlated features,
    but correlations alone are deceptive), and *Mutual Information* tests for nonlinear
    dependencies. These provide a *measure* of how much a feature contributes to determining
    the target label, and hence aids in feature selection, by keeping the most promising
    features. We can also test for features’ dependencies on each other, as well as
    their correlations and scatterplots. *Variance* thresholding removes features
    with little to no variance, on the premise that if a feature does not vary much
    within itself, it has little predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we train a decision tree on a data set? What function do we optimize?
    There are two functions that are usually optimized when *growing* decision trees:
    The *entropy* and the *gini impurity*. Using one or the other doesn’t make much
    difference in the resultant trees. We develop these next.'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy and Gini Impurity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we decide to split a node of the tree on the feature that is evaluated
    as the *most important*. *Entropy* and *Gini Impurity* are the two popular ways
    to measure importance of a feature. They are not mathematically equivalent but
    they both work and provide reasonable decision trees. Gini impurity is usually
    less expensive to compute, so it is the default in software packages, but you
    have the option to change from the default setting to entropy. Using Gini impurity
    tends to produce less balanced trees when there are classes with much higher frequency
    than others. These classes end up isolated in their own branches. However, in
    many cases, using either entropy or Gini impurity provides not much difference
    in the resulting decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: With the entropy approach, we look for the feature split that provides the *maximal
    information gain* (we’ll give its formula shortly). Information gain is borrowed
    from information theory, and it has to do with the concept of *entropy*. Entropy,
    in turn, is borrowed from Thermodynamics and Statistical Physics, and it quantifies
    the amount of disorder in a certain system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the gini impurity approach, we look for the feature split that provides
    children nodes with lowest average gini impurity (we’ll also give its formula
    shortly).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To maximize information gain (or minimize gini impurity), the algorithm that
    grows a decision tree has to go over each feature of the training data subset
    and calculate the information gain (or gini impurity) accomplished whether the
    tree uses that particular feature as a node to split on, then choose the feature
    that provides the highest information gain (or children nodes with lowest average
    gini impurity). Moreover, if the feature has real numerical values, the algorithm
    has to decide on *what question to ask at the node*, meaning what feature value
    to split on, for example, is <math alttext="x 5 less-than 0.1"><mrow><msub><mi>x</mi>
    <mn>5</mn></msub> <mo><</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn></mrow></math> ?
    The algorithm has to do this sequentially at each layer of the tree, calculating
    information gain (or gini impurities) over the features of the data instances
    in each node and sometimes on each split value possibility. This is easier to
    understand with examples. But first, we write the formulas for the entropy, information
    gain, and gini impurity.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy and Information Gain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The easiest way to understand the entropy formula is to rely on the intuition
    that if an event is highly probable, then there is little surprise associated
    with it happening. So when <math alttext="p left-parenthesis e v e n t right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>e</mi> <mi>v</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mo>)</mo></mrow></math>
    is large, its surprise is low. We can mathematically encode this with a function
    that decreases when its probability increases. The calculus function <math alttext="log
    StartFraction 1 Over x EndFraction"><mrow><mo form="prefix">log</mo> <mfrac><mn>1</mn>
    <mi>x</mi></mfrac></mrow></math> works and has the additional property that the
    surprises of independent events add up. Therefore, we can define:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper S u r p r i s e left-parenthesis e v e n t
    right-parenthesis equals log StartFraction 1 Over p left-parenthesis e v e n t
    right-parenthesis EndFraction equals minus log left-parenthesis p left-parenthesis
    e v e n t right-parenthesis right-parenthesis dollar-sign"><mrow><mi>S</mi> <mi>u</mi>
    <mi>r</mi> <mi>p</mi> <mi>r</mi> <mi>i</mi> <mi>s</mi> <mi>e</mi> <mrow><mo>(</mo>
    <mi>e</mi> <mi>v</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mo form="prefix">log</mo> <mfrac><mn>1</mn> <mrow><mi>p</mi><mo>(</mo><mi>e</mi><mi>v</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mo>-</mo> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>e</mi> <mi>v</mi> <mi>e</mi> <mi>n</mi> <mi>t</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the entropy of a random variable (which in our case is a particular feature
    in our training data set) is defined as the *expected surprise* associated with
    the random variable, so we must add up the surprises of each possible outcome
    of the random variable (surprise of each value of the feature in question) multiplied
    with their respective probabilities, obtaining:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper E n t r o p y left-parenthesis upper X right-parenthesis
    equals minus p left-parenthesis o u t c o m e 1 right-parenthesis log left-parenthesis
    p left-parenthesis o u t c o m e 1 right-parenthesis right-parenthesis minus p
    left-parenthesis o u t c o m e 2 right-parenthesis log left-parenthesis p left-parenthesis
    o u t c o m e 2 right-parenthesis right-parenthesis minus ellipsis minus p left-parenthesis
    o u t c o m e Subscript n Baseline right-parenthesis log left-parenthesis p left-parenthesis
    o u t c o m e Subscript n Baseline right-parenthesis right-parenthesis period
    dollar-sign"><mrow><mi>E</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>o</mi> <mi>p</mi>
    <mi>y</mi> <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi> <mi>o</mi>
    <mi>m</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi>
    <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>-</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi>
    <mi>t</mi> <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mo>⋯</mo> <mo>-</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi> <mi>o</mi>
    <mi>m</mi> <msub><mi>e</mi> <mi>n</mi></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>o</mi> <mi>u</mi> <mi>t</mi>
    <mi>c</mi> <mi>o</mi> <mi>m</mi> <msub><mi>e</mi> <mi>n</mi></msub> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The the entropy for one feature of our training data that assumes a bunch of
    values is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper E n t r o p y left-parenthesis upper F e a
    t u r e right-parenthesis equals minus p left-parenthesis v a l u e 1 right-parenthesis
    log left-parenthesis p left-parenthesis v a l u e 1 right-parenthesis right-parenthesis
    minus p left-parenthesis v a l u e 2 right-parenthesis log left-parenthesis p
    left-parenthesis v a l u e 2 right-parenthesis right-parenthesis minus ellipsis
    minus p left-parenthesis v a l u e Subscript n Baseline right-parenthesis log
    left-parenthesis p left-parenthesis v a l u e Subscript n Baseline right-parenthesis
    right-parenthesis dollar-sign"><mrow><mi>E</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi>
    <mi>o</mi> <mi>p</mi> <mi>y</mi> <mrow><mo>(</mo> <mi>F</mi> <mi>e</mi> <mi>a</mi>
    <mi>t</mi> <mi>u</mi> <mi>r</mi> <mi>e</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <mo>⋯</mo> <mo>-</mo> <mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mi>n</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our goal is to select to split on a feature that provides large information
    gain about the outcome (the label or the target feature), let’s first calculate
    the entropy of the outcome feature. Assume for simplicity that this is a binary
    classification problem, so the outcome feature only has two values: positive (in
    the class) and negative (not in the class). If we let *p* be the number of positive
    instances in the target feature and *n* be the number of negative ones, then *p+n=m*
    will be the number of instances in the training data subset. Now the probability
    to select a positive instance from that target column will be <math alttext="StartFraction
    p Over m EndFraction equals StartFraction p Over p plus n EndFraction"><mrow><mfrac><mi>p</mi>
    <mi>m</mi></mfrac> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac></mrow></math>
    , and the probability to select a negative instance is similarly <math alttext="StartFraction
    n Over m EndFraction equals StartFraction n Over p plus n EndFraction"><mrow><mfrac><mi>n</mi>
    <mi>m</mi></mfrac> <mo>=</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac></mrow></math>
    . Thus, the entropy of the outcome feature (without leveraging any information
    from the other features) is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column upper
    E n t r o p y left-parenthesis Outcome Feature right-parenthesis 2nd Row 1st Column
    Blank 2nd Column equals minus p left-parenthesis p o s i t i v e right-parenthesis
    log left-parenthesis p left-parenthesis p o s i t i v e right-parenthesis right-parenthesis
    minus p left-parenthesis n e g a t i v e right-parenthesis log left-parenthesis
    p left-parenthesis n e g a t i v e right-parenthesis right-parenthesis 3rd Row
    1st Column Blank 2nd Column equals minus StartFraction p Over p plus n EndFraction
    log left-parenthesis StartFraction p Over p plus n EndFraction right-parenthesis
    minus StartFraction n Over p plus n EndFraction log left-parenthesis StartFraction
    n Over p plus n EndFraction right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mi>E</mi> <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>o</mi>
    <mi>p</mi> <mi>y</mi> <mo>(</mo> <mtext>Outcome</mtext> <mtext>Feature</mtext>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo>
    <mi>p</mi> <mo>(</mo> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mo>)</mo> <mo form="prefix">log</mo> <mo>(</mo> <mi>p</mi>
    <mo>(</mo> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi>
    <mi>e</mi> <mo>)</mo> <mo>)</mo> <mo>-</mo> <mi>p</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi>
    <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>)</mo> <mo
    form="prefix">log</mo> <mo>(</mo> <mi>p</mi> <mo>(</mo> <mi>n</mi> <mi>e</mi>
    <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi> <mi>e</mi> <mo>)</mo> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Now we leverage information from one other feature and calculate the difference
    in entropy of the outcome feature, which we expect to decrease as we gain more
    information (more information generally results in less surprise).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we choose feature *A* to split a node of our decision tree on. Suppose
    Feature *A* assumes four values, and has <math alttext="k 1"><msub><mi>k</mi>
    <mn>1</mn></msub></math> instances with <math alttext="v a l u e 1"><mrow><mi>v</mi>
    <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub></mrow></math>
    , of these <math alttext="p 1"><msub><mi>p</mi> <mn>1</mn></msub></math> are labeled
    positive as their target outcome, and <math alttext="n 1"><msub><mi>n</mi> <mn>1</mn></msub></math>
    are labeled negative as their target outcome, so <math alttext="p 1 plus n 1 equals
    k 1"><mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>n</mi> <mn>1</mn></msub>
    <mo>=</mo> <msub><mi>k</mi> <mn>1</mn></msub></mrow></math> . Similarly, Feature
    *A* has <math alttext="k 2"><msub><mi>k</mi> <mn>2</mn></msub></math> instances
    with <math alttext="v a l u e 2"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>2</mn></msub></mrow></math> , of these <math alttext="p 2"><msub><mi>p</mi>
    <mn>2</mn></msub></math> are labeled positive as their target outcome, and <math
    alttext="n 2"><msub><mi>n</mi> <mn>2</mn></msub></math> are labeled negative as
    their target outcome, so <math alttext="p 2 plus n 2 equals k 2"><mrow><msub><mi>p</mi>
    <mn>2</mn></msub> <mo>+</mo> <msub><mi>n</mi> <mn>2</mn></msub> <mo>=</mo> <msub><mi>k</mi>
    <mn>2</mn></msub></mrow></math> . The same applies for <math alttext="v a l u
    e 3"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>
    and <math alttext="v a l u e 4"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>4</mn></msub></mrow></math> of Feature *A*. Note that <math
    alttext="k 1 plus k 2 plus k 3 plus k 4 equals m"><mrow><msub><mi>k</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>k</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>k</mi> <mn>3</mn></msub>
    <mo>+</mo> <msub><mi>k</mi> <mn>4</mn></msub> <mo>=</mo> <mi>m</mi></mrow></math>
    , the total number of instances in the training subset of the data set. Now the
    each value <math alttext="v a l u e Subscript k"><mrow><mi>v</mi> <mi>a</mi> <mi>l</mi>
    <mi>u</mi> <msub><mi>e</mi> <mi>k</mi></msub></mrow></math> of Feature *A* can
    be thought of as a random variable in its own respect, with <math alttext="p Subscript
    k"><msub><mi>p</mi> <mi>k</mi></msub></math> positive outcomes and <math alttext="n
    Subscript k"><msub><mi>n</mi> <mi>k</mi></msub></math> negative outcomes, so we
    can calculate its entropy (expected surprise):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Entropy left-parenthesis
    v a l u e 1 right-parenthesis 2nd Column equals minus StartFraction p 1 Over p
    1 plus n 1 EndFraction log left-parenthesis StartFraction p 1 Over p 1 plus n
    1 EndFraction right-parenthesis minus StartFraction n 1 Over p 1 plus n 1 EndFraction
    log left-parenthesis StartFraction n 1 Over p 1 plus n 1 EndFraction right-parenthesis
    2nd Row 1st Column Entropy left-parenthesis v a l u e 2 right-parenthesis 2nd
    Column equals minus StartFraction p 2 Over p 2 plus n 2 EndFraction log left-parenthesis
    StartFraction p 2 Over p 2 plus n 2 EndFraction right-parenthesis minus StartFraction
    n 2 Over p 2 plus n 2 EndFraction log left-parenthesis StartFraction n 2 Over
    p 2 plus n 2 EndFraction right-parenthesis 3rd Row 1st Column Entropy left-parenthesis
    v a l u e 3 right-parenthesis 2nd Column equals minus StartFraction p 3 Over p
    3 plus n 3 EndFraction log left-parenthesis StartFraction p 3 Over p 3 plus n
    3 EndFraction right-parenthesis minus StartFraction n 3 Over p 3 plus n 3 EndFraction
    log left-parenthesis StartFraction n 3 Over p 3 plus n 3 EndFraction right-parenthesis
    4th Row 1st Column Entropy left-parenthesis v a l u e 4 right-parenthesis 2nd
    Column equals minus StartFraction p 4 Over p 4 plus n 4 EndFraction log left-parenthesis
    StartFraction p 4 Over p 4 plus n 4 EndFraction right-parenthesis minus StartFraction
    n 4 Over p 4 plus n 4 EndFraction log left-parenthesis StartFraction n 4 Over
    p 4 plus n 4 EndFraction right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mtext>Entropy</mtext> <mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi> <mn>1</mn></msub>
    <mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi> <mn>1</mn></msub>
    <mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi> <mn>1</mn></msub> <mrow><msub><mi>p</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi> <mn>1</mn></msub>
    <mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Entropy</mtext>
    <mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi>
    <mn>2</mn></msub> <mrow><msub><mi>p</mi> <mn>2</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><mtext>Entropy</mtext> <mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi> <mn>3</mn></msub>
    <mrow><msub><mi>p</mi> <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi> <mn>3</mn></msub>
    <mrow><msub><mi>p</mi> <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi> <mn>3</mn></msub> <mrow><msub><mi>p</mi>
    <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi> <mn>3</mn></msub>
    <mrow><msub><mi>p</mi> <mn>3</mn></msub> <mo>+</mo><msub><mi>n</mi> <mn>3</mn></msub></mrow></mfrac>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mtext>Entropy</mtext>
    <mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>4</mn></msub>
    <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mo>-</mo> <mfrac><msub><mi>p</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>p</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo>)</mo></mrow> <mo>-</mo> <mfrac><msub><mi>n</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><msub><mi>n</mi>
    <mn>4</mn></msub> <mrow><msub><mi>p</mi> <mn>4</mn></msub> <mo>+</mo><msub><mi>n</mi>
    <mn>4</mn></msub></mrow></mfrac> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have this information, we can calculate the *expected entropy*
    after splitting on Feature *A*, so we add up the above four entropies each multiplied
    with its respective probability. Note that <math alttext="p left-parenthesis v
    a l u e 1 right-parenthesis equals StartFraction k 1 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>1</mn></msub>
    <mi>m</mi></mfrac></mrow></math> , <math alttext="p left-parenthesis v a l u e
    2 right-parenthesis equals StartFraction k 2 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>2</mn></msub>
    <mi>m</mi></mfrac></mrow></math> , <math alttext="p left-parenthesis v a l u e
    3 right-parenthesis equals StartFraction k 3 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>3</mn></msub>
    <mi>m</mi></mfrac></mrow></math> , and <math alttext="p left-parenthesis v a l
    u e 4 right-parenthesis equals StartFraction k 4 Over m EndFraction"><mrow><mi>p</mi>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msub><mi>k</mi> <mn>4</mn></msub>
    <mi>m</mi></mfrac></mrow></math> . Therefore, the expected entropy after splitting
    on Feature *A* would be:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column Expected
    Entropy left-parenthesis Feature upper A right-parenthesis 2nd Row 1st Column
    Blank 2nd Column equals p left-parenthesis v a l u e 1 right-parenthesis Entropy
    left-parenthesis v a l u e 1 right-parenthesis plus p left-parenthesis v a l u
    e 2 right-parenthesis Entropy left-parenthesis v a l u e 2 right-parenthesis 3rd
    Row 1st Column Blank 2nd Column plus p left-parenthesis v a l u e 3 right-parenthesis
    Entropy left-parenthesis v a l u e 3 right-parenthesis plus p left-parenthesis
    v a l u e 4 right-parenthesis Entropy left-parenthesis v a l u e 4 right-parenthesis
    4th Row 1st Column Blank 2nd Column equals StartFraction k 1 Over m EndFraction
    Entropy left-parenthesis v a l u e 1 right-parenthesis plus StartFraction k 2
    Over m EndFraction Entropy left-parenthesis v a l u e 2 right-parenthesis plus
    StartFraction k 3 Over m EndFraction Entropy left-parenthesis v a l u e 3 right-parenthesis
    plus StartFraction k 4 Over m EndFraction Entropy left-parenthesis v a l u e 4
    right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="left"><mrow><mtext>Expected</mtext> <mtext>Entropy</mtext> <mo>(</mo>
    <mtext>Feature</mtext> <mtext>A</mtext> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi>
    <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>+</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>v</mi>
    <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>4</mn></msub> <mo>)</mo></mrow>
    <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi>
    <msub><mi>e</mi> <mn>4</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mfrac><msub><mi>k</mi> <mn>1</mn></msub>
    <mi>m</mi></mfrac> <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <mfrac><msub><mi>k</mi> <mn>2</mn></msub> <mi>m</mi></mfrac> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mfrac><msub><mi>k</mi> <mn>3</mn></msub>
    <mi>m</mi></mfrac> <mtext>Entropy</mtext> <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi>
    <mi>l</mi> <mi>u</mi> <msub><mi>e</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mo>+</mo>
    <mfrac><msub><mi>k</mi> <mn>4</mn></msub> <mi>m</mi></mfrac> <mtext>Entropy</mtext>
    <mrow><mo>(</mo> <mi>v</mi> <mi>a</mi> <mi>l</mi> <mi>u</mi> <msub><mi>e</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: where the entropy of each value of Feature *A* is given in the previous paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what would the information gained from using feature *A* to split on be?
    It would the difference between the entropy of the outcome feature without any
    information from Feature *A* and the expected entropy of Feature *A*. Therefore,
    we have a formula for *information gain* given that we decide to split on Feature
    *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column Information
    Gain equals 2nd Row 1st Column Blank 2nd Column upper E n t r o p y left-parenthesis
    Outcome Feature right-parenthesis minus Expected Entropy left-parenthesis Feature
    upper A right-parenthesis equals 3rd Row 1st Column Blank 2nd Column minus StartFraction
    p Over p plus n EndFraction log left-parenthesis StartFraction p Over p plus n
    EndFraction right-parenthesis minus StartFraction n Over p plus n EndFraction
    log left-parenthesis StartFraction n Over p plus n EndFraction right-parenthesis
    minus Expected Entropy left-parenthesis Feature upper A right-parenthesis period
    EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mtext>Information</mtext>
    <mtext>Gain</mtext> <mo>=</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>E</mi>
    <mi>n</mi> <mi>t</mi> <mi>r</mi> <mi>o</mi> <mi>p</mi> <mi>y</mi> <mo>(</mo> <mtext>Outcome</mtext>
    <mtext>Feature</mtext> <mo>)</mo> <mo>-</mo> <mtext>Expected</mtext> <mtext>Entropy</mtext>
    <mo>(</mo> <mtext>Feature</mtext> <mtext>A</mtext> <mo>)</mo> <mo>=</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>-</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mi>n</mi> <mrow><mi>p</mi><mo>+</mo><mi>n</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>-</mo> <mtext>Expected</mtext> <mtext>Entropy</mtext> <mrow><mo>(</mo>
    <mtext>Feature</mtext> <mtext>A</mtext> <mo>)</mo></mrow> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Now it is easy to go through each feature of the training data subset and calculate
    the information gain resulting from using that feature to split on. Ultimately,
    the decision tree algorithm decides to split on the feature with highest information
    gain. The algorithm does this recursively for each node and at each layer of the
    tree, until it runs out of features to split on or of data instances. Therefore,
    we obtain our entropy based decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not too difficult to generalize the above logic to the case where we
    have a multi-class output, for example, a classification problem with three or
    more target labels. The classical [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris)
    from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)
    is a great example with three target labels. This data set has four features for
    a given Iris flower: Its sepal length and width, and its petal length and width.
    Note that each of these features is a continuous random variable, not discrete.
    So we have to devise a test to split on the values of each feature, *before* applying
    the above logic. This is part of the feature engineering stage of a data science
    project. The engineering step here is: Transform a continuous valued feature into
    a boolean feature, for example, *is the petal length>2.45*? We will not go over
    how to choose the number 2.45, but by now you probably can guess that there is
    an optimization process that should go on here as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each decision tree is characterized by its nodes, branches, and leaves. A node
    is considered *pure* if it only contains data instances from the training data
    subset that have the same target label (this means they belong to the same class).
    Note that a pure node is a desired node, since we know its class. Therefore, an
    algorithm would want to grow a tree in a way that minimizes the *impurity* of
    the nodes: If the data instances in a node do not all belong in the same class,
    then the node is *impure*. *Gini impurity* quantifies this impurity the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that our classification problem has three classes, like the [Iris Data
    Set](https://archive.ics.uci.edu/ml/datasets/iris). Suppose also that a certain
    node in a decision tree grown to fit this data set has *n* training instances,
    with <math alttext="n 1"><msub><mi>n</mi> <mn>1</mn></msub></math> of these belonging
    in the first class, <math alttext="n 2"><msub><mi>n</mi> <mn>2</mn></msub></math>
    in the second class, and <math alttext="n 3"><msub><mi>n</mi> <mn>3</mn></msub></math>
    in the third class (so <math alttext="n 1 plus n 2 plus n 3 equals n"><mrow><msub><mi>n</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>n</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>n</mi>
    <mn>3</mn></msub> <mo>=</mo> <mi>n</mi></mrow></math> ). Then the Gini impurity
    of this node is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign Gini impurity equals 1 minus left-parenthesis StartFraction
    n 1 Over n EndFraction right-parenthesis squared minus left-parenthesis StartFraction
    n 2 Over n EndFraction right-parenthesis squared minus left-parenthesis StartFraction
    n 3 Over n EndFraction right-parenthesis squared dollar-sign"><mrow><mtext>Gini</mtext>
    <mtext>impurity</mtext> <mo>=</mo> <mn>1</mn> <mo>-</mo> <msup><mrow><mo>(</mo><mfrac><msub><mi>n</mi>
    <mn>1</mn></msub> <mi>n</mi></mfrac><mo>)</mo></mrow> <mn>2</mn></msup> <mo>-</mo>
    <msup><mrow><mo>(</mo><mfrac><msub><mi>n</mi> <mn>2</mn></msub> <mi>n</mi></mfrac><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>-</mo> <msup><mrow><mo>(</mo><mfrac><msub><mi>n</mi> <mn>3</mn></msub>
    <mi>n</mi></mfrac><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: So for each node the fraction of the data instances belonging to each class
    is calculated, squared, then the sum of those is subtracted from 1\. Note that
    if all the data instances of a node belong in the same class, then the above formula
    gives a Gini impurity equal to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision tree growing algorithm now looks for the feature and split point
    in each feature that produce children nodes with the lowest Gini impurity, on
    average. This means the children of a node must on average be purer than the parent
    node. Thus, the algorithm tries to minimize a weighted average of the Gini impurities
    of two the children nodes (of a binary tree). Each child’s Gini impurity is weighted
    by its relative size, which is the ratio between its number of instances relative
    to total number of instances in that tree layer (which is the same as the number
    of instances as its parent’s). Thus, we end up having to search for the feature
    and the split point (for each feature) combination that solve the following minimization
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign min Underscript upper F e a t u r e comma upper F
    e a t u r e upper S p l i t upper V a l u e Endscripts StartFraction n Subscript
    l e f t Baseline Over n EndFraction upper G i n i left-parenthesis Left Node right-parenthesis
    plus StartFraction n Subscript r i g h t Baseline Over n EndFraction upper G i
    n i left-parenthesis Right Node right-parenthesis dollar-sign"><mrow><msub><mo
    form="prefix" movablelimits="true">min</mo> <mrow><mi>F</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>,</mo><mi>F</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>S</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mfrac><msub><mi>n</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub>
    <mi>n</mi></mfrac> <mi>G</mi> <mi>i</mi> <mi>n</mi> <mi>i</mi> <mrow><mo>(</mo>
    <mtext>Left</mtext> <mtext>Node</mtext> <mo>)</mo></mrow> <mo>+</mo> <mfrac><msub><mi>n</mi>
    <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub> <mi>n</mi></mfrac>
    <mi>G</mi> <mi>i</mi> <mi>n</mi> <mi>i</mi> <mrow><mo>(</mo> <mtext>Right</mtext>
    <mtext>Node</mtext> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="n Subscript l e f t"><msub><mi>n</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub></math>
    and <math alttext="n Subscript r i g h t"><msub><mi>n</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub></math>
    are the number of data instances that end up being in the left and right children
    nodes, and *n* is the number of data instances that are in the parent node (note
    that <math alttext="n Subscript l e f t"><msub><mi>n</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub></math>
    and <math alttext="n Subscript r i g h t"><msub><mi>n</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub></math>
    must add up to *n*).
  prefs: []
  type: TYPE_NORMAL
- en: Regression Decision Trees
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important to point out that decision trees can be used for both regression
    and classification. A regression decision tree returns a predicted value rather
    than a class, but a similar process to a classification tree applies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of splitting a node by selecting a feature and a feature value (for
    example, is height>3 feet?) that maximize information gain or minimize Gini impurity,
    we select a feature and a feature value that minimize a mean squared distance
    between the true labels and the average of the labels of all the instances in
    each of the left and right children nodes. That is, the algorithm chooses a feature
    and feature value to split on, then looks at the left and right children nodes
    resulting from that split, and calculates:'
  prefs: []
  type: TYPE_NORMAL
- en: The average value of all the labels of the training data instances in the left
    node. This average will be the left node value <math alttext="y Subscript l e
    f t"><msub><mi>y</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub></math>
    , and it is the value predicted by the decision tree if this node ends up being
    a leaf node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average value of all the labels of the training data instances in the right
    node. This average will be the right node value <math alttext="y Subscript r i
    g h t"><msub><mi>y</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub></math>
    . Similarly, this is the value predicted by the decision tree if this node ends
    up being a leaf node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the squared distance between the left node value and the true label
    of of each instance in the left node <math alttext="sigma-summation Underscript
    upper L e f t upper N o d e upper I n s t a n c e s Endscripts StartAbsoluteValue
    y Subscript t r u e Superscript i Baseline minus y Subscript l e f t Baseline
    EndAbsoluteValue squared"><mrow><msub><mo>∑</mo> <mrow><mi>L</mi><mi>e</mi><mi>f</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <msup><mrow><mo>|</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo><msub><mi>y</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub>
    <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math> .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of the squared distance between the right node value and the true label
    of of each instance in the right node <math alttext="sigma-summation Underscript
    upper R i g h t upper N o d e upper I n s t a n c e s Endscripts StartAbsoluteValue
    y Subscript t r u e Superscript i Baseline minus y Subscript r i g h t Baseline
    EndAbsoluteValue squared"><mrow><msub><mo>∑</mo> <mrow><mi>R</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <msup><mrow><mo>|</mo><msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo><msub><mi>y</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mo>|</mo></mrow> <mn>2</mn></msup></mrow></math> .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A weighted average of the above two sums, where each node is weighted by its
    size relative to the parent node, just like we did for the Gini impurity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartFraction n Subscript l e f t Baseline Over n
    EndFraction sigma-summation Underscript upper L e f t upper N o d e upper I n
    s t a n c e s Endscripts StartAbsoluteValue y Subscript t r u e Superscript i
    Baseline minus y Subscript l e f t Baseline EndAbsoluteValue squared plus StartFraction
    n Subscript r i g h t Baseline Over n EndFraction sigma-summation Underscript
    upper R i g h t upper N o d e upper I n s t a n c e s Endscripts StartAbsoluteValue
    y Subscript t r u e Superscript i Baseline minus y Subscript r i g h t Baseline
    EndAbsoluteValue squared right-parenthesis period dollar-sign"><mrow><mfrac><msub><mi>n</mi>
    <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub> <mi>n</mi></mfrac>
    <msub><mo>∑</mo> <mrow><mi>L</mi><mi>e</mi><mi>f</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <mrow><mo>|</mo></mrow> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>l</mi><mi>e</mi><mi>f</mi><mi>t</mi></mrow></msub>
    <msup><mrow><mo>|</mo></mrow> <mn>2</mn></msup> <mo>+</mo> <mfrac><msub><mi>n</mi>
    <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub> <mi>n</mi></mfrac>
    <msub><mo>∑</mo> <mrow><mi>R</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>I</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></msub>
    <mrow><mo>|</mo></mrow> <msubsup><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow>
    <mi>i</mi></msubsup> <mo>-</mo> <msub><mi>y</mi> <mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub>
    <mrow><msup><mo>|</mo> <mn>2</mn></msup> <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The above algorithm is *greedy* and computation heavy, in the sense that it
    has to do this for *each feature and each possible feature split value*, then
    choose the feature and feature split that provide the smallest weighted squared
    error average between the left and right children nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The CART (Classification And Regression Tree) algorithm is famous algorithm
    used by software packages, including Python’s Scikit Learn which we use in the
    Jupyter Notebooks supplementing this book. This algorithm produces trees whose
    nodes only have two children (binary trees) where the test at each node only has
    Yes or No answers. Other algorithms such as ID3 can produce trees with nodes that
    have two or more children.
  prefs: []
  type: TYPE_NORMAL
- en: Shortcomings of Decision Trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Decision trees are very easy to interpret and are popular for many good reasons:
    They adapt to large data sets, different data types (discrete and continuous features,
    no scaling of data needed), and can perform both regression and classification
    tasks. However, they can be unstable, in the sense that adding just one instance
    to the data set can change the tree at its root and hence result in a very different
    decision tree. They are also sensitive to rotations in the data, since their decision
    boundaries are usually horizontal and vertical (not slanted like support vector
    machines). This is because splits usually happen at specific feature values, so
    the decision boundaries end up parallel to the feature axes. One fix to that is
    to transform the data set to match its *pricipal axes*, using the *singular value
    decomposition method* presented later in [Chapter 11](ch11.xhtml#ch11). Decision
    trees tend to overfit the data so they need pruning. This is usually done using
    statistical tests. The greedy algorithms involved in constructing the trees, where
    the search happens over all features and their values makes them computationally
    expensive and less accurate. Random forests, discussed next, address some of these
    shortcomings.'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When I first learned about decision trees, the most perplexing aspects for
    me were:'
  prefs: []
  type: TYPE_NORMAL
- en: How do we start the tree, meaning how do we decide which data feature is the
    root feature?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At what particular feature value do we decide to split a node?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When do we stop?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, how do we grow a tree?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Note that we answered some of the above questions in the previous subsection.)
    It didn’t make matters any easier that I would surf the internet looking for answers,
    only to encounter declarations that decision trees are so easy to build and understand,
    so it felt like I was the only one deeply confused with decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'My puzzlement instantly disappeared when I learned about *random forests*.
    The amazing thing about random forests is that we can get incredibly good regression
    or classification results, *without* answering any of my bewildering questions.
    Randomizing the whole process, meaning, building many decision trees while answering
    all my questions with two words: *choose randomly*, then aggregating their predictions
    in an ensemble produces very good results, even better than one carefully crafted
    decision tree. It has been said that *randomization often produces reliability*!'
  prefs: []
  type: TYPE_NORMAL
- en: Another very useful property of random forests is that they give a measure of
    *feature importance*, helping us pinpoint which features significantly affect
    our predictions, and aid in feature selection as well.
  prefs: []
  type: TYPE_NORMAL
- en: k-means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common goal of data analysts is to partiton data into *clusters*, each cluster
    highlighting certain common traits. *k-means clustering* is a common machine learning
    method that partitions *n* data points (vectors) into *k* clusters, where each
    data point gets assigned to belongs to the cluster with the nearest mean. The
    mean of each cluster, or its centroid, serves as the prototype of the cluster.
    Overall, k-means clustering minimizes the variance (the squared Euclidean distances
    to the mean) within each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common algorithm for k-means clustering is iterative:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with an initial set of *k* means. This means that we specify the number
    of clusters ahead of time, which raises the question: How to initialize it? How
    to select the locations of the first *k* centroids? There is literature on that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign each data point to the cluster with the nearest mean in terms of squared
    Euclidean distance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recalculate the means of each cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm converges when the data point assignments to each cluster do not
    change anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Measures For Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is relatively easy to develop mathematical models that compute things and
    produce outputs. It is a completely different story to develop models that perform
    well for our desired tasks. Furthermore, models that perform well according to
    some metrics behave badly according to some other metrics. We need extra care
    developing performance metrics and deciding which ones to rely on, depending on
    our specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the performance of models that predict numerical values, such as regression
    models, is easier than classificaion models, since we have many ways to compute
    distances between numbers (good predictions and bad predictions). On the other
    hand, when our task is classification (we can use models such as logistic regression,
    softmax regression, support vector machines, decision trees, random forests, or
    neural networks), we have to put some extra thought into evaluating performance.
    Moreover, there are usually tradeoffs. For example, if our task is to classify
    YouTube videos as being safe for kids (positive) or not safe for kids (negative),
    do we tweak our model so as to reduce the number of false positives or false negatives?
    It is obviously more problematic if a video is classified as safe while in reality
    it is unsafe (false positive) than the other way around, so our performance metric
    needs to reflect that.
  prefs: []
  type: TYPE_NORMAL
- en: The following are the performance meausures commonly used for classification
    models. Do not worry about memorizing their names, as the way they are named do
    not make logical sense. Spend your time instead understanding their meanings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Percentage of times prediction model got the classification right:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A c c u r a c y equals StartFraction true positives
    plus true negatives Over all predicted positives plus all predicted negatives
    EndFraction dollar-sign"><mrow><mi>A</mi> <mi>c</mi> <mi>c</mi> <mi>u</mi> <mi>r</mi>
    <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext><mtext>+</mtext><mtext>true</mtext><mtext>negatives</mtext></mrow>
    <mrow><mtext>all</mtext><mtext>predicted</mtext><mtext>positives+</mtext><mtext>all</mtext><mtext>predicted</mtext><mtext>negatives</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix**: Counting all true positives, false positives, true negatives,
    and false negatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| True Negative | False Positive |'
  prefs: []
  type: TYPE_TB
- en: '| False Negative | True Positive |'
  prefs: []
  type: TYPE_TB
- en: '**Precision score**: Accuracy of positive predictions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper P r e c i s i o n equals StartFraction true
    positives Over all predicted positives EndFraction equals StartFraction true positives
    Over true positives plus false positives EndFraction dollar-sign"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>c</mi> <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow> <mrow><mtext>all</mtext><mtext>predicted</mtext><mtext>positives</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow> <mrow><mtext>true</mtext><mtext>positives</mtext><mo>+</mo><mtext>false</mtext><mtext>positives</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall score**: Ratio of the positive instances that are correctly classified:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper R e c a l l equals StartFraction true positives
    Over all positives labels EndFraction equals StartFraction true positives Over
    true positives plus false negatives EndFraction dollar-sign"><mrow><mi>R</mi>
    <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi> <mi>l</mi> <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow>
    <mrow><mtext>all</mtext><mtext>positives</mtext><mtext>labels</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>positives</mtext></mrow> <mrow><mtext>true</mtext><mtext>positives</mtext><mo>+</mo><mtext>false</mtext><mtext>negatives</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '**Specificity**: Ratio of the negative instances that are correctly classified:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper S p e c i f i c i t y equals StartFraction
    true negatives Over all negative labels EndFraction equals StartFraction true
    negatives Over true negatives plus false positives EndFraction dollar-sign"><mrow><mi>S</mi>
    <mi>p</mi> <mi>e</mi> <mi>c</mi> <mi>i</mi> <mi>f</mi> <mi>i</mi> <mi>c</mi> <mi>i</mi>
    <mi>t</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>negatives</mtext></mrow>
    <mrow><mtext>all</mtext><mtext>negative</mtext><mtext>labels</mtext></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mtext>true</mtext><mtext>negatives</mtext></mrow> <mrow><mtext>true</mtext><mtext>negatives</mtext><mo>+</mo><mtext>false</mtext><mtext>positives</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '**<math alttext="upper F 1"><msub><mi>F</mi> <mn>1</mn></msub></math> score**:
    This quantity is only high when both precision and recall scores are high:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper F 1 equals StartStartFraction 2 OverOver StartFraction
    1 Over p r e c i s i o n EndFraction plus StartFraction 1 Over r e c a l l EndFraction
    EndEndFraction dollar-sign"><mrow><msub><mi>F</mi> <mn>1</mn></msub> <mo>=</mo>
    <mfrac><mn>2</mn> <mrow><mfrac><mn>1</mn> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mfrac><mo>+</mo><mfrac><mn>1</mn>
    <mrow><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'AUC (Area Under the Curve) and ROC (Receiver Operating Characteristics) curves:
    These curves provide a performance measure for a classification model at various
    threshold values. We can use these curves to measure how well a certain variable
    predicts a certain outcome, for example, how well does the GRE subject test score
    predict passing a graduate school’s qualifying exam in the first year?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrew Ng’s hundred page book: Machine Learning Yearning provides an excellent
    guide for performance metrics’ best practices. Please read carefully before diving
    into real AI applications, since the book’s recipes are based on many trials,
    successes, and failures.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Looking Ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we surveyed some of the most popular machine learning models,
    emphasizing a particular mathematical structure that appears throughout the book:
    training function, loss function and optimization. We discussed linear, logistic,
    and softmax regression, then quickly breezed over support vector machines, decision
    trees, ensembles and random forests.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we made a descent case for studying the following topics from mathematics:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculus
  prefs: []
  type: TYPE_NORMAL
- en: The minimum and maximum happen at the boundary or at points where one derivative
    is zero or does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Algebra
  prefs: []
  type: TYPE_NORMAL
- en: 'Linearly combining features: <math alttext="omega 1 x 1 plus omega 2 x 2 plus
    ellipsis plus omega Subscript n Baseline x Subscript n"><mrow><msub><mi>ω</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <msub><mi>ω</mi> <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing various mathematical expressions using matrix and vector notation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scalar product of two vectors <math alttext="ModifyingAbove a With right-arrow
    Superscript t Baseline ModifyingAbove b With right-arrow"><mrow><msup><mover accent="true"><mi>a</mi>
    <mo>→</mo></mover> <mi>t</mi></msup> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The <math alttext="l squared"><msup><mi>l</mi> <mn>2</mn></msup></math> norm
    of a vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid working with ill conditioned matrices. Get rid of linearly dependent features.
    This also has to do with feature selection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid multiplying matrices by each other, this is too expensive. Multiply matrices
    by vectors instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization
  prefs: []
  type: TYPE_NORMAL
- en: For convex functions we do not worry about getting stuck at local minima since
    local minima are also global minima. We do worry about narrow valleys (next chapter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent methods, they need only one derivative (next chapter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newton’s methods, they need two derivatives or an approximation of two derivatives
    (inconvenient for large data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quadratic programming, the dual problem, and coordinate descent (all appear
    in support vector machines).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics
  prefs: []
  type: TYPE_NORMAL
- en: Correlation matrix and scatterplots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F-test and Mutual Information for feature selection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standarizing the data features (subtracting the mean and dividing by the standard
    deviation).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More steps we did not and will not go over (yet):'
  prefs: []
  type: TYPE_NORMAL
- en: Validating our models- tune the weight values and the hyper-parameters so as
    not to overfit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the trained model on the testing subset of the data, which our model had
    not used (or seen) during the training and validation steps (we do this in the
    accompanying Jupyter notebook).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy and monitor the finalized model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Never stop thinking on how to improve our models and how to better integrate
    them into the whole production pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next chapter, we step into the new and exciting era of neural networks.
  prefs: []
  type: TYPE_NORMAL
