- en: Chapter 15\. Multiple Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I don’t look at a problem and put variables in there that don’t affect it.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bill Parcells
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Although the VP is pretty impressed with your predictive model, she thinks
    you can do better. To that end, you’ve collected additional data: you know how
    many hours each of your users works each day, and whether they have a PhD. You’d
    like to use this additional data to improve your model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, you hypothesize a linear model with more independent variables:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="minutes equals alpha plus beta 1 friends plus beta 2 work hours
    plus beta 3 phd plus epsilon" display="block"><mrow><mtext>minutes</mtext> <mo>=</mo>
    <mi>α</mi> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mtext>friends</mtext>
    <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mtext>work</mtext> <mtext>hours</mtext>
    <mo>+</mo> <msub><mi>β</mi> <mn>3</mn></msub> <mtext>phd</mtext> <mo>+</mo> <mi>ε</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, whether a user has a PhD is not a number—but, as we mentioned in
    [Chapter 11](ch11.html#machine_learning), we can introduce a *dummy variable*
    that equals 1 for users with PhDs and 0 for users without, after which it’s just
    as numeric as the other variables.
  prefs: []
  type: TYPE_NORMAL
- en: The Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that in [Chapter 14](ch14.html#simple_linear_regression) we fit a model
    of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals alpha plus beta x Subscript i Baseline
    plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>α</mi> <mo>+</mo> <mi>β</mi> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now imagine that each input <math><msub><mi>x</mi> <mi>i</mi></msub></math>
    is not a single number but rather a vector of *k* numbers, <math><mrow><msub><mi>x</mi>
    <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>x</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub></mrow></math> . The multiple regression
    model assumes that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals alpha plus beta 1 x Subscript i
    Baseline 1 Baseline plus period period period plus beta Subscript k Baseline x
    Subscript i k Baseline plus epsilon Subscript i Baseline" display="block"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>α</mi> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi><mn>1</mn></mrow></msub> <mo>+</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>+</mo> <msub><mi>β</mi> <mi>k</mi></msub> <msub><mi>x</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub> <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In multiple regression the vector of parameters is usually called *β*. We’ll
    want this to include the constant term as well, which we can achieve by adding
    a column of 1s to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'and:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then our model is just:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this particular case, our independent variable `x` will be a list of vectors,
    each of which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Further Assumptions of the Least Squares Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a couple of further assumptions that are required for this model (and
    our solution) to make sense.
  prefs: []
  type: TYPE_NORMAL
- en: The first is that the columns of *x* are *linearly independent*—that there’s
    no way to write any one as a weighted sum of some of the others. If this assumption
    fails, it’s impossible to estimate `beta`. To see this in an extreme case, imagine
    we had an extra field `num_acquaintances` in our data that for every user was
    exactly equal to `num_friends`.
  prefs: []
  type: TYPE_NORMAL
- en: Then, starting with any `beta`, if we add *any* amount to the `num_friends`
    coefficient and subtract that same amount from the `num_acquaintances` coefficient,
    the model’s predictions will remain unchanged. This means that there’s no way
    to find *the* coefficient for `num_friends`. (Usually violations of this assumption
    won’t be so obvious.)
  prefs: []
  type: TYPE_NORMAL
- en: The second important assumption is that the columns of *x* are all uncorrelated
    with the errors <math><mi>ε</mi></math> . If this fails to be the case, our estimates
    of `beta` will be systematically wrong.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in [Chapter 14](ch14.html#simple_linear_regression), we built
    a model that predicted that each additional friend was associated with an extra
    0.90 daily minutes on the site.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine it’s also the case that:'
  prefs: []
  type: TYPE_NORMAL
- en: People who work more hours spend less time on the site.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People with more friends tend to work more hours.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That is, imagine that the “actual” model is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="minutes equals alpha plus beta 1 friends plus beta 2 work hours
    plus epsilon" display="block"><mrow><mtext>minutes</mtext> <mo>=</mo> <mi>α</mi>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mtext>friends</mtext> <mo>+</mo>
    <msub><mi>β</mi> <mn>2</mn></msub> <mtext>work</mtext> <mtext>hours</mtext> <mo>+</mo>
    <mi>ε</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math><msub><mi>β</mi> <mn>2</mn></msub></math> is negative, and that
    work hours and friends are positively correlated. In that case, when we minimize
    the errors of the single-variable model:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="minutes equals alpha plus beta 1 friends plus epsilon" display="block"><mrow><mtext>minutes</mtext>
    <mo>=</mo> <mi>α</mi> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mtext>friends</mtext>
    <mo>+</mo> <mi>ε</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: we will underestimate <math><msub><mi>β</mi> <mn>1</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Think about what would happen if we made predictions using the single-variable
    model with the “actual” value of <math><msub><mi>β</mi> <mn>1</mn></msub></math>
    . (That is, the value that arises from minimizing the errors of what we called
    the “actual” model.) The predictions would tend to be way too large for users
    who work many hours and a little too large for users who work few hours, because
    <math><mrow><msub><mi>β</mi> <mn>2</mn></msub> <mo><</mo> <mn>0</mn></mrow></math>
    and we “forgot” to include it. Because work hours is positively correlated with
    number of friends, this means the predictions tend to be way too large for users
    with many friends, and only slightly too large for users with few friends.
  prefs: []
  type: TYPE_NORMAL
- en: The result of this is that we can reduce the errors (in the single-variable
    model) by decreasing our estimate of <math><msub><mi>β</mi> <mn>1</mn></msub></math>
    , which means that the error-minimizing <math><msub><mi>β</mi> <mn>1</mn></msub></math>
    is smaller than the “actual” value. That is, in this case the single-variable
    least squares solution is biased to underestimate <math><msub><mi>β</mi> <mn>1</mn></msub></math>
    . And, in general, whenever the independent variables are correlated with the
    errors like this, our least squares solution will give us a biased estimate of
    <math><msub><mi>β</mi> <mn>1</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we did in the simple linear model, we’ll choose `beta` to minimize the sum
    of squared errors. Finding an exact solution is not simple to do by hand, which
    means we’ll need to use gradient descent. Again we’ll want to minimize the sum
    of the squared errors. The error function is almost identical to the one we used
    in [Chapter 14](ch14.html#simple_linear_regression), except that instead of expecting
    parameters `[alpha, beta]` it will take a vector of arbitrary length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you know calculus, it’s easy to compute the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Otherwise, you’ll need to take my word for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we’re ready to find the optimal `beta` using gradient descent.
    Let’s first write out a `least_squares_fit` function that can work with any dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then apply that to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, you wouldn’t estimate a linear regression using gradient descent;
    you’d get the exact coefficients using linear algebra techniques that are beyond
    the scope of this book. If you did so, you’d find the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>minutes</mtext> <mo>=</mo> <mn>30</mn> <mo>.</mo>
    <mn>58</mn> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>972</mn> <mtext>friends</mtext>
    <mo>-</mo> <mn>1</mn> <mo>.</mo> <mn>87</mn> <mtext>work</mtext> <mtext>hours</mtext>
    <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>923</mn> <mtext>phd</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: which is pretty close to what we found.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should think of the coefficients of the model as representing all-else-being-equal
    estimates of the impacts of each factor. All else being equal, each additional
    friend corresponds to an extra minute spent on the site each day. All else being
    equal, each additional hour in a user’s workday corresponds to about two fewer
    minutes spent on the site each day. All else being equal, having a PhD is associated
    with spending an extra minute on the site each day.
  prefs: []
  type: TYPE_NORMAL
- en: What this doesn’t (directly) tell us is anything about the interactions among
    the variables. It’s possible that the effect of work hours is different for people
    with many friends than it is for people with few friends. This model doesn’t capture
    that. One way to handle this case is to introduce a new variable that is the *product*
    of “friends” and “work hours.” This effectively allows the “work hours” coefficient
    to increase (or decrease) as the number of friends increases.
  prefs: []
  type: TYPE_NORMAL
- en: Or it’s possible that the more friends you have, the more time you spend on
    the site *up to a point*, after which further friends cause you to spend less
    time on the site. (Perhaps with too many friends the experience is just too overwhelming?)
    We could try to capture this in our model by adding another variable that’s the
    *square* of the number of friends.
  prefs: []
  type: TYPE_NORMAL
- en: Once we start adding variables, we need to worry about whether their coefficients
    “matter.” There are no limits to the numbers of products, logs, squares, and higher
    powers we could add.
  prefs: []
  type: TYPE_NORMAL
- en: Goodness of Fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again we can look at the R-squared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'which has now increased to 0.68:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind, however, that adding new variables to a regression will *necessarily*
    increase the R-squared. After all, the simple regression model is just the special
    case of the multiple regression model where the coefficients on “work hours” and
    “PhD” both equal 0\. The optimal multiple regression model will necessarily have
    an error at least as small as that one.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, in a multiple regression, we also need to look at the *standard
    errors* of the coefficients, which measure how certain we are about our estimates
    of each <math><msub><mi>β</mi> <mi>i</mi></msub></math> . The regression as a
    whole may fit our data very well, but if some of the independent variables are
    correlated (or irrelevant), their coefficients might not *mean* much.
  prefs: []
  type: TYPE_NORMAL
- en: The typical approach to measuring these errors starts with another assumption—that
    the errors <math><msub><mi>ε</mi> <mi>i</mi></msub></math> are independent normal
    random variables with mean 0 and some shared (unknown) standard deviation <math><mi>σ</mi></math>
    . In that case, we (or, more likely, our statistical software) can use some linear
    algebra to find the standard error of each coefficient. The larger it is, the
    less sure our model is about that coefficient. Unfortunately, we’re not set up
    to do that kind of linear algebra from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Digression: The Bootstrap'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine that we have a sample of *n* data points, generated by some (unknown
    to us) distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 5](ch05.html#statistics), we wrote a function that could compute
    the `median` of the sample, which we can use as an estimate of the median of the
    distribution itself.
  prefs: []
  type: TYPE_NORMAL
- en: But how confident can we be about our estimate? If all the data points in the
    sample are very close to 100, then it seems likely that the actual median is close
    to 100\. If approximately half the data points in the sample are close to 0 and
    the other half are close to 200, then we can’t be nearly as certain about the
    median.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we could repeatedly get new samples, we could compute the medians of many
    samples and look at the distribution of those medians. Often we can’t. In that
    case we can *bootstrap* new datasets by choosing *n* data points *with replacement*
    from our data. And then we can compute the medians of those synthetic datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, consider the two following datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you compute the `median`s of the two datasets, both will be very close to
    100\. However, if you look at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'you will mostly see numbers really close to 100\. But if you look at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: you will see a lot of numbers close to 0 and a lot of numbers close to 200.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `standard_deviation` of the first set of medians is close to 0, while that
    of the second set of medians is close to 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: (This extreme a case would be pretty easy to figure out by manually inspecting
    the data, but in general that won’t be true.)
  prefs: []
  type: TYPE_NORMAL
- en: Standard Errors of Regression Coefficients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can take the same approach to estimating the standard errors of our regression
    coefficients. We repeatedly take a `bootstrap_sample` of our data and estimate
    `beta` based on that sample. If the coefficient corresponding to one of the independent
    variables (say, `num_friends`) doesn’t vary much across samples, then we can be
    confident that our estimate is relatively tight. If the coefficient varies greatly
    across samples, then we can’t be at all confident in our estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only subtlety is that, before sampling, we’ll need to `zip` our `x` data
    and `y` data to make sure that corresponding values of the independent and dependent
    variables are sampled together. This means that `bootstrap_sample` will return
    a list of pairs `(x_i, y_i)`, which we’ll need to reassemble into an `x_sample`
    and a `y_sample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After which we can estimate the standard deviation of each coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: (We would likely get better estimates if we collected more than 100 samples
    and used more than 5,000 iterations to estimate each `beta`, but we don’t have
    all day.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use these to test hypotheses such as “does <math><msub><mi>β</mi> <mi>i</mi></msub></math>
    equal 0?” Under the null hypothesis <math><mrow><msub><mi>β</mi> <mi>i</mi></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> (and with our other assumptions about the
    distribution of <math><msub><mi>ε</mi> <mi>i</mi></msub></math> ), the statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="t Subscript j Baseline equals ModifyingAbove beta Subscript j
    Baseline With caret slash ModifyingAbove sigma Subscript j Baseline With caret"
    display="block"><mrow><msub><mi>t</mi> <mi>j</mi></msub> <mo>=</mo> <mover accent="true"><msub><mi>β</mi>
    <mi>j</mi></msub> <mo>^</mo></mover> <mo>/</mo> <mover accent="true"><msub><mi>σ</mi>
    <mi>j</mi></msub> <mo>^</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: which is our estimate of <math><msub><mi>β</mi> <mi>j</mi></msub></math> divided
    by our estimate of its standard error, follows a *Student’s t-distribution* with
    “ <math><mrow><mi>n</mi> <mo>-</mo> <mi>k</mi></mrow></math> degrees of freedom.”
  prefs: []
  type: TYPE_NORMAL
- en: If we had a `students_t_cdf` function, we could compute *p*-values for each
    least-squares coefficient to indicate how likely we would be to observe such a
    value if the actual coefficient were 0. Unfortunately, we don’t have such a function.
    (Although we would if we weren’t working from scratch.)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as the degrees of freedom get large, the *t*-distribution gets closer
    and closer to a standard normal. In a situation like this, where *n* is much larger
    than *k*, we can use `normal_cdf` and still feel good about ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: (In a situation *not* like this, we would probably be using statistical software
    that knows how to compute the *t*-distribution, as well as how to compute the
    exact standard errors.)
  prefs: []
  type: TYPE_NORMAL
- en: While most of the coefficients have very small *p*-values (suggesting that they
    are indeed nonzero), the coefficient for “PhD” is not “significantly” different
    from 0, which makes it likely that the coefficient for “PhD” is random rather
    than meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: In more elaborate regression scenarios, you sometimes want to test more elaborate
    hypotheses about the data, such as “at least one of the <math><msub><mi>β</mi>
    <mi>j</mi></msub></math> is nonzero” or “ <math><msub><mi>β</mi> <mn>1</mn></msub></math>
    equals <math><msub><mi>β</mi> <mn>2</mn></msub></math> *and* <math><msub><mi>β</mi>
    <mn>3</mn></msub></math> equals <math><msub><mi>β</mi> <mn>4</mn></msub></math>
    .” You can do this with an *F-test*, but alas, that falls outside the scope of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, you’d often like to apply linear regression to datasets with large
    numbers of variables. This creates a couple of extra wrinkles. First, the more
    variables you use, the more likely you are to overfit your model to the training
    set. And second, the more nonzero coefficients you have, the harder it is to make
    sense of them. If the goal is to *explain* some phenomenon, a sparse model with
    three factors might be more useful than a slightly better model with hundreds.
  prefs: []
  type: TYPE_NORMAL
- en: '*Regularization* is an approach in which we add to the error term a penalty
    that gets larger as `beta` gets larger. We then minimize the combined error and
    penalty. The more importance we place on the penalty term, the more we discourage
    large coefficients.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in *ridge regression*, we add a penalty proportional to the sum
    of the squares of the `beta_i` (except that typically we don’t penalize `beta_0`,
    the constant term):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then plug this into gradient descent in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: And then we just need to modify the `least_squares_fit` function to use the
    `sqerror_ridge_gradient` instead of `sqerror_gradient`. (I’m not going to repeat
    the code here.)
  prefs: []
  type: TYPE_NORMAL
- en: 'With `alpha` set to 0, there’s no penalty at all and we get the same results
    as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As we increase `alpha`, the goodness of fit gets worse, but the size of `beta`
    gets smaller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In particular, the coefficient on “PhD” vanishes as we increase the penalty,
    which accords with our previous result that it wasn’t significantly different
    from 0.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Usually you’d want to `rescale` your data before using this approach. After
    all, if you changed years of experience to centuries of experience, its least
    squares coefficient would increase by a factor of 100 and suddenly get penalized
    much more, even though it’s the same model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is *lasso regression*, which uses the penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Whereas the ridge penalty shrank the coefficients overall, the lasso penalty
    tends to force coefficients to be 0, which makes it good for learning sparse models.
    Unfortunately, it’s not amenable to gradient descent, which means that we won’t
    be able to solve it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression has a rich and expansive theory behind it. This is another place
    where you should consider reading a textbook, or at least a lot of Wikipedia articles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn has a [`linear_model` module](https://scikit-learn.org/stable/modules/linear_model.html)
    that provides a `LinearRegression` model similar to ours, as well as ridge regression,
    lasso regression, and other types of regularization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statsmodels](https://www.statsmodels.org) is another Python module that contains
    (among other things) linear regression models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
