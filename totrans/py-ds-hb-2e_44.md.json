["```py\nIn [1]: from sklearn.datasets import load_iris\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n```", "```py\nIn [2]: from sklearn.neighbors import KNeighborsClassifier\n        model = KNeighborsClassifier(n_neighbors=1)\n```", "```py\nIn [3]: model.fit(X, y)\n        y_model = model.predict(X)\n```", "```py\nIn [4]: from sklearn.metrics import accuracy_score\n        accuracy_score(y, y_model)\nOut[4]: 1.0\n```", "```py\nIn [5]: from sklearn.model_selection import train_test_split\n        # split the data with 50% in each set\n        X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n                                          train_size=0.5)\n\n        # fit the model on one set of data\n        model.fit(X1, y1)\n\n        # evaluate the model on the second set of data\n        y2_model = model.predict(X2)\n        accuracy_score(y2, y2_model)\nOut[5]: 0.9066666666666666\n```", "```py\nIn [6]: y2_model = model.fit(X1, y1).predict(X2)\n        y1_model = model.fit(X2, y2).predict(X1)\n        accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)\nOut[6]: (0.96, 0.9066666666666666)\n```", "```py\nIn [7]: from sklearn.model_selection import cross_val_score\n        cross_val_score(model, X, y, cv=5)\nOut[7]: array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1.        ])\n```", "```py\nIn [8]: from sklearn.model_selection import LeaveOneOut\n        scores = cross_val_score(model, X, y, cv=LeaveOneOut())\n        scores\nOut[8]: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n               1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n               1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n               1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n               1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n               1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n               1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n               0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n               1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n```", "```py\nIn [9]: scores.mean()\nOut[9]: 0.96\n```", "```py\nIn [10]: from sklearn.preprocessing import PolynomialFeatures\n         from sklearn.linear_model import LinearRegression\n         from sklearn.pipeline import make_pipeline\n\n         def PolynomialRegression(degree=2, **kwargs):\n             return make_pipeline(PolynomialFeatures(degree),\n                                  LinearRegression(**kwargs))\n```", "```py\nIn [11]: import numpy as np\n\n         def make_data(N, err=1.0, rseed=1):\n             # randomly sample the data\n             rng = np.random.RandomState(rseed)\n             X = rng.rand(N, 1) ** 2\n             y = 10 - 1. / (X.ravel() + 0.1)\n             if err > 0:\n                 y += err * rng.randn(N)\n             return X, y\n\n         X, y = make_data(40)\n```", "```py\nIn [12]: %matplotlib inline\n         import matplotlib.pyplot as plt\n         plt.style.use('seaborn-whitegrid')\n\n         X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n\n         plt.scatter(X.ravel(), y, color='black')\n         axis = plt.axis()\n         for degree in [1, 3, 5]:\n             y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n             plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\n         plt.xlim(-0.1, 1.0)\n         plt.ylim(-2, 12)\n         plt.legend(loc='best');\n```", "```py\nIn [13]: from sklearn.model_selection import validation_curve\n         degree = np.arange(0, 21)\n         train_score, val_score = validation_curve(\n             PolynomialRegression(), X, y,\n             param_name='polynomialfeatures__degree',\n             param_range=degree, cv=7)\n\n         plt.plot(degree, np.median(train_score, 1),\n                  color='blue', label='training score')\n         plt.plot(degree, np.median(val_score, 1),\n                  color='red', label='validation score')\n         plt.legend(loc='best')\n         plt.ylim(0, 1)\n         plt.xlabel('degree')\n         plt.ylabel('score');\n```", "```py\nIn [14]: plt.scatter(X.ravel(), y)\n         lim = plt.axis()\n         y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n         plt.plot(X_test.ravel(), y_test);\n         plt.axis(lim);\n```", "```py\nIn [15]: X2, y2 = make_data(200)\n         plt.scatter(X2.ravel(), y2);\n```", "```py\nIn [16]: degree = np.arange(21)\n         train_score2, val_score2 = validation_curve(\n             PolynomialRegression(), X2, y2,\n             param_name='polynomialfeatures__degree',\n             param_range=degree, cv=7)\n\n         plt.plot(degree, np.median(train_score2, 1),\n                  color='blue', label='training score')\n         plt.plot(degree, np.median(val_score2, 1),\n                  color='red', label='validation score')\n         plt.plot(degree, np.median(train_score, 1),\n                  color='blue', alpha=0.3, linestyle='dashed')\n         plt.plot(degree, np.median(val_score, 1),\n                  color='red', alpha=0.3, linestyle='dashed')\n         plt.legend(loc='lower center')\n         plt.ylim(0, 1)\n         plt.xlabel('degree')\n         plt.ylabel('score');\n```", "```py\nIn [17]: from sklearn.model_selection import learning_curve\n\n         fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n         fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\n         for i, degree in enumerate([2, 9]):\n             N, train_lc, val_lc = learning_curve(\n                 PolynomialRegression(degree), X, y, cv=7,\n                 train_sizes=np.linspace(0.3, 1, 25))\n\n             ax[i].plot(N, np.mean(train_lc, 1),\n                        color='blue', label='training score')\n             ax[i].plot(N, np.mean(val_lc, 1),\n                        color='red', label='validation score')\n             ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0],\n                          N[-1], color='gray', linestyle='dashed')\n\n             ax[i].set_ylim(0, 1)\n             ax[i].set_xlim(N[0], N[-1])\n             ax[i].set_xlabel('training size')\n             ax[i].set_ylabel('score')\n             ax[i].set_title('degree = {0}'.format(degree), size=14)\n             ax[i].legend(loc='best')\n```", "```py\nIn [18]: from sklearn.model_selection import GridSearchCV\n\n         param_grid = {'polynomialfeatures__degree': np.arange(21),\n                       'linearregression__fit_intercept': [True, False]}\n\n         grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n```", "```py\nIn [19]: grid.fit(X, y);\n```", "```py\nIn [20]: grid.best_params_\nOut[20]: {'linearregression__fit_intercept': False, 'polynomialfeatures__degree': 4}\n```", "```py\nIn [21]: model = grid.best_estimator_\n\n         plt.scatter(X.ravel(), y)\n         lim = plt.axis()\n         y_test = model.fit(X, y).predict(X_test)\n         plt.plot(X_test.ravel(), y_test);\n         plt.axis(lim);\n```"]