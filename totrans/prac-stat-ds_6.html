<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Statistical Machine Learning"><div class="chapter" id="StatisticalML">
<h1><span class="label">Chapter 6. </span>Statistical Machine Learning</h1>


<p>Recent advances in statistics have been devoted to developing more powerful automated techniques for predictive modeling—both regression and classification.<a data-type="indexterm" data-primary="statistical machine learning" id="ix_statML"/>
These methods, like those discussed in the previous chapter, are <em>supervised methods</em>—they are trained on data where outcomes are known and learn to predict outcomes in new data.<a data-type="indexterm" data-primary="supervised learning" id="idm46522845250856"/>  They fall under the umbrella of <em>statistical machine learning</em> and are distinguished from classical statistical methods in that they are data-driven and do not seek to impose linear or other overall structure on the data.<a data-type="indexterm" data-primary="machine learning" data-seealso="statistical machine learning; supervised learning; unsupervised learning" id="idm46522845249416"/>
The <em>K</em>-Nearest Neighbors method, for example, is quite simple: classify a record in accordance with how similar records are classified.<a data-type="indexterm" data-primary="K-Nearest Neighbors" id="idm46522845247816"/>
The most successful and widely used techniques are based on <em>ensemble learning</em> applied to <em>decision trees</em>.<a data-type="indexterm" data-primary="ensemble learning" id="idm46522845246072"/><a data-type="indexterm" data-primary="decision trees" data-secondary="ensemble learning applied to" id="idm46522845245336"/>
The basic idea of ensemble learning is to use many models to form a prediction, as opposed to using just a single model.
Decision trees are a flexible and automatic technique to learn rules about the relationships between predictor variables and outcome variables.
It turns out that the combination of ensemble learning with decision trees leads to some of the best performing off-the-shelf predictive modeling techniques.</p>

<p>The development of many of the techniques in statistical machine learning can be traced back<a data-type="indexterm" data-primary="Breiman, Leo" id="idm46522845243528"/> to the statisticians Leo Breiman (see <a data-type="xref" href="#LeoBreiman">Figure 6-1</a>) at the University of California at Berkeley and Jerry Friedman at Stanford University.<a data-type="indexterm" data-primary="Friedman, Jerome H. (Jerry)" id="idm46522845241784"/>
Their work, along with that of other researchers at Berkeley and Stanford, started with the development of tree models in 1984.
The subsequent development of ensemble methods of bagging and boosting in the 1990s established the foundation of statistical machine learning.</p>

<figure><div id="LeoBreiman" class="figure">
<img src="Images/psd2_0601.png" alt="Leo Breiman, who was a professor of statistics at UC Berkeley, was at the forefront of the development of many techniques in a data scientist's toolkit today" width="196" height="182"/>
<h6><span class="label">Figure 6-1. </span>Leo Breiman, who was a professor of statistics at UC Berkeley, was at the forefront of the development of many techniques in a data scientist’s toolkit today</h6>
</div></figure>
<div data-type="note" epub:type="note" id="MLVersusStats"><h1>Machine Learning Versus Statistics</h1>
<p>In the context of predictive modeling, what is the difference between machine learning and statistics?<a data-type="indexterm" data-primary="statistics versus machine learning" id="idm46522845236456"/><a data-type="indexterm" data-primary="machine learning" data-secondary="statistics versus" id="idm46522845235784"/><a data-type="indexterm" data-primary="predictive modeling" data-secondary="machine learning vs. statistics" id="idm46522845234840"/>
There is not a bright line dividing the two disciplines.
Machine learning tends to be focused more on developing efficient algorithms that scale to large data in order to optimize the predictive model.
Statistics generally pays more attention to the probabilistic theory and underlying structure of the model.<a data-type="indexterm" data-primary="bagging" id="idm46522845233432"/><a data-type="indexterm" data-primary="random forests" id="idm46522845232760"/>
Bagging, and the random forest (see <a data-type="xref" href="#Bagging">“Bagging and the Random Forest”</a>),  grew up firmly in the statistics camp.
Boosting (see <a data-type="xref" href="#Boosting">“Boosting”</a>), on the other hand, has been developed in both disciplines but receives more attention on the machine learning side of the divide.<a data-type="indexterm" data-primary="boosting" id="idm46522845230216"/>
Regardless of the history, the promise of boosting ensures that it will thrive as a technique in both statistics and machine learning.</p>
</div>






<section data-type="sect1" data-pdf-bookmark="K-Nearest Neighbors"><div class="sect1" id="KNN">
<h1>K-Nearest Neighbors</h1>

<p>The idea<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" id="ix_statMLKNN"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" id="ix_KNN"/> behind <em>K</em>-Nearest Neighbors (KNN) is very simple.<sup><a data-type="noteref" id="idm46522845224392-marker" href="ch06.xhtml#idm46522845224392">1</a></sup>
For each record to be classified or predicted:</p>
<ol>
<li>
<p>Find <em>K</em> records that have similar features (i.e., similar predictor values).<a data-type="indexterm" data-primary="KNN" data-see="K-Nearest Neighbors" id="idm46522845222088"/></p>
</li>
<li>
<p>For classification, find out what the majority class is among those similar records and assign that class to the new record.</p>
</li>
<li>
<p>For prediction (also called <em>KNN regression</em>), find the average among those similar records, and predict that average for the new record.</p>
</li>

</ol>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522845218280">
<h5>Key Terms for K-Nearest Neighbors</h5><dl>
<dt class="horizontal"><strong><em>Neighbor</em></strong></dt>
<dd>
<p>A record that has similar predictor values to another record.</p>
</dd>
<dt class="horizontal"><strong><em>Distance metrics</em></strong></dt>
<dd>
<p>Measures that sum up in a single number how far one record is from another.<a data-type="indexterm" data-primary="neighbors (in K-Nearest Neighbors)" id="idm46522845213240"/></p>
</dd>
<dt class="horizontal"><strong><em>Standardization</em></strong></dt>
<dd>
<p>Subtract the mean and divide by the standard deviation.</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Normalization<a data-type="indexterm" data-primary="distance metrics" id="idm46522845208920"/><a data-type="indexterm" data-primary="standardization" id="idm46522845208184"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>z-score</em></strong></dt>
<dd>
<p>The value that results after standardization.</p>
</dd>
<dt class="horizontal"><strong><em>K</em></strong></dt>
<dd>
<p>The number of neighbors considered in the nearest neighbor calculation.<a data-type="indexterm" data-primary="z-scores" id="idm46522845203544"/><a data-type="indexterm" data-primary="K (in K-Nearest Neighbors)" id="idm46522845202760"/></p>
</dd>
</dl>
</div></aside>

<p>KNN is one of the simpler prediction/classification techniques:
there is no model to be fit (as in regression).
This doesn’t mean that using KNN is an automatic procedure.
The prediction results depend on how the features are scaled,
how similarity is measured, and how big <em>K</em> is set.
Also, all predictors must be in numeric form.
We will illustrate how to use the KNN method with a classification example.</p>








<section data-type="sect2" data-pdf-bookmark="A Small Example: Predicting Loan Default"><div class="sect2" id="LoanExampleKNN">
<h2>A Small Example: Predicting Loan Default</h2>

<p><a data-type="xref" href="#loan_data">Table 6-1</a> shows a few records of personal loan data from LendingClub.
LendingClub is a leader in peer-to-peer lending in which pools of investors make personal loans to individuals.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="example, predicting loan default" id="ix_statMLKNNpred"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="example, predicting loan default" id="ix_KNNex"/><a data-type="indexterm" data-primary="prediction" data-secondary="predicting loan default with K-Nearest Neighbors" id="ix_predKNN"/>
The goal of an analysis would be to predict the outcome of a new potential loan: paid off versus default.</p>
<table id="loan_data">
<caption><span class="label">Table 6-1. </span>A few records and columns for LendingClub loan data</caption>
<thead>
<tr>
<th>Outcome</th>
<th>Loan amount</th>
<th>Income</th>
<th>Purpose</th>
<th>Years employed</th>
<th>Home ownership</th>
<th>State</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Paid off</p></td>
<td><p>10000</p></td>
<td><p>79100</p></td>
<td><p>debt_consolidation</p></td>
<td><p>11</p></td>
<td><p>MORTGAGE</p></td>
<td><p>NV</p></td>
</tr>
<tr>
<td><p>Paid off</p></td>
<td><p>9600</p></td>
<td><p>48000</p></td>
<td><p>moving</p></td>
<td><p>5</p></td>
<td><p>MORTGAGE</p></td>
<td><p>TN</p></td>
</tr>
<tr>
<td><p>Paid off</p></td>
<td><p>18800</p></td>
<td><p>120036</p></td>
<td><p>debt_consolidation</p></td>
<td><p>11</p></td>
<td><p>MORTGAGE</p></td>
<td><p>MD</p></td>
</tr>
<tr>
<td><p>Default</p></td>
<td><p>15250</p></td>
<td><p>232000</p></td>
<td><p>small_business</p></td>
<td><p>9</p></td>
<td><p>MORTGAGE</p></td>
<td><p>CA</p></td>
</tr>
<tr>
<td><p>Paid off</p></td>
<td><p>17050</p></td>
<td><p>35000</p></td>
<td><p>debt_consolidation</p></td>
<td><p>4</p></td>
<td><p>RENT</p></td>
<td><p>MD</p></td>
</tr>
<tr>
<td><p>Paid off</p></td>
<td><p>5500</p></td>
<td><p>43000</p></td>
<td><p>debt_consolidation</p></td>
<td><p>4</p></td>
<td><p>RENT</p></td>
<td><p>KS</p></td>
</tr>
</tbody>
</table>

<p>Consider a very simple model with
just two predictor variables: <code>dti</code>, which is the ratio of debt payments (excluding mortgage) to income, and <code>payment_inc_ratio</code>, which is the ratio of the loan payment to income.
Both ratios are multiplied by 100.
Using a small set of 200 loans, <code>loan200</code>, with known binary outcomes (default or no-default, specified in the predictor <code>outcome200</code>), and with <em>K</em> set to 20, the KNN estimate for a new loan to be predicted, <code>newloan</code>, with <code>dti=22.5</code> and <code>payment_inc_ratio=9</code> can be calculated in <em>R</em> as follows:<sup><a data-type="noteref" id="idm46522845155448-marker" href="ch06.xhtml#idm46522845155448">2</a></sup></p>

<pre data-type="programlisting" data-code-language="r"><code class="n">newloan</code> <code class="o">&lt;-</code> <code class="n">loan200</code><code class="p">[</code><code class="m">1</code><code class="p">,</code> <code class="m">2</code><code class="o">:</code><code class="m">3</code><code class="p">,</code> <code class="n">drop</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">]</code>
<code class="n">knn_pred</code> <code class="o">&lt;-</code> <code class="nf">knn</code><code class="p">(</code><code class="n">train</code><code class="o">=</code><code class="n">loan200</code><code class="p">[</code><code class="m">-1</code><code class="p">,</code> <code class="m">2</code><code class="o">:</code><code class="m">3</code><code class="p">],</code> <code class="n">test</code><code class="o">=</code><code class="n">newloan</code><code class="p">,</code> <code class="n">cl</code><code class="o">=</code><code class="n">loan200</code><code class="p">[</code><code class="m">-1</code><code class="p">,</code> <code class="m">1</code><code class="p">],</code> <code class="n">k</code><code class="o">=</code><code class="m">20</code><code class="p">)</code>
<code class="n">knn_pred</code> <code class="o">==</code> <code class="s">'paid off'</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="kc">TRUE</code></pre>

<p>The KNN prediction is for the loan to default.</p>

<p>While <em>R</em> has a native <code>knn</code> function,
the contributed <em>R</em> package <a href="https://oreil.ly/RMQFG"><code>FNN</code>, for Fast Nearest Neighbor</a>, scales more effectively to big data and provides more flexibility.</p>

<p>The <code>scikit-learn</code> package provides a fast and efficient implementation of KNN in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="s1">'dti'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">newloan</code> <code class="o">=</code> <code class="n">loan200</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">0</code><code class="p">,</code> <code class="n">predictors</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">loan200</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">:,</code> <code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan200</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">:,</code> <code class="n">outcome</code><code class="p">]</code>

<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">newloan</code><code class="p">)</code></pre>

<p><a data-type="xref" href="#LoanKNN">Figure 6-2</a> gives a visual display of this example.
The new loan to be predicted is the cross in the middle.
The squares (paid off) and circles (default) are the training data.
The large black circle shows the boundary of the nearest 20 points.
In this case, 9 defaulted loans lie within the circle, as compared with 11 paid-off loans.  Hence the predicted outcome of the loan is paid off. Note that if we consider only three nearest neighbors, the prediction would be that the loan defaults.</p>

<figure><div id="LoanKNN" class="figure">
<img src="Images/psd2_0602.png" alt="KNN prediction of loan default using two variables: debt-to-income ratio and loan-payment-to-income ratio" width="1579" height="1156"/>
<h6><span class="label">Figure 6-2. </span>KNN prediction of loan default using two variables: debt-to-income ratio and loan-payment-to-income ratio</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>While the output of KNN for classification is typically a binary decision,
such as default or paid off in the loan data,
KNN routines usually offer the opportunity to output a probability (propensity) between 0 and 1.<a data-type="indexterm" data-primary="probability" data-secondary="output by K-Nearest Neighbors" id="idm46522845103448"/>
The probability is based on the fraction of one class in the <em>K</em> nearest neighbors.
In the preceding example, this probability of default would have been estimated at <math alttext="nine-twenty-ths">
  <mfrac><mn>9</mn> <mn>20</mn></mfrac>
</math>, or 0.45.
Using a probability score lets you use classification rules other than simple majority votes (probability of 0.5).<a data-type="indexterm" data-primary="imbalanced data strategies for classification models" id="idm46522844985720"/>
This is especially important in problems with imbalanced classes; see <a data-type="xref" href="ch05.xhtml#ImbalancedData">“Strategies for Imbalanced Data”</a>.  For example, if the goal is to identify members of a rare class, the cutoff would typically be set below 50%.  One common approach is to set the cutoff at the probability of the rare event.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="example, predicting loan default" data-startref="ix_statMLKNNpred" id="idm46522844983816"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="example, predicting loan default" data-startref="ix_KNNex" id="idm46522844982392"/><a data-type="indexterm" data-primary="prediction" data-secondary="predicting loan default with K-Nearest Neighbors" data-startref="ix_predKNN" id="idm46522844981160"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Distance Metrics"><div class="sect2" id="DistanceMetrics">
<h2>Distance Metrics</h2>

<p>Similarity (nearness) is determined<a data-type="indexterm" data-primary="distance metrics" id="idm46522844978104"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="distance metrics" id="idm46522844977016"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="distance metrics" id="idm46522844976072"/> using a <em>distance metric</em>, which is a function that measures how far two records (<em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, …, <em>x<sub>p</sub></em>)  and  (<em>u<sub>1</sub></em>, <em>u<sub>2</sub></em>, …, <em>u<sub>p</sub></em>) are from one another.
The most popular distance metric between two vectors is <em>Euclidean distance</em>.
To measure <a data-type="indexterm" data-primary="Euclidean distance" id="idm46522844969544"/>the Euclidean distance between two vectors, subtract one from the other, square the differences, sum them, and take the square root:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msqrt>
      <mrow>
        <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>1</mn> </msub><mo>-</mo><msub><mi>u</mi> <mn>1</mn> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
        <mo>+</mo>
        <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn> </msub><mo>-</mo><msub><mi>u</mi> <mn>2</mn> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
        <mo>+</mo>
        <mo>⋯</mo>
        <mo>+</mo>
        <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi> </msub><mo>-</mo><msub><mi>u</mi> <mi>p</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
      </mrow>
    </msqrt>
    <mo>.</mo>
  </mrow>
</math>
</div>

<p>Another common <a data-type="indexterm" data-primary="Manhattan distance" id="idm46522844951480"/>distance metric for numeric data is <em>Manhattan distance</em>:</p>
<div data-type="equation">
<math display="block">
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>x</mi>
    <mn>1</mn>
  </msub>
  <mo>−<!-- − --></mo>
  <msub>
    <mi>u</mi>
    <mn>1</mn>
  </msub>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <mo>+</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>x</mi>
    <mn>2</mn>
  </msub>
  <mo>−<!-- − --></mo>
  <msub>
    <mi>u</mi>
    <mn>2</mn>
  </msub>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <mo>+</mo>
  <mo>⋯<!-- ⋯ --></mo>
  <mo>+</mo>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>x</mi>
    <mi>p</mi>
  </msub>
  <mo>−<!-- − --></mo>
  <msub>
    <mi>u</mi>
    <mi>p</mi>
  </msub>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
</math>
</div>

<p>Euclidean distance corresponds to the straight-line distance between two points (e.g., as the crow flies).
Manhattan distance is the distance between two points traversed in a single direction at a time (e.g., traveling along rectangular city blocks).
For this reason, Manhattan distance is a useful approximation if similarity is defined as point-to-point travel time.</p>

<p>In measuring distance between two vectors, variables (features) that are measured with comparatively large scale will dominate the measure.
For example, for the loan data, the distance would be almost solely a function of the income and loan amount variables, which are measured in tens or hundreds of thousands.
Ratio variables would count for practically nothing in comparison.
We address this <a data-type="indexterm" data-primary="standardization" id="idm46522844926568"/>problem by standardizing the data; see <a data-type="xref" href="#Standardization">“Standardization (Normalization, z-Scores)”</a>.</p>
<div data-type="note" epub:type="note" id="Mahalanobis"><h1>Other Distance Metrics</h1>
<p>There are numerous other metrics for measuring distance between vectors.
For numeric data,
<em>Mahalanobis distance</em> is attractive since it accounts for the correlation between two variables.<a data-type="indexterm" data-primary="Mahalanobis distance" id="idm46522844894664"/><a data-type="indexterm" data-primary="numeric variables" data-secondary="Mahalanobis distance" id="idm46522844893960"/>
This is useful since if two variables are highly correlated, Mahalanobis will essentially treat these as a single variable in terms of distance.
Euclidean and Manhattan distance do not account for the correlation, effectively placing greater weight on the attribute that underlies those features.
Mahalanobis distance is the Euclidean distance between the principal components (see <a data-type="xref" href="ch07.xhtml#PCA">“Principal Components Analysis”</a>).
The downside of using Mahalanobis distance is increased computational effort and complexity; it is computed using<a data-type="indexterm" data-primary="covariance matrix" id="idm46522844891560"/> the <em>covariance matrix</em> (see <a data-type="xref" href="ch05.xhtml#Covariance">“Covariance Matrix”</a>).</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="One Hot Encoder"><div class="sect2" id="OneHotEncoder">
<h2>One Hot Encoder</h2>

<p>The loan data in <a data-type="xref" href="#loan_data">Table 6-1</a> includes several factor (string) variables.<a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="one hot encoder and" id="idm46522844886424"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="one hot encoder" id="idm46522844885480"/>
Most statistical and machine learning models require this type of variable to be converted to a series of binary dummy variables<a data-type="indexterm" data-primary="binary dummy variables" id="idm46522844884024"/><a data-type="indexterm" data-primary="dummy variables" id="idm46522844883352"/> conveying the same information, as in <a data-type="xref" href="#home_ownership_dummy">Table 6-2</a>.
Instead of a single variable denoting the home occupant status as “owns with a mortgage,” “owns with no mortgage,” “rents,” or “other,” we end up with four binary variables.
The first would be “owns with a mortgage—Y/N,” the second would be “owns with no mortgage—Y/N,” and so on.<a data-type="indexterm" data-primary="one hot encoding" id="idm46522844881432"/>
This one predictor, home occupant status, thus yields a vector with one 1 and three 0s that can be used in statistical and machine learning algorithms.
The phrase <em>one hot encoding</em> comes from digital circuit terminology, where it describes circuit settings in which only one bit is allowed to be positive (hot).</p>
<table id="home_ownership_dummy">
<caption><span class="label">Table 6-2. </span>Representing home ownership factor data in <a data-type="xref" href="#loan_data">Table 6-1</a> as a numeric dummy variable</caption>
<thead>
<tr>
<th>OWNS_WITH_MORTGAGE</th>
<th>OWNS_WITHOUT_MORTGAGE</th>
<th>OTHER</th>
<th>RENT</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
<tr>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In linear and logistic regression, one hot encoding
causes problems with <a data-type="indexterm" data-primary="multicollinearity" data-secondary="problems caused by one hot encoding" id="idm46522844857304"/>multicollinearity; see <a data-type="xref" href="ch04.xhtml#Multicollinearity">“Multicollinearity”</a>.  In such cases, one dummy is omitted (its value can be inferred from the other values).<a data-type="indexterm" data-primary="linear regression" data-secondary="multicollinearity problems caused by one hot encoding" id="idm46522844855192"/><a data-type="indexterm" data-primary="logistic regression" data-secondary="multicollinearity problems caused by one hot encoding" id="idm46522844854152"/>
This is not an issue with KNN and other methods discussed in this book.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Standardization (Normalization, z-Scores)"><div class="sect2" id="Standardization">
<h2>Standardization (Normalization, z-Scores)</h2>

<p>In measurement, we are often not so much interested in “how much” but in “how different from the average.”
Standardization, also<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="standardization" id="ix_statMLKNNstd"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="standardization in" id="ix_KNNstd"/><a data-type="indexterm" data-primary="standardization" data-secondary="in K-Nearest Neighbors" id="ix_stdKNN"/> called <em>normalization</em>, puts all variables on similar scales by subtracting the mean and dividing by the standard deviation; in this way, we ensure that a variable does not overly influence a model simply due to the scale of its original measurement:<a data-type="indexterm" data-primary="normalization" data-seealso="standardization" id="idm46522844845736"/><a data-type="indexterm" data-primary="predictor variables" data-secondary="standardization in K-Nearest Neighbors" id="idm46522844844760"/></p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>z</mi>
    <mo>=</mo>
    <mfrac><mrow><mi>x</mi><mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>s</mi></mfrac>
  </mrow>
</math>
</div>

<p>The result of this transformation is commonly referred to as a <em>z-score</em>.
Measurements are then stated <a data-type="indexterm" data-primary="z-scores" data-secondary="in data standardization for KNN" id="idm46522844837448"/>in terms of “standard deviations away from the mean.”</p>
<div data-type="caution"><h6>Caution</h6>
<p><em>Normalization</em> in this statistical context is not to be confused with <em>database normalization</em>, which is the removal of redundant data and the verification of data dependencies.<a data-type="indexterm" data-primary="normalization" data-secondary="in statistics, vs. database normalization" id="idm46522844834440"/><a data-type="indexterm" data-primary="database normalization vs. normalization in statistics" id="idm46522844833368"/></p>
</div>

<p>For KNN and a few other procedures (e.g., principal components analysis and clustering),
it is essential to consider standardizing the data prior to applying the procedure.
To illustrate this idea, KNN is applied to the loan data using <code>dti</code> and <code>payment_inc_ratio</code> (see <a data-type="xref" href="#LoanExampleKNN">“A Small Example: Predicting Loan Default”</a>) plus two other variables: <code>revol_bal</code>, the total revolving credit available to the applicant in dollars, and <code>revol_util</code>, the percent of the credit being used.
The new record to be predicted is shown here:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">newloan</code>
  <code class="n">payment_inc_ratio</code> <code class="n">dti</code> <code class="n">revol_bal</code> <code class="n">revol_util</code>
<code class="m">1</code>            <code class="m">2.3932</code>   <code class="m">1</code>      <code class="m">1687</code>        <code class="m">9.4</code></pre>

<p>The magnitude of <code>revol_bal</code>, which is in dollars, is much bigger than that of the other variables.
The <code>knn</code> function returns the index of the nearest neighbors as an attribute <code>nn.index</code>, and this can be used to show the top-five closest rows in <code>loan_df</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">loan_df</code> <code class="o">&lt;-</code> <code class="nf">model.matrix</code><code class="p">(</code><code class="o">~</code> <code class="m">-1</code> <code class="o">+</code> <code class="n">payment_inc_ratio</code> <code class="o">+</code> <code class="n">dti</code> <code class="o">+</code> <code class="n">revol_bal</code> <code class="o">+</code>
                          <code class="n">revol_util</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">loan_data</code><code class="p">)</code>
<code class="n">newloan</code> <code class="o">&lt;-</code> <code class="n">loan_df</code><code class="p">[</code><code class="m">1</code><code class="p">,</code> <code class="p">,</code> <code class="n">drop</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">]</code>
<code class="n">loan_df</code> <code class="o">&lt;-</code> <code class="n">loan_df</code><code class="p">[</code><code class="m">-1</code><code class="p">,]</code>
<code class="n">outcome</code> <code class="o">&lt;-</code> <code class="n">loan_data</code><code class="p">[</code><code class="m">-1</code><code class="p">,</code> <code class="m">1</code><code class="p">]</code>
<code class="n">knn_pred</code> <code class="o">&lt;-</code> <code class="nf">knn</code><code class="p">(</code><code class="n">train</code><code class="o">=</code><code class="n">loan_df</code><code class="p">,</code> <code class="n">test</code><code class="o">=</code><code class="n">newloan</code><code class="p">,</code> <code class="n">cl</code><code class="o">=</code><code class="n">outcome</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="m">5</code><code class="p">)</code>
<code class="n">loan_df</code><code class="nf">[attr</code><code class="p">(</code><code class="n">knn_pred</code><code class="p">,</code> <code class="s">"nn.index"</code><code class="p">),]</code>

        <code class="n">payment_inc_ratio</code>  <code class="n">dti</code> <code class="n">revol_bal</code> <code class="n">revol_util</code>
<code class="m">35537</code>             <code class="m">1.47212</code> <code class="m">1.46</code>      <code class="m">1686</code>       <code class="m">10.0</code>
<code class="m">33652</code>             <code class="m">3.38178</code> <code class="m">6.37</code>      <code class="m">1688</code>        <code class="m">8.4</code>
<code class="m">25864</code>             <code class="m">2.36303</code> <code class="m">1.39</code>      <code class="m">1691</code>        <code class="m">3.5</code>
<code class="m">42954</code>             <code class="m">1.28160</code> <code class="m">7.14</code>      <code class="m">1684</code>        <code class="m">3.9</code>
<code class="m">43600</code>             <code class="m">4.12244</code> <code class="m">8.98</code>      <code class="m">1684</code>        <code class="m">7.2</code></pre>

<p>Following the model fit, we can use the <code>kneighbors</code> method to identify the five closest rows in the training set with <code>scikit-learn</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="s1">'dti'</code><code class="p">,</code> <code class="s1">'revol_bal'</code><code class="p">,</code> <code class="s1">'revol_util'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">newloan</code> <code class="o">=</code> <code class="n">loan_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">0</code><code class="p">,</code> <code class="n">predictors</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">loan_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">:,</code> <code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">:,</code> <code class="n">outcome</code><code class="p">]</code>

<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">nbrs</code> <code class="o">=</code> <code class="n">knn</code><code class="o">.</code><code class="n">kneighbors</code><code class="p">(</code><code class="n">newloan</code><code class="p">)</code>
<code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">nbrs</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">],</code> <code class="p">:]</code></pre>

<p>The value of <code>revol_bal</code> in these neighbors is very close to its value in the new record, but the other predictor variables are all over the map and essentially play no role in determining neighbors.</p>

<p>Compare this to KNN applied to the standardized data using the <em>R</em> function <code>scale</code>, which computes the <em>z</em>-score for each variable:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">loan_df</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">model.matrix</code><code class="p">(</code><code class="o">~</code><code> </code><code class="m">-1</code><code> </code><code class="o">+</code><code> </code><code class="n">payment_inc_ratio</code><code> </code><code class="o">+</code><code> </code><code class="n">dti</code><code> </code><code class="o">+</code><code> </code><code class="n">revol_bal</code><code> </code><code class="o">+</code><code>
                          </code><code class="n">revol_util</code><code class="p">,</code><code> </code><code class="n">data</code><code class="o">=</code><code class="n">loan_data</code><code class="p">)</code><code>
</code><code class="n">loan_std</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">scale</code><code class="p">(</code><code class="n">loan_df</code><code class="p">)</code><code>
</code><code class="n">newloan_std</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">loan_std</code><code class="p">[</code><code class="m">1</code><code class="p">,</code><code> </code><code class="p">,</code><code> </code><code class="n">drop</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">]</code><code>
</code><code class="n">loan_std</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">loan_std</code><code class="p">[</code><code class="m">-1</code><code class="p">,</code><code class="p">]</code><code>
</code><code class="n">loan_df</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">loan_df</code><code class="p">[</code><code class="m">-1</code><code class="p">,</code><code class="p">]</code><code>  </code><a class="co" id="co_statistical_machine_learning_CO1-1" href="#callout_statistical_machine_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">outcome</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">loan_data</code><code class="p">[</code><code class="m">-1</code><code class="p">,</code><code> </code><code class="m">1</code><code class="p">]</code><code>
</code><code class="n">knn_pred</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">knn</code><code class="p">(</code><code class="n">train</code><code class="o">=</code><code class="n">loan_std</code><code class="p">,</code><code> </code><code class="n">test</code><code class="o">=</code><code class="n">newloan_std</code><code class="p">,</code><code> </code><code class="n">cl</code><code class="o">=</code><code class="n">outcome</code><code class="p">,</code><code> </code><code class="n">k</code><code class="o">=</code><code class="m">5</code><code class="p">)</code><code>
</code><code class="n">loan_df</code><code class="nf">[attr</code><code class="p">(</code><code class="n">knn_pred</code><code class="p">,</code><code> </code><code class="s">"</code><code class="s">nn.index"</code><code class="p">)</code><code class="p">,</code><code class="p">]</code><code>
        </code><code class="n">payment_inc_ratio</code><code>   </code><code class="n">dti</code><code>  </code><code class="n">revol_bal</code><code>  </code><code class="n">revol_util</code><code>
</code><code class="m">2081</code><code>            </code><code class="m">2.61091</code><code>    </code><code class="m">1.03</code><code>       </code><code class="m">1218</code><code>         </code><code class="m">9.7</code><code>
</code><code class="m">1439</code><code>            </code><code class="m">2.34343</code><code>    </code><code class="m">0.51</code><code>        </code><code class="m">278</code><code>         </code><code class="m">9.9</code><code>
</code><code class="m">30216</code><code>           </code><code class="m">2.71200</code><code>    </code><code class="m">1.34</code><code>       </code><code class="m">1075</code><code>         </code><code class="m">8.5</code><code>
</code><code class="m">28543</code><code>           </code><code class="m">2.39760</code><code>    </code><code class="m">0.74</code><code>       </code><code class="m">2917</code><code>         </code><code class="m">7.4</code><code>
</code><code class="m">44738</code><code>           </code><code class="m">2.34309</code><code>    </code><code class="m">1.37</code><code>        </code><code class="m">488</code><code>         </code><code class="m">7.2</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_statistical_machine_learning_CO1-1" href="#co_statistical_machine_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>We need to remove the first row from <code>loan_df</code> as well, so that the row numbers correspond to each other.</p></dd>
</dl>

<p>The <code>sklearn.preprocessing.StandardScaler</code> method is first trained with the predictors and is subsequently used to transform the data set prior to training the KNN model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">newloan</code> <code class="o">=</code> <code class="n">loan_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">0</code><code class="p">,</code> <code class="n">predictors</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">loan_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">:,</code> <code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">:,</code> <code class="n">outcome</code><code class="p">]</code>

<code class="n">scaler</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">scaler</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code> <code class="o">*</code> <code class="mf">1.0</code><code class="p">)</code>

<code class="n">X_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X</code> <code class="o">*</code> <code class="mf">1.0</code><code class="p">)</code>
<code class="n">newloan_std</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">newloan</code> <code class="o">*</code> <code class="mf">1.0</code><code class="p">)</code>

<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_std</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">nbrs</code> <code class="o">=</code> <code class="n">knn</code><code class="o">.</code><code class="n">kneighbors</code><code class="p">(</code><code class="n">newloan_std</code><code class="p">)</code>
<code class="n">X</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">nbrs</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">],</code> <code class="p">:]</code></pre>

<p>The five nearest neighbors are much more alike in all the variables, providing a more sensible result.
Note that the results are displayed on the original scale, but KNN was applied to the scaled data and the new loan to be predicted.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Using the <em>z</em>-score is just one way to rescale variables.
Instead of the mean, a more robust estimate of location could be used, such as the median.<a data-type="indexterm" data-primary="variables" data-secondary="rescaling, methods other than z-scores" id="idm46522844279976"/><a data-type="indexterm" data-primary="rescaling variables, methods other than z-scores" id="idm46522844279032"/>
Likewise, a different estimate of scale such as the interquartile range could be used instead of the standard deviation.
Sometimes, variables are “squashed” into the 0–1 range.
It’s also important to realize that scaling each variable to have unit variance is somewhat arbitrary.
This implies that each variable is thought to have the same importance in predictive power.
If you have subjective knowledge that some variables are more important than others, then these could be scaled up.
For example, with the loan data, it is reasonable to expect that the payment-to-income ratio is very important.</p>
</div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Normalization (standardization) does not change the distributional shape of the data; it does not make it normally shaped if it was not already normally shaped (see <a data-type="xref" href="ch02.xhtml#NormalDist">“Normal Distribution”</a>).<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="standardization" data-startref="ix_statMLKNNstd" id="idm46522844191928"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="standardization in" data-startref="ix_KNNstd" id="idm46522844190440"/><a data-type="indexterm" data-primary="standardization" data-secondary="in K-Nearest Neighbors" data-startref="ix_stdKNN" id="idm46522844189224"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Choosing K"><div class="sect2" id="BiasVarianceTradeoff">
<h2>Choosing K</h2>

<p>The choice of <em>K</em> is very important to the performance of KNN.
The simplest choice is to set <math alttext="upper K equals 1">
  <mrow>
    <mi>K</mi>
    <mo>=</mo>
    <mn>1</mn>
  </mrow>
</math>, known as the 1-nearest neighbor classifier.<a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="choosing K" id="idm46522844183272"/>
The prediction is intuitive: it is based on finding the data record in the training set most similar to the new record to be predicted.
Setting <math alttext="upper K equals 1">
  <mrow>
    <mi>K</mi>
    <mo>=</mo>
    <mn>1</mn>
  </mrow>
</math> is rarely the best choice; you’ll almost always obtain superior performance by using <em>K</em> &gt; 1-nearest neighbors.</p>

<p>Generally speaking, if <em>K</em> is too low, we may be overfitting: including the noise in the data.
Higher values of <em>K</em> provide smoothing  that reduces the risk of overfitting in the training data.
On the other hand, if <em>K</em> is too high, we may oversmooth the data and miss out on KNN’s ability to capture the local structure in the data, one of its main advantages.</p>

<p>The <em>K</em> that best balances between overfitting and oversmoothing is typically determined by accuracy metrics and, in particular, accuracy with holdout or validation data.
There is no general rule about the best <em>K</em>—it depends greatly on the nature of the data.
For highly structured data with little noise, smaller values of <em>K</em> work best.
Borrowing a term from the signal processing community, this type of data is sometimes referred to as having a
high <em>signal-to-noise ratio</em> (<em>SNR</em>).<a data-type="indexterm" data-primary="signal to noise ratio (SNR)" id="idm46522844173816"/>
Examples of data with a typically high SNR are data sets for handwriting and speech recognition.
For noisy data with less structure (data with a low SNR), such as the loan data, larger values of <em>K</em> are appropriate.
Typically, values of <em>K</em> fall in the range  1 to 20.
Often, an odd number is chosen to avoid ties.</p>
<div data-type="note" epub:type="note" id="bvt_note"><h1>Bias-Variance Trade-off</h1>
<p>The tension between oversmoothing and overfitting is an instance of the <em>bias-variance trade-off</em>, a ubiquitous problem<a data-type="indexterm" data-primary="bias-variance trade-off" id="idm46522844169896"/><a data-type="indexterm" data-primary="fitting the model" data-secondary="bias-variance trade-off" id="idm46522844169160"/> in statistical model fitting.<a data-type="indexterm" data-primary="variance" data-secondary="bias-variance trade-off in fitting the model" id="idm46522844168088"/>
Variance refers to the modeling error that occurs because of the choice of training data; that is, if you were to choose a different set of training data, the resulting model would be different.
Bias refers to the modeling error that occurs because you have not properly identified the underlying real-world scenario; this error would not disappear if you simply added more training data.
When a flexible model is overfit, the variance increases.
You can reduce this by using a simpler model, but the bias may increase due to the loss of flexibility in modeling the real underlying situation.<a data-type="indexterm" data-primary="cross validation" id="idm46522844166312"/>
A general approach to handling this trade-off is through <em>cross-validation</em>.
See <a data-type="xref" href="ch04.xhtml#CrossValidation">“Cross-Validation”</a> for more details.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="KNN as a Feature Engine"><div class="sect2" id="KnnFeatureEngine">
<h2>KNN as a Feature Engine</h2>

<p>KNN gained its popularity due to its simplicity and intuitive nature.<a data-type="indexterm" data-primary="features" data-secondary="K-Nearest Neighbors as feature engine" id="ix_featKNN"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="KNN as feature engine" id="ix_statMLKNNfeat"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="as a feature engine" data-secondary-sortas="feature" id="ix_KNNfeat"/>
In terms of performance, KNN by itself is usually not competitive with more sophisticated classification techniques.
In practical model fitting, however, KNN can be used to add “local knowledge” in a staged process with other classification techniques:<a data-type="indexterm" data-primary="fitting the model" data-secondary="K-Nearest Neighbors, advantages of" id="idm46522844157480"/></p>
<ol>
<li>
<p>KNN is run on the data, and for each record, a classification (or quasi-probability of a class) is derived.</p>
</li>
<li>
<p>That result is added as a new feature to the record, and another classification method is then run on the data. The original predictor variables are thus used twice.<a data-type="indexterm" data-primary="predictor variables" data-secondary="used twice in KNN" id="idm46522844154328"/></p>
</li>

</ol>

<p>At first<a data-type="indexterm" data-primary="multicollinearity" data-secondary="and predictors used twice in KNN" id="idm46522844152712"/> you might wonder whether this process, since it uses some predictors twice, causes a problem with multicollinearity (see <a data-type="xref" href="ch04.xhtml#Multicollinearity">“Multicollinearity”</a>).  This is not an issue, since the information being incorporated into the second-stage model is highly local, derived only from a few nearby records, and is therefore additional information and not redundant.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can think of this staged use of KNN as a form of ensemble learning, in which multiple predictive modeling methods are used in conjunction with one another.<a data-type="indexterm" data-primary="predictive modeling" data-secondary="KNN as first stage for" id="idm46522844149240"/><a data-type="indexterm" data-primary="ensemble learning" data-secondary="staged use of K-Nearest Neighbors" id="idm46522844148264"/>  It can also be considered as a form of feature engineering  in which the aim is to derive features (predictor variables) that have predictive power.  Often this involves some manual review of the data; KNN gives a fairly automatic way to do this.</p>
</div>

<p>For example, consider the King County housing data.
In pricing a home for sale, a realtor will base the price on similar homes recently sold, known as “comps.”
In essence, realtors are doing a manual version of KNN:
by looking at the sale prices of similar homes, they can estimate what a home will sell for.
We can create a new feature for a statistical model to mimic the real estate professional by applying KNN to recent sales.
The predicted value is the sales price, and the existing predictor variables could include location, total square feet, type of structure, lot size, and number of bedrooms and bathrooms.
The new predictor variable (feature) that we add via KNN is the KNN predictor for each record (analogous to the realtors’ comps).
Since we are predicting a numerical value, the average of the <em>K</em>-Nearest Neighbors is used instead of a majority vote (known as <em>KNN regression</em>).</p>

<p>Similarly, for the loan data, we can create features that represent different aspects of the loan process.
For example, the following <em>R</em> code would build a feature that represents a borrower’s creditworthiness:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">borrow_df</code> <code class="o">&lt;-</code> <code class="nf">model.matrix</code><code class="p">(</code><code class="o">~</code> <code class="m">-1</code> <code class="o">+</code> <code class="n">dti</code> <code class="o">+</code> <code class="n">revol_bal</code> <code class="o">+</code> <code class="n">revol_util</code> <code class="o">+</code> <code class="n">open_acc</code> <code class="o">+</code>
                            <code class="n">delinq_2yrs_zero</code> <code class="o">+</code> <code class="n">pub_rec_zero</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">loan_data</code><code class="p">)</code>
<code class="n">borrow_knn</code> <code class="o">&lt;-</code> <code class="nf">knn</code><code class="p">(</code><code class="n">borrow_df</code><code class="p">,</code> <code class="n">test</code><code class="o">=</code><code class="n">borrow_df</code><code class="p">,</code> <code class="n">cl</code><code class="o">=</code><code class="n">loan_data</code><code class="p">[,</code> <code class="s">'outcome'</code><code class="p">],</code>
                  <code class="n">prob</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="m">20</code><code class="p">)</code>
<code class="n">prob</code> <code class="o">&lt;-</code> <code class="nf">attr</code><code class="p">(</code><code class="n">borrow_knn</code><code class="p">,</code> <code class="s">"prob"</code><code class="p">)</code>
<code class="n">borrow_feature</code> <code class="o">&lt;-</code> <code class="nf">ifelse</code><code class="p">(</code><code class="n">borrow_knn</code> <code class="o">==</code> <code class="s">'default'</code><code class="p">,</code> <code class="n">prob</code><code class="p">,</code> <code class="m">1</code> <code class="o">-</code> <code class="n">prob</code><code class="p">)</code>
<code class="nf">summary</code><code class="p">(</code><code class="n">borrow_feature</code><code class="p">)</code>
   <code class="n">Min.</code> <code class="m">1</code><code class="n">st</code> <code class="n">Qu.</code>  <code class="n">Median</code>    <code class="n">Mean</code> <code class="m">3</code><code class="n">rd</code> <code class="n">Qu.</code>    <code class="n">Max.</code>
  <code class="m">0.000</code>   <code class="m">0.400</code>   <code class="m">0.500</code>   <code class="m">0.501</code>   <code class="m">0.600</code>   <code class="m">0.950</code></pre>

<p>With <code>scikit-learn</code>, we use the <code>predict_proba</code> method of the trained model to get the probabilities:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'dti'</code><code class="p">,</code> <code class="s1">'revol_bal'</code><code class="p">,</code> <code class="s1">'revol_util'</code><code class="p">,</code> <code class="s1">'open_acc'</code><code class="p">,</code>
              <code class="s1">'delinq_2yrs_zero'</code><code class="p">,</code> <code class="s1">'pub_rec_zero'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>

<code class="n">loan_data</code><code class="p">[</code><code class="s1">'borrower_score'</code><code class="p">]</code> <code class="o">=</code> <code class="n">knn</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
<code class="n">loan_data</code><code class="p">[</code><code class="s1">'borrower_score'</code><code class="p">]</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code></pre>

<p>The result is a feature that predicts the likelihood a borrower will default based on his credit history.<a data-type="indexterm" data-primary="features" data-secondary="K-Nearest Neighbors as feature engine" data-startref="ix_featKNN" id="idm46522844002744"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-tertiary="KNN as feature engine" data-startref="ix_statMLKNNfeat" id="idm46522843889800"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="as a feature engine" data-secondary-sortas="feature" data-startref="ix_KNNfeat" id="idm46522843888376"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522843886760">
<h5>Key Ideas</h5>
<ul>
<li>
<p><em>K</em>-Nearest Neighbors (KNN) classifies a record by assigning it to the class that similar records belong to.</p>
</li>
<li>
<p>Similarity (distance) is determined by Euclidian distance or other related metrics.</p>
</li>
<li>
<p>The number of nearest neighbors to compare a record to, <em>K</em>, is determined by how well the algorithm performs on training data, using different values for <em>K</em>.<a data-type="indexterm" data-primary="K (in K-Nearest Neighbors)" id="idm46522843881512"/></p>
</li>
<li>
<p>Typically, the predictor variables are standardized so that variables of large scale do not dominate the distance metric.<a data-type="indexterm" data-primary="predictive modeling" data-secondary="KNN as first stage for" id="idm46522843879832"/></p>
</li>
<li>
<p>KNN is often used as a first stage in predictive modeling, and the predicted value is added back into the data as a <em>predictor</em> for second-stage (non-KNN) modeling.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="K-Nearest Neighbors" data-startref="ix_statMLKNN" id="idm46522843877480"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-startref="ix_KNN" id="idm46522843876168"/></p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Tree Models"><div class="sect1" id="TreeModels">
<h1>Tree Models</h1>

<p>Tree models, also<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" id="ix_statMLtree"/><a data-type="indexterm" data-primary="tree models" id="ix_tree"/> called <em>Classification and Regression Trees</em> (<em>CART</em>),<sup><a data-type="noteref" id="idm46522843869576-marker" href="ch06.xhtml#idm46522843869576">3</a></sup> <em>decision trees</em>, or just <em>trees</em>, are <a data-type="indexterm" data-primary="decision trees" id="idm46522843867864"/>an effective and popular classification (and regression) method initially developed by Leo Breiman and others in 1984.<a data-type="indexterm" data-primary="Breiman, Leo" id="idm46522843866872"/>
Tree models, and their more powerful descendants <em>random forests</em> and  <em>boosted trees</em> (see <a data-type="xref" href="#Bagging">“Bagging and the Random Forest”</a> and <a data-type="xref" href="#Boosting">“Boosting”</a>), form the basis for the most widely <span class="keep-together">used and powerful</span> predictive modeling tools in data science for regression and <span class="keep-together">classification</span>.<a data-type="indexterm" data-primary="random forests" id="idm46522843862200"/><a data-type="indexterm" data-primary="boosted trees" id="idm46522843861464"/><a data-type="indexterm" data-primary="Classification and Regression Trees (CART)" data-seealso="tree models" id="idm46522843860792"/><a data-type="indexterm" data-primary="partitions in trees" id="idm46522843859880"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522843872776">
<h5>Key Terms for Trees</h5><dl>
<dt class="horizontal"><strong><em>Recursive partitioning</em></strong></dt>
<dd>
<p>Repeatedly dividing and subdividing the data with the goal of making the outcomes in each final subdivision as homogeneous as possible.<a data-type="indexterm" data-primary="recursive partitioning" id="idm46522843856088"/><a data-type="indexterm" data-primary="split value" id="idm46522843855384"/><a data-type="indexterm" data-primary="nodes" id="idm46522843854712"/></p>
</dd>
<dt class="horizontal"><strong><em>Split value</em></strong></dt>
<dd>
<p>A predictor value that divides the records into those where that predictor is less than the split value, and those where it is more.</p>
</dd>
<dt class="horizontal"><strong><em>Node</em></strong></dt>
<dd>
<p>In the decision tree, or in the set of corresponding branching rules, a node is the graphical or rule representation of a split value.</p>
</dd>
<dt class="horizontal"><strong><em>Leaf</em></strong></dt>
<dd>
<p>The end of a set of if-then rules, or branches of a tree—the rules that bring you to that leaf provide one of the classification rules for any record in a tree.<a data-type="indexterm" data-primary="leaf" id="idm46522843848088"/><a data-type="indexterm" data-primary="loss" id="idm46522843847384"/></p>
</dd>
<dt class="horizontal"><strong><em>Loss</em></strong></dt>
<dd>
<p>The number of misclassifications at a stage in the splitting process; the more losses, the more impurity.<a data-type="indexterm" data-primary="impurity" id="idm46522843844776"/></p>
</dd>
<dt class="horizontal"><strong><em>Impurity</em></strong></dt>
<dd>
<p>The extent to which a mix of classes is found in a subpartition of the data (the more mixed, the more impure).</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Heterogeneity</p>
</dd>
<dt><em>Antonyms</em></dt>
<dd>
<p>Homogeneity, purity</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Pruning</em></strong></dt>
<dd>
<p>The process of taking a fully grown tree and progressively cutting its branches back to reduce overfitting.<a data-type="indexterm" data-primary="pruning" id="idm46522843836776"/></p>
</dd>
</dl>
</div></aside>

<p>A tree model is a set of “if-then-else” rules that are easy to understand and to implement.<a data-type="indexterm" data-primary="if-then-else rules (tree models)" id="idm46522843835448"/><a data-type="indexterm" data-primary="tree models" data-secondary="advantages of" id="idm46522843834776"/>
In contrast to linear and logistic regression, trees have the ability to discover hidden patterns corresponding to complex interactions in the data.
However, unlike KNN or naive Bayes, simple tree models can be expressed in terms of predictor relationships that are easily interpretable.</p>
<div data-type="warning" epub:type="warning"><h1>Decision Trees in Operations Research</h1>
<p>The term <em>decision trees</em> has a different (and older) meaning in decision science and operations research, where it refers to a human decision analysis process.<a data-type="indexterm" data-primary="decision trees" data-secondary="older meaning in human decision analysis" id="idm46522843831576"/>  In this meaning, decision points, possible outcomes, and their estimated probabilities are laid out in a branching diagram, and the decision path with the maximum expected value is chosen.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="A Simple Example"><div class="sect2" id="idm46522843829976">
<h2>A Simple Example</h2>

<p>The two main packages to fit <a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-tertiary="simple example" id="idm46522843828056"/><a data-type="indexterm" data-primary="tree models" data-secondary="simple example" id="idm46522843826744"/>tree models in <em>R</em> are <code>rpart</code> and <code>tree</code>.
Using the <code>rpart</code> package, a model is fit to a sample of 3,000 records of the loan data using the variables <code>payment_inc_ratio</code> and <code>borrower_score</code>
(see <a data-type="xref" href="#KNN">“K-Nearest Neighbors”</a> for a description of the data):</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">rpart</code><code class="p">)</code>
<code class="n">loan_tree</code> <code class="o">&lt;-</code> <code class="nf">rpart</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">borrower_score</code> <code class="o">+</code> <code class="n">payment_inc_ratio</code><code class="p">,</code>
                   <code class="n">data</code><code class="o">=</code><code class="n">loan3000</code><code class="p">,</code> <code class="n">control</code><code class="o">=</code><code class="nf">rpart.control</code><code class="p">(</code><code class="n">cp</code><code class="o">=</code><code class="m">0.005</code><code class="p">))</code>
<code class="nf">plot</code><code class="p">(</code><code class="n">loan_tree</code><code class="p">,</code> <code class="n">uniform</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">,</code> <code class="n">margin</code><code class="o">=</code><code class="m">0.05</code><code class="p">)</code>
<code class="nf">text</code><code class="p">(</code><code class="n">loan_tree</code><code class="p">)</code></pre>

<p>The <code>sklearn.tree.DecisionTreeClassifier</code> provides an implementation of a decision tree. The <code>dmba</code> package provides a convenience function to create a visualization inside a Jupyter notebook:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="s1">'payment_inc_ratio'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">loan_tree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">criterion</code><code class="o">=</code><code class="s1">'entropy'</code><code class="p">,</code>
                                   <code class="n">min_impurity_decrease</code><code class="o">=</code><code class="mf">0.003</code><code class="p">)</code>
<code class="n">loan_tree</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">plotDecisionTree</code><code class="p">(</code><code class="n">loan_tree</code><code class="p">,</code> <code class="n">feature_names</code><code class="o">=</code><code class="n">predictors</code><code class="p">,</code>
                 <code class="n">class_names</code><code class="o">=</code><code class="n">loan_tree</code><code class="o">.</code><code class="n">classes_</code><code class="p">)</code></pre>

<p>The resulting tree is shown in <a data-type="xref" href="#LoanTree">Figure 6-3</a>.
Due to the different implementations, you will find that the results from <em>R</em> and <em>Python</em> are not identical; this is expected.
These classification rules are determined by traversing through a hierarchical tree, starting at the root and moving left if the node is true and right if not, until a leaf is reached.</p>

<p>Typically, the tree is plotted upside-down, so the root is at the top and the leaves are at the bottom.
For example, if we get a loan with <code>borrower_score</code> of 0.6 and a <span class="keep-together"><code>payment_inc_ratio</code></span> of 8.0,
we end up at the leftmost leaf and predict the loan will be paid off.<a data-type="indexterm" data-primary="fitting the model" data-secondary="rules for simple tree model fit to loan data" id="idm46522843679864"/></p>

<figure><div id="LoanTree" class="figure">
<img src="Images/psd2_0603.png" alt="The rules for a simple tree model fit to the loan data." width="1076" height="850"/>
<h6><span class="label">Figure 6-3. </span>The rules for a simple tree model fit to the loan data</h6>
</div></figure>

<p>A nicely printed version of the tree is also easily produced in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">loan_tree</code>
<code class="n">n</code><code class="o">=</code> <code class="m">3000</code>

<code class="n">node</code><code class="p">),</code> <code class="n">split</code><code class="p">,</code> <code class="n">n</code><code class="p">,</code> <code class="n">loss</code><code class="p">,</code> <code class="n">yval</code><code class="p">,</code> <code class="p">(</code><code class="n">yprob</code><code class="p">)</code>
    <code class="o">*</code> <code class="n">denotes</code> <code class="n">terminal</code> <code class="n">node</code>

<code class="m">1</code><code class="p">)</code> <code class="n">root</code> <code class="m">3000</code> <code class="m">1445</code> <code class="n">paid</code> <code class="nf">off </code><code class="p">(</code><code class="m">0.5183333</code> <code class="m">0.4816667</code><code class="p">)</code>
  <code class="m">2</code><code class="p">)</code> <code class="n">borrower_score</code><code class="o">&gt;=</code><code class="m">0.575</code> <code class="m">878</code>  <code class="m">261</code> <code class="n">paid</code> <code class="nf">off </code><code class="p">(</code><code class="m">0.7027335</code> <code class="m">0.2972665</code><code class="p">)</code> <code class="o">*</code>
  <code class="m">3</code><code class="p">)</code> <code class="n">borrower_score</code><code class="o">&lt;</code> <code class="m">0.575</code> <code class="m">2122</code>  <code class="m">938</code> <code class="nf">default </code><code class="p">(</code><code class="m">0.4420358</code> <code class="m">0.5579642</code><code class="p">)</code>
    <code class="m">6</code><code class="p">)</code> <code class="n">borrower_score</code><code class="o">&gt;=</code><code class="m">0.375</code> <code class="m">1639</code>  <code class="m">802</code> <code class="nf">default </code><code class="p">(</code><code class="m">0.4893228</code> <code class="m">0.5106772</code><code class="p">)</code>
      <code class="m">12</code><code class="p">)</code> <code class="n">payment_inc_ratio</code><code class="o">&lt;</code> <code class="m">10.42265</code> <code class="m">1157</code>  <code class="m">547</code> <code class="n">paid</code> <code class="nf">off </code><code class="p">(</code><code class="m">0.5272256</code> <code class="m">0.4727744</code><code class="p">)</code>
        <code class="m">24</code><code class="p">)</code> <code class="n">payment_inc_ratio</code><code class="o">&lt;</code> <code class="m">4.42601</code> <code class="m">334</code>  <code class="m">139</code> <code class="n">paid</code> <code class="nf">off </code><code class="p">(</code><code class="m">0.5838323</code> <code class="m">0.4161677</code><code class="p">)</code> <code class="o">*</code>
        <code class="m">25</code><code class="p">)</code> <code class="n">payment_inc_ratio</code><code class="o">&gt;=</code><code class="m">4.42601</code> <code class="m">823</code>  <code class="m">408</code> <code class="n">paid</code> <code class="nf">off </code><code class="p">(</code><code class="m">0.5042527</code> <code class="m">0.4957473</code><code class="p">)</code>
          <code class="m">50</code><code class="p">)</code> <code class="n">borrower_score</code><code class="o">&gt;=</code><code class="m">0.475</code> <code class="m">418</code>  <code class="m">190</code> <code class="n">paid</code> <code class="nf">off </code><code class="p">(</code><code class="m">0.5454545</code> <code class="m">0.4545455</code><code class="p">)</code> <code class="o">*</code>
          <code class="m">51</code><code class="p">)</code> <code class="n">borrower_score</code><code class="o">&lt;</code> <code class="m">0.475</code> <code class="m">405</code>  <code class="m">187</code> <code class="nf">default </code><code class="p">(</code><code class="m">0.4617284</code> <code class="m">0.5382716</code><code class="p">)</code> <code class="o">*</code>
      <code class="m">13</code><code class="p">)</code> <code class="n">payment_inc_ratio</code><code class="o">&gt;=</code><code class="m">10.42265</code> <code class="m">482</code>  <code class="m">192</code> <code class="nf">default </code><code class="p">(</code><code class="m">0.3983402</code> <code class="m">0.6016598</code><code class="p">)</code> <code class="o">*</code>
    <code class="m">7</code><code class="p">)</code> <code class="n">borrower_score</code><code class="o">&lt;</code> <code class="m">0.375</code> <code class="m">483</code>  <code class="m">136</code> <code class="nf">default </code><code class="p">(</code><code class="m">0.2815735</code> <code class="m">0.7184265</code><code class="p">)</code> <code class="o">*</code></pre>

<p>The depth of the tree is shown by the indent.
Each node corresponds to a provisional classification determined by the prevalent outcome in that partition.
The “loss” is the number of misclassifications yielded by the provisional classification in a partition.<a data-type="indexterm" data-primary="loss" data-secondary="in simple tree model example" id="idm46522843673352"/>
For example, in node 2, there were 261 misclassifications out of a total of 878 total records.
The values in the parentheses correspond to the proportion of records that are paid off or in default, respectively.
For example, in node 13, which predicts default,
over 60 percent of the records are loans that are in default.</p>

<p>The <code>scikit-learn</code> documentation describes how to create a text representation of a decision tree model. We included a convenience function in our <code>dmba</code> package:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="n">textDecisionTree</code><code class="p">(</code><code class="n">loan_tree</code><code class="p">))</code>
<code class="o">--</code>
<code class="n">node</code><code class="o">=</code><code class="mi">0</code> <code class="n">test</code> <code class="n">node</code><code class="p">:</code> <code class="n">go</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">1</code> <code class="k">if</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="mf">0.5750000178813934</code> <code class="k">else</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">6</code>
  <code class="n">node</code><code class="o">=</code><code class="mi">1</code> <code class="n">test</code> <code class="n">node</code><code class="p">:</code> <code class="n">go</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">2</code> <code class="k">if</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="mf">0.32500000298023224</code> <code class="k">else</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">3</code>
    <code class="n">node</code><code class="o">=</code><code class="mi">2</code> <code class="n">leaf</code> <code class="n">node</code><code class="p">:</code> <code class="p">[[</code><code class="mf">0.785</code><code class="p">,</code> <code class="mf">0.215</code><code class="p">]]</code>
    <code class="n">node</code><code class="o">=</code><code class="mi">3</code> <code class="n">test</code> <code class="n">node</code><code class="p">:</code> <code class="n">go</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">4</code> <code class="k">if</code> <code class="mi">1</code> <code class="o">&lt;=</code> <code class="mf">10.42264986038208</code> <code class="k">else</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">5</code>
      <code class="n">node</code><code class="o">=</code><code class="mi">4</code> <code class="n">leaf</code> <code class="n">node</code><code class="p">:</code> <code class="p">[[</code><code class="mf">0.488</code><code class="p">,</code> <code class="mf">0.512</code><code class="p">]]</code>
      <code class="n">node</code><code class="o">=</code><code class="mi">5</code> <code class="n">leaf</code> <code class="n">node</code><code class="p">:</code> <code class="p">[[</code><code class="mf">0.613</code><code class="p">,</code> <code class="mf">0.387</code><code class="p">]]</code>
  <code class="n">node</code><code class="o">=</code><code class="mi">6</code> <code class="n">test</code> <code class="n">node</code><code class="p">:</code> <code class="n">go</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">7</code> <code class="k">if</code> <code class="mi">1</code> <code class="o">&lt;=</code> <code class="mf">9.19082498550415</code> <code class="k">else</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">10</code>
    <code class="n">node</code><code class="o">=</code><code class="mi">7</code> <code class="n">test</code> <code class="n">node</code><code class="p">:</code> <code class="n">go</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">8</code> <code class="k">if</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="mf">0.7249999940395355</code> <code class="k">else</code> <code class="n">to</code> <code class="n">node</code> <code class="mi">9</code>
      <code class="n">node</code><code class="o">=</code><code class="mi">8</code> <code class="n">leaf</code> <code class="n">node</code><code class="p">:</code> <code class="p">[[</code><code class="mf">0.247</code><code class="p">,</code> <code class="mf">0.753</code><code class="p">]]</code>
      <code class="n">node</code><code class="o">=</code><code class="mi">9</code> <code class="n">leaf</code> <code class="n">node</code><code class="p">:</code> <code class="p">[[</code><code class="mf">0.073</code><code class="p">,</code> <code class="mf">0.927</code><code class="p">]]</code>
    <code class="n">node</code><code class="o">=</code><code class="mi">10</code> <code class="n">leaf</code> <code class="n">node</code><code class="p">:</code> <code class="p">[[</code><code class="mf">0.457</code><code class="p">,</code> <code class="mf">0.543</code><code class="p">]]</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Recursive Partitioning Algorithm"><div class="sect2" id="RecursivePartitioning">
<h2>The Recursive Partitioning Algorithm</h2>

<p>The algorithm to construct a decision tree, called <em>recursive partitioning</em>, is straightforward and intuitive.<a data-type="indexterm" data-primary="partitions in trees" data-secondary="recursive partitioning algorithm" id="ix_partrec"/><a data-type="indexterm" data-primary="tree models" data-secondary="recursive partitioning algorithm" id="ix_treerecprt"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-tertiary="recursive partitioning" id="ix_statMLtreeRP"/><a data-type="indexterm" data-primary="recursive partitioning" id="ix_recprt"/>
The data is repeatedly partitioned using predictor values that do the best job of separating the data into relatively homogeneous partitions. <a data-type="xref" href="#LoanRecursivePartitioning">Figure 6-4</a> shows the partitions created for the tree in <a data-type="xref" href="#LoanTree">Figure 6-3</a>.
The first rule, depicted by rule 1, is  <code>borrower_score &gt;= 0.575</code> and segments the right portion of the plot.
The second rule is <code>borrower_score &lt; 0.375</code> and segments the left portion.</p>

<figure><div id="LoanRecursivePartitioning" class="figure">
<img src="Images/psd2_0604.png" alt="The first five rules for a simple tree model fit to the loan data." width="1734" height="1169"/>
<h6><span class="label">Figure 6-4. </span>The first three rules for a simple tree model fit to the loan data</h6>
</div></figure>

<p>Suppose we have  a response  variable <em>Y</em> and a set of <em>P</em> predictor variables <em>X<sub>j</sub></em> for <math alttext="j equals 1 comma ellipsis comma upper P">
  <mrow>
    <mi>j</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <mi>P</mi>
  </mrow>
</math>.
For a partition <em>A</em> of records,
recursive partitioning will find the best way to partition <em>A</em> into two subpartitions:</p>
<ol>
<li>
<p>For each predictor variable <em>X<sub>j</sub></em>:</p>
<ol>
<li>
<p>For each value <em>s<sub>j</sub></em> of <em>X<sub>j</sub></em>:</p>
<ol>
<li>
<p>Split the records in <em>A</em> with <em>X<sub>j</sub></em> values &lt; <em>s<sub>j</sub></em> as one partition, and the remaining records where <em>X<sub>j</sub></em> ≥ <em>s<sub>j</sub></em> as another partition.</p>
</li>
<li>
<p>Measure the homogeneity of classes within each subpartition of <em>A</em>.</p>
</li>

</ol>
</li>
<li>
<p>Select the value of <em>s<sub>j</sub></em> that produces maximum within-partition homogeneity of class.</p>
</li>

</ol>
</li>
<li>
<p>Select the variable <em>X<sub>j</sub></em> and the split value <em>s<sub>j</sub></em> that produces maximum within-partition homogeneity of class.</p>
</li>

</ol>

<p class="pagebreak-before">Now comes the recursive part:</p>
<ol>
<li>
<p>Initialize <em>A</em> with the entire data set.</p>
</li>
<li>
<p>Apply the partitioning algorithm to split <em>A</em> into two subpartitions, <em>A<sub>1</sub></em> and <em>A<sub>2</sub></em>.</p>
</li>
<li>
<p>Repeat step 2 on subpartitions <em>A<sub>1</sub></em> and <em>A<sub>2</sub></em>.</p>
</li>
<li>
<p>The algorithm terminates when no further partition can be made that sufficiently improves the homogeneity of the partitions.</p>
</li>

</ol>

<p>The end result is a partitioning of the data, as in <a data-type="xref" href="#LoanRecursivePartitioning">Figure 6-4</a>, except in <em>P</em>-dimensions, with each partition predicting an outcome of 0 or 1 depending on the majority vote of the response in that partition.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In addition to a binary 0/1 prediction,
tree models can produce a probability estimate based on the number of 0s and 1s in the partition.<a data-type="indexterm" data-primary="probability" data-secondary="produced by tree models" id="idm46522843155240"/>
The estimate is simply the sum of 0s or 1s in the partition divided by the number of observations in the partition:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>Prob</mtext>
    <mrow>
      <mo>(</mo>
      <mi>Y</mi>
      <mo>=</mo>
      <mn>1</mn>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfrac><mrow><mtext>Number</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>1s</mtext><mspace width="4.pt"/><mtext>in</mtext><mspace width="4.pt"/><mtext>the</mtext><mspace width="4.pt"/><mtext>partition</mtext></mrow> <mrow><mtext>Size</mtext><mspace width="4.pt"/><mtext>of</mtext><mspace width="4.pt"/><mtext>the</mtext><mspace width="4.pt"/><mtext>partition</mtext></mrow></mfrac>
  </mrow>
</math>
</div>

<p>The estimated <math alttext="Prob left-parenthesis upper Y equals 1 right-parenthesis">
  <mrow>
    <mtext>Prob</mtext>
    <mo>(</mo>
    <mi>Y</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>)</mo>
  </mrow>
</math> can then be converted to a binary decision; for example, set the estimate to 1 if Prob(<em>Y</em> = 1) &gt; 0.5.<a data-type="indexterm" data-primary="partitions in trees" data-secondary="recursive partitioning algorithm" data-startref="ix_partrec" id="idm46522843136520"/><a data-type="indexterm" data-primary="tree models" data-secondary="recursive partitioning algorithm" data-startref="ix_treerecprt" id="idm46522843135224"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-tertiary="recursive partitioning" data-startref="ix_statMLtreeRP" id="idm46522843133992"/><a data-type="indexterm" data-primary="recursive partitioning" data-startref="ix_recprt" id="idm46522843132488"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Measuring Homogeneity or Impurity"><div class="sect2" id="Gini">
<h2>Measuring Homogeneity or Impurity</h2>

<p>Tree models recursively create partitions (sets of records), <em>A</em>, that predict an outcome of <em>Y</em> = 0 or <em>Y</em> = 1.<a data-type="indexterm" data-primary="tree models" data-secondary="measuring homogeneity or impurity" id="idm46522843127976"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-tertiary="measuring homogeneity or impurity" id="idm46522843126904"/><a data-type="indexterm" data-primary="impurity" data-secondary="measuring" id="idm46522843125656"/>
You can see from the preceding algorithm that we need a way to measure homogeneity, also called <em>class purity</em>, within a partition.<a data-type="indexterm" data-primary="class purity" id="idm46522843124088"/><a data-type="indexterm" data-primary="homogeneity, measuring" id="idm46522843123352"/><a data-type="indexterm" data-primary="class purity" id="idm46522843122680"/>
Or equivalently, we need to measure the impurity of a partition.
The accuracy of the predictions is the proportion <em>p</em> of misclassified records within that partition,
which ranges from 0 (perfect) to 0.5 (purely random guessing).</p>

<p>It turns out that accuracy is not a good measure for impurity.
Instead, two common measures for impurity are the <em>Gini impurity</em>  and <em>entropy</em> of <em>information</em>.<a data-type="indexterm" data-primary="Gini impurity" id="idm46522843119288"/><a data-type="indexterm" data-primary="entropy of information" id="idm46522843118552"/>
While these (and other) impurity measures apply to classification problems with more than two classes,
we focus on the binary case.
The Gini impurity for a set of records <em>A</em> is:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>I</mi>
    <mo>(</mo>
    <mi>A</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mi>p</mi>
    <mo>(</mo>
    <mn>1</mn>
    <mo>-</mo>
    <mi>p</mi>
    <mo>)</mo>
  </mrow>
</math>
</div>

<p>The entropy measure is given by:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>I</mi>
    <mrow>
      <mo>(</mo>
      <mi>A</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mo>-</mo>
    <mi>p</mi>
    <msub><mo form="prefix">log</mo> <mn>2</mn> </msub>
    <mrow>
      <mo>(</mo>
      <mi>p</mi>
      <mo>)</mo>
    </mrow>
    <mo>-</mo>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>-</mo>
      <mi>p</mi>
      <mo>)</mo>
    </mrow>
    <msub><mo form="prefix">log</mo> <mn>2</mn> </msub>
    <mrow>
      <mo>(</mo>
      <mn>1</mn>
      <mo>-</mo>
      <mi>p</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p><a data-type="xref" href="#Impurity">Figure 6-5</a> shows that Gini impurity (rescaled) and entropy measures are similar, with entropy giving higher impurity scores for moderate and high accuracy rates.</p>

<figure><div id="Impurity" class="figure">
<img src="Images/psd2_0605.png" alt="Gini impurity and entropy measures." width="1431" height="1158"/>
<h6><span class="label">Figure 6-5. </span>Gini impurity and entropy measures</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h1>Gini Coefficient</h1>
<p>Gini impurity is <a data-type="indexterm" data-primary="Gini coefficient" id="idm46522843089080"/>not to be confused with the <em>Gini coefficient</em>. They represent similar concepts, but the Gini coefficient is limited to the binary classification problem and is related to the AUC metric (see <a data-type="xref" href="ch05.xhtml#AUC">“AUC”</a>).</p>
</div>

<p>The impurity metric is used in the splitting algorithm described earlier.
For each proposed partition of the data, impurity is measured for each of the partitions that result from the split.
A weighted average is then calculated, and whichever partition (at each stage) yields the lowest weighted average is selected.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Stopping the Tree from Growing"><div class="sect2" id="idm46522843130824">
<h2>Stopping the Tree from Growing</h2>

<p>As the tree grows bigger, the splitting rules become more detailed, and the tree gradually shifts from <a data-type="indexterm" data-primary="tree models" data-secondary="stopping tree growth" id="idm46522843084296"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-tertiary="stopping tree growth" id="idm46522843083320"/>identifying “big” rules that identify real and reliable relationships in the data to “tiny” rules that reflect only noise.
A fully grown tree results in completely pure leaves and, hence, 100% accuracy in classifying the data that it is trained on.
This accuracy is, of course, illusory—we have overfit (see <a data-type="xref" class="pagenum" href="#bvt_note">“Bias-Variance Trade-off”</a>) the data, fitting the noise in the training data, not the signal that we want to identify in new data.<a data-type="indexterm" data-primary="pruning" id="idm46522843080248"/></p>

<p>We need some way to determine when to stop growing a tree at a stage that will generalize to new data.
There are various ways to stop splitting in <em>R</em> and <em>Python</em>:</p>

<ul>
<li>
<p>Avoid splitting a partition if a resulting subpartition is too small, or if a terminal leaf is too small. In <code>rpart</code> (<em>R</em>), these constraints are controlled separately by the parameters <code>minsplit</code> and <code>minbucket</code>, respectively, with defaults of <code>20</code> and <code>7</code>. In <em>Python</em>’s <code>DecisionTreeClassifier</code>, we can control this using the parameters <code>min_samples_split</code> (default <code>2</code>) and <code>min_samples_leaf</code> (default <code>1</code>).</p>
</li>
<li>
<p>Don’t split a partition if the new partition does not “significantly” reduce the impurity. In <code>rpart</code>, this is controlled by the  <em>complexity parameter</em> <code>cp</code>, which is a measure of how complex a tree is—the more complex, the greater the value of <code>cp</code>.  In practice, <code>cp</code> is used to limit tree growth by attaching a penalty to additional complexity (splits) in a tree. <code>DecisionTreeClassifier</code> (<em>Python</em>) has the parameter <code>min_impurity_decrease</code>, which limits splitting based on a weighted impurity decrease value. Here, smaller values will lead to more complex trees.</p>
</li>
</ul>

<p>These methods involve arbitrary rules and can be useful for exploratory work, but we can’t easily determine optimum values (i.e., values that maximize predictive accuracy with new data). We need to combine cross-validation with either systematically changing the model parameters or modifying the tree through pruning.</p>










<section data-type="sect3" data-pdf-bookmark="Controlling tree complexity in R"><div class="sect3" id="idm46522843019640">
<h3>Controlling tree complexity in <em>R</em></h3>

<p>With the complexity parameter, <code>cp</code>, we can estimate what size tree will perform best with new data. If <code>cp</code> is too small, then the tree will overfit the data, fitting noise and not signal.
On the other hand, if <code>cp</code> is too large, then the tree will be too small and have little predictive power.
The default in <code>rpart</code> is 0.01, although for larger data sets, you are likely to find this is too large.
In the previous example, <code>cp</code> was set to <code>0.005</code> since the default led to a tree with a single split.  In exploratory analysis, it is sufficient to simply try a few values.</p>

<p class="pagebreak-before">Determining the optimum <code>cp</code> is an instance of the bias-variance trade-off.
The most common way to estimate a good value of <code>cp</code> is via cross-validation (see <a data-type="xref" href="ch04.xhtml#CrossValidation">“Cross-Validation”</a>):</p>
<ol>
<li>
<p>Partition the data into training and validation (holdout) sets.</p>
</li>
<li>
<p>Grow the tree with the training data.</p>
</li>
<li>
<p>Prune it successively, step by step, recording <code>cp</code> (using the <em>training</em> data) at each step.</p>
</li>
<li>
<p>Note the <code>cp</code> that corresponds to the minimum error (loss) on the <em>validation</em>  data.</p>
</li>
<li>
<p>Repartition the data into training and validation, and repeat the growing, pruning, and <code>cp</code> recording process.</p>
</li>
<li>
<p>Do this again and again, and average the <code>cp</code>s that reflect minimum error for each tree.</p>
</li>
<li>
<p>Go back to the original data, or future data, and grow a tree, stopping at this optimum <code>cp</code> value.</p>
</li>

</ol>

<p>In <code>rpart</code>, you can use the argument <code>cptable</code> to produce a table of the <code>cp</code> values and their associated cross-validation error (<code>xerror</code> in <em>R</em>), from which you can determine the <code>cp</code> value that has the lowest cross-validation error.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Controlling tree complexity in Python"><div class="sect3" id="idm46522842998680">
<h3>Controlling tree complexity in <em>Python</em></h3>

<p>Neither the complexity parameter nor pruning is available in <code>scikit-learn</code>’s decision tree implementation. The solution is to use grid search over combinations of different parameter values. For example, we can vary <code>max_depth</code> in the range 5 to 30 and <code>min_samples_split</code> between 20 and 100. The <code>GridSearchCV</code> method in <code>scikit-learn</code> is a convenient way to combine the exhaustive search through all combinations with cross-validation. An optimal parameter set is then selected using the cross-validated model performance.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Predicting a Continuous Value"><div class="sect2" id="idm46522842994184">
<h2>Predicting a Continuous Value</h2>

<p>Predicting a continuous value (also termed <em>regression</em>) with<a data-type="indexterm" data-primary="regression" data-secondary="with a tree" data-secondary-sortas="tree" id="idm46522842992280"/><a data-type="indexterm" data-primary="tree models" data-secondary="predicting a continuous value" id="idm46522842991000"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-tertiary="predicting a continuous value" id="idm46522842990088"/><a data-type="indexterm" data-primary="prediction" data-secondary="predicting a continuous value with tree model" id="idm46522842988840"/> a tree follows the same logic and procedure, except that impurity is measured by squared deviations<a data-type="indexterm" data-primary="impurity" data-secondary="measuring" id="idm46522842987560"/> from the mean (squared errors) in each subpartition, and predictive performance is judged by the square root of the mean squared error (RMSE) (see <a data-type="xref" href="ch04.xhtml#RMSE">“Assessing the Model”</a>) in each partition.<a data-type="indexterm" data-primary="root mean squared error (RMSE)" id="idm46522842985528"/></p>

<p><code>scikit-learn</code> has the <code>sklearn.tree.DecisionTreeRegressor</code> method to train a decision tree regression model.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="How Trees Are Used"><div class="sect2" id="idm46522842983560">
<h2>How Trees Are Used</h2>

<p>One of the big obstacles faced by predictive modelers in organizations is the perceived “black box” nature of the methods they use, which gives rise to opposition from other elements of the organization.<a data-type="indexterm" data-primary="tree models" data-secondary="how trees are used" id="idm46522842981800"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-tertiary="how trees are used" id="idm46522842980824"/>
In this regard, the tree model has two  appealing aspects:</p>

<ul>
<li>
<p>Tree models provide a visual tool for exploring the data, to gain an idea of what variables are important and how they relate to one another.  Trees can capture nonlinear relationships among predictor variables.<a data-type="indexterm" data-primary="predictor variables" data-secondary="nonlinear relationships among, captured by trees" id="idm46522842978136"/></p>
</li>
<li>
<p>Tree models provide a set of rules that can be effectively communicated to nonspecialists, either for implementation or to “sell” a data mining project.</p>
</li>
</ul>

<p>When it comes to prediction, however, harnessing the results from multiple trees is typically more powerful than using just a single tree.<a data-type="indexterm" data-primary="prediction" data-secondary="harnessing results from multiple trees" id="idm46522842975288"/>
In particular, the random forest and boosted tree algorithms almost always provide superior predictive accuracy and performance (see <a data-type="xref" href="#Bagging">“Bagging and the Random Forest”</a> and <a data-type="xref" href="#Boosting">“Boosting”</a>), but the aforementioned advantages of a single tree are lost.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522842972264">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Decision trees produce a set of rules to classify or predict an outcome.</p>
</li>
<li>
<p>The rules correspond to successive partitioning of the data into subpartitions.<a data-type="indexterm" data-primary="partitions in trees" id="idm46522842969160"/></p>
</li>
<li>
<p>Each partition, or split, references a specific value of a predictor variable and divides the data into records where that predictor value is above or below that split value.<a data-type="indexterm" data-primary="split value" id="idm46522842967432"/></p>
</li>
<li>
<p>At each stage, the tree algorithm chooses the split that minimizes the outcome impurity within each subpartition.</p>
</li>
<li>
<p>When no further splits can be made, the tree is fully grown and each terminal node, or leaf, has records of a single class; new cases following that rule (split) path would be assigned that class.</p>
</li>
<li>
<p>A fully grown tree overfits the data and must be pruned back so that it captures signal and not noise.</p>
</li>
<li>
<p>Multiple-tree algorithms like random forests and boosted trees yield better predictive performance, but they lose the rule-based communicative power of single trees.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522842962296">
<h2>Further Reading</h2>

<ul>
<li>
<p>Analytics Vidhya Content Team, <a href="https://oreil.ly/zOr4B">“Tree Based Algorithms: A Complete Tutorial from Scratch (in <em>R</em> &amp; <em>Python</em>)”</a>, April 12, 2016.</p>
</li>
<li>
<p>Terry M. Therneau, Elizabeth J. Atkinson, and the Mayo Foundation, <a href="https://oreil.ly/6rLGk">“An Introduction to Recursive Partitioning Using the RPART Routines”</a>, April 11, 2019.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="tree models" data-startref="ix_statMLtree" id="idm46522842957032"/><a data-type="indexterm" data-primary="tree models" data-startref="ix_tree" id="idm46522842955768"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Bagging and the Random Forest"><div class="sect1" id="Bagging">
<h1>Bagging and the Random Forest</h1>

<p>In 1906, the statistician Sir Francis Galton was visiting a county fair in England, at which a contest was being held to guess the dressed weight of an ox that was on exhibit.<a data-type="indexterm" data-primary="Galton, Francis" id="idm46522842952312"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" id="ix_statMLbagRF"/>
There were 800 guesses, and while the individual guesses varied widely, both the mean and the median came out within 1% of the ox’s true weight.
James Surowiecki has explored this phenomenon in his book <em>The Wisdom of Crowds</em> (Doubleday, 2004).
This principle applies to predictive models as well:  averaging (or taking majority votes) of multiple models—an <em>ensemble</em> of models—turns out to be more accurate than just selecting one model.<a data-type="indexterm" data-primary="ensemble of models" id="idm46522842949016"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522842948184">
<h5>Key Terms for Bagging and the Random Forest</h5><dl>
<dt class="horizontal"><strong><em>Ensemble</em></strong></dt>
<dd>
<p>Forming a prediction by using a collection of models.</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Model averaging</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Bagging</em></strong></dt>
<dd>
<p>A general technique to form a collection of models by bootstrapping the data.</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Bootstrap aggregation</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Random forest</em></strong></dt>
<dd>
<p>A type of bagged estimate based on decision tree models.</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Bagged decision trees<a data-type="indexterm" data-primary="random forests" id="idm46522842936088"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Variable importance</em></strong></dt>
<dd>
<p>A measure of the importance of a predictor variable in the performance of the model.<a data-type="indexterm" data-primary="variable importance" id="idm46522842933288"/></p>
</dd>
</dl>
</div></aside>

<p>The ensemble approach has been applied to and across many different modeling methods, most publicly in the Netflix Prize, in which Netflix offered a $1 million prize to any contestant who came up with a model that produced a 10% improvement in predicting the rating that a Netflix customer would award a movie.  The simple version of ensembles is as follows:</p>
<ol>
<li>
<p>Develop a predictive model and record the predictions for a given data set.</p>
</li>
<li>
<p>Repeat for multiple models on the same data.</p>
</li>
<li>
<p>For each record to be predicted, take an average (or a weighted average, or a majority vote) of the predictions.</p>
</li>

</ol>

<p>Ensemble methods have been applied most systematically and effectively to decision trees.
Ensemble tree models are so powerful that they provide a way to build good predictive models with relatively little effort.</p>

<p>Going beyond the simple ensemble algorithm, there are two main variants of ensemble models: <em>bagging</em> and <em>boosting</em>.<a data-type="indexterm" data-primary="ensemble of models" data-secondary="bagging and boosting" id="idm46522842926088"/>
In the case of ensemble tree models, these are referred to as <em>random forest</em> models and <em>boosted tree</em> models.<a data-type="indexterm" data-primary="tree models" data-secondary="ensemble, random forest and boosted trees" id="idm46522842924040"/><a data-type="indexterm" data-primary="boosted trees" id="idm46522842922936"/>
This section focuses on bagging; boosting is covered in <a data-type="xref" href="#Boosting">“Boosting”</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Bagging"><div class="sect2" id="BaggingAlgo">
<h2>Bagging</h2>

<p>Bagging, which stands for “bootstrap aggregating,” was introduced by Leo Breiman in 1994.<a data-type="indexterm" data-primary="bagging" id="idm46522842919080"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" data-tertiary="bagging" id="idm46522842918376"/><a data-type="indexterm" data-primary="bootstrap aggregating" data-see="bagging" id="idm46522842917176"/>
Suppose we have a response <em>Y</em> and <em>P</em> predictor variables <math alttext="bold upper X equals upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript upper P Baseline">
  <mrow>
    <mi>𝐗</mi>
    <mo>=</mo>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>X</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>P</mi> </msub>
  </mrow>
</math> with <em>N</em> records.</p>

<p>Bagging is like the basic algorithm for ensembles, except that, instead of fitting the various models to the same data, each new model is fitted to a bootstrap resample.
Here is the algorithm presented more formally:</p>
<ol>
<li>
<p>Initialize <em>M</em>,  the number of models to be fit, and <em>n</em>, the number of records to choose (<em>n</em> &lt; <em>N</em>). Set the iteration <math alttext="m equals 1">
  <mrow>
    <mi>m</mi>
    <mo>=</mo>
    <mn>1</mn>
  </mrow>
</math>.</p>
</li>
<li>
<p>Take a bootstrap resample (i.e., with replacement) of <em>n</em> records from the training data to form a subsample  <math alttext="upper Y Subscript m">
  <msub><mi>Y</mi> <mi>m</mi> </msub>
</math> and <math alttext="bold upper X Subscript m">
  <msub><mi>𝐗</mi> <mi>m</mi> </msub>
</math> (the bag).</p>
</li>
<li>
<p>Train a model using <math alttext="upper Y Subscript m">
  <msub><mi>Y</mi> <mi>m</mi> </msub>
</math> and <math alttext="bold upper X Subscript m">
  <msub><mi>𝐗</mi> <mi>m</mi> </msub>
</math> to create a set of decision rules <math alttext="ModifyingAbove f With caret Subscript m Baseline left-parenthesis bold upper X right-parenthesis">
  <mrow>
    <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi> </msub>
    <mrow>
      <mo>(</mo>
      <mi>𝐗</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>.</p>
</li>
<li>
<p>Increment the model counter <math alttext="m equals m plus 1">
  <mrow>
    <mi>m</mi>
    <mo>=</mo>
    <mi>m</mi>
    <mo>+</mo>
    <mn>1</mn>
  </mrow>
</math>. If <em>m</em> &lt;= <em>M</em>, go to step 2.</p>
</li>

</ol>

<p>In the case where <math alttext="ModifyingAbove f With caret Subscript m">
  <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi> </msub>
</math> predicts the probability <math alttext="upper Y equals 1">
  <mrow>
    <mi>Y</mi>
    <mo>=</mo>
    <mn>1</mn>
  </mrow>
</math>, the bagged estimate is given by:</p>
<div data-type="equation">
<math alttext="ModifyingAbove f With caret equals StartFraction 1 Over upper M EndFraction left-parenthesis ModifyingAbove f With caret Subscript 1 Baseline left-parenthesis bold upper X right-parenthesis plus ModifyingAbove f With caret Subscript 2 Baseline left-parenthesis bold upper X right-parenthesis plus ellipsis plus ModifyingAbove f With caret Subscript upper M Baseline left-parenthesis bold upper X right-parenthesis right-parenthesis" display="block">
  <mrow>
    <mover accent="true"><mi>f</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <mfrac><mn>1</mn> <mi>M</mi></mfrac>
    <mfenced separators="" open="(" close=")">
      <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
      <mrow>
        <mo>(</mo>
        <mi>𝐗</mi>
        <mo>)</mo>
      </mrow>
      <mo>+</mo>
      <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>2</mn> </msub>
      <mrow>
        <mo>(</mo>
        <mi>𝐗</mi>
        <mo>)</mo>
      </mrow>
      <mo>+</mo>
      <mo>⋯</mo>
      <mo>+</mo>
      <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>M</mi> </msub>
      <mrow>
        <mo>(</mo>
        <mi>𝐗</mi>
        <mo>)</mo>
      </mrow>
    </mfenced>
  </mrow>
</math>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Random Forest"><div class="sect2" id="RandomForest">
<h2>Random Forest</h2>

<p>The <em>random forest</em> is <a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" data-tertiary="random forests" id="ix_statMLbafRFRF"/><a data-type="indexterm" data-primary="random forests" id="ix_randfor"/>based on applying bagging to decision trees, with one important extension: in addition to sampling the records, the algorithm also samples the variables.<sup><a data-type="noteref" id="idm46522842847944-marker" href="ch06.xhtml#idm46522842847944">4</a></sup>
In traditional decision trees, to determine how to create a subpartition of a partition <em>A</em>, the algorithm makes the choice of variable and split point by minimizing a criterion such as Gini impurity (see <a data-type="xref" href="#Gini">“Measuring Homogeneity or Impurity”</a>).
With random forests, at each stage of the algorithm, the choice of variable is limited to a <em>random subset of variables</em>.<a data-type="indexterm" data-primary="random subset of variables (in random forest)" id="idm46522842844648"/><a data-type="indexterm" data-primary="partitions in trees" data-secondary="random forests" id="idm46522842843848"/>
Compared to the basic tree algorithm (see <a data-type="xref" href="#RecursivePartitioning">“The Recursive Partitioning Algorithm”</a>), the random forest algorithm adds two more steps: the bagging discussed earlier (see <a data-type="xref" href="#Bagging">“Bagging and the Random Forest”</a>), and the bootstrap sampling of variables at each split:</p>
<ol>
<li>
<p>Take a <a data-type="indexterm" data-primary="bootstrap" data-secondary="sampling of variables in random forest partitioning" id="idm46522842840184"/>bootstrap (with replacement) subsample from the <em>records</em>.</p>
</li>
<li>
<p>For the first split, sample <em>p</em> &lt; <em>P</em> <em>variables</em> at random without replacement.</p>
</li>
<li>
<p>For each of the sampled variables <math alttext="upper X Subscript j left-parenthesis 1 right-parenthesis Baseline comma upper X Subscript j left-parenthesis 2 right-parenthesis Baseline comma ellipsis comma upper X Subscript j left-parenthesis p right-parenthesis Baseline">
  <mrow>
    <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>1</mn><mo>)</mo></mrow> </msub>
    <mo>,</mo>
    <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>2</mn><mo>)</mo></mrow> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>p</mi><mo>)</mo></mrow> </msub>
  </mrow>
</math>, apply the splitting <span class="keep-together">algorithm</span>:</p>
<ol>
<li>
<p>For each value <math alttext="s Subscript j left-parenthesis k right-parenthesis">
  <msub><mi>s</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow> </msub>
</math> of <math alttext="upper X Subscript j left-parenthesis k right-parenthesis">
  <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow> </msub>
</math>:</p>
<ol>
<li>
<p>Split the records in partition <em>A</em>, with <em>X</em><sub><em>j</em>(<em>k</em>)</sub> &lt; <em>s</em><sub><em>j</em>(<em>k</em>)</sub> as one partition and the remaining records where <math alttext="upper X Subscript j left-parenthesis k right-parenthesis Baseline greater-than-or-equal-to s Subscript j left-parenthesis k right-parenthesis">
  <mrow>
    <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow> </msub>
    <mo>≥</mo>
    <msub><mi>s</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow> </msub>
  </mrow>
</math> as another partition.</p>
</li>
<li>
<p>Measure the homogeneity of classes within each subpartition of <em>A</em>.</p>
</li>

</ol>
</li>
<li>
<p>Select the value of <math alttext="s Subscript j left-parenthesis k right-parenthesis">
  <msub><mi>s</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow> </msub>
</math> that produces maximum within-partition homogeneity of class.</p>
</li>

</ol>
</li>
<li>
<p>Select the variable <math alttext="upper X Subscript j left-parenthesis k right-parenthesis">
  <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow> </msub>
</math> and the split value <math alttext="s Subscript j left-parenthesis k right-parenthesis">
  <msub><mi>s</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow> </msub>
</math> that produces maximum within-partition homogeneity of class.</p>
</li>
<li>
<p>Proceed to the next split and repeat the previous steps, starting with step 2.</p>
</li>
<li>
<p>Continue with additional splits, following the same procedure until the tree is grown.</p>
</li>
<li>
<p>Go back to step 1, take another bootstrap subsample, and start the process over again.</p>
</li>

</ol>

<p>How many variables to sample at each step? A rule of thumb is to choose <math alttext="StartRoot upper P EndRoot">
  <msqrt>
    <mi>P</mi>
  </msqrt>
</math> where <em>P</em> is the  number of predictor variables.
The package <code>randomForest</code> implements the random forest in <em>R</em>.
The following applies this package to the loan data (see <a data-type="xref" href="#KNN">“K-Nearest Neighbors”</a> for a description of the data):</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">rf</code> <code class="o">&lt;-</code> <code class="nf">randomForest</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">borrower_score</code> <code class="o">+</code> <code class="n">payment_inc_ratio</code><code class="p">,</code>
                   <code class="n">data</code><code class="o">=</code><code class="n">loan3000</code><code class="p">)</code>
<code class="n">rf</code>

<code class="n">Call</code><code class="o">:</code>
 <code class="nf">randomForest</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">outcome</code> <code class="o">~</code> <code class="n">borrower_score</code> <code class="o">+</code> <code class="n">payment_inc_ratio</code><code class="p">,</code>
     <code class="n">data</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">)</code>
           	<code class="n">Type</code> <code class="n">of</code> <code class="n">random</code> <code class="n">forest</code><code class="o">:</code> <code class="n">classification</code>
                     <code class="n">Number</code> <code class="n">of</code> <code class="n">trees</code><code class="o">:</code> <code class="m">500</code>
<code class="n">No.</code> <code class="n">of</code> <code class="n">variables</code> <code class="n">tried</code> <code class="n">at</code> <code class="n">each</code> <code class="n">split</code><code class="o">:</code> <code class="m">1</code>

    	<code class="n">OOB</code> <code class="n">estimate</code> <code class="n">of</code> <code class="n">error</code> <code class="n">rate</code><code class="o">:</code> <code class="m">39.17</code>%
<code class="n">Confusion</code> <code class="n">matrix</code><code class="o">:</code>
        <code class="n">default</code>  <code class="n">paid</code> <code class="n">off</code>  <code class="n">class.error</code>
<code class="n">default</code>     <code class="m">873</code>       <code class="m">572</code>   <code class="m">0.39584775</code>
<code class="n">paid</code> <code class="n">off</code>    <code class="m">603</code>       <code class="m">952</code>   <code class="m">0.38778135</code></pre>

<p>In <em>Python</em>, we use the method <code>sklearn.ensemble.RandomForestClassifier</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="s1">'payment_inc_ratio'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">rf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">oob_score</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">rf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>By default, 500 trees are trained.
Since there are only two variables in the predictor set, the algorithm randomly selects the variable on which to split at each stage (i.e., a bootstrap subsample of size 1).</p>

<p>The <em>out-of-bag</em> (<em>OOB</em>) estimate of error<a data-type="indexterm" data-primary="out-of-bag estimate of error" id="idm46522842588664"/> is the error rate for the trained models, applied to the data left out of the training set for that tree.
Using the output from the model,
the OOB error can be plotted versus the number of trees in the random forest in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">error_df</code> <code class="o">=</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">error_rate</code><code class="o">=</code><code class="n">rf</code><code class="o">$</code><code class="n">err.rate</code><code class="p">[,</code><code class="s">'OOB'</code><code class="p">],</code>
                      <code class="n">num_trees</code><code class="o">=</code><code class="m">1</code><code class="o">:</code><code class="n">rf</code><code class="o">$</code><code class="n">ntree</code><code class="p">)</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">error_df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">num_trees</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">error_rate</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_line</code><code class="p">()</code></pre>

<p>The <code>RandomForestClassifier</code> implementation has no easy way to get out-of-bag estimates as a function of number of trees in the random forest. We can train a sequence of classifiers with an increasing number of trees and keep track of the <code>oob_score_</code> values. This method is, however, not efficient:</p>

<pre data-type="programlisting" data-code-language="python" class="pagebreak-before"><code class="n">n_estimator</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">510</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">oobScores</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">n_estimator</code><code class="p">:</code>
    <code class="n">rf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="n">n</code><code class="p">,</code> <code class="n">criterion</code><code class="o">=</code><code class="s1">'entropy'</code><code class="p">,</code>
                                <code class="n">max_depth</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">oob_score</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
    <code class="n">rf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
    <code class="n">oobScores</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">rf</code><code class="o">.</code><code class="n">oob_score_</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code> <code class="s1">'n'</code><code class="p">:</code> <code class="n">n_estimator</code><code class="p">,</code> <code class="s1">'oobScore'</code><code class="p">:</code> <code class="n">oobScores</code> <code class="p">})</code>
<code class="n">df</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'n'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'oobScore'</code><code class="p">)</code></pre>

<p>The result is shown in <a data-type="xref" href="#RFAccuracy">Figure 6-6</a>.
The error rate rapidly decreases from over 0.44 before stabilizing around 0.385.
The predicted values can be obtained from the <code>predict</code> function and plotted as follows in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">rf</code><code class="p">,</code> <code class="n">prob</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">)</code>
<code class="n">rf_df</code> <code class="o">&lt;-</code> <code class="nf">cbind</code><code class="p">(</code><code class="n">loan3000</code><code class="p">,</code> <code class="n">pred</code> <code class="o">=</code> <code class="n">pred</code><code class="p">)</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">rf_df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">borrower_score</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">payment_inc_ratio</code><code class="p">,</code>
                       <code class="n">shape</code><code class="o">=</code><code class="n">pred</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">pred</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">pred</code><code class="p">))</code> <code class="o">+</code>
    <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="m">.8</code><code class="p">)</code> <code class="o">+</code>
    <code class="nf">scale_color_manual</code><code class="p">(</code><code class="n">values</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="s">'paid off'</code><code class="o">=</code><code class="s">'#b8e186'</code><code class="p">,</code> <code class="s">'default'</code><code class="o">=</code><code class="s">'#d95f02'</code><code class="p">))</code> <code class="o">+</code>
    <code class="nf">scale_shape_manual</code><code class="p">(</code><code class="n">values</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="s">'paid off'</code><code class="o">=</code><code class="m">0</code><code class="p">,</code> <code class="s">'default'</code><code class="o">=</code><code class="m">1</code><code class="p">))</code> <code class="o">+</code>
    <code class="nf">scale_size_manual</code><code class="p">(</code><code class="n">values</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="s">'paid off'</code><code class="o">=</code><code class="m">0.5</code><code class="p">,</code> <code class="s">'default'</code><code class="o">=</code><code class="m">2</code><code class="p">))</code></pre>

<p>In <em>Python</em>, we can create a similar plot as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictions</code> <code class="o">=</code> <code class="n">X</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
<code class="n">predictions</code><code class="p">[</code><code class="s1">'prediction'</code><code class="p">]</code> <code class="o">=</code> <code class="n">rf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">predictions</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>

<code class="n">predictions</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">predictions</code><code class="o">.</code><code class="n">prediction</code><code class="o">==</code><code class="s1">'paid off'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code>
    <code class="n">x</code><code class="o">=</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s1">'.'</code><code class="p">,</code>
    <code class="n">markerfacecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code> <code class="n">markeredgecolor</code><code class="o">=</code><code class="s1">'C1'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">predictions</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">predictions</code><code class="o">.</code><code class="n">prediction</code><code class="o">==</code><code class="s1">'default'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code>
    <code class="n">x</code><code class="o">=</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s1">'o'</code><code class="p">,</code>
    <code class="n">markerfacecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code> <code class="n">markeredgecolor</code><code class="o">=</code><code class="s1">'C0'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s1">'paid off'</code><code class="p">,</code> <code class="s1">'default'</code><code class="p">]);</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'borrower_score'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'payment_inc_ratio'</code><code class="p">)</code></pre>

<figure><div id="RFAccuracy" class="figure">
<img src="Images/psd2_0606.png" alt="The improvement in accuracy of the random forest with the addition of more trees." width="1440" height="1143"/>
<h6><span class="label">Figure 6-6. </span>An example of the improvement in accuracy of the random forest with the addition of more trees</h6>
</div></figure>

<p>The plot, shown in <a data-type="xref" href="#LoanRF">Figure 6-7</a>, is quite revealing about the nature of the random <span class="keep-together">forest</span>.</p>

<p>The random forest method is a “black box” method.  It produces more accurate predictions than a simple tree, but the simple tree’s intuitive decision rules are lost.<a data-type="indexterm" data-primary="prediction" data-secondary="from random forests" id="idm46522842075672"/>
The random forest predictions are also somewhat noisy: note that some borrowers with a very high score, indicating high creditworthiness, still end up with a prediction of default.
This is a result of some unusual records in the data and demonstrates the danger of overfitting by the random forest (see <a data-type="xref" class="pagenum" href="#bvt_note">“Bias-Variance Trade-off”</a>).<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" data-tertiary="random forests" data-startref="ix_statMLbafRFRF" id="idm46522842073048"/><a data-type="indexterm" data-primary="random forests" data-startref="ix_randfor" id="idm46522842071560"/></p>

<figure><div id="LoanRF" class="figure">
<img src="Images/psd2_0607.png" alt="The predicted outcomes from the random forest applied to the loan default data." width="1434" height="1169"/>
<h6><span class="label">Figure 6-7. </span>The predicted outcomes from the random forest applied to the loan default data</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Variable Importance"><div class="sect2" id="idm46522842852824">
<h2>Variable Importance</h2>

<p>The power of the random forest algorithm shows itself when you build predictive models for data with many features and records.<a data-type="indexterm" data-primary="random forests" data-secondary="variable importance in" id="ix_randforVI"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" data-tertiary="variable importance" id="ix_statMLbagRFVI"/><a data-type="indexterm" data-primary="variable importance" id="ix_varimp"/><a data-type="indexterm" data-primary="interactions" id="idm46522842063368"/>
It has the ability to automatically determine which predictors are important and discover complex relationships between predictors corresponding to interaction terms (see <a data-type="xref" href="ch04.xhtml#Interactions">“Interactions and Main Effects”</a>).
For example, fit a model to the loan default data<a data-type="indexterm" data-primary="fitting the model" data-secondary="random forest fit to loan default data" id="idm46522842061512"/> with all columns included. The following shows this in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">rf_all</code> <code class="o">&lt;-</code> <code class="nf">randomForest</code><code class="p">(</code><code class="n">outcome</code> <code class="o">~</code> <code class="n">.,</code> <code class="n">data</code><code class="o">=</code><code class="n">loan_data</code><code class="p">,</code> <code class="n">importance</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">)</code>
<code class="n">rf_all</code>
<code class="n">Call</code><code class="o">:</code>
 <code class="nf">randomForest</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">outcome</code> <code class="o">~</code> <code class="n">.,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">,</code> <code class="n">importance</code> <code class="o">=</code> <code class="kc">TRUE</code><code class="p">)</code>
               <code class="n">Type</code> <code class="n">of</code> <code class="n">random</code> <code class="n">forest</code><code class="o">:</code> <code class="n">classification</code>
                     <code class="n">Number</code> <code class="n">of</code> <code class="n">trees</code><code class="o">:</code> <code class="m">500</code>
<code class="n">No.</code> <code class="n">of</code> <code class="n">variables</code> <code class="n">tried</code> <code class="n">at</code> <code class="n">each</code> <code class="n">split</code><code class="o">:</code> <code class="m">4</code>

        <code class="n">OOB</code> <code class="n">estimate</code> <code class="n">of</code>  <code class="n">error</code> <code class="n">rate</code><code class="o">:</code> <code class="m">33.79</code>%

<code class="n">Confusion</code> <code class="n">matrix</code><code class="o">:</code>
         <code class="n">paid</code> <code class="n">off</code> <code class="n">default</code> <code class="n">class.error</code>
<code class="n">paid</code> <code class="n">off</code>    <code class="m">14676</code>    <code class="m">7995</code>   <code class="m">0.3526532</code>
<code class="n">default</code>      <code class="m">7325</code>   <code class="m">15346</code>   <code class="m">0.3231000</code></pre>

<p>And in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'loan_amnt'</code><code class="p">,</code> <code class="s1">'term'</code><code class="p">,</code> <code class="s1">'annual_inc'</code><code class="p">,</code> <code class="s1">'dti'</code><code class="p">,</code> <code class="s1">'payment_inc_ratio'</code><code class="p">,</code>
              <code class="s1">'revol_bal'</code><code class="p">,</code> <code class="s1">'revol_util'</code><code class="p">,</code> <code class="s1">'purpose'</code><code class="p">,</code> <code class="s1">'delinq_2yrs_zero'</code><code class="p">,</code>
              <code class="s1">'pub_rec_zero'</code><code class="p">,</code> <code class="s1">'open_acc'</code><code class="p">,</code> <code class="s1">'grade'</code><code class="p">,</code> <code class="s1">'emp_length'</code><code class="p">,</code> <code class="s1">'purpose_'</code><code class="p">,</code>
              <code class="s1">'home_'</code><code class="p">,</code> <code class="s1">'emp_len_'</code><code class="p">,</code> <code class="s1">'borrower_score'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">loan_data</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">rf_all</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">rf_all</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

<p>The argument <code>importance=TRUE</code> requests that the <code>randomForest</code> store additional information about the importance of different variables.
The function <code>varImpPlot</code> will plot the relative performance of the variables (relative to permuting that variable):</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">varImpPlot</code><code class="p">(</code><code class="n">rf_all</code><code class="p">,</code><code> </code><code class="n">type</code><code class="o">=</code><code class="m">1</code><code class="p">)</code><code> </code><a class="co" id="co_statistical_machine_learning_CO2-1" href="#callout_statistical_machine_learning_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="nf">varImpPlot</code><code class="p">(</code><code class="n">rf_all</code><code class="p">,</code><code> </code><code class="n">type</code><code class="o">=</code><code class="m">2</code><code class="p">)</code><code> </code><a class="co" id="co_statistical_machine_learning_CO2-2" href="#callout_statistical_machine_learning_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_statistical_machine_learning_CO2-1" href="#co_statistical_machine_learning_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>mean decrease in accuracy</p></dd>
<dt><a class="co" id="callout_statistical_machine_learning_CO2-2" href="#co_statistical_machine_learning_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>mean decrease in node impurity</p></dd>
</dl>

<p>In <em>Python</em>, the <code>RandomForestClassifier</code> collects information about feature importance during training and makes it available with the field <code>feature_importances_</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">importances</code> <code class="o">=</code> <code class="n">rf_all</code><code class="o">.</code><code class="n">feature_importances_</code></pre>

<p>The “Gini decrease” is available as the <code>feature_importance_</code> property of the fitted classifier.<a data-type="indexterm" data-primary="Gini impurity" id="idm46522841739944"/> Accuracy decrease, however, is not available out of the box for <em>Python</em>. We can calculate it (<code>scores</code>) using the following code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">rf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">500</code><code class="p">)</code>
<code class="n">scores</code> <code class="o">=</code> <code class="n">defaultdict</code><code class="p">(</code><code class="nb">list</code><code class="p">)</code>

<code class="c1"># cross-validate the scores on a number of different random splits of the data</code>
<code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">):</code>
    <code class="n">train_X</code><code class="p">,</code> <code class="n">valid_X</code><code class="p">,</code> <code class="n">train_y</code><code class="p">,</code> <code class="n">valid_y</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.3</code><code class="p">)</code>
    <code class="n">rf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_X</code><code class="p">,</code> <code class="n">train_y</code><code class="p">)</code>
    <code class="n">acc</code> <code class="o">=</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">valid_y</code><code class="p">,</code> <code class="n">rf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">valid_X</code><code class="p">))</code>
    <code class="k">for</code> <code class="n">column</code> <code class="ow">in</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">:</code>
        <code class="n">X_t</code> <code class="o">=</code> <code class="n">valid_X</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
        <code class="n">X_t</code><code class="p">[</code><code class="n">column</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">X_t</code><code class="p">[</code><code class="n">column</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">)</code>
        <code class="n">shuff_acc</code> <code class="o">=</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">valid_y</code><code class="p">,</code> <code class="n">rf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_t</code><code class="p">))</code>
        <code class="n">scores</code><code class="p">[</code><code class="n">column</code><code class="p">]</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="n">acc</code><code class="o">-</code><code class="n">shuff_acc</code><code class="p">)</code><code class="o">/</code><code class="n">acc</code><code class="p">)</code></pre>

<p>The result is shown in <a data-type="xref" href="#LoanVarImp">Figure 6-8</a>. A similar graph can be created with this <em>Python</em> code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code>
    <code class="s1">'feature'</code><code class="p">:</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code>
    <code class="s1">'Accuracy decrease'</code><code class="p">:</code> <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">scores</code><code class="p">[</code><code class="n">column</code><code class="p">])</code> <code class="k">for</code> <code class="n">column</code> <code class="ow">in</code> <code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">],</code>
    <code class="s1">'Gini decrease'</code><code class="p">:</code> <code class="n">rf_all</code><code class="o">.</code><code class="n">feature_importances_</code><code class="p">,</code>
<code class="p">})</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s1">'Accuracy decrease'</code><code class="p">)</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">ncols</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mf">4.5</code><code class="p">))</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'barh'</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s1">'feature'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'Accuracy decrease'</code><code class="p">,</code>
             <code class="n">legend</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code>

<code class="n">ax</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'barh'</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s1">'feature'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'Gini decrease'</code><code class="p">,</code>
             <code class="n">legend</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">get_yaxis</code><code class="p">()</code><code class="o">.</code><code class="n">set_visible</code><code class="p">(</code><code class="bp">False</code><code class="p">)</code></pre>

<p>There are two ways to measure variable importance:</p>

<ul>
<li>
<p>By the decrease in accuracy of the model if the values of a variable are randomly permuted (<code>type=1</code>). Randomly permuting the values has the effect of removing all predictive power for that variable. The accuracy is computed from the out-of-bag data (so this measure is effectively a cross-validated estimate).</p>
</li>
<li>
<p>By the mean decrease in the Gini impurity score (see <a data-type="xref" href="#Gini">“Measuring Homogeneity or Impurity”</a>) for all of the nodes that were split on a variable (<code>type=2</code>). This measures how much including that variable improves the purity of the nodes. This measure is based on the training set and is therefore less reliable than a measure calculated on out-of-bag data.</p>
</li>
</ul>

<figure><div id="LoanVarImp" class="figure">
<img src="Images/psd2_0608.png" alt="The importance of variables for the full model fit to the loan data." width="1170" height="1756"/>
<h6><span class="label">Figure 6-8. </span>The importance of variables for the full model fit to the loan data</h6>
</div></figure>

<p>The top and bottom panels of <a data-type="xref" href="#LoanVarImp">Figure 6-8</a> show variable importance according to the decrease in accuracy and in Gini impurity, respectively.
The variables in both panels are ranked by the decrease in accuracy.
The variable importance scores produced by these two measures are quite different.</p>

<p>Since the accuracy decrease is a more reliable metric, why should we use the Gini impurity decrease measure?
By default, <code>randomForest</code> computes only this Gini impurity: Gini impurity is a byproduct of the algorithm, whereas model accuracy by variable requires extra computations (randomly permuting the data and predicting this data).
In cases where computational complexity is important, such as in a production setting where thousands of models are being fit, it may not be worth the extra computational effort.
In addition, the Gini decrease sheds light on which variables the random forest is using to make its splitting rules (recall that this information, readily visible in a simple tree, is effectively lost in a random forest).<a data-type="indexterm" data-primary="random forests" data-secondary="variable importance in" data-startref="ix_randforVI" id="idm46522841409880"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" data-tertiary="variable importance" data-startref="ix_statMLbagRFVI" id="idm46522841408632"/><a data-type="indexterm" data-primary="variable importance" data-startref="ix_varimp" id="idm46522841407064"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hyperparameters"><div class="sect2" id="idm46522842068456">
<h2>Hyperparameters</h2>

<p>The random forest, as with many statistical machine learning algorithms, can be considered a black-box algorithm with knobs to adjust how the box works.<a data-type="indexterm" data-primary="hyperparameters" id="idm46522841404408"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" data-tertiary="hyperparameters" id="idm46522841403704"/><a data-type="indexterm" data-primary="random forests" data-secondary="hyperparameters" id="idm46522841402456"/>
These knobs are called <em>hyperparameters</em>, which are parameters that you need to set before fitting a model; they are not optimized as part of the training process.
While traditional statistical models require choices (e.g., the choice of predictors to use in a regression model),
the hyperparameters for random forest are more critical, especially to avoid overfitting.
In particular, the two most important hyperparameters for the random forest are:</p>
<dl>
<dt><code>nodesize</code>/<code>min_samples_leaf</code></dt>
<dd>
<p>The minimum size for terminal nodes (leaves in the tree). The default is 1 for classification and 5 for regression in <em>R</em>. The <code>scikit-learn</code> implementation in <em>Python</em> uses a default of 1 for both.</p>
</dd>
<dt><code>maxnodes</code>/<code>max_leaf_nodes</code></dt>
<dd>
<p>The maximum number of nodes in each decision tree. By default, there is no limit and the largest tree will be fit subject to the constraints of <code>nodesize</code>. Note that in <em>Python</em>, you specify the maximum number of terminal nodes. The two parameters are related:</p>
<div data-type="equation">
<math alttext="maxnodes equals 2 max reverse-solidus bar leaf reverse-solidus bar nodes negative 1" display="block">
  <mrow>
    <mi> maxnodes </mi>
    <mo>=</mo>
    <mn>2</mn>
    <mi> max </mi>
    <mo>_</mo>
    <mi> leaf </mi>
    <mo>_</mo>
    <mi> nodes </mi>
    <mo>-</mo>
    <mn>1</mn>
  </mrow>
</math>
</div>
</dd>
</dl>

<p>It may be tempting to ignore these parameters and simply go with the default values.
However, using the defaults may lead to overfitting when you apply the random forest to noisy data.
When you increase <code>nodesize</code>/<code>min_samples_leaf</code> or set <code>maxnodes</code>/<code>max_leaf_nodes</code>, the algorithm will fit smaller trees and is less likely to create <span class="keep-together">spurious</span> predictive rules.<a data-type="indexterm" data-primary="cross validation" data-secondary="using to test values of hyperparameters" id="idm46522841383736"/>
Cross-validation (see <a data-type="xref" href="ch04.xhtml#CrossValidation">“Cross-Validation”</a>) can be used to test the effects of setting different values for hyperparameters.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522841381800">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Ensemble models improve model accuracy by combining the results from many models.</p>
</li>
<li>
<p>Bagging is a particular type of ensemble model based on fitting many models to bootstrapped samples of the data and averaging the models.</p>
</li>
<li>
<p>Random forest is a special type of bagging applied to decision trees. In addition to resampling the data, the random forest algorithm samples the predictor variables when splitting the trees.</p>
</li>
<li>
<p>A useful output from the random forest is a measure of variable importance that ranks the predictors in terms of their contribution to model accuracy.</p>
</li>
<li>
<p>The random forest has a set of hyperparameters that should be tuned using cross-validation to avoid overfitting.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="bagging and the random forest" data-startref="ix_statMLbagRF" id="idm46522841375528"/></p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Boosting"><div class="sect1" id="Boosting">
<h1>Boosting</h1>

<p>Ensemble models have become a standard tool<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" id="ix_statMLboost"/><a data-type="indexterm" data-primary="boosting" id="ix_boost"/> for predictive modeling.<a data-type="indexterm" data-primary="ensemble of models" data-secondary="creating using boosting" id="idm46522841369768"/>
<em>Boosting</em> is a general technique to create an ensemble of models.
It was developed around the same time as <em>bagging</em> (see <a data-type="xref" href="#Bagging">“Bagging and the Random Forest”</a>).
Like bagging, boosting is most commonly used with decision trees.
Despite their similarities, boosting takes a very different approach—one that comes with many more bells and whistles.<a data-type="indexterm" data-primary="boosting" data-secondary="versus bagging" data-secondary-sortas="bagging" id="idm46522841366680"/><a data-type="indexterm" data-primary="bagging" data-secondary="boosting versus" id="idm46522841365464"/>
As a result, while bagging can be done with relatively little tuning, boosting requires much greater care in its application.
If these two methods were cars, bagging could be considered a Honda Accord (reliable and steady), whereas boosting could be considered a Porsche (powerful but requires more care).</p>

<p>In linear<a data-type="indexterm" data-primary="linear regression" data-secondary="examination of residuals to see if fit can be improved" id="idm46522841363592"/> regression models, the residuals are often examined to see if the fit can be improved (see <a data-type="xref" href="ch04.xhtml#PartialResidualPlots">“Partial Residual Plots and Nonlinearity”</a>).
Boosting takes this concept much further and fits a series of models, in which each successive model seeks to minimize the error of the previous model.
Several variants of the algorithm are <span class="keep-together">commonly</span> used: <em>Adaboost</em>, <em>gradient boosting</em>, and <em>stochastic gradient boosting</em>.<a data-type="indexterm" data-primary="Adaboost" id="idm46522841359240"/><a data-type="indexterm" data-primary="gradient boosting" id="idm46522841358536"/><a data-type="indexterm" data-primary="stochastic gradient boosting" id="idm46522841357864"/>
The latter, stochastic gradient boosting, is the most general and widely used.
Indeed, with the right choice of parameters, the algorithm can emulate the random forest.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522841356856">
<h5>Key Terms for Boosting</h5><dl>
<dt class="horizontal"><strong><em>Ensemble</em></strong></dt>
<dd>
<p>Forming a prediction by using a collection of models.</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Model averaging</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Boosting</em></strong></dt>
<dd>
<p>A general technique to fit a sequence of models by giving more weight to the records with large residuals for each successive round.</p>
</dd>
<dt class="horizontal"><strong><em>Adaboost</em></strong></dt>
<dd>
<p>An early version of boosting that reweights the data based on the <span class="keep-together">residuals</span>.</p>
</dd>
<dt class="horizontal"><strong><em>Gradient boosting</em></strong></dt>
<dd>
<p>A more general form of boosting that is cast in terms of minimizing a cost <span class="keep-together">function</span>.</p>
</dd>
<dt class="horizontal"><strong><em>Stochastic gradient boosting</em></strong></dt>
<dd>
<p>The most general algorithm for boosting that incorporates resampling of records and columns in each round.</p>
</dd>
<dt class="horizontal"><strong><em>Regularization</em></strong></dt>
<dd>
<p>A technique to avoid overfitting by adding a penalty term to the cost function on the number of parameters in the model.<a data-type="indexterm" data-primary="regularization" id="idm46522841340776"/></p>
</dd>
<dt class="horizontal"><strong><em>Hyperparameters</em></strong></dt>
<dd>
<p>Parameters that need to be set before fitting the algorithm.<a data-type="indexterm" data-primary="hyperparameters" id="idm46522841338264"/></p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="The Boosting Algorithm"><div class="sect2" id="BoostingAlgorithm">
<h2>The Boosting Algorithm</h2>

<p>There are various boosting algorithms, and the basic idea behind all of them is essentially the same.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-tertiary="boosting algorithm" id="idm46522841335512"/><a data-type="indexterm" data-primary="Adaboost" data-secondary="boosting algorithm" id="idm46522841334248"/><a data-type="indexterm" data-primary="boosting" data-secondary="boosting algorithm" id="idm46522841333304"/>
The easiest to understand is Adaboost, which proceeds as follows:</p>
<ol>
<li>
<p>Initialize <em>M</em>,  the maximum number of models to be fit, and set the iteration counter <math alttext="m equals 1">
  <mrow>
    <mi>m</mi>
    <mo>=</mo>
    <mn>1</mn>
  </mrow>
</math>.
Initialize the observation weights <math alttext="w Subscript i Baseline equals 1 slash upper N">
  <mrow>
    <msub><mi>w</mi> <mi>i</mi> </msub>
    <mo>=</mo>
    <mn>1</mn>
    <mo>/</mo>
    <mi>N</mi>
  </mrow>
</math> for <math alttext="i equals 1 comma 2 comma ellipsis comma upper N">
  <mrow>
    <mi>i</mi>
    <mo>=</mo>
    <mn>1</mn>
    <mo>,</mo>
    <mn>2</mn>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <mi>N</mi>
  </mrow>
</math>.
Initialize the ensemble model <math alttext="ModifyingAbove upper F With caret Subscript 0 Baseline equals 0">
  <mrow>
    <msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
    <mo>=</mo>
    <mn>0</mn>
  </mrow>
</math>.</p>
</li>
<li>
<p>Using the observation weights <math alttext="w 1 comma w 2 comma ellipsis comma w Subscript upper N Baseline">
  <mrow>
    <msub><mi>w</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>w</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>w</mi> <mi>N</mi> </msub>
  </mrow>
</math>, train a model <math alttext="ModifyingAbove f With caret Subscript m">
  <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi> </msub>
</math> that minimizes the weighted error <math alttext="e Subscript m">
  <msub><mi>e</mi> <mi>m</mi> </msub>
</math> defined by summing the weights for the misclassified <span class="keep-together">observations</span>.</p>
</li>
<li>
<p>Add the model to the ensemble: <math alttext="ModifyingAbove upper F With caret Subscript m Baseline equals ModifyingAbove upper F With caret Subscript m minus 1 Baseline plus alpha Subscript m Baseline ModifyingAbove f With caret Subscript m">
  <mrow>
    <msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mi>m</mi> </msub>
    <mo>=</mo>
    <msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow> </msub>
    <mo>+</mo>
    <msub><mi>α</mi> <mi>m</mi> </msub>
    <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi> </msub>
  </mrow>
</math> where <math alttext="alpha Subscript m Baseline equals StartFraction log 1 minus e Subscript m Baseline Over e Subscript m Baseline EndFraction">
  <mrow>
    <msub><mi>α</mi> <mi>m</mi> </msub>
    <mo>=</mo>
    <mfrac><mrow><mo form="prefix">log</mo><mn>1</mn><mo>-</mo><msub><mi>e</mi> <mi>m</mi> </msub></mrow> <msub><mi>e</mi> <mi>m</mi> </msub></mfrac>
  </mrow>
</math>.</p>
</li>
<li>
<p>Update the weights <math alttext="w 1 comma w 2 comma ellipsis comma w Subscript upper N Baseline">
  <mrow>
    <msub><mi>w</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>w</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>w</mi> <mi>N</mi> </msub>
  </mrow>
</math> so that the weights are increased for the observations that were misclassified. The size of the increase depends on <math alttext="alpha Subscript m">
  <msub><mi>α</mi> <mi>m</mi> </msub>
</math>, with larger values of <math alttext="alpha Subscript m">
  <msub><mi>α</mi> <mi>m</mi> </msub>
</math> leading to bigger weights.</p>
</li>
<li>
<p>Increment the model counter <math alttext="m equals m plus 1">
  <mrow>
    <mi>m</mi>
    <mo>=</mo>
    <mi>m</mi>
    <mo>+</mo>
    <mn>1</mn>
  </mrow>
</math>. If <math alttext="m less-than-or-equal-to upper M">
  <mrow>
    <mi>m</mi>
    <mo>≤</mo>
    <mi>M</mi>
  </mrow>
</math>, go to step 2.</p>
</li>

</ol>

<p>The boosted estimate is given by:</p>
<div data-type="equation">
<math alttext="ModifyingAbove upper F With caret equals alpha 1 ModifyingAbove f With caret Subscript 1 Baseline plus alpha 2 ModifyingAbove f With caret Subscript 2 Baseline plus ellipsis plus alpha Subscript upper M Baseline ModifyingAbove f With caret Subscript upper M" display="block">
  <mrow>
    <mover accent="true"><mi>F</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <msub><mi>α</mi> <mn>1</mn> </msub>
    <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
    <mo>+</mo>
    <msub><mi>α</mi> <mn>2</mn> </msub>
    <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>2</mn> </msub>
    <mo>+</mo>
    <mo>⋯</mo>
    <mo>+</mo>
    <msub><mi>α</mi> <mi>M</mi> </msub>
    <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>M</mi> </msub>
  </mrow>
</math>
</div>

<p>By increasing the weights for the observations that were misclassified, the algorithm forces the models to train more heavily on the data for which it performed poorly.
The factor <math alttext="alpha Subscript m">
  <msub><mi>α</mi> <mi>m</mi> </msub>
</math> ensures that models with lower error have a bigger weight.</p>

<p>Gradient boosting is similar to Adaboost but casts the problem as an optimization of a cost function.<a data-type="indexterm" data-primary="gradient boosting" id="idm46522841152232"/>
Instead of adjusting weights, gradient boosting  fits models to a <em>pseudo-residual</em>, which has the effect of training more heavily on the larger residuals.<a data-type="indexterm" data-primary="pseudo-residuals" id="idm46522841150936"/>
In the spirit of the random forest, stochastic gradient boosting adds randomness to the algorithm by sampling observations and predictor variables at each stage.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="XGBoost"><div class="sect2" id="XGBoost">
<h2>XGBoost</h2>

<p>The most widely used public domain software for boosting is XGBoost, an implementation of stochastic gradient boosting originally developed by Tianqi Chen and Carlos Guestrin at the University of Washington.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-tertiary="XGBoost software" id="ix_statMLboostXGB"/><a data-type="indexterm" data-primary="stochastic gradient boosting" data-secondary="XGBoost implementation" id="ix_stgrbo"/><a data-type="indexterm" data-primary="boosting" data-secondary="XGBoost software" id="ix_boostXGB"/><a data-type="indexterm" data-primary="XGBoost" id="ix_XGB"/>
A computationally efficient implementation with many options, it is available as a package for most major data science software languages.
In <em>R</em>, XGBoost is available as <a href="https://xgboost.readthedocs.io">the package <code>xgboost</code></a> and with the same name also for <em>Python</em>.</p>

<p>The method <code>xgboost</code> has many parameters that can, and should, be adjusted (see <a data-type="xref" href="#HyperparametersCV">“Hyperparameters and Cross-Validation”</a>).
Two very important parameters are <code>subsample</code>, which controls the fraction of observations that should be sampled at each iteration, and <code>eta</code>, a shrinkage factor applied to <math alttext="alpha Subscript m">
  <msub><mi>α</mi> <mi>m</mi> </msub>
</math> in the boosting algorithm (see <a data-type="xref" href="#BoostingAlgorithm">“The Boosting Algorithm”</a>).
Using <code>subsample</code> makes boosting act like the random forest except that the sampling is done without replacement.
The shrinkage parameter <code>eta</code> is helpful to prevent overfitting by reducing the change in the weights (a smaller change in the weights means the algorithm is less likely to overfit to the training set).
The following applies <code>xgboost</code> in <em>R</em> to the loan data with just two predictor variables:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">predictors</code> <code class="o">&lt;-</code> <code class="nf">data.matrix</code><code class="p">(</code><code class="n">loan3000</code><code class="p">[,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'borrower_score'</code><code class="p">,</code> <code class="s">'payment_inc_ratio'</code><code class="p">)])</code>
<code class="n">label</code> <code class="o">&lt;-</code> <code class="nf">as.numeric</code><code class="p">(</code><code class="n">loan3000</code><code class="p">[,</code><code class="s">'outcome'</code><code class="p">])</code> <code class="o">-</code> <code class="m">1</code>
<code class="n">xgb</code> <code class="o">&lt;-</code> <code class="nf">xgboost</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">predictors</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="n">label</code><code class="p">,</code> <code class="n">objective</code><code class="o">=</code><code class="s">"binary:logistic"</code><code class="p">,</code>
               <code class="n">params</code><code class="o">=</code><code class="nf">list</code><code class="p">(</code><code class="n">subsample</code><code class="o">=</code><code class="m">0.63</code><code class="p">,</code> <code class="n">eta</code><code class="o">=</code><code class="m">0.1</code><code class="p">),</code> <code class="n">nrounds</code><code class="o">=</code><code class="m">100</code><code class="p">)</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code>	<code class="n">train</code><code class="o">-</code><code class="n">error</code><code class="o">:</code><code class="m">0.358333</code>
<code class="p">[</code><code class="m">2</code><code class="p">]</code>	<code class="n">train</code><code class="o">-</code><code class="n">error</code><code class="o">:</code><code class="m">0.346333</code>
<code class="p">[</code><code class="m">3</code><code class="p">]</code>	<code class="n">train</code><code class="o">-</code><code class="n">error</code><code class="o">:</code><code class="m">0.347333</code>
<code class="kc">...</code>
<code class="p">[</code><code class="m">99</code><code class="p">]</code>	<code class="n">train</code><code class="o">-</code><code class="n">error</code><code class="o">:</code><code class="m">0.239333</code>
<code class="p">[</code><code class="m">100</code><code class="p">]</code>	<code class="n">train</code><code class="o">-</code><code class="n">error</code><code class="o">:</code><code class="m">0.241000</code></pre>

<p>Note that <code>xgboost</code> does not support the formula syntax, so the predictors need to be converted to a <code>data.matrix</code> and the response needs to be converted to 0/1 variables.
The <code>objective</code> argument tells <code>xgboost</code> what type of problem this is; based on this,  <code>xgboost</code> will choose a metric to optimize.</p>

<p>In <em>Python</em>, <code>xgboost</code> has two different interfaces: a <code>scikit-learn</code> API and a more functional interface like in <em>R</em>. To be consistent with other <code>scikit-learn</code> methods, some parameters were renamed. For example, <code>eta</code> is renamed to <code>learning_rate</code>; using <code>eta</code> will not fail, but it will not have the desired effect:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="s1">'payment_inc_ratio'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">loan3000</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code>

<code class="n">xgb</code> <code class="o">=</code> <code class="n">XGBClassifier</code><code class="p">(</code><code class="n">objective</code><code class="o">=</code><code class="s1">'binary:logistic'</code><code class="p">,</code> <code class="n">subsample</code><code class="o">=</code><code class="mf">0.63</code><code class="p">)</code>
<code class="n">xgb</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="o">--</code>
<code class="n">XGBClassifier</code><code class="p">(</code><code class="n">base_score</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">booster</code><code class="o">=</code><code class="s1">'gbtree'</code><code class="p">,</code> <code class="n">colsample_bylevel</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
       <code class="n">colsample_bynode</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">colsample_bytree</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
       <code class="n">max_delta_step</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">max_depth</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">min_child_weight</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">missing</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
       <code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">n_jobs</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">nthread</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">objective</code><code class="o">=</code><code class="s1">'binary:logistic'</code><code class="p">,</code>
       <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">reg_alpha</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">reg_lambda</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">scale_pos_weight</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
       <code class="n">silent</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">subsample</code><code class="o">=</code><code class="mf">0.63</code><code class="p">,</code> <code class="n">verbosity</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>The predicted values can be obtained from the <code>predict</code> function in <em>R</em> and, since there are only two variables, plotted versus the predictors:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">xgb</code><code class="p">,</code> <code class="n">newdata</code><code class="o">=</code><code class="n">predictors</code><code class="p">)</code>
<code class="n">xgb_df</code> <code class="o">&lt;-</code> <code class="nf">cbind</code><code class="p">(</code><code class="n">loan3000</code><code class="p">,</code> <code class="n">pred_default</code> <code class="o">=</code> <code class="n">pred</code> <code class="o">&gt;</code> <code class="m">0.5</code><code class="p">,</code> <code class="n">prob_default</code> <code class="o">=</code> <code class="n">pred</code><code class="p">)</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">xgb_df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">borrower_score</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">payment_inc_ratio</code><code class="p">,</code>
                  <code class="n">color</code><code class="o">=</code><code class="n">pred_default</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="n">pred_default</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">pred_default</code><code class="p">))</code> <code class="o">+</code>
         <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="m">.8</code><code class="p">)</code> <code class="o">+</code>
         <code class="nf">scale_color_manual</code><code class="p">(</code><code class="n">values</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="s">'FALSE'</code><code class="o">=</code><code class="s">'#b8e186'</code><code class="p">,</code> <code class="s">'TRUE'</code><code class="o">=</code><code class="s">'#d95f02'</code><code class="p">))</code> <code class="o">+</code>
         <code class="nf">scale_shape_manual</code><code class="p">(</code><code class="n">values</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="s">'FALSE'</code><code class="o">=</code><code class="m">0</code><code class="p">,</code> <code class="s">'TRUE'</code><code class="o">=</code><code class="m">1</code><code class="p">))</code> <code class="o">+</code>
         <code class="nf">scale_size_manual</code><code class="p">(</code><code class="n">values</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="s">'FALSE'</code><code class="o">=</code><code class="m">0.5</code><code class="p">,</code> <code class="s">'TRUE'</code><code class="o">=</code><code class="m">2</code><code class="p">))</code></pre>

<p>The same figure can be created in <em>Python</em> using the following code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>

<code class="n">xgb_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">xgb_df</code><code class="o">.</code><code class="n">prediction</code><code class="o">==</code><code class="s1">'paid off'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code>
    <code class="n">x</code><code class="o">=</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s1">'.'</code><code class="p">,</code>
    <code class="n">markerfacecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code> <code class="n">markeredgecolor</code><code class="o">=</code><code class="s1">'C1'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">xgb_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">xgb_df</code><code class="o">.</code><code class="n">prediction</code><code class="o">==</code><code class="s1">'default'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code>
    <code class="n">x</code><code class="o">=</code><code class="s1">'borrower_score'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s1">'o'</code><code class="p">,</code>
    <code class="n">markerfacecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">,</code> <code class="n">markeredgecolor</code><code class="o">=</code><code class="s1">'C0'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">legend</code><code class="p">([</code><code class="s1">'paid off'</code><code class="p">,</code> <code class="s1">'default'</code><code class="p">]);</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">25</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'borrower_score'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'payment_inc_ratio'</code><code class="p">)</code></pre>

<p>The result is shown in <a data-type="xref" href="#LoanXGB">Figure 6-9</a>.
Qualitatively, this is similar to the predictions from the random forest; see <a data-type="xref" href="#LoanRF">Figure 6-7</a>.
The predictions are somewhat noisy in that some borrowers with a very high borrower score still end up with a prediction of default.<a data-type="indexterm" data-primary="prediction" data-secondary="from XGBoost applied to loan default data" id="idm46522840487896"/></p>

<figure><div id="LoanXGB" class="figure">
<img src="Images/psd2_0609.png" alt="The predicted outcomes from XGBoost applied to the loan default data." width="1432" height="1169"/>
<h6><span class="label">Figure 6-9. </span>The predicted outcomes from XGBoost applied to the loan default data</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Regularization: Avoiding Overfitting"><div class="sect2" id="Regularization">
<h2>Regularization: Avoiding Overfitting</h2>

<p>Blind application of <code>xgboost</code> can lead to unstable models as a result of <em>overfitting</em> to the training data.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-tertiary="XGBoost software" data-startref="ix_statMLboostXGB" id="idm46522840481688"/><a data-type="indexterm" data-primary="stochastic gradient boosting" data-secondary="XGBoost implementation" data-startref="ix_stgrbo" id="idm46522840480280"/><a data-type="indexterm" data-primary="boosting" data-secondary="XGBoost software" data-startref="ix_boostXGB" id="idm46522840479048"/><a data-type="indexterm" data-primary="XGBoost" data-startref="ix_XGB" id="idm46522840477832"/><a data-type="indexterm" data-primary="boosting" data-secondary="regularization, avoiding overfitting with" id="ix_boostregu"/><a data-type="indexterm" data-primary="regularization" data-secondary="avoiding overfitting with" id="ix_regu"/><a data-type="indexterm" data-primary="overfitting" data-secondary="avoiding using regularization" id="ix_ovrft"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-tertiary="overfitting, avoiding with regularization" id="ix_statMLboostovr"/><a data-type="indexterm" data-primary="XGBoost" data-secondary="using regularization to avoid overfitting" id="ix_XGBreg"/>
The problem with overfitting is twofold:</p>

<ul class="pagebreak-before">
<li>
<p>The accuracy of the model on new data not in the training set will be degraded.</p>
</li>
<li>
<p>The predictions from the model are highly variable, leading to unstable results.</p>
</li>
</ul>

<p>Any modeling technique is potentially prone to overfitting.
For example, if too many variables are included in a regression equation, the model may end up with spurious predictions.
However, for most statistical techniques, overfitting can be avoided by a judicious selection of predictor variables.
Even the random forest generally produces a reasonable model without tuning the parameters.</p>

<p>This, however, is not the case for <code>xgboost</code>.
Fit <code>xgboost</code> to the loan data for a training set with all of the variables included in the model. In <em>R</em>, you can do this as follows:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">seed</code> <code class="o">&lt;-</code> <code class="m">400820</code>
<code class="n">predictors</code> <code class="o">&lt;-</code> <code class="nf">data.matrix</code><code class="p">(</code><code class="n">loan_data</code><code class="p">[,</code> <code class="o">-</code><code class="nf">which</code><code class="p">(</code><code class="nf">names</code><code class="p">(</code><code class="n">loan_data</code><code class="p">)</code> <code class="o">%in%</code>
                                       <code class="s">'outcome'</code><code class="p">)])</code>
<code class="n">label</code> <code class="o">&lt;-</code> <code class="nf">as.numeric</code><code class="p">(</code><code class="n">loan_data</code><code class="o">$</code><code class="n">outcome</code><code class="p">)</code> <code class="o">-</code> <code class="m">1</code>
<code class="n">test_idx</code> <code class="o">&lt;-</code> <code class="nf">sample</code><code class="p">(</code><code class="nf">nrow</code><code class="p">(</code><code class="n">loan_data</code><code class="p">),</code> <code class="m">10000</code><code class="p">)</code>

<code class="n">xgb_default</code> <code class="o">&lt;-</code> <code class="nf">xgboost</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">predictors</code><code class="p">[</code><code class="o">-</code><code class="n">test_idx</code><code class="p">,],</code> <code class="n">label</code><code class="o">=</code><code class="n">label</code><code class="p">[</code><code class="o">-</code><code class="n">test_idx</code><code class="p">],</code>
                       <code class="n">objective</code><code class="o">=</code><code class="s">'binary:logistic'</code><code class="p">,</code> <code class="n">nrounds</code><code class="o">=</code><code class="m">250</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="m">0</code><code class="p">)</code>
<code class="n">pred_default</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">xgb_default</code><code class="p">,</code> <code class="n">predictors</code><code class="p">[</code><code class="n">test_idx</code><code class="p">,])</code>
<code class="n">error_default</code> <code class="o">&lt;-</code> <code class="nf">abs</code><code class="p">(</code><code class="n">label</code><code class="p">[</code><code class="n">test_idx</code><code class="p">]</code> <code class="o">-</code> <code class="n">pred_default</code><code class="p">)</code> <code class="o">&gt;</code> <code class="m">0.5</code>
<code class="n">xgb_default</code><code class="o">$</code><code class="n">evaluation_log</code><code class="p">[</code><code class="m">250</code><code class="p">,]</code>
<code class="nf">mean</code><code class="p">(</code><code class="n">error_default</code><code class="p">)</code>
<code class="o">-</code>
<code class="n">iter</code> <code class="n">train_error</code>
<code class="m">1</code><code class="o">:</code>  <code class="m">250</code>    <code class="m">0.133043</code>

<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.3529</code></pre>

<p>We use the function <code>train_test_split</code> in <em>Python</em> to split the data set into training and test sets:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'loan_amnt'</code><code class="p">,</code> <code class="s1">'term'</code><code class="p">,</code> <code class="s1">'annual_inc'</code><code class="p">,</code> <code class="s1">'dti'</code><code class="p">,</code> <code class="s1">'payment_inc_ratio'</code><code class="p">,</code>
              <code class="s1">'revol_bal'</code><code class="p">,</code> <code class="s1">'revol_util'</code><code class="p">,</code> <code class="s1">'purpose'</code><code class="p">,</code> <code class="s1">'delinq_2yrs_zero'</code><code class="p">,</code>
              <code class="s1">'pub_rec_zero'</code><code class="p">,</code> <code class="s1">'open_acc'</code><code class="p">,</code> <code class="s1">'grade'</code><code class="p">,</code> <code class="s1">'emp_length'</code><code class="p">,</code> <code class="s1">'purpose_'</code><code class="p">,</code>
              <code class="s1">'home_'</code><code class="p">,</code> <code class="s1">'emp_len_'</code><code class="p">,</code> <code class="s1">'borrower_score'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'outcome'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">loan_data</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">([</code><code class="mi">1</code> <code class="k">if</code> <code class="n">o</code> <code class="o">==</code> <code class="s1">'default'</code> <code class="k">else</code> <code class="mi">0</code> <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="n">loan_data</code><code class="p">[</code><code class="n">outcome</code><code class="p">]])</code>

<code class="n">train_X</code><code class="p">,</code> <code class="n">valid_X</code><code class="p">,</code> <code class="n">train_y</code><code class="p">,</code> <code class="n">valid_y</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mi">10000</code><code class="p">)</code>

<code class="n">xgb_default</code> <code class="o">=</code> <code class="n">XGBClassifier</code><code class="p">(</code><code class="n">objective</code><code class="o">=</code><code class="s1">'binary:logistic'</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">250</code><code class="p">,</code>
                            <code class="n">max_depth</code><code class="o">=</code><code class="mi">6</code><code class="p">,</code> <code class="n">reg_lambda</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
                            <code class="n">subsample</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">xgb_default</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_X</code><code class="p">,</code> <code class="n">train_y</code><code class="p">)</code>

<code class="n">pred_default</code> <code class="o">=</code> <code class="n">xgb_default</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">valid_X</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
<code class="n">error_default</code> <code class="o">=</code> <code class="nb">abs</code><code class="p">(</code><code class="n">valid_y</code> <code class="o">-</code> <code class="n">pred_default</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mf">0.5</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'default: '</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">error_default</code><code class="p">))</code></pre>

<p>The test set consists of 10,000 randomly sampled records from the full data, and the training set consists of the remaining records.
Boosting leads to an error rate of only 13.3% for the training set.
The test set, however, has a much higher error rate of 35.3%.
This is a result of overfitting: while boosting can explain the variability in the training set very well, the prediction rules do not apply to new data.</p>

<p>Boosting provides several parameters to avoid overfitting, including the parameters <code>eta</code> (or <code>learning_rate</code>) and <code>subsample</code> (see <a data-type="xref" href="#XGBoost">“XGBoost”</a>).
Another approach is <em>regularization</em>, a technique that modifies the cost function in order to <em>penalize</em> the complexity of the model.<a data-type="indexterm" data-primary="penalty on model complexity" id="idm46522839931704"/>
Decision trees are fit by minimizing cost <span class="keep-together">criteria</span> such as Gini’s impurity score (see <a data-type="xref" href="#Gini">“Measuring Homogeneity or Impurity”</a>).
In <code>xgboost</code>, it is possible to modify the cost function by adding a term that measures the complexity of the model.</p>

<p>There are two parameters in <code>xgboost</code> to regularize the model: <code>alpha</code> and <code>lambda</code>, which correspond to Manhattan distance (L1-regularization) and squared Euclidean distance (L2-regularization), respectively (see <a data-type="xref" href="#DistanceMetrics">“Distance Metrics”</a>).<a data-type="indexterm" data-primary="regularization" data-secondary="L2 regularization" id="idm46522839926264"/><a data-type="indexterm" data-primary="regularization" data-secondary="L1 regularization" id="idm46522839925288"/> Increasing these parameters will penalize more complex models and reduce the size of the trees that are fit.
For example, see what happens if we set <code>lambda</code> to 1,000 in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">xgb_penalty</code> <code class="o">&lt;-</code> <code class="nf">xgboost</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">predictors</code><code class="p">[</code><code class="o">-</code><code class="n">test_idx</code><code class="p">,],</code> <code class="n">label</code><code class="o">=</code><code class="n">label</code><code class="p">[</code><code class="o">-</code><code class="n">test_idx</code><code class="p">],</code>
                       <code class="n">params</code><code class="o">=</code><code class="nf">list</code><code class="p">(</code><code class="n">eta</code><code class="o">=</code><code class="m">.1</code><code class="p">,</code> <code class="n">subsample</code><code class="o">=</code><code class="m">.63</code><code class="p">,</code> <code class="n">lambda</code><code class="o">=</code><code class="m">1000</code><code class="p">),</code>
                       <code class="n">objective</code><code class="o">=</code><code class="s">'binary:logistic'</code><code class="p">,</code> <code class="n">nrounds</code><code class="o">=</code><code class="m">250</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="m">0</code><code class="p">)</code>
<code class="n">pred_penalty</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">xgb_penalty</code><code class="p">,</code> <code class="n">predictors</code><code class="p">[</code><code class="n">test_idx</code><code class="p">,])</code>
<code class="n">error_penalty</code> <code class="o">&lt;-</code> <code class="nf">abs</code><code class="p">(</code><code class="n">label</code><code class="p">[</code><code class="n">test_idx</code><code class="p">]</code> <code class="o">-</code> <code class="n">pred_penalty</code><code class="p">)</code> <code class="o">&gt;</code> <code class="m">0.5</code>
<code class="n">xgb_penalty</code><code class="o">$</code><code class="n">evaluation_log</code><code class="p">[</code><code class="m">250</code><code class="p">,]</code>
<code class="nf">mean</code><code class="p">(</code><code class="n">error_penalty</code><code class="p">)</code>
<code class="o">-</code>
<code class="n">iter</code> <code class="n">train_error</code>
<code class="m">1</code><code class="o">:</code>  <code class="m">250</code>     <code class="m">0.30966</code>

<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.3286</code></pre>

<p>In the <code>scikit-learn</code> API, the parameters are called <code>reg_alpha</code> and <code>reg_lambda</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">xgb_penalty</code> <code class="o">=</code> <code class="n">XGBClassifier</code><code class="p">(</code><code class="n">objective</code><code class="o">=</code><code class="s1">'binary:logistic'</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">250</code><code class="p">,</code>
                            <code class="n">max_depth</code><code class="o">=</code><code class="mi">6</code><code class="p">,</code> <code class="n">reg_lambda</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
                            <code class="n">subsample</code><code class="o">=</code><code class="mf">0.63</code><code class="p">)</code>
<code class="n">xgb_penalty</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_X</code><code class="p">,</code> <code class="n">train_y</code><code class="p">)</code>
<code class="n">pred_penalty</code> <code class="o">=</code> <code class="n">xgb_penalty</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">valid_X</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
<code class="n">error_penalty</code> <code class="o">=</code> <code class="nb">abs</code><code class="p">(</code><code class="n">valid_y</code> <code class="o">-</code> <code class="n">pred_penalty</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mf">0.5</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'penalty: '</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">error_penalty</code><code class="p">))</code></pre>

<p>Now the training error is only slightly lower than the error on the test set.</p>

<p>The <code>predict</code> method in <em>R</em> offers a convenient argument, <code>ntreelimit</code>, that forces only the first <em>i</em> trees to be used in the prediction.
This lets us directly compare the in-sample versus out-of-sample error rates as more models are included:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">error_default</code> <code class="o">&lt;-</code> <code class="nf">rep</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">250</code><code class="p">)</code>
<code class="n">error_penalty</code> <code class="o">&lt;-</code> <code class="nf">rep</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">250</code><code class="p">)</code>
<code class="nf">for</code><code class="p">(</code><code class="n">i</code> <code class="n">in</code> <code class="m">1</code><code class="o">:</code><code class="m">250</code><code class="p">){</code>
  <code class="n">pred_def</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">xgb_default</code><code class="p">,</code> <code class="n">predictors</code><code class="p">[</code><code class="n">test_idx</code><code class="p">,],</code> <code class="n">ntreelimit</code><code class="o">=</code><code class="n">i</code><code class="p">)</code>
  <code class="n">error_default</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="nf">mean</code><code class="p">(</code><code class="nf">abs</code><code class="p">(</code><code class="n">label</code><code class="p">[</code><code class="n">test_idx</code><code class="p">]</code> <code class="o">-</code> <code class="n">pred_def</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="m">0.5</code><code class="p">)</code>
  <code class="n">pred_pen</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">xgb_penalty</code><code class="p">,</code> <code class="n">predictors</code><code class="p">[</code><code class="n">test_idx</code><code class="p">,],</code> <code class="n">ntreelimit</code><code class="o">=</code><code class="n">i</code><code class="p">)</code>
  <code class="n">error_penalty</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="nf">mean</code><code class="p">(</code><code class="nf">abs</code><code class="p">(</code><code class="n">label</code><code class="p">[</code><code class="n">test_idx</code><code class="p">]</code> <code class="o">-</code> <code class="n">pred_pen</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="m">0.5</code><code class="p">)</code>
<code class="p">}</code></pre>

<p>In <em>Python</em>, we can call the <code>predict_proba</code> method with the <code>ntree_limit</code> argument:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">results</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">250</code><code class="p">):</code>
    <code class="n">train_default</code> <code class="o">=</code> <code class="n">xgb_default</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">train_X</code><code class="p">,</code> <code class="n">ntree_limit</code><code class="o">=</code><code class="n">i</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
    <code class="n">train_penalty</code> <code class="o">=</code> <code class="n">xgb_penalty</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">train_X</code><code class="p">,</code> <code class="n">ntree_limit</code><code class="o">=</code><code class="n">i</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
    <code class="n">pred_default</code> <code class="o">=</code> <code class="n">xgb_default</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">valid_X</code><code class="p">,</code> <code class="n">ntree_limit</code><code class="o">=</code><code class="n">i</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
    <code class="n">pred_penalty</code> <code class="o">=</code> <code class="n">xgb_penalty</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">valid_X</code><code class="p">,</code> <code class="n">ntree_limit</code><code class="o">=</code><code class="n">i</code><code class="p">)[:,</code> <code class="mi">1</code><code class="p">]</code>
    <code class="n">results</code><code class="o">.</code><code class="n">append</code><code class="p">({</code>
        <code class="s1">'iterations'</code><code class="p">:</code> <code class="n">i</code><code class="p">,</code>
        <code class="s1">'default train'</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="nb">abs</code><code class="p">(</code><code class="n">train_y</code> <code class="o">-</code> <code class="n">train_default</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mf">0.5</code><code class="p">),</code>
        <code class="s1">'penalty train'</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="nb">abs</code><code class="p">(</code><code class="n">train_y</code> <code class="o">-</code> <code class="n">train_penalty</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mf">0.5</code><code class="p">),</code>
        <code class="s1">'default test'</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="nb">abs</code><code class="p">(</code><code class="n">valid_y</code> <code class="o">-</code> <code class="n">pred_default</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mf">0.5</code><code class="p">),</code>
        <code class="s1">'penalty test'</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="nb">abs</code><code class="p">(</code><code class="n">valid_y</code> <code class="o">-</code> <code class="n">pred_penalty</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mf">0.5</code><code class="p">),</code>
    <code class="p">})</code>

<code class="n">results</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">results</code><code class="p">)</code>
<code class="n">results</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p>The output from the model returns the error for the training set in the component <code>xgb_default$evaluation_log</code>.
By combining this with the out-of-sample errors, we can plot the errors versus the number of iterations:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">errors</code> <code class="o">&lt;-</code> <code class="nf">rbind</code><code class="p">(</code><code class="n">xgb_default</code><code class="o">$</code><code class="n">evaluation_log</code><code class="p">,</code>
                <code class="n">xgb_penalty</code><code class="o">$</code><code class="n">evaluation_log</code><code class="p">,</code>
                <code class="nf">ata.frame</code><code class="p">(</code><code class="n">iter</code><code class="o">=</code><code class="m">1</code><code class="o">:</code><code class="m">250</code><code class="p">,</code> <code class="n">train_error</code><code class="o">=</code><code class="n">error_default</code><code class="p">),</code>
                <code class="nf">data.frame</code><code class="p">(</code><code class="n">iter</code><code class="o">=</code><code class="m">1</code><code class="o">:</code><code class="m">250</code><code class="p">,</code> <code class="n">train_error</code><code class="o">=</code><code class="n">error_penalty</code><code class="p">))</code>
<code class="n">errors</code><code class="o">$</code><code class="n">type</code> <code class="o">&lt;-</code> <code class="nf">rep</code><code class="p">(</code><code class="nf">c</code><code class="p">(</code><code class="s">'default train'</code><code class="p">,</code> <code class="s">'penalty train'</code><code class="p">,</code>
                     <code class="s">'default test'</code><code class="p">,</code> <code class="s">'penalty test'</code><code class="p">),</code> <code class="nf">rep</code><code class="p">(</code><code class="m">250</code><code class="p">,</code> <code class="m">4</code><code class="p">))</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">errors</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">iter</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">train_error</code><code class="p">,</code> <code class="n">group</code><code class="o">=</code><code class="n">type</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_line</code><code class="p">(</code><code class="nf">aes</code><code class="p">(</code><code class="n">linetype</code><code class="o">=</code><code class="n">type</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">type</code><code class="p">))</code></pre>

<p>We can use the <code>pandas</code> plot method to create the line graph. The axis returned from the first plot allows us to overlay additional lines onto the same graph. This is a pattern that many of <em>Python</em>’s graph packages support:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">results</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'iterations'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'default test'</code><code class="p">)</code>
<code class="n">results</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'iterations'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'penalty test'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">results</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'iterations'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'default train'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">results</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'iterations'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'penalty train'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code></pre>

<p>The result, displayed in <a data-type="xref" href="#XGBoostError">Figure 6-10</a>, shows how the default model steadily improves the accuracy for the training set but actually gets worse for the test set.
The penalized model does not exhibit this behavior.</p>

<figure><div id="XGBoostError" class="figure">
<img src="Images/psd2_0610.png" alt="The error rate of the default XGBoost versus a penalized version of XGBoost." width="1440" height="963"/>
<h6><span class="label">Figure 6-10. </span>The error rate of the default XGBoost versus a penalized version of XGBoost</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522839148488">
<h5>Ridge Regression and the Lasso</h5>
<p>Adding a penalty on the complexity of a model to help avoid overfitting dates back to the 1970s.<a data-type="indexterm" data-primary="ridge regression" id="idm46522839146616"/>
Least squares regression minimizes the <a data-type="indexterm" data-primary="residual sum of squares (RSS)" id="idm46522839145784"/><a data-type="indexterm" data-primary="least squares regression" id="idm46522839145144"/>residual sum of squares (RSS); see <a data-type="xref" href="ch04.xhtml#OLS">“Least Squares”</a>.
<em>Ridge regression</em> minimizes the sum of squared residuals plus a penalty term that is a function of the number and size of<a data-type="indexterm" data-primary="regularization" data-secondary="L2 regularization" id="idm46522839143032"/> the coefficients:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>Y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn> </msub><mo>-</mo><msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub><msub><mi>X</mi> <mi>i</mi> </msub><mo>-</mo><mo>⋯</mo><mover accent="true"><mi>b</mi> <mo>^</mo></mover><msub><mi>X</mi> <mi>p</mi> </msub></mfenced> <mn>2</mn> </msup>
    <mo>+</mo>
    <mi>λ</mi>
    <mfenced separators="" open="(" close=")">
      <msubsup><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> <mn>2</mn> </msubsup>
      <mo>+</mo>
      <mo>⋯</mo>
      <mo>+</mo>
      <msubsup><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>p</mi> <mn>2</mn> </msubsup>
    </mfenced>
  </mrow>
</math>
</div>

<p>The value of <math alttext="lamda">
  <mi>λ</mi>
</math> determines how much the coefficients are penalized; larger values produce models that are less likely to overfit the data.<a data-type="indexterm" data-primary="lasso regression" id="idm46522839329864"/><a data-type="indexterm" data-primary="regularization" data-secondary="L1 regularization" id="idm46522839329160"/>
The <em>Lasso</em> is similar, except that it <a data-type="indexterm" data-primary="Manhattan distance" id="idm46522839327624"/>uses Manhattan distance instead of Euclidean distance as a penalty term:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>Y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn> </msub><mo>-</mo><msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub><msub><mi>X</mi> <mi>i</mi> </msub><mo>-</mo><mo>⋯</mo><mover accent="true"><mi>b</mi> <mo>^</mo></mover><msub><mi>X</mi> <mi>p</mi> </msub></mfenced> <mn>2</mn> </msup>
    <mo>+</mo>
    <mi>α</mi>
    <mfenced separators="" open="(" close=")">
      <mrow>
        <mo>|</mo>
      <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
        <mo>|</mo>
        <mo>+</mo>
        <mo>⋯</mo>
        <mo>+</mo>
        <mo>|</mo>
      <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>p</mi> </msub>
        <mo>|</mo>
      </mrow>
    </mfenced>
  </mrow>
</math>
</div>

<p>The <code>xgboost</code> parameters <code>lambda</code> (<code>reg_lambda</code>) and <code>alpha</code> (<code>reg_alpha</code>) are acting in a similar manner.<a data-type="indexterm" data-primary="XGBoost" data-secondary="using regularization to avoid overfitting" data-startref="ix_XGBreg" id="idm46522839082600"/><a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-tertiary="overfitting, avoiding with regularization" data-startref="ix_statMLboostovr" id="idm46522839081256"/><a data-type="indexterm" data-primary="overfitting" data-secondary="avoiding using regularization" data-startref="ix_ovrft" id="idm46522839079768"/><a data-type="indexterm" data-primary="regularization" data-secondary="avoiding overfitting with" data-startref="ix_regu" id="idm46522839078584"/><a data-type="indexterm" data-primary="boosting" data-secondary="regularization, avoiding overfitting with" data-startref="ix_boostregu" id="idm46522839077352"/></p>

<p>Using Euclidean distance is also known as L2 regularization, and using Manhattan distance as L1 regularization. The <code>xgboost</code> parameters <code>lambda</code> (<code>reg_lambda</code>) and <code>alpha</code> (<code>reg_alpha</code>) are acting in a similar manner.</p>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hyperparameters and Cross-Validation"><div class="sect2" id="HyperparametersCV">
<h2>Hyperparameters and Cross-Validation</h2>

<p><code>xgboost</code> has a daunting array of hyperparameters; see <a data-type="xref" href="#BoostingParameters">“XGBoost Hyperparameters”</a> for a discussion.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-tertiary="hyperparameters and cross-validation" id="ix_statMLboosthyp"/><a data-type="indexterm" data-primary="boosting" data-secondary="hyperparameters and cross-validation" id="ix_boosthypCV"/><a data-type="indexterm" data-primary="cross validation" data-secondary="using for hyperparameters" id="ix_crval"/>
As seen in <a data-type="xref" href="#Regularization">“Regularization: Avoiding Overfitting”</a>, the specific choice can dramatically change the model fit.
Given a huge combination of hyperparameters to choose from, how should we be guided in our choice?
A standard solution to this problem is to use <em>cross-validation</em>; see <a data-type="xref" href="ch04.xhtml#CrossValidation">“Cross-Validation”</a>.
Cross-validation randomly splits up the data into <em>K</em> different groups, also called <em>folds</em>.<a data-type="indexterm" data-primary="folds" id="idm46522839063400"/>
For each fold, a model is trained on the data not in the fold and then evaluated on the data in the fold.
This yields a measure of accuracy of the model on out-of-sample data.
The best set of hyperparameters is the one given by the model with the lowest overall error as computed by averaging the errors from each of the folds.</p>

<p>To illustrate the technique, we apply it to parameter selection for <code>xgboost</code>.
In this example, we explore two parameters: the shrinkage parameter <code>eta</code> (<code>learning_rate</code>—see <a data-type="xref" href="#XGBoost">“XGBoost”</a>) and the maximum depth of trees <code>max_depth</code>.
The parameter <code>max_depth</code> is the maximum depth of a leaf node to the root of the tree with a default value of six.
This gives us another way to control overfitting: deep trees
tend to be more complex and may overfit the data.
First we set up the folds and parameter list. In <em>R</em>, this is done as follows:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">N</code> <code class="o">&lt;-</code> <code class="nf">nrow</code><code class="p">(</code><code class="n">loan_data</code><code class="p">)</code>
<code class="n">fold_number</code> <code class="o">&lt;-</code> <code class="nf">sample</code><code class="p">(</code><code class="m">1</code><code class="o">:</code><code class="m">5</code><code class="p">,</code> <code class="n">N</code><code class="p">,</code> <code class="n">replace</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">)</code>
<code class="n">params</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">eta</code> <code class="o">=</code> <code class="nf">rep</code><code class="p">(</code><code class="nf">c</code><code class="p">(</code><code class="m">.1</code><code class="p">,</code> <code class="m">.5</code><code class="p">,</code> <code class="m">.9</code><code class="p">),</code> <code class="m">3</code><code class="p">),</code>
                     <code class="n">max_depth</code> <code class="o">=</code> <code class="nf">rep</code><code class="p">(</code><code class="nf">c</code><code class="p">(</code><code class="m">3</code><code class="p">,</code> <code class="m">6</code><code class="p">,</code> <code class="m">12</code><code class="p">),</code> <code class="nf">rep</code><code class="p">(</code><code class="m">3</code><code class="p">,</code><code class="m">3</code><code class="p">)))</code></pre>

<p>Now we apply the preceding algorithm to compute the error for each model and each fold using five folds:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">error</code> <code class="o">&lt;-</code> <code class="nf">matrix</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="n">nrow</code><code class="o">=</code><code class="m">9</code><code class="p">,</code> <code class="n">ncol</code><code class="o">=</code><code class="m">5</code><code class="p">)</code>
<code class="nf">for</code><code class="p">(</code><code class="n">i</code> <code class="n">in</code> <code class="m">1</code><code class="o">:</code><code class="nf">nrow</code><code class="p">(</code><code class="n">params</code><code class="p">)){</code>
  <code class="nf">for</code><code class="p">(</code><code class="n">k</code> <code class="n">in</code> <code class="m">1</code><code class="o">:</code><code class="m">5</code><code class="p">){</code>
    <code class="n">fold_idx</code> <code class="o">&lt;-</code> <code class="p">(</code><code class="m">1</code><code class="o">:</code><code class="n">N</code><code class="p">)[</code><code class="n">fold_number</code> <code class="o">==</code> <code class="n">k</code><code class="p">]</code>
    <code class="n">xgb</code> <code class="o">&lt;-</code> <code class="nf">xgboost</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">predictors</code><code class="p">[</code><code class="o">-</code><code class="n">fold_idx</code><code class="p">,],</code> <code class="n">label</code><code class="o">=</code><code class="n">label</code><code class="p">[</code><code class="o">-</code><code class="n">fold_idx</code><code class="p">],</code>
                   <code class="n">params</code><code class="o">=</code><code class="nf">list</code><code class="p">(</code><code class="n">eta</code><code class="o">=</code><code class="n">params</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="s">'eta'</code><code class="p">],</code>
                               <code class="n">max_depth</code><code class="o">=</code><code class="n">params</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="s">'max_depth'</code><code class="p">]),</code>
                   <code class="n">objective</code><code class="o">=</code><code class="s">'binary:logistic'</code><code class="p">,</code> <code class="n">nrounds</code><code class="o">=</code><code class="m">100</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="m">0</code><code class="p">)</code>
    <code class="n">pred</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">xgb</code><code class="p">,</code> <code class="n">predictors</code><code class="p">[</code><code class="n">fold_idx</code><code class="p">,])</code>
    <code class="n">error</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="n">k</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="nf">mean</code><code class="p">(</code><code class="nf">abs</code><code class="p">(</code><code class="n">label</code><code class="p">[</code><code class="n">fold_idx</code><code class="p">]</code> <code class="o">-</code> <code class="n">pred</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="m">0.5</code><code class="p">)</code>
  <code class="p">}</code>
<code class="p">}</code></pre>

<p>In the following <em>Python</em> code, we create all possible combinations of hyperparameters and fit and evaluate models with each combination:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">idx</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">size</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">replace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code>
</code><code class="n">error</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code class="k">for</code><code> </code><code class="n">eta</code><code class="p">,</code><code> </code><code class="n">max_depth</code><code> </code><code class="ow">in</code><code> </code><code class="n">product</code><code class="p">(</code><code class="p">[</code><code class="mf">0.1</code><code class="p">,</code><code> </code><code class="mf">0.5</code><code class="p">,</code><code> </code><code class="mf">0.9</code><code class="p">]</code><code class="p">,</code><code> </code><code class="p">[</code><code class="mi">3</code><code class="p">,</code><code> </code><code class="mi">6</code><code class="p">,</code><code> </code><code class="mi">9</code><code class="p">]</code><code class="p">)</code><code class="p">:</code><code>  </code><a class="co" id="co_statistical_machine_learning_CO3-1" href="#callout_statistical_machine_learning_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>    </code><code class="n">xgb</code><code> </code><code class="o">=</code><code> </code><code class="n">XGBClassifier</code><code class="p">(</code><code class="n">objective</code><code class="o">=</code><code class="s1">'</code><code class="s1">binary:logistic</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">250</code><code class="p">,</code><code>
</code><code>                        </code><code class="n">max_depth</code><code class="o">=</code><code class="n">max_depth</code><code class="p">,</code><code> </code><code class="n">learning_rate</code><code class="o">=</code><code class="n">eta</code><code class="p">)</code><code>
</code><code>    </code><code class="n">cv_error</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>    </code><code class="k">for</code><code> </code><code class="n">k</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code><code class="p">:</code><code>
</code><code>        </code><code class="n">fold_idx</code><code> </code><code class="o">=</code><code> </code><code class="n">idx</code><code> </code><code class="o">==</code><code> </code><code class="n">k</code><code>
</code><code>        </code><code class="n">train_X</code><code> </code><code class="o">=</code><code> </code><code class="n">X</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="o">~</code><code class="n">fold_idx</code><code class="p">]</code><code class="p">;</code><code> </code><code class="n">train_y</code><code> </code><code class="o">=</code><code> </code><code class="n">y</code><code class="p">[</code><code class="o">~</code><code class="n">fold_idx</code><code class="p">]</code><code>
</code><code>        </code><code class="n">valid_X</code><code> </code><code class="o">=</code><code> </code><code class="n">X</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">fold_idx</code><code class="p">]</code><code class="p">;</code><code> </code><code class="n">valid_y</code><code> </code><code class="o">=</code><code> </code><code class="n">y</code><code class="p">[</code><code class="n">fold_idx</code><code class="p">]</code><code>
</code><code>
</code><code>        </code><code class="n">xgb</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_X</code><code class="p">,</code><code> </code><code class="n">train_y</code><code class="p">)</code><code>
</code><code>        </code><code class="n">pred</code><code> </code><code class="o">=</code><code> </code><code class="n">xgb</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">valid_X</code><code class="p">)</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">]</code><code>
</code><code>        </code><code class="n">cv_error</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="nb">abs</code><code class="p">(</code><code class="n">valid_y</code><code> </code><code class="o">-</code><code> </code><code class="n">pred</code><code class="p">)</code><code> </code><code class="o">&gt;</code><code> </code><code class="mf">0.5</code><code class="p">)</code><code class="p">)</code><code>
</code><code>    </code><code class="n">error</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">{</code><code>
</code><code>        </code><code class="s1">'</code><code class="s1">eta</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">eta</code><code class="p">,</code><code>
</code><code>        </code><code class="s1">'</code><code class="s1">max_depth</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">max_depth</code><code class="p">,</code><code>
</code><code>        </code><code class="s1">'</code><code class="s1">avg_error</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">cv_error</code><code class="p">)</code><code>
</code><code>    </code><code class="p">}</code><code class="p">)</code><code>
</code><code>    </code><code class="k">print</code><code class="p">(</code><code class="n">error</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="p">)</code><code>
</code><code class="n">errors</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">error</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_statistical_machine_learning_CO3-1" href="#co_statistical_machine_learning_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>We use the function <code>itertools.product</code> from the <em>Python</em> standard library to create all possible combinations of the two hyperparameters.</p></dd>
</dl>

<p>Since we are fitting 45 total models, this can take a while.
The errors are stored as a matrix with the models along the rows and folds along the columns.
Using the function <code>rowMeans</code>, we can compare the error rate for the different parameter sets:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">avg_error</code> <code class="o">&lt;-</code> <code class="m">100</code> <code class="o">*</code> <code class="nf">round</code><code class="p">(</code><code class="nf">rowMeans</code><code class="p">(</code><code class="n">error</code><code class="p">),</code> <code class="m">4</code><code class="p">)</code>
<code class="nf">cbind</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">avg_error</code><code class="p">)</code>
  <code class="n">eta</code> <code class="n">max_depth</code> <code class="n">avg_error</code>
<code class="m">1</code> <code class="m">0.1</code>         <code class="m">3</code>     <code class="m">32.90</code>
<code class="m">2</code> <code class="m">0.5</code>         <code class="m">3</code>     <code class="m">33.43</code>
<code class="m">3</code> <code class="m">0.9</code>         <code class="m">3</code>     <code class="m">34.36</code>
<code class="m">4</code> <code class="m">0.1</code>         <code class="m">6</code>     <code class="m">33.08</code>
<code class="m">5</code> <code class="m">0.5</code>         <code class="m">6</code>     <code class="m">35.60</code>
<code class="m">6</code> <code class="m">0.9</code>         <code class="m">6</code>     <code class="m">37.82</code>
<code class="m">7</code> <code class="m">0.1</code>        <code class="m">12</code>     <code class="m">34.56</code>
<code class="m">8</code> <code class="m">0.5</code>        <code class="m">12</code>     <code class="m">36.83</code>
<code class="m">9</code> <code class="m">0.9</code>        <code class="m">12</code>     <code class="m">38.18</code></pre>

<p>Cross-validation suggests that using shallower trees with a smaller value of <code>eta</code>/<code>learning_rate</code> yields more accurate results.
Since these models are also more stable, the best parameters to use are  <code>eta=0.1</code> and <code>max_depth=3</code> (or possibly  <code>max_depth=6</code>).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="BoostingParameters">
<h5>XGBoost Hyperparameters</h5>
<p>The hyperparameters for <code>xgboost</code> are primarily used to balance overfitting with the accuracy and computational complexity.<a data-type="indexterm" data-primary="XGBoost" data-secondary="hyperparameters" id="idm46522838244056"/>
For a complete discussion of the parameters, refer to the <a href="https://oreil.ly/xC_OY"><code>xgboost</code> documentation</a>.</p>
<dl>
<dt><code>eta</code>/<code>learning_rate</code></dt>
<dd>
<p>The shrinkage factor between 0 and 1 applied to <math alttext="alpha">
  <mi>α</mi>
</math> in the boosting algorithm. The default is 0.3, but for noisy data, smaller values are recommended (e.g., 0.1). In <em>Python</em>, the default value is 0.1.</p>
</dd>
<dt><code>nrounds</code>/<code>n_estimators</code></dt>
<dd>
<p>The number of boosting rounds. If <code>eta</code> is set to a small value, it is important to increase the number of rounds since the algorithm learns more slowly. As long as some parameters are included to prevent overfitting, having more rounds doesn’t hurt.</p>
</dd>
<dt><code>max_depth</code></dt>
<dd>
<p>The maximum depth of the tree (the default is 6). In contrast to the random forest, which fits very deep trees, boosting usually fits shallow trees. This has the advantage of avoiding spurious complex interactions in the model that can arise from noisy data. In <em>Python</em>, the default is 3.</p>
</dd>
<dt><code>subsample</code> and <code>colsample_bytree</code></dt>
<dd>
<p>Fraction of the records to sample without replacement and the fraction of predictors to sample for use in fitting the trees. These parameters, which are similar to those in random forests, help avoid overfitting. The default is 1.0.</p>
</dd>
<dt><code>lambda</code>/<code>reg_lambda</code> and <code>alpha</code>/<code>reg_alpha</code></dt>
<dd>
<p>The regularization parameters to help control overfitting (see <a data-type="xref" href="#Regularization">“Regularization: Avoiding Overfitting”</a>). Default values for <em>Python</em> are <code>reg_lambda=1</code> and <code>reg_alpha=0</code>. In <em>R</em>, both values have default of 0.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-tertiary="hyperparameters and cross-validation" data-startref="ix_statMLboosthyp" id="idm46522838819944"/><a data-type="indexterm" data-primary="cross validation" data-secondary="using for hyperparameters" data-startref="ix_crval" id="idm46522838818488"/><a data-type="indexterm" data-primary="boosting" data-secondary="hyperparameters and cross-validation" data-startref="ix_boosthypCV" id="idm46522838817304"/></p>
</dd>
</dl>
</div></aside>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522838815992">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Boosting is a class of ensemble models based on fitting a sequence of models, with more weight given to records with large errors in successive rounds.</p>
</li>
<li>
<p>Stochastic gradient boosting is the most general type of boosting and offers the best performance. The most common form of stochastic gradient boosting uses tree models.</p>
</li>
<li>
<p>XGBoost is a popular and computationally efficient software package for stochastic gradient boosting; it is available in all common languages used in data science.</p>
</li>
<li>
<p>Boosting is prone to overfitting the data, and the hyperparameters need to be tuned to avoid this.</p>
</li>
<li>
<p>Regularization is one way to avoid overfitting by including a penalty term on the number of parameters (e.g., tree size) in a model.</p>
</li>
<li>
<p>Cross-validation is especially important for boosting due to the large number of hyperparameters that need to be set.</p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46522839073128">
<h1>Summary</h1>

<p>This chapter<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="boosting" data-startref="ix_statMLboost" id="idm46522838577336"/><a data-type="indexterm" data-primary="boosting" data-startref="ix_boost" id="idm46522838576088"/> has described two classification and prediction methods that “learn” flexibly and locally from data, rather than starting with a structural model (e.g., a linear regression) that is fit to the entire data set.  <em>K</em>-Nearest Neighbors is a simple process that looks around at similar records and assigns their majority class (or average value) to the record being predicted.  Trying various cutoff (split) values of predictor variables, tree models iteratively divide the data into sections and subsections that are increasingly homogeneous with respect to class.  The most effective split values form a path, and also a “rule,” to a classification or prediction.   Tree models are a very powerful and popular predictive tool, often outperforming other methods. They have given rise to various ensemble methods (random forests, boosting, bagging) that sharpen the predictive power of trees.<a data-type="indexterm" data-primary="statistical machine learning" data-startref="ix_statML" id="idm46522838573704"/></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46522845224392"><sup><a href="ch06.xhtml#idm46522845224392-marker">1</a></sup> This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck; used with permission.</p><p data-type="footnote" id="idm46522845155448"><sup><a href="ch06.xhtml#idm46522845155448-marker">2</a></sup> For this example, we take the first row in the <code>loan200</code> data set as the <code>newloan</code> and exclude it from the data set for training.</p><p data-type="footnote" id="idm46522843869576"><sup><a href="ch06.xhtml#idm46522843869576-marker">3</a></sup> The term CART is a registered trademark of Salford Systems related to their specific implementation of tree models.</p><p data-type="footnote" id="idm46522842847944"><sup><a href="ch06.xhtml#idm46522842847944-marker">4</a></sup> The term <em>random forest</em> is a trademark of Leo Breiman and Adele Cutler and licensed to Salford Systems. There is no standard nontrademark name, and the term random forest is as synonymous with the algorithm as Kleenex is with facial tissues.</p></div></div></section></div>



  </body></html>