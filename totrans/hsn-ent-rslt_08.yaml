- en: Chapter 8\. Scaling Up on Google Cloud
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章\. 在 Google Cloud 上扩展
- en: In this chapter, we will work through how to scale up our entity resolution
    process to enable us to match large datasets in reasonable timeframes. We will
    use a cluster of virtual machines running in parallel on Google Cloud Platform
    (GCP) to divide up the workload and reduce the time taken to resolve our entities.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何扩展我们的实体解析过程，以便在合理的时间内匹配大型数据集。我们将使用在 Google Cloud Platform (GCP)
    上并行运行的虚拟机群集来分担工作量，减少解析实体所需的时间。
- en: We will walk through how to register a new account on the Cloud Platform and
    how to configure the storage and compute services we will need. Once our infrastructure
    is ready, we will rerun our company matching example from [Chapter 6](ch06.html#chapter_6),
    splitting both model training and entity resolution steps across a managed cluster
    of compute resources.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步介绍如何在 Cloud Platform 上注册新账户以及如何配置我们将需要的存储和计算服务。一旦我们的基础设施准备好，我们将重新运行[第 6
    章](ch06.html#chapter_6)中的公司匹配示例，将模型训练和实体解析步骤分割到一个托管的计算资源群集中。
- en: Lastly, we will check that our performance is consistent and make sure we tidy
    up fully, deleting the cluster and returning the virtual machines we have borrowed
    to ensure we don’t continue to run up any additional fees.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将检查我们的性能是否一致，并确保我们完全清理，删除集群并返回我们借用的虚拟机，以确保我们不会继续产生额外的费用。
- en: Google Cloud Setup
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud 设置
- en: To build our cloud infrastructure, we first need to register for an account
    on the GCP. To do this, visit *cloud.google.com* on your browser. From here, you
    can click Get Started to begin the registration process. You’ll need to register
    with a Google email address or alternatively create a new account. This is shown
    in [Figure 8-1](#fig-8-1).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的云基础设施，我们首先需要在 GCP 上注册一个账户。为此，请在浏览器中访问*cloud.google.com*。从这里，您可以点击“开始”以开始注册过程。您需要使用
    Google 邮箱地址注册，或者创建一个新账户。如[图 8-1](#fig-8-1)所示。
- en: '![](assets/hoer_0801.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0801.png)'
- en: Figure 8-1\. GCP sign in
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. GCP 登录
- en: You’ll need to select your country, read and then accept the Google terms of
    service, and click Continue. See [Figure 8-2](#fig-8-2).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要选择您的国家，阅读并接受 Google 的服务条款，然后点击“继续”。见[图 8-2](#fig-8-2)。
- en: '![](assets/hoer_0802.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0802.png)'
- en: Figure 8-2\. Register for GCP, Account Information Step 1
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 注册 GCP，账户信息第一步
- en: On the next page, you will be asked to verify your address and payment information
    before you can click Start My Free Trial.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一页上，您将被要求验证您的地址和支付信息，然后才能点击“开始我的免费试用”。
- en: Google Cloud Platform Fees
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud Platform 费用
- en: Please be warned that it’s your responsibility to understand the ongoing charges
    associated with using any of the products on the Google Cloud Platform. From personal
    experience I can say it is very easy to leave virtual machines running or overlook
    persistent disks that you will still be charged for.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，了解使用 Google Cloud Platform 上任何产品所带来的持续费用是您的责任。根据个人经验，我可以说，很容易忽略持续运行的虚拟机或忽略您仍需付费的持久性磁盘。
- en: At the time of writing, Google Cloud is offering $300 credit for free to spend
    over the first 90 days of your usage of the platform. They are also stating that
    no autocharge will be applied after the free trial ends, so if you use a credit
    or debit card, you won’t be charged unless you manually upgrade to a paid account.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Google Cloud 提供首次使用平台的 300 美元免费信用额，可在前 90 天内使用。此外，他们还声明在免费试用结束后不会自动收费，因此如果您使用信用卡或借记卡，除非您手动升级到付费账户，否则不会收取费用。
- en: '*Of course, these terms are subject to change, so please read the terms carefully
    when you sign up.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*当然，这些条款可能会更改，因此请在注册时仔细阅读条款。*'
- en: Once you’ve signed up, you’ll be taken to the Google Cloud console.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您注册成功，您将被带到 Google Cloud 控制台。
- en: Setting Up Project Storage
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置项目存储
- en: Your first task is to create a project. On GCP, a project is a logical group
    of resources and data that you manage. For the purpose of this book, all our work
    will be grouped together in one project.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您的第一个任务是创建一个项目。在 GCP 上，项目是您管理的资源和数据的逻辑组。为了本书的目的，我们所有的工作将被组织在一个项目中。
- en: To begin, choose your preferred project name and Google will suggest a corresponding
    Project ID for you. You might wish to edit their suggestion to shorten or simplify
    it a little as you’ll potentially be typing in this Project ID a fair number of
    times.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择您喜欢的项目名称，Google 将为您建议一个相应的项目 ID。您可能希望编辑他们的建议，以简化或缩短这个项目 ID，因为您可能需要多次输入这个项目
    ID。
- en: As an individual user, you don’t need to specify an organization owner of your
    project, as illustrated in [Figure 8-3](#fig-8-3).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为个人用户，您不需要指定项目的组织所有者，如[图 8-3](#fig-8-3)所示。
- en: '![](assets/hoer_0803.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0803.png)'
- en: Figure 8-3\. “Create a Project” dialog box
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. “创建项目”对话框
- en: Once you’ve created your project, you’ll be taken to the project dashboard.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 创建项目后，您将进入项目仪表板。
- en: The first thing we need is somewhere to store our data on GCP. The standard
    data storage product is called Cloud Storage, and within that, specific data containers
    are called buckets. Buckets have a globally unique name and a geographic location
    where the bucket and its data contents are stored. A bucket can have the same
    name as your Project ID if you wish.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要在 GCP 上存储我们的数据的地方。标准数据存储产品称为 Cloud Storage，其中具体的数据容器称为存储桶。存储桶具有全局唯一的名称和存储桶及其数据内容存储的地理位置。如果您愿意，存储桶的名称可以与您的项目
    ID 相同。
- en: To create a bucket, you can click on the navigation menu home (three horizontal
    lines within a circle, top left of the screen) to select Cloud Storage and then
    Buckets from the drop-down navigation menu. [Figure 8-4](#fig-8-4) shows the menu
    options.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个存储桶，您可以单击导航菜单主页（屏幕左上角的三个水平线内的圆圈）选择云存储，然后从下拉导航菜单中选择桶。[图 8-4](#fig-8-4)显示了菜单选项。
- en: '![](assets/hoer_0804.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0804.png)'
- en: Figure 8-4\. Navigation menu—Cloud Storage
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-4\. 导航菜单—云存储
- en: From here, click Create Bucket from the menu at the top, select your preferred
    name, and then click Continue. See [Figure 8-5](#fig-8-5).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，从顶部菜单中单击创建存储桶，选择您喜欢的名称，然后单击继续。参见[图 8-5](#fig-8-5)。
- en: '![](assets/hoer_0805.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0805.png)'
- en: Figure 8-5\. Create bucket—naming
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-5\. 创建存储桶—命名
- en: Next you need to select your preferred storage location, as illustrated in [Figure 8-6](#fig-8-6).
    For the purposes of this project, you can accept the default or pick a different
    region if you prefer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要选择首选存储位置，如[图 8-6](#fig-8-6)所示。对于本项目，您可以接受默认设置，或者如果您愿意，可以选择不同的区域。
- en: You can press Continue to view the remaining advanced configuration options
    or just jump straight to Create. Now that we have some storage space defined,
    our next step is to reserve some compute resources to run our entity resolution
    process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按“继续”查看剩余的高级配置选项，或者直接跳转到“创建”。现在我们已经定义了一些存储空间，下一步是保留一些计算资源来运行我们的实体解析过程。
- en: '![](assets/hoer_0806.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0806.png)'
- en: Figure 8-6\. Create bucket—data storage location
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-6\. 创建存储桶—数据存储位置
- en: Creating a Dataproc Cluster
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Dataproc 集群
- en: As in previous chapters, we will be using the Splink framework to perform matching.
    To scale up our process to run across multiple machines, we need to switch from
    using DuckDB as our backend database to Spark.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，我们将使用 Splink 框架执行匹配。为了将我们的过程扩展到多台计算机上运行，我们需要将后端数据库从 DuckDB 切换到 Spark。
- en: A convenient way to run Spark on the GCP is to use a *Dataproc cluster*, which
    takes care of creating a number of virtual machines and configuring them to execute
    a Spark job.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GCP 上运行 Spark 的一个方便的方法是使用 *Dataproc 集群*，它负责创建一些虚拟机并配置它们来执行 Spark 作业。
- en: To create a cluster, we must first enable the Cloud Dataproc API. Return to
    the navigation menu and select Dataproc and then Clusters as per [Figure 8-7](#fig-8-7).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个集群，我们首先需要启用 Cloud Dataproc API。返回导航菜单，然后选择 Dataproc，然后像[图 8-7](#fig-8-7)那样选择
    Clusters。
- en: '![](assets/hoer_0807.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0807.png)'
- en: Figure 8-7\. Navigation menu—Dataproc clusters
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-7\. 导航菜单—Dataproc 集群
- en: You’ll then be presented with the API screen. Make sure you read and accept
    the terms and associated fees and then click Enable. See [Figure 8-8](#fig-8-8).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将看到 API 屏幕。确保您阅读并接受条款和相关费用，然后单击启用。参见[图 8-8](#fig-8-8)。
- en: '![](assets/hoer_0808.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0808.png)'
- en: Figure 8-8\. Enable the Cloud Dataproc API
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-8\. 启用 Cloud Dataproc API
- en: Once the API is enabled, you can click on Create Cluster to configure your Dataproc
    instance. Dataproc clusters can be built directly on Compute Engine virtual machines
    or via GKE (Google Kubernetes Engine). For the purposes of this example, the distinction
    between the two isn’t important, so I suggest you select Compute Engine as it
    is the simpler of the two.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 API 后，您可以单击创建集群以配置您的 Dataproc 实例。Dataproc 集群可以直接构建在 Compute Engine 虚拟机上，也可以通过
    GKE（Google Kubernetes Engine）构建。对于本示例，两者之间的区别并不重要，因此建议您选择 Compute Engine，因为它是两者中较简单的一个。
- en: You should then be presented with the screen in [Figure 8-9](#fig-8-9).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将看到[图 8-9](#fig-8-9)中的屏幕。
- en: '![](assets/hoer_0809.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Create cluster on Compute Engine
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here you can name your cluster, select the location in which it resides, and
    choose the type of cluster. Next, scroll down to the Component section and select
    Component Gateway and Jupyter Notebook, as shown in [Figure 8-10](#fig-8-10).
    This is important as it allows us to configure the cluster and use Jupyter to
    execute our entity resolution notebook.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0810.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Dataproc components
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve configured the Components, you can accept the default settings for
    the rest of this page—see [Figure 8-11](#fig-8-11)—and then select the Configure
    Nodes option.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0811.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Configure worker nodes
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The next step is to configure both the Manager and Worker nodes within our cluster.
    Again, you can accept the defaults, checking that the number of workers is set
    to 2 before moving on to Customize Cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: A final step, but an important one, is to consider scheduling deletion of the
    cluster to avoid any ongoing fees should you forget to remove your cluster manually
    when you’re finished with it. I’d also recommend configuring the Cloud Storage
    staging bucket to use the bucket you created earlier; otherwise the Dataproc process
    will create a storage bucket for you that can easily get left behind in the clean-up
    operation. See [Figure 8-12](#fig-8-12).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0812.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Customize cluster—deletion and staging bucket
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, click Create to instruct GCP to create the cluster for you. This will
    take a few moments.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a Dataproc Cluster
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the basic cluster is up and running, we can connect to it by clicking on
    the cluster name and then selecting Jupyter from the Web Interfaces section shown
    in [Figure 8-13](#fig-8-13).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0813.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. Cluster web interfaces—Jupyter
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This will launch a familiar Jupyter environment in a new browser window.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next task is to download and configure the software and data we need. From
    the New menu, select Terminal to bring up a command prompt in a second browser
    window. Switch to the home directory:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then clone the repository from the GitHub repo and switch into the newly created
    directory:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, return to the Jupyter environment and open the *Chapter6.ipynb* notebook.
    Run the data acquisition and standardization sections of the notebook to re-create
    the clean Mari and Basic datasets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the “Saving to Local Storage” section to save the files to:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have reconstructed our datasets, we need to copy them to the Cloud
    Storage bucket we created earlier so that they are accessible to all the nodes
    in our cluster. We do this at the terminal with:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note: Remember to substitute your bucket name!'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: This will create the directory *handsonentityresolution* in your bucket and
    copy the GitHub repository files across. You’ll need these for this chapter and
    the next one.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we need to install Splink:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Previously, we relied on the approximate string matching functions, like Jaro-Winkler,
    that were built into DuckDB. These routines aren’t available by default in Spark,
    so we need to download and install a Java ARchive (JAR) file containing these
    user-defined functions (UDFs) that Splink will call:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们依赖于内置于 DuckDB 中的近似字符串匹配函数，如 Jaro-Winkler。这些例程在 Spark 中默认情况下不可用，因此我们需要下载并安装一个包含这些用户定义函数（UDF）的
    Java ARchive（JAR）文件，Splink 将调用它们：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, we copy this file into our bucket so that these functions are available
    to the cluster worker nodes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将此文件复制到我们的存储桶中，以便这些函数可以供集群工作节点使用：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To tell our cluster where to pick up this file on startup, we need to browse
    to the *spark-defaults.conf* file in Jupyter at path */Local Disk/etc/spark/conf.dist/*
    and add the following line, remembering to substitute your bucket name:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要告知我们的集群在启动时从哪里获取此文件，我们需要在 Jupyter 中的路径 */Local Disk/etc/spark/conf.dist/* 中浏览到
    *spark-defaults.conf* 文件，并添加以下行，记得替换您的存储桶名称：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To activate this file you need to close your Jupyter windows, return to the
    cluster menu, and then STOP and START your cluster.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要激活此文件，您需要关闭您的 Jupyter 窗口，返回到集群菜单，然后停止并重新启动您的集群。
- en: Entity Resolution on Spark
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 上的实体解析
- en: Finally, we are ready to begin our matching process. Open *Chapter8.ipynb* in
    Jupyter Notebook.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备开始我们的匹配过程。在 Jupyter Notebook 中打开 *Chapter8.ipynb*。
- en: 'To begin, we load the data files that we saved to our bucket earlier into pandas
    DataFrames:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载之前保存到我们存储桶中的数据文件到 pandas DataFrames 中：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next we configure our Splink settings. These are a little different from the
    settings we used with the DuckDB backend:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们配置 Splink 设置。这些与我们在 DuckDB 后端使用的设置略有不同：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: First, we import `pyspark` functions that allow us to create a new Spark session
    from Python. Next, we set the configuration parameters to define the amount of
    parallel processing we want. Then we create the `SparkSession` and set a `Checkpoint`
    directory that Spark uses as a temporary store.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入 `pyspark` 函数，允许我们从 Python 创建一个新的 Spark 会话。接下来，我们设置配置参数来定义我们想要的并行处理量。然后我们创建
    `SparkSession` 并设置一个 `Checkpoint` 目录，Spark 将其用作临时存储。
- en: Lastly, we register a new Java function so that Splink can pick up the Jaro-Winkler
    similarity routine from the JAR file we set up earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们注册一个新的 Java 函数，以便 Splink 可以从之前设置的 JAR 文件中获取 Jaro-Winkler 相似度算法。
- en: 'Next we need to set up a Spark schema that we can map our data onto:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置一个 Spark 模式，我们可以将我们的数据映射到其中：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we can create Spark DataFrames (`dfs`) from the pandas DataFrames (`df`)
    and the schema we have just defined. As both datasets have the same structure,
    we can use the same schema:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以从 pandas DataFrames (`df`) 和我们刚刚定义的模式创建 Spark DataFrames (`dfs`)。由于两个数据集具有相同的结构，我们可以使用相同的模式：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our next step is to configure Splink. These settings are the same as we used
    in [Chapter 6](ch06.html#chapter_6):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是配置 Splink。这些设置与我们在[第6章](ch06.html#chapter_6)中使用的设置相同：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we set up a `SparkLinker` using the Spark DataFrames and settings we have
    created:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用我们创建的 Spark DataFrames 和设置来设置一个 `SparkLinker`：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As in [Chapter 6](ch06.html#chapter_6), we train the *u* and *m* values using
    random sampling and the expectation-maximization algorithm, respectively:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第6章](ch06.html#chapter_6)中一样，我们使用随机抽样和期望最大化算法分别训练 *u* 和 *m* 值：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This is where we begin to see the benefit of switching to Spark. Whereas model
    training previously took over an hour, now it is completed in only a few minutes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们开始看到切换到 Spark 的好处的地方。以前的模型训练需要超过一个小时，现在仅需几分钟即可完成。
- en: 'Alternatively, you can load a pretrained model, *Chapter8_Splink_Settings.json,* from
    the repository:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以从存储库加载预训练模型 *Chapter8_Splink_Settings.json*。
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can then run our predictions and get our results:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以运行我们的预测并获得我们的结果：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Measuring Performance
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能测量
- en: As expected, switching to Spark doesn’t substantially change our results. At
    a 0.1 match threshold we have 192 matches. Our results are shown in [Table 8-1](#table-8-1).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，切换到 Spark 并没有实质性地改变我们的结果。在 0.1 的匹配阈值下，我们有 192 个匹配项。我们的结果显示在[表 8-1](#table-8-1)中。
- en: Table 8-1\. MCA match results (Spark)—low threshold
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-1\. MCA 匹配结果（Spark）—低阈值
- en: '| **Match threshold = 0.1** | **Number of matches** | **Unique entities matched**
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| **匹配阈值 = 0.1** | **匹配数量** | **唯一匹配实体** |'
- en: '| --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Name and postcode match | 47 | 45 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 名称和邮政编码匹配 | 47 | 45 |'
- en: '| Name match only | 37 | 31 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 仅名称匹配 | 37 | 31 |'
- en: '| Postcode match only | 108 | 27 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 仅邮政编码匹配 | 108 | 27 |'
- en: '| **Total matches** | **192** | **85 (deduped)** |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **总匹配数** | **192** | **85（去重后）** |'
- en: '| Unmatched |   | 11 (of which 2 dissolved) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 未匹配 |   | 11（其中2个溶解） |'
- en: '| **Total organizations** |   | **96** |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **总组织数** |   | **96** |'
- en: This gives a slight improvement in precision and accuracy, due to slight variation
    in the calculated model parameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这会因模型参数计算中的轻微变化而略微提高精确度和准确性。
- en: Tidy Up!
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整理一下！
- en: To ensure you aren’t charged for continuing to run the virtual machines and
    their disks, make sure you *DELETE your cluster (not just STOP, which will continue
    to accrue disk fees) from the Cluster menu*.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保您不会继续支付虚拟机及其磁盘的费用，请确保您从集群菜单中**删除您的集群（不仅仅是停止，这将继续积累磁盘费用）**。
- en: You may wish to retain the files in your Cloud Storage bucket for use in the
    following chapter. However, make sure to delete any staging or temporary buckets
    if these have been created, as shown in [Figure 8-14](#fig-8-14).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望在下一章中继续使用Cloud Storage存储桶中的文件，请确保删除任何已创建的暂存或临时存储桶，如[图 8-14](#fig-8-14)所示。
- en: '![](assets/hoer_0814.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hoer_0814.png)'
- en: Figure 8-14\. Delete staging and temporary buckets
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-14\. 删除暂存和临时存储桶
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to scale up our entity resolution process to
    run on multiple machines. This gives us the ability to match larger datasets than
    we could cope with on a single machine, or in a reasonable execution timeframe.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何将我们的实体解析过程扩展到多台机器上运行。这使我们能够匹配比单台机器或合理的执行时间范围更大的数据集。
- en: Along the way we’ve seen how to use Google Cloud Platform to provision compute
    and storage resources that we can use on demand and pay only for the bandwidth
    we need.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们已经看到如何使用Google Cloud Platform来提供计算和存储资源，我们可以按需使用，并且只支付我们所需的带宽费用。
- en: We’ve also seen that even with a relatively straightforward example there is
    a large amount of configuration work we need to do before we can run our entity
    resolution process. In the next chapter, we will take a look at how the cloud
    providers provide APIs that offer to abstract away much of this complexity.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，即使是一个相对简单的示例，我们在运行实体解析过程之前需要大量的配置工作。在下一章中，我们将看到云服务提供商提供的API可以抽象出大部分这些复杂性。
