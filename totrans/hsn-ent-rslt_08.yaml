- en: Chapter 8\. Scaling Up on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will work through how to scale up our entity resolution
    process to enable us to match large datasets in reasonable timeframes. We will
    use a cluster of virtual machines running in parallel on Google Cloud Platform
    (GCP) to divide up the workload and reduce the time taken to resolve our entities.
  prefs: []
  type: TYPE_NORMAL
- en: We will walk through how to register a new account on the Cloud Platform and
    how to configure the storage and compute services we will need. Once our infrastructure
    is ready, we will rerun our company matching example from [Chapter 6](ch06.html#chapter_6),
    splitting both model training and entity resolution steps across a managed cluster
    of compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will check that our performance is consistent and make sure we tidy
    up fully, deleting the cluster and returning the virtual machines we have borrowed
    to ensure we don’t continue to run up any additional fees.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build our cloud infrastructure, we first need to register for an account
    on the GCP. To do this, visit *cloud.google.com* on your browser. From here, you
    can click Get Started to begin the registration process. You’ll need to register
    with a Google email address or alternatively create a new account. This is shown
    in [Figure 8-1](#fig-8-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. GCP sign in
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll need to select your country, read and then accept the Google terms of
    service, and click Continue. See [Figure 8-2](#fig-8-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Register for GCP, Account Information Step 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the next page, you will be asked to verify your address and payment information
    before you can click Start My Free Trial.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Platform Fees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please be warned that it’s your responsibility to understand the ongoing charges
    associated with using any of the products on the Google Cloud Platform. From personal
    experience I can say it is very easy to leave virtual machines running or overlook
    persistent disks that you will still be charged for.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Google Cloud is offering $300 credit for free to spend
    over the first 90 days of your usage of the platform. They are also stating that
    no autocharge will be applied after the free trial ends, so if you use a credit
    or debit card, you won’t be charged unless you manually upgrade to a paid account.
  prefs: []
  type: TYPE_NORMAL
- en: '*Of course, these terms are subject to change, so please read the terms carefully
    when you sign up.*'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve signed up, you’ll be taken to the Google Cloud console.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Project Storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your first task is to create a project. On GCP, a project is a logical group
    of resources and data that you manage. For the purpose of this book, all our work
    will be grouped together in one project.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, choose your preferred project name and Google will suggest a corresponding
    Project ID for you. You might wish to edit their suggestion to shorten or simplify
    it a little as you’ll potentially be typing in this Project ID a fair number of
    times.
  prefs: []
  type: TYPE_NORMAL
- en: As an individual user, you don’t need to specify an organization owner of your
    project, as illustrated in [Figure 8-3](#fig-8-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. “Create a Project” dialog box
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve created your project, you’ll be taken to the project dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need is somewhere to store our data on GCP. The standard
    data storage product is called Cloud Storage, and within that, specific data containers
    are called buckets. Buckets have a globally unique name and a geographic location
    where the bucket and its data contents are stored. A bucket can have the same
    name as your Project ID if you wish.
  prefs: []
  type: TYPE_NORMAL
- en: To create a bucket, you can click on the navigation menu home (three horizontal
    lines within a circle, top left of the screen) to select Cloud Storage and then
    Buckets from the drop-down navigation menu. [Figure 8-4](#fig-8-4) shows the menu
    options.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Navigation menu—Cloud Storage
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From here, click Create Bucket from the menu at the top, select your preferred
    name, and then click Continue. See [Figure 8-5](#fig-8-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Create bucket—naming
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next you need to select your preferred storage location, as illustrated in [Figure 8-6](#fig-8-6).
    For the purposes of this project, you can accept the default or pick a different
    region if you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: You can press Continue to view the remaining advanced configuration options
    or just jump straight to Create. Now that we have some storage space defined,
    our next step is to reserve some compute resources to run our entity resolution
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Create bucket—data storage location
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Creating a Dataproc Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in previous chapters, we will be using the Splink framework to perform matching.
    To scale up our process to run across multiple machines, we need to switch from
    using DuckDB as our backend database to Spark.
  prefs: []
  type: TYPE_NORMAL
- en: A convenient way to run Spark on the GCP is to use a *Dataproc cluster*, which
    takes care of creating a number of virtual machines and configuring them to execute
    a Spark job.
  prefs: []
  type: TYPE_NORMAL
- en: To create a cluster, we must first enable the Cloud Dataproc API. Return to
    the navigation menu and select Dataproc and then Clusters as per [Figure 8-7](#fig-8-7).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Navigation menu—Dataproc clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ll then be presented with the API screen. Make sure you read and accept
    the terms and associated fees and then click Enable. See [Figure 8-8](#fig-8-8).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Enable the Cloud Dataproc API
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the API is enabled, you can click on Create Cluster to configure your Dataproc
    instance. Dataproc clusters can be built directly on Compute Engine virtual machines
    or via GKE (Google Kubernetes Engine). For the purposes of this example, the distinction
    between the two isn’t important, so I suggest you select Compute Engine as it
    is the simpler of the two.
  prefs: []
  type: TYPE_NORMAL
- en: You should then be presented with the screen in [Figure 8-9](#fig-8-9).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Create cluster on Compute Engine
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here you can name your cluster, select the location in which it resides, and
    choose the type of cluster. Next, scroll down to the Component section and select
    Component Gateway and Jupyter Notebook, as shown in [Figure 8-10](#fig-8-10).
    This is important as it allows us to configure the cluster and use Jupyter to
    execute our entity resolution notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Dataproc components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve configured the Components, you can accept the default settings for
    the rest of this page—see [Figure 8-11](#fig-8-11)—and then select the Configure
    Nodes option.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Configure worker nodes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The next step is to configure both the Manager and Worker nodes within our cluster.
    Again, you can accept the defaults, checking that the number of workers is set
    to 2 before moving on to Customize Cluster.
  prefs: []
  type: TYPE_NORMAL
- en: A final step, but an important one, is to consider scheduling deletion of the
    cluster to avoid any ongoing fees should you forget to remove your cluster manually
    when you’re finished with it. I’d also recommend configuring the Cloud Storage
    staging bucket to use the bucket you created earlier; otherwise the Dataproc process
    will create a storage bucket for you that can easily get left behind in the clean-up
    operation. See [Figure 8-12](#fig-8-12).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Customize cluster—deletion and staging bucket
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, click Create to instruct GCP to create the cluster for you. This will
    take a few moments.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a Dataproc Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the basic cluster is up and running, we can connect to it by clicking on
    the cluster name and then selecting Jupyter from the Web Interfaces section shown
    in [Figure 8-13](#fig-8-13).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. Cluster web interfaces—Jupyter
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This will launch a familiar Jupyter environment in a new browser window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next task is to download and configure the software and data we need. From
    the New menu, select Terminal to bring up a command prompt in a second browser
    window. Switch to the home directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then clone the repository from the GitHub repo and switch into the newly created
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, return to the Jupyter environment and open the *Chapter6.ipynb* notebook.
    Run the data acquisition and standardization sections of the notebook to re-create
    the clean Mari and Basic datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Edit the “Saving to Local Storage” section to save the files to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have reconstructed our datasets, we need to copy them to the Cloud
    Storage bucket we created earlier so that they are accessible to all the nodes
    in our cluster. We do this at the terminal with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note: Remember to substitute your bucket name!'
  prefs: []
  type: TYPE_NORMAL
- en: This will create the directory *handsonentityresolution* in your bucket and
    copy the GitHub repository files across. You’ll need these for this chapter and
    the next one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we need to install Splink:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Previously, we relied on the approximate string matching functions, like Jaro-Winkler,
    that were built into DuckDB. These routines aren’t available by default in Spark,
    so we need to download and install a Java ARchive (JAR) file containing these
    user-defined functions (UDFs) that Splink will call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we copy this file into our bucket so that these functions are available
    to the cluster worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To tell our cluster where to pick up this file on startup, we need to browse
    to the *spark-defaults.conf* file in Jupyter at path */Local Disk/etc/spark/conf.dist/*
    and add the following line, remembering to substitute your bucket name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To activate this file you need to close your Jupyter windows, return to the
    cluster menu, and then STOP and START your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Entity Resolution on Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we are ready to begin our matching process. Open *Chapter8.ipynb* in
    Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we load the data files that we saved to our bucket earlier into pandas
    DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we configure our Splink settings. These are a little different from the
    settings we used with the DuckDB backend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: First, we import `pyspark` functions that allow us to create a new Spark session
    from Python. Next, we set the configuration parameters to define the amount of
    parallel processing we want. Then we create the `SparkSession` and set a `Checkpoint`
    directory that Spark uses as a temporary store.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we register a new Java function so that Splink can pick up the Jaro-Winkler
    similarity routine from the JAR file we set up earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we need to set up a Spark schema that we can map our data onto:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can create Spark DataFrames (`dfs`) from the pandas DataFrames (`df`)
    and the schema we have just defined. As both datasets have the same structure,
    we can use the same schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Our next step is to configure Splink. These settings are the same as we used
    in [Chapter 6](ch06.html#chapter_6):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we set up a `SparkLinker` using the Spark DataFrames and settings we have
    created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As in [Chapter 6](ch06.html#chapter_6), we train the *u* and *m* values using
    random sampling and the expectation-maximization algorithm, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is where we begin to see the benefit of switching to Spark. Whereas model
    training previously took over an hour, now it is completed in only a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can load a pretrained model, *Chapter8_Splink_Settings.json,* from
    the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then run our predictions and get our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Measuring Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As expected, switching to Spark doesn’t substantially change our results. At
    a 0.1 match threshold we have 192 matches. Our results are shown in [Table 8-1](#table-8-1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. MCA match results (Spark)—low threshold
  prefs: []
  type: TYPE_NORMAL
- en: '| **Match threshold = 0.1** | **Number of matches** | **Unique entities matched**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Name and postcode match | 47 | 45 |'
  prefs: []
  type: TYPE_TB
- en: '| Name match only | 37 | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| Postcode match only | 108 | 27 |'
  prefs: []
  type: TYPE_TB
- en: '| **Total matches** | **192** | **85 (deduped)** |'
  prefs: []
  type: TYPE_TB
- en: '| Unmatched |   | 11 (of which 2 dissolved) |'
  prefs: []
  type: TYPE_TB
- en: '| **Total organizations** |   | **96** |'
  prefs: []
  type: TYPE_TB
- en: This gives a slight improvement in precision and accuracy, due to slight variation
    in the calculated model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Tidy Up!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To ensure you aren’t charged for continuing to run the virtual machines and
    their disks, make sure you *DELETE your cluster (not just STOP, which will continue
    to accrue disk fees) from the Cluster menu*.
  prefs: []
  type: TYPE_NORMAL
- en: You may wish to retain the files in your Cloud Storage bucket for use in the
    following chapter. However, make sure to delete any staging or temporary buckets
    if these have been created, as shown in [Figure 8-14](#fig-8-14).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. Delete staging and temporary buckets
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to scale up our entity resolution process to
    run on multiple machines. This gives us the ability to match larger datasets than
    we could cope with on a single machine, or in a reasonable execution timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: Along the way we’ve seen how to use Google Cloud Platform to provision compute
    and storage resources that we can use on demand and pay only for the bandwidth
    we need.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also seen that even with a relatively straightforward example there is
    a large amount of configuration work we need to do before we can run our entity
    resolution process. In the next chapter, we will take a look at how the cloud
    providers provide APIs that offer to abstract away much of this complexity.
  prefs: []
  type: TYPE_NORMAL
