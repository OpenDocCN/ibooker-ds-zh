<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 47. In Depth: k-Means Clustering" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0511-k-means">
<h1><span class="label">Chapter 47. </span>In Depth: k-Means Clustering</h1>
<p><a data-primary="clustering" data-secondary="k-means" data-type="indexterm" id="ix_ch47-asciidoc0"/><a data-primary="k-means clustering" data-type="indexterm" id="ix_ch47-asciidoc1"/><a data-primary="machine learning" data-secondary="k-means clustering" data-type="indexterm" id="ix_ch47-asciidoc2"/>In the previous chapters we explored unsupervised machine learning
models for dimensionality reduction. Now we will move on to another
class of unsupervised machine learning models: clustering algorithms.
Clustering algorithms seek to learn, from the properties of the data, an
optimal division or discrete labeling of groups of points.</p>
<p>Many clustering algorithms are available in Scikit-Learn and elsewhere,
but perhaps the simplest to understand is an algorithm known as <em>k-means
clustering</em>, which is implemented in <code>sklearn.cluster.KMeans</code>.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>
<section data-pdf-bookmark="Introducing k-Means" data-type="sect1"><div class="sect1" id="ch_0511-k-means_introducing-k-means">
<h1>Introducing k-Means</h1>
<p><a data-primary="k-means clustering" data-secondary="basics" data-type="indexterm" id="ix_ch47-asciidoc3"/>The <em>k</em>-means algorithm searches for a predetermined number of clusters
within an unlabeled multidimensional dataset. It accomplishes this using
a simple conception of what the optimal clustering looks like:</p>
<ul>
<li>
<p>The <em>cluster center</em> is the arithmetic mean of all the points
belonging to the cluster.</p>
</li>
<li>
<p>Each point is closer to its own cluster center than to other cluster
centers.</p>
</li>
</ul>
<p>Those two assumptions are the basis of the <em>k</em>-means model. We will soon
dive into exactly <em>how</em> the algorithm reaches this solution, but for now
let’s take a look at a simple dataset and see the <em>k</em>-means
result.</p>
<p>First, let’s generate a two-dimensional dataset containing
four distinct blobs. To emphasize that this is an unsupervised
algorithm, we will leave the labels out of the visualization (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_5_0">Figure 47-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>
        <code class="n">X</code><code class="p">,</code> <code class="n">y_true</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
                               <code class="n">cluster_std</code><code class="o">=</code><code class="mf">0.60</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_5_0">
<img alt="output 5 0" height="392" src="assets/output_5_0.png" width="600"/>
<h6><span class="label">Figure 47-1. </span>Data for demonstration of clustering</h6>
</div></figure>
<p>By eye, it is relatively easy to pick out the four clusters. The
<em>k</em>-means algorithm does this automatically, and in Scikit-Learn uses
the typical estimator API:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
        <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>
        <code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">y_kmeans</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>
<p>Let’s visualize the results by plotting the data colored by
these labels (<a data-type="xref" href="#fig_0511-k-means_files_in_output_9_0">Figure 47-2</a>). We will also plot the cluster
centers as determined by the <em>k</em>-means estimator:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y_kmeans</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">)</code>

        <code class="n">centers</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">centers</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">centers</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">200</code><code class="p">);</code></pre>
<p>The good news is that the <em>k</em>-means algorithm (at least in this simple
case) assigns the points to clusters very similarly to how we might
assign them by eye. But you might wonder how this algorithm finds these
clusters so quickly: after all, the number of possible combinations of
cluster assignments is exponential in the number of data points—an
exhaustive search would be very, very costly. Fortunately for us, such
an exhaustive search is not necessary: instead, the typical approach to
<em>k</em>-means involves an intuitive iterative approach known as
<em>expectation–maximization</em>.<a data-startref="ix_ch47-asciidoc3" data-type="indexterm" id="idm45858724102048"/></p>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_9_0">
<img alt="output 9 0" height="398" src="assets/output_9_0.png" width="600"/>
<h6><span class="label">Figure 47-2. </span><span class="roman">k</span>-means cluster centers with clusters indicated by color</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Expectation–Maximization" data-type="sect1"><div class="sect1" id="ch_0511-k-means_expectation-maximization">
<h1>Expectation–Maximization</h1>
<p><a data-primary="expectation-maximization (E-M) algorithm" data-secondary="k-means clustering and" data-type="indexterm" id="ix_ch47-asciidoc4"/><a data-primary="k-means clustering" data-secondary="expectation-maximization algorithm" data-type="indexterm" id="ix_ch47-asciidoc5"/>Expectation–maximization (E–M) is a powerful algorithm that comes up in
a variety of contexts within data science. <em>k</em>-means is a particularly
simple and easy-to-understand application of the algorithm; we’ll
walk through it briefly here. In short, the expectation–maximization
approach here consists of the following 
<span class="keep-together">procedure</span>:</p>
<ol>
<li>
<p>Guess some cluster centers.</p>
</li>
<li>
<p>Repeat until converged:</p>
<ol>
<li>
<p><em>E-step</em>: Assign points to the nearest cluster center.</p>
</li>
<li>
<p><em>M-step</em>:
Set the cluster centers to the mean of their assigned points.</p>
</li>
</ol>
</li>
</ol>
<p>Here the <em>E-step</em> or <em>expectation step</em> is so named because it involves
updating our expectation of which cluster each point belongs to. The
<em>M-step</em> or <em>maximization step</em> is so named because it involves
maximizing some fitness function that defines the locations of the
cluster centers—in this case, that maximization is accomplished by
taking a simple mean of the data in each cluster.</p>
<p>The literature about this algorithm is vast, but can be summarized as
follows: under typical circumstances, each repetition of the E-step and
M-step will always result in a better estimate of the cluster
characteristics.</p>
<p>We can visualize the algorithm as shown in <a data-type="xref" href="#fig_images_in_0511-expectation-maximization">Figure 47-3</a>. For the
particular initialization shown here, the clusters converge in just
three iterations. (For an interactive version of this figure, refer to
the code in the online
<a href="https://oreil.ly/wFnok">appendix</a>.)</p>
<figure><div class="figure" id="fig_images_in_0511-expectation-maximization">
<img alt="05.11 expectation maximization" height="150" src="assets/05.11-expectation-maximization.png" width="600"/>
<h6><span class="label">Figure 47-3. </span>Visualization of the E–M algorithm for <span class="roman">k</span>-means<sup><a data-type="noteref" href="ch47.xhtml#idm45858724046864" id="idm45858724046864-marker">1</a></sup></h6>
</div></figure>
<p>The <em>k</em>-means algorithm is simple enough that we can write it in a few
lines of code. The following is a very basic implementation (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_15_0">Figure 47-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">pairwise_distances_argmin</code>

        <code class="k">def</code> <code class="nf">find_clusters</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">n_clusters</code><code class="p">,</code> <code class="n">rseed</code><code class="o">=</code><code class="mi">2</code><code class="p">):</code>
            <code class="c1"># 1. Randomly choose clusters</code>
            <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="n">rseed</code><code class="p">)</code>
            <code class="n">i</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])[:</code><code class="n">n_clusters</code><code class="p">]</code>
            <code class="n">centers</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>

            <code class="k">while</code> <code class="kc">True</code><code class="p">:</code>
                <code class="c1"># 2a. Assign labels based on closest center</code>
                <code class="n">labels</code> <code class="o">=</code> <code class="n">pairwise_distances_argmin</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">centers</code><code class="p">)</code>

                <code class="c1"># 2b. Find new centers from means of points</code>
                <code class="n">new_centers</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">X</code><code class="p">[</code><code class="n">labels</code> <code class="o">==</code> <code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
                                        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_clusters</code><code class="p">)])</code>

                <code class="c1"># 2c. Check for convergence</code>
                <code class="k">if</code> <code class="n">np</code><code class="o">.</code><code class="n">all</code><code class="p">(</code><code class="n">centers</code> <code class="o">==</code> <code class="n">new_centers</code><code class="p">):</code>
                    <code class="k">break</code>
                <code class="n">centers</code> <code class="o">=</code> <code class="n">new_centers</code>

            <code class="k">return</code> <code class="n">centers</code><code class="p">,</code> <code class="n">labels</code>

        <code class="n">centers</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">find_clusters</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>
                    <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_15_0">
<img alt="output 15 0" height="401" src="assets/output_15_0.png" width="600"/>
<h6><span class="label">Figure 47-4. </span>Data labeled with <span class="roman">k</span>-means</h6>
</div></figure>
<p>Most well-tested implementations will do a bit more than this under the
hood, but the preceding function gives the gist of the
expectation–maximization approach. <a data-primary="expectation-maximization (E-M) algorithm" data-secondary="limitations" data-type="indexterm" id="ix_ch47-asciidoc6"/>There are a few caveats to be aware of when using the
expectation–maximization algorithm:</p>
<dl>
<dt>The globally optimal result may not be achieved</dt>
<dd>
<p>First, although the E–M procedure is guaranteed to improve the result in
each step, there is no assurance that it will lead to the <em>global</em> best
solution. For example, if we use a different random seed in our simple
procedure, the particular starting guesses lead to poor results (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_19_0">Figure 47-5</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">centers</code><code class="p">,</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">find_clusters</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="n">rseed</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>
                    <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_19_0">
<img alt="output 19 0" height="398" src="assets/output_19_0.png" width="600"/>
<h6><span class="label">Figure 47-5. </span>An example of poor convergence in <span class="roman">k</span>-means</h6>
</div></figure>
<p>Here the E–M approach has converged, but has not converged to a globally
optimal configuration. For this reason, it is common for the algorithm
to be run for multiple starting guesses, as indeed Scikit-Learn does by
default (the number is set by the <code>n_init</code> parameter, which defaults to
10).</p>
</dd>
<dt>The number of clusters must be selected beforehand</dt>
<dd>
<p>Another common challenge with <em>k</em>-means is that you must tell it how
many clusters you expect: it cannot learn the number of clusters from
the data. For example, if we ask the algorithm to identify six clusters,
it will happily proceed and find the best six clusters, as shown in
<a data-type="xref" href="ch40.xhtml#fig_0504-feature-engineering_files_in_output_24_0">Figure 40-1</a>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>
                    <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_22_0">
<img alt="output 22 0" height="398" src="assets/output_22_0.png" width="600"/>
<h6><span class="label">Figure 47-6. </span>An example where the number of clusters is chosen poorly</h6>
</div></figure>
<p>Whether the result is meaningful is a question that is difficult to
answer definitively; one approach that is rather intuitive, but that we
won’t discuss further here, is called
<a href="https://oreil.ly/xybmq">silhouette
analysis</a>.</p>
<p>Alternatively, you might use a more complicated clustering algorithm
that has a better quantitative measure of the fitness per number of
clusters (e.g., Gaussian mixture models; see
<a data-type="xref" href="ch48.xhtml#section-0512-gaussian-mixtures">Chapter 48</a>)
or which <em>can</em> choose a suitable number of clusters (e.g., DBSCAN,
mean-shift, or affinity propagation, all available in the
<code>sklearn.cluster</code> submodule).</p>
</dd>
<dt><span class="roman">k</span>-means is limited to linear cluster boundaries</dt>
<dd>
<p>The fundamental model assumptions of <em>k</em>-means (points will be closer to
their own cluster center than to others) means that the algorithm will
often be ineffective if the clusters have complicated geometries.</p>
<p>In particular, the boundaries between <em>k</em>-means clusters will always be
linear, which means that it will fail for more complicated boundaries.
Consider the following data, along with the cluster labels found by the
typical <em>k</em>-means approach (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_26_0">Figure 47-7</a>).</p>
<pre class="pagebreak-before less_space" data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>
        <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="mi">200</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">.05</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>
                    <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_26_0">
<img alt="output 26 0" height="377" src="assets/output_26_0.png" width="600"/>
<h6><span class="label">Figure 47-7. </span>Failure of <span class="roman">k</span>-means with nonlinear boundaries</h6>
</div></figure>
<p>This situation is reminiscent of the discussion in
<a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>, where we used a kernel transformation to project the data
into a higher dimension where a linear separation is possible. We might
imagine using the same trick to allow <em>k</em>-means to discover non-linear
boundaries.</p>
<p>One version of this kernelized <em>k</em>-means is implemented in Scikit-Learn
within the <code>SpectralClustering</code> estimator. It uses the graph of nearest
neighbors to compute a higher-dimensional representation of the data,
and then assigns labels using a <em>k</em>-means algorithm (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_28_0">Figure 47-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">SpectralClustering</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">SpectralClustering</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
                                    <code class="n">affinity</code><code class="o">=</code><code class="s1">'nearest_neighbors'</code><code class="p">,</code>
                                    <code class="n">assign_labels</code><code class="o">=</code><code class="s1">'kmeans'</code><code class="p">)</code>
         <code class="n">labels</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code>
                     <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_28_0">
<img alt="output 28 0" height="685" src="assets/output_28_0.png" width="600"/>
<h6><span class="label">Figure 47-8. </span>Nonlinear boundaries learned by SpectralClustering</h6>
</div></figure>
<p>We see that with this kernel transform approach, the kernelized
<em>k</em>-means is able to find the more complicated nonlinear boundaries
between clusters.</p>
</dd>
<dt><span class="roman">k</span>-means can be slow for large numbers of samples</dt>
<dd>
<p>Because each iteration of <em>k</em>-means must access every point in the
dataset, the algorithm can be relatively slow as the number of samples
grows. You might wonder if this requirement to use all data at each
iteration can be relaxed; for example, you might just use a subset of
the data to update the cluster centers at each step. This is the idea
behind batch-based <em>k</em>-means algorithms, one form of which is
implemented in <code>sklearn.cluster.MiniBatchKMeans</code>. The interface for this
is the same as for standard <code>KMeans</code>; we will see an example of its use
as we continue our discussion<a data-startref="ix_ch47-asciidoc6" data-type="indexterm" id="idm45858723491360"/>.<a data-startref="ix_ch47-asciidoc5" data-type="indexterm" id="idm45858723490528"/><a data-startref="ix_ch47-asciidoc4" data-type="indexterm" id="idm45858723489824"/></p>
</dd>
</dl>
</div></section>
<section data-pdf-bookmark="Examples" data-type="sect1"><div class="sect1" id="ch_0511-k-means_examples">
<h1>Examples</h1>
<p>Being careful about these limitations of the algorithm, we can use
<em>k</em>-means to our advantage in a variety of situations. We’ll
now take a look at a couple of examples.</p>
<section data-pdf-bookmark="Example 1: k-Means on Digits" data-type="sect2"><div class="sect2" id="ch_0511-k-means_example-1-k-means-on-digits">
<h2>Example 1: k-Means on Digits</h2>
<p><a data-primary="k-means clustering" data-secondary="simple digits data application" data-type="indexterm" id="ix_ch47-asciidoc7"/><a data-primary="optical character recognition" data-secondary="k-means clustering" data-type="indexterm" id="ix_ch47-asciidoc8"/>To start, let’s take a look at applying <em>k</em>-means on the
same simple digits data that we saw in Chapters
<a href="ch44.xhtml#section-0508-random-forests">44</a> and <a href="ch45.xhtml#section-0509-principal-component-analysis">45</a>. Here we will attempt to use <em>k</em>-means to
try to identify similar digits <em>without using the original label
information</em>; this might be similar to a first step in extracting
meaning from a new dataset about which you don’t have any <em>a
priori</em> label information.</p>
<p>We will start by loading the dataset, then find the clusters. Recall
that the digits dataset consists of 1,797 samples with 64 features,
where each of the 64 features is the brightness of one pixel in an 8 × 8
image:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
         <code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>
         <code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">64</code><code class="p">)</code></pre>
<p>The clustering can be performed as we did before:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">clusters</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">64</code><code class="p">)</code></pre>
<p>The result is 10 clusters in 64 dimensions. Notice that the cluster
centers themselves are 64-dimensional points, and can be interpreted as
representing the “typical” digit within the cluster. Let’s
see what these cluster centers look like (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_37_0">Figure 47-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
         <code class="n">centers</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code>
         <code class="k">for</code> <code class="n">axi</code><code class="p">,</code> <code class="n">center</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">,</code> <code class="n">centers</code><code class="p">):</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[])</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">center</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'nearest'</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">binary</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_37_0">
<img alt="output 37 0" height="413" src="assets/output_37_0.png" width="600"/>
<h6><span class="label">Figure 47-9. </span>Cluster centers learned by <span class="roman">k</span>-means</h6>
</div></figure>
<p>We see that <em>even without the labels</em>, <code>KMeans</code> is able to find clusters
whose centers are recognizable digits, with perhaps the exception of 1
and 8.</p>
<p>Because <em>k</em>-means knows nothing about the identities of the clusters,
the 0–9 labels may be permuted. We can fix this by matching each learned
cluster label with the true labels found in the clusters:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">scipy.stats</code> <code class="kn">import</code> <code class="n">mode</code>

         <code class="n">labels</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros_like</code><code class="p">(</code><code class="n">clusters</code><code class="p">)</code>
         <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>
             <code class="n">mask</code> <code class="o">=</code> <code class="p">(</code><code class="n">clusters</code> <code class="o">==</code> <code class="n">i</code><code class="p">)</code>
             <code class="n">labels</code><code class="p">[</code><code class="n">mask</code><code class="p">]</code> <code class="o">=</code> <code class="n">mode</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">[</code><code class="n">mask</code><code class="p">])[</code><code class="mi">0</code><code class="p">]</code></pre>
<p>Now we can check how accurate our unsupervised clustering was in finding
similar digits within the data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
         <code class="n">accuracy_score</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="mf">0.7935447968836951</code></pre>
<p>With just a simple <em>k</em>-means algorithm, we discovered the correct
grouping for 80% of the input digits! Let’s check the
confusion matrix for this, visualized in <a data-type="xref" href="#fig_0511-k-means_files_in_output_43_0">Figure 47-10</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>
         <code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
         <code class="n">mat</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>
         <code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">mat</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">fmt</code><code class="o">=</code><code class="s1">'d'</code><code class="p">,</code>
                     <code class="n">cbar</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'Blues'</code><code class="p">,</code>
                     <code class="n">xticklabels</code><code class="o">=</code><code class="n">digits</code><code class="o">.</code><code class="n">target_names</code><code class="p">,</code>
                     <code class="n">yticklabels</code><code class="o">=</code><code class="n">digits</code><code class="o">.</code><code class="n">target_names</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'true label'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'predicted label'</code><code class="p">);</code></pre>
<figure class="width-70"><div class="figure" id="fig_0511-k-means_files_in_output_43_0">
<img alt="output 43 0" height="595" src="assets/output_43_0.png" width="600"/>
<h6><span class="label">Figure 47-10. </span>Confusion matrix for the <span class="roman">k</span>-means classifier</h6>
</div></figure>
<p>As we might expect from the cluster centers we visualized before, the
main point of confusion is between the eights and ones. But this still
shows that using <em>k</em>-means, we can essentially build a digit classifier
<em>without reference to any known labels</em>!</p>
<p><a data-primary="t-distributed stochastic neighbor embedding (t-SNE)" data-type="indexterm" id="idm45858722949904"/>Just for fun, let’s try to push this even farther. We can
use the t-distributed stochastic neighbor embedding algorithm (mentioned
in <a data-type="xref" href="ch46.xhtml#section-0510-manifold-learning">Chapter 46</a>) to
preprocess the data before performing <em>k</em>-means. t-SNE is a nonlinear
embedding algorithm that is particularly adept at preserving points
within clusters. Let’s see how it does:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">TSNE</code>

         <code class="c1"># Project the data: this step will take several seconds</code>
         <code class="n">tsne</code> <code class="o">=</code> <code class="n">TSNE</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="s1">'random'</code><code class="p">,</code>
                     <code class="n">learning_rate</code><code class="o">=</code><code class="s1">'auto'</code><code class="p">,</code><code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">digits_proj</code> <code class="o">=</code> <code class="n">tsne</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>

         <code class="c1"># Compute the clusters</code>
         <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">clusters</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">digits_proj</code><code class="p">)</code>

         <code class="c1"># Permute the labels</code>
         <code class="n">labels</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros_like</code><code class="p">(</code><code class="n">clusters</code><code class="p">)</code>
         <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>
             <code class="n">mask</code> <code class="o">=</code> <code class="p">(</code><code class="n">clusters</code> <code class="o">==</code> <code class="n">i</code><code class="p">)</code>
             <code class="n">labels</code><code class="p">[</code><code class="n">mask</code><code class="p">]</code> <code class="o">=</code> <code class="n">mode</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">[</code><code class="n">mask</code><code class="p">])[</code><code class="mi">0</code><code class="p">]</code>

         <code class="c1"># Compute the accuracy</code>
         <code class="n">accuracy_score</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="mf">0.9415692821368948</code></pre>
<p>That’s a 94% classification accuracy <em>without using the
labels</em>. This is the power of unsupervised learning when used carefully:
it can extract information from the dataset that it might be difficult
to extract by hand or by eye.<a data-startref="ix_ch47-asciidoc8" data-type="indexterm" id="idm45858722944384"/><a data-startref="ix_ch47-asciidoc7" data-type="indexterm" id="idm45858722876880"/></p>
</div></section>
<section data-pdf-bookmark="Example 2: k-Means for Color Compression" data-type="sect2"><div class="sect2" id="ch_0511-k-means_example-2-k-means-for-color-compression">
<h2>Example 2: k-Means for Color Compression</h2>
<p><a data-primary="color compression" data-type="indexterm" id="ix_ch47-asciidoc9"/><a data-primary="k-means clustering" data-secondary="color compression example" data-type="indexterm" id="ix_ch47-asciidoc10"/>One interesting application of clustering is in color compression within
images (this example is adapted from Scikit-Learn’s
<a href="https://oreil.ly/TwsxU">“Color
Quantization Using K-Means”</a>). For example, imagine you have an image
with millions of colors. In most images, a large number of the colors
will be unused, and many of the pixels in the image will have similar or
even identical colors.</p>
<p>For example, consider the image shown in <a data-type="xref" href="#fig_0511-k-means_files_in_output_48_0">Figure 47-11</a>, which is
from the Scikit-Learn <code>datasets</code> module (for this to work,
you’ll have to have the <code>PIL</code> Python package installed):<sup><a data-type="noteref" href="ch47.xhtml#idm45858722773584" id="idm45858722773584-marker">2</a></sup></p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="c1"># Note: this requires the PIL package to be installed</code>
         <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_sample_image</code>
         <code class="n">china</code> <code class="o">=</code> <code class="n">load_sample_image</code><code class="p">(</code><code class="s2">"china.jpg"</code><code class="p">)</code>
         <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">axes</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[])</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">china</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_48_0">
<img alt="output 48 0" height="401" src="assets/output_48_0.png" width="600"/>
<h6><span class="label">Figure 47-11. </span>The input image</h6>
</div></figure>
<p>The image itself is stored in a three-dimensional array of size
<code>(height, width, RGB)</code>, containing red/blue/green contributions as
integers from 0 to 255:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="n">china</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="p">(</code><code class="mi">427</code><code class="p">,</code> <code class="mi">640</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre>
<p>One way we can view this set of pixels is as a cloud of points in a
three-dimensional color space. We will reshape the data to
<code>[n_samples, n_features]</code> and rescale the colors so that they lie
between 0 and 1:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="n">data</code> <code class="o">=</code> <code class="n">china</code> <code class="o">/</code> <code class="mf">255.0</code>  <code class="c1"># use 0...1 scale</code>
         <code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
         <code class="n">data</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="p">(</code><code class="mi">273280</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre>
<p>We can visualize these pixels in this color space, using a subset of
10,000 pixels for efficiency (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_55_0">Figure 47-12</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">plot_pixels</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">title</code><code class="p">,</code> <code class="n">colors</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">N</code><code class="o">=</code><code class="mi">10000</code><code class="p">):</code>
             <code class="k">if</code> <code class="n">colors</code> <code class="ow">is</code> <code class="kc">None</code><code class="p">:</code>
                 <code class="n">colors</code> <code class="o">=</code> <code class="n">data</code>

             <code class="c1"># choose a random subset</code>
             <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">default_rng</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
             <code class="n">i</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])[:</code><code class="n">N</code><code class="p">]</code>
             <code class="n">colors</code> <code class="o">=</code> <code class="n">colors</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>
             <code class="n">R</code><code class="p">,</code> <code class="n">G</code><code class="p">,</code> <code class="n">B</code> <code class="o">=</code> <code class="n">data</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">T</code>

             <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">R</code><code class="p">,</code> <code class="n">G</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">colors</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'.'</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s1">'Red'</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s1">'Green'</code><code class="p">,</code> <code class="n">xlim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>

             <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">R</code><code class="p">,</code> <code class="n">B</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">colors</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'.'</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s1">'Red'</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s1">'Blue'</code><code class="p">,</code> <code class="n">xlim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>

             <code class="n">fig</code><code class="o">.</code><code class="n">suptitle</code><code class="p">(</code><code class="n">title</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">20</code><code class="p">);</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="n">plot_pixels</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="s1">'Input color space: 16 million possible colors'</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_55_0">
<img alt="output 55 0" height="253" src="assets/output_55_0.png" width="600"/>
<h6><span class="label">Figure 47-12. </span>The distribution of the pixels in RGB color space<sup><a data-type="noteref" href="ch47.xhtml#idm45858722326976" id="idm45858722326976-marker">3</a></sup></h6>
</div></figure>
<p>Now let’s reduce these 16 million colors to just 16 colors,
using a <em>k</em>-means clustering across the pixel space. Because we are
dealing with a very large dataset, we will use the mini-batch <em>k</em>-means,
which operates on subsets of the data to compute the result (shown in
<a data-type="xref" href="#fig_0511-k-means_files_in_output_57_0">Figure 47-13</a>) much more quickly than the standard <em>k</em>-means
algorithm:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">MiniBatchKMeans</code>
         <code class="n">kmeans</code> <code class="o">=</code> <code class="n">MiniBatchKMeans</code><code class="p">(</code><code class="mi">16</code><code class="p">)</code>
         <code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
         <code class="n">new_colors</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">[</code><code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">data</code><code class="p">)]</code>

         <code class="n">plot_pixels</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">colors</code><code class="o">=</code><code class="n">new_colors</code><code class="p">,</code>
                     <code class="n">title</code><code class="o">=</code><code class="s2">"Reduced color space: 16 colors"</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_57_0">
<img alt="output 57 0" height="253" src="assets/output_57_0.png" width="600"/>
<h6><span class="label">Figure 47-13. </span>16 clusters in RGB color space<sup><a data-type="noteref" href="ch47.xhtml#idm45858722264912" id="idm45858722264912-marker">4</a></sup></h6>
</div></figure>
<p>The result is a recoloring of the original pixels, where each pixel is
assigned the color of its closest cluster center. Plotting these new
colors in the image space rather than the pixel space shows us the
effect of this (see <a data-type="xref" href="#fig_0511-k-means_files_in_output_59_0">Figure 47-14</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="n">china_recolored</code> <code class="o">=</code> <code class="n">new_colors</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">china</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>

         <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">6</code><code class="p">),</code>
                                <code class="n">subplot_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[]))</code>
         <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">wspace</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">china</code><code class="p">)</code>
         <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'Original Image'</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">16</code><code class="p">)</code>
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">china_recolored</code><code class="p">)</code>
         <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'16-color Image'</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">16</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0511-k-means_files_in_output_59_0">
<img alt="output 59 0" height="207" src="assets/output_59_0.png" width="600"/>
<h6><span class="label">Figure 47-14. </span>A comparison of the full-color image (left) and the 16-color image (right)</h6>
</div></figure>
<p>Some detail is certainly lost in the rightmost panel, but the overall
image is still easily recognizable. In terms of the bytes required to
store the raw data, the image on the right achieves a compression factor
of around 1 million! Now, this kind of approach is not going to match
the fidelity of purpose-built image compression schemes like JPEG, but
the example shows the power of thinking outside of the box with
unsupervised methods like <em>k</em>-means<a data-startref="ix_ch47-asciidoc10" data-type="indexterm" id="idm45858722155184"/><a data-startref="ix_ch47-asciidoc9" data-type="indexterm" id="idm45858722154448"/>.<a data-startref="ix_ch47-asciidoc2" data-type="indexterm" id="idm45858722153648"/><a data-startref="ix_ch47-asciidoc1" data-type="indexterm" id="idm45858722152944"/><a data-startref="ix_ch47-asciidoc0" data-type="indexterm" id="idm45858722152272"/></p>
</div></section>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858724046864"><sup><a href="ch47.xhtml#idm45858724046864-marker">1</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/yo6GV">online appendix</a>.</p><p data-type="footnote" id="idm45858722773584"><sup><a href="ch47.xhtml#idm45858722773584-marker">2</a></sup> For a color version of this and following images, see the <a href="https://oreil.ly/PDSH_GitHub">online version of this book</a>.</p><p data-type="footnote" id="idm45858722326976"><sup><a href="ch47.xhtml#idm45858722326976-marker">3</a></sup> A full-size version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p><p data-type="footnote" id="idm45858722264912"><sup><a href="ch47.xhtml#idm45858722264912-marker">4</a></sup> A full-size version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p></div></div></section></div></body></html>