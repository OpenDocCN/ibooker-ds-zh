["```py\nimport math\n\ndef predict_probability(x, b0, b1):\n    p = 1.0 / (1.0 + math.exp(-(b0 + b1 * x)))\n    return p\n```", "```py\nfrom sympy import *\nb0, b1, x = symbols('b0 b1 x')\n\np = 1.0 / (1.0 + exp(-(b0 + b1 * x)))\n\np = p.subs(b0,-2.823)\np = p.subs(b1, 0.620)\nprint(p)\n\nplot(p)\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the data\ndf = pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# Perform logistic regression\n# Turn off penalty\nmodel = LogisticRegression(penalty='none')\nmodel.fit(X, Y)\n\n# print beta1\nprint(model.coef_.flatten()) # 0.69267212\n\n# print beta0\nprint(model.intercept_.flatten()) # -3.17576395\n```", "```py\nimport math\nimport pandas as pd\n\npatient_data = pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\").itertuples()\n\nb0 = -3.17576395\nb1 = 0.69267212\n\ndef logistic_function(x):\n    p = 1.0 / (1.0 + math.exp(-(b0 + b1 * x)))\n    return p\n\n# Calculate the joint likelihood\njoint_likelihood = 1.0\n\nfor p in patient_data:\n    if p.y == 1.0:\n        joint_likelihood *= logistic_function(p.x)\n    elif p.y == 0.0:\n        joint_likelihood *= (1.0 - logistic_function(p.x))\n\nprint(joint_likelihood) # 4.7911180221699105e-05\n```", "```py\nfor p in patient_data:\n    joint_likelihood *= logistic_function(p.x) ** p.y * \\\n                        (1.0 - logistic_function(p.x)) ** (1.0 - p.y)\n```", "```py\n# Calculate the joint likelihood\njoint_likelihood = 0.0\n\nfor p in patient_data:\n    joint_likelihood += math.log(logistic_function(p.x) ** p.y * \\\n                                 (1.0 - logistic_function(p.x)) ** (1.0 - p.y))\n\njoint_likelihood = math.exp(joint_likelihood)\n```", "```py\njoint_likelihood = Sum(log((1.0 / (1.0 + exp(-(b + m * x(i)))))**y(i) * \\\n\t(1.0 - (1.0 / (1.0 + exp(-(b + m * x(i))))))**(1-y(i))), (i, 0, n))\n```", "```py\nfrom sympy import *\nimport pandas as pd\n\npoints = list(pd.read_csv(\"https://tinyurl.com/y2cocoo7\").itertuples())\n\nb1, b0, i, n = symbols('b1 b0 i n')\nx, y = symbols('x y', cls=Function)\njoint_likelihood = Sum(log((1.0 / (1.0 + exp(-(b0 + b1 * x(i))))) ** y(i) \\\n\t* (1.0 - (1.0 / (1.0 + exp(-(b0 + b1 * x(i)))))) ** (1 - y(i))), (i, 0, n))\n\n# Partial derivative for m, with points substituted\nd_b1 = diff(joint_likelihood, b1) \\\n\t\t   .subs(n, len(points) - 1).doit() \\\n\t\t   .replace(x, lambda i: points[i].x) \\\n\t\t   .replace(y, lambda i: points[i].y)\n\n# Partial derivative for m, with points substituted\nd_b0 = diff(joint_likelihood, b0) \\\n\t\t   .subs(n, len(points) - 1).doit() \\\n\t\t   .replace(x, lambda i: points[i].x) \\\n\t\t   .replace(y, lambda i: points[i].y)\n\n# compile using lambdify for faster computation\nd_b1 = lambdify([b1, b0], d_b1)\nd_b0 = lambdify([b1, b0], d_b0)\n\n# Perform Gradient Descent\nb1 = 0.01\nb0 = 0.01\nL = .01\n\nfor j in range(10_000):\n    b1 += d_b1(b1, b0) * L\n    b0 += d_b0(b1, b0) * L\n\nprint(b1, b0)\n# 0.6926693075370812 -3.175751550409821\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\nemployee_data = pd.read_csv(\"https://tinyurl.com/y6r7qjrp\")\n\n# grab independent variable columns\ninputs = employee_data.iloc[:, :-1]\n\n# grab dependent \"did_quit\" variable column\noutput = employee_data.iloc[:, -1]\n\n# build logistic regression\nfit = LogisticRegression(penalty='none').fit(inputs, output)\n\n# Print coefficients:\nprint(\"COEFFICIENTS: {0}\".format(fit.coef_.flatten()))\nprint(\"INTERCEPT: {0}\".format(fit.intercept_.flatten()))\n\n# Interact and test with new employee data\ndef predict_employee_will_stay(sex, age, promotions, years_employed):\n    prediction = fit.predict([[sex, age, promotions, years_employed]])\n    probabilities = fit.predict_proba([[sex, age, promotions, years_employed]])\n    if prediction == [[1]]:\n        return \"WILL LEAVE: {0}\".format(probabilities)\n    else:\n        return \"WILL STAY: {0}\".format(probabilities)\n\n# Test a prediction\nwhile True:\n    n = input(\"Predict employee will stay or leave {sex},\n        {age},{promotions},{years employed}: \")\n    (sex, age, promotions, years_employed) = n.split(\",\")\n    print(predict_employee_will_stay(int(sex), int(age), int(promotions),\n          int(years_employed)))\n```", "```py\nfrom math import log, exp\nimport pandas as pd\n\npatient_data = pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\").itertuples()\n\nb0 = -3.17576395\nb1 = 0.69267212\n\ndef logistic_function(x):\n    p = 1.0 / (1.0 + exp(-(b0 + b1 * x)))\n    return p\n\n# Sum the log-likelihoods\nlog_likelihood_fit = 0.0\n\nfor p in patient_data:\n    if p.y == 1.0:\n        log_likelihood_fit += log(logistic_function(p.x))\n    elif p.y == 0.0:\n        log_likelihood_fit += log(1.0 - logistic_function(p.x))\n\nprint(log_likelihood_fit) # -9.946161673231583\n```", "```py\nlog_likelihood_fit = sum(log(logistic_function(p.x)) * p.y +\n                         log(1.0 - logistic_function(p.x)) * (1.0 - p.y)\n                         for p in patient_data)\n```", "```py\nimport pandas as pd\nfrom math import log, exp\n\npatient_data = list(pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\") \\\n     .itertuples())\n\nlikelihood = sum(p.y for p in patient_data) / len(patient_data)\n\nlog_likelihood = 0.0\n\nfor p in patient_data:\n    if p.y == 1.0:\n        log_likelihood += log(likelihood)\n    elif p.y == 0.0:\n        log_likelihood += log(1.0 - likelihood)\n\nprint(log_likelihood) # -14.341070198709906\n```", "```py\nlog_likelihood = sum(log(likelihood)*p.y + log(1.0 - likelihood)*(1.0 - p.y) \\\n\tfor p in patient_data)\n```", "```py\nimport pandas as pd\nfrom math import log, exp\n\npatient_data = list(pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\") \\\n                                .itertuples())\n\n# Declare fitted logistic regression\nb0 = -3.17576395\nb1 = 0.69267212\n\ndef logistic_function(x):\n    p = 1.0 / (1.0 + exp(-(b0 + b1 * x)))\n    return p\n\n# calculate the log likelihood of the fit\nlog_likelihood_fit = sum(log(logistic_function(p.x)) * p.y +\n                         log(1.0 - logistic_function(p.x)) * (1.0 - p.y)\n                         for p in patient_data)\n\n# calculate the log likelihood without fit\nlikelihood = sum(p.y for p in patient_data) / len(patient_data)\n\nlog_likelihood = sum(log(likelihood) * p.y + log(1.0 - likelihood) * (1.0 - p.y) \\\n\tfor p in patient_data)\n\n# calculate R-Square\nr2 = (log_likelihood - log_likelihood_fit) / log_likelihood\n\nprint(r2)  # 0.306456105756576\n```", "```py\nimport pandas as pd\nfrom math import log, exp\nfrom scipy.stats import chi2\n\npatient_data = list(pd.read_csv('https://bit.ly/33ebs2R', delimiter=\",\").itertuples())\n\n# Declare fitted logistic regression\nb0 = -3.17576395\nb1 = 0.69267212\n\ndef logistic_function(x):\n    p = 1.0 / (1.0 + exp(-(b0 + b1 * x)))\n    return p\n\n# calculate the log likelihood of the fit\nlog_likelihood_fit = sum(log(logistic_function(p.x)) * p.y +\n                         log(1.0 - logistic_function(p.x)) * (1.0 - p.y)\n                         for p in patient_data)\n\n# calculate the log likelihood without fit\nlikelihood = sum(p.y for p in patient_data) / len(patient_data)\n\nlog_likelihood = sum(log(likelihood) * p.y + log(1.0 - likelihood) * (1.0 - p.y) \\\n                     for p in patient_data)\n\n# calculate p-value\nchi2_input = 2 * (log_likelihood_fit - log_likelihood)\np_value = chi2.pdf(chi2_input, 1) # 1 degree of freedom (n - 1)\n\nprint(p_value)  # 0.0016604875618753787\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Load the data\ndf = pd.read_csv(\"https://tinyurl.com/y6r7qjrp\", delimiter=\",\")\n\nX = df.values[:, :-1]\nY = df.values[:, -1]\n\n# \"random_state\" is the random seed, which we fix to 7\nkfold = KFold(n_splits=3, random_state=7, shuffle=True)\nmodel = LogisticRegression(penalty='none')\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(\"Accuracy Mean: %.3f (stdev=%.3f)\" % (results.mean(), results.std()))\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndf = pd.read_csv('https://bit.ly/3cManTi', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\\\nY = df.values[:, -1]\n\nmodel = LogisticRegression(solver='liblinear')\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33,\n    random_state=10)\nmodel.fit(X_train, Y_train)\nprediction = model.predict(X_test)\n\n\"\"\"\nThe confusion matrix evaluates accuracy within each category.\n[[truepositives falsenegatives]\n [falsepositives truenegatives]]\n\nThe diagonal represents correct predictions,\nso we want those to be higher\n\"\"\"\nmatrix = confusion_matrix(y_true=Y_test, y_pred=prediction)\nprint(matrix)\n```", "```py\n# put Scikit_learn model here\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc')\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n# AUC: 0.791 (0.051)\n```", "```py\nX, Y = ...\nX_train, X_test, Y_train, Y_test =  \\\n\ttrain_test_split(X, Y, test_size=.33, stratify=Y)\n```"]