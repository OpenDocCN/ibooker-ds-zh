- en: 'Chapter 47\. In Depth: k-Means Clustering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters we explored unsupervised machine learning models for
    dimensionality reduction. Now we will move on to another class of unsupervised
    machine learning models: clustering algorithms. Clustering algorithms seek to
    learn, from the properties of the data, an optimal division or discrete labeling
    of groups of points.'
  prefs: []
  type: TYPE_NORMAL
- en: Many clustering algorithms are available in Scikit-Learn and elsewhere, but
    perhaps the simplest to understand is an algorithm known as *k-means clustering*,
    which is implemented in `sklearn.cluster.KMeans`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Introducing k-Means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *k*-means algorithm searches for a predetermined number of clusters within
    an unlabeled multidimensional dataset. It accomplishes this using a simple conception
    of what the optimal clustering looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: The *cluster center* is the arithmetic mean of all the points belonging to the
    cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each point is closer to its own cluster center than to other cluster centers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Those two assumptions are the basis of the *k*-means model. We will soon dive
    into exactly *how* the algorithm reaches this solution, but for now let’s take
    a look at a simple dataset and see the *k*-means result.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s generate a two-dimensional dataset containing four distinct blobs.
    To emphasize that this is an unsupervised algorithm, we will leave the labels
    out of the visualization (see [Figure 47-1](#fig_0511-k-means_files_in_output_5_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![output 5 0](assets/output_5_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-1\. Data for demonstration of clustering
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By eye, it is relatively easy to pick out the four clusters. The *k*-means
    algorithm does this automatically, and in Scikit-Learn uses the typical estimator
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s visualize the results by plotting the data colored by these labels ([Figure 47-2](#fig_0511-k-means_files_in_output_9_0)).
    We will also plot the cluster centers as determined by the *k*-means estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The good news is that the *k*-means algorithm (at least in this simple case)
    assigns the points to clusters very similarly to how we might assign them by eye.
    But you might wonder how this algorithm finds these clusters so quickly: after
    all, the number of possible combinations of cluster assignments is exponential
    in the number of data points—an exhaustive search would be very, very costly.
    Fortunately for us, such an exhaustive search is not necessary: instead, the typical
    approach to *k*-means involves an intuitive iterative approach known as *expectation–maximization*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![output 9 0](assets/output_9_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-2\. k-means cluster centers with clusters indicated by color
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Expectation–Maximization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety
    of contexts within data science. *k*-means is a particularly simple and easy-to-understand
    application of the algorithm; we’ll walk through it briefly here. In short, the
    expectation–maximization approach here consists of the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Guess some cluster centers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat until converged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*E-step*: Assign points to the nearest cluster center.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*M-step*: Set the cluster centers to the mean of their assigned points.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Here the *E-step* or *expectation step* is so named because it involves updating
    our expectation of which cluster each point belongs to. The *M-step* or *maximization
    step* is so named because it involves maximizing some fitness function that defines
    the locations of the cluster centers—in this case, that maximization is accomplished
    by taking a simple mean of the data in each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The literature about this algorithm is vast, but can be summarized as follows:
    under typical circumstances, each repetition of the E-step and M-step will always
    result in a better estimate of the cluster characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the algorithm as shown in [Figure 47-3](#fig_images_in_0511-expectation-maximization).
    For the particular initialization shown here, the clusters converge in just three
    iterations. (For an interactive version of this figure, refer to the code in the
    online [appendix](https://oreil.ly/wFnok).)
  prefs: []
  type: TYPE_NORMAL
- en: '![05.11 expectation maximization](assets/05.11-expectation-maximization.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-3\. Visualization of the E–M algorithm for k-means^([1](ch47.xhtml#idm45858724046864))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *k*-means algorithm is simple enough that we can write it in a few lines
    of code. The following is a very basic implementation (see [Figure 47-4](#fig_0511-k-means_files_in_output_15_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![output 15 0](assets/output_15_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-4\. Data labeled with k-means
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Most well-tested implementations will do a bit more than this under the hood,
    but the preceding function gives the gist of the expectation–maximization approach.
    There are a few caveats to be aware of when using the expectation–maximization
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: The globally optimal result may not be achieved
  prefs: []
  type: TYPE_NORMAL
- en: First, although the E–M procedure is guaranteed to improve the result in each
    step, there is no assurance that it will lead to the *global* best solution. For
    example, if we use a different random seed in our simple procedure, the particular
    starting guesses lead to poor results (see [Figure 47-5](#fig_0511-k-means_files_in_output_19_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![output 19 0](assets/output_19_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-5\. An example of poor convergence in k-means
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here the E–M approach has converged, but has not converged to a globally optimal
    configuration. For this reason, it is common for the algorithm to be run for multiple
    starting guesses, as indeed Scikit-Learn does by default (the number is set by
    the `n_init` parameter, which defaults to 10).
  prefs: []
  type: TYPE_NORMAL
- en: The number of clusters must be selected beforehand
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common challenge with *k*-means is that you must tell it how many clusters
    you expect: it cannot learn the number of clusters from the data. For example,
    if we ask the algorithm to identify six clusters, it will happily proceed and
    find the best six clusters, as shown in [Figure 40-1](ch40.xhtml#fig_0504-feature-engineering_files_in_output_24_0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![output 22 0](assets/output_22_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-6\. An example where the number of clusters is chosen poorly
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Whether the result is meaningful is a question that is difficult to answer definitively;
    one approach that is rather intuitive, but that we won’t discuss further here,
    is called [silhouette analysis](https://oreil.ly/xybmq).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you might use a more complicated clustering algorithm that has
    a better quantitative measure of the fitness per number of clusters (e.g., Gaussian
    mixture models; see [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures)) or
    which *can* choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or
    affinity propagation, all available in the `sklearn.cluster` submodule).
  prefs: []
  type: TYPE_NORMAL
- en: k-means is limited to linear cluster boundaries
  prefs: []
  type: TYPE_NORMAL
- en: The fundamental model assumptions of *k*-means (points will be closer to their
    own cluster center than to others) means that the algorithm will often be ineffective
    if the clusters have complicated geometries.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the boundaries between *k*-means clusters will always be linear,
    which means that it will fail for more complicated boundaries. Consider the following
    data, along with the cluster labels found by the typical *k*-means approach (see
    [Figure 47-7](#fig_0511-k-means_files_in_output_26_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![output 26 0](assets/output_26_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-7\. Failure of k-means with nonlinear boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This situation is reminiscent of the discussion in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines),
    where we used a kernel transformation to project the data into a higher dimension
    where a linear separation is possible. We might imagine using the same trick to
    allow *k*-means to discover non-linear boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: One version of this kernelized *k*-means is implemented in Scikit-Learn within
    the `SpectralClustering` estimator. It uses the graph of nearest neighbors to
    compute a higher-dimensional representation of the data, and then assigns labels
    using a *k*-means algorithm (see [Figure 47-8](#fig_0511-k-means_files_in_output_28_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![output 28 0](assets/output_28_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-8\. Nonlinear boundaries learned by SpectralClustering
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We see that with this kernel transform approach, the kernelized *k*-means is
    able to find the more complicated nonlinear boundaries between clusters.
  prefs: []
  type: TYPE_NORMAL
- en: k-means can be slow for large numbers of samples
  prefs: []
  type: TYPE_NORMAL
- en: Because each iteration of *k*-means must access every point in the dataset,
    the algorithm can be relatively slow as the number of samples grows. You might
    wonder if this requirement to use all data at each iteration can be relaxed; for
    example, you might just use a subset of the data to update the cluster centers
    at each step. This is the idea behind batch-based *k*-means algorithms, one form
    of which is implemented in `sklearn.cluster.MiniBatchKMeans`. The interface for
    this is the same as for standard `KMeans`; we will see an example of its use as
    we continue our discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Being careful about these limitations of the algorithm, we can use *k*-means
    to our advantage in a variety of situations. We’ll now take a look at a couple
    of examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: k-Means on Digits'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start, let’s take a look at applying *k*-means on the same simple digits
    data that we saw in Chapters [44](ch44.xhtml#section-0508-random-forests) and
    [45](ch45.xhtml#section-0509-principal-component-analysis). Here we will attempt
    to use *k*-means to try to identify similar digits *without using the original
    label information*; this might be similar to a first step in extracting meaning
    from a new dataset about which you don’t have any *a priori* label information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the dataset, then find the clusters. Recall that the
    digits dataset consists of 1,797 samples with 64 features, where each of the 64
    features is the brightness of one pixel in an 8 × 8 image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The clustering can be performed as we did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The result is 10 clusters in 64 dimensions. Notice that the cluster centers
    themselves are 64-dimensional points, and can be interpreted as representing the
    “typical” digit within the cluster. Let’s see what these cluster centers look
    like (see [Figure 47-9](#fig_0511-k-means_files_in_output_37_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 37 0](assets/output_37_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-9\. Cluster centers learned by k-means
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We see that *even without the labels*, `KMeans` is able to find clusters whose
    centers are recognizable digits, with perhaps the exception of 1 and 8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because *k*-means knows nothing about the identities of the clusters, the 0–9
    labels may be permuted. We can fix this by matching each learned cluster label
    with the true labels found in the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can check how accurate our unsupervised clustering was in finding similar
    digits within the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With just a simple *k*-means algorithm, we discovered the correct grouping for
    80% of the input digits! Let’s check the confusion matrix for this, visualized
    in [Figure 47-10](#fig_0511-k-means_files_in_output_43_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![output 43 0](assets/output_43_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-10\. Confusion matrix for the k-means classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we might expect from the cluster centers we visualized before, the main point
    of confusion is between the eights and ones. But this still shows that using *k*-means,
    we can essentially build a digit classifier *without reference to any known labels*!
  prefs: []
  type: TYPE_NORMAL
- en: 'Just for fun, let’s try to push this even farther. We can use the t-distributed
    stochastic neighbor embedding algorithm (mentioned in [Chapter 46](ch46.xhtml#section-0510-manifold-learning))
    to preprocess the data before performing *k*-means. t-SNE is a nonlinear embedding
    algorithm that is particularly adept at preserving points within clusters. Let’s
    see how it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s a 94% classification accuracy *without using the labels*. This is the
    power of unsupervised learning when used carefully: it can extract information
    from the dataset that it might be difficult to extract by hand or by eye.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: k-Means for Color Compression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One interesting application of clustering is in color compression within images
    (this example is adapted from Scikit-Learn’s [“Color Quantization Using K-Means”](https://oreil.ly/TwsxU)).
    For example, imagine you have an image with millions of colors. In most images,
    a large number of the colors will be unused, and many of the pixels in the image
    will have similar or even identical colors.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the image shown in [Figure 47-11](#fig_0511-k-means_files_in_output_48_0),
    which is from the Scikit-Learn `datasets` module (for this to work, you’ll have
    to have the `PIL` Python package installed):^([2](ch47.xhtml#idm45858722773584))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![output 48 0](assets/output_48_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-11\. The input image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The image itself is stored in a three-dimensional array of size `(height, width,
    RGB)`, containing red/blue/green contributions as integers from 0 to 255:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'One way we can view this set of pixels is as a cloud of points in a three-dimensional
    color space. We will reshape the data to `[n_samples, n_features]` and rescale
    the colors so that they lie between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We can visualize these pixels in this color space, using a subset of 10,000
    pixels for efficiency (see [Figure 47-12](#fig_0511-k-means_files_in_output_55_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![output 55 0](assets/output_55_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-12\. The distribution of the pixels in RGB color space^([3](ch47.xhtml#idm45858722326976))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let’s reduce these 16 million colors to just 16 colors, using a *k*-means
    clustering across the pixel space. Because we are dealing with a very large dataset,
    we will use the mini-batch *k*-means, which operates on subsets of the data to
    compute the result (shown in [Figure 47-13](#fig_0511-k-means_files_in_output_57_0))
    much more quickly than the standard *k*-means algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![output 57 0](assets/output_57_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-13\. 16 clusters in RGB color space^([4](ch47.xhtml#idm45858722264912))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The result is a recoloring of the original pixels, where each pixel is assigned
    the color of its closest cluster center. Plotting these new colors in the image
    space rather than the pixel space shows us the effect of this (see [Figure 47-14](#fig_0511-k-means_files_in_output_59_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![output 59 0](assets/output_59_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 47-14\. A comparison of the full-color image (left) and the 16-color
    image (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some detail is certainly lost in the rightmost panel, but the overall image
    is still easily recognizable. In terms of the bytes required to store the raw
    data, the image on the right achieves a compression factor of around 1 million!
    Now, this kind of approach is not going to match the fidelity of purpose-built
    image compression schemes like JPEG, but the example shows the power of thinking
    outside of the box with unsupervised methods like *k*-means.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch47.xhtml#idm45858724046864-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/yo6GV).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch47.xhtml#idm45858722773584-marker)) For a color version of this and
    following images, see the [online version of this book](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch47.xhtml#idm45858722326976-marker)) A full-size version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch47.xhtml#idm45858722264912-marker)) A full-size version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
