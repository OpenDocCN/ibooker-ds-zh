- en: Chapter 19\. Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A little learning is a dangerous thing; Drink deep, or taste not the Pierian
    spring.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alexander Pope
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Deep learning* originally referred to the application of “deep” neural networks
    (that is, networks with more than one hidden layer), although in practice the
    term now encompasses a wide variety of neural architectures (including the “simple”
    neural networks we developed in [Chapter 18](ch18.html#neural_networks)).'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll build on our previous work and look at a wider variety
    of neural networks. To do so, we’ll introduce a number of abstractions that allow
    us to think about neural networks in a more general way.
  prefs: []
  type: TYPE_NORMAL
- en: The Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we made a distinction between vectors (one-dimensional arrays) and
    matrices (two-dimensional arrays). When we start working with more complicated
    neural networks, we’ll need to use higher-dimensional arrays as well.
  prefs: []
  type: TYPE_NORMAL
- en: In many neural network libraries, *n*-dimensional arrays are referred to as
    *tensors*, which is what we’ll call them too. (There are pedantic mathematical
    reasons not to refer to *n*-dimensional arrays as tensors; if you are such a pedant,
    your objection is noted.)
  prefs: []
  type: TYPE_NORMAL
- en: If I were writing an entire book about deep learning, I’d implement a full-featured
    `Tensor` class that overloaded Python’s arithmetic operators and could handle
    a variety of other operations. Such an implementation would take an entire chapter
    on its own. Here we’ll cheat and say that a `Tensor` is just a `list`. This is
    true in one direction—all of our vectors and matrices and higher-dimensional analogues
    *are* lists. It is certainly not true in the other direction—most Python `list`s
    are not *n*-dimensional arrays in our sense.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Ideally you’d like to do something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'However, Python won’t let you define recursive types like that. And even if
    it did that definition is still not right, as it allows for bad “tensors” like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: whose rows have different sizes, which makes it not an *n*-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, like I said, we’ll just cheat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And we’ll write a helper function to find a tensor’s *shape*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Because tensors can have any number of dimensions, we’ll typically need to
    work with them recursively. We’ll do one thing in the one-dimensional case and
    recurse in the higher-dimensional case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'which we can use to write a recursive `tensor_sum` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re not used to thinking recursively, you should ponder this until it
    makes sense, because we’ll use the same logic throughout this chapter. However,
    we’ll create a couple of helper functions so that we don’t have to rewrite this
    logic everywhere. The first applies a function elementwise to a single tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this to write a function that creates a zero tensor with the same
    shape as a given tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll also need to apply a function to corresponding elements from two tensors
    (which had better be the exact same shape, although we won’t check that):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The Layer Abstraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter we built a simple neural net that allowed us to stack
    two layers of neurons, each of which computed `sigmoid(dot(weights, inputs))`.
  prefs: []
  type: TYPE_NORMAL
- en: Although that’s perhaps an idealized representation of what an actual neuron
    does, in practice we’d like to allow a wider variety of things. Perhaps we’d like
    the neurons to remember something about their previous inputs. Perhaps we’d like
    to use a different activation function than `sigmoid`. And frequently we’d like
    to use more than two layers. (Our `feed_forward` function actually handled any
    number of layers, but our gradient computations did not.)
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll build machinery for implementing such a variety of neural
    networks. Our fundamental abstraction will be the `Layer`, something that knows
    how to apply some function to its inputs and that knows how to backpropagate gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of thinking about the neural networks we built in [Chapter 18](ch18.html#neural_networks)
    is as a “linear” layer, followed by a “sigmoid” layer, then another linear layer
    and another sigmoid layer. We didn’t distinguish them in these terms, but doing
    so will allow us to experiment with much more general structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `forward` and `backward` methods will have to be implemented in our concrete
    subclasses. Once we build a neural net, we’ll want to train it using gradient
    descent, which means we’ll want to update each parameter in the network using
    its gradient. Accordingly, we insist that each layer be able to tell us its parameters
    and gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Some layers (for example, a layer that applies `sigmoid` to each of its inputs)
    have no parameters to update, so we provide a default implementation that handles
    that case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at that layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There are a couple of things to notice here. One is that during the forward
    pass we saved the computed sigmoids so that we could use them later in the backward
    pass. Our layers will typically need to do this sort of thing.
  prefs: []
  type: TYPE_NORMAL
- en: Second, you may be wondering where the `sig * (1 - sig) * grad` comes from.
    This is just the chain rule from calculus and corresponds to the `output * (1
    - output) * (output - target)` term in our previous neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you can see how we were able to make use of the `tensor_apply` and
    the `tensor_combine` functions. Most of our layers will use these functions similarly.
  prefs: []
  type: TYPE_NORMAL
- en: The Linear Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other piece we’ll need to duplicate the neural networks from [Chapter 18](ch18.html#neural_networks)
    is a “linear” layer that represents the `dot(weights, inputs)` part of the neurons.
  prefs: []
  type: TYPE_NORMAL
- en: This layer will have parameters, which we’d like to initialize with random values.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the initial parameter values can make a huge difference in
    how quickly (and sometimes *whether*) the network trains. If weights are too big,
    they may produce large outputs in a range where the activation function has near-zero
    gradients. And parts of the network that have zero gradients necessarily can’t
    learn anything via gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, we’ll implement three different schemes for randomly generating
    our weight tensors. The first is to choose each value from the random uniform
    distribution on [0, 1]—that is, as a `random.random()`. The second (and default)
    is to choose each value randomly from a standard normal distribution. And the
    third is to use *Xavier initialization*, where each weight is initialized with
    a random draw from a normal distribution with mean 0 and variance 2 / (`num_inputs`
    + `num_outputs`). It turns out this often works nicely for neural network weights.
    We’ll implement these with a `random_uniform` function and a `random_normal` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And then wrap them all in a `random_tensor` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can define our linear layer. We need to initialize it with the dimension
    of the inputs (which tells us how many weights each neuron needs), the dimension
    of the outputs (which tells us how many neurons we should have), and the initialization
    scheme we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In case you’re wondering how important the initialization schemes are, some
    of the networks in this chapter I couldn’t get to train at all with different
    initializations than the ones I used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `forward` method is easy to implement. We’ll get one output per neuron,
    which we stick in a vector. And each neuron’s output is just the `dot` of its
    weights with the input, plus its bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `backward` method is more involved, but if you know calculus it’s not difficult:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a “real” tensor library, these (and many other) operations would be represented
    as matrix or tensor multiplications, which those libraries are designed to do
    very quickly. Our library is *very* slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, here we do need to implement `params` and `grads`. We have two parameters
    and two corresponding gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Neural Networks as a Sequence of Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’d like to think of neural networks as sequences of layers, so let’s come
    up with a way to combine multiple layers into one. The resulting neural network
    is itself a layer, and it implements the `Layer` methods in the obvious ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'So we could represent the neural network we used for XOR as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: But we still need a little more machinery to train it.
  prefs: []
  type: TYPE_NORMAL
- en: Loss and Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously we wrote out individual loss functions and gradient functions for
    our models. Here we’ll want to experiment with different loss functions, so (as
    usual) we’ll introduce a new `Loss` abstraction that encapsulates both the loss
    computation and the gradient computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve already worked many times with the loss that’s the sum of the squared
    errors, so we should have an easy time implementing that. The only trick is that
    we’ll need to use `tensor_combine`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: (We’ll look at a different loss function in a bit.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The last piece to figure out is gradient descent. Throughout the book we’ve
    done all of our gradient descent manually by having a training loop that involves
    something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Here that won’t quite work for us, for a couple reasons. The first is that our
    neural nets will have many parameters, and we’ll need to update all of them. The
    second is that we’d like to be able to use more clever variants of gradient descent,
    and we don’t want to have to rewrite them each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accordingly, we’ll introduce a (you guessed it) `Optimizer` abstraction, of
    which gradient descent will be a specific instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After that it’s easy to implement gradient descent, again using `tensor_combine`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The only thing that’s maybe surprising is the “slice assignment,” which is a
    reflection of the fact that reassigning a list doesn’t change its original value.
    That is, if you just did `param = tensor_combine(. . .)`, you would be redefining
    the local variable `param`, but you would not be affecting the original parameter
    tensor stored in the layer. If you assign to the slice `[:]`, however, it actually
    changes the values inside the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example to demonstrate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you are somewhat inexperienced in Python, this behavior may be surprising,
    so meditate on it and try examples yourself until it makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate the value of this abstraction, let’s implement another optimizer
    that uses *momentum*. The idea is that we don’t want to overreact to each new
    gradient, and so we maintain a running average of the gradients we’ve seen, updating
    it with each new gradient and taking a step in the direction of the average:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Because we used an `Optimizer` abstraction, we can easily switch between our
    different optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: XOR Revisited'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s see how easy it is to use our new framework to train a network that can
    compute XOR. We start by re-creating the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'and then we define the network, although now we can leave off the last sigmoid
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now write a simple training loop, except that now we can use the abstractions
    of `Optimizer` and `Loss`. This allows us to easily try different ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This should train quickly, and you should see the loss go down. And now we
    can inspect the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For my network I find roughly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: So `hidden1` activates if neither input is 1\. `hidden2` activates if both inputs
    are 1. And `output` activates if neither hidden output is 1—that is, if it’s not
    the case that neither input is 1 and it’s also not the case that both inputs are
    1\. Indeed, this is exactly the logic of XOR.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this network learned different features than the one we trained
    in [Chapter 18](ch18.html#neural_networks), but it still manages to do the same
    thing.
  prefs: []
  type: TYPE_NORMAL
- en: Other Activation Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sigmoid` function has fallen out of favor for a couple of reasons. One
    reason is that `sigmoid(0)` equals 1/2, which means that a neuron whose inputs
    sum to 0 has a positive output. Another is that its gradient is very close to
    0 for very large and very small inputs, which means that its gradients can get
    “saturated” and its weights can get stuck.
  prefs: []
  type: TYPE_NORMAL
- en: 'One popular replacement is `tanh` (“hyperbolic tangent”), which is a different
    sigmoid-shaped function that ranges from –1 to 1 and outputs 0 if its input is
    0\. The derivative of `tanh(x)` is just `1 - tanh(x) ** 2`, which makes the layer
    easy to write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In larger networks another popular replacement is `Relu`, which is 0 for negative
    inputs and the identity for positive inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: There are many others. I encourage you to play around with them in your networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: FizzBuzz Revisited'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now use our “deep learning” framework to reproduce our solution from
    [“Example: Fizz Buzz”](ch18.html#fizzbuzz). Let’s set up the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'and create the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'As we’re training, let’s also track our accuracy on the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: After 1,000 training iterations, the model gets 90% accuracy on the test set;
    if you keep training it longer, it should do even better. (I don’t think it’s
    possible to train to 100% accuracy with only 25 hidden units, but it’s definitely
    possible if you go up to 50 hidden units.)
  prefs: []
  type: TYPE_NORMAL
- en: Softmaxes and Cross-Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The neural net we used in the previous section ended in a `Sigmoid` layer, which
    means that its output was a vector of numbers between 0 and 1. In particular,
    it could output a vector that was entirely 0s, or it could output a vector that
    was entirely 1s. Yet when we’re doing classification problems, we’d like to output
    a 1 for the correct class and a 0 for all the incorrect classes. Generally our
    predictions will not be so perfect, but we’d at least like to predict an actual
    probability distribution over the classes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have two classes, and our model outputs `[0, 0]`, it’s hard
    to make much sense of that. It doesn’t think the output belongs in either class?
  prefs: []
  type: TYPE_NORMAL
- en: But if our model outputs `[0.4, 0.6]`, we can interpret it as a prediction that
    there’s a probability of 0.4 that our input belongs to the first class and 0.6
    that our input belongs to the second class.
  prefs: []
  type: TYPE_NORMAL
- en: In order to accomplish this, we typically forgo the final `Sigmoid` layer and
    instead use the `softmax` function, which converts a vector of real numbers to
    a vector of probabilities. We compute `exp(x)` for each number in the vector,
    which results in a vector of positive numbers. After that, we just divide each
    of those positive numbers by the sum, which gives us a bunch of positive numbers
    that add up to 1—that is, a vector of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we ever end up trying to compute, say, `exp(1000)` we will get a Python
    error, so before taking the `exp` we subtract off the largest value. This turns
    out to result in the same probabilities; it’s just safer to compute in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Once our network produces probabilities, we often use a different loss function
    called *cross-entropy* (or sometimes “negative log likelihood”).
  prefs: []
  type: TYPE_NORMAL
- en: You may recall that in [“Maximum Likelihood Estimation”](ch14.html#maximum_likelihood_estimation),
    we justified the use of least squares in linear regression by appealing to the
    fact that (under certain assumptions) the least squares coefficients maximized
    the likelihood of the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we can do something similar: if our network outputs are probabilities,
    the cross-entropy loss represents the negative log likelihood of the observed
    data, which means that minimizing that loss is the same as maximizing the log
    likelihood (and hence the likelihood) of the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Typically we won’t include the `softmax` function as part of the neural network
    itself. This is because it turns out that if `softmax` is part of your loss function
    but not part of the network itself, the gradients of the loss with respect to
    the network outputs are very easy to compute.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: If I now train the same Fizz Buzz network using `SoftmaxCrossEntropy` loss,
    I find that it typically trains much faster (that is, in many fewer epochs). Presumably
    this is because it is much easier to find weights that `softmax` to a given distribution
    than it is to find weights that `sigmoid` to a given distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, if I need to predict class 0 (a vector with a 1 in the first position
    and 0s in the remaining positions), in the `linear` + `sigmoid` case I need the
    first output to be a large positive number and the remaining outputs to be large
    negative numbers. In the `softmax` case, however, I just need the first output
    to be *larger than* the remaining outputs. Clearly there are a lot more ways for
    the second case to happen, which suggests that it should be easier to find weights
    that make it so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like most machine learning models, neural networks are prone to overfitting
    to their training data. We’ve previously seen ways to ameliorate this; for example,
    in [“Regularization”](ch15.html#regularization) we penalized large weights and
    that helped prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: A common way of regularizing neural networks is using *dropout*. At training
    time, we randomly turn off each neuron (that is, replace its output with 0) with
    some fixed probability. This means that the network can’t learn to depend on any
    individual neuron, which seems to help with overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'At evaluation time, we don’t want to dropout any neurons, so a `Dropout` layer
    will need to know whether it’s training or not. In addition, at training time
    a `Dropout` layer only passes on some random fraction of its input. To make its
    output comparable during evaluation, we’ll scale down the outputs (uniformly)
    using that same fraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use this to help prevent our deep learning models from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: MNIST'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits
    that everyone uses to learn deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: It is available in a somewhat tricky binary format, so we’ll install the `mnist`
    library to work with it. (Yes, this part is technically not “from scratch.”)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the first 100 training images to see what they look like ([Figure 19-1](#mnist_images)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![MNIST images](assets/dsf2_1901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19-1\. MNIST images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can see that indeed they look like handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: My first attempt at showing the images resulted in yellow numbers on black backgrounds.
    I am neither clever nor subtle enough to know that I needed to add `cmap=*Greys*`
    to get black-and-white images; I Googled it and found the solution on Stack Overflow.
    As a data scientist you will become quite adept at this workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to load the test images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Each image is 28 × 28 pixels, but our linear layers can only deal with one-dimensional
    inputs, so we’ll just flatten them (and also divide by 256 to get them between
    0 and 1). In addition, our neural net will train better if our inputs are 0 on
    average, so we’ll subtract out the average value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to one-hot-encode the targets, since we have 10 outputs. First
    let’s write a `one_hot_encode` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'and then apply it to our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: One of the strengths of our abstractions is that we can use the same training/evaluation
    loop with a variety of models. So let’s write that first. We’ll pass it our model,
    the data, a loss function, and (if we’re training) an optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'It will make a pass through our data, track performance, and (if we passed
    in an optimizer) update our parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As a baseline, we can use our deep learning library to train a (multiclass)
    logistic regression model, which is just a single linear layer followed by a softmax.
    This model (in essence) just looks for 10 linear functions such that if the input
    represents, say, a 5, then the 5th linear function produces the largest output.
  prefs: []
  type: TYPE_NORMAL
- en: 'One pass through our 60,000 training examples should be enough to learn the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This gets about 89% accuracy. Let’s see if we can do better with a deep neural
    network. We’ll use two hidden layers, the first with 30 neurons, and the second
    with 10 neurons. And we’ll use our `Tanh` activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: And we can just use the same training loop!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Our deep model gets better than 92% accuracy on the test set, which is a nice
    improvement from the simple logistic model.
  prefs: []
  type: TYPE_NORMAL
- en: The [MNIST website](http://yann.lecun.com/exdb/mnist/) describes a variety of
    models that outperform these. Many of them could be implemented using the machinery
    we’ve developed so far but would take an extremely long time to train in our lists-as-tensors
    framework. Some of the best models involve *convolutional* layers, which are important
    but unfortunately quite out of scope for an introductory book on data science.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Loading Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These models take a long time to train, so it would be nice if we could save
    them so that we don’t have to train them every time. Luckily, we can use the `json`
    module to easily serialize model weights to a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'For saving, we can use `Layer.params` to collect the weights, stick them in
    a list, and use `json.dump` to save that list to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Loading the weights back is only a little more work. We just use `json.load`
    to get the list of weights back from the file and slice assignment to set the
    weights of our model.
  prefs: []
  type: TYPE_NORMAL
- en: (In particular, this means that we have to instantiate the model ourselves and
    *then* load the weights. An alternative approach would be to also save some representation
    of the model architecture and use that to instantiate the model. That’s not a
    terrible idea, but it would require a lot more code and changes to all our `Layer`s,
    so we’ll stick with the simpler way.)
  prefs: []
  type: TYPE_NORMAL
- en: Before we load the weights, we’d like to check that they have the same shapes
    as the model params we’re loading them into. (This is a safeguard against, for
    example, trying to load the weights for a saved deep network into a shallow network,
    or similar issues.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: JSON stores your data as text, which makes it an extremely inefficient representation.
    In real applications you’d probably use the `pickle` serialization library, which
    serializes things to a more efficient binary format. Here I decided to keep it
    simple and human-readable.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the weights for the various networks we train from [the book’s
    GitHub repository](https://github.com/joelgrus/data-science-from-scratch).
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is really hot right now, and in this chapter we barely scratched
    its surface. There are many good books and blog posts (and many, many bad blog
    posts) about almost any aspect of deep learning you’d like to know about.
  prefs: []
  type: TYPE_NORMAL
- en: The canonical textbook [*Deep Learning*](https://www.deeplearningbook.org/),
    by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press), is freely available
    online. It is very good, but it involves quite a bit of mathematics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Francois Chollet’s [*Deep Learning with Python*](https://www.manning.com/books/deep-learning-with-python)
    (Manning) is a great introduction to the Keras library, after which our deep learning
    library is sort of patterned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I myself mostly use [PyTorch](https://pytorch.org/) for deep learning. Its website
    has lots of documentation and tutorials.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
