- en: Chapter 6\. Text Classification Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 文本分类算法
- en: 'The internet is often referred to as the great enabler: it allows us to accomplish
    a lot in our daily lives with the help of online tools and platforms. On the other
    hand, it can also be a source of information overload and endless search. Whether
    it is communicating with colleagues and customers, partners, or vendors, emails
    and other messaging tools are an inherent part of our daily work lives. Brands
    interact with customers and get valuable feedback on their products through social
    media channels like Facebook and Twitter. Software developers and product managers
    communicate using ticketing applications like [Trello](https://trello.com) to
    track development tasks, while open source communities use [GitHub](https://github.com)
    issues and [Bugzilla](https://bugzilla.org) to track software bugs that need to
    be fixed or new functionality that needs to be added.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网通常被称为巨大的促成者：它通过在线工具和平台帮助我们在日常生活中取得很多成就。另一方面，它也可能是信息超载和无休止搜索的来源。无论是与同事、客户、合作伙伴还是供应商进行沟通，电子邮件和其他消息工具都是我们日常工作生活中固有的一部分。品牌通过社交媒体平台如Facebook和Twitter与客户互动，并获取产品的宝贵反馈。软件开发者和产品经理使用类似[Trello](https://trello.com)的工单应用程序来跟踪开发任务，而开源社区则使用[GitHub](https://github.com)的问题跟踪和[Bugzilla](https://bugzilla.org)来追踪需要修复的软件缺陷或需要添加的新功能。
- en: While these tools are useful for getting work done, they can also become overwhelming
    and quickly turn into a deluge of information. A lot of emails contain promotional
    content, spam, and marketing newsletters that are often a distraction. Similarly,
    software developers can easily get buried under a mountain of bug reports and
    feature requests that take away their productivity. In order to make the best
    use of these tools, we must also use techniques to categorize, filter, and prioritize
    the more important information from the less relevant pieces, and text classification
    is one such technique that can help us achieve this.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些工具对完成工作很有用，但它们也可能变得无法控制，并迅速成为信息泛滥的源头。许多电子邮件包含推广内容、垃圾邮件和营销通讯，通常会分散注意力。同样，软件开发者很容易被大量的错误报告和功能请求淹没，这些会降低他们的生产力。为了充分利用这些工具，我们必须采用分类、过滤和优先处理更重要信息与不太相关信息的技术。文本分类是其中一种技术，可以帮助我们实现这一目标。
- en: The most common example of this is spam detection that is provided by email
    providers. In this application of text classification, every incoming email is
    analyzed to determine whether it contains meaningful and useful content or irrelevant
    information that is not useful. This allows the email application to show only
    the relevant and important emails and take away the deluge of less useful information.
    Another application is the classification of incoming customer service requests
    or software bug reports. If we are able to classify and assign them to the right
    person or department, then they will be resolved faster. There are several applications
    of text classification, and in this chapter we will develop a blueprint that can
    be applied across several of them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的例子是由电子邮件提供商提供的垃圾邮件检测。在这种文本分类应用中，每封收件箱中的电子邮件都会被分析，以确定其是否包含有意义和有用的内容，或者是否是无用的无关信息。这样一来，邮件应用程序就可以仅展示相关和重要的电子邮件，过滤掉不太有用的信息泛滥。另一个应用是分类进入的客户服务请求或软件错误报告。如果我们能够对它们进行分类并分配给正确的人员或部门，那么它们将会更快地得到解决。文本分类有多种应用场景，在本章中，我们将开发一个可以跨多个应用场景应用的蓝图。
- en: What You’ll Learn and What We’ll Build
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么以及我们将构建什么
- en: In this chapter, we will build a blueprint for text classification using a supervised
    learning technique. We will use a dataset containing bug reports of a software
    application and use the blueprint to predict the priority of these bugs and the
    specific module that a particular bug belongs to. After studying this chapter,
    you will understand how to apply supervised learning techniques, splitting the
    data into train and test parts, validating model performance using accuracy measures,
    and applying cross-validation techniques. You will also learn about different
    types of text classification such as binary and multiclass classifications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用监督学习技术构建文本分类的蓝图。我们将使用包含某个软件应用程序错误报告的数据集，并使用这个蓝图来预测这些错误的优先级及特定模块。学习完本章后，您将了解如何应用监督学习技术，将数据分为训练和测试部分，使用准确度指标验证模型性能，并应用交叉验证技术。您还将了解二元分类和多类分类等不同类型的文本分类。
- en: Introducing the Java Development Tools Bug Dataset
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 Java 开发工具 Bug 数据集
- en: Software technology products are often complex and consist of several interacting
    components. For example, let’s say you are part of a team developing an Android
    application that plays podcasts. Apart from the player itself, there can be separate
    components such as the library manager, search and discover, and so on. If a user
    reports that they are unable to play any podcasts, then it’s important to recognize
    that this is a critical bug that needs immediate attention. Another user might
    report an issue with their favorite podcast not showing up. This may not be as
    critical, but it’s important to determine whether this needs to be looked at by
    the library manager team or if it’s actually a problem for the search and discover
    team. To ensure fast response times, it’s important to classify issues accurately
    and assign them to the right team. Bugs are an inevitable part of any software
    product, but a quick response will ensure that customers will be happy and continue
    to use your product.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 软件技术产品通常很复杂，并且由几个互动组件组成。例如，假设您是一个开发 Android 应用程序播放播客的团队的一部分。除了播放器本身外，还可以有诸如库管理器、搜索和发现等单独的组件。如果用户报告无法播放任何播客，则需要意识到这是一个需要立即解决的关键
    bug。另一位用户可能会报告他们喜欢的播客未显示的问题。这可能不那么关键，但重要的是确定这是否需要由库管理团队处理，或者实际上是搜索和发现团队的问题。为了确保快速响应时间，准确分类问题并将其分配给正确的团队至关重要。Bug
    是任何软件产品不可避免的一部分，但快速响应将确保客户满意并继续使用您的产品。
- en: In this chapter, we will use blueprints to classify bugs and issues raised during
    the development of the Java Development Tools (JDT) [open source project](https://eclipse.org/jdt).
    The JDT project is a part of the Eclipse foundation, which develops the Eclipse
    integrated development environment (IDE). JDT provides all the functionality needed
    by software developers to write code using Java in the Eclipse IDE. Users of JDT
    report bugs and track issues with the tool Bugzilla, a popular open source bug
    tracking software. Bugzilla is also used by other open source projects like Firefox
    and the Eclipse Platform. A dataset containing the bugs for all these projects
    can be found on [GitHub](https://oreil.ly/giRWx), and we will use the bugs dataset
    of the JDT project.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用蓝图对 Java 开发工具（JDT）[开源项目](https://eclipse.org/jdt) 中开发期间提出的 bug 和问题进行分类。JDT
    项目是 Eclipse 基金会的一部分，该基金会开发 Eclipse 集成开发环境（IDE）。JDT 提供了开发人员使用 Eclipse IDE 编写 Java
    代码所需的所有功能。JDT 用户使用 Bugzilla 工具报告 bug 和跟踪问题，Bugzilla 是一款流行的开源 bug 跟踪软件，也被 Firefox
    和 Eclipse 平台等其他开源项目使用。包含所有这些项目的 bug 的数据集可以在 [GitHub](https://oreil.ly/giRWx) 上找到，我们将使用
    JDT 项目的 bug 数据集。
- en: 'The following section loads a *CSV* file that contains the JDT bugs dataset.
    This dataset contains 45,296 bugs and some of the available characteristics for
    each bug. We print a list of all the features reported for a bug and look at some
    of them in more detail to see what the bug reports look like:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分加载了一个包含 JDT bug 数据集的 *CSV* 文件。该数据集包含 45,296 个 bug 和每个 bug 的一些可用特征。我们列出了报告的所有特征的列表，并更详细地查看了其中一些，以了解
    bug 报告的具体内容：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Out:`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|   | Issue_id | Priority | Component | Title | Description |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|   | 问题编号 | 优先级 | 组件 | 标题 | 描述 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 38438 | 239715 | P3 | UI | No property tester for TestCaseElement for property
    projectNature | I20080613-2000; ; Not sure if this belongs to JDT/Debug or Platform/Debug.;
    ; I saw this error message several times today in my error log but Im not yet
    sure how to reproduce it.; ; -- Error Deta... |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 38438 | 239715 | P3 | UI | TestCaseElement 的属性测试器不存在 | I20080613-2000; ;
    不确定这是否属于 JDT/Debug 还是 Platform/Debug。; ; 我今天在我的错误日志中看到了这个错误消息多次，但我还不确定如何重现它。;
    ; -- 错误详细信息... |'
- en: '| 44129 | 395007 | P3 | UI | [package explorer] Refresh action not available
    on Java package folders | M3.; ; F5 (Refresh) is available as a context menu entry
    for ordinary source folders but not for Java package folders in the e4 Java Package
    explorer.; ; Please restore the 3.x functionality. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 44129 | 395007 | P3 | UI | [package explorer] Java 包文件夹上不可用的刷新操作 | M3.; ;
    对于普通源文件夹，F5（刷新）作为上下文菜单项可用，但对于 e4 Java 包资源管理器中的 Java 包文件夹则不可用。; ; 请恢复 3.x 的功能。
    |'
- en: 'Based on the details shown in the previous table, we can see that each bug
    report contains the following important features:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前表显示的细节，我们可以看到每个 bug 报告包含以下重要特征：
- en: Issue_id
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 问题编号
- en: The primary key for the issue used to track the bug.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用于跟踪 bug 的问题的主键。
- en: Priority
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级
- en: This varies from P1 (most critical) to P5 (least critical) and defines the severity
    of the bug (a categorical field).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个从 P1（最关键）到 P5（最不关键）变化，并定义了 bug 的严重程度（一个分类字段）。
- en: Component
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 组件
- en: This refers to the specific architectural part of the project where the bug
    occurs. This could be the UI, the APT, etc. (a categorical field).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这指的是项目中特定的架构部分，bug 出现的地方。这可以是 UI、APT 等（一个分类字段）。
- en: Title
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 标题
- en: This is a short summary entered by the user that briefly describes the bug (a
    full text field).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用户输入的简短摘要，简要描述 bug（一个全文字段）。
- en: Description
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 描述
- en: This is a more detailed description of the software behavior that produces the
    bug and its impact on usage (a full text field).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对产生 bug 的软件行为及其对使用影响的更详细描述（一个全文字段）。
- en: While creating the bug reports, users follow the guidelines mentioned on the
    JDT Bugzilla website. This describes what information the user needs to provide
    while raising a bug so that the developer can find a quick resolution. The website
    also includes guidelines that help the user identify what priority should be given
    for a particular bug. Our blueprint will use these bug reports to develop a supervised
    learning algorithm that can be used to automatically assign a priority to any
    bug that is raised in the future.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 bug 报告时，用户遵循 JDT Bugzilla 网站上提到的指南。这些指南描述了用户在提出 bug 时需要提供的信息，以便开发人员能够快速解决问题。该网站还包括帮助用户确定给特定
    bug 分配优先级的指南。我们的蓝图将使用这些 bug 报告开发一个监督学习算法，该算法可用于自动为未来提出的任何 bug 分配优先级。
- en: 'In the previous section, we got a high-level understanding of the dataset and
    the various features for each bug report. Let’s now explore a single bug report
    in more detail. We randomly sample a single bug (you can choose a different value
    for `random_state` to see a different bug) and transpose the results so that the
    results can be displayed with more detail. If we do not transpose, the Description
    feature would be shown in a truncated manner, whereas now we can see all the contents:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们对数据集和每个 bug 报告的各种特征有了高层次的理解。现在让我们更详细地探索单个 bug 报告。我们随机抽样一个单个 bug（您可以选择不同的
    `random_state` 值以查看不同的 bug），并转置结果，以便更详细地显示结果。如果不进行转置，描述特性将以截断的方式显示，而现在我们可以看到所有内容：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | 11811 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|   | 11811 |'
- en: '| --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Issue_id | 33113 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Issue_id | 33113 |'
- en: '| Priority | P3 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| Priority | P3 |'
- en: '| Component | Debug |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| Component | 调试 |'
- en: '| Title | Evaluating for loop suspends in URLClassLoader |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Title | 评估 URLClassLoader 中的 for 循环挂起问题 |'
- en: '| Description | Debug to a breakpoint in some HelloWorld program. In the DisplayView;
    highlight and ; Display the following code snippet:; ; for (int i = 0; i < 10;
    i++) {; System.out.println(i);; }; ; Instead of just reporting No explicit return
    value; the debugger suspends in the ; URLClassLoader; apparently trying to load
    the class int. You have hit Resume several ; more times before the evaluation
    completes. The DebugView does not indicate why it ; has stopped (the thread is
    just labeled Evaluating). This behavior does not happen if ; you turn off the
    Suspend on uncaught exceptions preference. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Description | 调试 HelloWorld 程序中断到断点。在 DisplayView 中；突出显示并；显示以下代码片段：；； for
    (int i = 0; i < 10; i++) {； System.out.println(i);； }；； 而不仅仅报告没有明确的返回值；调试器在； URLClassLoader；显然尝试加载
    int 类。您需要多次单击“继续”按钮，直到评估完成。DebugView 不显示暂停的原因（线程仅标记为“评估”）。如果关闭“挂起未捕获异常”偏好设置，此行为将不会发生。'
- en: '| Status | VERIFIED |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Status | 验证通过 |'
- en: '| Resolution | FIXED |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Resolution | 已修复 |'
- en: '| Version | 2.1 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Version | 2.1 |'
- en: '| Created_time | 2003-02-25 15:40:00 -0500 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Created_time | 2003-02-25 15:40:00 -0500 |'
- en: '| Resolved_time | 2003-03-05 17:11:17 -0500 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Resolved_time | 2003-03-05 17:11:17 -0500 |'
- en: We can see from the previous table that this bug was raised in the Debug component
    where the program would crash while evaluating a `for` loop. We can also see that
    the user has assigned a medium priority (P3) and that this bug was fixed in a
    week’s time. We can see that the reporter of this bug has followed the guidelines
    and provided a lot of information that also helps the software developer understand
    and identify the problem and provide a fix. Most software users are aware that
    the more information they provide, the easier it would be for a developer to understand
    the issue and provide a fix. Therefore, we can assume that most bug reports contain
    enough information for us to create a supervised learning model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从上表中我们可以看到，这个错误是在调试组件中引发的，程序在评估`for`循环时会崩溃。我们还可以看到，用户给了一个中等优先级（P3），这个错误在一周内被修复了。我们可以看到，这个错误的报告者遵循了指南并提供了大量信息，这也帮助软件开发人员理解和识别问题并提供修复。大多数软件用户知道，他们提供的信息越多，开发人员理解问题并提供修复就越容易。因此，我们可以假设大多数错误报告包含足够的信息，以便我们创建一个监督学习模型。
- en: The output graph describes the distribution of bug reports across different
    priorities. We can see that most bugs have been assigned a level of P3\. While
    this might be because Bugzilla assigns P3 as the default option, it is more likely
    that this reflects the natural tendency of users to pick a medium level for their
    bug reports. They believe that the bug does not have a high priority (P1) and
    at the same time do not want their bug to not be looked at all by choosing a P5\.
    This is reflected in a lot of real-world phenomena and is generally referred to
    as the normal distribution, where a lot of observations are found at the center
    or mean with fewer observations at the ends. This could be also visualized as
    a bell curve.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出图描述了不同优先级的错误报告分布情况。我们可以看到大多数错误被分配了P3级别。尽管这可能是因为Bugzilla将P3作为默认选项，但更可能的是这反映了用户在选择其错误报告的优先级时的自然倾向。他们认为该错误不具有高优先级（P1），同时又不希望他们的错误报告完全不被考虑，因此选择了P5。这在许多现实现象中都有所体现，并通常称为正态分布，其中大多数观测值位于中心或平均值处，而末端的观测值较少。这也可以被视为钟形曲线的可视化。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Out:`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_06in01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_06in01.jpg)'
- en: 'The vast difference between the number of bugs with priority P3 versus other
    priorities is a problem for building a supervised learning model and is referred
    to as *class imbalance*. Because the class P3 has an order of magnitude greater
    number of observations than the other classes, the text classification algorithm
    will have much more information on P3 bugs than the other priorities: P1, P2,
    P4, and P5\. We will see how the class imbalance of the Priority feature impacts
    our solution and also attempt to overcome it later in the blueprint. This is similar
    to learning something as a human. If you have seen more examples of one outcome,
    you will “predict” more of the same.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级为P3与其他优先级之间的巨大差异是构建监督学习模型的问题，并被称为*类别不平衡*。因为类别P3的观察数量比其他类别（P1、P2、P4和P5）大一个数量级，文本分类算法对P3错误的信息要比其他优先级（P1、P2、P4和P5）多得多：我们将看到优先级特征的类别不平衡如何影响我们的解决方案，并试图在蓝图中稍后克服这一问题。这与人类学习某些东西相似。如果你见过更多的某种结果的例子，你会更多“预测”相同的结果。
- en: 'In the following snippet, we can see how many bugs are reported against each
    component of the JDT. The UI and Core components have a much greater number of
    bugs than the Doc or APT components. This is expected since some components of
    a software system are larger and more important than others. The Doc component,
    for example, consists of the documentation section of the software and is used
    by software developers to understand the functionality but is probably not a working
    component. The Core component, on the other hand, is an important functional component
    of JDT and therefore has many more bugs assigned to it:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的片段中，我们可以看到针对JDT的每个组件报告了多少个错误。UI和核心组件比文档或APT组件报告的错误要多得多。这是预期的，因为软件系统的某些组件比其他组件更大更重要。例如，文档组件包括软件的文档部分，被软件开发人员用来理解功能，但可能不是一个工作组件。另一方面，核心组件是JDT的一个重要功能组件，因此分配给它的错误要多得多：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Out:`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Blueprint: Building a Text Classification System'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：构建文本分类系统
- en: 'We will take a step-by-step approach to building a text classification system
    and then combine all of these steps to present a unified blueprint. This *text
    classification* system falls under the broader category of *supervised learning*
    models. *Supervised learning* refers to a domain of machine learning algorithms
    that uses labeled data points as training data to learn the relationship between
    independent variables and the target variable. The process of learning the relationship
    is also referred to as *training a machine learning model*. If the target variable
    is a continuous numeric variable like distance, sales units, or transaction amounts,
    we would train a *regression* model. However, in our case, the target variable
    (Priority) is a categorical variable like the priority or component, and we will
    choose a *classification* method to train a supervised learning model. This model
    will use independent variables such as title or description to predict the priority
    or component of the bug. A supervised machine learning method aims to learn the
    mapping function from input to output variable(s), defined mathematically as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步构建文本分类系统，并将所有这些步骤结合起来，提供一个统一的蓝图。这种*文本分类*系统属于更广泛的*监督学习*模型类别。*监督学习*是指一类机器学习算法，它使用标记的数据点作为训练数据，来学习独立变量和目标变量之间的关系。学习这种关系的过程也称为*训练机器学习模型*。如果目标变量是连续的数值变量，如距离、销售单位或交易金额，我们会训练一个*回归*模型。然而，在我们的情况下，目标变量（优先级）是一个类别变量，我们将选择一个*分类*方法来训练监督学习模型。该模型将使用标题或描述等独立变量来预测错误的优先级或组件。监督机器学习方法旨在学习从输入到输出变量的映射函数，数学上定义如下：
- en: <math alttext="y equals f left-parenthesis upper X right-parenthesis"><mrow><mi>y</mi>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals f left-parenthesis upper X right-parenthesis"><mrow><mi>y</mi>
    <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math>
- en: In the preceding equation, <math alttext="y"><mi>y</mi></math> is the output
    or target variable, <math alttext="f"><mi>f</mi></math> is the mapping function,
    and <math alttext="upper X"><mi>X</mi></math> is the input variable or set of
    variables.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，<math alttext="y"><mi>y</mi></math> 是输出或目标变量，<math alttext="f"><mi>f</mi></math>
    是映射函数，<math alttext="upper X"><mi>X</mi></math> 是输入变量或一组变量。
- en: 'Since we are using data that contains the labeled target variable, this is
    referred to as *supervised learning*. [Figure 6-1](#fig-supervised-learning) illustrates
    the workflow of a supervised learning model. There are two phases of the workflow:
    the training phase and the predicting phase. The training phase starts with the
    training data that includes the training observations (which could be text data
    like bug reports) and the associated labels (which is what we would want to predict
    like priority or software component). While many features of the training observations
    could be used as is, this alone may not be enough to learn the mapping function,
    and we would like to add domain knowledge to help the model understand the relationship
    better. For example, we could add a feature that shows on which day of the week
    the bug was reported since bugs are likely to be fixed sooner if they are reported
    earlier in the week. This step is referred to as *feature engineering*, and the
    result is a set of *feature vectors* for each document. The training step of a
    supervised learning model accepts as input the feature vectors and their associated
    labels and tries to learn the mapping function. At the end of the training step,
    we have the mapping function, which is also called the trained model and can be
    used to generate predictions.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用包含标记目标变量的数据，这被称为*监督学习*。[图 6-1](#fig-supervised-learning)说明了监督学习模型的工作流程。工作流程分为两个阶段：训练阶段和预测阶段。训练阶段从包含训练观测（如错误报告）和相关标签（我们想要预测的优先级或软件组件）的训练数据开始。虽然许多训练观测的特征可以直接使用，但仅此可能不足以学习映射函数，我们希望增加领域知识以帮助模型更好地理解关系。例如，我们可以添加一个显示错误报告何时报告的特征，因为如果错误在周初报告，则很可能更快修复。这一步骤称为*特征工程*，其结果是每个文档的一组*特征向量*。监督学习模型的训练步骤接受特征向量及其相关标签作为输入，并试图学习映射函数。在训练步骤结束时，我们得到了映射函数，也称为训练模型，可以用来生成预测。
- en: During the prediction phase, the model receives a new input observation (for
    example, a bug report) and transforms the documents in the same way as applied
    during the training phase to produce the feature vectors. The new feature vectors
    are fed into the trained model to generate the prediction (for example, a bug
    priority). In this manner we have achieved an automated way of predicting a label.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测阶段，模型接收到一个新的输入观察值（例如一个错误报告），并且像在训练阶段应用的方式一样转换文档以生成特征向量。新的特征向量被馈送到训练好的模型中以生成预测结果（例如一个错误的优先级）。通过这种方式，我们实现了一种预测标签的自动化方式。
- en: '![](Images/btap_0601.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0601.jpg)'
- en: Figure 6-1\. Workflow of a supervised learning algorithm used for classification.
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 用于分类的监督学习算法工作流程。
- en: 'Text classification is an example of a supervised learning algorithm where
    we use text data and NLP techniques such as text vectorization to assign a categorical
    target variable to a given document. Classification algorithms can be characterized
    into the following categories:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是一个监督学习算法的示例，其中我们使用文本数据和文本向量化等自然语言处理技术来为给定的文档分配一个分类目标变量。分类算法可以归为以下几类：
- en: Binary classification
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类
- en: This is actually a special case of multiclass classification where an observation
    can have any one of two values (binary). For example, a given email can be marked
    as spam or not spam. But each observation will have only one label.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是多类分类的特殊情况，其中一个观察值可以有两个值中的任何一个（二元）。例如，给定的电子邮件可以标记为垃圾邮件或非垃圾邮件。但是每个观察值只会有一个标签。
- en: Multiclass classification
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类
- en: In this type of classification algorithm, each observation is associated with
    one label. For example, a bug report can have a single value of priority from
    any of the five categories P1, P2, P3, P4, or P5\. Similarly, when attempting
    to identify the software component that a bug is reported in, each bug can be
    in one of six categories (UI, Core, Debug, Text, APT, or Doc).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的分类算法中，每个观察值与一个标签相关联。例如，错误报告可以从优先级的五个类别 P1、P2、P3、P4 或 P5 中选择一个单一值。类似地，当尝试识别错误报告所在的软件组件时，每个错误可以属于六个类别之一（UI、核心、调试、文本、APT
    或 Doc）。
- en: Multilabel classification
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类
- en: In this type of classification algorithm each observation can be assigned to
    multiple labels. For example, a single news article could be tagged with multiple
    labels, such as Security, Tech, and Blockchain. Several strategies can be used
    to solve a multilabel classification problem, including the use of multiple binary
    classification models to generate the final result, but we will not cover this
    in our blueprint.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的分类算法中，每个观察值可以分配给多个标签。例如，一篇单一的新闻文章可以被标记为多个标签，如安全性、技术和区块链。可以使用多个二元分类模型来生成最终结果，但我们不会在我们的蓝图中涵盖此部分。
- en: 'Step 1: Data Preparation'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：数据准备
- en: 'Before proceeding to build the text classification model, we must perform some
    necessary preprocessing steps to clean the data and format it in a manner that
    is suitable for the application of machine learning algorithms. Since our objective
    is to identify the priority of a bug report given its title and description, we
    select only those columns that are relevant for the text classification model.
    We also remove any rows that contain empty values using the `dropna` function.
    Finally, we combine the title and description columns to create a single text
    value and apply the text cleaning blueprint from [Chapter 4](ch04.xhtml#ch-preparation)
    to remove special characters. After removing the special characters, we filter
    out those observations that have fewer than 50 characters in the text field. These
    bug reports have not been filled out correctly and contain very little description
    of the problem and are not helpful in training the model:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续构建文本分类模型之前，我们必须执行一些必要的预处理步骤来清洁数据，并以适合机器学习算法应用的方式格式化数据。由于我们的目标是根据标题和描述来识别错误报告的优先级，我们只选择与文本分类模型相关的列。我们还使用
    `dropna` 函数删除任何包含空值的行。最后，我们组合标题和描述列以创建单个文本值，并且应用 [第四章](ch04.xhtml#ch-preparation)
    中的文本清洁蓝图来删除特殊字符。在删除特殊字符后，我们过滤掉那些文本字段少于 50 个字符的观察值。这些错误报告填写不正确，并且包含的问题描述很少，对于训练模型没有帮助：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Out:`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Out:`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|   | Priority | text |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|   | 优先级 | 文本 |'
- en: '| --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 28311 | P3 | Need to re-run APT on anti-dependencies when files are generated
    If a generated file satisfies a missing type in another file we should rerun APT
    on the file which would be fixed by the new type. Currently java compilation does
    the correct thing but APT does not. Need to keep track of files with missing types
    and recompile at the end of the round if new types are generated. For good perf
    need to track the names and only compile those missing types that were generated
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 28311 | P3 | 需要在生成文件时重新运行APT反依赖项 如果生成的文件满足另一个文件中的缺失类型，我们应在该文件上重新运行APT，以修复新类型。当前的Java编译执行了正确的操作，但APT没有。需要跟踪具有缺失类型的文件，并在回合结束时重新编译生成新类型的文件。为了良好的性能，需要跟踪名称，并仅编译那些生成的缺失类型。'
- en: '| 25026 | P2 | Externalize String wizard: usability improvements M6 Test pass
    Since most of the Java developers will not be faces with the Eclipses mode I would
    move the check box down to the area of the Accessor class. Furthermore the wizard
    shouldnt provide the option if org.eclipse.osgi.util.NLS isnt present in the workspace.
    This will avoid that normal Java developers are faces with the option at all |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 25026 | P2 | 外部化字符串向导：可用性改进 M6 测试通过 由于大多数Java开发者不会面对Eclipse模式，我会将复选框移动到Accessor类的区域下方。此外，如果工作空间中不存在org.eclipse.osgi.util.NLS，向导不应提供此选项。这将避免普通Java开发者面对此选项。'
- en: We can see from the preceding summary of the text feature for two bug reports
    that our cleaning steps have removed a lot of special characters; we still have
    retained a lot of the code structure and statements that form part of the description.
    This is useful information that the model can use to understand the bug and will
    have an impact on whether it belongs to a higher priority.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从前面两个缺陷报告的文本特征总结中看到，我们的清理步骤已删除了许多特殊字符；我们仍然保留了形成描述的代码结构和语句的大部分。这是模型可以用来理解缺陷的有用信息，也会影响其是否属于更高优先级的因素。
- en: 'Step 2: Train-Test Split'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：训练-测试分离
- en: During the process of training a supervised learning model, we are attempting
    to learn a function that most closely resembles the real-world behavior. We use
    the information available in the training data to learn this function. Afterward,
    it is important to evaluate how close our learned function is to the real world,
    and we split our entire data into train and test splits to achieve this. We split
    the data, typically using a percentage, with the larger share assigned to the
    train split. For example, if we have a dataset with 100 observations and apply
    a train-test split in the ratio of 80-20, then 80 observations will become part
    of the train split and 20 observations will become part of the test split. The
    model is now trained on the train split, which uses only the 80 observations to
    learn the function. We will use the test split of 20 observations to evaluate
    the learned function. An illustration of this is shown in [Figure 6-2](#fig-train-test-split).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练监督学习模型的过程中，我们试图学习一个最接近真实世界行为的函数。我们利用训练数据中的信息来学习这个函数。随后，评估我们学到的函数与真实世界行为的接近程度至关重要，因此我们将整个数据集划分为训练集和测试集来实现这一目标。我们通常使用百分比划分数据，其中较大份额分配给训练集。例如，如果数据集有100个观测值，并且按80-20的比例进行训练-测试分离，则训练集将包含80个观测值，测试集将包含20个观测值。模型现在在训练集上进行训练，仅使用这80个观测值来学习函数。我们将使用这20个观测值的测试集来评估学习到的函数。如图[6-2](#fig-train-test-split)所示，这一过程进行了说明。
- en: 'During training phase:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段：
- en: <math alttext="y Subscript t r a i n Baseline equals upper F left-parenthesis
    upper X Subscript t r a i n Baseline right-parenthesis"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub> <mo>=</mo>
    <mi>F</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript t r a i n Baseline equals upper F left-parenthesis
    upper X Subscript t r a i n Baseline right-parenthesis"><mrow><msub><mi>y</mi>
    <mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub> <mo>=</mo>
    <mi>F</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: 'During evaluation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估过程中：
- en: <math alttext="y Subscript p r e d i c t i o n Baseline equals upper F left-parenthesis
    upper X Subscript t e s t Baseline right-parenthesis"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mi>F</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript p r e d i c t i o n Baseline equals upper F left-parenthesis
    upper X Subscript t e s t Baseline right-parenthesis"><mrow><msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mi>F</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub>
    <mo>)</mo></mrow></mrow></math>
- en: '![](Images/btap_0602.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0602.jpg)'
- en: Figure 6-2\. A train-test split in the ratio 80-20.
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2. 以80-20比例划分的训练-测试集。
- en: 'The model has seen only the 80 observations in the train split, and the learned
    function is now applied on a completely independent and unseen test split to generate
    the predictions. We know the real values of the target variable in the test split,
    and comparing these with the predictions will give us a true measure of how well
    the learned function performs and how close it is to real-world behavior:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模型仅看到训练集中的80个观测数据，并且学到的函数现在应用于完全独立和未见过的测试集上以生成预测。我们知道测试集中目标变量的真实值，并将这些与预测进行比较，以真实地评估学到的函数的表现以及它与真实世界行为的接近程度：
- en: <math alttext="a c c u r a c y equals e r r o r normal bar m e t r i c left-parenthesis
    y Subscript p r e d i c t i o n Baseline comma y Subscript t r u e Baseline right-parenthesis"><mrow><mi>a</mi>
    <mi>c</mi> <mi>c</mi> <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo>
    <mi>e</mi> <mi>r</mi> <mi>r</mi> <mi>o</mi> <mi>r</mi> <mo>_</mo> <mi>m</mi> <mi>e</mi>
    <mi>t</mi> <mi>r</mi> <mi>i</mi> <mi>c</mi> <mo>(</mo> <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub>
    <mo>,</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="a c c u r a c y equals e r r o r normal bar m e t r i c left-parenthesis
    y Subscript p r e d i c t i o n Baseline comma y Subscript t r u e Baseline right-parenthesis"><mrow><mi>a</mi>
    <mi>c</mi> <mi>c</mi> <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo>
    <mi>e</mi> <mi>r</mi> <mi>r</mi> <mi>o</mi> <mi>r</mi> <mo>_</mo> <mi>m</mi> <mi>e</mi>
    <mi>t</mi> <mi>r</mi> <mi>i</mi> <mi>c</mi> <mo>(</mo> <msub><mi>y</mi> <mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub>
    <mo>,</mo> <msub><mi>y</mi> <mrow><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi></mrow></msub>
    <mo>)</mo></mrow></math>
- en: 'Evaluating the learned model on the test split provides an unbiased estimate
    of the error of the text classification model since the observations in the test
    split have been randomly sampled from the training observations and are not part
    of the learning process. The test split will be used during model evaluation,
    and there are several metrics that can be used to measure this error, which will
    be discussed in [“Step 4: Model Evaluation”](#ch06step4modeleval).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试分割上评估学习到的模型提供了文本分类模型错误的无偏估计，因为测试分割中的观察结果是从训练观察结果中随机抽样的，不是学习过程的一部分。测试分割将在模型评估过程中使用，并且有几种可以用来衡量此错误的度量标准，这将在[“第
    4 步：模型评估”](#ch06step4modeleval)中讨论。
- en: 'We use the `sklearn.model_selection.train_test_split` function to implement
    the train-test split, and we provide 0.2 as the argument for the `test_size` (denoting
    20% of our data as our test split). In addition, we must also specify our independent
    and target variables, and the method returns to us a list of four elements; the
    first two elements are the independent variables split into train and test splits,
    and the next two elements are the target variable splits. One important argument
    of the function to note is the `random_state`. This number influences how the
    rows are sampled and therefore which set of observations goes to the train split
    and which set of observations goes to the test split. If you provide a different
    number, the 80-20 split will remain the same, but a different selection of observations
    will go to the train and test splits. It’s important to remember that to reproduce
    the same results you must choose the same value of the `random_state`. For example,
    if you want to check what happens to the model on adding a new independent variable,
    you must be able to compare the accuracy before and after adding the new variable.
    Therefore, you must use the same `random_state` so that you can determine whether
    a change occurred. The last parameter to take note of is `stratify`, which ensures
    that the distribution of the target variable is maintained in the train and test
    splits. If this is not maintained, then the training split can have a much higher
    number of observations of a certain class, which does not reflect the distribution
    in the training data and leads to the model learning an unrealistic function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`sklearn.model_selection.train_test_split`函数来实现训练-测试分割，并将`test_size`参数设为0.2（表示我们的数据的20%作为测试分割）。此外，我们还必须指定我们的自变量和目标变量，该方法会返回一个包含四个元素的列表；前两个元素是自变量拆分为训练和测试分割，后两个元素是目标变量拆分。该函数的一个重要参数是`random_state`。这个数字影响着如何对行进行抽样，因此哪一组观察结果进入训练分割，哪一组观察结果进入测试分割。如果提供不同的数字，80-20分割将保持不变，但不同的观察结果将进入训练和测试分割。重要的是要记住，要复制相同的结果，你必须选择相同的`random_state`值。例如，如果你想要检查在添加新的自变量后模型的变化情况，你必须能够比较添加新变量前后的准确度。因此，你必须使用相同的`random_state`，以便确定是否发生了变化。要注意的最后一个参数是`stratify`，它确保目标变量的分布在训练和测试分割中保持不变。如果这个分布没有保持不变，那么训练分割中某个类别的观察结果可能会有更多，这不符合训练数据中的分布，导致模型学习一个不现实的函数：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Step 3: Training the Machine Learning Model'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：训练机器学习模型
- en: Our next step in creating the text classification blueprint is to train a supervised
    machine learning model using a suitable algorithm. SVM is one of the popular algorithms
    used when working with text classification, and we will first provide an introduction
    to the method and then illustrate why it’s well-suited to our task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 创建文本分类蓝图的下一步是使用适当的算法训练监督式机器学习模型。当处理文本分类时，SVM是一种常用的算法之一，我们将首先介绍该方法，然后说明为什么它非常适合我们的任务。
- en: 'Consider a set of points in the X-Y plane with each point belonging to one
    of two classes: cross or circle, as represented in [Figure 6-3](#fig-svm-illustration).
    The SVM works by choosing a line that clearly separates the two classes. Of course,
    there could be several such lines (shown by the dotted options), and the algorithm
    chooses the line that provides the maximum separation between the closest cross
    and circle points (identified with a box around them). These closest cross and
    circle points are referred to as *support vectors*. In the illustration, we are
    able to identify a hyperplane that clearly separates the cross and circle points,
    but in reality, it might be difficult to achieve this. For example, there may
    be a few circle points that lie on the extreme left, and it would be impossible
    to then generate a hyperplane. The algorithm manages this with the tolerance parameter
    `tol` that allows for some flexibility and accepts an error in the form of misclassified
    points when deciding a hyperplane.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个在 X-Y 平面上的点集，每个点属于两个类别中的一个：十字或圆圈，如[图 6-3](#fig-svm-illustration)所示。支持向量机通过选择一条清晰地分隔这两个类别的直线来工作。当然，可能存在几条这样的直线（用虚线选项表示），算法选择能在最靠近的十字和圆圈点之间提供最大分离的直线。这些最靠近的十字和圆圈点称为*支持向量*。在示例中，我们能够识别出一个能够清晰分隔十字和圆圈点的超平面，但实际情况中可能难以实现这一点。例如，可能有几个圆圈点位于极左侧，这时生成超平面就不可能了。算法通过允许一定灵活性的容差参数`tol`来处理这种情况，并在决定超平面时接受误分类点的错误。
- en: '![](Images/btap_0603.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0603.jpg)'
- en: Figure 6-3\. Hyperplane and support vectors in a simple two-dimensional classification
    example.
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 简单二维分类示例中的超平面和支持向量。
- en: 'Before proceeding to run the SVM model, we must prepare our text data in a
    suitable format that can be used by the algorithm. This means that we must find
    a way to represent text data in a numeric format. The simplest way is to count
    the number of times each word occurs in a bug report and combine the counts of
    all words to create a numeric representation for each observation. This technique
    has the disadvantage that commonly occurring words will have large values and
    could be understood as important features when this is not true. Therefore, we
    use the preferred option of representing the text using a Term-Frequency Inverse
    Document Frequency (TF-IDF) vectorization, which is explained in more detail in
    [Chapter 5](ch05.xhtml#ch-vectorization):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续运行支持向量机模型之前，我们必须将文本数据准备成算法可以使用的合适格式。这意味着我们必须找到一种方法将文本数据表示为数值格式。最简单的方法是计算每个词在一个缺陷报告中出现的次数，并将所有词的计数组合起来，为每个观察结果创建一个数值表示。这种技术的缺点是常见的单词将有很大的值，并可能被误认为是重要特征，这种情况并非真实。因此，我们采用首选选项，即使用词频逆文档频率（TF-IDF）向量化来表示文本，详细解释请参见[第
    5 章](ch05.xhtml#ch-vectorization)。
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The TF-IDF vectorization performed in the previous step results in a sparse
    matrix. The SVM algorithm is preferred when working with text data because it
    is more suited to work with sparse data compared to other algorithms like [Random
    Forest](https://oreil.ly/uFkYZ). They are also better suited to work with input
    features that are purely numeric (as in our case), while other algorithms are
    capable of handling a mixture of numeric and categorical input features. For our
    text classification model we will use the `sklearn.svm.LinearSVC` module that
    is provided by the scikit-learn library. SVMs can actually be initialized with
    different kernel functions, and the linear kernel is recommended for use with
    text data as there are a large number of features that can be considered linearly
    separable. It is also faster to fit since it has fewer parameters to optimize.
    The scikit-learn package provides different implementations of a linear SVM, and
    if you are interested, you can learn the differences between them as described
    in [“SVC Versus LinearSVC Versus SGDClassifier”](#svcvslinearsvcvssgdclassif).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步执行的TF-IDF向量化生成了一个稀疏矩阵。当处理文本数据时，SVM算法更为适用，因为它更适合处理稀疏数据，相比其他算法如[随机森林](https://oreil.ly/uFkYZ)。它们还更适合处理纯数值型输入特征（就像我们的情况），而其他算法则能够处理数值和分类输入特征的混合。对于我们的文本分类模型，我们将使用由scikit-learn库提供的`sklearn.svm.LinearSVC`模块。实际上，SVM可以使用不同的核函数进行初始化，线性核函数在处理文本数据时推荐使用，因为可以考虑到大量线性可分的特征。它也更快速适应，因为需要优化的参数更少。scikit-learn包提供了线性SVM的不同实现，如果你有兴趣，可以通过阅读[“SVC
    Versus LinearSVC Versus SGDClassifier”](#svcvslinearsvcvssgdclassif)来了解它们之间的区别。
- en: 'In the following code, we initialize the model with a certain `random_state`
    and specify a tolerance value of 0.00001\. The arguments are specific to the type
    of model we use, and we will show later in this chapter how we can arrive at the
    optimal parameter values for these arguments. For now we start by specifying some
    default values and then call the `fit` method, making sure to use the vectorized
    independent variables that we created in the previous step:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用特定的`random_state`初始化模型，并指定了容差值为0.00001。这些参数是针对我们使用的模型类型具体指定的，我们将在本章后面展示如何为这些参数值找到最优值。现在，我们从指定一些默认值开始，然后调用`fit`方法，确保使用我们在前一步创建的向量化独立变量：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Out:`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: On executing the preceding code, we fit a model using the training data, and
    the result shows us the various parameters of the model that was generated. Most
    of these are the default values since we specified only the `random_state` and
    tolerance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行上述代码后，我们使用训练数据拟合了一个模型，结果显示了生成的模型的各种参数。由于我们只指定了`random_state`和容差，大多数参数都是默认值。
- en: 'Step 4: Model Evaluation'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步：模型评估
- en: We now have a model that can be used to predict the target variable for all
    the observations in the test split. For these observations, we also know the real
    target variable, and therefore we can calculate the performance of our model.
    There are many metrics that can be used to quantify the accuracy of our model,
    and we will introduce three of them in this section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个可以用来预测测试集中所有观测目标变量的模型。对于这些观测，我们也知道真实的目标变量，因此我们可以计算我们模型的表现。有许多可以用来量化我们模型准确性的指标，在本节中我们将介绍其中三个。
- en: 'The simplest way to validate our text classification model is accuracy: the
    ratio of the number of predictions that the model got right to the total number
    of observations. This can be expressed mathematically as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 验证我们的文本分类模型最简单的方法是通过准确率：模型正确预测数量与观测总数的比率。数学上可以表示如下：
- en: <math alttext="upper A c c u r a c y equals StartFraction upper N u m b e r
    o f c o r r e c t p r e d i c t i o n s Over upper T o t a l n u m b e r o f p
    r e d i c t i o n s m a d e EndFraction"><mrow><mi>A</mi> <mi>c</mi> <mi>c</mi>
    <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow>
    <mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>m</mi><mi>a</mi><mi>d</mi><mi>e</mi></mrow></mfrac></mrow></math>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A c c u r a c y equals StartFraction upper N u m b e r
    o f c o r r e c t p r e d i c t i o n s Over upper T o t a l n u m b e r o f p
    r e d i c t i o n s m a d e EndFraction"><mrow><mi>A</mi> <mi>c</mi> <mi>c</mi>
    <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi></mrow>
    <mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>s</mi><mi>m</mi><mi>a</mi><mi>d</mi><mi>e</mi></mrow></mfrac></mrow></math>
- en: 'To measure the accuracy of the model, we use the trained model to generate
    predictions and compare with the real values. To generate the predictions, we
    must apply the same vectorization to the test split of the independent variable
    and then call the predict method of the trained model. Once we have the predictions,
    we can use the `accuracy_score` method shown next that automatically generates
    this metric by comparing the true values and the model predictions of the test
    split:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量模型的准确性，我们使用训练好的模型生成预测并与真实值进行比较。为了生成预测，我们必须对独立变量的测试集应用相同的向量化，然后调用训练模型的预测方法。一旦我们有了预测结果，我们可以使用下面展示的`accuracy_score`方法来自动生成这个度量，通过比较测试集的真实值和模型预测值来完成：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Out:`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, we have achieved a high accuracy score of 87.5%, which indicates
    that we have a good model that is able to predict the priority of bugs accurately.
    Please note that if you initialized the model with a different `random_state`,
    you might not get the same score, but it would be similar. It is always a good
    idea to compare the performance of a trained model with a simple baseline approach
    that could be based on simple rules of thumb or business knowledge. The objective
    is to check whether the trained model performs better than the baseline and therefore
    adds value. We can use the `sklearn.svm.DummyClassifier` module, which provides
    simple strategies like `most_frequent`, where the baseline model always predicts
    the class with highest frequency, or which is `stratified`, which generates predictions
    that respect the training data distribution:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们取得了87.5%的高准确率，表明我们有一个能够准确预测缺陷优先级的好模型。请注意，如果您使用不同的`random_state`初始化模型，则可能得到不同但相似的分数。始终比较训练模型与基线方法（可能基于简单的经验法则或业务知识）的表现是个好主意。我们可以使用`sklearn.svm.DummyClassifier`模块，它提供诸如`most_frequent`的简单策略，基线模型始终预测出现频率最高的类别，或者`stratified`，它生成符合训练数据分布的预测：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Out:`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can clearly see that our trained model is not adding any value since it performs
    just as well as a baseline that always chooses the class P3\. Another aspect that
    we must dig deeper to investigate is how well the model is performing for the
    different priority levels. Is it better at predicting priority P1 or P5? To analyze
    this, we can use another evaluation tool known as the *confusion matrix*. The
    confusion matrix is a grid that compares the predicted values with the actual
    values for all the classified observations. The most common representation of
    a confusion matrix is for a binary classification problem with only two labels.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，我们训练的模型并未增加任何价值，因为其表现与始终选择P3类别的基线相当。我们还需深入挖掘模型在不同优先级上的表现如何。它在预测P1或P5优先级方面表现更好吗？为了分析这一点，我们可以使用另一个评估工具，称为*混淆矩阵*。混淆矩阵是一个网格，比较了所有分类观察的预测值与实际值。混淆矩阵最常见的表示是针对只有两个标签的二元分类问题。
- en: We can modify our multiclass classification problem to suit this representation
    by considering one class as P3 and the other class as all of the rest. Let’s look
    at [Figure 6-4](#fig-confusion-matrix), a sample representation of the confusion
    matrix that predicts only whether a particular bug has a priority P3 or not.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将一个类别视为P3，将另一个类别视为所有其余类别，来修改我们的多类别分类问题以适应这种表示。让我们看看[图6-4](#fig-confusion-matrix)，这是一个仅预测特定缺陷是否具有优先级P3的混淆矩阵的示例表示。
- en: '![](Images/btap_0604.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0604.jpg)'
- en: Figure 6-4\. Confusion matrix for priority P3 and not P3.
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4. 优先级P3和非P3的混淆矩阵。
- en: 'The rows depict the predictions, and the columns depict the actual values.
    Each slot in the matrix is the count of observations falling in that slot:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 行代表预测结果，列代表实际值。矩阵中的每个单元格都是落入该格的观察计数：
- en: True Positive
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性
- en: The count of those observations that were predicted to be positive and are indeed
    positive.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为正且确实为正的观察计数。
- en: True Negative
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 真阴性
- en: The count of those observations that were predicted to be negative and are indeed
    negative.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为负且确实为负的观察计数。
- en: False Positive
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性
- en: The count of those observations that were predicted to be positive but are actually
    negative.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为正但实际为负的观察计数。
- en: False Negative
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 假阴性
- en: The count of those observations that were predicted to be negative but are actually
    positive.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为负但实际为正的观察计数。
- en: 'Based on this list, we can automatically derive the accuracy measure using
    the following equation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此列表，我们可以使用以下方程自动推导出准确度度量：
- en: <math alttext="upper A c c u r a c y equals StartFraction left-parenthesis upper
    T r u e upper P o s i t i v e plus upper T r u e upper N e g a t i v e right-parenthesis
    Over left-parenthesis upper T r u e upper P o s i t i v e plus upper T r u e upper
    N e g a t i v e plus upper F a l s e upper P o s i t i v e plus upper F a l s
    e upper N e g a t i v e right-parenthesis EndFraction"><mrow><mi>A</mi> <mi>c</mi>
    <mi>c</mi> <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper A c c u r a c y equals StartFraction left-parenthesis upper
    T r u e upper P o s i t i v e plus upper T r u e upper N e g a t i v e right-parenthesis
    Over left-parenthesis upper T r u e upper P o s i t i v e plus upper T r u e upper
    N e g a t i v e plus upper F a l s e upper P o s i t i v e plus upper F a l s
    e upper N e g a t i v e right-parenthesis EndFraction"><mrow><mi>A</mi> <mi>c</mi>
    <mi>c</mi> <mi>u</mi> <mi>r</mi> <mi>a</mi> <mi>c</mi> <mi>y</mi> <mo>=</mo> <mfrac><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: This is nothing but a ratio of all the predictions that were correct and the
    total number of predictions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这不过是所有预测正确与总预测数的比率而已。
- en: Precision and recall
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确率和召回率
- en: The real value of using the confusion matrix is in other measures like Precision
    and Recall, which give us more insight into how the model performs for different
    classes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混淆矩阵的真正价值在于精确率和召回率等其他度量，这些度量能够更深入地了解模型在不同类别下的表现。
- en: 'Let’s take the positive (P3) class and consider the Precision:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑正（P3）类，并考虑精确率：
- en: <math alttext="upper P r e c i s i o n equals StartFraction upper T r u e upper
    P o s i t i v e Over left-parenthesis upper T r u e upper P o s i t i v e plus
    upper F a l s e upper P o s i t i v e right-parenthesis EndFraction"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>c</mi> <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi>
    <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P r e c i s i o n equals StartFraction upper T r u e upper
    P o s i t i v e Over left-parenthesis upper T r u e upper P o s i t i v e plus
    upper F a l s e upper P o s i t i v e right-parenthesis EndFraction"><mrow><mi>P</mi>
    <mi>r</mi> <mi>e</mi> <mi>c</mi> <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi>
    <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: This metric tells us what proportion of predicted positives is actually positive,
    or how accurate our model is at predicting the positive class. If we want to be
    sure of our positive predictions, then this is a metric we must maximize. For
    example, if we are classifying emails as spam (positive), then we must be accurate
    at this; otherwise, a good email might accidentally be sent to the spam folder.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此指标告诉我们预测的正例中实际上是正例的比例，或者说我们的模型在预测正类时的准确性。如果我们希望对我们的正面预测有把握，那么这是一个必须最大化的指标。例如，如果我们将电子邮件分类为垃圾邮件（正类），那么我们必须在这方面做到准确；否则，一封好的电子邮件可能会意外地发送到垃圾邮件文件夹。
- en: 'Another metric that is derived from the confusion matrix is Recall:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 源自混淆矩阵的另一个衡量指标是召回率：
- en: <math alttext="upper R e c a l l equals StartFraction upper T r u e upper P
    o s i t i v e Over left-parenthesis upper T r u e upper P o s i t i v e plus upper
    F a l s e upper N e g a t i v e right-parenthesis EndFraction"><mrow><mi>R</mi>
    <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi> <mi>l</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R e c a l l equals StartFraction upper T r u e upper P
    o s i t i v e Over left-parenthesis upper T r u e upper P o s i t i v e plus upper
    F a l s e upper N e g a t i v e right-parenthesis EndFraction"><mrow><mi>R</mi>
    <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi> <mi>l</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: This metric tells us what proportion of real positive values is actually identified
    by our model. A high recall means that our model is able to capture most of the
    positive classifications in reality. This is especially important when the cost
    of not identifying a positive case is very high, for example, if a patient has
    cancer but our model does not identify it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此指标告诉我们实际正值中被我们的模型识别的比例。高召回率意味着我们的模型能够捕捉现实中大多数的正类分类。这在成本未识别正例很高的情况下尤为重要，例如，如果一个患者患有癌症但我们的模型未能识别出来。
- en: 'From the previous discussion, we can conclude that both precision and recall
    are important metrics depending on the application of the model. The *F1 score*
    is a metric that creates a harmonic mean of both of these measures and can also
    be used as a proxy to evaluate the overall accuracy of the model:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的讨论中，我们可以得出结论，无论模型的应用是什么，精确度和召回率都是重要的指标。*F1分数*是一个创建这两个度量的调和平均值的指标，也可以用作评估模型整体准确性的代理：
- en: <math alttext="upper F Baseline 1 upper S c o r e equals StartFraction 2 asterisk
    left-parenthesis upper P r e c i s i o n asterisk upper R e c a l l right-parenthesis
    Over left-parenthesis upper P r e c i s i o n plus upper R e c a l l right-parenthesis
    EndFraction"><mrow><mi>F</mi> <mn>1</mn> <mi>S</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><mn>2</mn><mo>*</mo><mo>(</mo><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>*</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>)</mo></mrow>
    <mrow><mo>(</mo><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper F Baseline 1 upper S c o r e equals StartFraction 2 asterisk
    left-parenthesis upper P r e c i s i o n asterisk upper R e c a l l right-parenthesis
    Over left-parenthesis upper P r e c i s i o n plus upper R e c a l l right-parenthesis
    EndFraction"><mrow><mi>F</mi> <mn>1</mn> <mi>S</mi> <mi>c</mi> <mi>o</mi> <mi>r</mi>
    <mi>e</mi> <mo>=</mo> <mfrac><mrow><mn>2</mn><mo>*</mo><mo>(</mo><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>*</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>)</mo></mrow>
    <mrow><mo>(</mo><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>)</mo></mrow></mfrac></mrow></math>
- en: 'Now that we have developed an understanding of the confusion matrix, let’s
    come back to our blueprint and add the step to evaluate the confusion matrix of
    the trained model. Note that the earlier representation was simplified as a binary
    classification, whereas our model is actually a multiclass classification problem,
    and therefore the confusion matrix will change accordingly. For example, the confusion
    matrix for our model can be generated with the function `confusion_matrix`, as
    shown here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对混淆矩阵有了理解，让我们回到我们的蓝图，并添加评估训练模型的混淆矩阵的步骤。请注意，早期的表示被简化为二元分类，而我们的模型实际上是一个多类分类问题，因此混淆矩阵会相应地改变。例如，我们模型的混淆矩阵可以通过函数`confusion_matrix`生成，如下所示：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Out:`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This can also be visualized in the form of a heatmap by using the `plot_confusion_matrix`
    function as shown here:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以通过使用`plot_confusion_matrix`函数以热图形式进行可视化，如下所示：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](Images/btap_06in02.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_06in02.jpg)'
- en: We can define the precision and recall for each category using the same methodology
    as described earlier but will now include the count of observations that were
    incorrectly classified into other categories as well.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与前述相同的方法为每个类别定义精确度和召回率，但现在还将包括被错误分类到其他类别的观察计数。
- en: 'For example, the precision of the category P3 can be calculated as the ratio
    of correctly predicted P3 values (7,821) and all predicted P3 values (195 + 579
    + 7,821 + 194 + 50), resulting in the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，类别P3的精度可以计算为正确预测的P3值（7,821）与所有预测的P3值（195 + 579 + 7,821 + 194 + 50）的比率，结果如下：
- en: '*Precision (P3)* = 7,821 / 8,839 = 0.88'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确度（P3）* = 7,821 / 8,839 = 0.88'
- en: 'Similarly, the recall for P3 can be calculated as the ratio of correctly predicted
    P3 values and all actual P3 values (21 + 43 + 7,821 + 13 + 0), resulting in the
    following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，P3的召回率可以计算为正确预测的P3值与所有实际P3值（21 + 43 + 7,821 + 13 + 0）的比率，结果如下：
- en: '*Recall (P2)* = 7,821 / 7,898 = 0.99'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率（P2）* = 7,821 / 7,898 = 0.99'
- en: 'An easier way to determine these measures directly is to use the `classification_report`
    function from scikit-learn that automatically calculates these values for us:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 直接确定这些度量的更简单方法是使用scikit-learn的`classification_report`函数，它可以自动计算这些值：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Out:`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Based on our calculations and the previous classification report, one issue
    becomes glaringly obvious: while the recall and precision values for the class
    P3 are quite high, these values for the other classes are low and even 0 in some
    cases (P5). The overall accuracy of the model is 88%, but if we hard-coded our
    prediction to always be P3, this would also be correct 88% of the time. This makes
    it clear that our model has not learned much of significance and is merely predicting
    the majority class. This highlights the fact that during model evaluation we must
    analyze several metrics and not rely on the accuracy alone.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的计算和之前的分类报告，一个问题变得明显：尽管P3类别的召回率和精确度值相当高，但其他类别的这些值很低，甚至在某些情况下为0（P5）。模型的整体准确率为88%，但如果我们硬编码我们的预测始终为P3，这也将在88%的时间内是正确的。这清楚地表明我们的模型并未学习到太多显著的信息，而只是预测了多数类别。这凸显了在模型评估期间，我们必须分析几个指标，而不能仅依赖准确率。
- en: Class imbalance
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别不平衡
- en: The reason for the model to behave in this manner is due to the *class imbalance*
    in the priority classes that we observed earlier. While there were close to 36,000
    bugs with a priority of P3, the number of bugs with other priority classes was
    only about 4,000 and even fewer in other cases. This means that when we trained
    our model, it was able to learn the characteristics of the P3 class alone.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 模型表现如此的原因是由于我们之前观察到的优先级类别中的*类别不平衡*。尽管P3优先级有接近36,000个错误，但其他优先级类别的错误数量只有大约4,000个，其他情况更少。这意味着当我们训练我们的模型时，它只能学习P3类别的特征。
- en: 'There are several techniques we can use to overcome the issue of class imbalance.
    They belong to two categories of upsampling and downsampling techniques. Upsampling
    techniques refer to methods used to artificially increase the number of observations
    of the minority class (non-P3 classes in our example). These techniques can vary
    from simply adding multiple copies to generating new observations using a method
    like SMOTE.^([1](ch06.xhtml#idm45634194378904)) Downsampling techniques refer
    to methods that are used to reduce the number of observations of the majority
    class (P3 in our example). We will choose to randomly downsample the P3 class
    to have a similar number of observations as the other classes:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种技术可以用来解决类别不平衡的问题。它们属于上采样和下采样技术的两类。上采样技术是指用于人工增加少数类观测数量（例如我们例子中的非P3类别）的方法。这些技术可以从简单地添加多个副本到使用SMOTE等方法生成新观测数据。^([1](ch06.xhtml#idm45634194378904))
    下采样技术是指用于减少多数类观测数量（例如我们例子中的P3类别）的方法。我们将选择随机下采样P3类别，使其观测数量与其他类别相似：
- en: '[PRE23]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`Out:`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Please note that in performing the downsampling, we are losing information,
    and this is not generally a good idea. However, whenever we come across a class
    imbalance problem, this prevents our model from learning the right information.
    We try to overcome this by using upsampling and downsampling techniques, but this
    will always involve a compromise with regard to data quality. While we have chosen
    a simplistic approach, please see the following sidebar to understand various
    ways to deal with the situation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在执行下采样时，我们正在丢失信息，这通常不是一个好主意。但是，每当遇到类别不平衡的问题时，这会阻止我们的模型学习正确的信息。我们尝试通过使用上采样和下采样技术来克服这一问题，但这将始终涉及到数据质量的妥协。虽然我们选择了一种简单的方法，请查看下面的侧边栏，了解处理这种情况的各种方法。
- en: Final Blueprint for Text Classification
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类最终蓝图
- en: 'We will now combine all the steps we have listed so far to create our blueprint
    for text classification:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将结合到目前为止列出的所有步骤，创建我们的文本分类蓝图：
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Out:`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Based on the results, we can see that our accuracy is now at 49%, which is
    not good. Analyzing further, we can see that precision and recall values have
    improved for priority P1 and P2, indicating that we are able to better predict
    bugs with this priority. However, it’s also obvious that for bugs with priority
    P5, this model does not offer anything. We see that this model does perform better
    than a simple baseline using a *stratified* strategy, as shown next. Even though
    the earlier model had a higher accuracy, it wasn’t actually a good model because
    it was ineffective. This model is also not good but at least presents a true picture
    and informs us that we must not use it for generating predictions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们可以看到我们的准确率现在达到了49%，这不太好。进一步分析，我们可以看到对于P1和P2优先级，精确度和召回率值已经提高，这表明我们能够更好地预测具有这些优先级的错误。然而，显然对于P5优先级的错误，这个模型并没有提供任何信息。我们看到这个模型比使用*分层*策略的简单基线模型表现更好，如下所示。尽管早期的模型具有更高的准确性，但实际上并不是一个好模型，因为它是无效的。这个模型也不好，但至少呈现了一个真实的画面，并告诉我们我们不能用它来生成预测：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following are some examples of where our model predictions for these priorities
    are accurate:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些我们模型对这些优先级的预测准确的示例：
- en: '[PRE29]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|   | text | actual | predicted |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|   | 文本 | 实际 | 预测 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 64 | Java launcher: Dont prompt for element to launch if theres only one
    I went to debug a CU by selecting it and clicking the debug tool item. I was prompted
    to select a launcher and I also had to select the only available class on the
    second page. The second step shouldnt be necessary. The next button on the first
    page should be disabled. NOTES: DW The first time you launch something in your
    workspace you must go through this pain...This is due to the debugger being pluggable
    for different lauguages. In this case the launcher selection is generic debug
    support and the choosing of a class to launch is java specific debug support.
    To promote lazy plugin loading and to avoid launchers doing exhaustive searching
    for launchable targets the launcher selection page does not poll the pluggable
    launch page to see if it can finish with the current selection. Once you have
    selected a defualt launcher for a project the launcher selection page will not
    bother you again. Moved to inactive for post-June consideratio | P2 | P2 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 64 | Java启动器：如果只有一个元素，不要提示要启动的元素。我想通过选择它并单击调试工具项来调试一个CU。我被提示选择一个启动器，然后我还必须在第二页上选择唯一可用的类。第二步是不必要的。第一页上的下一步按钮应该被禁用。注意：DW，第一次在工作空间中启动某个东西时，你必须经历这种痛苦...这是由于调试器对不同语言是可插拔的。在这种情况下，启动器选择是通用调试支持，选择要启动的类是特定于Java调试支持。为了促进懒加载插件并避免启动器对可启动目标进行详尽搜索，启动器选择页面不会轮询可插拔的启动页面，以查看是否可以使用当前选择完成。一旦你为项目选择了默认启动器，启动器选择页面就不会再打扰你。移至非活动状态以供6月后考虑
    |'
- en: '| 5298 | Rapid stepping toString When you do rapid stepping and have an object
    selected displaying details we get exceptions in the log. This is because the
    toString attempts an evaluation while a step is in progress. We have to allow
    stepping during evaluations so this is a tricky timing issue. </log-entr | P1
    | P1 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 5298 | 快速步进toString当您快速步进并选择一个对象显示详细信息时，我们会在日志中得到异常。这是因为toString尝试在步骤进行时进行评估。我们必须允许在评估过程中进行步进，所以这是一个棘手的时间问题。
    </log-entr | P1 | P1 |'
- en: 'Here are some cases where the model prediction is inaccurate:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型预测不准确的情况：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Out:`'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|   | text | actual | predicted |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|   | 文本 | 实际 | 预测 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 4707 | Javadoc wizard: Problems with default package 20020328 1\. empty project.
    create A.java in default package 2\. Start export wizard select the default package
    press Finish 3\. Creation fails javadoc: No source files for package A Loading
    source files for package A... 1 error Dont know if this is a general javadoc probl
    | P1 | P2 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 4707 | Javadoc向导：默认包存在问题20020328 1. 空项目。在默认包中创建A.java 2. 启动导出向导选择默认包按下完成按钮
    3. 创建失败javadoc：包A的源文件不存在 为包A加载源文件... 1个错误 不知道这是否是一般的javadoc问题 | P1 | P2 |'
- en: '| 16976 | Breakpoint condition compiler should not matter about NON-NLS strings
    Ive a project in which Ive set compiler option usage of non-externalized strings
    to Warning. When I want to set a condition on a breakpoint which contains a string
    object.equals for example I break all the time at this point due to a compilation
    error... Then Im obliged to write my condition as: boolean cond = object.equals
    //$NON-NLS-1$ return cond to avoid this problem. Wont it be possible that debugger
    uses a specific compiler which would ignore current project/workspace compiler
    options but only uses default one | P2 | P3 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 16976 | 断点条件编译器不应关心非 NLS 字符串 我有一个项目，在这个项目中，我设置了编译器选项，将非外部化字符串的使用设置为警告。当我想在包含字符串对象.equals的断点条件上设置条件时，由于编译错误，我总是在这一点上中断……然后我不得不这样写我的条件：boolean
    cond = object.equals //$NON-NLS-1$ return cond 以避免这个问题。调试器是否可以使用特定的编译器，它将忽略当前项目/工作区的编译器选项，而仅使用默认的选项呢？
    | P2 | P3 |'
- en: Our model is not accurate, and from observing the predictions, it is not clear
    whether a relationship between description and priority exists. To improve the
    accuracy of our model, we have to perform additional data cleaning steps and perform
    steps such as lemmatization, removing noisy tokens, modifying `min_df` and `max_df`,
    including trigrams, and so on. We recommend that you modify the current `clean`
    function provided in [“Feature Extraction on a Large Dataset”](ch04.xhtml#ch04largedatasetfeatureextract)
    and check the performance. Another option is also to determine the right hyperparameters
    for the selected model, and in the next section, we will introduce the cross-validation
    and grid search techniques, which can help us better understand model performance
    and arrive at an optimized model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型不准确，从观察预测结果来看，不清楚描述和优先级之间是否存在关系。为了提高模型的准确性，我们必须执行额外的数据清理步骤，如词形还原，去除噪声标记，修改`min_df`和`max_df`，包括三元组等。我们建议您修改[“大数据集上的特征提取”](ch04.xhtml#ch04largedatasetfeatureextract)中提供的当前`clean`函数，并检查其性能。另一种选择是确定所选模型的正确超参数，在下一节中，我们将介绍交叉验证和网格搜索技术，这些技术可以帮助我们更好地理解模型性能，并得出优化的模型。
- en: 'Blueprint: Using Cross-Validation to Estimate Realistic Accuracy Metrics'
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Blueprint: 使用交叉验证估算实际准确度指标'
- en: Before training the model, we created a train-test split so that we can accurately
    evaluate our model. Based on the test split, we got an accuracy of 48.7%. However,
    it is desirable to improve this accuracy. Some of the techniques that we could
    use include adding additional features such as trigrams, adding additional text
    cleaning steps, choosing different model parameters, and then checking performance
    on the test split. Our result is always based on a single hold-out dataset that
    we created using the train-test split. If we go back and change the `random_state`
    or `shuffle` our data, then we might get a different test split, which might have
    different accuracy for the same model. Therefore, we rely heavily on a given test
    split to determine the accuracy of our model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前，我们创建了一个训练-测试分离，以便能够准确评估我们的模型。根据测试分离，我们得到了48.7%的准确度。然而，我们希望提高这个准确度。我们可以使用的一些技术包括添加额外的特征，如三元组，添加额外的文本清理步骤，选择不同的模型参数，然后在测试分离上检查性能。我们的结果始终基于一个我们使用训练-测试分离创建的单个留出数据集。如果我们返回并更改`random_state`或`shuffle`我们的数据，那么我们可能会得到一个不同的测试分离，对于相同的模型可能会有不同的准确度。因此，我们严重依赖于给定的测试分离来确定我们模型的准确度。
- en: '*Cross-validation* is a technique that allows us to train on different splits
    of data and validate also on different splits of data in a repetitive manner so
    that the final model that is trained achieves the right balance between *underfitting*
    and *overfitting*. Underfitting is the phenomenon where our trained model does
    not learn the underlying relationship well and makes similar predictions for every
    observation that are far away from the real value. This is because the chosen
    model is not complex enough to model the phenomenon (wrong choice of model) or
    there are insufficient observations from which to learn the relationship. Overfitting
    is the phenomenon where the chosen model is complex and has fit the underlying
    pattern very well during training but produces significant deviations on the test
    data. This indicates that the trained model does not generalize well to unseen
    data. By using a cross-validation technique, we become aware of these drawbacks
    by training and testing on multiple splits of the data and can arrive at a more
    realistic performance of our model.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*交叉验证* 是一种技术，允许我们在数据的不同分割上进行训练和验证，以便最终训练的模型在*欠拟合*和*过拟合*之间取得适当的平衡。欠拟合是指我们训练的模型未能很好地学习底层关系，并对每个观察结果进行类似的预测，这些预测与真实值相去甚远。这是因为所选模型复杂性不足以建模现象（错误的模型选择）或者学习关系的观察样本不足。过拟合是指选择的模型非常复杂，在训练过程中很好地拟合了底层模式，但在测试数据上产生显著偏差。这表明训练的模型在未见数据上泛化能力不强。通过使用交叉验证技术，我们可以通过在数据的多个分割上进行训练和测试，意识到这些缺点，并得出模型更真实的性能。'
- en: There are many variants of cross-validation, and the most widely used is K-fold
    cross-validation. [Figure 6-5](#fig-cross-validation) demonstrates a K-fold strategy,
    where we first divide the entire training dataset into K folds. In each iteration,
    a model is trained on a different set of K-1 folds of the data, and validation
    is performed on the held-out Kth fold. The overall performance is taken to be
    the average of the performance on all hold-out K folds. In this way we are not
    basing our model accuracy on just one test split but multiple such splits, and
    similarly we are also training the model on multiple splits of the training data.
    This allows us to use all the observations for training our model as we do not
    need to have a separate hold-out test split.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多交叉验证的变体，其中最广泛使用的是 K 折交叉验证。[图 6-5](#fig-cross-validation) 展示了一种 K 折策略，我们首先将整个训练数据集分成
    K 份。在每次迭代中，模型在不同的 K-1 折数据集上进行训练，并在保留的第 K 折上进行验证。整体性能被视为所有保留的 K 折上性能的平均值。通过这种方式，我们不仅仅基于一个测试分割来评估模型的准确性，而是基于多个这样的分割，同样我们也在多个训练数据的分割上进行模型训练。这使我们可以利用所有观察样本来训练我们的模型，因为我们不需要单独的保留测试分割。
- en: '![](Images/btap_0605.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0605.jpg)'
- en: Figure 6-5\. A K-fold cross-validation strategy where a different hold-out set
    (shaded) is chosen each time the model is trained. The rest of the sets form part
    of the training data.
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 一种 K 折交叉验证策略，每次训练模型时选择不同的留出集（阴影部分）。其余集合形成训练数据的一部分。
- en: 'To perform cross-validation, we will use the `cross_val_score` method from
    scikit-learn. This takes as arguments the model that needs to be fit, the training
    dataset, and the number of folds that we want to use. In this case, we use a five-fold
    cross-validation strategy, and, depending on the number of training observations
    and availability of computing infrastructure, this can vary between 5 and 10\.
    The method returns the validation score for each iteration of the cross-validation,
    and we can calculate the mean value obtained across all validation folds. From
    the results, we can see that the validation score varies from 36% up to 47%. This
    indicates that the model accuracy we reported earlier on the test dataset was
    optimistic and an artifact of the specific way in which the train-test split occurred.
    A more realistic accuracy that we can expect from this model is actually the average
    score of 44% derived from cross-validation. It’s important to perform this exercise
    to understand the true potential of any model. We perform the vectorization step
    again because we are going to use the entire dataset and not just the train split:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行交叉验证，我们将使用scikit-learn中的`cross_val_score`方法。它的参数包括需要拟合的模型、训练数据集以及我们想要使用的折数。在这种情况下，我们使用五折交叉验证策略，根据训练观测数和计算基础设施的可用性，这可以在5到10之间变化。该方法返回每次交叉验证迭代的验证分数，并且我们可以计算所有验证折叠中得到的平均值。从结果中，我们可以看到验证分数从36%变化到47%不等。这表明我们之前在测试数据集上报告的模型准确率是乐观的，并且是特定的训练测试分割方式的产物。从交叉验证中得到的更实际的准确率平均分为44%。执行此练习以理解任何模型的真实潜力非常重要。我们再次执行向量化步骤，因为我们将使用整个数据集，而不仅仅是训练分割：
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`Out:`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Using a cross-validation technique allows us to use all observations, and we
    do not need to create a separate hold-out test split. This gives the model more
    data to learn from.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证技术允许我们使用所有观测数据，而不需要创建单独的保留测试分割。这为模型提供了更多的学习数据。
- en: 'Blueprint: Performing Hyperparameter Tuning with Grid Search'
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用网格搜索执行超参数调优
- en: '*Grid search* is a useful technique to improve the accuracy of the model by
    evaluating different parameters that are used as arguments for the model. It does
    so by trying different combinations of hyperparameters that can maximize a given
    metric (e.g., accuracy) for the machine learning model. For example, if we use
    the `sklearn.svm.SVC` model, it has a parameter named [`kernel`](https://oreil.ly/30Xsq)
    that can take several values: `linear`, `rbf` (radial basis function), `poly`
    (polynomial), and so on. Furthermore, by setting up a preprocessing pipeline,
    we could also test with different values of `ngram_range` for the TF-IDF vectorization.
    When we do a grid search, we provide the set of parameter values that we would
    like to evaluate, and combined with the cross-validation method of training a
    model, this identifies the set of hyperparameters that maximizes model accuracy.
    The biggest drawback of this technique is that it is CPU- and time-intensive;
    in a way we, are testing many possible combinations of hyperparameters to arrive
    at the set of values that perform best.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*网格搜索*是一种通过评估不同作为模型参数的参数来提高模型准确性的有用技术。它通过尝试不同的超参数组合来最大化给定指标（例如准确率）来实现这一目标。例如，如果我们使用`sklearn.svm.SVC`模型，它有一个名为[`kernel`](https://oreil.ly/30Xsq)的参数，可以采用几个值：`linear`、`rbf`（径向基函数）、`poly`（多项式）等等。此外，通过设置预处理流水线，我们还可以测试不同的`ngram_range`值用于TF-IDF向量化。当我们进行网格搜索时，我们提供要评估的参数值集，并结合交叉验证方法来训练模型，从而确定最大化模型准确性的超参数集。这种技术的最大缺点是它对CPU和时间要求高；换句话说，我们需要测试许多可能的超参数组合，以确定表现最佳的数值集。'
- en: 'To test the right choice of hyperparameters for our model, we first create
    a `training_pipeline` where we define the steps that we would like to run. In
    this case, we specify the TF-IDF vectorization and the LinearSVC model training.
    We then define a set of parameters that we would like to test using the variable
    `grid_param`. Since a parameter value is specific to a certain step in the pipeline,
    we use the name of the step as the prefix when specifying the `grid_param`. For
    example, `min_df` is a parameter used by the vectorization step and is therefore
    referred to as `tfidf__min_df`. Finally, we use the `GridSearchCV` method, which
    provides the functionality to test multiple versions of the entire pipeline with
    different sets of hyperparameters and produces the cross-validation scores from
    which we pick the best-performing version:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们模型的超参数的正确选择，我们首先创建了一个`training_pipeline`，在其中定义我们想要运行的步骤。在这种情况下，我们指定了TF-IDF向量化和LinearSVC模型训练。然后，我们定义了一组参数，我们希望使用变量`grid_param`进行测试。由于参数值特定于管道中的某个步骤，因此在指定`grid_param`时，我们使用步骤的名称作为前缀。例如，`min_df`是向量化步骤使用的参数，因此称为`tfidf__min_df`。最后，我们使用`GridSearchCV`方法，该方法提供了测试整个管道的多个版本以及不同超参数集合的功能，并生成交叉验证分数，从中选择性能最佳的版本：
- en: '[PRE33]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Out:`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE34]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have evaluated two values of `min_df` and `ngram_range` with two different
    sets of model parameters. In the first set, we tried with the l2 `model_penalty`
    and hinge `model_loss` with a maximum of 1,000 iterations. In the second set,
    we tried to vary the value of the regularization parameter `C` and `tolerance`
    values of the model. While we saw the parameters of the best model earlier, we
    can also check the performance of all other models that were generated to see
    how the parameter values interact. You can see the top five models and their parameter
    values as in the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了两个`min_df`和`ngram_range`的值，并使用两组不同的模型参数。在第一组中，我们尝试了l2 `model_penalty`和hinge
    `model_loss`，最多进行了1,000次迭代。在第二组中，我们尝试改变正则化参数`C`和模型的`tolerance`值。虽然我们之前看到了最佳模型的参数，但我们也可以检查生成的所有其他模型的性能，以了解参数值之间的相互作用。您可以查看前五个模型及其参数值如下：
- en: '[PRE35]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '|   | rank_test_score | mean_test_score | params |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|   | rank_test_score | mean_test_score | params |'
- en: '| --- | --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 3 | 1 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 10, ‘tfidf__ngram_range’: (1, 6)} |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 10, ‘tfidf__ngram_range’: (1, 6)} |'
- en: '| 2 | 2 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 10, ‘tfidf__ngram_range’: (1, 3)} |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 10, ‘tfidf__ngram_range’: (1, 3)} |'
- en: '| 0 | 3 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’: (1, 3)} |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 3 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’: (1, 3)} |'
- en: '| 1 | 4 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’: (1, 6)} |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4 | 0.46 | {''model__loss’: ‘hinge'', ‘model__max_iter’: 10000, ‘model__penalty’:
    ‘l2'', ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’: (1, 6)} |'
- en: '| 5 | 5 | 0.45 | {''model__C’: 1, ‘model__tol’: 0.01, ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’:
    (1, 6)} |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5 | 0.45 | {''model__C’: 1, ‘model__tol’: 0.01, ‘tfidf__min_df’: 5, ‘tfidf__ngram_range’:
    (1, 6)} |'
- en: Blueprint Recap and Conclusion
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图总结与结论
- en: Let’s recap the steps of the blueprint for text classification by applying this
    to a different classification task. If you recall, we mentioned at the beginning
    of the chapter that to enable quick bug fixes, we must identify the priority of
    the bug and also assign it to the right team. The assignment can be done automatically
    by identifying which part of the software the bug belongs to. We have seen that
    the bug reports have a feature named `Component` with values including `Core`,
    `UI`, and `Doc`. This can be helpful in assigning the bug to the right team or
    individual, leading to a faster resolution. This task is similar to identifying
    the bug priority and will help us understand how the blueprint can be applied
    to any other application.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将其应用于不同的分类任务来总结文本分类的蓝图步骤。如果您还记得，我们在本章开头提到，为了能够快速修复错误，我们必须确定错误的优先级，并将其分配给正确的团队。可以通过识别错误属于软件的哪个部分来自动执行分配。我们已经看到，错误报告有一个名为`Component`的功能，其中的值包括`Core`、`UI`和`Doc`。这有助于将错误分配给正确的团队或个人，从而加快解决速度。这项任务类似于确定错误优先级，并将帮助我们理解蓝图如何应用于任何其他应用程序。
- en: 'We update the blueprint with the following changes:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用以下更改更新蓝图：
- en: Additional step to include grid search to identify the best hyperparameters
    and limit the number of options tested to increase runtime
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附加步骤，包括网格搜索以确定最佳超参数，并限制测试的选项数量以增加运行时
- en: Additional option to use the `sklearn.svm.SVC` function to compare performance
    and try nonlinear kernels
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sklearn.svm.SVC`函数的额外选项来比较性能并尝试非线性核函数
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`Out:`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE37]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Based on the accuracy and classification report, we have achieved an accuracy
    of 73%, and we can conclude that this model is able to predict the software component
    that a bug is referring to more accurately than the priority. While some of the
    improvement is due to the additional steps of grid search and cross-validation,
    most of it is simply because there is a relationship that the model could identify
    between the bug description and the Component it refers to. The Component feature
    does not show the same level of the class imbalance problem that we noticed earlier.
    However, even within Component, we can see the poor results for the software component
    Doc, which has few observations compared to the other components. Also, comparing
    with the baseline, we can see that this model improves in performance. We can
    try to balance our data, or we can make an informed business decision that it’s
    more important for the model to predict those software components that have a
    larger number of bugs:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 基于准确性和分类报告，我们实现了73%的准确性，我们可以得出结论，该模型能够更准确地预测软件组件所指的Bug，而不是优先级。虽然部分改进归功于网格搜索和交叉验证的额外步骤，但大部分只是因为模型能够识别描述与其所指的组件之间的关系。组件功能并没有显示出我们之前注意到的同等级别的类不平衡问题。但是，即使在组件内部，我们也可以看到对Doc软件组件的差结果，该组件的观察数量较其他组件少。另外，与基线相比，我们可以看到这个模型在性能上有所提高。我们可以尝试平衡我们的数据，或者我们可以做出一个明智的商业决策，即模型更重要的是预测那些具有较多Bug的软件组件：
- en: '[PRE38]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`Out:`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE39]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let’s also attempt to understand how this model tries to make its predictions
    by looking at where it works well and where it fails. We will first sample two
    observations where the predictions were accurate:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也尝试了解这个模型如何进行预测，看看它在哪些方面表现良好，在哪些方面失败。我们首先将采样两个预测准确的观察结果：
- en: '[PRE40]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`Out:`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|   | text | actual | predicted |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|   | 文本 | 实际 | 预测 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 28225 | Move static initializer lacks atomic undo.When a method is moved
    the move can be atomically undone with a single Undo command. But when a static
    initializer is moved it can only be undone with an Undo command issued in both
    the source and destination files | UI | UI |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 28225 | 移动静态初始化程序缺乏原子撤销。当移动方法时，可以使用单个撤销命令原子地撤销移动。但是当移动静态初始化程序时，只能在源文件和目标文件中发出Undo命令来撤销。
    | UI | UI |'
- en: '| 30592 | Debug view steals focus when breakpoint hitM5 - I20060217-1115 When
    you debug a program that has breakpoints when the debugger hits a breakpoint pressing
    Ctrl+Sht+B does not remove the breakpoint even though the line looks like it has
    the focus. To actually remove the breakpoint one has to click in the editor on
    the proper line and repress the keys | Debug | Debug |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 30592 | 断点命中时，调试视图窃取焦点M5 - I20060217-1115 当您调试具有断点的程序时，当调试器命中断点时，按下Ctrl+Sht+B不会删除断点，即使该行看起来已经获得焦点。要实际删除断点，必须在编辑器中单击正确的行并重新按下键
    | Debug | Debug |'
- en: 'We can see that when a bug is classified as belonging to the Debug component
    the description makes use of terms like *debugger* and *breakpoint*, whereas when
    a bug is classified in UI, we see an indication of *Undo* and *movement*. This
    seems to indicate that the trained model is able to learn associations between
    words in the description and the corresponding software component. Let’s also
    look at some observations where the predictions were incorrect:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当将错误分类为Debug组件时，描述中使用了诸如*debugger*和*breakpoint*之类的术语，而当将错误分类为UI时，我们看到了*Undo*和*movement*的指示。这似乎表明训练好的模型能够学习描述中的单词与相应软件组件之间的关联。让我们也看看一些预测错误的观察结果：
- en: '[PRE41]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Out:`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|   | text | actual | predicted |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|   | 文本 | 实际 | 预测 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 16138 | Line wrapping on @see tags creates a new warning Invalid parameters
    declarationIn Eclipce 3.0M5 with the javadoc checking enabled linewrapping will
    cause a warning Javadoc: Invalid parameters declaration This will cause the warning:
    /** * @see com.xyz.util.monitoring.MonitoringObserver#monitorSetValue */ where
    this will not : /** * @see com.xyz.util.monitoring.MonitoringObserver#monitorSetValue
    * | Text | Core |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 16138 | @see标签上的行包装创建了一个新的警告，即无效的参数声明。在启用了javadoc检查的eclipse 3.0M5中，行包装将导致警告Javadoc:
    Invalid parameters declaration 这将导致警告: /** * @see com.xyz.util.monitoring.MonitoringObserver#monitorSetValue
    */ 这样不会: /** * @see com.xyz.util.monitoring.MonitoringObserver#monitorSetValue
    * | 文本 | 核心 |'
- en: '| 32903 | After a String array is created eclipse fails to recognize methods
    for an object.Type these lines in any program. String abc = new String {a b c}
    System. After System. eclipse wont list all the available methods | Core | UI
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 32903 | 创建字符串数组后，eclipse无法识别对象的方法。在任何程序中键入这些行。 String abc = new String {a
    b c} System。在System后。eclipse将不会列出所有可用的方法 | 核心 | UI |'
- en: Here, it becomes more difficult to identify reasons for an incorrect classification,
    but we must analyze further if we want to improve the accuracy of our model. After
    we build a model, we must investigate our predictions and understand why the model
    made these predictions. There are several techniques that we can use to explain
    model predictions, and this will be covered in more detail in [Chapter 7](ch07.xhtml#ch-explain).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，要识别不正确分类的原因更加困难，但如果我们想要提高模型的准确性，我们必须进一步分析。在建立模型之后，我们必须调查我们的预测并理解为什么模型做出这些预测。有几种技术可以用来解释模型的预测，这将在[第7章](ch07.xhtml#ch-explain)中更详细地讨论。
- en: Closing Remarks
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: In this chapter, we presented a blueprint for performing the different steps
    in building a supervised text classification model. It starts with the data preparation
    steps, including the balancing of classes, if required. We then showed the steps
    for creating train and test splits, including the use of cross-validation as the
    preferred technique to arrive at an accurate measure of model accuracy. We then
    presented grid search as one of the techniques to validate different settings
    of hyperparameters to find the most optimal combination. Supervised machine learning
    is a broad area with multiple applications like loan default prediction, ad-click
    prediction, etc. This blueprint presents an end-to-end technique for building
    a supervised machine learning model and can be extended to problems outside of
    text classification as well.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们提出了在构建监督文本分类模型的不同步骤中执行的蓝图。它始于数据准备步骤，包括如有必要的类平衡。然后我们展示了创建训练和测试分割的步骤，包括使用交叉验证作为到达模型准确度的首选技术。然后我们介绍了网格搜索作为验证不同超参数设置以找到最优组合的技术之一。监督学习是一个广泛的领域，有多种应用，如贷款违约预测、广告点击预测等。这个蓝图提供了一个端到端的技术，用于构建监督学习模型，并且也可以扩展到文本分类以外的问题。
- en: Further Reading
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Bergstra, James, and Yoshua Bengio. “Random Search for Hyper-Parameter Optimization.”
    2012\. [*http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf*](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: James Bergstra和Yoshua Bengio的文章“Random Search for Hyper-Parameter Optimization.”
    2012年. [*http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf*](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).
- en: Berwick, R. “An Idiot’s guide to Support Vector Machines.” [*http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf*](http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R. Berwick的文章“An Idiot’s guide to Support Vector Machines.” [*http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf*](http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf).
- en: Kohavi, Ron. “A Study of CrossValidation and Bootstrap for Accuracy Estimation
    and Model Selection.” [*http://ai.stanford.edu/~ronnyk/accEst.pdf*](http://ai.stanford.edu/~ronnyk/accEst.pdf).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ron Kohavi的文章“A Study of CrossValidation and Bootstrap for Accuracy Estimation
    and Model Selection.” [*http://ai.stanford.edu/~ronnyk/accEst.pdf*](http://ai.stanford.edu/~ronnyk/accEst.pdf).
- en: Raschka, Sebastian. “Model Evaluation, Model Selection, and Algorithm Selection
    in Machine Learning.” 2018\. [*https://arxiv.org/pdf/1811.12808.pdf*](https://arxiv.org/pdf/1811.12808.pdf).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastian Raschka的文章“Model Evaluation, Model Selection, and Algorithm Selection
    in Machine Learning.” 2018年. [*https://arxiv.org/pdf/1811.12808.pdf*](https://arxiv.org/pdf/1811.12808.pdf).
- en: ^([1](ch06.xhtml#idm45634194378904-marker)) Nitesh Chawla et al. “Synthetic
    Minority Over-Sampling Technique.” *Journal of Artificial Intelligence Research*
    16 (June 2002). [*https://arxiv.org/pdf/1106.1813.pdf*](https://arxiv.org/pdf/1106.1813.pdf).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#idm45634194378904-marker)) Nitesh Chawla等人的文章“Synthetic Minority
    Over-Sampling Technique.” *人工智能研究杂志* 16 (2002年6月). [*https://arxiv.org/pdf/1106.1813.pdf*](https://arxiv.org/pdf/1106.1813.pdf).
