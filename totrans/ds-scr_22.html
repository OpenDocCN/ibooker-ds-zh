<html><head></head><body><section data-pdf-bookmark="Chapter 21. Natural Language Processing" data-type="chapter" epub:type="chapter"><div class="chapter" id="natural_language_processing">&#13;
<h1><span class="label">Chapter 21. </span>Natural Language Processing</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>They have been at a great feast of languages, and stolen the scraps.</p>&#13;
    <p data-type="attribution">William Shakespeare</p>&#13;
</blockquote>&#13;
&#13;
<p><em>Natural language processing</em> (NLP) refers<a data-primary="natural language processing (NLP)" data-secondary="definition of term" data-type="indexterm" id="idm45635721748280"/> to computational techniques involving language.  It’s a broad field, but we’ll look at a few techniques, both simple and not simple.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Word Clouds" data-type="sect1"><div class="sect1" id="idm45635721746824">&#13;
<h1>Word Clouds</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch01.html#introduction">Chapter 1</a>, we<a data-primary="natural language processing (NLP)" data-secondary="word clouds" data-type="indexterm" id="idm45635721744264"/><a data-primary="word clouds" data-type="indexterm" id="idm45635721743272"/> computed word counts of users’ interests.&#13;
One approach to visualizing words and counts is <em>word clouds</em>,&#13;
which artistically depict the words at sizes proportional to their counts.</p>&#13;
&#13;
<p>Generally, though, data scientists don’t think much of word clouds,&#13;
in large part because the placement of the words doesn’t mean anything other than&#13;
“here’s some space where I was able to fit a word.”</p>&#13;
&#13;
<p>If you ever are forced to create a word cloud, think about&#13;
whether you can make the axes convey something.&#13;
For example, imagine that,&#13;
for each of some collection of data science–related&#13;
buzzwords, you have two numbers between 0 and 100—the first representing how frequently it appears in job postings, and the second how frequently it appears on résumés:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">data</code> <code class="o">=</code> <code class="p">[</code> <code class="p">(</code><code class="s2">"big data"</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">15</code><code class="p">),</code> <code class="p">(</code><code class="s2">"Hadoop"</code><code class="p">,</code> <code class="mi">95</code><code class="p">,</code> <code class="mi">25</code><code class="p">),</code> <code class="p">(</code><code class="s2">"Python"</code><code class="p">,</code> <code class="mi">75</code><code class="p">,</code> <code class="mi">50</code><code class="p">),</code>&#13;
         <code class="p">(</code><code class="s2">"R"</code><code class="p">,</code> <code class="mi">50</code><code class="p">,</code> <code class="mi">40</code><code class="p">),</code> <code class="p">(</code><code class="s2">"machine learning"</code><code class="p">,</code> <code class="mi">80</code><code class="p">,</code> <code class="mi">20</code><code class="p">),</code> <code class="p">(</code><code class="s2">"statistics"</code><code class="p">,</code> <code class="mi">20</code><code class="p">,</code> <code class="mi">60</code><code class="p">),</code>&#13;
         <code class="p">(</code><code class="s2">"data science"</code><code class="p">,</code> <code class="mi">60</code><code class="p">,</code> <code class="mi">70</code><code class="p">),</code> <code class="p">(</code><code class="s2">"analytics"</code><code class="p">,</code> <code class="mi">90</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code>&#13;
         <code class="p">(</code><code class="s2">"team player"</code><code class="p">,</code> <code class="mi">85</code><code class="p">,</code> <code class="mi">85</code><code class="p">),</code> <code class="p">(</code><code class="s2">"dynamic"</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">90</code><code class="p">),</code> <code class="p">(</code><code class="s2">"synergies"</code><code class="p">,</code> <code class="mi">70</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code>&#13;
         <code class="p">(</code><code class="s2">"actionable insights"</code><code class="p">,</code> <code class="mi">40</code><code class="p">,</code> <code class="mi">30</code><code class="p">),</code> <code class="p">(</code><code class="s2">"think out of the box"</code><code class="p">,</code> <code class="mi">45</code><code class="p">,</code> <code class="mi">10</code><code class="p">),</code>&#13;
         <code class="p">(</code><code class="s2">"self-starter"</code><code class="p">,</code> <code class="mi">30</code><code class="p">,</code> <code class="mi">50</code><code class="p">),</code> <code class="p">(</code><code class="s2">"customer focus"</code><code class="p">,</code> <code class="mi">65</code><code class="p">,</code> <code class="mi">15</code><code class="p">),</code>&#13;
         <code class="p">(</code><code class="s2">"thought leadership"</code><code class="p">,</code> <code class="mi">35</code><code class="p">,</code> <code class="mi">35</code><code class="p">)]</code></pre>&#13;
&#13;
<p>The<a data-primary="Buzzword clouds" data-type="indexterm" id="idm45635721738712"/> word cloud approach is just to arrange the words on a page in a cool-looking font (<a data-type="xref" href="#word_cloud_wordle">Figure 21-1</a>).</p>&#13;
&#13;
<figure><div class="figure" id="word_cloud_wordle">&#13;
<img alt="Buzzword Cloud." src="assets/dsf2_2101.png"/>&#13;
<h6><span class="label">Figure 21-1. </span>Buzzword cloud</h6>&#13;
</div></figure>&#13;
&#13;
<p>This looks neat but doesn’t really tell us anything.&#13;
A more interesting approach might be to&#13;
scatter them so that horizontal position indicates&#13;
posting popularity and vertical position indicates&#13;
résumé popularity, which produces a visualization that conveys a few insights (<a data-type="xref" href="#word_cloud_scatter">Figure 21-2</a>):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">text_size</code><code class="p">(</code><code class="n">total</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""equals 8 if total is 0, 28 if total is 200"""</code>&#13;
    <code class="k">return</code> <code class="mi">8</code> <code class="o">+</code> <code class="n">total</code> <code class="o">/</code> <code class="mi">200</code> <code class="o">*</code> <code class="mi">20</code>&#13;
&#13;
<code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">job_popularity</code><code class="p">,</code> <code class="n">resume_popularity</code> <code class="ow">in</code> <code class="n">data</code><code class="p">:</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="n">job_popularity</code><code class="p">,</code> <code class="n">resume_popularity</code><code class="p">,</code> <code class="n">word</code><code class="p">,</code>&#13;
             <code class="n">ha</code><code class="o">=</code><code class="s1">'center'</code><code class="p">,</code> <code class="n">va</code><code class="o">=</code><code class="s1">'center'</code><code class="p">,</code>&#13;
             <code class="n">size</code><code class="o">=</code><code class="n">text_size</code><code class="p">(</code><code class="n">job_popularity</code> <code class="o">+</code> <code class="n">resume_popularity</code><code class="p">))</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Popularity on Job Postings"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Popularity on Resumes"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">100</code><code class="p">])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">([])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">yticks</code><code class="p">([])</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<figure><div class="figure" id="word_cloud_scatter">&#13;
<img alt="A more meaningful (if less attractive) word cloud." src="assets/dsf2_2102.png"/>&#13;
<h6><span class="label">Figure 21-2. </span>A more meaningful (if less attractive) word cloud</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="n-Gram Language Models" data-type="sect1"><div class="sect1" id="idm45635721746232">&#13;
<h1>n-Gram Language Models</h1>&#13;
&#13;
<p>The<a data-primary="natural language processing (NLP)" data-secondary="n-gram language models" data-type="indexterm" id="idm45635721452104"/><a data-primary="" data-startref="n-gram language models" data-type="indexterm" id="idm45635721451128"/><a data-primary="language models" data-type="indexterm" id="idm45635721450184"/><a data-primary="models of language" data-type="indexterm" id="idm45635721449512"/> DataSciencester VP of Search Engine Marketing wants to create thousands of web pages about data science so that your site will rank higher in search results for data science–related terms.  (You attempt to explain to her that search engine algorithms are clever enough that this won’t actually work, but she refuses to listen.)</p>&#13;
&#13;
<p>Of course, she doesn’t want to write thousands of web pages, nor does she want to pay a horde of “content strategists” to do so.  Instead, she asks you whether you can somehow programmatically generate these web pages.  To do this, we’ll need some way of modeling language.</p>&#13;
&#13;
<p>One approach is to start with a corpus of documents and learn<a data-primary="statistical models of language" data-type="indexterm" id="idm45635721447256"/> a statistical model of language.  In our case, we’ll start with Mike Loukides’s essay <a href="http://oreil.ly/1Cd6ykN">“What Is Data Science?”</a></p>&#13;
&#13;
<p>As in <a data-type="xref" href="ch09.html#getting_data">Chapter 9</a>, we’ll use the Requests and Beautiful Soup libraries to retrieve the data.&#13;
There are a couple of issues worth calling attention to.</p>&#13;
&#13;
<p>The first is that the apostrophes in the text are actually the Unicode character <code>u"\u2019"</code>. We’ll create a helper function to replace them with normal apostrophes:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">fix_unicode</code><code class="p">(</code><code class="n">text</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">text</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s-Affix">u</code><code class="s2">"</code><code class="se">\u2019</code><code class="s2">"</code><code class="p">,</code> <code class="s2">"'"</code><code class="p">)</code></pre>&#13;
&#13;
<p>The second issue is that once we get the text of the web page,&#13;
we’ll want to split it into a sequence of  words and periods (so that we can tell where sentences end).&#13;
We can do this using <code>re.findall</code>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">re</code>&#13;
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
&#13;
<code class="n">url</code> <code class="o">=</code> <code class="s2">"https://www.oreilly.com/ideas/what-is-data-science"</code>&#13;
<code class="n">html</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code><code class="o">.</code><code class="n">text</code>&#13;
<code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html5lib'</code><code class="p">)</code>&#13;
&#13;
<code class="n">content</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"div"</code><code class="p">,</code> <code class="s2">"article-body"</code><code class="p">)</code>   <code class="c1"># find article-body div</code>&#13;
<code class="n">regex</code> <code class="o">=</code> <code class="s-Affix">r</code><code class="s2">"[\w']+|[\.]"</code>                       <code class="c1"># matches a word or a period</code>&#13;
&#13;
<code class="n">document</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">paragraph</code> <code class="ow">in</code> <code class="n">content</code><code class="p">(</code><code class="s2">"p"</code><code class="p">):</code>&#13;
    <code class="n">words</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">regex</code><code class="p">,</code> <code class="n">fix_unicode</code><code class="p">(</code><code class="n">paragraph</code><code class="o">.</code><code class="n">text</code><code class="p">))</code>&#13;
    <code class="n">document</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">words</code><code class="p">)</code></pre>&#13;
&#13;
<p>We certainly could (and likely should) clean this data further.&#13;
There is still some amount of&#13;
extraneous text in the document (for example, the first word is <em>Section</em>),&#13;
and we’ve split on midsentence periods (for example, in <em>Web 2.0</em>), and&#13;
there are a handful of captions and lists sprinkled throughout. Having said that,&#13;
we’ll work with the document as it is.</p>&#13;
&#13;
<p>Now that we have the text as a sequence of words,&#13;
we can model language in the following way:&#13;
given some starting word (say, <em>book</em>) we look at all the words that follow it&#13;
in the source document.&#13;
We randomly choose one of these to be the next word, and we repeat the process&#13;
until we get to a period, which signifies the end of the sentence.&#13;
We<a data-primary="bigram models" data-type="indexterm" id="idm45635721256456"/> call this a <em>bigram model</em>, as it is determined completely by the frequencies of the&#13;
bigrams (word pairs) in the original data.</p>&#13;
&#13;
<p>What about a starting word?  We can just pick randomly from words that&#13;
<em>follow</em> a period.  To start, let’s precompute the possible word transitions.&#13;
Recall that <code>zip</code> stops when any of its inputs is done, so that <code>zip(document, document[1:])</code>&#13;
gives us precisely the pairs of consecutive elements of <code>document</code>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">defaultdict</code>&#13;
&#13;
<code class="n">transitions</code> <code class="o">=</code> <code class="n">defaultdict</code><code class="p">(</code><code class="nb">list</code><code class="p">)</code>&#13;
<code class="k">for</code> <code class="n">prev</code><code class="p">,</code> <code class="n">current</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">document</code><code class="p">,</code> <code class="n">document</code><code class="p">[</code><code class="mi">1</code><code class="p">:]):</code>&#13;
    <code class="n">transitions</code><code class="p">[</code><code class="n">prev</code><code class="p">]</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">current</code><code class="p">)</code></pre>&#13;
&#13;
<p>Now we’re ready to generate sentences:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">generate_using_bigrams</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>&#13;
    <code class="n">current</code> <code class="o">=</code> <code class="s2">"."</code>   <code class="c1"># this means the next word will start a sentence</code>&#13;
    <code class="n">result</code> <code class="o">=</code> <code class="p">[]</code>&#13;
    <code class="k">while</code> <code class="bp">True</code><code class="p">:</code>&#13;
        <code class="n">next_word_candidates</code> <code class="o">=</code> <code class="n">transitions</code><code class="p">[</code><code class="n">current</code><code class="p">]</code>    <code class="c1"># bigrams (current, _)</code>&#13;
        <code class="n">current</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">next_word_candidates</code><code class="p">)</code>  <code class="c1"># choose one at random</code>&#13;
        <code class="n">result</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">current</code><code class="p">)</code>                         <code class="c1"># append it to results</code>&#13;
        <code class="k">if</code> <code class="n">current</code> <code class="o">==</code> <code class="s2">"."</code><code class="p">:</code> <code class="k">return</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">result</code><code class="p">)</code>     <code class="c1"># if "." we're done</code></pre>&#13;
&#13;
<p>The sentences it produces are gibberish, but they’re the kind of gibberish you might put on your website if you were trying to sound data-sciencey.  For example:</p>&#13;
<blockquote>&#13;
<p>If you may know which are you want to data sort the data feeds web friend someone on trending topics as the data in Hadoop is the data science requires a book demonstrates why visualizations are but we do massive correlations across many commercial disk drives in Python language and creates more tractable form making connections then use and uses it to solve a data.</p>&#13;
<p data-type="attribution">Bigram Model</p>&#13;
</blockquote>&#13;
&#13;
<p>We<a data-primary="trigrams" data-type="indexterm" id="idm45635721214008"/> can make the sentences less gibberishy by looking at <em>trigrams</em>, triplets of&#13;
consecutive words.  (More generally, you might look at <em>n-grams</em> consisting&#13;
of <em>n</em> consecutive words, but three will be plenty for us.)  Now the transitions&#13;
will depend on the previous <em>two</em> words:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">trigram_transitions</code> <code class="o">=</code> <code class="n">defaultdict</code><code class="p">(</code><code class="nb">list</code><code class="p">)</code>&#13;
<code class="n">starts</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">prev</code><code class="p">,</code> <code class="n">current</code><code class="p">,</code> <code class="nb">next</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">document</code><code class="p">,</code> <code class="n">document</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">document</code><code class="p">[</code><code class="mi">2</code><code class="p">:]):</code>&#13;
&#13;
    <code class="k">if</code> <code class="n">prev</code> <code class="o">==</code> <code class="s2">"."</code><code class="p">:</code>              <code class="c1"># if the previous "word" was a period</code>&#13;
        <code class="n">starts</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">current</code><code class="p">)</code>   <code class="c1"># then this is a start word</code>&#13;
&#13;
    <code class="n">trigram_transitions</code><code class="p">[(</code><code class="n">prev</code><code class="p">,</code> <code class="n">current</code><code class="p">)]</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">next</code><code class="p">)</code></pre>&#13;
&#13;
<p>Notice that now we have to track the starting words separately.&#13;
We can generate sentences in pretty much the same way:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">generate_using_trigrams</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>&#13;
    <code class="n">current</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">starts</code><code class="p">)</code>   <code class="c1"># choose a random starting word</code>&#13;
    <code class="n">prev</code> <code class="o">=</code> <code class="s2">"."</code>                        <code class="c1"># and precede it with a '.'</code>&#13;
    <code class="n">result</code> <code class="o">=</code> <code class="p">[</code><code class="n">current</code><code class="p">]</code>&#13;
    <code class="k">while</code> <code class="bp">True</code><code class="p">:</code>&#13;
        <code class="n">next_word_candidates</code> <code class="o">=</code> <code class="n">trigram_transitions</code><code class="p">[(</code><code class="n">prev</code><code class="p">,</code> <code class="n">current</code><code class="p">)]</code>&#13;
        <code class="n">next_word</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">next_word_candidates</code><code class="p">)</code>&#13;
&#13;
        <code class="n">prev</code><code class="p">,</code> <code class="n">current</code> <code class="o">=</code> <code class="n">current</code><code class="p">,</code> <code class="n">next_word</code>&#13;
        <code class="n">result</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">current</code><code class="p">)</code>&#13;
&#13;
        <code class="k">if</code> <code class="n">current</code> <code class="o">==</code> <code class="s2">"."</code><code class="p">:</code>&#13;
            <code class="k">return</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">result</code><code class="p">)</code></pre>&#13;
&#13;
<p>This produces better sentences like:</p>&#13;
<blockquote>&#13;
<p>In hindsight MapReduce seems like an epidemic and if so does that give us new insights into how economies work That’s not a question we could even have asked a few years there has been instrumented.</p>&#13;
<p data-type="attribution">Trigram Model</p>&#13;
</blockquote>&#13;
&#13;
<p>Of course, they sound better because at each step the generation process has fewer choices,&#13;
and at many steps only a single choice.&#13;
This means that we frequently generate sentences (or at least long phrases)&#13;
that were seen verbatim in the original data.  Having more data would help;&#13;
it would also work better if we collected <em>n</em>-grams from multiple essays about data science.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Grammars" data-type="sect1"><div class="sect1" id="idm45635721452952">&#13;
<h1>Grammars</h1>&#13;
&#13;
<p>A<a data-primary="natural language processing (NLP)" data-secondary="grammars" data-type="indexterm" id="idm45635720905576"/><a data-primary="grammars" data-type="indexterm" id="idm45635720904600"/> different approach to modeling language is with <em>grammars</em>, rules for generating acceptable sentences.  In elementary school, you probably learned about parts of speech and how to combine them.  For example, if you had a really bad English teacher, you might say that a sentence necessarily consists of a <em>noun</em> followed by a <em>verb</em>.  If you then have a list of nouns and verbs, you can generate sentences according to the rule.</p>&#13;
&#13;
<p>We’ll define a slightly more complicated grammar:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code><code class="p">,</code> <code class="n">Dict</code>&#13;
&#13;
<code class="c1"># Type alias to refer to grammars later</code>&#13;
<code class="n">Grammar</code> <code class="o">=</code> <code class="n">Dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]]</code>&#13;
&#13;
<code class="n">grammar</code> <code class="o">=</code> <code class="p">{</code>&#13;
    <code class="s2">"_S"</code>  <code class="p">:</code> <code class="p">[</code><code class="s2">"_NP _VP"</code><code class="p">],</code>&#13;
    <code class="s2">"_NP"</code> <code class="p">:</code> <code class="p">[</code><code class="s2">"_N"</code><code class="p">,</code>&#13;
             <code class="s2">"_A _NP _P _A _N"</code><code class="p">],</code>&#13;
    <code class="s2">"_VP"</code> <code class="p">:</code> <code class="p">[</code><code class="s2">"_V"</code><code class="p">,</code>&#13;
             <code class="s2">"_V _NP"</code><code class="p">],</code>&#13;
    <code class="s2">"_N"</code>  <code class="p">:</code> <code class="p">[</code><code class="s2">"data science"</code><code class="p">,</code> <code class="s2">"Python"</code><code class="p">,</code> <code class="s2">"regression"</code><code class="p">],</code>&#13;
    <code class="s2">"_A"</code>  <code class="p">:</code> <code class="p">[</code><code class="s2">"big"</code><code class="p">,</code> <code class="s2">"linear"</code><code class="p">,</code> <code class="s2">"logistic"</code><code class="p">],</code>&#13;
    <code class="s2">"_P"</code>  <code class="p">:</code> <code class="p">[</code><code class="s2">"about"</code><code class="p">,</code> <code class="s2">"near"</code><code class="p">],</code>&#13;
    <code class="s2">"_V"</code>  <code class="p">:</code> <code class="p">[</code><code class="s2">"learns"</code><code class="p">,</code> <code class="s2">"trains"</code><code class="p">,</code> <code class="s2">"tests"</code><code class="p">,</code> <code class="s2">"is"</code><code class="p">]</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>I made up the convention that names starting with underscores refer to <em>rules</em> that need further expanding, and that other names are <em>terminals</em> that don’t need further processing.</p>&#13;
&#13;
<p>So, for example, <code>"_S"</code> is the “sentence” rule,&#13;
which produces an <code>"_NP"</code> (“noun phrase”) rule followed by a <code>"_VP"</code> (“verb phrase”) rule.</p>&#13;
&#13;
<p>The verb phrase rule can produce either the <code>"_V"</code> (“verb”) rule, or the verb rule followed by the noun phrase rule.</p>&#13;
&#13;
<p>Notice that the <code>"_NP"</code> rule contains itself in one of its productions.  Grammars can be recursive, which allows even finite grammars like this to generate infinitely many different sentences.</p>&#13;
&#13;
<p>How do we generate sentences from this grammar?  We’ll start with a list containing the sentence rule <code>["_S"]</code>.  And then we’ll repeatedly expand each rule by replacing it with a randomly chosen one of its productions.  We stop when we have a list consisting solely of terminals.</p>&#13;
&#13;
<p>For example, one such progression might look like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="p">[</code><code class="s1">'_S'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'_NP'</code><code class="p">,</code><code class="s1">'_VP'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'_N'</code><code class="p">,</code><code class="s1">'_VP'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'_VP'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'_V'</code><code class="p">,</code><code class="s1">'_NP'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'_NP'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'_A'</code><code class="p">,</code><code class="s1">'_NP'</code><code class="p">,</code><code class="s1">'_P'</code><code class="p">,</code><code class="s1">'_A'</code><code class="p">,</code><code class="s1">'_N'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'_NP'</code><code class="p">,</code><code class="s1">'_P'</code><code class="p">,</code><code class="s1">'_A'</code><code class="p">,</code><code class="s1">'_N'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'_N'</code><code class="p">,</code><code class="s1">'_P'</code><code class="p">,</code><code class="s1">'_A'</code><code class="p">,</code><code class="s1">'_N'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'data science'</code><code class="p">,</code><code class="s1">'_P'</code><code class="p">,</code><code class="s1">'_A'</code><code class="p">,</code><code class="s1">'_N'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'data science'</code><code class="p">,</code><code class="s1">'about'</code><code class="p">,</code><code class="s1">'_A'</code><code class="p">,</code> <code class="s1">'_N'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'data science'</code><code class="p">,</code><code class="s1">'about'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'_N'</code><code class="p">]</code>&#13;
<code class="p">[</code><code class="s1">'Python'</code><code class="p">,</code><code class="s1">'trains'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'data science'</code><code class="p">,</code><code class="s1">'about'</code><code class="p">,</code><code class="s1">'logistic'</code><code class="p">,</code><code class="s1">'Python'</code><code class="p">]</code></pre>&#13;
&#13;
<p>How do we implement this?&#13;
Well, to start, we’ll create a simple helper function to identify terminals:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">is_terminal</code><code class="p">(</code><code class="n">token</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">bool</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">token</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">!=</code> <code class="s2">"_"</code></pre>&#13;
&#13;
<p>Next we need to write a function to turn a list of tokens into a sentence. We’ll look for the first nonterminal token. If we can’t find one, that means we have a completed sentence and we’re done.</p>&#13;
&#13;
<p>If we do find a nonterminal, then we randomly choose one of its productions. If that production is a terminal (i.e., a word), we simply replace the token with it. Otherwise, it’s a sequence of space-separated nonterminal tokens that we need to <code>split</code> and then splice into the current tokens. Either way, we repeat the process on the new set of tokens.</p>&#13;
&#13;
<p>Putting it all together, we get:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">expand</code><code class="p">(</code><code class="n">grammar</code><code class="p">:</code> <code class="n">Grammar</code><code class="p">,</code> <code class="n">tokens</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]:</code>&#13;
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">token</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">tokens</code><code class="p">):</code>&#13;
        <code class="c1"># If this is a terminal token, skip it.</code>&#13;
        <code class="k">if</code> <code class="n">is_terminal</code><code class="p">(</code><code class="n">token</code><code class="p">):</code> <code class="k">continue</code>&#13;
&#13;
        <code class="c1"># Otherwise, it's a nonterminal token,</code>&#13;
        <code class="c1"># so we need to choose a replacement at random.</code>&#13;
        <code class="n">replacement</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">grammar</code><code class="p">[</code><code class="n">token</code><code class="p">])</code>&#13;
&#13;
        <code class="k">if</code> <code class="n">is_terminal</code><code class="p">(</code><code class="n">replacement</code><code class="p">):</code>&#13;
            <code class="n">tokens</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">replacement</code>&#13;
        <code class="k">else</code><code class="p">:</code>&#13;
            <code class="c1"># Replacement could be, e.g., "_NP _VP", so we need to</code>&#13;
            <code class="c1"># split it on spaces and splice it in.</code>&#13;
            <code class="n">tokens</code> <code class="o">=</code> <code class="n">tokens</code><code class="p">[:</code><code class="n">i</code><code class="p">]</code> <code class="o">+</code> <code class="n">replacement</code><code class="o">.</code><code class="n">split</code><code class="p">()</code> <code class="o">+</code> <code class="n">tokens</code><code class="p">[(</code><code class="n">i</code><code class="o">+</code><code class="mi">1</code><code class="p">):]</code>&#13;
&#13;
        <code class="c1"># Now call expand on the new list of tokens.</code>&#13;
        <code class="k">return</code> <code class="n">expand</code><code class="p">(</code><code class="n">grammar</code><code class="p">,</code> <code class="n">tokens</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># If we get here, we had all terminals and are done.</code>&#13;
    <code class="k">return</code> <code class="n">tokens</code></pre>&#13;
&#13;
<p>And now we can start generating sentences:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">generate_sentence</code><code class="p">(</code><code class="n">grammar</code><code class="p">:</code> <code class="n">Grammar</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]:</code>&#13;
    <code class="k">return</code> <code class="n">expand</code><code class="p">(</code><code class="n">grammar</code><code class="p">,</code> <code class="p">[</code><code class="s2">"_S"</code><code class="p">])</code></pre>&#13;
&#13;
<p>Try changing the grammar—add more words, add more rules, add your own parts of speech—until you’re ready to generate as many web pages as your company needs.</p>&#13;
&#13;
<p>Grammars are actually more interesting when they’re used in the other direction.&#13;
Given a sentence, we can use a grammar to <em>parse</em> the sentence.&#13;
This then allows us to identify subjects and verbs and helps us make sense of the sentence.</p>&#13;
&#13;
<p>Using data science to generate text is a neat trick; using it to <em>understand</em> text is more magical. (See <a data-type="xref" href="#nlp-further-invest">“For Further Exploration”</a> for libraries that you could use for this.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="An Aside: Gibbs Sampling" data-type="sect1"><div class="sect1" id="idm45635720906424">&#13;
<h1>An Aside: Gibbs Sampling</h1>&#13;
&#13;
<p>Generating<a data-primary="Gibbs sampling" data-type="indexterm" id="idm45635720321208"/><a data-primary="natural language processing (NLP)" data-secondary="Gibbs sampling" data-type="indexterm" id="idm45635720320472"/> samples from some distributions is easy.&#13;
We can get uniform random variables with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code></pre>&#13;
&#13;
<p>and normal random variables with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">inverse_normal_cdf</code><code class="p">(</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">())</code></pre>&#13;
&#13;
<p>But some distributions are harder to sample from.&#13;
<em>Gibbs sampling</em> is a technique for generating samples&#13;
from multidimensional distributions&#13;
when we only know some of the conditional distributions.</p>&#13;
&#13;
<p>For example, imagine rolling two dice.  Let <em>x</em> be the value of the first die and <em>y</em> be the sum of the dice, and imagine you wanted to generate lots of (<em>x</em>, <em>y</em>) pairs.  In this case&#13;
it’s easy to generate the samples directly:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Tuple</code>&#13;
<code class="kn">import</code> <code class="nn">random</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">roll_a_die</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">])</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">direct_sample</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="nb">int</code><code class="p">]:</code>&#13;
    <code class="n">d1</code> <code class="o">=</code> <code class="n">roll_a_die</code><code class="p">()</code>&#13;
    <code class="n">d2</code> <code class="o">=</code> <code class="n">roll_a_die</code><code class="p">()</code>&#13;
    <code class="k">return</code> <code class="n">d1</code><code class="p">,</code> <code class="n">d1</code> <code class="o">+</code> <code class="n">d2</code></pre>&#13;
&#13;
<p>But imagine that you only knew the conditional distributions.  The distribution of <em>y</em> conditional on <em>x</em> is easy—if you know the value of <em>x</em>, <em>y</em> is equally likely to be <em>x</em> + 1, <em>x</em> + 2, <em>x</em> + 3, <em>x</em> + 4, <em>x</em> + 5, or <em>x</em> + 6:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">random_y_given_x</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
    <code class="sd">"""equally likely to be x + 1, x + 2, ... , x + 6"""</code>&#13;
    <code class="k">return</code> <code class="n">x</code> <code class="o">+</code> <code class="n">roll_a_die</code><code class="p">()</code></pre>&#13;
&#13;
<p>The other direction is more complicated.  For example, if you know that <em>y</em> is 2, then necessarily&#13;
<em>x</em> is 1 (since the only way two dice can sum to 2 is if both of them are 1).  If you know <em>y</em> is 3, then <em>x</em> is equally likely to be 1 or 2.  Similarly, if <em>y</em> is 11, then <em>x</em> has to be either 5 or 6:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">random_x_given_y</code><code class="p">(</code><code class="n">y</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="n">y</code> <code class="o">&lt;=</code> <code class="mi">7</code><code class="p">:</code>&#13;
        <code class="c1"># if the total is 7 or less, the first die is equally likely to be</code>&#13;
        <code class="c1"># 1, 2, ..., (total - 1)</code>&#13;
        <code class="k">return</code> <code class="n">random</code><code class="o">.</code><code class="n">randrange</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="c1"># if the total is 7 or more, the first die is equally likely to be</code>&#13;
        <code class="c1"># (total - 6), (total - 5), ..., 6</code>&#13;
        <code class="k">return</code> <code class="n">random</code><code class="o">.</code><code class="n">randrange</code><code class="p">(</code><code class="n">y</code> <code class="o">-</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">7</code><code class="p">)</code></pre>&#13;
&#13;
<p>The way Gibbs sampling works is that we start with any (valid) values for <em>x</em> and <em>y</em> and then repeatedly alternate replacing <em>x</em> with a random value picked conditional on <em>y</em> and replacing <em>y</em> with a random value picked conditional on <em>x</em>.  After a number of iterations, the resulting values&#13;
of <em>x</em> and <em>y</em> will represent a sample from the unconditional joint distribution:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">gibbs_sample</code><code class="p">(</code><code class="n">num_iters</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">100</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="nb">int</code><code class="p">]:</code>&#13;
    <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code> <code class="c1"># doesn't really matter</code>&#13;
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_iters</code><code class="p">):</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="n">random_x_given_y</code><code class="p">(</code><code class="n">y</code><code class="p">)</code>&#13;
        <code class="n">y</code> <code class="o">=</code> <code class="n">random_y_given_x</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code></pre>&#13;
&#13;
<p>You can check that this gives similar results to the direct sample:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">compare_distributions</code><code class="p">(</code><code class="n">num_samples</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">1000</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Dict</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="n">List</code><code class="p">[</code><code class="nb">int</code><code class="p">]]:</code>&#13;
    <code class="n">counts</code> <code class="o">=</code> <code class="n">defaultdict</code><code class="p">(</code><code class="k">lambda</code><code class="p">:</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code>&#13;
    <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_samples</code><code class="p">):</code>&#13;
        <code class="n">counts</code><code class="p">[</code><code class="n">gibbs_sample</code><code class="p">()][</code><code class="mi">0</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
        <code class="n">counts</code><code class="p">[</code><code class="n">direct_sample</code><code class="p">()][</code><code class="mi">1</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
    <code class="k">return</code> <code class="n">counts</code></pre>&#13;
&#13;
<p>We’ll use this technique in the next section.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Topic Modeling" data-type="sect1"><div class="sect1" id="topic_modeling">&#13;
<h1>Topic Modeling</h1>&#13;
&#13;
<p>When<a data-primary="natural language processing (NLP)" data-secondary="topic modeling" data-type="indexterm" id="NLPtopic21"/><a data-primary="topic modeling" data-type="indexterm" id="topmod21"/><a data-primary="modeling" data-type="indexterm" id="mod21"/> we built our “Data Scientists You May Know” recommender in <a data-type="xref" href="ch01.html#introduction">Chapter 1</a>,&#13;
we simply looked for exact matches in people’s stated interests.</p>&#13;
&#13;
<p>A<a data-primary="Latent Dirichlet Analysis (LDA)" data-type="indexterm" id="idm45635719926840"/> more sophisticated approach to understanding our users’ interests&#13;
might try to identify the <em>topics</em> that underlie those interests.&#13;
A technique called <em>latent Dirichlet allocation</em> (LDA) is commonly used to identify common topics in a set of documents.  We’ll apply it to documents that consist of each user’s interests.</p>&#13;
&#13;
<p>LDA has some similarities to the Naive Bayes classifier we built in <a data-type="xref" href="ch13.html#naive_bayes">Chapter 13</a>,&#13;
in that it assumes a probabilistic model for documents.&#13;
We’ll gloss over the hairier mathematical details,&#13;
but for our purposes the model assumes that:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>There is some fixed number <em>K</em> of topics.</p>&#13;
</li>&#13;
<li>&#13;
<p>There is a random variable that assigns each topic an associated probability distribution over words.  You should think of this distribution as the probability of seeing word <em>w</em> given topic <em>k</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>There is another random variable that assigns each document a probability distribution over topics.  You should think of this distribution as the mixture of topics in document <em>d</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Each word in a document was generated by first randomly picking a topic (from the document’s distribution of topics) and then randomly picking a word (from the topic’s distribution of words).</p>&#13;
</li>&#13;
</ul>&#13;
<!--there are another two bad quotes in the preceding list-->&#13;
&#13;
<p>In particular, we have a collection of <code>documents</code>, each of which is a <code>list</code> of words.  And we have a corresponding collection of <code>document_topics</code> that assigns a topic (here a number between 0 and <em>K</em> – 1) to each word in each document.</p>&#13;
&#13;
<p>So, the fifth word in the fourth document is:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">documents</code><code class="p">[</code><code class="mi">3</code><code class="p">][</code><code class="mi">4</code><code class="p">]</code></pre>&#13;
&#13;
<p>and the topic from which that word was chosen is:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">document_topics</code><code class="p">[</code><code class="mi">3</code><code class="p">][</code><code class="mi">4</code><code class="p">]</code></pre>&#13;
&#13;
<p>This very explicitly defines each document’s distribution over topics, and it implicitly defines each topic’s distribution over words.</p>&#13;
&#13;
<p>We can estimate the likelihood that topic 1 produces a certain word by comparing how many times topic 1 produces that word with how many times topic 1 produces <em>any</em> word.  (Similarly, when we built a spam filter in <a data-type="xref" href="ch13.html#naive_bayes">Chapter 13</a>, we compared how many times each word appeared in spams with the total number of words appearing in spams.)</p>&#13;
&#13;
<p>Although these topics are just numbers, we can give them descriptive names by looking at the words on which they put the heaviest weight.  We just have to somehow generate the <code>document_topics</code>.  This is where Gibbs sampling comes into play.</p>&#13;
&#13;
<p>We start by assigning every word in every document a topic completely at random.  Now we go through each document one word at a time.  For that word and document, we construct weights for each topic that depend on the&#13;
(current) distribution of topics in that document and the (current) distribution of words for that topic.  We then use those weights to sample a new topic for that word.  If we iterate this process many times, we will end up with a joint sample from the topic–word distribution and the document–topic distribution.</p>&#13;
&#13;
<p>To start with, we’ll need a function to randomly choose an index based on an arbitrary set of weights:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">sample_from</code><code class="p">(</code><code class="n">weights</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
    <code class="sd">"""returns i with probability weights[i] / sum(weights)"""</code>&#13;
    <code class="n">total</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">weights</code><code class="p">)</code>&#13;
    <code class="n">rnd</code> <code class="o">=</code> <code class="n">total</code> <code class="o">*</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code>      <code class="c1"># uniform between 0 and total</code>&#13;
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">w</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">weights</code><code class="p">):</code>&#13;
        <code class="n">rnd</code> <code class="o">-=</code> <code class="n">w</code>                       <code class="c1"># return the smallest i such that</code>&#13;
        <code class="k">if</code> <code class="n">rnd</code> <code class="o">&lt;=</code> <code class="mi">0</code><code class="p">:</code> <code class="k">return</code> <code class="n">i</code>          <code class="c1"># weights[0] + ... + weights[i] &gt;= rnd</code></pre>&#13;
&#13;
<p>For instance, if you give it weights <code>[1, 1, 3]</code>, then one-fifth of the time it will return 0, one-fifth of the time it will return 1, and three-fifths of the time it will return 2. Let’s write a test:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>&#13;
&#13;
<code class="c1"># Draw 1000 times and count</code>&#13;
<code class="n">draws</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">(</code><code class="n">sample_from</code><code class="p">([</code><code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">,</code> <code class="mf">0.8</code><code class="p">])</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">))</code>&#13;
<code class="k">assert</code> <code class="mi">10</code> <code class="o">&lt;</code> <code class="n">draws</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">190</code>   <code class="c1"># should be ~10%, this is a really loose test</code>&#13;
<code class="k">assert</code> <code class="mi">10</code> <code class="o">&lt;</code> <code class="n">draws</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">190</code>   <code class="c1"># should be ~10%, this is a really loose test</code>&#13;
<code class="k">assert</code> <code class="mi">650</code> <code class="o">&lt;</code> <code class="n">draws</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">950</code>  <code class="c1"># should be ~80%, this is a really loose test</code>&#13;
<code class="k">assert</code> <code class="n">draws</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">+</code> <code class="n">draws</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">+</code> <code class="n">draws</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="o">==</code> <code class="mi">1000</code></pre>&#13;
&#13;
<p>Our documents are our users’ interests, which look like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">documents</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="p">[</code><code class="s2">"Hadoop"</code><code class="p">,</code> <code class="s2">"Big Data"</code><code class="p">,</code> <code class="s2">"HBase"</code><code class="p">,</code> <code class="s2">"Java"</code><code class="p">,</code> <code class="s2">"Spark"</code><code class="p">,</code> <code class="s2">"Storm"</code><code class="p">,</code> <code class="s2">"Cassandra"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"NoSQL"</code><code class="p">,</code> <code class="s2">"MongoDB"</code><code class="p">,</code> <code class="s2">"Cassandra"</code><code class="p">,</code> <code class="s2">"HBase"</code><code class="p">,</code> <code class="s2">"Postgres"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"Python"</code><code class="p">,</code> <code class="s2">"scikit-learn"</code><code class="p">,</code> <code class="s2">"scipy"</code><code class="p">,</code> <code class="s2">"numpy"</code><code class="p">,</code> <code class="s2">"statsmodels"</code><code class="p">,</code> <code class="s2">"pandas"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"R"</code><code class="p">,</code> <code class="s2">"Python"</code><code class="p">,</code> <code class="s2">"statistics"</code><code class="p">,</code> <code class="s2">"regression"</code><code class="p">,</code> <code class="s2">"probability"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"machine learning"</code><code class="p">,</code> <code class="s2">"regression"</code><code class="p">,</code> <code class="s2">"decision trees"</code><code class="p">,</code> <code class="s2">"libsvm"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"Python"</code><code class="p">,</code> <code class="s2">"R"</code><code class="p">,</code> <code class="s2">"Java"</code><code class="p">,</code> <code class="s2">"C++"</code><code class="p">,</code> <code class="s2">"Haskell"</code><code class="p">,</code> <code class="s2">"programming languages"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"statistics"</code><code class="p">,</code> <code class="s2">"probability"</code><code class="p">,</code> <code class="s2">"mathematics"</code><code class="p">,</code> <code class="s2">"theory"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"machine learning"</code><code class="p">,</code> <code class="s2">"scikit-learn"</code><code class="p">,</code> <code class="s2">"Mahout"</code><code class="p">,</code> <code class="s2">"neural networks"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"neural networks"</code><code class="p">,</code> <code class="s2">"deep learning"</code><code class="p">,</code> <code class="s2">"Big Data"</code><code class="p">,</code> <code class="s2">"artificial intelligence"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"Hadoop"</code><code class="p">,</code> <code class="s2">"Java"</code><code class="p">,</code> <code class="s2">"MapReduce"</code><code class="p">,</code> <code class="s2">"Big Data"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"statistics"</code><code class="p">,</code> <code class="s2">"R"</code><code class="p">,</code> <code class="s2">"statsmodels"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"C++"</code><code class="p">,</code> <code class="s2">"deep learning"</code><code class="p">,</code> <code class="s2">"artificial intelligence"</code><code class="p">,</code> <code class="s2">"probability"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"pandas"</code><code class="p">,</code> <code class="s2">"R"</code><code class="p">,</code> <code class="s2">"Python"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"databases"</code><code class="p">,</code> <code class="s2">"HBase"</code><code class="p">,</code> <code class="s2">"Postgres"</code><code class="p">,</code> <code class="s2">"MySQL"</code><code class="p">,</code> <code class="s2">"MongoDB"</code><code class="p">],</code>&#13;
    <code class="p">[</code><code class="s2">"libsvm"</code><code class="p">,</code> <code class="s2">"regression"</code><code class="p">,</code> <code class="s2">"support vector machines"</code><code class="p">]</code>&#13;
<code class="p">]</code></pre>&#13;
&#13;
<p>And we’ll try to find:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">K</code> <code class="o">=</code> <code class="mi">4</code></pre>&#13;
&#13;
<p>topics. In order to calculate the sampling weights, we’ll need to keep track of several counts.  Let’s first create the data structures for them.</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>How many times each topic is assigned to each document:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># a list of Counters, one for each document</code>&#13;
<code class="n">document_topic_counts</code> <code class="o">=</code> <code class="p">[</code><code class="n">Counter</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">documents</code><code class="p">]</code></pre>&#13;
</li>&#13;
<li>&#13;
<p>How many times each word is assigned to each topic:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># a list of Counters, one for each topic</code>&#13;
<code class="n">topic_word_counts</code> <code class="o">=</code> <code class="p">[</code><code class="n">Counter</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">K</code><code class="p">)]</code></pre>&#13;
</li>&#13;
<li>&#13;
<p>The total number of words assigned to each topic:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># a list of numbers, one for each topic</code>&#13;
<code class="n">topic_counts</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">K</code><code class="p">)]</code></pre>&#13;
</li>&#13;
<li>&#13;
<p>The total number of words contained in each document:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># a list of numbers, one for each document</code>&#13;
<code class="n">document_lengths</code> <code class="o">=</code> <code class="p">[</code><code class="nb">len</code><code class="p">(</code><code class="n">document</code><code class="p">)</code> <code class="k">for</code> <code class="n">document</code> <code class="ow">in</code> <code class="n">documents</code><code class="p">]</code></pre>&#13;
</li>&#13;
<li>&#13;
<p>The number of distinct words:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">distinct_words</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">word</code> <code class="k">for</code> <code class="n">document</code> <code class="ow">in</code> <code class="n">documents</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">document</code><code class="p">)</code>&#13;
<code class="n">W</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">distinct_words</code><code class="p">)</code></pre>&#13;
</li>&#13;
<li>&#13;
<p>And the number of documents:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">D</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">documents</code><code class="p">)</code></pre>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Once we populate these, we can find, for example, the number of words in <code>documents[3]</code> associated with topic 1 as follows:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">document_topic_counts</code><code class="p">[</code><code class="mi">3</code><code class="p">][</code><code class="mi">1</code><code class="p">]</code></pre>&#13;
&#13;
<p>And we can find the number of times <em>nlp</em> is associated with topic 2 as follows:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">topic_word_counts</code><code class="p">[</code><code class="mi">2</code><code class="p">][</code><code class="s2">"nlp"</code><code class="p">]</code></pre>&#13;
&#13;
<p>Now we’re ready to define our conditional probability functions.  As in <a data-type="xref" href="ch13.html#naive_bayes">Chapter 13</a>, each has a smoothing term that ensures every topic has a nonzero chance of being chosen in any document and that every word has a nonzero chance of being chosen for any topic:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">p_topic_given_document</code><code class="p">(</code><code class="n">topic</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">d</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">alpha</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    The fraction of words in document 'd'</code>&#13;
<code class="sd">    that are assigned to 'topic' (plus some smoothing)</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="p">((</code><code class="n">document_topic_counts</code><code class="p">[</code><code class="n">d</code><code class="p">][</code><code class="n">topic</code><code class="p">]</code> <code class="o">+</code> <code class="n">alpha</code><code class="p">)</code> <code class="o">/</code>&#13;
            <code class="p">(</code><code class="n">document_lengths</code><code class="p">[</code><code class="n">d</code><code class="p">]</code> <code class="o">+</code> <code class="n">K</code> <code class="o">*</code> <code class="n">alpha</code><code class="p">))</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">p_word_given_topic</code><code class="p">(</code><code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">topic</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    The fraction of words assigned to 'topic'</code>&#13;
<code class="sd">    that equal 'word' (plus some smoothing)</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="p">((</code><code class="n">topic_word_counts</code><code class="p">[</code><code class="n">topic</code><code class="p">][</code><code class="n">word</code><code class="p">]</code> <code class="o">+</code> <code class="n">beta</code><code class="p">)</code> <code class="o">/</code>&#13;
            <code class="p">(</code><code class="n">topic_counts</code><code class="p">[</code><code class="n">topic</code><code class="p">]</code> <code class="o">+</code> <code class="n">W</code> <code class="o">*</code> <code class="n">beta</code><code class="p">))</code></pre>&#13;
&#13;
<p>We’ll use these to create the weights for updating topics:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">topic_weight</code><code class="p">(</code><code class="n">d</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">k</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Given a document and a word in that document,</code>&#13;
<code class="sd">    return the weight for the kth topic</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="n">p_word_given_topic</code><code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">k</code><code class="p">)</code> <code class="o">*</code> <code class="n">p_topic_given_document</code><code class="p">(</code><code class="n">k</code><code class="p">,</code> <code class="n">d</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">choose_new_topic</code><code class="p">(</code><code class="n">d</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">sample_from</code><code class="p">([</code><code class="n">topic_weight</code><code class="p">(</code><code class="n">d</code><code class="p">,</code> <code class="n">word</code><code class="p">,</code> <code class="n">k</code><code class="p">)</code>&#13;
                        <code class="k">for</code> <code class="n">k</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">K</code><code class="p">)])</code></pre>&#13;
&#13;
<p>There are solid mathematical reasons why <code>topic_weight</code> is defined the way&#13;
it is, but their details would lead us too far afield.  Hopefully it makes&#13;
at least intuitive sense that—given a word and its document—the likelihood of any topic choice&#13;
depends on both how likely that topic is for the document and&#13;
how likely that word is for the topic.</p>&#13;
&#13;
<p>This is all the machinery we need.  We start by assigning every word&#13;
to a random topic and populating our counters appropriately:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">document_topics</code> <code class="o">=</code> <code class="p">[[</code><code class="n">random</code><code class="o">.</code><code class="n">randrange</code><code class="p">(</code><code class="n">K</code><code class="p">)</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">document</code><code class="p">]</code>&#13;
                   <code class="k">for</code> <code class="n">document</code> <code class="ow">in</code> <code class="n">documents</code><code class="p">]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">d</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">D</code><code class="p">):</code>&#13;
    <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">topic</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">documents</code><code class="p">[</code><code class="n">d</code><code class="p">],</code> <code class="n">document_topics</code><code class="p">[</code><code class="n">d</code><code class="p">]):</code>&#13;
        <code class="n">document_topic_counts</code><code class="p">[</code><code class="n">d</code><code class="p">][</code><code class="n">topic</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
        <code class="n">topic_word_counts</code><code class="p">[</code><code class="n">topic</code><code class="p">][</code><code class="n">word</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
        <code class="n">topic_counts</code><code class="p">[</code><code class="n">topic</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code></pre>&#13;
&#13;
<p>Our goal is to get a joint sample of the topics–word distribution&#13;
and the documents–topic distribution.  We do this using a form of Gibbs sampling that uses the conditional probabilities defined previously:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">tqdm</code>&#13;
&#13;
<code class="k">for</code> <code class="nb">iter</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="mi">1000</code><code class="p">):</code>&#13;
    <code class="k">for</code> <code class="n">d</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">D</code><code class="p">):</code>&#13;
        <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">topic</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">documents</code><code class="p">[</code><code class="n">d</code><code class="p">],</code>&#13;
                                              <code class="n">document_topics</code><code class="p">[</code><code class="n">d</code><code class="p">])):</code>&#13;
&#13;
            <code class="c1"># remove this word / topic from the counts</code>&#13;
            <code class="c1"># so that it doesn't influence the weights</code>&#13;
            <code class="n">document_topic_counts</code><code class="p">[</code><code class="n">d</code><code class="p">][</code><code class="n">topic</code><code class="p">]</code> <code class="o">-=</code> <code class="mi">1</code>&#13;
            <code class="n">topic_word_counts</code><code class="p">[</code><code class="n">topic</code><code class="p">][</code><code class="n">word</code><code class="p">]</code> <code class="o">-=</code> <code class="mi">1</code>&#13;
            <code class="n">topic_counts</code><code class="p">[</code><code class="n">topic</code><code class="p">]</code> <code class="o">-=</code> <code class="mi">1</code>&#13;
            <code class="n">document_lengths</code><code class="p">[</code><code class="n">d</code><code class="p">]</code> <code class="o">-=</code> <code class="mi">1</code>&#13;
&#13;
            <code class="c1"># choose a new topic based on the weights</code>&#13;
            <code class="n">new_topic</code> <code class="o">=</code> <code class="n">choose_new_topic</code><code class="p">(</code><code class="n">d</code><code class="p">,</code> <code class="n">word</code><code class="p">)</code>&#13;
            <code class="n">document_topics</code><code class="p">[</code><code class="n">d</code><code class="p">][</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">new_topic</code>&#13;
&#13;
            <code class="c1"># and now add it back to the counts</code>&#13;
            <code class="n">document_topic_counts</code><code class="p">[</code><code class="n">d</code><code class="p">][</code><code class="n">new_topic</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
            <code class="n">topic_word_counts</code><code class="p">[</code><code class="n">new_topic</code><code class="p">][</code><code class="n">word</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
            <code class="n">topic_counts</code><code class="p">[</code><code class="n">new_topic</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
            <code class="n">document_lengths</code><code class="p">[</code><code class="n">d</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code></pre>&#13;
&#13;
<p>What are the topics?  They’re just numbers 0, 1, 2, and 3.  If we want&#13;
names for them, we have to do that ourselves.&#13;
Let’s look at the five most heavily weighted words for each (<a data-type="xref" href="#table20-1">Table 21-1</a>):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">for</code> <code class="n">k</code><code class="p">,</code> <code class="n">word_counts</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">topic_word_counts</code><code class="p">):</code>&#13;
    <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">count</code> <code class="ow">in</code> <code class="n">word_counts</code><code class="o">.</code><code class="n">most_common</code><code class="p">():</code>&#13;
        <code class="k">if</code> <code class="n">count</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>&#13;
            <code class="k">print</code><code class="p">(</code><code class="n">k</code><code class="p">,</code> <code class="n">word</code><code class="p">,</code> <code class="n">count</code><code class="p">)</code></pre>&#13;
<table id="table20-1">&#13;
<caption><span class="label">Table 21-1. </span>Most common words per topic</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Topic 0</th>&#13;
<th>Topic 1</th>&#13;
<th>Topic 2</th>&#13;
<th>Topic 3</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Java</p></td>&#13;
<td><p>R</p></td>&#13;
<td><p>HBase</p></td>&#13;
<td><p>regression</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Big Data</p></td>&#13;
<td><p>statistics</p></td>&#13;
<td><p>Postgres</p></td>&#13;
<td><p>libsvm</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Hadoop</p></td>&#13;
<td><p>Python</p></td>&#13;
<td><p>MongoDB</p></td>&#13;
<td><p>scikit-learn</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>deep learning</p></td>&#13;
<td><p>probability</p></td>&#13;
<td><p>Cassandra</p></td>&#13;
<td><p>machine learning</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>artificial intelligence</p></td>&#13;
<td><p>pandas</p></td>&#13;
<td><p>NoSQL</p></td>&#13;
<td><p>neural networks</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Based on these I’d probably assign topic names:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">topic_names</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"Big Data and programming languages"</code><code class="p">,</code>&#13;
               <code class="s2">"Python and statistics"</code><code class="p">,</code>&#13;
               <code class="s2">"databases"</code><code class="p">,</code>&#13;
               <code class="s2">"machine learning"</code><code class="p">]</code></pre>&#13;
&#13;
<p>at which point we can see how the model assigns topics to each user’s interests:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">for</code> <code class="n">document</code><code class="p">,</code> <code class="n">topic_counts</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">documents</code><code class="p">,</code> <code class="n">document_topic_counts</code><code class="p">):</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">document</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="n">topic</code><code class="p">,</code> <code class="n">count</code> <code class="ow">in</code> <code class="n">topic_counts</code><code class="o">.</code><code class="n">most_common</code><code class="p">():</code>&#13;
        <code class="k">if</code> <code class="n">count</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>&#13;
            <code class="k">print</code><code class="p">(</code><code class="n">topic_names</code><code class="p">[</code><code class="n">topic</code><code class="p">],</code> <code class="n">count</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">()</code></pre>&#13;
&#13;
<p>which gives:</p>&#13;
&#13;
<pre data-type="programlisting">['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']&#13;
Big Data and programming languages 4 databases 3&#13;
['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']&#13;
databases 5&#13;
['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']&#13;
Python and statistics 5 machine learning 1</pre>&#13;
&#13;
<p>and so on.  Given the “ands” we needed in some of our topic names, it’s possible&#13;
we should use more topics, although most likely we don’t have enough&#13;
data to successfully learn them.<a data-primary="" data-startref="mod21" data-type="indexterm" id="idm45635718413256"/><a data-primary="" data-startref="topmod21" data-type="indexterm" id="idm45635718412344"/><a data-primary="" data-startref="NLPtopic21" data-type="indexterm" id="idm45635718411400"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Word Vectors" data-type="sect1"><div class="sect1" id="word_vectors">&#13;
<h1>Word Vectors</h1>&#13;
&#13;
<p>A<a data-primary="natural language processing (NLP)" data-secondary="word vectors" data-type="indexterm" id="NLPworvec21"/><a data-primary="word vectors" data-type="indexterm" id="wvect21"/> lot of recent advances in NLP involve deep learning.&#13;
In the rest of this chapter we’ll look at a couple of them&#13;
using the machinery we developed in <a data-type="xref" href="ch19.html#deep_learning">Chapter 19</a>.</p>&#13;
&#13;
<p>One important innovation involves representing words as low-dimensional vectors.&#13;
These vectors can be compared, added together, fed into machine learning models, or&#13;
anything else you want to do with them. They usually have nice properties; for example,&#13;
similar words tend to have similar vectors. That is, typically&#13;
the word vector for <em>big</em> is pretty close to the word vector for <em>large</em>, so that a model operating on word vectors can (to some degree) handle things like synonymy for free.</p>&#13;
&#13;
<p>Frequently the vectors will exhibit delightful arithmetic properties as well. For instance, in some such models if you take the vector for <em>king</em>, subtract the vector for <em>man</em>, and add the vector for <em>woman</em>, you will end up with a vector that’s very close to the vector for <em>queen</em>. It can be interesting to ponder what this means about what the word vectors actually “learn,” although we won’t spend time on that here.</p>&#13;
&#13;
<p>Coming up with such vectors for a large vocabulary of words is a difficult undertaking,&#13;
so typically we’ll <em>learn</em> them from a corpus of text. There are a couple of different schemes,&#13;
but at a high level the task typically looks something like this:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Get a bunch of text.</p>&#13;
</li>&#13;
<li>&#13;
<p>Create a dataset where the goal is to predict a word given nearby words&#13;
(or alternatively, to predict nearby words given a word).</p>&#13;
</li>&#13;
<li>&#13;
<p>Train a neural net to do well on this task.</p>&#13;
</li>&#13;
<li>&#13;
<p>Take the internal states of the trained neural net as the word vectors.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>In particular, because the task is to predict a word given nearby words,&#13;
words that occur in similar contexts (and hence have similar nearby words)&#13;
should have similar internal states and therefore similar word vectors.</p>&#13;
&#13;
<p>Here<a data-primary="cosine similarity" data-type="indexterm" id="idm45635718395096"/> we’ll measure “similarity” using <em>cosine similarity</em>,&#13;
which is a number between –1 and 1 that measures the degree to which&#13;
two vectors point in the same direction:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">dot</code><code class="p">,</code> <code class="n">Vector</code>&#13;
<code class="kn">import</code> <code class="nn">math</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">cosine_similarity</code><code class="p">(</code><code class="n">v1</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">v2</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">dot</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="n">v2</code><code class="p">)</code> <code class="o">/</code> <code class="n">math</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">dot</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="n">v1</code><code class="p">)</code> <code class="o">*</code> <code class="n">dot</code><code class="p">(</code><code class="n">v2</code><code class="p">,</code> <code class="n">v2</code><code class="p">))</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">cosine_similarity</code><code class="p">([</code><code class="mf">1.</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mf">2.</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"same direction"</code>&#13;
<code class="k">assert</code> <code class="n">cosine_similarity</code><code class="p">([</code><code class="o">-</code><code class="mf">1.</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mf">2.</code><code class="p">,</code> <code class="mi">2</code><code class="p">])</code> <code class="o">==</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code>    <code class="s2">"opposite direction"</code>&#13;
<code class="k">assert</code> <code class="n">cosine_similarity</code><code class="p">([</code><code class="mf">1.</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code><code class="p">,</code>       <code class="s2">"orthogonal"</code></pre>&#13;
&#13;
<p>Let’s learn some word vectors to see how this works.</p>&#13;
&#13;
<p>To start with, we’ll need a toy dataset.&#13;
The commonly used word vectors are typically derived from training&#13;
on millions or even billions of words.&#13;
As our toy library can’t cope with that much data,&#13;
we’ll create an artificial dataset with some structure to it:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">colors</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"red"</code><code class="p">,</code> <code class="s2">"green"</code><code class="p">,</code> <code class="s2">"blue"</code><code class="p">,</code> <code class="s2">"yellow"</code><code class="p">,</code> <code class="s2">"black"</code><code class="p">,</code> <code class="s2">""</code><code class="p">]</code>&#13;
<code class="n">nouns</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"bed"</code><code class="p">,</code> <code class="s2">"car"</code><code class="p">,</code> <code class="s2">"boat"</code><code class="p">,</code> <code class="s2">"cat"</code><code class="p">]</code>&#13;
<code class="n">verbs</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"is"</code><code class="p">,</code> <code class="s2">"was"</code><code class="p">,</code> <code class="s2">"seems"</code><code class="p">]</code>&#13;
<code class="n">adverbs</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"very"</code><code class="p">,</code> <code class="s2">"quite"</code><code class="p">,</code> <code class="s2">"extremely"</code><code class="p">,</code> <code class="s2">""</code><code class="p">]</code>&#13;
<code class="n">adjectives</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"slow"</code><code class="p">,</code> <code class="s2">"fast"</code><code class="p">,</code> <code class="s2">"soft"</code><code class="p">,</code> <code class="s2">"hard"</code><code class="p">]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">make_sentence</code><code class="p">()</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">([</code>&#13;
        <code class="s2">"The"</code><code class="p">,</code>&#13;
        <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">colors</code><code class="p">),</code>&#13;
        <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">nouns</code><code class="p">),</code>&#13;
        <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">verbs</code><code class="p">),</code>&#13;
        <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">adverbs</code><code class="p">),</code>&#13;
        <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">adjectives</code><code class="p">),</code>&#13;
        <code class="s2">"."</code>&#13;
    <code class="p">])</code>&#13;
&#13;
<code class="n">NUM_SENTENCES</code> <code class="o">=</code> <code class="mi">50</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code><code class="n">make_sentence</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">NUM_SENTENCES</code><code class="p">)]</code></pre>&#13;
&#13;
<p>This will generate lots of sentences with similar structure but different words;&#13;
for example, “The green boat seems quite slow.” Given this setup, the colors&#13;
will mostly appear in “similar” contexts, as will the nouns, and so on.&#13;
So if we do a good job of assigning word vectors, the colors should get&#13;
similar vectors, and so on.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In practical usage, you’d probably have a corpus of millions of sentences, in which case you’d get “enough” context from the sentences as they are. Here, with only 50 sentences, we have to make them somewhat artificial.</p>&#13;
</div>&#13;
&#13;
<p>As mentioned earlier, we’ll<a data-primary="one-hot-encoding" data-type="indexterm" id="idm45635718246872"/> want to one-hot-encode our words,&#13;
which means we’ll need to convert them to IDs. We’ll introduce&#13;
a <code>Vocabulary</code> class to keep track of this mapping:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.deep_learning</code> <code class="kn">import</code> <code class="n">Tensor</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Vocabulary</code><code class="p">:</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">words</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="bp">None</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w2i</code><code class="p">:</code> <code class="n">Dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="p">{}</code>  <code class="c1"># mapping word -&gt; word_id</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">i2w</code><code class="p">:</code> <code class="n">Dict</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="p">{}</code>  <code class="c1"># mapping word_id -&gt; word</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="p">(</code><code class="n">words</code> <code class="ow">or</code> <code class="p">[]):</code>     <code class="c1"># If words were provided,</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">word</code><code class="p">)</code>             <code class="c1"># add them.</code>&#13;
&#13;
    <code class="nd">@property</code>&#13;
    <code class="k">def</code> <code class="nf">size</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
        <code class="sd">"""how many words are in the vocabulary"""</code>&#13;
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w2i</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">add</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">w2i</code><code class="p">:</code>        <code class="c1"># If the word is new to us:</code>&#13;
            <code class="n">word_id</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w2i</code><code class="p">)</code>     <code class="c1"># Find the next id.</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">w2i</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="o">=</code> <code class="n">word_id</code>    <code class="c1"># Add to the word -&gt; word_id map.</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">i2w</code><code class="p">[</code><code class="n">word_id</code><code class="p">]</code> <code class="o">=</code> <code class="n">word</code>    <code class="c1"># Add to the word_id -&gt; word map.</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">get_id</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
        <code class="sd">"""return the id of the word (or None)"""</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">w2i</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">word</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">get_word</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">word_id</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>&#13;
        <code class="sd">"""return the word with the given id (or None)"""</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">i2w</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">word_id</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">one_hot_encode</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="n">word_id</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">get_id</code><code class="p">(</code><code class="n">word</code><code class="p">)</code>&#13;
        <code class="k">assert</code> <code class="n">word_id</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">,</code> <code class="n">f</code><code class="s2">"unknown word {word}"</code>&#13;
&#13;
        <code class="k">return</code> <code class="p">[</code><code class="mf">1.0</code> <code class="k">if</code> <code class="n">i</code> <code class="o">==</code> <code class="n">word_id</code> <code class="k">else</code> <code class="mf">0.0</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">size</code><code class="p">)]</code></pre>&#13;
&#13;
<p>These are all things we could do manually, but it’s handy to have it in a class.&#13;
We should probably test it:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">vocab</code> <code class="o">=</code> <code class="n">Vocabulary</code><code class="p">([</code><code class="s2">"a"</code><code class="p">,</code> <code class="s2">"b"</code><code class="p">,</code> <code class="s2">"c"</code><code class="p">])</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">size</code> <code class="o">==</code> <code class="mi">3</code><code class="p">,</code>              <code class="s2">"there are 3 words in the vocab"</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">get_id</code><code class="p">(</code><code class="s2">"b"</code><code class="p">)</code> <code class="o">==</code> <code class="mi">1</code><code class="p">,</code>       <code class="s2">"b should have word_id 1"</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="s2">"b"</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">get_id</code><code class="p">(</code><code class="s2">"z"</code><code class="p">)</code> <code class="ow">is</code> <code class="bp">None</code><code class="p">,</code>    <code class="s2">"z is not in the vocab"</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">get_word</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code> <code class="o">==</code> <code class="s2">"c"</code><code class="p">,</code>     <code class="s2">"word_id 2 should be c"</code>&#13;
<code class="n">vocab</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="s2">"z"</code><code class="p">)</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">size</code> <code class="o">==</code> <code class="mi">4</code><code class="p">,</code>              <code class="s2">"now there are 4 words in the vocab"</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">get_id</code><code class="p">(</code><code class="s2">"z"</code><code class="p">)</code> <code class="o">==</code> <code class="mi">3</code><code class="p">,</code>       <code class="s2">"now z should have id 3"</code>&#13;
<code class="k">assert</code> <code class="n">vocab</code><code class="o">.</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="s2">"z"</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code></pre>&#13;
&#13;
<p>We should also write simple helper functions to save and load a vocabulary,&#13;
just as we have for our deep learning models:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">save_vocab</code><code class="p">(</code><code class="n">vocab</code><code class="p">:</code> <code class="n">Vocabulary</code><code class="p">,</code> <code class="n">filename</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>&#13;
        <code class="n">json</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="n">vocab</code><code class="o">.</code><code class="n">w2i</code><code class="p">,</code> <code class="n">f</code><code class="p">)</code>       <code class="c1"># Only need to save w2i</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">load_vocab</code><code class="p">(</code><code class="n">filename</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vocabulary</code><code class="p">:</code>&#13;
    <code class="n">vocab</code> <code class="o">=</code> <code class="n">Vocabulary</code><code class="p">()</code>&#13;
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>&#13;
        <code class="c1"># Load w2i and generate i2w from it</code>&#13;
        <code class="n">vocab</code><code class="o">.</code><code class="n">w2i</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">)</code>&#13;
        <code class="n">vocab</code><code class="o">.</code><code class="n">i2w</code> <code class="o">=</code> <code class="p">{</code><code class="nb">id</code><code class="p">:</code> <code class="n">word</code> <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="nb">id</code> <code class="ow">in</code> <code class="n">vocab</code><code class="o">.</code><code class="n">w2i</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>&#13;
    <code class="k">return</code> <code class="n">vocab</code></pre>&#13;
&#13;
<p>We’ll<a data-primary="skip-gram model" data-type="indexterm" id="idm45635717798824"/> be using a word vector model called <em>skip-gram</em> that takes as input a word and generates probabilities for what words are likely to be seen near it. We will feed it training pairs&#13;
<code>(word, nearby_word)</code> and try to minimize the <code>SoftmaxCrossEntropy</code> loss.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Another<a data-primary="continuous bag-of-words (CBOW)" data-type="indexterm" id="idm45635717761480"/> common model, <em>continuous bag-of-words</em> (CBOW), takes the nearby words as the inputs and tries to predict the original word.</p>&#13;
</div>&#13;
&#13;
<p>Let’s<a data-primary="embedding layer" data-type="indexterm" id="idm45635717759640"/> design our neural network. At its heart will be an <em>embedding</em> layer&#13;
that takes as input a word ID and returns a word vector.&#13;
Under the covers we can just use a lookup table for this.</p>&#13;
&#13;
<p>We’ll then pass the word vector to a <code>Linear</code> layer with the same number of outputs&#13;
as we have words in our vocabulary. As before, we’ll use <code>softmax</code> to convert these outputs&#13;
to probabilities over nearby words. As we use gradient descent to train the model, we will&#13;
be updating the vectors in the lookup table. Once we’ve finished training, that lookup table&#13;
gives us our word vectors.</p>&#13;
&#13;
<p>Let’s create that embedding layer.&#13;
In practice we might want to embed things other than words,&#13;
so we’ll construct a more general <code>Embedding</code> layer.&#13;
(Later we’ll write a <code>TextEmbedding</code> subclass that’s specifically for word vectors.)</p>&#13;
&#13;
<p>In its constructor we’ll provide the number and dimension of our embedding vectors, so it can create the embeddings (which will be standard random normals, initially):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Iterable</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.deep_learning</code> <code class="kn">import</code> <code class="n">Layer</code><code class="p">,</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">random_tensor</code><code class="p">,</code> <code class="n">zeros_like</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Embedding</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">num_embeddings</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">num_embeddings</code> <code class="o">=</code> <code class="n">num_embeddings</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code> <code class="o">=</code> <code class="n">embedding_dim</code>&#13;
&#13;
        <code class="c1"># One vector of size embedding_dim for each desired embedding</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">embeddings</code> <code class="o">=</code> <code class="n">random_tensor</code><code class="p">(</code><code class="n">num_embeddings</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">grad</code> <code class="o">=</code> <code class="n">zeros_like</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embeddings</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># Save last input id</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">last_input_id</code> <code class="o">=</code> <code class="bp">None</code></pre>&#13;
&#13;
<p>In our case we’ll only be embedding one word at a time.&#13;
However, in other models we might want to embed a sequence of words&#13;
and get back a sequence of word vectors.  (For example, if we wanted&#13;
to train the CBOW model described earlier.)&#13;
So an alternative design would take sequences of word IDs.&#13;
We’ll stick with one at a time, to make things simpler.</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_id</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="sd">"""Just select the embedding vector corresponding to the input id"""</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">input_id</code> <code class="o">=</code> <code class="n">input_id</code>    <code class="c1"># remember for use in backpropagation</code>&#13;
&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">embeddings</code><code class="p">[</code><code class="n">input_id</code><code class="p">]</code></pre>&#13;
&#13;
<p>For the backward pass we’ll get a gradient corresponding&#13;
to the chosen embedding vector, and we’ll need to construct&#13;
the corresponding gradient for <code>self.embeddings</code>, which is&#13;
zero for every embedding other than the chosen one:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="c1"># Zero out the gradient corresponding to the last input.</code>&#13;
        <code class="c1"># This is way cheaper than creating a new all-zero tensor each time.</code>&#13;
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">last_input_id</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">:</code>&#13;
            <code class="n">zero_row</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">embedding_dim</code><code class="p">)]</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">grad</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">last_input_id</code><code class="p">]</code> <code class="o">=</code> <code class="n">zero_row</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">last_input_id</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">input_id</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">grad</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">input_id</code><code class="p">]</code> <code class="o">=</code> <code class="n">gradient</code></pre>&#13;
&#13;
<p>Because we have parameters and gradients, we need to override those methods:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">params</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">embeddings</code><code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">grads</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">grad</code><code class="p">]</code></pre>&#13;
&#13;
<p>As mentioned earlier, we’ll want a subclass specifically for word vectors. In that case our number of embeddings is determined by our vocabulary, so let’s just pass that in instead:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">TextEmbedding</code><code class="p">(</code><code class="n">Embedding</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab</code><code class="p">:</code> <code class="n">Vocabulary</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="c1"># Call the superclass constructor</code>&#13;
        <code class="nb">super</code><code class="p">()</code><code class="o">.</code><code class="nf-Magic">__init__</code><code class="p">(</code><code class="n">vocab</code><code class="o">.</code><code class="n">size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># And hang onto the vocab</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">vocab</code> <code class="o">=</code> <code class="n">vocab</code></pre>&#13;
&#13;
<p>The other built-in methods will all work as is, but we’ll add a couple&#13;
more methods specific to working with text. For example, we’d like&#13;
to be able to retrieve the vector for a given word.&#13;
(This is not part of the <code>Layer</code> interface, but we are always free to add extra&#13;
 methods to specific layers as we like.)</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf-Magic">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="n">word_id</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">vocab</code><code class="o">.</code><code class="n">get_id</code><code class="p">(</code><code class="n">word</code><code class="p">)</code>&#13;
        <code class="k">if</code> <code class="n">word_id</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">:</code>&#13;
            <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">embeddings</code><code class="p">[</code><code class="n">word_id</code><code class="p">]</code>&#13;
        <code class="k">else</code><code class="p">:</code>&#13;
            <code class="k">return</code> <code class="bp">None</code></pre>&#13;
&#13;
<p>This dunder method will allow us to retrieve word vectors using indexing:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">word_vector</code> <code class="o">=</code> <code class="n">embedding</code><code class="p">[</code><code class="s2">"black"</code><code class="p">]</code></pre>&#13;
&#13;
<p>And we’d also like the embedding layer to tell us the closest words to a given word:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">closest</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">word</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">n</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">5</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">Tuple</code><code class="p">[</code><code class="nb">float</code><code class="p">,</code> <code class="nb">str</code><code class="p">]]:</code>&#13;
        <code class="sd">"""Returns the n closest words based on cosine similarity"""</code>&#13;
        <code class="n">vector</code> <code class="o">=</code> <code class="bp">self</code><code class="p">[</code><code class="n">word</code><code class="p">]</code>&#13;
&#13;
        <code class="c1"># Compute pairs (similarity, other_word), and sort most similar first</code>&#13;
        <code class="n">scores</code> <code class="o">=</code> <code class="p">[(</code><code class="n">cosine_similarity</code><code class="p">(</code><code class="n">vector</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">embeddings</code><code class="p">[</code><code class="n">i</code><code class="p">]),</code> <code class="n">other_word</code><code class="p">)</code>&#13;
                  <code class="k">for</code> <code class="n">other_word</code><code class="p">,</code> <code class="n">i</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">vocab</code><code class="o">.</code><code class="n">w2i</code><code class="o">.</code><code class="n">items</code><code class="p">()]</code>&#13;
        <code class="n">scores</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
&#13;
        <code class="k">return</code> <code class="n">scores</code><code class="p">[:</code><code class="n">n</code><code class="p">]</code></pre>&#13;
&#13;
<p>Our embedding layer just outputs vectors, which we can feed into a <code>Linear</code> layer.</p>&#13;
&#13;
<p>Now we’re ready to assemble our training data. For each input word,&#13;
we’ll choose as target words the two&#13;
words to its left and the two words to its right.</p>&#13;
&#13;
<p>Let’s start by lowercasing the sentences and splitting them into words:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">re</code>&#13;
&#13;
<code class="c1"># This is not a great regex, but it works on our data.</code>&#13;
<code class="n">tokenized_sentences</code> <code class="o">=</code> <code class="p">[</code><code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s2">"[a-z]+|[.]"</code><code class="p">,</code> <code class="n">sentence</code><code class="o">.</code><code class="n">lower</code><code class="p">())</code>&#13;
                       <code class="k">for</code> <code class="n">sentence</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">]</code></pre>&#13;
&#13;
<p>at which point we can construct a vocabulary:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># Create a vocabulary (that is, a mapping word -&gt; word_id) based on our text.</code>&#13;
<code class="n">vocab</code> <code class="o">=</code> <code class="n">Vocabulary</code><code class="p">(</code><code class="n">word</code>&#13;
                   <code class="k">for</code> <code class="n">sentence_words</code> <code class="ow">in</code> <code class="n">tokenized_sentences</code>&#13;
                   <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">sentence_words</code><code class="p">)</code></pre>&#13;
&#13;
<p>And now we can create training data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.deep_learning</code> <code class="kn">import</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">one_hot_encode</code>&#13;
&#13;
<code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>&#13;
<code class="n">targets</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">sentence</code> <code class="ow">in</code> <code class="n">tokenized_sentences</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">word</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">sentence</code><code class="p">):</code>          <code class="c1"># For each word</code>&#13;
        <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="p">[</code><code class="n">i</code> <code class="o">-</code> <code class="mi">2</code><code class="p">,</code> <code class="n">i</code> <code class="o">-</code> <code class="mi">1</code><code class="p">,</code> <code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">i</code> <code class="o">+</code> <code class="mi">2</code><code class="p">]:</code>   <code class="c1"># take the nearby locations</code>&#13;
            <code class="k">if</code> <code class="mi">0</code> <code class="o">&lt;=</code> <code class="n">j</code> <code class="o">&lt;</code> <code class="nb">len</code><code class="p">(</code><code class="n">sentence</code><code class="p">):</code>           <code class="c1"># that aren't out of bounds</code>&#13;
                <code class="n">nearby_word</code> <code class="o">=</code> <code class="n">sentence</code><code class="p">[</code><code class="n">j</code><code class="p">]</code>        <code class="c1"># and get those words.</code>&#13;
&#13;
                <code class="c1"># Add an input that's the original word_id</code>&#13;
                <code class="n">inputs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">vocab</code><code class="o">.</code><code class="n">get_id</code><code class="p">(</code><code class="n">word</code><code class="p">))</code>&#13;
&#13;
                <code class="c1"># Add a target that's the one-hot-encoded nearby word</code>&#13;
                <code class="n">targets</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">vocab</code><code class="o">.</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="n">nearby_word</code><code class="p">))</code></pre>&#13;
&#13;
<p>With the machinery we’ve built up, it’s now easy to create our model:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.deep_learning</code> <code class="kn">import</code> <code class="n">Sequential</code><code class="p">,</code> <code class="n">Linear</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">EMBEDDING_DIM</code> <code class="o">=</code> <code class="mi">5</code>  <code class="c1"># seems like a good size</code>&#13;
&#13;
<code class="c1"># Define the embedding layer separately, so we can reference it.</code>&#13;
<code class="n">embedding</code> <code class="o">=</code> <code class="n">TextEmbedding</code><code class="p">(</code><code class="n">vocab</code><code class="o">=</code><code class="n">vocab</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="o">=</code><code class="n">EMBEDDING_DIM</code><code class="p">)</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">([</code>&#13;
    <code class="c1"># Given a word (as a vector of word_ids), look up its embedding.</code>&#13;
    <code class="n">embedding</code><code class="p">,</code>&#13;
    <code class="c1"># And use a linear layer to compute scores for "nearby words."</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">EMBEDDING_DIM</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="n">vocab</code><code class="o">.</code><code class="n">size</code><code class="p">)</code>&#13;
<code class="p">])</code></pre>&#13;
&#13;
<p>Using the machinery from <a data-type="xref" href="ch19.html#deep_learning">Chapter 19</a>, it’s easy to train our model:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.deep_learning</code> <code class="kn">import</code> <code class="n">SoftmaxCrossEntropy</code><code class="p">,</code> <code class="n">Momentum</code><code class="p">,</code> <code class="n">GradientDescent</code>&#13;
&#13;
<code class="n">loss</code> <code class="o">=</code> <code class="n">SoftmaxCrossEntropy</code><code class="p">()</code>&#13;
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">GradientDescent</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">)</code>&#13;
&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">100</code><code class="p">):</code>&#13;
    <code class="n">epoch_loss</code> <code class="o">=</code> <code class="mf">0.0</code>&#13;
    <code class="k">for</code> <code class="nb">input</code><code class="p">,</code> <code class="n">target</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">targets</code><code class="p">):</code>&#13;
        <code class="n">predicted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
        <code class="n">epoch_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">loss</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>&#13;
        <code class="n">gradient</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>&#13;
        <code class="n">model</code><code class="o">.</code><code class="n">backward</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="n">epoch_loss</code><code class="p">)</code>            <code class="c1"># Print the loss</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">embedding</code><code class="o">.</code><code class="n">closest</code><code class="p">(</code><code class="s2">"black"</code><code class="p">))</code>   <code class="c1"># and also a few nearest words</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">embedding</code><code class="o">.</code><code class="n">closest</code><code class="p">(</code><code class="s2">"slow"</code><code class="p">))</code>    <code class="c1"># so we can see what's being</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">embedding</code><code class="o">.</code><code class="n">closest</code><code class="p">(</code><code class="s2">"car"</code><code class="p">))</code>     <code class="c1"># learned.</code></pre>&#13;
&#13;
<p>As you watch this train, you can see the colors getting closer to each other,&#13;
the adjectives getting closer to each other, and the nouns getting closer&#13;
to each other.</p>&#13;
&#13;
<p>Once the model is trained, it’s fun to explore the most similar words:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">pairs</code> <code class="o">=</code> <code class="p">[(</code><code class="n">cosine_similarity</code><code class="p">(</code><code class="n">embedding</code><code class="p">[</code><code class="n">w1</code><code class="p">],</code> <code class="n">embedding</code><code class="p">[</code><code class="n">w2</code><code class="p">]),</code> <code class="n">w1</code><code class="p">,</code> <code class="n">w2</code><code class="p">)</code>&#13;
         <code class="k">for</code> <code class="n">w1</code> <code class="ow">in</code> <code class="n">vocab</code><code class="o">.</code><code class="n">w2i</code>&#13;
         <code class="k">for</code> <code class="n">w2</code> <code class="ow">in</code> <code class="n">vocab</code><code class="o">.</code><code class="n">w2i</code>&#13;
         <code class="k">if</code> <code class="n">w1</code> <code class="o">&lt;</code> <code class="n">w2</code><code class="p">]</code>&#13;
<code class="n">pairs</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">reverse</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">pairs</code><code class="p">[:</code><code class="mi">5</code><code class="p">])</code></pre>&#13;
&#13;
<p>which (for me) results in:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="p">[(</code><code class="mf">0.9980283554864815</code><code class="p">,</code> <code class="s1">'boat'</code><code class="p">,</code> <code class="s1">'car'</code><code class="p">),</code>&#13;
 <code class="p">(</code><code class="mf">0.9975147744587706</code><code class="p">,</code> <code class="s1">'bed'</code><code class="p">,</code> <code class="s1">'cat'</code><code class="p">),</code>&#13;
 <code class="p">(</code><code class="mf">0.9953153441218054</code><code class="p">,</code> <code class="s1">'seems'</code><code class="p">,</code> <code class="s1">'was'</code><code class="p">),</code>&#13;
 <code class="p">(</code><code class="mf">0.9927107440377975</code><code class="p">,</code> <code class="s1">'extremely'</code><code class="p">,</code> <code class="s1">'quite'</code><code class="p">),</code>&#13;
 <code class="p">(</code><code class="mf">0.9836183658415987</code><code class="p">,</code> <code class="s1">'bed'</code><code class="p">,</code> <code class="s1">'car'</code><code class="p">)]</code></pre>&#13;
&#13;
<p>(Obviously <em>bed</em> and <em>cat</em> are not really similar,&#13;
 but in our training sentences they appear to be,&#13;
 and that’s what the model is capturing.)</p>&#13;
&#13;
<p>We can also extract the first two principal components and plot them:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.working_with_data</code> <code class="kn">import</code> <code class="n">pca</code><code class="p">,</code> <code class="n">transform</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="c1"># Extract the first two principal components and transform the word vectors</code>&#13;
<code class="n">components</code> <code class="o">=</code> <code class="n">pca</code><code class="p">(</code><code class="n">embedding</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>&#13;
<code class="n">transformed</code> <code class="o">=</code> <code class="n">transform</code><code class="p">(</code><code class="n">embedding</code><code class="o">.</code><code class="n">embeddings</code><code class="p">,</code> <code class="n">components</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Scatter the points (and make them white so they're "invisible")</code>&#13;
<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">()</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="o">*</code><code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="n">transformed</code><code class="p">),</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'.'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'w'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Add annotations for each word at its transformed location</code>&#13;
<code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">idx</code> <code class="ow">in</code> <code class="n">vocab</code><code class="o">.</code><code class="n">w2i</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>&#13;
    <code class="n">ax</code><code class="o">.</code><code class="n">annotate</code><code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">transformed</code><code class="p">[</code><code class="n">idx</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># And hide the axes</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">get_xaxis</code><code class="p">()</code><code class="o">.</code><code class="n">set_visible</code><code class="p">(</code><code class="bp">False</code><code class="p">)</code>&#13;
<code class="n">ax</code><code class="o">.</code><code class="n">get_yaxis</code><code class="p">()</code><code class="o">.</code><code class="n">set_visible</code><code class="p">(</code><code class="bp">False</code><code class="p">)</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p>which shows that similar words are indeed clustering together (<a data-type="xref" href="#word_vectors_plot">Figure 21-3</a>):</p>&#13;
&#13;
<figure><div class="figure" id="word_vectors_plot">&#13;
<img alt="Word Vectors" src="assets/dsf2_2103.png"/>&#13;
<h6><span class="label">Figure 21-3. </span>Word vectors</h6>&#13;
</div></figure>&#13;
&#13;
<p>If you’re interested, it’s not hard to train CBOW word vectors.&#13;
You’ll have to do a little work. First, you’ll need to modify&#13;
the <code>Embedding</code> layer so that it takes as input a <em>list</em> of IDs&#13;
and outputs a <em>list</em> of embedding vectors.&#13;
Then you’ll have to create a new layer (<code>Sum</code>?) that takes&#13;
a list of vectors and returns their sum.</p>&#13;
&#13;
<p>Each word represents a training example where the input&#13;
is the word IDs for the surrounding words, and the target&#13;
is the one-hot encoding of the word itself.</p>&#13;
&#13;
<p>The modified <code>Embedding</code> layer turns the surrounding words&#13;
into a list of vectors, the new <code>Sum</code> layer collapses the list&#13;
of vectors down to a single vector, and then a <code>Linear</code> layer&#13;
can produce scores that can be <code>softmax</code>ed&#13;
to get a distribution representing “most likely words, given this context.”</p>&#13;
&#13;
<p>I found the CBOW model harder to train than the skip-gram one,&#13;
but I encourage you to try it out.<a data-primary="" data-startref="wvect21" data-type="indexterm" id="idm45635716081560"/><a data-primary="" data-startref="NLPworvec21" data-type="indexterm" id="idm45635716080584"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recurrent Neural Networks" data-type="sect1"><div class="sect1" id="idm45635718408904">&#13;
<h1>Recurrent Neural Networks</h1>&#13;
&#13;
<p>The<a data-primary="recurrent neural networks (RNNs)" data-type="indexterm" id="idm45635716078728"/><a data-primary="natural language processing (NLP)" data-secondary="recurrent neural networks (RNNs)" data-type="indexterm" id="idm45635716077960"/> word vectors we developed in the previous section are often used&#13;
as the inputs to neural networks. One challenge to doing this is that sentences&#13;
have varying lengths: you could think of a 3-word sentence as a&#13;
<code>[3, embedding_dim]</code> tensor and a 10-word sentence as a <code>[10, embedding_dim]</code>&#13;
tensor. In order to, say, pass them to a <code>Linear</code> layer, we need to do something&#13;
about that first variable-length dimension.</p>&#13;
&#13;
<p>One option<a data-primary="Sum layer" data-type="indexterm" id="idm45635716074936"/> is to use a <code>Sum</code> layer (or a variant that takes the average);&#13;
however, the <em>order</em> of the words in a sentence is usually important to its meaning.&#13;
To take a common example, “dog bites man” and “man bites dog” are two very different stories!</p>&#13;
&#13;
<p>Another way of handling this is using <em>recurrent neural networks</em> (RNNs),&#13;
which have a <em>hidden state</em> they maintain between inputs.&#13;
In the simplest case, each input is combined with the current hidden state&#13;
to produce an output, which is then used as the new hidden state.&#13;
This allows such networks to “remember” (in a sense) the inputs they’ve seen,&#13;
and to build up to a final output that depends on all the inputs and their order.</p>&#13;
&#13;
<p>We’ll create pretty much the simplest possible RNN layer, which will accept a single&#13;
input (corresponding to, e.g., a single word in a sentence, or a single character in a word),&#13;
and which will maintain its hidden state between calls.</p>&#13;
&#13;
<p>Recall that our <code>Linear</code> layer had some weights, <code>w</code>, and a bias, <code>b</code>.&#13;
It took a vector <code>input</code> and produced a different vector as <code>output</code>&#13;
using the logic:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">output</code><code class="p">[</code><code class="n">o</code><code class="p">]</code> <code class="o">=</code> <code class="n">dot</code><code class="p">(</code><code class="n">w</code><code class="p">[</code><code class="n">o</code><code class="p">],</code> <code class="nb">input</code><code class="p">)</code> <code class="o">+</code> <code class="n">b</code><code class="p">[</code><code class="n">o</code><code class="p">]</code></pre>&#13;
&#13;
<p>Here we’ll want to incorporate our hidden state, so we’ll have <em>two</em> sets of weights—one to apply to the <code>input</code> and one to apply to the previous <code>hidden</code> state:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">output</code><code class="p">[</code><code class="n">o</code><code class="p">]</code> <code class="o">=</code> <code class="n">dot</code><code class="p">(</code><code class="n">w</code><code class="p">[</code><code class="n">o</code><code class="p">],</code> <code class="nb">input</code><code class="p">)</code> <code class="o">+</code> <code class="n">dot</code><code class="p">(</code><code class="n">u</code><code class="p">[</code><code class="n">o</code><code class="p">],</code> <code class="n">hidden</code><code class="p">)</code> <code class="o">+</code> <code class="n">b</code><code class="p">[</code><code class="n">o</code><code class="p">]</code></pre>&#13;
&#13;
<p>Next, we’ll use the <code>output</code> vector as the new value of <code>hidden</code>.&#13;
This isn’t a huge change, but it will allow our networks to do wonderful things.</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.deep_learning</code> <code class="kn">import</code> <code class="n">tensor_apply</code><code class="p">,</code> <code class="n">tanh</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">SimpleRnn</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="sd">"""Just about the simplest possible recurrent layer."""</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">input_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">input_dim</code> <code class="o">=</code> <code class="n">input_dim</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code> <code class="o">=</code> <code class="n">hidden_dim</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">random_tensor</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="n">input_dim</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="s1">'xavier'</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">u</code> <code class="o">=</code> <code class="n">random_tensor</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="s1">'xavier'</code><code class="p">)</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">b</code> <code class="o">=</code> <code class="n">random_tensor</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">)</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">reset_hidden_state</code><code class="p">()</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">reset_hidden_state</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">hidden</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code><code class="p">)]</code></pre>&#13;
&#13;
<p>You can see that we start out the hidden state as a vector of 0s, and we provide a function that people using the network can call to&#13;
reset the hidden state.</p>&#13;
&#13;
<p>Given this setup, the <code>forward</code> function is reasonably straightforward&#13;
(at least, it is if you remember and understand how our <code>Linear</code> layer worked):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">input</code> <code class="o">=</code> <code class="nb">input</code>              <code class="c1"># Save both input and previous</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">prev_hidden</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden</code>  <code class="c1"># hidden state to use in backprop.</code>&#13;
&#13;
        <code class="n">a</code> <code class="o">=</code> <code class="p">[(</code><code class="n">dot</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">[</code><code class="n">h</code><code class="p">],</code> <code class="nb">input</code><code class="p">)</code> <code class="o">+</code>           <code class="c1"># weights @ input</code>&#13;
              <code class="n">dot</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">u</code><code class="p">[</code><code class="n">h</code><code class="p">],</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden</code><code class="p">)</code> <code class="o">+</code>     <code class="c1"># weights @ hidden</code>&#13;
              <code class="bp">self</code><code class="o">.</code><code class="n">b</code><code class="p">[</code><code class="n">h</code><code class="p">])</code>                        <code class="c1"># bias</code>&#13;
             <code class="k">for</code> <code class="n">h</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code><code class="p">)]</code>&#13;
&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">hidden</code> <code class="o">=</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="n">tanh</code><code class="p">,</code> <code class="n">a</code><code class="p">)</code>  <code class="c1"># Apply tanh activation</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden</code>                   <code class="c1"># and return the result.</code></pre>&#13;
&#13;
<p>The <code>backward</code> pass is similar to the one in our <code>Linear</code> layer,&#13;
except that it needs to compute an additional set of gradients for the <code>u</code> weights:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">):</code>&#13;
        <code class="c1"># Backpropagate through the tanh</code>&#13;
        <code class="n">a_grad</code> <code class="o">=</code> <code class="p">[</code><code class="n">gradient</code><code class="p">[</code><code class="n">h</code><code class="p">]</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">hidden</code><code class="p">[</code><code class="n">h</code><code class="p">]</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code>&#13;
                  <code class="k">for</code> <code class="n">h</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code><code class="p">)]</code>&#13;
&#13;
        <code class="c1"># b has the same gradient as a</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">b_grad</code> <code class="o">=</code> <code class="n">a_grad</code>&#13;
&#13;
        <code class="c1"># Each w[h][i] is multiplied by input[i] and added to a[h],</code>&#13;
        <code class="c1"># so each w_grad[h][i] = a_grad[h] * input[i]</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w_grad</code> <code class="o">=</code> <code class="p">[[</code><code class="n">a_grad</code><code class="p">[</code><code class="n">h</code><code class="p">]</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">input</code><code class="p">[</code><code class="n">i</code><code class="p">]</code>&#13;
                        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">input_dim</code><code class="p">)]</code>&#13;
                       <code class="k">for</code> <code class="n">h</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code><code class="p">)]</code>&#13;
&#13;
        <code class="c1"># Each u[h][h2] is multiplied by hidden[h2] and added to a[h],</code>&#13;
        <code class="c1"># so each u_grad[h][h2] = a_grad[h] * prev_hidden[h2]</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">u_grad</code> <code class="o">=</code> <code class="p">[[</code><code class="n">a_grad</code><code class="p">[</code><code class="n">h</code><code class="p">]</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">prev_hidden</code><code class="p">[</code><code class="n">h2</code><code class="p">]</code>&#13;
                        <code class="k">for</code> <code class="n">h2</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code><code class="p">)]</code>&#13;
                       <code class="k">for</code> <code class="n">h</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code><code class="p">)]</code>&#13;
&#13;
        <code class="c1"># Each input[i] is multiplied by every w[h][i] and added to a[h],</code>&#13;
        <code class="c1"># so each input_grad[i] = sum(a_grad[h] * w[h][i] for h in ...)</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="nb">sum</code><code class="p">(</code><code class="n">a_grad</code><code class="p">[</code><code class="n">h</code><code class="p">]</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">[</code><code class="n">h</code><code class="p">][</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">h</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">hidden_dim</code><code class="p">))</code>&#13;
                <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">input_dim</code><code class="p">)]</code></pre>&#13;
&#13;
<p>And finally we need to override the <code>params</code> and <code>grads</code> methods:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">params</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">u</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">b</code><code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">grads</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">w_grad</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">u_grad</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">b_grad</code><code class="p">]</code></pre>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>This “simple” RNN is so simple that you probably shouldn’t use it in practice.</p>&#13;
</div>&#13;
&#13;
<p>Our <code>SimpleRnn</code> has a couple of undesirable features.&#13;
One is that its entire hidden state is used to update the input every time you call it.&#13;
The other is that the entire hidden state is overwritten every time you call it.&#13;
Both of these make it difficult to train; in particular, they make it difficult&#13;
for the model to learn long-range dependencies.</p>&#13;
&#13;
<p>For<a data-primary="LSTM (long short-term memory)" data-type="indexterm" id="idm45635715436920"/> this reason, almost no one uses this kind of simple RNN. Instead, they use&#13;
more complicated variants like the LSTM (“long short-term memory”) or the<a data-primary="GRU (gated recurrent unit)" data-type="indexterm" id="idm45635715435912"/> GRU&#13;
(“gated recurrent unit”), which have many more parameters and use parameterized “gates”&#13;
that allow only some of the state to be updated (and only some of the state to be used)&#13;
at each timestep.</p>&#13;
&#13;
<p>There is nothing particularly <em>difficult</em> about these variants; however, they involve&#13;
a great deal more code, which would not be (in my opinion) correspondingly more edifying to read.&#13;
The code for this chapter on <a href="https://github.com/joelgrus/data-science-from-scratch">GitHub</a> includes an LSTM implementation.&#13;
 I encourage you to check it out, but it’s somewhat tedious and so we won’t discuss it further here.</p>&#13;
&#13;
<p>One other quirk of our implementation is that it takes only one “step” at a time and requires&#13;
us to manually reset the hidden state. A more practical RNN implementation might accept&#13;
sequences of inputs, set its hidden state to 0s at the beginning of each sequence,&#13;
and produce sequences of outputs. Ours could certainly be modified to behave this way;&#13;
again, this would require more code and complexity for little gain in understanding.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Using a Character-Level RNN" data-type="sect1"><div class="sect1" id="idm45635716079384">&#13;
<h1>Example: Using a Character-Level RNN</h1>&#13;
&#13;
<p>The<a data-primary="natural language processing (NLP)" data-secondary="character-level RNN example" data-type="indexterm" id="idm45635715430440"/><a data-primary="character-level RNNs" data-type="indexterm" id="idm45635715429400"/> newly hired VP of Branding did not come up with the name <em>DataSciencester</em> himself,&#13;
and (accordingly) he suspects that a better name might lead to more success for the company.&#13;
He asks you to use data science to suggest candidates for replacement.</p>&#13;
&#13;
<p>One “cute” application of RNNs involves using <em>characters</em> (rather than words)&#13;
as their inputs, training them to learn the subtle language patterns in some dataset,&#13;
and then using them to generate fictional instances from that dataset.</p>&#13;
&#13;
<p>For example, you could train an RNN on the names of alternative bands,&#13;
use the trained model to generate new names for fake alternative bands,&#13;
and then hand-select the funniest ones and share them on Twitter. Hilarity!</p>&#13;
&#13;
<p>Having seen this trick enough times to no longer consider it clever,&#13;
you decide to give it a shot.</p>&#13;
&#13;
<p>After some digging, you find that the startup accelerator Y Combinator has published&#13;
<a href="https://www.ycombinator.com/topcompanies/">a list of its top 100&#13;
(actually 101) most successful startups</a>, which seems like a good starting point.&#13;
Checking the page, you find that the company names all live inside <code>&lt;b class="h4"&gt;</code>&#13;
tags, which means it’s easy to use your web scraping skills to retrieve them:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">BeautifulSoup</code>&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
&#13;
<code class="n">url</code> <code class="o">=</code> <code class="s2">"https://www.ycombinator.com/topcompanies/"</code>&#13;
<code class="n">soup</code> <code class="o">=</code> <code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code><code class="o">.</code><code class="n">text</code><code class="p">,</code> <code class="s1">'html5lib'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># We get the companies twice, so use a set comprehension to deduplicate.</code>&#13;
<code class="n">companies</code> <code class="o">=</code> <code class="nb">list</code><code class="p">({</code><code class="n">b</code><code class="o">.</code><code class="n">text</code>&#13;
                  <code class="k">for</code> <code class="n">b</code> <code class="ow">in</code> <code class="n">soup</code><code class="p">(</code><code class="s2">"b"</code><code class="p">)</code>&#13;
                  <code class="k">if</code> <code class="s2">"h4"</code> <code class="ow">in</code> <code class="n">b</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"class"</code><code class="p">,</code> <code class="p">())})</code>&#13;
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">companies</code><code class="p">)</code> <code class="o">==</code> <code class="mi">101</code></pre>&#13;
&#13;
<p>As always, the page may change (or vanish), in which case this code won’t work.&#13;
If so, you can use your newly learned data science skills to fix it&#13;
or just get the list from the book’s GitHub site.</p>&#13;
&#13;
<p>So what is our plan? We’ll train a model to predict the next character&#13;
of a name, given the current character <em>and</em> a hidden state representing&#13;
all the characters we’ve seen so far.</p>&#13;
&#13;
<p>As usual, we’ll actually predict a probability distribution over characters&#13;
and train our model to minimize the <code>SoftmaxCrossEntropy</code> loss.</p>&#13;
&#13;
<p>Once our model is trained, we can use it to generate some probabilities,&#13;
randomly sample a character according to those probabilities, and then&#13;
feed that character as its next input. This will allow us to <em>generate</em>&#13;
company names using the learned weights.</p>&#13;
&#13;
<p>To start with, we should build a <code>Vocabulary</code> from the characters in the names:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">vocab</code> <code class="o">=</code> <code class="n">Vocabulary</code><code class="p">([</code><code class="n">c</code> <code class="k">for</code> <code class="n">company</code> <code class="ow">in</code> <code class="n">companies</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">company</code><code class="p">])</code></pre>&#13;
&#13;
<p>In addition, we’ll use special tokens to signify the start and end of a company name.&#13;
This allows the model to learn which characters should <em>begin</em> a company name&#13;
and also to learn when a company name is <em>finished</em>.</p>&#13;
&#13;
<p>We’ll just use the regex characters for start and end, which (luckily) don’t appear in our list of companies:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">START</code> <code class="o">=</code> <code class="s2">"^"</code>&#13;
<code class="n">STOP</code> <code class="o">=</code> <code class="s2">"$"</code>&#13;
&#13;
<code class="c1"># We need to add them to the vocabulary too.</code>&#13;
<code class="n">vocab</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">START</code><code class="p">)</code>&#13;
<code class="n">vocab</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">STOP</code><code class="p">)</code></pre>&#13;
&#13;
<p>For our model, we’ll one-hot-encode each character,&#13;
pass it through two <code>SimpleRnn</code>s, and then use a <code>Linear</code> layer&#13;
to generate the scores for each possible next character:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">HIDDEN_DIM</code> <code class="o">=</code> <code class="mi">32</code>  <code class="c1"># You should experiment with different sizes!</code>&#13;
&#13;
<code class="n">rnn1</code> <code class="o">=</code>  <code class="n">SimpleRnn</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">vocab</code><code class="o">.</code><code class="n">size</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="n">HIDDEN_DIM</code><code class="p">)</code>&#13;
<code class="n">rnn2</code> <code class="o">=</code>  <code class="n">SimpleRnn</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">HIDDEN_DIM</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="n">HIDDEN_DIM</code><code class="p">)</code>&#13;
<code class="n">linear</code> <code class="o">=</code> <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">HIDDEN_DIM</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="n">vocab</code><code class="o">.</code><code class="n">size</code><code class="p">)</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">([</code>&#13;
    <code class="n">rnn1</code><code class="p">,</code>&#13;
    <code class="n">rnn2</code><code class="p">,</code>&#13;
    <code class="n">linear</code>&#13;
<code class="p">])</code></pre>&#13;
&#13;
<p>Imagine for the moment that we’ve trained this model.&#13;
Let’s write the function that uses it to generate new company names,&#13;
using the <code>sample_from</code> function from <a data-type="xref" href="#topic_modeling">“Topic Modeling”</a>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.deep_learning</code> <code class="kn">import</code> <code class="n">softmax</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">generate</code><code class="p">(</code><code class="n">seed</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="n">START</code><code class="p">,</code> <code class="n">max_len</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">50</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>&#13;
    <code class="n">rnn1</code><code class="o">.</code><code class="n">reset_hidden_state</code><code class="p">()</code>  <code class="c1"># Reset both hidden states</code>&#13;
    <code class="n">rnn2</code><code class="o">.</code><code class="n">reset_hidden_state</code><code class="p">()</code>&#13;
    <code class="n">output</code> <code class="o">=</code> <code class="p">[</code><code class="n">seed</code><code class="p">]</code>            <code class="c1"># Start the output with the specified seed</code>&#13;
&#13;
    <code class="c1"># Keep going until we produce the STOP character or reach the max length</code>&#13;
    <code class="k">while</code> <code class="n">output</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">!=</code> <code class="n">STOP</code> <code class="ow">and</code> <code class="nb">len</code><code class="p">(</code><code class="n">output</code><code class="p">)</code> <code class="o">&lt;</code> <code class="n">max_len</code><code class="p">:</code>&#13;
        <code class="c1"># Use the last character as the input</code>&#13;
        <code class="nb">input</code> <code class="o">=</code> <code class="n">vocab</code><code class="o">.</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code>&#13;
&#13;
        <code class="c1"># Generate scores using the model</code>&#13;
        <code class="n">predicted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># Convert them to probabilities and draw a random char_id</code>&#13;
        <code class="n">probabilities</code> <code class="o">=</code> <code class="n">softmax</code><code class="p">(</code><code class="n">predicted</code><code class="p">)</code>&#13;
        <code class="n">next_char_id</code> <code class="o">=</code> <code class="n">sample_from</code><code class="p">(</code><code class="n">probabilities</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># Add the corresponding char to our output</code>&#13;
        <code class="n">output</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">vocab</code><code class="o">.</code><code class="n">get_word</code><code class="p">(</code><code class="n">next_char_id</code><code class="p">))</code>&#13;
&#13;
    <code class="c1"># Get rid of START and END characters and return the word</code>&#13;
    <code class="k">return</code> <code class="s1">''</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">output</code><code class="p">[</code><code class="mi">1</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code></pre>&#13;
&#13;
<p>At long last, we’re ready to train our character-level RNN. It will take a while!</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">loss</code> <code class="o">=</code> <code class="n">SoftmaxCrossEntropy</code><code class="p">()</code>&#13;
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">Momentum</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>&#13;
&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">300</code><code class="p">):</code>&#13;
    <code class="n">random</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">companies</code><code class="p">)</code>  <code class="c1"># Train in a different order each epoch.</code>&#13;
    <code class="n">epoch_loss</code> <code class="o">=</code> <code class="mi">0</code>             <code class="c1"># Track the loss.</code>&#13;
    <code class="k">for</code> <code class="n">company</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">tqdm</code><code class="p">(</code><code class="n">companies</code><code class="p">):</code>&#13;
        <code class="n">rnn1</code><code class="o">.</code><code class="n">reset_hidden_state</code><code class="p">()</code>  <code class="c1"># Reset both hidden states.</code>&#13;
        <code class="n">rnn2</code><code class="o">.</code><code class="n">reset_hidden_state</code><code class="p">()</code>&#13;
        <code class="n">company</code> <code class="o">=</code> <code class="n">START</code> <code class="o">+</code> <code class="n">company</code> <code class="o">+</code> <code class="n">STOP</code>   <code class="c1"># Add START and STOP characters.</code>&#13;
&#13;
        <code class="c1"># The rest is just our usual training loop, except that the inputs</code>&#13;
        <code class="c1"># and target are the one-hot-encoded previous and next characters.</code>&#13;
        <code class="k">for</code> <code class="n">prev</code><code class="p">,</code> <code class="nb">next</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">company</code><code class="p">,</code> <code class="n">company</code><code class="p">[</code><code class="mi">1</code><code class="p">:]):</code>&#13;
            <code class="nb">input</code> <code class="o">=</code> <code class="n">vocab</code><code class="o">.</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="n">prev</code><code class="p">)</code>&#13;
            <code class="n">target</code> <code class="o">=</code> <code class="n">vocab</code><code class="o">.</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="nb">next</code><code class="p">)</code>&#13;
            <code class="n">predicted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
            <code class="n">epoch_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">loss</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>&#13;
            <code class="n">gradient</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">target</code><code class="p">)</code>&#13;
            <code class="n">model</code><code class="o">.</code><code class="n">backward</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
            <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># Each epoch, print the loss and also generate a name.</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="n">epoch_loss</code><code class="p">,</code> <code class="n">generate</code><code class="p">())</code>&#13;
&#13;
    <code class="c1"># Turn down the learning rate for the last 100 epochs.</code>&#13;
    <code class="c1"># There's no principled reason for this, but it seems to work.</code>&#13;
    <code class="k">if</code> <code class="n">epoch</code> <code class="o">==</code> <code class="mi">200</code><code class="p">:</code>&#13;
        <code class="n">optimizer</code><code class="o">.</code><code class="n">lr</code> <code class="o">*=</code> <code class="mf">0.1</code></pre>&#13;
&#13;
<p>After training, the model generates some actual names from the list&#13;
(which isn’t surprising, since the model has a fair amount of capacity&#13;
 and not a lot of training data), as well as names that&#13;
are only slightly different from training names (Scripe, Loinbare, Pozium), names that seem genuinely creative (Benuus, Cletpo, Equite, Vivest), and names that are garbage-y but still sort of word-like (SFitreasy, Sint ocanelp, GliyOx, Doorboronelhav).</p>&#13;
&#13;
<p>Unfortunately, like most character-level-RNN outputs, these are only mildly clever, and the VP of Branding ends up unable to use them.</p>&#13;
&#13;
<p>If I up the hidden dimension to 64, I get a lot more names verbatim from the list;&#13;
if I drop it to 8, I get mostly garbage. The vocabulary and final weights&#13;
for all these model sizes&#13;
are available on <a href="https://github.com/joelgrus/data-science-from-scratch">the book’s GitHub site</a>, and you can use <code>load_weights</code> and <code>load_vocab</code>&#13;
to use them yourself.</p>&#13;
&#13;
<p>As mentioned previously, the GitHub code for this chapter also contains an implementation for an LSTM,&#13;
which you should feel free to swap in as a replacement&#13;
for the <code>SimpleRnn</code>s in our company name model.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="nlp-further-invest">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="http://www.nltk.org/">NLTK</a> is<a data-primary="natural language processing (NLP)" data-secondary="resources for learning about" data-type="indexterm" id="idm45635714762648"/><a data-primary="NLTK" data-type="indexterm" id="idm45635714761656"/><a data-primary="natural language processing (NLP)" data-secondary="tools for" data-type="indexterm" id="idm45635714760984"/> a popular library of NLP tools for Python.  It has its own entire <a href="http://www.nltk.org/book/">book</a>, which is available to read online.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="http://radimrehurek.com/gensim/">gensim</a> is a<a data-primary="gensim" data-type="indexterm" id="idm45635714757736"/> Python library for topic modeling, which is a better bet than our from-scratch model.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://spacy.io/">spaCy</a> is<a data-primary="spaCy" data-type="indexterm" id="idm45635714755480"/> a library for “Industrial Strength Natural Language Processing in Python” and is also quite popular.</p>&#13;
</li>&#13;
<li>&#13;
<p>Andrej Karpathy has a famous blog post,  <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">“The Unreasonable Effectiveness of Recurrent Neural Networks”</a>, that’s very much worth reading.</p>&#13;
</li>&#13;
<li>&#13;
<p>My<a data-primary="AllenNLP" data-type="indexterm" id="idm45635714752040"/> day job involves building <a href="https://allennlp.org/">AllenNLP</a>, a Python library for doing NLP research. (At least, as of the time this book went to press, it did.) The library is quite beyond the scope of this book, but you might still find it interesting, and it has a cool interactive demo of many state-of-the-art NLP models.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>