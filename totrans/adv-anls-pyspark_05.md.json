["```py\nhead -n 1 data/kddcup.data\n\n...\n\n0,tcp,http,SF,215,45076,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,...\n```", "```py\ndata_without_header = spark.read.option(\"inferSchema\", True).\\\n                                  option(\"header\", False).\\\n                                  csv(\"data/kddcup.data\")\n\ncolumn_names = [  \"duration\", \"protocol_type\", \"service\", \"flag\",\n  \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n  \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n  \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n  \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n  \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n  \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n  \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n  \"dst_host_count\", \"dst_host_srv_count\",\n  \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n  \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n  \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n  \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n  \"label\"]\n\ndata = data_without_header.toDF(*column_names)\n```", "```py\nfrom pyspark.sql.functions import col\ndata.select(\"label\").groupBy(\"label\").count().\\\n      orderBy(col(\"count\").desc()).show(25)\n\n...\n+----------------+-------+\n|           label|  count|\n+----------------+-------+\n|          smurf.|2807886|\n|        neptune.|1072017|\n|         normal.| 972781|\n|          satan.|  15892|\n...\n|            phf.|      4|\n|           perl.|      3|\n|            spy.|      2|\n+----------------+-------+\n```", "```py\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans, KMeansModel\nfrom pyspark.ml import Pipeline\n\nnumeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n\nassembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).\\\n                              setOutputCol(\"featureVector\")\n\nkmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n\npipeline = Pipeline().setStages([assembler, kmeans])\npipeline_model = pipeline.fit(numeric_only)\nkmeans_model = pipeline_model.stages[1]\n\nfrom pprint import pprint\npprint(kmeans_model.clusterCenters())\n\n...\n[array([4.83401949e+01, 1.83462155e+03, 8.26203190e+02, 5.71611720e-06,\n       6.48779303e-04, 7.96173468e-06...]),\n array([1.0999000e+04, 0.0000000e+00, 1.3099374e+09, 0.0000000e+00,\n       0.0000000e+00, 0.0000000e+00,...])]\n```", "```py\nwith_cluster = pipeline_model.transform(numeric_only)\n\nwith_cluster.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count().\\\n              orderBy(col(\"cluster\"), col(\"count\").desc()).show(25)\n\n...\n+-------+----------------+-------+\n|cluster|           label|  count|\n+-------+----------------+-------+\n|      0|          smurf.|2807886|\n|      0|        neptune.|1072017|\n|      0|         normal.| 972781|\n|      0|          satan.|  15892|\n|      0|        ipsweep.|  12481|\n...\n|      0|            phf.|      4|\n|      0|           perl.|      3|\n|      0|            spy.|      2|\n|      1|      portsweep.|      1|\n+-------+----------------+-------+\n```", "```py\nfrom pyspark.sql import DataFrame\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\nfrom random import randint\n\ndef clustering_score(input_data, k):\n    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n    assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).\\\n                                  setOutputCol(\"featureVector\")\n    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).\\\n                      setPredictionCol(\"cluster\").\\\n                      setFeaturesCol(\"featureVector\")\n    pipeline = Pipeline().setStages([assembler, kmeans])\n    pipeline_model = pipeline.fit(input_numeric_only)\n\n    evaluator = ClusteringEvaluator(predictionCol='cluster',\n                                    featuresCol=\"featureVector\")\n    predictions = pipeline_model.transform(numeric_only)\n    score = evaluator.evaluate(predictions)\n    return score\n\nfor k in list(range(20,100, 20)):\n    print(clustering_score(numeric_only, k)) ![1](assets/1.png)\n\n...\n(20,6.649218115128446E7)\n(40,2.5031424366033625E7)\n(60,1.027261913057096E7)\n(80,1.2514131711109027E7)\n(100,7235531.565096531)\n```", "```py\ndef clustering_score_1(input_data, k):\n    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n    assembler = VectorAssembler().\\\n                  setInputCols(input_numeric_only.columns[:-1]).\\\n                  setOutputCol(\"featureVector\")\n    kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).\\ ![1](assets/1.png)\n      setTol(1.0e-5).\\ ![2](assets/2.png)\n      setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n    pipeline = Pipeline().setStages([assembler, kmeans])\n    pipeline_model = pipeline.fit(input_numeric_only)\n    #\n    evaluator = ClusteringEvaluator(predictionCol='cluster',\n                                    featuresCol=\"featureVector\")\n    predictions = pipeline_model.transform(numeric_only)\n    score = evaluator.evaluate(predictions)\n    #\n    return score\n\nfor k in list(range(20,101, 20)):\n    print(k, clustering_score_1(numeric_only, k))\n```", "```py\n(20,1.8041795813813403E8)\n(40,6.33056876207124E7)\n(60,9474961.544965891)\n(80,9388117.93747141)\n(100,8783628.926311461)\n```", "```py\nSys.setenv(SPARK_HOME = \"*`/path/to/spark`*\") ![1](assets/1.png)\nSys.setenv(JAVA_HOME = \"*`/path/to/java`*\") library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))) sparkR.session(master = \"local[*]\",\n sparkConfig = list(spark.driver.memory = \"4g\"))\n```", "```py\nclusters_data <- read.df(\"*`/path/to/kddcup.data`*\", \"csv\", ![1](assets/1.png)\n inferSchema = \"true\", header = \"false\") colnames(clusters_data) <- c( ![2](assets/2.png)\n \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\") \nnumeric_only <- cache(drop(clusters_data, ![3](assets/3.png)\n c(\"protocol_type\", \"service\", \"flag\", \"label\"))) \nkmeans_model <- spark.kmeans(numeric_only, ~ ., ![4](assets/4.png)\n k = 100, maxIter = 40, initMode = \"k-means||\")\n```", "```py\nclustering <- predict(kmeans_model, numeric_only) clustering_sample <- collect(sample(clustering, FALSE, 0.01)) ![1](assets/1.png)\n\nstr(clustering_sample) \n... 'data.frame': 48984 obs. of  39 variables:\n $ duration                   : int  0 0 0 0 0 0 0 0 0 0 ... $ src_bytes                  : int  181 185 162 254 282 310 212 214 181 ... $ dst_bytes                  : int  5450 9020 4528 849 424 1981 2917 3404 ... $ land                       : int  0 0 0 0 0 0 0 0 0 0 ... ...\n $ prediction                 : int  33 33 33 0 0 0 0 0 33 33 ...\n```", "```py\nclusters <- clustering_sample[\"prediction\"] ![1](assets/1.png)\ndata <- data.matrix(within(clustering_sample, rm(\"prediction\"))) ![2](assets/2.png)\n\ntable(clusters) \n... clusters\n 0    11    14    18    23    25    28    30    31    33    36    ... 47294     3     1     2     2   308   105     1    27  1219    15    ...\n```", "```py\ninstall.packages(\"rgl\")\nlibrary(rgl)\n```", "```py\nrandom_projection <- matrix(data = rnorm(3*ncol(data)), ncol = 3) ![1](assets/1.png)\nrandom_projection_norm <-\n random_projection / sqrt(rowSums(random_projection*random_projection)) \nprojected_data <- data.frame(data %*% random_projection_norm) ![2](assets/2.png)\n```", "```py\nnum_clusters <- max(clusters)\npalette <- rainbow(num_clusters)\ncolors = sapply(clusters, function(c) palette[c])\nplot3d(projected_data, col = colors, size = 10)\n```", "```py\nfrom pyspark.ml.feature import StandardScaler\n\ndef clustering_score_2(input_data, k):\n    input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n    assembler = VectorAssembler().\\\n                setInputCols(input_numeric_only.columns[:-1]).\\\n                setOutputCol(\"featureVector\")\n    scaler = StandardScaler().setInputCol(\"featureVector\").\\\n                              setOutputCol(\"scaledFeatureVector\").\\\n                              setWithStd(True).setWithMean(False)\n    kmeans = KMeans().setSeed(randint(100,100000)).\\\n                      setK(k).setMaxIter(40).\\\n                      setTol(1.0e-5).setPredictionCol(\"cluster\").\\\n                      setFeaturesCol(\"scaledFeatureVector\")\n    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n    pipeline_model = pipeline.fit(input_numeric_only)\n    #\n    evaluator = ClusteringEvaluator(predictionCol='cluster',\n                                    featuresCol=\"scaledFeatureVector\")\n    predictions = pipeline_model.transform(numeric_only)\n    score = evaluator.evaluate(predictions)\n    #\n    return score\n\nfor k in list(range(60, 271, 30)):\n    print(k, clustering_score_2(numeric_only, k))\n...\n(60,1.2454250178069293)\n(90,0.7767730051608682)\n(120,0.5070473497003614)\n(150,0.4077081720067704)\n(180,0.3344486714980788)\n(210,0.276237617334138)\n(240,0.24571877339169032)\n(270,0.21818167354866858)\n```", "```py\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\n\ndef one_hot_pipeline(input_col):\n    indexer = StringIndexer().setInputCol(input_col).\\\n                              setOutputCol(input_col + \"-_indexed\")\n    encoder = OneHotEncoder().setInputCol(input_col + \"indexed\").\\\n                              setOutputCol(input_col + \"_vec\")\n    pipeline = Pipeline().setStages([indexer, encoder])\n    return pipeline, input_col + \"_vec\" ![1](assets/1.png)\n```", "```py\n(60,39.739250062068685)\n(90,15.814341529964691)\n(120,3.5008631362395413)\n(150,2.2151974068685547)\n(180,1.587330730808905)\n(210,1.3626704802348888)\n(240,1.1202477806210747)\n(270,0.9263659836264369)\n```", "```py\nfrom math import log\n\ndef entropy(counts):\n    values = [c for c in counts if (c > 0)]\n    n = sum(values)\n    p = [v/n for v in values]\n    return sum([-1*(p_v) * log(p_v) for p_v in p])\n```", "```py\nfrom pyspark.sql import functions as fun\nfrom pyspark.sql import Window\n\ncluster_label = pipeline_model.\\\n                    transform(data).\\\n                    select(\"cluster\", \"label\") ![1](assets/1.png)\n\ndf = cluster_label.\\\n        groupBy(\"cluster\", \"label\").\\\n        count().orderBy(\"cluster\") ![2](assets/2.png)\n\nw = Window.partitionBy(\"cluster\")\n\np_col = df['count'] / fun.sum(df['count']).over(w)\nwith_p_col = df.withColumn(\"p_col\", p_col)\n\nresult = with_p_col.groupBy(\"cluster\").\\\n              agg(-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\")))\\\n                        .alias(\"entropy\"),\n                    fun.sum(col(\"count\"))\\\n                        .alias(\"cluster_size\"))\n\nresult = result.withColumn('weightedClusterEntropy',\n                          col('entropy') * col('cluster_size')) ![3](assets/3.png)\n\nweighted_cluster_entropy_avg = result.\\\n                            agg(fun.sum(\n                              col('weightedClusterEntropy'))).\\\n                            collect()\nweighted_cluster_entropy_avg[0][0]/data.count()\n```", "```py\n(60,0.03475331900669869)\n(90,0.051512668026335535)\n(120,0.02020028911919293)\n(150,0.019962563512905682)\n(180,0.01110240886325257)\n(210,0.01259738444250231)\n(240,0.01357435960663116)\n(270,0.010119881917660544)\n```", "```py\npipeline_model = fit_pipeline_4(data, 180) ![1](assets/1.png)\ncount_by_cluster_label = pipeline_model.transform(data).\\\n                                        select(\"cluster\", \"label\").\\\n                                        groupBy(\"cluster\", \"label\").\\\n                                        count().orderBy(\"cluster\", \"label\")\ncount_by_cluster_label.show()\n\n...\n+-------+----------+------+\n|cluster|     label| count|\n+-------+----------+------+\n|      0|     back.|   324|\n|      0|   normal.| 42921|\n|      1|  neptune.|  1039|\n|      1|portsweep.|     9|\n|      1|    satan.|     2|\n|      2|  neptune.|365375|\n|      2|portsweep.|   141|\n|      3|portsweep.|     2|\n|      3|    satan.| 10627|\n|      4|  neptune.|  1033|\n|      4|portsweep.|     6|\n|      4|    satan.|     1|\n...\n```", "```py\nimport numpy as np\n\nfrom pyspark.spark.ml.linalg import Vector, Vectors\nfrom pyspark.sql.functions import udf\n\nk_means_model = pipeline_model.stages[-1]\ncentroids = k_means_model.clusterCenters\n\nclustered = pipeline_model.transform(data)\n\ndef dist_func(cluster, vec):\n    return float(np.linalg.norm(centroids[cluster] - vec))\ndist = udf(dist_func)\n\nthreshold = clustered.select(\"cluster\", \"scaledFeatureVector\").\\\n    withColumn(\"dist_value\",\n        dist(col(\"cluster\"), col(\"scaledFeatureVector\"))).\\\n    orderBy(col(\"dist_value\").desc()).take(100)\n```"]