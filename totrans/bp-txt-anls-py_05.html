<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Feature Engineering and Syntactic Similarity"><div class="chapter" id="ch-vectorization">
<h1><span class="label">Chapter 5. </span>Feature Engineering and <span class="keep-together">Syntactic Similarity</span></h1>

<p>As we saw in <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a>, text is <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="about" id="ch5_term1"/>significantly different from structured data. One of the most striking differences is that text is represented by words, while structured data (mostly) uses numbers. From a scientific point of view, centuries of mathematical research have led to an extremely good understanding of numbers and sophisticated methods. Information science has picked up that mathematical research, and many creative algorithms have been invented on top of that. Recent advances in <a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="data for" id="idm45634198943160"/>machine learning have generalized a lot of formerly very specific algorithms and made them applicable to many different use cases. These methods “learn” directly from data and provide an unbiased view.</p>

<p>To use these instruments, we have to find a mapping of text to numbers. Considering the richness and complexity of text, it is clear that a single number will not be enough to represent the meaning of a document. Something more complex is needed. The <a contenteditable="false" data-type="indexterm" data-primary="vectors" id="idm45634198940744"/>natural extension of real numbers in mathematics is a tuple of real numbers, called <span class="keep-together">a <em>vector</em>.</span> Almost all text representations in text analytics and machine learning use <span class="keep-together">vectors</span>; see <a data-type="xref" href="ch06.xhtml#ch-classification">Chapter 6</a> for more.</p>

<p>Vectors live in a vector space, and most vector spaces have additional properties such as norms and distances, which will be helpful for us as they imply the concept of similarity. As we will see in subsequent chapters, measuring the similarity between documents is absolutely crucial for most text analytics applications, but it is also interesting on its own.</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634198935640">
<h1>What You’ll Learn and What We’ll Build</h1>


<p>In this chapter we talk about the vectorization of documents. This means we will convert unstructured text into vectors that contain numbers.</p>

<p>There are quite a few ways to vectorize documents. As document vectorization is the basis for all machine learning tasks, we will spend some time designing and implementing our own vectorizer. You can use that as a blueprint if you need a specialized vectorizer for your own projects.</p>

<p>Afterward, we will focus on two popular models that are already implemented in scikit-learn: the bag-of words model and the TF-IDF improvements to it. We will download a large dataset of documents and vectorize it with these methods. As you will see, there can be many problems that are related to data volume and scalability.</p>

<p>Although <a contenteditable="false" data-type="indexterm" data-primary="documents" data-secondary="vectorizing" id="idm45634198932056"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="of documents" data-secondary-sortas="documents" id="idm45634198930648"/>vectorization is a base technology for more sophisticated machine learning algorithms, it can also be used on its own to calculate similarities between documents. We will take a detailed look at how this works, how it can be optimized, and how we can make it scalable. For a richer representation of words, see <a data-type="xref" href="ch10.xhtml#ch-embeddings">Chapter 10</a>, and for a more contextualized approach, see <a data-type="xref" href="ch11.xhtml#ch-sentiment">Chapter 11</a>.</p>

<p>After studying this chapter, you will understand how to convert documents to numbers (<em>vectors</em>) using words or combinations as <em>features</em>.<sup><a data-type="noteref" id="idm45634198925240-marker" href="ch05.xhtml#idm45634198925240">1</a></sup> We will try different methods of vectorizing documents, and you will be able to determine the correct method for your use case. You will learn why the similarity of documents is important and a standard way to calculate it. We will go into detail with an example that has many documents and show how to vectorize them and calculate similarities effectively.</p>

<p>The first section introduces the concept of a vectorizer by actually building your own simple one. This can be used as a blueprint for more sophisticated vectorizers that you might have to build in your own projects. Counting <a contenteditable="false" data-type="indexterm" data-primary="bag-of-words models" id="idm45634198921080"/><a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="with bag-of-words models" data-secondary-sortas="bag-of-words models" id="idm45634198919976"/>word occurrences and using them as vectors is called <em>bag-of-words</em> and already creates very versatile models.</p>

<p>Together with the dataset (which has more than 1,000,000 news headlines), we introduce a use case and present a scalable blueprint architecture in the TF-IDF section. We will build a blueprint for vectorizing documents and a similarity search for documents. Even more challenging, we will try to identify the most similar (but nonidentical) headlines in <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term1" id="idm45634198916856"/>this corpus.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="A Toy Dataset for Experimentation"><div class="sect1" id="idm45634198915224">
<h1>A Toy Dataset for Experimentation</h1>

<p>Quite surprisingly, a lot of experiments have shown that for many text analytics problems it is enough to know which words appear in documents. It is not necessary to understand the meaning of the words or take word order into account. As the underlying mappings are particularly easy and fast to calculate, we will start with these mappings and use the words as features.</p>

<p>For the <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="datasets for" id="idm45634198913096"/>first blueprints, we will concentrate on the methods and therefore use a few sentences from the novel <a href="https://oreil.ly/rfmPH"><em>A Tale of Two Cities</em></a> by Charles Dickens as a toy dataset. We will use the following sentences:</p>

<ul>
	<li>It was the best of times.</li>
	<li>It was the worst of times.</li>
	<li>It was the age of wisdom.</li>
	<li>It was the age of foolishness.</li>
</ul>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Building Your Own Vectorizer"><div class="sect1" id="idm45634198908008">
<h1>Blueprint: Building Your Own Vectorizer</h1>

<p>As vectorizing <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="by building vectorizer" data-secondary-sortas="building vectorizer" id="ch5_term5"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="blueprint for building" id="ch5_term6"/>documents is the base for nearly all of the following chapters in this book, we take an in-depth look at how vectorizers work. This works best by implementing our own vectorizer. You can use the methods in this section if you need to implement a custom vectorizer in your own projects or need to adapt an existing vectorizer to your specific requirements.</p>

<p>To make it as <a contenteditable="false" data-type="indexterm" data-primary="one-hot vectorizer" id="idm45634198901560"/>simple as possible, we will implement a so-called <em>one-hot vectorizer</em>. This vectorizer creates binary vectors from documents by noting if a word appears in a document or not, yielding 1 or 0, respectively.</p>

<p>We will start by creating a vocabulary and assigning numbers to words, then perform the vectorization, and finally analyze similarity in this binary space.</p>

<section data-type="sect2" data-pdf-bookmark="Enumerating the Vocabulary"><div class="sect2" id="idm45634198899000">
<h2>Enumerating the Vocabulary</h2>

<p>Starting with the <a contenteditable="false" data-type="indexterm" data-primary="enumeration of vocabulary" id="ch5_term2"/><a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="enumeration of" id="ch5_term4"/>words as features, we have to find a way to convert words to the dimensions of the vectors. Extracting the words from the text is done via tokenization, as explained in <a data-type="xref" href="ch02.xhtml#ch-api">Chapter 2</a>.<sup><a data-type="noteref" id="idm45634198893064-marker" href="ch05.xhtml#idm45634198893064">2</a></sup></p>

<p>As we are interested only in whether a word appears in a document or not, we can just enumerate the words:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">sentences</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"It was the best of times"</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code><code class="s2">"it was the worst of times"</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code><code class="s2">"it was the age of wisdom"</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code><code class="s2">"it was the age of foolishness"</code><code class="p">]</code>
</pre>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tokenized_sentences</code> <code class="o">=</code> <code class="p">[[</code><code class="n">t</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">sentence</code><code class="o">.</code><code class="n">split</code><code class="p">()]</code> <code class="k">for</code> <code class="n">sentence</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">]</code>
<code class="n">vocabulary</code> <code class="o">=</code> <code class="nb">set</code><code class="p">([</code><code class="n">w</code> <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">tokenized_sentences</code> <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">s</code><code class="p">])</code>

<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">([[</code><code class="n">w</code><code class="p">,</code> <code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code><code class="p">,</code><code class="n">w</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">vocabulary</code><code class="p">)])</code>
</pre>

<p><code>Out:</code></p>

<table class="border">
	<tbody>
		<tr>
			<td>It</td>
			<td>0</td>
		</tr>
		<tr>
			<td>age</td>
			<td>1</td>
		</tr>
		<tr>
			<td>best</td>
			<td>2</td>
		</tr>
		<tr>
			<td>foolishness</td>
			<td>3</td>
		</tr>
		<tr>
			<td>it</td>
			<td>4</td>
		</tr>
		<tr>
			<td>of</td>
			<td>5</td>
		</tr>
		<tr>
			<td>the</td>
			<td>6</td>
		</tr>
		<tr>
			<td>times</td>
			<td>7</td>
		</tr>
		<tr>
			<td>was</td>
			<td>8</td>
		</tr>
		<tr>
			<td>wisdom</td>
			<td>9</td>
		</tr>
		<tr>
			<td>worst</td>
			<td>10</td>
		</tr>
	</tbody>
</table>



<p>As you can see, the words have been numbered according to their first occurrence. This is what we call a <em>dictionary</em>, consisting of words (the vocabulary) and their respective numbers. Instead of having to refer to words, we can now use the numbers and arrange them in the following vectors.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Vectorizing Documents"><div class="sect2" id="idm45634198748520">
<h2>Vectorizing Documents</h2>

<p>To compare <a contenteditable="false" data-type="indexterm" data-primary="documents" data-secondary="vectorizing" id="ch5_term9"/><a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="by vectorizing documents" data-secondary-sortas="vectorizing documents" id="ch5_term10"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="of documents" data-secondary-sortas="documents" id="ch5_term11"/>vectors, calculate similarities, and so forth, we have to make sure that vectors for each document have the same number of dimensions. To achieve that, we use the same dictionary for all documents. If the document doesn’t contain a word, we just put a 0 at the corresponding position; otherwise, we will use a 1. By <a contenteditable="false" data-type="indexterm" data-primary="row vectors" id="ch5_term12"/>convention, row vectors are used for documents. The dimension of the vectors is as big <span class="keep-together">as the length</span> of the dictionary. In our example, this is not a problem as we have <span class="keep-together">only a few</span> words. However, in large projects, the vocabulary can easily exceed <span class="keep-together">100,000 words.</span></p>

<p class="pagebreak-before">Let’s calculate the <a contenteditable="false" data-type="indexterm" data-primary="one-hot vectorizer" id="ch5_term8"/>one-hot encoding of all sentences before actually using a library <span class="keep-together">for this:</span></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="k">def</code> <code class="nf">onehot_encode</code><code class="p">(</code><code class="n">tokenized_sentence</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">return</code> <code class="p">[</code><code class="mi">1</code> <code class="k">if</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">tokenized_sentence</code> <code class="k">else</code> <code class="mi">0</code> <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">vocabulary</code><code class="p">]</code>

<code class="n">onehot</code> <code class="o">=</code> <code class="p">[</code><code class="n">onehot_encode</code><code class="p">(</code><code class="n">tokenized_sentence</code><code class="p">)</code>
         <code class="k">for</code> <code class="n">tokenized_sentence</code> <code class="ow">in</code> <code class="n">tokenized_sentences</code><code class="p">]</code>

<code class="k">for</code> <code class="p">(</code><code class="n">sentence</code><code class="p">,</code> <code class="n">oh</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">onehot</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="si">%s</code><code class="s2">: </code><code class="si">%s</code><code class="s2">"</code> <code class="o">%</code> <code class="p">(</code><code class="n">oh</code><code class="p">,</code> <code class="n">sentence</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]: It was the best of times
[1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0]: it was the worst of times
[0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0]: it was the age of wisdom
[0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]: it was the age of foolishness
</pre>

<p>For each sentence, we have now calculated a vector representation. Converting documents to one-hot vectors, we have lost information about how often words occur in documents and in which order.</p>


<section data-type="sect3" data-pdf-bookmark="Out-of-vocabulary documents"><div class="sect3" id="idm45634198641848">
<h3>Out-of-vocabulary documents</h3>

<p>What happens <a contenteditable="false" data-type="indexterm" data-primary="out-of-vocabulary documents" id="idm45634198640184"/>if we try to keep the vocabulary fixed and add new documents? That depends on whether the words of the documents are already contained in the dictionary. Of course, it can happen that all words are already known:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">onehot_encode</code><code class="p">(</code><code class="s2">"the age of wisdom is the best of times"</code><code class="o">.</code><code class="n">split</code><code class="p">())</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]
</pre>

<p>However, the opposite is also quite possible. If <a contenteditable="false" data-type="indexterm" data-primary="null vector" id="idm45634198614216"/>we try to vectorize a sentence with only unknown words, we get a null vector:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">onehot_encode</code><code class="p">(</code><code class="s2">"John likes to watch movies. Mary likes movies too."</code><code class="o">.</code><code class="n">split</code><code class="p">())</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</pre>

<p>This sentence does not “interact” with the other sentences in the corpus. From a strict point of view, this sentence is not similar to any sentence in the corpus. This is no problem for a single sentence; if this happens more frequently, the vocabulary or the corpus needs <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term2" id="idm45634198626968"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term4" id="idm45634198625624"/>to be adjusted.</p>
</div></section>
</div></section>

<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="The Document-Term Matrix"><div class="sect2" id="idm45634198641256">
<h2>The Document-Term Matrix</h2>

<p>Arranging the <a contenteditable="false" data-type="indexterm" data-primary="document-term matrix" id="idm45634198622344"/>row vectors for each document in a matrix with the rows enumerating the documents, we arrive at the document-term matrix. The <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term12" id="idm45634198554296"/>document-term matrix is the vector representation of all documents and the most basic building block for nearly all machine learning tasks throughout this book. In this chapter, we will use it for calculating document similarities:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">onehot</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">vocabulary</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<figure><div class="figure">
<img src="Images/btap_05in01.jpg" width="1183" height="579"/>
<h6/>
</div></figure>

<p>Be careful: using lists and arrays for the document-term matrix works best with a small vocabulary. With large vocabularies, we will have to find a cleverer <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="vectorization with" id="idm45634198525256"/>representation. Scikit-learn takes care of this and uses so-called sparse vectors and matrices from <a href="https://oreil.ly/yk1wx">SciPy</a>.</p>
<section data-type="sect3" data-pdf-bookmark="Calculating similarities"><div class="sect3" id="idm45634198519400">
<h3>Calculating similarities</h3>

<p>Calculating the <a contenteditable="false" data-type="indexterm" data-primary="similarities" data-secondary="calculations for document" id="ch5_term15"/><a contenteditable="false" data-type="indexterm" data-primary="documents" data-secondary="calculating similarities of" id="ch5_term16"/>similarities between documents works by calculating the number of common 1s at the corresponding positions. In one-hot encoding, this is an extremely fast operation, as it can be calculated on the bit level by <code>AND</code>ing the vectors and counting the number of 1s in the resulting vector. Let’s calculate the similarity of the first two sentences:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">sim</code> <code class="o">=</code> <code class="p">[</code><code class="n">onehot</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="n">i</code><code class="p">]</code> <code class="o">&amp;</code> <code class="n">onehot</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">vocabulary</code><code class="p">))]</code>
<code class="nb">sum</code><code class="p">(</code><code class="n">sim</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
4
</pre>

<p>Another possible way to <a contenteditable="false" data-type="indexterm" data-primary="scalar product calculation" id="idm45634198464408"/><a contenteditable="false" data-type="indexterm" data-primary="dot product calculation" id="idm45634198463336"/>calculate the similarity that we will encounter frequently is using <em>scalar product</em> (often called <em>dot product</em>) of the two document vectors. The scalar product is calculated by multiplying corresponding components of the two vectors and adding up these products. By observing the fact that a product can only be 1 if both factors are 1, we effectively calculate the number of common 1s in the vectors. Let’s try it:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">onehot</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">onehot</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
4
</pre>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="The Similarity Matrix"><div class="sect2" id="idm45634198451384">
<h2>The Similarity Matrix</h2>

<p>If we are interested in finding the <a contenteditable="false" data-type="indexterm" data-primary="similarity matrix " id="ch5_term14"/>similarity of all documents to each other, there is a fantastic shortcut for calculating all the numbers with just one command! Generalizing the formula from the previous section, we find the similarity of document <em>i</em> and document <em>j</em> to be as follows:</p>

<div data-type="equation">
  <p><math alttext="upper S Subscript i j Baseline equals normal d Subscript i Baseline dot normal d Subscript j">
  <mrow>
    <msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>
    <mo>=</mo>
    <msub><mi mathvariant="normal">d</mi> <mi>i</mi> </msub>
    <mo>·</mo>
    <msub><mi mathvariant="normal">d</mi> <mi>j</mi> </msub>
  </mrow>
</math></p>
</div>

<p>If we want to use the document-term matrix from earlier, we can write the scalar product as a sum:</p>

<div data-type="equation">
  <p><math alttext="upper S Subscript i j Baseline equals sigma-summation Underscript k Endscripts upper D Subscript i k Baseline upper D Subscript j k Baseline equals sigma-summation Underscript k Endscripts upper D Subscript i k Baseline left-parenthesis upper D Superscript upper T Baseline right-parenthesis Subscript k j Baseline equals left-parenthesis normal upper D dot normal upper D Superscript upper T Baseline right-parenthesis Subscript i j">
  <mrow>
    <msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>
    <mo>=</mo>
    <msub><mo>∑</mo> <mi>k</mi> </msub>
    <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow> </msub>
    <msub><mi>D</mi> <mrow><mi>j</mi><mi>k</mi></mrow> </msub>
    <mo>=</mo>
    <msub><mo>∑</mo> <mi>k</mi> </msub>
    <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow> </msub>
    <msub><mrow><mo>(</mo><msup><mi>D</mi> <mi>T</mi> </msup><mo>)</mo></mrow> <mrow><mi>k</mi><mi>j</mi></mrow> </msub>
    <mo>=</mo>
    <msub><mrow><mo>(</mo><mi mathvariant="normal">D</mi><mo>·</mo><msup><mrow><mi mathvariant="normal">D</mi></mrow> <mi>T</mi> </msup><mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow> </msub>
  </mrow>
</math></p>
</div>

<p>So, this is just the matrix product of our document-term matrix with itself transposed. In Python, that’s now easy to calculate (the sentences in the output have been added for easier checking the similarity):<sup><a data-type="noteref" id="idm45634198361912-marker" href="ch05.xhtml#idm45634198361912">3</a></sup></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">onehot</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">onehot</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
array([[6, 4, 3, 3],       # It was the best of times
       [4, 6, 4, 4],       # it was the worst of times
       [3, 4, 6, 5],       # it was the age of wisdom
       [3, 4, 5, 6]])      # it was the age of foolishness
</pre>

<p>Obviously, the highest numbers are on the diagonal, as each document is most similar to itself. The matrix has to be symmetric, as document <em>A </em>has the same similarity to <em>B</em> as <em>B</em> to <em>A</em>. Apart from that, we can see that the second sentence is on average most similar to all others, whereas the third and last document is the most similar pairwise (they differ only by one word). The same would be true of the first and second documents if we ignored case.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634198348568">
<h5>One-Hot Encoding with scikit-learn</h5>

<p>As discussed earlier, the <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="vectorization with" id="ch5_term17"/>same vectorization can be achieved with scikit-learn. <span class="keep-together">Don’t be</span> tempted to use <code>OneHotEncoder</code>, which is suitable only for categorical features. <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="MultiLabelBinarizer of" id="idm45634198339896"/><a contenteditable="false" data-type="indexterm" data-primary="MultiLabelBinarizer" id="idm45634198338520"/><span class="keep-together">As each sentence</span> has several words, the correct class to use in this case is <span class="keep-together"><code>MultiLabelBinarizer</code></span>:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">MultiLabelBinarizer</code>
<code class="n">lb</code> <code class="o">=</code> <code class="n">MultiLabelBinarizer</code><code class="p">()</code>
<code class="n">lb</code><code class="o">.</code><code class="n">fit</code><code class="p">([</code><code class="n">vocabulary</code><code class="p">])</code>
<code class="n">lb</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">words</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
array([[1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0],
       [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1],
       [0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0],
       [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0]])
</pre>

<p>Here we can already see a pattern that is typical for scikit-learn. All vectorizers (and many other classes) have a fit method and a transform method. The fit method “learns” the vocabulary, whereas the transform method converts the documents to vectors. Fortunately, we got the same results as in our own vectorizer.</p>
</div></aside>

<p>Understanding how a document vectorizer works is crucial for implementing your own, but it’s also helpful for appreciating all the functionalities and parameters of existing vectorizers. This is why we have implemented our own. We have taken a detailed look at the different stages of vectorization, starting with building a vocabulary and then converting the documents to binary vectors.</p>

<p>Afterward, we analyzed the similarity of the documents. It turned out that the dot product of their corresponding vectors is a good measure for this similarity.</p>

<p>One-hot vectors are also used in practice, for example, in document classification and clustering. However, scikit-learn also offers more sophisticated vectorizers, which we will use in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term5" id="idm45634198288440"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term6" id="idm45634198287064"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term8" id="idm45634198285688"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term14" id="idm45634198284312"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term15" id="idm45634198282936"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term16" id="idm45634198281560"/>next sections.</p>

</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Bag-of-Words Models"><div class="sect1" id="idm45634198450792">
<h1>Bag-of-Words Models</h1>

<p>One-hot encoding has already provided us with a basic representation of documents as vectors. However, <a contenteditable="false" data-type="indexterm" data-primary="bag-of-words models" id="ch5_term21"/><a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="with bag-of-words models" data-secondary-sortas="bag-of-words models" id="ch5_term22"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="bag-of-words models for" id="ch5_term23"/>it did not take care of words appearing many times in documents. If we want to calculate the frequency of words for each document, then we should use what is called a <em>bag-of-words</em> representation.</p>

<p>Although somewhat simplistic, these models are in wide use. For cases such as classification and sentiment detection, they work reasonably. Moreover, there are <a contenteditable="false" data-type="indexterm" data-primary="LDA (Latent Dirichlet Allocation) method" id="idm45634198272184"/>topic modeling methods like Latent Dirichlet Allocation (LDA), which explicitly requires a bag-of-words model.<sup><a data-type="noteref" id="idm45634198247384-marker" href="ch05.xhtml#idm45634198247384">4</a></sup></p>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Using scikit-learn’s CountVectorizer"><div class="sect2" id="idm45634198245704">
<h2>Blueprint: Using scikit-learn’s CountVectorizer</h2>

<p>Instead of <a contenteditable="false" data-type="indexterm" data-primary="CountVectorizer" id="ch5_term18"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="CountVectorizer of" id="ch5_term20"/>implementing a bag-of-words model on our own, we use the algorithm that scikit-learn provides.</p>

<p>Notice that the corresponding class is called <code>CountVectorizer</code>, which is our first encounter with feature extraction in scikit-learn. We will take a detailed look at the design of the classes and in which order their methods should be called:</p>

<pre data-type="programlisting">
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
</pre>

<p>Our example sentences from the one-hot encoder is really trivial, as no sentence in our dataset contains words more than once. Let’s add some more sentences and use that as a basis for the <code>CountVectorizer</code>:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">more_sentences</code> <code class="o">=</code> <code class="n">sentences</code> <code class="o">+</code> \
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code><code class="p">[</code><code class="s2">"John likes to watch movies. Mary likes movies too."</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s2">"Mary also likes to watch football games."</code><code class="p">]</code>
</pre>

<p><code>CountVectorizer</code> works in two distinct phases: first it has to learn the vocabulary; afterward it can transform the documents to vectors.</p>
<section data-type="sect3" data-pdf-bookmark="Fitting the vocabulary"><div class="sect3" id="idm45634198211160">
<h3>Fitting the vocabulary</h3>

<p>First, it needs to <a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="feature names as" id="idm45634198209720"/><a contenteditable="false" data-type="indexterm" data-primary="feature names as vocabulary" id="idm45634198208312"/>learn about the vocabulary. This is simpler now, as we can just pass the array with the sentences:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">cv</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">more_sentences</code><code class="p">)</code>
</pre>

<pre data-type="programlisting" data-code-language="python">
<code class="n">CountVectorizer</code><code class="p">(</code><code class="n">analyzer</code><code class="o">=</code><code class="s1">'word'</code><code class="p">,</code> <code class="n">binary</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">decode_error</code><code class="o">=</code><code class="s1">'strict'</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">dtype</code><code class="o">=&lt;</code><code class="k">class</code> <code class="err">'</code><code class="nc">numpy</code><code class="o">.</code><code class="n">int64</code><code class="s1">'&gt;, encoding='</code><code class="n">utf</code><code class="o">-</code><code class="mi">8</code><code class="s1">', input='</code><code class="n">content</code><code class="s1">',</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">lowercase</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">max_df</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">max_features</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">preprocessor</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">stop_words</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">strip_accents</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">token_pattern</code><code class="o">=</code><code class="s1">'(?u)</code><code class="se">\\</code><code class="s1">b</code><code class="se">\\</code><code class="s1">w</code><code class="se">\\</code><code class="s1">w+</code><code class="se">\\</code><code class="s1">b'</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">tokenizer</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">vocabulary</code><code class="o">=</code><code class="bp">None</code><code class="p">)</code>
</pre>

<p>Don’t worry about all these parameters; we will talk about the important ones later. Let’s first see what <code>CountVectorizer</code> used as vocabulary, which is called <em>feature names</em> here:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="n">cv</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
['age', 'also', 'best', 'foolishness', 'football', 'games',
 'it',  'john', 'likes', 'mary', 'movies', 'of', 'the', 'times',
 'to', 'too', 'was', 'watch', 'wisdom', 'worst']
</pre>

<p>We have created a vocabulary and so-called features using <code>CountVectorizer</code>. Conveniently, the vocabulary is sorted alphabetically, which makes it easier for us to decide whether a specific word is included.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Transforming the documents to vectors"><div class="sect3" id="idm45634198042568">
<h3>Transforming the documents to vectors</h3>

<p>In the second step, we will use <code>CountVectorizer</code> to transform the <a contenteditable="false" data-type="indexterm" data-primary="document-term matrix" id="idm45634198040456"/>documents to the vector representation:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">dt</code> <code class="o">=</code> <code class="n">cv</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">more_sentences</code><code class="p">)</code>
</pre>

<p>The result is the document-term matrix that we have already encountered in the previous section. However, it is a different object, as <code>CountVectorizer</code> has created a sparse matrix. Let’s check:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;6x20 sparse matrix of type '&lt;class 'numpy.int64'&gt;'
with 38 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>Sparse <a contenteditable="false" data-type="indexterm" data-primary="sparse matrices" id="idm45634198028968"/>matrices are extremely efficient. Instead of storing 6 × 20 = 120 elements, it just has to save 38! Sparse matrices achieve that by skipping all zero elements.</p>

<p>Let’s try to recover our former document-term matrix. For this, we must transform the sparse matrix to a (dense) array. To <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634197907144"/><a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634197905768"/>make it easier to read, we convert it into a Pandas <code>DataFrame</code>:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">dt</code><code class="o">.</code><code class="n">toarray</code><code class="p">(),</code> <code class="n">columns</code><code class="o">=</code><code class="n">cv</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code></pre>

<p class="pagebreak-before"><code>Out:</code></p>

<figure><div class="figure">
<img src="Images/btap_05in02.jpg" width="2147" height="737"/>
<h6/>
</div></figure>

<p>The document-term matrix looks very similar to the one from our one-hot vectorizer. Note, however, that the columns are in alphabetical order, and observe several 2s in the fifth row. This originates from the document <code>"John likes to watch movies. Mary likes movies too."</code>, which has many <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term18" id="idm45634197994728"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term20" id="idm45634197993464"/>duplicate words.</p>
</div></section>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Calculating Similarities"><div class="sect2" id="idm45634198245368">
<h2>Blueprint: Calculating Similarities</h2>

<p>Finding <a contenteditable="false" data-type="indexterm" data-primary="similarities" data-secondary="calculations for document" id="ch5_term24"/><a contenteditable="false" data-type="indexterm" data-primary="documents" data-secondary="calculating similarities of" id="ch5_term25"/>similarities between documents is now more difficult as it is not enough to count the common 1s in the documents. In general, the number of occurrences of each word can be bigger, and we have to take that into account.  The <a contenteditable="false" data-type="indexterm" data-primary="dot product calculation" id="idm45634197856536"/><a contenteditable="false" data-type="indexterm" data-primary="scalar product calculation" id="idm45634197855432"/>dot product cannot be used for this, as it is also sensitive to the length of the vector (the number of words in the documents). Also, a Euclidean distance is not very useful in high-dimensional vector spaces. This is why most commonly the angle between document vectors is used as a measure of similarity. The cosine of the angle between two vectors is defined by the following:</p>

<div data-type="equation">
  <p><math alttext="normal c normal o normal s left-parenthesis bold a comma bold b right-parenthesis equals StartFraction bold a dot bold b Over StartAbsoluteValue EndAbsoluteValue bold a StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue EndAbsoluteValue bold b StartAbsoluteValue EndAbsoluteValue EndFraction equals StartFraction sigma-summation a Subscript i Baseline b Subscript i Baseline Over StartRoot sigma-summation a Subscript i Baseline a Subscript i Baseline EndRoot StartRoot sigma-summation b Subscript i Baseline b Subscript i Baseline EndRoot EndFraction">
  <mrow>
    <mi> cos </mi>
    <mrow>
      <mo>(</mo>
      <mi>𝐚</mi>
      <mo>,</mo>
      <mi>𝐛</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfrac><mrow><mi>𝐚</mi><mo>·</mo><mi>𝐛</mi></mrow> <mrow><mo>|</mo><mo>|</mo><mi>𝐚</mi><mo>|</mo><mo>|</mo><mo>·</mo><mo>|</mo><mo>|</mo><mi>𝐛</mi><mo>|</mo><mo>|</mo></mrow></mfrac>
    <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi> </msub><msub><mi>b</mi> <mi>i</mi> </msub></mrow> <mrow><msqrt><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi> </msub><msub><mi>a</mi> <mi>i</mi> </msub></mrow></msqrt><msqrt><mrow><mo>∑</mo><msub><mi>b</mi> <mi>i</mi> </msub><msub><mi>b</mi> <mi>i</mi> </msub></mrow></msqrt></mrow></mfrac>
  </mrow>
</math></p>
</div>

<p>Scikit-learn <a contenteditable="false" data-type="indexterm" data-primary="cosine_similarity function" id="idm45634197809352"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="cosine_similarity function of" id="idm45634197808344"/>simplifies this calculation by offering a <code>cosine_similarity</code> utility function. Let’s check the similarity of the first two sentences:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.metrics.pairwise</code> <code class="kn">import</code> <code class="n">cosine_similarity</code>
<code class="n">cosine_similarity</code><code class="p">(</code><code class="n">dt</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">dt</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
array([[0.83333333]])
</pre>

<p>Compared to our handmade similarity in the earlier sections, <code>cosine_similarity</code> offers some advantages, as it is properly normalized and can take only values between 0 and 1.</p>

<p>Calculating the similarity of all documents is of course also possible; scikit-learn has optimized the <code>cosine_similarity</code>, so it is possible to directly pass matrices:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">cosine_similarity</code><code class="p">(</code><code class="n">dt</code><code class="p">,</code> <code class="n">dt</code><code class="p">)))</code></pre>

<p><code>Out:</code></p>

<table class="border">
	<thead>
		<tr>
			<th class="xl63"> </th>
			<th class="xl63">0</th>
			<th class="xl63">1</th>
			<th class="xl63">2</th>
			<th class="xl63">3</th>
			<th class="xl63">4</th>
			<th class="xl63">5</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th class="xl63">0</th>
			<td class="xl64">1.000000</td>
			<td class="xl65">0.833333</td>
			<td class="xl65">0.666667</td>
			<td class="xl65">0.666667</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
		</tr>
		<tr>
			<th class="xl63">1</th>
			<td class="xl65">0.833333</td>
			<td class="xl64">1.000000</td>
			<td class="xl65">0.666667</td>
			<td class="xl65">0.666667</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
		</tr>
		<tr>
			<th class="xl63">2</th>
			<td class="xl65">0.666667</td>
			<td class="xl65">0.666667</td>
			<td class="xl64">1.000000</td>
			<td class="xl65">0.833333</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
		</tr>
		<tr>
			<th class="xl63">3</th>
			<td class="xl65">0.666667</td>
			<td class="xl65">0.666667</td>
			<td class="xl65">0.833333</td>
			<td class="xl64">1.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
		</tr>
		<tr>
			<th class="xl63">4</th>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl64">1.000000</td>
			<td class="xl65">0.524142</td>
		</tr>
		<tr>
			<th class="xl63">5</th>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.524142</td>
			<td class="xl64">1.000000</td>
		</tr>
	</tbody>
</table>

<p>Again, the matrix is symmetric with the highest values on the diagonal. It’s also easy to see that document pairs 0/1 and 2/3 are most similar. Documents 4/5 have no similarity at all to the other documents but have some similarity to each other. Taking a look back at the sentences, this is exactly what <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term17" id="idm45634197693352"/>one would expect.</p>

<p>Bag-of-words models <a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for feature dimensions" data-secondary-sortas="feature dimensions" id="idm45634197691400"/><a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="use cases for" id="idm45634197689720"/>are suitable for a variety of use cases. For classification, sentiment detection, and many topic models, they create a bias toward the most frequent words as they have the highest numbers in the document-term matrix. Often these words do not carry much meaning and could be defined as stop words.</p>

<p>As these would be highly domain-specific, a more generic approach “punishes” words that appear too often in the corpus of all documents. This is called a <em>TF-IDF model</em> and will be discussed in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term21" id="idm45634197686920"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term22" id="idm45634197685544"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term23" id="idm45634197684168"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term24" id="idm45634197682792"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term25" id="idm45634197681416"/>next section.</p>
</div></section>
</div></section>


<section data-type="sect1" data-pdf-bookmark="TF-IDF Models"><div class="sect1" id="idm45634197991368">
<h1>TF-IDF Models</h1>

<p>In our previous example, many sentences started with the words “it was the time of.” This contributed a lot to their similarity, but in reality, the actual information you get by the words is minimal. <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="with TF-IDF models" data-secondary-sortas="TF-IDF models" id="ch5_term26"/><a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="vectorization with" id="ch5_term27"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with TF-IDF weighting" data-secondary-sortas="TF-IDF weighting" id="ch5_term28"/>TF-IDF will take care of that by counting the number of total word occurrences. It will reduce weights of frequent words and at the same time increase the weights of uncommon words. Apart from the information-theoretical measure,<sup><a data-type="noteref" id="idm45634197672360-marker" href="ch05.xhtml#idm45634197672360">5</a></sup> this is also something that you can observe when reading documents: if you encounter uncommon words, it is likely that the author wants to convey an important message with them.</p>

<section data-type="sect2" data-pdf-bookmark="Optimized Document Vectors with TfidfTransformer"><div class="sect2" id="idm45634197670200">
<h2>Optimized Document Vectors with TfidfTransformer</h2>

<p>As we saw in <a data-type="xref" href="ch02.xhtml#ch-api">Chapter 2</a>, a better measure for <a contenteditable="false" data-type="indexterm" data-primary="TfidfTransformer" id="idm45634197667544"/>information compared to counting is calculating the inverted document frequency and using a penalty for very common words. The TF-IDF weight can be calculated from the bag-of-words model. Let’s try this again with the previous model and see how the weights of the document-term matrix change:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfTransformer</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfTransformer</code><code class="p">()</code>
<code class="n">tfidf_dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">dt</code><code class="p">)</code>
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">tfidf_dt</code><code class="o">.</code><code class="n">toarray</code><code class="p">(),</code> <code class="n">columns</code><code class="o">=</code><code class="n">cv</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code>
</pre>

<p><code>Out:</code></p>

<figure><div class="figure">
<img src="Images/btap_05in03.jpg" width="2147" height="737"/>
<h6/>
</div></figure>

<p>As you can see, some words have been scaled to smaller values (like “it”), while others have not been scaled down so much (like “wisdom”). Let’s see the effect on the similarity matrix:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">cosine_similarity</code><code class="p">(</code><code class="n">tfidf_dt</code><code class="p">,</code> <code class="n">tfidf_dt</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<table class="border">
	<thead>
		<tr>
			<th class="xl65"> </th>
			<th class="xl65">0</th>
			<th class="xl65">1</th>
			<th class="xl65">2</th>
			<th class="xl65">3</th>
			<th class="xl65">4</th>
			<th class="xl65">5</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th class="xl65">0</th>
			<td class="xl66">1.000000</td>
			<td class="xl65">0.675351</td>
			<td class="xl65">0.457049</td>
			<td class="xl65">0.457049</td>
			<td class="xl65">0.00000</td>
			<td class="xl65">0.00000</td>
		</tr>
		<tr>
			<th class="xl65">1</th>
			<td class="xl65">0.675351</td>
			<td class="xl66">1.000000</td>
			<td class="xl65">0.457049</td>
			<td class="xl65">0.457049</td>
			<td class="xl65">0.00000</td>
			<td class="xl65">0.00000</td>
		</tr>
		<tr>
			<th class="xl65">2</th>
			<td class="xl65">0.457049</td>
			<td class="xl65">0.457049</td>
			<td class="xl66">1.000000</td>
			<td class="xl65">0.675351</td>
			<td class="xl65">0.00000</td>
			<td class="xl65">0.00000</td>
		</tr>
		<tr>
			<th class="xl65">3</th>
			<td class="xl65">0.457049</td>
			<td class="xl65">0.457049</td>
			<td class="xl65">0.675351</td>
			<td class="xl66">1.000000</td>
			<td class="xl65">0.00000</td>
			<td class="xl65">0.00000</td>
		</tr>
		<tr>
			<th class="xl65">4</th>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl66">1.000000</td>
			<td class="xl65">0.43076</td>
		</tr>
		<tr>
			<th class="xl65">5</th>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.000000</td>
			<td class="xl65">0.43076</td>
			<td class="xl66">1.000000</td>
		</tr>
	</tbody>
</table>

<p>We get exactly the effect we have hoped for! Document pairs 0/1 and 2/3 are still very similar, but the number has also decreased to a more reasonable level as the document pairs differ in <em>significant words</em>. The more common words now have <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term9" id="idm45634197532728"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term10" id="idm45634197531352"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term11" id="idm45634197529976"/>lower weights.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Introducing the ABC Dataset"><div class="sect2" id="idm45634197669608">
<h2>Introducing the ABC Dataset</h2>

<p>As a real-word <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="use cases for" id="idm45634197526392"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for feature dimensions" data-secondary-sortas="feature dimensions" id="idm45634197525016"/>use case, we will take a <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="from Kaggle" data-secondary-sortas="Kaggle" id="idm45634197523240"/><a href="https://oreil.ly/hg5R3">dataset from Kaggle</a> that contains news headlines. Headlines originate from <a contenteditable="false" data-type="indexterm" data-primary="ABC news headlines" id="ch5_term29"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="ABC news headlines" id="ch5_term30"/><a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="datasets for" id="ch5_term32"/>Australian news source ABC and are from 2003 to 2017. The CSV file contains only a timestamp and the headline without punctuation in lowercase. We load the CSV file into a <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634197515944"/><a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634197514568"/>Pandas <code>DataFrame</code> and take a look at the first few documents:</p>

<pre class="pre drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-type="programlisting">
headlines = pd.read_csv("abcnews-date-text.csv", parse_dates=["publish_date"])
print(len(headlines))
headlines.head()
</pre>

<p><code>Out:</code></p>

<pre class="pre drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-type="programlisting">
1103663</pre>

<table class="border">
	<thead>
		<tr>
			<th> </th>
			<th>publish_date</th>
			<th>headline_text</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>0</th>
			<td>2003-02-19</td>
			<td>aba decides against community broadcasting lic...</td>
		</tr>
		<tr>
			<th>1</th>
			<td>2003-02-19</td>
			<td>act fire witnesses must be aware of defamation</td>
		</tr>
		<tr>
			<th>2</th>
			<td>2003-02-19</td>
			<td>a g calls for infrastructure protection summit</td>
		</tr>
		<tr>
			<th>3</th>
			<td>2003-02-19</td>
			<td>air nz staff in aust strike for pay rise</td>
		</tr>
		<tr>
			<th>4</th>
			<td>2003-02-19</td>
			<td>air nz strike to affect australian travellers</td>
		</tr>
	</tbody>
</table>

<p>There are 1,103,663  headlines in this dataset. Note that the headlines do not include punctuation and are all transformed to lowercase. Apart from the text, the dataset includes the publication date of each headline.</p>

<p>As we saw earlier, the TF-IDF vectors can be calculated using the bag-of-words model (the <em>count vectors</em> in scikit-learn terminology). As it is so common to use TF-IDF document vectors, scikit-learn has created a “shortcut” to skip the count vectors and directly calculate the TF-IDF vectors. The corresponding <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="TfidfVectorizer of" id="idm45634197495784"/><a contenteditable="false" data-type="indexterm" data-primary="TfidfVectorizer" id="idm45634197494408"/>class is called <code>TfidfVectorizer</code>, and we will use it next.</p>


<p class="pagebreak-before">In the following<a contenteditable="false" data-type="indexterm" data-primary="fit_transform method" id="idm45634197492040"/>, we have also combined the calls to fit and to transform in <code>fit_transform</code>, which is convenient:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">()</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
</pre>

<p>This might take a while, as so many documents have to be analyzed and vectorized. Take a look at the dimensions of the document-term matrix:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x95878 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 7001357 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>The number of rows was expected, but the number of columns (the vocabulary) is really large, with almost 100,000 words. Doing the math shows that a naive storage of data would have led to  1,103,663 * 95,878 elements with 8 bytes per float and have used roughly 788 GB RAM. This shows the incredible effectiveness of <a contenteditable="false" data-type="indexterm" data-primary="sparse matrices" id="idm45634197477960"/>sparse matrices as the real memory used is “only” 56,010,856 bytes (roughly 0.056 GB; found out via <code>dt.data.nbytes</code>). It’s still a lot, but it’s manageable.</p>

<p>Calculating the similarity between two vectors is another story, though. <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="vectorization with" id="idm45634197466776"/>Scikit-learn (and SciPy as a basis) is highly optimized for working with sparse vectors, but it still takes some time doing the sample calculation (similarities of the first 10,000 <span class="keep-together">documents</span>):</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="o">%%</code><code class="n">time</code>
<code class="n">cosine_similarity</code><code class="p">(</code><code class="n">dt</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">10000</code><code class="p">],</code> <code class="n">dt</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">10000</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
CPU times: user 154 ms, sys: 261 ms, total: 415 ms

Wall time: 414 ms

array([[1.      , 0.      , 0.      , ..., 0.        , 0.        , 0.        ],
       [0.      , 1.      , 0.      , ..., 0.        , 0.        , 0.        ],
       [0.      , 0.      , 1.      , ..., 0.        , 0.        , 0.        ],
       ...,
       [0.      , 0.      , 0.      , ..., 1.        , 0.16913596, 0.16792138],
       [0.      , 0.      , 0.      , ..., 0.16913596, 1.        , 0.33258708],
       [0.      , 0.      , 0.      , ..., 0.16792138, 0.33258708, 1.        ]])
</pre>

<p>For machine learning in the next chapters, many of these linear algebra calculations are necessary and have to be repeated over and over. Often operations scale quadratically with the number of features (O(N<sup>2</sup>)). Optimizing the vectorization by removing unnecessary features is therefore not only helpful for calculating the similarities but also crucial for scalable machine <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term29" id="idm45634197398392"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term30" id="idm45634197397016"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term32" id="idm45634197395640"/>learning.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Reducing Feature Dimensions"><div class="sect2" id="idm45634197527560">
<h2>Blueprint: Reducing Feature Dimensions</h2>

<p>We have <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="about" id="idm45634197392696"/>now found <a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="feature dimensions of" id="ch5_term34"/>features for our documents and used them to calculate document vectors. As we have seen in the example, the number of features can get quite large. Lots of machine learning algorithms are computationally intensive and scale with the number of features, often even polynomially. One part of feature engineering is therefore focused on reducing these features to the ones that are really necessary.</p>

<p>In this section, we show a blueprint for how this can be achieved and measure their impact on the number of features.</p>
<section data-type="sect3" data-pdf-bookmark="Removing stop words"><div class="sect3" id="idm45634197388296">
<h3>Removing stop words</h3>

<p>In the first place, we <a contenteditable="false" data-type="indexterm" data-primary="stop words" id="idm45634197386504"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="idm45634197385368"/>can think about removing the words that carry the least meaning. Although this is domain-dependent, there are lists of the most common English words that common sense tells us can normally be neglected. These words are called <em>stop words</em>. Common stop words are determiners, auxiliary verbs, and pronouns. <span class="keep-together">For a more</span> detailed discussion, see <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>. Be careful when removing stop words <span class="keep-together">as they</span> can contain certain words that might carry a domain-specific meaning in <span class="keep-together">special texts!</span></p>

<p>This does not reduce the number of dimensions tremendously as there are only a few hundred common stop words in almost any language. However, it should drastically decrease the number of stored elements as stop words are so common. This leads <span class="keep-together">to less</span> memory consumption and faster calculations, as fewer numbers need to be <span class="keep-together">multiplied</span>.</p>

<p>Let’s use the standard spaCy stop words and check the effects on the document-term matrix. Note that we pass stop words as a named parameter to the <code>TfidfVectorizer</code>:</p>

<pre class="pre drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">from</code> <code class="nn">spacy.lang.en.stop_words</code> <code class="kn">import</code> <code class="n">STOP_WORDS</code> <code class="k">as</code> <code class="n">stopwords</code>
<code class="k">print</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">stopwords</code><code class="p">))</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
305
&lt;1103663x95600 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 5644186 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>With only 305 stop words, we managed to reduce the number of stored elements by 20%. The dimensions of the matrix are almost the same, with fewer columns due to the 95,878 – 95,600 = 278 stop words that actually appeared in the headlines.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Minimum frequency"><div class="sect3" id="idm45634197326088">
<h3>Minimum frequency</h3>

<p>Taking a <a contenteditable="false" data-type="indexterm" data-primary="minimum frequency of words" id="idm45634197324168"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="max_df and min_df parameters of" id="idm45634197323064"/>look at the <a contenteditable="false" data-type="indexterm" data-primary="cosine similarity" id="idm45634197321592"/>definition of the cosine similarity, we can easily see that <span class="keep-together">components</span> can contribute only if both vectors have a nonzero value at the corresponding index. This means that we can neglect all words occurring less than twice! <span class="keep-together"><code>TfidfVectorizer</code></span> (and <code>CountVectorizer</code>) have a <a contenteditable="false" data-type="indexterm" data-primary="min_df parameter" id="idm45634197318200"/>parameter for that called <code>min_df</code>.</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x58527 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 5607113 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>Obviously, there are a lot of words appearing just once (95,600 – 58,527 = 37,073). Those words should also be stored only once; checking with the number of stored elements, we should get the same result: 5,644,186 – 5,607,113 = 37,073. Performing this kind of transformation, it is always useful to integrate such plausibility checks.</p>

<div data-type="warning" epub:type="warning">
<h1>Losing Information</h1>

<p>Be careful: by using <code>min_df=2</code>, we have not lost any information in vectorizing the headlines of this document corpus. If we plan to vectorize more documents later with the same vocabulary, we might lose information, as words appearing again in the new documents that were only present once in the original documents will not be found in the vocabulary.</p>
</div>

<p><code>min_df</code> can also take float values. This means that a word has to occur in a minimum fraction of documents. Normally, this reduces the vocabulary drastically even for low numbers of <code>min_df</code>:</p>

<pre class="drop-out-of-bounds drop-out-of-bounds-bottom drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=.</code><code class="mo">0001</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x6772 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 4816381 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>This transformation is probably too strict and reduces the vocabulary too far. Depending on the number of documents, you should set <code>min_df</code> to a low integer and check the effects on the vocabulary.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Maximum frequency"><div class="sect3" id="idm45634197325496">
<h3>Maximum frequency</h3>

<p>Sometimes a <a contenteditable="false" data-type="indexterm" data-primary="max_df parameter" id="idm45634197202056"/>text corpus <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="use cases for" id="idm45634197200792"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for feature dimensions" data-secondary-sortas="feature dimensions" id="idm45634197199416"/>might have a special vocabulary with lots of repeating terms that are too specific to be contained in stop word lists. For this use case, scikit-learn offers the <code>max_df</code> parameter, which eliminates terms occurring too often in the corpus. Let’s check how the dimensions are reduced when we eliminate all the words that appear in at least 10% of the headlines:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">max_df</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x95600 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 5644186 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>Setting <code>max_df</code> to a low value of 10% does not eliminate a single word!<sup><a data-type="noteref" id="idm45634197156184-marker" href="ch05.xhtml#idm45634197156184">6</a></sup> Our news headlines are very diverse. Depending on the type of corpus you have, experimenting with <code>max_df</code> can be quite useful. In any case, you should always check how the dimensions change.</p>
</div></section>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Improving Features by Making Them More Specific"><div class="sect2" id="idm45634197152456">
<h2>Blueprint: Improving Features by Making Them <span class="keep-together">More Specific</span></h2>

<p>So far, we have only used the original words of the headlines and reduced the number of dimensions by stop words and counting frequencies. We <a contenteditable="false" data-type="indexterm" data-primary="specificity in feature engineering" id="ch5_term36"/>have not yet changed the features themselves. Using linguistic analysis, there are more possibilities.</p>
<section data-type="sect3" data-pdf-bookmark="Performing linguistic analysis"><div class="sect3" id="idm45634197148456">
<h3>Performing linguistic analysis</h3>

<p>Using spaCy, we can <a contenteditable="false" data-type="indexterm" data-primary="lemmatization" id="idm45634197147160"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for lemmas with part-of-speech, extracting" data-secondary-sortas="lemmas with part-of-speech, extracting" id="idm45634197146024"/><a contenteditable="false" data-type="indexterm" data-primary="part-of-speech tags" id="idm45634197144344"/>lemmatize all headlines and just keep the lemmas. This takes some time, but we expect to find a smaller vocabulary. First, we have to perform a linguistic analysis, which might take some time to finish (see <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a> for more details):</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">spacy</code>

<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"en"</code><code class="p">)</code>
<code class="n">nouns_adjectives_verbs</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"NOUN"</code><code class="p">,</code> <code class="s2">"PROPN"</code><code class="p">,</code> <code class="s2">"ADJ"</code><code class="p">,</code> <code class="s2">"ADV"</code><code class="p">,</code> <code class="s2">"VERB"</code><code class="p">]</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">headlines</code><code class="o">.</code><code class="n">iterrows</code><code class="p">():</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="nb">str</code><code class="p">(</code><code class="n">row</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">]))</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">headlines</code><code class="o">.</code><code class="n">at</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="s2">"lemmas"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">token</code><code class="o">.</code><code class="n">lemma_</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">])</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">headlines</code><code class="o">.</code><code class="n">at</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="s2">"nav"</code><code class="p">]</code> <code class="o">=</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">token</code><code class="o">.</code><code class="n">lemma_</code> <code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">doc</code>
<code class="err"> </code>                    <code class="k">if</code> <code class="n">token</code><code class="o">.</code><code class="n">pos_</code> <code class="ow">in</code> <code class="n">nouns_adjectives_verbs</code><code class="p">])</code>
</pre>
</div></section>
</div></section>


<section data-type="sect2" data-pdf-bookmark="Blueprint: Using Lemmas Instead of Words for Vectorizing Documents"><div class="sect2" id="idm45634197138936">
<h2>Blueprint: Using Lemmas Instead of Words for Vectorizing Documents</h2>

<p>Now, we can <a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="of documents" data-secondary-sortas="documents" id="idm45634197015720"/><a contenteditable="false" data-type="indexterm" data-primary="documents" data-secondary="vectorizing" id="idm45634197014072"/><a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="by vectorizing documents" data-secondary-sortas="vectorizing documents" id="idm45634197012696"/>vectorize the data using the lemmas and see how the vocabulary decreased:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"lemmas"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="nb">str</code><code class="p">))</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x71921 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 5053610 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>Saving almost 25,000 dimensions is a lot. In <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="use cases for" id="idm45634196998504"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for feature dimensions" data-secondary-sortas="feature dimensions" id="idm45634196986888"/>news headlines, lemmatizing the data probably does not lose any information. In other use cases like those in <a data-type="xref" href="ch11.xhtml#ch-sentiment">Chapter 11</a>, it’s a completely different story.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634196983896">
<h5>Features, Dimensions, and Precision/Recall</h5>

<p>Lemmatizing considerably reduces the vocabulary size. For news headlines, the tense, for example, is not important. When interpreting a novel, it might play a crucial role, however. Depending on your use case, you should think carefully about which text/NLP transformation is useful.</p>

<p>Reducing the <a contenteditable="false" data-type="indexterm" data-primary="max_df parameter" id="idm45634196981880"/><a contenteditable="false" data-type="indexterm" data-primary="min_df parameter" id="idm45634196980744"/><a contenteditable="false" data-type="indexterm" data-primary="minimum frequency of words" id="idm45634196979640"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="max_df and min_df parameters of" id="idm45634196978568"/>feature dimensions with <code>min_df</code> and <code>max_df</code> is also a double-sided sword. Removing infrequent features might be good for a corpus, but if you add additional documents, there might be too few features.</p>

<p>In later chapters, we will introduce <a contenteditable="false" data-type="indexterm" data-primary="precision and recall" id="idm45634196975544"/><a contenteditable="false" data-type="indexterm" data-primary="recall" id="idm45634196974440"/>precision and recall as quality metrics for information retrieval. We can then quantify the effects of reducing dimensions (with NLP and vectorization tuning) and increasing dimensions (with bigrams, for example) by observing the impact on <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term36" id="idm45634196972952"/>these metrics.</p>
</div></aside>
</div></section>

<section data-type="sect2" class="blueprint pagebreak-before less_space" data-pdf-bookmark="Blueprint: Limit Word Types"><div class="sect2" id="idm45634196971192">
<h2>Blueprint: Limit Word Types</h2>

<p>Using the <a contenteditable="false" data-type="indexterm" data-primary="word types, limiting" id="idm45634196969336"/>data generated earlier, we can restrict ourselves to considering just nouns, adjectives, and verbs for the vectorization, as prepositions, conjugations, and so on are supposed to carry little meaning. This again reduces the vocabulary:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"nav"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="nb">str</code><code class="p">))</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x68426 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 4889344 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>There’s not much to win here, which is probably due to the headlines mainly containing nouns, adjectives, and verbs. But this might look totally different in your own projects. Depending on the type of texts you are analyzing, restricting word types will not only reduce the size of the vocabulary but will also lead to much lower noise. It’s a good idea to try this with a small part of the corpus first to avoid long waiting times due to the expensive linguistic analysis.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Remove Most Common Words"><div class="sect2" id="idm45634196875448">
<h2>Blueprint: Remove Most Common Words</h2>

<p>As we <a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="filtering of" id="ch5_term38"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="filtering" id="ch5_term39"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="removing common" id="ch5_term40"/>learned, removing frequent words can lead to document-term matrices with far fewer entries. This is especially helpful when you perform unsupervised learning, as you will normally not be interested in common words that are common anyway.</p>

<p>To remove even more noise, we will now try to eliminate the most common English words. Be careful, as there will normally also be words involved that might carry important meaning. There are various lists with those words; they can easily be found on the internet. The <a href="https://oreil.ly/bOho1">list from Google</a> is rather popular and directly available on GitHub. Pandas can directly read the list if we tell it to be a CSV file without column headers. We will then instruct the <code>TfidfVectorizer</code> to use that list as stop words:</p>

<pre class="pre drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">top_10000</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s2">"https://raw.githubusercontent.com/first20hours/</code><code class="se">\</code>
<code class="s2">google-10000-english/master/google-10000-english.txt"</code><code class="p">,</code> <code class="n">header</code><code class="o">=</code><code class="bp">None</code><code class="p">)</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="nb">set</code><code class="p">(</code><code class="n">top_10000</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">))</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"nav"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="nb">str</code><code class="p">))</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x61630 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 1298200 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>As you can see, the matrix now has 3.5 million fewer stored elements. The vocabulary shrunk by 68,426 – 61,630 = 6,796 words, so <a contenteditable="false" data-type="indexterm" data-primary="ABC news headlines" id="idm45634196773688"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="ABC news headlines" id="idm45634196772616"/>more than 3,000 of the most frequent English words were not even used in the ABC headlines.</p>

<p>Removing frequent words is an excellent method to remove noise from the dataset and concentrate on the uncommon words. However, you should be careful using this from the beginning as even frequent words do have a meaning, and they might also have a special meaning in your document corpus. We recommend performing such analyses additionally but not exclusively.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Adding Context via N-Grams"><div class="sect2" id="idm45634196770072">
<h2>Blueprint: Adding Context via N-Grams</h2>

<p>So far, we have <a contenteditable="false" data-type="indexterm" data-primary="n-grams" id="ch5_term43"/>used only single words as features (dimensions of our document vectors) as the basis for our vectorization. With this strategy, we have lost a lot of context information. Using single words as features does not respect the context in which the words appear. <a data-type="link" href="ch10.xhtml#ch-embeddings">In later chapters</a> we will learn how to overcome that limitation with sophisticated models like word embeddings. In our current example, we will use a simpler method and take advantage of word combinations, so called <em>n-grams</em>. Two-word combinations <a contenteditable="false" data-type="indexterm" data-primary="bigrams" id="ch5_term44"/>are called <em>bigrams</em>; for three words, they are called <em>trigrams</em>.</p>

<p>Fortunately, <code>CountVectorizer</code> and <code>TfidfVectorizer</code> have <a contenteditable="false" data-type="indexterm" data-primary="CountVectorizer" id="idm45634196760680"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="CountVectorizer of" id="idm45634196759544"/><a contenteditable="false" data-type="indexterm" data-primary="TfidfVectorizer" id="ch5_term41"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="TfidfVectorizer of" id="ch5_term42"/>the corresponding options. Contrary to the last few sections where we tried to reduce the vocabulary, we now enhance the vocabulary with word combinations. There are many such combinations; their number (and vocabulary size) is growing almost exponentially with <em>n</em>.<sup><a data-type="noteref" id="idm45634196754328-marker" href="ch05.xhtml#idm45634196754328">7</a></sup> We will therefore be careful and start with bigrams:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="n">dt</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">dt</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">nbytes</code><code class="p">)</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">3</code><code class="p">),</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="n">dt</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">dt</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">nbytes</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
(1103663, 559961)
67325400
(1103663, 747988)
72360104
</pre>

<p>Increasing the feature dimensions from 95,600 to 2,335,132 or even 5,339,558 is quite painful even though the RAM size has not grown too much. For some tasks that need context-specific information (like sentiment analysis), n-grams are extremely useful. It is always useful to keep an eye on the dimensions, though.</p>

<p>Combining n-grams with linguistic features and common words is also possible and reduces the vocabulary size considerably:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code>
        <code class="n">stop_words</code><code class="o">=</code><code class="nb">set</code><code class="p">(</code><code class="n">top_10000</code><code class="o">.</code><code class="n">iloc</code><code class="p">[:,</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">))</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"nav"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="nb">str</code><code class="p">))</code>
<code class="n">dt</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;1103663x385857 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 1753239 stored elements in Compressed Sparse Row format&gt;
Compared to the original bigram vectorization with min_df=2 above,
there are just 82,370 dimensions left from 67,325,400
</pre>

<p>Scikit-learn <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="vectorization with" id="idm45634196529128"/>offers many different vectorizers. Normally, starting with <code>TfidfVectorizer</code> is a good idea, as it is one of the most versatile.</p>


<section data-type="sect3" data-pdf-bookmark="Options of TfidfVectorizer"><div class="sect3" id="idm45634196527016">
<h3>Options of TfidfVectorizer</h3>

<p>TF-IDF can even be switched off so there is a seamless fallback to <code>CountVectorizer</code>. Because of the many parameters, it can take some time to find the perfect set of options.</p>

<p>Finding the correct set of features is often tedious and requires experimentation with the (many) parameters of <code>TfidfVectorizer</code>, like <code>min_df</code>, <code>max_df</code>, or simplified text via NLP. In our work, we have had good experiences with setting <code>min_df</code> to <code>5</code>, for example, and <code>max_df</code> to <code>0.7</code>. In the end, this time is excellently invested as the results will depend heavily on correct vectorization. There is <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="use cases for" id="idm45634196615192"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for feature dimensions" data-secondary-sortas="feature dimensions" id="idm45634196613848"/>no golden bullet, though, and this <em>feature engineering</em> depends heavily on the use case and the planned use of the vectors.</p>

<p>The TF-IDF method itself can be improved by using a subnormal term frequency or normalizing the resulting vectors. The latter is useful for quickly calculating similarities, and we demonstrate its use later in the chapter. The former is mainly interesting for long documents to avoid repeating words getting a too <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term41" id="idm45634196611032"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term42" id="idm45634196609656"/>high-weight.</p>

<section data-type="sect4" data-pdf-bookmark="Think very carefully about feature dimensions"><div class="sect4" id="idm45634196608024">
<h4>Think very carefully about feature dimensions</h4>

<p>In our previous examples, we have used single words and bigrams as features. Depending on the use case, this might already be enough. This works well for texts with common vocabulary, like news. But often you will encounter special vocabularies (for example, scientific publications or letters to an insurance company), which will require more <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term44" id="idm45634196606328"/>sophisticated feature engineering.</p>
</div></section>

<section data-type="sect4" data-pdf-bookmark="Keep number of dimensions in mind"><div class="sect4" id="idm45634196604568">
<h4>Keep number of dimensions in mind</h4>

<p>As we have seen, using parameters like <code>ngram_range</code> can lead to large feature spaces. Apart from the RAM usage, this will also be a problem for many <a contenteditable="false" data-type="indexterm" data-primary="overfitting of trained model" id="idm45634196602520"/>machine learning algorithms due to overfitting. Therefore, it’s a good idea to always consider the (increase of) feature dimensions when changing parameters or <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term26" id="idm45634196601096"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term27" id="idm45634196599720"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term28" id="idm45634196598344"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term34" id="idm45634196596968"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term38" id="idm45634196595592"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term39" id="idm45634196594216"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term40" id="idm45634196592840"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term43" id="idm45634196498824"/>vectorization methods.</p>
</div></section>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Syntactic Similarity in the ABC Dataset"><div class="sect1" id="idm45634197679448">
<h1>Syntactic Similarity in the ABC Dataset</h1>

<p>Similarity is <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="syntactic similarities in" id="ch5_term56"/><a contenteditable="false" data-type="indexterm" data-primary="syntactic similarities" id="ch5_term49"/><a contenteditable="false" data-type="indexterm" data-primary="similarities" data-secondary="syntactic" id="ch5_term48"/>one of the most basic concepts in machine learning and text analytics. In this section, we take a <a contenteditable="false" data-type="indexterm" data-primary="ABC news headlines" id="ch5_term45"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="ABC news headlines" id="ch5_term46"/><a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="datasets for" id="ch5_term51"/>look at some challenging problems finding similar documents in the ABC dataset.</p>

<p>After taking a look at possible vectorizations in the previous section, we will now use one of them to calculate the similarities. We will present a blueprint to show how you can perform these calculations efficiently from both a CPU and a RAM perspective. As we are handling large amounts of data here, we have to make <a contenteditable="false" data-type="indexterm" data-primary="NumPy library" id="idm45634196485288"/>extensive use of the <a href="https://numpy.org">NumPy library</a>.</p>

<p>In the first step, we <a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with finding syntactic similarity" data-secondary-sortas="finding syntactic similarity" id="ch5_term50"/>vectorize the data using stop words and bigrams:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="c1"># there are "test" headlines in the corpus</code>
<code class="n">stopwords</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="s2">"test"</code><code class="p">)</code>
<code class="n">tfidf</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">ngram_range</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">),</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> \
                        <code class="n">norm</code><code class="o">=</code><code class="s1">'l2'</code><code class="p">)</code>
<code class="n">dt</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
</pre>

<p>We are now ready to use these vectors for our blueprints.</p>
<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Finding Most Similar Headlines to a Made-up Headline"><div class="sect2" id="idm45634196401640">
<h2>Blueprint: Finding Most Similar Headlines to a <span class="keep-together">Made-up Headline</span></h2>

<p>Let’s say we <a contenteditable="false" data-type="indexterm" data-primary="headlines, finding similar" id="ch5_term55"/>want to find a headline in our data that most closely matches a headline that we remember, but only roughly. This is quite easy to solve, as we just have to vectorize our new document:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">made_up</code> <code class="o">=</code> <code class="n">tfidf</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="s2">"australia and new zealand discuss optimal apple </code><code class="se">\</code>
<code class="s2">                            size"</code><code class="p">])</code>
</pre>

<p>Now we have to calculate the cosine similarity to each headline in the corpus. We could implement this in a loop, but it’s <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="cosine_similarity function of" id="idm45634196376856"/><a contenteditable="false" data-type="indexterm" data-primary="cosine_similarity function" id="idm45634196375640"/><a contenteditable="false" data-type="indexterm" data-primary="cosine similarity" id="idm45634196374600"/>easier with the <code>cosine_similarity</code> function of scikit-learn:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">sim</code> <code class="o">=</code> <code class="n">cosine_similarity</code><code class="p">(</code><code class="n">made_up</code><code class="p">,</code> <code class="n">dt</code><code class="p">)</code>
</pre>

<p>The result is a “number of headlines in the corpus” × 1 matrix, where each number represents the similarity to a document in the corpus. Using <code>np.argmax</code> gives us the index of the most similar document:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">headlines</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">sim</code><code class="p">)]</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
publish_date           2011-08-17 00:00:00
headline_text    new zealand apple imports
Name: 633392, dtype: object
</pre>

<p>No <em>sizes</em> of apples and no <em>Australia </em>are present in the most similar headline, but it definitely bears some similarity with our invented headline.</p>
</div></section>

<section data-type="sect2" class="blueprint pagebreak-after" data-pdf-bookmark="Blueprint: Finding the Two Most Similar Documents in a Large Corpus (Much More Difficult)"><div class="sect2" id="idm45634196435832">
<h2>Blueprint: Finding the Two Most Similar Documents in a Large Corpus (Much More Difficult)</h2>

<p>When handling a <a contenteditable="false" data-type="indexterm" data-primary="similarities" data-secondary="calculations for document" id="ch5_term52"/>corpus of many documents, you might often be asked questions such as “Are there duplicates?” or “Has this been mentioned before?” They all boil down to finding the most similar (maybe even identical) documents in the corpus. We will explain how to accomplish this and again use the ABC dataset as our example. The number of headlines will turn out to be a challenge.</p>

<p>You might think that finding the most similar documents in the corpus is as easy as calculating <!--a contenteditable="false" data-type="indexterm" data-primary="cosine similarity">&nbsp;</a-->the <code>cosine_similarity</code> between all documents. However, this is not possible as 1,103,663 × 1,103,663 = 1,218,072,017,569. More than one trillion elements do not fit in the <a contenteditable="false" data-type="indexterm" data-primary="RAM, limitations with" id="idm45634196319176"/>RAM of even the most advanced computers. It is perfectly possible to perform the necessary matrix multiplications without having to wait for ages.</p>

<p>Clearly, this problem needs optimization. As text analytics often has to cope with many documents, this is a very typical challenge. Often, the first optimization is to take an intensive look at all the needed numbers. We can easily observe that the document similarity relation is symmetrical and normalized.</p>

<p>In other words, we <a contenteditable="false" data-type="indexterm" data-primary="similarity matrix " id="idm45634196316632"/>just need to calculate the subdiagonal elements of the similarity matrix (<a data-type="xref" href="#btap_0501">Figure 5-1</a>)</p>

<figure><div id="btap_0501" class="figure"><img src="Images/btap_0501.jpg" width="576" height="607"/>
<h6><span class="label">Figure 5-1. </span>Elements that need to be calculated in the similarity matrix. Only the elements below the diagonal need to be calculated, as their numbers are identical to the ones mirrored on the diagonal. The diagonal elements are all 1.</h6>
</div></figure>

<p>This reduces the number of elements to 1,103,663 × 1,103,662 / 2 = 609,035,456,953, which could be calculated in loop iterations, keeping only the most similar documents. However, calculating all these elements separately is not a good option, as the necessary Python loops (where each iteration calculates just a single matrix element) will eat up a lot of CPU performance.</p>

<p>Instead of calculating individual elements of the similarity matrix, we divide the problem into different blocks and calculate 10,000 × 10,000 similarity submatrices<sup><a data-type="noteref" id="idm45634196311384-marker" href="ch05.xhtml#idm45634196311384">8</a></sup>  at once by <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="with similarity matrix calculations" data-secondary-sortas="similarity matrix calculations" id="idm45634196310264"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with TF-IDF weighting" data-secondary-sortas="TF-IDF weighting" id="idm45634196308712"/>taking blocks of 10,000 TF-IDF vectors from the document matrix. Each of these matrices contains 100,000,000 similarities, which will still fit in RAM. Of course, this leads to calculating too many elements, and we have to perform this for 111 × 110 / 2 = 6,105 submatrices (see <a data-type="xref" href="#btap_0502">Figure 5-2</a>).</p>



<p>From the previous section, we know that iteration takes roughly 500 ms to calculate. Another advantage of this approach is that leveraging data locality gives us a bigger chance of having the necessary matrix elements already in the CPU cache. We estimate that everything should run in about 3,000 seconds, which is roughly one hour.</p>

<figure><div id="btap_0502" class="figure"><img src="Images/btap_0502.jpg" width="605" height="660"/>
	<h6><span class="label">Figure 5-2. </span>Dividing the matrix into submatrices, which we can calculate more easily; the problem is divided into blocks (here, 4 × 4), and the white and diagonal elements within the blocks are redundantly calculated.</h6>
	</div></figure>

<p>Can we improve this? Yes, indeed another speedup of a factor of 10 is actually possible. This works by <a contenteditable="false" data-type="indexterm" data-primary="TfidfVectorizer" id="idm45634196283080"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="TfidfVectorizer of" id="idm45634196281976"/>normalizing the TF-IDF vectors via the corresponding option of <code>TfidfVectorizer</code>. Afterward, the similarity can be calculated with <code>np.dot</code>:<sup><a data-type="noteref" id="idm45634196279640-marker" href="ch05.xhtml#idm45634196279640">9</a></sup></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="o">%%</code><code class="n">time</code>
<code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">dt</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">10000</code><code class="p">],</code> <code class="n">np</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">dt</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">10000</code><code class="p">]))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
CPU times: user 16.4 ms, sys: 0 ns, total: 16.4 ms
Wall time: 16 ms
&lt;10000x10000 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
with 1818931 stored elements in Compressed Sparse Row format&gt;
</pre>

<p>In each iteration we save the most similar documents and their similarity and adjust them during the iterations. To skip identical documents (or more precisely, documents with identical document vectors), we only consider similarities &lt; 0.9999. As it turns out, using <code>&lt;</code> relations with a sparse matrix is extremely inefficient, as all non-existent elements are supposed to be 0. Therefore, we must be creative and find another way:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="o">%%</code><code class="n">time</code>
<code class="n">batch</code> <code class="o">=</code> <code class="mi">10000</code>
<code class="n">max_sim</code> <code class="o">=</code> <code class="mf">0.0</code>
<code class="n">max_a</code> <code class="o">=</code> <code class="bp">None</code>
<code class="n">max_b</code> <code class="o">=</code> <code class="bp">None</code>
<code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">dt</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">batch</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">for</code> <code class="n">b</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">a</code><code class="o">+</code><code class="n">batch</code><code class="p">,</code> <code class="n">batch</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">print</code><code class="p">(</code><code class="n">a</code><code class="p">,</code> <code class="n">b</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">r</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">dt</code><code class="p">[</code><code class="n">a</code><code class="p">:</code><code class="n">a</code><code class="o">+</code><code class="n">batch</code><code class="p">],</code> <code class="n">np</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">dt</code><code class="p">[</code><code class="n">b</code><code class="p">:</code><code class="n">b</code><code class="o">+</code><code class="n">batch</code><code class="p">]))</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="c1"># eliminate identical vectors</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="c1"># by setting their similarity to np.nan which gets sorted out</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="n">r</code> <code class="o">&gt;</code> <code class="mf">0.9999</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">nan</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">sim</code> <code class="o">=</code> <code class="n">r</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">if</code> <code class="n">sim</code> <code class="o">&gt;</code> <code class="n">max_sim</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="c1"># argmax returns a single value which we have to</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="c1"># map to the two dimensions</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="p">(</code><code class="n">max_a</code><code class="p">,</code> <code class="n">max_b</code><code class="p">)</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">unravel_index</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">r</code><code class="p">),</code> <code class="n">r</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="c1"># adjust offsets in corpus (this is a submatrix)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">max_a</code> <code class="o">+=</code> <code class="n">a</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">max_b</code> <code class="o">+=</code> <code class="n">b</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">max_sim</code> <code class="o">=</code> <code class="n">sim</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
CPU times: user 6min 12s, sys: 2.11 s, total: 6min 14s
Wall time: 6min 12s
</pre>

<p>That did not take too long, fortunately! <code>max_a</code> and <code>max_b</code> contain the indices of the headlines with maximum similarity (avoiding identical headlines). Let’s take a look at the results:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="n">headlines</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">max_a</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="n">headlines</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">max_b</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
publish_date                                2014-09-18 00:00:00
headline_text    vline fails to meet punctuality targets report
Name: 904965, dtype: object
publish_date                         2008-02-15 00:00:00
headline_text    vline fails to meet punctuality targets
Name: 364042, dtype: object
</pre>

<p>Using the <a contenteditable="false" data-type="indexterm" data-primary="block calculation approach" id="idm45634196007992"/>block calculation approach, we have calculated <a contenteditable="false" data-type="indexterm" data-primary="execution time" id="idm45634196006792"/><a contenteditable="false" data-type="indexterm" data-primary="time, execution" id="idm45634196005688"/>almost a trillion similarities in just a few minutes. The results are interpretable as we have found similar, but not identical, documents. The different date shows that these are definitely also <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term52" id="idm45634196004248"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term55" id="idm45634196002872"/>separate headlines.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Finding Related Words"><div class="sect2" id="idm45634196347176">
<h2>Blueprint: Finding Related Words</h2>

<p>Until now, we have analyzed documents with respect to their similarity. But the corpus implicitly has much more information, specifically information about <a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="related" id="idm45634195999592"/>related words. In our sense, words are related if they are used in the same documents. Words should be “more” related if they frequently appear together in the documents. As an example, consider the word <em>zealand</em>, which almost always occurs together with <em>new</em>; therefore, these two words are <em>related.</em></p>

<p>Instead of working with a <a contenteditable="false" data-type="indexterm" data-primary="document-term matrix" id="idm45634195996328"/>document-term matrix, we would like to work with a <a contenteditable="false" data-type="indexterm" data-primary="term-document matrix" id="idm45634195995032"/>term-document matrix, which is just its transposed form. Instead of taking row vectors, we now take column vectors. However, we need to re-vectorize the data. Assume two words are infrequently used and that both happen to be present only once in the same headline. Their vectors would then be identical, but this is not what we are looking for. As an example, let’s think of a person named <i>Zaphod Beeblebrox</i>, who is mentioned in two articles. Our algorithm would assign a 100% related score to these words. Although this is correct, it is not very significant. We therefore only consider words that appear at least 1,000 times to get a decent statistical significance:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">tfidf_word</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
<code class="n">dt_word</code> <code class="o">=</code> <code class="n">tfidf_word</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">headlines</code><code class="p">[</code><code class="s2">"headline_text"</code><code class="p">])</code>
</pre>

<p>The vocabulary is quite small, and <a contenteditable="false" data-type="indexterm" data-primary="cosine similarity" id="idm45634195963800"/>we can directly calculate the cosine similarity. Changing row for column vectors, we just transpose the matrix, using the <a contenteditable="false" data-type="indexterm" data-primary="NumPy library" id="idm45634195962552"/>convenient <code>.T</code> method of NumPy:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">r</code> <code class="o">=</code> <code class="n">cosine_similarity</code><code class="p">(</code><code class="n">dt_word</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">dt_word</code><code class="o">.</code><code class="n">T</code><code class="p">)</code>
<code class="n">np</code><code class="o">.</code><code class="n">fill_diagonal</code><code class="p">(</code><code class="n">r</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
</pre>

<p>Finding the largest entries is easiest if we convert it to a one-dimensional array, get the index of the sorted elements via <code>np.argsort</code>, and restore the original indices for the vocabulary lookup:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">voc</code> <code class="o">=</code> <code class="n">tfidf_word</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">()</code>
<code class="n">size</code> <code class="o">=</code> <code class="n">r</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="c1"># quadratic</code>
<code class="k">for</code> <code class="n">index</code> <code class="ow">in</code> <code class="n">np</code><code class="o">.</code><code class="n">argsort</code><code class="p">(</code><code class="n">r</code><code class="o">.</code><code class="n">flatten</code><code class="p">())[::</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">:</code><code class="mi">40</code><code class="p">]:</code>
<code class="err">   </code> <code class="n">a</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="n">index</code><code class="o">/</code><code class="n">size</code><code class="p">)</code>
<code class="err">   </code> <code class="n">b</code> <code class="o">=</code> <code class="n">index</code><code class="o">%</code><code class="n">size</code>
<code class="err">   </code> <code class="k">if</code> <code class="n">a</code> <code class="o">&gt;</code> <code class="n">b</code><code class="p">:</code><code class="err"> </code> <code class="c1"># avoid repetitions</code>
<code class="err">       </code> <code class="k">print</code><code class="p">(</code><code class="s1">'"</code><code class="si">%s</code><code class="s1">" related to "</code><code class="si">%s</code><code class="s1">"'</code> <code class="o">%</code> <code class="p">(</code><code class="n">voc</code><code class="p">[</code><code class="n">a</code><code class="p">],</code> <code class="n">voc</code><code class="p">[</code><code class="n">b</code><code class="p">]))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
"sri" related to "lanka"
"hour" related to "country"
"seekers" related to "asylum"
"springs" related to "alice"
"pleads" related to "guilty"
"hill" related to "broken"
"trump" related to "donald"
"violence" related to "domestic"
"climate" related to "change"
"driving" related to "drink"
"care" related to "aged"
"gold" related to "coast"
"royal" related to "commission"
"mental" related to "health"
"wind" related to "farm"
"flu" related to "bird"
"murray" related to "darling"
"world" related to "cup"
"hour" related to "2014"
"north" related to "korea"
</pre>

<p>It’s quite easy to interpret these results. For some word combinations like <em>climate change</em>, we have restored frequent bigrams. On the other hand, we can also see related words that don’t appear next to each other in the headlines, such as <em>drink</em> and <em>driving</em>. By using the transposed document-term matrix, we have performed a <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term45" id="idm45634195699512"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term46" id="idm45634195698136"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term50" id="idm45634195696760"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term51" id="idm45634195695384"/><a contenteditable="false" data-type="indexterm" data-primary="co-occurrence analysis" id="idm45634195694008"/>kind of <em>co-occurrence analysis</em>.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634195692328">
<h5>Improving Similarity Measures</h5>

<p>Does using n-grams, certain word types, or combinations to find most similar documents change the results?</p>

<p>Documents that have a high similarity and are also published at roughly the same time probably describe the same event. Find a way to remove these from the similarities or—the other way around—focus on these events to detect duplication of news.</p>
</div></aside>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Tips for Long-Running Programs like Syntactic Similarity"><div class="sect2" id="idm45634196001112">
<h2>Tips for Long-Running Programs like Syntactic Similarity</h2>

The following are <a contenteditable="false" data-type="indexterm" data-primary="RAM, limitations with" id="idm45634195689080"/><a contenteditable="false" data-type="indexterm" data-primary="execution time" id="idm45634195687976"/><a contenteditable="false" data-type="indexterm" data-primary="time, execution" id="idm45634195686872"/>some efficiency tips for long-running programs:
<dl>
<dt>Perform benchmarking before waiting too long</dt>
<dd><p>Before <a contenteditable="false" data-type="indexterm" data-primary="benchmarking" id="idm45634195684328"/>performing calculations on the whole dataset, it is often useful to run a single calculation and extrapolate how long the whole algorithm will run and how much memory it will need. You should definitely try to understand how runtime and memory grow with increased complexity (linear, polynomial, exponential). Otherwise, you might have to wait for a long time and find out that after a few hours (or even days) only 10% progress memory is exhausted.</p></dd>

<dt>Try to divide your problem into smaller parts</dt>
<dd><p>Dividing a problem into smaller blocks can help here tremendously. As we have seen in the most similar document of the news corpus, this took only 20 minutes or so to run and used no significant memory. Compared to a naive approach, we would have found out after considerable runtime that the RAM would not have been enough. Furthermore, by dividing the problem into parts, you can make use of multicore architectures or even distribute the problem on many <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term48" id="idm45634195681096"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term49" id="idm45634195679720"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch5_term56" id="idm45634195678344"/>computers.</p></dd>
</dl>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Summary and Conclusion"><div class="sect1" id="idm45634196496728">
<h1>Summary and Conclusion</h1>

<p>In this section, we <a contenteditable="false" data-type="indexterm" data-primary="feature engineering and vectorization" data-secondary="about" id="idm45634195675048"/>have prepared blueprints for vectorization and syntactic similarity. Almost all machine learning projects with text (such as classification, topic modeling, and sentiment detection) need document vectors at their very base.</p>

<p>It turns out that feature engineering is one of the most powerful levers for achieving outstanding performance with these sophisticated machine learning algorithms. Therefore, it’s an excellent idea to try different vectorizers, play with their parameters, and watch the resulting feature space. There are really many possibilities, and for good reason: although optimizing this takes some time, it is usually well-invested as the results of the subsequent steps in the analysis pipeline will benefit tremendously.</p>

<p>The similarity measure used in this chapter is just an example for document similarities. For more complicated requirements, there are more sophisticated similarity algorithms that you will learn about in the following chapters.</p>

<p>Finding similar documents is a well-known problem in information retrieval. There are more sophisticated scores, such as <a href="https://oreil.ly/s47TC">BM25</a>. If you want a scalable solution, the very popular <a href="http://lucene.apache.org">Apache Lucene</a> library (which is the basis of search engines like <a href="https://oreil.ly/R5y0E">Apache Solr</a> and <a href="https://oreil.ly/2qfAL">Elasticsearch</a>) makes use of this and is used for really big document collections in production scenarios.</p>

<p>In the following chapters, we will revisit similarity quite often. We will take a look at integrating word semantics and document semantics, and we will use transfer learning to leverage predefined language models that have been trained with extremely large documents corpora to achieve state-of-the-art performance.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45634198925240"><sup><a href="ch05.xhtml#idm45634198925240-marker">1</a></sup> In later chapters, we will take a look at other possibilities of vectorizing words (<a data-type="xref" href="ch10.xhtml#ch-embeddings">Chapter 10</a>) and documents (<a data-type="xref" href="ch11.xhtml#ch-sentiment">Chapter 11</a>).</p><p data-type="footnote" id="idm45634198893064"><sup><a href="ch05.xhtml#idm45634198893064-marker">2</a></sup> There are much more sophisticated algorithms for determining the vocabulary, like <a href="https://oreil.ly/A6TEl">SentencePiece</a> and <a href="https://oreil.ly/tVDgu">BPE</a>, which are worth taking a look at if you want to reduce the number of features.</p><p data-type="footnote" id="idm45634198361912"><sup><a href="ch05.xhtml#idm45634198361912-marker">3</a></sup> Confusingly, <code>numpy.dot</code> is used both for the dot product (inner product) and for matrix multiplication. If Numpy detects two row or column vectors (i.e., one-dimensional arrays) with the same dimension, it calculates the dot product and yields a scalar. If not and the passed two-dimensional arrays are suitable for matrix multiplication, it performs this operation and yields a matrix. All other cases produce errors. That’s convenient, but it’s a lot of heuristics.</p><p data-type="footnote" id="idm45634198247384"><sup><a href="ch05.xhtml#idm45634198247384-marker">4</a></sup> See <a data-type="xref" href="ch08.xhtml#ch-topicmodels">Chapter 8</a> for more on LDA.</p><p data-type="footnote" id="idm45634197672360"><sup><a href="ch05.xhtml#idm45634197672360-marker">5</a></sup> See, for example, the <a href="https://oreil.ly/3qTpX">definition of entropy</a> as a measure of uncertainty and information. Basically, this says that a low-probability value carries more information than a more likely value.</p><p data-type="footnote" id="idm45634197156184"><sup><a href="ch05.xhtml#idm45634197156184-marker">6</a></sup> This is, of course, related to the stop word list that has already been used. In news articles, the most common words are stop words. In domain-specific texts, it might be completely different. Using stop words is often the safer choice, as these lists have been curated.</p><p data-type="footnote" id="idm45634196754328"><sup><a href="ch05.xhtml#idm45634196754328-marker">7</a></sup> It would be growing exponentially if all word combinations were possible and would be used. As this is unlikely, the dimensions are growing subexponentially.</p><p data-type="footnote" id="idm45634196311384"><sup><a href="ch05.xhtml#idm45634196311384-marker">8</a></sup> We have chosen 10,000 dimensions, as the resulting matrix can be kept in RAM (using roughly 1 GB should be possible even on moderate hardware).</p><p data-type="footnote" id="idm45634196279640"><sup><a href="ch05.xhtml#idm45634196279640-marker">9</a></sup> All calculations can be sped up considerably by using processor-specific libraries, e.g., by subscribing to the Intel channel in Anaconda. This will use AVX2, AVX-512, and similar instructions and use parallelization. <a href="https://oreil.ly/pa1zj">MKL</a> and <a href="https://oreil.ly/jZYSG">OpenBlas</a> are good candidates for linear algebra <span class="keep-together">libraries</span>.</p></div></div></section></div>



  </body></html>