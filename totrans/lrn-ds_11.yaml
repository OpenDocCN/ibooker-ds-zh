- en: Chapter 8\. Wrangling Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before you can work with data in Python, it helps to understand the files that
    store the source of the data. You want answers to a couple of basic questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How much data do you have?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the source file formatted?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to these questions can be very helpful. For example, if your file is
    too large or is not formatted the way you expect, you might not be able to properly
    load it into a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Although many types of structures can represent data, in this book we primarily
    work with data tables, such as Pandas DataFrames and SQL relations. (But do note
    that [Chapter 13](ch13.html#ch-text) examines less-structured text data, and [Chapter 14](ch14.html#ch-web)
    introduces hierarchical formats and binary files.) We focus on data tables for
    several reasons. Research on how to store and manipulate data tables has resulted
    in stable and efficient tools for working with tables. Plus, data in a tabular
    format are close cousins of matrices, the mathematical objects of the immensely
    rich field of linear algebra. And of course, data tables are quite common.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we introduce typical file formats and encodings for plain
    text, describe measures of file size, and use Python tools to examine source files.
    Later in the chapter, we introduce an alternative approach for working with files:
    the shell interpreter. Shell commands give us a programmatic way to get information
    about a file outside the Python environment, and the shell can be very useful
    with big data. Finally, we check the data table’s shape (the number of rows and
    columns) and granularity (what a row represents). These simple checks are the
    starting point for cleaning and analyzing our data.'
  prefs: []
  type: TYPE_NORMAL
- en: We first provide brief descriptions of the datasets that we use as examples
    throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Data Source Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have selected two examples to demonstrate file wrangling concepts: a government
    survey about drug abuse, and administrative data from the San Francisco Department
    of Public Health about restaurant inspections. Before we start wrangling, we give
    an overview of the data scope for these examples (see [Chapter 2](ch02.html#ch-data-scope)).'
  prefs: []
  type: TYPE_NORMAL
- en: Drug Abuse Warning Network (DAWN) Survey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DAWN is a national health-care survey that monitors trends in drug abuse. The
    survey aims to estimate the impact of drug abuse on the country’s health-care
    system and improve how emergency departments monitor substance abuse crises. DAWN
    was administered annually from 1998 through 2011 by the [Substance Abuse and Mental
    Health Services Administration (SAMHSA)](https://www.samhsa.gov). In 2018, due
    in part to the opioid epidemic, the DAWN survey was restarted. In this example,
    we look at the 2011 data, which have been made available through the [SAMHSA Data
    Archive](https://oreil.ly/Y2SKG).
  prefs: []
  type: TYPE_NORMAL
- en: The target population consists of all drug-related emergency room visits in
    the US. These visits are accessed through a frame of emergency rooms in hospitals
    (and their records). Hospitals are selected for the survey through probability
    sampling (see [Chapter 3](ch03.html#ch-theory-datadesign)), and all drug-related
    visits to the sampled hospital’s emergency room are included in the survey. All
    types of drug-related visits are included, such as drug misuse, abuse, accidental
    ingestion, suicide attempts, malicious poisonings, and adverse reactions. For
    each visit, the record may contain up to 16 different drugs, including illegal
    drugs, prescription drugs, and over-the-counter medications.
  prefs: []
  type: TYPE_NORMAL
- en: The source file for this dataset is an example of fixed-width formatting that
    requires external documentation, like a codebook, to decipher. Also, it is a reasonably
    large file and so motivates the topic of how to find a file’s size. And the granularity
    is unusual because an ER visit, not a person, is the subject of investigation.
  prefs: []
  type: TYPE_NORMAL
- en: The San Francisco restaurant files have other characteristics that make them
    a good example for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: San Francisco Restaurant Food Safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [San Francisco Department of Public Health](https://oreil.ly/kG1PN) routinely
    makes unannounced visits to restaurants and inspects them for food safety. The
    inspector calculates a score based on the violations found and provides descriptions
    of the violations. The target population here is all restaurants in San Francisco.
    These restaurants are accessed through a frame of restaurant inspections that
    were conducted between 2013 and 2016\. Some restaurants have multiple inspections
    in a year, and not all of the 7,000+ restaurants are inspected annually.
  prefs: []
  type: TYPE_NORMAL
- en: Food safety scores are available through the city’s [Open Data initiative](https://oreil.ly/kwh-F),
    called [DataSF](https://datasf.org). DataSF is one example a city government making
    their data publicly available; the DataSF mission is to “empower the use of data
    in decision making and service delivery” with the goal of improving the quality
    of life and work for residents, employers, employees, and visitors.
  prefs: []
  type: TYPE_NORMAL
- en: San Francisco requires restaurants to publicly display their scores (see [Figure 8-1](#scorecard)
    for an example placard).^([1](ch08.html#id1015)) These data offer an example of
    multiple files with different structures, fields, and granularity. One dataset
    contains summary results of inspections, another provides details about the violations
    found, and a third contains general information about the restaurants. The violations
    include both serious problems related to the transmission of foodborne illnesses
    and minor issues such as not properly displaying the inspection placard.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. A food safety scorecard displayed in a restaurant; scores range
    between 0 and 100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Both the DAWN survey data and the San Francisco restaurant inspection data are
    available online as plain-text files. However, their formats are quite different,
    and in the next section, we demonstrate how to figure out a file format so that
    we can read the data into a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: File Formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A *file format* describes how data are stored on a computer’s disk or other
    storage device. Understanding the file format helps us figure out how to read
    the data into Python in order to work with it as a data table. In this section,
    we introduce several popular formats used to store data tables. These are all
    plain-text formats, meaning they are easy for us to read with a text editor like
    VS Code, Sublime, Vim, or Emacs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The file format and the *structure* of the data are two different things. We
    consider the data structure to be a mental representation of the data that tells
    us what kinds of operations we can do. For example, a table structure corresponds
    to data values arranged in rows and columns. But the same table can be stored
    in many different types of file formats.
  prefs: []
  type: TYPE_NORMAL
- en: The first format we describe is the delimited file format.
  prefs: []
  type: TYPE_NORMAL
- en: Delimited Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Delimited formats use a specific character to separate data values. Usually,
    these separators are either a comma (comma-separated values, or CSV for short),
    a tab (tab-separated values, or TSV), whitespace, or a colon. These formats are
    natural for storing data that have a table structure. Each line in the file represents
    a record, which is delimited by newline (`\n` or `\r\n`) characters. And within
    a line, the record’s information is delimited by the comma character (`,`) for
    CSV or the tab character (`\t`) for TSV, and so on. The first line of these files
    often contains the names of the table’s columns/features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The San Francisco restaurant scores are stored in CSV-formatted files. Let’s
    display the first few lines of the *inspections.csv* file. In Python, the built-in
    `pathlib` library has a useful `Path` object to specify paths to files and folders
    that work across platforms. This file is within the *data* folder, so we use `Path()`
    to create the full pathname:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Paths are tricky when working across different operating systems (OSs). For
    instance, a typical path in Windows might look like *C:\files\data.csv*, while
    a path in Unix or macOS might look like *~/files/data.csv*. Because of this, code
    that works on one OS can fail to run on other OSs.
  prefs: []
  type: TYPE_NORMAL
- en: The `pathlib` Python library was created to avoid OS-specific path issues. By
    using it, the code shown here is more *portable*—it works across Windows, macOS,
    and Unix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Path` object in the following code has many useful methods, such as `read_text()`,
    which reads in the entire contents of the file as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the field names appear in the first line of the file; these names
    are comma separated and in quotes. We see four fields: the business identifier,
    the restaurant’s score, the date of the inspection, and the type of inspection.
    Each line in the file corresponds to one inspection, and the ID, score, date,
    and type values are separated by commas. In addition to identifying the file format,
    we also want to identify the format of the features. We see two things of note:
    the scores and dates both appear as strings. We will want to convert the scores
    to numbers so that we can calculate summary statistics and create visualizations.
    And we will convert the date into a date-time format so that we can make time-series
    plots. We show how to carry out these transformations in [Chapter 9](ch09.html#ch-wrangling).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Displaying the first few lines of a file is something we’ll do often, so we
    create a function as a shortcut:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: People often confuse CSV and TSV files with spreadsheets. This is in part because
    most spreadsheet software (like Microsoft Excel) will automatically display a
    CSV file as a table in a workbook. Behind the scenes, Excel looks at the file
    format and encoding just like we’ve done in this section. However, Excel files
    have a different format than CSV and TSV files, and we need to use different `pandas`
    functions to read these formats into Python.
  prefs: []
  type: TYPE_NORMAL
- en: All three of the restaurant source files are CSV formatted. In contrast, the
    DAWN source file has a fixed-width format. We describe this kind of formatting
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed-Width Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fixed-width format (FWF) does not use delimiters to separate data values.
    Instead, the values for a specific field appear in the exact same position in
    each line. The DAWN source file has this format. Each line in the file is very
    long. For display purposes, we only show the first few characters from the first
    five lines in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the values appear to align from one row to the next. For example,
    there is a decimal point in the same position (the 19th character) in each line.
    Notice also that some of the values seem to be squished together, and we need
    to know the exact position of each piece of information in a line in order to
    make sense of it. SAMHSA provides a 2,000-page [codebook](https://oreil.ly/a4OFo)
    with all of this information, including some basic checks, so that we can confirm
    that we have correctly read the file. For instance, the codebook tells us that
    the age field appears in positions 34–35 and is coded in intervals from 1 to 11\.
    The first two records shown in the preceding code have age categories of 4 and
    11; the codebook tells us that a 4 stands for the age bracket “6 to 11” and 11
    is for “65+.”
  prefs: []
  type: TYPE_NORMAL
- en: Other plain-text formats that are popular include hierarchical formats and loosely
    formatted text (in contrast to formats that directly support table structures).
    These are covered in greater detail in other chapters, but for completeness, we
    briefly describe them here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A widely adopted convention is to use the filename extension, such as *.csv*,
    *.tsv*, and *.txt*, to indicate the format of the contents of the file. Filenames
    that end with *.csv* are expected to contain comma-separated values, and those
    ending with *.tsv* are expected to contain tab-separated values; *.txt* generally
    denotes plain text without a designated format. However, these extension names
    are only suggestions. Even if a file has a *.csv* extension, the actual contents
    might not be formatted properly! It’s a good practice to inspect the contents
    of the file before loading it into a dataframe. If the file is not too large,
    you can open and examine it with a plain-text editor. Otherwise, you can view
    a couple of lines using `.readline()` or shell command.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical formats store data in a nested form. For instance, JavaScript Object
    Notation (JSON), which is commonly used for communication by web servers, includes
    key-value pairs and arrays that can be nested, similar to a Python dictionary.
    XML and HTML are other common formats for storing documents on the internet. Like
    JSON, these files have a hierarchical, key-value format. We cover both formats
    (JSON and XML) in more detail in [Chapter 14](ch14.html#ch-web).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we briefly describe other plain-text files that don’t fall into any of
    the previous categories but still have some structure to them that enables us
    to read and extract information.
  prefs: []
  type: TYPE_NORMAL
- en: Loosely Formatted Text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Web logs, instrument readings, and program logs typically provide data in plain
    text. For example, here is one line of a web log (we’ve split it across multiple
    lines for readability). It contains information such as the date, time, and type
    of request made to a website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are organizational patterns present, but not in a simple delimited format.
    This is what we mean by “loosely formatted.” We see that the date and time appear
    between square brackets, and the type of request (`GET` in this case) follows
    the date-time information and appears in quotes. In [Chapter 13](ch13.html#ch-text),
    we use these observations about the web log’s format and string manipulation tools
    to extract values of interest into a data table.
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, here is a single record taken from a wireless device log.
    The device reports the timestamp, the identifier, its location, and the signal
    strengths that it picks up from other devices. This information uses a combination
    of formats: key-value pairs, semicolon-delimited values, and comma-delimited values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Like with the web logs, we can use string manipulation and the patterns in the
    records to extract features into a table.
  prefs: []
  type: TYPE_NORMAL
- en: We have primarily introduced formats for plain-text data that are widely used
    for storing and exchanging tables. The CSV format is the most common, but others,
    such as tab-separated and fixed-width formats, are also prevalent. And there are
    many types of file formats that store data!
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have used the term *plain text* to broadly cover formats that can
    be viewed with a text editor. However, a plain-text file may have different encodings,
    and if we don’t specify the encoding correctly, the values in the dataframe might
    contain gibberish. We give an overview of file encoding next.
  prefs: []
  type: TYPE_NORMAL
- en: File Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computers store data as sequences of *bits*: 0s and 1s. *Character encodings*,
    like ASCII, tell the computer how to translate between bits and text. For example,
    in ASCII, the bits `100 001` stand for the letter A and `100 010` for B. The most
    basic kind of plain text supports only standard ASCII characters, which includes
    the uppercase and lowercase English letters, numbers, punctuation symbols, and
    spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: ASCII encoding does not include a lot of special characters or characters from
    other languages. Other, more modern character encodings have many more characters
    that can be represented. Common encodings for documents and web pages are Latin-1
    (ISO-8859-1) and UTF-8\. UTF-8 has over a million characters and is backward compatible
    with ASCII, meaning that it uses the same representation for English letters,
    numbers, and punctuation as ASCII.
  prefs: []
  type: TYPE_NORMAL
- en: When we have a text file, we usually need to figure out its encoding. If we
    choose the wrong encoding to read in a file, Python either reads incorrect values
    or throws an error. The best way to find the encoding is by checking the data’s
    documentation, which often explicitly says what the encoding is.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we don’t know the encoding, we have to make a guess. The `chardet` package
    has a function called `detect()` that infers a file’s encoding. Since these guesses
    are imperfect, the function also returns a confidence level between 0 and 1\.
    We use this function to look at the files from our examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The detection function is quite certain that all but one of the files are ASCII
    encoded. The exception is *businesses.csv*, which appears to have an ISO-8859-1
    encoding. We run into trouble if we ignore this encoding and try to read the businesses
    file into `pandas` without specifying the special encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To successfully read the data, we must specify the ISO-8859-1 encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|   | business_id | name | address | postal_code |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | 94109
    |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND
    FLOOR | 94104 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 31 | NORMAN’S ICE CREAM AND FREEZES | 2801 LEAVENWORTH ST | 94133
    |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 45 | CHARLIE’S DELI CAFE | 3202 FOLSOM ST | 94110 |'
  prefs: []
  type: TYPE_TB
- en: File encoding can be a bit mysterious to figure out, and unless there is metadata
    that explicitly gives us the encoding, guesswork comes into play. When an encoding
    is not 100% confirmed, it’s a good idea to seek additional documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Another potentially important aspect of a source file is its size. If a file
    is huge, then we might not be able to read it into a dataframe. In the next section,
    we discuss how to figure out a source file’s size.
  prefs: []
  type: TYPE_NORMAL
- en: File Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computers have finite resources. You have likely encountered these limits firsthand
    if your computer has slowed down from having too many applications open at once.
    We want to make sure that we do not exceed the computer’s limits while working
    with data, and we might choose to examine a file differently depending on its
    size. If we know that our dataset is relatively small, then a text editor or a
    spreadsheet can be convenient for looking at the data. On the other hand, for
    large datasets, a more programmatic exploration or even distributed computing
    tools may be needed.
  prefs: []
  type: TYPE_NORMAL
- en: In many situations, we analyze datasets downloaded from the internet. These
    files reside on the computer’s *disk storage*. In order to use Python to explore
    and manipulate the data, we need to read the data into the computer’s *memory*,
    also known as random access memory (RAM). All Python code requires the use of
    RAM, no matter how short the code is. A computer’s RAM is typically much smaller
    than its disk storage. For example, one computer model released in 2018 had 32
    times more disk storage than RAM. Unfortunately, this means that datafiles can
    often be much bigger than what is feasible to read into memory.
  prefs: []
  type: TYPE_NORMAL
- en: Both disk storage and RAM capacity are measured in terms of *bytes* (eight 0s
    and 1s). Roughly speaking, each character in a text file adds one byte to a file’s
    size. To succinctly describe the sizes of larger files, we use the prefixes described
    in [Table 8-1](#byte-prefixes); for example, a file that contains 52,428,800 characters
    will take up <math><mn>5</mn> <mo>,</mo> <mn>242</mn> <mo>,</mo> <mn>8800</mn>
    <mrow><mo>/</mo></mrow> <mn>1</mn> <mo>,</mo> <msup><mn>024</mn> <mn>2</mn></msup>
    <mo>=</mo> <mn>50</mn>  <mrow><mtext>mebibytes</mtext></mrow></math> , or 50 MiB
    on disk.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Prefixes for common file sizes
  prefs: []
  type: TYPE_NORMAL
- en: '| Multiple | Notation | Number of bytes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Kibibyte | KiB | 1,024 |'
  prefs: []
  type: TYPE_TB
- en: '| Mebibyte | MiB | 1,024² |'
  prefs: []
  type: TYPE_TB
- en: '| Gibibyte | GiB | 1,024³ |'
  prefs: []
  type: TYPE_TB
- en: '| Tebibyte | TiB | 1,024⁴ |'
  prefs: []
  type: TYPE_TB
- en: '| Pebibyte | PiB | 1,024⁵ |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why use multiples of 1,024 instead of simple multiples of 1,000 for these prefixes?
    This is a historical result of the fact that most computers use a binary number
    scheme where powers of 2 are simpler to represent ( <math><mn>1</mn> <mo>,</mo>
    <mn>024</mn> <mo>=</mo> <msup><mn>2</mn> <mrow><mn>10</mn></mrow></msup></math>
    ). You also see the typical SI prefixes used to describe size—kilobytes, megabytes,
    and gigabytes, for example. Unfortunately, these prefixes are used inconsistently.
    Sometimes a kilobyte refers to 1,000 bytes; other times, a kilobyte refers to
    1,024 bytes. To avoid confusion, we stick to kibi-, mebi-, and gibibytes, which
    clearly represent multiples of 1,024.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not uncommon to have a datafile happily stored on a computer that will
    overflow the computer’s memory if we attempt to manipulate it with a program.
    So we often begin our data work by making sure the files are of manageable size.
    To do this, we use the built-in `os` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We see that the *businesses.csv* file takes up 645 KiB on disk, making it well
    within the memory capacities of most systems. Although the *violations.csv* file
    takes up 3.6 MiB of disk storage, most machines can easily read it into a `pandas`
    `DataFrame` too. But *DAWN-Data.txt*, which contains the DAWN survey data, is
    much larger.
  prefs: []
  type: TYPE_NORMAL
- en: The DAWN file takes up roughly 270 MiB of disk storage, and while some computers
    can work with this file in memory, it can slow down other systems. To make this
    data more manageable in Python, we can, for example, load in a subset of the columns
    rather than all of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes we are interested in the total size of a folder instead of the size
    of individual files. For example, we have three restaurant files, and we might
    like to see whether we can combine all the data into a single dataframe. In the
    following code, we calculate the size of the *data* folder, including all files
    in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a rule of thumb, reading in a file using `pandas` usually requires at least
    five times the available memory as the file size. For example, reading in a 1
    GiB file typically requires at least 5 GiB of available memory. Memory is shared
    by all programs running on a computer, including the operating system, web browsers,
    and Jupyter notebook itself. A computer with 4 GiB total RAM might have only 1
    GiB available RAM with many applications running. With 1 GiB available RAM, it
    is unlikely that `pandas` will be able to read in a 1 GiB file.
  prefs: []
  type: TYPE_NORMAL
- en: There are several strategies for working with data that are far larger than
    what is feasible to load into memory. We describe a few of them next.
  prefs: []
  type: TYPE_NORMAL
- en: The popular term *big data* generally refers to the scenario where the data
    are large enough that even top-of-the-line computers can’t read the data directly
    into memory. This is a common scenario in scientific domains like astronomy, where
    telescopes capture images of space that can be petabytes ( <math><msup><mn>2</mn>
    <mrow><mn>50</mn></mrow></msup></math> ) in size. While not quite as big, social
    media giants, health-care providers, and other companies can also struggle with
    large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figuring out how to draw insights from these datasets is an important research
    problem that motivates the fields of database engineering and distributed computing.
    While we won’t cover these fields in this book, we provide a brief overview of
    basic approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Subset the data.
  prefs: []
  type: TYPE_NORMAL
- en: One simple approach is to work with portions of data. Rather than loading in
    the entire source file, we can either select a specific part of it (e.g., one
    day’s worth of data) or randomly sample the dataset. Because of its simplicity,
    we use this approach quite often in this book. The natural downside is that we
    lose many of the benefits of analyzing a large dataset, like being able to study
    rare events.
  prefs: []
  type: TYPE_NORMAL
- en: Use a database system.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 7](ch07.html#ch-sql), relational database management
    systems (RDBMSs) are specifically designed to store large datasets. SQLite is
    a useful system for working with datasets that are too large to fit in memory
    but small enough to fit on disk for a single machine. For datasets that are too
    large to fit on a single machine, more scalable database systems like MySQL and
    PostgreSQL can be used. These systems can manipulate data that are too big to
    fit into memory by using SQL queries. Because of their advantages, RDBMSs are
    commonly used for data storage in research and industry settings. One downside
    is that they often require a separate server for the data that needs its own configuration.
    Another downside is that SQL is less flexible in what it can compute than Python,
    which becomes especially relevant for modeling. A useful hybrid approach is to
    use SQL to subset, aggregate, or sample the data into batches that are small enough
    to read into Python. Then we can use Python for more sophisticated analyses.
  prefs: []
  type: TYPE_NORMAL
- en: Use a distributed computing system.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to handling complex computations on large datasets is to use
    a distributed computing system like MapReduce, Spark, or Ray. These systems work
    best on tasks that can be split into many smaller parts where they divide datasets
    into smaller pieces and run programs on all of the smaller datasets at once. These
    systems have great flexibility and can be used in a variety of scenarios. Their
    main downside is that they can require a lot of work to install and configure
    properly because they are typically installed across many computers that need
    to coordinate with one another.
  prefs: []
  type: TYPE_NORMAL
- en: It can be convenient to use Python to determine a file format, encoding, and
    size. Another powerful tool for working with files is the shell; the shell is
    widely used and has a more succinct syntax than Python. In the next section, we
    introduce a few command-line tools available in the shell for carrying out the
    same tasks of finding out information about a file before reading it into a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: The Shell and Command-Line Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nearly all computers provide access to a *shell interpreter*, such as `sh` or
    `bash` or `zsh`. These interpreters typically perform operations on the files
    on a computer with their own language, syntax, and built-in commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the term *command-line interface (CLI) tools* to refer to the commands
    available in a shell interpreter. Although we only cover a few CLI tools here,
    there are many useful CLI tools that enable all sorts of operations on files.
    For instance, the following command in the `bash` shell produces a list of all
    the files in the *figures/* folder for this chapter along with their file sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The dollar sign is the shell prompt, showing the user where to type. It’s not
    part of the command itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic syntax for a shell command is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: CLI tools often take one or more *arguments*, similar to how Python functions
    take arguments. In the shell, we wrap arguments with spaces, not with parentheses
    or commas. The arguments appear at the end of the command line, and they are usually
    the name of a file or some text. In the `ls` example, the argument to `ls` is
    `figures/`. Additionally, CLI tools support *flags* that provide additional options.
    These flags are specified immediately following the command name using a dash
    as a delimiter. In the `ls` example, we provided the flags `-l` (to provide extra
    information about each file) and `-h` (to provide file sizes in a more human-readable
    format). Many commands have default arguments and options, and the `man` tool
    prints a list of acceptable options, examples, and defaults for any command. For
    example, `man ls` describes the 30 or so flags available for `ls`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All CLI tools we cover in this book are specific to the `sh` shell interpreter,
    the default interpreter for Jupyter installations on macOS and Linux systems at
    the time of this writing. Windows systems have a different interpreter, and the
    commands shown in the book may not run on Windows, although Windows gives access
    to an `sh` interpreter through its Linux Subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: The commands in this section can be run in a terminal application, or through
    a terminal opened by Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with an exploration of the filesystem containing the content for this
    chapter, using the `ls` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To dive deeper and list the files in the *data/* directory, we provide the
    directory name as an argument to `ls`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We added the `-l` flag to the command to get more information about each file.
    The file size appears in the fifth column of the listing, and it’s more readable
    as specified by the `-h` flag. When we have multiple simple option flags like
    `-l`, `-h`, and `-L`, we can combine them together as a shorthand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When working with datasets in this book, our code will often use an additional
    `-L` flag for `ls` and other CLI tools, such as `du`. We do this because we set
    up the datasets in the book using shortcuts (called *symlinks*). Usually, your
    code won’t need the `-L` flag unless you’re working with symlinks too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other CLI tools for checking file size are `wc` and `du`. The command `wc`
    (short for *word count*) provides helpful information about a file’s size in terms
    of the number of lines, words, and characters in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can see from the output that *DAWN-Data.txt* has 229,211 lines and 280,095,842
    characters. (The middle value is the file’s word count, which is useful for files
    that contain sentences and paragraphs but not very useful for files containing
    data, such as FWF-formatted values.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ls` tool does not calculate the cumulative size of the contents of a folder.
    To properly calculate the total size of a folder, including the files in the folder,
    we use `du` (short for *disk usage*). By default, the `du` tool shows the size
    in units called *blocks*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We commonly add the `-s` flag to `du` to show the file sizes for both files
    and folders and the `-h` flag to display quantities in the standard KiB, MiB,
    or GiB format. The asterisk in `data/*` in the following code tells `du` to show
    the size of every item in the_data_ folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the formatting of a file, we can examine the first few lines with
    the `head` command or the last few lines with `tail`. These CLIs are very useful
    for peeking at a file’s contents to determine whether it’s formatted as CSV, TSV,
    and so on. As an example, let’s look at the *inspections.csv* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: By default, `head` displays the first 10 lines of a file. If we want to show,
    say, four lines, then we add the option `-n 4` to our command (or just `-4` for
    short).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can print the entire contents of the file using the `cat` command. However,
    you should take care when using this command, as printing a large file can cause
    a crash. The *legend.csv* file is small, and we can use `cat` to concatenate and
    print its contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In many cases, using `head` or `tail` alone gives us a good enough sense of
    the file structure to proceed with loading it into a dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the `file` command can help us determine a file’s encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We see (again) that all of the files are ASCII, except for *businesses.csv*,
    which has an ISO-8859-1 encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Commonly, we open a terminal program to start a shell interpreter. However,
    Jupyter notebooks provide a convenience: if a line of code in a Python code cell
    is prefixed with the `!` character, the line will go directly to the system’s
    shell interpreter. For example, running `!ls` in a Python cell lists the files
    in the current directory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shell commands give us a programmatic way to work with files, rather than a
    point-and-click “manual” approach. They are useful for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs: []
  type: TYPE_NORMAL
- en: If you need to record what you did.
  prefs: []
  type: TYPE_NORMAL
- en: Error reduction
  prefs: []
  type: TYPE_NORMAL
- en: If you want to reduce typographical errors and other simple but potentially
    harmful mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility
  prefs: []
  type: TYPE_NORMAL
- en: If you need to repeat the same process in the future or you plan to share your
    process with others. This gives you a record of your actions.
  prefs: []
  type: TYPE_NORMAL
- en: Volume
  prefs: []
  type: TYPE_NORMAL
- en: If you have many repetitive operations to perform, the size of the file you
    are working with is large, or you need to perform things quickly. CLI tools can
    help in all these cases.
  prefs: []
  type: TYPE_NORMAL
- en: After the data have been loaded into a dataframe, our next task is to figure
    out the table’s shape and granularity. We start by finding the number of rows
    and columns in the table (its shape). Then we need to understand what a row represents
    before we begin to check the quality of the data. We cover these topics in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Table Shape and Granularity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described earlier, we refer to a dataset’s *structure* as a mental representation
    of the data, and in particular, we represent data that have a *table* structure
    by arranging values in rows and columns. We use the term *granularity* to describe
    what each row in the table represents, and the term *shape* quantifies the table’s
    rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have determined the format of the restaurant-related files, we
    load them into dataframes and examine their shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We find that the table with the restaurant information (the business table)
    has 6,406 rows and 9 columns. Now let’s figure out the granularity of this table.
    To start, we can look at the first two rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | business_id | name | address | city | ... | postal_code | latitude |
    longitude | phone_number |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | San Francisco
    | ... | 94109 | 37.79 | -122.42 | +14157763262 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND
    FLOOR | San Francisco | ... | 94104 | 37.79 | -122.40 | +14156779494 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'These two rows give us the impression that each record represents a particular
    restaurant. But, we can’t tell from just two records whether or not this is the
    case. The field named `business_id` implies that it is the unique identifier for
    the restaurant. We can confirm this by checking whether the number of records
    in the dataframe matches the number of unique values in the field `business_id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The number of unique `business_id`s matches the number of rows in the table,
    so it seems safe to assume that each row represents a restaurant. Since `business_id`
    uniquely identifies each record in the dataframe, we treat `business_id` as the
    *primary key* for the table. We can use primary keys to join tables (see [Chapter 6](ch06.html#ch-pandas)).
    Sometimes a primary key consists of two (or more) features. This is the case for
    the other two restaurant files. Let’s continue the examination of the inspections
    and violations dataframes and find their granularity.
  prefs: []
  type: TYPE_NORMAL
- en: Granularity of Restaurant Inspections and Violations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We just saw that there are many more rows in the inspection table than the
    business table. Let’s take a closer look at the first few inspections:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | business_id | score | date | type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 19 | 94 | 20160513 | routine |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 19 | 94 | 20171211 | routine |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 24 | 98 | 20171101 | routine |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 24 | 98 | 20161005 | routine |'
  prefs: []
  type: TYPE_TB
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The combination of restaurant ID and inspection date uniquely identifies each
    record in this table, with the exception of three restaurants that have two records
    for their ID-date combination. Let’s examine the rows for restaurant `64859`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '|   | business_id | score | date | type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **7742** | 64859 | 96 | 20150924 | routine |'
  prefs: []
  type: TYPE_TB
- en: '| **7744** | 64859 | 91 | 20150924 | routine |'
  prefs: []
  type: TYPE_TB
- en: This restaurant got two different inspection scores on the same day! How could
    this happen? It may be that the restaurant had two inspections in one day, or
    it might be an error. We address these sorts of questions when we consider data
    quality in [Chapter 9](ch09.html#ch-wrangling). Since there are only three of
    these double-inspection days, we can ignore the issue until we clean the data.
    So the primary key would be the combination of restaurant ID and inspection date
    if same-day inspections are removed from the table.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `business_id` field in the inspections table acts as a reference
    to the primary key in the business table. So `business_id` in `insp` is a *foreign
    key* because it links each record in the inspections table to a record in the
    business table. This means that we can readily join these two tables together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we examine the granularity of the third table, the one that contains
    the violations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | business_id | date | description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 19 | 20171211 | Inadequate food safety knowledge or lack of ce...
    |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 19 | 20171211 | Unapproved or unmaintained equipment or utensils
    |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 19 | 20160513 | Unapproved or unmaintained equipment or utensi...
    |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **39039** | 94231 | 20171214 | High risk vermin infestation [ date violation...
    |'
  prefs: []
  type: TYPE_TB
- en: '| **39040** | 94231 | 20171214 | Moderate risk food holding temperature [ dat...
    |'
  prefs: []
  type: TYPE_TB
- en: '| **39041** | 94231 | 20171214 | Wiping cloths not clean or properly stored
    or ... |'
  prefs: []
  type: TYPE_TB
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the first few records in this table, we see that each inspection
    has multiple entries. The granularity appears to be at the level of a violation
    found in an inspection. Reading the descriptions, we see that if corrected, a
    date is listed in the description within square brackets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In brief, we have found that the three food safety tables have different granularities.
    Since we have identified primary and foreign keys for them, we can potentially
    join these tables. If we are interested in studying inspections, we can join the
    violations and inspections together using the business ID and inspection date.
    This would let us connect the number of violations found during an inspection
    to the inspection score.
  prefs: []
  type: TYPE_NORMAL
- en: We can also reduce the inspection table to one per restaurant by selecting the
    most recent inspection for each restaurant. This reduced data table essentially
    has a granularity of restaurant and may be useful for a restaurant-based analysis.
    In [Chapter 9](ch09.html#ch-wrangling), we cover these kinds of actions that reshape
    a data table, transform columns, and create new columns.
  prefs: []
  type: TYPE_NORMAL
- en: We conclude this section with a look at the shape and granularity of the DAWN
    survey data.
  prefs: []
  type: TYPE_NORMAL
- en: DAWN Survey Shape and Granularity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As noted earlier in this chapter, the DAWN file has fixed-width formatting,
    and we need to rely on a codebook to find out where the fields are. As an example,
    a snippet of the codebook in [Figure 8-2](#dawn-age) tells us that age appears
    in positions 34 and 35 in a row, and it is categorized into 11 age groups: 1 stands
    for age 5 and under, 2 for ages 6 to 11, …, and 11 for ages 65 and older. Also,
    –8 represents a missing value.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Screenshot of a portion of the DAWN coding for age
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Earlier, we determined that the file contains 200,000 lines and over 280 million
    characters, so on average, there are about 1,200 characters per line. This might
    be why they used a fixed-width rather than a CSV format. Think how much larger
    the file would be if there was a comma between every field!
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the tremendous amount of information on each line, let’s read just a
    few features into a dataframe. We can use the `pandas.read_fwf` method to do this.
    We specify the exact positions of the fields to extract, and we provide names
    for these fields and other information about the header and index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '|   | wt | age | sex | race | type |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| id |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.94 | 4 | 1 | 2 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 5.99 | 11 | 1 | 3 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 4.72 | 11 | 2 | 2 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 4.08 | 2 | 1 | 3 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | 5.18 | 6 | 1 | 3 | 8 |'
  prefs: []
  type: TYPE_TB
- en: 'We can compare the rows in the table to the number of lines in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The number of rows in the dataframe matches the number of lines in the file.
    That’s good. The granularity of the dataframe is a bit complicated due to the
    survey design. Recall that these data are part of a large scientific study, with
    a complex sampling scheme. A row represents an emergency room visit, so the granularity
    is at the emergency room visit level. However, in order to reflect the sampling
    scheme and be representative of the population of all drug-related ER visits in
    a year, weights are provided. We must apply the weight to each record when we
    compute summary statistics, build histograms, and fit models. (The `wt` field
    contains these values.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights take into account the chance of an ER visit like this one appearing
    in the sample. By “like this one” we mean a visit with similar features, such
    as the visitor age, race, visit location, and time of day. Let’s examine the different
    values in `wt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'It is critical to include the survey weights in your analysis to get data that
    represents the population at large. For example, we can compare the calculation
    of the proportion of females among the ER visits both with and without the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: These figures differ by more than 4 percentage points. The weighted version
    is a more accurate estimate of the proportion of females among the entire population
    of drug-related ER visits.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the granularity can be tricky to figure out, like we saw with the
    inspections data. And at other times, we need to take sampling weights into account,
    like for the DAWN data. These examples show it’s important to take your time and
    review the data descriptions before proceeding with analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data wrangling is an essential part of data analysis. Without it, we risk overlooking
    problems in data that can have major consequences for future analysis. This chapter
    covered an important first step in data wrangling: reading data from a plain-text
    source file into a Python dataframe and identifying its granularity. We introduced
    different types of file formats and encodings, and we wrote code that can read
    data from these formats. We checked the size of source files and considered alternative
    tools for working with large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced command-line tools as an alternative to Python for checking
    the format, encoding, and size of a file. These CLI tools are especially handy
    for filesystem-oriented tasks because of their simple syntax. We’ve only touched
    the surface of what CLI tools can do. In practice, the shell is capable of sophisticated
    data processing and is well worth learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding the shape and granularity of a table gives us insight into what
    a row in a data table represents. This helps us determine whether the granularity
    is mixed, aggregation is needed, or weights are required. After looking at the
    granularity of your dataset, you should have answers to the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What does a record represent?
  prefs: []
  type: TYPE_NORMAL
- en: Clarity on this will help you correctly analyze data and state your findings.
  prefs: []
  type: TYPE_NORMAL
- en: Do all records in a table capture granularity at the same level?
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes a table contains additional summary rows that have a different granularity,
    and you want to use only those rows that are at the right level of detail.
  prefs: []
  type: TYPE_NORMAL
- en: If the data are aggregated, how was the aggregation performed?
  prefs: []
  type: TYPE_NORMAL
- en: Summing and averaging are common types of aggregation. With averaged data, the
    variability in the measurements is typically reduced and relationships often appear
    stronger.
  prefs: []
  type: TYPE_NORMAL
- en: What kinds of aggregations might you perform on the data?
  prefs: []
  type: TYPE_NORMAL
- en: Aggregations might be useful or necessary to combine one data table with another.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing your table’s granularity is a first step to cleaning your data, and
    it informs you of how to analyze the data. For example, we saw the granularity
    of the DAWN survey is an ER visit. That naturally leads us to think about comparisons
    of patient demographics to the US as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: The wrangling techniques in this chapter help us bring data from a source file
    into a dataframe and understand its structure. Once we have a dataframe, further
    wrangling is needed to assess and improve quality and prepare the data for analysis.
    We cover these topics in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.html#id1015-marker)) In 2020, the city began giving restaurants color-coded
    placards indicating whether the restaurant passed (green), conditionally passed
    (yellow), or failed (red) the inspection. These new placards no longer display
    a numeric inspection score. However, a restaurant’s scores and violations are
    still available at DataSF.
  prefs: []
  type: TYPE_NORMAL
