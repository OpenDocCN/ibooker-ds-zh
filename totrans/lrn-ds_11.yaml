- en: Chapter 8\. Wrangling Files
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。整理文件
- en: 'Before you can work with data in Python, it helps to understand the files that
    store the source of the data. You want answers to a couple of basic questions:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Python处理数据之前，了解存储数据源的文件是很有帮助的。您想要了解一些基本问题的答案：
- en: How much data do you have?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您有多少数据？
- en: How is the source file formatted?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源文件的格式是怎样的？
- en: Answers to these questions can be very helpful. For example, if your file is
    too large or is not formatted the way you expect, you might not be able to properly
    load it into a dataframe.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的答案可能非常有帮助。例如，如果您的文件太大或格式不符合您的期望，您可能无法正确加载它到数据框中。
- en: Although many types of structures can represent data, in this book we primarily
    work with data tables, such as Pandas DataFrames and SQL relations. (But do note
    that [Chapter 13](ch13.html#ch-text) examines less-structured text data, and [Chapter 14](ch14.html#ch-web)
    introduces hierarchical formats and binary files.) We focus on data tables for
    several reasons. Research on how to store and manipulate data tables has resulted
    in stable and efficient tools for working with tables. Plus, data in a tabular
    format are close cousins of matrices, the mathematical objects of the immensely
    rich field of linear algebra. And of course, data tables are quite common.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多类型的结构都可以表示数据，在本书中，我们主要使用数据表，如Pandas DataFrames和SQL关系。（但请注意，[第13章](ch13.html#ch-text)研究了结构较少的文本数据，[第14章](ch14.html#ch-web)介绍了分层格式和二进制文件。）我们之所以专注于数据表，有几个原因。研究如何存储和操作数据表已经产生了稳定高效的工具来处理表格。此外，表格格式的数据与矩阵密切相关，矩阵是线性代数领域非常丰富的数学对象。当然，数据表非常常见。
- en: 'In this chapter, we introduce typical file formats and encodings for plain
    text, describe measures of file size, and use Python tools to examine source files.
    Later in the chapter, we introduce an alternative approach for working with files:
    the shell interpreter. Shell commands give us a programmatic way to get information
    about a file outside the Python environment, and the shell can be very useful
    with big data. Finally, we check the data table’s shape (the number of rows and
    columns) and granularity (what a row represents). These simple checks are the
    starting point for cleaning and analyzing our data.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了纯文本的典型文件格式和编码，描述了文件大小的度量，并使用Python工具检查源文件。在本章的后面部分，我们介绍了一种用于处理文件的替代方法：shell解释器。Shell命令为我们提供了一种在Python环境之外获取文件信息的程序化方式，而且对于大数据，shell可能非常有用。最后，我们检查数据表的形状（行数和列数）和粒度（行代表什么）。这些简单的检查是清理和分析数据的起点。
- en: We first provide brief descriptions of the datasets that we use as examples
    throughout this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先简要描述了我们在本章中始终使用的示例数据集。
- en: Data Source Examples
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据源示例
- en: 'We have selected two examples to demonstrate file wrangling concepts: a government
    survey about drug abuse, and administrative data from the San Francisco Department
    of Public Health about restaurant inspections. Before we start wrangling, we give
    an overview of the data scope for these examples (see [Chapter 2](ch02.html#ch-data-scope)).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了两个示例来演示文件整理概念：一个关于药物滥用的政府调查，以及旧金山公共卫生部门有关餐馆检查的行政数据。在我们开始整理之前，我们先概述一下这些示例的数据范围（见[第2章](ch02.html#ch-data-scope)）。
- en: Drug Abuse Warning Network (DAWN) Survey
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 药物滥用警戒网络（DAWN）调查
- en: DAWN is a national health-care survey that monitors trends in drug abuse. The
    survey aims to estimate the impact of drug abuse on the country’s health-care
    system and improve how emergency departments monitor substance abuse crises. DAWN
    was administered annually from 1998 through 2011 by the [Substance Abuse and Mental
    Health Services Administration (SAMHSA)](https://www.samhsa.gov). In 2018, due
    in part to the opioid epidemic, the DAWN survey was restarted. In this example,
    we look at the 2011 data, which have been made available through the [SAMHSA Data
    Archive](https://oreil.ly/Y2SKG).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: DAWN是一个全国性的医疗保健调查，旨在监测药物滥用趋势。该调查旨在估计药物滥用对国家医疗保健系统的影响，并改善急诊科监测物质滥用危机的方式。DAWN由[药物滥用和精神卫生服务管理局（SAMHSA）](https://www.samhsa.gov)于1998年至2011年每年进行一次。2018年，由于阿片类药物流行，DAWN调查得以重新启动。在这个例子中，我们查看了2011年的数据，这些数据已通过[SAMHSA数据存档](https://oreil.ly/Y2SKG)提供。
- en: The target population consists of all drug-related emergency room visits in
    the US. These visits are accessed through a frame of emergency rooms in hospitals
    (and their records). Hospitals are selected for the survey through probability
    sampling (see [Chapter 3](ch03.html#ch-theory-datadesign)), and all drug-related
    visits to the sampled hospital’s emergency room are included in the survey. All
    types of drug-related visits are included, such as drug misuse, abuse, accidental
    ingestion, suicide attempts, malicious poisonings, and adverse reactions. For
    each visit, the record may contain up to 16 different drugs, including illegal
    drugs, prescription drugs, and over-the-counter medications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 目标人群包括美国所有药物相关急诊室就诊者。这些访问通过医院急诊室（及其记录）的框架进行访问。医院通过概率抽样进行调查选择（参见[第三章](ch03.html#ch-theory-datadesign)），并且样本医院急诊室的所有药物相关访问都包括在调查中。所有类型的药物相关访问都包括在内，例如药物滥用、滥用、意外吞食、自杀企图、恶意中毒和不良反应。对于每次访问，记录可能包含最多16种不同的药物，包括非法药物、处方药物和非处方药物。
- en: The source file for this dataset is an example of fixed-width formatting that
    requires external documentation, like a codebook, to decipher. Also, it is a reasonably
    large file and so motivates the topic of how to find a file’s size. And the granularity
    is unusual because an ER visit, not a person, is the subject of investigation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集的源文件是一个需要外部文档（如代码书）来解释的固定宽度格式的示例。此外，由于它是一个相当大的文件，所以激发了如何找到文件大小的话题。而且，其粒度不同寻常，因为调查的主题是急诊访问，而不是个人。
- en: The San Francisco restaurant files have other characteristics that make them
    a good example for this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山餐厅文件具有其他特征，使它们成为本章的良好示例。
- en: San Francisco Restaurant Food Safety
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 旧金山餐厅食品安全
- en: The [San Francisco Department of Public Health](https://oreil.ly/kG1PN) routinely
    makes unannounced visits to restaurants and inspects them for food safety. The
    inspector calculates a score based on the violations found and provides descriptions
    of the violations. The target population here is all restaurants in San Francisco.
    These restaurants are accessed through a frame of restaurant inspections that
    were conducted between 2013 and 2016\. Some restaurants have multiple inspections
    in a year, and not all of the 7,000+ restaurants are inspected annually.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[旧金山公共卫生部门](https://oreil.ly/kG1PN)定期对餐厅进行未经预先通知的访问，并检查其食品安全情况。检查员根据发现的违规行为计算评分，并提供违规行为的描述。这里的目标人群是旧金山所有餐厅。这些餐厅是通过2013年至2016年进行的餐厅检查框架来访问的。一些餐厅一年内进行多次检查，而不是所有7000多家餐厅每年都接受检查。'
- en: Food safety scores are available through the city’s [Open Data initiative](https://oreil.ly/kwh-F),
    called [DataSF](https://datasf.org). DataSF is one example a city government making
    their data publicly available; the DataSF mission is to “empower the use of data
    in decision making and service delivery” with the goal of improving the quality
    of life and work for residents, employers, employees, and visitors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 食品安全评分可通过城市的[开放数据计划](https://oreil.ly/kwh-F)获得，称为[DataSF](https://datasf.org)。
    DataSF是城市政府公开其数据的一个例子；DataSF的使命是“在决策和服务交付中使用数据”，旨在改善居民、雇主、员工和访客的生活和工作质量。
- en: San Francisco requires restaurants to publicly display their scores (see [Figure 8-1](#scorecard)
    for an example placard).^([1](ch08.html#id1015)) These data offer an example of
    multiple files with different structures, fields, and granularity. One dataset
    contains summary results of inspections, another provides details about the violations
    found, and a third contains general information about the restaurants. The violations
    include both serious problems related to the transmission of foodborne illnesses
    and minor issues such as not properly displaying the inspection placard.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山要求餐厅公开展示其评分（参见[图8-1](#scorecard)作为示例标牌）。^([1](ch08.html#id1015)) 这些数据提供了不同结构、字段和粒度的多个文件的示例。一个数据集包含检查结果的摘要，另一个提供有关发现的违规行为的详细信息，第三个包含有关餐厅的一般信息。违规行为包括与食源性疾病传播有关的严重问题以及像未正确展示检查标牌这样的小问题。
- en: '![](assets/leds_0801.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_0801.png)'
- en: Figure 8-1\. A food safety scorecard displayed in a restaurant; scores range
    between 0 and 100
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. 展示在餐厅中的食品安全评分卡；分数范围在0到100之间。
- en: Both the DAWN survey data and the San Francisco restaurant inspection data are
    available online as plain-text files. However, their formats are quite different,
    and in the next section, we demonstrate how to figure out a file format so that
    we can read the data into a dataframe.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DAWN调查数据和旧金山餐厅检查数据都可以作为纯文本文件在线获取。然而，它们的格式有很大不同，在下一节中，我们将演示如何确定文件格式，以便将数据读入数据框架中。
- en: File Formats
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件格式
- en: A *file format* describes how data are stored on a computer’s disk or other
    storage device. Understanding the file format helps us figure out how to read
    the data into Python in order to work with it as a data table. In this section,
    we introduce several popular formats used to store data tables. These are all
    plain-text formats, meaning they are easy for us to read with a text editor like
    VS Code, Sublime, Vim, or Emacs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*文件格式*描述了数据如何存储在计算机的磁盘或其他存储设备上。了解文件格式帮助我们弄清楚如何将数据读入Python，以便将其作为数据表进行处理。在本节中，我们介绍了几种用于存储数据表的流行格式。这些都是纯文本格式，意味着我们可以使用VS
    Code、Sublime、Vim或Emacs等文本编辑器轻松阅读它们。'
- en: Note
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The file format and the *structure* of the data are two different things. We
    consider the data structure to be a mental representation of the data that tells
    us what kinds of operations we can do. For example, a table structure corresponds
    to data values arranged in rows and columns. But the same table can be stored
    in many different types of file formats.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文件格式和数据的*结构*是两个不同的事物。我们认为数据结构是数据的一种心理表示，告诉我们可以进行哪些操作。例如，表结构对应于按行和列排列的数据值。但是同一个表可以存储在许多不同类型的文件格式中。
- en: The first format we describe is the delimited file format.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述的第一种格式是分隔文件格式。
- en: Delimited Format
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分隔格式
- en: Delimited formats use a specific character to separate data values. Usually,
    these separators are either a comma (comma-separated values, or CSV for short),
    a tab (tab-separated values, or TSV), whitespace, or a colon. These formats are
    natural for storing data that have a table structure. Each line in the file represents
    a record, which is delimited by newline (`\n` or `\r\n`) characters. And within
    a line, the record’s information is delimited by the comma character (`,`) for
    CSV or the tab character (`\t`) for TSV, and so on. The first line of these files
    often contains the names of the table’s columns/features.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔格式使用特定字符来分隔数据值。通常，这些分隔符可以是逗号（逗号分隔值，或简称CSV），制表符（制表符分隔值，或TSV），空格或冒号。这些格式适合存储具有表结构的数据。文件中的每一行表示一个记录，由换行符（`\n`或`\r\n`）分隔。而在一行内，记录的信息则由逗号字符（`,`）用于CSV或制表符字符（`\t`）用于TSV等分隔。这些文件的第一行通常包含表的列名/特征的名称。
- en: 'The San Francisco restaurant scores are stored in CSV-formatted files. Let’s
    display the first few lines of the *inspections.csv* file. In Python, the built-in
    `pathlib` library has a useful `Path` object to specify paths to files and folders
    that work across platforms. This file is within the *data* folder, so we use `Path()`
    to create the full pathname:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 旧金山餐厅评分存储在CSV格式的文件中。让我们显示*inspections.csv*文件的前几行。在Python中，内置的`pathlib`库具有一个有用的`Path`对象，用于指定跨平台的文件和文件夹路径。该文件位于*data*文件夹中，因此我们使用`Path()`来创建完整的文件路径名：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Paths are tricky when working across different operating systems (OSs). For
    instance, a typical path in Windows might look like *C:\files\data.csv*, while
    a path in Unix or macOS might look like *~/files/data.csv*. Because of this, code
    that works on one OS can fail to run on other OSs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理不同操作系统（OSs）时，路径是棘手的。例如，Windows中的典型路径可能看起来像*C:\files\data.csv*，而Unix或macOS中的路径可能看起来像*~/files/data.csv*。因此，适用于一个操作系统的代码可能无法在其他操作系统上运行。
- en: The `pathlib` Python library was created to avoid OS-specific path issues. By
    using it, the code shown here is more *portable*—it works across Windows, macOS,
    and Unix.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`pathlib` Python库的创建是为了避免特定于操作系统的路径问题。通过使用它，这里显示的代码更具*可移植性* —— 它可以在Windows、macOS和Unix上运行。'
- en: 'The `Path` object in the following code has many useful methods, such as `read_text()`,
    which reads in the entire contents of the file as a string:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码中的`Path`对象具有许多有用的方法，例如`read_text()`，它将整个文件内容作为字符串读取：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Notice that the field names appear in the first line of the file; these names
    are comma separated and in quotes. We see four fields: the business identifier,
    the restaurant’s score, the date of the inspection, and the type of inspection.
    Each line in the file corresponds to one inspection, and the ID, score, date,
    and type values are separated by commas. In addition to identifying the file format,
    we also want to identify the format of the features. We see two things of note:
    the scores and dates both appear as strings. We will want to convert the scores
    to numbers so that we can calculate summary statistics and create visualizations.
    And we will convert the date into a date-time format so that we can make time-series
    plots. We show how to carry out these transformations in [Chapter 9](ch09.html#ch-wrangling).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，字段名出现在文件的第一行；这些名称用逗号分隔并带引号。我们看到四个字段：业务标识符、餐厅得分、检查日期和检查类型。文件中的每一行对应一次检查，ID、分数、日期和类型的值用逗号分隔。除了识别文件格式外，我们还希望识别特征的格式。我们注意到两点：分数和日期都显示为字符串。我们希望将分数转换为数字，以便可以计算摘要统计信息并创建可视化图。我们将日期转换为日期时间格式，以便可以制作时间序列图。我们展示如何在[第9章](ch09.html#ch-wrangling)中执行这些转换。
- en: 'Displaying the first few lines of a file is something we’ll do often, so we
    create a function as a shortcut:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 显示文件的前几行是我们经常做的事情，因此我们创建一个函数作为快捷方式：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: People often confuse CSV and TSV files with spreadsheets. This is in part because
    most spreadsheet software (like Microsoft Excel) will automatically display a
    CSV file as a table in a workbook. Behind the scenes, Excel looks at the file
    format and encoding just like we’ve done in this section. However, Excel files
    have a different format than CSV and TSV files, and we need to use different `pandas`
    functions to read these formats into Python.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常将CSV和TSV文件与电子表格混淆。部分原因是大多数电子表格软件（如Microsoft Excel）会自动将CSV文件显示为工作簿中的表格。在幕后，Excel会像我们在本节中所做的那样查看文件格式和编码。然而，Excel文件与CSV和TSV文件具有不同的格式，我们需要使用不同的`pandas`函数将这些格式读入Python。
- en: All three of the restaurant source files are CSV formatted. In contrast, the
    DAWN source file has a fixed-width format. We describe this kind of formatting
    next.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三个餐厅源文件都是CSV格式的。相比之下，DAWN源文件采用固定宽度格式。我们接下来描述这种格式化方式。
- en: Fixed-Width Format
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 固定宽度格式
- en: 'The fixed-width format (FWF) does not use delimiters to separate data values.
    Instead, the values for a specific field appear in the exact same position in
    each line. The DAWN source file has this format. Each line in the file is very
    long. For display purposes, we only show the first few characters from the first
    five lines in the file:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 固定宽度格式（FWF）不使用定界符来分隔数据值。相反，每行中特定字段的值出现在完全相同的位置。DAWN源文件采用这种格式。文件中的每一行都非常长。为了显示目的，我们只展示文件中前五行的前几个字符：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice how the values appear to align from one row to the next. For example,
    there is a decimal point in the same position (the 19th character) in each line.
    Notice also that some of the values seem to be squished together, and we need
    to know the exact position of each piece of information in a line in order to
    make sense of it. SAMHSA provides a 2,000-page [codebook](https://oreil.ly/a4OFo)
    with all of this information, including some basic checks, so that we can confirm
    that we have correctly read the file. For instance, the codebook tells us that
    the age field appears in positions 34–35 and is coded in intervals from 1 to 11\.
    The first two records shown in the preceding code have age categories of 4 and
    11; the codebook tells us that a 4 stands for the age bracket “6 to 11” and 11
    is for “65+.”
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，值如何从一行到下一行对齐。例如，每行的第19个字符处都有一个小数点。还要注意，一些值似乎被挤在一起，我们需要知道每行中每个信息片段的确切位置才能理解它。SAMHSA提供了一个有2000页的[代码手册](https://oreil.ly/a4OFo)，其中包含所有这些信息，包括一些基本检查，以便我们可以确认我们已正确读取文件。例如，代码手册告诉我们年龄字段出现在34-35位置，并以1到11的间隔编码。前面代码中显示的前两条记录的年龄类别分别为4和11；代码手册告诉我们，4代表年龄段“6到11岁”，而11代表“65岁及以上”。
- en: Other plain-text formats that are popular include hierarchical formats and loosely
    formatted text (in contrast to formats that directly support table structures).
    These are covered in greater detail in other chapters, but for completeness, we
    briefly describe them here.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其他流行的纯文本格式包括分层格式和松散格式化文本（与直接支持表结构的格式形成对比）。这些在其他章节中有更详细的介绍，但为了完整起见，我们在这里简要描述它们。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A widely adopted convention is to use the filename extension, such as *.csv*,
    *.tsv*, and *.txt*, to indicate the format of the contents of the file. Filenames
    that end with *.csv* are expected to contain comma-separated values, and those
    ending with *.tsv* are expected to contain tab-separated values; *.txt* generally
    denotes plain text without a designated format. However, these extension names
    are only suggestions. Even if a file has a *.csv* extension, the actual contents
    might not be formatted properly! It’s a good practice to inspect the contents
    of the file before loading it into a dataframe. If the file is not too large,
    you can open and examine it with a plain-text editor. Otherwise, you can view
    a couple of lines using `.readline()` or shell command.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一种广泛采用的约定是使用文件名扩展名来指示文件内容的格式，例如 *.csv*、*.tsv* 和 *.txt*。文件名以 *.csv* 结尾通常包含逗号分隔值，以
    *.tsv* 结尾的文件通常包含制表符分隔值；*.txt* 通常表示没有指定格式的纯文本。但是，这些扩展名只是建议。即使文件的扩展名为 *.csv*，实际内容可能格式不正确！在加载到数据框之前检查文件内容是一个好习惯。如果文件不太大，可以使用纯文本编辑器打开和查看。否则，可以使用
    `.readline()` 或 shell 命令查看几行。
- en: Hierarchical Formats
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层格式
- en: Hierarchical formats store data in a nested form. For instance, JavaScript Object
    Notation (JSON), which is commonly used for communication by web servers, includes
    key-value pairs and arrays that can be nested, similar to a Python dictionary.
    XML and HTML are other common formats for storing documents on the internet. Like
    JSON, these files have a hierarchical, key-value format. We cover both formats
    (JSON and XML) in more detail in [Chapter 14](ch14.html#ch-web).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分层格式以嵌套形式存储数据。例如，JavaScript 对象表示法（JSON）通常用于Web服务器的通信，包括可以嵌套的键值对和数组，类似于Python字典。XML
    和HTML是其他常见的用于在互联网上存储文档的格式。与JSON类似，这些文件具有分层的键值格式。我们在[第14章](ch14.html#ch-web)中更详细地介绍了这两种格式（JSON和XML）。
- en: Next, we briefly describe other plain-text files that don’t fall into any of
    the previous categories but still have some structure to them that enables us
    to read and extract information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们简要描述了其他不属于先前任何类别但仍具有一定结构以便于读取和提取信息的纯文本文件。
- en: Loosely Formatted Text
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 松散格式文本
- en: 'Web logs, instrument readings, and program logs typically provide data in plain
    text. For example, here is one line of a web log (we’ve split it across multiple
    lines for readability). It contains information such as the date, time, and type
    of request made to a website:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 网络日志、仪器读数和程序日志通常以纯文本形式提供数据。例如，这是网络日志的一行（我们已经将其分成多行以便阅读）。它包含日期、时间和对网站发出的请求类型等信息：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are organizational patterns present, but not in a simple delimited format.
    This is what we mean by “loosely formatted.” We see that the date and time appear
    between square brackets, and the type of request (`GET` in this case) follows
    the date-time information and appears in quotes. In [Chapter 13](ch13.html#ch-text),
    we use these observations about the web log’s format and string manipulation tools
    to extract values of interest into a data table.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 存在组织模式，但不是简单的分隔格式。这就是我们所说的“松散格式”。我们看到日期和时间出现在方括号之间，并且请求类型（本例中为 `GET`）跟随日期时间信息，并以引号形式出现。在[第13章](ch13.html#ch-text)中，我们利用这些关于网络日志格式和字符串操作工具的观察，将感兴趣的值提取到数据表中。
- en: 'As another example, here is a single record taken from a wireless device log.
    The device reports the timestamp, the identifier, its location, and the signal
    strengths that it picks up from other devices. This information uses a combination
    of formats: key-value pairs, semicolon-delimited values, and comma-delimited values:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，这是从无线设备日志中获取的单条记录。设备报告时间戳、标识符、位置以及从其他设备接收到的信号强度。此信息使用了多种格式：键值对、分号分隔值和逗号分隔值：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Like with the web logs, we can use string manipulation and the patterns in the
    records to extract features into a table.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就像网络日志一样，我们可以利用字符串操作和记录中的模式将特征提取到表中。
- en: We have primarily introduced formats for plain-text data that are widely used
    for storing and exchanging tables. The CSV format is the most common, but others,
    such as tab-separated and fixed-width formats, are also prevalent. And there are
    many types of file formats that store data!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要介绍了用于存储和交换表格的纯文本数据格式。CSV 格式是最常见的，但其他格式，如制表符分隔和固定宽度格式，也很普遍。还有许多种存储数据的文件格式！
- en: So far, we have used the term *plain text* to broadly cover formats that can
    be viewed with a text editor. However, a plain-text file may have different encodings,
    and if we don’t specify the encoding correctly, the values in the dataframe might
    contain gibberish. We give an overview of file encoding next.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用术语 *plain text* 来广泛覆盖可以在文本编辑器中查看的格式。然而，纯文本文件可能有不同的编码，如果我们没有正确指定编码，数据框中的值可能会包含无意义的内容。接下来我们概述文件编码。
- en: File Encoding
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件编码
- en: 'Computers store data as sequences of *bits*: 0s and 1s. *Character encodings*,
    like ASCII, tell the computer how to translate between bits and text. For example,
    in ASCII, the bits `100 001` stand for the letter A and `100 010` for B. The most
    basic kind of plain text supports only standard ASCII characters, which includes
    the uppercase and lowercase English letters, numbers, punctuation symbols, and
    spaces.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机将数据存储为 *比特* 的序列：0 和 1。像 ASCII 这样的 *字符编码* 告诉计算机如何在比特和文本之间进行转换。例如，在 ASCII 中，比特
    `100 001` 表示字母 A，比特 `100 010` 表示 B。最基本的纯文本仅支持标准 ASCII 字符，包括大写和小写英文字母、数字、标点符号和空格。
- en: ASCII encoding does not include a lot of special characters or characters from
    other languages. Other, more modern character encodings have many more characters
    that can be represented. Common encodings for documents and web pages are Latin-1
    (ISO-8859-1) and UTF-8\. UTF-8 has over a million characters and is backward compatible
    with ASCII, meaning that it uses the same representation for English letters,
    numbers, and punctuation as ASCII.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ASCII 编码不包括许多特殊字符或其他语言的字符。其他更现代的字符编码有更多可以表示的字符。文档和网页的常见编码是 Latin-1（ISO-8859-1）和
    UTF-8。UTF-8 具有超过一百万个字符，并且向后兼容 ASCII，这意味着它与英文字母、数字和标点的表示方式与 ASCII 相同。
- en: When we have a text file, we usually need to figure out its encoding. If we
    choose the wrong encoding to read in a file, Python either reads incorrect values
    or throws an error. The best way to find the encoding is by checking the data’s
    documentation, which often explicitly says what the encoding is.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个文本文件时，通常需要弄清楚它的编码。如果我们选择错误的编码来读取文件，Python 要么读取错误的值，要么抛出错误。找到编码的最佳方法是检查数据的文档，通常文档会明确指出编码是什么。
- en: 'When we don’t know the encoding, we have to make a guess. The `chardet` package
    has a function called `detect()` that infers a file’s encoding. Since these guesses
    are imperfect, the function also returns a confidence level between 0 and 1\.
    We use this function to look at the files from our examples:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不知道编码时，必须猜测。`chardet` 包有一个名为 `detect()` 的函数，可以推断文件的编码。由于这些猜测并不完美，该函数还返回一个介于
    0 和 1 之间的置信度。我们使用这个函数来查看我们示例中的文件：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The detection function is quite certain that all but one of the files are ASCII
    encoded. The exception is *businesses.csv*, which appears to have an ISO-8859-1
    encoding. We run into trouble if we ignore this encoding and try to read the businesses
    file into `pandas` without specifying the special encoding:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 检测函数非常确信除了一个文件外，所有文件都是 ASCII 编码的。例外是 *businesses.csv*，它似乎是 ISO-8859-1 编码的。如果我们忽略这种编码并尝试在不指定特殊编码的情况下将业务文件读入
    `pandas` 中，我们将遇到麻烦：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To successfully read the data, we must specify the ISO-8859-1 encoding:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功读取数据，我们必须指定 ISO-8859-1 编码：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|   | business_id | name | address | postal_code |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | name | address | postal_code |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | 94109
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | 94109
    |'
- en: '| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND
    FLOOR | 94104 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND
    FLOOR | 94104 |'
- en: '| **2** | 31 | NORMAN’S ICE CREAM AND FREEZES | 2801 LEAVENWORTH ST | 94133
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 31 | NORMAN’S ICE CREAM AND FREEZES | 2801 LEAVENWORTH ST | 94133
    |'
- en: '| **3** | 45 | CHARLIE’S DELI CAFE | 3202 FOLSOM ST | 94110 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 45 | CHARLIE’S DELI CAFE | 3202 FOLSOM ST | 94110 |'
- en: File encoding can be a bit mysterious to figure out, and unless there is metadata
    that explicitly gives us the encoding, guesswork comes into play. When an encoding
    is not 100% confirmed, it’s a good idea to seek additional documentation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 文件编码可能有点神秘，除非有明确给出编码的元数据，否则就要猜测。当编码没有完全确认时，最好寻找额外的文档。
- en: Another potentially important aspect of a source file is its size. If a file
    is huge, then we might not be able to read it into a dataframe. In the next section,
    we discuss how to figure out a source file’s size.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能重要的源文件方面是其大小。如果文件很大，那么我们可能无法将其读入数据框架。在下一节中，我们将讨论如何确定源文件的大小。
- en: File Size
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件大小
- en: Computers have finite resources. You have likely encountered these limits firsthand
    if your computer has slowed down from having too many applications open at once.
    We want to make sure that we do not exceed the computer’s limits while working
    with data, and we might choose to examine a file differently depending on its
    size. If we know that our dataset is relatively small, then a text editor or a
    spreadsheet can be convenient for looking at the data. On the other hand, for
    large datasets, a more programmatic exploration or even distributed computing
    tools may be needed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机资源是有限的。如果您的计算机因打开太多应用程序而变慢，您可能已经亲身经历了这些限制。我们希望确保在处理数据时不超出计算机的限制，并且可能会根据数据集的大小选择不同的文件查看方法。如果我们知道我们的数据集相对较小，那么使用文本编辑器或电子表格软件查看数据会很方便。另一方面，对于大型数据集，可能需要更多的程序化探索甚至分布式计算工具。
- en: In many situations, we analyze datasets downloaded from the internet. These
    files reside on the computer’s *disk storage*. In order to use Python to explore
    and manipulate the data, we need to read the data into the computer’s *memory*,
    also known as random access memory (RAM). All Python code requires the use of
    RAM, no matter how short the code is. A computer’s RAM is typically much smaller
    than its disk storage. For example, one computer model released in 2018 had 32
    times more disk storage than RAM. Unfortunately, this means that datafiles can
    often be much bigger than what is feasible to read into memory.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们分析从互联网下载的数据集。这些文件存储在计算机的*磁盘存储*上。为了使用Python探索和操作数据，我们需要将数据读入计算机的*内存*，也称为随机访问存储器（RAM）。无论代码有多短，所有Python代码都需要使用RAM。计算机的RAM通常比磁盘存储小得多。例如，2018年发布的某一款计算机型号的磁盘存储比RAM多32倍。不幸的是，这意味着数据文件通常比可读入内存的数据量要大得多。
- en: Both disk storage and RAM capacity are measured in terms of *bytes* (eight 0s
    and 1s). Roughly speaking, each character in a text file adds one byte to a file’s
    size. To succinctly describe the sizes of larger files, we use the prefixes described
    in [Table 8-1](#byte-prefixes); for example, a file that contains 52,428,800 characters
    will take up <math><mn>5</mn> <mo>,</mo> <mn>242</mn> <mo>,</mo> <mn>8800</mn>
    <mrow><mo>/</mo></mrow> <mn>1</mn> <mo>,</mo> <msup><mn>024</mn> <mn>2</mn></msup>
    <mo>=</mo> <mn>50</mn>  <mrow><mtext>mebibytes</mtext></mrow></math> , or 50 MiB
    on disk.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘存储和RAM容量都是以*字节*（八个0和1）为单位测量的。粗略地说，文本文件中的每个字符增加一个字节的文件大小。为了简洁描述较大文件的大小，我们使用[表 8-1](#byte-prefixes)中描述的前缀；例如，包含52,428,800个字符的文件将占用<math><mn>5</mn>
    <mo>,</mo> <mn>242</mn> <mo>,</mo> <mn>8800</mn> <mrow><mo>/</mo></mrow> <mn>1</mn>
    <mo>,</mo> <msup><mn>024</mn> <mn>2</mn></msup> <mo>=</mo> <mn>50</mn>  <mrow><mtext>mebibytes</mtext></mrow></math>
    ，即50 MiB的磁盘空间。
- en: Table 8-1\. Prefixes for common file sizes
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8-1\. 常见文件大小的前缀
- en: '| Multiple | Notation | Number of bytes |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Multiple | Notation | Number of bytes |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Kibibyte | KiB | 1,024 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Kibibyte | KiB | 1,024 |'
- en: '| Mebibyte | MiB | 1,024² |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Mebibyte | MiB | 1,024² |'
- en: '| Gibibyte | GiB | 1,024³ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Gibibyte | GiB | 1,024³ |'
- en: '| Tebibyte | TiB | 1,024⁴ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Tebibyte | TiB | 1,024⁴ |'
- en: '| Pebibyte | PiB | 1,024⁵ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Pebibyte | PiB | 1,024⁵ |'
- en: Note
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why use multiples of 1,024 instead of simple multiples of 1,000 for these prefixes?
    This is a historical result of the fact that most computers use a binary number
    scheme where powers of 2 are simpler to represent ( <math><mn>1</mn> <mo>,</mo>
    <mn>024</mn> <mo>=</mo> <msup><mn>2</mn> <mrow><mn>10</mn></mrow></msup></math>
    ). You also see the typical SI prefixes used to describe size—kilobytes, megabytes,
    and gigabytes, for example. Unfortunately, these prefixes are used inconsistently.
    Sometimes a kilobyte refers to 1,000 bytes; other times, a kilobyte refers to
    1,024 bytes. To avoid confusion, we stick to kibi-, mebi-, and gibibytes, which
    clearly represent multiples of 1,024.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用1,024的倍数而不是简单的1,000倍数来表示这些前缀？这是历史的结果，因为大多数计算机使用二进制数方案，其中2的幂更简单表示（<math><mn>1</mn>
    <mo>,</mo> <mn>024</mn> <mo>=</mo> <msup><mn>2</mn> <mrow><mn>10</mn></mrow></msup></math>）。您还会看到用于描述大小的典型SI前缀—例如，千字节、兆字节和千兆字节。不幸的是，这些前缀的使用不一致。有时，千字节指的是1,000字节；其他时候，千字节指的是1,024字节。为了避免混淆，我们坚持使用kibi-、mebi-和gibibytes这些清楚表示1,024的倍数的前缀。
- en: 'It is not uncommon to have a datafile happily stored on a computer that will
    overflow the computer’s memory if we attempt to manipulate it with a program.
    So we often begin our data work by making sure the files are of manageable size.
    To do this, we use the built-in `os` library:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试用程序操作一个超出计算机内存容量的数据文件，那么在计算机上快乐存储的数据文件通常会溢出。因此，我们通常会通过使用内置的`os`库来确保文件的大小可管理：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We see that the *businesses.csv* file takes up 645 KiB on disk, making it well
    within the memory capacities of most systems. Although the *violations.csv* file
    takes up 3.6 MiB of disk storage, most machines can easily read it into a `pandas`
    `DataFrame` too. But *DAWN-Data.txt*, which contains the DAWN survey data, is
    much larger.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到*businesses.csv*文件在磁盘上占据了645 KiB，远低于大多数系统的内存容量。虽然*violations.csv*文件占据了3.6
    MiB的磁盘存储空间，但大多数机器也可以轻松将其读入`pandas`的`DataFrame`中。但包含DAWN调查数据的*DAWN-Data.txt*文件则要大得多。
- en: The DAWN file takes up roughly 270 MiB of disk storage, and while some computers
    can work with this file in memory, it can slow down other systems. To make this
    data more manageable in Python, we can, for example, load in a subset of the columns
    rather than all of them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: DAWN文件占用大约270 MiB的磁盘存储空间，尽管一些计算机可以在内存中处理此文件，但可能会减慢其他系统的速度。为了在Python中使此数据更易管理，我们可以选择仅加载部分列而不是全部列。
- en: 'Sometimes we are interested in the total size of a folder instead of the size
    of individual files. For example, we have three restaurant files, and we might
    like to see whether we can combine all the data into a single dataframe. In the
    following code, we calculate the size of the *data* folder, including all files
    in it:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们对文件夹的总大小感兴趣，而不是单个文件的大小。例如，我们有三个餐厅文件，我们可能想看看是否可以将所有数据合并到一个单一的数据框架中。在以下代码中，我们计算*data*文件夹的大小，包括其中所有的文件：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As a rule of thumb, reading in a file using `pandas` usually requires at least
    five times the available memory as the file size. For example, reading in a 1
    GiB file typically requires at least 5 GiB of available memory. Memory is shared
    by all programs running on a computer, including the operating system, web browsers,
    and Jupyter notebook itself. A computer with 4 GiB total RAM might have only 1
    GiB available RAM with many applications running. With 1 GiB available RAM, it
    is unlikely that `pandas` will be able to read in a 1 GiB file.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，使用`pandas`读取文件需要至少五倍于文件大小的可用内存。例如，读取1 GiB文件通常需要至少5 GiB的可用内存。内存由计算机上运行的所有程序共享，包括操作系统、Web浏览器和Jupyter笔记本本身。具有4
    GiB总内存的计算机可能只有1 GiB可用内存。在只有1 GiB可用内存的情况下，`pandas`可能无法读取1 GiB文件。
- en: There are several strategies for working with data that are far larger than
    what is feasible to load into memory. We describe a few of them next.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种处理远远大于可加载到内存的数据的策略。接下来我们将介绍其中的一些。
- en: The popular term *big data* generally refers to the scenario where the data
    are large enough that even top-of-the-line computers can’t read the data directly
    into memory. This is a common scenario in scientific domains like astronomy, where
    telescopes capture images of space that can be petabytes ( <math><msup><mn>2</mn>
    <mrow><mn>50</mn></mrow></msup></math> ) in size. While not quite as big, social
    media giants, health-care providers, and other companies can also struggle with
    large amounts of data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的术语*大数据*通常指的是数据足够大，以至于即使顶级计算机也无法直接读取这些数据到内存中。这在科学领域如天文学中很常见，例如望远镜捕捉的空间图像可以达到PB（
    <math><msup><mn>2</mn> <mrow><mn>50</mn></mrow></msup></math>）级大小。虽然不及如此之大，社交媒体巨头、医疗保健提供者和其他公司也可能面临大量数据的挑战。
- en: 'Figuring out how to draw insights from these datasets is an important research
    problem that motivates the fields of database engineering and distributed computing.
    While we won’t cover these fields in this book, we provide a brief overview of
    basic approaches:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数据集中提取见解是数据库工程和分布式计算领域的一个重要研究问题的核心动机。尽管本书不涵盖这些领域，我们提供了基本方法的简要概述：
- en: Subset the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据进行子集处理。
- en: One simple approach is to work with portions of data. Rather than loading in
    the entire source file, we can either select a specific part of it (e.g., one
    day’s worth of data) or randomly sample the dataset. Because of its simplicity,
    we use this approach quite often in this book. The natural downside is that we
    lose many of the benefits of analyzing a large dataset, like being able to study
    rare events.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是处理数据的部分。与加载整个源文件不同，我们可以选择其中的特定部分（例如一天的数据）或随机抽样数据集。由于其简单性，我们在本书中经常使用这种方法。其自然缺点是我们失去了分析大数据集时的许多优势，例如能够研究罕见事件。
- en: Use a database system.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据库系统。
- en: As discussed in [Chapter 7](ch07.html#ch-sql), relational database management
    systems (RDBMSs) are specifically designed to store large datasets. SQLite is
    a useful system for working with datasets that are too large to fit in memory
    but small enough to fit on disk for a single machine. For datasets that are too
    large to fit on a single machine, more scalable database systems like MySQL and
    PostgreSQL can be used. These systems can manipulate data that are too big to
    fit into memory by using SQL queries. Because of their advantages, RDBMSs are
    commonly used for data storage in research and industry settings. One downside
    is that they often require a separate server for the data that needs its own configuration.
    Another downside is that SQL is less flexible in what it can compute than Python,
    which becomes especially relevant for modeling. A useful hybrid approach is to
    use SQL to subset, aggregate, or sample the data into batches that are small enough
    to read into Python. Then we can use Python for more sophisticated analyses.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第7章](ch07.html#ch-sql)中讨论的那样，关系数据库管理系统（RDBMSs）专门设计用于存储大型数据集。SQLite是一个有用的系统，用于处理太大以至于无法完全放入内存但足够小以适合单台机器磁盘的数据集。对于太大以至于无法放入单台机器的数据集，可以使用更可扩展的数据库系统，如MySQL和PostgreSQL。这些系统可以通过SQL查询操作无法完全放入内存的数据。由于其优势，RDBMSs常用于研究和工业设置中的数据存储。其一个缺点是通常需要一个单独的服务器来存储数据，并需要其自己的配置。另一个缺点是SQL在计算能力上不如Python灵活，尤其在建模方面尤为明显。一种有用的混合方法是使用SQL来对数据进行子集化、聚合或抽样，将数据批处理成足够小的批次以便读入Python。然后我们可以使用Python进行更复杂的分析。
- en: Use a distributed computing system.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分布式计算系统。
- en: Another approach to handling complex computations on large datasets is to use
    a distributed computing system like MapReduce, Spark, or Ray. These systems work
    best on tasks that can be split into many smaller parts where they divide datasets
    into smaller pieces and run programs on all of the smaller datasets at once. These
    systems have great flexibility and can be used in a variety of scenarios. Their
    main downside is that they can require a lot of work to install and configure
    properly because they are typically installed across many computers that need
    to coordinate with one another.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大数据集上复杂计算的另一种方法是使用MapReduce、Spark或Ray等分布式计算系统。这些系统在能够分解为许多较小部分的任务上效果最好，在这些任务中，它们将数据集分成较小的部分并同时在所有较小数据集上运行程序。这些系统具有很大的灵活性，并可在各种场景中使用。它们的主要缺点是通常需要大量工作来正确安装和配置，因为它们通常安装在需要彼此协调的许多计算机上。
- en: It can be convenient to use Python to determine a file format, encoding, and
    size. Another powerful tool for working with files is the shell; the shell is
    widely used and has a more succinct syntax than Python. In the next section, we
    introduce a few command-line tools available in the shell for carrying out the
    same tasks of finding out information about a file before reading it into a dataframe.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python确定文件格式、编码和大小可能很方便。另一个处理文件的强大工具是shell；shell广泛使用，其语法比Python更为简洁。在接下来的部分中，我们将介绍shell中可用的几个命令行工具，以执行在读取到数据帧之前查找文件信息的相同任务。
- en: The Shell and Command-Line Tools
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Shell和命令行工具
- en: Nearly all computers provide access to a *shell interpreter*, such as `sh` or
    `bash` or `zsh`. These interpreters typically perform operations on the files
    on a computer with their own language, syntax, and built-in commands.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有计算机都提供对*shell解释器*的访问，如`sh`、`bash`或`zsh`。这些解释器通常使用它们自己的语言、语法和内置命令在计算机上执行文件操作。
- en: 'We use the term *command-line interface (CLI) tools* to refer to the commands
    available in a shell interpreter. Although we only cover a few CLI tools here,
    there are many useful CLI tools that enable all sorts of operations on files.
    For instance, the following command in the `bash` shell produces a list of all
    the files in the *figures/* folder for this chapter along with their file sizes:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用术语*命令行界面（CLI）工具*来指代 shell 解释器中可用的命令。虽然我们在这里只涵盖了一些 CLI 工具，但还有许多有用的 CLI 工具可以对文件执行各种操作。例如，在
    `bash` shell 中，以下命令将列出本章 *figures/* 文件夹中的所有文件以及它们的文件大小：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-120
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The dollar sign is the shell prompt, showing the user where to type. It’s not
    part of the command itself.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 美元符号是 shell 提示符，显示用户在哪里输入。它不是命令本身的一部分。
- en: 'The basic syntax for a shell command is:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: shell 命令的基本语法是：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: CLI tools often take one or more *arguments*, similar to how Python functions
    take arguments. In the shell, we wrap arguments with spaces, not with parentheses
    or commas. The arguments appear at the end of the command line, and they are usually
    the name of a file or some text. In the `ls` example, the argument to `ls` is
    `figures/`. Additionally, CLI tools support *flags* that provide additional options.
    These flags are specified immediately following the command name using a dash
    as a delimiter. In the `ls` example, we provided the flags `-l` (to provide extra
    information about each file) and `-h` (to provide file sizes in a more human-readable
    format). Many commands have default arguments and options, and the `man` tool
    prints a list of acceptable options, examples, and defaults for any command. For
    example, `man ls` describes the 30 or so flags available for `ls`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: CLI 工具通常需要一个或多个*参数*，类似于 Python 函数需要参数。在 shell 中，我们使用空格包裹参数，而不是使用括号或逗号。参数出现在命令行的末尾，它们通常是文件的名称或一些文本。在
    `ls` 示例中，`ls` 的参数是 `figures/`。此外，CLI 工具支持*标志*，提供附加选项。这些标志紧跟在命令名称后面，使用破折号作为分隔符。在
    `ls` 示例中，我们提供了 `-l`（提供有关每个文件的额外信息）和 `-h`（以更易读的格式提供文件大小）标志。许多命令具有默认参数和选项，`man`
    工具会打印出任何命令的可接受选项、示例和默认值列表。例如，`man ls`描述了`ls`可用的约30个标志。
- en: Note
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All CLI tools we cover in this book are specific to the `sh` shell interpreter,
    the default interpreter for Jupyter installations on macOS and Linux systems at
    the time of this writing. Windows systems have a different interpreter, and the
    commands shown in the book may not run on Windows, although Windows gives access
    to an `sh` interpreter through its Linux Subsystem.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中涵盖的所有 CLI 工具都是针对 `sh` shell 解释器的，这是当前 macOS 和 Linux 系统上 Jupyter 安装的默认解释器。Windows
    系统有一个不同的解释器，书中显示的命令可能无法在 Windows 上运行，尽管 Windows 可通过其 Linux 子系统访问 `sh` 解释器。
- en: The commands in this section can be run in a terminal application, or through
    a terminal opened by Jupyter.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的命令可以在终端应用程序中运行，也可以通过 Jupyter 打开的终端运行。
- en: 'We begin with an exploration of the filesystem containing the content for this
    chapter, using the `ls` tool:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从探索包含本章内容的文件系统开始，使用 `ls` 工具：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To dive deeper and list the files in the *data/* directory, we provide the
    directory name as an argument to `ls`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地查看并列出 *data/* 目录中的文件，我们将目录名称作为 `ls` 的参数提供：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We added the `-l` flag to the command to get more information about each file.
    The file size appears in the fifth column of the listing, and it’s more readable
    as specified by the `-h` flag. When we have multiple simple option flags like
    `-l`, `-h`, and `-L`, we can combine them together as a shorthand:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在命令中添加了 `-l` 标志以获取有关每个文件的更多信息。文件大小显示在列表的第五列中，并且通过 `-h` 标志指定的方式更易读。当我们有多个简单选项标志（如
    `-l`、`-h` 和 `-L`）时，我们可以将它们组合在一起作为简写：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When working with datasets in this book, our code will often use an additional
    `-L` flag for `ls` and other CLI tools, such as `du`. We do this because we set
    up the datasets in the book using shortcuts (called *symlinks*). Usually, your
    code won’t need the `-L` flag unless you’re working with symlinks too.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中处理数据集时，我们的代码通常会为 `ls` 和其他 CLI 工具使用额外的 `-L` 标志，如 `du`。我们这样做是因为我们在书中使用快捷方式（称为*符号链接*）设置了数据集。通常情况下，您的代码不需要
    `-L` 标志，除非您也在使用符号链接。
- en: 'Other CLI tools for checking file size are `wc` and `du`. The command `wc`
    (short for *word count*) provides helpful information about a file’s size in terms
    of the number of lines, words, and characters in the file:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 用于检查文件大小的其他 CLI 工具是 `wc` 和 `du`。`wc` 命令（缩写为 *word count*）提供有关文件大小的有用信息，以行数、单词数和文件中的字符数表示：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can see from the output that *DAWN-Data.txt* has 229,211 lines and 280,095,842
    characters. (The middle value is the file’s word count, which is useful for files
    that contain sentences and paragraphs but not very useful for files containing
    data, such as FWF-formatted values.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从输出中看到 *DAWN-Data.txt* 有 229,211 行和 280,095,842 个字符。（中间值是文件的单词数，对于包含句子和段落的文件有用，但对于包含数据（如
    FWF 格式值）的文件并不十分有用。）
- en: 'The `ls` tool does not calculate the cumulative size of the contents of a folder.
    To properly calculate the total size of a folder, including the files in the folder,
    we use `du` (short for *disk usage*). By default, the `du` tool shows the size
    in units called *blocks*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`ls` 工具不计算文件夹内容的累计大小。要正确计算文件夹的总大小（包括文件夹中的文件），我们使用 `du`（磁盘使用情况的缩写）。默认情况下，`du`
    工具以称为 *blocks* 的单位显示大小：'
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We commonly add the `-s` flag to `du` to show the file sizes for both files
    and folders and the `-h` flag to display quantities in the standard KiB, MiB,
    or GiB format. The asterisk in `data/*` in the following code tells `du` to show
    the size of every item in the_data_ folder:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常会在 `du` 命令中添加 `-s` 标志来显示文件和文件夹的大小，并添加 `-h` 标志以标准 KiB、MiB 或 GiB 格式显示数量。在下面的代码中，`data/*`
    中的星号告诉 `du` 显示 _data_ 文件夹中每个项的大小：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To check the formatting of a file, we can examine the first few lines with
    the `head` command or the last few lines with `tail`. These CLIs are very useful
    for peeking at a file’s contents to determine whether it’s formatted as CSV, TSV,
    and so on. As an example, let’s look at the *inspections.csv* file:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查文件的格式，我们可以使用 `head` 命令查看前几行，或者使用 `tail` 命令查看后几行。这些 CLI 对于查看文件内容以确定其是否为 CSV、TSV
    等格式非常有用。例如，让我们来看一下 *inspections.csv* 文件：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: By default, `head` displays the first 10 lines of a file. If we want to show,
    say, four lines, then we add the option `-n 4` to our command (or just `-4` for
    short).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`head` 显示文件的前 10 行。如果我们想显示四行，我们可以在命令中添加选项 `-n 4`（或者简写为 `-4`）。
- en: 'We can print the entire contents of the file using the `cat` command. However,
    you should take care when using this command, as printing a large file can cause
    a crash. The *legend.csv* file is small, and we can use `cat` to concatenate and
    print its contents:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `cat` 命令打印文件的全部内容。但是，在使用此命令时需要小心，因为打印大文件可能会导致崩溃。*legend.csv* 文件很小，我们可以使用
    `cat` 将其内容连接并打印出来：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In many cases, using `head` or `tail` alone gives us a good enough sense of
    the file structure to proceed with loading it into a dataframe.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，仅使用 `head` 或 `tail` 就足以让我们对文件结构有足够的了解，以便将其加载到数据框中进行进一步处理。
- en: 'Finally, the `file` command can help us determine a file’s encoding:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`file` 命令可以帮助我们确定文件的编码：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We see (again) that all of the files are ASCII, except for *businesses.csv*,
    which has an ISO-8859-1 encoding.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次看到所有文件都是 ASCII 编码，除了 *businesses.csv* 使用 ISO-8859-1 编码。
- en: Note
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Commonly, we open a terminal program to start a shell interpreter. However,
    Jupyter notebooks provide a convenience: if a line of code in a Python code cell
    is prefixed with the `!` character, the line will go directly to the system’s
    shell interpreter. For example, running `!ls` in a Python cell lists the files
    in the current directory.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们打开终端程序以启动 shell 解释器。但是，Jupyter 笔记本提供了一个方便的功能：如果 Python 代码单元格中的代码行以 `!`
    字符开头，则该行将直接发送到系统的 shell 解释器。例如，在 Python 单元格中运行 `!ls` 将列出当前目录中的文件。
- en: 'Shell commands give us a programmatic way to work with files, rather than a
    point-and-click “manual” approach. They are useful for the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Shell 命令为我们提供了一种程序化处理文件的方式，而不是点-and-click 的“手动”方法。它们对以下情况非常有用：
- en: Documentation
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 文档
- en: If you need to record what you did.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要记录你所做的事情。
- en: Error reduction
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 减少错误
- en: If you want to reduce typographical errors and other simple but potentially
    harmful mistakes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想减少排版错误和其他简单但潜在有害的错误。
- en: Reproducibility
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可重复性
- en: If you need to repeat the same process in the future or you plan to share your
    process with others. This gives you a record of your actions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将来需要重复相同的过程，或者计划与他人分享你的过程。这样可以记录你的操作。
- en: Volume
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 体积
- en: If you have many repetitive operations to perform, the size of the file you
    are working with is large, or you need to perform things quickly. CLI tools can
    help in all these cases.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有许多重复操作要执行，你正在处理的文件很大，或者你需要快速完成任务。CLI 工具可以在所有这些情况下帮助你。
- en: After the data have been loaded into a dataframe, our next task is to figure
    out the table’s shape and granularity. We start by finding the number of rows
    and columns in the table (its shape). Then we need to understand what a row represents
    before we begin to check the quality of the data. We cover these topics in the
    next section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据加载到数据框之后，我们的下一个任务是弄清楚表格的形状和粒度。我们首先找出表格中的行数和列数（其形状）。然后我们需要理解一行代表什么，然后才能开始检查数据的质量。我们在下一节中讨论这些话题。
- en: Table Shape and Granularity
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表格形状和粒度
- en: As described earlier, we refer to a dataset’s *structure* as a mental representation
    of the data, and in particular, we represent data that have a *table* structure
    by arranging values in rows and columns. We use the term *granularity* to describe
    what each row in the table represents, and the term *shape* quantifies the table’s
    rows and columns.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，我们将数据集的*结构*称为数据的心理表示，特别是我们通过将值按行和列排列来表示具有*表*结构的数据。我们使用术语*粒度*来描述表中每一行代表的内容，术语*形状*量化了表的行和列。
- en: 'Now that we have determined the format of the restaurant-related files, we
    load them into dataframes and examine their shapes:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了与餐厅相关的文件的格式，我们将它们加载到数据框中并检查它们的形状：
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We find that the table with the restaurant information (the business table)
    has 6,406 rows and 9 columns. Now let’s figure out the granularity of this table.
    To start, we can look at the first two rows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现餐厅信息表（商业表）有6,406行和9列。现在让我们来弄清楚这张表的粒度。首先，我们可以看一下前两行：
- en: '|   | business_id | name | address | city | ... | postal_code | latitude |
    longitude | phone_number |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | 名称 | 地址 | 城市 | ... | 邮政编码 | 纬度 | 经度 | 电话号码 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | San Francisco
    | ... | 94109 | 37.79 | -122.42 | +14157763262 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | NRGIZE LIFESTYLE CAFE | 1200 VAN NESS AVE, 3RD FLOOR | 旧金山 |
    ... | 94109 | 37.79 | -122.42 | +14157763262 |'
- en: '| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND
    FLOOR | San Francisco | ... | 94104 | 37.79 | -122.40 | +14156779494 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 24 | OMNI S.F. HOTEL - 2ND FLOOR PANTRY | 500 CALIFORNIA ST, 2ND
    FLOOR | 旧金山 | ... | 94104 | 37.79 | -122.40 | +14156779494 |'
- en: '[PRE30]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'These two rows give us the impression that each record represents a particular
    restaurant. But, we can’t tell from just two records whether or not this is the
    case. The field named `business_id` implies that it is the unique identifier for
    the restaurant. We can confirm this by checking whether the number of records
    in the dataframe matches the number of unique values in the field `business_id`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这两行给我们的印象是每个记录代表一个特定的餐厅。但是，我们无法仅凭两个记录就知道这是否正确。名为`business_id`的字段暗示它是餐厅的唯一标识符。我们可以通过检查数据框中的记录数是否与字段`business_id`中的唯一值数目匹配来确认这一点：
- en: '[PRE31]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The number of unique `business_id`s matches the number of rows in the table,
    so it seems safe to assume that each row represents a restaurant. Since `business_id`
    uniquely identifies each record in the dataframe, we treat `business_id` as the
    *primary key* for the table. We can use primary keys to join tables (see [Chapter 6](ch06.html#ch-pandas)).
    Sometimes a primary key consists of two (or more) features. This is the case for
    the other two restaurant files. Let’s continue the examination of the inspections
    and violations dataframes and find their granularity.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的`business_id`数目与表中的行数相匹配，因此可以安全地假设每一行代表一个餐厅。由于`business_id`在数据框中唯一标识每条记录，我们将`business_id`视为该表的*主键*。我们可以使用主键来连接表（参见[第六章](ch06.html#ch-pandas)）。有时主键由两个（或更多）特征组成。这是其他两个餐厅文件的情况。让我们继续检查检查和违规数据框，并找出它们的粒度。
- en: Granularity of Restaurant Inspections and Violations
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 餐厅检查和违规的粒度
- en: 'We just saw that there are many more rows in the inspection table than the
    business table. Let’s take a closer look at the first few inspections:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到，检查表中的行比商业表中的行要多得多。让我们仔细看一下前几次检查：
- en: '|   | business_id | score | date | type |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | 分数 | 日期 | 类型 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 94 | 20160513 | routine |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 94 | 20160513 | 常规 |'
- en: '| **1** | 19 | 94 | 20171211 | routine |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 19 | 94 | 20171211 | 常规 |'
- en: '| **2** | 24 | 98 | 20171101 | routine |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 24 | 98 | 20171101 | 常规 |'
- en: '| **3** | 24 | 98 | 20161005 | routine |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 24 | 98 | 20161005 | 常规 |'
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The combination of restaurant ID and inspection date uniquely identifies each
    record in this table, with the exception of three restaurants that have two records
    for their ID-date combination. Let’s examine the rows for restaurant `64859`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 餐馆ID和检查日期的组合在这张表中唯一标识每条记录，除了三家餐馆的ID-日期组合有两条记录。让我们检查餐馆`64859`的行：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '|   | business_id | score | date | type |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | score | date | type |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **7742** | 64859 | 96 | 20150924 | routine |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **7742** | 64859 | 96 | 20150924 | 常规 |'
- en: '| **7744** | 64859 | 91 | 20150924 | routine |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **7744** | 64859 | 91 | 20150924 | 常规 |'
- en: This restaurant got two different inspection scores on the same day! How could
    this happen? It may be that the restaurant had two inspections in one day, or
    it might be an error. We address these sorts of questions when we consider data
    quality in [Chapter 9](ch09.html#ch-wrangling). Since there are only three of
    these double-inspection days, we can ignore the issue until we clean the data.
    So the primary key would be the combination of restaurant ID and inspection date
    if same-day inspections are removed from the table.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这家餐馆在同一天得到了两个不同的检查分数！这怎么可能发生？可能是餐馆在一天内接受了两次检查，或者可能是一个错误。我们在考虑[第9章](ch09.html#ch-wrangling)中的数据质量时会解决这类问题。由于这种双重检查只有三次，我们可以在清理数据之前忽略这个问题。因此，如果从表中删除同一天的检查，主键将是餐馆ID和检查日期的组合。
- en: Note that the `business_id` field in the inspections table acts as a reference
    to the primary key in the business table. So `business_id` in `insp` is a *foreign
    key* because it links each record in the inspections table to a record in the
    business table. This means that we can readily join these two tables together.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，检查表中的`business_id`字段充当对业务表主键的引用。因此，在`insp`中的`business_id`是一个*外键*，因为它将检查表中的每条记录链接到业务表中的一条记录。这意味着我们可以很容易地将这两个表连接在一起。
- en: 'Next, we examine the granularity of the third table, the one that contains
    the violations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来检查第三个表的粒度，即包含违规的表：
- en: '|   | business_id | date | description |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | date | description |'
- en: '| --- | --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **0** | 19 | 20171211 | Inadequate food safety knowledge or lack of ce...
    |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 20171211 | 食品安全知识不足或没有...'
- en: '| **1** | 19 | 20171211 | Unapproved or unmaintained equipment or utensils
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 19 | 20171211 | 未经批准或未维护的设备或器具 |'
- en: '| **2** | 19 | 20160513 | Unapproved or unmaintained equipment or utensi...
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 19 | 20160513 | 未经批准或未维护的设备或器具...'
- en: '| **...** | ... | ... | ... |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| **...** | ... | ... | ... |'
- en: '| **39039** | 94231 | 20171214 | High risk vermin infestation [ date violation...
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **39039** | 94231 | 20171214 | 高危害害虫侵扰...'
- en: '| **39040** | 94231 | 20171214 | Moderate risk food holding temperature [ dat...
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **39040** | 94231 | 20171214 | 中度风险食品保持温度...'
- en: '| **39041** | 94231 | 20171214 | Wiping cloths not clean or properly stored
    or ... |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| **39041** | 94231 | 20171214 | 擦拭布不干净或未正确存放...'
- en: '[PRE36]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Looking at the first few records in this table, we see that each inspection
    has multiple entries. The granularity appears to be at the level of a violation
    found in an inspection. Reading the descriptions, we see that if corrected, a
    date is listed in the description within square brackets:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 查看此表中的前几条记录，我们发现每次检查都有多个条目。粒度似乎是在检查中发现的违规水平。阅读描述，我们看到如果得到纠正，描述中会列出方括号中的日期。
- en: '[PRE37]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In brief, we have found that the three food safety tables have different granularities.
    Since we have identified primary and foreign keys for them, we can potentially
    join these tables. If we are interested in studying inspections, we can join the
    violations and inspections together using the business ID and inspection date.
    This would let us connect the number of violations found during an inspection
    to the inspection score.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们发现这三个食品安全表格具有不同的粒度。因为我们已经为它们确定了主键和外键，所以我们可以潜在地将这些表格连接起来。如果我们有兴趣研究检查，我们可以使用商业ID和检查日期将违规和检查一起连接起来。这将使我们能够将检查中发现的违规数量与检查分数联系起来。
- en: We can also reduce the inspection table to one per restaurant by selecting the
    most recent inspection for each restaurant. This reduced data table essentially
    has a granularity of restaurant and may be useful for a restaurant-based analysis.
    In [Chapter 9](ch09.html#ch-wrangling), we cover these kinds of actions that reshape
    a data table, transform columns, and create new columns.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: We conclude this section with a look at the shape and granularity of the DAWN
    survey data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: DAWN Survey Shape and Granularity
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As noted earlier in this chapter, the DAWN file has fixed-width formatting,
    and we need to rely on a codebook to find out where the fields are. As an example,
    a snippet of the codebook in [Figure 8-2](#dawn-age) tells us that age appears
    in positions 34 and 35 in a row, and it is categorized into 11 age groups: 1 stands
    for age 5 and under, 2 for ages 6 to 11, …, and 11 for ages 65 and older. Also,
    –8 represents a missing value.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_0802.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Screenshot of a portion of the DAWN coding for age
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Earlier, we determined that the file contains 200,000 lines and over 280 million
    characters, so on average, there are about 1,200 characters per line. This might
    be why they used a fixed-width rather than a CSV format. Think how much larger
    the file would be if there was a comma between every field!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the tremendous amount of information on each line, let’s read just a
    few features into a dataframe. We can use the `pandas.read_fwf` method to do this.
    We specify the exact positions of the fields to extract, and we provide names
    for these fields and other information about the header and index:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '|   | wt | age | sex | race | type |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| id |   |   |   |   |   |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.94 | 4 | 1 | 2 | 8 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| **2** | 5.99 | 11 | 1 | 3 | 4 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| **3** | 4.72 | 11 | 2 | 2 | 4 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| **4** | 4.08 | 2 | 1 | 3 | 4 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| **5** | 5.18 | 6 | 1 | 3 | 8 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: 'We can compare the rows in the table to the number of lines in the file:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The number of rows in the dataframe matches the number of lines in the file.
    That’s good. The granularity of the dataframe is a bit complicated due to the
    survey design. Recall that these data are part of a large scientific study, with
    a complex sampling scheme. A row represents an emergency room visit, so the granularity
    is at the emergency room visit level. However, in order to reflect the sampling
    scheme and be representative of the population of all drug-related ER visits in
    a year, weights are provided. We must apply the weight to each record when we
    compute summary statistics, build histograms, and fit models. (The `wt` field
    contains these values.)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights take into account the chance of an ER visit like this one appearing
    in the sample. By “like this one” we mean a visit with similar features, such
    as the visitor age, race, visit location, and time of day. Let’s examine the different
    values in `wt`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It is critical to include the survey weights in your analysis to get data that
    represents the population at large. For example, we can compare the calculation
    of the proportion of females among the ER visits both with and without the weights:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的分析中包括调查权重非常关键，以获取代表大多数人口的数据。例如，我们可以比较包含和不包含权重计算的急诊女性比例：
- en: '[PRE44]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: These figures differ by more than 4 percentage points. The weighted version
    is a more accurate estimate of the proportion of females among the entire population
    of drug-related ER visits.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字相差超过4个百分点。加权版本是女性在整个与药物相关的急诊访问人口中比例的更准确估计。
- en: Sometimes the granularity can be tricky to figure out, like we saw with the
    inspections data. And at other times, we need to take sampling weights into account,
    like for the DAWN data. These examples show it’s important to take your time and
    review the data descriptions before proceeding with analysis.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，像我们在检查数据中看到的那样，粒度可能很难确定。而其他时候，我们需要考虑抽样权重，比如DAWN数据。这些示例表明，在进行分析之前花时间审查数据描述是非常重要的。
- en: Summary
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Data wrangling is an essential part of data analysis. Without it, we risk overlooking
    problems in data that can have major consequences for future analysis. This chapter
    covered an important first step in data wrangling: reading data from a plain-text
    source file into a Python dataframe and identifying its granularity. We introduced
    different types of file formats and encodings, and we wrote code that can read
    data from these formats. We checked the size of source files and considered alternative
    tools for working with large datasets.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是数据分析的重要组成部分。没有它，我们可能会忽略数据中可能对未来分析产生重大影响的问题。本章介绍了数据清洗的重要第一步：从纯文本源文件中读取数据到Python数据框架并确定其粒度。我们介绍了不同类型的文件格式和编码，并编写了可以从这些格式读取数据的代码。我们检查了源文件的大小，并考虑了用于处理大型数据集的替代工具。
- en: We also introduced command-line tools as an alternative to Python for checking
    the format, encoding, and size of a file. These CLI tools are especially handy
    for filesystem-oriented tasks because of their simple syntax. We’ve only touched
    the surface of what CLI tools can do. In practice, the shell is capable of sophisticated
    data processing and is well worth learning.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了命令行工具作为检查文件格式、编码和大小的Python替代方案。由于其简单的语法，这些CLI工具在面向文件系统任务时尤为方便。我们只是触及了CLI工具的表面。在实践中，shell能够进行复杂的数据处理，是值得学习的工具。
- en: 'Understanding the shape and granularity of a table gives us insight into what
    a row in a data table represents. This helps us determine whether the granularity
    is mixed, aggregation is needed, or weights are required. After looking at the
    granularity of your dataset, you should have answers to the following questions:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 理解表的形状和粒度使我们能够洞察数据表中的一行代表什么。这有助于我们确定粒度是否混合，是否需要聚合或是否需要权重。在查看数据集的粒度后，您应该能回答以下问题：
- en: What does a record represent?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 记录代表什么？
- en: Clarity on this will help you correctly analyze data and state your findings.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 弄清这一点将帮助您正确分析数据并陈述您的发现。
- en: Do all records in a table capture granularity at the same level?
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表中的所有记录是否以相同的粒度捕获？
- en: Sometimes a table contains additional summary rows that have a different granularity,
    and you want to use only those rows that are at the right level of detail.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，表中包含其他摘要行，其粒度不同，您希望仅使用那些具有正确细节级别的行。
- en: If the data are aggregated, how was the aggregation performed?
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据已经聚合，聚合是如何执行的？
- en: Summing and averaging are common types of aggregation. With averaged data, the
    variability in the measurements is typically reduced and relationships often appear
    stronger.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 汇总和平均值是常见的聚合类型。对于平均化的数据，通常可以减少测量中的变异性，并且关系通常看起来更强。
- en: What kinds of aggregations might you perform on the data?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能对数据执行哪些类型的聚合？
- en: Aggregations might be useful or necessary to combine one data table with another.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合可能对将一个数据表与另一个数据表合并非常有用或必要。
- en: Knowing your table’s granularity is a first step to cleaning your data, and
    it informs you of how to analyze the data. For example, we saw the granularity
    of the DAWN survey is an ER visit. That naturally leads us to think about comparisons
    of patient demographics to the US as a whole.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 确定您的表的粒度是清理数据的第一步，也指导您如何分析数据。例如，我们看到DAWN调查的粒度是急诊就诊。这自然引导我们思考病人人口统计数据与整个美国的比较。
- en: The wrangling techniques in this chapter help us bring data from a source file
    into a dataframe and understand its structure. Once we have a dataframe, further
    wrangling is needed to assess and improve quality and prepare the data for analysis.
    We cover these topics in the next chapter.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的数据整理技术帮助我们将数据从源文件导入数据框架，并了解其结构。一旦我们有了数据框架，就需要进一步整理数据，评估和提高数据质量，并为分析准备数据。我们将在下一章中涵盖这些主题。
- en: ^([1](ch08.html#id1015-marker)) In 2020, the city began giving restaurants color-coded
    placards indicating whether the restaurant passed (green), conditionally passed
    (yellow), or failed (red) the inspection. These new placards no longer display
    a numeric inspection score. However, a restaurant’s scores and violations are
    still available at DataSF.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.html#id1015-marker)) 2020年，该市开始向餐馆提供彩色编码的牌子，指示餐馆是否通过（绿色）、有条件通过（黄色）或未通过（红色）检查。这些新的牌子不再显示数字检查得分。然而，餐馆的得分和违规仍然可以在
    DataSF 上查看。
