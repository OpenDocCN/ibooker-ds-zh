<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 50. Application: A Face Detection Pipeline" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0514-image-features">
<h1><span class="label">Chapter 50. </span>Application: A Face Detection Pipeline</h1>
<p><a data-primary="face recognition" data-secondary="Histogram of Oriented Gradients" data-type="indexterm" id="ix_ch50-asciidoc0"/><a data-primary="Histogram of Oriented Gradients (HOG)" data-secondary="for face detection pipeline" data-secondary-sortas="face detection" data-type="indexterm" id="ix_ch50-asciidoc1"/><a data-primary="machine learning" data-secondary="face detection pipeline" data-type="indexterm" id="ix_ch50-asciidoc2"/>This part of the book has explored a number of the central concepts and
algorithms of machine learning. But moving from these concepts to a
real-world application can be a challenge. Real-world datasets are noisy
and heterogeneous; they may have missing features, and data may be in a
form that is difficult to map to a clean <code>[n_samples, n_features]</code>
matrix. Before applying any of the methods discussed here, you must
first extract these features from your data: there is no formula for how
to do this that applies across all domains, and thus this is where you
as a data scientist must exercise your own intuition and expertise.</p>
<p>One interesting and compelling application of machine learning is to
images, and we have already seen a few examples of this where
pixel-level features are used for classification. Again, the real world
data is rarely so uniform, and simple pixels will not be suitable: this
has led to a large literature on <em>feature extraction</em> methods for image
data (see <a data-type="xref" href="ch40.xhtml#section-0504-feature-engineering">Chapter 40</a>).</p>
<p>In this chapter we will take a look at one such feature extraction
technique: the
<a href="https://oreil.ly/eiJ4X">histogram
of oriented gradients (HOG)</a>, which transforms image pixels into a
vector representation that is sensitive to broadly informative image
features regardless of confounding factors like illumination. We will
use these features to develop a simple face detection pipeline, using
machine learning algorithms and concepts we’ve seen
throughout this part of the book.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>
<section data-pdf-bookmark="HOG Features" data-type="sect1"><div class="sect1" id="ch_0514-image-features_hog-features">
<h1>HOG Features</h1>
<p><a data-primary="Histogram of Oriented Gradients (HOG)" data-secondary="features" data-type="indexterm" id="idm45858717529152"/>HOG is a straightforward feature extraction procedure that was developed
in the context of identifying pedestrians within images. It involves the
following steps:</p>
<ol>
<li>
<p>Optionally prenormalize the images. This leads to features that resist
dependence on variations in illumination.</p>
</li>
<li>
<p>Convolve the image
with two filters that are sensitive to horizontal and vertical
brightness gradients. These capture edge, contour, and texture
information.</p>
</li>
<li>
<p>Subdivide the image into cells of a predetermined
size, and compute a histogram of the gradient orientations within each
cell.</p>
</li>
<li>
<p>Normalize the histograms in each cell by comparing to the
block of neighboring cells. This further suppresses the effect of
illumination across the image.</p>
</li>
<li>
<p>Construct a one-dimensional
feature vector from the information in each cell.</p>
</li>
</ol>
<p>A fast HOG extractor is built into the Scikit-Image project, and we can
try it out relatively quickly and visualize the oriented gradients
within each cell (see <a data-type="xref" href="#fig_0514-image-features_files_in_output_4_0">Figure 50-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">skimage</code> <code class="kn">import</code> <code class="n">data</code><code class="p">,</code> <code class="n">color</code><code class="p">,</code> <code class="n">feature</code>
        <code class="kn">import</code> <code class="nn">skimage.data</code>

        <code class="n">image</code> <code class="o">=</code> <code class="n">color</code><code class="o">.</code><code class="n">rgb2gray</code><code class="p">(</code><code class="n">data</code><code class="o">.</code><code class="n">chelsea</code><code class="p">())</code>
        <code class="n">hog_vec</code><code class="p">,</code> <code class="n">hog_vis</code> <code class="o">=</code> <code class="n">feature</code><code class="o">.</code><code class="n">hog</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">visualize</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

        <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">6</code><code class="p">),</code>
                               <code class="n">subplot_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[]))</code>
        <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">)</code>
        <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'input image'</code><code class="p">)</code>

        <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">hog_vis</code><code class="p">)</code>
        <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'visualization of HOG features'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0514-image-features_files_in_output_4_0">
<img alt="output 4 0" height="194" src="assets/output_4_0.png" width="600"/>
<h6><span class="label">Figure 50-1. </span>Visualization of HOG features computed from an image</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="HOG in Action: A Simple Face Detector" data-type="sect1"><div class="sect1" id="ch_0514-image-features_hog-in-action-a-simple-face-detector">
<h1>HOG in Action: A Simple Face Detector</h1>
<p><a data-primary="Histogram of Oriented Gradients (HOG)" data-secondary="simple face detector" data-type="indexterm" id="ix_ch50-asciidoc3"/>Using these HOG features, we can build up a simple facial detection
algorithm with any Scikit-Learn estimator; <a data-primary="support vector machines (SVMs)" data-secondary="simple face detector" data-type="indexterm" id="idm45858717412160"/>here we will use a linear
support vector machine (refer back to
<a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a> if you need a refresher on this). The steps are as follows:</p>
<ol>
<li>
<p>Obtain a set of image thumbnails of faces to constitute “positive”
training 
<span class="keep-together">samples</span>.</p>
</li>
<li>
<p>Obtain a set of image thumbnails of non-faces
to constitute “negative” training samples.</p>
</li>
<li>
<p>Extract HOG
features from these training samples.</p>
</li>
<li>
<p>Train a linear SVM
classifier on these samples.</p>
</li>
<li>
<p>For an “unknown” image, pass a
sliding window across the image, using the model to evaluate whether
that window contains a face or not.</p>
</li>
<li>
<p>If detections overlap,
combine them into a single window.</p>
</li>
</ol>
<p>Let’s go through these steps and try it out.</p>
<section data-pdf-bookmark="1. Obtain a Set of Positive Training Samples" data-type="sect2"><div class="sect2" id="ch_0514-image-features_1.-obtain-a-set-of-positive-training-samples">
<h2>1. Obtain a Set of Positive Training Samples</h2>
<p>We’ll start by finding some positive training samples that
show a variety of faces. We have one easy set of data to work with—the
Labeled Faces in the Wild dataset, which can be downloaded by
Scikit-Learn:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_lfw_people</code>
        <code class="n">faces</code> <code class="o">=</code> <code class="n">fetch_lfw_people</code><code class="p">()</code>
        <code class="n">positive_patches</code> <code class="o">=</code> <code class="n">faces</code><code class="o">.</code><code class="n">images</code>
        <code class="n">positive_patches</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="p">(</code><code class="mi">13233</code><code class="p">,</code> <code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">)</code></pre>
<p>This gives us a sample of 13,000 face images to use for training.</p>
</div></section>
<section data-pdf-bookmark="2. Obtain a Set of Negative Training Samples" data-type="sect2"><div class="sect2" id="ch_0514-image-features_2.-obtain-a-set-of-negative-training-samples">
<h2>2. Obtain a Set of Negative Training Samples</h2>
<p>Next we need a set of similarly sized thumbnails that <em>do not</em> have a
face in them. One way to obtain this is to take any corpus of input
images, and extract thumbnails from them at a variety of scales. Here
we’ll use some of the images shipped with Scikit-Image,
along with Scikit-Learn’s <code>PatchExtractor</code>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">data</code><code class="o">.</code><code class="n">camera</code><code class="p">()</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="p">(</code><code class="mi">512</code><code class="p">,</code> <code class="mi">512</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">skimage</code> <code class="kn">import</code> <code class="n">data</code><code class="p">,</code> <code class="n">transform</code>

        <code class="n">imgs_to_use</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'camera'</code><code class="p">,</code> <code class="s1">'text'</code><code class="p">,</code> <code class="s1">'coins'</code><code class="p">,</code> <code class="s1">'moon'</code><code class="p">,</code>
                       <code class="s1">'page'</code><code class="p">,</code> <code class="s1">'clock'</code><code class="p">,</code> <code class="s1">'immunohistochemistry'</code><code class="p">,</code>
                       <code class="s1">'chelsea'</code><code class="p">,</code> <code class="s1">'coffee'</code><code class="p">,</code> <code class="s1">'hubble_deep_field'</code><code class="p">]</code>
        <code class="n">raw_images</code> <code class="o">=</code> <code class="p">(</code><code class="nb">getattr</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">name</code><code class="p">)()</code> <code class="k">for</code> <code class="n">name</code> <code class="ow">in</code> <code class="n">imgs_to_use</code><code class="p">)</code>
        <code class="n">images</code> <code class="o">=</code> <code class="p">[</code><code class="n">color</code><code class="o">.</code><code class="n">rgb2gray</code><code class="p">(</code><code class="n">image</code><code class="p">)</code> <code class="k">if</code> <code class="n">image</code><code class="o">.</code><code class="n">ndim</code> <code class="o">==</code> <code class="mi">3</code> <code class="k">else</code> <code class="n">image</code>
                  <code class="k">for</code> <code class="n">image</code> <code class="ow">in</code> <code class="n">raw_images</code><code class="p">]</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.feature_extraction.image</code> <code class="kn">import</code> <code class="n">PatchExtractor</code>

        <code class="k">def</code> <code class="nf">extract_patches</code><code class="p">(</code><code class="n">img</code><code class="p">,</code> <code class="n">N</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">patch_size</code><code class="o">=</code><code class="n">positive_patches</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">):</code>
            <code class="n">extracted_patch_size</code> <code class="o">=</code> <code class="nb">tuple</code><code class="p">((</code><code class="n">scale</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">patch_size</code><code class="p">))</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">))</code>
            <code class="n">extractor</code> <code class="o">=</code> <code class="n">PatchExtractor</code><code class="p">(</code><code class="n">patch_size</code><code class="o">=</code><code class="n">extracted_patch_size</code><code class="p">,</code>
                                       <code class="n">max_patches</code><code class="o">=</code><code class="n">N</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
            <code class="n">patches</code> <code class="o">=</code> <code class="n">extractor</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">img</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">])</code>
            <code class="k">if</code> <code class="n">scale</code> <code class="o">!=</code> <code class="mi">1</code><code class="p">:</code>
                <code class="n">patches</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">transform</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">patch</code><code class="p">,</code> <code class="n">patch_size</code><code class="p">)</code>
                                    <code class="k">for</code> <code class="n">patch</code> <code class="ow">in</code> <code class="n">patches</code><code class="p">])</code>
            <code class="k">return</code> <code class="n">patches</code>

        <code class="n">negative_patches</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">([</code><code class="n">extract_patches</code><code class="p">(</code><code class="n">im</code><code class="p">,</code> <code class="mi">1000</code><code class="p">,</code> <code class="n">scale</code><code class="p">)</code>
                                      <code class="k">for</code> <code class="n">im</code> <code class="ow">in</code> <code class="n">images</code> <code class="k">for</code> <code class="n">scale</code> <code class="ow">in</code> <code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">,</code> <code class="mf">2.0</code><code class="p">]])</code>
        <code class="n">negative_patches</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="p">(</code><code class="mi">30000</code><code class="p">,</code> <code class="mi">62</code><code class="p">,</code> <code class="mi">47</code><code class="p">)</code></pre>
<p>We now have 30,000 suitable image patches that do not contain faces.
Let’s visualize a few of them to get an idea of what they
look like (see <a data-type="xref" href="#fig_0514-image-features_files_in_output_14_0">Figure 50-2</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">axi</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
            <code class="n">axi</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">negative_patches</code><code class="p">[</code><code class="mi">500</code> <code class="o">*</code> <code class="n">i</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">)</code>
            <code class="n">axi</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'off'</code><code class="p">)</code></pre>
<p>Our hope is that these will sufficiently cover the space of
“non-faces” that our algorithm is likely to see.</p>
<figure><div class="figure" id="fig_0514-image-features_files_in_output_14_0">
<img alt="output 14 0" height="395" src="assets/output_14_0.png" width="600"/>
<h6><span class="label">Figure 50-2. </span>Negative image patches, which don’t include faces</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="3. Combine Sets and Extract HOG Features" data-type="sect2"><div class="sect2" id="ch_0514-image-features_3.-combine-sets-and-extract-hog-features">
<h2>3. Combine Sets and Extract HOG Features</h2>
<p>Now that we have these positive samples and negative samples, we can
combine them and compute HOG features. This step takes a little while,
because it involves a nontrivial computation for each image:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">itertools</code> <code class="kn">import</code> <code class="n">chain</code>
        <code class="n">X_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">feature</code><code class="o">.</code><code class="n">hog</code><code class="p">(</code><code class="n">im</code><code class="p">)</code>
                            <code class="k">for</code> <code class="n">im</code> <code class="ow">in</code> <code class="n">chain</code><code class="p">(</code><code class="n">positive_patches</code><code class="p">,</code>
                                            <code class="n">negative_patches</code><code class="p">)])</code>
        <code class="n">y_train</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">X_train</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
        <code class="n">y_train</code><code class="p">[:</code><code class="n">positive_patches</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code> <code class="o">=</code> <code class="mi">1</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">X_train</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="p">(</code><code class="mi">43233</code><code class="p">,</code> <code class="mi">1215</code><code class="p">)</code></pre>
<p>We are left with 43,000 training samples in 1,215 dimensions, and we now
have our data in a form that we can feed into Scikit-Learn!</p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="4. Train a Support Vector Machine" data-type="sect2"><div class="sect2" id="ch_0514-image-features_4.-train-a-support-vector-machine">
<h2>4. Train a Support Vector Machine</h2>
<p>Next we use the tools we have been exploring here to create a classifier
of thumbnail patches. For such a high-dimensional binary classification
task, a linear support vector machine is a good choice. We will use
Scikit-Learn’s <code>LinearSVC</code>, because in comparison to <code>SVC</code>
it often has better scaling for a large number of samples.</p>
<p><a data-primary="Gaussian naive Bayes classification" data-type="indexterm" id="idm45858716726576"/>First, though, let’s use a simple Gaussian naive Bayes
estimator to get a quick baseline:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>
         <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code>

         <code class="n">cross_val_score</code><code class="p">(</code><code class="n">GaussianNB</code><code class="p">(),</code> <code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code><code class="mf">0.94795883</code><code class="p">,</code> <code class="mf">0.97143518</code><code class="p">,</code> <code class="mf">0.97224471</code><code class="p">,</code> <code class="mf">0.97501735</code><code class="p">,</code> <code class="mf">0.97374508</code><code class="p">])</code></pre>
<p>We see that on our training data, even a simple naive Bayes algorithm
gets us upwards of 95% accuracy. Let’s try the support
vector machine, with a grid search over a few choices of the <code>C</code>
parameter:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVC</code>
         <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>
         <code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">LinearSVC</code><code class="p">(),</code> <code class="p">{</code><code class="s1">'C'</code><code class="p">:</code> <code class="p">[</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">2.0</code><code class="p">,</code> <code class="mf">4.0</code><code class="p">,</code> <code class="mf">8.0</code><code class="p">]})</code>
         <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
         <code class="n">grid</code><code class="o">.</code><code class="n">best_score_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="mf">0.9885272620319941</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">grid</code><code class="o">.</code><code class="n">best_params_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="p">{</code><code class="s1">'C'</code><code class="p">:</code> <code class="mf">1.0</code><code class="p">}</code></pre>
<p>This pushes us up to near 99% accuracy. Let’s take the best
estimator and retrain it on the full dataset:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">best_estimator_</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="n">LinearSVC</code><code class="p">()</code></pre>
</div></section>
<section data-pdf-bookmark="5. Find Faces in a New Image" data-type="sect2"><div class="sect2" id="ch_0514-image-features_5.-find-faces-in-a-new-image">
<h2>5. Find Faces in a New Image</h2>
<p>Now that we have this model in place, let’s grab a new image
and see how the model does. We will use one portion of the astronaut
image shown in <a data-type="xref" href="#fig_0514-image-features_files_in_output_28_0">Figure 50-3</a> for simplicity (see discussion of
this in the following section, and run a sliding window over it and
evaluate each patch:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">test_image</code> <code class="o">=</code> <code class="n">skimage</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">astronaut</code><code class="p">()</code>
         <code class="n">test_image</code> <code class="o">=</code> <code class="n">skimage</code><code class="o">.</code><code class="n">color</code><code class="o">.</code><code class="n">rgb2gray</code><code class="p">(</code><code class="n">test_image</code><code class="p">)</code>
         <code class="n">test_image</code> <code class="o">=</code> <code class="n">skimage</code><code class="o">.</code><code class="n">transform</code><code class="o">.</code><code class="n">rescale</code><code class="p">(</code><code class="n">test_image</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">)</code>
         <code class="n">test_image</code> <code class="o">=</code> <code class="n">test_image</code><code class="p">[:</code><code class="mi">160</code><code class="p">,</code> <code class="mi">40</code><code class="p">:</code><code class="mi">180</code><code class="p">]</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">test_image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'off'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0514-image-features_files_in_output_28_0">
<img alt="output 28 0" height="685" src="assets/output_28_0.png" width="600"/>
<h6><span class="label">Figure 50-3. </span>An image in which we will attempt to locate a face</h6>
</div></figure>
<p>Next, let’s create a window that iterates over patches of
this image, and compute HOG features for each patch:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">sliding_window</code><code class="p">(</code><code class="n">img</code><code class="p">,</code> <code class="n">patch_size</code><code class="o">=</code><code class="n">positive_patches</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code><code class="p">,</code>
                            <code class="n">istep</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">jstep</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">scale</code><code class="o">=</code><code class="mf">1.0</code><code class="p">):</code>
             <code class="n">Ni</code><code class="p">,</code> <code class="n">Nj</code> <code class="o">=</code> <code class="p">(</code><code class="nb">int</code><code class="p">(</code><code class="n">scale</code> <code class="o">*</code> <code class="n">s</code><code class="p">)</code> <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">patch_size</code><code class="p">)</code>
             <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">img</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">-</code> <code class="n">Ni</code><code class="p">,</code> <code class="n">istep</code><code class="p">):</code>
                 <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">img</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">-</code> <code class="n">Ni</code><code class="p">,</code> <code class="n">jstep</code><code class="p">):</code>
                     <code class="n">patch</code> <code class="o">=</code> <code class="n">img</code><code class="p">[</code><code class="n">i</code><code class="p">:</code><code class="n">i</code> <code class="o">+</code> <code class="n">Ni</code><code class="p">,</code> <code class="n">j</code><code class="p">:</code><code class="n">j</code> <code class="o">+</code> <code class="n">Nj</code><code class="p">]</code>
                     <code class="k">if</code> <code class="n">scale</code> <code class="o">!=</code> <code class="mi">1</code><code class="p">:</code>
                         <code class="n">patch</code> <code class="o">=</code> <code class="n">transform</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">patch</code><code class="p">,</code> <code class="n">patch_size</code><code class="p">)</code>
                     <code class="k">yield</code> <code class="p">(</code><code class="n">i</code><code class="p">,</code> <code class="n">j</code><code class="p">),</code> <code class="n">patch</code>

         <code class="n">indices</code><code class="p">,</code> <code class="n">patches</code> <code class="o">=</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="n">sliding_window</code><code class="p">(</code><code class="n">test_image</code><code class="p">))</code>
         <code class="n">patches_hog</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">feature</code><code class="o">.</code><code class="n">hog</code><code class="p">(</code><code class="n">patch</code><code class="p">)</code> <code class="k">for</code> <code class="n">patch</code> <code class="ow">in</code> <code class="n">patches</code><code class="p">])</code>
         <code class="n">patches_hog</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1911</code><code class="p">,</code> <code class="mi">1215</code><code class="p">)</code></pre>
<p>Finally, we can take these HOG-featured patches and use our model to
evaluate whether each patch contains a face:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">labels</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">patches_hog</code><code class="p">)</code>
         <code class="n">labels</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="mf">48.0</code></pre>
<p>We see that out of nearly 2,000 patches, we have found 48 detections.
Let’s use the information we have about these patches to
show where they lie on our test image, drawing them as rectangles (see
<a data-type="xref" href="#fig_0514-image-features_files_in_output_34_0">Figure 50-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">()</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">test_image</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'off'</code><code class="p">)</code>

         <code class="n">Ni</code><code class="p">,</code> <code class="n">Nj</code> <code class="o">=</code> <code class="n">positive_patches</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code>
         <code class="n">indices</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">indices</code><code class="p">)</code>

         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">j</code> <code class="ow">in</code> <code class="n">indices</code><code class="p">[</code><code class="n">labels</code> <code class="o">==</code> <code class="mi">1</code><code class="p">]:</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">add_patch</code><code class="p">(</code><code class="n">plt</code><code class="o">.</code><code class="n">Rectangle</code><code class="p">((</code><code class="n">j</code><code class="p">,</code> <code class="n">i</code><code class="p">),</code> <code class="n">Nj</code><code class="p">,</code> <code class="n">Ni</code><code class="p">,</code> <code class="n">edgecolor</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code>
                                        <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">facecolor</code><code class="o">=</code><code class="s1">'none'</code><code class="p">))</code></pre>
<figure><div class="figure" id="fig_0514-image-features_files_in_output_34_0">
<img alt="output 34 0" height="685" src="assets/output_34_0.png" width="600"/>
<h6><span class="label">Figure 50-4. </span>Windows that were determined to contain a face</h6>
</div></figure>
<p>All of the detected patches overlap and found the face in the image! Not
bad for a few lines of Python.<a data-startref="ix_ch50-asciidoc3" data-type="indexterm" id="idm45858716024608"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Caveats and Improvements" data-type="sect1"><div class="sect1" id="ch_0514-image-features_caveats-and-improvements">
<h1>Caveats and Improvements</h1>
<p><a data-primary="Histogram of Oriented Gradients (HOG)" data-secondary="caveats and improvements" data-type="indexterm" id="ix_ch50-asciidoc4"/>If you dig a bit deeper into the preceding code and examples,
you’ll see that we still have a bit of work to do before we
can claim a production-ready face detector. There are several issues
with what we’ve done, and several improvements that could be
made. In particular:</p>
<dl>
<dt>Our training set, especially for negative features, is not very complete</dt>
<dd>
<p>The central issue is that there are many face-like textures that are not
in the training set, and so our current model is very prone to false
positives. You can see this if you try out the algorithm on the <em>full</em>
astronaut image: the current model leads to many false detections in
other regions of the image.</p>
<p>We might imagine addressing this by adding a wider variety of images to
the negative training set, and this would probably yield some
improvement. <a data-primary="hard negative mining" data-type="indexterm" id="idm45858716018480"/>Another option would be to use a more directed approach,
such as <em>hard negative mining</em>, where we take a new set of images that
our classifier has not seen, find all the patches representing false
positives, and explicitly add them as negative instances in the training
set before retraining the classifier.</p>
</dd>
<dt>Our current pipeline searches only at one scale</dt>
<dd>
<p>As currently written, our algorithm will miss faces that are not
approximately 62 × 47 pixels. This can be straightforwardly addressed by
using sliding windows of a variety of sizes, and resizing each patch
using <code>skimage.transform.resize</code> before feeding it into the model. In
fact, the <code>sliding_window</code> utility used here is already built with this
in mind.</p>
</dd>
<dt>We should combine overlapped detection patches</dt>
<dd>
<p>For a production-ready pipeline, we would prefer not to have 30
detections of the same face, but to somehow reduce overlapping groups of
detections down to a single detection. This could be done via an
unsupervised clustering approach (mean shift clustering is one good
candidate for this), or via a procedural approach such as <em>non-maximum
suppression</em>, an algorithm common in machine vision.</p>
</dd>
<dt>The pipeline should be streamlined</dt>
<dd>
<p>Once we address the preceding issues, it would also be nice to create a
more streamlined pipeline for ingesting training images and predicting
sliding-window outputs. This is where Python as a data science tool
really shines: with a bit of work, we could take our prototype code and
package it with a well-designed object-oriented API that gives the user
the ability to use it easily. I will leave this as a proverbial
“exercise for the reader.”</p>
</dd>
<dt>More recent advances: deep learning</dt>
<dd>
<p><a data-primary="deep learning" data-type="indexterm" id="idm45858715958624"/>Finally, I should add that in machine learning contexts, HOG and other
procedural feature extraction methods are not always used. <a data-primary="neural networks" data-type="indexterm" id="idm45858715957792"/>Instead, many
modern object detection pipelines use variants of deep neural networks
(often referred to as <em>deep learning</em>): one way to think of neural
networks is as estimators that determine optimal feature extraction
strategies from the data, rather than relying on the intuition of the
user.</p>
<p>Though the field has produced fantastic results in recent years, deep
learning is not all that conceptually different from the machine
learning models explored in the previous chapters. The main advance is
the ability to utilize modern computing hardware (often large clusters
of powerful machines) to train much more flexible models on much larger
corpuses of training data. But though the scale differs, the end goal is
very much the same the same: building models from data.</p>
</dd>
</dl>
<p>If you’re interested in going further, the list of
references in the following section should provide a useful place to
start<a data-startref="ix_ch50-asciidoc4" data-type="indexterm" id="idm45858715955680"/><a data-startref="ix_ch50-asciidoc2" data-type="indexterm" id="idm45858715954976"/><a data-startref="ix_ch50-asciidoc1" data-type="indexterm" id="idm45858715954304"/>!<a data-startref="ix_ch50-asciidoc0" data-type="indexterm" id="idm45858715953504"/></p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Further Machine Learning Resources" data-type="sect1"><div class="sect1" id="section-0515-learning-more">
<h1>Further Machine Learning Resources</h1>
<p><a data-primary="machine learning" data-secondary="educational resources" data-type="indexterm" id="idm45858715951088"/>This part of the book has been a quick tour of machine learning in
Python, primarily using the tools within the Scikit-Learn library. As
long as these chapters are, they are still too short to cover many
interesting and important algorithms, approaches, and discussions. Here
I want to suggest some resources to learn more about machine learning in
Python, for those who are interested:</p>
<dl>
<dt><a href="http://scikit-learn.org">The Scikit-Learn website</a></dt>
<dd>
<p>The Scikit-Learn
website has an impressive breadth of documentation and examples covering
some of the models discussed here, and much, much more. If you want a
brief survey of the most important and often-used machine learning
algorithms, this is a good place to start.</p>
</dd>
<dt><em>SciPy, PyCon, and PyData tutorial videos</em></dt>
<dd>
<p>Scikit-Learn and other
machine learning topics are perennial favorites in the tutorial tracks
of many Python-focused conference series, in particular the PyCon,
SciPy, and PyData conferences. Most of these conferences publish videos
of their keynotes, talks, and tutorials for free online, and you should
be able to find these easily via a suitable web search (for example,
“PyCon 2022 videos”).</p>
</dd>
<dt><a class="orm:hideurl" href="https://oreil.ly/kaQQs"><em>Introduction to Machine Learning with Python</em></a>, by Andreas C. Müller and Sarah Guido (O’Reilly)</dt>
<dd>
<p>This book covers many of the machine learning
fundamentals discussed in these chapters, but is particularly relevant
for its coverage of more advanced features of Scikit-Learn, including
additional estimators, model validation approaches, and pipelining.</p>
</dd>
<dt><a href="https://oreil.ly/p268i"><em>Machine Learning with PyTorch and Scikit-Learn</em></a>, by Sebastian Raschka (Packt)</dt>
<dd>
<p>Sebastian Raschka’s most recent book starts with some of the
fundamental topics covered in these chapters, but goes deeper and shows
how those concepts apply to more sophisticated and computationally
intensive deep learing and reinforcement learning models using the
well-known <a href="https://pytorch.org">PyTorch library</a>.</p>
</dd>
</dl>
</div></section>
</div></section></div></body></html>