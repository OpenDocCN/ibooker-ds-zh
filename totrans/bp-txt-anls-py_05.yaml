- en: Chapter 5\. Feature Engineering and Syntactic Similarity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬5ç«  ç‰¹å¾å·¥ç¨‹å’Œå¥æ³•ç›¸ä¼¼æ€§
- en: As we saw in [ChapterÂ 1](ch01.xhtml#ch-exploration), text is significantly different
    from structured data. One of the most striking differences is that text is represented
    by words, while structured data (mostly) uses numbers. From a scientific point
    of view, centuries of mathematical research have led to an extremely good understanding
    of numbers and sophisticated methods. Information science has picked up that mathematical
    research, and many creative algorithms have been invented on top of that. Recent
    advances in machine learning have generalized a lot of formerly very specific
    algorithms and made them applicable to many different use cases. These methods
    â€œlearnâ€ directly from data and provide an unbiased view.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨[ç¬¬1ç« ](ch01.xhtml#ch-exploration)ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œæ–‡æœ¬ä¸ç»“æ„åŒ–æ•°æ®æœ‰ç€æ˜¾è‘—çš„ä¸åŒã€‚æœ€å¼•äººæ³¨ç›®çš„å·®å¼‚ä¹‹ä¸€æ˜¯ï¼Œæ–‡æœ¬ç”±å•è¯è¡¨ç¤ºï¼Œè€Œç»“æ„åŒ–æ•°æ®ï¼ˆå¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼‰ä½¿ç”¨æ•°å­—ã€‚ä»ç§‘å­¦è§’åº¦æ¥çœ‹ï¼Œæ•°å­¦ç ”ç©¶çš„å‡ ä¸ªä¸–çºªå·²ç»å¯¹æ•°å­—æœ‰äº†éå¸¸å¥½çš„ç†è§£å’Œå¤æ‚çš„æ–¹æ³•ã€‚ä¿¡æ¯ç§‘å­¦å¸æ”¶äº†è¿™äº›æ•°å­¦ç ”ç©¶ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå‘æ˜äº†è®¸å¤šåˆ›é€ æ€§çš„ç®—æ³•ã€‚æœºå™¨å­¦ä¹ çš„æœ€æ–°è¿›å±•å·²ç»å°†è®¸å¤šä»¥å‰éå¸¸ç‰¹å®šçš„ç®—æ³•æ³›åŒ–ï¼Œå¹¶ä½¿å…¶é€‚ç”¨äºè®¸å¤šä¸åŒçš„ç”¨ä¾‹ã€‚è¿™äº›æ–¹æ³•ç›´æ¥ä»æ•°æ®ä¸­â€œå­¦ä¹ â€ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ— åè§çš„è§†è§’ã€‚
- en: To use these instruments, we have to find a mapping of text to numbers. Considering
    the richness and complexity of text, it is clear that a single number will not
    be enough to represent the meaning of a document. Something more complex is needed.
    The natural extension of real numbers in mathematics is a tuple of real numbers,
    called a *vector*. Almost all text representations in text analytics and machine
    learning use vectors; see [ChapterÂ 6](ch06.xhtml#ch-classification) for more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨è¿™äº›å·¥å…·ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°æ–‡æœ¬åˆ°æ•°å­—çš„æ˜ å°„ã€‚è€ƒè™‘åˆ°æ–‡æœ¬çš„ä¸°å¯Œæ€§å’Œå¤æ‚æ€§ï¼Œæ˜¾ç„¶å•ä¸€çš„æ•°å­—æ— æ³•ä»£è¡¨æ–‡æ¡£çš„å«ä¹‰ã€‚éœ€è¦æ›´å¤æ‚çš„ä¸œè¥¿ã€‚åœ¨æ•°å­¦ä¸­ï¼Œå®æ•°çš„è‡ªç„¶æ‰©å±•æ˜¯ä¸€ç»„å®æ•°ï¼Œç§°ä¸º*å‘é‡*ã€‚å‡ ä¹æ‰€æœ‰æ–‡æœ¬åˆ†æå’Œæœºå™¨å­¦ä¹ ä¸­çš„æ–‡æœ¬è¡¨ç¤ºéƒ½ä½¿ç”¨å‘é‡ï¼›è¯¦è§[ç¬¬6ç« ](ch06.xhtml#ch-classification)ã€‚
- en: Vectors live in a vector space, and most vector spaces have additional properties
    such as norms and distances, which will be helpful for us as they imply the concept
    of similarity. As we will see in subsequent chapters, measuring the similarity
    between documents is absolutely crucial for most text analytics applications,
    but it is also interesting on its own.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é‡å­˜åœ¨äºå‘é‡ç©ºé—´ä¸­ï¼Œå¤§å¤šæ•°å‘é‡ç©ºé—´å…·æœ‰é¢å¤–çš„å±æ€§ï¼Œå¦‚èŒƒæ•°å’Œè·ç¦»ï¼Œè¿™å¯¹æˆ‘ä»¬æ˜¯æœ‰å¸®åŠ©çš„ï¼Œå› ä¸ºå®ƒä»¬æš—ç¤ºäº†ç›¸ä¼¼æ€§çš„æ¦‚å¿µã€‚æ­£å¦‚æˆ‘ä»¬å°†åœ¨åç»­ç« èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œæµ‹é‡æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§å¯¹äºå¤§å¤šæ•°æ–‡æœ¬åˆ†æåº”ç”¨è‡³å…³é‡è¦ï¼Œä½†å®ƒæœ¬èº«ä¹Ÿå¾ˆæœ‰è¶£ã€‚
- en: What Youâ€™ll Learn and What Weâ€™ll Build
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‚¨å°†å­¦åˆ°ä»€ä¹ˆï¼Œæˆ‘ä»¬å°†æ„å»ºä»€ä¹ˆ
- en: In this chapter we talk about the vectorization of documents. This means we
    will convert unstructured text into vectors that contain numbers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ–‡æ¡£çš„å‘é‡åŒ–ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†æŠŠéç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸ºåŒ…å«æ•°å­—çš„å‘é‡ã€‚
- en: There are quite a few ways to vectorize documents. As document vectorization
    is the basis for all machine learning tasks, we will spend some time designing
    and implementing our own vectorizer. You can use that as a blueprint if you need
    a specialized vectorizer for your own projects.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥ç”¨æ¥å‘é‡åŒ–æ–‡æ¡£ã€‚ç”±äºæ–‡æ¡£å‘é‡åŒ–æ˜¯æ‰€æœ‰æœºå™¨å­¦ä¹ ä»»åŠ¡çš„åŸºç¡€ï¼Œæˆ‘ä»¬å°†èŠ±ä¸€äº›æ—¶é—´æ¥è®¾è®¡å’Œå®ç°æˆ‘ä»¬è‡ªå·±çš„å‘é‡åŒ–å™¨ã€‚å¦‚æœæ‚¨éœ€è¦ä¸€ä¸ªä¸“é—¨ç”¨äºè‡ªå·±é¡¹ç›®çš„å‘é‡åŒ–å™¨ï¼Œæ‚¨å¯ä»¥å°†å…¶ç”¨ä½œè“å›¾ã€‚
- en: 'Afterward, we will focus on two popular models that are already implemented
    in scikit-learn: the bag-of words model and the TF-IDF improvements to it. We
    will download a large dataset of documents and vectorize it with these methods.
    As you will see, there can be many problems that are related to data volume and
    scalability.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: éšåï¼Œæˆ‘ä»¬å°†å…³æ³¨ä¸¤ç§å·²åœ¨scikit-learnä¸­å®ç°çš„æµè¡Œæ¨¡å‹ï¼šè¯è¢‹æ¨¡å‹å’ŒTF-IDFæ”¹è¿›æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›æ–¹æ³•ä¸‹è½½å¤§é‡æ–‡æ¡£æ•°æ®é›†å¹¶è¿›è¡Œå‘é‡åŒ–ã€‚æ­£å¦‚æ‚¨å°†çœ‹åˆ°çš„é‚£æ ·ï¼Œæ•°æ®é‡å’Œå¯æ‰©å±•æ€§å¯èƒ½ä¼šå¸¦æ¥è®¸å¤šé—®é¢˜ã€‚
- en: Although vectorization is a base technology for more sophisticated machine learning
    algorithms, it can also be used on its own to calculate similarities between documents.
    We will take a detailed look at how this works, how it can be optimized, and how
    we can make it scalable. For a richer representation of words, see [ChapterÂ 10](ch10.xhtml#ch-embeddings),Â and
    for aÂ more contextualized approach, see [ChapterÂ 11](ch11.xhtml#ch-sentiment).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‘é‡åŒ–æ˜¯æ›´å¤æ‚çš„æœºå™¨å­¦ä¹ ç®—æ³•çš„åŸºç¡€æŠ€æœ¯ï¼Œå®ƒä¹Ÿå¯ä»¥å•ç‹¬ç”¨äºè®¡ç®—æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºå…¶å·¥ä½œåŸç†ã€ä¼˜åŒ–æ–¹æ³•ä»¥åŠå¦‚ä½•å®ç°å¯æ‰©å±•æ€§ã€‚å¯¹äºæ›´ä¸°å¯Œçš„è¯è¡¨ç¤ºï¼Œè¯·å‚é˜…[ç¬¬10ç« ](ch10.xhtml#ch-embeddings)ï¼Œå¯¹äºæ›´ä¸Šä¸‹æ–‡åŒ–çš„æ–¹æ³•ï¼Œè¯·å‚é˜…[ç¬¬11ç« ](ch11.xhtml#ch-sentiment)ã€‚
- en: After studying this chapter, you will understand how to convert documents to
    numbers (*vectors*) using words or combinations as *features*.^([1](ch05.xhtml#idm45634198925240))
    We will try different methods of vectorizing documents, and you will be able to
    determine the correct method for your use case. You will learn why the similarity
    of documents is important and a standard way to calculate it. We will go into
    detail with an example that has many documents and show how to vectorize them
    and calculate similarities effectively.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å­¦ä¹ äº†æœ¬ç« åï¼Œæ‚¨å°†äº†è§£å¦‚ä½•ä½¿ç”¨å•è¯æˆ–ç»„åˆä½œä¸º*ç‰¹å¾*å°†æ–‡æ¡£è½¬æ¢ä¸ºæ•°å­—ï¼ˆ*å‘é‡*ï¼‰ã€‚^([1](ch05.xhtml#idm45634198925240))
    æˆ‘ä»¬å°†å°è¯•ä¸åŒçš„æ–‡æ¡£å‘é‡åŒ–æ–¹æ³•ï¼Œæ‚¨å°†èƒ½å¤Ÿç¡®å®šé€‚åˆæ‚¨ç”¨ä¾‹çš„æ­£ç¡®æ–¹æ³•ã€‚æ‚¨å°†äº†è§£æ–‡æ¡£ç›¸ä¼¼æ€§ä¸ºä½•é‡è¦ä»¥åŠè®¡ç®—å®ƒçš„æ ‡å‡†æ–¹æ³•ã€‚æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªåŒ…å«è®¸å¤šæ–‡æ¡£çš„ç¤ºä¾‹è¯¦ç»†ä»‹ç»å¦‚ä½•æœ‰æ•ˆåœ°å‘é‡åŒ–å®ƒä»¬å¹¶è®¡ç®—ç›¸ä¼¼æ€§ã€‚
- en: The first section introduces the concept of a vectorizer by actually building
    your own simple one. This can be used as a blueprint for more sophisticated vectorizers
    that you might have to build in your own projects. Counting word occurrences and
    using them as vectors is called *bag-of-words* and already creates very versatile
    models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€éƒ¨åˆ†é€šè¿‡å®é™…æ„å»ºä¸€ä¸ªç®€å•çš„å‘é‡åŒ–å™¨ä»‹ç»äº†å‘é‡åŒ–å™¨çš„æ¦‚å¿µã€‚è¿™å¯ä»¥ä½œä¸ºæ‚¨åœ¨é¡¹ç›®ä¸­å¿…é¡»æ„å»ºçš„æ›´å¤æ‚çš„å‘é‡åŒ–å™¨çš„è“å›¾ã€‚è®¡ç®—å•è¯å‡ºç°æ¬¡æ•°å¹¶å°†å…¶ç”¨ä½œå‘é‡ç§°ä¸º*è¯è¢‹æ¨¡å‹*ï¼Œå¹¶ä¸”å·²ç»åˆ›å»ºäº†éå¸¸å¤šåŠŸèƒ½çš„æ¨¡å‹ã€‚
- en: Together with the dataset (which has more than 1,000,000 news headlines), we
    introduce a use case and present a scalable blueprint architecture in the TF-IDF
    section. We will build a blueprint for vectorizing documents and a similarity
    search for documents. Even more challenging, we will try to identify the most
    similar (but nonidentical) headlines in this corpus.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ•°æ®é›†ï¼ˆæ‹¥æœ‰è¶…è¿‡1,000,000ä¸ªæ–°é—»æ ‡é¢˜ï¼‰ä¸€èµ·ï¼Œæˆ‘ä»¬åœ¨TF-IDFéƒ¨åˆ†ä»‹ç»äº†ä¸€ä¸ªç”¨ä¾‹ï¼Œå¹¶å±•ç¤ºäº†å¯æ‰©å±•çš„è“å›¾æ¶æ„ã€‚æˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªæ–‡æ¡£å‘é‡åŒ–çš„è“å›¾å’Œæ–‡æ¡£çš„ç›¸ä¼¼æ€§æœç´¢ã€‚æ›´å…·æŒ‘æˆ˜æ€§çš„æ˜¯ï¼Œæˆ‘ä»¬å°†å°è¯•è¯†åˆ«è¯­æ–™åº“ä¸­æœ€ç›¸ä¼¼ï¼ˆä½†éå®Œå…¨ç›¸åŒï¼‰çš„æ ‡é¢˜ã€‚
- en: A Toy Dataset for Experimentation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨äºå®éªŒçš„ç©å…·æ•°æ®é›†
- en: Quite surprisingly, a lot of experiments have shown that for many text analytics
    problems it is enough to know which words appear in documents. It is not necessary
    to understand the meaning of the words or take word order into account. As the
    underlying mappings are particularly easy and fast to calculate, we will start
    with these mappings and use the words as features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè®¸å¤šå®éªŒè¯æ˜ï¼Œå¯¹äºè®¸å¤šæ–‡æœ¬åˆ†æé—®é¢˜ï¼Œåªéœ€çŸ¥é“å•è¯æ˜¯å¦å‡ºç°åœ¨æ–‡æ¡£ä¸­å°±è¶³å¤Ÿäº†ã€‚ä¸å¿…ç†è§£å•è¯çš„å«ä¹‰æˆ–è€ƒè™‘å•è¯é¡ºåºã€‚ç”±äºåº•å±‚æ˜ å°„ç‰¹åˆ«ç®€å•ä¸”è®¡ç®—é€Ÿåº¦å¿«ï¼Œæˆ‘ä»¬å°†ä»è¿™äº›æ˜ å°„å¼€å§‹ï¼Œå¹¶ä½¿ç”¨å•è¯ä½œä¸ºç‰¹å¾ã€‚
- en: 'For the first blueprints, we will concentrate on the methods and therefore
    use a few sentences from the novel [*A Tale of Two Cities*](https://oreil.ly/rfmPH)
    by Charles Dickens as a toy dataset. We will use the following sentences:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬ä¸€ä¸ªè“å›¾ï¼Œæˆ‘ä»¬å°†é›†ä¸­åœ¨æ–¹æ³•ä¸Šï¼Œå› æ­¤ä½¿ç”¨æŸ¥å°”æ–¯Â·ç‹„æ›´æ–¯çš„å°è¯´[*åŒåŸè®°*](https://oreil.ly/rfmPH)ä¸­çš„å‡ å¥è¯ä½œä¸ºç©å…·æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å¥å­ï¼š
- en: It was the best of times.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€å¥½çš„æ—¶ä»£å·²ç»æ¥ä¸´ã€‚
- en: It was the worst of times.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åçš„æ—¶ä»£å·²ç»è¿‡å»ã€‚
- en: It was the age of wisdom.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ™ºæ…§çš„æ—¶ä»£å·²ç»æ¥ä¸´ã€‚
- en: It was the age of foolishness.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„šè ¢çš„æ—¶ä»£å·²ç»è¿‡å»ã€‚
- en: 'Blueprint: Building Your Own Vectorizer'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šå»ºç«‹æ‚¨è‡ªå·±çš„å‘é‡åŒ–å™¨
- en: As vectorizing documents is the base for nearly all of the following chapters
    in this book, we take an in-depth look at how vectorizers work. This works best
    by implementing our own vectorizer. You can use the methods in this section if
    you need to implement a custom vectorizer in your own projects or need to adapt
    an existing vectorizer to your specific requirements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå‘é‡åŒ–æ–‡æ¡£æ˜¯æœ¬ä¹¦åç»­ç« èŠ‚çš„åŸºç¡€ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†å‘é‡åŒ–å™¨çš„å·¥ä½œåŸç†ã€‚é€šè¿‡å®ç°æˆ‘ä»¬è‡ªå·±çš„å‘é‡åŒ–å™¨æ¥æœ€å¥½åœ°å®ç°è¿™ä¸€ç‚¹ã€‚å¦‚æœæ‚¨éœ€è¦åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­å®ç°è‡ªå®šä¹‰å‘é‡åŒ–å™¨æˆ–éœ€è¦æ ¹æ®ç‰¹å®šè¦æ±‚è°ƒæ•´ç°æœ‰å‘é‡åŒ–å™¨ï¼Œå¯ä»¥ä½¿ç”¨æœ¬èŠ‚ä¸­çš„æ–¹æ³•ã€‚
- en: To make it as simple as possible, we will implement a so-called *one-hot vectorizer*.
    This vectorizer creates binary vectors from documents by noting if a word appears
    in a document or not, yielding 1 or 0, respectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°½å¯èƒ½ç®€åŒ–ï¼Œæˆ‘ä»¬å°†å®ç°æ‰€è°“çš„*å•çƒ­å‘é‡åŒ–å™¨*ã€‚è¯¥å‘é‡åŒ–å™¨é€šè¿‡è®°å½•å•è¯æ˜¯å¦å‡ºç°åœ¨æ–‡æ¡£ä¸­æ¥åˆ›å»ºäºŒè¿›åˆ¶å‘é‡ï¼Œå¦‚æœå‡ºç°åˆ™ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚
- en: We will start by creating a vocabulary and assigning numbers to words, then
    perform the vectorization, and finally analyze similarity in this binary space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¼€å§‹åˆ›å»ºä¸€ä¸ªè¯æ±‡è¡¨å¹¶ä¸ºå•è¯åˆ†é…ç¼–å·ï¼Œç„¶åè¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶åœ¨æ­¤äºŒè¿›åˆ¶ç©ºé—´ä¸­åˆ†æç›¸ä¼¼æ€§ã€‚
- en: Enumerating the Vocabulary
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æšä¸¾è¯æ±‡è¡¨
- en: Starting with the words as features, we have to find a way to convert words
    to the dimensions of the vectors. Extracting the words from the text is done via
    tokenization, as explained in [ChapterÂ 2](ch02.xhtml#ch-api).^([2](ch05.xhtml#idm45634198893064))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å•è¯ä½œä¸ºç‰¹å¾å¼€å§‹ï¼Œæˆ‘ä»¬å¿…é¡»æ‰¾åˆ°ä¸€ç§å°†å•è¯è½¬æ¢ä¸ºå‘é‡ç»´åº¦çš„æ–¹æ³•ã€‚ä»æ–‡æœ¬ä¸­æå–å•è¯é€šè¿‡æ ‡è®°åŒ–å®Œæˆï¼Œå¦‚[ç¬¬2ç« ](ch02.xhtml#ch-api)ä¸­è§£é‡Šçš„é‚£æ ·ã€‚^([2](ch05.xhtml#idm45634198893064))
- en: 'As we are interested only in whether a word appears in a document or not, we
    can just enumerate the words:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬åªå…³å¿ƒä¸€ä¸ªå•è¯æ˜¯å¦å‡ºç°åœ¨æ–‡æ¡£ä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€åˆ—ä¸¾è¿™äº›å•è¯ï¼š
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Out:`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '| It | 0 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| å®ƒ | 0 |'
- en: '| age | 1 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| å¹´é¾„ | 1 |'
- en: '| best | 2 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| æœ€å¥½ | 2 |'
- en: '| foolishness | 3 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| æ„šè ¢ | 3 |'
- en: '| it | 4 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| å®ƒ | 4 |'
- en: '| of | 5 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| çš„ | 5 |'
- en: '| the | 6 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| çš„ | 6 |'
- en: '| times | 7 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| æ—¶ä»£ | 7 |'
- en: '| was | 8 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| æ˜¯ | 8 |'
- en: '| wisdom | 9 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| æ™ºæ…§ | 9 |'
- en: '| worst | 10 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| æœ€å | 10 |'
- en: As you can see, the words have been numbered according to their first occurrence.
    This is what we call a *dictionary*, consisting of words (the vocabulary) and
    their respective numbers. Instead of having to refer to words, we can now use
    the numbers and arrange them in the following vectors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå•è¯æ ¹æ®å®ƒä»¬ç¬¬ä¸€æ¬¡å‡ºç°çš„é¡ºåºè¿›è¡Œäº†ç¼–å·ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„â€œå­—å…¸â€ï¼ŒåŒ…æ‹¬å•è¯ï¼ˆè¯æ±‡è¡¨ï¼‰åŠå…¶ç›¸åº”çš„ç¼–å·ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ•°å­—è€Œä¸æ˜¯å•è¯æ¥æ’åˆ—å®ƒä»¬åˆ°ä»¥ä¸‹å‘é‡ä¸­ã€‚
- en: Vectorizing Documents
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æ¡£å‘é‡åŒ–
- en: To compare vectors, calculate similarities, and so forth, we have to make sure
    that vectors for each document have the same number of dimensions. To achieve
    that,Â we use the same dictionary for all documents. If the document doesnâ€™t contain
    a word, we just put a 0 at the corresponding position; otherwise, we will use
    a 1\. By convention, row vectors are used for documents. The dimension of the
    vectors is as big as the length of the dictionary. In our example, this is not
    a problem as we have only a few words. However, in large projects, the vocabulary
    can easily exceed 100,000 words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ¯”è¾ƒå‘é‡ã€è®¡ç®—ç›¸ä¼¼æ€§ç­‰ç­‰ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿æ¯ä¸ªæ–‡æ¡£çš„å‘é‡å…·æœ‰ç›¸åŒçš„ç»´åº¦ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰æ–‡æ¡£ä½¿ç”¨ç›¸åŒçš„è¯å…¸ã€‚å¦‚æœæ–‡æ¡£ä¸­ä¸åŒ…å«æŸä¸ªè¯ï¼Œæˆ‘ä»¬å°±åœ¨ç›¸åº”çš„ä½ç½®æ”¾ç½®ä¸€ä¸ª0ï¼›å¦åˆ™ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨1ã€‚æŒ‰ç…§æƒ¯ä¾‹ï¼Œè¡Œå‘é‡ç”¨äºè¡¨ç¤ºæ–‡æ¡£ã€‚å‘é‡çš„ç»´åº¦ä¸è¯å…¸çš„é•¿åº¦ä¸€æ ·å¤§ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè¿™ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬åªæœ‰å°‘æ•°å‡ ä¸ªè¯ã€‚ç„¶è€Œï¼Œåœ¨å¤§å‹é¡¹ç›®ä¸­ï¼Œè¯æ±‡è¡¨å¾ˆå®¹æ˜“è¶…è¿‡10ä¸‡ä¸ªè¯ã€‚
- en: 'Letâ€™s calculate the one-hot encoding of all sentences before actually using
    a library for this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨å®é™…ä½¿ç”¨åº“ä¹‹å‰è®¡ç®—æ‰€æœ‰å¥å­çš„ä¸€çƒ­ç¼–ç ï¼š
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For each sentence, we have now calculated a vector representation. Converting
    documents to one-hot vectors, we have lost information about how often words occur
    in documents and in which order.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªå¥å­ï¼Œæˆ‘ä»¬ç°åœ¨è®¡ç®—äº†ä¸€ä¸ªå‘é‡è¡¨ç¤ºã€‚å°†æ–‡æ¡£è½¬æ¢ä¸ºä¸€çƒ­å‘é‡æ—¶ï¼Œæˆ‘ä»¬ä¸¢å¤±äº†å…³äºå•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°é¢‘ç‡åŠé¡ºåºçš„ä¿¡æ¯ã€‚
- en: Out-of-vocabulary documents
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¶…å‡ºè¯æ±‡è¡¨çš„æ–‡æ¡£
- en: 'What happens if we try to keep the vocabulary fixed and add new documents?
    That depends on whether the words of the documents are already contained in the
    dictionary. Of course, it can happen that all words are already known:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°è¯•ä¿æŒè¯æ±‡è¡¨å›ºå®šå¹¶æ·»åŠ æ–°æ–‡æ¡£ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè¿™å–å†³äºæ–‡æ¡£çš„å•è¯æ˜¯å¦å·²ç»åŒ…å«åœ¨è¯å…¸ä¸­ã€‚å½“ç„¶ï¼Œå¯èƒ½æ‰€æœ‰å•è¯éƒ½å·²çŸ¥ï¼š
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Out:`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, the opposite is also quite possible. If we try to vectorize a sentence
    with only unknown words, we get a null vector:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåä¹‹ä¹Ÿå®Œå…¨å¯èƒ½ã€‚å¦‚æœæˆ‘ä»¬è¯•å›¾å°†åªåŒ…å«æœªçŸ¥å•è¯çš„å¥å­å‘é‡åŒ–ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªé›¶å‘é‡ï¼š
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Out:`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This sentence does not â€œinteractâ€ with the other sentences in the corpus. From
    a strict point of view, this sentence is not similar to any sentence in the corpus.
    This is no problem for a single sentence; if this happens more frequently, the
    vocabulary or the corpus needs to be adjusted.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå¥å­åœ¨è¯­æ–™åº“ä¸­ä¸å…¶ä»–å¥å­æ²¡æœ‰â€œäº¤äº’â€ã€‚ä»ä¸¥æ ¼çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªå¥å­ä¸è¯­æ–™åº“ä¸­çš„ä»»ä½•å¥å­éƒ½ä¸ç›¸ä¼¼ã€‚å¯¹äºå•ä¸ªå¥å­æ¥è¯´ï¼Œè¿™æ²¡æœ‰é—®é¢˜ï¼›å¦‚æœè¿™ç§æƒ…å†µç»å¸¸å‘ç”Ÿï¼Œéœ€è¦è°ƒæ•´è¯æ±‡è¡¨æˆ–è¯­æ–™åº“ã€‚
- en: The Document-Term Matrix
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–‡æ¡£-è¯é¡¹çŸ©é˜µ
- en: 'Arranging the row vectors for each document in a matrix with the rows enumerating
    the documents, we arrive at the document-term matrix. The document-term matrix
    is the vector representation of all documents and the most basic building block
    for nearly all machine learning tasks throughout this book. In this chapter, we
    will use it for calculating document similarities:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ¯ä¸ªæ–‡æ¡£çš„è¡Œå‘é‡æ’åˆ—æˆä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­è¡Œæšä¸¾æ–‡æ¡£ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ–‡æ¡£-è¯é¡¹çŸ©é˜µã€‚æ–‡æ¡£-è¯é¡¹çŸ©é˜µæ˜¯æ‰€æœ‰æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºï¼Œä¹Ÿæ˜¯æœ¬ä¹¦ä¸­å‡ ä¹æ‰€æœ‰æœºå™¨å­¦ä¹ ä»»åŠ¡çš„æœ€åŸºæœ¬æ„å»ºå—ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç”¨å®ƒæ¥è®¡ç®—æ–‡æ¡£ç›¸ä¼¼æ€§ï¼š
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Out:`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '![](Images/btap_05in01.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_05in01.jpg)'
- en: 'Be careful: using lists and arrays for the document-term matrix works best
    with a small vocabulary. With large vocabularies, we will have to find a cleverer
    representation. Scikit-learn takes care of this and uses so-called sparse vectors
    and matrices from [SciPy](https://oreil.ly/yk1wx).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå¯¹äºæ–‡æ¡£-è¯é¡¹çŸ©é˜µï¼Œä½¿ç”¨åˆ—è¡¨å’Œæ•°ç»„åœ¨è¯æ±‡é‡è¾ƒå°æ—¶æ•ˆæœæœ€ä½³ã€‚å¯¹äºå¤§è¯æ±‡é‡ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸æ‰¾åˆ°æ›´èªæ˜çš„è¡¨ç¤ºæ–¹å¼ã€‚Scikit-learnè´Ÿè´£æ­¤äº‹ï¼Œå¹¶ä½¿ç”¨æ‰€è°“çš„ç¨€ç–å‘é‡å’ŒçŸ©é˜µæ¥è‡ª[SciPy](https://oreil.ly/yk1wx)ã€‚
- en: Calculating similarities
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è®¡ç®—ç›¸ä¼¼æ€§
- en: 'Calculating the similarities between documents works by calculating the number
    of common 1s at the corresponding positions. In one-hot encoding, this is an extremely
    fast operation, as it can be calculated on the bit level by `AND`ing the vectors
    and counting the number of 1s in the resulting vector. Letâ€™s calculate the similarity
    of the first two sentences:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§æ˜¯é€šè¿‡è®¡ç®—å¯¹åº”ä½ç½®çš„å…±åŒ1çš„æ•°é‡æ¥è¿›è¡Œçš„ã€‚åœ¨ä¸€çƒ­ç¼–ç ä¸­ï¼Œè¿™æ˜¯ä¸€ç§éå¸¸å¿«é€Ÿçš„æ“ä½œï¼Œå› ä¸ºå¯ä»¥é€šè¿‡å¯¹å‘é‡è¿›è¡Œ`AND`è¿ç®—å¹¶è®¡ç®—ç»“æœå‘é‡ä¸­çš„1çš„æ•°é‡æ¥è®¡ç®—ã€‚è®©æˆ‘ä»¬è®¡ç®—å‰ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼š
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Another possible way to calculate the similarity that we will encounter frequently
    is using *scalar product* (often called *dot product*) of the two document vectors.
    The scalar product is calculated by multiplying corresponding components of the
    two vectors and adding up these products. By observing the fact that a product
    can only be 1 if both factors are 1, we effectively calculate the number of common
    1s in the vectors. Letâ€™s try it:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°çš„å¦ä¸€ç§è®¡ç®—ç›¸ä¼¼æ€§çš„å¯èƒ½æ–¹å¼æ˜¯ä½¿ç”¨ä¸¤ä¸ªæ–‡æ¡£å‘é‡çš„*æ ‡é‡ç§¯*ï¼ˆé€šå¸¸ç§°ä¸º*ç‚¹ç§¯*ï¼‰ã€‚æ ‡é‡ç§¯é€šè¿‡å°†ä¸¤ä¸ªå‘é‡çš„å¯¹åº”åˆ†é‡ç›¸ä¹˜å¹¶å°†è¿™äº›ä¹˜ç§¯ç›¸åŠ æ¥è®¡ç®—ã€‚é€šè¿‡è§‚å¯Ÿä¹˜ç§¯åªæœ‰åœ¨ä¸¤ä¸ªå› å­éƒ½ä¸º1æ—¶æ‰ä¸º1çš„äº‹å®ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°è®¡ç®—äº†å‘é‡ä¸­å…±åŒ1çš„æ•°é‡ã€‚è®©æˆ‘ä»¬è¯•ä¸€è¯•ï¼š
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Out:`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Similarity Matrix
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›¸ä¼¼æ€§çŸ©é˜µ
- en: 'If we are interested in finding the similarity of all documents to each other,
    there is a fantastic shortcut for calculating all the numbers with just one command!
    Generalizing the formula from the previous section, we find the similarity of
    document *i* and document *j* to be as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æœ‰å…´è¶£æ‰¾å‡ºæ‰€æœ‰æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œæœ‰ä¸€ä¸ªå¾ˆæ£’çš„å¿«æ·æ–¹å¼å¯ä»¥åªç”¨ä¸€ä¸ªå‘½ä»¤è®¡ç®—æ‰€æœ‰çš„æ•°å€¼ï¼ä»å‰ä¸€èŠ‚çš„å…¬å¼æ¨å¹¿ï¼Œæˆ‘ä»¬å¾—å‡ºæ–‡æ¡£ *i* å’Œæ–‡æ¡£ *j* çš„ç›¸ä¼¼æ€§å¦‚ä¸‹ï¼š
- en: <math alttext="upper S Subscript i j Baseline equals normal d Subscript i Baseline
    dot normal d Subscript j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="normal">d</mi> <mi>i</mi></msub> <mo>Â·</mo>
    <msub><mi mathvariant="normal">d</mi> <mi>j</mi></msub></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S Subscript i j Baseline equals normal d Subscript i Baseline
    dot normal d Subscript j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="normal">d</mi> <mi>i</mi></msub> <mo>Â·</mo>
    <msub><mi mathvariant="normal">d</mi> <mi>j</mi></msub></mrow></math>
- en: 'If we want to use the document-term matrix from earlier, we can write the scalar
    product as a sum:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³è¦ä½¿ç”¨ä¹‹å‰çš„æ–‡æ¡£-è¯é¡¹çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥å°†æ ‡é‡ç§¯å†™æˆä¸€ä¸ªå’Œï¼š
- en: <math alttext="upper S Subscript i j Baseline equals sigma-summation Underscript
    k Endscripts upper D Subscript i k Baseline upper D Subscript j k Baseline equals
    sigma-summation Underscript k Endscripts upper D Subscript i k Baseline left-parenthesis
    upper D Superscript upper T Baseline right-parenthesis Subscript k j Baseline
    equals left-parenthesis normal upper D dot normal upper D Superscript upper T
    Baseline right-parenthesis Subscript i j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mo>âˆ‘</mo> <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>D</mi> <mrow><mi>j</mi><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mo>âˆ‘</mo>
    <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mrow><mo>(</mo><msup><mi>D</mi>
    <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>k</mi><mi>j</mi></mrow></msub> <mo>=</mo>
    <msub><mrow><mo>(</mo><mi mathvariant="normal">D</mi><mo>Â·</mo><msup><mrow><mi
    mathvariant="normal">D</mi></mrow> <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S Subscript i j Baseline equals sigma-summation Underscript
    k Endscripts upper D Subscript i k Baseline upper D Subscript j k Baseline equals
    sigma-summation Underscript k Endscripts upper D Subscript i k Baseline left-parenthesis
    upper D Superscript upper T Baseline right-parenthesis Subscript k j Baseline
    equals left-parenthesis normal upper D dot normal upper D Superscript upper T
    Baseline right-parenthesis Subscript i j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mo>âˆ‘</mo> <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>D</mi> <mrow><mi>j</mi><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mo>âˆ‘</mo>
    <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mrow><mo>(</mo><msup><mi>D</mi>
    <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>k</mi><mi>j</mi></mrow></msub> <mo>=</mo>
    <msub><mrow><mo>(</mo><mi mathvariant="normal">D</mi><mo>Â·</mo><msup><mrow><mi
    mathvariant="normal">D</mi></mrow> <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
- en: So, this is just the matrix product of our document-term matrix with itself
    transposed. In Python, thatâ€™s now easy to calculate (the sentences in the output
    have been added for easier checking the similarity):^([3](ch05.xhtml#idm45634198361912))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™åªæ˜¯æˆ‘ä»¬çš„æ–‡æ¡£-è¯é¡¹çŸ©é˜µä¸å…¶è½¬ç½®çš„çŸ©é˜µä¹˜ç§¯ã€‚åœ¨Pythonä¸­ï¼Œè¿™ç°åœ¨å¾ˆå®¹æ˜“è®¡ç®—ï¼ˆè¾“å‡ºä¸­çš„å¥å­å·²æ·»åŠ ï¼Œä»¥ä¾¿æ›´è½»æ¾åœ°æ£€æŸ¥ç›¸ä¼¼æ€§ï¼‰ï¼š^([3](ch05.xhtml#idm45634198361912))
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Out:`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Obviously, the highest numbers are on the diagonal, as each document is most
    similar to itself. The matrix has to be symmetric, as document *A* has the same
    similarity to *B* as *B* to *A*. Apart from that, we can see that the second sentence
    is on average most similar to all others, whereas the third and last document
    is the most similar pairwise (they differ only by one word). The same would be
    true of the first and second documents if we ignored case.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œæœ€é«˜çš„æ•°å€¼ä½äºå¯¹è§’çº¿ä¸Šï¼Œå› ä¸ºæ¯ä¸ªæ–‡æ¡£æœ€ç›¸ä¼¼äºå®ƒè‡ªå·±ã€‚çŸ©é˜µå¿…é¡»æ˜¯å¯¹ç§°çš„ï¼Œå› ä¸ºæ–‡æ¡£ *A* ä¸ *B* çš„ç›¸ä¼¼æ€§ä¸ *B* ä¸ *A* çš„ç›¸ä¼¼æ€§ç›¸åŒã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¬¬äºŒä¸ªå¥å­å¹³å‡æ¥è¯´ä¸æ‰€æœ‰å…¶ä»–å¥å­æœ€ç›¸ä¼¼ï¼Œè€Œç¬¬ä¸‰ä¸ªå’Œæœ€åä¸€ä¸ªæ–‡æ¡£æˆå¯¹æœ€ç›¸ä¼¼ï¼ˆå®ƒä»¬ä»…ç›¸å·®ä¸€ä¸ªå•è¯ï¼‰ã€‚å¦‚æœå¿½ç•¥å¤§å°å†™ï¼Œç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªæ–‡æ¡£ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: Understanding how a document vectorizer works is crucial for implementing your
    own, but itâ€™s also helpful for appreciating all the functionalities and parameters
    of existing vectorizers. This is why we have implemented our own. We have taken
    a detailed look at the different stages of vectorization, starting with building
    a vocabulary and then converting the documents to binary vectors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£æ–‡æ¡£å‘é‡åŒ–å™¨çš„å·¥ä½œåŸç†å¯¹äºå®ç°è‡ªå·±çš„å‘é‡åŒ–å™¨è‡³å…³é‡è¦ï¼Œä½†ä¹Ÿæœ‰åŠ©äºæ¬£èµç°æœ‰å‘é‡åŒ–å™¨çš„æ‰€æœ‰åŠŸèƒ½å’Œå‚æ•°ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å®ç°äº†æˆ‘ä»¬è‡ªå·±çš„å‘é‡åŒ–å™¨ã€‚æˆ‘ä»¬è¯¦ç»†æŸ¥çœ‹äº†å‘é‡åŒ–çš„ä¸åŒé˜¶æ®µï¼Œä»æ„å»ºè¯æ±‡è¡¨å¼€å§‹ï¼Œç„¶åå°†æ–‡æ¡£è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡ã€‚
- en: Afterward, we analyzed the similarity of the documents. It turned out that the
    dot product of their corresponding vectors is a good measure for this similarity.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åæ¥ï¼Œæˆ‘ä»¬åˆ†æäº†æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚äº‹å®è¯æ˜ï¼Œå®ƒä»¬å¯¹åº”å‘é‡çš„ç‚¹ç§¯æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åº¦é‡ã€‚
- en: One-hot vectors are also used in practice, for example, in document classification
    and clustering. However, scikit-learn also offers more sophisticated vectorizers,
    which we will use in the next sections.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç‹¬çƒ­å‘é‡åœ¨å®è·µä¸­ä¹Ÿè¢«å¹¿æ³›ä½¿ç”¨ï¼Œä¾‹å¦‚åœ¨æ–‡æ¡£åˆ†ç±»å’Œèšç±»ä¸­ã€‚ç„¶è€Œï¼Œscikit-learn è¿˜æä¾›äº†æ›´å¤æ‚çš„å‘é‡åŒ–å™¨ï¼Œåœ¨æ¥ä¸‹æ¥çš„å‡ èŠ‚ä¸­æˆ‘ä»¬å°†ä½¿ç”¨å®ƒä»¬ã€‚
- en: Bag-of-Words Models
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯è¢‹æ¨¡å‹
- en: One-hot encoding has already provided us with a basic representation of documents
    as vectors. However, it did not take care of words appearing many times in documents.
    If we want to calculate the frequency of words for each document, then we should
    use what is called a *bag-of-words* representation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç‹¬çƒ­ç¼–ç å·²ç»ä¸ºæˆ‘ä»¬æä¾›äº†æ–‡æ¡£çš„åŸºæœ¬è¡¨ç¤ºå½¢å¼ä½œä¸ºå‘é‡ã€‚ç„¶è€Œï¼Œå®ƒæ²¡æœ‰å¤„ç†æ–‡æ¡£ä¸­å•è¯çš„å‡ºç°æ¬¡æ•°ã€‚å¦‚æœæˆ‘ä»¬æƒ³è®¡ç®—æ¯ä¸ªæ–‡æ¡£ä¸­å•è¯çš„é¢‘ç‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥ä½¿ç”¨æ‰€è°“çš„*è¯è¢‹*è¡¨ç¤ºæ³•ã€‚
- en: Although somewhat simplistic, these models are in wide use. For cases such as
    classification and sentiment detection, they work reasonably. Moreover, there
    are topic modeling methods like Latent Dirichlet Allocation (LDA),Â which explicitly
    requires a bag-of-words model.^([4](ch05.xhtml#idm45634198247384))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ‰äº›ç®€å•ï¼Œä½†è¿™äº›æ¨¡å‹è¢«å¹¿æ³›ä½¿ç”¨ã€‚å¯¹äºåˆ†ç±»å’Œæƒ…æ„Ÿæ£€æµ‹ç­‰æƒ…å†µï¼Œå®ƒä»¬è¡¨ç°åˆç†ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰åƒæ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…ï¼ˆLDAï¼‰è¿™æ ·çš„ä¸»é¢˜å»ºæ¨¡æ–¹æ³•ï¼Œæ˜¾å¼åœ°éœ€è¦è¯è¢‹æ¨¡å‹ã€‚^([4](ch05.xhtml#idm45634198247384))
- en: 'Blueprint: Using scikit-learnâ€™s CountVectorizer'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šä½¿ç”¨ scikit-learn çš„ CountVectorizer
- en: Instead of implementing a bag-of-words model on our own, we use the algorithm
    that scikit-learn provides.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¿…è‡ªå·±å®ç°è¯è¢‹æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn æä¾›çš„ç®—æ³•ã€‚
- en: 'Notice that the corresponding class is called `CountVectorizer`, which is our
    first encounter with feature extraction in scikit-learn. We will take a detailed
    look at the design of the classes and in which order their methods should be called:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åˆ°ç›¸åº”çš„ç±»è¢«ç§°ä¸º`CountVectorizer`ï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨ scikit-learn ä¸­è¿›è¡Œç‰¹å¾æå–çš„ç¬¬ä¸€æ¬¡æ¥è§¦ã€‚æˆ‘ä»¬å°†è¯¦ç»†æŸ¥çœ‹è¿™äº›ç±»çš„è®¾è®¡åŠå…¶æ–¹æ³•è°ƒç”¨çš„é¡ºåºï¼š
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our example sentences from the one-hot encoder is really trivial, as no sentence
    in our dataset contains words more than once. Letâ€™s add some more sentences and
    use that as a basis for the `CountVectorizer`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¥è‡ªç‹¬çƒ­ç¼–ç çš„ç¤ºä¾‹å¥å­å®é™…ä¸Šéå¸¸ç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ä¸­æ²¡æœ‰å¥å­åŒ…å«å¤šæ¬¡å•è¯ã€‚è®©æˆ‘ä»¬å†æ·»åŠ ä¸€äº›å¥å­ï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€ä½¿ç”¨ `CountVectorizer`ã€‚
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`CountVectorizer` works in two distinct phases: first it has to learn the vocabulary;
    afterward it can transform the documents to vectors.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` åˆ†ä¸ºä¸¤ä¸ªæ˜æ˜¾çš„é˜¶æ®µï¼šé¦–å…ˆå®ƒå¿…é¡»å­¦ä¹ è¯æ±‡è¡¨ï¼›ä¹‹åå®ƒå¯ä»¥å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ã€‚'
- en: Fitting the vocabulary
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‹Ÿåˆè¯æ±‡è¡¨
- en: 'First, it needs to learn about the vocabulary. This is simpler now, as we can
    just pass the array with the sentences:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå®ƒéœ€è¦å­¦ä¹ è¯æ±‡è¡¨ã€‚ç°åœ¨è¿™æ›´ç®€å•äº†ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ç›´æ¥ä¼ é€’åŒ…å«å¥å­çš„æ•°ç»„ï¼š
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Donâ€™t worry about all these parameters; we will talk about the important ones
    later. Letâ€™s first see what `CountVectorizer` used as vocabulary, which is called
    *feature names* here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¦æ‹…å¿ƒæ‰€æœ‰è¿™äº›å‚æ•°ï¼›æˆ‘ä»¬ç¨åä¼šè®¨è®ºé‡è¦çš„å‚æ•°ã€‚è®©æˆ‘ä»¬é¦–å…ˆçœ‹çœ‹ `CountVectorizer` ä½¿ç”¨çš„è¯æ±‡è¡¨ï¼Œè¿™é‡Œç§°ä¸º*ç‰¹å¾åç§°*ï¼š
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Out:`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have created a vocabulary and so-called features using `CountVectorizer`.
    Conveniently, the vocabulary is sorted alphabetically, which makes it easier for
    us to decide whether a specific word is included.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»åˆ›å»ºäº†ä¸€ä¸ªè¯æ±‡è¡¨å’Œæ‰€è°“çš„ç‰¹å¾ï¼Œä½¿ç”¨ `CountVectorizer`ã€‚æ–¹ä¾¿åœ°ï¼Œè¯æ±‡è¡¨æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œè¿™ä½¿æˆ‘ä»¬æ›´å®¹æ˜“å†³å®šæ˜¯å¦åŒ…å«ç‰¹å®šå•è¯ã€‚
- en: Transforming the documents to vectors
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡
- en: 'In the second step, we will use `CountVectorizer` to transform the documents
    to the vector representation:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `CountVectorizer` å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼š
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The result is the document-term matrix that we have already encountered in
    the previous section. However, it is a different object, as `CountVectorizer`
    has created a sparse matrix. Letâ€™s check:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­å·²ç»é‡åˆ°çš„æ–‡æ¡£-æœ¯è¯­çŸ©é˜µã€‚ç„¶è€Œï¼Œå®ƒæ˜¯ä¸€ä¸ªä¸åŒçš„å¯¹è±¡ï¼Œå› ä¸º `CountVectorizer` åˆ›å»ºäº†ä¸€ä¸ªç¨€ç–çŸ©é˜µã€‚è®©æˆ‘ä»¬æ¥æ£€æŸ¥ä¸€ä¸‹ï¼š
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`Out:`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Sparse matrices are extremely efficient. Instead of storing 6 Ã— 20 = 120 elements,
    it just has to save 38! Sparse matrices achieve that by skipping all zero elements.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨€ç–çŸ©é˜µéå¸¸é«˜æ•ˆã€‚å®ƒåªéœ€ä¿å­˜ 38 ä¸ªå…ƒç´ ï¼Œè€Œä¸æ˜¯å­˜å‚¨ 6 Ã— 20 = 120 ä¸ªå…ƒç´ ï¼ç¨€ç–çŸ©é˜µé€šè¿‡è·³è¿‡æ‰€æœ‰é›¶å…ƒç´ æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: 'Letâ€™s try to recover our former document-term matrix. For this, we must transform
    the sparse matrix to a (dense) array. To make it easier to read, we convert it
    into a Pandas `DataFrame`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¯•ç€æ¢å¤æˆ‘ä»¬å…ˆå‰çš„æ–‡æ¡£-æœ¯è¯­çŸ©é˜µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å°†ç¨€ç–çŸ©é˜µè½¬æ¢ä¸ºï¼ˆç¨ å¯†çš„ï¼‰æ•°ç»„ã€‚ä¸ºäº†ä½¿å…¶æ›´æ˜“è¯»ï¼Œæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸º Pandas çš„ `DataFrame`ï¼š
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_05in02.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_05in02.jpg)'
- en: The document-term matrix looks very similar to the one from our one-hot vectorizer.
    Note, however, that the columns are in alphabetical order, and observe several
    2s in the fifth row. This originates from the document `"John likes to watch movies.
    Mary likes movies too."`, which has many duplicate words.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£-è¯é¡¹çŸ©é˜µçœ‹èµ·æ¥ä¸æˆ‘ä»¬çš„å•çƒ­å‘é‡åŒ–å™¨éå¸¸ç›¸ä¼¼ã€‚ä½†è¯·æ³¨æ„ï¼Œåˆ—æ˜¯æŒ‰å­—æ¯é¡ºåºæ’åˆ—çš„ï¼Œå¹¶ä¸”æ³¨æ„ç¬¬äº”è¡Œæœ‰å‡ ä¸ª2ã€‚è¿™æºè‡ªæ–‡æ¡£`"John likes to watch
    movies. Mary likes movies too."`ï¼Œå…¶ä¸­æœ‰å¾ˆå¤šé‡å¤è¯è¯­ã€‚
- en: 'Blueprint: Calculating Similarities'
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šè®¡ç®—ç›¸ä¼¼æ€§
- en: 'Finding similarities between documents is now more difficult as it is not enough
    to count the common 1s in the documents. In general, the number of occurrences
    of each word can be bigger, and we have to take that into account.Â  The dot product
    cannot be used for this, as it is also sensitive to the length of the vector (the
    number of words in the documents). Also, a Euclidean distance is not very useful
    in high-dimensional vector spaces. This is why most commonly the angle between
    document vectors is used as a measure of similarity.Â The cosine of the angle between
    two vectors is defined by the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åœ¨æ–‡æ¡£ä¹‹é—´æ‰¾åˆ°ç›¸ä¼¼æ€§æ›´åŠ å›°éš¾ï¼Œå› ä¸ºä»…ä»…è®¡ç®—æ–‡æ¡£ä¸­å…±åŒå‡ºç°çš„1ä¸å†è¶³å¤Ÿã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæ¯ä¸ªè¯çš„å‡ºç°æ¬¡æ•°å¯èƒ½æ›´å¤šï¼Œæˆ‘ä»¬å¿…é¡»è€ƒè™‘è¿™ä¸€ç‚¹ã€‚ä¸èƒ½ä½¿ç”¨ç‚¹ç§¯æ¥åšè¿™ä¸ªï¼Œå› ä¸ºå®ƒä¹Ÿå¯¹å‘é‡çš„é•¿åº¦ï¼ˆæ–‡æ¡£ä¸­çš„è¯æ•°ï¼‰æ•æ„Ÿã€‚æ­¤å¤–ï¼Œæ¬§æ°è·ç¦»åœ¨é«˜ç»´å‘é‡ç©ºé—´ä¸­å¹¶ä¸æ˜¯å¾ˆæœ‰ç”¨ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé€šå¸¸ä½¿ç”¨æ–‡æ¡£å‘é‡ä¹‹é—´çš„è§’åº¦ä½œä¸ºç›¸ä¼¼æ€§çš„åº¦é‡ã€‚ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’çš„ä½™å¼¦å®šä¹‰å¦‚ä¸‹ï¼š
- en: <math alttext="normal c normal o normal s left-parenthesis bold a comma bold
    b right-parenthesis equals StartFraction bold a dot bold b Over StartAbsoluteValue
    EndAbsoluteValue bold a StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue
    EndAbsoluteValue bold b StartAbsoluteValue EndAbsoluteValue EndFraction equals
    StartFraction sigma-summation a Subscript i Baseline b Subscript i Baseline Over
    StartRoot sigma-summation a Subscript i Baseline a Subscript i Baseline EndRoot
    StartRoot sigma-summation b Subscript i Baseline b Subscript i Baseline EndRoot
    EndFraction"><mrow><mi>cos</mi> <mrow><mo>(</mo> <mi>ğš</mi> <mo>,</mo> <mi>ğ›</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>ğš</mi><mo>Â·</mo><mi>ğ›</mi></mrow>
    <mrow><mo>|</mo><mo>|</mo><mi>ğš</mi><mo>|</mo><mo>|</mo><mo>Â·</mo><mo>|</mo><mo>|</mo><mi>ğ›</mi><mo>|</mo><mo>|</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mo>âˆ‘</mo><msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi>
    <mi>i</mi></msub></mrow> <mrow><msqrt><mrow><mo>âˆ‘</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <msub><mi>a</mi> <mi>i</mi></msub></mrow></msqrt> <msqrt><mrow><mo>âˆ‘</mo><msub><mi>b</mi>
    <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub></mrow></msqrt></mrow></mfrac></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal c normal o normal s left-parenthesis bold a comma bold
    b right-parenthesis equals StartFraction bold a dot bold b Over StartAbsoluteValue
    EndAbsoluteValue bold a StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue
    EndAbsoluteValue bold b StartAbsoluteValue EndAbsoluteValue EndFraction equals
    StartFraction sigma-summation a Subscript i Baseline b Subscript i Baseline Over
    StartRoot sigma-summation a Subscript i Baseline a Subscript i Baseline EndRoot
    StartRoot sigma-summation b Subscript i Baseline b Subscript i Baseline EndRoot
    EndFraction"><mrow><mi>cos</mi> <mrow><mo>(</mo> <mi>ğš</mi> <mo>,</mo> <mi>ğ›</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>ğš</mi><mo>Â·</mo><mi>ğ›</mi></mrow>
    <mrow><mo>|</mo><mo>|</mo><mi>ğš</mi><mo>|</mo><mo>|</mo><mo>Â·</mo><mo>|</mo><mo>|</mo><mi>ğ›</mi><mo>|</mo><mo>|</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mo>âˆ‘</mo><msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi>
    <mi>i</mi></msub></mrow> <mrow><msqrt><mrow><mo>âˆ‘</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <msub><mi>a</mi> <mi>i</mi></msub></mrow></msqrt> <msqrt><mrow><mo>âˆ‘</mo><msub><mi>b</mi>
    <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub></mrow></msqrt></mrow></mfrac></mrow></math>
- en: 'Scikit-learn simplifies this calculation by offering a `cosine_similarity`
    utility function. Letâ€™s check the similarity of the first two sentences:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learné€šè¿‡æä¾›`cosine_similarity`å®ç”¨å‡½æ•°ç®€åŒ–äº†è¿™ä¸ªè®¡ç®—ã€‚è®©æˆ‘ä»¬æ¥æ£€æŸ¥å‰ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼š
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Out:`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Compared to our handmade similarity in the earlier sections, `cosine_similarity`
    offers some advantages, as it is properly normalized and can take only values
    between 0 and 1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ—©æœŸç« èŠ‚ä¸­æ‰‹å·¥ç›¸ä¼¼æ€§æ¯”è¾ƒèµ·æ¥ï¼Œ`cosine_similarity`æä¾›äº†ä¸€äº›ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒè¢«é€‚å½“åœ°æ ‡å‡†åŒ–ï¼Œå¹¶ä¸”å€¼åªèƒ½åœ¨0åˆ°1ä¹‹é—´ã€‚
- en: 'Calculating the similarity of all documents is of course also possible; scikit-learn
    has optimized the `cosine_similarity`, so it is possible to directly pass matrices:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼æ€§å½“ç„¶ä¹Ÿæ˜¯å¯èƒ½çš„ï¼›scikit-learnå·²ç»ä¼˜åŒ–äº†`cosine_similarity`ï¼Œå› æ­¤å¯ä»¥ç›´æ¥ä¼ é€’çŸ©é˜µï¼š
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '| Â  | 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Â  | 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 1.000000 | 0.833333 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.000000 | 0.833333 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
- en: '| 1 | 0.833333 | 1.000000 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.833333 | 1.000000 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
- en: '| 2 | 0.666667 | 0.666667 | 1.000000 | 0.833333 | 0.000000 | 0.000000 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.666667 | 0.666667 | 1.000000 | 0.833333 | 0.000000 | 0.000000 |'
- en: '| 3 | 0.666667 | 0.666667 | 0.833333 | 1.000000 | 0.000000 | 0.000000 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.666667 | 0.666667 | 0.833333 | 1.000000 | 0.000000 | 0.000000 |'
- en: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.524142 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.524142 |'
- en: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.524142 | 1.000000 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.524142 | 1.000000 |'
- en: Again, the matrix is symmetric with the highest values on the diagonal. Itâ€™s
    also easy to see that document pairs 0/1 and 2/3 are most similar. Documents 4/5
    have no similarity at all to the other documents but have some similarity to each
    other. Taking a look back at the sentences, this is exactly what one would expect.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼ŒçŸ©é˜µåœ¨å¯¹è§’çº¿ä¸Šå…·æœ‰æœ€é«˜å€¼æ˜¯å¯¹ç§°çš„ã€‚å¾ˆå®¹æ˜“çœ‹å‡ºæ–‡æ¡£å¯¹0/1å’Œ2/3æœ€ç›¸ä¼¼ã€‚æ–‡æ¡£4/5ä¸å…¶ä»–æ–‡æ¡£æ²¡æœ‰ä»»ä½•ç›¸ä¼¼æ€§ï¼Œä½†å®ƒä»¬å½¼æ­¤ä¹‹é—´æœ‰äº›ç›¸ä¼¼æ€§ã€‚å›é¡¾è¿™äº›å¥å­ï¼Œè¿™æ­£æ˜¯äººä»¬æ‰€æœŸæœ›çš„ã€‚
- en: Bag-of-words models are suitable for a variety of use cases. For classification,
    sentiment detection, and many topic models, they create a bias toward the most
    frequent words as they have the highest numbers in the document-term matrix. Often
    these words do not carry much meaning and could be defined as stop words.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è¢‹æ¨¡å‹é€‚ç”¨äºå„ç§ç”¨ä¾‹ã€‚å¯¹äºåˆ†ç±»ã€æƒ…æ„Ÿæ£€æµ‹å’Œè®¸å¤šä¸»é¢˜æ¨¡å‹ï¼Œå®ƒä»¬ä¼šåå‘äºæœ€é¢‘ç¹å‡ºç°çš„è¯è¯­ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ–‡æ¡£-è¯é¡¹çŸ©é˜µä¸­çš„æ•°å€¼æœ€é«˜ã€‚é€šå¸¸è¿™äº›è¯è¯­å¹¶ä¸å¸¦æœ‰å¤ªå¤šæ„ä¹‰ï¼Œå¯ä»¥å®šä¹‰ä¸ºåœç”¨è¯ã€‚
- en: As these would be highly domain-specific, a more generic approach â€œpunishesâ€
    words that appear too often in the corpus of all documents. This is called a *TF-IDF
    model* and will be discussed in the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¿™äº›æ–¹æ³•é«˜åº¦ä¾èµ–é¢†åŸŸç‰¹å®šï¼Œæ›´é€šç”¨çš„æ–¹æ³•ä¼šâ€œæƒ©ç½šâ€é‚£äº›åœ¨æ‰€æœ‰æ–‡æ¡£è¯­æ–™åº“ä¸­å‡ºç°å¤ªé¢‘ç¹çš„è¯è¯­ã€‚è¿™è¢«ç§°ä¸º*TF-IDFæ¨¡å‹*ï¼Œå°†åœ¨ä¸‹ä¸€èŠ‚è®¨è®ºã€‚
- en: TF-IDF Models
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDFæ¨¡å‹
- en: 'In our previous example, many sentences started with the words â€œit was the
    time of.â€ This contributed a lot to their similarity, but in reality, the actual
    information you get by the words is minimal. TF-IDF will take care of that by
    counting the number of total word occurrences. It will reduce weights of frequent
    words and at the same time increase the weights of uncommon words. Apart from
    the information-theoretical measure,^([5](ch05.xhtml#idm45634197672360)) this
    is also something that you can observe when reading documents: if you encounter
    uncommon words, it is likely that the author wants to convey an important message
    with them.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ä¸­ï¼Œè®¸å¤šå¥å­ä»¥â€œè¿™æ˜¯æ—¶å€™â€å¼€å¤´ã€‚è¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå¢åŠ äº†å®ƒä»¬çš„ç›¸ä¼¼æ€§ï¼Œä½†å®é™…ä¸Šï¼Œæ‚¨é€šè¿‡è¿™äº›è¯è·å¾—çš„å®é™…ä¿¡æ¯å¾ˆå°‘ã€‚TF-IDFé€šè¿‡è®¡ç®—æ€»è¯å‡ºç°æ¬¡æ•°æ¥å¤„ç†è¿™ä¸€ç‚¹ã€‚å®ƒä¼šå‡å°‘å¸¸è§è¯çš„æƒé‡ï¼ŒåŒæ—¶å¢åŠ ä¸å¸¸è§è¯çš„æƒé‡ã€‚é™¤äº†ä¿¡æ¯ç†è®ºçš„æµ‹é‡[^5]ä¹‹å¤–ï¼Œåœ¨é˜…è¯»æ–‡æ¡£æ—¶ï¼Œæ‚¨è¿˜å¯ä»¥è§‚å¯Ÿåˆ°ï¼šå¦‚æœé‡åˆ°ä¸å¸¸è§çš„è¯ï¼Œä½œè€…å¾ˆå¯èƒ½æƒ³è¦ç”¨å®ƒä»¬ä¼ è¾¾é‡è¦ä¿¡æ¯ã€‚
- en: Optimized Document Vectors with TfidfTransformer
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TfidfTransformerä¼˜åŒ–æ–‡æ¡£å‘é‡
- en: 'As we saw in [ChapterÂ 2](ch02.xhtml#ch-api), a better measure for information
    compared to counting is calculating the inverted document frequency and using
    a penalty for very common words. The TF-IDF weight can be calculated from the
    bag-of-words model. Letâ€™s try this again with the previous model and see how the
    weights of the document-term matrix change:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬åœ¨[ç¬¬2ç« ](ch02.xhtml#ch-api)ä¸­æ‰€è§ï¼Œä¸è®¡æ•°ç›¸æ¯”ï¼Œæ›´å¥½çš„ä¿¡æ¯è¡¡é‡æ–¹æ³•æ˜¯è®¡ç®—å€’æ’æ–‡æ¡£é¢‘ç‡ï¼Œå¹¶å¯¹éå¸¸å¸¸è§çš„å•è¯ä½¿ç”¨æƒ©ç½šã€‚TF-IDFæƒé‡å¯ä»¥ä»è¯è¢‹æ¨¡å‹è®¡ç®—å‡ºæ¥ã€‚è®©æˆ‘ä»¬å†æ¬¡å°è¯•ä½¿ç”¨å…ˆå‰çš„æ¨¡å‹ï¼Œçœ‹çœ‹æ–‡æ¡£-æœ¯è¯­çŸ©é˜µçš„æƒé‡å¦‚ä½•å˜åŒ–ï¼š
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`Out:`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_05in03.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_05in03.jpg)'
- en: 'As you can see, some words have been scaled to smaller values (like â€œitâ€),
    while others have not been scaled down so much (like â€œwisdomâ€). Letâ€™s see the
    effect on the similarity matrix:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€è§ï¼Œæœ‰äº›è¯å·²ç»è¢«ç¼©å°äº†ï¼ˆä¾‹å¦‚â€œitâ€ï¼‰ï¼Œè€Œå…¶ä»–è¯åˆ™æ²¡æœ‰è¢«ç¼©å°é‚£ä¹ˆå¤šï¼ˆä¾‹å¦‚â€œwisdomâ€ï¼‰ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¯¹ç›¸ä¼¼æ€§çŸ©é˜µçš„å½±å“ï¼š
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '| Â  | 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Â  | 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 1.000000 | 0.675351 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.000000 | 0.675351 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
- en: '| 1 | 0.675351 | 1.000000 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.675351 | 1.000000 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
- en: '| 2 | 0.457049 | 0.457049 | 1.000000 | 0.675351 | 0.00000 | 0.00000 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.457049 | 0.457049 | 1.000000 | 0.675351 | 0.00000 | 0.00000 |'
- en: '| 3 | 0.457049 | 0.457049 | 0.675351 | 1.000000 | 0.00000 | 0.00000 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.457049 | 0.457049 | 0.675351 | 1.000000 | 0.00000 | 0.00000 |'
- en: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.43076 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.43076 |'
- en: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.43076 | 1.000000 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.43076 | 1.000000 |'
- en: We get exactly the effect we have hoped for! Document pairs 0/1 and 2/3 are
    still very similar, but the number has also decreased to a more reasonable level
    as the document pairs differ in *significant words*. The more common words now
    have lower weights.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¡®å®è¾¾åˆ°äº†æœŸæœ›çš„æ•ˆæœï¼æ–‡æ¡£å¯¹0/1å’Œ2/3ä»ç„¶éå¸¸ç›¸ä¼¼ï¼Œä½†æ•°å­—ä¹Ÿå‡å°‘åˆ°ä¸€ä¸ªæ›´åˆç†çš„æ°´å¹³ï¼Œå› ä¸ºæ–‡æ¡£å¯¹åœ¨*é‡è¦è¯è¯­*ä¸Šæœ‰æ‰€ä¸åŒã€‚ç°åœ¨å¸¸è§è¯çš„æƒé‡è¾ƒä½ã€‚
- en: Introducing the ABC Dataset
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼•å…¥ABCæ•°æ®é›†
- en: 'As a real-word use case, we will take a [dataset from Kaggle](https://oreil.ly/hg5R3)
    that contains news headlines. Headlines originate from Australian news source
    ABC and are from 2003 to 2017\. The CSV file contains only a timestamp and the
    headline without punctuation in lowercase. We load the CSV file into a Pandas
    `DataFrame` and take a look at the first few documents:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºå®é™…çš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä»½æ¥è‡ªKaggleçš„[æ•°æ®é›†](https://oreil.ly/hg5R3)ï¼Œå…¶ä¸­åŒ…å«æ–°é—»æ ‡é¢˜ã€‚æ ‡é¢˜æºè‡ªæ¾³å¤§åˆ©äºšæ–°é—»æºABCï¼Œæ—¶é—´è·¨åº¦ä¸º2003è‡³2017å¹´ã€‚CSVæ–‡ä»¶åªåŒ…å«æ—¶é—´æˆ³å’Œæ ‡é¢˜ï¼Œæ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œä¸”å…¨éƒ¨å°å†™ã€‚æˆ‘ä»¬å°†CSVæ–‡ä»¶åŠ è½½åˆ°Pandasçš„`DataFrame`ä¸­ï¼Œå¹¶æŸ¥çœ‹å‰å‡ ä¸ªæ–‡æ¡£ï¼š
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Out:`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '| Â  | publish_date | headline_text |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Â  | å‘å¸ƒæ—¥æœŸ | æ–°é—»æ ‡é¢˜ |'
- en: '| --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 2003-02-19 | aba decides against community broadcasting lic... |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2003-02-19 | ABAå†³å®šä¸æˆäºˆç¤¾åŒºå¹¿æ’­è®¸å¯è¯... |'
- en: '| 1 | 2003-02-19 | act fire witnesses must be aware of defamation |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2003-02-19 | æ¾³å¤§åˆ©äºšACTå·çš„ç«ç¾ç›®å‡»è€…å¿…é¡»æ„è¯†åˆ°è¯½è°¤é—®é¢˜ |'
- en: '| 2 | 2003-02-19 | a g calls for infrastructure protection summit |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2003-02-19 | A Gå‘¼åä¸¾è¡ŒåŸºç¡€è®¾æ–½ä¿æŠ¤å³°ä¼š |'
- en: '| 3 | 2003-02-19 | air nz staff in aust strike for pay rise |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2003-02-19 | ç©ºä¸­æ–°è¥¿å…°å‘˜å·¥åœ¨æ¾³å¤§åˆ©äºšç½¢å·¥è¦æ±‚åŠ è–ª |'
- en: '| 4 | 2003-02-19 | air nz strike to affect australian travellers |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2003-02-19 | ç©ºä¸­æ–°è¥¿å…°ç½¢å·¥å°†å½±å“æ¾³å¤§åˆ©äºšæ—…å®¢ |'
- en: There are 1,103,663 Â headlines in this dataset. Note that the headlines do not
    include punctuation and are all transformed to lowercase. Apart from the text,
    the dataset includes the publication date of each headline.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤æ•°æ®é›†ä¸­æœ‰1,103,663ä¸ªæ ‡é¢˜ã€‚è¯·æ³¨æ„ï¼Œæ ‡é¢˜ä¸åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼Œå¹¶ä¸”å…¨éƒ¨è½¬æ¢ä¸ºå°å†™ã€‚é™¤äº†æ–‡æœ¬ä¹‹å¤–ï¼Œæ•°æ®é›†è¿˜åŒ…æ‹¬æ¯ä¸ªæ ‡é¢˜çš„å‡ºç‰ˆæ—¥æœŸã€‚
- en: As we saw earlier, the TF-IDF vectors can be calculated using the bag-of-words
    model (the *count vectors* in scikit-learn terminology). As it is so common to
    use TF-IDF document vectors, scikit-learn has created a â€œshortcutâ€ to skip the
    count vectors and directly calculate the TF-IDF vectors. The corresponding class
    is called `TfidfVectorizer`, and we will use it next.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œå¯ä»¥ä½¿ç”¨è¯è¢‹æ¨¡å‹ï¼ˆåœ¨scikit-learnæœ¯è¯­ä¸­çš„*è®¡æ•°å‘é‡*ï¼‰è®¡ç®—TF-IDFå‘é‡ã€‚ç”±äºä½¿ç”¨TF-IDFæ–‡æ¡£å‘é‡éå¸¸å¸¸è§ï¼Œå› æ­¤scikit-learnåˆ›å»ºäº†ä¸€ä¸ªâ€œå¿«æ·æ–¹å¼â€æ¥è·³è¿‡è®¡æ•°å‘é‡ï¼Œç›´æ¥è®¡ç®—TF-IDFå‘é‡ã€‚ç›¸åº”çš„ç±»ç§°ä¸º`TfidfVectorizer`ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹é¢ä½¿ç”¨å®ƒã€‚
- en: 'In the following, we have also combined the calls to fit and to transform in
    `fit_transform`, which is convenient:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„å†…å®¹ä¸­ï¼Œæˆ‘ä»¬è¿˜å°†`fit`å’Œ`transform`çš„è°ƒç”¨ç»„åˆæˆäº†`fit_transform`ï¼Œè¿™æ ·åšå¾ˆæ–¹ä¾¿ï¼š
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This might take a while, as so many documents have to be analyzed and vectorized.
    Take a look at the dimensions of the document-term matrix:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´ï¼Œå› ä¸ºéœ€è¦åˆ†æå’Œå‘é‡åŒ–è®¸å¤šæ–‡æ¡£ã€‚æŸ¥çœ‹æ–‡æ¡£-æœ¯è¯­çŸ©é˜µçš„ç»´åº¦ï¼š
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Out:`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The number of rows was expected, but the number of columns (the vocabulary)
    is really large, with almost 100,000 words. Doing the math shows that a naive
    storage of data would have led toÂ  1,103,663 * 95,878 elements with 8 bytes per
    float and have used roughly 788 GB RAM. This shows the incredible effectiveness
    of sparse matrices as the real memory used is â€œonlyâ€ 56,010,856 bytes (roughly
    0.056 GB; found out via `dt.data.nbytes`). Itâ€™s still a lot, but itâ€™s manageable.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œæ•°æ˜¯é¢„æœŸçš„ï¼Œä½†æ˜¯åˆ—æ•°ï¼ˆè¯æ±‡è¡¨ï¼‰éå¸¸å¤§ï¼Œå‡ ä¹æœ‰100,000ä¸ªå•è¯ã€‚é€šè¿‡ç®€å•çš„è®¡ç®—å¯ä»¥å¾—å‡ºï¼Œå¯¹æ•°æ®è¿›è¡Œå¤©çœŸçš„å­˜å‚¨ä¼šå¯¼è‡´1,103,663 * 95,878ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªæµ®ç‚¹æ•°ä½¿ç”¨8å­—èŠ‚ï¼Œå¤§çº¦ä½¿ç”¨788GBçš„RAMã€‚è¿™æ˜¾ç¤ºäº†ç¨€ç–çŸ©é˜µçš„ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æœ‰æ•ˆæ€§ï¼Œå› ä¸ºå®é™…ä½¿ç”¨çš„å†…å­˜åªæœ‰â€œä»…â€56,010,856å­—èŠ‚ï¼ˆå¤§çº¦0.056GBï¼›é€šè¿‡`dt.data.nbytes`æ‰¾åˆ°ï¼‰ã€‚è¿™ä»ç„¶å¾ˆå¤šï¼Œä½†æ˜¯å¯ä»¥ç®¡ç†ã€‚
- en: 'Calculating the similarity between two vectors is another story, though. Scikit-learn
    (and SciPy as a basis) is highly optimized for working with sparse vectors, but
    it still takes some time doing the sample calculation (similarities of the first
    10,000 documents):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œè®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§åˆæ˜¯å¦ä¸€å›äº‹äº†ã€‚Scikit-learnï¼ˆä»¥åŠå…¶åŸºç¡€SciPyï¼‰é’ˆå¯¹ç¨€ç–å‘é‡è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ï¼Œä½†æ˜¯è¿›è¡Œç¤ºä¾‹è®¡ç®—ï¼ˆå‰10,000ä¸ªæ–‡æ¡£çš„ç›¸ä¼¼æ€§ï¼‰ä»ç„¶éœ€è¦ä¸€äº›æ—¶é—´ï¼š
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`Out:`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: For machine learning in the next chapters, many of these linear algebra calculations
    are necessary and have to be repeated over and over. Often operations scale quadratically
    with the number of features (O(NÂ²)). Optimizing the vectorization by removing
    unnecessary features is therefore not only helpful for calculating the similarities
    but also crucial for scalable machine learning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­è¿›è¡Œæœºå™¨å­¦ä¹ æ—¶ï¼Œè®¸å¤šè¿™äº›çº¿æ€§ä»£æ•°è®¡ç®—æ˜¯å¿…è¦çš„ï¼Œå¹¶ä¸”å¿…é¡»ä¸€éåˆä¸€éåœ°é‡å¤ã€‚é€šå¸¸æ“ä½œéšç€ç‰¹å¾æ•°é‡å‘ˆäºŒæ¬¡æ–¹å¢é•¿ï¼ˆO(NÂ²)ï¼‰ã€‚ä¼˜åŒ–çŸ¢é‡åŒ–ï¼Œé€šè¿‡ç§»é™¤ä¸å¿…è¦çš„ç‰¹å¾ï¼Œä¸ä»…æœ‰åŠ©äºè®¡ç®—ç›¸ä¼¼æ€§ï¼Œè€Œä¸”å¯¹äºå¯æ‰©å±•çš„æœºå™¨å­¦ä¹ è‡³å…³é‡è¦ã€‚
- en: 'Blueprint: Reducing Feature Dimensions'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šé™ä½ç‰¹å¾ç»´åº¦
- en: We have now found features for our documents and used them to calculate document
    vectors. As we have seen in the example, the number of features can get quite
    large. Lots of machine learning algorithms are computationally intensive and scale
    with the number of features, often even polynomially. One part of feature engineering
    is therefore focused on reducing these features to the ones that are really necessary.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä¸ºæˆ‘ä»¬çš„æ–‡æ¡£æ‰¾åˆ°äº†ç‰¹å¾ï¼Œå¹¶ç”¨å®ƒä»¬æ¥è®¡ç®—æ–‡æ¡£å‘é‡ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¤ºä¾‹ä¸­çœ‹åˆ°çš„ï¼Œç‰¹å¾æ•°é‡å¯èƒ½ä¼šéå¸¸å¤§ã€‚è®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•éœ€è¦å¤§é‡è®¡ç®—ï¼Œå¹¶ä¸”éšç€ç‰¹å¾æ•°é‡çš„å¢åŠ è€Œæ‰©å±•ï¼Œé€šå¸¸ç”šè‡³æ˜¯å¤šé¡¹å¼çš„ã€‚å› æ­¤ï¼Œç‰¹å¾å·¥ç¨‹çš„ä¸€éƒ¨åˆ†ä¾§é‡äºå‡å°‘è¿™äº›çœŸæ­£å¿…è¦çš„ç‰¹å¾ã€‚
- en: In this section, we show a blueprint for how this can be achieved and measure
    their impact on the number of features.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å®ç°è¿™ä¸€ç‚¹çš„è“å›¾ï¼Œå¹¶è¡¡é‡äº†å®ƒä»¬å¯¹ç‰¹å¾æ•°é‡çš„å½±å“ã€‚
- en: Removing stop words
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç§»é™¤åœç”¨è¯
- en: In the first place, we can think about removing the words that carry the least
    meaning. Although this is domain-dependent, there are lists of the most common
    English words that common sense tells us can normally be neglected. These words
    are called *stop words*. Common stop words are determiners, auxiliary verbs, and
    pronouns. For a more detailed discussion, see [ChapterÂ 4](ch04.xhtml#ch-preparation).
    Be careful when removing stop words as they can contain certain words that might
    carry a domain-specific meaning in special texts!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘åˆ é™¤å…·æœ‰æœ€å°‘å«ä¹‰çš„è¯è¯­ã€‚å°½ç®¡è¿™å–å†³äºé¢†åŸŸï¼Œä½†é€šå¸¸æœ‰ä¸€äº›æœ€å¸¸è§çš„è‹±è¯­å•è¯åˆ—è¡¨ï¼Œå¸¸è¯†å‘Šè¯‰æˆ‘ä»¬é€šå¸¸å¯ä»¥å¿½ç•¥å®ƒä»¬ã€‚è¿™äº›è¯è¢«ç§°ä¸º*åœç”¨è¯*ã€‚å¸¸è§çš„åœç”¨è¯åŒ…æ‹¬å† è¯ã€åŠ©åŠ¨è¯å’Œä»£è¯ã€‚æœ‰å…³æ›´è¯¦ç»†çš„è®¨è®ºï¼Œè¯·å‚é˜…[ç¬¬4ç« ](ch04.xhtml#ch-preparation)ã€‚åœ¨åˆ é™¤åœç”¨è¯æ—¶è¦å°å¿ƒï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½åŒ…å«åœ¨ç‰¹æ®Šæ–‡æœ¬ä¸­å…·æœ‰ç‰¹å®šé¢†åŸŸå«ä¹‰çš„æŸäº›è¯è¯­ï¼
- en: This does not reduce the number of dimensions tremendously as there are only
    a few hundred common stop words in almost any language. However, it should drastically
    decrease the number of stored elements as stop words are so common. This leads
    to less memory consumption and faster calculations, as fewer numbers need to be
    multiplied.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå‡ ä¹ä»»ä½•è¯­è¨€ä¸­éƒ½æœ‰å‡ ç™¾ä¸ªå¸¸è§çš„åœç”¨è¯ï¼Œå› æ­¤è¿™å¹¶æ²¡æœ‰æå¤§åœ°å‡å°‘ç»´åº¦ã€‚ç„¶è€Œï¼Œç”±äºåœç”¨è¯éå¸¸å¸¸è§ï¼Œè¿™åº”è¯¥æ˜¾è‘—å‡å°‘å­˜å‚¨å…ƒç´ çš„æ•°é‡ã€‚è¿™å¯¼è‡´å†…å­˜æ¶ˆè€—æ›´å°‘ï¼Œå¹¶ä¸”è®¡ç®—é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºéœ€è¦ç›¸ä¹˜çš„æ•°å­—æ›´å°‘ã€‚
- en: 'Letâ€™s use the standard spaCy stop words and check the effects on the document-term
    matrix. Note that we pass stop words as a namedÂ parameter to the `TfidfVectorizer`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„ spaCy åœç”¨è¯ï¼Œå¹¶æ£€æŸ¥å¯¹æ–‡æ¡£-æœ¯è¯­çŸ©é˜µçš„å½±å“ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†åœç”¨è¯ä½œä¸ºå‘½åå‚æ•°ä¼ é€’ç»™ `TfidfVectorizer`ï¼š
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`Out:`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: With only 305 stop words, we managed to reduce the number of stored elements
    by 20%. The dimensions of the matrix are almost the same, with fewer columns due
    to the 95,878 â€“ 95,600 = 278 stop words that actually appeared in the headlines.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ä½¿ç”¨ 305 ä¸ªåœç”¨è¯ï¼Œæˆ‘ä»¬æˆåŠŸå°†å­˜å‚¨çš„å…ƒç´ æ•°é‡å‡å°‘äº† 20%ã€‚çŸ©é˜µçš„ç»´æ•°å‡ ä¹ç›¸åŒï¼Œä½†ç”±äºç¡®å®å‡ºç°åœ¨æ ‡é¢˜ä¸­çš„ 95,878 - 95,600 = 278
    ä¸ªåœç”¨è¯è¾ƒå°‘ï¼Œåˆ—æ•°æ›´å°‘ã€‚
- en: Minimum frequency
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€å°é¢‘ç‡
- en: Taking a look at the definition of the cosine similarity, we can easily see
    that components can contribute only if both vectors have a nonzero value at the
    corresponding index. This means that we can neglect all words occurring less than
    twice! `TfidfVectorizer` (and `CountVectorizer`) have a parameter for that called
    `min_df`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹ä½™å¼¦ç›¸ä¼¼åº¦çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°çœ‹åˆ°ï¼Œåªæœ‰å½“ä¸¤ä¸ªå‘é‡åœ¨ç›¸åº”ç´¢å¼•å¤„å…·æœ‰éé›¶å€¼æ—¶ï¼Œå®ƒä»¬çš„åˆ†é‡æ‰ä¼šæœ‰è´¡çŒ®ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å¿½ç•¥æ‰€æœ‰å‡ºç°å°‘äºä¸¤æ¬¡çš„è¯ï¼`TfidfVectorizer`ï¼ˆä»¥åŠ
    `CountVectorizer`ï¼‰æœ‰ä¸€ä¸ªç§°ä¸º `min_df` çš„å‚æ•°ã€‚
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`Out:`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Obviously, there are a lot of words appearing just once (95,600 â€“ 58,527 =
    37,073). Those words should also be stored only once; checking with the number
    of stored elements, we should get the same result: 5,644,186 â€“ 5,607,113 = 37,073\.
    Performing this kind of transformation, it is always useful to integrate such
    plausibility checks.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œæœ‰å¾ˆå¤šå•è¯ä»…å‡ºç°ä¸€æ¬¡ï¼ˆ95,600 - 58,527 = 37,073ï¼‰ã€‚è¿™äº›å•è¯ä¹Ÿåº”è¯¥åªå­˜å‚¨ä¸€æ¬¡ï¼›é€šè¿‡å­˜å‚¨å…ƒç´ æ•°é‡çš„æ£€æŸ¥ï¼Œæˆ‘ä»¬åº”è¯¥å¾—åˆ°ç›¸åŒçš„ç»“æœï¼š5,644,186
    - 5,607,113 = 37,073ã€‚åœ¨æ‰§è¡Œæ­¤ç±»è½¬æ¢æ—¶ï¼Œé›†æˆè¿™äº›åˆç†æ€§æ£€æŸ¥æ€»æ˜¯å¾ˆæœ‰ç”¨çš„ã€‚
- en: Losing Information
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸¢å¤±ä¿¡æ¯
- en: 'Be careful: by using `min_df=2`, we have not lost any information in vectorizing
    the headlines of this document corpus. If we plan to vectorize more documents
    later with the same vocabulary, we might lose information, as words appearing
    again in the new documents that were only present once in the original documents
    will not be found in the vocabulary.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šé€šè¿‡ä½¿ç”¨ `min_df=2`ï¼Œæˆ‘ä»¬åœ¨å‘é‡åŒ–æ­¤æ–‡æ¡£è¯­æ–™åº“çš„æ ‡é¢˜æ—¶æ²¡æœ‰ä¸¢å¤±ä»»ä½•ä¿¡æ¯ã€‚å¦‚æœæˆ‘ä»¬è®¡åˆ’ä»¥åç”¨ç›¸åŒçš„è¯æ±‡é‡å‘é‡åŒ–æ›´å¤šæ–‡æ¡£ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šä¸¢å¤±ä¿¡æ¯ï¼Œå› ä¸ºåœ¨åŸå§‹æ–‡æ¡£ä¸­ä»…å‡ºç°ä¸€æ¬¡çš„å•è¯ï¼Œåœ¨æ–°æ–‡æ¡£ä¸­å†æ¬¡å‡ºç°æ—¶ï¼Œå°†æ— æ³•åœ¨è¯æ±‡è¡¨ä¸­æ‰¾åˆ°ã€‚
- en: '`min_df` can also take float values. This means that a word has to occur in
    a minimum fraction of documents. Normally, this reduces the vocabulary drastically
    even for low numbers of `min_df`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_df` ä¹Ÿå¯ä»¥é‡‡ç”¨æµ®ç‚¹å€¼ã€‚è¿™æ„å‘³ç€ä¸€ä¸ªè¯å¿…é¡»åœ¨è‡³å°‘ä¸€éƒ¨åˆ†æ–‡æ¡£ä¸­å‡ºç°ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œå³ä½¿å¯¹äºè¾ƒä½çš„ `min_df` æ•°é‡ï¼Œè¿™ä¹Ÿä¼šæ˜¾è‘—å‡å°‘è¯æ±‡é‡ï¼š'
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Out:`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This transformation is probably too strict and reduces the vocabulary too far.
    Depending on the number of documents, you should set `min_df` to a low integer
    and check the effects on the vocabulary.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è½¬æ¢å¯èƒ½è¿‡äºä¸¥æ ¼ï¼Œå¯¼è‡´è¯æ±‡é‡è¿‡ä½ã€‚æ ¹æ®æ–‡æ¡£çš„æ•°é‡ï¼Œæ‚¨åº”å°† `min_df` è®¾ç½®ä¸ºä¸€ä¸ªè¾ƒä½çš„æ•´æ•°ï¼Œå¹¶æ£€æŸ¥å¯¹è¯æ±‡è¡¨çš„å½±å“ã€‚
- en: Maximum frequency
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æœ€å¤§é¢‘ç‡
- en: 'Sometimes a text corpus might have a special vocabulary with lots of repeating
    terms that are too specific to be contained in stop word lists. For this use case,
    scikit-learn offers the `max_df` parameter, which eliminates terms occurring too
    often in the corpus. Letâ€™s check how the dimensions are reduced when we eliminate
    all the words that appear in at least 10% of the headlines:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶æ–‡æœ¬è¯­æ–™åº“å¯èƒ½æœ‰ä¸€ä¸ªç‰¹æ®Šçš„è¯æ±‡è¡¨ï¼Œå…¶ä¸­æœ‰å¾ˆå¤šé‡å¤å‡ºç°çš„æœ¯è¯­ï¼Œè¿™äº›æœ¯è¯­å¤ªç‰¹å®šï¼Œä¸èƒ½åŒ…å«åœ¨åœç”¨è¯åˆ—è¡¨ä¸­ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œscikit-learn æä¾›äº†`max_df`å‚æ•°ï¼Œå¯ä»¥æ¶ˆé™¤è¯­æ–™åº“ä¸­è¿‡äºé¢‘ç¹å‡ºç°çš„æœ¯è¯­ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬æ¶ˆé™¤æ‰€æœ‰è‡³å°‘åœ¨
    10% çš„æ ‡é¢˜ä¸­å‡ºç°çš„è¯æ—¶ï¼Œç»´åº¦æ˜¯å¦‚ä½•å‡å°‘çš„ï¼š
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`Out:`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Setting `max_df` to a low value of 10% does not eliminate a single word!^([6](ch05.xhtml#idm45634197156184))
    Our news headlines are very diverse. Depending on the type of corpus you have,
    experimenting with `max_df` can be quite useful. In any case, you should always
    check how the dimensions change.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å°†`max_df`è®¾ç½®ä¸ºä½è‡³ 10% çš„å€¼å¹¶æ²¡æœ‰æ¶ˆé™¤ä»»ä½•ä¸€ä¸ªè¯ï¼^([6](ch05.xhtml#idm45634197156184))æˆ‘ä»¬çš„æ–°é—»æ ‡é¢˜éå¸¸å¤šæ ·åŒ–ã€‚æ ¹æ®æ‚¨æ‹¥æœ‰çš„è¯­æ–™åº“ç±»å‹ï¼Œå°è¯•ä½¿ç”¨`max_df`å¯èƒ½éå¸¸æœ‰ç”¨ã€‚æ— è®ºå¦‚ä½•ï¼Œæ‚¨éƒ½åº”è¯¥å§‹ç»ˆæ£€æŸ¥ç»´åº¦å¦‚ä½•å˜åŒ–ã€‚
- en: 'Blueprint: Improving Features by Making Them More Specific'
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šé€šè¿‡ä½¿ç‰¹å¾æ›´å…·ä½“æ¥æ”¹è¿›ç‰¹å¾
- en: So far, we have only used the original words of the headlines and reduced the
    number of dimensions by stop words and counting frequencies. We have not yet changed
    the features themselves. Using linguistic analysis, there are more possibilities.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªä½¿ç”¨äº†æ ‡é¢˜çš„åŸå§‹è¯ï¼Œå¹¶é€šè¿‡åœç”¨è¯å’Œé¢‘ç‡è®¡æ•°å‡å°‘äº†ç»´åº¦ã€‚æˆ‘ä»¬è¿˜æ²¡æœ‰æ”¹å˜ç‰¹å¾æœ¬èº«ã€‚é€šè¿‡è¯­è¨€åˆ†æï¼Œæœ‰æ›´å¤šçš„å¯èƒ½æ€§ã€‚
- en: Performing linguistic analysis
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿›è¡Œè¯­è¨€åˆ†æ
- en: 'Using spaCy, we can lemmatize all headlines and just keep the lemmas. This
    takes some time, but we expect to find a smaller vocabulary. First, we have to
    perform a linguistic analysis, which might take some time to finishÂ (see [ChapterÂ 4](ch04.xhtml#ch-preparation)Â for
    more details):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ spaCyï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ‰€æœ‰æ ‡é¢˜è¿›è¡Œè¯å½¢è¿˜åŸï¼Œå¹¶åªä¿ç•™è¯å½¢è¿˜åŸå½¢å¼ã€‚è¿™éœ€è¦ä¸€äº›æ—¶é—´ï¼Œä½†æˆ‘ä»¬é¢„è®¡ä¼šæ‰¾åˆ°ä¸€ä¸ªæ›´å°çš„è¯æ±‡è¡¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»è¿›è¡Œè¯­è¨€åˆ†æï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´æ‰èƒ½å®Œæˆï¼ˆå‚è§[ç¬¬
    4 ç« ](ch04.xhtml#ch-preparation)äº†è§£æ›´å¤šç»†èŠ‚ï¼‰ï¼š
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Blueprint: Using Lemmas Instead of Words for Vectorizing Documents'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šä½¿ç”¨è¯å½¢è¿˜åŸä»£æ›¿å•è¯è¿›è¡Œæ–‡æ¡£å‘é‡åŒ–
- en: 'Now, we can vectorize the data using the lemmas and see how the vocabulary
    decreased:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯å½¢è¿˜åŸå¯¹æ•°æ®è¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶æŸ¥çœ‹è¯æ±‡è¡¨çš„å‡å°‘æƒ…å†µï¼š
- en: '[PRE46]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '`Out:`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE47]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Saving almost 25,000 dimensions is a lot. In news headlines, lemmatizing the
    data probably does not lose any information. In other use cases like those in
    [ChapterÂ 11](ch11.xhtml#ch-sentiment), itâ€™s a completely different story.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚çœè¿‘ 25,000 ä¸ªç»´åº¦æ˜¯å¾ˆå¤šçš„ã€‚åœ¨æ–°é—»æ ‡é¢˜ä¸­ï¼Œå¯¹æ•°æ®è¿›è¡Œè¯å½¢è¿˜åŸå¯èƒ½ä¸ä¼šä¸¢å¤±ä»»ä½•ä¿¡æ¯ã€‚åœ¨å…¶ä»–ç”¨ä¾‹ä¸­ï¼Œæ¯”å¦‚[ç¬¬ 11 ç« ](ch11.xhtml#ch-sentiment)ï¼Œæƒ…å†µå®Œå…¨ä¸åŒã€‚
- en: 'Blueprint: Limit Word Types'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šé™åˆ¶è¯ç±»
- en: 'Using the data generated earlier, we can restrict ourselves to considering
    just nouns, adjectives, and verbs for the vectorization, as prepositions, conjugations,
    and so on are supposed to carry little meaning. This again reduces the vocabulary:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥é™åˆ¶è‡ªå·±åªè€ƒè™‘åè¯ã€å½¢å®¹è¯å’ŒåŠ¨è¯è¿›è¡Œå‘é‡åŒ–ï¼Œå› ä¸ºä»‹è¯ã€è¿è¯ç­‰è¢«è®¤ä¸ºå¸¦æœ‰å¾ˆå°‘çš„æ„ä¹‰ã€‚è¿™ä¼šå†æ¬¡å‡å°‘è¯æ±‡é‡ï¼š
- en: '[PRE48]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`Out:`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE49]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Thereâ€™s not much to win here, which is probably due to the headlines mainly
    containing nouns, adjectives, and verbs. But this might look totally different
    in your own projects. Depending on the type of texts you are analyzing, restricting
    word types will not only reduce the size of the vocabulary but will also lead
    to much lower noise. Itâ€™s a good idea to try this with a small part of the corpus
    first to avoid long waiting times due to the expensive linguistic analysis.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œå‡ ä¹æ²¡æœ‰ä»€ä¹ˆå¯ä»¥è·å¾—çš„ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºæ ‡é¢˜ä¸»è¦åŒ…å«åè¯ã€å½¢å®¹è¯å’ŒåŠ¨è¯ã€‚ä½†æ˜¯åœ¨æ‚¨è‡ªå·±çš„é¡¹ç›®ä¸­ï¼Œæƒ…å†µå¯èƒ½å®Œå…¨ä¸åŒã€‚æ ¹æ®æ‚¨åˆ†æçš„æ–‡æœ¬ç±»å‹ï¼Œé™åˆ¶è¯ç±»ä¸ä»…ä¼šå‡å°‘è¯æ±‡é‡ï¼Œè¿˜ä¼šå‡å°‘å™ªéŸ³ã€‚å»ºè®®å…ˆå°è¯•å¯¹è¯­æ–™åº“çš„ä¸€å°éƒ¨åˆ†è¿›è¡Œæ“ä½œï¼Œä»¥é¿å…ç”±äºæ˜‚è´µçš„è¯­è¨€åˆ†æè€Œå¯¼è‡´é•¿æ—¶é—´ç­‰å¾…ã€‚
- en: 'Blueprint: Remove Most Common Words'
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šç§»é™¤æœ€å¸¸è§çš„å•è¯
- en: As we learned, removing frequent words can lead to document-term matrices with
    far fewer entries. This is especially helpful when you perform unsupervised learning,
    as you will normally not be interested in common words that are common anyway.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘ä»¬çš„å­¦ä¹ ï¼Œå»é™¤é¢‘ç¹å‡ºç°çš„è¯å¯ä»¥å¯¼è‡´æ–‡æ¡£-è¯çŸ©é˜µçš„æ¡ç›®å¤§å¤§å‡å°‘ã€‚åœ¨è¿›è¡Œæ— ç›‘ç£å­¦ä¹ æ—¶å°¤å…¶æœ‰ç”¨ï¼Œå› ä¸ºé€šå¸¸ä¸ä¼šå¯¹å¸¸è§çš„ã€æ— è¶³è½»é‡çš„å¸¸ç”¨è¯æ„Ÿå…´è¶£ã€‚
- en: 'To remove even more noise, we will now try to eliminate the most common English
    words. Be careful, as there will normally also be words involved that might carry
    important meaning. There are various lists with those words; they can easily be
    found on the internet. The [list from Google](https://oreil.ly/bOho1) is rather
    popular and directly available on GitHub. Pandas can directly read the list if
    we tell it to be a CSV file without column headers. We will then instruct the
    `TfidfVectorizer` to use that list as stop words:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘å™ªéŸ³ï¼Œæˆ‘ä»¬ç°åœ¨å°è¯•æ¶ˆé™¤æœ€å¸¸è§çš„è‹±æ–‡å•è¯ã€‚è¯·æ³¨æ„ï¼Œé€šå¸¸è¿˜ä¼šæ¶‰åŠå¯èƒ½å…·æœ‰é‡è¦å«ä¹‰çš„å•è¯ã€‚æœ‰å„ç§å„æ ·çš„å•è¯åˆ—è¡¨ï¼›å®ƒä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨äº’è”ç½‘ä¸Šæ‰¾åˆ°ã€‚[æ¥è‡ªGoogleçš„åˆ—è¡¨](https://oreil.ly/bOho1)éå¸¸æµè¡Œï¼Œå¹¶ç›´æ¥å¯åœ¨GitHubä¸Šè·å–ã€‚Pandaså¯ä»¥ç›´æ¥è¯»å–è¯¥åˆ—è¡¨ï¼Œåªéœ€å‘Šè¯‰å®ƒæ˜¯ä¸€ä¸ªæ²¡æœ‰åˆ—æ ‡é¢˜çš„CSVæ–‡ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æŒ‡ç¤º`TfidfVectorizer`ä½¿ç”¨è¯¥åˆ—è¡¨ä½œä¸ºåœç”¨è¯ï¼š
- en: '[PRE50]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`Out:`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE51]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As you can see, the matrix now has 3.5 million fewer stored elements. The vocabulary
    shrunk by 68,426 â€“ 61,630 = 6,796 words, so more than 3,000 of the most frequent
    English words were not even used in the ABC headlines.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æ‚¨æ‰€è§ï¼ŒçŸ©é˜µç°åœ¨å‡å°‘äº†350ä¸‡ä¸ªå­˜å‚¨çš„å…ƒç´ ã€‚è¯æ±‡é‡å‡å°‘äº†68,426 - 61,630 = 6,796ä¸ªè¯ï¼Œå› æ­¤ABCæ ‡é¢˜ä¸­ç”šè‡³æœ‰è¶…è¿‡3,000ä¸ªæœ€å¸¸è§çš„è‹±æ–‡å•è¯æ²¡æœ‰è¢«ä½¿ç”¨ã€‚
- en: Removing frequent words is an excellent method to remove noise from the dataset
    and concentrate on the uncommon words. However, you should be careful using this
    from the beginning as even frequent words do have a meaning, and they might also
    have a special meaning in your document corpus. We recommend performing such analyses
    additionally but not exclusively.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ é™¤é¢‘ç¹å•è¯æ˜¯ä»æ•°æ®é›†ä¸­å»é™¤å™ªéŸ³å¹¶é›†ä¸­äºä¸å¸¸è§å•è¯çš„ä¼˜ç§€æ–¹æ³•ã€‚ä½†æ˜¯ï¼Œåˆšå¼€å§‹ä½¿ç”¨æ—¶åº”è¯¥å°å¿ƒï¼Œå› ä¸ºå³ä½¿é¢‘ç¹å•è¯ä¹Ÿæœ‰æ„ä¹‰ï¼Œå¹¶ä¸”å®ƒä»¬åœ¨æ‚¨çš„æ–‡æ¡£è¯­æ–™åº“ä¸­å¯èƒ½ä¹Ÿå…·æœ‰ç‰¹æ®Šå«ä¹‰ã€‚æˆ‘ä»¬å»ºè®®é¢å¤–æ‰§è¡Œè¿™æ ·çš„åˆ†æï¼Œä½†ä¸åº”ä»…é™äºæ­¤ã€‚
- en: 'Blueprint: Adding Context via N-Grams'
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šé€šè¿‡N-Gramsæ·»åŠ ä¸Šä¸‹æ–‡
- en: So far, we have used only single words as features (dimensions of our document
    vectors) as the basis for our vectorization. With this strategy, we have lost
    a lot of context information. Using single words as features does not respect
    the context in which the words appear. [In later chapters](ch10.xhtml#ch-embeddings)
    we will learn how to overcome that limitation with sophisticated models like word
    embeddings. In our current example, we will use a simpler method and take advantage
    of word combinations, so called *n-grams*. Two-word combinations are called *bigrams*;
    for three words, they are called *trigrams*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä»…ä½¿ç”¨å•è¯ä½œä¸ºç‰¹å¾ï¼ˆæˆ‘ä»¬æ–‡æ¡£å‘é‡çš„ç»´åº¦ï¼‰ï¼Œä½œä¸ºæˆ‘ä»¬å‘é‡åŒ–çš„åŸºç¡€ã€‚ä½¿ç”¨è¿™ç§ç­–ç•¥ï¼Œæˆ‘ä»¬å¤±å»äº†å¤§é‡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä½¿ç”¨å•è¯ä½œä¸ºç‰¹å¾ä¸å°Šé‡å•è¯å‡ºç°ä¸Šä¸‹æ–‡ã€‚[åœ¨åç»­ç« èŠ‚](ch10.xhtml#ch-embeddings)ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•é€šè¿‡åƒè¯åµŒå…¥è¿™æ ·çš„å¤æ‚æ¨¡å‹å…‹æœè¿™ç§é™åˆ¶ã€‚åœ¨æˆ‘ä»¬å½“å‰çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ç§æ›´ç®€å•çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨å•è¯ç»„åˆï¼Œå³æ‰€è°“çš„*n-grams*ã€‚ä¸¤ä¸ªè¯çš„ç»„åˆç§°ä¸º*bigrams*ï¼›ä¸‰ä¸ªè¯çš„ç»„åˆç§°ä¸º*trigrams*ã€‚
- en: 'Fortunately, `CountVectorizer` and `TfidfVectorizer` have the corresponding
    options. Contrary to the last few sections where we tried to reduce the vocabulary,
    we now enhance the vocabulary with word combinations. There are many such combinations;
    their number (and vocabulary size) is growing almost exponentially with *n*.^([7](ch05.xhtml#idm45634196754328))
    We will therefore be careful and start with bigrams:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œ`CountVectorizer`å’Œ`TfidfVectorizer`å…·æœ‰ç›¸åº”çš„é€‰é¡¹ã€‚ä¸å‰å‡ èŠ‚è¯•å›¾å‡å°‘è¯æ±‡é‡çš„åšæ³•ç›¸åï¼Œæˆ‘ä»¬ç°åœ¨é€šè¿‡è¯ç»„å¢å¼ºè¯æ±‡é‡ã€‚æœ‰è®¸å¤šè¿™æ ·çš„ç»„åˆï¼›å®ƒä»¬çš„æ•°é‡ï¼ˆä»¥åŠè¯æ±‡é‡ï¼‰å‡ ä¹ä¸*n*çš„æŒ‡æ•°çº§å¢é•¿ã€‚^([7](ch05.xhtml#idm45634196754328))
    å› æ­¤ï¼Œæˆ‘ä»¬è¦å°å¿ƒï¼Œå¹¶ä»bigramså¼€å§‹ï¼š
- en: '[PRE52]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`Out:`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE53]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Increasing the feature dimensions from 95,600 to 2,335,132 or even 5,339,558
    is quite painful even though the RAM size has not grown too much. For some tasks
    that need context-specific information (like sentiment analysis), n-grams are
    extremely useful. It is always useful to keep an eye on the dimensions, though.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡RAMå¤§å°å¹¶æ²¡æœ‰å¢åŠ å¤ªå¤šï¼Œä½†å°†ç‰¹å¾ç»´åº¦ä»95,600å¢åŠ åˆ°2,335,132ç”šè‡³5,339,558æ˜¯ç›¸å½“ç—›è‹¦çš„ã€‚å¯¹äºæŸäº›éœ€è¦ç‰¹å®šä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä»»åŠ¡ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰ï¼Œn-gramséå¸¸æœ‰ç”¨ã€‚ä½†æ˜¯ï¼Œå§‹ç»ˆæ³¨æ„ç»´åº¦æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚
- en: 'Combining n-grams with linguistic features and common words is also possible
    and reduces the vocabulary size considerably:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜å¯ä»¥å°†n-gramsä¸è¯­è¨€ç‰¹å¾å’Œå¸¸è§å•è¯ç»“åˆèµ·æ¥ï¼Œå¤§å¤§å‡å°‘è¯æ±‡é‡ï¼š
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`Out:`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE55]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Scikit-learn offers many different vectorizers. Normally, starting with `TfidfVectorizer`
    is a good idea, as it is one of the most versatile.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learnæä¾›äº†è®¸å¤šä¸åŒçš„å‘é‡åŒ–å™¨ã€‚é€šå¸¸ï¼Œä»`TfidfVectorizer`å¼€å§‹æ˜¯ä¸ªä¸é”™çš„ä¸»æ„ï¼Œå› ä¸ºå®ƒæ˜¯æœ€å¤šæ‰å¤šè‰ºçš„ä¹‹ä¸€ã€‚
- en: Options of TfidfVectorizer
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TfidfVectorizerçš„é€‰é¡¹
- en: TF-IDF can even be switched off so there is a seamless fallback to `CountVectorizer`.
    Because of the many parameters, it can take some time to find the perfect set
    of options.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDFç”šè‡³å¯ä»¥å…³é—­ï¼Œå› æ­¤å¯ä»¥æ— ç¼åˆ‡æ¢åˆ°`CountVectorizer`ã€‚ç”±äºå‚æ•°ä¼—å¤šï¼Œæ‰¾åˆ°å®Œç¾çš„é€‰é¡¹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚
- en: Finding the correct set of features is often tedious and requires experimentation
    with the (many) parameters of `TfidfVectorizer`, like `min_df`, `max_df`, or simplified
    text via NLP. In our work, we have had good experiences with setting `min_df`
    to `5`, for example, and `max_df` to `0.7`. In the end, this time is excellently
    invested as the results will depend heavily on correct vectorization. There is
    no golden bullet, though, and this *feature engineering* depends heavily on the
    use case and the planned use of the vectors.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°æ­£ç¡®çš„ç‰¹å¾é›†é€šå¸¸æ˜¯ä¹å‘³çš„ï¼Œå¹¶éœ€è¦é€šè¿‡`TfidfVectorizer`çš„ï¼ˆè®¸å¤šï¼‰å‚æ•°è¿›è¡Œå®éªŒï¼Œå¦‚`min_df`ã€`max_df`æˆ–é€šè¿‡NLPç®€åŒ–æ–‡æœ¬ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å·²ç»é€šè¿‡å°†`min_df`è®¾ç½®ä¸º`5`å’Œ`max_df`è®¾ç½®ä¸º`0.7`è·å¾—äº†è‰¯å¥½çš„ç»éªŒã€‚æœ€ç»ˆï¼Œè¿™äº›æ—¶é—´çš„æŠ•èµ„æ˜¯éå¸¸å€¼å¾—çš„ï¼Œå› ä¸ºç»“æœå°†ä¸¥é‡ä¾èµ–äºæ­£ç¡®çš„å‘é‡åŒ–ã€‚ç„¶è€Œï¼Œå¹¶æ²¡æœ‰é‡‘å¼¹ï¼Œè¿™ç§*ç‰¹å¾å·¥ç¨‹*ä¸¥é‡ä¾èµ–äºä½¿ç”¨æƒ…å†µå’Œå‘é‡è®¡åˆ’ä½¿ç”¨ã€‚
- en: The TF-IDF method itself can be improved by using a subnormal term frequency
    or normalizing the resulting vectors. The latter is useful for quickly calculating
    similarities, and we demonstrate its use later in the chapter. The former is mainly
    interesting for long documents to avoid repeating words getting a too high-weight.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDFæ–¹æ³•æœ¬èº«å¯ä»¥é€šè¿‡ä½¿ç”¨æ¬¡æ­£å¸¸æœ¯è¯­é¢‘ç‡æˆ–å½’ä¸€åŒ–æ‰€å¾—åˆ°çš„å‘é‡æ¥æ”¹è¿›ã€‚åè€…å¯¹äºå¿«é€Ÿè®¡ç®—ç›¸ä¼¼æ€§éå¸¸æœ‰ç”¨ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢æ¼”ç¤ºå…¶ä½¿ç”¨ã€‚å‰è€…ä¸»è¦é€‚ç”¨äºé•¿æ–‡æ¡£ï¼Œä»¥é¿å…é‡å¤å•è¯è·å¾—è¿‡é«˜çš„æƒé‡ã€‚
- en: Think very carefully about feature dimensions
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: éå¸¸ä»”ç»†åœ°è€ƒè™‘ç‰¹å¾ç»´åº¦
- en: In our previous examples, we have used single words and bigrams as features.
    Depending on the use case, this might already be enough. This works well for texts
    with common vocabulary, like news. But often you will encounter special vocabularies
    (for example, scientific publications or letters to an insurance company), which
    will require more sophisticated feature engineering.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ä»¥å‰çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å•è¯å’ŒäºŒå…ƒç»„ä½œä¸ºç‰¹å¾ã€‚æ ¹æ®ä½¿ç”¨æƒ…å†µï¼Œè¿™å¯èƒ½å·²ç»è¶³å¤Ÿäº†ã€‚è¿™å¯¹äºåƒæ–°é—»è¿™æ ·æœ‰å¸¸è§è¯æ±‡çš„æ–‡æœ¬æ•ˆæœå¾ˆå¥½ã€‚ä½†æ˜¯ï¼Œæ‚¨ç»å¸¸ä¼šé‡åˆ°ç‰¹æ®Šè¯æ±‡çš„æƒ…å†µï¼ˆä¾‹å¦‚ç§‘å­¦å‡ºç‰ˆç‰©æˆ–å†™ç»™ä¿é™©å…¬å¸çš„ä¿¡å‡½ï¼‰ï¼Œè¿™å°†éœ€è¦æ›´å¤æ‚çš„ç‰¹å¾å·¥ç¨‹ã€‚
- en: Keep number of dimensions in mind
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è¦ç‰¢è®°ç»´åº¦çš„æ•°é‡ã€‚
- en: As we have seen, using parameters like `ngram_range` can lead to large feature
    spaces. Apart from the RAM usage, this will also be a problem for many machine
    learning algorithms due to overfitting. Therefore, itâ€™s a good idea to always
    consider the (increase of) feature dimensions when changing parameters or vectorization
    methods.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œä½¿ç”¨è¯¸å¦‚`ngram_range`ä¹‹ç±»çš„å‚æ•°å¯èƒ½ä¼šå¯¼è‡´å¤§çš„ç‰¹å¾ç©ºé—´ã€‚é™¤äº†RAMä½¿ç”¨æƒ…å†µå¤–ï¼Œè¿™ä¹Ÿå°†æˆä¸ºè®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•çš„é—®é¢˜ï¼Œå› ä¸ºä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚å› æ­¤ï¼Œå½“æ›´æ”¹å‚æ•°æˆ–å‘é‡åŒ–æ–¹æ³•æ—¶ï¼Œå§‹ç»ˆè€ƒè™‘ï¼ˆå¢åŠ ï¼‰ç‰¹å¾ç»´åº¦æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚
- en: Syntactic Similarity in the ABC Dataset
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ABCæ•°æ®é›†ä¸­çš„è¯­æ³•ç›¸ä¼¼æ€§
- en: Similarity is one of the most basic concepts in machine learning and text analytics.
    In this section, we take a look at some challenging problems finding similar documents
    in the ABC dataset.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸ä¼¼æ€§æ˜¯æœºå™¨å­¦ä¹ å’Œæ–‡æœ¬åˆ†æä¸­æœ€åŸºæœ¬çš„æ¦‚å¿µä¹‹ä¸€ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹ä¸€äº›åœ¨ABCæ•°æ®é›†ä¸­æ‰¾åˆ°ç›¸ä¼¼æ–‡æ¡£çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚
- en: After taking a look at possible vectorizations in the previous section, we will
    now use one of them to calculate the similarities. We will present a blueprint
    to show how you can perform these calculations efficiently from both a CPU and
    a RAM perspective. As we are handling large amounts of data here, we have to make
    extensive use of the [NumPy library](https://numpy.org).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€èŠ‚ä¸­æŸ¥çœ‹å¯èƒ½çš„å‘é‡åŒ–ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨å…¶ä¸­çš„ä¸€ç§æ–¹æ³•æ¥è®¡ç®—ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬å°†æä¾›ä¸€ä¸ªè“å›¾ï¼Œå±•ç¤ºå¦‚ä½•ä»CPUå’ŒRAMçš„è§’åº¦é«˜æ•ˆæ‰§è¡Œè¿™äº›è®¡ç®—ã€‚ç”±äºæˆ‘ä»¬å¤„ç†å¤§é‡æ•°æ®ï¼Œå› æ­¤å¿…é¡»å¹¿æ³›ä½¿ç”¨[NumPyåº“](https://numpy.org)ã€‚
- en: 'In the first step, we vectorize the data using stop words and bigrams:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åœç”¨è¯å’ŒäºŒå…ƒç»„å¯¹æ•°æ®è¿›è¡Œå‘é‡åŒ–ï¼š
- en: '[PRE56]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We are now ready to use these vectors for our blueprints.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å‘é‡ç”¨äºæˆ‘ä»¬çš„è“å›¾ã€‚
- en: 'Blueprint: Finding Most Similar Headlines to a Made-up Headline'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šæŸ¥æ‰¾æœ€æ¥è¿‘è™šæ„æ ‡é¢˜çš„æ ‡é¢˜
- en: 'Letâ€™s say we want to find a headline in our data that most closely matches
    a headline that we remember, but only roughly. This is quite easy to solve, as
    we just have to vectorize our new document:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³è¦åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸æˆ‘ä»¬è®°å¾—çš„æ ‡é¢˜æœ€æ¥è¿‘çš„æ ‡é¢˜ï¼Œä½†åªæ˜¯ç²—ç•¥åœ°ã€‚è¿™å¾ˆå®¹æ˜“è§£å†³ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€å¯¹æˆ‘ä»¬çš„æ–°æ–‡æ¡£è¿›è¡Œå‘é‡åŒ–ï¼š
- en: '[PRE57]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now we have to calculate the cosine similarity to each headline in the corpus.
    We could implement this in a loop, but itâ€™s easier with the `cosine_similarity`
    function of scikit-learn:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¿…é¡»è®¡ç®—ä¸è¯­æ–™åº“ä¸­æ¯ä¸ªæ ‡é¢˜çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å¾ªç¯æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä½†ä½¿ç”¨scikit-learnçš„`cosine_similarity`å‡½æ•°ä¼šæ›´å®¹æ˜“ï¼š
- en: '[PRE58]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The result is a â€œnumber of headlines in the corpusâ€ Ã— 1 matrix, where each
    number represents the similarity to a document in the corpus. Using `np.argmax`
    gives us the index of the most similar document:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ä¸€ä¸ªâ€œè¯­æ–™åº“ä¸­çš„å¤´æ¡æ•°é‡â€ Ã— 1 çŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸ªæ•°å­—è¡¨ç¤ºä¸è¯­æ–™åº“ä¸­æ–‡æ¡£çš„ç›¸ä¼¼æ€§ã€‚ä½¿ç”¨`np.argmax`ç»™å‡ºæœ€ç›¸ä¼¼æ–‡æ¡£çš„ç´¢å¼•ï¼š
- en: '[PRE59]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`Out:`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE60]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: No *sizes* of apples and no *Australia* are present in the most similar headline,
    but it definitely bears some similarity with our invented headline.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç›¸ä¼¼çš„å¤´æ¡ä¸­æ²¡æœ‰*è‹¹æœçš„å¤§å°*å’Œ*æ¾³å¤§åˆ©äºš*ï¼Œä½†å®ƒç¡®å®ä¸æˆ‘ä»¬è™šæ„çš„å¤´æ¡æœ‰äº›ç›¸ä¼¼ã€‚
- en: 'Blueprint: Finding the Two Most Similar Documents in a Large Corpus (Much More
    Difficult)'
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šåœ¨å¤§å‹è¯­æ–™åº“ä¸­æ‰¾åˆ°ä¸¤ä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£ï¼ˆæ›´åŠ å›°éš¾ï¼‰
- en: When handling a corpus of many documents, you might often be asked questions
    such as â€œAre there duplicates?â€ or â€œHas this been mentioned before?â€ They all
    boil down to finding the most similar (maybe even identical) documents in the
    corpus. We will explain how to accomplish this and again use the ABC dataset as
    our example. The number of headlines will turn out to be a challenge.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å¤„ç†å¤šä¸ªæ–‡æ¡£çš„è¯­æ–™åº“æ—¶ï¼Œæ‚¨å¯èƒ½ç»å¸¸ä¼šè¢«é—®åˆ°â€œæ˜¯å¦æœ‰é‡å¤ï¼Ÿâ€æˆ–â€œè¿™ä¹‹å‰æåˆ°è¿‡å—ï¼Ÿâ€ è¿™äº›é—®é¢˜éƒ½å½’ç»“ä¸ºåœ¨è¯­æ–™åº“ä¸­æŸ¥æ‰¾æœ€ç›¸ä¼¼ï¼ˆç”šè‡³å¯èƒ½æ˜¯å®Œå…¨ç›¸åŒï¼‰çš„æ–‡æ¡£ã€‚æˆ‘ä»¬å°†è§£é‡Šå¦‚ä½•å®ç°è¿™ä¸€ç‚¹ï¼Œå¹¶å†æ¬¡ä»¥æˆ‘ä»¬çš„ç¤ºä¾‹æ•°æ®é›†ABCæ¥è¯´æ˜ã€‚å¤´æ¡çš„æ•°é‡å°†è¢«è¯æ˜æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚
- en: You might think that finding the most similar documents in the corpus is as
    easy as calculating the `cosine_similarity` between all documents. However, this
    is not possible as 1,103,663 Ã— 1,103,663 = 1,218,072,017,569\. More than one trillion
    elements do not fit in the RAM of even the most advanced computers. It is perfectly
    possible to perform the necessary matrix multiplications without having to wait
    for ages.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯èƒ½ä¼šè®¤ä¸ºåœ¨è¯­æ–™åº“ä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æ¡£å°±åƒè®¡ç®—æ‰€æœ‰æ–‡æ¡£ä¹‹é—´çš„`cosine_similarity`ä¸€æ ·ç®€å•ã€‚ä½†æ˜¯ï¼Œè¿™æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸º 1,103,663
    Ã— 1,103,663 = 1,218,072,017,569ã€‚å³ä½¿æ˜¯æœ€å…ˆè¿›çš„è®¡ç®—æœºçš„ RAM ä¹Ÿæ— æ³•å®¹çº³è¶…è¿‡ä¸€ä¸‡äº¿ä¸ªå…ƒç´ ã€‚å®Œå…¨å¯ä»¥æ‰§è¡Œæ‰€éœ€çš„çŸ©é˜µä¹˜æ³•ï¼Œè€Œæ— éœ€ç­‰å¾…å¤ªé•¿æ—¶é—´ã€‚
- en: Clearly, this problem needs optimization. As text analytics often has to cope
    with many documents, this is a very typical challenge. Often, the first optimization
    is to take an intensive look at all the needed numbers. We can easily observe
    that the document similarity relation is symmetrical and normalized.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œè¿™ä¸ªé—®é¢˜éœ€è¦ä¼˜åŒ–ã€‚ç”±äºæ–‡æœ¬åˆ†æç»å¸¸éœ€è¦å¤„ç†è®¸å¤šæ–‡æ¡£ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å…¸å‹çš„æŒ‘æˆ˜ã€‚é€šå¸¸ï¼Œç¬¬ä¸€ä¸ªä¼˜åŒ–æ­¥éª¤æ˜¯æ·±å…¥ç ”ç©¶æ‰€æœ‰éœ€è¦çš„æ•°å­—ã€‚æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°è§‚å¯Ÿåˆ°æ–‡æ¡£ç›¸ä¼¼æ€§å…³ç³»æ˜¯å¯¹ç§°å’Œæ ‡å‡†åŒ–çš„ã€‚
- en: In other words, we just need to calculate the subdiagonal elements of the similarity
    matrix ([FigureÂ 5-1](#btap_0501))
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åªéœ€è¦è®¡ç®—ç›¸ä¼¼çŸ©é˜µçš„æ¬¡å¯¹è§’çº¿å…ƒç´ ï¼ˆ[å›¾5-1](#btap_0501)ï¼‰
- en: '![](Images/btap_0501.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0501.jpg)'
- en: Figure 5-1\. Elements that need to be calculated in the similarity matrix. Only
    the elements below the diagonal need to be calculated, as their numbers are identical
    to the ones mirrored on the diagonal. The diagonal elements are all 1.
  id: totrans-282
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾5-1\. éœ€è¦åœ¨ç›¸ä¼¼çŸ©é˜µä¸­è®¡ç®—çš„å…ƒç´ ã€‚åªæœ‰å¯¹è§’çº¿ä»¥ä¸‹çš„å…ƒç´ éœ€è¦è®¡ç®—ï¼Œå› ä¸ºå®ƒä»¬çš„æ•°é‡ä¸å¯¹è§’çº¿ä¸Šçš„å…ƒç´ é•œåƒç›¸åŒã€‚å¯¹è§’çº¿ä¸Šçš„å…ƒç´ éƒ½æ˜¯ 1ã€‚
- en: This reduces the number of elements to 1,103,663 Ã— 1,103,662 / 2 = 609,035,456,953,
    which could be calculated in loop iterations, keeping only the most similar documents.
    However, calculating all these elements separately is not a good option, as the
    necessary Python loops (where each iteration calculates just a single matrix element)
    will eat up a lot of CPU performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†æŠŠå…ƒç´ æ•°é‡å‡å°‘åˆ° 1,103,663 Ã— 1,103,662 / 2 = 609,035,456,953ï¼Œå¯ä»¥åœ¨å¾ªç¯è¿­ä»£ä¸­è®¡ç®—ï¼Œå¹¶ä¿ç•™åªæœ‰æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚ç„¶è€Œï¼Œå•ç‹¬è®¡ç®—æ‰€æœ‰è¿™äº›å…ƒç´ å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½é€‰æ‹©ï¼Œå› ä¸ºå¿…è¦çš„
    Python å¾ªç¯ï¼ˆæ¯æ¬¡è¿­ä»£ä»…è®¡ç®—ä¸€ä¸ªçŸ©é˜µå…ƒç´ ï¼‰ä¼šæ¶ˆè€—å¤§é‡ CPU æ€§èƒ½ã€‚
- en: Instead of calculating individual elements of the similarity matrix, we divide
    the problem into different blocks and calculate 10,000 Ã— 10,000 similarity submatrices^([8](ch05.xhtml#idm45634196311384))Â 
    at once by taking blocks of 10,000 TF-IDF vectors from the document matrix. Each
    of these matrices contains 100,000,000 similarities, which will still fit in RAM.
    Of course, this leads to calculating too many elements, and we have to perform
    this for 111 Ã— 110 / 2 = 6,105 submatrices (see [FigureÂ 5-2](#btap_0502)).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ˜¯è®¡ç®—ç›¸ä¼¼çŸ©é˜µçš„å„ä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬å°†é—®é¢˜åˆ†æˆä¸åŒçš„å—ï¼Œå¹¶ä¸€æ¬¡è®¡ç®— 10,000 Ã— 10,000 ä¸ª TF-IDF å‘é‡çš„ç›¸ä¼¼å­çŸ©é˜µ^([8](ch05.xhtml#idm45634196311384))ã€‚æ¯ä¸ªè¿™æ ·çš„çŸ©é˜µåŒ…å«
    100,000,000 ä¸ªç›¸ä¼¼åº¦ï¼Œä»ç„¶é€‚åˆåœ¨ RAM ä¸­ã€‚å½“ç„¶ï¼Œè¿™ä¼šå¯¼è‡´è®¡ç®—å¤ªå¤šå…ƒç´ ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹ 111 Ã— 110 / 2 = 6,105 ä¸ªå­çŸ©é˜µæ‰§è¡Œæ­¤æ“ä½œï¼ˆå‚è§[å›¾5-2](#btap_0502)ï¼‰ã€‚
- en: From the previous section, we know that iteration takes roughly 500 ms to calculate.
    Another advantage of this approach is that leveraging data locality gives us a
    bigger chance of having the necessary matrix elements already in the CPU cache.
    We estimate that everything should run in about 3,000 seconds, which is roughly
    one hour.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‰é¢çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬çŸ¥é“è¿­ä»£å¤§çº¦éœ€è¦ 500 æ¯«ç§’æ¥è®¡ç®—ã€‚è¿™ç§æ–¹æ³•çš„å¦ä¸€ä¸ªä¼˜ç‚¹æ˜¯åˆ©ç”¨æ•°æ®å±€éƒ¨æ€§ï¼Œä½¿æˆ‘ä»¬æ›´æœ‰å¯èƒ½åœ¨ CPU ç¼“å­˜ä¸­æ‹¥æœ‰å¿…è¦çš„çŸ©é˜µå…ƒç´ ã€‚æˆ‘ä»¬ä¼°è®¡ä¸€åˆ‡åº”è¯¥åœ¨å¤§çº¦
    3,000 ç§’å†…å®Œæˆï¼Œå¤§çº¦ç›¸å½“äºä¸€å°æ—¶ã€‚
- en: '![](Images/btap_0502.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0502.jpg)'
- en: Figure 5-2\. Dividing the matrix into submatrices, which we can calculate more
    easily; the problem is divided into blocks (here, 4 Ã— 4), and the white and diagonal
    elements within the blocks are redundantly calculated.
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 5-2\. å°†çŸ©é˜µåˆ†æˆå­çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥æ›´è½»æ¾åœ°è®¡ç®—ï¼›é—®é¢˜è¢«åˆ†æˆå—ï¼ˆè¿™é‡Œæ˜¯ 4 Ã— 4ï¼‰ï¼Œå—å†…çš„ç™½è‰²å’Œå¯¹è§’çº¿å…ƒç´ åœ¨è®¡ç®—æ—¶æ˜¯å†—ä½™çš„ã€‚
- en: Can we improve this? Yes, indeed another speedup of a factor of 10 is actually
    possible. This works by normalizing the TF-IDF vectors via the corresponding option
    of `TfidfVectorizer`. Afterward, the similarity can be calculated with `np.dot`:^([9](ch05.xhtml#idm45634196279640))
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦æ”¹è¿›è¿™ä¸€ç‚¹ï¼Ÿæ˜¯çš„ï¼Œäº‹å®ä¸Šï¼Œå¯ä»¥å®ç°å¦ä¸€ä¸ª 10 å€çš„åŠ é€Ÿã€‚è¿™é€šè¿‡ä½¿ç”¨ `TfidfVectorizer` çš„ç›¸åº”é€‰é¡¹æ¥å¯¹ TF-IDF å‘é‡è¿›è¡Œå½’ä¸€åŒ–æ¥å®ç°ã€‚ä¹‹åï¼Œå¯ä»¥ä½¿ç”¨
    `np.dot` è®¡ç®—ç›¸ä¼¼æ€§ï¼š^([9](ch05.xhtml#idm45634196279640))
- en: '[PRE61]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`Out:`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In each iteration we save the most similar documents and their similarity and
    adjust them during the iterations. To skip identical documents (or more precisely,
    documents with identical document vectors), we only consider similarities < 0.9999\.
    As it turns out, using `<` relations with a sparse matrix is extremely inefficient,
    as all non-existent elements are supposed to be 0\. Therefore, we must be creative
    and find another way:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬ä¿å­˜æœ€ç›¸ä¼¼çš„æ–‡æ¡£åŠå…¶ç›¸ä¼¼æ€§ï¼Œå¹¶åœ¨è¿­ä»£è¿‡ç¨‹ä¸­è¿›è¡Œè°ƒæ•´ã€‚ä¸ºäº†è·³è¿‡ç›¸åŒçš„æ–‡æ¡£ï¼ˆæˆ–æ›´ç²¾ç¡®åœ°è¯´ï¼Œå…·æœ‰ç›¸åŒæ–‡æ¡£å‘é‡çš„æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬åªè€ƒè™‘ç›¸ä¼¼æ€§ `<
    0.9999`ã€‚äº‹å®è¯æ˜ï¼Œåœ¨ç¨€ç–çŸ©é˜µä¸­ä½¿ç”¨ `<` å…³ç³»æ˜¯æå…¶ä½æ•ˆçš„ï¼Œå› ä¸ºæ‰€æœ‰ä¸å­˜åœ¨çš„å…ƒç´ éƒ½è¢«å‡å®šä¸º 0ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»å¯Œæœ‰åˆ›é€ æ€§åœ°å¯»æ‰¾å¦ä¸€ç§æ–¹æ³•ï¼š
- en: '[PRE63]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`Out:`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE64]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'That did not take too long, fortunately! `max_a` and `max_b` contain the indices
    of the headlines with maximum similarity (avoiding identical headlines). Letâ€™s
    take a look at the results:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œè¿™æ²¡æœ‰èŠ±å¤ªå¤šæ—¶é—´ï¼`max_a` å’Œ `max_b` åŒ…å«å…·æœ‰æœ€å¤§ç›¸ä¼¼æ€§çš„æ ‡é¢˜çš„ç´¢å¼•ï¼ˆé¿å…ç›¸åŒçš„æ ‡é¢˜ï¼‰ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç»“æœï¼š
- en: '[PRE65]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`Out:`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`è¾“å‡ºï¼š`'
- en: '[PRE66]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Using the block calculation approach, we have calculated almost a trillion similarities
    in just a few minutes. The results are interpretable as we have found similar,
    but not identical, documents. The different date shows that these are definitely
    also separate headlines.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å—è®¡ç®—æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨å‡ åˆ†é’Ÿå†…è®¡ç®—äº†è¿‘ä¸‡äº¿çš„ç›¸ä¼¼æ€§ã€‚ç”±äºæˆ‘ä»¬æ‰¾åˆ°äº†ç›¸ä¼¼ä½†ä¸ç›¸åŒçš„æ–‡æ¡£ï¼Œå› æ­¤ç»“æœæ˜¯å¯ä»¥è§£é‡Šçš„ã€‚ä¸åŒçš„æ—¥æœŸè¡¨æ˜è¿™äº›ç»å¯¹ä¹Ÿæ˜¯ä¸åŒçš„æ ‡é¢˜ã€‚
- en: 'Blueprint: Finding Related Words'
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è“å›¾ï¼šæŸ¥æ‰¾ç›¸å…³è¯æ±‡
- en: Until now, we have analyzed documents with respect to their similarity. But
    the corpus implicitly has much more information, specifically information about
    related words. In our sense, words are related if they are used in the same documents.
    Words should be â€œmoreâ€ related if they frequently appear together in the documents.
    As an example, consider the word *zealand*, which almost always occurs together
    with *new*; therefore, these two words areÂ *related.*
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»åˆ†æäº†æ–‡æ¡£çš„ç›¸ä¼¼æ€§ã€‚ä½†è¯­æ–™åº“åœ¨éšå«ä¸­å…·æœ‰æ›´å¤šä¿¡æ¯ï¼Œå…·ä½“è€Œè¨€æ˜¯æœ‰å…³ç›¸å…³è¯æ±‡çš„ä¿¡æ¯ã€‚åœ¨æˆ‘ä»¬çš„æ„ä¹‰ä¸Šï¼Œå¦‚æœå•è¯åœ¨ç›¸åŒçš„æ–‡æ¡£ä¸­ä½¿ç”¨ï¼Œåˆ™å®ƒä»¬æ˜¯ç›¸å…³çš„ã€‚å¦‚æœè¿™äº›å•è¯ç»å¸¸ä¸€èµ·å‡ºç°åœ¨æ–‡æ¡£ä¸­ï¼Œé‚£ä¹ˆå®ƒä»¬åº”è¯¥â€œæ›´â€ç›¸å…³ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œè€ƒè™‘å•è¯
    *zealand*ï¼Œå®ƒå‡ ä¹æ€»æ˜¯ä¸ *new* ä¸€èµ·å‡ºç°ï¼›å› æ­¤ï¼Œè¿™ä¸¤ä¸ªå•è¯æ˜¯*ç›¸å…³çš„*ã€‚
- en: 'Instead of working with a document-term matrix, we would like to work with
    a term-document matrix, which is just its transposed form. Instead of taking row
    vectors, we now take column vectors. However, we need to re-vectorize the data.
    Assume two words are infrequently used and that both happen to be present only
    once in the same headline. Their vectors would then be identical, but this is
    not what we are looking for. As an example, letâ€™s think of a person named *Zaphod
    Beeblebrox*, who is mentioned in two articles. Our algorithm would assign a 100%
    related score to these words. Although this is correct, it is not very significant.Â We
    therefore only consider words that appear at least 1,000 times to get a decent
    statistical significance:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›ä¸æ–‡æ¡£-æœ¯è¯­çŸ©é˜µè€Œéæ–‡æ¡£-æœ¯è¯­çŸ©é˜µä¸€èµ·å·¥ä½œï¼Œè¿™åªæ˜¯å…¶è½¬ç½®å½¢å¼ã€‚æˆ‘ä»¬ä¸å†å–è¡Œå‘é‡ï¼Œè€Œæ˜¯å–åˆ—å‘é‡ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°å‘é‡åŒ–æ•°æ®ã€‚å‡è®¾ä¸¤ä¸ªè¯å¾ˆå°‘ä½¿ç”¨ï¼Œå¹¶ä¸”å®ƒä»¬æ°å¥½åŒæ—¶å‡ºç°åœ¨åŒä¸€æ ‡é¢˜ä¸­ã€‚å®ƒä»¬çš„å‘é‡å°†æ˜¯ç›¸åŒçš„ï¼Œä½†è¿™ä¸æ˜¯æˆ‘ä»¬è¦å¯»æ‰¾çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªåä¸º
    *æ‰ç¦å¾·Â·æ¯•å¸ƒç½—å…‹æ–¯* çš„äººï¼Œåœ¨ä¸¤ç¯‡æ–‡ç« ä¸­æåˆ°äº†ä»–ã€‚æˆ‘ä»¬çš„ç®—æ³•å°†ä¸ºè¿™äº›è¯åˆ†é…100%çš„ç›¸å…³åˆ†æ•°ã€‚å°½ç®¡è¿™æ˜¯æ­£ç¡®çš„ï¼Œä½†ä¸æ˜¯éå¸¸æ˜¾è‘—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªè€ƒè™‘å‡ºç°è‡³å°‘1000æ¬¡çš„å•è¯ï¼Œä»¥è·å¾—è‰¯å¥½çš„ç»Ÿè®¡æ˜¾è‘—æ€§ï¼š
- en: '[PRE67]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The vocabulary is quite small, and we can directly calculate the cosine similarity.
    Changing row for column vectors, we just transpose the matrix, using the convenient
    `.T` method of NumPy:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: è¯æ±‡é‡éå¸¸å°ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼æ€§ã€‚å°†è¡Œå‘é‡å˜ä¸ºåˆ—å‘é‡ï¼Œæˆ‘ä»¬åªéœ€è½¬ç½®çŸ©é˜µï¼Œä½¿ç”¨NumPyçš„æ–¹ä¾¿ `.T` æ–¹æ³•ï¼š
- en: '[PRE68]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finding the largest entries is easiest if we convert it to a one-dimensional
    array, get the index of the sorted elements via `np.argsort`, and restore the
    original indices for the vocabulary lookup:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¦æ‰¾åˆ°æœ€å¤§æ¡ç›®ï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯å°†å…¶è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„ï¼Œé€šè¿‡ `np.argsort` è·å–æ’åºå…ƒç´ çš„ç´¢å¼•ï¼Œå¹¶æ¢å¤ç”¨äºè¯æ±‡æŸ¥æ‰¾çš„åŸå§‹ç´¢å¼•ï¼š
- en: '[PRE69]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '`Out:`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE70]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Itâ€™s quite easy to interpret these results. For some word combinations like
    *climate change*, we have restored frequent bigrams. On the other hand, we can
    also see related words that donâ€™t appear next to each other in the headlines,
    such as *drink* and *driving*. By using the transposed document-term matrix, we
    have performed a kind of *co-occurrence analysis*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå®¹æ˜“è§£é‡Šè¿™äº›ç»“æœã€‚å¯¹äºä¸€äº›è¯ç»„åˆï¼ˆå¦‚ *æ°”å€™å˜åŒ–* ï¼‰ï¼Œæˆ‘ä»¬å·²ç»æ¢å¤äº†é¢‘ç¹çš„äºŒå…ƒç»„ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°åœ¨æ ‡é¢˜ä¸­å¹¶æœªç›¸é‚»çš„ç›¸å…³è¯æ±‡ï¼Œå¦‚ *é¥®é…’*
    å’Œ *é©¾é©¶* ã€‚é€šè¿‡ä½¿ç”¨è½¬ç½®çš„æ–‡æ¡£-æœ¯è¯­çŸ©é˜µï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç§ *å…±ç°åˆ†æ* ã€‚
- en: Tips for Long-Running Programs like Syntactic Similarity
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é•¿æ—¶é—´è¿è¡Œç¨‹åºçš„æŠ€å·§ï¼Œå¦‚å¥æ³•ç›¸ä¼¼æ€§
- en: 'The following are some efficiency tips for long-running programs:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯é•¿æ—¶é—´è¿è¡Œç¨‹åºçš„ä¸€äº›æ•ˆç‡æç¤ºï¼š
- en: Perform benchmarking before waiting too long
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç­‰å¾…è¿‡é•¿ä¹‹å‰è¿›è¡ŒåŸºå‡†æµ‹è¯•
- en: Before performing calculations on the whole dataset, it is often useful to run
    a single calculation and extrapolate how long the whole algorithm will run and
    how much memory it will need. You should definitely try to understand how runtime
    and memory grow with increased complexity (linear, polynomial, exponential). Otherwise,
    you might have to wait for a long time and find out that after a few hours (or
    even days) only 10% progress memory is exhausted.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œè®¡ç®—ä¹‹å‰ï¼Œé€šå¸¸å…ˆè¿è¡Œå•ä¸ªè®¡ç®—å¹¶æ¨æ–­æ•´ä¸ªç®—æ³•å°†è¿è¡Œå¤šé•¿æ—¶é—´ä»¥åŠéœ€è¦å¤šå°‘å†…å­˜æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚æ‚¨åº”è¯¥åŠªåŠ›ç†è§£éšç€å¤æ‚æ€§å¢åŠ è¿è¡Œæ—¶é—´å’Œå†…å­˜æ¶ˆè€—çš„å¢é•¿æ–¹å¼ï¼ˆçº¿æ€§ã€å¤šé¡¹å¼ã€æŒ‡æ•°ï¼‰ã€‚å¦åˆ™ï¼Œæ‚¨å¯èƒ½ä¸å¾—ä¸ç­‰å¾…å¾ˆé•¿æ—¶é—´ï¼Œç„¶åå‘ç°å‡ ä¸ªå°æ—¶ï¼ˆç”šè‡³å‡ å¤©ï¼‰åä»…å®Œæˆäº†10%çš„è¿›åº¦è€Œå†…å­˜å·²ç»è€—å°½ã€‚
- en: Try to divide your problem into smaller parts
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•å°†é—®é¢˜åˆ†è§£ä¸ºè¾ƒå°çš„éƒ¨åˆ†
- en: Dividing a problem into smaller blocks can help here tremendously. As we have
    seen in the most similar document of the news corpus, this took only 20 minutes
    or so to run and used no significant memory. Compared to a naive approach, we
    would have found out after considerable runtime that the RAM would not have been
    enough. Furthermore, by dividing the problem into parts, you can make use of multicore
    architectures or even distribute the problem on many computers.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: å°†é—®é¢˜åˆ†è§£ä¸ºè¾ƒå°çš„å—å¯ä»¥æå¤§åœ°å¸®åŠ©ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æ–°é—»è¯­æ–™åº“ä¸­æœ€ç›¸ä¼¼æ–‡æ¡£ä¸­æ‰€è§ï¼Œè¿™æ ·çš„è¿è¡Œåªéœ€å¤§çº¦20åˆ†é’Ÿï¼Œå¹¶ä¸”æ²¡æœ‰ä½¿ç”¨å¤§é‡å†…å­˜ã€‚ä¸æœ´ç´ æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬å°†åœ¨é•¿æ—¶é—´è¿è¡Œåå‘ç°å†…å­˜ä¸è¶³ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºéƒ¨åˆ†ï¼Œæ‚¨å¯ä»¥åˆ©ç”¨å¤šæ ¸æ¶æ„ç”šè‡³å°†é—®é¢˜åˆ†å¸ƒåœ¨å¤šå°è®¡ç®—æœºä¸Šã€‚
- en: Summary and Conclusion
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸ç»“è®º
- en: In this section, we have prepared blueprints for vectorization and syntactic
    similarity. Almost all machine learning projects with text (such as classification,
    topic modeling, and sentiment detection) need document vectors at their very base.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½å‘é‡åŒ–å’Œå¥æ³•ç›¸ä¼¼æ€§çš„è“å›¾ã€‚å‡ ä¹æ‰€æœ‰æ–‡æœ¬ç›¸å…³çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼ˆå¦‚åˆ†ç±»ã€ä¸»é¢˜å»ºæ¨¡å’Œæƒ…æ„Ÿæ£€æµ‹ï¼‰éƒ½éœ€è¦æ–‡æ¡£å‘é‡ä½œä¸ºå…¶åŸºç¡€ã€‚
- en: 'It turns out that feature engineering is one of the most powerful levers for
    achieving outstanding performance with these sophisticated machine learning algorithms.
    Therefore, itâ€™s an excellent idea to try different vectorizers, play with their
    parameters, and watch the resulting feature space. There are really many possibilities,
    and for good reason: although optimizing this takes some time, it is usually well-invested
    as the results of the subsequent steps in the analysis pipeline will benefit tremendously.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œç‰¹å¾å·¥ç¨‹æ˜¯å®ç°è¿™äº›å¤æ‚æœºå™¨å­¦ä¹ ç®—æ³•å‡ºè‰²æ€§èƒ½çš„æœ€å¼ºå¤§æ æ†ä¹‹ä¸€ã€‚å› æ­¤ï¼Œå°è¯•ä¸åŒçš„å‘é‡åŒ–å™¨ï¼Œè°ƒæ•´å®ƒä»¬çš„å‚æ•°ï¼Œå¹¶è§‚å¯Ÿç”Ÿæˆçš„ç‰¹å¾ç©ºé—´æ˜¯ä¸€ä¸ªç»ä½³çš„ä¸»æ„ã€‚ç¡®å®æœ‰å¾ˆå¤šå¯èƒ½æ€§ï¼Œè€Œä¸”æœ‰å……åˆ†çš„ç†ç”±ï¼šå°½ç®¡ä¼˜åŒ–è¿™ä¸€æ­¥éª¤éœ€è¦ä¸€äº›æ—¶é—´ï¼Œä½†é€šå¸¸æ˜¯éå¸¸å€¼å¾—çš„ï¼Œå› ä¸ºåˆ†æç®¡é“åç»­æ­¥éª¤çš„ç»“æœå°†æå¤§å—ç›Šäºæ­¤ã€‚
- en: The similarity measure used in this chapter is just an example for document
    similarities. For more complicated requirements, there are more sophisticated
    similarity algorithms that you will learn about in the following chapters.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« ä¸­ä½¿ç”¨çš„ç›¸ä¼¼æ€§åº¦é‡ä»…ä½œä¸ºæ–‡æ¡£ç›¸ä¼¼æ€§çš„ç¤ºä¾‹ã€‚å¯¹äºæ›´å¤æ‚çš„éœ€æ±‚ï¼Œè¿˜æœ‰æ›´å¤æ‚çš„ç›¸ä¼¼æ€§ç®—æ³•ï¼Œä½ å°†åœ¨åç»­ç« èŠ‚ä¸­äº†è§£åˆ°ã€‚
- en: Finding similar documents is a well-known problem in information retrieval.
    There are more sophisticated scores, such as [BM25](https://oreil.ly/s47TC). If
    you want a scalable solution, the very popular [Apache Lucene](http://lucene.apache.org)
    library (which is the basis of search engines like [Apache Solr](https://oreil.ly/R5y0E)
    and [Elasticsearch](https://oreil.ly/2qfAL)) makes use of this and is used for
    really big document collections in production scenarios.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¿¡æ¯æ£€ç´¢ä¸­ï¼Œå¯»æ‰¾ç›¸ä¼¼æ–‡æ¡£æ˜¯ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„é—®é¢˜ã€‚è¿˜æœ‰æ›´å¤æ‚çš„è¯„åˆ†æ–¹æ³•ï¼Œå¦‚[BM25](https://oreil.ly/s47TC)ã€‚å¦‚æœä½ éœ€è¦ä¸€ä¸ªå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œéå¸¸æµè¡Œçš„[Apache
    Lucene](http://lucene.apache.org)åº“ï¼ˆå®ƒæ˜¯åƒ[Apache Solr](https://oreil.ly/R5y0E)å’Œ[Elasticsearch](https://oreil.ly/2qfAL)è¿™æ ·çš„æœç´¢å¼•æ“çš„åŸºç¡€ï¼‰åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œåœ¨ç”Ÿäº§åœºæ™¯ä¸­ç”¨äºéå¸¸å¤§çš„æ–‡æ¡£é›†åˆã€‚
- en: In the following chapters, we will revisit similarity quite often. We will take
    a look at integrating word semantics and document semantics, and we will use transfer
    learning to leverage predefined language models that have been trained with extremely
    large documents corpora to achieve state-of-the-art performance.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç»å¸¸é‡æ–°è®¨è®ºç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•æ•´åˆå•è¯è¯­ä¹‰å’Œæ–‡æ¡£è¯­ä¹‰ï¼Œå¹¶åˆ©ç”¨é¢„å…ˆè®­ç»ƒè¿‡çš„å¤§å‹æ–‡æ¡£è¯­æ–™åº“æ¥å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½¿ç”¨è¿ç§»å­¦ä¹ ã€‚
- en: ^([1](ch05.xhtml#idm45634198925240-marker)) In later chapters, we will take
    a look at other possibilities of vectorizing words ([ChapterÂ 10](ch10.xhtml#ch-embeddings))
    and documents ([ChapterÂ 11](ch11.xhtml#ch-sentiment)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm45634198925240-marker)) åœ¨åé¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å‘é‡åŒ–å•è¯ï¼ˆ[ç¬¬10ç« ](ch10.xhtml#ch-embeddings)ï¼‰å’Œæ–‡æ¡£ï¼ˆ[ç¬¬11ç« ](ch11.xhtml#ch-sentiment)ï¼‰çš„å…¶ä»–å¯èƒ½æ€§ã€‚
- en: ^([2](ch05.xhtml#idm45634198893064-marker)) There are much more sophisticated
    algorithms for determining the vocabulary, like [SentencePiece](https://oreil.ly/A6TEl)
    and [BPE](https://oreil.ly/tVDgu), which are worth taking a look at if you want
    to reduce the number of features.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm45634198893064-marker)) æœ‰æ›´å¤æ‚çš„ç®—æ³•æ¥ç¡®å®šè¯æ±‡ï¼Œæ¯”å¦‚[SentencePiece](https://oreil.ly/A6TEl)å’Œ[BPE](https://oreil.ly/tVDgu)ï¼Œå¦‚æœä½ æƒ³å‡å°‘ç‰¹å¾æ•°ï¼Œè¿™äº›éƒ½å€¼å¾—ä¸€çœ‹ã€‚
- en: ^([3](ch05.xhtml#idm45634198361912-marker)) Confusingly, `numpy.dot` is used
    both for the dot product (inner product) and for matrix multiplication. If Numpy
    detects two row or column vectors (i.e., one-dimensional arrays) with the same
    dimension, it calculates the dot product and yields a scalar. If not and the passed
    two-dimensional arrays are suitable for matrix multiplication, it performs this
    operation and yields a matrix. All other cases produce errors. Thatâ€™s convenient,
    but itâ€™s a lot of heuristics.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#idm45634198361912-marker)) ä»¤äººå›°æƒ‘çš„æ˜¯ï¼Œ`numpy.dot`æ—¢ç”¨äºç‚¹ç§¯ï¼ˆå†…ç§¯ï¼‰ï¼Œä¹Ÿç”¨äºçŸ©é˜µä¹˜æ³•ã€‚å¦‚æœNumpyæ£€æµ‹åˆ°ä¸¤ä¸ªè¡Œå‘é‡æˆ–åˆ—å‘é‡ï¼ˆå³ä¸€ç»´æ•°ç»„ï¼‰å…·æœ‰ç›¸åŒçš„ç»´åº¦ï¼Œå®ƒè®¡ç®—ç‚¹ç§¯å¹¶ç”Ÿæˆä¸€ä¸ªæ ‡é‡ã€‚å¦‚æœä¸æ˜¯ï¼Œè€Œä¸”ä¼ é€’çš„äºŒç»´æ•°ç»„é€‚åˆçŸ©é˜µä¹˜æ³•ï¼Œå®ƒæ‰§è¡Œè¿™ä¸ªæ“ä½œå¹¶ç”Ÿæˆä¸€ä¸ªçŸ©é˜µã€‚æ‰€æœ‰å…¶ä»–æƒ…å†µéƒ½ä¼šäº§ç”Ÿé”™è¯¯ã€‚è¿™å¾ˆæ–¹ä¾¿ï¼Œä½†æ¶‰åŠåˆ°å¾ˆå¤šå¯å‘å¼æ–¹æ³•ã€‚
- en: ^([4](ch05.xhtml#idm45634198247384-marker)) See [ChapterÂ 8](ch08.xhtml#ch-topicmodels)
    for more on LDA.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.xhtml#idm45634198247384-marker)) æ›´å¤šå…³äºLDAçš„å†…å®¹è¯·å‚è§[ç¬¬8ç« ](ch08.xhtml#ch-topicmodels)ã€‚
- en: ^([5](ch05.xhtml#idm45634197672360-marker)) See, for example, the [definition
    of entropy](https://oreil.ly/3qTpX) as a measure of uncertainty and information.
    Basically, this says that a low-probability value carries more information than
    a more likely value.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.xhtml#idm45634197672360-marker)) ä¾‹å¦‚ï¼Œå‚è§[ç†µçš„å®šä¹‰](https://oreil.ly/3qTpX)ï¼Œä½œä¸ºä¸ç¡®å®šæ€§å’Œä¿¡æ¯çš„åº¦é‡ã€‚åŸºæœ¬ä¸Šï¼Œè¿™è¡¨æ˜ä½æ¦‚ç‡å€¼æ¯”æ›´å¯èƒ½çš„å€¼åŒ…å«æ›´å¤šä¿¡æ¯ã€‚
- en: ^([6](ch05.xhtml#idm45634197156184-marker)) This is, of course, related to the
    stop word list that has already been used. In news articles, the most common words
    are stop words. In domain-specific texts, it might be completely different. Using
    stop words is often the safer choice, as these lists have been curated.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.xhtml#idm45634196754328-marker)) It would be growing exponentially
    if all word combinations were possible and would be used. As this is unlikely,
    the dimensions are growing subexponentially.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.xhtml#idm45634196311384-marker)) We have chosen 10,000 dimensions,
    as the resulting matrix can be kept in RAM (using roughly 1 GB should be possible
    even on moderate hardware).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.xhtml#idm45634196279640-marker)) All calculations can be sped up
    considerably by using processor-specific libraries, e.g., by subscribing to the
    Intel channel in Anaconda. This will use AVX2, AVX-512, and similar instructions
    and use parallelization. [MKL](https://oreil.ly/pa1zj) and [OpenBlas](https://oreil.ly/jZYSG)
    are good candidates for linear algebra libraries.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
