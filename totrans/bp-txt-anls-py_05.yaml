- en: Chapter 5\. Feature Engineering and Syntactic Similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 1](ch01.xhtml#ch-exploration), text is significantly different
    from structured data. One of the most striking differences is that text is represented
    by words, while structured data (mostly) uses numbers. From a scientific point
    of view, centuries of mathematical research have led to an extremely good understanding
    of numbers and sophisticated methods. Information science has picked up that mathematical
    research, and many creative algorithms have been invented on top of that. Recent
    advances in machine learning have generalized a lot of formerly very specific
    algorithms and made them applicable to many different use cases. These methods
    “learn” directly from data and provide an unbiased view.
  prefs: []
  type: TYPE_NORMAL
- en: To use these instruments, we have to find a mapping of text to numbers. Considering
    the richness and complexity of text, it is clear that a single number will not
    be enough to represent the meaning of a document. Something more complex is needed.
    The natural extension of real numbers in mathematics is a tuple of real numbers,
    called a *vector*. Almost all text representations in text analytics and machine
    learning use vectors; see [Chapter 6](ch06.xhtml#ch-classification) for more.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors live in a vector space, and most vector spaces have additional properties
    such as norms and distances, which will be helpful for us as they imply the concept
    of similarity. As we will see in subsequent chapters, measuring the similarity
    between documents is absolutely crucial for most text analytics applications,
    but it is also interesting on its own.
  prefs: []
  type: TYPE_NORMAL
- en: What You’ll Learn and What We’ll Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we talk about the vectorization of documents. This means we
    will convert unstructured text into vectors that contain numbers.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few ways to vectorize documents. As document vectorization
    is the basis for all machine learning tasks, we will spend some time designing
    and implementing our own vectorizer. You can use that as a blueprint if you need
    a specialized vectorizer for your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we will focus on two popular models that are already implemented
    in scikit-learn: the bag-of words model and the TF-IDF improvements to it. We
    will download a large dataset of documents and vectorize it with these methods.
    As you will see, there can be many problems that are related to data volume and
    scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: Although vectorization is a base technology for more sophisticated machine learning
    algorithms, it can also be used on its own to calculate similarities between documents.
    We will take a detailed look at how this works, how it can be optimized, and how
    we can make it scalable. For a richer representation of words, see [Chapter 10](ch10.xhtml#ch-embeddings), and
    for a more contextualized approach, see [Chapter 11](ch11.xhtml#ch-sentiment).
  prefs: []
  type: TYPE_NORMAL
- en: After studying this chapter, you will understand how to convert documents to
    numbers (*vectors*) using words or combinations as *features*.^([1](ch05.xhtml#idm45634198925240))
    We will try different methods of vectorizing documents, and you will be able to
    determine the correct method for your use case. You will learn why the similarity
    of documents is important and a standard way to calculate it. We will go into
    detail with an example that has many documents and show how to vectorize them
    and calculate similarities effectively.
  prefs: []
  type: TYPE_NORMAL
- en: The first section introduces the concept of a vectorizer by actually building
    your own simple one. This can be used as a blueprint for more sophisticated vectorizers
    that you might have to build in your own projects. Counting word occurrences and
    using them as vectors is called *bag-of-words* and already creates very versatile
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Together with the dataset (which has more than 1,000,000 news headlines), we
    introduce a use case and present a scalable blueprint architecture in the TF-IDF
    section. We will build a blueprint for vectorizing documents and a similarity
    search for documents. Even more challenging, we will try to identify the most
    similar (but nonidentical) headlines in this corpus.
  prefs: []
  type: TYPE_NORMAL
- en: A Toy Dataset for Experimentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quite surprisingly, a lot of experiments have shown that for many text analytics
    problems it is enough to know which words appear in documents. It is not necessary
    to understand the meaning of the words or take word order into account. As the
    underlying mappings are particularly easy and fast to calculate, we will start
    with these mappings and use the words as features.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first blueprints, we will concentrate on the methods and therefore
    use a few sentences from the novel [*A Tale of Two Cities*](https://oreil.ly/rfmPH)
    by Charles Dickens as a toy dataset. We will use the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: It was the best of times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It was the worst of times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It was the age of wisdom.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It was the age of foolishness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blueprint: Building Your Own Vectorizer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As vectorizing documents is the base for nearly all of the following chapters
    in this book, we take an in-depth look at how vectorizers work. This works best
    by implementing our own vectorizer. You can use the methods in this section if
    you need to implement a custom vectorizer in your own projects or need to adapt
    an existing vectorizer to your specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: To make it as simple as possible, we will implement a so-called *one-hot vectorizer*.
    This vectorizer creates binary vectors from documents by noting if a word appears
    in a document or not, yielding 1 or 0, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by creating a vocabulary and assigning numbers to words, then
    perform the vectorization, and finally analyze similarity in this binary space.
  prefs: []
  type: TYPE_NORMAL
- en: Enumerating the Vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting with the words as features, we have to find a way to convert words
    to the dimensions of the vectors. Extracting the words from the text is done via
    tokenization, as explained in [Chapter 2](ch02.xhtml#ch-api).^([2](ch05.xhtml#idm45634198893064))
  prefs: []
  type: TYPE_NORMAL
- en: 'As we are interested only in whether a word appears in a document or not, we
    can just enumerate the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| It | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| age | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| best | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| foolishness | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| it | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| of | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| the | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| times | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| was | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| wisdom | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| worst | 10 |'
  prefs: []
  type: TYPE_TB
- en: As you can see, the words have been numbered according to their first occurrence.
    This is what we call a *dictionary*, consisting of words (the vocabulary) and
    their respective numbers. Instead of having to refer to words, we can now use
    the numbers and arrange them in the following vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorizing Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To compare vectors, calculate similarities, and so forth, we have to make sure
    that vectors for each document have the same number of dimensions. To achieve
    that, we use the same dictionary for all documents. If the document doesn’t contain
    a word, we just put a 0 at the corresponding position; otherwise, we will use
    a 1\. By convention, row vectors are used for documents. The dimension of the
    vectors is as big as the length of the dictionary. In our example, this is not
    a problem as we have only a few words. However, in large projects, the vocabulary
    can easily exceed 100,000 words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the one-hot encoding of all sentences before actually using
    a library for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For each sentence, we have now calculated a vector representation. Converting
    documents to one-hot vectors, we have lost information about how often words occur
    in documents and in which order.
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-vocabulary documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What happens if we try to keep the vocabulary fixed and add new documents?
    That depends on whether the words of the documents are already contained in the
    dictionary. Of course, it can happen that all words are already known:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the opposite is also quite possible. If we try to vectorize a sentence
    with only unknown words, we get a null vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This sentence does not “interact” with the other sentences in the corpus. From
    a strict point of view, this sentence is not similar to any sentence in the corpus.
    This is no problem for a single sentence; if this happens more frequently, the
    vocabulary or the corpus needs to be adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: The Document-Term Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Arranging the row vectors for each document in a matrix with the rows enumerating
    the documents, we arrive at the document-term matrix. The document-term matrix
    is the vector representation of all documents and the most basic building block
    for nearly all machine learning tasks throughout this book. In this chapter, we
    will use it for calculating document similarities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_05in01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Be careful: using lists and arrays for the document-term matrix works best
    with a small vocabulary. With large vocabularies, we will have to find a cleverer
    representation. Scikit-learn takes care of this and uses so-called sparse vectors
    and matrices from [SciPy](https://oreil.ly/yk1wx).'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating similarities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Calculating the similarities between documents works by calculating the number
    of common 1s at the corresponding positions. In one-hot encoding, this is an extremely
    fast operation, as it can be calculated on the bit level by `AND`ing the vectors
    and counting the number of 1s in the resulting vector. Let’s calculate the similarity
    of the first two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Another possible way to calculate the similarity that we will encounter frequently
    is using *scalar product* (often called *dot product*) of the two document vectors.
    The scalar product is calculated by multiplying corresponding components of the
    two vectors and adding up these products. By observing the fact that a product
    can only be 1 if both factors are 1, we effectively calculate the number of common
    1s in the vectors. Let’s try it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The Similarity Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we are interested in finding the similarity of all documents to each other,
    there is a fantastic shortcut for calculating all the numbers with just one command!
    Generalizing the formula from the previous section, we find the similarity of
    document *i* and document *j* to be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper S Subscript i j Baseline equals normal d Subscript i Baseline
    dot normal d Subscript j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="normal">d</mi> <mi>i</mi></msub> <mo>·</mo>
    <msub><mi mathvariant="normal">d</mi> <mi>j</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to use the document-term matrix from earlier, we can write the scalar
    product as a sum:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper S Subscript i j Baseline equals sigma-summation Underscript
    k Endscripts upper D Subscript i k Baseline upper D Subscript j k Baseline equals
    sigma-summation Underscript k Endscripts upper D Subscript i k Baseline left-parenthesis
    upper D Superscript upper T Baseline right-parenthesis Subscript k j Baseline
    equals left-parenthesis normal upper D dot normal upper D Superscript upper T
    Baseline right-parenthesis Subscript i j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mo>∑</mo> <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>D</mi> <mrow><mi>j</mi><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mo>∑</mo>
    <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mrow><mo>(</mo><msup><mi>D</mi>
    <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>k</mi><mi>j</mi></mrow></msub> <mo>=</mo>
    <msub><mrow><mo>(</mo><mi mathvariant="normal">D</mi><mo>·</mo><msup><mrow><mi
    mathvariant="normal">D</mi></mrow> <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: So, this is just the matrix product of our document-term matrix with itself
    transposed. In Python, that’s now easy to calculate (the sentences in the output
    have been added for easier checking the similarity):^([3](ch05.xhtml#idm45634198361912))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, the highest numbers are on the diagonal, as each document is most
    similar to itself. The matrix has to be symmetric, as document *A* has the same
    similarity to *B* as *B* to *A*. Apart from that, we can see that the second sentence
    is on average most similar to all others, whereas the third and last document
    is the most similar pairwise (they differ only by one word). The same would be
    true of the first and second documents if we ignored case.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how a document vectorizer works is crucial for implementing your
    own, but it’s also helpful for appreciating all the functionalities and parameters
    of existing vectorizers. This is why we have implemented our own. We have taken
    a detailed look at the different stages of vectorization, starting with building
    a vocabulary and then converting the documents to binary vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, we analyzed the similarity of the documents. It turned out that the
    dot product of their corresponding vectors is a good measure for this similarity.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot vectors are also used in practice, for example, in document classification
    and clustering. However, scikit-learn also offers more sophisticated vectorizers,
    which we will use in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-Words Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One-hot encoding has already provided us with a basic representation of documents
    as vectors. However, it did not take care of words appearing many times in documents.
    If we want to calculate the frequency of words for each document, then we should
    use what is called a *bag-of-words* representation.
  prefs: []
  type: TYPE_NORMAL
- en: Although somewhat simplistic, these models are in wide use. For cases such as
    classification and sentiment detection, they work reasonably. Moreover, there
    are topic modeling methods like Latent Dirichlet Allocation (LDA), which explicitly
    requires a bag-of-words model.^([4](ch05.xhtml#idm45634198247384))
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using scikit-learn’s CountVectorizer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of implementing a bag-of-words model on our own, we use the algorithm
    that scikit-learn provides.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the corresponding class is called `CountVectorizer`, which is our
    first encounter with feature extraction in scikit-learn. We will take a detailed
    look at the design of the classes and in which order their methods should be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Our example sentences from the one-hot encoder is really trivial, as no sentence
    in our dataset contains words more than once. Let’s add some more sentences and
    use that as a basis for the `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`CountVectorizer` works in two distinct phases: first it has to learn the vocabulary;
    afterward it can transform the documents to vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the vocabulary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, it needs to learn about the vocabulary. This is simpler now, as we can
    just pass the array with the sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Don’t worry about all these parameters; we will talk about the important ones
    later. Let’s first see what `CountVectorizer` used as vocabulary, which is called
    *feature names* here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We have created a vocabulary and so-called features using `CountVectorizer`.
    Conveniently, the vocabulary is sorted alphabetically, which makes it easier for
    us to decide whether a specific word is included.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the documents to vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the second step, we will use `CountVectorizer` to transform the documents
    to the vector representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the document-term matrix that we have already encountered in
    the previous section. However, it is a different object, as `CountVectorizer`
    has created a sparse matrix. Let’s check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Sparse matrices are extremely efficient. Instead of storing 6 × 20 = 120 elements,
    it just has to save 38! Sparse matrices achieve that by skipping all zero elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to recover our former document-term matrix. For this, we must transform
    the sparse matrix to a (dense) array. To make it easier to read, we convert it
    into a Pandas `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_05in02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The document-term matrix looks very similar to the one from our one-hot vectorizer.
    Note, however, that the columns are in alphabetical order, and observe several
    2s in the fifth row. This originates from the document `"John likes to watch movies.
    Mary likes movies too."`, which has many duplicate words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Calculating Similarities'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finding similarities between documents is now more difficult as it is not enough
    to count the common 1s in the documents. In general, the number of occurrences
    of each word can be bigger, and we have to take that into account.  The dot product
    cannot be used for this, as it is also sensitive to the length of the vector (the
    number of words in the documents). Also, a Euclidean distance is not very useful
    in high-dimensional vector spaces. This is why most commonly the angle between
    document vectors is used as a measure of similarity. The cosine of the angle between
    two vectors is defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal c normal o normal s left-parenthesis bold a comma bold
    b right-parenthesis equals StartFraction bold a dot bold b Over StartAbsoluteValue
    EndAbsoluteValue bold a StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue
    EndAbsoluteValue bold b StartAbsoluteValue EndAbsoluteValue EndFraction equals
    StartFraction sigma-summation a Subscript i Baseline b Subscript i Baseline Over
    StartRoot sigma-summation a Subscript i Baseline a Subscript i Baseline EndRoot
    StartRoot sigma-summation b Subscript i Baseline b Subscript i Baseline EndRoot
    EndFraction"><mrow><mi>cos</mi> <mrow><mo>(</mo> <mi>𝐚</mi> <mo>,</mo> <mi>𝐛</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>𝐚</mi><mo>·</mo><mi>𝐛</mi></mrow>
    <mrow><mo>|</mo><mo>|</mo><mi>𝐚</mi><mo>|</mo><mo>|</mo><mo>·</mo><mo>|</mo><mo>|</mo><mi>𝐛</mi><mo>|</mo><mo>|</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi>
    <mi>i</mi></msub></mrow> <mrow><msqrt><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <msub><mi>a</mi> <mi>i</mi></msub></mrow></msqrt> <msqrt><mrow><mo>∑</mo><msub><mi>b</mi>
    <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub></mrow></msqrt></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn simplifies this calculation by offering a `cosine_similarity`
    utility function. Let’s check the similarity of the first two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Compared to our handmade similarity in the earlier sections, `cosine_similarity`
    offers some advantages, as it is properly normalized and can take only values
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the similarity of all documents is of course also possible; scikit-learn
    has optimized the `cosine_similarity`, so it is possible to directly pass matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | 0 | 1 | 2 | 3 | 4 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.000000 | 0.833333 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.833333 | 1.000000 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.666667 | 0.666667 | 1.000000 | 0.833333 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.666667 | 0.666667 | 0.833333 | 1.000000 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.524142 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.524142 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: Again, the matrix is symmetric with the highest values on the diagonal. It’s
    also easy to see that document pairs 0/1 and 2/3 are most similar. Documents 4/5
    have no similarity at all to the other documents but have some similarity to each
    other. Taking a look back at the sentences, this is exactly what one would expect.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-words models are suitable for a variety of use cases. For classification,
    sentiment detection, and many topic models, they create a bias toward the most
    frequent words as they have the highest numbers in the document-term matrix. Often
    these words do not carry much meaning and could be defined as stop words.
  prefs: []
  type: TYPE_NORMAL
- en: As these would be highly domain-specific, a more generic approach “punishes”
    words that appear too often in the corpus of all documents. This is called a *TF-IDF
    model* and will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our previous example, many sentences started with the words “it was the
    time of.” This contributed a lot to their similarity, but in reality, the actual
    information you get by the words is minimal. TF-IDF will take care of that by
    counting the number of total word occurrences. It will reduce weights of frequent
    words and at the same time increase the weights of uncommon words. Apart from
    the information-theoretical measure,^([5](ch05.xhtml#idm45634197672360)) this
    is also something that you can observe when reading documents: if you encounter
    uncommon words, it is likely that the author wants to convey an important message
    with them.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimized Document Vectors with TfidfTransformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 2](ch02.xhtml#ch-api), a better measure for information
    compared to counting is calculating the inverted document frequency and using
    a penalty for very common words. The TF-IDF weight can be calculated from the
    bag-of-words model. Let’s try this again with the previous model and see how the
    weights of the document-term matrix change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_05in03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, some words have been scaled to smaller values (like “it”),
    while others have not been scaled down so much (like “wisdom”). Let’s see the
    effect on the similarity matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | 0 | 1 | 2 | 3 | 4 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1.000000 | 0.675351 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.675351 | 1.000000 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.457049 | 0.457049 | 1.000000 | 0.675351 | 0.00000 | 0.00000 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.457049 | 0.457049 | 0.675351 | 1.000000 | 0.00000 | 0.00000 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.43076 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.43076 | 1.000000 |'
  prefs: []
  type: TYPE_TB
- en: We get exactly the effect we have hoped for! Document pairs 0/1 and 2/3 are
    still very similar, but the number has also decreased to a more reasonable level
    as the document pairs differ in *significant words*. The more common words now
    have lower weights.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the ABC Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a real-word use case, we will take a [dataset from Kaggle](https://oreil.ly/hg5R3)
    that contains news headlines. Headlines originate from Australian news source
    ABC and are from 2003 to 2017\. The CSV file contains only a timestamp and the
    headline without punctuation in lowercase. We load the CSV file into a Pandas
    `DataFrame` and take a look at the first few documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '|   | publish_date | headline_text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2003-02-19 | aba decides against community broadcasting lic... |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2003-02-19 | act fire witnesses must be aware of defamation |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2003-02-19 | a g calls for infrastructure protection summit |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2003-02-19 | air nz staff in aust strike for pay rise |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2003-02-19 | air nz strike to affect australian travellers |'
  prefs: []
  type: TYPE_TB
- en: There are 1,103,663  headlines in this dataset. Note that the headlines do not
    include punctuation and are all transformed to lowercase. Apart from the text,
    the dataset includes the publication date of each headline.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, the TF-IDF vectors can be calculated using the bag-of-words
    model (the *count vectors* in scikit-learn terminology). As it is so common to
    use TF-IDF document vectors, scikit-learn has created a “shortcut” to skip the
    count vectors and directly calculate the TF-IDF vectors. The corresponding class
    is called `TfidfVectorizer`, and we will use it next.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following, we have also combined the calls to fit and to transform in
    `fit_transform`, which is convenient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This might take a while, as so many documents have to be analyzed and vectorized.
    Take a look at the dimensions of the document-term matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The number of rows was expected, but the number of columns (the vocabulary)
    is really large, with almost 100,000 words. Doing the math shows that a naive
    storage of data would have led to  1,103,663 * 95,878 elements with 8 bytes per
    float and have used roughly 788 GB RAM. This shows the incredible effectiveness
    of sparse matrices as the real memory used is “only” 56,010,856 bytes (roughly
    0.056 GB; found out via `dt.data.nbytes`). It’s still a lot, but it’s manageable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the similarity between two vectors is another story, though. Scikit-learn
    (and SciPy as a basis) is highly optimized for working with sparse vectors, but
    it still takes some time doing the sample calculation (similarities of the first
    10,000 documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: For machine learning in the next chapters, many of these linear algebra calculations
    are necessary and have to be repeated over and over. Often operations scale quadratically
    with the number of features (O(N²)). Optimizing the vectorization by removing
    unnecessary features is therefore not only helpful for calculating the similarities
    but also crucial for scalable machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Reducing Feature Dimensions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now found features for our documents and used them to calculate document
    vectors. As we have seen in the example, the number of features can get quite
    large. Lots of machine learning algorithms are computationally intensive and scale
    with the number of features, often even polynomially. One part of feature engineering
    is therefore focused on reducing these features to the ones that are really necessary.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we show a blueprint for how this can be achieved and measure
    their impact on the number of features.
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the first place, we can think about removing the words that carry the least
    meaning. Although this is domain-dependent, there are lists of the most common
    English words that common sense tells us can normally be neglected. These words
    are called *stop words*. Common stop words are determiners, auxiliary verbs, and
    pronouns. For a more detailed discussion, see [Chapter 4](ch04.xhtml#ch-preparation).
    Be careful when removing stop words as they can contain certain words that might
    carry a domain-specific meaning in special texts!
  prefs: []
  type: TYPE_NORMAL
- en: This does not reduce the number of dimensions tremendously as there are only
    a few hundred common stop words in almost any language. However, it should drastically
    decrease the number of stored elements as stop words are so common. This leads
    to less memory consumption and faster calculations, as fewer numbers need to be
    multiplied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the standard spaCy stop words and check the effects on the document-term
    matrix. Note that we pass stop words as a named parameter to the `TfidfVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: With only 305 stop words, we managed to reduce the number of stored elements
    by 20%. The dimensions of the matrix are almost the same, with fewer columns due
    to the 95,878 – 95,600 = 278 stop words that actually appeared in the headlines.
  prefs: []
  type: TYPE_NORMAL
- en: Minimum frequency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Taking a look at the definition of the cosine similarity, we can easily see
    that components can contribute only if both vectors have a nonzero value at the
    corresponding index. This means that we can neglect all words occurring less than
    twice! `TfidfVectorizer` (and `CountVectorizer`) have a parameter for that called
    `min_df`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Obviously, there are a lot of words appearing just once (95,600 – 58,527 =
    37,073). Those words should also be stored only once; checking with the number
    of stored elements, we should get the same result: 5,644,186 – 5,607,113 = 37,073\.
    Performing this kind of transformation, it is always useful to integrate such
    plausibility checks.'
  prefs: []
  type: TYPE_NORMAL
- en: Losing Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Be careful: by using `min_df=2`, we have not lost any information in vectorizing
    the headlines of this document corpus. If we plan to vectorize more documents
    later with the same vocabulary, we might lose information, as words appearing
    again in the new documents that were only present once in the original documents
    will not be found in the vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_df` can also take float values. This means that a word has to occur in
    a minimum fraction of documents. Normally, this reduces the vocabulary drastically
    even for low numbers of `min_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This transformation is probably too strict and reduces the vocabulary too far.
    Depending on the number of documents, you should set `min_df` to a low integer
    and check the effects on the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum frequency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sometimes a text corpus might have a special vocabulary with lots of repeating
    terms that are too specific to be contained in stop word lists. For this use case,
    scikit-learn offers the `max_df` parameter, which eliminates terms occurring too
    often in the corpus. Let’s check how the dimensions are reduced when we eliminate
    all the words that appear in at least 10% of the headlines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Setting `max_df` to a low value of 10% does not eliminate a single word!^([6](ch05.xhtml#idm45634197156184))
    Our news headlines are very diverse. Depending on the type of corpus you have,
    experimenting with `max_df` can be quite useful. In any case, you should always
    check how the dimensions change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Improving Features by Making Them More Specific'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have only used the original words of the headlines and reduced the
    number of dimensions by stop words and counting frequencies. We have not yet changed
    the features themselves. Using linguistic analysis, there are more possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Performing linguistic analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using spaCy, we can lemmatize all headlines and just keep the lemmas. This
    takes some time, but we expect to find a smaller vocabulary. First, we have to
    perform a linguistic analysis, which might take some time to finish (see [Chapter 4](ch04.xhtml#ch-preparation) for
    more details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Blueprint: Using Lemmas Instead of Words for Vectorizing Documents'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can vectorize the data using the lemmas and see how the vocabulary
    decreased:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Saving almost 25,000 dimensions is a lot. In news headlines, lemmatizing the
    data probably does not lose any information. In other use cases like those in
    [Chapter 11](ch11.xhtml#ch-sentiment), it’s a completely different story.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Limit Word Types'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the data generated earlier, we can restrict ourselves to considering
    just nouns, adjectives, and verbs for the vectorization, as prepositions, conjugations,
    and so on are supposed to carry little meaning. This again reduces the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: There’s not much to win here, which is probably due to the headlines mainly
    containing nouns, adjectives, and verbs. But this might look totally different
    in your own projects. Depending on the type of texts you are analyzing, restricting
    word types will not only reduce the size of the vocabulary but will also lead
    to much lower noise. It’s a good idea to try this with a small part of the corpus
    first to avoid long waiting times due to the expensive linguistic analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Remove Most Common Words'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we learned, removing frequent words can lead to document-term matrices with
    far fewer entries. This is especially helpful when you perform unsupervised learning,
    as you will normally not be interested in common words that are common anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove even more noise, we will now try to eliminate the most common English
    words. Be careful, as there will normally also be words involved that might carry
    important meaning. There are various lists with those words; they can easily be
    found on the internet. The [list from Google](https://oreil.ly/bOho1) is rather
    popular and directly available on GitHub. Pandas can directly read the list if
    we tell it to be a CSV file without column headers. We will then instruct the
    `TfidfVectorizer` to use that list as stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the matrix now has 3.5 million fewer stored elements. The vocabulary
    shrunk by 68,426 – 61,630 = 6,796 words, so more than 3,000 of the most frequent
    English words were not even used in the ABC headlines.
  prefs: []
  type: TYPE_NORMAL
- en: Removing frequent words is an excellent method to remove noise from the dataset
    and concentrate on the uncommon words. However, you should be careful using this
    from the beginning as even frequent words do have a meaning, and they might also
    have a special meaning in your document corpus. We recommend performing such analyses
    additionally but not exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Adding Context via N-Grams'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have used only single words as features (dimensions of our document
    vectors) as the basis for our vectorization. With this strategy, we have lost
    a lot of context information. Using single words as features does not respect
    the context in which the words appear. [In later chapters](ch10.xhtml#ch-embeddings)
    we will learn how to overcome that limitation with sophisticated models like word
    embeddings. In our current example, we will use a simpler method and take advantage
    of word combinations, so called *n-grams*. Two-word combinations are called *bigrams*;
    for three words, they are called *trigrams*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, `CountVectorizer` and `TfidfVectorizer` have the corresponding
    options. Contrary to the last few sections where we tried to reduce the vocabulary,
    we now enhance the vocabulary with word combinations. There are many such combinations;
    their number (and vocabulary size) is growing almost exponentially with *n*.^([7](ch05.xhtml#idm45634196754328))
    We will therefore be careful and start with bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Increasing the feature dimensions from 95,600 to 2,335,132 or even 5,339,558
    is quite painful even though the RAM size has not grown too much. For some tasks
    that need context-specific information (like sentiment analysis), n-grams are
    extremely useful. It is always useful to keep an eye on the dimensions, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining n-grams with linguistic features and common words is also possible
    and reduces the vocabulary size considerably:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Scikit-learn offers many different vectorizers. Normally, starting with `TfidfVectorizer`
    is a good idea, as it is one of the most versatile.
  prefs: []
  type: TYPE_NORMAL
- en: Options of TfidfVectorizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TF-IDF can even be switched off so there is a seamless fallback to `CountVectorizer`.
    Because of the many parameters, it can take some time to find the perfect set
    of options.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the correct set of features is often tedious and requires experimentation
    with the (many) parameters of `TfidfVectorizer`, like `min_df`, `max_df`, or simplified
    text via NLP. In our work, we have had good experiences with setting `min_df`
    to `5`, for example, and `max_df` to `0.7`. In the end, this time is excellently
    invested as the results will depend heavily on correct vectorization. There is
    no golden bullet, though, and this *feature engineering* depends heavily on the
    use case and the planned use of the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The TF-IDF method itself can be improved by using a subnormal term frequency
    or normalizing the resulting vectors. The latter is useful for quickly calculating
    similarities, and we demonstrate its use later in the chapter. The former is mainly
    interesting for long documents to avoid repeating words getting a too high-weight.
  prefs: []
  type: TYPE_NORMAL
- en: Think very carefully about feature dimensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our previous examples, we have used single words and bigrams as features.
    Depending on the use case, this might already be enough. This works well for texts
    with common vocabulary, like news. But often you will encounter special vocabularies
    (for example, scientific publications or letters to an insurance company), which
    will require more sophisticated feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Keep number of dimensions in mind
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we have seen, using parameters like `ngram_range` can lead to large feature
    spaces. Apart from the RAM usage, this will also be a problem for many machine
    learning algorithms due to overfitting. Therefore, it’s a good idea to always
    consider the (increase of) feature dimensions when changing parameters or vectorization
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Syntactic Similarity in the ABC Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarity is one of the most basic concepts in machine learning and text analytics.
    In this section, we take a look at some challenging problems finding similar documents
    in the ABC dataset.
  prefs: []
  type: TYPE_NORMAL
- en: After taking a look at possible vectorizations in the previous section, we will
    now use one of them to calculate the similarities. We will present a blueprint
    to show how you can perform these calculations efficiently from both a CPU and
    a RAM perspective. As we are handling large amounts of data here, we have to make
    extensive use of the [NumPy library](https://numpy.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first step, we vectorize the data using stop words and bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to use these vectors for our blueprints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Finding Most Similar Headlines to a Made-up Headline'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s say we want to find a headline in our data that most closely matches
    a headline that we remember, but only roughly. This is quite easy to solve, as
    we just have to vectorize our new document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have to calculate the cosine similarity to each headline in the corpus.
    We could implement this in a loop, but it’s easier with the `cosine_similarity`
    function of scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a “number of headlines in the corpus” × 1 matrix, where each
    number represents the similarity to a document in the corpus. Using `np.argmax`
    gives us the index of the most similar document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: No *sizes* of apples and no *Australia* are present in the most similar headline,
    but it definitely bears some similarity with our invented headline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Finding the Two Most Similar Documents in a Large Corpus (Much More
    Difficult)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When handling a corpus of many documents, you might often be asked questions
    such as “Are there duplicates?” or “Has this been mentioned before?” They all
    boil down to finding the most similar (maybe even identical) documents in the
    corpus. We will explain how to accomplish this and again use the ABC dataset as
    our example. The number of headlines will turn out to be a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: You might think that finding the most similar documents in the corpus is as
    easy as calculating the `cosine_similarity` between all documents. However, this
    is not possible as 1,103,663 × 1,103,663 = 1,218,072,017,569\. More than one trillion
    elements do not fit in the RAM of even the most advanced computers. It is perfectly
    possible to perform the necessary matrix multiplications without having to wait
    for ages.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, this problem needs optimization. As text analytics often has to cope
    with many documents, this is a very typical challenge. Often, the first optimization
    is to take an intensive look at all the needed numbers. We can easily observe
    that the document similarity relation is symmetrical and normalized.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we just need to calculate the subdiagonal elements of the similarity
    matrix ([Figure 5-1](#btap_0501))
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0501.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Elements that need to be calculated in the similarity matrix. Only
    the elements below the diagonal need to be calculated, as their numbers are identical
    to the ones mirrored on the diagonal. The diagonal elements are all 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This reduces the number of elements to 1,103,663 × 1,103,662 / 2 = 609,035,456,953,
    which could be calculated in loop iterations, keeping only the most similar documents.
    However, calculating all these elements separately is not a good option, as the
    necessary Python loops (where each iteration calculates just a single matrix element)
    will eat up a lot of CPU performance.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of calculating individual elements of the similarity matrix, we divide
    the problem into different blocks and calculate 10,000 × 10,000 similarity submatrices^([8](ch05.xhtml#idm45634196311384)) 
    at once by taking blocks of 10,000 TF-IDF vectors from the document matrix. Each
    of these matrices contains 100,000,000 similarities, which will still fit in RAM.
    Of course, this leads to calculating too many elements, and we have to perform
    this for 111 × 110 / 2 = 6,105 submatrices (see [Figure 5-2](#btap_0502)).
  prefs: []
  type: TYPE_NORMAL
- en: From the previous section, we know that iteration takes roughly 500 ms to calculate.
    Another advantage of this approach is that leveraging data locality gives us a
    bigger chance of having the necessary matrix elements already in the CPU cache.
    We estimate that everything should run in about 3,000 seconds, which is roughly
    one hour.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0502.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Dividing the matrix into submatrices, which we can calculate more
    easily; the problem is divided into blocks (here, 4 × 4), and the white and diagonal
    elements within the blocks are redundantly calculated.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Can we improve this? Yes, indeed another speedup of a factor of 10 is actually
    possible. This works by normalizing the TF-IDF vectors via the corresponding option
    of `TfidfVectorizer`. Afterward, the similarity can be calculated with `np.dot`:^([9](ch05.xhtml#idm45634196279640))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'In each iteration we save the most similar documents and their similarity and
    adjust them during the iterations. To skip identical documents (or more precisely,
    documents with identical document vectors), we only consider similarities < 0.9999\.
    As it turns out, using `<` relations with a sparse matrix is extremely inefficient,
    as all non-existent elements are supposed to be 0\. Therefore, we must be creative
    and find another way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'That did not take too long, fortunately! `max_a` and `max_b` contain the indices
    of the headlines with maximum similarity (avoiding identical headlines). Let’s
    take a look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Using the block calculation approach, we have calculated almost a trillion similarities
    in just a few minutes. The results are interpretable as we have found similar,
    but not identical, documents. The different date shows that these are definitely
    also separate headlines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Finding Related Words'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we have analyzed documents with respect to their similarity. But
    the corpus implicitly has much more information, specifically information about
    related words. In our sense, words are related if they are used in the same documents.
    Words should be “more” related if they frequently appear together in the documents.
    As an example, consider the word *zealand*, which almost always occurs together
    with *new*; therefore, these two words are *related.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of working with a document-term matrix, we would like to work with
    a term-document matrix, which is just its transposed form. Instead of taking row
    vectors, we now take column vectors. However, we need to re-vectorize the data.
    Assume two words are infrequently used and that both happen to be present only
    once in the same headline. Their vectors would then be identical, but this is
    not what we are looking for. As an example, let’s think of a person named *Zaphod
    Beeblebrox*, who is mentioned in two articles. Our algorithm would assign a 100%
    related score to these words. Although this is correct, it is not very significant. We
    therefore only consider words that appear at least 1,000 times to get a decent
    statistical significance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The vocabulary is quite small, and we can directly calculate the cosine similarity.
    Changing row for column vectors, we just transpose the matrix, using the convenient
    `.T` method of NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Finding the largest entries is easiest if we convert it to a one-dimensional
    array, get the index of the sorted elements via `np.argsort`, and restore the
    original indices for the vocabulary lookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: It’s quite easy to interpret these results. For some word combinations like
    *climate change*, we have restored frequent bigrams. On the other hand, we can
    also see related words that don’t appear next to each other in the headlines,
    such as *drink* and *driving*. By using the transposed document-term matrix, we
    have performed a kind of *co-occurrence analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for Long-Running Programs like Syntactic Similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are some efficiency tips for long-running programs:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform benchmarking before waiting too long
  prefs: []
  type: TYPE_NORMAL
- en: Before performing calculations on the whole dataset, it is often useful to run
    a single calculation and extrapolate how long the whole algorithm will run and
    how much memory it will need. You should definitely try to understand how runtime
    and memory grow with increased complexity (linear, polynomial, exponential). Otherwise,
    you might have to wait for a long time and find out that after a few hours (or
    even days) only 10% progress memory is exhausted.
  prefs: []
  type: TYPE_NORMAL
- en: Try to divide your problem into smaller parts
  prefs: []
  type: TYPE_NORMAL
- en: Dividing a problem into smaller blocks can help here tremendously. As we have
    seen in the most similar document of the news corpus, this took only 20 minutes
    or so to run and used no significant memory. Compared to a naive approach, we
    would have found out after considerable runtime that the RAM would not have been
    enough. Furthermore, by dividing the problem into parts, you can make use of multicore
    architectures or even distribute the problem on many computers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we have prepared blueprints for vectorization and syntactic
    similarity. Almost all machine learning projects with text (such as classification,
    topic modeling, and sentiment detection) need document vectors at their very base.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that feature engineering is one of the most powerful levers for
    achieving outstanding performance with these sophisticated machine learning algorithms.
    Therefore, it’s an excellent idea to try different vectorizers, play with their
    parameters, and watch the resulting feature space. There are really many possibilities,
    and for good reason: although optimizing this takes some time, it is usually well-invested
    as the results of the subsequent steps in the analysis pipeline will benefit tremendously.'
  prefs: []
  type: TYPE_NORMAL
- en: The similarity measure used in this chapter is just an example for document
    similarities. For more complicated requirements, there are more sophisticated
    similarity algorithms that you will learn about in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Finding similar documents is a well-known problem in information retrieval.
    There are more sophisticated scores, such as [BM25](https://oreil.ly/s47TC). If
    you want a scalable solution, the very popular [Apache Lucene](http://lucene.apache.org)
    library (which is the basis of search engines like [Apache Solr](https://oreil.ly/R5y0E)
    and [Elasticsearch](https://oreil.ly/2qfAL)) makes use of this and is used for
    really big document collections in production scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will revisit similarity quite often. We will take
    a look at integrating word semantics and document semantics, and we will use transfer
    learning to leverage predefined language models that have been trained with extremely
    large documents corpora to achieve state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.xhtml#idm45634198925240-marker)) In later chapters, we will take
    a look at other possibilities of vectorizing words ([Chapter 10](ch10.xhtml#ch-embeddings))
    and documents ([Chapter 11](ch11.xhtml#ch-sentiment)).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.xhtml#idm45634198893064-marker)) There are much more sophisticated
    algorithms for determining the vocabulary, like [SentencePiece](https://oreil.ly/A6TEl)
    and [BPE](https://oreil.ly/tVDgu), which are worth taking a look at if you want
    to reduce the number of features.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.xhtml#idm45634198361912-marker)) Confusingly, `numpy.dot` is used
    both for the dot product (inner product) and for matrix multiplication. If Numpy
    detects two row or column vectors (i.e., one-dimensional arrays) with the same
    dimension, it calculates the dot product and yields a scalar. If not and the passed
    two-dimensional arrays are suitable for matrix multiplication, it performs this
    operation and yields a matrix. All other cases produce errors. That’s convenient,
    but it’s a lot of heuristics.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.xhtml#idm45634198247384-marker)) See [Chapter 8](ch08.xhtml#ch-topicmodels)
    for more on LDA.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.xhtml#idm45634197672360-marker)) See, for example, the [definition
    of entropy](https://oreil.ly/3qTpX) as a measure of uncertainty and information.
    Basically, this says that a low-probability value carries more information than
    a more likely value.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.xhtml#idm45634197156184-marker)) This is, of course, related to the
    stop word list that has already been used. In news articles, the most common words
    are stop words. In domain-specific texts, it might be completely different. Using
    stop words is often the safer choice, as these lists have been curated.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.xhtml#idm45634196754328-marker)) It would be growing exponentially
    if all word combinations were possible and would be used. As this is unlikely,
    the dimensions are growing subexponentially.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.xhtml#idm45634196311384-marker)) We have chosen 10,000 dimensions,
    as the resulting matrix can be kept in RAM (using roughly 1 GB should be possible
    even on moderate hardware).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.xhtml#idm45634196279640-marker)) All calculations can be sped up
    considerably by using processor-specific libraries, e.g., by subscribing to the
    Intel channel in Anaconda. This will use AVX2, AVX-512, and similar instructions
    and use parallelization. [MKL](https://oreil.ly/pa1zj) and [OpenBlas](https://oreil.ly/jZYSG)
    are good candidates for linear algebra libraries.
  prefs: []
  type: TYPE_NORMAL
