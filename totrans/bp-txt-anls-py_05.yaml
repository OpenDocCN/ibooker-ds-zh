- en: Chapter 5\. Feature Engineering and Syntactic Similarity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 特征工程和句法相似性
- en: As we saw in [Chapter 1](ch01.xhtml#ch-exploration), text is significantly different
    from structured data. One of the most striking differences is that text is represented
    by words, while structured data (mostly) uses numbers. From a scientific point
    of view, centuries of mathematical research have led to an extremely good understanding
    of numbers and sophisticated methods. Information science has picked up that mathematical
    research, and many creative algorithms have been invented on top of that. Recent
    advances in machine learning have generalized a lot of formerly very specific
    algorithms and made them applicable to many different use cases. These methods
    “learn” directly from data and provide an unbiased view.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.xhtml#ch-exploration)中看到的那样，文本与结构化数据有着显著的不同。最引人注目的差异之一是，文本由单词表示，而结构化数据（大部分情况下）使用数字。从科学角度来看，数学研究的几个世纪已经对数字有了非常好的理解和复杂的方法。信息科学吸收了这些数学研究，并在此基础上发明了许多创造性的算法。机器学习的最新进展已经将许多以前非常特定的算法泛化，并使其适用于许多不同的用例。这些方法直接从数据中“学习”，并提供了一个无偏见的视角。
- en: To use these instruments, we have to find a mapping of text to numbers. Considering
    the richness and complexity of text, it is clear that a single number will not
    be enough to represent the meaning of a document. Something more complex is needed.
    The natural extension of real numbers in mathematics is a tuple of real numbers,
    called a *vector*. Almost all text representations in text analytics and machine
    learning use vectors; see [Chapter 6](ch06.xhtml#ch-classification) for more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这些工具，我们必须找到文本到数字的映射。考虑到文本的丰富性和复杂性，显然单一的数字无法代表文档的含义。需要更复杂的东西。在数学中，实数的自然扩展是一组实数，称为*向量*。几乎所有文本分析和机器学习中的文本表示都使用向量；详见[第6章](ch06.xhtml#ch-classification)。
- en: Vectors live in a vector space, and most vector spaces have additional properties
    such as norms and distances, which will be helpful for us as they imply the concept
    of similarity. As we will see in subsequent chapters, measuring the similarity
    between documents is absolutely crucial for most text analytics applications,
    but it is also interesting on its own.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 向量存在于向量空间中，大多数向量空间具有额外的属性，如范数和距离，这对我们是有帮助的，因为它们暗示了相似性的概念。正如我们将在后续章节中看到的，测量文档之间的相似性对于大多数文本分析应用至关重要，但它本身也很有趣。
- en: What You’ll Learn and What We’ll Build
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么，我们将构建什么
- en: In this chapter we talk about the vectorization of documents. This means we
    will convert unstructured text into vectors that contain numbers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论文档的向量化。这意味着我们将把非结构化文本转换为包含数字的向量。
- en: There are quite a few ways to vectorize documents. As document vectorization
    is the basis for all machine learning tasks, we will spend some time designing
    and implementing our own vectorizer. You can use that as a blueprint if you need
    a specialized vectorizer for your own projects.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以用来向量化文档。由于文档向量化是所有机器学习任务的基础，我们将花一些时间来设计和实现我们自己的向量化器。如果您需要一个专门用于自己项目的向量化器，您可以将其用作蓝图。
- en: 'Afterward, we will focus on two popular models that are already implemented
    in scikit-learn: the bag-of words model and the TF-IDF improvements to it. We
    will download a large dataset of documents and vectorize it with these methods.
    As you will see, there can be many problems that are related to data volume and
    scalability.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将关注两种已在scikit-learn中实现的流行模型：词袋模型和TF-IDF改进模型。我们将使用这些方法下载大量文档数据集并进行向量化。正如您将看到的那样，数据量和可扩展性可能会带来许多问题。
- en: Although vectorization is a base technology for more sophisticated machine learning
    algorithms, it can also be used on its own to calculate similarities between documents.
    We will take a detailed look at how this works, how it can be optimized, and how
    we can make it scalable. For a richer representation of words, see [Chapter 10](ch10.xhtml#ch-embeddings), and
    for a more contextualized approach, see [Chapter 11](ch11.xhtml#ch-sentiment).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管向量化是更复杂的机器学习算法的基础技术，它也可以单独用于计算文档之间的相似性。我们将详细讨论其工作原理、优化方法以及如何实现可扩展性。对于更丰富的词表示，请参阅[第10章](ch10.xhtml#ch-embeddings)，对于更上下文化的方法，请参阅[第11章](ch11.xhtml#ch-sentiment)。
- en: After studying this chapter, you will understand how to convert documents to
    numbers (*vectors*) using words or combinations as *features*.^([1](ch05.xhtml#idm45634198925240))
    We will try different methods of vectorizing documents, and you will be able to
    determine the correct method for your use case. You will learn why the similarity
    of documents is important and a standard way to calculate it. We will go into
    detail with an example that has many documents and show how to vectorize them
    and calculate similarities effectively.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了本章后，您将了解如何使用单词或组合作为*特征*将文档转换为数字（*向量*）。^([1](ch05.xhtml#idm45634198925240))
    我们将尝试不同的文档向量化方法，您将能够确定适合您用例的正确方法。您将了解文档相似性为何重要以及计算它的标准方法。我们将通过一个包含许多文档的示例详细介绍如何有效地向量化它们并计算相似性。
- en: The first section introduces the concept of a vectorizer by actually building
    your own simple one. This can be used as a blueprint for more sophisticated vectorizers
    that you might have to build in your own projects. Counting word occurrences and
    using them as vectors is called *bag-of-words* and already creates very versatile
    models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分通过实际构建一个简单的向量化器介绍了向量化器的概念。这可以作为您在项目中必须构建的更复杂的向量化器的蓝图。计算单词出现次数并将其用作向量称为*词袋模型*，并且已经创建了非常多功能的模型。
- en: Together with the dataset (which has more than 1,000,000 news headlines), we
    introduce a use case and present a scalable blueprint architecture in the TF-IDF
    section. We will build a blueprint for vectorizing documents and a similarity
    search for documents. Even more challenging, we will try to identify the most
    similar (but nonidentical) headlines in this corpus.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据集（拥有超过1,000,000个新闻标题）一起，我们在TF-IDF部分介绍了一个用例，并展示了可扩展的蓝图架构。我们将建立一个文档向量化的蓝图和文档的相似性搜索。更具挑战性的是，我们将尝试识别语料库中最相似（但非完全相同）的标题。
- en: A Toy Dataset for Experimentation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于实验的玩具数据集
- en: Quite surprisingly, a lot of experiments have shown that for many text analytics
    problems it is enough to know which words appear in documents. It is not necessary
    to understand the meaning of the words or take word order into account. As the
    underlying mappings are particularly easy and fast to calculate, we will start
    with these mappings and use the words as features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 非常令人惊讶的是，许多实验证明，对于许多文本分析问题，只需知道单词是否出现在文档中就足够了。不必理解单词的含义或考虑单词顺序。由于底层映射特别简单且计算速度快，我们将从这些映射开始，并使用单词作为特征。
- en: 'For the first blueprints, we will concentrate on the methods and therefore
    use a few sentences from the novel [*A Tale of Two Cities*](https://oreil.ly/rfmPH)
    by Charles Dickens as a toy dataset. We will use the following sentences:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个蓝图，我们将集中在方法上，因此使用查尔斯·狄更斯的小说[*双城记*](https://oreil.ly/rfmPH)中的几句话作为玩具数据集。我们将使用以下句子：
- en: It was the best of times.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最好的时代已经来临。
- en: It was the worst of times.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最坏的时代已经过去。
- en: It was the age of wisdom.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智慧的时代已经来临。
- en: It was the age of foolishness.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 愚蠢的时代已经过去。
- en: 'Blueprint: Building Your Own Vectorizer'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：建立您自己的向量化器
- en: As vectorizing documents is the base for nearly all of the following chapters
    in this book, we take an in-depth look at how vectorizers work. This works best
    by implementing our own vectorizer. You can use the methods in this section if
    you need to implement a custom vectorizer in your own projects or need to adapt
    an existing vectorizer to your specific requirements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于向量化文档是本书后续章节的基础，我们深入探讨了向量化器的工作原理。通过实现我们自己的向量化器来最好地实现这一点。如果您需要在自己的项目中实现自定义向量化器或需要根据特定要求调整现有向量化器，可以使用本节中的方法。
- en: To make it as simple as possible, we will implement a so-called *one-hot vectorizer*.
    This vectorizer creates binary vectors from documents by noting if a word appears
    in a document or not, yielding 1 or 0, respectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能简化，我们将实现所谓的*单热向量化器*。该向量化器通过记录单词是否出现在文档中来创建二进制向量，如果出现则为1，否则为0。
- en: We will start by creating a vocabulary and assigning numbers to words, then
    perform the vectorization, and finally analyze similarity in this binary space.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始创建一个词汇表并为单词分配编号，然后进行向量化，并在此二进制空间中分析相似性。
- en: Enumerating the Vocabulary
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 枚举词汇表
- en: Starting with the words as features, we have to find a way to convert words
    to the dimensions of the vectors. Extracting the words from the text is done via
    tokenization, as explained in [Chapter 2](ch02.xhtml#ch-api).^([2](ch05.xhtml#idm45634198893064))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从单词作为特征开始，我们必须找到一种将单词转换为向量维度的方法。从文本中提取单词通过标记化完成，如[第2章](ch02.xhtml#ch-api)中解释的那样。^([2](ch05.xhtml#idm45634198893064))
- en: 'As we are interested only in whether a word appears in a document or not, we
    can just enumerate the words:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只关心一个单词是否出现在文档中，所以我们只需列举这些单词：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Out:`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '| It | 0 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 它 | 0 |'
- en: '| age | 1 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 年龄 | 1 |'
- en: '| best | 2 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 最好 | 2 |'
- en: '| foolishness | 3 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 愚蠢 | 3 |'
- en: '| it | 4 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 它 | 4 |'
- en: '| of | 5 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 的 | 5 |'
- en: '| the | 6 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 的 | 6 |'
- en: '| times | 7 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 时代 | 7 |'
- en: '| was | 8 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 8 |'
- en: '| wisdom | 9 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 智慧 | 9 |'
- en: '| worst | 10 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 最坏 | 10 |'
- en: As you can see, the words have been numbered according to their first occurrence.
    This is what we call a *dictionary*, consisting of words (the vocabulary) and
    their respective numbers. Instead of having to refer to words, we can now use
    the numbers and arrange them in the following vectors.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，单词根据它们第一次出现的顺序进行了编号。这就是我们所说的“字典”，包括单词（词汇表）及其相应的编号。现在，我们可以使用这些数字而不是单词来排列它们到以下向量中。
- en: Vectorizing Documents
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档向量化
- en: To compare vectors, calculate similarities, and so forth, we have to make sure
    that vectors for each document have the same number of dimensions. To achieve
    that, we use the same dictionary for all documents. If the document doesn’t contain
    a word, we just put a 0 at the corresponding position; otherwise, we will use
    a 1\. By convention, row vectors are used for documents. The dimension of the
    vectors is as big as the length of the dictionary. In our example, this is not
    a problem as we have only a few words. However, in large projects, the vocabulary
    can easily exceed 100,000 words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较向量、计算相似性等等，我们必须确保每个文档的向量具有相同的维度。为了实现这一点，我们对所有文档使用相同的词典。如果文档中不包含某个词，我们就在相应的位置放置一个0；否则，我们将使用1。按照惯例，行向量用于表示文档。向量的维度与词典的长度一样大。在我们的例子中，这不是问题，因为我们只有少数几个词。然而，在大型项目中，词汇表很容易超过10万个词。
- en: 'Let’s calculate the one-hot encoding of all sentences before actually using
    a library for this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在实际使用库之前计算所有句子的一热编码：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For each sentence, we have now calculated a vector representation. Converting
    documents to one-hot vectors, we have lost information about how often words occur
    in documents and in which order.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个句子，我们现在计算了一个向量表示。将文档转换为一热向量时，我们丢失了关于单词在文档中出现频率及顺序的信息。
- en: Out-of-vocabulary documents
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超出词汇表的文档
- en: 'What happens if we try to keep the vocabulary fixed and add new documents?
    That depends on whether the words of the documents are already contained in the
    dictionary. Of course, it can happen that all words are already known:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试保持词汇表固定并添加新文档会发生什么？这取决于文档的单词是否已经包含在词典中。当然，可能所有单词都已知：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Out:`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, the opposite is also quite possible. If we try to vectorize a sentence
    with only unknown words, we get a null vector:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，反之也完全可能。如果我们试图将只包含未知单词的句子向量化，我们会得到一个零向量：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Out:`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This sentence does not “interact” with the other sentences in the corpus. From
    a strict point of view, this sentence is not similar to any sentence in the corpus.
    This is no problem for a single sentence; if this happens more frequently, the
    vocabulary or the corpus needs to be adjusted.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子在语料库中与其他句子没有“交互”。从严格的角度来看，这个句子与语料库中的任何句子都不相似。对于单个句子来说，这没有问题；如果这种情况经常发生，需要调整词汇表或语料库。
- en: The Document-Term Matrix
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档-词项矩阵
- en: 'Arranging the row vectors for each document in a matrix with the rows enumerating
    the documents, we arrive at the document-term matrix. The document-term matrix
    is the vector representation of all documents and the most basic building block
    for nearly all machine learning tasks throughout this book. In this chapter, we
    will use it for calculating document similarities:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个文档的行向量排列成一个矩阵，其中行枚举文档，我们得到了文档-词项矩阵。文档-词项矩阵是所有文档的向量表示，也是本书中几乎所有机器学习任务的最基本构建块。在本章中，我们将用它来计算文档相似性：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Out:`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_05in01.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_05in01.jpg)'
- en: 'Be careful: using lists and arrays for the document-term matrix works best
    with a small vocabulary. With large vocabularies, we will have to find a cleverer
    representation. Scikit-learn takes care of this and uses so-called sparse vectors
    and matrices from [SciPy](https://oreil.ly/yk1wx).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对于文档-词项矩阵，使用列表和数组在词汇量较小时效果最佳。对于大词汇量，我们将不得不找到更聪明的表示方式。Scikit-learn负责此事，并使用所谓的稀疏向量和矩阵来自[SciPy](https://oreil.ly/yk1wx)。
- en: Calculating similarities
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算相似性
- en: 'Calculating the similarities between documents works by calculating the number
    of common 1s at the corresponding positions. In one-hot encoding, this is an extremely
    fast operation, as it can be calculated on the bit level by `AND`ing the vectors
    and counting the number of 1s in the resulting vector. Let’s calculate the similarity
    of the first two sentences:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 计算文档之间的相似性是通过计算对应位置的共同1的数量来进行的。在一热编码中，这是一种非常快速的操作，因为可以通过对向量进行`AND`运算并计算结果向量中的1的数量来计算。让我们计算前两个句子的相似性：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Another possible way to calculate the similarity that we will encounter frequently
    is using *scalar product* (often called *dot product*) of the two document vectors.
    The scalar product is calculated by multiplying corresponding components of the
    two vectors and adding up these products. By observing the fact that a product
    can only be 1 if both factors are 1, we effectively calculate the number of common
    1s in the vectors. Let’s try it:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常会遇到的另一种计算相似性的可能方式是使用两个文档向量的*标量积*（通常称为*点积*）。标量积通过将两个向量的对应分量相乘并将这些乘积相加来计算。通过观察乘积只有在两个因子都为1时才为1的事实，我们有效地计算了向量中共同1的数量。让我们试一试：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Out:`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Similarity Matrix
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相似性矩阵
- en: 'If we are interested in finding the similarity of all documents to each other,
    there is a fantastic shortcut for calculating all the numbers with just one command!
    Generalizing the formula from the previous section, we find the similarity of
    document *i* and document *j* to be as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有兴趣找出所有文档之间的相似性，有一个很棒的快捷方式可以只用一个命令计算所有的数值！从前一节的公式推广，我们得出文档 *i* 和文档 *j* 的相似性如下：
- en: <math alttext="upper S Subscript i j Baseline equals normal d Subscript i Baseline
    dot normal d Subscript j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="normal">d</mi> <mi>i</mi></msub> <mo>·</mo>
    <msub><mi mathvariant="normal">d</mi> <mi>j</mi></msub></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S Subscript i j Baseline equals normal d Subscript i Baseline
    dot normal d Subscript j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="normal">d</mi> <mi>i</mi></msub> <mo>·</mo>
    <msub><mi mathvariant="normal">d</mi> <mi>j</mi></msub></mrow></math>
- en: 'If we want to use the document-term matrix from earlier, we can write the scalar
    product as a sum:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要使用之前的文档-词项矩阵，我们可以将标量积写成一个和：
- en: <math alttext="upper S Subscript i j Baseline equals sigma-summation Underscript
    k Endscripts upper D Subscript i k Baseline upper D Subscript j k Baseline equals
    sigma-summation Underscript k Endscripts upper D Subscript i k Baseline left-parenthesis
    upper D Superscript upper T Baseline right-parenthesis Subscript k j Baseline
    equals left-parenthesis normal upper D dot normal upper D Superscript upper T
    Baseline right-parenthesis Subscript i j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mo>∑</mo> <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>D</mi> <mrow><mi>j</mi><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mo>∑</mo>
    <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mrow><mo>(</mo><msup><mi>D</mi>
    <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>k</mi><mi>j</mi></mrow></msub> <mo>=</mo>
    <msub><mrow><mo>(</mo><mi mathvariant="normal">D</mi><mo>·</mo><msup><mrow><mi
    mathvariant="normal">D</mi></mrow> <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S Subscript i j Baseline equals sigma-summation Underscript
    k Endscripts upper D Subscript i k Baseline upper D Subscript j k Baseline equals
    sigma-summation Underscript k Endscripts upper D Subscript i k Baseline left-parenthesis
    upper D Superscript upper T Baseline right-parenthesis Subscript k j Baseline
    equals left-parenthesis normal upper D dot normal upper D Superscript upper T
    Baseline right-parenthesis Subscript i j"><mrow><msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mo>∑</mo> <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>D</mi> <mrow><mi>j</mi><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mo>∑</mo>
    <mi>k</mi></msub> <msub><mi>D</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mrow><mo>(</mo><msup><mi>D</mi>
    <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>k</mi><mi>j</mi></mrow></msub> <mo>=</mo>
    <msub><mrow><mo>(</mo><mi mathvariant="normal">D</mi><mo>·</mo><msup><mrow><mi
    mathvariant="normal">D</mi></mrow> <mi>T</mi></msup> <mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
- en: So, this is just the matrix product of our document-term matrix with itself
    transposed. In Python, that’s now easy to calculate (the sentences in the output
    have been added for easier checking the similarity):^([3](ch05.xhtml#idm45634198361912))
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这只是我们的文档-词项矩阵与其转置的矩阵乘积。在Python中，这现在很容易计算（输出中的句子已添加，以便更轻松地检查相似性）：^([3](ch05.xhtml#idm45634198361912))
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Out:`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Obviously, the highest numbers are on the diagonal, as each document is most
    similar to itself. The matrix has to be symmetric, as document *A* has the same
    similarity to *B* as *B* to *A*. Apart from that, we can see that the second sentence
    is on average most similar to all others, whereas the third and last document
    is the most similar pairwise (they differ only by one word). The same would be
    true of the first and second documents if we ignored case.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，最高的数值位于对角线上，因为每个文档最相似于它自己。矩阵必须是对称的，因为文档 *A* 与 *B* 的相似性与 *B* 与 *A* 的相似性相同。除此之外，我们可以看到第二个句子平均来说与所有其他句子最相似，而第三个和最后一个文档成对最相似（它们仅相差一个单词）。如果忽略大小写，第一个和第二个文档也是如此。
- en: Understanding how a document vectorizer works is crucial for implementing your
    own, but it’s also helpful for appreciating all the functionalities and parameters
    of existing vectorizers. This is why we have implemented our own. We have taken
    a detailed look at the different stages of vectorization, starting with building
    a vocabulary and then converting the documents to binary vectors.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 理解文档向量化器的工作原理对于实现自己的向量化器至关重要，但也有助于欣赏现有向量化器的所有功能和参数。这就是为什么我们实现了我们自己的向量化器。我们详细查看了向量化的不同阶段，从构建词汇表开始，然后将文档转换为二进制向量。
- en: Afterward, we analyzed the similarity of the documents. It turned out that the
    dot product of their corresponding vectors is a good measure for this similarity.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，我们分析了文档之间的相似性。事实证明，它们对应向量的点积是一个很好的度量。
- en: One-hot vectors are also used in practice, for example, in document classification
    and clustering. However, scikit-learn also offers more sophisticated vectorizers,
    which we will use in the next sections.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 独热向量在实践中也被广泛使用，例如在文档分类和聚类中。然而，scikit-learn 还提供了更复杂的向量化器，在接下来的几节中我们将使用它们。
- en: Bag-of-Words Models
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋模型
- en: One-hot encoding has already provided us with a basic representation of documents
    as vectors. However, it did not take care of words appearing many times in documents.
    If we want to calculate the frequency of words for each document, then we should
    use what is called a *bag-of-words* representation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码已经为我们提供了文档的基本表示形式作为向量。然而，它没有处理文档中单词的出现次数。如果我们想计算每个文档中单词的频率，那么我们应该使用所谓的*词袋*表示法。
- en: Although somewhat simplistic, these models are in wide use. For cases such as
    classification and sentiment detection, they work reasonably. Moreover, there
    are topic modeling methods like Latent Dirichlet Allocation (LDA), which explicitly
    requires a bag-of-words model.^([4](ch05.xhtml#idm45634198247384))
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有些简单，但这些模型被广泛使用。对于分类和情感检测等情况，它们表现合理。此外，还有像潜在狄利克雷分配（LDA）这样的主题建模方法，显式地需要词袋模型。^([4](ch05.xhtml#idm45634198247384))
- en: 'Blueprint: Using scikit-learn’s CountVectorizer'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用 scikit-learn 的 CountVectorizer
- en: Instead of implementing a bag-of-words model on our own, we use the algorithm
    that scikit-learn provides.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不必自己实现词袋模型，我们使用 scikit-learn 提供的算法。
- en: 'Notice that the corresponding class is called `CountVectorizer`, which is our
    first encounter with feature extraction in scikit-learn. We will take a detailed
    look at the design of the classes and in which order their methods should be called:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到相应的类被称为`CountVectorizer`，这是我们在 scikit-learn 中进行特征提取的第一次接触。我们将详细查看这些类的设计及其方法调用的顺序：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Our example sentences from the one-hot encoder is really trivial, as no sentence
    in our dataset contains words more than once. Let’s add some more sentences and
    use that as a basis for the `CountVectorizer`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来自独热编码的示例句子实际上非常简单，因为我们的数据集中没有句子包含多次单词。让我们再添加一些句子，并以此为基础使用 `CountVectorizer`。
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`CountVectorizer` works in two distinct phases: first it has to learn the vocabulary;
    afterward it can transform the documents to vectors.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 分为两个明显的阶段：首先它必须学习词汇表；之后它可以将文档转换为向量。'
- en: Fitting the vocabulary
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拟合词汇表
- en: 'First, it needs to learn about the vocabulary. This is simpler now, as we can
    just pass the array with the sentences:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它需要学习词汇表。现在这更简单了，因为我们可以直接传递包含句子的数组：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Don’t worry about all these parameters; we will talk about the important ones
    later. Let’s first see what `CountVectorizer` used as vocabulary, which is called
    *feature names* here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 不要担心所有这些参数；我们稍后会讨论重要的参数。让我们首先看看 `CountVectorizer` 使用的词汇表，这里称为*特征名称*：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Out:`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We have created a vocabulary and so-called features using `CountVectorizer`.
    Conveniently, the vocabulary is sorted alphabetically, which makes it easier for
    us to decide whether a specific word is included.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个词汇表和所谓的特征，使用 `CountVectorizer`。方便地，词汇表按字母顺序排序，这使我们更容易决定是否包含特定单词。
- en: Transforming the documents to vectors
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将文档转换为向量
- en: 'In the second step, we will use `CountVectorizer` to transform the documents
    to the vector representation:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，我们将使用 `CountVectorizer` 将文档转换为向量表示：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The result is the document-term matrix that we have already encountered in
    the previous section. However, it is a different object, as `CountVectorizer`
    has created a sparse matrix. Let’s check:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是我们在上一节中已经遇到的文档-术语矩阵。然而，它是一个不同的对象，因为 `CountVectorizer` 创建了一个稀疏矩阵。让我们来检查一下：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`Out:`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Sparse matrices are extremely efficient. Instead of storing 6 × 20 = 120 elements,
    it just has to save 38! Sparse matrices achieve that by skipping all zero elements.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵非常高效。它只需保存 38 个元素，而不是存储 6 × 20 = 120 个元素！稀疏矩阵通过跳过所有零元素来实现这一点。
- en: 'Let’s try to recover our former document-term matrix. For this, we must transform
    the sparse matrix to a (dense) array. To make it easier to read, we convert it
    into a Pandas `DataFrame`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着恢复我们先前的文档-术语矩阵。为此，我们必须将稀疏矩阵转换为（稠密的）数组。为了使其更易读，我们将其转换为 Pandas 的 `DataFrame`：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_05in02.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_05in02.jpg)'
- en: The document-term matrix looks very similar to the one from our one-hot vectorizer.
    Note, however, that the columns are in alphabetical order, and observe several
    2s in the fifth row. This originates from the document `"John likes to watch movies.
    Mary likes movies too."`, which has many duplicate words.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 文档-词项矩阵看起来与我们的单热向量化器非常相似。但请注意，列是按字母顺序排列的，并且注意第五行有几个2。这源自文档`"John likes to watch
    movies. Mary likes movies too."`，其中有很多重复词语。
- en: 'Blueprint: Calculating Similarities'
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：计算相似性
- en: 'Finding similarities between documents is now more difficult as it is not enough
    to count the common 1s in the documents. In general, the number of occurrences
    of each word can be bigger, and we have to take that into account.  The dot product
    cannot be used for this, as it is also sensitive to the length of the vector (the
    number of words in the documents). Also, a Euclidean distance is not very useful
    in high-dimensional vector spaces. This is why most commonly the angle between
    document vectors is used as a measure of similarity. The cosine of the angle between
    two vectors is defined by the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在文档之间找到相似性更加困难，因为仅仅计算文档中共同出现的1不再足够。一般来说，每个词的出现次数可能更多，我们必须考虑这一点。不能使用点积来做这个，因为它也对向量的长度（文档中的词数）敏感。此外，欧氏距离在高维向量空间中并不是很有用。这就是为什么通常使用文档向量之间的角度作为相似性的度量。两个向量之间的夹角的余弦定义如下：
- en: <math alttext="normal c normal o normal s left-parenthesis bold a comma bold
    b right-parenthesis equals StartFraction bold a dot bold b Over StartAbsoluteValue
    EndAbsoluteValue bold a StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue
    EndAbsoluteValue bold b StartAbsoluteValue EndAbsoluteValue EndFraction equals
    StartFraction sigma-summation a Subscript i Baseline b Subscript i Baseline Over
    StartRoot sigma-summation a Subscript i Baseline a Subscript i Baseline EndRoot
    StartRoot sigma-summation b Subscript i Baseline b Subscript i Baseline EndRoot
    EndFraction"><mrow><mi>cos</mi> <mrow><mo>(</mo> <mi>𝐚</mi> <mo>,</mo> <mi>𝐛</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>𝐚</mi><mo>·</mo><mi>𝐛</mi></mrow>
    <mrow><mo>|</mo><mo>|</mo><mi>𝐚</mi><mo>|</mo><mo>|</mo><mo>·</mo><mo>|</mo><mo>|</mo><mi>𝐛</mi><mo>|</mo><mo>|</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi>
    <mi>i</mi></msub></mrow> <mrow><msqrt><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <msub><mi>a</mi> <mi>i</mi></msub></mrow></msqrt> <msqrt><mrow><mo>∑</mo><msub><mi>b</mi>
    <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub></mrow></msqrt></mrow></mfrac></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal c normal o normal s left-parenthesis bold a comma bold
    b right-parenthesis equals StartFraction bold a dot bold b Over StartAbsoluteValue
    EndAbsoluteValue bold a StartAbsoluteValue EndAbsoluteValue dot StartAbsoluteValue
    EndAbsoluteValue bold b StartAbsoluteValue EndAbsoluteValue EndFraction equals
    StartFraction sigma-summation a Subscript i Baseline b Subscript i Baseline Over
    StartRoot sigma-summation a Subscript i Baseline a Subscript i Baseline EndRoot
    StartRoot sigma-summation b Subscript i Baseline b Subscript i Baseline EndRoot
    EndFraction"><mrow><mi>cos</mi> <mrow><mo>(</mo> <mi>𝐚</mi> <mo>,</mo> <mi>𝐛</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>𝐚</mi><mo>·</mo><mi>𝐛</mi></mrow>
    <mrow><mo>|</mo><mo>|</mo><mi>𝐚</mi><mo>|</mo><mo>|</mo><mo>·</mo><mo>|</mo><mo>|</mo><mi>𝐛</mi><mo>|</mo><mo>|</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi></msub> <msub><mi>b</mi>
    <mi>i</mi></msub></mrow> <mrow><msqrt><mrow><mo>∑</mo><msub><mi>a</mi> <mi>i</mi></msub>
    <msub><mi>a</mi> <mi>i</mi></msub></mrow></msqrt> <msqrt><mrow><mo>∑</mo><msub><mi>b</mi>
    <mi>i</mi></msub> <msub><mi>b</mi> <mi>i</mi></msub></mrow></msqrt></mrow></mfrac></mrow></math>
- en: 'Scikit-learn simplifies this calculation by offering a `cosine_similarity`
    utility function. Let’s check the similarity of the first two sentences:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn通过提供`cosine_similarity`实用函数简化了这个计算。让我们来检查前两个句子的相似性：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Out:`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Compared to our handmade similarity in the earlier sections, `cosine_similarity`
    offers some advantages, as it is properly normalized and can take only values
    between 0 and 1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期章节中手工相似性比较起来，`cosine_similarity`提供了一些优势，因为它被适当地标准化，并且值只能在0到1之间。
- en: 'Calculating the similarity of all documents is of course also possible; scikit-learn
    has optimized the `cosine_similarity`, so it is possible to directly pass matrices:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 计算所有文档的相似性当然也是可能的；scikit-learn已经优化了`cosine_similarity`，因此可以直接传递矩阵：
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|   | 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 1.000000 | 0.833333 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.000000 | 0.833333 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
- en: '| 1 | 0.833333 | 1.000000 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.833333 | 1.000000 | 0.666667 | 0.666667 | 0.000000 | 0.000000 |'
- en: '| 2 | 0.666667 | 0.666667 | 1.000000 | 0.833333 | 0.000000 | 0.000000 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.666667 | 0.666667 | 1.000000 | 0.833333 | 0.000000 | 0.000000 |'
- en: '| 3 | 0.666667 | 0.666667 | 0.833333 | 1.000000 | 0.000000 | 0.000000 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.666667 | 0.666667 | 0.833333 | 1.000000 | 0.000000 | 0.000000 |'
- en: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.524142 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.524142 |'
- en: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.524142 | 1.000000 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.524142 | 1.000000 |'
- en: Again, the matrix is symmetric with the highest values on the diagonal. It’s
    also easy to see that document pairs 0/1 and 2/3 are most similar. Documents 4/5
    have no similarity at all to the other documents but have some similarity to each
    other. Taking a look back at the sentences, this is exactly what one would expect.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，矩阵在对角线上具有最高值是对称的。很容易看出文档对0/1和2/3最相似。文档4/5与其他文档没有任何相似性，但它们彼此之间有些相似性。回顾这些句子，这正是人们所期望的。
- en: Bag-of-words models are suitable for a variety of use cases. For classification,
    sentiment detection, and many topic models, they create a bias toward the most
    frequent words as they have the highest numbers in the document-term matrix. Often
    these words do not carry much meaning and could be defined as stop words.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型适用于各种用例。对于分类、情感检测和许多主题模型，它们会偏向于最频繁出现的词语，因为它们在文档-词项矩阵中的数值最高。通常这些词语并不带有太多意义，可以定义为停用词。
- en: As these would be highly domain-specific, a more generic approach “punishes”
    words that appear too often in the corpus of all documents. This is called a *TF-IDF
    model* and will be discussed in the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些方法高度依赖领域特定，更通用的方法会“惩罚”那些在所有文档语料库中出现太频繁的词语。这被称为*TF-IDF模型*，将在下一节讨论。
- en: TF-IDF Models
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF模型
- en: 'In our previous example, many sentences started with the words “it was the
    time of.” This contributed a lot to their similarity, but in reality, the actual
    information you get by the words is minimal. TF-IDF will take care of that by
    counting the number of total word occurrences. It will reduce weights of frequent
    words and at the same time increase the weights of uncommon words. Apart from
    the information-theoretical measure,^([5](ch05.xhtml#idm45634197672360)) this
    is also something that you can observe when reading documents: if you encounter
    uncommon words, it is likely that the author wants to convey an important message
    with them.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中，许多句子以“这是时候”开头。这在很大程度上增加了它们的相似性，但实际上，您通过这些词获得的实际信息很少。TF-IDF通过计算总词出现次数来处理这一点。它会减少常见词的权重，同时增加不常见词的权重。除了信息理论的测量[^5]之外，在阅读文档时，您还可以观察到：如果遇到不常见的词，作者很可能想要用它们传达重要信息。
- en: Optimized Document Vectors with TfidfTransformer
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TfidfTransformer优化文档向量
- en: 'As we saw in [Chapter 2](ch02.xhtml#ch-api), a better measure for information
    compared to counting is calculating the inverted document frequency and using
    a penalty for very common words. The TF-IDF weight can be calculated from the
    bag-of-words model. Let’s try this again with the previous model and see how the
    weights of the document-term matrix change:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第2章](ch02.xhtml#ch-api)中所见，与计数相比，更好的信息衡量方法是计算倒排文档频率，并对非常常见的单词使用惩罚。TF-IDF权重可以从词袋模型计算出来。让我们再次尝试使用先前的模型，看看文档-术语矩阵的权重如何变化：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`Out:`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_05in03.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_05in03.jpg)'
- en: 'As you can see, some words have been scaled to smaller values (like “it”),
    while others have not been scaled down so much (like “wisdom”). Let’s see the
    effect on the similarity matrix:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，有些词已经被缩小了（例如“it”），而其他词则没有被缩小那么多（例如“wisdom”）。让我们看看对相似性矩阵的影响：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | 0 | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|   | 0 | 1 | 2 | 3 | 4 | 5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 1.000000 | 0.675351 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1.000000 | 0.675351 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
- en: '| 1 | 0.675351 | 1.000000 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.675351 | 1.000000 | 0.457049 | 0.457049 | 0.00000 | 0.00000 |'
- en: '| 2 | 0.457049 | 0.457049 | 1.000000 | 0.675351 | 0.00000 | 0.00000 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.457049 | 0.457049 | 1.000000 | 0.675351 | 0.00000 | 0.00000 |'
- en: '| 3 | 0.457049 | 0.457049 | 0.675351 | 1.000000 | 0.00000 | 0.00000 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.457049 | 0.457049 | 0.675351 | 1.000000 | 0.00000 | 0.00000 |'
- en: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.43076 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.43076 |'
- en: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.43076 | 1.000000 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.43076 | 1.000000 |'
- en: We get exactly the effect we have hoped for! Document pairs 0/1 and 2/3 are
    still very similar, but the number has also decreased to a more reasonable level
    as the document pairs differ in *significant words*. The more common words now
    have lower weights.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实达到了期望的效果！文档对0/1和2/3仍然非常相似，但数字也减少到一个更合理的水平，因为文档对在*重要词语*上有所不同。现在常见词的权重较低。
- en: Introducing the ABC Dataset
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入ABC数据集
- en: 'As a real-word use case, we will take a [dataset from Kaggle](https://oreil.ly/hg5R3)
    that contains news headlines. Headlines originate from Australian news source
    ABC and are from 2003 to 2017\. The CSV file contains only a timestamp and the
    headline without punctuation in lowercase. We load the CSV file into a Pandas
    `DataFrame` and take a look at the first few documents:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实际的用例，我们将使用一份来自Kaggle的[数据集](https://oreil.ly/hg5R3)，其中包含新闻标题。标题源自澳大利亚新闻源ABC，时间跨度为2003至2017年。CSV文件只包含时间戳和标题，没有标点符号，且全部小写。我们将CSV文件加载到Pandas的`DataFrame`中，并查看前几个文档：
- en: '[PRE30]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Out:`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '|   | publish_date | headline_text |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|   | 发布日期 | 新闻标题 |'
- en: '| --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 2003-02-19 | aba decides against community broadcasting lic... |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2003-02-19 | ABA决定不授予社区广播许可证... |'
- en: '| 1 | 2003-02-19 | act fire witnesses must be aware of defamation |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2003-02-19 | 澳大利亚ACT州的火灾目击者必须意识到诽谤问题 |'
- en: '| 2 | 2003-02-19 | a g calls for infrastructure protection summit |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2003-02-19 | A G呼吁举行基础设施保护峰会 |'
- en: '| 3 | 2003-02-19 | air nz staff in aust strike for pay rise |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2003-02-19 | 空中新西兰员工在澳大利亚罢工要求加薪 |'
- en: '| 4 | 2003-02-19 | air nz strike to affect australian travellers |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2003-02-19 | 空中新西兰罢工将影响澳大利亚旅客 |'
- en: There are 1,103,663  headlines in this dataset. Note that the headlines do not
    include punctuation and are all transformed to lowercase. Apart from the text,
    the dataset includes the publication date of each headline.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集中有1,103,663个标题。请注意，标题不包括标点符号，并且全部转换为小写。除了文本之外，数据集还包括每个标题的出版日期。
- en: As we saw earlier, the TF-IDF vectors can be calculated using the bag-of-words
    model (the *count vectors* in scikit-learn terminology). As it is so common to
    use TF-IDF document vectors, scikit-learn has created a “shortcut” to skip the
    count vectors and directly calculate the TF-IDF vectors. The corresponding class
    is called `TfidfVectorizer`, and we will use it next.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，可以使用词袋模型（在scikit-learn术语中的*计数向量*）计算TF-IDF向量。由于使用TF-IDF文档向量非常常见，因此scikit-learn创建了一个“快捷方式”来跳过计数向量，直接计算TF-IDF向量。相应的类称为`TfidfVectorizer`，我们将在下面使用它。
- en: 'In the following, we have also combined the calls to fit and to transform in
    `fit_transform`, which is convenient:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的内容中，我们还将`fit`和`transform`的调用组合成了`fit_transform`，这样做很方便：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This might take a while, as so many documents have to be analyzed and vectorized.
    Take a look at the dimensions of the document-term matrix:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一段时间，因为需要分析和向量化许多文档。查看文档-术语矩阵的维度：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Out:`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The number of rows was expected, but the number of columns (the vocabulary)
    is really large, with almost 100,000 words. Doing the math shows that a naive
    storage of data would have led to  1,103,663 * 95,878 elements with 8 bytes per
    float and have used roughly 788 GB RAM. This shows the incredible effectiveness
    of sparse matrices as the real memory used is “only” 56,010,856 bytes (roughly
    0.056 GB; found out via `dt.data.nbytes`). It’s still a lot, but it’s manageable.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 行数是预期的，但是列数（词汇表）非常大，几乎有100,000个单词。通过简单的计算可以得出，对数据进行天真的存储会导致1,103,663 * 95,878个元素，每个浮点数使用8字节，大约使用788GB的RAM。这显示了稀疏矩阵的令人难以置信的有效性，因为实际使用的内存只有“仅”56,010,856字节（大约0.056GB；通过`dt.data.nbytes`找到）。这仍然很多，但是可以管理。
- en: 'Calculating the similarity between two vectors is another story, though. Scikit-learn
    (and SciPy as a basis) is highly optimized for working with sparse vectors, but
    it still takes some time doing the sample calculation (similarities of the first
    10,000 documents):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，计算两个向量之间的相似性又是另一回事了。Scikit-learn（以及其基础SciPy）针对稀疏向量进行了高度优化，但是进行示例计算（前10,000个文档的相似性）仍然需要一些时间：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`Out:`'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: For machine learning in the next chapters, many of these linear algebra calculations
    are necessary and have to be repeated over and over. Often operations scale quadratically
    with the number of features (O(N²)). Optimizing the vectorization by removing
    unnecessary features is therefore not only helpful for calculating the similarities
    but also crucial for scalable machine learning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中进行机器学习时，许多这些线性代数计算是必要的，并且必须一遍又一遍地重复。通常操作随着特征数量呈二次方增长（O(N²)）。优化矢量化，通过移除不必要的特征，不仅有助于计算相似性，而且对于可扩展的机器学习至关重要。
- en: 'Blueprint: Reducing Feature Dimensions'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：降低特征维度
- en: We have now found features for our documents and used them to calculate document
    vectors. As we have seen in the example, the number of features can get quite
    large. Lots of machine learning algorithms are computationally intensive and scale
    with the number of features, often even polynomially. One part of feature engineering
    is therefore focused on reducing these features to the ones that are really necessary.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为我们的文档找到了特征，并用它们来计算文档向量。正如我们在示例中看到的，特征数量可能会非常大。许多机器学习算法需要大量计算，并且随着特征数量的增加而扩展，通常甚至是多项式的。因此，特征工程的一部分侧重于减少这些真正必要的特征。
- en: In this section, we show a blueprint for how this can be achieved and measure
    their impact on the number of features.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何实现这一点的蓝图，并衡量了它们对特征数量的影响。
- en: Removing stop words
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除停用词
- en: In the first place, we can think about removing the words that carry the least
    meaning. Although this is domain-dependent, there are lists of the most common
    English words that common sense tells us can normally be neglected. These words
    are called *stop words*. Common stop words are determiners, auxiliary verbs, and
    pronouns. For a more detailed discussion, see [Chapter 4](ch04.xhtml#ch-preparation).
    Be careful when removing stop words as they can contain certain words that might
    carry a domain-specific meaning in special texts!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以考虑删除具有最少含义的词语。尽管这取决于领域，但通常有一些最常见的英语单词列表，常识告诉我们通常可以忽略它们。这些词被称为*停用词*。常见的停用词包括冠词、助动词和代词。有关更详细的讨论，请参阅[第4章](ch04.xhtml#ch-preparation)。在删除停用词时要小心，因为它们可能包含在特殊文本中具有特定领域含义的某些词语！
- en: This does not reduce the number of dimensions tremendously as there are only
    a few hundred common stop words in almost any language. However, it should drastically
    decrease the number of stored elements as stop words are so common. This leads
    to less memory consumption and faster calculations, as fewer numbers need to be
    multiplied.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于几乎任何语言中都有几百个常见的停用词，因此这并没有极大地减少维度。然而，由于停用词非常常见，这应该显著减少存储元素的数量。这导致内存消耗更少，并且计算速度更快，因为需要相乘的数字更少。
- en: 'Let’s use the standard spaCy stop words and check the effects on the document-term
    matrix. Note that we pass stop words as a named parameter to the `TfidfVectorizer`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用标准的 spaCy 停用词，并检查对文档-术语矩阵的影响。请注意，我们将停用词作为命名参数传递给 `TfidfVectorizer`：
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`Out:`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: With only 305 stop words, we managed to reduce the number of stored elements
    by 20%. The dimensions of the matrix are almost the same, with fewer columns due
    to the 95,878 – 95,600 = 278 stop words that actually appeared in the headlines.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用 305 个停用词，我们成功将存储的元素数量减少了 20%。矩阵的维数几乎相同，但由于确实出现在标题中的 95,878 - 95,600 = 278
    个停用词较少，列数更少。
- en: Minimum frequency
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小频率
- en: Taking a look at the definition of the cosine similarity, we can easily see
    that components can contribute only if both vectors have a nonzero value at the
    corresponding index. This means that we can neglect all words occurring less than
    twice! `TfidfVectorizer` (and `CountVectorizer`) have a parameter for that called
    `min_df`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 查看余弦相似度的定义，我们可以很容易地看到，只有当两个向量在相应索引处具有非零值时，它们的分量才会有贡献。这意味着我们可以忽略所有出现少于两次的词！`TfidfVectorizer`（以及
    `CountVectorizer`）有一个称为 `min_df` 的参数。
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`Out:`'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Obviously, there are a lot of words appearing just once (95,600 – 58,527 =
    37,073). Those words should also be stored only once; checking with the number
    of stored elements, we should get the same result: 5,644,186 – 5,607,113 = 37,073\.
    Performing this kind of transformation, it is always useful to integrate such
    plausibility checks.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，有很多单词仅出现一次（95,600 - 58,527 = 37,073）。这些单词也应该只存储一次；通过存储元素数量的检查，我们应该得到相同的结果：5,644,186
    - 5,607,113 = 37,073。在执行此类转换时，集成这些合理性检查总是很有用的。
- en: Losing Information
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 丢失信息
- en: 'Be careful: by using `min_df=2`, we have not lost any information in vectorizing
    the headlines of this document corpus. If we plan to vectorize more documents
    later with the same vocabulary, we might lose information, as words appearing
    again in the new documents that were only present once in the original documents
    will not be found in the vocabulary.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通过使用 `min_df=2`，我们在向量化此文档语料库的标题时没有丢失任何信息。如果我们计划以后用相同的词汇量向量化更多文档，我们可能会丢失信息，因为在原始文档中仅出现一次的单词，在新文档中再次出现时，将无法在词汇表中找到。
- en: '`min_df` can also take float values. This means that a word has to occur in
    a minimum fraction of documents. Normally, this reduces the vocabulary drastically
    even for low numbers of `min_df`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_df` 也可以采用浮点值。这意味着一个词必须在至少一部分文档中出现。通常情况下，即使对于较低的 `min_df` 数量，这也会显著减少词汇量：'
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Out:`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This transformation is probably too strict and reduces the vocabulary too far.
    Depending on the number of documents, you should set `min_df` to a low integer
    and check the effects on the vocabulary.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换可能过于严格，导致词汇量过低。根据文档的数量，您应将 `min_df` 设置为一个较低的整数，并检查对词汇表的影响。
- en: Maximum frequency
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大频率
- en: 'Sometimes a text corpus might have a special vocabulary with lots of repeating
    terms that are too specific to be contained in stop word lists. For this use case,
    scikit-learn offers the `max_df` parameter, which eliminates terms occurring too
    often in the corpus. Let’s check how the dimensions are reduced when we eliminate
    all the words that appear in at least 10% of the headlines:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 有时文本语料库可能有一个特殊的词汇表，其中有很多重复出现的术语，这些术语太特定，不能包含在停用词列表中。对于这种情况，scikit-learn 提供了`max_df`参数，可以消除语料库中过于频繁出现的术语。让我们看看当我们消除所有至少在
    10% 的标题中出现的词时，维度是如何减少的：
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`Out:`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Setting `max_df` to a low value of 10% does not eliminate a single word!^([6](ch05.xhtml#idm45634197156184))
    Our news headlines are very diverse. Depending on the type of corpus you have,
    experimenting with `max_df` can be quite useful. In any case, you should always
    check how the dimensions change.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 将`max_df`设置为低至 10% 的值并没有消除任何一个词！^([6](ch05.xhtml#idm45634197156184))我们的新闻标题非常多样化。根据您拥有的语料库类型，尝试使用`max_df`可能非常有用。无论如何，您都应该始终检查维度如何变化。
- en: 'Blueprint: Improving Features by Making Them More Specific'
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：通过使特征更具体来改进特征
- en: So far, we have only used the original words of the headlines and reduced the
    number of dimensions by stop words and counting frequencies. We have not yet changed
    the features themselves. Using linguistic analysis, there are more possibilities.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了标题的原始词，并通过停用词和频率计数减少了维度。我们还没有改变特征本身。通过语言分析，有更多的可能性。
- en: Performing linguistic analysis
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进行语言分析
- en: 'Using spaCy, we can lemmatize all headlines and just keep the lemmas. This
    takes some time, but we expect to find a smaller vocabulary. First, we have to
    perform a linguistic analysis, which might take some time to finish (see [Chapter 4](ch04.xhtml#ch-preparation) for
    more details):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy，我们可以对所有标题进行词形还原，并只保留词形还原形式。这需要一些时间，但我们预计会找到一个更小的词汇表。首先，我们必须进行语言分析，这可能需要一些时间才能完成（参见[第
    4 章](ch04.xhtml#ch-preparation)了解更多细节）：
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Blueprint: Using Lemmas Instead of Words for Vectorizing Documents'
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用词形还原代替单词进行文档向量化
- en: 'Now, we can vectorize the data using the lemmas and see how the vocabulary
    decreased:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用词形还原对数据进行向量化，并查看词汇表的减少情况：
- en: '[PRE46]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '`Out:`'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE47]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Saving almost 25,000 dimensions is a lot. In news headlines, lemmatizing the
    data probably does not lose any information. In other use cases like those in
    [Chapter 11](ch11.xhtml#ch-sentiment), it’s a completely different story.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 节省近 25,000 个维度是很多的。在新闻标题中，对数据进行词形还原可能不会丢失任何信息。在其他用例中，比如[第 11 章](ch11.xhtml#ch-sentiment)，情况完全不同。
- en: 'Blueprint: Limit Word Types'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：限制词类
- en: 'Using the data generated earlier, we can restrict ourselves to considering
    just nouns, adjectives, and verbs for the vectorization, as prepositions, conjugations,
    and so on are supposed to carry little meaning. This again reduces the vocabulary:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前生成的数据，我们可以限制自己只考虑名词、形容词和动词进行向量化，因为介词、连词等被认为带有很少的意义。这会再次减少词汇量：
- en: '[PRE48]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`Out:`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE49]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: There’s not much to win here, which is probably due to the headlines mainly
    containing nouns, adjectives, and verbs. But this might look totally different
    in your own projects. Depending on the type of texts you are analyzing, restricting
    word types will not only reduce the size of the vocabulary but will also lead
    to much lower noise. It’s a good idea to try this with a small part of the corpus
    first to avoid long waiting times due to the expensive linguistic analysis.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里几乎没有什么可以获得的，这可能是因为标题主要包含名词、形容词和动词。但是在您自己的项目中，情况可能完全不同。根据您分析的文本类型，限制词类不仅会减少词汇量，还会减少噪音。建议先尝试对语料库的一小部分进行操作，以避免由于昂贵的语言分析而导致长时间等待。
- en: 'Blueprint: Remove Most Common Words'
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：移除最常见的单词
- en: As we learned, removing frequent words can lead to document-term matrices with
    far fewer entries. This is especially helpful when you perform unsupervised learning,
    as you will normally not be interested in common words that are common anyway.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的学习，去除频繁出现的词可以导致文档-词矩阵的条目大大减少。在进行无监督学习时尤其有用，因为通常不会对常见的、无足轻重的常用词感兴趣。
- en: 'To remove even more noise, we will now try to eliminate the most common English
    words. Be careful, as there will normally also be words involved that might carry
    important meaning. There are various lists with those words; they can easily be
    found on the internet. The [list from Google](https://oreil.ly/bOho1) is rather
    popular and directly available on GitHub. Pandas can directly read the list if
    we tell it to be a CSV file without column headers. We will then instruct the
    `TfidfVectorizer` to use that list as stop words:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步减少噪音，我们现在尝试消除最常见的英文单词。请注意，通常还会涉及可能具有重要含义的单词。有各种各样的单词列表；它们可以很容易地在互联网上找到。[来自Google的列表](https://oreil.ly/bOho1)非常流行，并直接可在GitHub上获取。Pandas可以直接读取该列表，只需告诉它是一个没有列标题的CSV文件。然后，我们将指示`TfidfVectorizer`使用该列表作为停用词：
- en: '[PRE50]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`Out:`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE51]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As you can see, the matrix now has 3.5 million fewer stored elements. The vocabulary
    shrunk by 68,426 – 61,630 = 6,796 words, so more than 3,000 of the most frequent
    English words were not even used in the ABC headlines.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，矩阵现在减少了350万个存储的元素。词汇量减少了68,426 - 61,630 = 6,796个词，因此ABC标题中甚至有超过3,000个最常见的英文单词没有被使用。
- en: Removing frequent words is an excellent method to remove noise from the dataset
    and concentrate on the uncommon words. However, you should be careful using this
    from the beginning as even frequent words do have a meaning, and they might also
    have a special meaning in your document corpus. We recommend performing such analyses
    additionally but not exclusively.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 删除频繁单词是从数据集中去除噪音并集中于不常见单词的优秀方法。但是，刚开始使用时应该小心，因为即使频繁单词也有意义，并且它们在您的文档语料库中可能也具有特殊含义。我们建议额外执行这样的分析，但不应仅限于此。
- en: 'Blueprint: Adding Context via N-Grams'
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：通过N-Grams添加上下文
- en: So far, we have used only single words as features (dimensions of our document
    vectors) as the basis for our vectorization. With this strategy, we have lost
    a lot of context information. Using single words as features does not respect
    the context in which the words appear. [In later chapters](ch10.xhtml#ch-embeddings)
    we will learn how to overcome that limitation with sophisticated models like word
    embeddings. In our current example, we will use a simpler method and take advantage
    of word combinations, so called *n-grams*. Two-word combinations are called *bigrams*;
    for three words, they are called *trigrams*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅使用单词作为特征（我们文档向量的维度），作为我们向量化的基础。使用这种策略，我们失去了大量的上下文信息。使用单词作为特征不尊重单词出现上下文。[在后续章节](ch10.xhtml#ch-embeddings)中，我们将学习如何通过像词嵌入这样的复杂模型克服这种限制。在我们当前的示例中，我们将使用一种更简单的方法，并利用单词组合，即所谓的*n-grams*。两个词的组合称为*bigrams*；三个词的组合称为*trigrams*。
- en: 'Fortunately, `CountVectorizer` and `TfidfVectorizer` have the corresponding
    options. Contrary to the last few sections where we tried to reduce the vocabulary,
    we now enhance the vocabulary with word combinations. There are many such combinations;
    their number (and vocabulary size) is growing almost exponentially with *n*.^([7](ch05.xhtml#idm45634196754328))
    We will therefore be careful and start with bigrams:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`CountVectorizer`和`TfidfVectorizer`具有相应的选项。与前几节试图减少词汇量的做法相反，我们现在通过词组增强词汇量。有许多这样的组合；它们的数量（以及词汇量）几乎与*n*的指数级增长。^([7](ch05.xhtml#idm45634196754328))
    因此，我们要小心，并从bigrams开始：
- en: '[PRE52]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`Out:`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE53]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Increasing the feature dimensions from 95,600 to 2,335,132 or even 5,339,558
    is quite painful even though the RAM size has not grown too much. For some tasks
    that need context-specific information (like sentiment analysis), n-grams are
    extremely useful. It is always useful to keep an eye on the dimensions, though.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RAM大小并没有增加太多，但将特征维度从95,600增加到2,335,132甚至5,339,558是相当痛苦的。对于某些需要特定上下文信息的任务（如情感分析），n-grams非常有用。但是，始终注意维度是非常有用的。
- en: 'Combining n-grams with linguistic features and common words is also possible
    and reduces the vocabulary size considerably:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以将n-grams与语言特征和常见单词结合起来，大大减少词汇量：
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`Out:`'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE55]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Scikit-learn offers many different vectorizers. Normally, starting with `TfidfVectorizer`
    is a good idea, as it is one of the most versatile.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了许多不同的向量化器。通常，从`TfidfVectorizer`开始是个不错的主意，因为它是最多才多艺的之一。
- en: Options of TfidfVectorizer
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TfidfVectorizer的选项
- en: TF-IDF can even be switched off so there is a seamless fallback to `CountVectorizer`.
    Because of the many parameters, it can take some time to find the perfect set
    of options.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF甚至可以关闭，因此可以无缝切换到`CountVectorizer`。由于参数众多，找到完美的选项可能需要一些时间。
- en: Finding the correct set of features is often tedious and requires experimentation
    with the (many) parameters of `TfidfVectorizer`, like `min_df`, `max_df`, or simplified
    text via NLP. In our work, we have had good experiences with setting `min_df`
    to `5`, for example, and `max_df` to `0.7`. In the end, this time is excellently
    invested as the results will depend heavily on correct vectorization. There is
    no golden bullet, though, and this *feature engineering* depends heavily on the
    use case and the planned use of the vectors.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 找到正确的特征集通常是乏味的，并需要通过`TfidfVectorizer`的（许多）参数进行实验，如`min_df`、`max_df`或通过NLP简化文本。在我们的工作中，我们已经通过将`min_df`设置为`5`和`max_df`设置为`0.7`获得了良好的经验。最终，这些时间的投资是非常值得的，因为结果将严重依赖于正确的向量化。然而，并没有金弹，这种*特征工程*严重依赖于使用情况和向量计划使用。
- en: The TF-IDF method itself can be improved by using a subnormal term frequency
    or normalizing the resulting vectors. The latter is useful for quickly calculating
    similarities, and we demonstrate its use later in the chapter. The former is mainly
    interesting for long documents to avoid repeating words getting a too high-weight.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF方法本身可以通过使用次正常术语频率或归一化所得到的向量来改进。后者对于快速计算相似性非常有用，我们将在本章后面演示其使用。前者主要适用于长文档，以避免重复单词获得过高的权重。
- en: Think very carefully about feature dimensions
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非常仔细地考虑特征维度
- en: In our previous examples, we have used single words and bigrams as features.
    Depending on the use case, this might already be enough. This works well for texts
    with common vocabulary, like news. But often you will encounter special vocabularies
    (for example, scientific publications or letters to an insurance company), which
    will require more sophisticated feature engineering.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们以前的例子中，我们使用了单词和二元组作为特征。根据使用情况，这可能已经足够了。这对于像新闻这样有常见词汇的文本效果很好。但是，您经常会遇到特殊词汇的情况（例如科学出版物或写给保险公司的信函），这将需要更复杂的特征工程。
- en: Keep number of dimensions in mind
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 要牢记维度的数量。
- en: As we have seen, using parameters like `ngram_range` can lead to large feature
    spaces. Apart from the RAM usage, this will also be a problem for many machine
    learning algorithms due to overfitting. Therefore, it’s a good idea to always
    consider the (increase of) feature dimensions when changing parameters or vectorization
    methods.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，使用诸如`ngram_range`之类的参数可能会导致大的特征空间。除了RAM使用情况外，这也将成为许多机器学习算法的问题，因为会导致过拟合。因此，当更改参数或向量化方法时，始终考虑（增加）特征维度是一个好主意。
- en: Syntactic Similarity in the ABC Dataset
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ABC数据集中的语法相似性
- en: Similarity is one of the most basic concepts in machine learning and text analytics.
    In this section, we take a look at some challenging problems finding similar documents
    in the ABC dataset.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性是机器学习和文本分析中最基本的概念之一。在这一部分中，我们将看一些在ABC数据集中找到相似文档的具有挑战性的问题。
- en: After taking a look at possible vectorizations in the previous section, we will
    now use one of them to calculate the similarities. We will present a blueprint
    to show how you can perform these calculations efficiently from both a CPU and
    a RAM perspective. As we are handling large amounts of data here, we have to make
    extensive use of the [NumPy library](https://numpy.org).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中查看可能的向量化之后，我们现在将使用其中的一种方法来计算相似性。我们将提供一个蓝图，展示如何从CPU和RAM的角度高效执行这些计算。由于我们处理大量数据，因此必须广泛使用[NumPy库](https://numpy.org)。
- en: 'In the first step, we vectorize the data using stop words and bigrams:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们使用停用词和二元组对数据进行向量化：
- en: '[PRE56]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We are now ready to use these vectors for our blueprints.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这些向量用于我们的蓝图。
- en: 'Blueprint: Finding Most Similar Headlines to a Made-up Headline'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：查找最接近虚构标题的标题
- en: 'Let’s say we want to find a headline in our data that most closely matches
    a headline that we remember, but only roughly. This is quite easy to solve, as
    we just have to vectorize our new document:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要在我们的数据中找到一个与我们记得的标题最接近的标题，但只是粗略地。这很容易解决，因为我们只需对我们的新文档进行向量化：
- en: '[PRE57]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now we have to calculate the cosine similarity to each headline in the corpus.
    We could implement this in a loop, but it’s easier with the `cosine_similarity`
    function of scikit-learn:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须计算与语料库中每个标题的余弦相似度。我们可以通过循环来实现这一点，但使用scikit-learn的`cosine_similarity`函数会更容易：
- en: '[PRE58]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The result is a “number of headlines in the corpus” × 1 matrix, where each
    number represents the similarity to a document in the corpus. Using `np.argmax`
    gives us the index of the most similar document:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个“语料库中的头条数量” × 1 矩阵，其中每个数字表示与语料库中文档的相似性。使用`np.argmax`给出最相似文档的索引：
- en: '[PRE59]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`Out:`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE60]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: No *sizes* of apples and no *Australia* are present in the most similar headline,
    but it definitely bears some similarity with our invented headline.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最相似的头条中没有*苹果的大小*和*澳大利亚*，但它确实与我们虚构的头条有些相似。
- en: 'Blueprint: Finding the Two Most Similar Documents in a Large Corpus (Much More
    Difficult)'
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：在大型语料库中找到两个最相似的文档（更加困难）
- en: When handling a corpus of many documents, you might often be asked questions
    such as “Are there duplicates?” or “Has this been mentioned before?” They all
    boil down to finding the most similar (maybe even identical) documents in the
    corpus. We will explain how to accomplish this and again use the ABC dataset as
    our example. The number of headlines will turn out to be a challenge.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理多个文档的语料库时，您可能经常会被问到“是否有重复？”或“这之前提到过吗？” 这些问题都归结为在语料库中查找最相似（甚至可能是完全相同）的文档。我们将解释如何实现这一点，并再次以我们的示例数据集ABC来说明。头条的数量将被证明是一个挑战。
- en: You might think that finding the most similar documents in the corpus is as
    easy as calculating the `cosine_similarity` between all documents. However, this
    is not possible as 1,103,663 × 1,103,663 = 1,218,072,017,569\. More than one trillion
    elements do not fit in the RAM of even the most advanced computers. It is perfectly
    possible to perform the necessary matrix multiplications without having to wait
    for ages.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会认为在语料库中找到最相似的文档就像计算所有文档之间的`cosine_similarity`一样简单。但是，这是不可能的，因为 1,103,663
    × 1,103,663 = 1,218,072,017,569。即使是最先进的计算机的 RAM 也无法容纳超过一万亿个元素。完全可以执行所需的矩阵乘法，而无需等待太长时间。
- en: Clearly, this problem needs optimization. As text analytics often has to cope
    with many documents, this is a very typical challenge. Often, the first optimization
    is to take an intensive look at all the needed numbers. We can easily observe
    that the document similarity relation is symmetrical and normalized.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这个问题需要优化。由于文本分析经常需要处理许多文档，这是一个非常典型的挑战。通常，第一个优化步骤是深入研究所有需要的数字。我们可以轻松地观察到文档相似性关系是对称和标准化的。
- en: In other words, we just need to calculate the subdiagonal elements of the similarity
    matrix ([Figure 5-1](#btap_0501))
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们只需要计算相似矩阵的次对角线元素（[图5-1](#btap_0501)）
- en: '![](Images/btap_0501.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0501.jpg)'
- en: Figure 5-1\. Elements that need to be calculated in the similarity matrix. Only
    the elements below the diagonal need to be calculated, as their numbers are identical
    to the ones mirrored on the diagonal. The diagonal elements are all 1.
  id: totrans-282
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 需要在相似矩阵中计算的元素。只有对角线以下的元素需要计算，因为它们的数量与对角线上的元素镜像相同。对角线上的元素都是 1。
- en: This reduces the number of elements to 1,103,663 × 1,103,662 / 2 = 609,035,456,953,
    which could be calculated in loop iterations, keeping only the most similar documents.
    However, calculating all these elements separately is not a good option, as the
    necessary Python loops (where each iteration calculates just a single matrix element)
    will eat up a lot of CPU performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把元素数量减少到 1,103,663 × 1,103,662 / 2 = 609,035,456,953，可以在循环迭代中计算，并保留只有最相似的文档。然而，单独计算所有这些元素并不是一个好选择，因为必要的
    Python 循环（每次迭代仅计算一个矩阵元素）会消耗大量 CPU 性能。
- en: Instead of calculating individual elements of the similarity matrix, we divide
    the problem into different blocks and calculate 10,000 × 10,000 similarity submatrices^([8](ch05.xhtml#idm45634196311384)) 
    at once by taking blocks of 10,000 TF-IDF vectors from the document matrix. Each
    of these matrices contains 100,000,000 similarities, which will still fit in RAM.
    Of course, this leads to calculating too many elements, and we have to perform
    this for 111 × 110 / 2 = 6,105 submatrices (see [Figure 5-2](#btap_0502)).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 不是计算相似矩阵的各个元素，我们将问题分成不同的块，并一次计算 10,000 × 10,000 个 TF-IDF 向量的相似子矩阵^([8](ch05.xhtml#idm45634196311384))。每个这样的矩阵包含
    100,000,000 个相似度，仍然适合在 RAM 中。当然，这会导致计算太多元素，我们必须对 111 × 110 / 2 = 6,105 个子矩阵执行此操作（参见[图5-2](#btap_0502)）。
- en: From the previous section, we know that iteration takes roughly 500 ms to calculate.
    Another advantage of this approach is that leveraging data locality gives us a
    bigger chance of having the necessary matrix elements already in the CPU cache.
    We estimate that everything should run in about 3,000 seconds, which is roughly
    one hour.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的部分，我们知道迭代大约需要 500 毫秒来计算。这种方法的另一个优点是利用数据局部性，使我们更有可能在 CPU 缓存中拥有必要的矩阵元素。我们估计一切应该在大约
    3,000 秒内完成，大约相当于一小时。
- en: '![](Images/btap_0502.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0502.jpg)'
- en: Figure 5-2\. Dividing the matrix into submatrices, which we can calculate more
    easily; the problem is divided into blocks (here, 4 × 4), and the white and diagonal
    elements within the blocks are redundantly calculated.
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 将矩阵分成子矩阵，我们可以更轻松地计算；问题被分成块（这里是 4 × 4），块内的白色和对角线元素在计算时是冗余的。
- en: Can we improve this? Yes, indeed another speedup of a factor of 10 is actually
    possible. This works by normalizing the TF-IDF vectors via the corresponding option
    of `TfidfVectorizer`. Afterward, the similarity can be calculated with `np.dot`:^([9](ch05.xhtml#idm45634196279640))
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否改进这一点？是的，事实上，可以实现另一个 10 倍的加速。这通过使用 `TfidfVectorizer` 的相应选项来对 TF-IDF 向量进行归一化来实现。之后，可以使用
    `np.dot` 计算相似性：^([9](ch05.xhtml#idm45634196279640))
- en: '[PRE61]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`Out:`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In each iteration we save the most similar documents and their similarity and
    adjust them during the iterations. To skip identical documents (or more precisely,
    documents with identical document vectors), we only consider similarities < 0.9999\.
    As it turns out, using `<` relations with a sparse matrix is extremely inefficient,
    as all non-existent elements are supposed to be 0\. Therefore, we must be creative
    and find another way:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中，我们保存最相似的文档及其相似性，并在迭代过程中进行调整。为了跳过相同的文档（或更精确地说，具有相同文档向量的文档），我们只考虑相似性 `<
    0.9999`。事实证明，在稀疏矩阵中使用 `<` 关系是极其低效的，因为所有不存在的元素都被假定为 0。因此，我们必须富有创造性地寻找另一种方法：
- en: '[PRE63]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`Out:`'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE64]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'That did not take too long, fortunately! `max_a` and `max_b` contain the indices
    of the headlines with maximum similarity (avoiding identical headlines). Let’s
    take a look at the results:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这没有花太多时间！`max_a` 和 `max_b` 包含具有最大相似性的标题的索引（避免相同的标题）。让我们来看一下结果：
- en: '[PRE65]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`Out:`'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE66]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Using the block calculation approach, we have calculated almost a trillion similarities
    in just a few minutes. The results are interpretable as we have found similar,
    but not identical, documents. The different date shows that these are definitely
    also separate headlines.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用块计算方法，我们在几分钟内计算了近万亿的相似性。由于我们找到了相似但不相同的文档，因此结果是可以解释的。不同的日期表明这些绝对也是不同的标题。
- en: 'Blueprint: Finding Related Words'
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：查找相关词汇
- en: Until now, we have analyzed documents with respect to their similarity. But
    the corpus implicitly has much more information, specifically information about
    related words. In our sense, words are related if they are used in the same documents.
    Words should be “more” related if they frequently appear together in the documents.
    As an example, consider the word *zealand*, which almost always occurs together
    with *new*; therefore, these two words are *related.*
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经分析了文档的相似性。但语料库在隐含中具有更多信息，具体而言是有关相关词汇的信息。在我们的意义上，如果单词在相同的文档中使用，则它们是相关的。如果这些单词经常一起出现在文档中，那么它们应该“更”相关。举个例子，考虑单词
    *zealand*，它几乎总是与 *new* 一起出现；因此，这两个单词是*相关的*。
- en: 'Instead of working with a document-term matrix, we would like to work with
    a term-document matrix, which is just its transposed form. Instead of taking row
    vectors, we now take column vectors. However, we need to re-vectorize the data.
    Assume two words are infrequently used and that both happen to be present only
    once in the same headline. Their vectors would then be identical, but this is
    not what we are looking for. As an example, let’s think of a person named *Zaphod
    Beeblebrox*, who is mentioned in two articles. Our algorithm would assign a 100%
    related score to these words. Although this is correct, it is not very significant. We
    therefore only consider words that appear at least 1,000 times to get a decent
    statistical significance:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望与文档-术语矩阵而非文档-术语矩阵一起工作，这只是其转置形式。我们不再取行向量，而是取列向量。但是，我们需要重新向量化数据。假设两个词很少使用，并且它们恰好同时出现在同一标题中。它们的向量将是相同的，但这不是我们要寻找的情况。例如，让我们考虑一个名为
    *扎福德·毕布罗克斯* 的人，在两篇文章中提到了他。我们的算法将为这些词分配100%的相关分数。尽管这是正确的，但不是非常显著。因此，我们只考虑出现至少1000次的单词，以获得良好的统计显著性：
- en: '[PRE67]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The vocabulary is quite small, and we can directly calculate the cosine similarity.
    Changing row for column vectors, we just transpose the matrix, using the convenient
    `.T` method of NumPy:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇量非常小，我们可以直接计算余弦相似性。将行向量变为列向量，我们只需转置矩阵，使用NumPy的方便 `.T` 方法：
- en: '[PRE68]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finding the largest entries is easiest if we convert it to a one-dimensional
    array, get the index of the sorted elements via `np.argsort`, and restore the
    original indices for the vocabulary lookup:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要找到最大条目，最简单的方法是将其转换为一维数组，通过 `np.argsort` 获取排序元素的索引，并恢复用于词汇查找的原始索引：
- en: '[PRE69]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '`Out:`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE70]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: It’s quite easy to interpret these results. For some word combinations like
    *climate change*, we have restored frequent bigrams. On the other hand, we can
    also see related words that don’t appear next to each other in the headlines,
    such as *drink* and *driving*. By using the transposed document-term matrix, we
    have performed a kind of *co-occurrence analysis*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易解释这些结果。对于一些词组合（如 *气候变化* ），我们已经恢复了频繁的二元组。另一方面，我们还可以看到在标题中并未相邻的相关词汇，如 *饮酒*
    和 *驾驶* 。通过使用转置的文档-术语矩阵，我们进行了一种 *共现分析* 。
- en: Tips for Long-Running Programs like Syntactic Similarity
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长时间运行程序的技巧，如句法相似性
- en: 'The following are some efficiency tips for long-running programs:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是长时间运行程序的一些效率提示：
- en: Perform benchmarking before waiting too long
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在等待过长之前进行基准测试
- en: Before performing calculations on the whole dataset, it is often useful to run
    a single calculation and extrapolate how long the whole algorithm will run and
    how much memory it will need. You should definitely try to understand how runtime
    and memory grow with increased complexity (linear, polynomial, exponential). Otherwise,
    you might have to wait for a long time and find out that after a few hours (or
    even days) only 10% progress memory is exhausted.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在对整个数据集进行计算之前，通常先运行单个计算并推断整个算法将运行多长时间以及需要多少内存是非常有用的。您应该努力理解随着复杂性增加运行时间和内存消耗的增长方式（线性、多项式、指数）。否则，您可能不得不等待很长时间，然后发现几个小时（甚至几天）后仅完成了10%的进度而内存已经耗尽。
- en: Try to divide your problem into smaller parts
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将问题分解为较小的部分
- en: Dividing a problem into smaller blocks can help here tremendously. As we have
    seen in the most similar document of the news corpus, this took only 20 minutes
    or so to run and used no significant memory. Compared to a naive approach, we
    would have found out after considerable runtime that the RAM would not have been
    enough. Furthermore, by dividing the problem into parts, you can make use of multicore
    architectures or even distribute the problem on many computers.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 将问题分解为较小的块可以极大地帮助。正如我们在新闻语料库中最相似文档中所见，这样的运行只需大约20分钟，并且没有使用大量内存。与朴素方法相比，我们将在长时间运行后发现内存不足。此外，通过将问题分解为部分，您可以利用多核架构甚至将问题分布在多台计算机上。
- en: Summary and Conclusion
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与结论
- en: In this section, we have prepared blueprints for vectorization and syntactic
    similarity. Almost all machine learning projects with text (such as classification,
    topic modeling, and sentiment detection) need document vectors at their very base.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已准备好向量化和句法相似性的蓝图。几乎所有文本相关的机器学习项目（如分类、主题建模和情感检测）都需要文档向量作为其基础。
- en: 'It turns out that feature engineering is one of the most powerful levers for
    achieving outstanding performance with these sophisticated machine learning algorithms.
    Therefore, it’s an excellent idea to try different vectorizers, play with their
    parameters, and watch the resulting feature space. There are really many possibilities,
    and for good reason: although optimizing this takes some time, it is usually well-invested
    as the results of the subsequent steps in the analysis pipeline will benefit tremendously.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，特征工程是实现这些复杂机器学习算法出色性能的最强大杠杆之一。因此，尝试不同的向量化器，调整它们的参数，并观察生成的特征空间是一个绝佳的主意。确实有很多可能性，而且有充分的理由：尽管优化这一步骤需要一些时间，但通常是非常值得的，因为分析管道后续步骤的结果将极大受益于此。
- en: The similarity measure used in this chapter is just an example for document
    similarities. For more complicated requirements, there are more sophisticated
    similarity algorithms that you will learn about in the following chapters.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的相似性度量仅作为文档相似性的示例。对于更复杂的需求，还有更复杂的相似性算法，你将在后续章节中了解到。
- en: Finding similar documents is a well-known problem in information retrieval.
    There are more sophisticated scores, such as [BM25](https://oreil.ly/s47TC). If
    you want a scalable solution, the very popular [Apache Lucene](http://lucene.apache.org)
    library (which is the basis of search engines like [Apache Solr](https://oreil.ly/R5y0E)
    and [Elasticsearch](https://oreil.ly/2qfAL)) makes use of this and is used for
    really big document collections in production scenarios.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息检索中，寻找相似文档是一个众所周知的问题。还有更复杂的评分方法，如[BM25](https://oreil.ly/s47TC)。如果你需要一个可扩展的解决方案，非常流行的[Apache
    Lucene](http://lucene.apache.org)库（它是像[Apache Solr](https://oreil.ly/R5y0E)和[Elasticsearch](https://oreil.ly/2qfAL)这样的搜索引擎的基础）利用这一点，在生产场景中用于非常大的文档集合。
- en: In the following chapters, we will revisit similarity quite often. We will take
    a look at integrating word semantics and document semantics, and we will use transfer
    learning to leverage predefined language models that have been trained with extremely
    large documents corpora to achieve state-of-the-art performance.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将经常重新讨论相似性。我们将探讨如何整合单词语义和文档语义，并利用预先训练过的大型文档语料库来实现最先进的性能，使用迁移学习。
- en: ^([1](ch05.xhtml#idm45634198925240-marker)) In later chapters, we will take
    a look at other possibilities of vectorizing words ([Chapter 10](ch10.xhtml#ch-embeddings))
    and documents ([Chapter 11](ch11.xhtml#ch-sentiment)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm45634198925240-marker)) 在后面的章节中，我们将探讨向量化单词（[第10章](ch10.xhtml#ch-embeddings)）和文档（[第11章](ch11.xhtml#ch-sentiment)）的其他可能性。
- en: ^([2](ch05.xhtml#idm45634198893064-marker)) There are much more sophisticated
    algorithms for determining the vocabulary, like [SentencePiece](https://oreil.ly/A6TEl)
    and [BPE](https://oreil.ly/tVDgu), which are worth taking a look at if you want
    to reduce the number of features.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm45634198893064-marker)) 有更复杂的算法来确定词汇，比如[SentencePiece](https://oreil.ly/A6TEl)和[BPE](https://oreil.ly/tVDgu)，如果你想减少特征数，这些都值得一看。
- en: ^([3](ch05.xhtml#idm45634198361912-marker)) Confusingly, `numpy.dot` is used
    both for the dot product (inner product) and for matrix multiplication. If Numpy
    detects two row or column vectors (i.e., one-dimensional arrays) with the same
    dimension, it calculates the dot product and yields a scalar. If not and the passed
    two-dimensional arrays are suitable for matrix multiplication, it performs this
    operation and yields a matrix. All other cases produce errors. That’s convenient,
    but it’s a lot of heuristics.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#idm45634198361912-marker)) 令人困惑的是，`numpy.dot`既用于点积（内积），也用于矩阵乘法。如果Numpy检测到两个行向量或列向量（即一维数组）具有相同的维度，它计算点积并生成一个标量。如果不是，而且传递的二维数组适合矩阵乘法，它执行这个操作并生成一个矩阵。所有其他情况都会产生错误。这很方便，但涉及到很多启发式方法。
- en: ^([4](ch05.xhtml#idm45634198247384-marker)) See [Chapter 8](ch08.xhtml#ch-topicmodels)
    for more on LDA.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.xhtml#idm45634198247384-marker)) 更多关于LDA的内容请参见[第8章](ch08.xhtml#ch-topicmodels)。
- en: ^([5](ch05.xhtml#idm45634197672360-marker)) See, for example, the [definition
    of entropy](https://oreil.ly/3qTpX) as a measure of uncertainty and information.
    Basically, this says that a low-probability value carries more information than
    a more likely value.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.xhtml#idm45634197672360-marker)) 例如，参见[熵的定义](https://oreil.ly/3qTpX)，作为不确定性和信息的度量。基本上，这表明低概率值比更可能的值包含更多信息。
- en: ^([6](ch05.xhtml#idm45634197156184-marker)) This is, of course, related to the
    stop word list that has already been used. In news articles, the most common words
    are stop words. In domain-specific texts, it might be completely different. Using
    stop words is often the safer choice, as these lists have been curated.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.xhtml#idm45634196754328-marker)) It would be growing exponentially
    if all word combinations were possible and would be used. As this is unlikely,
    the dimensions are growing subexponentially.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.xhtml#idm45634196311384-marker)) We have chosen 10,000 dimensions,
    as the resulting matrix can be kept in RAM (using roughly 1 GB should be possible
    even on moderate hardware).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.xhtml#idm45634196279640-marker)) All calculations can be sped up
    considerably by using processor-specific libraries, e.g., by subscribing to the
    Intel channel in Anaconda. This will use AVX2, AVX-512, and similar instructions
    and use parallelization. [MKL](https://oreil.ly/pa1zj) and [OpenBlas](https://oreil.ly/jZYSG)
    are good candidates for linear algebra libraries.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
