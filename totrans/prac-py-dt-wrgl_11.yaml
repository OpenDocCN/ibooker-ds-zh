- en: Chapter 11\. Beyond Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is an exceptionally powerful and versatile tool for working with data,
    and if you’ve followed along with the exercises in this book, you are hopefully
    starting to feel confident about using it to move your own data wrangling projects
    forward. Thanks to the vibrant Python community and the constantly evolving suite
    of helpful libraries that its members create and maintain, the work you’ve put
    into learning the fundamentals in this book will still be valuable whether your
    next data wrangling project comes along tomorrow or next year. Also, while Python
    as a programming language is unique in many ways, the programming skills and vocabulary
    that you’ve acquired here will give you a head start with other programming languages,
    especially ones relatively object-oriented ones like JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: Still, one thing I’ve tried to clarify throughout this book is that there are
    times when the “programmatic” solution to a problem is not *really* the most efficient
    one. Our work with Excel and XML files in [Chapter 4](ch04.html#chapter4), for
    example, highlighted that sometimes trying to do things programmatically just
    doesn’t make sense. For example, while in [Example 4-12](ch04.html#xml_parsing)
    we *could* have written a Python script to traverse our entire XML document in
    order to discover its structure, it was undoubtedly faster and easier to simply
    *look* at our data, identify the elements that interested us, and write our Python
    program to target them directly. Likewise, there are times when writing a Python
    program *at all* can be more effort than a particular data wrangling project really
    requires, especially if it is smaller or more exploratory. Though *pandas* is
    an incredibly useful library, you can still end up writing a fair amount of code
    just to get a basic sense of what a new dataset contains. In other words, while
    I fully believe that the power and versatility of Python makes it an indispensable
    data wrangling tool, I also want to highlight a few other free and/or open source
    tools that I think you’ll find useful^([1](ch11.html#idm45143393541072)) as a
    complement to Python in your data wrangling projects.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Tools for Data Review
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python offers great speed and flexibility when it comes to actually accessing
    and wrangling data, but it’s not particularly well suited to letting you actually
    *look* at your data. So throughout this book we’ve relied on basic text editors
    (and occasionally web browsers) when we want to browse our dataset visually. While
    text editors are a great first step for this, you’ll also want to get comfortable
    with at least one of each of the following program types to help you get a quick
    initial overview of your data—especially if the files you’re working with are
    not too large.
  prefs: []
  type: TYPE_NORMAL
- en: Spreadsheet Programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s possible that before you began reading this book you were already familiar
    with spreadsheet programs—whether online versions like Google Sheets, paid local
    software options like Microsoft Excel, or contribution-supported, open source
    alternatives like Libre Office Calc. Spreadsheet programs typically come bundled
    with “office”-style software suites and offer basic calculation, analysis, and
    charting functionality. In general, there is not a huge variation in what these
    programs can do, though some are more flexible than others. I prefer [LibreOffice](https://libreoffice.org),
    for example, because it is free, open source, and works across platforms (including
    less common ones, like Linux). It even has a certified app for Chromebooks and
    Android devices called [Collabora](https://collaboraoffice.com/press-releases/collabora-office-ships-for-chromebooks).
    That said, if you already have or are familiar with a particular spreadsheet program,
    there is no important reason to switch to any other, as long as you are not going
    broke paying for it. Whatever you do, do *not* use pirated software; in a world
    where ransomware is running rampant—i.e., this one—it’s simply not worth the risk
    to your devices and data!
  prefs: []
  type: TYPE_NORMAL
- en: 'While many spreadsheet programs have advanced functions that approximate what
    Python does (on a much smaller scale), I usually find myself turning to them for
    very specific data wrangling and assessment tasks. In particular, I will often
    use a spreadsheet program to quickly do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Change file formats
  prefs: []
  type: TYPE_NORMAL
- en: For example, if my data is provided as an multisheet XLSX file, I might open
    it in a spreadsheet program and save only the sheet I am interested in as a *.csv*.
  prefs: []
  type: TYPE_NORMAL
- en: Rename columns
  prefs: []
  type: TYPE_NORMAL
- en: If there are not many columns, I may change those with awkward or nondescript
    headers to be something more readable and/or intuitive for my purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Get a “feel” for data values
  prefs: []
  type: TYPE_NORMAL
- en: Are the values provided in a “date” column actually dates? Or are they simply
    years? Are there a lot of obviously missing data values? If my dataset is relatively
    small, just visually scanning through the data in a spreadsheet program can sometimes
    be enough to determine whether I have the data I need—or if I need to move on.
  prefs: []
  type: TYPE_NORMAL
- en: Generate basic summary statistics
  prefs: []
  type: TYPE_NORMAL
- en: Of course I can do this in Python, and most of the time I do. But if I have
    only a few hundred rows of data, typing `=MEDIAN()` and then selecting the cells
    of interest is sometimes faster, especially if my original data file has metadata
    in it that would otherwise need to be stripped out (as we saw in [Chapter 4](ch04.html#chapter4)
    and again in Chapters [7](ch07.html#chapter7) and [8](ch08.html#chapter8)).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, every tool comes with its trade-offs, and previewing data in a spreadsheet
    program can create some unexpected results. As you might guess from our extended
    adventures in dealing with XLS-style “dates,” previewing a file that contains
    date-like values can cause them to display very differently according to particular
    spreadsheet program and its default handling and rendering of dates. As a result,
    you should *always* inspect date-like values using a text editor if the original
    data format is text based (e.g., *.csv*, *.tsv*, or *.txt*). Likewise, be sure
    to confirm the formatting (whether default or applied) on any number-containing
    cells, as truncating or rounding of values can obscure important variations in
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: OpenRefine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the tools I turn to most for my initial exploration of larger, structured
    datasets is [OpenRefine](https://openrefine.org). In my experience, OpenRefine
    has proved to be a unique software tool that helps bridge the gap between traditional
    spreadsheet programs and full-on programming languages like Python. Like spreadsheet
    programs, OpenRefine operates through a graphical user interface (GUI), so most
    of your work with it will involve pointing and clicking with a mouse. *Unlike*
    spreadsheet programs, however, you don’t scroll through rows of data to get a
    sense of what they contain; instead, you can use menu options to create *facets*
    that provide summary information similar to that provided by the [pandas `value_counts()`
    method](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)—but
    without needing to write any code. OpenRefine also supports batch editing, implements
    several algorithms for string matching (including the fingerprinting method we
    used in [Example 6-11](ch06.html#ppp_lender_names)), and can import large files
    in segments (e.g., 100,000 rows at a time). In truth, OpenRefine is often my first
    stop when wrangling a new dataset, because it easily opens a variety of data formats
    and even offers a handy live preview of how the data will be parsed based on your
    selection of delimiters, for example. Once your dataset is loaded, OpenRefine
    can also provide almost one-click answers to questions like “What is the most
    common value in column *x*?” Finally, any time you make an actual change to a
    data file in OpenRefine (as opposed to just clustering or faceting it), it automatically
    records your actions in an exportable *.json* file that you can then apply to
    a *different* OpenRefine file in order to have those actions automatically repeated
    (usually in seconds). This is incredibly useful if you need to rename or rearrange
    data columns for a dataset that is regularly updated by the data provider. It’s
    even *more* useful, however, if you need *someone else* to be able to do it and
    they don’t have or cannot use Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the most part, I use OpenRefine to easily do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Preview small pieces of large datasets
  prefs: []
  type: TYPE_NORMAL
- en: OpenRefine allows you to load (and skip) as many rows in your dataset as you
    like. This is especially handy for large datasets when I want to get a reasonable
    sense of what they contain, but I know truly nothing about them. I can start by
    loading 50,000 or 100,000 rows and use facets and other functions to get an overview
    of, say, what the data types are and how the overall dataset is organized.
  prefs: []
  type: TYPE_NORMAL
- en: Get quick top-line information about a dataset
  prefs: []
  type: TYPE_NORMAL
- en: What’s [the most common type of film permit requested in New York City](https://data.cityofnewyork.us/City-Government/Film-Permits/tg4x-b46p)?
    And what is the most popular borough? As shown in [Figure 11-1](#openrefine_film_facets),
    OpenRefine can give you these counts in one or two clicks and allows you to create
    cross-tabulations just as quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Do basic transformations that spreadsheet programs don’t support
  prefs: []
  type: TYPE_NORMAL
- en: Some spreadsheet programs lack functionality, like the ability to split a string
    on a particular character, or may have limited regular expression support. One
    of my favorite features of OpenRefine is batch editing, which you can do quickly
    and easily through the lefthand facet window.
  prefs: []
  type: TYPE_NORMAL
- en: Autogenerate macros
  prefs: []
  type: TYPE_NORMAL
- en: Many spreadsheet programs will let you record *macros* that automate certain
    actions, but OpenRefine records these for you by default—making it a more powerful
    tool with a lower learning curve for this type of task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, there are some aspects of working with OpenRefine that can take
    some getting used to. First, while installation is getting more user friendly,
    it relies on having another programming language, called Java, installed on your
    computer, so getting it up and running can be a multistep process. Launching the
    program once it’s installed is also a little unusual: You need to both click (or
    double-click) on the program icon to launch and, in some cases, open a browser
    window pointed to your “localhost” address (typically `http://127.0.0.1:3333/`
    or `http://localhost:3333`). Like Jupyter Notebook, OpenRefine actually runs through
    a tiny web server on your computer, and the interface is just a web page that
    behaves sort of like a supercharged spreadsheet program. Despite these quirks,
    OpenRefine is *incredibly* useful and often a great place to start when you want
    to do some initial exploration of a (potentially messy) dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenRefine NYC Film Permit Facets.](assets/ppdw_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. OpenRefine NYC film permit facets
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Additional Tools for Sharing and Presenting Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.html#chapter10), we focused on how to select and refine
    visualizations using Python and key libraries like `seaborn` and `matplotlib`.
    While the degree of customization that you can achieve using these tools is impressive,
    there are times when you may need to make a small tweak to a visualization and
    you may not want—or be able—to regenerate it from the original data source with
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to quickly add or change something minor on a visualization, having
    access to image editing software is valuable. And while you’re probably familiar
    with the very powerful—and very expensive—commercial software applications for
    editing images, you may not realize that there are similarly powerful tools that
    are free and open source.
  prefs: []
  type: TYPE_NORMAL
- en: Image Editing for JPGs, PNGs, and GIFs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For editing pixel-based images, the GNU Image Manipulation Program (GIMP) is
    an especially good option if you’re looking for something powerful but you can’t
    (or don’t want to) pay a lot for it. [GIMP](https://gimp.org) is free and open
    source, and it works across platforms. While the style of its user interface is
    decidedly outdated (the interface is being overhauled at the time of this writing),
    the reality is that the program itself can probably do whatever basic (and not-so-basic)
    high-quality image editing you may need, especially if you’re just looking to
    add (or remove) some text or annotations from an image, update the axis labels,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: It’s true that GIMP can have a somewhat steep learning curve. The keyboard shortcuts
    may not be what you expect, and the placement of some menus and the look of their
    icons do not match what you’ll see in commercial software. That said, unless you
    are an expert in another image editing program and you are willing to pay (and
    continue to pay) for access to it, whatever time you invest in learning GIMP will
    be well worth it. Especially if what you need is occasional access to image editing
    software, GIMP is a powerful and flexible choice.
  prefs: []
  type: TYPE_NORMAL
- en: Software for Editing SVGs and Other Vector Formats
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you plan to use your visualizations for print or other high-resolution (or
    flexible-resolution) contexts, you may well choose to save it in a vector format.
    Although the file sizes are larger, vector graphics are much more flexible than
    their pixel-driven counterparts; they can be scaled up and down without losing
    quality by becoming pixelated or blurry. They can’t, however, be edited effectively
    with bitmap software like GIMP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, if you have the budget for commercial software, you should go ahead
    and use it—but here, too, you have a free and open source option. Like GIMP, [Inkscape](https://inkscape.org)
    is free, open source, and cross platform. And, like GIMP, it has almost all the
    same features as expensive commercial vector editing software. Even better, if
    you take the time to get comfortable with vector editing software, it won’t just
    let you tweak your digital-to-print data graphics—vector editing software is also
    essential to T-shirt printing, laser cutting, and lots of other digital-to-physical
    work. If you’re just starting out, Inkscape is also definitely the right price:
    free!'
  prefs: []
  type: TYPE_NORMAL
- en: Reflecting on Ethics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main focus of this book has been on building data wrangling skills—in large
    part to support our interest in assessing and improving data quality. Along the
    way, we’ve touched on the broader implications of data quality, both abstractly
    and concretely. Poor data quality can lead to analyses that produce a misleading,
    distorted, or discriminatory view of the world; couple this with [the scale and
    ubiquity of data-driven systems](https://penguinrandomhouse.com/books/241363/weapons-of-math-destruction-by-cathy-oneil)
    today, and the resulting harms can be substantial and far-reaching. While you
    can use the methods in this book to test and improve the quality of your data,
    there is unfortunately still plenty of room for “good-quality” data to be obtained
    unethically. And just as with every other part of the data wrangling process,
    it’s up to you to decide what kind of data you’re comfortable working with and
    for what purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'One strategy for ensuring that your data wrangling work doesn’t unintentionally
    violate your own ethical standards is to develop a checklist. By developing a
    list of questions you ask yourself about your data sources and how the output
    of your analysis will be used, you can determine early on if a given data wrangling
    project is one you’re willing to pursue. The following checklist is adapted from
    one shared by data experts [DJ Patil, Hilary Mason, and Mike Loukides](https://oreilly.com/radar/of-oaths-and-checklists).
    Like the list of data characteristics in [Chapter 3](ch03.html#chapter3), the
    purpose here is not to reject a data project unless the answer to every question
    is “yes”; the goal is to think critically about *all* aspects of data quality—including
    those that may be outside our control. True, we may only be able to decline (rather
    than change) a project that doesn’t meet our ethical standards, but if you voice
    your concerns, you may help make room for others to do the same. At worse, the
    project is taken on by someone else and your conscience is (somewhat) clear. At
    best, you may inspire others to consider the ethical implications of their work
    before they take on their next project as well. Here are some questions that you
    might want to include:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the design of the data collection reflect the values of the community it
    is about?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the members of that community know that it was collected, and did they have
    a meaningful way to decline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Has the data been evaluated for representativeness?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a way to test the data for bias?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are our data features accurate proxies for the phenomena we want to describe?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Will our analysis be replaced if and when the data becomes out of date?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ultimately, you may decide that your own concerns about data have a different
    focus. Whatever you choose to include in your own checklist, however, you’ll find
    your data principles much easier to stick to if you lay them out in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of this book, we have covered everything from the basics of
    Python programming and data quality assessment to wrangling data from half-a-dozen
    file formats and APIs. We’ve applied our skills to some typically messy and problematic
    real-world data and refined our code to make future projects easier. We’ve even
    explored how to do basic data analysis and visually present our data in support
    of our insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’ve made it this far, then I imagine by this point you’ve caught some
    sort of “bug”: for programming, for data, for analysis and visualization—or maybe
    all of the above. Whatever brought you to this book, I hope you’ve found at least
    some of what you were looking for, including, perhaps, the confidence to take
    the next step. Because whatever else may change about the world of data wrangling
    in the coming years, one thing is sure to be true: we need as many people as possible
    doing this work critically and thoughtfully. Why shouldn’t one of them be you?'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11.html#idm45143393541072-marker)) I certainly do!
  prefs: []
  type: TYPE_NORMAL
