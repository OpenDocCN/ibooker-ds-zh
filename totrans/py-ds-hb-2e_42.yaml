- en: Chapter 37\. What Is Machine Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第37章 什么是机器学习？
- en: Before we take a look at the details of several machine learning methods, let’s
    start by looking at what machine learning is, and what it isn’t. Machine learning
    is often categorized as a subfield of artificial intelligence, but I find that
    categorization can be misleading. The study of machine learning certainly arose
    from research in this context, but in the data science application of machine
    learning methods, it’s more helpful to think of machine learning as a means of
    *building models of data*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解几种机器学习方法的细节之前，让我们先来看看机器学习的定义及其非定义部分。机器学习通常被归类为人工智能的一个子领域，但我发现这种分类可能会产生误导。机器学习的研究确实起源于这一背景的研究，但在数据科学应用机器学习方法时，将机器学习视为一种*构建数据模型*的手段更为有帮助。
- en: In this context, “learning” enters the fray when we give these models *tunable
    parameters* that can be adapted to observed data; in this way the program can
    be considered to be “learning” from the data. Once these models have been fit
    to previously seen data, they can be used to predict and understand aspects of
    newly observed data. I’ll leave to the reader the more philosophical digression
    regarding the extent to which this type of mathematical, model-based “learning”
    is similar to the “learning” exhibited by the human brain.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“学习”进入战场时，我们为这些模型提供*可调参数*，这些参数可以根据观察到的数据进行调整；通过这种方式，程序可以被认为是从数据中“学习”。一旦这些模型适应于先前看到的数据，它们就可以用于预测和理解新观察到的数据的各个方面。关于这种基于数学模型的“学习”与人脑展现的“学习”在多大程度上相似，我将留给读者更多哲学的探讨。
- en: Understanding the problem setting in machine learning is essential to using
    these tools effectively, and so we will start with some broad categorizations
    of the types of approaches we’ll discuss here.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 理解机器学习中的问题设置对有效使用这些工具至关重要，因此我们将从这里讨论的一些方法类型的广泛分类开始。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: All of the figures in this chapter are generated based on actual machine learning
    computations; the code behind them can be found in the [online appendix](https://oreil.ly/o1Zya).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有图表均基于实际机器学习计算生成；其背后的代码可以在[在线附录](https://oreil.ly/o1Zya)中找到。
- en: Categories of Machine Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的分类
- en: 'Machine learning can be categorized into two main types: supervised learning
    and unsupervised learning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以分为两种主要类型：监督学习和无监督学习。
- en: '*Supervised learning* involves somehow modeling the relationship between measured
    features of data and some labels associated with the data; once this model is
    determined, it can be used to apply labels to new, unknown data. This is sometimes
    further subdivided into classification tasks and regression tasks: in *classification*,
    the labels are discrete categories, while in *regression*, the labels are continuous
    quantities. You will see examples of both types of supervised learning in the
    following section.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督学习*涉及对数据的测量特征与与数据相关的一些标签之间关系的建模；确定了此模型后，它可以用于对新的未知数据应用标签。有时这进一步细分为分类任务和回归任务：在*分类*中，标签是离散类别，而在*回归*中，标签是连续数量。您将在以下部分看到这两种类型的监督学习的例子。'
- en: '*Unsupervised learning* involves modeling the features of a dataset without
    reference to any label. These models include tasks such as *clustering* and *dimensionality
    reduction.* Clustering algorithms identify distinct groups of data, while dimensionality
    reduction algorithms search for more succinct representations of the data. You
    will also see examples of both types of unsupervised learning in the following
    section.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习*涉及对数据集的特征进行建模，而不参考任何标签。这些模型包括诸如*聚类*和*降维*等任务。聚类算法识别数据的不同组，而降维算法寻找数据更简洁的表示。您也将在以下部分看到这两种类型的无监督学习的例子。'
- en: In addition, there are so-called *semi-supervised learning* methods, which fall
    somewhere between supervised learning and unsupervised learning. Semi-supervised
    learning methods are often useful when only incomplete labels are available.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有所谓的*半监督学习*方法，介于监督学习和无监督学习之间。半监督学习方法在只有不完整标签可用时通常很有用。
- en: Qualitative Examples of Machine Learning Applications
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习应用的定性例子
- en: To make these ideas more concrete, let’s take a look at a few very simple examples
    of a machine learning task. These examples are meant to give an intuitive, non-quantitative
    overview of the types of machine learning tasks we will be looking at in this
    part of the book. In later chapters, we will go into more depth regarding the
    particular models and how they are used. For a preview of these more technical
    aspects, you can find the Python source that generates the figures in the online
    [appendix](https://oreil.ly/o1Zya).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些想法更具体，让我们来看一些非常简单的机器学习任务示例。这些示例旨在给出本书这部分将要讨论的机器学习任务类型的直观非量化概述。在后面的章节中，我们将更深入地讨论特定的模型以及它们的使用方式。如果想预览这些更技术性的方面，可以在在线[附录](https://oreil.ly/o1Zya)中找到生成这些图表的Python源代码。
- en: 'Classification: Predicting Discrete Labels'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类：预测离散标签
- en: We will first take a look at a simple classification task, in which we are given
    a set of labeled points and want to use these to classify some unlabeled points.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看一个简单的分类任务，我们会获得一组带标签的点，并希望利用这些点来对一些未标记的点进行分类。
- en: 'Imagine that we have the data shown in [Figure 37-1](#fig_images_in_0501-classification-1).
    This data is two-dimensional: that is, we have two *features* for each point,
    represented by the (x,y) positions of the points on the plane. In addition, we
    have one of two *class labels* for each point, here represented by the colors
    of the points. From these features and labels, we would like to create a model
    that will let us decide whether a new point should be labeled “blue” or “red.”'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们拥有的数据显示在[图37-1](#fig_images_in_0501-classification-1)中。这些数据是二维的：也就是说，每个点有两个*特征*，由点在平面上的(x,y)位置表示。此外，每个点有两个*类别标签*之一，这里由点的颜色表示。通过这些特征和标签，我们希望创建一个模型，让我们能够决定一个新点应该被标记为“蓝色”还是“红色”。
- en: '![05.01 classification 1](assets/05.01-classification-1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 classification 1](assets/05.01-classification-1.png)'
- en: Figure 37-1\. A simple dataset for classification
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图37-1\. 用于分类的简单数据集
- en: There are a number of possible models for such a classification task, but we
    will start with a very simple one. We will make the assumption that the two groups
    can be separated by drawing a straight line through the plane between them, such
    that points on each side of the line all fall in the same group. Here the *model*
    is a quantitative version of the statement “a straight line separates the classes,”
    while the *model parameters* are the particular numbers describing the location
    and orientation of that line for our data. The optimal values for these model
    parameters are learned from the data (this is the “learning” in machine learning),
    which is often called *training the model*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的分类任务，有许多可能的模型，但我们将从一个非常简单的模型开始。我们假设这两组数据可以通过在它们之间绘制一条直线来分开，这样，线的两边的点都属于同一组。这里的*模型*是声明“一条直线分隔类别”的定量版本，而*模型参数*则是描述该线在我们的数据中位置和方向的特定数值。这些模型参数的最佳值是从数据中学习得来的（这就是机器学习中的“学习”），通常称为*训练模型*。
- en: '[Figure 37-2](#fig_images_in_0501-classification-2) shows a visual representation
    of what the trained model looks like for this data.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图37-2](#fig_images_in_0501-classification-2)展示了这个数据的训练模型的视觉表示。'
- en: '![05.01 classification 2](assets/05.01-classification-2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 classification 2](assets/05.01-classification-2.png)'
- en: Figure 37-2\. A simple classification model
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图37-2\. 一个简单的分类模型
- en: Now that this model has been trained, it can be generalized to new, unlabeled
    data. In other words, we can take a new set of data, draw this line through it,
    and assign labels to the new points based on this model (see [Figure 37-3](#fig_images_in_0501-classification-3)).
    This stage is usually called *prediction*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个模型已经被训练好了，它可以推广到新的未标记数据上。换句话说，我们可以拿到新的数据集，通过这条线进行划分，并根据这个模型为新点分配标签（参见[图37-3](#fig_images_in_0501-classification-3)）。这个阶段通常被称为*预测*。
- en: '![05.01 classification 3](assets/05.01-classification-3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 classification 3](assets/05.01-classification-3.png)'
- en: Figure 37-3\. Applying a classification model to new data
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图37-3\. 将分类模型应用于新数据
- en: 'This is the basic idea of a classification task in machine learning, where
    “classification” indicates that the data has discrete class labels. At first glance
    this may seem trivial: it’s easy to look at our data and draw such a discriminatory
    line to accomplish this classification. A benefit of the machine learning approach,
    however, is that it can generalize to much larger datasets in many more dimensions.
    For example, this is similar to the task of automated spam detection for email.
    In this case, we might use the following features and labels:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习分类任务的基本概念，其中“分类”表示数据具有离散的类标签。乍一看，这可能显得微不足道：很容易看到我们的数据并绘制这样的分界线来完成分类。然而，机器学习方法的好处在于它能够推广到更大的数据集和更多的维度。例如，这类似于电子邮件自动垃圾邮件检测的任务。在这种情况下，我们可能会使用以下特征和标签：
- en: '*feature 1*, *feature 2*, etc. <math alttext="right-arrow"><mo>→</mo></math>
    normalized counts of important words or phrases (“Viagra”, “Extended warranty”,
    etc.)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征 1*, *特征 2* 等 <math alttext="right-arrow"><mo>→</mo></math> 重要单词或短语的标准化计数（如“伟哥”，“延长保修”等）'
- en: '*label* <math alttext="right-arrow"><mo>→</mo></math> “spam” or “not spam”'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签* <math alttext="right-arrow"><mo>→</mo></math> “垃圾邮件”或“非垃圾邮件”'
- en: For the training set, these labels might be determined by individual inspection
    of a small representative sample of emails; for the remaining emails, the label
    would be determined using the model. For a suitably trained classification algorithm
    with enough well-constructed features (typically thousands or millions of words
    or phrases), this type of approach can be very effective. We will see an example
    of such text-based classification in [Chapter 41](ch41.xhtml#section-0505-naive-bayes).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练集，这些标签可能是通过对一小部分代表性电子邮件的个别检查来确定的；对于其余的电子邮件，标签将使用模型确定。对于足够训练良好且特征构造良好的分类算法（通常有数千或数百万个单词或短语），这种方法非常有效。我们将在[第41章](ch41.xhtml#section-0505-naive-bayes)中看到一个基于文本的分类的示例。
- en: Some important classification algorithms that we will discuss in more detail
    are Gaussian naive Bayes (see [Chapter 41](ch41.xhtml#section-0505-naive-bayes)),
    support vector machines (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)),
    and random forest classification (see [Chapter 44](ch44.xhtml#section-0508-random-forests)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论的一些重要分类算法包括高斯朴素贝叶斯（见[第41章](ch41.xhtml#section-0505-naive-bayes)）、支持向量机（见[第43章](ch43.xhtml#section-0507-support-vector-machines)）和随机森林分类（见[第44章](ch44.xhtml#section-0508-random-forests)）。
- en: 'Regression: Predicting Continuous Labels'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归：预测连续标签
- en: In contrast with the discrete labels of a classification algorithm, we will
    next look at a simple regression task in which the labels are continuous quantities.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类算法的离散标签相比，我们接下来将看一个简单的回归任务，其中标签是连续的量。
- en: Consider the data shown in [Figure 37-4](#fig_images_in_0501-regression-1),
    which consists of a set of points each with a continuous label.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[图 37-4](#fig_images_in_0501-regression-1)中显示的数据，其中包含一组具有连续标签的点。
- en: '![05.01 regression 1](assets/05.01-regression-1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 regression 1](assets/05.01-regression-1.png)'
- en: Figure 37-4\. A simple dataset for regression
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 37-4\. 用于回归的简单数据集
- en: 'As with the classification example, we have two-dimensional data: that is,
    there are two features describing each data point. The color of each point represents
    the continuous label for that point.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 就像分类示例一样，我们有二维数据：也就是说，每个数据点有两个描述特征。每个点的颜色代表该点的连续标签。
- en: There are a number of possible regression models we might use for this type
    of data, but here we will use a simple linear regression model to predict the
    points. This simple model assumes that if we treat the label as a third spatial
    dimension, we can fit a plane to the data. This is a higher-level generalization
    of the well-known problem of fitting a line to data with two coordinates.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用多种可能的回归模型来处理这类数据，但在这里我们将使用简单的线性回归模型来预测这些点。这个简单的模型假设，如果我们将标签视为第三个空间维度，我们可以将一个平面拟合到数据中。这是对将两个坐标数据拟合一条直线这一已知问题的更高级的泛化。
- en: We can visualize this setup as shown in [Figure 37-5](#fig_images_in_0501-regression-2).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这种设置视觉化，如[图 37-5](#fig_images_in_0501-regression-2)所示。
- en: '![05.01 regression 2](assets/05.01-regression-2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 regression 2](assets/05.01-regression-2.png)'
- en: Figure 37-5\. A three-dimensional view of the regression data
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 37-5\. 回归数据的三维视图
- en: Notice that the *feature 1–feature 2* plane here is the same as in the two-dimensional
    plot in [Figure 37-4](#fig_images_in_0501-regression-1); in this case, however,
    we have represented the labels by both color and three-dimensional axis position.
    From this view, it seems reasonable that fitting a plane through this three-dimensional
    data would allow us to predict the expected label for any set of input parameters.
    Returning to the two-dimensional projection, when we fit such a plane we get the
    result shown in [Figure 37-6](#fig_images_in_0501-regression-3).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里的 *特征 1–特征 2* 平面与 [Figure 37-4](#fig_images_in_0501-regression-1) 中的二维图是相同的；然而，在这种情况下，我们通过颜色和三维轴位置表示了标签。从这个视角看，通过这三维数据拟合平面来预测任何输入参数的预期标签似乎是合理的。回到二维投影，当我们拟合这样一个平面时，我们得到了
    [Figure 37-6](#fig_images_in_0501-regression-3) 中显示的结果。
- en: '![05.01 regression 3](assets/05.01-regression-3.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 regression 3](assets/05.01-regression-3.png)'
- en: Figure 37-6\. A representation of the regression model
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 37-6\. 回归模型的表示
- en: This plane of fit gives us what we need to predict labels for new points. Visually,
    we find the results shown in [Figure 37-7](#fig_images_in_0501-regression-4).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个拟合平面为我们提供了预测新点标签所需的信息。从视觉上看，我们找到了在 [Figure 37-7](#fig_images_in_0501-regression-4)
    中展示的结果。
- en: '![05.01 regression 4](assets/05.01-regression-4.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 regression 4](assets/05.01-regression-4.png)'
- en: Figure 37-7\. Applying the regression model to new data
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 37-7\. 应用回归模型到新数据上
- en: 'As with the classification example, this task may seem trivial in a low number
    of dimensions. But the power of these methods is that they can be straightforwardly
    applied and evaluated in the case of data with many, many features. For example,
    this is similar to the task of computing the distance to galaxies observed through
    a telescope—in this case, we might use the following features and labels:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类示例一样，这个任务在低维度下可能看起来微不足道。但这些方法的力量在于它们可以在具有许多特征的数据中直接应用和评估。例如，这类似于通过望远镜观测到的星系的距离任务——在这种情况下，我们可能使用以下特征和标签：
- en: '*feature 1*, *feature 2*, etc. <math alttext="right-arrow"><mo>→</mo></math>
    brightness of each galaxy at one of several wavelengths or colors'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征 1*、*特征 2* 等 <math alttext="right-arrow"><mo>→</mo></math> 每个星系在几个波长或颜色之一上的亮度'
- en: '*label* <math alttext="right-arrow"><mo>→</mo></math> distance or redshift
    of the galaxy'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签* <math alttext="right-arrow"><mo>→</mo></math> 星系的距离或红移'
- en: The distances for a small number of these galaxies might be determined through
    an independent set of (typically more expensive or complex) observations. Distances
    to remaining galaxies could then be estimated using a suitable regression model,
    without the need to employ the more expensive observation across the entire set.
    In astronomy circles, this is known as the “photometric redshift” problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其中一小部分星系的距离可能通过独立的（通常更昂贵或复杂）观测来确定。然后可以使用适当的回归模型估计其余星系的距离，而无需在整个集合上使用更昂贵的观测。在天文学界，这被称为“光度红移”问题。
- en: Some important regression algorithms that we will discuss are linear regression
    (see [Chapter 42](ch42.xhtml#section-0506-linear-regression)), support vector
    machines (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)),
    and random forest regression (see [Chapter 44](ch44.xhtml#section-0508-random-forests)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的一些重要回归算法包括线性回归（参见 [Chapter 42](ch42.xhtml#section-0506-linear-regression)）、支持向量机（参见
    [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)）和随机森林回归（参见 [Chapter 44](ch44.xhtml#section-0508-random-forests)）。
- en: 'Clustering: Inferring Labels on Unlabeled Data'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类：推断未标记数据的标签
- en: The classification and regression illustrations we just saw are examples of
    supervised learning algorithms, in which we are trying to build a model that will
    predict labels for new data. Unsupervised learning involves models that describe
    data without reference to any known labels.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到的分类和回归示例都是监督学习算法的例子，我们试图建立一个可以预测新数据标签的模型。无监督学习涉及描述数据而不涉及任何已知标签的模型。
- en: One common case of unsupervised learning is “clustering,” in which data is automatically
    assigned to some number of discrete groups. For example, we might have some two-dimensional
    data like that shown in [Figure 37-8](#fig_images_in_0501-clustering-1).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的一个常见情况是“聚类”，其中数据自动分配给一些离散的组。例如，我们可能有一些类似于 [Figure 37-8](#fig_images_in_0501-clustering-1)
    中所示的二维数据。
- en: '![05.01 clustering 1](assets/05.01-clustering-1.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 clustering 1](assets/05.01-clustering-1.png)'
- en: Figure 37-8\. Example data for clustering
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 37-8\. 聚类示例数据
- en: By eye, it is clear that each of these points is part of a distinct group. Given
    this input, a clustering model will use the intrinsic structure of the data to
    determine which points are related. Using the very fast and intuitive *k*-means
    algorithm (see [Chapter 47](ch47.xhtml#section-0511-k-means)), we find the clusters
    shown in [Figure 37-9](#fig_images_in_0501-clustering-2).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过目测，很明显每个点都属于一个明显的组。基于数据的内在结构，聚类模型将确定哪些点是相关的。使用非常快速和直观的*k*-means算法（参见[第47章](ch47.xhtml#section-0511-k-means)），我们找到如[图37-9](#fig_images_in_0501-clustering-2)所示的聚类。
- en: '![05.01 clustering 2](assets/05.01-clustering-2.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 clustering 2](assets/05.01-clustering-2.png)'
- en: Figure 37-9\. Data labeled with a k-means clustering model
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图37-9\. 使用k-means聚类模型标记的数据
- en: '*k*-means fits a model consisting of *k* cluster centers; the optimal centers
    are assumed to be those that minimize the distance of each point from its assigned
    center. Again, this might seem like a trivial exercise in two dimensions, but
    as our data becomes larger and more complex such clustering algorithms can continue
    to be employed to extract useful information from the dataset.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means算法适配了一个模型，包括*k*个聚类中心；最优的中心被认为是最小化每个点到其分配中心距离的那些中心。再次强调，在二维数据中这可能看起来像是一个微不足道的练习，但随着数据变得更大更复杂，这样的聚类算法可以继续从数据集中提取有用信息。'
- en: We will discuss the *k*-means algorithm in more depth in [Chapter 47](ch47.xhtml#section-0511-k-means).
    Other important clustering algorithms include Gaussian mixture models (see [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures))
    and spectral clustering (see [Scikit-Learn’s clustering documentation](https://oreil.ly/9FHKO)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第47章](ch47.xhtml#section-0511-k-means)更深入地讨论*k*-means算法。其他重要的聚类算法包括高斯混合模型（参见[第48章](ch48.xhtml#section-0512-gaussian-mixtures)）和谱聚类（参见[Scikit-Learn的聚类文档](https://oreil.ly/9FHKO)）。
- en: 'Dimensionality Reduction: Inferring Structure of Unlabeled Data'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维：推断未标记数据的结构
- en: Dimensionality reduction is another example of an unsupervised algorithm, in
    which labels or other information are inferred from the structure of the dataset
    itself. Dimensionality reduction is a bit more abstract than the examples we looked
    at before, but generally it seeks to pull out some low-dimensional representation
    of data that in some way preserves relevant qualities of the full dataset. Different
    dimensionality reduction routines measure these relevant qualities in different
    ways, as we will see in [Chapter 46](ch46.xhtml#section-0510-manifold-learning).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是无监督算法的另一个示例，其中标签或其他信息是从数据集本身的结构中推断出来的。降维比我们之前看过的例子更加抽象，但通常它试图提取数据的一些低维表示，以某种方式保留完整数据集的相关特性。不同的降维例程以不同的方式衡量这些相关特性，正如我们将在[第46章](ch46.xhtml#section-0510-manifold-learning)中看到的那样。
- en: As an example of this, consider the data shown in [Figure 37-10](#fig_images_in_0501-dimesionality-1).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑显示在[图37-10](#fig_images_in_0501-dimesionality-1)中的数据。
- en: '![05.01 dimesionality 1](assets/05.01-dimesionality-1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 dimesionality 1](assets/05.01-dimesionality-1.png)'
- en: Figure 37-10\. Example data for dimensionality reduction
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图37-10\. 降维的示例数据
- en: 'Visually, it is clear that there is some structure in this data: it is drawn
    from a one-dimensional line that is arranged in a spiral within this two-dimensional
    space. In a sense, you could say that this data is “intrinsically” only one-dimensional,
    though this one-dimensional data is embedded in two-dimensional space. A suitable
    dimensionality reduction model in this case would be sensitive to this nonlinear
    embedded structure and be able to detect this lower-dimensionality representation.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，很明显这些数据中存在一些结构：它们来自一个一维线，在二维空间内以螺旋的方式排列。从某种意义上说，你可以说这些数据“本质上”只有一维，尽管这些一维数据嵌入在二维空间中。在这种情况下，一个合适的降维模型应该对这种非线性嵌入结构敏感，并能够检测到这种较低维度的表示。
- en: '[Figure 37-11](#fig_images_in_0501-dimesionality-2) shows a visualization of
    the results of the Isomap algorithm, a manifold learning algorithm that does exactly
    this.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[图37-11](#fig_images_in_0501-dimesionality-2)展示了Isomap算法的结果可视化，这是一种能够实现这一目标的流形学习算法。'
- en: Notice that the colors (which represent the extracted one-dimensional latent
    variable) change uniformly along the spiral, which indicates that the algorithm
    did in fact detect the structure we saw by eye. As with the previous examples,
    the power of dimensionality reduction algorithms becomes clearer in higher-dimensional
    cases. For example, we might wish to visualize important relationships within
    a dataset that has 100 or 1,000 features. Visualizing 1,000-dimensional data is
    a challenge, and one way we can make this more manageable is to use a dimensionality
    reduction technique to reduce the data to 2 or 3 dimensions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，颜色（代表提取的一维潜变量）沿螺旋线均匀变化，这表明算法确实检测到了我们肉眼看到的结构。与前面的例子一样，降维算法在高维情况下的作用变得更加明显。例如，我们可能希望可视化一个具有100或1000个特征的数据集中的重要关系。可视化1000维数据是一项挑战，我们可以通过使用降维技术将数据降低到2或3维来使其更易管理。
- en: Some important dimensionality reduction algorithms that we will discuss are
    principal component analysis (see [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis))
    and various manifold learning algorithms, including Isomap and locally linear
    embedding (see [Chapter 46](ch46.xhtml#section-0510-manifold-learning)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论一些重要的降维算法，包括主成分分析（参见[第45章](ch45.xhtml#section-0509-principal-component-analysis)）和各种流形学习算法，包括Isomap和局部线性嵌入（参见[第46章](ch46.xhtml#section-0510-manifold-learning)）。
- en: '![05.01 dimesionality 2](assets/05.01-dimesionality-2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![05.01 dimesionality 2](assets/05.01-dimesionality-2.png)'
- en: Figure 37-11\. Data with labels learned via dimensionality reduction
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图37-11\. 通过降维学习得到的带标签数据
- en: Summary
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Here we have seen a few simple examples of some of the basic types of machine
    learning approaches. Needless to say, there are a number of important practical
    details that we have glossed over, but this chapter was designed to give you a
    basic idea of what types of problems machine learning approaches can solve.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了一些基本的机器学习方法的简单示例。不用说，有许多重要的实际细节我们没有详细讨论，但本章旨在让您了解机器学习方法可以解决哪些类型的问题的基本概念。
- en: 'In short, we saw the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们看到了以下内容：
- en: '*Supervised learning*: Models that can predict labels based on labeled training
    data'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监督学习*：基于标记的训练数据可以预测标签的模型。'
- en: '*Classification*: Models that predict labels as two or more discrete categories'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类*：预测两个或更多离散类别标签的模型'
- en: '*Regression*: Models that predict continuous labels'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*回归*：预测连续标签的模型'
- en: '*Unsupervised learning*: Models that identify structure in unlabeled data'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督学习*：识别无标签数据中结构的模型'
- en: '*Clustering*: Models that detect and identify distinct groups in the data'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聚类*：检测并识别数据中不同组的模型'
- en: '*Dimensionality reduction*: Models that detect and identify lower-dimensional
    structure in higher-dimensional data'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降维*：检测并识别高维数据中的低维结构的模型'
- en: In the following chapters, we will go into much greater depth within these categories,
    and see some more interesting examples of where these concepts can be useful.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨这些类别，并看到这些概念在哪些场景中更加有用。
