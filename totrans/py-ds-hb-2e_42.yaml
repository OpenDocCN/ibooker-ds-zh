- en: Chapter 37\. What Is Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we take a look at the details of several machine learning methods, let’s
    start by looking at what machine learning is, and what it isn’t. Machine learning
    is often categorized as a subfield of artificial intelligence, but I find that
    categorization can be misleading. The study of machine learning certainly arose
    from research in this context, but in the data science application of machine
    learning methods, it’s more helpful to think of machine learning as a means of
    *building models of data*.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, “learning” enters the fray when we give these models *tunable
    parameters* that can be adapted to observed data; in this way the program can
    be considered to be “learning” from the data. Once these models have been fit
    to previously seen data, they can be used to predict and understand aspects of
    newly observed data. I’ll leave to the reader the more philosophical digression
    regarding the extent to which this type of mathematical, model-based “learning”
    is similar to the “learning” exhibited by the human brain.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem setting in machine learning is essential to using
    these tools effectively, and so we will start with some broad categorizations
    of the types of approaches we’ll discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All of the figures in this chapter are generated based on actual machine learning
    computations; the code behind them can be found in the [online appendix](https://oreil.ly/o1Zya).
  prefs: []
  type: TYPE_NORMAL
- en: Categories of Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning can be categorized into two main types: supervised learning
    and unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Supervised learning* involves somehow modeling the relationship between measured
    features of data and some labels associated with the data; once this model is
    determined, it can be used to apply labels to new, unknown data. This is sometimes
    further subdivided into classification tasks and regression tasks: in *classification*,
    the labels are discrete categories, while in *regression*, the labels are continuous
    quantities. You will see examples of both types of supervised learning in the
    following section.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unsupervised learning* involves modeling the features of a dataset without
    reference to any label. These models include tasks such as *clustering* and *dimensionality
    reduction.* Clustering algorithms identify distinct groups of data, while dimensionality
    reduction algorithms search for more succinct representations of the data. You
    will also see examples of both types of unsupervised learning in the following
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are so-called *semi-supervised learning* methods, which fall
    somewhere between supervised learning and unsupervised learning. Semi-supervised
    learning methods are often useful when only incomplete labels are available.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative Examples of Machine Learning Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make these ideas more concrete, let’s take a look at a few very simple examples
    of a machine learning task. These examples are meant to give an intuitive, non-quantitative
    overview of the types of machine learning tasks we will be looking at in this
    part of the book. In later chapters, we will go into more depth regarding the
    particular models and how they are used. For a preview of these more technical
    aspects, you can find the Python source that generates the figures in the online
    [appendix](https://oreil.ly/o1Zya).
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification: Predicting Discrete Labels'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will first take a look at a simple classification task, in which we are given
    a set of labeled points and want to use these to classify some unlabeled points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we have the data shown in [Figure 37-1](#fig_images_in_0501-classification-1).
    This data is two-dimensional: that is, we have two *features* for each point,
    represented by the (x,y) positions of the points on the plane. In addition, we
    have one of two *class labels* for each point, here represented by the colors
    of the points. From these features and labels, we would like to create a model
    that will let us decide whether a new point should be labeled “blue” or “red.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 classification 1](assets/05.01-classification-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-1\. A simple dataset for classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a number of possible models for such a classification task, but we
    will start with a very simple one. We will make the assumption that the two groups
    can be separated by drawing a straight line through the plane between them, such
    that points on each side of the line all fall in the same group. Here the *model*
    is a quantitative version of the statement “a straight line separates the classes,”
    while the *model parameters* are the particular numbers describing the location
    and orientation of that line for our data. The optimal values for these model
    parameters are learned from the data (this is the “learning” in machine learning),
    which is often called *training the model*.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 37-2](#fig_images_in_0501-classification-2) shows a visual representation
    of what the trained model looks like for this data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 classification 2](assets/05.01-classification-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-2\. A simple classification model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that this model has been trained, it can be generalized to new, unlabeled
    data. In other words, we can take a new set of data, draw this line through it,
    and assign labels to the new points based on this model (see [Figure 37-3](#fig_images_in_0501-classification-3)).
    This stage is usually called *prediction*.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 classification 3](assets/05.01-classification-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-3\. Applying a classification model to new data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is the basic idea of a classification task in machine learning, where
    “classification” indicates that the data has discrete class labels. At first glance
    this may seem trivial: it’s easy to look at our data and draw such a discriminatory
    line to accomplish this classification. A benefit of the machine learning approach,
    however, is that it can generalize to much larger datasets in many more dimensions.
    For example, this is similar to the task of automated spam detection for email.
    In this case, we might use the following features and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '*feature 1*, *feature 2*, etc. <math alttext="right-arrow"><mo>→</mo></math>
    normalized counts of important words or phrases (“Viagra”, “Extended warranty”,
    etc.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*label* <math alttext="right-arrow"><mo>→</mo></math> “spam” or “not spam”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the training set, these labels might be determined by individual inspection
    of a small representative sample of emails; for the remaining emails, the label
    would be determined using the model. For a suitably trained classification algorithm
    with enough well-constructed features (typically thousands or millions of words
    or phrases), this type of approach can be very effective. We will see an example
    of such text-based classification in [Chapter 41](ch41.xhtml#section-0505-naive-bayes).
  prefs: []
  type: TYPE_NORMAL
- en: Some important classification algorithms that we will discuss in more detail
    are Gaussian naive Bayes (see [Chapter 41](ch41.xhtml#section-0505-naive-bayes)),
    support vector machines (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)),
    and random forest classification (see [Chapter 44](ch44.xhtml#section-0508-random-forests)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Regression: Predicting Continuous Labels'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast with the discrete labels of a classification algorithm, we will
    next look at a simple regression task in which the labels are continuous quantities.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the data shown in [Figure 37-4](#fig_images_in_0501-regression-1),
    which consists of a set of points each with a continuous label.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 regression 1](assets/05.01-regression-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-4\. A simple dataset for regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As with the classification example, we have two-dimensional data: that is,
    there are two features describing each data point. The color of each point represents
    the continuous label for that point.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of possible regression models we might use for this type
    of data, but here we will use a simple linear regression model to predict the
    points. This simple model assumes that if we treat the label as a third spatial
    dimension, we can fit a plane to the data. This is a higher-level generalization
    of the well-known problem of fitting a line to data with two coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize this setup as shown in [Figure 37-5](#fig_images_in_0501-regression-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 regression 2](assets/05.01-regression-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-5\. A three-dimensional view of the regression data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that the *feature 1–feature 2* plane here is the same as in the two-dimensional
    plot in [Figure 37-4](#fig_images_in_0501-regression-1); in this case, however,
    we have represented the labels by both color and three-dimensional axis position.
    From this view, it seems reasonable that fitting a plane through this three-dimensional
    data would allow us to predict the expected label for any set of input parameters.
    Returning to the two-dimensional projection, when we fit such a plane we get the
    result shown in [Figure 37-6](#fig_images_in_0501-regression-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 regression 3](assets/05.01-regression-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-6\. A representation of the regression model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This plane of fit gives us what we need to predict labels for new points. Visually,
    we find the results shown in [Figure 37-7](#fig_images_in_0501-regression-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 regression 4](assets/05.01-regression-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-7\. Applying the regression model to new data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As with the classification example, this task may seem trivial in a low number
    of dimensions. But the power of these methods is that they can be straightforwardly
    applied and evaluated in the case of data with many, many features. For example,
    this is similar to the task of computing the distance to galaxies observed through
    a telescope—in this case, we might use the following features and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '*feature 1*, *feature 2*, etc. <math alttext="right-arrow"><mo>→</mo></math>
    brightness of each galaxy at one of several wavelengths or colors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*label* <math alttext="right-arrow"><mo>→</mo></math> distance or redshift
    of the galaxy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distances for a small number of these galaxies might be determined through
    an independent set of (typically more expensive or complex) observations. Distances
    to remaining galaxies could then be estimated using a suitable regression model,
    without the need to employ the more expensive observation across the entire set.
    In astronomy circles, this is known as the “photometric redshift” problem.
  prefs: []
  type: TYPE_NORMAL
- en: Some important regression algorithms that we will discuss are linear regression
    (see [Chapter 42](ch42.xhtml#section-0506-linear-regression)), support vector
    machines (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)),
    and random forest regression (see [Chapter 44](ch44.xhtml#section-0508-random-forests)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering: Inferring Labels on Unlabeled Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The classification and regression illustrations we just saw are examples of
    supervised learning algorithms, in which we are trying to build a model that will
    predict labels for new data. Unsupervised learning involves models that describe
    data without reference to any known labels.
  prefs: []
  type: TYPE_NORMAL
- en: One common case of unsupervised learning is “clustering,” in which data is automatically
    assigned to some number of discrete groups. For example, we might have some two-dimensional
    data like that shown in [Figure 37-8](#fig_images_in_0501-clustering-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 clustering 1](assets/05.01-clustering-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-8\. Example data for clustering
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By eye, it is clear that each of these points is part of a distinct group. Given
    this input, a clustering model will use the intrinsic structure of the data to
    determine which points are related. Using the very fast and intuitive *k*-means
    algorithm (see [Chapter 47](ch47.xhtml#section-0511-k-means)), we find the clusters
    shown in [Figure 37-9](#fig_images_in_0501-clustering-2).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 clustering 2](assets/05.01-clustering-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-9\. Data labeled with a k-means clustering model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*k*-means fits a model consisting of *k* cluster centers; the optimal centers
    are assumed to be those that minimize the distance of each point from its assigned
    center. Again, this might seem like a trivial exercise in two dimensions, but
    as our data becomes larger and more complex such clustering algorithms can continue
    to be employed to extract useful information from the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss the *k*-means algorithm in more depth in [Chapter 47](ch47.xhtml#section-0511-k-means).
    Other important clustering algorithms include Gaussian mixture models (see [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures))
    and spectral clustering (see [Scikit-Learn’s clustering documentation](https://oreil.ly/9FHKO)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality Reduction: Inferring Structure of Unlabeled Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dimensionality reduction is another example of an unsupervised algorithm, in
    which labels or other information are inferred from the structure of the dataset
    itself. Dimensionality reduction is a bit more abstract than the examples we looked
    at before, but generally it seeks to pull out some low-dimensional representation
    of data that in some way preserves relevant qualities of the full dataset. Different
    dimensionality reduction routines measure these relevant qualities in different
    ways, as we will see in [Chapter 46](ch46.xhtml#section-0510-manifold-learning).
  prefs: []
  type: TYPE_NORMAL
- en: As an example of this, consider the data shown in [Figure 37-10](#fig_images_in_0501-dimesionality-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 dimesionality 1](assets/05.01-dimesionality-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-10\. Example data for dimensionality reduction
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Visually, it is clear that there is some structure in this data: it is drawn
    from a one-dimensional line that is arranged in a spiral within this two-dimensional
    space. In a sense, you could say that this data is “intrinsically” only one-dimensional,
    though this one-dimensional data is embedded in two-dimensional space. A suitable
    dimensionality reduction model in this case would be sensitive to this nonlinear
    embedded structure and be able to detect this lower-dimensionality representation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 37-11](#fig_images_in_0501-dimesionality-2) shows a visualization of
    the results of the Isomap algorithm, a manifold learning algorithm that does exactly
    this.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the colors (which represent the extracted one-dimensional latent
    variable) change uniformly along the spiral, which indicates that the algorithm
    did in fact detect the structure we saw by eye. As with the previous examples,
    the power of dimensionality reduction algorithms becomes clearer in higher-dimensional
    cases. For example, we might wish to visualize important relationships within
    a dataset that has 100 or 1,000 features. Visualizing 1,000-dimensional data is
    a challenge, and one way we can make this more manageable is to use a dimensionality
    reduction technique to reduce the data to 2 or 3 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Some important dimensionality reduction algorithms that we will discuss are
    principal component analysis (see [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis))
    and various manifold learning algorithms, including Isomap and locally linear
    embedding (see [Chapter 46](ch46.xhtml#section-0510-manifold-learning)).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.01 dimesionality 2](assets/05.01-dimesionality-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 37-11\. Data with labels learned via dimensionality reduction
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we have seen a few simple examples of some of the basic types of machine
    learning approaches. Needless to say, there are a number of important practical
    details that we have glossed over, but this chapter was designed to give you a
    basic idea of what types of problems machine learning approaches can solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, we saw the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Supervised learning*: Models that can predict labels based on labeled training
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Classification*: Models that predict labels as two or more discrete categories'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regression*: Models that predict continuous labels'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unsupervised learning*: Models that identify structure in unlabeled data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clustering*: Models that detect and identify distinct groups in the data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dimensionality reduction*: Models that detect and identify lower-dimensional
    structure in higher-dimensional data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapters, we will go into much greater depth within these categories,
    and see some more interesting examples of where these concepts can be useful.
  prefs: []
  type: TYPE_NORMAL
