- en: Chapter 38\. Introducing Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several Python libraries provide solid implementations of a range of machine
    learning algorithms. One of the best known is [Scikit-Learn](http://scikit-learn.org),
    a package that provides efficient versions of a large number of common algorithms.
    Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well
    as by very useful and complete documentation. A benefit of this uniformity is
    that once you understand the basic use and syntax of Scikit-Learn for one type
    of model, switching to a new model or algorithm is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides an overview of the Scikit-Learn API. A solid understanding
    of these API elements will form the foundation for understanding the deeper practical
    discussion of machine learning algorithms and approaches in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by covering data representation in Scikit-Learn, then delve into
    the Estimator API, and finally go through a more interesting example of using
    these tools for exploring a set of images of handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: Data Representation in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is about creating models from data; for that reason, we’ll
    start by discussing how data can be represented. The best way to think about data
    within Scikit-Learn is in terms of *tables*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A basic table is a two-dimensional grid of data, in which the rows represent
    individual elements of the dataset, and the columns represent quantities related
    to each of these elements. For example, consider the [Iris dataset](https://oreil.ly/TeWYs),
    famously analyzed by Ronald Fisher in 1936\. We can download this dataset in the
    form of a Pandas `DataFrame` using the [Seaborn library](http://seaborn.pydata.org),
    and take a look at the first few items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here each row of the data refers to a single observed flower, and the number
    of rows is the total number of flowers in the dataset. In general, we will refer
    to the rows of the matrix as *samples*, and the number of rows as `n_samples`.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, each column of the data refers to a particular quantitative piece
    of information that describes each sample. In general, we will refer to the columns
    of the matrix as *features*, and the number of columns as `n_features`.
  prefs: []
  type: TYPE_NORMAL
- en: The Features Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The table layout makes clear that the information can be thought of as a two-dimensional
    numerical array or matrix, which we will call the *features matrix*. By convention,
    this matrix is often stored in a variable named `X`. The features matrix is assumed
    to be two-dimensional, with shape `[n_samples, n_features]`, and is most often
    contained in a NumPy array or a Pandas `DataFrame`, though some Scikit-Learn models
    also accept SciPy sparse matrices.
  prefs: []
  type: TYPE_NORMAL
- en: The samples (i.e., rows) always refer to the individual objects described by
    the dataset. For example, a sample might represent a flower, a person, a document,
    an image, a sound file, a video, an astronomical object, or anything else you
    can describe with a set of quantitative measurements.
  prefs: []
  type: TYPE_NORMAL
- en: The features (i.e., columns) always refer to the distinct observations that
    describe each sample in a quantitative manner. Features are often real-valued,
    but may be Boolean or discrete-valued in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The Target Array
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the feature matrix `X`, we also generally work with a *label*
    or *target* array, which by convention we will usually call `y`. The target array
    is usually one-dimensional, with length `n_samples`, and is generally contained
    in a NumPy array or Pandas `Series`. The target array may have continuous numerical
    values, or discrete classes/labels. While some Scikit-Learn estimators do handle
    multiple target values in the form of a two-dimensional, `[n_samples, n_targets]`
    target array, we will primarily be working with the common case of a one-dimensional
    target array.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common point of confusion is how the target array differs from the other
    feature columns. The distinguishing characteristic of the target array is that
    it is usually the quantity we want to *predict from the features*: in statistical
    terms, it is the dependent variable. For example, given the preceding data we
    may wish to construct a model that can predict the species of flower based on
    the other measurements; in this case, the `species` column would be considered
    the target array.'
  prefs: []
  type: TYPE_NORMAL
- en: With this target array in mind, we can use Seaborn (discussed in [Chapter 36](ch36.xhtml#section-0414-visualization-with-seaborn))
    to conveniently visualize the data (see [Figure 38-1](#fig_0502-introducing-scikit-learn_files_in_output_9_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![output 9 0](assets/output_9_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-1\. A visualization of the Iris dataset^([1](ch38.xhtml#idm45858744412448))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For use in Scikit-Learn, we will extract the features matrix and target array
    from the `DataFrame`, which we can do using some of the Pandas `DataFrame` operations
    discussed in [Part III](part03.xhtml#section-0300-introduction-to-pandas):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To summarize, the expected layout of features and target values is visualized
    in [Figure 38-2](#fig_images_in_0502-samples-features).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.02 samples features](assets/05.02-samples-features.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-2\. Scikit-Learn’s data layout^([2](ch38.xhtml#idm45858744315904))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With this data properly formatted, we can move on to consider Scikit-Learn’s
    Estimator API.
  prefs: []
  type: TYPE_NORMAL
- en: The Estimator API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Scikit-Learn API is designed with the following guiding principles in mind,
    as outlined in the [Scikit-Learn API paper](http://arxiv.org/abs/1309.0238):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Consistency*'
  prefs: []
  type: TYPE_NORMAL
- en: All objects share a common interface drawn from a limited set of methods, with
    consistent documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Inspection*'
  prefs: []
  type: TYPE_NORMAL
- en: All specified parameter values are exposed as public attributes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Limited object hierarchy*'
  prefs: []
  type: TYPE_NORMAL
- en: Only algorithms are represented by Python classes; datasets are represented
    in standard formats (NumPy arrays, Pandas `DataFrame` objects, SciPy sparse matrices)
    and parameter names use standard Python strings.
  prefs: []
  type: TYPE_NORMAL
- en: '*Composition*'
  prefs: []
  type: TYPE_NORMAL
- en: Many machine learning tasks can be expressed as sequences of more fundamental
    algorithms, and Scikit-Learn makes use of this wherever possible.
  prefs: []
  type: TYPE_NORMAL
- en: '*Sensible defaults*'
  prefs: []
  type: TYPE_NORMAL
- en: When models require user-specified parameters, the library defines an appropriate
    default value.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, these principles make Scikit-Learn very easy to use, once the basic
    principles are understood. Every machine learning algorithm in Scikit-Learn is
    implemented via the Estimator API, which provides a consistent interface for a
    wide range of machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of the API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most commonly, the steps in using the Scikit-Learn Estimator API are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a class of model by importing the appropriate estimator class from Scikit-Learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose model hyperparameters by instantiating this class with desired values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arrange data into a features matrix and target vector, as outlined earlier in
    this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to your data by calling the `fit` method of the model instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apply the model to new data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For supervised learning, often we predict labels for unknown data using the
    `predict` method.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For unsupervised learning, we often transform or infer properties of the data
    using the `transform` or `predict` method.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now step through several simple examples of applying supervised and
    unsupervised learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised Learning Example: Simple Linear Regression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an example of this process, let’s consider a simple linear regression—that
    is, the common case of fitting a line to <math alttext="left-parenthesis x comma
    y right-parenthesis"><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math>
    data. We will use the following simple data for our regression example (see [Figure 38-3](#fig_0502-introducing-scikit-learn_files_in_output_20_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![output 20 0](assets/output_20_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-3\. Data for linear regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With this data in place, we can use the recipe outlined earlier. We’ll walk
    through the process in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Choose a class of model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, every class of model is represented by a Python class. So,
    for example, if we would like to compute a simple `LinearRegression` model, we
    can import the linear regression class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that other more general linear regression models exist as well; you can
    read more about them in the [`sklearn.linear_model` module documentation](https://oreil.ly/YVOFd).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Choose model hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An important point is that *a class of model is not the same as an instance
    of a model*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have decided on our model class, there are still some options open
    to us. Depending on the model class we are working with, we might need to answer
    one or more questions like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Would we like to fit for the offset (i.e., *y*-intercept)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Would we like the model to be normalized?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Would we like to preprocess our features to add model flexibility?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What degree of regularization would we like to use in our model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many model components would we like to use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are examples of the important choices that must be made *once the model
    class is selected*. These choices are often represented as *hyperparameters*,
    or parameters that must be set before the model is fit to data. In Scikit-Learn,
    hyperparameters are chosen by passing values at model instantiation. We will explore
    how you can quantitatively choose hyperparameters in [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our linear regression example, we can instantiate the `LinearRegression`
    class and specify that we’d like to fit the intercept using the `fit_intercept`
    hyperparameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep in mind that when the model is instantiated, the only action is the storing
    of these hyperparameter values. In particular, we have not yet applied the model
    to any data: the Scikit-Learn API makes very clear the distinction between *choice
    of model* and *application of model to data*.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Arrange data into a features matrix and target vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously we examined the Scikit-Learn data representation, which requires
    a two-dimensional features matrix and a one-dimensional target array. Here our
    target variable `y` is already in the correct form (a length-`n_samples` array),
    but we need to massage the data `x` to make it a matrix of size `[n_samples, n_features]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, this amounts to a simple reshaping of the one-dimensional array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Fit the model to the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now it is time to apply our model to the data. This can be done with the `fit`
    method of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This `fit` command causes a number of model-dependent internal computations
    to take place, and the results of these computations are stored in model-specific
    attributes that the user can explore. In Scikit-Learn, by convention all model
    parameters that were learned during the `fit` process have trailing underscores;
    for example in this linear model, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'These two parameters represent the slope and intercept of the simple linear
    fit to the data. Comparing the results to the data definition, we see that they
    are close to the values used to generate the data: a slope of 2 and intercept
    of –1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One question that frequently comes up regards the uncertainty in such internal
    model parameters. In general, Scikit-Learn does not provide tools to draw conclusions
    from internal model parameters themselves: interpreting model parameters is much
    more a *statistical modeling* question than a *machine learning* question. Machine
    learning instead focuses on what the model *predicts*. If you would like to dive
    into the meaning of fit parameters within the model, other tools are available,
    including the [`statsmodels` Python package](https://oreil.ly/adDFZ).'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Predict labels for unknown data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the model is trained, the main task of supervised machine learning is
    to evaluate it based on what it says about new data that was not part of the training
    set. In Scikit-Learn, this can be done using the `predict` method. For the sake
    of this example, our “new data” will be a grid of *x* values, and we will ask
    what *y* values the model predicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we need to coerce these *x* values into a `[n_samples, n_features]`
    features matrix, after which we can feed it to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let’s visualize the results by plotting first the raw data, and then
    this model fit (see [Figure 38-4](#fig_0502-introducing-scikit-learn_files_in_output_41_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![output 41 0](assets/output_41_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-4\. A simple linear regression fit to the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Typically the efficacy of the model is evaluated by comparing its results to
    some known baseline, as we will see in the next example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised Learning Example: Iris Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a look at another example of this process, using the Iris dataset
    we discussed earlier. Our question will be this: given a model trained on a portion
    of the Iris data, how well can we predict the remaining labels?'
  prefs: []
  type: TYPE_NORMAL
- en: For this task, we will use a simple generative model known as *Gaussian naive
    Bayes*, which proceeds by assuming each class is drawn from an axis-aligned Gaussian
    distribution (see [Chapter 41](ch41.xhtml#section-0505-naive-bayes) for more details).
    Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes
    is often a good model to use as a baseline classification, before exploring whether
    improvements can be found through more sophisticated models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would like to evaluate the model on data it has not seen before, so we will
    split the data into a *training set* and a *testing set*. This could be done by
    hand, but it is more convenient to use the `train_test_split` utility function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data arranged, we can follow our recipe to predict the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use the `accuracy_score` utility to see the fraction of predicted
    labels that match their true values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: With an accuracy topping 97%, we see that even this very naive classification
    algorithm is effective for this particular dataset!
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised Learning Example: Iris Dimensionality'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As an example of an unsupervised learning problem, let’s take a look at reducing
    the dimensionality of the Iris data so as to more easily visualize it. Recall
    that the Iris data is four-dimensional: there are four features recorded for each
    sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The task of dimensionality reduction centers around determining whether there
    is a suitable lower-dimensional representation that retains the essential features
    of the data. Often dimensionality reduction is used as an aid to visualizing data:
    after all, it is much easier to plot data in two dimensions than in four dimensions
    or more!'
  prefs: []
  type: TYPE_NORMAL
- en: Here we will use *principal component analysis* (PCA; see [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis)),
    which is a fast linear dimensionality reduction technique. We will ask the model
    to return two components—that is, a two-dimensional representation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the sequence of steps outlined earlier, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s plot the results. A quick way to do this is to insert the results
    into the original Iris `DataFrame`, and use Seaborn’s `lmplot` to show the results
    (see [Figure 38-5](#fig_0502-introducing-scikit-learn_files_in_output_53_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We see that in the two-dimensional representation, the species are fairly well
    separated, even though the PCA algorithm had no knowledge of the species labels!
    This suggests to us that a relatively straightforward classification will probably
    be effective on the dataset, as we saw before.
  prefs: []
  type: TYPE_NORMAL
- en: '![output 53 0](assets/output_53_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-5\. The Iris data projected to two dimensions^([3](ch38.xhtml#idm45858743482640))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Unsupervised Learning Example: Iris Clustering'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s next look at applying clustering to the Iris data. A clustering algorithm
    attempts to find distinct groups of data without reference to any labels. Here
    we will use a powerful clustering method called a *Gaussian mixture model* (GMM),
    discussed in more detail in [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures).
    A GMM attempts to model the data as a collection of Gaussian blobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can fit the Gaussian mixture model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As before, we will add the cluster label to the Iris `DataFrame` and use Seaborn
    to plot the results (see [Figure 38-6](#fig_0502-introducing-scikit-learn_files_in_output_58_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![output 58 0](assets/output_58_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-6\. k-means clusters within the Iris data^([4](ch38.xhtml#idm45858743295632))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By splitting the data by cluster number, we see exactly how well the GMM algorithm
    has recovered the underlying labels: the *setosa* species is separated perfectly
    within cluster 0, while there remains a small amount of mixing between *versicolor*
    and *virginica*. This means that even without an expert to tell us the species
    labels of the individual flowers, the measurements of these flowers are distinct
    enough that we could *automatically* identify the presence of these different
    groups of species with a simple clustering algorithm! This sort of algorithm might
    further give experts in the field clues as to the relationships between the samples
    they are observing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Application: Exploring Handwritten Digits'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate these principles on a more interesting problem, let’s consider
    one piece of the optical character recognition problem: the identification of
    handwritten digits. In the wild, this problem involves both locating and identifying
    characters in an image. Here we’ll take a shortcut and use Scikit-Learn’s set
    of preformatted digits, which is built into the library.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading and Visualizing the Digits Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use Scikit-Learn’s data access interface to take a look at this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The images data is a three-dimensional array: 1,797 samples each consisting
    of an 8 × 8 grid of pixels. Let’s visualize the first hundred of these (see [Figure 38-7](#fig_0502-introducing-scikit-learn_files_in_output_65_0)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![output 65 0](assets/output_65_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-7\. The handwritten digits data; each sample is represented by one
    8 × 8 grid of pixels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In order to work with this data within Scikit-Learn, we need a two-dimensional,
    `[n_samples, n_features]` representation. We can accomplish this by treating each
    pixel in the image as a feature: that is, by flattening out the pixel arrays so
    that we have a length-64 array of pixel values representing each digit. Additionally,
    we need the target array, which gives the previously determined label for each
    digit. These two quantities are built into the digits dataset under the `data`
    and `target` attributes, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We see here that there are 1,797 samples and 64 features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised Learning Example: Dimensionality Reduction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’d like to visualize our points within the 64-dimensional parameter space,
    but it’s difficult to effectively visualize points in such a high-dimensional
    space. Instead, we’ll reduce the number of dimensions, using an unsupervised method.
    Here, we’ll make use of a manifold learning algorithm called Isomap (see [Chapter 46](ch46.xhtml#section-0510-manifold-learning))
    and transform the data to two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We see that the projected data is now two-dimensional. Let’s plot this data
    to see if we can learn anything from its structure (see [Figure 38-8](#fig_0502-introducing-scikit-learn_files_in_output_73_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This plot gives us some good intuition into how well various numbers are separated
    in the larger 64-dimensional space. For example, zeros and ones have very little
    overlap in the parameter space. Intuitively, this makes sense: a zero is empty
    in the middle of the image, while a one will generally have ink in the middle.
    On the other hand, there seems to be a more or less continuous spectrum between
    ones and fours: we can understand this by realizing that some people draw ones
    with “hats” on them, which causes them to look similar to fours.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, however, despite some mixing at the edges, the different groups appear
    to be fairly well localized in the parameter space: this suggests that even a
    very straightforward supervised classification algorithm should perform suitably
    on the full high-dimensional dataset. Let’s give it a try.'
  prefs: []
  type: TYPE_NORMAL
- en: '![output 73 0](assets/output_73_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-8\. An Isomap embedding of the digits data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Classification on Digits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s apply a classification algorithm to the digits data. As we did with the
    Iris data previously, we will split the data into training and testing sets and
    fit a Gaussian naive Bayes model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the model’s predictions, we can gauge its accuracy by comparing
    the true values of the test set to the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: With even this very simple model, we find about 83% accuracy for classification
    of the digits! However, this single number doesn’t tell us where we’ve gone wrong.
    One nice way to do this is to use the *confusion matrix*, which we can compute
    with Scikit-Learn and plot with Seaborn (see [Figure 38-9](#fig_0502-introducing-scikit-learn_files_in_output_81_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us where the mislabeled points tend to be: for example, many of
    the twos here are misclassified as either ones or eights.'
  prefs: []
  type: TYPE_NORMAL
- en: '![output 81 0](assets/output_81_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-9\. A confusion matrix showing the frequency of misclassifications
    by our classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another way to gain intuition into the characteristics of the model is to plot
    the inputs again, with their predicted labels. We’ll use green for correct labels
    and red for incorrect labels; see [Figure 38-10](#fig_0502-introducing-scikit-learn_files_in_output_83_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Examining this subset of the data can give us some insight into where the algorithm
    might be not performing optimally. To go beyond our 83% classification success
    rate, we might switch to a more sophisticated algorithm such as support vector
    machines (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)),
    random forests (see [Chapter 44](ch44.xhtml#section-0508-random-forests)), or
    another classification approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![output 83 0](assets/output_83_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 38-10\. Data showing correct (green) and incorrect (red) labels; for
    a color version of this plot, see the [online version of the book](https://oreil.ly/PDSH_GitHub)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we covered the essential features of the Scikit-Learn data representation
    and the Estimator API. Regardless of the type of estimator used, the same import/instantiate/fit/predict
    pattern holds. Armed with this information, you can explore the Scikit-Learn documentation
    and try out various models on your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will explore perhaps the most important topic in machine
    learning: how to select and validate your model.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch38.xhtml#idm45858744412448-marker)) A full-size, full-color version
    of this figure can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch38.xhtml#idm45858744315904-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/J8V6U).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch38.xhtml#idm45858743482640-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch38.xhtml#idm45858743295632-marker)) A full-size, full-color version
    of this figure can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
