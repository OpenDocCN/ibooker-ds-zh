<html><head></head><body><section data-pdf-bookmark="Chapter 2. Foundations of Probability" data-type="chapter" epub:type="chapter"><div class="chapter" id="foundations-of-probability">&#13;
<h1><span class="label">Chapter 2. </span>Foundations of Probability</h1>&#13;
&#13;
&#13;
<p>Have you ever stopped to consider what your meteorologist really means&#13;
by a 30% chance of rain? Barring a crystal ball, they can’t say for sure that it&#13;
will rain. That is, they are <em>uncertain</em> about an <em>outcome</em>. What they <em>can</em> do is <em>quantify</em> that uncertainty as a value between 0% (certain&#13;
it will not rain) and 100% (certain it will rain).</p>&#13;
&#13;
<p>Data analysts, like meteorologists, do not possess crystal balls. Often,&#13;
we want to make claims about an entire population while only possessing&#13;
the data for a sample. So we too will need to <a data-primary="probability" data-secondary="uncertainty and" data-type="indexterm" id="idm46274547707352"/><a data-primary="probability" data-type="indexterm" id="ch2_term1"/>quantify uncertainty as a probability.</p>&#13;
&#13;
<p>We’ll start this chapter by digging deeper into how probability works and how probabilities are derived. We’ll also use Excel to&#13;
simulate some of the most important theorems in statistics, which are largely based on probability. This will put&#13;
you on excellent footing for <a data-type="xref" href="ch03.html#foundations-of-inference">Chapter 3</a> and <a data-type="xref" href="ch04.html#foundations-of-data-analytics">Chapter 4</a>,&#13;
where we’ll perform inferential statistics in Excel.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Probability and Randomness" data-type="sect1"><div class="sect1" id="idm46274547702712">&#13;
<h1>Probability and Randomness</h1>&#13;
&#13;
<p>Colloquially, we say that <a data-primary="probability" data-secondary="randomness in" data-type="indexterm" id="idm46274547701320"/><a data-primary="randomness in probability" data-type="indexterm" id="idm46274547700344"/>something is “random” when it seems&#13;
out of context or haphazard. In probability, something is <em>random</em> when&#13;
we know an event will <em>have</em> an outcome, but we’re not sure what that&#13;
outcome will be.</p>&#13;
&#13;
<p>Take a six-sided die, for example. When we toss the die, we know it will land on one side—it won’t disappear or land on multiple sides. Knowing that we’ll get <em>an</em> outcome, but not <em>which</em> outcome, is what’s meant by randomness in statistics.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Probability and Sample Space" data-type="sect1"><div class="sect1" id="idm46274547696776">&#13;
<h1>Probability and Sample Space</h1>&#13;
&#13;
<p>We know that when the die lands, it will display a number between one and&#13;
six. This <a data-primary="probability" data-secondary="sample space in" data-type="indexterm" id="idm46274547695064"/><a data-primary="sample space in probability" data-type="indexterm" id="idm46274547694088"/>set of all outcomes is called a <em>sample space</em>. Each of these&#13;
outcomes is assigned a probability greater than zero, because it’s possible that the die may land on any of its sides. Summed together, these probabilities come to 1, because we are certain the outcome will be one of these possibilities in the sample space.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Probability and Experiments" data-type="sect1"><div class="sect1" id="idm46274547692296">&#13;
<h1>Probability and Experiments</h1>&#13;
&#13;
<p>We’ve determined that rolling a die is random, and we’ve outlined&#13;
its <em>sample space</em>. We can now begin to <a data-primary="experiments in probability" data-type="indexterm" id="idm46274547690536"/><a data-primary="probability" data-secondary="experiments in" data-type="indexterm" id="idm46274547689736"/>build experiments for this&#13;
random event. In probability, experiments are procedures that can be&#13;
infinitely replicated with a consistent sample space of possible outcomes.</p>&#13;
&#13;
<p>Some experiments take many years of planning, but ours is fortunately&#13;
simple: roll a die. Each time we do, we get another value between one&#13;
and six. The result is&#13;
our outcome. Each die roll is known as a <em>trial</em> of the experiment.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Unconditional and Conditional Probability" data-type="sect1"><div class="sect1" id="idm46274547687192">&#13;
<h1>Unconditional and Conditional Probability</h1>&#13;
&#13;
<p>Given what we know about probability so far, a <a data-primary="unconditional probability" data-type="indexterm" id="idm46274547685624"/><a data-primary="marginal probability" data-type="indexterm" id="idm46274547684904"/>typical probabilistic question about die rolls might be: “What is the probability of&#13;
rolling a four?” This is called the <em>marginal</em> or <em>unconditional</em>&#13;
probability, as we are only looking at one event in isolation.</p>&#13;
&#13;
<p>But what about a <a data-primary="conditional probability" data-type="indexterm" id="idm46274547682408"/><a data-primary="joint probability" data-type="indexterm" id="idm46274547681672"/>question like “What is the probability of rolling a two, given that we rolled a one in the last trial?” To answer this, we would be discussing <em>joint</em> probability. Sometimes when we are studying the probability of two events, we know&#13;
the outcome of one but not the other. This is known as <em>conditional</em>&#13;
probability, and one way to calculate it is with <a data-primary="Bayes' rule" data-type="indexterm" id="idm46274547679688"/><a data-primary="probability" data-secondary="Bayes' rule in" data-type="indexterm" id="idm46274547678984"/>Bayes’ rule.</p>&#13;
&#13;
<p>We will not cover Bayes’ rule, and the many areas of probability and&#13;
statistics that apply it, in this book, but it’s well worth your future&#13;
study. Check out <a data-primary="probability" data-secondary="further reading on" data-type="indexterm" id="idm46274547677448"/>Will Kurt’s <em>Bayesian Statistics the Fun Way</em> (No Starch Press) for a fantastic introduction. You will see that&#13;
Bayesianism offers a unique approach to working with data with some&#13;
impressive applications for analytics.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The schools of thought developed around Bayes’ rule&#13;
diverge from the so-called frequentist approaches used in this book&#13;
and much of classical statistics.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Probability Distributions" data-type="sect1"><div class="sect1" id="idm46274547674328">&#13;
<h1>Probability Distributions</h1>&#13;
&#13;
<p>So far, we’ve learned what makes our die toss a random experiment, and&#13;
we’ve enumerated the sample space of what possible values a trial might&#13;
take. We know that the sum of the probabilities of each outcome must equal&#13;
1, but what is the <a data-primary="probability" data-secondary="probability distributions" data-type="indexterm" id="ch2_term2"/><a data-primary="probability" data-secondary="visualizations of distributions of" data-type="indexterm" id="ch2_term3"/><a data-primary="visualization of data" data-secondary="of probability distributions" data-secondary-sortas="probability distributions" data-type="indexterm" id="ch2_term4"/>relative probability for each outcome? For this we can refer to <em>probability distributions</em>. A probability distribution is a listing of what possible outcomes an event can take, and how common each outcome is. While a probability distribution could be written as a formal mathematical function, we will instead focus on&#13;
its quantitative output.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch01.html#foundations-of-eda">Chapter 1</a>, you learned about the difference between&#13;
discrete and continuous variables. There are also related discrete and continuous probability <em>distributions</em>. Let’s learn more, starting with the former.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Discrete Probability Distributions" data-type="sect2"><div class="sect2" id="idm46274547665784">&#13;
<h2>Discrete Probability Distributions</h2>&#13;
&#13;
<p>We’ll continue with our example of a die toss. This is <a data-primary="discrete uniform probability distributions" data-type="indexterm" id="ch2_term5"/><a data-primary="distributions" data-secondary="discrete probability" data-type="indexterm" id="ch2_term6"/><a data-primary="probability" data-secondary="discrete uniform distributions" data-type="indexterm" id="ch2_term9"/>considered a&#13;
<em>discrete</em> probability distribution because there are a countable number&#13;
of outcomes: for example, while a die toss can result in a 2 or a&#13;
3, it can never result in a 2.25.</p>&#13;
&#13;
<p>In particular, a die <a data-primary="discrete probability distributions" data-type="indexterm" id="idm46274547659000"/><a data-primary="uniform probability distribution" data-type="indexterm" id="idm46274547658248"/>toss is a <em>discrete uniform</em> probability&#13;
distribution because each outcome is equally likely for any trial: that&#13;
is, we are just as likely to roll a 4 as we are a 2, and so forth. To be more specific, we have a one-in-six probability for each outcome.</p>&#13;
&#13;
<p>To follow along with this and the other <a data-primary="Excel" data-secondary="visualizations with" data-type="indexterm" id="ch2_term8"/><a data-primary="visualization of data" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="ch2_term11"/><a data-primary="Excel" data-secondary="demos and worksheets for" data-type="indexterm" id="idm46274547653240"/><a data-primary="Excel" data-secondary="probability foundations with" data-type="indexterm" id="idm46274547652328"/><a data-primary="probability" data-secondary="Excel with" data-type="indexterm" id="idm46274547651368"/>Excel demos in this chapter, head to the <em>ch-2.xlsx</em> file in this book’s <a href="https://oreil.ly/1hlYj">repository</a>. For most of these exercises, I completed some staging of the worksheet already and will work through the remainder with you here. Start with the <em>uniform-distribution</em> worksheet. Each possible outcome <em>X</em> is listed in the range <code>A2:A7</code>. We know that it’s equally likely to get any outcome, so our formula in <code>B2:B7</code> should be <code>=1/6</code>. <em>P(X=x)</em> indicates the probability that a given event will result in the listed outcome.</p>&#13;
&#13;
<p>Now, <a data-primary="Excel" data-secondary="Clustered Columns in" data-type="indexterm" id="idm46274547646232"/><a data-primary="Clustered Columns, Excel" data-type="indexterm" id="idm46274547645224"/><a data-primary="visualization of data" data-secondary="Clustered Columns, Excel" data-type="indexterm" id="idm46274547644536"/>select the range <code>A1:B7</code> and from the ribbon, go to Insert &gt;&#13;
Clustered Column. Your probability distribution and visualization&#13;
should look like <a data-type="xref" href="#die-toss-distribution">Figure 2-1</a>.</p>&#13;
&#13;
<p>Welcome to your first, if unexciting, probability distribution. Notice&#13;
the gaps between values in our visualization? This is a wise choice to&#13;
indicate that these are <em>discrete</em> and not continuous outcomes.</p>&#13;
&#13;
<p>Sometimes we may want to <a data-primary="cumulative probability distributions" data-type="indexterm" id="idm46274547640312"/><a data-primary="probability" data-secondary="cumulative distributions" data-type="indexterm" id="idm46274547639544"/>know the <em>cumulative</em> probability of an&#13;
outcome. In this case, we take a running total of all probabilities&#13;
until we reach 100% (because the sample space must sum to 1). We’ll find the probability of an event being <em>less than or equal to</em> a&#13;
given outcome in column <code>C</code>. We can set up a running total in the range <code>C2:C7</code> with the&#13;
formula <code>=SUM($B$2:B2)</code>.</p>&#13;
&#13;
<figure><div class="figure" id="die-toss-distribution">&#13;
<img alt="The probability distribution of a six-sided die toss" src="assets/aina_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>The probability distribution of a six-sided die toss</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now, select the range <code>A1:A7</code>, hold down the Ctrl key for Windows or Cmd for Mac, and highlight <code>C1:C7</code>. With this noncontiguous range selected, create a second clustered&#13;
column chart. Do you see the difference between a probability&#13;
distribution and a <em>cumulative</em> probability distribution in <a data-type="xref" href="#cumulative-probability">Figure 2-2</a>?</p>&#13;
&#13;
<figure><div class="figure" id="cumulative-probability">&#13;
<img alt="Probability versus cumulative distribution" src="assets/aina_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>The probability versus cumulative probability distribution of a six-sided &#13;
<span class="keep-together">die toss</span></h6>&#13;
</div></figure>&#13;
&#13;
<p>Based on logic and mathematical reasoning, we’ve been assuming a one-in-six probability of landing on any side of the die. This is <a data-primary="probability" data-secondary="theoretical" data-type="indexterm" id="idm46274547627736"/><a data-primary="theoretical probability" data-type="indexterm" id="idm46274547626760"/>called our&#13;
<em>theoretical probability</em>. We could also find the <a data-primary="experimental probability" data-type="indexterm" id="ch2_term13"/><a data-primary="probability" data-secondary="experimental" data-type="indexterm" id="ch2_term14"/>probability distribution empirically by rolling a&#13;
die many times and recording the results. This is called an&#13;
<em>experimental probability</em>. After all, we could find through experiments that the probability for each side of the die is really <em>not</em> one in six as theoretically reasoned, and that the die is biased toward a given side.</p>&#13;
&#13;
<p>We have a couple of options for deriving experimental probabilities: first, we could indeed conduct a <a data-primary="experiments in probability" data-type="indexterm" id="idm46274547621752"/><a data-primary="probability" data-secondary="experiments in" data-type="indexterm" id="idm46274547621032"/>real experiment. Of course, rolling a&#13;
die dozens of times and recording the results might get quite tedious.&#13;
Our alternative is to get the <a data-primary="simulations for experimental probability" data-type="indexterm" id="ch2_term12"/>computer to do the heavy lifting, and&#13;
<em>simulate</em> the experiment. Simulation often provides a decent&#13;
approximation of reality, and is frequently used when running&#13;
experiments in real life is too difficult or time-consuming. The downside of simulation is that it can fail to reflect any anomalies&#13;
or idiosyncrasies from the real-life experiment that it intends to represent.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Simulation is frequently used in analytics to glean what might happen in real life when finding out through actual experiments is too difficult or even impossible.</p>&#13;
</div>&#13;
&#13;
<p>To simulate the experiment of rolling a die, we need a <a data-primary="randomness in probability" data-type="indexterm" id="idm46274547616024"/><a data-primary="probability" data-secondary="randomness in" data-type="indexterm" id="idm46274547615304"/>way to&#13;
consistently choose a number between one and six at random. We can do this using <a data-primary="Excel" data-secondary="probability foundations with" data-type="indexterm" id="idm46274547614120"/><a data-primary="probability" data-secondary="Excel with" data-type="indexterm" id="idm46274547613160"/><a data-primary="RANDBETWEEN() function, Excel" data-type="indexterm" id="idm46274547612216"/><a data-primary="random number generator" data-type="indexterm" id="idm46274547611528"/>Excel’s random number generator, <code>RANDBETWEEN()</code>. The results you&#13;
see in the book will be different than what you get when you try it yourself…but they will <em>all</em> be random numbers between one and six.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Your individual&#13;
results using Excel’s random number generator will look different from what’s been recorded in the book.</p>&#13;
</div>&#13;
&#13;
<p>Now, go to the <em>experimental-probability</em> worksheet. In column <code>A</code>, we have labeled 100 die toss trials for which we’d like to record the outcome. At this point, you could start rolling a real die and recording the&#13;
results in column <code>B</code>. Your more efficient, if less realistic,&#13;
alternative is to simulate the results with <code>RANDBETWEEN()</code>.</p>&#13;
&#13;
<p>This function takes two arguments:</p>&#13;
<pre>RANDBETWEEN(bottom, top)</pre>&#13;
&#13;
<p>We are using a six-sided die, which makes our range go between one and six:</p>&#13;
<pre>RANDBETWEEN(1, 6)</pre>&#13;
&#13;
<p><code>RANDBETWEEN()</code> only returns whole numbers, which is what we want in&#13;
this case: again, this is a <em>discrete</em> distribution. Using the fill handle, you can generate an outcome for all 100 trials. Don’t get too <a data-primary="Calculate Now, Excel" data-type="indexterm" id="idm46274547602952"/>attached to your current outcomes: press F9 in Windows, fn-F9 for Mac, or from the ribbon select&#13;
Formulas → Calculate Now. This will recalculate your workbook, and&#13;
regenerate your random numbers.</p>&#13;
&#13;
<p>Let’s compare our <a data-primary="probability" data-secondary="theoretical" data-type="indexterm" id="idm46274547601528"/><a data-primary="theoretical probability" data-type="indexterm" id="idm46274547600168"/>theoretical versus experimental probabilities of a die toss in columns <code>D-F</code>. Column <code>D</code> will be used to enumerate our sample space: the numbers one through six. In column <code>E</code>, take the theoretical distribution: <code>1/6</code>, or <code>16.67%</code>. In column <code>F</code>, calculate the <a data-primary="experimental distribution formula" data-type="indexterm" id="idm46274547596632"/><a data-primary="Excel" data-secondary="experimental distribution formula" data-type="indexterm" id="idm46274547595832"/>experimental distribution from columns <code>A</code>&#13;
and <code>B</code>. This is the percentage of times we find each outcome across all trials. You can find this using the &#13;
<span class="keep-together">formula</span>:</p>&#13;
<pre>=COUNTIF($B$2:$B$101, D2)/COUNT($A$2:$A$101)</pre>&#13;
&#13;
<p>Select your range <code>D1:F7</code> and from the <a data-primary="Excel" data-secondary="Clustered Columns in" data-type="indexterm" id="idm46274547591992"/><a data-primary="Clustered Columns, Excel" data-type="indexterm" id="idm46274547590984"/><a data-primary="visualization of data" data-secondary="Clustered Columns, Excel" data-type="indexterm" id="idm46274547590296"/>ribbon go to Insert → Clustered Column. Your worksheet should now look like <a data-type="xref" href="#theoretical-experimental-probabilities">Figure 2-3</a>. Try recalculating it a couple&#13;
of times.</p>&#13;
&#13;
<figure><div class="figure" id="theoretical-experimental-probabilities">&#13;
<img alt="Theoretical versus experimental probabilities" src="assets/aina_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>Theoretical versus experimental probabilities of a six-sided die toss</h6>&#13;
</div></figure>&#13;
&#13;
<p>It looks like, based on our experimental distribution, we were right to&#13;
predict an equal likelihood of rolling any number. Of course, our&#13;
experimental distribution isn’t <em>perfectly</em> like the theoretical: there&#13;
will always be some error due to <a data-primary="probability" data-secondary="randomness in" data-type="indexterm" id="idm46274547585080"/><a data-primary="randomness in probability" data-type="indexterm" id="idm46274547584024"/>random chance.</p>&#13;
&#13;
<p>It could be the case, however, that were we to conduct the experiment in real life, the results would differ from what we derived from simulation. Perhaps the real-life die of interest is <em>not</em>&#13;
fair, and we’ve overlooked that by relying on our own reasoning and&#13;
Excel’s algorithm. It seems like a trivial point, but&#13;
often probabilities in real life don’t behave as we (or our computers)&#13;
expect <a data-startref="ch2_term12" data-type="indexterm" id="idm46274547581944"/>them to.</p>&#13;
&#13;
<p>The discrete uniform is one of many discrete <a data-startref="ch2_term5" data-type="indexterm" id="idm46274547580696"/><a data-startref="ch2_term6" data-type="indexterm" id="idm46274547579992"/><a data-startref="ch2_term8" data-type="indexterm" id="idm46274547579320"/><a data-startref="ch2_term9" data-type="indexterm" id="idm46274547578648"/><a data-startref="ch2_term11" data-type="indexterm" id="idm46274547577976"/>probability distributions; others commonly used in analytics include the binomial and Poisson distributions.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Continuous Probability Distributions" data-type="sect2"><div class="sect2" id="idm46274547664840">&#13;
<h2>Continuous Probability Distributions</h2>&#13;
&#13;
<p>A distribution is considered <a data-primary="continuous probability distributions" data-type="indexterm" id="ch2_term19"/><a data-primary="distributions" data-secondary="continuous probability" data-type="indexterm" id="ch2_term20"/><a data-primary="normal distributions" data-type="indexterm" id="ch2_term21"/><a data-primary="probability" data-secondary="continuous distributions" data-type="indexterm" id="ch2_term22"/><a data-primary="probability" data-secondary="normal distribution in" data-type="indexterm" id="ch2_term23"/>continuous when an outcome can take any possible&#13;
value between two other values. We will focus here on the normal&#13;
distribution, or the <em>bell curve</em>, as depicted with a <a data-primary="histograms" data-secondary="of normal probability distributions" data-secondary-sortas="normal probability distributions" data-type="indexterm" id="ch2_term15"/><a data-primary="normal distributions" data-secondary="visualizations of" data-type="indexterm" id="ch2_term16"/><a data-primary="probability" data-secondary="visualizations of distributions of" data-type="indexterm" id="ch2_term17"/><a data-primary="visualization of data" data-secondary="of probability distributions" data-secondary-sortas="probability distributions" data-type="indexterm" id="ch2_term18"/>histogram. You may be familiar with&#13;
this famous shape, seen in <a data-type="xref" href="#normal-distribution">Figure 2-4</a>.</p>&#13;
&#13;
<p>You’ll see in this chart a perfectly symmetrical distribution centered&#13;
around the <a data-primary="mean" data-secondary="in probability" data-secondary-sortas="probability" data-type="indexterm" id="idm46274547561448"/><a data-primary="probability" data-secondary="mean in" data-type="indexterm" id="idm46274547560200"/>variable’s mean (μ). Let’s dig in on what the normal distribution is and what it&#13;
tells us, using Excel to illustrate the fundamental statistical concepts that are based on it.</p>&#13;
&#13;
<figure><div class="figure" id="normal-distribution">&#13;
<img alt="Normal distribution" src="assets/aina_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>The normal distribution depicted with a histogram</h6>&#13;
</div></figure>&#13;
&#13;
<p>The normal distribution is worth reviewing in part because it is so&#13;
common in the natural world. For example, <a data-type="xref" href="#real-normal-distributions">Figure 2-5</a> shows&#13;
histograms of the distribution of student heights and wine pH levels,&#13;
respectively. These datasets are available in the book’s <a href="https://oreil.ly/1hlYj">repository</a> for you to&#13;
explore in the <em>datasets</em> folder under <em>heights</em> and <em>wine</em>, respectively.</p>&#13;
&#13;
<figure><div class="figure" id="real-normal-distributions">&#13;
<img alt="Normal variables" src="assets/aina_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>Two normally distributed variables from real life: student heights and wine pH levels</h6>&#13;
</div></figure>&#13;
&#13;
<p>You may be wondering how we know when a variable is normally&#13;
distributed. Good question. Think back to our die toss example: we enumerated all possible outcomes, derived a <a data-primary="probability" data-secondary="theoretical" data-type="indexterm" id="idm46274547550440"/><a data-primary="theoretical probability" data-type="indexterm" id="idm46274547549464"/>theoretical distribution, then derived an&#13;
experimental distribution (via simulation) to compare the two. Consider the histograms in <a data-type="xref" href="#real-normal-distributions">Figure 2-5</a> as <em>experimental distributions</em> of&#13;
their own: in this case, the data was <a data-startref="ch2_term15" data-type="indexterm" id="idm46274547547272"/><a data-startref="ch2_term16" data-type="indexterm" id="idm46274547546600"/><a data-startref="ch2_term17" data-type="indexterm" id="idm46274547545928"/><a data-startref="ch2_term18" data-type="indexterm" id="idm46274547545256"/>collected manually rather than&#13;
relying on a simulation.</p>&#13;
&#13;
<p>There are several ways to determine whether a real-life dataset with its experimental distribution is close enough to a theoretical normal distribution. For now, we’ll look out for the telltale bell curve histogram: a&#13;
symmetrical shape, with the majority of values found near the center. Other ways include evaluating skewness and kurtosis, which are two additional summary statistics measuring the distribution’s symmetry and peakedness, respectively. It’s also possible to test for normality using methods of statistical inference. You’ll learn the basics of statistical inference in <a data-type="xref" href="ch03.html#foundations-of-inference">Chapter 3</a>. But for now, we’ll go by the rule: “You know it&#13;
when you see it.”</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>When you are dealing with real-life data, you&#13;
are dealing with <em>experimental</em> distributions. They will never perfectly&#13;
match theoretical distributions.</p>&#13;
</div>&#13;
&#13;
<p>The normal <a data-startref="ch2_term13" data-type="indexterm" id="idm46274547540472"/><a data-startref="ch2_term14" data-type="indexterm" id="idm46274547539736"/>distribution offers some easy-to-remember guidelines for what&#13;
percentage of observations we expect to find within a given number of&#13;
standard deviations of the mean. Specifically, for a normally distributed variable we <a data-primary="mean" data-secondary="in probability" data-secondary-sortas="probability" data-type="indexterm" id="idm46274547538696"/><a data-primary="probability" data-secondary="mean in" data-type="indexterm" id="idm46274547537480"/><a data-primary="descriptive statistics" data-secondary="for standard deviations" data-secondary-sortas="standard deviations" data-type="indexterm" id="ch2_term24"/><a data-primary="probability" data-secondary="standard deviations in" data-type="indexterm" id="ch2_term25"/><a data-primary="standard deviations" data-type="indexterm" id="ch2_term26"/>expect:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>68% of observations to fall within one standard deviation of the mean</p>&#13;
</li>&#13;
<li>&#13;
<p>95% of observations to fall within two standard deviations of the&#13;
mean</p>&#13;
</li>&#13;
<li>&#13;
<p>99.7% of observations to fall within three standard deviations of the&#13;
mean</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This <a data-primary="empirical (68–95–99.7) rule in probability" data-type="indexterm" id="ch2_term27"/><a data-primary="Excel" data-secondary="empirical rule, calculating" data-type="indexterm" id="ch2_term28"/><a data-primary="normal distributions" data-secondary="empirical rule for" data-type="indexterm" id="ch2_term29"/><a data-primary="probability" data-secondary="empirical (68–95–99.7) rule in" data-type="indexterm" id="ch2_term32"/>is known as the <em>empirical rule</em>, or the <em>68–95–99.7&#13;
rule</em>. Let’s see it in action using <a data-primary="Excel" data-secondary="visualizations with" data-type="indexterm" id="idm46274547522936"/><a data-primary="visualization of data" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274547521960"/><a data-primary="Excel" data-secondary="probability foundations with" data-type="indexterm" id="ch2_term33"/><a data-primary="probability" data-secondary="Excel with" data-type="indexterm" id="ch2_term34"/>Excel. Next, go <a data-primary="Excel" data-secondary="demos and worksheets for" data-type="indexterm" id="ch2_term31"/>to the <em>empirical-rule</em> worksheet, as shown in <a data-type="xref" href="#empirical-rule-start">Figure 2-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="empirical-rule-start">&#13;
<img alt="The empirical rule demonstrated in Excel" src="assets/aina_0206.png"/>&#13;
<h6><span class="label">Figure 2-6. </span>The start of the <em>empirical-rule</em> worksheet</h6>&#13;
</div></figure>&#13;
&#13;
<p>In cells <code>A10:A109</code> we have the values 1–100. Our objective in cells <code>B10:B109</code> is to find what percentage of observations will take on these values for a normally distributed variable with a mean of 50 and a standard deviation of 10 (cells <code>B1</code> and <code>B2</code>, respectively). We will then find what percentage of observations fall within one, two, and three standard deviations of the mean in <code>C10:E109</code>. Once we do so, the charts to the right will be populated. Cells <code>C4:E4</code> will also find the total percentages for each &#13;
<span class="keep-together">column</span>.</p>&#13;
&#13;
<p>The normal distribution is continuous, which means that observations can&#13;
theoretically take on any value between two other values. This makes for&#13;
a <em>lot</em> of outcomes to assign a probability to. For simplicity, it’s common to bin these observations into discrete&#13;
ranges. The <a data-primary="PMF (probability mass function)" data-type="indexterm" id="ch2_term37"/><a data-primary="probability mass function (PMF)" data-type="indexterm" id="ch2_term38"/>probability mass function (PMF) will return the probabilities found for each discrete bin in the range of observations. We’ll use <a data-primary="NORM.DIST() function, Excel" data-type="indexterm" id="idm46274547505400"/>Excel’s <code>NORM.DIST()</code> function to calculate a PMF for our variable in the range 1–100. This function is more involved than others used so far, so I’ve&#13;
described each <a data-primary="mean" data-secondary="in probability" data-secondary-sortas="probability" data-type="indexterm" id="ch2_term35"/><a data-primary="probability" data-secondary="mean in" data-type="indexterm" id="ch2_term36"/>argument in <a data-type="xref" href="#norm-dist">Table 2-1</a>.</p>&#13;
<table id="norm-dist" style="width: 100%">&#13;
<caption><span class="label">Table 2-1. </span>The arguments needed for <code>NORM.DIST()</code></caption>&#13;
<thead>&#13;
<tr>&#13;
<th><strong>Argument</strong></th>&#13;
<th><strong>Description</strong></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p><code>X</code></p></td>&#13;
<td><p>The outcome for which you want to find the probability</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>Mean</code></p></td>&#13;
<td><p>The mean of the distribution</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>Standard_dev</code></p></td>&#13;
<td><p>The standard deviation of the distribution</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><code>Cumulative</code></p></td>&#13;
<td><p>If <code>TRUE</code>, a cumulative function is returned; if&#13;
<code>FALSE</code>, the mass function is returned</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Column <code>A</code> of our worksheet contains our outcomes, <code>B1</code> and <code>B2</code> contain our mean and&#13;
standard deviation, respectively, and we want a mass <a data-primary="probability" data-secondary="cumulative distributions" data-type="indexterm" id="idm46274547485880"/><a data-primary="cumulative probability distributions" data-type="indexterm" id="idm46274547484888"/>instead of a&#13;
cumulative distribution. Cumulative would return a running sum of the probability, which we don’t want here. That makes our formula for <code>B10</code>:</p>&#13;
<pre>=NORM.DIST(A10, $B$1, $B$2, 0)</pre>&#13;
&#13;
<p>Using the fill handle, you’ll get the percentage likelihood of an&#13;
observation taking on each value from 0 to 100. For example, you’ll see&#13;
in cell <code>B43</code> that there is approximately a 1.1% chance of an&#13;
observation being equal to 34.</p>&#13;
&#13;
<p>We can see in cell <code>B4</code> that an outcome is likely to be between 1 and&#13;
100 over 99.99% of the time. Importantly, this number is not equal to&#13;
100%, because an observation in a continuous distribution can take on&#13;
<em>any</em> possible value—not just those from 1 to 100. In cells <code>C7:E8</code> I’ve written formulas to find the range of values&#13;
within one, two and three standard deviations of our mean.</p>&#13;
&#13;
<p>We can use these thresholds along with conditional logic to find what&#13;
parts of our probability mass function in column <code>B</code> can be found within&#13;
these respective regions. In cell <code>C10</code> enter the following formula:</p>&#13;
<pre>=IF(AND($A10 &gt; C$7, $A10 &lt; C$8), $B10, "")</pre>&#13;
&#13;
<p>This function will carry the probability over from column <code>B</code> if the&#13;
value of column <code>A</code> falls within the standard deviation range. If it&#13;
falls outside the range, the cell is left blank. Using the fill handle, you can apply this formula to the entire range&#13;
<code>C10:E109</code>. Your worksheet <a data-primary="Excel" data-secondary="visualizations with" data-type="indexterm" id="ch2_term43"/><a data-primary="visualization of data" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="ch2_term44"/>should now look like <a data-type="xref" href="#empirical-rule-excel">Figure 2-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="empirical-rule-excel">&#13;
<img alt="The empirical rule demonstrated in Excel" src="assets/aina_0207.png"/>&#13;
<h6><span class="label">Figure 2-7. </span>The empirical rule demonstrated in Excel</h6>&#13;
</div></figure>&#13;
&#13;
<p>Cells <code>C4:E4</code> indicate that we find approximately 65.8%, 94.9%, and 99.7%&#13;
of values within one, two, and three standard deviations of the mean, respectively.&#13;
These numbers are very close to matching the 68–95–99.7 rule.</p>&#13;
&#13;
<p>Now, take a look at the <a data-primary="histograms" data-secondary="of normal probability distributions" data-secondary-sortas="normal probability distributions" data-type="indexterm" id="ch2_term39"/><a data-primary="normal distributions" data-secondary="visualizations of" data-type="indexterm" id="ch2_term40"/><a data-primary="probability" data-secondary="visualizations of distributions of" data-type="indexterm" id="ch2_term41"/><a data-primary="visualization of data" data-secondary="of probability distributions" data-secondary-sortas="probability distributions" data-type="indexterm" id="ch2_term42"/>resulting visualizations: we see that a&#13;
sizable majority of observations can be found within one standard&#13;
deviation, and more still within two. By three standard deviations, it’s&#13;
hard to see the part of the chart in <a data-type="xref" href="#empirical-rule-visualized">Figure 2-8</a> that is <em>not</em> covered, but it’s still&#13;
there. (Remember, this is only 0.3% of all observations.)</p>&#13;
&#13;
<figure><div class="figure" id="empirical-rule-visualized">&#13;
<img alt="Empirical rule charts" src="assets/aina_0208.png"/>&#13;
<h6><span class="label">Figure 2-8. </span>The empirical rule visualized in Excel</h6>&#13;
</div></figure>&#13;
&#13;
<p>What happens when you change the standard deviation of our example to&#13;
eight? To 12? The bell curve shape remains symmetrically centered around the mean of 50 but contracts and expands: a&#13;
lower standard deviation leads to a “tighter” curve and vice versa. In&#13;
any case, the empirical rule roughly applies to the data. If you shift&#13;
the mean to 49 or 51, you see the “center” of the <a data-primary="x-axis, mapping" data-type="indexterm" id="idm46274547458888"/>curve move along the&#13;
x-axis. A variable can have any mean and standard deviation and still be&#13;
normally distributed; its resulting probability mass function <a data-startref="ch2_term37" data-type="indexterm" id="idm46274547457880"/><a data-startref="ch2_term38" data-type="indexterm" id="idm46274547457208"/>will be&#13;
different.</p>&#13;
&#13;
<p><a data-type="xref" href="#different-normal-distributions">Figure 2-9</a> shows two normal distributions with different means and standard deviations. Despite their very different shapes, they both still follow the empirical rule.</p>&#13;
&#13;
<figure><div class="figure" id="different-normal-distributions">&#13;
<img alt="Different normal distributions" src="assets/aina_0209.png"/>&#13;
<h6><span class="label">Figure 2-9. </span>Different normal distributions</h6>&#13;
</div></figure>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>A normal distribution can have any possible&#13;
combination of mean and standard deviation. The resulting probability&#13;
density function will change, but will roughly follow the empirical&#13;
rule.</p>&#13;
</div>&#13;
&#13;
<p>The normal <a data-startref="ch2_term24" data-type="indexterm" id="idm46274547451096"/><a data-startref="ch2_term25" data-type="indexterm" id="idm46274547450360"/><a data-startref="ch2_term26" data-type="indexterm" id="idm46274547449688"/><a data-startref="ch2_term27" data-type="indexterm" id="idm46274547449016"/><a data-startref="ch2_term28" data-type="indexterm" id="idm46274547448344"/><a data-startref="ch2_term29" data-type="indexterm" id="idm46274547447672"/><a data-startref="ch2_term31" data-type="indexterm" id="idm46274547447000"/><a data-startref="ch2_term32" data-type="indexterm" id="idm46274547446328"/><a data-startref="ch2_term35" data-type="indexterm" id="idm46274547445656"/><a data-startref="ch2_term36" data-type="indexterm" id="idm46274547444984"/>distribution is also important because of its place in the <a data-primary="central limit theorem (CLT)" data-type="indexterm" id="ch2_term46"/><a data-primary="CLT (central limit theorem)" data-type="indexterm" id="ch2_term47"/><a data-primary="normal distributions" data-secondary="in central limit theorem (CLT)" data-secondary-sortas="central limit theorem (CLT)" data-type="indexterm" id="ch2_term48"/><a data-primary="probability" data-secondary="central limit theorem (CLT) in" data-type="indexterm" id="ch2_term49"/>central limit theorem. I call this theorem the “missing link of&#13;
statistics” for reasons you’ll see in this and following chapters.</p>&#13;
&#13;
<p>For an example of the central limit theorem, we will use another common game of chance: roulette. A European&#13;
roulette wheel is equally likely to return any number between 0 and 36 (compared to the American wheel, which has slots labeled both 0 and 00). Based&#13;
on what you’ve learned about die tosses, what kind of probability&#13;
distribution is this? It’s a <a data-primary="probability" data-secondary="discrete uniform distributions" data-type="indexterm" id="idm46274547438296"/><a data-primary="discrete uniform probability distributions" data-type="indexterm" id="idm46274547437352"/>discrete uniform. Does it seem odd that we’re analyzing this distribution in a demo I said was about the normal distribution? Well, we have the central limit&#13;
theorem to thank here. To see this theorem in action for <a data-primary="RANDBETWEEN() function, Excel" data-type="indexterm" id="ch2_term45"/>yourself, go to the <em>roulette-dist</em> worksheet and simulate 100 spins of a roulette wheel in <code>B2:B101</code> using <code>RANDBETWEEN()</code>:</p>&#13;
<pre>RANDBETWEEN(0, 36)</pre>&#13;
&#13;
<p>Visualize the result using a histogram. Your worksheet should look like <a data-type="xref" href="#distribution-roulette-spins">Figure 2-10</a>. Try recalculating a few times. You will see that each&#13;
time you get a rather flat-looking histogram. This is indeed a discrete&#13;
uniform distribution, where it’s equally likely to land on any number&#13;
between 0 and 36.</p>&#13;
&#13;
<figure><div class="figure" id="distribution-roulette-spins">&#13;
<img alt="Histogram of roulette spins" src="assets/aina_0210.png"/>&#13;
<h6><span class="label">Figure 2-10. </span>The distribution of simulated roulette spins</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now go <a data-primary="Excel" data-secondary="demos and worksheets for" data-type="indexterm" id="idm46274547429624"/>to the <em>roulette-sample-mean-dist</em> worksheet. Here we will be doing&#13;
something a little different: we’ll simulate 100 spins, then take the&#13;
average of those spins. We’ll do this 100 times and plot the&#13;
distribution of these trial averages, again as a <a data-primary="average of averages (sample mean)" data-type="indexterm" id="ch2_term50"/><a data-primary="mean" data-secondary="in probability" data-secondary-sortas="probability" data-type="indexterm" id="ch2_term51"/><a data-primary="mean" data-secondary="sample" data-type="indexterm" id="ch2_term52"/><a data-primary="probability" data-secondary="mean in" data-type="indexterm" id="ch2_term53"/><a data-primary="probability" data-secondary="sample mean in" data-type="indexterm" id="ch2_term54"/><a data-primary="sample mean" data-type="indexterm" id="ch2_term55"/>histogram. This “average of averages” is known as a <em>sample mean</em>. Once you’ve done that <a data-primary="AVERAGE() function, Excel" data-type="indexterm" id="idm46274547419800"/>using the <code>RANDBETWEEN()</code> and <code>AVERAGE()</code>&#13;
functions, you should see something like <a data-type="xref" href="#distribution-sample-means">Figure 2-11</a>.</p>&#13;
&#13;
<figure><div class="figure" id="distribution-sample-means">&#13;
<img alt="Histogram of sample means roulette spins" src="assets/aina_0211.png"/>&#13;
<h6><span class="label">Figure 2-11. </span>The distribution of sample means of simulated roulette spins</h6>&#13;
</div></figure>&#13;
&#13;
<p>This distribution no longer looks like a rectangle: in fact, it looks like a bell curve. It’s symmetrical, and most observations are&#13;
clustered around the center: we now have a normal distribution. How <a data-startref="ch2_term39" data-type="indexterm" id="idm46274547414664"/><a data-startref="ch2_term40" data-type="indexterm" id="idm46274547413960"/><a data-startref="ch2_term41" data-type="indexterm" id="idm46274547413288"/><a data-startref="ch2_term42" data-type="indexterm" id="idm46274547412616"/>could our distribution of sample means be normally distributed, when the roulette spins themselves are not? Welcome to the very special kind of magic known as the <em>central limit&#13;
theorem</em> (CLT).</p>&#13;
&#13;
<p>Put formally, the CLT tells us that:</p>&#13;
<blockquote>&#13;
<p>The distribution of sample means will be normal or nearly normal if the sample size is large enough.</p></blockquote>&#13;
&#13;
<p>This phenomenon is a game changer, because it allows us to use the&#13;
unique traits of the normal distribution (such as the empirical rule) to&#13;
make claims about the sample means of a variable, even when that&#13;
variable itself isn’t normally distributed.</p>&#13;
&#13;
<p>Did you catch the fine print? The CLT only applies <em>when the <a data-primary="probability" data-secondary="sample size in" data-type="indexterm" id="ch2_term56"/><a data-primary="sample size" data-secondary="effects of" data-type="indexterm" id="ch2_term57"/>sample size&#13;
is large enough</em>. It’s an important disclaimer, but also an ambiguous one:&#13;
how large is large enough? Let’s gather some ideas with another <a data-primary="Excel" data-secondary="demos and worksheets for" data-type="indexterm" id="idm46274547405480"/><a data-primary="Excel" data-secondary="law of large numbers with" data-type="indexterm" id="idm46274547404504"/><a data-primary="law of large numbers (LLN)" data-type="indexterm" id="idm46274547403544"/><a data-primary="LLN (law of large numbers)" data-type="indexterm" id="idm46274547402856"/><a data-primary="normal distributions" data-secondary="law of large numbers (LLN) in" data-type="indexterm" id="idm46274547402168"/><a data-primary="probability" data-secondary="law of large numbers (LLN) in" data-type="indexterm" id="idm46274547401208"/>Excel demo. Head to the <em>law-of-large-numbers</em> worksheet to follow&#13;
along. In column <code>B</code>, we can simulate the outcomes of 300 trials of a roulette&#13;
spin using <code>RANDBETWEEN(0, 36)</code>.</p>&#13;
&#13;
<p>In column <code>C</code>, we want to take a running average of the outcome. We can&#13;
do so using mixed references; in column <code>C</code> enter the following and drag it alongside your 300 &#13;
<span class="keep-together">trials</span>:</p>&#13;
<pre>=AVERAGE($B$2:B2)</pre>&#13;
&#13;
<p>This will result in finding the running average of column <code>B</code>. Select your resulting data in column <code>C</code> and then head to the ribbon and <a data-primary="Line chart, Excel" data-type="indexterm" id="idm46274547394696"/><a data-primary="normal distributions" data-secondary="visualizations of" data-type="indexterm" id="idm46274547393992"/><a data-primary="probability" data-secondary="visualizations of distributions of" data-type="indexterm" id="idm46274547393048"/><a data-primary="visualization of data" data-secondary="line chart" data-type="indexterm" id="idm46274547392040"/><a data-primary="visualization of data" data-secondary="of probability distributions" data-secondary-sortas="probability distributions" data-type="indexterm" id="idm46274547391096"/>click Insert → Line. Take a look at your line chart and recalculate the workbook a few times. Each resulting simulation will be different than what’s shown in <a data-type="xref" href="#lln-excel">Figure 2-12</a>, but as a pattern the average&#13;
tends to converge to 18 with more spins, which makes sense: it’s the average between 0 and 36. This anticipated figure is known as the&#13;
<em>expected value</em>.</p>&#13;
&#13;
<figure><div class="figure" id="lln-excel">&#13;
<img alt="The law of large numbers" src="assets/aina_0212.png"/>&#13;
<h6><span class="label">Figure 2-12. </span>The law of large numbers, visualized in Excel</h6>&#13;
</div></figure>&#13;
&#13;
<p>This phenomenon is <a data-startref="ch2_term33" data-type="indexterm" id="idm46274547385848"/><a data-startref="ch2_term34" data-type="indexterm" id="idm46274547385112"/><a data-startref="ch2_term43" data-type="indexterm" id="idm46274547384440"/><a data-startref="ch2_term44" data-type="indexterm" id="idm46274547383768"/><a data-startref="ch2_term45" data-type="indexterm" id="idm46274547383096"/>known as the <em>law of large numbers</em> (LLN). Stated&#13;
formally, the LLN tells us:</p>&#13;
<blockquote>&#13;
<p>The average of results obtained from trials become closer to the&#13;
expected value as more trials are performed.</p></blockquote>&#13;
&#13;
<p>This definition, though, begs the question we first asked: how large&#13;
does a sample size need to be for the CLT to apply? You’ll often hear 30 given as a threshold. More conservative guidelines&#13;
may call for a sample size of 60 or 100. Given these sample size guidelines, look back&#13;
to <a data-type="xref" href="#lln-excel">Figure 2-12</a>. See how it indeed settles more closely&#13;
to the expected value at around these thresholds?</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The law of large numbers provides a loose rule of thumb for&#13;
an adequate sample size that meets the CLT.</p>&#13;
</div>&#13;
&#13;
<p>Sample sizes of 30, 60, and 100 are purely rules of thumb; there are more&#13;
rigorous ways to determine what sample sizes are needed for the CLT to apply. For now, remember this: given that our sample size meets these thresholds, our&#13;
sample mean should be close to the expected value (thanks to the LLN),&#13;
and should also be normally <a data-startref="ch2_term46" data-type="indexterm" id="idm46274547377112"/><a data-startref="ch2_term47" data-type="indexterm" id="idm46274547376408"/><a data-startref="ch2_term48" data-type="indexterm" id="idm46274547375736"/><a data-startref="ch2_term49" data-type="indexterm" id="idm46274547375064"/><a data-startref="ch2_term50" data-type="indexterm" id="idm46274547374392"/><a data-startref="ch2_term51" data-type="indexterm" id="idm46274547373720"/><a data-startref="ch2_term52" data-type="indexterm" id="idm46274547373048"/><a data-startref="ch2_term53" data-type="indexterm" id="idm46274547372376"/><a data-startref="ch2_term54" data-type="indexterm" id="idm46274547371704"/><a data-startref="ch2_term55" data-type="indexterm" id="idm46274547371032"/><a data-startref="ch2_term56" data-type="indexterm" id="idm46274547370360"/><a data-startref="ch2_term57" data-type="indexterm" id="idm46274547369688"/>distributed (thanks to the CLT).</p>&#13;
&#13;
<p>Several continuous probability distributions exist, such as the&#13;
exponential and triangular. We focused on the <a data-startref="ch2_term2" data-type="indexterm" id="idm46274547368504"/><a data-startref="ch2_term3" data-type="indexterm" id="idm46274547367800"/><a data-startref="ch2_term4" data-type="indexterm" id="idm46274547367128"/><a data-startref="ch2_term19" data-type="indexterm" id="idm46274547366456"/><a data-startref="ch2_term20" data-type="indexterm" id="idm46274547365784"/><a data-startref="ch2_term21" data-type="indexterm" id="idm46274547365112"/><a data-startref="ch2_term22" data-type="indexterm" id="idm46274547364440"/><a data-startref="ch2_term23" data-type="indexterm" id="idm46274547363768"/>normal distribution both because of its&#13;
ubiquity in the real world and because of its special statistical&#13;
properties.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm46274547575496">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>As mentioned at the beginning of this chapter, data analysts live in&#13;
a world of uncertainty. Specifically, we often want to make claims about&#13;
an entire population, while only possessing a sample’s worth of data. Using the framework of probability covered in this chapter, we will be&#13;
able to do this while also quantifying its inherent uncertainty. In <a data-type="xref" href="ch03.html#foundations-of-inference">Chapter 3</a>, we’ll dig into the elements of hypothesis testing,&#13;
a core method of data <a data-startref="ch2_term1" data-type="indexterm" id="idm46274547361352"/>analytics.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Exercises" data-type="sect1"><div class="sect1" id="idm46274547360296">&#13;
<h1>Exercises</h1>&#13;
&#13;
<p>Using <a data-primary="probability" data-secondary="exercises" data-type="indexterm" id="idm46274547358936"/>Excel and your knowledge of probability, consider the following:</p>&#13;
<ol>&#13;
<li>&#13;
<p>What is the expected value of a six-sided die toss?</p>&#13;
</li>&#13;
<li>&#13;
<p>Consider a variable that is normally distributed with a mean of 100&#13;
and standard deviation of 10.</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>What is the probability that an observation from this variable would&#13;
take the value 87?</p>&#13;
</li>&#13;
<li>&#13;
<p>What percentage of observations would you expect to find between 80 and&#13;
120?</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>If the expected value of a European roulette spin is 18, does that&#13;
mean you are better off betting on 18 than other numbers?</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>