- en: Chapter 13\. Working with Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章. 处理文本
- en: 'Data can reside not just as numbers but also in words: names of dog breeds,
    restaurant violation descriptions, street addresses, speeches, blog posts, internet
    reviews, and much more. To organize and analyze information contained in text,
    we often need to do some of the following tasks:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不仅可以以数字形式存在，还可以以文字形式存在：狗品种的名称、餐馆违规描述、街道地址、演讲、博客文章、网评等等。为了组织和分析文本中包含的信息，我们经常需要执行以下一些任务：
- en: Convert text into a standard format
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换为标准格式
- en: This is also referred to as *canonicalizing text*. For example, we might need
    to convert characters to lowercase, use common spellings and abbreviations, or
    remove punctuation and blank spaces.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为*规范化文本*。例如，我们可能需要将字符转换为小写，使用常见拼写和缩写，或删除标点和空格。
- en: Extract a piece of text to create a feature
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 提取文本片段以创建特征
- en: As an example, a string might contain a date embedded in it, and we want to
    pull it out from the string to create a date feature.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，一个字符串可能包含嵌入其中的日期，我们希望从字符串中提取出来以创建一个日期特征。
- en: Transform text into features
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换为特征
- en: We might want to encode particular words or phrases as 0-1 features to indicate
    their presence in a string.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能希望将特定词语或短语编码为0-1特征，以指示它们在字符串中的存在。
- en: Analyze text
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分析文本
- en: In order to compare entire documents at once, we can transform a document into
    a vector of word counts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了一次性比较整个文档，我们可以将文档转换为单词计数的向量。
- en: This chapter introduces common techniques for working with text data. We show
    how simple string manipulation tools are often all we need to put text in a standard
    form or extract portions of strings. We also introduce regular expressions for
    more general and robust pattern matching. To demonstrate these text operations
    we use several examples. We first introduce these examples and describe the work
    we want to do to prepare the text for analysis.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章介绍了处理文本数据的常用技术。我们展示了简单的字符串操作工具通常足以将文本整理成标准形式或提取字符串的部分内容。我们还介绍了正则表达式，用于更通用和稳健的模式匹配。为了演示这些文本操作，我们使用了几个例子。我们首先介绍这些例子，并描述我们想要为分析准备文本的工作。
- en: Examples of Text and Tasks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本和任务示例
- en: For each type of task just introduced, we provide a motivating example. These
    examples are based on real tasks that we have carried out, but to focus on the
    concept, we’ve reduced the data to snippets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于刚刚介绍的每种任务，我们提供一个激励性的例子。这些例子基于我们实际完成的任务，但为了专注于概念，我们已经将数据简化为片段。
- en: Convert Text into a Standard Format
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本转换为标准格式
- en: 'Let’s say we want to study connections between population demographics and
    election results. To do this, we’ve taken election data from Wikipedia and population
    data from the US Census Bureau. The granularity of the data is at the county level,
    and we need to use the county names to join the tables. Unfortunately, the county
    names in these two tables don’t always match:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要研究人口统计数据与选举结果之间的联系。为此，我们从维基百科获取了选举数据，从美国人口普查局获取了人口数据。数据的粒度是以县为单位的，我们需要使用县名来连接这两个表格。不幸的是，这两个表格中的县名并不总是匹配的：
- en: '|   | County | State | Voted |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|   | 县 | 州 | 投票数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **0** | De Witt County | IL | 97.8 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 德维特县 | 伊利诺伊州 | 97.8 |'
- en: '| **1** | Lac qui Parle County | MN | 98.8 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 拉克奎帕尔县 | 明尼苏达州 | 98.8 |'
- en: '| **2** | Lewis and Clark County | MT | 95.2 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 列威斯和克拉克县 | 蒙大拿州 | 95.2 |'
- en: '| **3** | St John the Baptist Parish | LA | 52.6 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 圣约翰大洗礼者教区 | 路易斯安那州 | 52.6 |'
- en: '|   | County | State | Population |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|   | 县 | 州 | 人口 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **0** | DeWitt | IL | 16,798 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 德威特 | 伊利诺伊州 | 16,798 |'
- en: '| **1** | Lac Qui Parle | MN | 8,067 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 拉克奎帕尔 | 明尼苏达州 | 8,067 |'
- en: '| **2** | Lewis & Clark | MT | 55,716 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 列威斯和克拉克 | 蒙大拿州 | 55,716 |'
- en: '| **3** | St. John the Baptist | LA | 43,044 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 圣约翰大洗礼者 | 路易斯安那州 | 43,044 |'
- en: We can’t join the tables until we clean the strings to have a common format
    for county names. We need to change the case of characters, use common spellings
    and abbreviations, and address punctuation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法将表格连接起来，直到我们将字符串清理为县名的共同格式为止。我们需要更改字符的大小写，使用常见的拼写和缩写，并处理标点符号。
- en: Extract a Piece of Text to Create a Feature
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取文本片段以创建特征
- en: 'Text data  sometimes has a lot of structure, especially when it was generated
    by a computer. As an example, the following is a web server’s log entry. Notice
    how the entry has multiple pieces of data, but the pieces don’t have a consistent
    delimiter—for instance, the date appears in square brackets, but other parts of
    the data appear in quotes and parentheses:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据有时具有很多结构，特别是当它是由计算机生成时。例如，以下是一个 Web 服务器的日志条目。注意条目中有多个数据片段，但这些片段没有一致的分隔符——例如，日期出现在方括号中，但数据的其他部分出现在引号和括号中：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Even though the file format doesn’t align with one of the simple formats we
    saw in [Chapter 8](ch08.html#ch-files), we can use text processing techniques
    to extract pieces of text to create features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文件格式与我们在 [第 8 章](ch08.html#ch-files) 中看到的简单格式之一不符，但我们可以使用文本处理技术提取文本片段以创建特征。
- en: Transform Text into Features
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本转换为特征
- en: 'In [Chapter 9](ch09.html#ch-wrangling), we created a categorical feature based
    on the content of the strings. There, we examined the descriptions of restaurant
    violations and we created nominal variables for the presence of particular words.
    We’ve displayed a few example violations here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 9 章](ch09.html#ch-wrangling) 中，我们基于字符串内容创建了一个分类特征。在那里，我们检查了餐厅违规描述，并为特定词语的存在创建了名义变量。这里展示了一些示例违规行为：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These new features can be used in an analysis of food safety scores. Previously,
    we made simple features that marked whether a description contained a word like
    *glove* or *hair*. In this chapter, we more formally introduce the regular expression
    tools that we used to create these features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新功能可以用于食品安全评分的分析。以前，我们制作了简单的特征，标记了描述中是否包含诸如 *手套* 或 *头发* 这样的词语。在本章中，我们更正式地介绍了我们用来创建这些特征的正则表达式工具。
- en: Text Analysis
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本分析
- en: 'Sometimes we want to compare entire documents. For example, the US president
    gives a State of the Union speech every year. Here are the first few lines of
    the very first speech:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们想比较整个文档。例如，美国总统每年都会发表国情咨文演讲。以下是第一次演讲的前几行：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We might wonder: How have the State of the Union speeches changed over time?
    Do different political parties focus on different topics or use different language
    in their speeches? To answer these questions, we can transform the speeches into
    a numeric form that lets us use statistics to compare them.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会想：国情咨文演讲随时间如何变化？不同政党是否专注于不同的主题或在演讲中使用不同的语言？为了回答这些问题，我们可以将演讲转换为数字形式，以便使用统计方法进行比较。
- en: These examples serve to illustrate the ideas of string manipulation, regular
    expressions, and text analysis. We start with describing simple string manipulation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例用来说明字符串操作、正则表达式和文本分析的思想。我们从描述简单的字符串操作开始。
- en: String Manipulation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串操作
- en: 'There are a handful of basic string manipulation tools that we use a lot when
    we work with text:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理文本时，有几种基本的字符串操作工具我们经常使用。
- en: Transform uppercase characters to lowercase (or vice versa).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大写字符转换为小写（或反之）。
- en: Replace a substring with another or delete the substring.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用另一个子字符串替换或删除子字符串。
- en: Split a string into pieces at a particular character.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将字符串在特定字符处分割成片段。
- en: Slice a string at specified locations.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在指定的位置切片字符串。
- en: We show how we can combine these basic operations to clean up the county names
    data. Remember that we have two tables that we want to join, but the county names
    are written inconsistently.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何组合这些基本操作来清理县名数据。请记住，我们有两张表需要连接，但县名的写法不一致。
- en: Let’s start by converting the county names to a standard format.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将县名转换为标准格式。
- en: Converting Text to a Standard Format with Python String Methods
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 字符串方法将文本转换为标准格式
- en: 'We need to address the following inconsistencies between the county names in
    the two tables:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要解决两张表中县名之间的以下不一致性：
- en: 'Capitalization: `qui` versus `Qui`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小写问题：`qui` 与 `Qui`。
- en: 'Omission of words: `County` and `Parish` are absent from the `census` table.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 省略词语：`County` 和 `Parish` 在 `census` 表中不存在。
- en: 'Different abbreviation conventions: `&` versus `and`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的缩写约定：`&` 与 `and`。
- en: 'Different punctuation conventions: `St.` versus `St`.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的标点符号约定：`St.` 与 `St`。
- en: 'Use of whitespace: `DeWitt` versus `De Witt`.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用空白符：`DeWitt` 与 `De Witt`。
- en: When we clean text, it’s often easiest to first convert all of the characters
    to lowercase. It’s easier to work entirely with lowercase characters than to try
    to track combinations of uppercase and lowercase. Next, we want to fix inconsistent
    words by replacing `&` with `and` and removing `County` and `Parish`. Finally,
    we need to fix up punctuation and whitespace inconsistencies.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们清理文本时，通常最容易的方法是先将所有字符转换为小写。全使用小写字符比尝试跟踪大写和小写的组合要容易。接下来，我们想通过将`&`替换为`and`并删除`County`和`Parish`来修复不一致的单词。最后，我们需要修复标点符号和空格的不一致。
- en: 'With just two Python string methods, `lower` and `replace`, we can take all
    of these actions and clean the county names. These are combined into a method
    called `clean_county`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 只需使用两个Python字符串方法，`lower`和`replace`，我们就可以执行所有这些操作并清理县名。这些方法被合并到一个名为`clean_county`的方法中：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Although simple, these methods are the primitives that we can piece together
    to form more complex string operations. These methods are conveniently defined
    on all Python strings and do not require importing other modules. It is worth
    familiarizing yourself with [the complete list of string methods](https://oreil.ly/YWl9d),
    but we describe a few of the most commonly used methods in [Table 13-1](#string-methods).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管简单，这些方法是我们可以组合成更复杂的字符串操作的基本构建块。这些方法方便地定义在所有Python字符串上，无需导入其他模块。值得熟悉的是[字符串方法的完整列表](https://oreil.ly/YWl9d)，但我们在[表格 13-1](#string-methods)中描述了一些最常用的方法。
- en: Table 13-1\. String methods
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 13-1. 字符串方法
- en: '| Method | Description |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 描述 |'
- en: '| --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `str.lower()` | Returns a copy of a string with all letters converted to
    lowercase |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `str.lower()` | 返回字符串的副本，所有字母都转换为小写 |'
- en: '| `str.replace(a, b)` | Replaces all instances of the substring `a` in `str`
    with substring `b` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `str.replace(a, b)` | 将`str`中所有子字符串`a`替换为子字符串`b` |'
- en: '| `str.strip()` | Removes leading and trailing whitespace from `str` |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `str.strip()` | 从`str`中移除前导和尾随的空格 |'
- en: '| `str.split(a)` | Returns substrings of `str` split at a substring `a` |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `str.split(a)` | 返回在子字符串`a`处分割的`str`的子字符串 |'
- en: '| `str[x:y]` | Slices `str`, returning indices x (inclusive) to y (not inclusive)
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `str[x:y]` | 切片 `str`，返回从索引x（包括）到y（不包括）的部分 |'
- en: 'We next verify that the `clean_county` method produces matching county names:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们验证`clean_county`方法是否生成匹配的县名：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Since the county names now have consistent representations, we can successfully
    join the two tables.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自从县名现在有了统一的表示方式，我们可以成功地将这两个表格连接起来。
- en: String Methods in pandas
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pandas中的字符串方法
- en: In the preceding code, we used a loop to transform each county name. The `pandas`
    `Series` objects provide a convenient way to apply string methods to each item
    in the series.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们使用循环来转换每个县名。`pandas`的`Series`对象提供了一种便捷的方法，可以将字符串方法应用于系列中的每个项。
- en: 'The `.str` property on `pandas` `Series` exposes the same Python string methods.
    Calling a method on the `.str` property calls the method on each item in the series.
    This allows us to transform each string in the series without using a loop. We
    save the transformed counties back into their originating tables. First we transform
    the county names in the election table:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pandas`的`Series`上，`.str`属性公开了相同的Python字符串方法。在`.str`属性上调用方法会在系列中的每个项上调用该方法。这使我们能够在不使用循环的情况下转换系列中的每个字符串。我们将转换后的县名保存回其原始表中。首先，我们在选举表中转换县名：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We also transform the names in the census table so that the two tables contain
    the same representations of the county names. We can join these tables:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将人口普查表中的名称转换，以便这两个表格包含相同的县名表示。我们可以连接这些表格：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|   | County | State | Voted | Population |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|   | 县名 | 州名 | 投票率 | 人口 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **0** | dewitt | IL | 97.8 | 16,798 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **0** | dewitt | IL | 97.8 | 16,798 |'
- en: '| **1** | lacquiparle | MN | 98.8 | 8,067 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **1** | lacquiparle | MN | 98.8 | 8,067 |'
- en: '| **2** | lewisandclark | MT | 95.2 | 55,716 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **2** | lewisandclark | MT | 95.2 | 55,716 |'
- en: '| **3** | stjohnthebaptist | LA | 52.6 | 43,044 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| **3** | stjohnthebaptist | LA | 52.6 | 43,044 |'
- en: Note
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note that we merged on two columns: the county name and the state. We did this
    because some states have counties with the same name. For example, California
    and New York both have a county called King.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们根据县名和州名两列进行了合并。这是因为一些州有同名的县。例如，加利福尼亚州和纽约州都有一个名为金县的县。
- en: To see the complete list of string methods, we recommend looking at the [Python
    documentation on `str` methods](https://oreil.ly/Fb34C) and the [`pandas` documentation
    for the `.str` accessor](https://oreil.ly/njVi3). We did the canonicalization
    task using only `str.lower()` and multiple calls to `str.replace()`. Next, we
    extract text with another string method, `str.split()`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看完整的字符串方法列表，我们建议查看[Python关于`str`方法的文档](https://oreil.ly/Fb34C)和[`pandas`关于`.str`访问器的文档](https://oreil.ly/njVi3)。我们只使用了`str.lower()`和多次调用`str.replace()`来完成规范化任务。接下来，我们使用另一个字符串方法`str.split()`提取文本。
- en: Splitting Strings to Extract Pieces of Text
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割字符串以提取文本片段
- en: 'Let’s say we want to extract the date from the web server’s log entry:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想从网络服务器的日志条目中提取日期：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'String splitting can help us home in on the pieces of information that form
    the date. For example, when we split the string on the left bracket, we get two
    strings:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串分割可以帮助我们定位构成日期的信息片段。例如，当我们在左括号上分割字符串时，我们得到两个字符串：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The second string has the date information, and to get the day, month, and
    year, we can split that string on a colon:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个字符串包含日期信息，为了获取日、月和年，我们可以在该字符串上以冒号分割：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To separate out the day, month, and year, we can split on the forward slash.
    Altogether we split the original string three times, each time keeping only the
    pieces we are interested in:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要分开日、月和年，我们可以在斜杠上分割。总共我们分割原始字符串三次，每次只保留我们感兴趣的部分：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'By repeatedly using `split()`, we can extract many of the parts of the log
    entry. But this approach is complicated—if we wanted to also get the hour, minute,
    second, and time zone of the activity, we would need to use `split()` six times
    in total. There’s a simpler way to extract these parts:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反复使用`split()`，我们可以提取日志条目的许多部分。但是这种方法很复杂——如果我们还想获取活动的小时、分钟、秒钟和时区，我们需要总共使用`split()`六次。有一种更简单的方法来提取这些部分：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This alternative approach uses a powerful tool called a regular expression,
    which we cover in the next section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种替代方法使用了一个称为正则表达式的强大工具，我们将在下一节中介绍。
- en: Regular Expressions
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则表达式
- en: '*Regular expressions* (or *regex* for short) are special patterns that we use
    to match parts of strings. Think about the format of a Social Security number
    (SSN) like `134-42-2012`. To describe this format, we might say that SSNs consist
    of three digits, then a dash, two digits, another dash, then four digits. Regexes
    let us capture this pattern in code. Regexes give us a compact and powerful way
    to describe this pattern of digits and dashes. The syntax of regular expressions
    is fortunately quite simple to learn; we introduce nearly all of the syntax in
    this section alone.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则表达式*（或简称*regex*）是我们用来匹配字符串部分的特殊模式。想想社会安全号码（SSN）的格式，例如`134-42-2012`。为了描述这种格式，我们可以说SSN由三位数字、一个短划线、两位数字、另一个短划线，然后是四位数字。正则表达式让我们能够在代码中捕获这种模式。正则表达式为我们提供了一种紧凑而强大的方式来描述这种数字和短划线的模式。正则表达式的语法非常简单，我们在本节中几乎介绍了所有的语法。'
- en: As we introduce the concepts, we tackle some of the examples described in an
    earlier section and show how to carry out the tasks with regular expressions.
    Almost all programming languages have a library to match patterns using regular
    expressions, making regular expressions useful in any programming language. We
    use some of the common methods available in the Python built-in `re` module to
    accomplish the tasks from the examples. These methods are summarized in [Table 13-7](#regex-methods)
    at the end of this section, where the basic usage and return value are briefly
    described. Since we only cover a few of the most commonly used methods, you may
    find it useful to consult [the official documentation on the `re` module](https://oreil.ly/IXWol)
    as well.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍这些概念时，我们解决了前一节中描述的一些示例，并展示了如何使用正则表达式执行任务。几乎所有编程语言都有一个库，用于使用正则表达式匹配模式，这使得正则表达式在任何编程语言中都很有用。我们使用Python内置的`re`模块中的一些常见方法来完成示例中的任务。这些方法在本节末尾的[表格 13-7](#regex-methods)中进行了总结，其中简要描述了基本用法和返回值。由于我们只涵盖了一些最常用的方法，您可能会发现参考[官方文档关于`re`模块的信息](https://oreil.ly/IXWol)也很有用。
- en: Regular expressions are based on searching a string one character (aka *literal*)
    at a time for a pattern. We call this notion *concatenation of literals*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式基于逐个字符（也称为*字面量*）搜索模式的字符串。我们称这种概念为*字面量的连接*。
- en: Concatenation of Literals
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字面量的连接
- en: Concatenation is best explained with a basic example. Suppose we are looking
    for the pattern `cat` in the string `cards scatter!`. [Figure 13-1](#fig-regex-literals)
    contains a diagram that shows how the search proceeds through the string one character
    at a time. Notice that a “c” is found in the first position, followed by “a,”
    but not “t,” so the search backs up to the second character in the string and
    begins searching for a “c” again. The pattern “cat” is found within the string
    `cards scatter!` in positions 8–10\. Once you get the hang of this process, you
    can move on to the richer set of patterns; they all follow this basic paradigm.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 串联最好通过一个基本示例来解释。假设我们在字符串`cards scatter!`中寻找模式`cat`。[图13-1](#fig-regex-literals)包含了一个图表，展示了搜索如何逐个字符地进行。请注意，在字符串的第一个位置找到了“c”，接着是“a”，但没有“t”，所以搜索会回到字符串的第二个字符，并开始再次搜索“c”。模式“cat”在字符串`cards
    scatter!`中的位置为8到10。一旦你掌握了这个过程，你可以继续进行更丰富的模式；它们都遵循这个基本的范例。
- en: '![](assets/leds_1301.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_1301.png)'
- en: Figure 13-1\. To match literal patterns, the regex engine moves along the string
    and checks one literal at a time for a match of the entire pattern. Notice that
    the pattern is found within the word `scatters` and that a partial match is found
    in `cards`.
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-1\. 为了匹配文字模式，正则引擎沿着字符串移动，并逐个检查是否符合整个模式。请注意，模式在单词`scatters`中找到，并且在`cards`中找到部分匹配。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the preceding example, we observe that regular expressions can match patterns
    that appear anywhere in the input string. In Python, this behavior differs depending
    on the method used to match the regex—some methods only return a match if the
    regex appears at the start of the string; other methods return a match anywhere
    in the string.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们观察到正则表达式可以匹配出现在输入字符串中的任何模式。在Python中，这种行为根据用于匹配正则表达式的方法而异——某些方法仅在字符串开头匹配正则表达式时返回匹配；其他方法则在字符串中的任何位置返回匹配。
- en: These richer patterns are made of character classes and metacharacters like
    wildcards. We describe them in the subsections that follow.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更丰富的模式由字符类和通配符等元字符组成。我们将在接下来的小节中描述它们。
- en: Character classes
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符类
- en: 'We can make patterns more flexible by using a *character class* (also known
    as a *character set*), which lets us specify a collection of equivalent characters
    to match. This allows us to create more relaxed matches. To create a character
    class, wrap the set of desired characters in brackets `[ ]`. For example, the
    pattern `[0123456789]` means “match any literal within the brackets”—in this case,
    any single digit. Then, the following regular expression matches three digits:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用*字符类*（也称为*字符集*）使模式更加灵活，它允许我们指定要匹配的一组等效字符。这使我们能够创建更宽松的匹配。要创建一个字符类，请将所需的字符集合包含在方括号`[
    ]`中。例如，模式`[0123456789]`表示“匹配方括号内的任何文字”—在这种情况下是任何单个数字。然后，以下正则表达式匹配三个数字：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is such a commonly used character class that there is a shorthand notation
    for the range of digits, `[0-9]`. Character classes allow us to create a regex
    for SSNs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常用的字符类，有一个数字范围的简写表示法`[0-9]`。字符类允许我们创建一个匹配社保号码（SSNs）的正则表达式：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Two other ranges that are commonly used in character classes are `[a-z]` for
    lowercase and `[A-Z]` for uppercase letters. We can combine ranges with other
    equivalent characters and use partial ranges. For example, `[a-cX-Z27]` is equivalent
    to the character class `[abcXYZ27]`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个常用的字符类范围是小写字母`[a-z]`和大写字母`[A-Z]`。我们可以将范围与其他等效字符结合使用，并使用部分范围。例如，`[a-cX-Z27]`等效于字符类`[abcXYZ27]`。
- en: 'Let’s return to our original pattern `cat` and modify it to include two character
    classes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们最初的模式`cat`，并修改它以包含两个字符类：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This pattern matches `cat`, but it also matches `cot`, `cad`, and `cod`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式匹配`cat`，但也匹配`cot`，`cad`和`cod`：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The idea of moving through the string one character at a time still remains
    the core notion, but now there’s a bit more flexibility in which literal is considered
    a match.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每次仍然逐个字符地移动字符串的核心概念仍然存在，但现在在哪个文字被视为匹配方面有了更多的灵活性。
- en: Wildcard character
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通配符字符
- en: When we really don’t care what the literal is, we can specify this with `.`,
    the period character. This matches any character except a newline.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们真的不关心文字是什么时，我们可以用`.`，即句号字符来指定。这可以匹配除换行符以外的任何字符。
- en: Negated character classes
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 否定字符类
- en: A *negated character class* matches any character *except* those between the
    square brackets. To create a negated character class, place the caret symbol as
    the first character after the left square bracket. For example, `[^0-9]` matches
    any character except a digit.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*否定字符类* 匹配方括号内除了那些字符外的任何字符。要创建否定字符类，请在左方括号后面放置插入符号。例如，`[^0-9]` 匹配除数字外的任何字符。'
- en: Shorthands for character classes
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符类的简写形式
- en: 'Some character sets are so common that there are shorthands for them. For example,
    `\d` is short for `[0-9]`. We can use this shorthand to simplify our search for
    SSN:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有些字符集如此常见，以至于有它们的简写形式。例如，`\d`代表`[0-9]`。我们可以使用这些简写来简化对社会安全号码的搜索：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Our regular expression for SSNs isn’t quite bulletproof. If the string has
    extra digits at the beginning or end of the pattern we’re looking for, then we
    still get a match. Note that we add the `r` character before the quotes to create
    a raw string, which makes regexes easier to write:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的社会安全号码的正则表达式并不是完全可靠的。如果字符串在我们寻找的模式开头或结尾有额外的数字，那么我们仍然能够匹配到。注意，我们在引号前添加`r`字符以创建原始字符串，这样可以更容易地编写正则表达式：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can remedy the situation with a different sort of metacharacter: one that
    matches a word boundary.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过不同类型的元字符来修正这种情况：一个可以匹配单词边界的元字符。
- en: Anchors and boundaries
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 锚点和边界
- en: 'At times we want to match a position before, after, or between characters.
    One example is to locate the beginning or end of a string; these are called *anchors*.
    Another is to locate the beginning or end of a word, which we call a *boundary*.
    The metacharacter `\b` denotes the boundary of a word. It has 0 length, and it
    matches whitespace or punctuation on the boundary of the pattern. We can use it
    to fix our regular expression for SSNs:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们想要匹配字符之前、之后或之间的位置。一个例子是定位字符串的开头或结尾；这些称为*锚点*。另一个是定位单词的开头或结尾，我们称之为*边界*。元字符`\b`表示单词的边界。它长度为0，并且匹配模式边界上的空白或标点符号。我们可以用它来修复我们的社会安全号码的正则表达式：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Escaping metacharacters
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转义元字符
- en: 'We have now seen several special characters, called *metacharacters*: `[` and
    `]` denote a character class, `^` switches to a negated character class, `.` represents
    any character, and `-` denotes a range. But sometimes we might want to create
    a pattern that matches one of these literals. When this happens, we must escape
    it with a backslash. For example, we can match the literal left bracket character
    using the regex `\[`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经见过几个特殊字符，称为*元字符*：`[` 和 `]` 表示字符类，`^` 切换到否定字符类，`.` 表示任何字符，`-` 表示范围。但有时我们可能想要创建一个匹配其中一个这些文字的模式。当这种情况发生时，我们必须用反斜杠进行转义。例如，我们可以使用正则表达式
    `\[` 来匹配字面上的左方括号字符：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, we show how quantifiers can help create a more compact and clear regular
    expression for SSNs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示量词如何帮助创建更紧凑和清晰的正则表达式来匹配社会安全号码。
- en: Quantifiers
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量词
- en: 'To create a regex to match SSNs, we wrote:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个用于匹配社会安全号码的正则表达式，我们写道：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This matches a “word” consisting of three digits, a dash, two more digits, a
    dash, and four more digits.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这匹配由三个数字、一个破折号、另外两个数字、一个破折号和另外四个数字组成的“单词”。
- en: Quantifiers allow us to match multiple consecutive appearances of a literal.
    We specify the number of repetitions by placing the number in curly braces `{
    }`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 量词允许我们匹配多个连续出现的字符。我们通过在花括号`{ }`中放置数字来指定重复的次数。
- en: 'Let’s use Python’s built-in `re` module for matching this pattern:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python的内置`re`模块来匹配这种模式：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Our pattern shouldn’t match phone numbers. Let’s try it:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式不应匹配电话号码。让我们试试：
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: A quantifier always modifies the character or character class to its immediate
    left. [Table 13-2](#quantifier-ex) shows the complete syntax for quantifiers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 量词总是修改其左侧的字符或字符类。[Table 13-2](#quantifier-ex) 显示了量词的完整语法。
- en: Table 13-2\. Quantifier examples
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Table 13-2\. 量词示例
- en: '| Quantifier | Meaning |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: Table 13-3\. 简写量词
- en: '| --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| {m, n} | Match the preceding character m to n times. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| {m, n} | 匹配前面的字符 m 到 n 次。 |'
- en: '| {m} | Match the preceding character exactly m times. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| {m} | 匹配前面的字符恰好 m 次。 |'
- en: '| {m,} | Match the preceding character at least m times. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| {m,} | 匹配前面的字符至少 m 次。 |'
- en: '| {,n} | Match the preceding character at most n times. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| {,n} | 匹配前面的字符最多 n 次。 |'
- en: Some commonly used quantifiers have a shorthand, as shown in [Table 13-3](#short-quantifiers).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常用的量词有简写形式，如[Table 13-3](#short-quantifiers)所示。
- en: Table 13-3\. Shorthand quantifiers
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Table 13-3\. 简写量词
- en: '| Symbol | Quantifier | Meaning |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Symbol | 量词 | 含义 |'
- en: '| --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `*` | {0,} | Match the preceding character 0 or more times. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| `*` | {0,} | 匹配前一个字符0或多次。 |'
- en: '| `+` | {1,} | Match the preceding character 1 or more times. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `+` | {1,} | 匹配前一个字符1次或多次。 |'
- en: '| `?` | {0,1} | Match the preceding character 0 or 1 time. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `?` | {0,1} | 匹配前一个字符0或1次。 |'
- en: Quantifiers are greedy and will return the longest match possible. This sometimes
    results in surprising behavior. Since an SSN starts and ends with a digit, we
    might think the following shorter regex will be a simpler approach for finding
    SSNs. Can you figure out what went wrong in the matching?
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 量词是贪婪的，会返回可能的最长匹配。这有时会导致意想不到的行为。由于社会安全号码以数字开头和结尾，我们可能认为以下较短的正则表达式将是查找SSN的简单方法。你能想出匹配出现错误的原因吗？
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Notice that we use the metacharacter `.` to match any character. In many cases,
    using a more specific character class prevents these false “overmatches.” Our
    earlier pattern that includes word boundaries does this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用元字符`.`匹配任意字符。在许多情况下，使用更具体的字符类可以避免这些虚假的“超匹配”。我们之前包含单词边界的模式就是这样做的：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Some platforms allow you to turn off greedy matching and use *lazy* matching,
    which returns the shortest string.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有些平台允许关闭贪婪匹配并使用*惰性*匹配，返回最短的字符串。
- en: 'Literal concatenation and quantifiers are two of the core concepts in regular
    expressions. Next, we introduce two more core concepts: alternation and grouping.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 文字连接和量词是正则表达式中的两个核心概念。接下来，我们介绍另外两个核心概念：选择和分组。
- en: Alternation and Grouping to Create Features
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用选择和分组创建特征
- en: 'Character classes let us match multiple options for a single literal. We can
    use alternation to match multiple options for a group of literals. For instance,
    in the food safety example in [Chapter 9](ch09.html#ch-wrangling), we marked violations
    related to body parts by seeing if the violation had the substring `hand`, `nail`,
    `hair`, or `glove`. We can use the `|` character in a regex to specify this alteration:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 字符类允许我们匹配单个文字的多个选项。我们可以使用选择来匹配一组文字的多个选项。例如，在[第9章](ch09.html#ch-wrangling)中的食品安全示例中，我们标记与身体部位相关的违规行为，通过检查违规行为是否包含`hand`、`nail`、`hair`或`glove`子串。我们可以在正则表达式中使用`|`字符指定这种选择：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'With parentheses we can locate parts of a pattern, which are called *regex
    groups*. For example, we can use regex groups to extract the day, month, year,
    and time from the web server log entry:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用括号我们可以定位模式的部分，这称为*正则表达式组*。例如，我们可以使用正则表达式组从Web服务器日志条目中提取日期、月份、年份和时间：
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As we can see, `re.findall` returns a list of tuples containing the individual
    components of the date and time of the web log.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，`re.findall` 返回一个包含日期和时间各个组件的元组列表。
- en: We have introduced a lot of terminology, so in the next section we bring it
    all together into a set of tables for easy reference.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了许多术语，因此在下一节中，我们将它们整合到一组表格中，以便轻松查阅。
- en: Reference Tables
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考表格
- en: We conclude this section with a few tables that summarize order of operation,
    metacharacters, and shorthands for character classes. We also provide tables summarizing
    the handful of methods in the `re` Python library that we have used in this section.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过几个总结操作顺序、元字符和字符类速记的表格来结束本节。我们还提供了总结在本节中使用的`re` Python库少数方法的表格。
- en: The four basic operations for regular expressions—concatenation, quantifying,
    alternation, and grouping—have an order of precedence, which we make explicit
    in [Table 13-4](#regex-order).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式的四个基本操作——连接、量化、选择和分组——具有优先顺序，在[表格 13-4](#regex-order) 中我们明确说明了这一点。
- en: Table 13-4\. Order of operations
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 13-4\. 操作顺序
- en: '| Operation | Order | Example | Matches |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | 顺序 | 示例 | 匹配 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Concatenation | 3 | `cat` | `cat` |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 连接 | 3 | `cat` | `cat` |'
- en: '| Alternation | 4 | `cat&#124;mouse` | `cat` and `mouse` |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 选择 | 4 | `cat&#124;mouse` | `cat` 和 `mouse` |'
- en: '| Quantifying | 2 | `cat?` | `ca` and `cat` |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | 2 | `cat?` | `ca` 和 `cat` |'
- en: '| Grouping | 1 | c(at)? | `c` and `cat` |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 分组 | 1 | c(at)? | `c` 和 `cat` |'
- en: '[Table 13-5](#regex-meta) provides a list of the metacharacters introduced
    in this section, plus a few more. The column labeled “Doesn’t match” gives examples
    of strings that the example regexes don’t match.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 13-5](#regex-meta) 提供了本节介绍的元字符列表，以及一些额外的内容。标有“不匹配”的列举了示例正则表达式不匹配的字符串。'
- en: Table 13-5\. Metacharacters
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 13-5\. 元字符
- en: '| Char | Description | Example | Matches | Doesn’t match |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 字符 | 描述 | 示例 | 匹配 | 不匹配 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| . | Any character except \n | `...` | abc | ab |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| . | 除 \n 外的任意字符 | `...` | abc | ab |'
- en: '| [ ] | Any character inside brackets | `[cb.]ar` | car .ar | jar |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| [ ] | 方括号内的任意字符 | `[cb.]ar` | car .ar | jar |'
- en: '| [^ ] | Any character *not* inside brackets | `[^b]ar` | car par | bar ar
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| [^ ] | 方括号内 *不* 包含的任意字符 | `[^b]ar` | car par | bar ar |'
- en: '| * | ≥ 0 or more of previous symbol, shorthand for {0,} | `[pb]*ark` | bbark
    ark | dark |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| * | ≥ 0 或更多前一个符号，简写为 {0,} | `[pb]*ark` | bbark ark | dark |'
- en: '| + | ≥ 1 or more of previous symbol, shorthand for {1,} | `[pb]+ark` | bbpark
    bark | dark ark |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| + | ≥ 1 或更多前一个符号，简写为 {1,} | `[pb]+ark` | bbpark bark | dark ark |'
- en: '| ? | 0 or 1 of previous symbol, shorthand for {0,1} | `s?he` | she he | the
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| ? | 0 或 1 个前一个符号，简写为 {0,1} | `s?he` | she he | the |'
- en: '| {*n*} | Exactly *n* of previous symbol | `hello{3}` | hellooo | hello |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| {*n*} | 前一个符号恰好 *n* 次 | `hello{3}` | hellooo | hello |'
- en: '| &#124; | Pattern before or after bar | `we&#124;[ui]s` | we us'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '| &#124; | 竖线前后的模式 | `we&#124;[ui]s` | we us'
- en: is | es e
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: is | es e
- en: s |
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: s |
- en: '| \ | Escape next character | `\[hi\]` | [hi] | hi |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| \ | 转义下一个字符 | `\[hi\]` | [hi] | hi |'
- en: '| ^ | Beginning of line | `^ark` | ark two | dark |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| ^ | 行首 | `^ark` | ark two | dark |'
- en: '| $ | End of line | `ark$` | noahs ark | noahs arks |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| $ | 行尾 | `ark$` | noahs ark | noahs arks |'
- en: '| \b | Word boundary | `ark\b` | ark of noah | noahs arks |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| \b | 单词边界 | `ark\b` | ark of noah | noahs arks |'
- en: Additionally, in [Table 13-6](#regex-shorthand), we provide shorthands for some
    commonly used character sets. These shorthands don’t need `[ ]`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 [表 13-6](#regex-shorthand) 中，我们为一些常用字符集提供了简写。这些简写不需要 `[ ]`。
- en: Table 13-6\. Character class shorthands
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13-6\. 字符类简写
- en: '| Description | Bracket form | Shorthand |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 方括号形式 | 简写 |'
- en: '| --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Alphanumeric character | `[a-zA-Z0-9_]` | `\w` |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 字母数字字符 | `[a-zA-Z0-9_]` | `\w` |'
- en: '| Not an alphanumeric character | `[^a-zA-Z0-9_]` | `\W` |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 非字母数字字符 | `[^a-zA-Z0-9_]` | `\W` |'
- en: '| Digit | `[0-9]` | `\d` |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 数字 | `[0-9]` | `\d` |'
- en: '| Not a digit | `[^0-9]` | `\D` |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 非数字 | `[^0-9]` | `\D` |'
- en: '| Whitespace | `[\t\n\f\r\p{Z}]` | `\s` |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 空白字符 | `[\t\n\f\r\p{Z}]` | `\s` |'
- en: '| Not whitespace | `[^\t\n\f\r\p{z}]` | `\S` |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 非空白字符 | `[\t\n\f\r\p{z}]` | `\S` |'
- en: 'We used the following methods in `re` in this chapter. The names of the methods
    are indicative of the functionality they perform: *search* or *match* a pattern
    in a string; *find all* cases of a pattern in a string; *sub*stitute all occurrences
    of a pattern with a substring; and *split* a string into pieces at the pattern.
    Each requires a pattern and string to be specified, and some have extra arguments.
    [Table 13-7](#regex-methods) provides the format of the method usage and a description
    of the return value.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中使用了以下 `re` 方法。方法名称指示了它们执行的功能：在字符串中 *搜索* 或 *匹配* 模式；在字符串中 *查找* 所有模式的情况；将模式的所有出现
    *替换* 为子字符串；以及在模式处 *分割* 字符串。每个方法都需要指定一个模式和一个字符串，并且一些还有额外的参数。[表 13-7](#regex-methods)
    提供了方法使用的格式以及返回值的描述。
- en: Table 13-7\. Regular expression methods
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13-7\. 正则表达式方法
- en: '| Method | Return value |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 返回值 |'
- en: '| --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `re.search(pattern, string)` | Match object if the pattern is found anywhere
    in the string, otherwise `None` |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `re.search(pattern, string)` | 如果模式在字符串任何位置找到，则为匹配对象，否则为 `None` |'
- en: '| `re.match(pattern, string)` | Match object if the pattern is found at the
    beginning of the string, otherwise `None` |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `re.match(pattern, string)` | 如果模式在字符串开头找到，则为匹配对象，否则为 `None` |'
- en: '| `re.findall(pattern, string)` | List of all matches of `pattern` in `string`
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `re.findall(pattern, string)` | 字符串 `string` 中所有 `pattern` 的匹配项列表 |'
- en: '| `re.sub(pattern, replacement, string)` | String where all occurrences of
    `pattern` are replaced by `replacement` in the `string` |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `re.sub(pattern, replacement, string)` | 字符串 `string` 中所有 `pattern` 的出现都被
    `replacement` 替换的字符串 |'
- en: '| `re.split(pattern, string)` | List of the pieces of `string` around the occurrences
    of `pattern` |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `re.split(pattern, string)` | 围绕 `pattern` 出现的 `string` 片段列表 |'
- en: As we saw in the previous section, `pandas` `Series` objects have a `.str` property
    that supports string manipulation using Python string methods. Conveniently, the
    `.str` property also supports some functions from the `re` module. [Table 13-8](#regex-pandas)
    shows the analogous functionality from [Table 13-7](#regex-methods) of the `re`
    methods. Each requires a pattern. See [the `pandas` docs](https://oreil.ly/aHJRz)
    for a complete list of string methods.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中看到的，`pandas` 的 `Series` 对象具有一个 `.str` 属性，支持使用 Python 字符串方法进行字符串操作。方便地，`.str`
    属性还支持 `re` 模块的一些函数。[表 13-8](#regex-pandas) 展示了与 `re` 方法的 [表 13-7](#regex-methods)
    相似的功能。每个都需要一个模式。请参阅 [pandas 文档](https://oreil.ly/aHJRz) 获取完整的字符串方法列表。
- en: Table 13-8\. Regular expressions in `pandas`
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13-8\. `pandas` 中的正则表达式
- en: '| Method | Return value |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 返回值 |'
- en: '| --- | --- |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `str.contains(pattern, regex=True)` | Series of booleans indicating whether
    the `pattern` is found |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `str.contains(pattern, regex=True)` | 指示 `pattern` 是否找到的布尔序列 |'
- en: '| `str.findall(pattern, regex=True)` | List of all matches of `pattern` |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `str.findall(pattern, regex=True)` | 匹配 `pattern` 的所有结果的列表 |'
- en: '| `str.replace(pattern, replacement, regex=True)` | Series with all matching
    occurrences of `pattern` replaced by `replacement` |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `str.replace(pattern, replacement, regex=True)` | Series with all matching
    occurrences of `pattern` replaced by `replacement` |'
- en: '| `str.split(pattern, regex=True)` | Series of lists of strings around given
    `pattern` |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `str.split(pattern, regex=True)` | 给定 `pattern` 周围字符串列表的序列 |'
- en: 'Regular expressions are a powerful tool, but they’re somewhat notorious for
    being difficult to read and debug. We close with some advice for using regexes:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式是一个强大的工具，但它们以难以阅读和调试著称。最后我们提出了一些建议来使用正则表达式：
- en: Develop your regular expression on simple test strings to see what the pattern
    matches.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在简单的测试字符串上开发你的正则表达式，看看模式匹配的情况。
- en: If a pattern matches nothing, try weakening it by dropping part of the pattern.
    Then tighten it incrementally to see how the matching evolves. (Online regex-checking
    tools can be very helpful here.)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个模式没有匹配到任何内容，尝试通过减少模式的部分来削弱它。然后逐步加强它，看看匹配的进展。（在线正则表达式检查工具在这里非常有帮助。）
- en: Make the pattern only as specific as it needs to be for the data at hand.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让模式尽可能具体以适应手头的数据。
- en: Use raw strings whenever possible for cleaner patterns, especially when a pattern
    includes a backslash.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能使用原始字符串以获得更清晰的模式，特别是当模式包含反斜杠时。
- en: When you have lots of long strings, consider using compiled patterns because
    they can be faster to match (see `compile()` in the `re` library).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你有很多长字符串时，考虑使用编译后的模式，因为它们可以更快速地匹配（见 `re` 库中的 `compile()`）。
- en: In the next section, we carry out an example text analysis. We clean the data
    using regular expressions and string manipulation, convert the text into quantitative
    data, and analyze the text via these derived quantities.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们进行一个示例文本分析。我们使用正则表达式和字符串操作清理数据，将文本转换为定量数据，并通过这些派生数量分析文本。
- en: Text Analysis
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析
- en: So far, we’ve used Python methods and regular expressions to clean short text
    fields and strings. In this section, we analyze entire documents using a technique
    called *text mining*, which transforms free-form text into a quantitative representation
    to uncover meaningful patterns and insights.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用 Python 方法和正则表达式来清理短文本字段和字符串。在本节中，我们将使用一种称为*文本挖掘*的技术来分析整个文档，该技术将自由形式的文本转换为定量表达，以揭示有意义的模式和洞见。
- en: Text mining is a deep topic. Instead of a comprehensive treatment, we introduce
    a few key ideas through an example, where we analyze the State of the Union speeches
    from 1790 to 2022\. Every year, the US president gives a State of the Union speech
    to Congress. These speeches talk about current events in the country and make
    recommendations for Congress to consider. [The American Presidency Project](https://oreil.ly/JbpO4)
    makes these speeches available online.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 文本挖掘是一个深奥的主题。我们不打算进行全面的讲解，而是通过一个例子介绍几个关键思想，我们将分析从 1790 年到 2022 年的国情咨文演讲。每年，美国总统向国会发表国情咨文演讲。这些演讲谈论国家当前事件，并提出国会应考虑的建议。[美国总统项目](https://oreil.ly/JbpO4)
    在线提供这些演讲。
- en: 'Let’s begin by opening the file that has all of the speeches:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从打开包含所有演讲的文件开始：
- en: '[PRE44]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'At the beginning of this chapter, we saw that each speech in the data begins
    with a line with three asterisks: `***`. We can use a regular expression to count
    the number of times the string `***` appears:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们看到数据中每篇演讲都以三个星号开头的一行：`***`。我们可以使用正则表达式来计算字符串 `***` 出现的次数：
- en: '[PRE45]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In text analysis, a *document* refers to a single piece of text that we want
    to analyze. Here, each speech is a document. We split apart the `text` variable
    into its individual documents:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分析中，*文档* 指的是我们想要分析的单个文本。在这里，每篇演讲都是一个文档。我们将 `text` 变量分解为其各个文档：
- en: '[PRE47]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then we can put the speeches into a dataframe:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以把演讲放入一个数据框中：
- en: '[PRE48]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '|   | name | date | text |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|   | 名称 | 日期 | 文本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **0** | George Washington | January 8, 1790 | Fellow-Citizens of the Senate
    and House of Rep... |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 乔治·华盛顿 | 1790 年 1 月 8 日 | 参议院和众议院的同胞们... |'
- en: '| **1** | George Washington | December 8, 1790 | Fellow-Citizens of the Senate
    and House of Rep... |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 乔治·华盛顿 | 1790 年 12 月 8 日 | 参议院和众议院的同胞们... |'
- en: '| **2** | George Washington | October 25, 1791 | Fellow-Citizens of the Senate
    and House of Rep... |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 乔治·华盛顿 | 1791年10月25日 | 参议院和众议院的同胞们...'
- en: '| **...** | ... | ... | ... |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| **...** | ... | ... | ... |'
- en: '| **229** | Donald J. Trump | February 4, 2020 | Thank you very much. Thank
    you. Thank you very... |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| **229** | 唐纳德·J·特朗普 | 2020年2月4日 | 非常感谢。谢谢。非常感谢你...'
- en: '| **230** | Joseph R. Biden, Jr. | April 28, 2021 | Thank you. Thank you. Thank
    you. Good to be ba... |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| **230** | 约瑟夫·R·拜登 | 2021年4月28日 | 谢谢你。谢谢你。谢谢你。很高兴回来...'
- en: '| **231** | Joseph R. Biden, Jr. | March 1, 2022 | Madam Speaker, Madam Vice
    President, our First... |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| **231** | 约瑟夫·R·拜登 | 2022年3月1日 | 女士们，众议长，女副总统，我们的第一...'
- en: '[PRE49]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now that we have the speeches loaded into a dataframe, we want to transform
    the speeches to see how they have changed over time. Our basic idea is to look
    at the words in the speeches—if two speeches contain very different words, our
    analysis should tell us that. With some kind of measure of document similarity,
    we can see how the speeches differ from one another.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将演讲加载到数据框中，我们希望转换演讲，以查看它们随时间的变化。我们的基本思想是查看演讲中的单词 - 如果两个演讲包含非常不同的单词，我们的分析应告诉我们这一点。通过某种文档相似度的度量，我们可以看到演讲彼此之间的差异。
- en: 'There are a few problems in the documents that we need to take care of first:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中有一些问题，我们需要先解决：
- en: 'Capitalization shouldn’t matter: `Citizens` and `citizens` should be considered
    the same word. We can address this by lowercasing the text.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小写不应该影响：`Citizens` 和 `citizens` 应被视为相同的单词。我们可以通过将文本转换为小写来解决这个问题。
- en: 'There are unspoken remarks in the text: `[laughter]` points out where the audience
    laughed, but these shouldn’t count as part of the speech. We can address this
    by using a regex to remove text within brackets: `\[[^\]]+\]`. Remember that `\[`
    and `\]` match the literal left and right brackets, and `[^\]]` matches any character
    that isn’t a right bracket.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本中有未发表的言论：`[laughter]` 指出观众笑了的地方，但这些不应该算作演讲的一部分。我们可以通过使用正则表达式来删除方括号内的文本：`\[[^\]]+\]`。请记住，`\[`
    和 `\]` 匹配文字的左右括号，`[^\]]` 匹配任何不是右括号的字符。
- en: 'We should take out characters that aren’t letters or whitespace: some speeches
    talk about finances, but a dollar amount shouldn’t count as a word. We can use
    the regex `[^a-z\s]` to remove these characters. This regex matches any character
    that isn’t a lowercase letter (`a-z`) or a whitespace character (`\s`):'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该去掉不是字母或空格的字符：一些演讲谈到财务问题，但金额不应该被视为单词。我们可以使用正则表达式 `[^a-z\s]` 来删除这些字符。这个正则表达式匹配任何不是小写字母
    (`a-z`) 或空格字符 (`\s`) 的字符：
- en: '[PRE50]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '|   | name | date | text |'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|   | 名字 | 日期 | 文本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0 | George Washington | January 8, 1790 | fellow citizens of the senate and
    house of rep... |'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 乔治·华盛顿 | 1790年1月8日 | 参议院和众议院的同胞们...'
- en: '| 1 | George Washington | December 8, 1790 | fellow citizens of the senate
    and house of rep... |'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 乔治·华盛顿 | 1790年12月8日 | 参议院和众议院的同胞们...'
- en: '| 2 | George Washington | October 25, 1791 | fellow citizens of the senate
    and house of rep... |'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2 | 乔治·华盛顿 | 1791年10月25日 | 参议院和众议院的同胞们...'
- en: '| ... | ... | ... | ... |'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ... | ... | ... | ... |'
- en: '| 229 | Donald J. Trump | February 4, 2020 | thank you very much thank you
    thank you very... |'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 229 | 唐纳德·J·特朗普 | 2020年2月4日 | 非常感谢。谢谢。非常感谢你...'
- en: '| 230 | Joseph R. Biden, Jr. | April 28, 2021 | thank you thank you thank you
    good to be ba... |'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 230 | 约瑟夫·R·拜登 | 2021年4月28日 | 谢谢你。谢谢你。谢谢你。很高兴回来...'
- en: '| 231 | Joseph R. Biden, Jr. | March 1, 2022 | madam speaker madam vice president
    our first... |'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 231 | 约瑟夫·R·拜登 | 2022年3月1日 | 女士们，众议长，女副总统，我们的第一...'
- en: '[PRE51]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we look at some more complex issues:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们看一些更复杂的问题：
- en: '*Stop words* like `is`, `and`, `the`, and `but` appear so often that we would
    like to just remove them.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*停用词*，如 `is`、`and`、`the` 和 `but` 出现得太频繁，我们希望将它们删除。'
- en: '`argue` and `arguing` should count as the same word, even though they appear
    differently in the text. To address this, we’ll use *word stemming*, which transforms
    both words to `argu`.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`argue` 和 `arguing` 应被视为相同的单词，尽管它们在文本中显示不同。为了解决这个问题，我们将使用*词干提取*，将两个单词转换为 `argu`。'
- en: To handle these issues, we can use built-in methods from [the `nltk` library](https://www.nltk.org).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理这些问题，我们可以使用[nltk库](https://www.nltk.org)中的内置方法。
- en: Finally, we transform the speeches into *word vectors*. A word vector represents
    a document using a vector of numbers. For example, one basic type of word vector
    counts up how many times each word appears in the text, as depicted in [Figure 13-2](#fig-word-vectors).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将演讲转换为*词向量*。词向量使用一组数字来表示一个文档。例如，一种基本的词向量类型统计了文本中每个词出现的次数，如[图13-2](#fig-word-vectors)所示。
- en: '![](assets/leds_1302.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_1302.png)'
- en: Figure 13-2\. Bag-of-words vectors for three small example documents
  id: totrans-297
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2。三个小示例文档的词袋向量
- en: This simple transform is called *bag-of-words*, and we apply it on all of our
    speeches. Then we calculate the *term frequency-inverse document frequency* (*tf-idf*
    for short) to normalize the counts and measure the rareness of a word. The tf-idf
    puts more weight on words that only appear in a few documents. The idea is that
    if just a few documents mention the word *sanction*, say, then this word is extra
    useful for distinguishing documents from each other. [The `scikit-learn` library](https://oreil.ly/3A6a5)
    has a complete description of the transform and an implementation, which we use.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的转换被称为*词袋模型*，我们将其应用于所有的演讲稿。然后，我们计算*词频-逆文档频率*（*tf-idf*简称）来规范化计数并测量单词的稀有性。tf-idf会对只出现在少数文档中的单词给予更多的权重。其思想是，如果只有少数文档提到了*制裁*这个词，那么这个词对于区分不同文档就非常有用。我们使用的[scikit-learn库](https://oreil.ly/3A6a5)完整描述了这一转换和实现。
- en: 'After applying these transforms, we have a two-dimensional array, `speech_vectors`.
    Each row of this array is one speech transformed into a vector:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些转换后，我们得到了一个二维数组，`speech_vectors`。该数组的每一行是一个转换为向量的演讲：
- en: '[PRE52]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We have 232 speeches, and each speech was transformed into a length-13,211 vector.
    To visualize these speeches, we use a technique called *principal component analysis*
    to represent the data table of 13,211 features by a new set of features that are
    orthogonal to one another. The first vector accounts for the maximum variation
    in the original features, the second for the maximum variance that is orthogonal
    to the first, and so on. Often the first two components, which we can plot as
    pairs of points, reveal clusters and outliers.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有232篇演讲，每篇演讲都被转换为一个长度为13,211的向量。为了可视化这些演讲，我们使用一种称为*主成分分析*的技术，将13,211个特征的数据表通过一组新的正交特征重新表示。第一个向量解释了原始特征的最大变化，第二个向量解释了与第一个正交的最大方差，依此类推。通常，我们可以将前两个主成分作为点对进行绘制，从而揭示聚类和异常值。
- en: 'Next, we plot the first two principal components. Each point is one speech,
    and we’ve colored the points according to the year of the speech. Points that
    are close together represent similar speeches, and points that are far away from
    one another represent dissimilar speeches:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们绘制了前两个主成分。每个点代表一个演讲，我们根据演讲的年份对点进行了颜色编码。靠近一起的点表示相似的演讲，而远离的点表示不同的演讲：
- en: '![](assets/leds_13in01.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_13in01.png)'
- en: We see a clear difference in speeches over time—speeches given in the 1800s
    used very different words than speeches given after 2000\. It’s also interesting
    to see that the speeches cluster tightly in the same time period. This suggests
    that speeches within the same period sound relatively similar, even though the
    speakers were from different political parties.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清晰地看到随时间变化的演讲之间存在明显的差异——19世纪的演讲使用了与21世纪后的演讲非常不同的词汇。同一时间段的演讲聚集在一起也是非常有趣的现象。这表明，同一时期的演讲在语言风格上相对相似，即使演讲者来自不同的政党。
- en: This section gave a whirlwind introduction to text analysis. We used text manipulation
    tools from previous sections to clean up the presidential speeches. Then we used
    more advanced techniques like stemming, the tf-idf transform, and principal component
    analysis to compare speeches. Although we don’t have enough space in this book
    to cover all of these techniques in detail, we hope that this section piqued your
    interest in the exciting world of text analysis.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 本节对文本分析进行了简要介绍。我们使用了前几节的文本操作工具来清理总统演讲稿。然后，我们采用了更高级的技术，如词干提取、tf-idf转换和主成分分析来比较演讲。尽管本书无法详细介绍所有这些技术，但我们希望这一部分能引起您对文本分析这一激动人心领域的兴趣。
- en: Summary
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced techniques for working with text to clean and analyze
    data, including string manipulation, regular expressions, and document analysis.
    Text data has rich information about how people live, work, and think. But this
    data is also hard for computers to use—think about all the creative ways people
    manage to spell the same word. The techniques in this chapter let us correct typos,
    extract features from logs, and compare documents.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了处理文本以清洁和分析数据的技术，包括字符串操作、正则表达式和文档分析。文本数据包含了关于人们生活、工作和思考方式的丰富信息。但这些数据对计算机来说也很难使用——想想人们为了表达同一个词而创造出的各种形式。本章的技术使我们能够纠正打字错误，从日志中提取特征，并比较文档。
- en: 'We don’t recommend you use regular expressions to:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不建议您使用正则表达式来：
- en: Parse hierarchical structures such as JSON or HTML; use a parser instead
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析层次结构，如JSON或HTML；请使用解析器
- en: Search for complex properties, like palindromes and balanced parentheses
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索复杂属性，如回文和平衡的括号
- en: Validate a complex feature, such as a valid email address
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证复杂功能，比如有效的电子邮件地址
- en: Regular expressions, while powerful, are terrible at these types of tasks. However,
    in our experience, even the basics of text manipulation can enable all sorts of
    interesting analyses—a little bit goes a long way.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式虽然功能强大，但在这类任务中表现很差。然而，根据我们的经验，即使是基本的文本处理技能也能实现各种有趣的分析——少量技巧也能带来长远影响。
- en: 'We have one final caution about regular expressions: they can be computationally
    expensive. You will want to consider the trade-offs between these concise, clear
    expressions and the overhead they create if they’re being put into production
    code.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还要特别提醒一下正则表达式：它们可能会消耗大量计算资源。在将它们用于生产代码时，您需要权衡这些简洁明了的表达式与它们可能带来的开销。
- en: The next chapter considers other sorts of data, such as data in binary formats,
    and the highly structured text of JSON and HTML. Our focus will be on loading
    these data into dataframes and other Python data structures.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论其他类型的数据，如二进制格式的数据以及JSON和HTML中高度结构化的文本。我们将重点放在将这些数据加载到数据框和其他Python数据结构中。
