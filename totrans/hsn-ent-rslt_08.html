<html><head></head><body><section data-pdf-bookmark="Chapter 8. Scaling Up on Google Cloud" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_8">&#13;
<h1><span class="label">Chapter 8. </span>Scaling Up on Google Cloud</h1>&#13;
&#13;
<p>In<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="scaling up on" data-type="indexterm" id="id542"/> this chapter, we will work through how to scale up our entity resolution process to enable us to match large datasets in reasonable timeframes. We will use a cluster of virtual machines running in parallel on Google Cloud Platform (GCP) to divide up the workload and reduce the time taken to resolve our entities.</p>&#13;
&#13;
<p>We will walk through how to register a new account on the Cloud Platform and how to configure the storage and compute services we will need. Once our infrastructure is ready, we will rerun our company matching example from <a data-type="xref" href="ch06.html#chapter_6">Chapter 6</a>, splitting both model training and entity resolution steps across a managed cluster of compute resources.</p>&#13;
&#13;
<p>Lastly, we will check that our performance is consistent and make sure we tidy up fully, deleting the cluster and returning the virtual machines we have borrowed to ensure we don’t continue to run up any additional fees.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id543">&#13;
<h1>Google Cloud Platform</h1>&#13;
&#13;
<p>Google Cloud Platform (GCP)<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="benefits of" data-type="indexterm" id="id544"/> is an integrated set of compute, storage, data, and analytics products. End users, from individuals to large enterprises, can rent capacity on these platforms. Amazon and Microsoft offer similar environments.</p>&#13;
&#13;
<p>The ability to temporarily hire computing facilities at an affordable rate, and without the large up-front cost of purchasing additional hardware, is ideal for development, testing, and educational purposes.</p>&#13;
&#13;
<p class="pagebreak-before">In production the scalable nature of these cloud platforms is ideal for “peaky” workloads, like batch entity resolution jobs, which periodically require a sizeable compute infrastructure for a short period of time before often sitting idle until the next scheduled job. Using a cloud provider you can set up your operations so that you pay only for the capacity you use, offering substantial savings against a poorly utilized standing IT estate.</p>&#13;
</div></aside>&#13;
&#13;
<section data-pdf-bookmark="Google Cloud Setup" data-type="sect1"><div class="sect1" id="id131">&#13;
<h1>Google Cloud Setup</h1>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="setup" data-type="indexterm" id="id545"/> build our cloud infrastructure, we first need to register for an account on the GCP. To do this, visit <em>cloud.google.com</em> on your browser. From here, you can click Get Started to begin the registration process. You’ll need to register with a Google email address or alternatively create a new account. This is shown in <a data-type="xref" href="#fig-8-1">Figure 8-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-1"><img alt="" class="iimagesch08ch08gcpsigninpng" src="assets/hoer_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>GCP sign in</h6>&#13;
</div></figure>&#13;
&#13;
<p>You’ll need to select your country, read and then accept the Google terms of service, and click Continue. See <a data-type="xref" href="#fig-8-2">Figure 8-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-2"><img alt="" class="iimagesch08ch08gcpaccountstep1png" src="assets/hoer_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Register for GCP, Account Information Step 1</h6>&#13;
</div></figure>&#13;
&#13;
<p>On the next page, you will be asked to verify your address and payment information before you can click Start My Free Trial.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Google Cloud Platform Fees</h1>&#13;
&#13;
<p>Please<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="fees" data-type="indexterm" id="id546"/> be warned that it’s your responsibility to understand the ongoing charges associated with using any of the products on the Google Cloud Platform. From personal experience I can say it is very easy to leave virtual machines running or overlook persistent disks that you will still be charged for.</p>&#13;
&#13;
<p>At the time of writing, Google Cloud is offering $300 credit for free to spend over the first 90 days of your usage of the platform. They are also stating that no autocharge will be applied after the free trial ends, so if you use a credit or debit card, you won’t be charged unless you manually upgrade to a paid account.</p>&#13;
&#13;
<p><em>Of course, these terms are subject to change, so please read the terms carefully when you sign up.</em></p>&#13;
</div>&#13;
&#13;
<p>Once you’ve signed up, you’ll be taken to the Google Cloud console.</p>&#13;
&#13;
<section data-pdf-bookmark="Setting Up Project Storage" data-type="sect2"><div class="sect2" id="id66">&#13;
<h2>Setting Up Project Storage</h2>&#13;
&#13;
<p>Your<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="project storage" data-type="indexterm" id="GCPpstor08"/> first task is to create a project. On GCP, a project is a logical group of resources and data that you manage. For the purpose of this book, all our work will be grouped together in one project.</p>&#13;
&#13;
<p>To begin, choose your preferred project name and Google will suggest a corresponding Project ID for you. You might wish to edit their suggestion to shorten or simplify it a little as you’ll potentially be typing in this Project ID a fair number of times.</p>&#13;
&#13;
<p>As an individual user, you don’t need to specify an organization owner of your project, as illustrated in <a data-type="xref" href="#fig-8-3">Figure 8-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-3"><img alt="" class="iimagesch08ch08createprojectidpng" src="assets/hoer_0803.png"/>&#13;
<h6><span class="label">Figure 8-3. </span>“Create a Project” dialog box</h6>&#13;
</div></figure>&#13;
&#13;
<p>Once you’ve created your project, you’ll be taken to the project dashboard.</p>&#13;
&#13;
<p>The first thing we need is somewhere to store our data on GCP. The standard data storage product is called Cloud Storage, and within that, specific data containers are called buckets. Buckets have a globally unique name and a geographic location where the bucket and its data contents are stored. A bucket can have the same name as your Project ID if you wish.</p>&#13;
&#13;
<p class="pagebreak-before">To create a bucket, you can click on the navigation menu home (three horizontal lines within a circle, top left of the screen) to select Cloud Storage and then Buckets from the drop-down navigation menu. <a data-type="xref" href="#fig-8-4">Figure 8-4</a> shows the menu options.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-4"><img alt="" class="iimagesch08ch08cloudstoragemenupng" src="assets/hoer_0804.png"/>&#13;
<h6><span class="label">Figure 8-4. </span>Navigation menu—Cloud Storage</h6>&#13;
</div></figure>&#13;
&#13;
<p>From here, click Create Bucket from the menu at the top, select your preferred name, and then click Continue. See <a data-type="xref" href="#fig-8-5">Figure 8-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-5"><img alt="" class="iimagesch08ch08createbucket1png" src="assets/hoer_0805.png"/>&#13;
<h6><span class="label">Figure 8-5. </span>Create bucket—naming</h6>&#13;
</div></figure>&#13;
&#13;
<p>Next you need to select your preferred storage location, as illustrated in <a data-type="xref" href="#fig-8-6">Figure 8-6</a>. For the purposes of this project, you can accept the default or pick a different region if you prefer.</p>&#13;
&#13;
<p>You can press Continue to view the remaining advanced configuration options or just jump straight to Create. Now that we have some storage space defined, our next step is to reserve some compute resources to run our entity resolution process.<a contenteditable="false" data-primary="" data-startref="GCPpstor08" data-type="indexterm" id="id547"/></p>&#13;
&#13;
<figure><div class="figure" id="fig-8-6"><img alt="" class="iimagesch08ch08createbucket2png" src="assets/hoer_0806.png"/>&#13;
<h6><span class="label">Figure 8-6. </span>Create bucket—data storage location</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Creating a Dataproc Cluster" data-type="sect1"><div class="sect1" id="id67">&#13;
<h1>Creating a Dataproc Cluster</h1>&#13;
&#13;
<p>As<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="Dataproc cluster creation" data-type="indexterm" id="GCPclcreat08"/><a contenteditable="false" data-primary="Dataproc clusters" data-secondary="creating" data-type="indexterm" id="DCcreat08"/><a contenteditable="false" data-primary="clustering" data-secondary="Dataproc cluster creation" data-type="indexterm" id="Cdpcreat08"/> in previous chapters, we will be using the Splink framework to perform matching. To scale up our process to run across multiple machines, we need to switch from using DuckDB as our backend database to Spark.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id548">&#13;
<h1>Spark</h1>&#13;
&#13;
<p>Apache Spark<a contenteditable="false" data-primary="Apache Spark" data-see="Spark" data-type="indexterm" id="id549"/><a contenteditable="false" data-primary="Spark" data-secondary="basics of" data-type="indexterm" id="id550"/> is an analytics engine optimized for executing data processes on clusters of machines. It is open source and maintained by the Apache Software Foundation.</p>&#13;
</div></aside>&#13;
&#13;
<p>A<a contenteditable="false" data-primary="Spark" data-secondary="Dataproc cluster creation" data-type="indexterm" id="Sclcreat08"/> convenient way to run Spark on the GCP is to use a <em>Dataproc cluster</em>, which takes care of creating a number of virtual machines and configuring them to execute a Spark job.</p>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Cloud Dataproc API" data-type="indexterm" id="id551"/> create a cluster, we must first enable the Cloud Dataproc API. Return to the navigation menu and select Dataproc and then Clusters as per <a data-type="xref" href="#fig-8-7">Figure 8-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-7"><img alt="" class="iimagesch08ch08createclustermenupng" src="assets/hoer_0807.png"/>&#13;
<h6><span class="label">Figure 8-7. </span>Navigation menu—Dataproc clusters</h6>&#13;
</div></figure>&#13;
&#13;
<p>You’ll then be presented with the API screen. Make sure you read and accept the terms and associated fees and then click Enable. See <a data-type="xref" href="#fig-8-8">Figure 8-8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-8"><img alt="" class="iimagesch08ch08gcpdataprocapipng" src="assets/hoer_0808.png"/>&#13;
<h6><span class="label">Figure 8-8. </span>Enable the Cloud Dataproc API</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Once the API is enabled, you can click on Create Cluster to configure your Dataproc instance. Dataproc clusters can be built directly on<a contenteditable="false" data-primary="Compute Engine virtual machines" data-type="indexterm" id="id552"/> Compute Engine virtual machines or via<a contenteditable="false" data-primary=" GKE (Google Kubernetes Engine)" data-type="indexterm" id="id553"/><a contenteditable="false" data-primary="Google Kubernetes Engine (GKE)" data-type="indexterm" id="id554"/> GKE (Google Kubernetes Engine). For the purposes of this example, the distinction between the two isn’t important, so I suggest you select Compute Engine as it is the simpler of the two.</p>&#13;
&#13;
<p>You should then be presented with the screen in <a data-type="xref" href="#fig-8-9">Figure 8-9</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-9"><img alt="" class="iimagesch08ch08createcluster1png" src="assets/hoer_0809.png"/>&#13;
<h6><span class="label">Figure 8-9. </span>Create cluster on Compute Engine</h6>&#13;
</div></figure>&#13;
&#13;
<p>Here you can name your cluster, select the location in which it resides, and choose the type of cluster. Next, scroll down to the Component section and select Component Gateway and Jupyter Notebook, as shown in <a data-type="xref" href="#fig-8-10">Figure 8-10</a>. This is important as it allows us to configure the cluster and use Jupyter to execute our entity resolution notebook.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-10"><img alt="" class="iimagesch08ch08createcluster1bpng" src="assets/hoer_0810.png"/>&#13;
<h6><span class="label">Figure 8-10. </span>Dataproc components</h6>&#13;
</div></figure>&#13;
&#13;
<p>Once you’ve configured the Components, you can accept the default settings for the rest of this page—see <a data-type="xref" href="#fig-8-11">Figure 8-11</a>—and then select the Configure Nodes option.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-11"><img alt="" class="iimagesch08ch08createcluster2png" src="assets/hoer_0811.png"/>&#13;
<h6><span class="label">Figure 8-11. </span>Configure worker nodes</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">The next step is to configure both the Manager and Worker nodes within our cluster. Again, you can accept the defaults, checking that the number of workers is set to 2 before moving on to Customize Cluster.</p>&#13;
&#13;
<p>A final step, but an important one, is to consider scheduling deletion of the cluster to avoid any ongoing<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="fees" data-type="indexterm" id="id555"/> fees should you forget to remove your<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="deleting clusters" data-type="indexterm" id="id556"/> cluster manually when you’re finished with it. I’d also recommend configuring the Cloud Storage staging bucket to use the bucket you created earlier; otherwise the Dataproc process will create a storage bucket for you that can easily get left behind in the clean-up operation. See <a data-type="xref" href="#fig-8-12">Figure 8-12</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-12"><img alt="" class="iimagesch08ch08createcluster3png" src="assets/hoer_0812.png"/>&#13;
<h6><span class="label">Figure 8-12. </span>Customize cluster—deletion and staging bucket</h6>&#13;
</div></figure>&#13;
&#13;
<p>Finally, click Create to instruct GCP to create the cluster for you. This will take a few moments.<a contenteditable="false" data-primary="" data-startref="Sclcreat08" data-type="indexterm" id="id557"/><a contenteditable="false" data-primary="" data-startref="GCPclcreat08" data-type="indexterm" id="id558"/><a contenteditable="false" data-primary="" data-startref="DCcreat08" data-type="indexterm" id="id559"/><a contenteditable="false" data-primary="" data-startref="Cdpcreat08" data-type="indexterm" id="id560"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Configuring a Dataproc Cluster" data-type="sect1"><div class="sect1" id="id68">&#13;
<h1>Configuring a Dataproc Cluster</h1>&#13;
&#13;
<p>Once<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="Dataproc cluster configuration" data-type="indexterm" id="GCPconfig08"/><a contenteditable="false" data-primary="Dataproc clusters" data-secondary="configuring" data-type="indexterm" id="DCconfig08"/><a contenteditable="false" data-primary="Spark" data-secondary="Dataproc cluster configuration" data-type="indexterm" id="Sdpclconfig08"/><a contenteditable="false" data-primary="clustering" data-secondary="Dataproc cluster configuration" data-type="indexterm" id="Cdpconfig08"/> the basic cluster is up and running, we can connect to it by clicking on the cluster name and then selecting Jupyter from the Web Interfaces section shown in <a data-type="xref" href="#fig-8-13">Figure 8-13</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-13"><img alt="" class="iimagesch08ch08configurecluster1png" src="assets/hoer_0813.png"/>&#13;
<h6><span class="label">Figure 8-13. </span>Cluster web interfaces—Jupyter</h6>&#13;
</div></figure>&#13;
&#13;
<p>This will launch a familiar Jupyter environment in a new browser window.</p>&#13;
&#13;
<p>Our next task is to download and configure the software and data we need. From the New menu, select Terminal to bring up a command prompt in a second browser window. Switch to the home directory:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;cd /home</strong></pre>&#13;
&#13;
<p>Then clone the repository from the GitHub repo and switch into the newly created directory:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;git clone https://github.com/mshearer0/handsonentityresolution</strong>&#13;
&#13;
<strong>&gt;&gt;&gt;cd handsonentityresolution</strong></pre>&#13;
&#13;
<p>Next, return to the Jupyter environment and open the <em>Chapter6.ipynb</em> notebook. Run the data acquisition and standardization sections of the notebook to re-create the clean Mari and Basic datasets.</p>&#13;
&#13;
<p class="pagebreak-before">Edit the “Saving to Local Storage” section to save the files to:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_c.to_csv('/home/handsonentityresolution/basic_clean.csv')&#13;
&#13;
df_m.to_csv('/home/handsonentityresolution/mari_clean.csv',&#13;
   index=False)</pre>&#13;
&#13;
<p>Now that we have reconstructed our datasets, we need to copy them to the Cloud Storage bucket we created earlier so that they are accessible to all the nodes in our cluster. We do this at the terminal with:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;gsutil cp /home/handsonentityresolution/* gs://&lt;<em>your&#13;
   bucket</em>&gt;/handsonentityresolution/</strong></pre>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Note: Remember to substitute your bucket name!</p>&#13;
</div>&#13;
&#13;
<p>This will create the directory <em>handsonentityresolution</em> in your bucket and copy the GitHub repository files across. You’ll need these for this chapter and the next one.</p>&#13;
&#13;
<p>Next<a contenteditable="false" data-primary="Splink" data-secondary="installing" data-type="indexterm" id="id561"/> we need to install Splink:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;pip install splink</strong></pre>&#13;
&#13;
<p>Previously, we relied on the approximate string matching functions, like Jaro-Winkler, that were built into DuckDB. These routines aren’t available by default in Spark, so we need to download and install a Java ARchive (JAR) file containing these user-defined functions (UDFs) that Splink will call:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
​<strong>&gt;&gt;&gt;wget https://github.com/moj-analytical-services/&#13;
   splink_scalaudfs/raw/spark3_x/jars/scala-udf-similarity-&#13;
   0.1.1_spark3.x.jar</strong></pre>&#13;
&#13;
<p>Again, we copy this file into our bucket so that these functions are available to the cluster worker nodes:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;gsutil cp /home/handsonentityresolution/*.jar&#13;
   gs://&lt;<em>your bucket</em>&gt;/handsonentityresolution/</strong></pre>&#13;
&#13;
<p>To tell our cluster where to pick up this file on startup, we need to browse to the <em>spark-defaults.conf</em> file in Jupyter at path <em>/Local Disk/etc/spark/conf.dist/ </em> and add the following line, remembering to substitute your bucket name:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
spark.jars=gs://&lt;<em>your_bucket</em>&gt;/handsonentityresolution/&#13;
   scala-udf-similarity-0.1.1_spark3.x.jar</pre>&#13;
&#13;
<p>To activate this file you need to close your Jupyter windows, return to the cluster menu, and then STOP and START your cluster.<a contenteditable="false" data-primary="" data-startref="DCconfig08" data-type="indexterm" id="id562"/><a contenteditable="false" data-primary="" data-startref="GCPconfig08" data-type="indexterm" id="id563"/><a contenteditable="false" data-primary="" data-startref="Sdpclconfig08" data-type="indexterm" id="id564"/><a contenteditable="false" data-primary="" data-startref="Cdpconfig08" data-type="indexterm" id="id565"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Entity Resolution on Spark" data-type="sect1"><div class="sect1" id="id69">&#13;
<h1>Entity Resolution on Spark</h1>&#13;
&#13;
<p>Finally, we<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="entity resolution on Spark" data-type="indexterm" id="GCPent08"/><a contenteditable="false" data-primary="Spark" data-secondary="entity resolution on" data-type="indexterm" id="Sentres08"/><a contenteditable="false" data-primary="entity resolution" data-secondary="on Spark" data-secondary-sortas="Spark" data-type="indexterm" id="ERspark08"/> are ready to begin our matching process. Open <em>Chapter8.ipynb</em> in Jupyter Notebook.</p>&#13;
&#13;
<p>To begin, we load the data files that we saved to our bucket earlier into pandas DataFrames:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_m = pd.read_csv('gs://&lt;<em>your bucket</em>&gt;/&#13;
   handsonentityresolution/mari_clean.csv')&#13;
df_c = pd.read_csv('gs://&lt;<em>your bucket</em>&gt;/&#13;
   handsonentityresolution/basic_clean.csv')</pre>&#13;
&#13;
<p>Next we configure our<a contenteditable="false" data-primary="Splink" data-secondary="configuring" data-type="indexterm" id="id566"/> Splink settings. These are a little different from the settings we used with the DuckDB backend:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
from pyspark import SparkContext, SparkConf&#13;
from pyspark.sql import SparkSession&#13;
from pyspark.sql import types&#13;
&#13;
conf = SparkConf()&#13;
conf.set("spark.default.parallelism", "240")&#13;
conf.set("spark.sql.shuffle.partitions", "240")&#13;
&#13;
sc = SparkContext.getOrCreate(conf=conf)&#13;
spark = SparkSession(sc)&#13;
spark.sparkContext.setCheckpointDir("gs://&lt;<em>your bucket</em>&gt;/&#13;
    handsonentityresolution/")&#13;
&#13;
spark.udfspark.udf.registerJavaFunction(&#13;
   "jaro_winkler_similarity",&#13;
   "uk.gov.moj.dash.linkage.JaroWinklerSimilarity",&#13;
   types.DoubleType())</pre>&#13;
&#13;
<p>First, we import <code>pyspark</code> functions that allow us to create a new Spark session from Python. Next, we set the configuration parameters to define the amount of parallel processing we want. Then we create the <code>SparkSession</code> and set a <code>Checkpoint</code> directory that Spark uses as a temporary store.</p>&#13;
&#13;
<p>Lastly, we register a new Java function so that Splink can pick up the Jaro-Winkler similarity routine from the JAR file we set up earlier.</p>&#13;
&#13;
<p>Next we need to set up a Spark schema that we can map our data onto:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
from pyspark.sql.types import StructType, StructField, StringType, IntegerType&#13;
&#13;
schema = StructType(&#13;
   [StructField("Postcode", StringType()),&#13;
    StructField("CompanyName", StringType()),&#13;
    StructField("unique_id", IntegerType())]&#13;
)</pre>&#13;
&#13;
<p>Then we can create Spark DataFrames (<code>dfs</code>) from the pandas DataFrames (<code>df</code>) and the schema we have just defined. As both datasets have the same structure, we can use the same schema:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
dfs_m = spark.createDataFrame(df_m, schema)&#13;
dfs_c = spark.createDataFrame(df_c, schema)</pre>&#13;
&#13;
<p>Our next step is to configure Splink. These settings are the same as we used in <a data-type="xref" href="ch06.html#chapter_6">Chapter 6</a>:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
import splink.spark.comparison_library as cl&#13;
&#13;
settings = {&#13;
   "link_type": "link_only",&#13;
   "blocking_rules_to_generate_predictions": [ "l.Postcode = r.Postcode",&#13;
   "l.CompanyName = r.CompanyName", ],&#13;
   "comparisons": [ cl.jaro_winkler_at_thresholds("CompanyName",[0.9,0.8]), ],&#13;
   "retain_intermediate_calculation_columns" : True,&#13;
   "retain_matching_columns" : True&#13;
}</pre>&#13;
&#13;
<p>Then we set up a <code>SparkLinker</code> using the Spark DataFrames and settings we have created:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
from splink.spark.linker import SparkLinker&#13;
linker = SparkLinker([dfs_m, dfs_c], settings, input_table_aliases=&#13;
["dfs_m", "dfs_c"])</pre>&#13;
&#13;
<p>As in <a data-type="xref" href="ch06.html#chapter_6">Chapter 6</a>, we train the <em>u</em> and <em>m</em> values using random sampling and the expectation-maximization algorithm, respectively:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
linker.estimate_u_using_random_sampling(max_pairs=5e7)&#13;
linker.estimate_parameters_using_expectation_maximisation&#13;
   ("l.Postcode = r.Postcode")</pre>&#13;
       &#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>This is where we begin to see the benefit of switching to Spark. Whereas model training previously took over an hour, now it is completed in only a few minutes.</p>&#13;
</div>&#13;
&#13;
<p>Alternatively, you<a contenteditable="false" data-primary="Splink" data-secondary="pretrained models for" data-type="indexterm" id="id567"/> can load a pretrained model, <em>Chapter8_Splink_Settings.json,</em> from the repository:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
linker.load_model("&lt;<em>your_path</em>&gt;/Chapter8_Splink_Settings.json")</pre>&#13;
&#13;
<p>We can then run our predictions and get our<a contenteditable="false" data-primary="" data-startref="GCPent08" data-type="indexterm" id="id568"/><a contenteditable="false" data-primary="" data-startref="ERspark08" data-type="indexterm" id="id569"/><a contenteditable="false" data-primary="" data-startref="Sentres08" data-type="indexterm" id="id570"/> results:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_pred = linker.predict(threshold_match_probability=0.1)&#13;
   .as_pandas_dataframe()&#13;
len(df_pred)</pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Measuring Performance" data-type="sect1"><div class="sect1" id="id132">&#13;
<h1>Measuring Performance</h1>&#13;
&#13;
<p>As<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="measuring performance" data-type="indexterm" id="id571"/><a contenteditable="false" data-primary="performance, measuring" data-secondary="Spark and Google Cloud Platform" data-type="indexterm" id="id572"/><a contenteditable="false" data-primary="Spark" data-secondary="measuring performance" data-type="indexterm" id="id573"/> expected, switching to Spark doesn’t substantially change our results. At a 0.1 match threshold we have 192 matches. Our results are shown in <a data-type="xref" href="#table-8-1">Table 8-1</a>.</p>&#13;
&#13;
<table id="table-8-1">&#13;
	<caption><span class="label">Table 8-1. </span>MCA match results (Spark)—low threshold</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th scope="col"><strong>Match threshold = 0.1</strong></th>&#13;
			<th scope="col"><strong>Number of matches</strong></th>&#13;
			<th scope="col"><strong>Unique entities matched</strong></th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>Name and postcode match</td>&#13;
			<td>47</td>&#13;
			<td>45</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Name match only</td>&#13;
			<td>37</td>&#13;
			<td>31</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Postcode match only</td>&#13;
			<td>108</td>&#13;
			<td>27</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>Total matches</strong></td>&#13;
			<td><b>192</b></td>&#13;
			<td><strong>85 (deduped)</strong></td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Unmatched</td>&#13;
			<td> </td>&#13;
			<td>11 (of which 2 dissolved)</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>Total organizations</strong></td>&#13;
			<td> </td>&#13;
			<td><strong>96</strong></td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>This gives a slight improvement in precision and accuracy, due to slight variation in the calculated model parameters.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Tidy Up!" data-type="sect1"><div class="sect1" id="id133">&#13;
<h1>Tidy Up!</h1>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="deleting clusters" data-type="indexterm" id="id574"/><a contenteditable="false" data-primary="Spark" data-secondary="deleting clusters" data-type="indexterm" id="id575"/> ensure you aren’t charged for continuing to run the virtual machines and their disks, make sure you <em>DELETE your cluster (not just STOP, which will continue to accrue disk fees) from the Cluster menu</em>.</p>&#13;
&#13;
<p>You<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="project storage" data-type="indexterm" id="id576"/> may wish to retain the files in your Cloud Storage bucket for use in the following chapter. However, make sure to delete any staging or temporary buckets if these have been created, as shown in <a data-type="xref" href="#fig-8-14">Figure 8-14</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-8-14"><img alt="" class="iimagesch08ch08deletestagingbucketpng" src="assets/hoer_0814.png"/>&#13;
<h6><span class="label">Figure 8-14. </span>Delete staging and temporary buckets</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id256">&#13;
<h1 class="less_space">Summary</h1>&#13;
&#13;
<p>In this chapter, we learned how to scale up our entity resolution process to run on multiple machines. This gives us the ability to match larger datasets than we could cope with on a single machine, or in a reasonable execution timeframe.</p>&#13;
&#13;
<p>Along the way we’ve seen how to use Google Cloud Platform to provision compute and storage resources that we can use on demand and pay only for the bandwidth we need.</p>&#13;
&#13;
<p>We’ve also seen that even with a relatively straightforward example there is a large amount of configuration work we need to do before we can run our entity resolution process. In the next chapter, we will take a look at how the cloud providers provide APIs that offer to abstract away much of this complexity.</p>&#13;
</div></section>&#13;
</div></section></body></html>