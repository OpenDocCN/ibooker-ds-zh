- en: Chapter 14\. Artificial Intelligence, Ethics, Mathematics, And Policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Torture the data enough and it will confess to anything*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nobel Laureate and economist Ronald Coase (1910-2013)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'AI ethics is a wide and deep topic, and it is emerging as a new area at the
    intersection of the philosophy and AI fields. We can only scratch the surface
    in this chapter, highligting some issues and possible ways to address them, but
    leaving many equally important ones outside. Nevertheless, this chapter has a
    message that I don’t want the reader to miss:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We need more of us to be situated in both AI and policy.*'
  prefs: []
  type: TYPE_NORMAL
- en: In my learning journey from math to its applications in AI, I discovered that
    AI should not be disentangled from policy, and that the two should evolve together.
    I can sit and write about the million examples where there are ethical considerations
    associated with AI technology, such as data security, privacy, surveillance, democracy,
    freedom of expression, workforce considerations, equity, fairness, bias, discrimination,
    inclusivity, transparency, regulation, and weaponized AI, but this is not how
    I will approach this subject. My take on these issues is from a slightly different
    angle, where I have seen first hand how people try new weapons on populations
    in war torn areas, and yet the governments and the media either deny, do not comment,
    or say the unfortunate events were mistakes, that they will be investigated, then
    all move on to better things. When there is a new technology that affects people
    at a scale, the people developing the technology are the ones most qualified to
    know its ramifications, both good and bad. So they are the ones who should collaborate
    directly with policy makers to regulate its usage. Moreover, if there is a technology
    or otherwise an event that causes a massive disruption to society, we can thrust
    people into thinking, writing, and complying with policy. The massive disruption
    is not AI per se, or the amount of data that humans currently produce and own
    such as the data owned by Facebook, by Nasa surveys of space, the Human Genome
    project, or our Apple Watches, but it is the money that is invested into this
    technology, and more importantly, the public attention.
  prefs: []
  type: TYPE_NORMAL
- en: I was living in a little and perfect math bubble, where things can only be black
    and white, logical and correct, and if we don’t understand how some math works,
    we can always convince ourselves that we can learn it if we just spend a bit more
    time on it. What opened my eyes was working with our city’s Fire Department and
    Transportation Department. When my students were presenting at City Hall to city
    officials, public safety leaders, and policy makers, I realized that we, as technology
    specialists working with their data, had the power to tell them that our math
    models could do anything, whether these models did that or not. This realization
    was very scary for me. I am not a policy person by training, I am a math person,
    but I decided that I must go into policy. I inserted myself into small policy
    making venues to build up some policy expertise (re-doing Hiring Policy at my
    university; chairing College Council; chairing the Academic Policies committee;
    sitting on my university’s steering committee; running a data, policy and diplomacy
    class; developing a summer program in Europe on human security, technology, and
    entrepreneurship in the face of modern warfare; and giving talks and workshops
    on the subject).
  prefs: []
  type: TYPE_NORMAL
- en: I have learned that policy is not like math, there are a lot of grey areas and
    conflicting interests, and treading its treacherous waters is a different game.
    I learned about the complexity of establishing new policies and their intersections
    with existing policies. This is not unlike an AI system, where constant updates
    and consistency are of paramount importance, while at the same time staying efficient
    and not working ourselves and our systems down to a paralysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We must strive for concise and specific policies. Any technology with the potential
    to affect millions, must be developed, by its own experts, with awareness and
    an attitude similar to that of emergency response teams, thinking of worst case
    scenarios and guarding against them. The current state is that the world’s leading
    technology companies are accelerating humanity towards a new connected and AI
    powered world, while policy and regulation are playing catch up. AI, however,
    is still maturing, so now is an ideal time to design policies that gears it towards
    the public good. Technological development is not some random thing that just
    happens to us. We should be more than passive participants, recepients, or consumers,
    especially that we *ourselves* are the data: Our internet habits, our social media
    posts, our banking transactions, our medical records, our blood tests, our MRI
    scans, our grocery store runs, our Uber rides, our home thermostat preferences,
    our video game skills, our bus rides, our Apple Watch step and heartrate counts,
    our driving break and accelerate patterns, our entire lives. These are digitized
    and stored in data warehouses in some random buildings in random locations. Unlike
    financial data that goes into our FICO credit score, which is heavily regulated,
    most of today’s digital data is unregulated. One company can sell it to another,
    with all its inaccuracies, and the new company will build models and make decisions
    based on this unregulated data. Is someone’s driving habits affecting whether
    they get into a certain college somewhere? Or deciding on the pricing the premiums
    of their medical insurance? How about their daily commute that passes through
    a less affluent neighborhood? How about that minor offense that was cleared from
    someone’s record a decade ago? Did it get cleared from all datasets, including
    those that were sold to other companies years ago? Is that still affecting life
    changing and livelihood decisions such as loans, college accepatances, insurance
    premiums, job offers? Who knows? It is unregulated. When we opt into sharing our
    data with one company, are there laws that prohibit sharing or reselling this
    data to other companies for other uses?'
  prefs: []
  type: TYPE_NORMAL
- en: We can use our massive digital data for good, but we cannot bank on that without
    smart and effective policy and regulation.
  prefs: []
  type: TYPE_NORMAL
- en: Good AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Good AI should be trustworthy enough to be deployed and used in the public
    and private sectors. There is a tendency in the field to spend a lot of time defining
    terms such as explainability, interpretability (apparently these two are different),
    fairness, equity, and many others. I see this hyperfocus on vocabulary as a distraction.
    The end goal is more important:'
  prefs: []
  type: TYPE_NORMAL
- en: '*We need to trust our systems and make them accessible and understandable to
    those who need to use them*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we need our AI and the data that it serves and is built on top of
    to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Secure**: We have to keep maintaining and updating the physical and software
    security protocols as our systems evolve. Cloud computing have introduced a new
    layer of security requirememnts since nowadays neither our data nor the computations
    happen anywhere in the vicinity of our local machines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Private**: There are formal privacy notions and standards that are already
    in place for many application sectors. There is a lot more to be done in terms
    of who owns the data and for what purpose it can be used by an AI system. My addition
    here is transparency and information sharing: When we are transparent about what
    our system intends to do with certain data, such as medical data to discover new
    drugs, or to create personalized treatment plans, people may opt into share their
    data. Right now there is a culture of hesitation and mistrust between technology
    producers and technology consumers. We can amend this by spreading the knowledge
    and sharing the end goals and both successful and unsuccessful results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accomplishes what it is built for and what it claims to do**: There are formal
    methods that can check whether code is correct or not, but we need more in terms
    of continuous testing of the system, including edge cases, and being transparent
    with the system’s capabilities, limitations, and untested territories.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Robust to perturbations and noise**: Small perturbations to the input should
    not produce large changes in the output. When decision making relies on the predictions
    of an AI system, these predictions cannot be arbitrary. The AI system should be
    tolerant to noise in its inputs and that tolerance must be quantified.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficient**: Efficiency for AI systems should go without saying- They are
    founded on the promise of speed, automation, and their ability to manage large
    scale computations, taking into account more contributing variables than was ever
    possible before. We need to continue to improve existing systems and attend to
    those that work in theory but are not yet efficient for real world deployment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fair**: Many systems rely on biased data which goes down the pipeline then
    gets manifested with unfair decisions. Identifying biases in data and undoing
    them is a first step in the fairness direction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accessible and understandable to many users**: When a new technology is beneficial
    to the society it needs to be made accessible and easy to use and understand.
    Intentional efforts should be made to industrialize it, commercialize it, and
    address access issues to society sectors or communities who are disadvantaged.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transparent**: Transparency with data sources, models’ capabilities, use
    cases, limitations, and documentation is paramount. People usually have more tolerance
    with faulty systems when this information is continuously and clearly communicated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Policy Matters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI policy is starting to take shape. It is aimed towards harnessing and maximizing
    AI’s benefits while guarding against its potential harms.
  prefs: []
  type: TYPE_NORMAL
- en: Policy matters and makes a difference. One example is ClearviewAI and its issues
    with privacy. Clearview AI is the US company that created and sold to private
    companies a facial recognition software using a database of billions of personal
    photos downloaded from the web. Recently (May 2022), it settled a lawsuit, agreeing
    to comply with the state of Illinois’s privacy laws that give people control over
    their biometric data. ClearviewAI will restrict its facial identification technology
    primarily to law enforcement and other government agencies.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is Hikvision and its issues with surveillance. Hikvision is
    a Chinese company that manufactures millions of video surveillance cameras used
    in more than 190 countries, for purposes ranging from police surveillance systems
    to baby monitors. The company is now facing sanctions from the US government due
    to its close ties with the Chinese government. Hikvision played a role building
    China’s massive police surveillance system that the Chinese government used to
    oppress the Muslim minority groups in Xinjiang. The US Treasury is currently considering
    adding Hikvision to the Specially Designated Nationals and Blocked Persons List,
    which prohibits whomever or whatever is on this list from doing business with
    the US government, Americans, or US companies. Moreover, the assets of these entities
    or individuals are blocked by the US.
  prefs: []
  type: TYPE_NORMAL
- en: 'For organized efforts towards AI policy, one can look at governmental, intergovernmental,
    and global governance of AI initiatives (for trade, jobs, and geopolitical changes)
    that are taking shape in this direction: The United States’ National Artificial
    Intelligence Initiative, The EU’s Draft AI Ethics Guidelines, UAE’s Ministry of
    Artificial Intelligence, The Alan Turing Institute in the UK, Canada’s CIFAR Chairs
    in AI Program, Denmark’s Technology Pact, Japan’s Industrialization Roadmap, France’s
    Health Data Hub, Germany’s Ethics Commission on Automated and Connected Driving,
    India’s #AIforAll Strategy, China’s Global Governance of AI Plan, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can categorize AI related policies into:'
  prefs: []
  type: TYPE_NORMAL
- en: Investment into AI research and into training the workforce;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standards and regulation;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building solid and secure infrastructures of digital data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investment in development of skills and in the industrialization of technologies
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Government agencies are allocating funding for AI research, new AI institutions,
    workforce training and early Science, Technology, Engineering and Math (STEM)
    education, lifelong learning, and technology development. Governments are also
    encouraging the industrialization of AI technologies and private sector uptake.
    Moreover, governments themselves are investing in data driven initiatives and
    AI in their various departments, for public adminisration reform, and in order
    to make their operations more efficient and centralized (AI in the Government).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Regulations and Standards
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Regulations and standards include those for data security and usage, automotive
    AI such as self driving cars, and weaponized AI.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Data And Digital Infrastructure
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: High quality data is central to the ability of AI to work as intended. Governments
    are encouraging open datasets and developing platforms for the secure exchange
    of private data. There are also intentional efforts to remove bias from AI algorithms
    and data sets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What Could Go Wrong?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When designing a new system or analyzing an existing one, one of our guiding
    questions must be: What could go wrong? With this comes a list of check points:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the system intended to do?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data did it train on? How was the data collected? How were the noise and
    missing values dealt with?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who can be mostly underrepresented within the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What algorithms does it use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the algorithms’ thresholds for decision making?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given these thresholds, who can be harmed the most by these algorithmic decision?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we sample few examples (among many) that highlight the things
    that can go wrong and we must either guard against or try to standarize and regulate.
  prefs: []
  type: TYPE_NORMAL
- en: From Math To Weapons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One goal of this book is to highlight the mathematical foundations of AI models.
    The transition from math and to weapons is not new, given the development history
    of many weapons (e.g. the atomic bomb). This contribution is not only in one direction:
    Military and defense strategies and goals have influenced the development of entire
    math fields, such as *dynamic programming* which intially addressed military scheduling
    for training or logistics, and optimizing allocation of various resources.'
  prefs: []
  type: TYPE_NORMAL
- en: The book [*Weapons of Math Destruction (2017)*](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418831/)
    goes beyond military weaponization and lists with example after example the many
    harmful ways the mathematical algorithms that our society currently relies on
    for highly consequential and life altering decisions. The first few paragraphs
    of the book’s last chapter are worth quoting in full, since they reveal the intricate
    ways the algorithms deployed in seemingly different sectors interact with each
    other and influence each others outcomes. They also reveal how the exact same
    algorithms affect different populations in drastically different ways.
  prefs: []
  type: TYPE_NORMAL
- en: '*[…] we’ve visited school and college, the courts and the workplace, even the
    voting booth. Along the way, we’ve witnessed the destruction caused by Weapons
    of Math Destruction. Promising efficiency and fairness, they distort higher education,
    drive up debt, spur mass incarceration, pummel the poor at nearly every juncture,
    and undermine democracy. It might seem like the logical response is to disarm
    these weapons, one by one. The problem is that they’re feeding on each other.
    Poor people are more likely to have bad credit and live in high-crime neighborhoods,
    surrounded by other poor people. Once the dark universe of Weapons of Math Destruction
    digests that data, it showers them with predatory ads for subprime loans or for-profit
    schools. It sends more police to arrest them, and when they’re convicted it sentences
    them to longer terms. This data feeds into other Weapons of Math Destruction,
    which score the same people as high risks or easy targets and proceed to block
    them from jobs, while jacking up their rates for mortgages, car loans, and every
    kind of insurance imaginable. This drives their credit rating down further, creating
    nothing less than a death spiral of modeling. Being poor in a world of Weapons
    of Math Destruction is getting more and more dangerous and expensive.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The same Weapons of Math Destruction that abuse the poor also place the comfortable
    classes of society in their own marketing silos. They jet them off to vacations
    in Aruba and wait-list them at Wharton. For many of them, it can feel as though
    the world is getting smarter and easier. Models highlight bargains on prosciutto
    and chianti, recommend a great movie on Amazon Prime, or lead them, turn by turn,
    to a café in what used to be a “sketchy” neighborhood. The quiet and personal
    nature of this targeting keeps society’s winners from seeing how the very same
    models are destroying lives, sometimes just a few blocks away.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the math is correct and exactly the same for both sectors of the
    society, but what changed is the input to the model. Recall that if we wanted
    to sum this whole book into one math sentence, it would be: The features of the
    input to an AI model determine the final output. Poor and rich populations, for
    lack of better terms, have different features, so they get different outcomes.
    Our algorithms are fair in this sense, computing exactly what they are supposed
    to compute. I am not a fan of presenting problem without proposing solutions,
    or at least ideas for solutions. Maybe an initial way to improve the current situation
    is to train our algorithms separately using data from different groups of populations,
    so that a person’s poverty will not be a contributing factor in the algorithms’
    decision about their trustworthiness to pay back a certain loan, but other real
    factors will be.'
  prefs: []
  type: TYPE_NORMAL
- en: Chemical Warfare Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The destructive potential of AI models can manifest itself even with the models
    that are geared towards the utmost benefit to humanity: Generative AI models for
    drug discovery. The ease with which bad actors can misuse the models is alarming.
    All a bad actor needs to do is to learn how the model works: First the model maps
    the structure of a molecule to the way it acts in the body, then it optimizes
    for those molecules that maximize benefit and minimize toxicity. A bad actor can
    retrain the model, reversing its optimization objective from minimizing toxicity
    to maximizing toxicity. Mathematically, this is as simple as reversing the sign
    of the objective function in an optimization problem. This the point that Fabio
    Urbina and his colleagues at Collaborations Pharmaceuticals [recently highlighted](https://www.chemistryworld.com/news/drug-discovery-ai-that-developed-new-nerve-agents-raises-difficult-questions/4015462.article)
    about their work. To make this point, the team retrained their model with this
    *malicious* objective. In only six hours, the model generated 40,000 toxins, some
    of them actual chemical warfare agents that weren’t in the initial dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to conclude here that we need to be intentional, deliberate, introspective,
    and all kinds of adjectives on how to guard against this, without explicitly clarifying
    how, because the reality is that this is a complex issue. But how do we guard
    against this? My personal opinion here is we should approach this the same way
    we guard against mass destruction weapons in the non-AI world. No one can guarantee
    that bad players will not get their hands on the technology, but our job is to
    make it very difficult for them to develop it into deployable weapons.
  prefs: []
  type: TYPE_NORMAL
- en: AI And Politics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The role of TikTok, Facebook, and other social media platforms in politics is
    hard to overstate. They have already affected election results and overturned
    governments. Bots can generate fake news, fake history, fake reviews, fake comments,
    fake pages, fake tweets, and spread misinformation for political purposes. There
    are ways social media companies are trying to battle this problem, with multi-faceted
    approaches utilizing both machine learning to detect fraud or identify nodes spreading
    misinformation, employing third party fact checking organizations, working on
    better ranking algorithms for users’ news feeds, and other ways, with mixed results
    due to the scale at which these companies operate at, and sometimes due to the
    conflict of interest between the companies’ profitable objectives and its ethics
    departments. Personalized political campaigns, where the same politician caters
    to different ideologies based on who their targeted audience is, without the audience
    ever knowing that this is the case, is a real danger than can undermine democracies.
    Moreover, based on new information whether a certain state is swinging to the
    left or to the right, more funds can be allocated to target voters (again with
    personalized news feeds, political ads catering only to their preferred views
    based on their historical preferences along with those of their friends), to swing
    their votes in highly competitive battlegrounds. This can happen in real time,
    and affect the outcomes of entire elections. This has always been the case in
    politics, but again, in the digital era, this happens at scale, in real time,
    and with relatively no effort more than targeted deployment of algorithms backed
    with a giant database of our preferences and what makes us tick, click, pay, volunteer,
    or elect,
  prefs: []
  type: TYPE_NORMAL
- en: Unintended Outcomes of Generative Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large generative language models and text to image models are trained on internet
    scale data that inherits internet scale social biases, discrimination, and harmful
    content. This is best illustrated with [Imagen](https://imagen.research.google)’s
    section on the limitations of their text to image model generating high resolution
    images from text captions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[…] the data requirements of text-to-image models have led researchers to
    rely heavily on large, mostly uncurated, web-scraped datasets. While this approach
    has enabled rapid algorithmic advances in recent years, datasets of this nature
    often reflect social stereotypes, oppressive viewpoints, and derogatory, or otherwise
    harmful, associations to marginalized identity groups. While a subset of our training
    data was filtered to removed noise and undesirable content, such as pornographic
    imagery and toxic language, we also utilized LAION-400M dataset which is known
    to contain a wide range of inappropriate content including pornographic imagery,
    racist slurs, and harmful social stereotypes. Imagen relies on text encoders trained
    on uncurated web-scale data, and thus inherits the social biases and limitations
    of large language models. As such, there is a risk that Imagen has encoded harmful
    stereotypes and representations, which guides our decision to not release Imagen
    for public use without further safeguards in place. […] Imagen, may run into danger
    of dropping modes of the data distribution, which may further compound the social
    consequence of dataset bias. Imagen exhibits serious limitations when generating
    images depicting people. Our human evaluations found Imagen obtains significantly
    higher preference rates when evaluated on images that do not portray people, indicating
    a degradation in image fidelity. Preliminary assessment also suggests Imagen encodes
    several social biases and stereotypes, including an overall bias towards generating
    images of people with lighter skin tones and a tendency for images portraying
    different professions to align with Western gender stereotypes. Finally, even
    when we focus generations away from people, our preliminary analysis indicates
    Imagen encodes a range of social and cultural biases when generating images of
    activities, events, and objects. We aim to make progress on several of these open
    challenges and limitations in future work.*'
  prefs: []
  type: TYPE_NORMAL
- en: How To Fix It?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Awareness of harmful, biased, unfair, intrusive, and weaponized AI has risen
    in the past few years and efforts are ongoing to address these issues. The following
    are examples of such efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Underrepresentation In Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One theme that keeps repeating itself is the quality of the data that goes into
    training an AI model. Many biases appear because of the underrepresentation of
    nondominant groups, their cultural values or languages, in large data sets. For
    AI to benefit everyone, one solution is to ensure that the data is labeled by
    its own people. For example, the Intelligent Voices Of Wisdom AI project (which
    has now ended) led a data labeling workshop (in 2021) where Native Americans relabeled
    images related to their culture. Many of these images had been wrongly labeled
    by machine learning classification models. They also created a knowledge graph
    of native culinary techniques along with a chatbot to query the knowledge graph.
    Along with such efforts, AI can help preserve cultures, hsitory, and languages
    that are about to go extinct.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Bias In Word Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One first step in natural language processing is converting a language’s symbols
    such as words to vectors of numbers that carry the word’s semantics. In [Chapter 7](ch07.xhtml#ch07),
    we learned that language models construct these word vectors using a word’s context
    in the documents it appears in. So the meaning embedded in word vectors depends
    heavily on the type of corpus used to train the model. Corpuses are a product
    of the culture we live in. Many liberties and civil rights are relatively recent,
    and gender roles and sexual identities are no longer predetermined for us. Many
    corpuses that are used for training language models are based on internet news
    articles, Wikipedia pages, and others that are still biased, discriminatory, and
    contain harmful stereotypes or content. We want to make sure that the word vectors
    that make it into our AI model do not reinforce discrimination and disproportionately
    harm women and minorities.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the training corpus (such as Google News articles) is mostly
    from a society where women are overrepresented as nurses or as elementary school
    teachers and men are overrepresented as doctors or as software engineers, then
    the word vectors would inherit this gender bias. For example, the distance between
    the vector representing *man* and *software engineer* will be smaller than the
    distance between *woman* and *software engineer*. We need to identify and compensate
    for such biases in word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: One solution is nice and simple. Given that we are dealing with vectors of numbers,
    we can literary subtract gender bias and other biases from these vectors. So the
    vector representing *software engineer* would be adjusted by subtracting the vectors
    representing *man* and *male*, and even adding the vectors representing *woman*
    and *female*, if we choose to bias in the other direction. Recall that when we
    add or subtract word vectors from each other, the new vectors obtained still carry
    meaning, since each entry in the vector represents some intensity in some meaning
    dimension. That is, if we subtract the vector for *male* from the vector for *king*,
    we would get a vector close to that of *queen*.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Privacy issues are at the forefront of the concerns about big data and AI. Machine
    learning models need data to train on, and this data contains personal and sensitive
    information of real people. Moreover, a lot of the computations on private data
    happens on the cloud, which raises even more security and privacy concerns.
  prefs: []
  type: TYPE_NORMAL
- en: If anonymizing data is infeasible, or if it lowers the performance of the model
    (for example, age, weight, race, and gender information are important for medical
    purposes), then encryption is our next option. For this, we need models that are
    able to perform computations directly on encrypted data. Traditional encryption
    schemes, however, do not allow any computations on encrypted data. The solution
    is new encryption schemes that allow this. Secure devices can then encrypt data,
    send this encrypted data to machine learning models operating in the cloud, predict
    their results without having to decrypt them, send these results back to the secure
    devices, which finally decrypt them locally, securing all private data and at
    the same time taking advantage of the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Homomorphic encryption does exactly that. The [SIAM news article](https://sinews.siam.org/Details-Page/private-artificial-intelligence-machine-learning-on-encrypted-data)
    by [Kristin Lauter (MetaAI)](https://ai.facebook.com/people/kristin-lauter/),
    whose research is at the intersection of AI and cryptography, explains homomorphic
    encryption, and lists the following nice applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A cloud service that processes all workout, fitness, and location data in
    the cloud in encrypted form. The app displays summary statistics on a phone after
    locally decrypting the results of the analysis.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*An encrypted weather prediction service that takes an encrypted ZIP code and
    returns encrypted information about the weather at the location in question, which
    is then decrypted and displayed on the phone. The cloud service never learns the
    user’s location or the specifics of the weather data that was returned.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A private medical diagnosis application: The patient uploads an encrypted
    version of a chest X-ray image to the cloud service. The medical condition is
    diagnosed by running image recognition algorithms on the encrypted image in the
    cloud; the diagnosis is returned in encrypted form to the doctor or patient.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add a bit more about the math of homomorphic encryption here**'
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the efforts towards ensuring the security and privacy of our
    data in the age of the cloud and connected devices increases the public’s trust
    in the systems and their willingness to volunteer their data to enhance these
    technologies. That said, as anyone who has worked with real data knows, there
    is a lot to learn from being able to *see* the data we are working with. I am
    not sure how troubleshooting on encrypted data can work out.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Humans recognize unfairness on an intuitive level. How do we make sure that
    AI models are operating fairly? One way is to monitor the models for which stakeholders
    they are harming the most (such as older applicants for job opennings, or minorities
    eligible for parole in the criminal justice system), then working on ways to fix
    that, such as debiasing the training data, redefining the decision boundaries
    and thresholds, including humans in the loop, or reallocating resources for programs
    that lift disadvantaged groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fair AI does not only have to do with decision making algorithms. Fairness
    includes who benefits from the algorithms, for example, who gets informed about
    a job opening, vaccination availability, or education opportunities. The article
    [Adversarial Graph Embeddings for Fair Influence Maximization over Social Networks
    (2020)](https://www.ijcai.org/proceedings/2020/0594.pdf) poses this as a *fair*
    influence maximization problem in social media graphs. For influence maximization
    graph models, there is usually a tradeoff between selecting the nodes that have
    the most influence and those that reach minority groups that are not necessarily
    strongly connected to the big hubs in the graph. Thus, the final set of influenced
    nodes is not usually fairly distributed with respect to race, gender, country
    of origin, and other attributes. Adversarial networks are usually good to train
    models where there are competing objectives. The authors take advantage of this,
    introducing adversarial graph embeddings, where there are two networks trained
    together: An auto-encoder for graph embedding and a discriminator to discern the
    sensitive attributes. This leads to embeddings which are similarly distributed
    across sensitive attributes. Then they cluster the resulting graph embeddings
    in order to decide on a good initial seed set.'
  prefs: []
  type: TYPE_NORMAL
- en: Injecting Morality Into AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An AI agent has to know the difference between right and wrong, and ideally
    be flexible enough to handle the grey areas of morality. We need a model that
    emulates humans’ moral judgements with all their situational variations and complexities.
    [Ask Delphi](https://delphi.allenai.org) attempts to do exactly that. When we
    ask Delphi, who is still a prototype, questions such as: Is it okay to rob a bank?
    Is it okay not to talk to my husband? both our queries and Delphi’s answers are
    recorded, as well as whether we agree with Delphi, and our suggestions to improve
    Delphi’s response. As more people engage with Delphi, the training data is enhanced
    allowing Delphi to learn more complex situations and make better predictions (moral
    judgements). The following excerpts and disclaimers are from Delphi’s website.
    They are insightful into the model’s state-of-the-art:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Delphi is learning moral judgments from people who are carefully qualified
    on MTurk. Only the situations used in questions are harvested from Reddit, as
    it is a great source of ethically questionable situations. Delphi 1.0.4 demonstrates
    97.9% accuracy on race-related and 99.3% on gender-related statements. After its
    initial launch, we enhanced Delphi 1.0.0’s guards against statements about racism
    and sexism, which used to show 91.2% and 97.3% accuracy.*'
  prefs: []
  type: TYPE_NORMAL
- en: Terms & Conditions (v1.0.4)
  prefs: []
  type: TYPE_NORMAL
- en: '*Delphi is a research prototype designed to investigate the promises and more
    importantly, the limitations of modeling people’s moral judgments on a variety
    of everyday situations. The goal of Delphi is to help AI systems be more ethically-informed
    and equity-aware. By taking a step in this direction, we hope to inspire our research
    community to tackle the research challenges in this space head-on to build ethical,
    reliable, and inclusive AI systems.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*What are the limitations of Delphi? Large pretrained language models, such
    as GPT-3, are trained on mostly unfiltered internet data, and therefore are extremely
    quick to produce toxic, unethical, and harmful content, especially about minority
    groups. Delphi’s responses are automatically extrapolated from a survey of US
    crowd workers, which helps reduce this issue but may introduce its own biases.
    Thus, some responses from Delphi may contain inappropriate or offensive results.
    Please be mindful before sharing results.*'
  prefs: []
  type: TYPE_NORMAL
- en: Democratization And Accessibility Of AI To Nonexperts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To maximize the benefits of AI technologies, they have to be democratized, made
    easily accessible to populations at large as opposed to being restricted to experts.
    For this to happen, and for people to trust these systems, the models and the
    data systems they rely upon must be understandable, easy to use, and transparent
    about their inner workings, capabilities, and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Anna Fariha, Ph.D.](https://afariha.github.io) (Microsoft) is one researcher
    doing wonderful work towards this goal. She is interested in extending the capabilities
    of data systems to provide user-facing functionalities that help boost productivity
    and agility for a diverse group of users, ranging from end users to data scientists
    and developers.'
  prefs: []
  type: TYPE_NORMAL
- en: Prioritizing High Quality Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The examples in this chapter make the case for prioritizing, democratizing,
    and securing high quality data in order to obtain AI that is fair and benefecial
    to humanity. High quality data is clear, accurate, and impartial. It is stored
    in easy to query structures. The differences between data structures need to be
    explained to end users so they can decide which ones work best for them. For institutions
    who want to transition to data driven decision making, or to get on the AI bandwagon,
    or to stay competitive with younger companies where these technologies are built
    into their DNA, determining a plan to handle their data in an organized and consistent
    way is one step that is crucial for future success.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our work with our city’s Fire Department and Department of Public Transportation,
    we discovered many ways to improve the quality of their data. Implementing those
    at the very early stages of building their data structures and collecting the
    data would save an enormous amount of time, money, and resources down the pipeline.
    For example, with the bus routing project, data like buses in operation and number
    of drivers by month is not recorded, neither is information about bus stops such
    as which ones are marked and which are unmarked. Even when data is stored, it
    was impossible to retrieve: Our university’s Parking Services informed us that
    to get historical data from the parking decks, they would have to make over 5,000
    manual requests. All the data we obtained needed to be cleaned and transformed
    into a usable form. Sometimes, data obtained from the same source was inconsistent
    and a lot of work could’ve been saved had more care been taken right at the onset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Something else happened with our data that is a lifelong lesson: Late into
    our project, after we cleaned, joined, and transformed all the relevant data,
    and our models were producing results that are transferable to business decisions,
    such as identifying gaps between supply and demand in certain areas, and higlighting
    the most significant contributors, *etc.*, we discovered that *all* the bus stop
    data that we were given was scrambled. This meant that ridership and route for
    each bus stop in the city *did not correspond* to the bus stop that was in the
    data table, and we had no way to fix it other than running the original query
    to the database and tracking down what went wrong when writing the data file.
    Had we not discovered that, we would’ve based *all* our analysis on *wrong data*,
    garbage data! The transportation department would’ve acted on wrong results. We
    must always make sure that the data we work with corresponds accurately with what’s
    on the ground. We must plot, map, check, double check, and triple check. There
    is a responsibility that comes with our work, we cannot take it lightly. We should
    know our data and our models in and out. We should be prepared to answer all questions
    about our models, compare them to other models that are out there, and make sure
    we did our due diligence before giving our results to stakeholders.'
  prefs: []
  type: TYPE_NORMAL
- en: Like us, a general AI agent would look for the right data in the right places
    then transform it into usable form. Until then, we must refocus our efforts to
    collecting and storing good quality data and having better ways to access and
    query it. Because of low quality data and nonexistent digital infrastructures,
    many AI projects never see the light of the day, and many automation investments
    never see any returns. We should step back and think about how data will end up
    being represented as inputs for our models. This is what should guide how we acquire
    data, and how we can store it for future use. The AI field has operated on a paradigm
    that should be adopted universally, *representation first, acquisition second*.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing Bias From Discrimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of discussions that involve AI ethics use the terms *bias* and *discrimination*
    interchangeably, and I wanted to make sure that we highlight the difference between
    two before we finish the book. I was never the person to be hung on definitions
    of terms, especially that I speak English as a third language, and because I notice
    that redefining terms is often used as a cheap tactic to deflect from the main
    points of an argument or a debate. The reason I want to highlight the difference
    between bias and discrimination in particular is that each requires different
    mathematics to identify. Moreover, one is intentional, and the other is not. Both
    us and our machines should be able to reason about which one is which.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we can detect bias merely by observing the data. We cannot identify
    discrimination unless we ascend from mere observations to a higher level of reasoning,
    using the causal language of interventions and counterfactuals, which we went
    over in [Chapter 9](ch09.xhtml#ch09): What if I change the gender of the applicant
    on their resume, would they have gotten the job?'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bias is a pattern of association between a particular decision and a particular
    sex of applicant*. We can detect this pattern directly when observing the data
    of applicants and eventual hires.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Discrimination, on the other hand, had intentionality within it: *It is the
    exercise of decision influenced by the sex of the applicant when that is immaterial
    to the qualifications for entry*. The gender of the applicant affected the hiring
    decision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above definitions are highlighted in Judea Pearl’s The Book Of Why (2020).
    He goes on to mention the definition of discrimination in US case law, which also
    uses the language of counterfactuals: *In Carson v. Bethlehem Steel Corp. (1996),
    the Seventh Circuit Court wrote, “The central question in any employment-discrimination
    case is whether the employer would have taken the same action had the employee
    been of a different race (age, sex, religion, national origin, etc.) and everything
    else had been the same.*'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in order to distinguish bias from intentional discrimination, we
    need to use the do-calculus on conditional probabilities, which we introduced
    in both Chapters [9](ch09.xhtml#ch09) and [10](ch10.xhtml#ch10), and which you
    can learn more about from the excellent resources by Judea Pearl and his mathematical
    community.
  prefs: []
  type: TYPE_NORMAL
- en: The Hype
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AI field has been accused of being hyped up throughout its history. Nowadays,
    any computational approach to solving problems or building systems, whether traditional
    or more recent, is being reframed as AI. Traditional statistics is AI, operations
    research is AI, Data exploration and analysis is AI, quantum computing is AI,
    medical imaging is AI, *etc.*. Many start up companies are relying on inflated
    metrics, stretched truths, and on investors who chip in without much questions
    so as not to miss out on the next big thing (such as the busted Silicon valley’s
    blood testing company Theranos). Since we are at an age where AI has become a
    buzzword and household term, it is easy to get sweeped away thinking that any
    technology based on AI is going to work.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum computing is another technology that is still in infancy, being hyped,
    and being conflated with AI. It is no where close to being commercialized, but
    it is already being marketed at such. A lot of research needs to be done, and
    if successful, the technology has a large potential for useful applications. The
    most famous application, which spurred considerable research funding and governments’
    attention, is the Peter Shor’s 1994 theoretical demonstration that a quantum computer
    can solve the hard problem of finding the prime factors of large numbers exponentially
    faster than all classical schemes. The Rivest–Shamir–Adleman (RSA)-encryption
    is an algorithm used by modern computers to encrypt and decrypt messages, and
    prime factorization is at the core of breaking its code.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized AI is well developed compared to quantum computing, and one of the
    goals of this book is to discern the hype from the non-hype. Hyped or un-hyped,
    get in the field, enjoy, and work towards good goals and unlocking great potentials.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many sectors and industries are gravitating towards AI and data science. They
    want to leverage the substansive progress in the computational abilities and the
    advancement of highly expressive models transforming their data into meaningful
    insights and decisions. They also realize the potential for a sea change at an
    industry level, and they want to be part of that.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to get into this beautiful and exciting field, you can go into
    the applied side of it: Choose an application in an industry that interests you
    and that you feel passionate about. Start by formulating questions that you want
    to answer, find data, and start applying what you learned. Another path is to
    go into the research side, where we study the models themselves, how to improve
    them, scale them, analyze them and prove theorems about their behavior, or come
    up with entirely new ones. Again, only choose research projects that you are genuinely
    curious about. One more path is to go to the coding side of things, building packages,
    libraries, and better implementations for things. You will be doing us all a favor
    that way. I cannot imagine what many of us would do if keras and scikit learn
    (Python libraries for machine learning and neural networks) did not exist.'
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there are only 22,000 Ph.D. holding AI researchers in the world.
    Forty percent of those are in the United States. To fulfill demand, bring new
    ideas into the field, we need much more researchers, both domestically and internationally.
    I hope this book was able to fast track you into this fascinating field, and I
    hope you have enough foundation to be able to branch out on your own into any
    of the topics that interest you.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most exciting things for me, as someone who has always appreciated
    math and its astounding ability to model our universe, is that AI has ignited
    peoples’ interest in math. I hope this in return drives mathematicians to rethink
    how to present and teach math. Meanwhile, let’s all advocate for high quality
    and accurately labeled data, AI policy, and great honesty about what our systems
    account and do not account for. At the same time, we have to be very careful not
    to reduce our human experience to a stream of data and indicators, some measurable
    and others left to our falible models to predict and base decisions on. As this
    book demonstrated again and again: Experiences, click habits, zip codes, health
    records, comments on social media, images, tags, email correspondence, residence
    history, race, ethinicy, national origin, religion, marital status, age, our friends,
    our friends’ habits, dot dot dot, *all* find their way to becoming mere entries
    in a high dimensional vector, that gets fed to a machine learning model to make
    predictions. We want to make sure that we are not accidentally transforming ourselves
    into walking and talking high dimensional data points.'
  prefs: []
  type: TYPE_NORMAL
- en: '*My final thought, for now*: AI has tied many aspects of mathematics neatly
    together. Maybe this is not a coincidence. Maybe mathematics is the language that
    fits intelligence, and intelligence expresses itself most comfortably through
    mathematics. For intelligence to be artificially replicated, we need an agent
    that can represent the world, effortlessly, through its preferred language.'
  prefs: []
  type: TYPE_NORMAL
- en: About the Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hala Nelson** is an Associate Professor of Mathematics at James Madison University.
    She has a Ph.D. in Mathematics from the Courant Institute of Mathematical Sciences
    at New York University. Prior to her work at James Madison University, she was
    a postdoctoral Assistant Professor at the University of Michigan- Ann Arbor.'
  prefs: []
  type: TYPE_NORMAL
- en: Her research is in the areas of materials science, statistical mechanics, inverse
    problems, and the mathematics of machine learning and artificial intelligence.
    Her favorite math subjects are optimization, PDEs, numerical linear algebra, and
    probability theory. She likes to translate complex ideas into simple and practical
    terms. To her, most mathematical concepts are painless and relatable, unless the
    person presenting them either does not understand them very well, or is trying
    to show off.
  prefs: []
  type: TYPE_NORMAL
- en: '**Other facts**: Hala Nelson grew up in Lebanon, during the time of its brutal
    civil war. She lost her hair at a very young age in a missile explosion. This
    event and many that followed shaped her interests in human behavior, the nature
    of intelligence, and AI. Her dad taught her Math, at home and in French, until
    she graduated high school. Her favorite quote from her dad about math is, "*It
    is the one clean science''''*.'
  prefs: []
  type: TYPE_NORMAL
- en: Colophon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The animal on the cover of *Essential Math for AI* is a harnessed bushbuck (*Tragelaphus
    scriptus*).
  prefs: []
  type: TYPE_NORMAL
- en: Many of the animals on O’Reilly covers are endangered; all of them are important
    to the world.
  prefs: []
  type: TYPE_NORMAL
- en: The cover illustration is by Karen Montgomery, based on an antique line engraving
    from *Shaw’s Zoology*. The cover fonts are Gilroy Semibold and Guardian Sans.
    The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed;
    and the code font is Dalton Maag’s Ubuntu Mono.
  prefs: []
  type: TYPE_NORMAL
