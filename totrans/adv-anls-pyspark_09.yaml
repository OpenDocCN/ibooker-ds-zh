- en: Chapter 9\. Analyzing Genomics Data and the BDG Project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。分析基因组数据与BDG项目
- en: The advent of next-generation DNA sequencing (NGS) technology has rapidly transformed
    the life sciences into a data-driven field. However, making the best use of this
    data is butting up against a traditional computational ecosystem that builds on
    difficult-to-use, low-level primitives for distributed computing and a jungle
    of semistructured text-based file formats.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代DNA测序（NGS）技术的出现迅速将生命科学转变为一个数据驱动的领域。然而，充分利用这些数据却遭遇到了一个传统的计算生态系统，其构建在难以使用的低级原语上，用于分布式计算，以及一大堆半结构化的基于文本的文件格式。
- en: This chapter will serve two primary purposes. First, we introduce a set of popular
    serialization and file formats (Avro and Parquet) that simplify many problems
    in data management. These serialization technologies enable us to convert data
    into compact, machine-friendly binary representations. This helps with movement
    of data across networks and helps with cross-compatibility across programming
    languages. Although we will use data serialization techniques with genomics data,
    the concepts will be useful whenever processing large amounts of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将有两个主要目的。首先，我们介绍一套流行的序列化和文件格式（Avro和Parquet），它们简化了数据管理中的许多问题。这些序列化技术使我们能够将数据转换为紧凑的、机器友好的二进制表示形式。这有助于在网络间移动数据，并帮助不同编程语言之间的跨兼容性。虽然我们将在基因组数据中使用数据序列化技术，但这些概念在处理大量数据时也是有用的。
- en: Second, we show how to perform typical genomics tasks in the PySpark ecosystem.
    Specifically, we’ll use PySpark and the open source ADAM library to manipulate
    large quantities of genomics data and process data from multiple sources to create
    a dataset for predicting transcription factor (TF) binding sites. For this, we
    will join genome annotations from the [ENCODE dataset](https://oreil.ly/h0yOq).
    This chapter will serve as a tutorial to the ADAM project, which comprises a set
    of genomics-specific Avro schemas, PySpark-based APIs, and command-line tools
    for large-scale genomics analysis. Among other applications, ADAM provides a natively
    distributed implementation of the [Genome Analysis Toolkit (GATK)](https://oreil.ly/k2YZH)
    using PySpark.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们展示如何在PySpark生态系统中执行典型的基因组学任务。具体来说，我们将使用PySpark和开源的ADAM库来操作大量的基因组学数据，并处理来自多个来源的数据，创建一个用于预测转录因子（TF）结合位点的数据集。为此，我们将从[ENCODE数据集](https://oreil.ly/h0yOq)中获取基因组注释。本章将作为ADAM项目的教程，该项目包括一组面向基因组学的Avro模式、基于PySpark的API以及用于大规模基因组学分析的命令行工具。除了其他应用外，ADAM使用PySpark提供了[基因组分析工具包（GATK）](https://oreil.ly/k2YZH)的本地分布式实现。
- en: We’ll start by talking about the various data formats used in the bioinformatics
    domain, associated challenges, and how serialization formats can help. After that,
    we’ll install the ADAM project and explore its API using a sample dataset. We
    will then work with multiple genomics datasets to prepare a dataset that can be
    used for predicting binding sites in DNA sequences for a particular type of protein—CTCF
    transcription factor. The datasets will be obtained from the publicly available
    ENCODE dataset. Because the genome implies a 1D coordinate system, many genomics
    operations are spatial in nature. The ADAM project provides a genomics-targeted
    API for performing distributed spatial joins that we will use.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论生物信息学领域使用的各种数据格式，相关的挑战，以及序列化格式如何帮助解决问题。接着，我们将安装ADAM项目，并使用一个样本数据集探索其API。然后，我们将使用多个基因组学数据集准备一个数据集，用于预测特定类型蛋白质（CTCF转录因子）DNA序列中的结合位点。这些数据集将从公开可用的ENCODE数据集中获取。由于基因组暗示了一个一维坐标系统，许多基因组学操作具有空间性质。ADAM项目提供了一个针对基因组学的API，用于执行分布式空间连接。
- en: For those interested, a great introduction to biology is [Eric Lander’s EdX
    course](https://oreil.ly/WIky1). For an introduction to bioinformatics, see Arthur
    Lesk’s *Introduction to Bioinformatics* (Oxford University Press).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些感兴趣的人，Eric Lander的EdX课程《[生物学简介](https://oreil.ly/WIky1)》是一个很好的入门。而对于生物信息学的介绍，可以参考亚瑟·莱斯克的《[生物信息学简介](https://www.example.org/introduction_to_bioinformatics)》（牛津大学出版社）。
- en: Decoupling Storage from Modeling
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解耦存储与建模
- en: Bioinformaticians spend a disproportionate amount of time worrying about file
    formats—*.fasta*, *.fastq*, *.sam*, *.bam*, *.vcf*, *.gvcf*, *.bcf*, *.bed*, *.gff*,
    *.gtf*, *.narrowPeak*, *.wig*, *.bigWig*, *.bigBed*, *.ped*, and *.tped*, to name
    a few. Some scientists also feel it is necessary to specify their own custom format
    for their custom tool. On top of that, many of the format specifications are incomplete
    or ambiguous (which makes it hard to ensure implementations are consistent or
    compliant) and specify ASCII-encoded data. ASCII data is very common in bioinformatics,
    but it is inefficient and compresses relatively poorly. In addition, the data
    must always be parsed, necessitating additional compute cycles.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生物信息学家花费了大量时间担心文件格式 —— *.fasta* ， *.fastq* ， *.sam* ， *.bam* ， *.vcf* ， *.gvcf*
    ， *.bcf* ， *.bed* ， *.gff* ， *.gtf* ， *.narrowPeak* ， *.wig* ， *.bigWig* ， *.bigBed*
    ， *.ped* 和 *.tped* 等等。一些科学家还觉得有必要为他们的自定义工具指定自己的自定义格式。此外，许多格式规范是不完整或含糊不清的（这使得很难确保实现是一致的或符合规范），并指定了
    ASCII 编码的数据。ASCII 数据在生物信息学中非常常见，但效率低下且相对难以压缩。此外，数据必须始终进行解析，需要额外的计算周期。
- en: 'This is particularly troubling because all of these file formats essentially
    store just a few common object types: an aligned sequence read, a called genotype,
    a sequence feature, and a phenotype. (The term *sequence feature* is slightly
    overloaded in genomics, but in this chapter we mean it in the sense of an element
    from a track of the UCSC Genome Browser.) Libraries like [`biopython`](http://biopython.org)
    are popular because they are chock-full of parsers (e.g., `Bio.SeqIO`) that attempt
    to read all the file formats into a small number of common in-memory models (e.g.,
    `Bio.Seq`, `Bio.SeqRecord`, `Bio.SeqFeature`).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是特别令人担忧的，因为所有这些文件格式本质上只存储了几种常见的对象类型：对齐的序列读取，调用的基因型，序列特征和表型。（*序列特征* 这个术语在基因组学中有些多重含义，但在本章中我们指的是
    UCSC 基因组浏览器的轨迹元素。）像 [`biopython`](http://biopython.org) 这样的库非常流行，因为它们充斥着解析器（例如，
    `Bio.SeqIO` ），这些解析器试图将所有文件格式读取到少量常见的内存模型中（例如， `Bio.Seq` ， `Bio.SeqRecord` ， `Bio.SeqFeature`
    ）。
- en: 'We can solve all of these problems in one shot using a serialization framework
    like Apache Avro. The key lies in Avro’s separation of the data model (i.e., an
    explicit schema) from the underlying storage file format and also the language’s
    in-memory representation. Avro specifies how data of a certain type should be
    communicated between processes, whether that’s between running processes over
    the internet, or a process trying to write the data into a particular file format.
    For example, a Java program that uses Avro can write the data into multiple underlying
    file formats that are all compatible with Avro’s data model. This allows each
    process to stop worrying about compatibility with multiple file formats: the process
    only needs to know how to read Avro, and the filesystem needs to know how to supply
    Avro.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用像 Apache Avro 这样的序列化框架一举解决所有这些问题。关键在于 Avro 将数据模型（即显式模式）与底层存储文件格式以及语言的内存表示分离开来。Avro
    指定了如何在进程之间传递特定类型的数据，无论是在互联网上运行的进程之间，还是尝试将数据写入特定文件格式的进程。例如，使用 Avro 的 Java 程序可以将数据写入多种与
    Avro 数据模型兼容的底层文件格式。这使得每个进程都不再需要担心与多种文件格式的兼容性：进程只需要知道如何读取 Avro 数据，文件系统只需要知道如何提供
    Avro 数据。
- en: 'Let’s take the sequence feature as an example. We begin by specifying the desired
    schema for the object using the Avro interface definition language (IDL):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以序列特征为例。我们首先使用 Avro 接口定义语言（IDL）为对象指定所需的模式：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO1-1)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO1-1)'
- en: For example, “conservation,” “centipede,” “gene”
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“保守性”，“蜈蚣”，“基因”
- en: This data type could be used to encode, for example, conservation level, the
    presence of a promoter or ribosome binding site, a TF binding site, and so on
    at a particular location in the genome. One way to think about it is as a binary
    version of JSON, but more restricted and with higher performance. Given a particular
    data schema, the Avro spec then determines the precise binary encoding for the
    object so that it can be easily communicated between processes (even if written
    in different programming languages), over the network, or onto disk for storage.
    The Avro project includes modules for processing Avro-encoded data from many languages,
    including Java, C/C++, Python, and Perl; after that, the language is free to store
    the object in memory in whichever way is deemed most advantageous. The separation
    of data modeling from the storage format provides another level of flexibility/abstraction;
    Avro data can be stored as Avro-serialized binary objects (Avro container file),
    in a columnar file format for fast queries (Parquet file), or as text JSON data
    for maximum flexibility (minimum efficiency). Finally, Avro supports schema evolution,
    allowing the user to add new fields as they become necessary, while the software
    gracefully deals with new/old versions of the schema.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据类型可以用来编码，例如，基因组中特定位置的保守水平、启动子或核糖体结合位点的存在、TF结合位点等。可以将其视为JSON的二进制版本，但更受限制且具有更高的性能。根据特定的数据模式，Avro规范确定了对象的精确二进制编码方式，以便可以在不同编程语言的进程之间轻松传输（甚至写入到不同编程语言的进程之间），通过网络或者存储到磁盘上。Avro项目包括用于处理多种语言（包括Java、C/C++、Python和Perl）中的Avro编码数据的模块；在此之后，语言可以自由地将对象存储在内存中，以最有利的方式。数据建模与存储格式的分离提供了另一种灵活性/抽象层次；Avro数据可以存储为Avro序列化的二进制对象（Avro容器文件）、用于快速查询的列式文件格式（Parquet文件）或者作为文本JSON数据以实现最大的灵活性（最小的效率）。最后，Avro支持模式演化，允许用户在需要时添加新字段，同时软件会优雅地处理新/旧版本的模式。
- en: Overall, Avro is an efficient binary encoding that allows you to specify evolvable
    data schemas, process the same data from many programming languages, and store
    the data using many formats. Deciding to store your data using Avro schemas frees
    you from perpetually working with more and more custom data formats, while simultaneously
    increasing the performance of your computations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总体上，Avro是一种高效的二进制编码，允许您指定可适应变化的数据模式，从多种编程语言处理相同的数据，并使用多种格式存储数据。决定使用Avro模式存储数据将使您摆脱永远使用越来越多的自定义数据格式的困境，同时提高计算性能。
- en: 'The particular `SequenceFeature` model used in the preceding example is a bit
    simplistic for real data, but the Big Data Genomics (BDG) project has already
    defined Avro schemas to represent the following objects, as well as many others:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中使用的特定`SequenceFeature`模型对于真实数据来说有些简单，但是Big Data Genomics（BDG）项目已经定义了Avro模式来表示以下对象以及许多其他对象：
- en: '`AlignmentRecord` for reads'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AlignmentRecord` 用于读取。'
- en: '`Variant` for known genome variants and metadata'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Variant`用于已知的基因组变异和元数据。'
- en: '`Genotype` for a called genotype at a particular locus'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Genotype`用于特定位点的已调用基因型。'
- en: '`Feature` for a sequence feature (annotation on a genome segment)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Feature`用于序列特征（基因组片段上的注释）。'
- en: The actual schemas can be found in [the `bdg-formats` GitHub repo](https://oreil.ly/gCf1f).
    The BDG formats can function as a replacement of the ubiquitous “legacy” formats
    (like BAM and VCF), but more commonly function as high-performance “intermediate”
    formats. (The original goal of these BDG formats was to replace the use of BAM
    and VCF, but their stubborn ubiquity has proved this goal to be difficult to attain.)
    Avro provides many performance and data modeling benefits over the custom ASCII
    status quo.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的模式可以在[bdg-formats GitHub存储库](https://oreil.ly/gCf1f)中找到。BDG格式可以用作广泛使用的“传统”格式（如BAM和VCF）的替代，但更常用作高性能的“中间”格式。（这些BDG格式的最初目标是取代BAM和VCF的使用，但它们的固执普及性使得实现这一目标变得困难。）Avro相对于自定义ASCII格式提供了许多性能和数据建模的优势。
- en: In the remainder of the chapter, we’ll use some of the BDG schemas to accomplish
    some typical genomics tasks. Before we can do that, we will need to install the
    ADAM project. That’s what we’ll do in the next section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将使用一些BDG模式来完成一些典型的基因组学任务。在我们能够做到这一点之前，我们需要安装ADAM项目。这将是我们接下来将要做的事情。
- en: Setting Up ADAM
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ADAM的设置
- en: BDG’s core set of genomics tools is called ADAM. Starting from a set of mapped
    reads, this core includes tools that can perform mark-duplicates, base quality
    score recalibration, indel realignment, and variant calling, among other tasks.
    ADAM also contains a command-line interface that wraps the core for ease of use.
    In contrast to traditional HPC tools, ADAM can automatically parallelize across
    a cluster without having to split files or schedule jobs manually.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: BDG的核心基因组工具称为ADAM。从一组映射的读取开始，该核心包括可以执行标记重复、基质质量分数重新校准、插入/缺失重对齐和变体调用等任务的工具。ADAM还包含一个命令行界面，用于简化使用。与传统的HPC工具不同，ADAM可以自动跨集群并行处理，无需手动分割文件或手动调度作业。
- en: 'We can start by installing ADAM using pip:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过pip安装ADAM：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Alternative installation methods can be found on the [GitHub page](https://oreil.ly/4eFnX).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[GitHub页面](https://oreil.ly/4eFnX)找到其他安装方法。
- en: 'ADAM also comes with a submission script that facilitates interfacing with
    Spark’s `spark-submit` script:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ADAM还配备了一个提交脚本，可方便与Spark的`spark-submit`脚本进行交互：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At this point, you should be able to run ADAM from the command line and get
    the usage message. As noted in the usage message, Spark arguments are given before
    ADAM-specific arguments.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能够从命令行运行ADAM并获得使用消息。正如使用消息中所指出的，Spark参数在ADAM特定参数之前给出。
- en: With ADAM set up, we can start working with genomic data. We will explore ADAM’s
    API by working with a sample dataset next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 安装好ADAM后，我们可以开始处理基因组数据。接下来，我们将通过处理一个示例数据集来探索ADAM的API。
- en: Introduction to Working with Genomics Data Using ADAM
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ADAM处理基因组数据入门
- en: 'We’ll start by taking a *.bam* file containing some mapped NGS reads, converting
    them to the corresponding BDG format (`AlignedRecord` in this case), and saving
    them to HDFS. First, we get our hands on a suitable *.bam* file:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从包含一些映射的NGS读取的*.bam*文件开始，将其转换为相应的BDG格式（在本例中为`AlignedRecord`），并保存到HDFS中。首先，我们找到一个合适的*.bam*文件：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Move the downloaded file into a directory where we’ll store all data for this
    chapter:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 将下载的文件移动到一个目录中，我们将在这一章节中存储所有数据：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next up, we’ll use the ADAM CLI.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用ADAM CLI。
- en: File Format Conversion with the ADAM CLI
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ADAM CLI进行文件格式转换
- en: 'We can then use the ADAM `transform` command to convert the *.bam* file to
    Parquet format (described in [“Parquet Format and Columnar Storage”](#parquet-format)).
    This would work both on a cluster and in `local` mode:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用ADAM的`transform`命令将*.bam*文件转换为Parquet格式（在[“Parquet格式和列式存储”](#parquet-format)中有描述）。这在集群和`local`模式下都可以使用：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-1)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-1)'
- en: Example Spark args for running on YARN
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在YARN上运行的示例Spark参数
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-2)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO2-2)'
- en: The ADAM subcommand itself
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ADAM子命令本身
- en: This should kick off a pretty large amount of output to the console, including
    the URL to track the progress of the job.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会启动相当大量的输出到控制台，包括跟踪作业进度的URL。
- en: The resulting dataset is the concatenation of all the files in the *data/genomics/reads/HG00103/*
    directory, where each *part-*.parquet* file is the output from one of the PySpark
    tasks. You’ll also notice that the data has been compressed more efficiently than
    the initial *.bam* file (which is gzipped underneath) thanks to the columnar storage
    (see [“Parquet Format and Columnar Storage”](#parquet-format)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据集是*data/genomics/reads/HG00103/*目录中所有文件的串联，其中每个*part-*.parquet*文件是一个PySpark任务的输出。您还会注意到，由于列式存储，数据比初始的*.bam*文件（底层为gzipped）压缩得更有效（请参见[“Parquet格式和列式存储”](#parquet-format)）。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s see what one of these objects looks like in an interactive session.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看交互会话中的一个对象是什么样子。
- en: Ingesting Genomics Data Using PySpark and ADAM
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PySpark和ADAM摄取基因组数据
- en: First, we start up the PySpark shell using the ADAM helper command. It loads
    all of the JARs that are necessary.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用ADAM助手命令启动PySpark shell。它加载所有必需的JAR文件。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In some cases, you can encounter a TypeError error with a mention of JavaPackage
    object not being when trying to use ADAM with PySpark. It is a known issue and
    is documented [here](https://oreil.ly/67uBd).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，当尝试使用ADAM与PySpark时，您可能会遇到TypeError错误，并提到JavaPackage对象未被加载的问题。这是一个已知问题，并在[此处](https://oreil.ly/67uBd)有文档记录。
- en: 'In such a scenario, please try the solutions suggested in the thread. One could
    be running the following command to start PySpark shell with ADAM:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，请尝试线程中建议的解决方案。可以通过运行以下命令来启动带有ADAM的PySpark shell：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we’ll load the aligned read data as an `AlignmentDataset`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将加载对齐的读取数据作为`AlignmentDataset`：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You may get a different read because the partitioning of the data may be different
    on your system, so there is no guarantee which read will come back first.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会得到不同的读取，因为数据的分区在您的系统上可能不同，所以无法保证哪个读取会先返回。
- en: Now we can interactively ask questions about our dataset, all while executing
    the computations across a cluster in the background. How many reads do we have
    in this dataset?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以与数据集互动地提出问题，同时在后台集群中执行计算。这个数据集有多少个读取数？
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Do the reads in this dataset derive from all human chromosomes?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的读取是否来自所有人类染色体？
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Yep, we observe reads from chromosomes 1 through 22, X and Y, along with some
    other chromosomal chunks that are not part of the “main” chromosomes or whose
    locations are unknown. Let’s analyze the code a little more closely:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们观察到来自染色体1到22、X和Y的读取，以及一些其他不属于“主”染色体的染色体片段或位置未知的染色体块。让我们更仔细地分析一下代码：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-1)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-1)'
- en: '`AlignmentDataset`: an ADAM type that contains all our data.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`AlignmentDataset`：包含所有数据的ADAM类型。'
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-2)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-2)'
- en: '`DataFrame`: the underlying Spark DataFrame.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`：底层的Spark DataFrame。'
- en: '[![3](assets/3.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-3)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-3)'
- en: This will aggregate all the distinct contig names; it will be small.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这将聚合所有不同的contig名称；这将很小。
- en: '[![4](assets/4.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-4)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO3-4)'
- en: This triggers the computation and brings the data in the DataFrame back to the
    client app (the shell).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将触发计算，并将DataFrame中的数据返回到客户端应用程序（Shell）。
- en: 'For a more clinical example, say we are testing an individual’s genome to check
    whether they carry any gene variants that put them at risk for having a child
    with cystic fibrosis (CF). Our genetic test uses next-generation DNA sequencing
    to generate reads from multiple relevant genes, such as the CFTR gene (whose mutations
    can cause CF). After running our data through our genotyping pipeline, we determine
    that the CFTR gene appears to have a premature stop codon that destroys its function.
    However, this mutation has never been reported before in the [Human Gene Mutation
    Database](https://oreil.ly/wULRR), nor is it in the [Sickkids CFTR database](https://oreil.ly/u1L0j),
    which aggregates CF gene variants. We want to go back to the raw sequencing data
    to see if the potentially deleterious genotype call is a false positive. To do
    so, we need to manually analyze all the reads that map to that variant locus,
    say, chromosome 7 at 117149189 (see [Figure 9-1](#IGV_HG00103_CFTR)):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 举个更临床的例子，假设我们正在测试一个个体的基因组，以检查他们是否携带任何可能导致其子女患囊性纤维化（CF）的基因变异。我们的基因检测使用下一代DNA测序从多个相关基因生成读取数，比如CFTR基因（其突变可以导致CF）。在通过我们的基因分型流程运行数据后，我们确定CFTR基因似乎有一个导致其功能受损的早期终止密码子。然而，这种突变在[Human
    Gene Mutation Database](https://oreil.ly/wULRR)中从未报告过，也不在[Sickkids CFTR database](https://oreil.ly/u1L0j)中（这是聚合CF基因变异的数据库）。我们想回到原始测序数据，以查看可能有害的基因型调用是否为假阳性。为此，我们需要手动分析所有映射到该变异位点的读数，比如染色体7上的117149189位置（参见[图 9-1](#IGV_HG00103_CFTR)）：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It is now possible to manually inspect these nine reads, or process them through
    a custom aligner, for example, and check whether the reported pathogenic variant
    is a false positive.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以手动检查这九个读取，或者例如通过自定义对齐器处理它们，并检查报告的致病变异是否为假阳性。
- en: '![aaps 0901](assets/aaps_0901.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0901](assets/aaps_0901.png)'
- en: Figure 9-1\. Integrative Genomic Viewer visualization of the HG00103 at chr7:117149189
    in the CFTR gene
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. 在CFTR基因中chr7:117149189处的HG00103的综合基因组查看器可视化
- en: Say we’re running a clinical lab that is performing such carrier screening as
    a service to clinicians. Archiving the raw data using a cloud storage system such
    as AWS S3 ensures that the data stays relatively warm (compared with, say, tape
    archive). In addition to having a reliable system for actually performing the
    data processing, we can easily access all of the past data for quality control
    or for cases where there needs to be manual interventions, like the CFTR example
    presented earlier. In addition to the rapid access to the totality of the data,
    the centrality also makes it easy to perform large analytical studies, like population
    genetics, large-scale quality-control analyses, and so on.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在运营一个临床实验室，为临床医生提供携带者筛查服务。使用像AWS S3这样的云存储系统存档原始数据可以确保数据保持相对温暖（相对于磁带存档来说）。除了有一个可靠的数据处理系统外，我们还可以轻松访问所有过去的数据进行质量控制，或者在需要手动干预的情况下，例如之前介绍的CFTR案例。除了快速访问所有数据的便利性外，中心化还使得执行大规模分析研究（如人群遗传学、大规模质量控制分析等）变得更加容易。
- en: Now that we are familiar with the ADAM API, let’s start work on creation of
    our transcription factor prediction dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了ADAM API，让我们开始创建我们的转录因子预测数据集。
- en: Predicting Transcription Factor Binding Sites from ENCODE Data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从ENCODE数据预测转录因子结合位点
- en: In this example, we will use publicly available sequence feature data to build
    a simple model for transcription factor binding. TFs are proteins that bind to
    specific DNA sequences in the genome and help control the expression of different
    genes. As a result, they are critical in determining the phenotype of a particular
    cell and are involved in many physiological and disease processes. ChIP-seq is
    an NGS-based assay that allows the genome-wide characterization of binding sites
    for a particular TF in a particular cell/tissue type. However, in addition to
    ChIP-seq’s cost and technical difficulty, it requires a separate experiment for
    each tissue/TF pair. In contrast, DNase-seq is an assay that finds regions of
    open chromatin genome-wide and needs to be performed only once per tissue type.
    Instead of assaying TF binding sites by performing a ChIP-seq experiment for each
    tissue/TF combination, we’d like to predict TF binding sites in a new tissue type
    assuming only the availability of DNase-seq data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用公开可用的序列特征数据来构建一个简单的转录因子结合模型。TFs是结合基因组中特定DNA序列的蛋白质，并帮助控制不同基因的表达。因此，它们在确定特定细胞的表型方面至关重要，并参与许多生理和疾病过程。ChIP-seq是一种基于NGS的分析方法，允许对特定细胞/组织类型中特定TF结合位点进行全基因组特征化。然而，除了ChIP-seq的成本和技术难度之外，它还需要为每种组织/TF组合进行单独的实验。相比之下，DNase-seq是一种在全基因组范围内查找开放染色质区域的分析方法，并且只需每种组织类型执行一次。与为每种组织/TF组合执行ChIP-seq实验以测定TF结合位点不同，我们希望在仅有DNase-seq数据的情况下预测新组织类型中的TF结合位点。
- en: In particular, we will predict the binding sites for the CTCF TF using DNase-seq
    data along with known sequence motif data (from [HT-SELEX](https://oreil.ly/t5OEkL))
    and other data from [the publicly available ENCODE dataset](https://oreil.ly/eFJ9n).
    We have chosen six different cell types that have available DNase-seq and CTCF
    ChIP-seq data for training. A training example will be a DNase hypersensitivity
    (HS) peak (a segment of the genome), and the binary label for whether the TF is
    bound/unbound will be derived from the ChIP-seq data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将使用来自[HT-SELEX](https://oreil.ly/t5OEkL)的已知序列模体数据和其他来自[公开可用的ENCODE数据集](https://oreil.ly/eFJ9n)的DNase-seq数据来预测CTCF
    TF的结合位点。我们选择了六种不同的细胞类型，这些类型具有可用的DNase-seq和CTCF ChIP-seq数据用于训练。训练示例将是一个DNase敏感性（HS）峰（基因组的一个片段），TF是否结合/未结合的二进制标签将从ChIP-seq数据中导出。
- en: 'To summarize the overall data flow: the main training/test examples will be
    derived from the DNase-seq data. Each region of open chromatin (an interval on
    the genome) will be used to generate a prediction of whether a particular TF in
    a particular tissue type will be bound there. To do so, we spatially join the
    ChIP-seq data to the DNase-seq data; every overlap is a positive label for the
    DNase seq objects. Finally, to improve the prediction accuracy, we generate an
    additional feature at each interval in the DNase-seq data—distance to a transcription
    start site (using the GENCODE dataset). The feature is added into the training
    examples by performing a spatial join (with a possible aggregation).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 总结整体数据流程：主要的训练/测试样本将从DNase-seq数据中派生。每个开放染色质区域（基因组上的一个区间）将用于生成是否会在那里结合特定组织类型中的特定转录因子的预测。为此，我们将ChIP-seq数据空间连接到DNase-seq数据；每个重叠都是DNase序列对象的正标签。最后，为了提高预测准确性，我们在DNase-seq数据的每个区间中生成一个额外的特征——到转录起始位点的距离（使用GENCODE数据集）。通过执行空间连接（可能进行聚合），将该特征添加到训练样本中。
- en: 'We will use data from the following cell lines:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下细胞系的数据：
- en: GM12878
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GM12878
- en: Commonly studied lymphoblastoid cell line
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 常研究的淋巴母细胞系
- en: K562
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: K562
- en: Female chronic myelogenous leukemia
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 女性慢性髓系白血病
- en: BJ
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: BJ
- en: Skin fibroblast
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤成纤维细胞
- en: HEK293
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: HEK293
- en: Embryonic kidney
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 胚胎肾
- en: H54
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: H54
- en: Glioblastoma
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 胶质母细胞瘤
- en: HepG2
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: HepG2
- en: Hepatocellular carcinoma
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 肝细胞癌
- en: 'First, we download the DNase data for each cell line in *.narrowPeak* format:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们下载每个细胞系的DNase数据，格式为*.narrowPeak*：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO4-1)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO4-1)'
- en: Streaming decompression
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 流式解压缩
- en: 'Next, we download the ChIP-seq data for the CTCF TF, also in *.narrowPeak*
    format, and the GENCODE data, in GTF format:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们下载CTCF TF的ChIP-seq数据，也是*.narrowPeak*格式，以及GTF格式的GENCODE数据：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note how we unzip the stream of data with `gunzip` on the way to depositing
    it in our filesystem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们如何在将数据存入文件系统的过程中使用`gunzip`解压缩数据流。
- en: 'From all of this raw data, we want to generate a training set with a schema
    like the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从所有这些原始数据中，我们希望生成一个具有以下模式的训练集：
- en: Chromosome
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 染色体
- en: Start
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 起始位点
- en: End
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结束
- en: Distance to closest transcription start site (TSS)
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 距离最近的转录起始位点（TSS）
- en: TF identity (always “CTCF” in this case)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TF标识（在本例中始终为“CTCF”）
- en: Cell line
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 细胞系
- en: TF binding status (boolean; the target variable)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TF结合状态（布尔值；目标变量）
- en: 'This dataset can easily be converted into a DataFrame to carry into a machine
    learning library. Since we need to generate the data for multiple cell lines,
    we will define a DataFrame for each cell line individually and concatenate them
    at the end:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以轻松转换为DataFrame，用于输入到机器学习库中。由于我们需要为多个细胞系生成数据，我们将为每个细胞系单独定义一个DataFrame，并在最后进行连接：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We define a utility function and a broadcast variable that will be used to
    generate the features:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个实用函数和一个广播变量，用于生成特征：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we have loaded the data necessary for defining our training examples,
    we define the body of the “loop” for computing the data on each cell line. Note
    how we read the text representations of the ChIP-seq and DNase data, because the
    datasets are not so large that they will hurt performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经加载了定义我们训练样本所需的数据，我们定义了计算每个细胞系数据的“循环”的主体。请注意我们如何读取ChIP-seq和DNase数据的文本表示，因为这些数据集并不大，不会影响性能。
- en: 'To do so, we load the DNase and ChIP-seq data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们加载DNase和ChIP-seq数据：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-1)'
- en: '`FeatureDataset`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`FeatureDataset`'
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO5-2)'
- en: Columns in Dnase DataFrame
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Dnase DataFrame中的列
- en: Sites that overlap a ChIP-seq peak, as defined by a `ReferenceRegion` in `chipseq_data`,
    have TF binding sites and are therefore labeled `true`, while the rest of the
    sites are labeled `false`. This is accomplished using the 1D spatial join primitives
    provided in the ADAM API. The join functionality requires an RDD that is keyed
    by a `ReferenceRegion` and will produce tuples that have overlapping regions,
    according to usual join semantics (e.g., inner versus outer).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `chipseq_data` 中的 `ReferenceRegion` 重叠的位点具有 TF 结合位点，因此标记为 `true`，而其余的位点标记为
    `false`。这是通过 ADAM API 中提供的 1D 空间连接原语来实现的。连接功能需要一个按 `ReferenceRegion` 键入的 RDD，并将生成根据通常连接语义（例如内连接与外连接）重叠区域的元组。
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we compute the final set of features on each DNase peak:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在每个 DNase 峰上计算最终的特征集：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-1)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-1)'
- en: Left join with `tss_df` created earlier.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前创建的 `tss_df` 进行左连接。
- en: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-2)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_analyzing_genomics_data___span_class__keep_together__and_the_bdg_project__span__CO6-2)'
- en: Get the closest TSS distance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 获取最接近的 TSS 距离。
- en: 'This final DF is computed in each pass of the loop over the cell lines. Finally,
    we union each DF from each cell line and cache this data in memory in preparation
    for training models off of it:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环遍历各个细胞系时，每次通过后计算这个最终的 DF。最后，我们将每个细胞系的每个 DF 合并并将这些数据缓存在内存中，以便为训练模型做准备。
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: At this point, the data in `preTrainingData` can be normalized and converted
    into a DataFrame for training a classifier, as described in [“Random Forests”](ch04.xhtml#RandomDecisionForests).
    Note that you should perform cross-validation, where in each fold, you hold out
    the data from one of the cell lines.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`preTrainingData` 中的数据可以被归一化并转换为一个 DataFrame，用于训练分类器，如 [“Random Forests”](ch04.xhtml#RandomDecisionForests)
    中所述。请注意，应执行交叉验证，在每个折叠中，您应保留一个细胞系的数据。
- en: Where to Go from Here
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来该去哪里
- en: Many computations in genomics fit nicely into the PySpark computational paradigm.
    When you’re performing ad hoc analysis, the most valuable contribution that projects
    like ADAM provide is the set of Avro schemas that represents the underlying analytical
    objects (along with the conversion tools). We saw how once data is converted into
    the corresponding Avro schemas, many large-scale computations become relatively
    easy to express and distribute.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基因组学中的计算都很适合于 PySpark 的计算范式。当您进行即席分析时，像 ADAM 这样的项目最有价值的贡献是提供了表示底层分析对象的 Avro
    模式集合（以及转换工具）。我们看到一旦数据转换为相应的 Avro 模式，许多大规模计算变得相对容易表达和分布。
- en: While there may still be a relative dearth of tools for performing scientific
    research on PySpark, there do exist a few projects that could help avoid reinventing
    the wheel. We explored the core functionality implemented in ADAM, but the project
    already has implementations for the entire GATK best-practices pipeline, including
    indel realignment, and deduplication. In addition to ADAM, the Broad Institute
    is now developing major software projects using Spark, including the newest version
    of the [GATK4](https://oreil.ly/hGR87) and a project called [Hail](https://oreil.ly/V6Wpl)
    for large-scale population genetics computations. All of these tools are open
    source, so if you start using them in your own work, please consider contributing
    improvements!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在 PySpark 上进行科学研究的工具可能仍然相对匮乏，但确实存在一些项目，可以帮助避免重复造轮子。我们探索了 ADAM 中实现的核心功能，但该项目已经为整个
    GATK 最佳实践管道（包括插入缺失重整和去重复）提供了实现。除了 ADAM 外，Broad Institute 现在还在使用 Spark 开发主要软件项目，包括
    [GATK4](https://oreil.ly/hGR87) 的最新版本和一个名为 [Hail](https://oreil.ly/V6Wpl) 的用于大规模人群遗传学计算的项目。所有这些工具都是开源的，因此如果您开始在自己的工作中使用它们，请考虑贡献改进！
