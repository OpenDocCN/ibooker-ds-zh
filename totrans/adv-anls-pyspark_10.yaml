- en: Chapter 10\. Image Similarity Detection with Deep Learning and PySpark LSH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you encounter them on social media or e-commerce stores, images are
    integral to our digital lives. In fact, it was an image dataset—ImageNet—which
    was a key component for sparking the current deep learning revolution. A remarkable
    performance by a classification model in the ImageNet 2012 challenge was an important
    milestone and led to widespread attention. It is no wonder then that you are likely
    to encounter image data at some point as a data science practitioner.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will gain experience scaling a deep learning workflow for
    a visual task, namely, image similarity detection, with PySpark. The task of identifying
    images that are similar to each other comes intuitively to humans, but it is a
    complex computational task. At scale, it becomes even more difficult. In this
    chapter, we will introduce an approximate method for finding similar items called
    locality sensitive hashing, or LSH, and apply it to images. We’ll use deep learning
    to convert image data into a numerical vector representation. PySpark’s LSH algorithm
    will be applied to the resulting vectors, which will allow us to find similar
    images given a new input image.
  prefs: []
  type: TYPE_NORMAL
- en: On a high level, this example mirrors one of the approaches used by photo sharing
    apps such as Instagram and Pinterest for image similarity detection. This helps
    their users make sense of the deluge of visual data that exists on their platforms.
    This also depicts how a deep learning workflow can benefit from PySpark’s scalability.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by briefly introducing PyTorch, a deep learning framework. It has
    gained prominence in recent years for its relatively easier learning curve compared
    to other major low-level deep learning libraries. Then we’ll download and prepare
    our dataset. The dataset being used for our task is the Cars dataset released
    in 2013 by Stanford AI Lab. PyTorch will be used for image preprocessing. This
    will be followed by conversion of our input image data into a vector representation
    (image embeddings). We’ll then import the resulting embeddings into PySpark and
    transform them using the LSH algorithm. We’ll finish up by taking a new image
    and performing a nearest neighbors search using our LSH-transformed dataset to
    find similar images.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by introducing and setting up PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is a library for building deep learning projects. It emphasizes flexibility
    and allows deep learning models to be expressed in idiomatic Python. It found
    early adopters in the research community. Recently, it has grown into one of the
    most prominent deep learning tools across a broad range of applications due to
    its ease of use. Along with TensorFlow, it is the most popular library for deep
    learning as of now.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s simple and flexible interface enables fast experimentation. You can
    load data, apply transforms, and build models with a few lines of code. Then,
    you have the flexibility to write customized training, validation, and test loops
    and deploy trained models with ease. It is consistently being used in professional
    contexts for real-world, mission-critical work. Being able to use GPUs (graphical
    processing units) for training resource-intensive models has been a big factor
    for making deep learning popular. PyTorch provides great GPU support, although
    we won’t need that for our task.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the [PyTorch website](https://oreil.ly/CHkJo), you can easily obtain the
    installation instructions based on your system configuration, as shown in [Figure 10-1](#pytorch_installation_cpu_support).
  prefs: []
  type: TYPE_NORMAL
- en: '![PyTorch installation CPU support](assets/aaps_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. PyTorch installation, CPU support
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Execute the provided command and follow the instructions for your configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will not be relying on a GPU and, hence, will choose CPU as a compute platform.
    If you have a GPU setup that you want to use, choose options accordingly to obtain
    the required instructions. We will not be needing Torchaudio for this chapter
    either, so we skip its installation.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using the [Stanford Cars dataset](https://oreil.ly/gxo8Q). It was
    released as part of the ICCV 2013 paper “3D Object Representations for Fine-Grained
    Categorization” by Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the images from Kaggle or using the source link provided by
    Stanford AI Lab.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it’s downloaded, unzip the train and test image directories and place
    them in a directory called *cars_data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get a CSV file containing labels for the training dataset [here](https://oreil.ly/UoHXh).
    Download it, rename it to *cars_train_data.csv*, and place it in the data directory.
    Let’s have a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Ignore all columns other than `Class` and `image`. The other columns are related
    to the original research project that this dataset was derived from and will not
    be used for our task.
  prefs: []
  type: TYPE_NORMAL
- en: Resizing Images Using PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we head further, we’ll need to preprocess our images. Preprocessing data
    is very common in machine learning since deep learning models (neural networks)
    expect the input to meet certain requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to apply a series of preprocessing steps, called *transforms*, to convert
    input images into the proper format for the models. In our case, we need them
    to be 224 x 224-pixel JPEG-formatted images, since that is a requirement for the
    ResNet-18 model that we’ll use in the next section. We perform this transformation
    using PyTorch’s Torchvision package in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we use a single transformation that resizes the image to fit within the
    neural networks. However, we can use the `Compose` transform to define a series
    of transforms used to preprocess our image too.
  prefs: []
  type: TYPE_NORMAL
- en: Our dataset is in place now. In the next section, we will convert our image
    data into a vector representation fit for use with PySpark’s LSH algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Model for Vector Representation of Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional neural networks, or CNNs, are the standard neural network architectures
    used for prediction when the input observations are images. We won’t be using
    them for any prediction task but rather for generating a vector representation
    of images. Specifically, we will use the ResNet-18 architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Residual Network (ResNet) was introduced by Shaoqing Ren, Kaiming He, Jian Sun,
    and Xiangyu Zhang in their 2015 paper “Deep Residual Learning for Image Recognition.”
    The 18 in ResNet-18 stands for the number of layers that exist in the neural network
    architecture. Other popular variants of ResNet include 34 and 50 layers. A larger
    number of layers results in improved performance at the cost of increased computation.
  prefs: []
  type: TYPE_NORMAL
- en: Image Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An *image embedding* is a representation of an image in a vector space. The
    basic idea is that if a given image is close to another image, their embedding
    will also be similar and close in the spatial dimension.
  prefs: []
  type: TYPE_NORMAL
- en: The image in [Figure 10-2](#ILSVRC_2012), [released by Andrej Karpathy](https://oreil.ly/YRhhT),
    shows how images can be represented in a lower dimensional space. As an example,
    you can notice vehicles near the top and birds in the bottom-left space.
  prefs: []
  type: TYPE_NORMAL
- en: '![ILSVRC 2012 image embeddings in a 2-D space](assets/aaps_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. ILSVRC 2012 image embeddings in a 2-D space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can obtain image embeddings from ResNet-18 by taking the output of its second-to-last,
    fully connected layer, which has a dimension of 512\. Next, we create a class
    that, provided an image, can return its numeric vector form representation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Convert images into the PyTorch tensor format.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Rescale the range of pixel values between 0 and 1\. The values for the mean
    and standard deviation (std) were precomputed based on the data used to train
    the model. Normalizing the image improves the accuracy of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: We now initialize the `Img2VecResnet18` class and apply the `getVec` method
    to all of the images to obtain their image embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For a larger dataset, you may want to sequentially write the vector output
    to a file rather than keeping it in memory to avoid an out-of-memory error. The
    data is manageable here, so we create a dictionary, which we save as a CSV file
    in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Since we are working locally, we went with the CSV format for saving the vector
    output. However, Parquet format is more appropriate for data of this nature. You
    could easily save the data in Parquet format by replacing `to_csv` with `to_parquet`
    in the previous code.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the required image embeddings, we can import them into PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Import Image Embeddings into PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the PySpark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the image embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'PySpark’s LSH implementation requires a vector column as an input. We can create
    one by combining the relevant columns in our dataframe using the `VectorAssembler`
    transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will use the LSH algorithm to create a way for us to
    find similar images from our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Image Similarity Search Using PySpark LSH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Locality sensitive hashing is an important class of hashing techniques, which
    is commonly used in clustering, approximate nearest neighbor search, and outlier
    detection with large datasets. Locality sensitive functions take two data points
    and decide whether or not they should be a candidate pair.
  prefs: []
  type: TYPE_NORMAL
- en: The general idea of LSH is to use a family of functions (“LSH families”) to
    hash data points into buckets so that the data points that are close to each other
    are in the same buckets with high probability, while data points that are far
    away from each other are very likely in different buckets. The data points that
    map to the same buckets are considered a candidate pair.
  prefs: []
  type: TYPE_NORMAL
- en: In PySpark, different LSH families are implemented in separate classes (e.g.,
    `MinHash` and `BucketedRandomProjection`), and APIs for feature transformation,
    approximate similarity join, and approximate nearest neighbor are provided in
    each class.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the BucketedRandomProjection implementation of LSH.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first create our model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the BucketedRandomProjection LSH implementation, the bucket length can be
    used to control the average size of hash buckets (and thus the number of buckets).
    A larger bucket length (i.e., fewer buckets) increases the probability of features
    being hashed to the same bucket (increasing the number of true and false positives).
  prefs: []
  type: TYPE_NORMAL
- en: 'We now transform the input DataFrame using the newly created LSH model object.
    The resulting DataFrame will contain a `hashes` column containing hashed representation
    of the image embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With our LSH-transformed dataset ready, we’ll put our work to the test in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Nearest Neighbor Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s try to find a similar image using a new image. For now, we will pick
    one from the input dataset itself ([Figure 10-3](#random-car)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Randomly picked car image](assets/aaps_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Randomly picked car image from our dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'First, we’ll need to convert the input image into a vector format using our
    `I⁠m⁠g⁠2⁠V⁠e⁠c​R⁠e⁠s⁠n⁠e⁠t⁠1⁠8` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we perform an approximate nearest neighbor search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the images in Figures [10-4](#result-image1) through [10-8](#result-image5)
    to see that the model gets it somewhat right already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Result image 1](assets/aaps_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Result image 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Result image 2](assets/aaps_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Result image 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Result image 3](assets/aaps_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Result image 3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Result image 4](assets/aaps_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Result image 4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Result image 5](assets/aaps_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. Result image 5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input image is on top of the list as one would expect.
  prefs: []
  type: TYPE_NORMAL
- en: Where to Go from Here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how PySpark can be combined with a modern deep learning
    framework to scale an image similarity detection workflow.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to improve this implementation. You can try using a
    better model or improving the preprocessing to get better quality of embeddings.
    Further, the LSH model can be tweaked. In a real-life setting, you may need to
    update the reference dataset consistently to account for new images coming into
    the system. The simplest way to do this is by running a batch job at periodic
    intervals to create new LSH models. You can explore all of these depending on
    your need and interest.
  prefs: []
  type: TYPE_NORMAL
