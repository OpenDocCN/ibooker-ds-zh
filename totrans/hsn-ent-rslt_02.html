<html><head></head><body><section data-pdf-bookmark="Chapter 2. Data Standardization" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_2">&#13;
<h1><span class="label">Chapter 2. </span>Data Standardization</h1>&#13;
&#13;
<p>As we discussed in <a data-type="xref" href="ch01.html#chapter_1">Chapter 1</a>, before we can successfully match or deduplicate data sources we need to ensure our data is presented in a consistent manner and that any anomalies are removed or corrected. We will use the term<a contenteditable="false" data-primary="data standardization" data-secondary="definition of term" data-type="indexterm" id="id307"/> <em>data standardization</em> to cover both the transformation of datasets into consistent formats and the cleansing of data to remove unhelpful extra characters that would otherwise interfere with the matching process.</p>&#13;
&#13;
<p>In this chapter, we will get hands on and work through a real-world example of this process. We will create our working environment, acquire the data we need, cleanse that data, and then perform a simple entity resolution exercise to allow us to perform some simple analysis. We will conclude by examining the performance of our data matching process and consider how we might improve it.</p>&#13;
&#13;
<p>First, let’s introduce our example and why we need entity resolution to solve it.</p>&#13;
&#13;
<section data-pdf-bookmark="Sample Problem" data-type="sect1"><div class="sect1" id="id13">&#13;
<h1>Sample Problem</h1>&#13;
&#13;
<p>Let’s work through an example problem to illustrate some of the common challenges we see in resolving entities between data sources and why data cleansing is an essential first step. As we are constrained to use openly available public sources of data, the example is slightly contrived but hopefully illustrates the need for entity resolution.</p>&#13;
&#13;
<p>Let’s imagine we are researching factors that may influence whether members of the House of Commons, the lower house of the Parliament of the United Kingdom (UK), are reelected. We surmise that politicians with an active<a contenteditable="false" data-primary="social media presence" data-type="indexterm" id="socialmedia02"/> social media presence might be more successful in securing reelection. For the purposes of this example, we are going to consider Facebook presence, and so we look at the last UK general election and examine how many politicians who held onto their seat have Facebook accounts.</p>&#13;
&#13;
<p>Wikipedia has a web page that lists the members of Parliament (MPs) returned at the 2019 general election, including whether they were reelected, but it lacks social media information for those individuals. However, the <a href="https://theyworkforyou.com">TheyWorkForYou website</a> does record information on current MPs, including links to their Facebook accounts. So if we combine these datasets we can begin to test our hypothesis that reelection and social media presence are related.</p>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h3>TheyWorkForYou</h3>&#13;
&#13;
<p>TheyWorkForYou was founded to make Parliament more accessible and accountable. TheyWorkForYou is run by mySociety, a UK charity that puts power in more people’s hands through the use of digital tools and data.</p>&#13;
</div>&#13;
&#13;
<p>How can we<a contenteditable="false" data-primary="joins/joining" data-secondary="multiple datasets" data-type="indexterm" id="id308"/> join these two datasets? Although both datasets include the name of the constituency that each MP represents, we can’t use this as a common key, because since the 2019 general election, a number of by-elections have taken place, returning new MPs.<sup><a data-type="noteref" href="ch02.html#id309" id="id309-marker">1</a></sup> These new members may have Facebook accounts but should not be considered in the reelection population as this might skew our analysis. Therefore, we need to connect our data by matching the names of the MPs between the two sets of records, i.e., resolving these entities so that we can create a single combined record for each MP.<a contenteditable="false" data-primary="" data-startref="socialmedia02" data-type="indexterm" id="id310"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Environment Setup" data-type="sect1"><div class="sect1" id="id14">&#13;
<h1>Environment Setup</h1>&#13;
&#13;
<p>Our<a contenteditable="false" data-primary="data standardization" data-secondary="environment setup" data-type="indexterm" id="DSenvset02"/><a contenteditable="false" data-primary="environment setup" data-secondary="for data standardization" data-secondary-sortas="data standardization" data-type="indexterm" id="ESdstand02"/> first task is to set up our entity resolution environment. In this book, we will be using Python and the JupyterLab IDE.</p>&#13;
&#13;
<p>To begin, you’ll need<a contenteditable="false" data-primary="Python" data-secondary="downloading and installing" data-type="indexterm" id="id311"/> Python installed on your machine. If you don’t already have it, you can download it from <a href="http://www.python.org">their website</a>.<sup><a data-type="noteref" href="ch02.html#id312" id="id312-marker">2</a></sup></p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Add Python to PATH</h1>&#13;
&#13;
<p>If installing Python for the first time, make sure to select the “Add Python to PATH” option to ensure you can run Python from your command line.</p>&#13;
</div>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Git version control system" data-type="indexterm" id="id313"/><a contenteditable="false" data-primary="code examples, obtaining and using" data-type="indexterm" id="id314"/> download the code examples that accompany this book it is convenient to use the Git version control system. A guide to installing Git can be found on the <a href="https://github.com/git-guides/install-git"> GitHub website</a>.</p>&#13;
&#13;
<p>Once Git is installed, you can clone (that is, take a copy of) the GitHub repository that accompanies this book onto your machine. Run this command from the parent directory of your choice:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>git clone https://github.com/mshearer0/HandsOnEntityResolution</strong></pre>&#13;
&#13;
<p>This will create a subdirectory called <em>HandsOnEntityResolution</em>.</p>&#13;
&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
&#13;
<h3>Python Virtual Environment</h3>&#13;
&#13;
<p>I<a contenteditable="false" data-primary="Python" data-secondary="virtual environment" data-type="indexterm" id="id315"/> recommend you use a virtual Python environment to work through the examples in this book. This will allow you to maintain the necessary Python package configuration without interfering with any other projects you may have. The following command creates a new environment in the <em>HandsOnEntityResolution</em> directory created by Git:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>python -m venv HandsOnEntityResolution</strong></pre>&#13;
&#13;
<p>To activate the environment, run the following:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>.\HandsOnEntityResolution\Scripts\activate.bat&#13;
(Windows)</strong>&#13;
&#13;
&gt;&gt;&gt;<strong>source HandsOnEntityResolution/bin/activate &#13;
(Linux)</strong></pre>&#13;
&#13;
<p>This will prefix your command prompt to show the environment name based on the directory name:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>(HandsOnEntityResolution) &#13;
    your_path\HandsOnEntityResolution</strong></pre>&#13;
&#13;
<p>Once you’ve finished, it’s important to deactivate the environment:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>deactivate (Windows)</strong>&#13;
&#13;
&gt;&gt;&gt;<strong>deactivate (Linux)</strong></pre>&#13;
</div>&#13;
&#13;
<p>Next, change into this directory:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>cd HandsOnEntityResolution</strong></pre>&#13;
&#13;
<p>To set up our<a contenteditable="false" data-primary="JupyterLab" data-secondary="setup" data-type="indexterm" id="id316"/> JupyterLab code environment and the packages required, we will use the<a contenteditable="false" data-primary="Python" data-secondary="pip package manager" data-type="indexterm" id="id317"/><a contenteditable="false" data-primary="pip package manager" data-type="indexterm" id="id318"/><a contenteditable="false" data-primary="package managers" data-type="indexterm" id="id319"/> Python package manager pip, which should be included with your Python installation. You can check using:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>python -m pip --version</strong>&#13;
pip 23.0.1 from your_path\HandsOnEntityResolution\lib\&#13;
   site-packages\pip (python 3.7)</pre>&#13;
&#13;
<p>You can then install the packages you will need throughout the book from the <em>requirements.txt</em> file using:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>pip install -r requirements.txt</strong></pre>&#13;
&#13;
<p>Next, configure a<a contenteditable="false" data-primary="Python" data-secondary="kernel configuration" data-type="indexterm" id="id320"/><a contenteditable="false" data-primary="kernel configuration" data-type="indexterm" id="id321"/> Python kernel associated with our virtual environment for our notebooks to pick up:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>python -m ipykernel install --user&#13;
   --name=handsonentityresolution</strong></pre>&#13;
&#13;
<p>Then<a contenteditable="false" data-primary="JupyterLab" data-secondary="starting" data-type="indexterm" id="id322"/> start JupyterLab with:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>jupyter-lab</strong></pre>&#13;
&#13;
<p>While<a contenteditable="false" data-primary="JupyterLab" data-secondary="documentation" data-type="indexterm" id="id323"/> it’s pretty self-explanatory, instructions on how to get started with Jupyter are available in <a class="link" href="https://docs.jupyter.org/en/latest">the documentation</a>.<a contenteditable="false" data-primary="" data-startref="DSenvset02" data-type="indexterm" id="id324"/><a contenteditable="false" data-primary="" data-startref="ESdstand02" data-type="indexterm" id="id325"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Acquiring Data" data-type="sect1"><div class="sect1" id="id107">&#13;
<h1>Acquiring Data</h1>&#13;
&#13;
<p>Now that<a contenteditable="false" data-primary="data standardization" data-secondary="acquiring data" data-type="indexterm" id="DSacquir02"/> we have our environment configured, our next task is to acquire the data we need. It’s often the case that the data we need comes in a variety of formats and presentations. The examples included in this book will illustrate how to deal with some of the most common formats we encounter.</p>&#13;
&#13;
<section data-pdf-bookmark="Wikipedia Data" data-type="sect2"><div class="sect2" id="id15">&#13;
<h2>Wikipedia Data</h2>&#13;
&#13;
<p>Opening<a contenteditable="false" data-primary="data, acquiring" data-secondary="Wikipedia data" data-type="indexterm" id="id326"/><a contenteditable="false" data-primary="Wikipedia data" data-secondary="acquiring" data-type="indexterm" id="id327"/> <em>Chapter2.ipynb</em> in our Jupyter environment, we start by defining the Wikipedia URL for the list of MPs returned in the 2019 UK general election:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
url = "https://en.wikipedia.org/wiki/&#13;
       List_of_MPs_elected_in_the_2019_United_Kingdom_general_election"</pre>&#13;
&#13;
<p>Then we can import the requests and Beautiful Soup Python packages and use them to download a copy of the Wikipedia text. Then run an <code>html parser</code> to extract all the tables present on the page:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
import requests&#13;
from bs4 import BeautifulSoup&#13;
&#13;
website_url = requests.get(url).text&#13;
soup = BeautifulSoup(website_url,'html.parser')&#13;
tables = soup.find_all('table')</pre>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Beautiful Soup</h1>&#13;
&#13;
<p>Beautiful Soup<a contenteditable="false" data-primary="Beautiful Soup" data-type="indexterm" id="id328"/><a contenteditable="false" data-primary="Python" data-secondary="Beautiful Soup package" data-type="indexterm" id="id329"/> is a Python package that makes it easy to<a contenteditable="false" data-primary="web scraping" data-type="indexterm" id="id330"/><a contenteditable="false" data-primary="scraping web pages" data-type="indexterm" id="id331"/> scrape information from web pages. More details are available <a href="https://oreil.ly/YB8H3">online</a>.</p>&#13;
</div>&#13;
&#13;
<p>Next, we need to find the table we want within the page. In this case we select the table that includes the text “Member returned” (a column name). Within this table, we extract the column names as headers and then iterate through all the remaining rows and elements, building a list of lists. These<a contenteditable="false" data-primary="pandas DataFrames" data-secondary="selecting Wikipedia data" data-type="indexterm" id="id332"/> lists are then loaded into a pandas DataFrame, setting the extracted headers as DataFrame column names:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
import pandas as pd&#13;
&#13;
for table in tables:&#13;
   if 'Member returned' in table.text:&#13;
      headers = [header.text.strip() for header in table.find_all('th')]&#13;
      headers = headers[:5]&#13;
      dfrows = []&#13;
      table_rows = table.find_all('tr')&#13;
      for row in table_rows:&#13;
         td = row.find_all('td')&#13;
         dfrow = [row.text for row in td if row.text!='\n']&#13;
         dfrows.append(dfrow)&#13;
&#13;
df_w = pd.DataFrame(dfrows, columns=headers)</pre>&#13;
&#13;
<p>The result is a pandas DataFrame, shown in <a data-type="xref" href="#fig-2-1">Figure 2-1</a>, which we can examine using the <code>info</code> method.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-1"><img alt="" class="iimagesch02ch02winfopng" src="assets/hoer_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>Wikipedia MP information</h6>&#13;
</div></figure>&#13;
&#13;
<p>We have 652 entries of 5 columns. This looks encouraging because in each column, 650 rows have nonnull values, which matches the number of UK House of Commons parliamentary constituencies.</p>&#13;
&#13;
<p>Finally, we can simplify our dataset by retaining only the columns we need:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w = df_w[['Constituency','Member returned','Notes']]</pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="TheyWorkForYou Data" data-type="sect2"><div class="sect2" id="id108">&#13;
<h2>TheyWorkForYou Data</h2>&#13;
&#13;
<p>Now<a contenteditable="false" data-primary="TheyWorkForYou data" data-secondary="acquiring" data-type="indexterm" id="TWFYstand02"/><a contenteditable="false" data-primary="pandas DataFrames" data-secondary="selecting TheyWorkForYou data" data-type="indexterm" id="PDFworkforyou02"/><a contenteditable="false" data-primary="data, acquiring" data-secondary="TheyWorkForYou data" data-type="indexterm" id="DAworkforyou02"/> we can move on to downloading our second dataset and loading it into a separate DataFrame, as shown in <a data-type="xref" href="#fig-2-2">Figure 2-2</a>:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
url = "https://www.theyworkforyou.com/mps/?f=csv"&#13;
df_t = pd.read_csv(url, header=0)</pre>&#13;
&#13;
<figure><div class="figure" id="fig-2-2"><img alt="" class="iimagesch02ch02tinfopng" src="assets/hoer_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>TheyWorkForYou MP information</h6>&#13;
</div></figure>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Post 2024/25 UK General Election</h1>&#13;
&#13;
<p>If you’re reading this book after the 2024/25 UK general election, then the TheyWorkForYou website will likely be updated with the new MPs. If you’re following along on your own machine, then please use the <em>mps_they_raw.csv</em> file supplied in the GitHub repository that accompanies this book. The raw Wikipedia data <em>mps_wiki_raw.csv</em> is also provided.</p>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#fig-2-3">Figure 2-3</a> lists the first few rows of the DataFrame so that we can see information these fields typically contain.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-3"><img alt="" class="iimagesch02ch02theadpng" src="assets/hoer_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>First five rows of the TheyWorkForYou dataset</h6>&#13;
</div></figure>&#13;
&#13;
<p>To discover whether each MP has an associated Facebook account we need to follow the link in the URI column to look up their TheyWorkForYou homepage. We’ll need to do this for each row, so we define a function that we can apply along the axis of the DataFrame.</p>&#13;
&#13;
<section data-pdf-bookmark="Adding Facebook links" data-type="sect3"><div class="sect3" id="id16">&#13;
<h3>Adding Facebook links</h3>&#13;
&#13;
<p>The function uses the same<a contenteditable="false" data-primary="Beautiful Soup" data-type="indexterm" id="id333"/><a contenteditable="false" data-primary="Python" data-secondary="Beautiful Soup package" data-type="indexterm" id="id334"/> Beautiful Soup package we used to parse the Wikipedia web page. In this case, we extract all the links to <em>facebook.com</em>. We then examine the first link. If this link is the account of TheyWorkForYou, then the site doesn’t have a Facebook account listed for the MP, so we return a nil string; if it does, then we return that link:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
def facelink(url):&#13;
   website_url = requests.get(url).text&#13;
   soup = BeautifulSoup(website_url,'html.parser')&#13;
   flinks = [f"{item['href']}" for item in soup.select&#13;
      ("a[href*='facebook.com']")]&#13;
   if flinks[0]!="https://www.facebook.com/TheyWorkForYou":&#13;
      return(flinks[0])&#13;
   else:&#13;
      return("")</pre>&#13;
&#13;
&#13;
&#13;
<p>We can apply this function to every row in the DataFrame using the <code>apply</code> method to call the <code>facelink</code> function, passing the <code>URI</code> value as the URL. The value returned from the function is added to a new column that Flink appended to the DataFrame.</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_t['Flink'] = df_t.apply(lambda x: facelink(x.URI), axis=1)</pre>&#13;
&#13;
<p>Be patient—this function has to do quite a bit of work, so it may take a few minutes to run on your machine. Once this completes we can view the first few rows again, as shown in <a data-type="xref" href="#fig-2-4">Figure 2-4</a>, to check if we are getting the Facebook links we expect.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-4"><img alt="" class="iimagesch02ch02tflinkpng" src="assets/hoer_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>First five rows of the TheyWorkForYou dataset with Facebook links</h6>&#13;
</div></figure>&#13;
&#13;
<p>Finally, we can simplify our dataset by retaining only the columns we<a contenteditable="false" data-primary="" data-startref="TWFYstand02" data-type="indexterm" id="id335"/><a contenteditable="false" data-primary="" data-startref="DSacquir02" data-type="indexterm" id="id336"/><a contenteditable="false" data-primary="" data-startref="PDFworkforyou02" data-type="indexterm" id="id337"/><a contenteditable="false" data-primary="" data-startref="DAworkforyou02" data-type="indexterm" id="id338"/> need:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_t = df_t[['Constituency','First name','Last name','Flink']]</pre>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Cleansing Data" data-type="sect1"><div class="sect1" id="id109">&#13;
<h1>Cleansing Data</h1>&#13;
&#13;
<p>Now<a contenteditable="false" data-primary="data standardization" data-secondary="cleansing data" data-type="indexterm" id="DScleansing02"/> that we have our raw datasets we can begin our data cleansing process. We will perform some initial cleansing on the Wikipedia dataset first and then the TheyWorkForYou data. We will then attempt to<a contenteditable="false" data-primary="joins/joining" data-secondary="cleansing data prior to" data-type="indexterm" id="JJcleans02"/> join these datasets and see what further inconsistencies are revealed that we need to standardize.</p>&#13;
&#13;
<section data-pdf-bookmark="Wikipedia" data-type="sect2"><div class="sect2" id="id17">&#13;
<h2>Wikipedia</h2>&#13;
&#13;
<p>Let’s<a contenteditable="false" data-primary="data, cleansing" data-secondary="Wikipedia data" data-type="indexterm" id="id339"/><a contenteditable="false" data-primary="Wikipedia data" data-secondary="cleansing" data-type="indexterm" id="id340"/> have a look at the first and last few rows in the Wikipedia dataset, as shown in <a data-type="xref" href="#fig-2-5">Figure 2-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-5"><img alt="" class="iimagesch02ch02wheadtailpng" src="assets/hoer_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>First and last 5 rows of the Wikipedia data</h6>&#13;
</div></figure>&#13;
&#13;
<p>The first task in our cleansing process is to standardize our column names:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w = df_w.rename(columns={ 'Member returned' : 'Fullname'})</pre>&#13;
&#13;
<p>We can also see that the output of our parser has a blank row at the start and end of our DataFrame, and it appears we have <code>\n</code> characters appended to each element. These additions would clearly interfere with our match, so they need to be removed.</p>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="blank rows, removing" data-type="indexterm" id="id341"/><a contenteditable="false" data-primary="pandas DataFrames" data-secondary="removing blank rows" data-type="indexterm" id="id342"/> remove the blank rows we can use:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df = df.dropna()</pre>&#13;
&#13;
<p>To remove the trailing <code>\n</code> characters:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w['Constituency'] = df_w['Constituency'].str.rstrip("\n")&#13;
df_w['Fullname'] = df_w['Fullname'].str.rstrip("\n")</pre>&#13;
&#13;
<p>To be sure we now have a clean <code>Fullname</code> we can check for any other <code>\n</code> characters.</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w[df_w['Fullname'].astype(str).str.contains('\n')]</pre>&#13;
&#13;
<p>This simple check shows that we also have leading values that we need to remove:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w['Fullname'] = df_w['Fullname'].str.lstrip("\n")</pre>&#13;
&#13;
<p>Our next task is to split our <code>Fullname</code> into <code>Firstname</code> and <code>Lastname</code> so that we can match these values independently. For the purposes of this example, we are going to use a simple method, selecting the first substring as the <code>Firstname</code> and the remaining substrings, separated by spaces, as the <code>Lastname</code>.</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w['Firstname'] = df_w['Fullname'].str.split().str[0]&#13;
df_w['Lastname'] = df_w['Fullname'].astype(str).apply(lambda x:&#13;
   ' '.join(x.split()[1:]))</pre>&#13;
&#13;
<p>We<a contenteditable="false" data-primary="names, checking for spaces in" data-type="indexterm" id="id343"/><a contenteditable="false" data-primary="whitespace, checking for" data-type="indexterm" id="id344"/><a contenteditable="false" data-primary="spaces, checking for in names" data-type="indexterm" id="id345"/> can check how well this basic method has worked by looking for <code>Lastname</code> entries that contain spaces. <a data-type="xref" href="#fig-2-6">Figure 2-6</a> shows the remaining <code>Lastname</code> entries with spaces present.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-6"><img alt="" class="iimagesch02ch02wlastnamepng" src="assets/hoer_0206.png"/>&#13;
<h6><span class="label">Figure 2-6. </span>Check for compound <code>Lastname</code> entries in Wikipedia data</h6>&#13;
</div></figure>&#13;
&#13;
<p>We now have a sufficiently clean dataset to attempt a first match, so we’ll move on to our second dataset.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="TheyWorkForYou" data-type="sect2"><div class="sect2" id="id18">&#13;
<h2>TheyWorkForYou</h2>&#13;
&#13;
<p>As<a contenteditable="false" data-primary="data, cleansing" data-secondary="TheyWorkForYou data" data-type="indexterm" id="id346"/><a contenteditable="false" data-primary="pandas DataFrames" data-secondary="standardizing column names" data-type="indexterm" id="id347"/> we saw earlier, the TheyWorkForYou data is already pretty clean, so at this stage all we need to do is standardize the column names with those of the previous DataFrame. This will make our life easier as we attempt to<a contenteditable="false" data-primary="" data-startref="DScleansing02" data-type="indexterm" id="id348"/><a contenteditable="false" data-primary="" data-startref="JJcleans02" data-type="indexterm" id="id349"/> match:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_t = df_t.rename(columns={'Last name' : 'Lastname',&#13;
                             'First name' : 'Firstname'})</pre>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Attribute Comparison" data-type="sect1"><div class="sect1" id="id110">&#13;
<h1>Attribute Comparison</h1>&#13;
&#13;
<p>Now<a contenteditable="false" data-primary="data standardization" data-secondary="attribute comparison" data-type="indexterm" id="id350"/><a contenteditable="false" data-primary="attribute comparison" data-secondary="data standardization" data-type="indexterm" id="id351"/> that we have two similarly formatted DataFrames, we can experiment with the next stage of the entity resolution process. Because our datasets are small we don’t need to employ record blocking, and so we can proceed directly to try a simple exact match of <code>Firstname</code>, <code>Lastname</code>, and <code>Constituency</code>. The <code>merge</code> method (similar to a database <code>join</code>) does this exact matching for us:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
len(df_w.merge(df_t, on=['Constituency','Firstname','Lastname']))&#13;
599</pre>&#13;
&#13;
<p>We find that 599 of 650 are perfect matches of all three attributes—not bad! Matching on just <code>Constituency</code> and <code>Lastname</code> gives us 607 perfect matches, so we clearly have 8 mismatching <code>Firstname</code> entries:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
len(df_w.merge(df_t, on=['Constituency','Lastname']))&#13;
607</pre>&#13;
&#13;
<p>Repeating the process for the remaining permutations of <code>Firstname</code>, <code>Lastname</code>, and <code>Constituency</code> gives us the Venn diagram of match counts shown in <a data-type="xref" href="#fig-2-7">Figure 2-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-7"><img alt="" class="iimagesch02ch02vennpng" src="assets/hoer_0207.png"/>&#13;
<h6><span class="label">Figure 2-7. </span>Venn diagram</h6>&#13;
</div></figure>&#13;
&#13;
<p>A simple join on <code>Firstname</code> gives 2,663 matches and the equivalent match on <span class="keep-together"><code>Lastname</code></span> has 982 matches. These counts exceed the number of MPs and arise because of repeated common names that match more than once between the two datasets.</p>&#13;
&#13;
<p>We have 599 matches out of 650 so far, but can we do better? Let’s start with examining the <code>Constituency</code> attribute in our datasets. As a categorical variable, we would expect this to be pretty easy to match:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
len(df_w.merge(df_t, on=['Constituency'] ))&#13;
623</pre>&#13;
&#13;
<p>We have 623 matches, leaving 27 unmatched. Why? Surely we’d expect the same constituencies to be present in both datasets, so what is going wrong?</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Constituency" data-type="sect1"><div class="sect1" id="id19">&#13;
<h1>Constituency</h1>&#13;
&#13;
<p>Let’s<a contenteditable="false" data-primary="data standardization" data-secondary="resolving mismatches" data-type="indexterm" id="DSmismatch02"/><a contenteditable="false" data-primary="Wikipedia data" data-secondary="resolving mismatches" data-type="indexterm" id="WDmismatch02"/><a contenteditable="false" data-primary="TheyWorkForYou data" data-secondary="resolving mismatches" data-type="indexterm" id="TWFYmismatch02"/><a contenteditable="false" data-primary="mismatches, resolving" data-type="indexterm" id="mismatch02"/> have a look at the first five of the unmatched population in both datasets. To do this we perform an<a contenteditable="false" data-primary="outer joins" data-type="indexterm" id="id352"/><a contenteditable="false" data-primary="joins/joining" data-secondary="outer join" data-type="indexterm" id="id353"/><a contenteditable="false" data-primary="pandas DataFrames" data-secondary="outer joins" data-type="indexterm" id="id354"/> outer join between the DataFrames using the <code>Constituency</code> attribute and then select those records found in either the right (Wikipedia) or left (TheyWorkForYou) DataFrame. The results are shown in <a data-type="xref" href="#fig-2-8">Figure 2-8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-8"><img alt="" class="iimagesch02ch02wconstituencypng" src="assets/hoer_0208.png"/>&#13;
<h6><span class="label">Figure 2-8. </span>Constituency mismatches</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can see that the first dataset from the TheyWorkForYou website has commas embedded in the constituency names, whereas the Wikipedia data does not. This explains why they don’t match. To ensure consistency, let’s remove any commas from both DataFrames:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_t['Constituency'] = df_t['Constituency'].str.replace(',', '')&#13;
df_w['Constituency'] = df_w['Constituency'].str.replace(',', '')</pre>&#13;
&#13;
<p>After applying this cleansing we have a perfect match on all 650 constituencies:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
len(df_w.merge(df_t, on=['Constituency']))&#13;
650</pre>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Case Sensitivity</h1>&#13;
&#13;
<p>In<a contenteditable="false" data-primary="case sensitivity" data-type="indexterm" id="id355"/><a contenteditable="false" data-primary="upper case, converting to" data-type="indexterm" id="id356"/><a contenteditable="false" data-primary="lower case, converting to" data-type="indexterm" id="id357"/> this simple example, we have matching case conventions (e.g., initial capitalization) between the two datasets. In many situations this won’t be the case, and you’ll need to standardize on upper- or lowercase characters. We’ll see how this can be done in later chapters.</p>&#13;
</div>&#13;
&#13;
<p>Repeating our perfect match on all three attributes, we can now match 624 records:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
len(df_w.merge(df_t, on=['Constituency','Firstname','Lastname']))&#13;
624</pre>&#13;
&#13;
<p>What about the other 26?</p>&#13;
&#13;
<p>A little domain knowledge is useful here. As we considered at the start of the chapter, between the election in 2019 and the time of writing, a number of by-elections took place. If we look at constituencies where neither the first name nor the last name matches then, for this simple example at least, we can identify likely candidates, as shown in <a data-type="xref" href="#fig-2-9">Figure 2-9</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-9"><img alt="" class="iimagesch02ch02namematchpng" src="assets/hoer_0209.png"/>&#13;
<h6><span class="label">Figure 2-9. </span>Potential by-elections</h6>&#13;
</div></figure>&#13;
&#13;
<p>Of our 14 by-election candidates, we have 13 cases where the names are entirely different, suggesting we are correct to discount them, but the candidate for Newton Abbot appears to be a potential match because the middle name “Morris” has been included in the last name in one dataset and in the first name in the other, frustrating our exact match on both attributes.</p>&#13;
&#13;
<p>In fact, we can verify our conclusion with data from the <a href="https://oreil.ly/eWhWf">UK Parliament website</a>. This confirms that by-elections have been held in the matching constituencies. So this explains 13 of our 26 unmatched records—what about the rest? Let’s pick out where either the first name or the last name matches but the other doesn’t. This subset is shown in <a data-type="xref" href="#fig-2-10">Figure 2-10</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-10"><img alt="" class="iimagesch02ch02firstorlastpng" src="assets/hoer_0210.png"/>&#13;
<h6><span class="label">Figure 2-10. </span>Potential by-elections</h6>&#13;
</div></figure>&#13;
&#13;
<p>We<a contenteditable="false" data-primary="middle initials" data-type="indexterm" id="id358"/> can see that the remaining 12 records, as listed in <a data-type="xref" href="#table-2-1">Table 2-1</a>, display a variety of the matching issues that we discussed in <a data-type="xref" href="ch01.html#chapter_1">Chapter 1</a>.</p>&#13;
&#13;
<table id="table-2-1">&#13;
	<caption><span class="label">Table 2-1. </span>Matching issues summary table</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th scope="col">Matching issue</th>&#13;
			<th scope="col"><b>TheyWorkForYou</b></th>&#13;
			<th scope="col"><b>Wikipedia</b></th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>Shortened names</td>&#13;
			<td>Dan</td>&#13;
			<td>Daniel</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td> </td>&#13;
			<td>Tan</td>&#13;
			<td>Tanmanjeet</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td> </td>&#13;
			<td>Liz</td>&#13;
			<td>Elizabeth</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td> </td>&#13;
			<td>Chris</td>&#13;
			<td>Christopher</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td> </td>&#13;
			<td>Nus</td>&#13;
			<td>Nusrat</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Middle initials included</td>&#13;
			<td>Diana R.</td>&#13;
			<td>Diana</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td> </td>&#13;
			<td>Jeffrey M.</td>&#13;
			<td>Jeffrey</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Middle name included</td>&#13;
			<td>Preet Kaur</td>&#13;
			<td>Preet</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td> </td>&#13;
			<td>John Martin</td>&#13;
			<td>John</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Last name suffix</td>&#13;
			<td>Paisley Jnr</td>&#13;
			<td>Paisley</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Double-barreled last names</td>&#13;
			<td>Docherty</td>&#13;
			<td>Docherty-Hughes</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>There is one remaining mismatch that is really hard to resolve: a change of the last name Kniveton (previously Griffiths) in the Burton constituency. Now we have accounted for all 650 constituencies.</p>&#13;
&#13;
<p>If we further cleanse the <code>Firstname</code> from the TheyWorkForYou data, removing any middle initials or names, we can improve our matches still further:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_t['Firstname'] = df_t['Firstname'].str.split().str[0]</pre>&#13;
&#13;
<p>We can now match another four records:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_resolved = df_w.merge(df_t, on=['Firstname','Lastname'] )&#13;
&#13;
len(df_resolved)&#13;
628</pre>&#13;
&#13;
<p>This brings us to end of our introduction to basic data cleansing techniques. We now have only nine unresolved records, as shown in <a data-type="xref" href="#fig-2-11">Figure 2-11</a>. In the next chapter, we will see how more approximate text matching techniques can help us resolve some of these too.<a contenteditable="false" data-primary="" data-startref="DSmismatch02" data-type="indexterm" id="id359"/><a contenteditable="false" data-primary="" data-startref="WDmismatch02" data-type="indexterm" id="id360"/><a contenteditable="false" data-primary="" data-startref="TWFYmismatch02" data-type="indexterm" id="id361"/><a contenteditable="false" data-primary="" data-startref="mismatch02" data-type="indexterm" id="id362"/></p>&#13;
&#13;
<figure><div class="figure" id="fig-2-11"><img alt="" class="iimagesch02ch02unresolvedpng" src="assets/hoer_0211.png"/>&#13;
<h6><span class="label">Figure 2-11. </span>Unresolved entities</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Measuring Performance" data-type="sect1"><div class="sect1" id="id20">&#13;
<h1>Measuring Performance</h1>&#13;
&#13;
<p>Let’s<a contenteditable="false" data-primary="data standardization" data-secondary="measuring performance" data-type="indexterm" id="id363"/><a contenteditable="false" data-primary="performance, measuring" data-secondary="data standardization" data-type="indexterm" id="id364"/><a contenteditable="false" data-primary="true positive/true negative" data-type="indexterm" id="id365"/><a contenteditable="false" data-primary="false positive/false negative" data-type="indexterm" id="id366"/><a contenteditable="false" data-primary="recall" data-type="indexterm" id="id367"/><a contenteditable="false" data-primary="precision" data-type="indexterm" id="id368"/> evaluate our performance using a simple exact matching method based on the metrics we defined in <a data-type="xref" href="ch01.html#chapter_1">Chapter 1</a>. Our total population size is 650, within which we have:</p>&#13;
&#13;
<div data-type="equation">&#13;
&#13;
<p><math alttext="upper T r u e p o s i t i v e m a t c h e s left-parenthesis upper T upper P right-parenthesis equals 628">&#13;
  <mrow>&#13;
    <mi>T</mi>&#13;
    <mi>r</mi>&#13;
    <mi>u</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>p</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>T</mi>&#13;
    <mi>P</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>628</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper F a l s e p o s i t i v e m a t c h e s left-parenthesis upper F upper P right-parenthesis equals 0">&#13;
  <mrow>&#13;
    <mi>F</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>s</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>p</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>F</mi>&#13;
    <mi>P</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper T r u e n e g a t i v e m a t c h e s left-parenthesis upper T upper N right-parenthesis equals 13 left-parenthesis upper B y minus e l e c t i o n s right-parenthesis">&#13;
  <mrow>&#13;
    <mi>T</mi>&#13;
    <mi>r</mi>&#13;
    <mi>u</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>n</mi>&#13;
    <mi>e</mi>&#13;
    <mi>g</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>T</mi>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>13</mn>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>B</mi>&#13;
    <mi>y</mi>&#13;
    <mo>-</mo>&#13;
    <mi>e</mi>&#13;
    <mi>l</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>o</mi>&#13;
    <mi>n</mi>&#13;
    <mi>s</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper F a l s e n e g a t i v e m a t c h e s left-parenthesis upper F upper N right-parenthesis equals 9">&#13;
  <mrow>&#13;
    <mi>F</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>s</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>n</mi>&#13;
    <mi>e</mi>&#13;
    <mi>g</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>F</mi>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>9</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
</div>&#13;
&#13;
<p>We can calculate our performance metrics as:</p>&#13;
&#13;
<div data-type="equation">&#13;
<p><math alttext="upper P r e c i s i o n equals StartFraction upper T upper P Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction equals StartFraction 628 Over left-parenthesis 628 plus 0 right-parenthesis EndFraction equals 100 percent-sign">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>i</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>o</mi>&#13;
    <mi>n</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>628</mn> <mrow><mo>(</mo><mn>628</mn><mo>+</mo><mn>0</mn><mo>)</mo></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mn>100</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper R e c a l l equals StartFraction upper T upper P Over left-parenthesis upper T upper P plus upper F upper N right-parenthesis EndFraction equals StartFraction 628 Over left-parenthesis 628 plus 9 right-parenthesis EndFraction almost-equals 98.6 percent-sign">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>l</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>628</mn> <mrow><mo>(</mo><mn>628</mn><mo>+</mo><mn>9</mn><mo>)</mo></mrow></mfrac>&#13;
    <mo>≈</mo>&#13;
    <mn>98</mn>&#13;
    <mo>.</mo>&#13;
    <mn>6</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper A c c u r a c y equals StartFraction left-parenthesis upper T upper P plus upper T upper N right-parenthesis Over left-parenthesis upper T upper P plus upper T upper N plus upper F upper P plus upper F upper N right-parenthesis EndFraction equals StartFraction left-parenthesis 628 plus 13 right-parenthesis Over 650 EndFraction almost-equals 98.6 percent-sign">&#13;
  <mrow>&#13;
    <mi>A</mi>&#13;
    <mi>c</mi>&#13;
    <mi>c</mi>&#13;
    <mi>u</mi>&#13;
    <mi>r</mi>&#13;
    <mi>a</mi>&#13;
    <mi>c</mi>&#13;
    <mi>y</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>)</mo></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>T</mi><mi>N</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mo>(</mo><mn>628</mn><mo>+</mo><mn>13</mn><mo>)</mo></mrow> <mn>650</mn></mfrac>&#13;
    <mo>≈</mo>&#13;
    <mn>98</mn>&#13;
    <mo>.</mo>&#13;
    <mn>6</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
</div>&#13;
&#13;
<p>Our<a contenteditable="false" data-primary="accuracy" data-type="indexterm" id="id369"/> precision is perfect because we are setting a very high bar—an exact match on first name, last name, and constituency; if we declare a match, we always get it right. Our recall is also extremely good; we rarely fail to find a match we should have found. Finally, our overall accuracy is also very high.</p>&#13;
&#13;
<p>Of course, this is a simple example with relatively high-quality data and we have the advantage of a very strong categorical variable (constituency) to match against.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Sample Calculation" data-type="sect1"><div class="sect1" id="id111">&#13;
<h1>Sample Calculation</h1>&#13;
&#13;
<p>We<a contenteditable="false" data-primary="data standardization" data-secondary="sample calculation" data-type="indexterm" id="id370"/> have successfully resolved the names between our two datasets, so now we can use the combined information to test our hypothesis about the correlation between social media presence and the likelihood of reelection. Our resolved data now has everything we need in one table. <a data-type="xref" href="#fig-2-12">Figure 2-12</a> shows the first few rows of this table.</p>&#13;
&#13;
<figure><div class="figure" id="fig-2-12"><img alt="" class="iimagesch02ch02resolvedpng" src="assets/hoer_0212.png"/>&#13;
<h6><span class="label">Figure 2-12. </span>Sample of resolved entities</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can calculate the number of MPs who currently have Facebook accounts who held their seats in the 2019 election:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_heldwithface = df_resolved[(df_resolved['Flink']!="") &amp;&#13;
      (df_resolved['Notes']=="Seat held\n")]&#13;
len(df_heldwithface)&#13;
<strong>474</strong></pre>&#13;
&#13;
<p>As a percentage: <math alttext="StartFraction 474 Over 628 EndFraction almost-equals 75 percent-sign">&#13;
  <mrow>&#13;
    <mfrac><mn>474</mn> <mn>628</mn></mfrac>&#13;
    <mo>≈</mo>&#13;
    <mn>75</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>Finally, we’ll save our cleansed datasets locally so that we can use them in subsequent chapters:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w.to_csv('mps_wiki_clean.csv', index=False)&#13;
df_t.to_csv('mps_they_clean.csv', index=False)</pre>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id245">&#13;
<h1 class="less_space">Summary</h1>&#13;
&#13;
<p>To summarize, we used five simple techniques to standardize and cleanse our data:</p>&#13;
&#13;
<ul>&#13;
	<li>Removed null records</li>&#13;
	<li>Removed leading and trailing unwanted characters</li>&#13;
	<li>Split full name into first name and last name</li>&#13;
	<li>Removed commas from constituency</li>&#13;
	<li>Removed middle names and initials from first name</li>&#13;
</ul>&#13;
&#13;
<p>As a result we were able to join our datasets and then calculate a simple metric that we otherwise could not. Alas, there is no ubiquitous cleansing process; it depends on the datasets you have.</p>&#13;
&#13;
<p>In the next chapter we will see how fuzzy matching techniques can improve our performance even more.</p>&#13;
</div></section>&#13;
<div data-type="footnotes"><p data-type="footnote" id="id309"><sup><a href="ch02.html#id309-marker">1</a></sup> A by-election, also known as a special election in the United States, is an election used to fill an office that has become vacant between general elections. In the UK Parliament, a seat in the House of Commons can become vacant when an MP resigns or dies.</p><p data-type="footnote" id="id312"><sup><a href="ch02.html#id312-marker">2</a></sup> Software products identified in this book are suggestions only. You are responsible for evaluating whether to use any particular software and accept its license terms.</p></div></div></section></body></html>