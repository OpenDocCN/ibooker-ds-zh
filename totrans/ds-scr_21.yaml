- en: Chapter 20\. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Where we such clusters had
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As made us nobly wild, not mad
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Robert Herrick
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Most of the algorithms in this book are what’s known as *supervised learning*
    algorithms, in that they start with a set of labeled data and use that as the
    basis for making predictions about new, unlabeled data. Clustering, however, is
    an example of *unsupervised learning*, in which we work with completely unlabeled
    data (or in which our data has labels but we ignore them).
  prefs: []
  type: TYPE_NORMAL
- en: The Idea
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever you look at some source of data, it’s likely that the data will somehow
    form *clusters*. A dataset showing where millionaires live probably has clusters
    in places like Beverly Hills and Manhattan. A dataset showing how many hours people
    work each week probably has a cluster around 40 (and if it’s taken from a state
    with laws mandating special benefits for people who work at least 20 hours a week,
    it probably has another cluster right around 19). A dataset of demographics of
    registered voters likely forms a variety of clusters (e.g., “soccer moms,” “bored
    retirees,” “unemployed millennials”) that pollsters and political consultants
    consider relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike some of the problems we’ve looked at, there is generally no “correct”
    clustering. An alternative clustering scheme might group some of the “unemployed
    millennials” with “grad students,” and others with “parents’ basement dwellers.”
    Neither scheme is necessarily more correct—instead, each is likely more optimal
    with respect to its own “how good are the clusters?” metric.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the clusters won’t label themselves. You’ll have to do that by
    looking at the data underlying each one.
  prefs: []
  type: TYPE_NORMAL
- en: The Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For us, each `input` will be a vector in *d*-dimensional space, which, as usual,
    we will represent as a list of numbers. Our goal will be to identify clusters
    of similar inputs and (sometimes) to find a representative value for each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: For example, each input could be a numeric vector that represents the title
    of a blog post, in which case the goal might be to find clusters of similar posts,
    perhaps in order to understand what our users are blogging about. Or imagine that
    we have a picture containing thousands of `(red, green, blue)` colors and that
    we need to screen-print a 10-color version of it. Clustering can help us choose
    10 colors that will minimize the total “color error.”
  prefs: []
  type: TYPE_NORMAL
- en: One of the simplest clustering methods is *k*-means, in which the number of
    clusters *k* is chosen in advance, after which the goal is to partition the inputs
    into sets <math><mrow><msub><mi>S</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>S</mi> <mi>k</mi></msub></mrow></math> in a way that minimizes
    the total sum of squared distances from each point to the mean of its assigned
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of ways to assign *n* points to *k* clusters, which means that
    finding an optimal clustering is a very hard problem. We’ll settle for an iterative
    algorithm that usually finds a good clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a set of *k*-means, which are points in *d*-dimensional space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each point to the mean to which it is closest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If no point’s assignment has changed, stop and keep the clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If some point’s assignment has changed, recompute the means and return to step
    2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the `vector_mean` function from [Chapter 4](ch04.html#linear_algebra),
    it’s pretty simple to create a class that does this.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we’ll create a helper function that measures how many coordinates
    two vectors differ in. We’ll use this to track our training progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need a function that, given some vectors and their assignments to clusters,
    computes the means of the clusters. It may be the case that some cluster has no
    points assigned to it. We can’t take the mean of an empty collection, so in that
    case we’ll just randomly pick one of the points to serve as the “mean” of that
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we’re ready to code up our clusterer. As usual, we’ll use `tqdm` to
    track our progress, but here we don’t know how many iterations it will take, so
    we then use `itertools.count`, which creates an infinite iterable, and we’ll `return`
    out of it when we’re done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at how this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Meetups'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To celebrate DataSciencester’s growth, your VP of User Rewards wants to organize
    several in-person meetups for your hometown users, complete with beer, pizza,
    and DataSciencester t-shirts. You know the locations of all your local users ([Figure 20-1](#user_locations)),
    and she’d like you to choose meetup locations that make it convenient for everyone
    to attend.
  prefs: []
  type: TYPE_NORMAL
- en: '![User locations.](assets/dsf2_2001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-1\. The locations of your hometown users
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on how you look at it, you probably see two or three clusters. (It’s
    easy to do visually because the data is only two-dimensional. With more dimensions,
    it would be a lot harder to eyeball.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine first that she has enough budget for three meetups. You go to your
    computer and try this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You find three clusters centered at [–44, 5], [–16, –10], and [18, 20], and
    you look for meetup venues near those locations ([Figure 20-2](#user_locations_3_means)).
  prefs: []
  type: TYPE_NORMAL
- en: '![User locations with 3 means.](assets/dsf2_2002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-2\. User locations grouped into three clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You show your results to the VP, who informs you that now she only has enough
    budgeted for *two* meetups.
  prefs: []
  type: TYPE_NORMAL
- en: '“No problem,” you say:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As shown in [Figure 20-3](#user_locations_2_means), one meetup should still
    be near [18, 20], but now the other should be near [–26, –5].
  prefs: []
  type: TYPE_NORMAL
- en: '![User locations with 2 means.](assets/dsf2_2003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-3\. User locations grouped into two clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Choosing k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous example, the choice of *k* was driven by factors outside of
    our control. In general, this won’t be the case. There are various ways to choose
    a *k*. One that’s reasonably easy to understand involves plotting the sum of squared
    errors (between each point and the mean of its cluster) as a function of *k* and
    looking at where the graph “bends”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'which we can apply to our previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Looking at [Figure 20-4](#choosing_a_k), this method agrees with our original
    eyeballing that three is the “right” number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing a k.](assets/dsf2_2004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-4\. Choosing a *k*
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Example: Clustering Colors'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The VP of Swag has designed attractive DataSciencester stickers that he’d like
    you to hand out at meetups. Unfortunately, your sticker printer can print at most
    five colors per sticker. And since the VP of Art is on sabbatical, the VP of Swag
    asks if there’s some way you can take his design and modify it so that it contains
    only five colors.
  prefs: []
  type: TYPE_NORMAL
- en: Computer images can be represented as two-dimensional arrays of pixels, where
    each pixel is itself a three-dimensional vector `(red, green, blue)` indicating
    its color.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a five-color version of the image, then, entails:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing five colors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assigning one of those colors to each pixel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It turns out this is a great task for *k*-means clustering, which can partition
    the pixels into five clusters in red-green-blue space. If we then recolor the
    pixels in each cluster to the mean color, we’re done.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we’ll need a way to load an image into Python. We can do this
    with matplotlib, if we first install the pillow library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can just use `matplotlib.image.imread`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Behind the scenes `img` is a NumPy array, but for our purposes, we can treat
    it as a list of lists of lists.
  prefs: []
  type: TYPE_NORMAL
- en: '`img[i][j]` is the pixel in the *i*th row and *j*th column, and each pixel
    is a list `[red, green, blue]` of numbers between 0 and 1 indicating the [color
    of that pixel](http://en.wikipedia.org/wiki/RGB_color_model):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In particular, we can get a flattened list of all the pixels as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'and then feed them to our clusterer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once it finishes, we just construct a new image with the same format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'and display it, using `plt.imshow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It is difficult to show color results in a black-and-white book, but [Figure 20-5](#mt_both)
    shows grayscale versions of a full-color picture and the output of using this
    process to reduce it to five colors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Original picture and its 5-means decoloring.](assets/dsf2_2005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-5\. Original picture and its 5-means decoloring
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bottom-Up Hierarchical Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An alternative approach to clustering is to “grow” clusters from the bottom
    up. We can do this in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: Make each input its own cluster of one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As long as there are multiple clusters remaining, find the two closest clusters
    and merge them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end, we’ll have one giant cluster containing all the inputs. If we keep
    track of the merge order, we can re-create any number of clusters by unmerging.
    For example, if we want three clusters, we can just undo the last two merges.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use a really simple representation of clusters. Our values will live
    in *leaf* clusters, which we will represent as `NamedTuple`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use these to grow *merged* clusters, which we will also represent as
    `NamedTuple`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is another case where Python’s type annotations have let us down. You’d
    like to type hint `Merged.children` as `Tuple[Cluster, Cluster]` but `mypy` doesn’t
    allow recursive types like that.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll talk about merge order in a bit, but first let’s create a helper function
    that recursively returns all the values contained in a (possibly merged) cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to merge the closest clusters, we need some notion of the distance
    between clusters. We’ll use the *minimum* distance between elements of the two
    clusters, which merges the two clusters that are closest to touching (but will
    sometimes produce large chain-like clusters that aren’t very tight). If we wanted
    tight spherical clusters, we might use the *maximum* distance instead, as it merges
    the two clusters that fit in the smallest ball. Both are common choices, as is
    the *average* distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use the merge order slot to track the order in which we did the merging.
    Smaller numbers will represent *later* merges. This means when we want to unmerge
    clusters, we do so from lowest merge order to highest. Since `Leaf` clusters were
    never merged, we’ll assign them infinity, the highest possible value. And since
    they don’t have an `.order` property, we’ll create a helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, since `Leaf` clusters don’t have children, we’ll create and add
    a helper function for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to create the clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Its use is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a clustering that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The numbers at the top indicate “merge order.” Since we had 20 inputs, it took
    19 merges to get to this one cluster. The first merge created cluster 18 by combining
    the leaves [19, 28] and [21, 27]. And the last merge created cluster 0.
  prefs: []
  type: TYPE_NORMAL
- en: If you wanted only two clusters, you’d split at the first fork (“0”), creating
    one cluster with six points and a second with the rest. For three clusters, you’d
    continue to the second fork (“1”), which indicates to split that first cluster
    into the cluster with ([19, 28], [21, 27], [20, 23], [26, 13]) and the cluster
    with ([11, 15], [13, 13]). And so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, though, we don’t want to be squinting at nasty text representations
    like this. Instead, let’s write a function that generates any number of clusters
    by performing the appropriate number of unmerges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'So, for example, if we want to generate three clusters, we can just do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'which we can easily plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This gives very different results than *k*-means did, as shown in [Figure 20-6](#hierarchical_clustering_image).
  prefs: []
  type: TYPE_NORMAL
- en: '![Three Bottom-Up Clusters Using Min Distance.](assets/dsf2_2006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-6\. Three bottom-up clusters using min distance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As mentioned previously, this is because using `min` in `cluster_distance` tends
    to give chain-like clusters. If we instead use `max` (which gives tight clusters),
    it looks the same as the 3-means result ([Figure 20-7](#hierarchical_clustering_image_max)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The previous `bottom_up_clustering` implementation is relatively simple, but
    also shockingly inefficient. In particular, it recomputes the distance between
    each pair of inputs at every step. A more efficient implementation might instead
    precompute the distances between each pair of inputs and then perform a lookup
    inside `cluster_distance`. A *really* efficient implementation would likely also
    remember the `cluster_distance`s from the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Three Bottom-Up Clusters Using Max Distance.](assets/dsf2_2007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-7\. Three bottom-up clusters using max distance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn has an entire module, [`sklearn.cluster`](http://scikit-learn.org/stable/modules/clustering.html),
    that contains several clustering algorithms including `KMeans` and the `Ward`
    hierarchical clustering algorithm (which uses a different criterion for merging
    clusters than ours did).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SciPy](http://www.scipy.org/) has two clustering models: `scipy.cluster.vq`,
    which does *k*-means, and `scipy.cluster.hierarchy`, which has a variety of hierarchical
    clustering algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
