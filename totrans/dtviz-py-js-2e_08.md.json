["```py\n$ pip install requests\nDownloading/unpacking requests\n...\nCleaning up...\n```", "```py\n$ pip install --upgrade ndg-httpsclient\n```", "```py\n$ ipython\nPython 3.8.9 (default, Apr  3 2021, 01:02:10)\n...\n\nIn [1]: import requests\n```", "```py\nresponse = requests.get(\\\n\"https://en.wikipedia.org/wiki/Nobel_Prize\")\n```", "```py\ndir(response)\n```", "```py\nOut:\n...\n ['content',\n 'cookies',\n 'elapsed',\n 'encoding',\n 'headers',\n ...\n 'iter_content',\n 'iter_lines',\n 'json',\n 'links',\n ...\n 'status_code',\n 'text',\n 'url']\n```", "```py\nresponse.status_code\nOut: 200\n```", "```py\nnot_found_response = requests.get(\\\n\"http://en.wikipedia.org/wiki/SNobel_Prize\")\nnot_found_response.status_code\nOut: 404\n```", "```py\nresponse.headers\n```", "```py\nOut: {\n  'date': 'Sat, 23 Oct 2021 23:58:49 GMT',\n  'server': 'mw1435.eqiad.wmnet',\n  'content-encoding': 'gzip', ...\n  'last-modified': 'Sat, 23 Oct 2021 17:14:09 GMT', ...\n  'content-type': 'text/html; charset=UTF-8'...\n  'content-length': '88959'\n  }\n```", "```py\nresponse.text\n#Out: u'<!DOCTYPE html>\\n<html lang=\"en\"\n#dir=\"ltr\" class=\"client-nojs\">\\n<head>\\n<meta charset=\"UTF-8\"\n#/>\\n<title>Nobel Prize - Wikipedia, the free\n#encyclopedia</title>\\n<script>document.documentElement... =\n```", "```py\n<root_url>/<dsname>/<dim 1>.<dim 2>...<dim n>\n/all?param1=foo&param2=baa..\n<dim 1> = 'AUS'+'AUT'+'BEL'...\n```", "```py\nhttp://stats.oecd.org/sdmx-json/data/QNA   ![1](assets/1.png)\n    /AUS+AUT.GDP+B1_GE.CUR+VOBARSA.Q       ![2](assets/2.png)\n    /all?startTime=2009-Q2&endTime=2011-Q4 ![3](assets/3.png)\n```", "```py\nOECD_ROOT_URL = 'http://stats.oecd.org/sdmx-json/data'\n\ndef make_OECD_request(dsname, dimensions, params=None, \\ root_dir=OECD_ROOT_URL):\n    \"\"\" Make a URL for the OECD API and return a response \"\"\"\n\n    if not params: ![1](assets/1.png)\n        params = {}\n\n    dim_args = ['+'.join(d) for d in dimensions] ![2](assets/2.png)\n    dim_str = '.'.join(dim_args)\n\n    url = root_dir + '/' + dsname + '/' + dim_str + '/all'\n    print('Requesting URL: ' + url)\n    return requests.get(url, params=params) ![3](assets/3.png)\n```", "```py\nresponse = make_OECD_request('QNA',\n    (('USA', 'AUS'),('GDP', 'B1_GE'),('CUR', 'VOBARSA'), ('Q')),\n    {'startTime':'2009-Q1', 'endTime':'2010-Q1'})\n```", "```py\nRequesting URL: http://stats.oecd.org/sdmx-json/data/QNA/\n    USA+AUS.GDP+B1_GE.CUR+VOBARSA.Q/all\n```", "```py\nif response.status_code == 200:\n   json = response.json()\n   json.keys()\nOut: [u'header', u'dataSets', u'structure']\n```", "```py\nhttps://restcountries.com/v3.1/<field>/<name>?<params>\n```", "```py\nREST_EU_ROOT_URL = \"https://restcountries.com/v3.1\"\n\ndef REST_country_request(field='all', name=None, params=None):\n\n    headers={'User-Agent': 'Mozilla/5.0'} ![1](assets/1.png)\n\n    if not params:\n        params = {}\n\n    if field == 'all':\n         response = requests.get(REST_EU_ROOT_URL + '/all')\n         return response.json()\n\n    url = '%s/%s/%s'%(REST_EU_ROOT_URL, field, name)\n    print('Requesting URL: ' + url)\n    response = requests.get(url, params=params, headers=headers)\n\n    if not response.status_code == 200: ![2](assets/2.png)\n        raise Exception('Request failed with status code ' \\\n        + str(response.status_code))\n\n     return response.json() # JSON encoded data\n```", "```py\nresponse = REST_country_request('currency', 'usd')\nresponse\nOut:\n[{u'alpha2Code': u'AS',\n  u'alpha3Code': u'ASM',\n  u'altSpellings': [u'AS',\n  ...\n  u'capital': u'Pago Pago',\n  u'currencies': [u'USD'],\n  u'demonym': u'American Samoan',\n  ...\n  u'latlng': [12.15, -68.266667],\n  u'name': u'Bonaire',\n  ...\n  u'name': u'British Indian Ocean Territory',\n  ...\n  u'name': u'United States Minor Outlying Islands',\n  ... ]}]\n```", "```py\nimport json\n\ncountry_data = REST_country_request() # all world data\n\nwith open('data/world_country_data.json', 'w') as json_file:\n    json.dump(country_data, json_file)\n```", "```py\n$ pip install gspread\n$ pip install --upgrade google-auth\n```", "```py\n$ pip install PyOpenSSL\n```", "```py\nimport gspread\n\ngc = gspread.service_account(\\\n                   filename='data/google_credentials.json') ![1](assets/1.png)\n\nss = gc.open(\"Microbe-scope\") ![2](assets/2.png)\n```", "```py\nss.worksheets()\nOut:\n[<Worksheet 'bugs' id:0>,\n <Worksheet 'outrageous facts' id:430583748>,\n <Worksheet 'physicians per 1,000' id:1268911119>,\n <Worksheet 'amends' id:1001992659>]\n\nws = ss.worksheet('bugs')\n```", "```py\nws.col_values(1)\nOut: [None,\n 'grey = not plotted',\n 'Anthrax (untreated)',\n 'Bird Flu (H5N1)',\n 'Bubonic Plague (untreated)',\n 'C.Difficile',\n 'Campylobacter',\n 'Chicken Pox',\n 'Cholera',...]\n```", "```py\ndf = pd.DataFrame(ws.get_all_records(expected_headers=[]))\ndf.info()\nOut:\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 41 entries, 0 to 40\nData columns (total 23 columns):\n                                          41 non-null object\naverage basic reproductive rate           41 non-null object\ncase fatality rate                        41 non-null object\ninfectious dose                           41 non-null object\n...\nupper R0                                  41 non-null object\nviral load in acute stage                 41 non-null object\nyearly fatalities                         41 non-null object\ndtypes: object(23)\nmemory usage: 7.5+ KB\n```", "```py\n# The user credential variables to access Twitter API\naccess_token = \"2677230157-Ze3bWuBAw4kwoj4via2dEntU86...TD7z\"\naccess_token_secret = \"DxwKAvVzMFLq7WnQGnty49jgJ39Acu...paR8ZH\"\nconsumer_key = \"pIorGFGQHShuYQtIxzYWk1jMD\"\nconsumer_secret = \"yLc4Hw82G0Zn4vTi4q8pSBcNyHkn35BfIe...oVa4P7R\"\n```", "```py\nIn [0]: import tweepy\n\n        auth = tweepy.OAuthHandler(consumer_key,\\\n                                   consumer_secret)\n        auth.set_access_token(access_token, access_token_secret)\n\n        api = tweepy.API(auth)\n\n        public_tweets = api.home_timeline()\n        for tweet in public_tweets:\n            print(tweet.text)\n```", "```py\nRT @Glinner: Read these tweets https://t.co/QqzJPsDxUD\nVolodymyr Bilyachat https://t.co/VIyOHlje6b +1 bmeyer\n#javascript\nRT @bbcworldservice: If scientists edit genes to\nmake people healthier does it change what it means to be\nhuman? https://t.co/Vciuyu6BCx h…\nRT @ForrestTheWoods:\nLaunching something pretty cool tomorrow. I'm excited. Keep\n...\n```", "```py\nmy_follower_ids = api.get_follower_ids() ![1](assets/1.png)\n\nfollowers_tree = {'followers': []}\nfor id in my_follower_ids:\n    # get the followers of your followers\n    try:\n         follower_ids = api.get_follower_ids(user_id=id) ![2](assets/2.png)\n    except tweepy.errors.Unauthorized:\n         print(\"Unauthorized to access user %d's followers\"\\\n               %(id))\n\n    followers_tree['followers'].append(\\\n        {'id': id, 'follower_ids': follower_ids})\n```", "```py\nimport json\n\nclass MyStream(tweepy.Stream):\n    \"\"\" Customized tweet stream \"\"\"\n\n    def on_data(self, tweet):\n        \"\"\"Do something with the tweet data...\"\"\"\n        print(tweet)\n\n    def on_error(self, status):\n        return True # keep stream open\n\nstream = MyStream(consumer_key, consumer_secret,\\\n                     access_token, access_token_secret)\n# Start the stream with track list of keywords\nstream.filter(track=['python', 'javascript', 'dataviz'])\n```", "```py\n$ pip install beautifulsoup4\n$ pip install lxml\n```", "```py\nfrom bs4 import BeautifulSoup\nimport requests\n\nBASE_URL = 'http://en.wikipedia.org'\n# Wikipedia will reject our request unless we add\n# a 'User-Agent' attribute to our http header.\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef get_Nobel_soup():\n    \"\"\" Return a parsed tag tree of our Nobel prize page \"\"\"\n    # Make a request to the Nobel page, setting valid headers\n    response = requests.get(\n        BASE_URL + '/wiki/List_of_Nobel_laureates',\n        headers=HEADERS)\n    # Return the content of the response parsed by Beautiful Soup\n    return BeautifulSoup(response.content, \"lxml\") ![1](assets/1.png)\n```", "```py\nsoup = get_Nobel_soup()\n```", "```py\nIn[3]: soup.find('table', {'class':'wikitable sortable'})\nOut[3]:\n<table class=\"wikitable sortable\">\n<tr>\n<th>Year</th>\n...\n```", "```py\nIn[4]: soup.find('table', {'class':'sortable wikitable'})\n# nothing returned\n```", "```py\nIn[5]: soup.select('table.sortable.wikitable')\nOut[5]:\n[<table class=\"wikitable sortable\">\n <tr>\n <th>Year</th>\n ...\n]\n```", "```py\nIn[8]: table = soup.select_one('table.sortable.wikitable')\n\nIn[9]: table.select('th')\nOut[9]:\n[<th>Year</th>,\n <th width=\"18%\"><a href=\"/wiki/..._in_Physics..</a></th>,\n <th width=\"16%\"><a href=\"/wiki/..._in_Chemis..</a></th>,\n ...\n]\n```", "```py\ntable.select('th')\ntable('th')\n```", "```py\n <tr>\n  <td align=\"center\">\n   1901\n  </td>\n  <td>\n   <span class=\"sortkey\">\n    Röntgen, Wilhelm\n   </span>\n   <span class=\"vcard\">\n    <span class=\"fn\">\n     <a href=\"/wiki/Wilhelm_R%C3%B6ntgen\" \\\n        title=\"Wilhelm Röntgen\">\n      Wilhelm Röntgen\n     </a>\n    </span>\n   </span>\n  </td>\n  <td>\n  ...\n</tr>\n```", "```py\ndef get_column_titles(table):\n    \"\"\" Get the Nobel categories from the table header \"\"\"\n    cols = []\n    for th in table.select_one('tr').select('th')[1:]: ![1](assets/1.png)\n        link = th.select_one('a')\n        # Store the category name and any Wikipedia link it has\n        if link:\n            cols.append({'name':link.text,\\\n                         'href':link.attrs['href']})\n        else:\n            cols.append({'name':th.text, 'href':None})\n    return cols\n```", "```py\nget_column_titles(table)\nOut:\n[{'name': 'Physics', \\\n  'href': '/wiki/List_of_Nobel_laureates_in_Physics'},\n {'name': 'Chemistry',\\\n  'href': '/wiki/List_of_Nobel_laureates_in_Chemistry'},...\n]\n\ndef get_Nobel_winners(table):\n    cols = get_column_titles(table)\n    winners = []\n    for row in table.select('tr')[1:-1]: ![1](assets/1.png)\n        year = int(row.select_one('td').text) # Gets 1st <td>\n        for i, td in enumerate(row.select('td')[1:]): ![2](assets/2.png)\n            for winner in td.select('a'):\n                href = winner.attrs['href']\n                if not href.startswith('#endnote'):\n                    winners.append({\n                        'year':year,\n                        'category':cols[i]['name'],\n                        'name':winner.text,\n                        'link':winner.attrs['href']\n                    })\n    return winners\n```", "```py\nget_Nobel_winners(table)\n```", "```py\n[{'year': 1901,\n  'category': 'Physics',\n  'name': 'Wilhelm Röntgen',\n  'link': '/wiki/Wilhelm_R%C3%B6ntgen'},\n {'year': 1901,\n  'category': 'Chemistry',\n  'name': \"Jacobus Henricus van 't Hoff\",\n  'link': '/wiki/Jacobus_Henricus_van_%27t_Hoff'},\n {'year': 1901,\n  'category': 'Physiologyor Medicine',\n  'name': 'Emil Adolf von Behring',\n  'link': '/wiki/Emil_Adolf_von_Behring'},\n {'year': 1901,\n ...}]\n```", "```py\n$ pip install --upgrade requests-cache\n```", "```py\nimport requests\nimport requests_cache\n\nrequests_cache.install_cache()\n# use requests as usual...\n```", "```py\nrequests_cache.install_cache('nobel_pages',\\\n                         backend='sqlite', expire_after=7200)\n```", "```py\nHEADERS = {'User-Agent': 'Mozilla/5.0'}\n\ndef get_winner_nationality(w):\n    \"\"\" scrape biographic data from the winner's wikipedia page \"\"\"\n    response = requests.get('http://en.wikipedia.org' \\\n                                + w['link'], headers=HEADERS)\n    content = response.content.decode('utf-8')\n    soup = BeautifulSoup(content)\n    person_data = {'name': w['name']}\n    attr_rows = soup.select('table.infobox tr') ![1](assets/1.png)\n    for tr in attr_rows:                        ![2](assets/2.png)\n        try:\n            attribute = tr.select_one('th').text\n            if attribute == 'Nationality':\n                person_data[attribute] = tr.select_one('td').text\n        except AttributeError:\n            pass\n\n    return person_data\n```", "```py\nwdata = []\n# test first 50 winners\nfor w in winners[:50]:\n    wdata.append(get_winner_nationality(w))\nmissing_nationality = []\nfor w in wdata:\n    # if missing 'Nationality' add to list\n    if not w.get('Nationality'):\n        missing_nationality.append(w)\n# output list\nmissing_nationality\n\n[{'name': 'Theodor Mommsen'},\n {'name': 'Élie Ducommun'},\n {'name': 'Charles Albert Gobat'},\n {'name': 'Pierre Curie'},\n {'name': 'Marie Curie'},\n {'name': 'Niels Ryberg Finsen'},\n ...\n {'name': 'Theodore Roosevelt'}, ... ]\n```"]