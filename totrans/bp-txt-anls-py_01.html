<html><head></head><body><div id="sbo-rt-content"><section class="pagenumrestart" data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 1. Gaining Early Insights from Textual Data"><div class="chapter" id="ch-exploration">
<h1><span class="label">Chapter 1. </span>Gaining Early Insights from Textual Data</h1>

<p>One of the first tasks in every data analytics and machine learning project is to become familiar with the data. In fact, it is always essential to have a basic understanding of the data to achieve robust results. Descriptive statistics provide reliable and robust insights and help to assess data quality and distribution.</p>

<p>When considering texts, frequency analysis of words and phrases is one of the main methods for data exploration. Though absolute word frequencies usually are not very interesting, relative or weighted frequencies are. When analyzing text about politics, for example, the most common words will probably contain many obvious and unsurprising terms such as <em>people</em>, <em>country</em>, <em>government</em>, etc. But if you compare relative word frequencies in text from different political parties or even from politicians in the same party, you can learn a lot from the differences.</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634210729896">
<h1>What You’ll Learn and What We’ll Build</h1>

<p>This chapter presents blueprints for the <a contenteditable="false" data-primary="statistical analysis of datasets" data-seealso="text data analysis" data-type="indexterm" id="idm45634210728472"/>statistical <a contenteditable="false" data-primary="text data analysis, introduction" data-secondary="about" data-type="indexterm" id="ch1_term1"/>analysis of text. It gets you started quickly and introduces basic concepts that you will need to know in subsequent chapters. We will start by analyzing categorical metadata and then focus on word <span class="keep-together">frequency</span> analysis and visualization.</p>

<p>After studying this chapter, you will have basic knowledge about text processing and analysis. You will know how to tokenize text, filter stop words, and analyze textual content with frequency diagrams and word clouds. We will also introduce TF-IDF weighting as an important concept that will be picked up later in the book for text vectorization.</p>

<p>The blueprints in this chapter focus on quick results and follow the <em>KISS</em> principle: “Keep it simple, stupid!” Thus, we primarily use Pandas as our library of choice for data analysis in combination with regular expressions and Python core functionality. <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a> will discuss advanced linguistic methods for data preparation.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Exploratory Data Analysis"><div class="sect1" id="idm45634210721192">
<h1>Exploratory Data Analysis</h1>

<p>Exploratory <a contenteditable="false" data-primary="exploratory data analysis" data-type="indexterm" id="idm45634210719608"/>data analysis is the process of systematically examining data on an aggregated level. Typical methods include summary statistics for numerical features as well as frequency counts for categorical features. Histograms and box plots will illustrate the distribution of values, and time-series plots will show their evolution.</p>

<p>A dataset consisting of text documents such as news, tweets, emails, or service calls is <a contenteditable="false" data-type="indexterm" data-primary="corpus" data-seealso="datasets" id="idm45634210717416"/>called a <em>corpus</em> in natural language processing. The statistical exploration of such a corpus has different facets. Some analyses focus on metadata attributes, while others deal with the textual content. <a data-type="xref" href="#fig-statistical-features">Figure 1-1</a> shows typical attributes of a text corpus, some of which are included in the data source, while others could be calculated or derived. The document metadata comprise multiple descriptive attributes, which are useful for aggregation and filtering. Time-like attributes are essential to understanding the evolution of the corpus. If available, author-related attributes allow you to analyze groups of authors and to benchmark these groups against one another.</p>

<figure><div id="fig-statistical-features" class="figure">
	<img src="Images/btap_0101.jpg" width="744" height="360"/>
<h6><span class="label">Figure 1-1. </span>Statistical features for text data exploration.</h6>
</div></figure>

<p>Statistical analysis of the content is based on the frequencies of words and phrases. With the linguistic data preprocessing methods described in <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>, we will extend the space of analysis to certain word types and named entities. Besides that, <a contenteditable="false" data-primary="scoring of corpus" data-type="indexterm" id="idm45634210710424"/>descriptive scores for the documents could be included in the dataset or derived by some kind of feature modeling. For example, the number of replies to a user’s post could be taken as a measure of popularity. Finally, interesting soft facts such as sentiment or emotionality scores can be determined by one of the methods described later in this book.</p>

<p>Note that absolute figures are generally not very interesting when working with text. The mere fact that the word <em>problem</em> appears a hundred times does not contain any relevant information. But the fact that the relative frequency of <em>problem</em> has doubled within a week <a contenteditable="false" data-primary="" data-startref="ch1_term1" data-type="indexterm" id="idm45634210939080"/>can be remarkable.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Introducing the Dataset"><div class="sect1" id="idm45634210937560">
<h1>Introducing the Dataset</h1>

<p>Analyzing political text, be it news or programs of political parties or parliamentary debates, can give interesting insights on national and international topics. Often, <a contenteditable="false" data-primary="text data analysis, introduction" data-secondary="datasets for" data-type="indexterm" id="idm45634210935608"/>text from many years is publicly available so that an insight into the zeitgeist can be gained. Let’s jump into the role of a political analyst who wants to get a feeling for the analytical potential of such a dataset.</p>

<p>For that, we will work with <a contenteditable="false" data-primary="UN General Debates dataset" data-type="indexterm" id="idm45634210933432"/>the <a contenteditable="false" data-primary="datasets, examples of" data-secondary="UN General Debates" data-type="indexterm" id="idm45634210932136"/><a href="https://oreil.ly/lHHUm">UN General Debate dataset</a>. The corpus consists of 7,507 speeches held at the annual sessions of the United Nations General Assembly from 1970 to 2016. It was created in 2017 by Mikhaylov, Baturo, and Dasandi at Harvard “for understanding and measuring state preferences in world politics.” Each of the almost 200 countries in the United Nations has the opportunity to present its views on global topics such international conflicts, terrorism, or climate change at the annual General Debate.</p>

<p>The <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="from Kaggle" data-secondary-sortas="Kaggle" id="idm45634210929176"/>original dataset on Kaggle is provided in the form of two CSV files, a big one containing the speeches and a smaller one with information about the speakers. To simplify matters, we prepared a single zipped CSV file containing all the information. You can find the code for the preparation as well as the resulting file in our <a href="https://oreil.ly/btap-code">GitHub repository</a>.</p>

<p>In <a contenteditable="false" data-primary="Pandas library" data-secondary="about" data-type="indexterm" id="idm45634210925832"/>Pandas, a CSV file can be loaded with <code>pd.read_csv()</code>. Let’s load the file and display two random records of the <code>DataFrame</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="nb">file</code> <code class="o">=</code> <code class="s2">"un-general-debates-blueprint.csv"</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="nb">file</code><code class="p">)</code>
<code class="n">df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>session</th>
			<th>year</th>
			<th>country</th>
			<th>country_name</th>
			<th>speaker</th>
			<th>position</th>
			<th>text</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>3871</th>
			<td>51</td>
			<td>1996</td>
			<td>PER</td>
			<td>Peru</td>
			<td>Francisco Tudela Van Breughel Douglas</td>
			<td>Minister for Foreign Affairs</td>
			<td>At the outset, allow me,\nSir, to convey to you and to this Assembly the greetings\nand congratulations of the Peruvian people, as well as\ntheir...</td>
		</tr>
		<tr>
			<th>4697</th>
			<td>56</td>
			<td>2001</td>
			<td>GBR</td>
			<td>United Kingdom</td>
			<td>Jack Straw</td>
			<td>Minister for Foreign Affairs</td>
			<td>Please allow me\nwarmly to congratulate you, Sir, on your assumption of\nthe presidency of the fifty-sixth session of the General\nAssembly.\nThi...</td>
		</tr>
	</tbody>
</table>

<p>The first column contains the index of the records. The combination of session number and year can be considered as the logical primary key of the table. The <code>country</code> column contains a standardized three-letter country ISO code and is followed by the textual description. Then we have two columns about the speaker and their position. The last column contains the actual text of the speech.</p>

<p>Our dataset is small; it contains only a few thousand records. It is a great dataset to use because we will not run into performance problems. If your dataset is larger, check out <a data-type="xref" data-xrefstyle="select:nopage" href="#sb-working-with-large-datasets">“Working with Large Datasets”</a> for options.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sb-working-with-large-datasets">
<h5>Working with Large Datasets</h5>

<p>Don’t start data exploration with millions and billions of records. Instead, use a small sample of the data to get started. This way you can quickly develop the statements and visualizations you need. Once the analyses are prepared, you can rerun everything on the large dataset to get the full view.</p>

<p>There are multiple ways to select a <a contenteditable="false" data-primary="sample function (Pandas)" data-type="indexterm" id="idm45634219171032"/>sample of the data. The simplest and most useful one is Pandas’s <code>sample</code> function, which is used in the following command to replace a <code>DataFrame</code> by a random sample of 10% of its records:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">frac</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code></pre>

<p>The drawback of this method is that the full dataset must be loaded into main memory before it can be sampled. Alternatively, you can load only a subset of the data. For example, <code>pd.read_csv</code> has two optional parameters, <code>nrows</code> and <code>skiprows</code>, which can be used to read a slice of the whole file. However, this will select a range of subsequent rows, not a random sample. If your data is stored in a relational database, you should check whether it supports random sampling. You could also use some poor man’s SQL for random sampling like this:</p>
<pre data-type="programlisting">ORDER BY Rand() LIMIT 10000</pre>
<p>Or:</p>
<pre data-type="programlisting">WHERE id%10 = 0</pre>
</div></aside>
</div></section>

<section class="blueprint" data-type="sect1" data-pdf-bookmark="Blueprint: Getting an Overview of the Data with Pandas"><div class="sect1" id="idm45634210936904">
<h1>Blueprint: Getting an Overview of the Data with Pandas</h1>

<p>In our <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="data overview with Pandas for" id="ch1_term3"/>first <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="for text analysis overview" data-secondary-sortas="text analysis overview" id="ch1_term2"/>blueprint, we use only metadata and record counts to explore data distribution and quality; we will not yet look at the textual content. We will work through the following steps:</p>

<ol>
	<li>Calculate summary statistics.</li>
	<li>Check for missing values.</li>
	<li>Plot distributions of interesting attributes.</li>
	<li>Compare distributions across categories.</li>
	<li>Visualize developments over time.</li>
</ol>

<p>Before we can start analyzing the data, we need at least some information about the structure of the <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframe commands in" id="idm45634211502264"/><a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634211500888"/><code>DataFrame</code>. <a data-type="xref" href="#tab-pandas-info">Table 1-1</a> shows some important descriptive properties or functions.</p>

<table id="tab-pandas-info">
	<caption><span class="label">Table 1-1. </span>Pandas commands to get information about dataframes</caption>
	<tbody>
		<tr>
			<td><code>df.columns</code></td>
			<td>List of column names</td>
			<td> </td>
		</tr>
		<tr>
			<td><code>df.dtypes</code></td>
			<td>Tuples (column name, data type)</td>
			<td>Strings are represented as object in versions before Pandas 1.0.</td>
		</tr>
		<tr>
			<td><code>df.info()</code></td>
			<td>Dtypes plus memory consumption</td>
			<td>Use with <code>memory_usage='deep'</code> for good estimates on text.</td>
		</tr>
		<tr>
			<td><code>df.describe()</code></td>
			<td>Summary statistics</td>
			<td>Use with <code>include='O'</code> for categorical data.</td>
		</tr>
	</tbody>
</table>

<section data-type="sect2" data-pdf-bookmark="Calculating Summary Statistics for Columns"><div class="sect2" id="idm45634211449144">
<h2>Calculating Summary Statistics for Columns</h2>

<p>Pandas’s <code>describe</code> <a contenteditable="false" data-type="indexterm" data-primary="summary statistics for columns" id="idm45634211446744"/>function computes statistical summaries for the columns of the <code>DataFrame</code>. It works on a single series as well as on the complete <code>DataFrame</code>. The default output in the latter case is restricted to numerical columns. <a contenteditable="false" data-type="indexterm" data-primary="UN General Debates dataset" id="ch1_term5"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="UN General Debates" id="ch1_term6"/>Currently, our <code>DataFrame</code> contains only the session number and the year as numerical data. Let’s add a new numerical column to the <code>DataFrame</code> containing the text length to get some additional information about the distribution of the lengths of the speeches. We recommend transposing the result with <code>describe().T</code> to switch rows and columns in the representation:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'length'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">len</code><code class="p">()</code>

<code class="n">df</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code><code class="o">.</code><code class="n">T</code>
</pre>
<p><code>Out:</code></p>
<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>count</th>
			<th>mean</th>
			<th>std</th>
			<th>min</th>
			<th>25%</th>
			<th>50%</th>
			<th>75%</th>
			<th>max</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>session</th>
			<td>7507.00</td>
			<td>49.61</td>
			<td>12.89</td>
			<td>25.00</td>
			<td>39.00</td>
			<td>51.00</td>
			<td>61.00</td>
			<td>70.00</td>
		</tr>
		<tr>
			<th>year</th>
			<td>7507.00</td>
			<td>1994.61</td>
			<td>12.89</td>
			<td>1970.00</td>
			<td>1984.00</td>
			<td>1996.00</td>
			<td>2006.00</td>
			<td>2015.00</td>
		</tr>
		<tr>
			<th>length</th>
			<td>7507.00</td>
			<td>17967.28</td>
			<td>7860.04</td>
			<td>2362.00</td>
			<td>12077.00</td>
			<td>16424.00</td>
			<td>22479.50</td>
			<td>72041.00</td>
		</tr>
	</tbody>
</table>

<p><code>describe()</code>, without additional parameters, computes the total count of values, their mean and standard deviation, and a <a href="https://oreil.ly/h2nrN">five-number summary</a> of only the numerical columns. The <code>DataFrame</code> contains 7,507 entries for <code>session</code>, <code>year</code>, and <code>length</code>. Mean and standard deviation do not make much sense for <code>year</code> and <span class="keep-together"><code>session</code></span>, but minimum and maximum are still interesting. Obviously, <a contenteditable="false" data-type="indexterm" data-primary="speeches, UN" data-secondary="statistical analysis of" id="ch1_term40"/>our dataset <span class="keep-together">contains</span> speeches from the 25th to the 70th UN General Debate sessions, spanning the years 1970 to 2015.</p>

<p>A summary for nonnumerical columns can be produced by specifying <code>include='O'</code> (the alias for <code>np.object</code>). In this case, we also get the count, the number of unique values, the top-most element (or one of them if there are many with the same number of occurrences), and its frequency. As the number of unique values is not useful for textual data, let’s just analyze the <code>country</code> and <code>speaker</code> columns:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[[</code><code class="s1">'country'</code><code class="p">,</code> <code class="s1">'speaker'</code><code class="p">]]</code><code class="o">.</code><code class="n">describe</code><code class="p">(</code><code class="n">include</code><code class="o">=</code><code class="s1">'O'</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>
</pre>
<p><code>Out:</code></p>
<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th> </th>
			<th>count</th>
			<th>unique</th>
			<th>top</th>
			<th>freq</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>country</th>
			<td>7507</td>
			<td>199</td>
			<td>ITA</td>
			<td>46</td>
		</tr>
		<tr>
			<th>speaker</th>
			<td>7480</td>
			<td>5428</td>
			<td>Seyoum Mesfin</td>
			<td>12</td>
		</tr>
	</tbody>
</table>

<p>The dataset contains data from 199 unique countries and apparently 5,428 speakers. The number of countries is valid, as this column contains standardized ISO codes. But counting the unique values of text columns like <code>speaker</code> usually does not give valid results, as we will show in the next <a contenteditable="false" data-type="indexterm" data-primary="missing data" id="idm45634211340136"/>section.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Checking for Missing Data"><div class="sect2" id="idm45634211448488">
<h2>Checking for Missing Data</h2>

<p>By looking at the counts in the previous table, we can see that the <code>speaker</code> column has missing values. So, let’s check all columns for null values by using <code>df.isna()</code> (the alias to <code>df.isnull()</code>) and compute a summary of the result:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="o">.</code><code class="n">isna</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
session            0
year               0
country            0
country_name       0
speaker           27
position        3005
text               0
length             0
dtype: int64
</pre>

<p>We need to be careful using the <code>speaker</code> and <code>position</code> columns, as the output tells us that this information is not always available! To prevent any problems, we could substitute the missing values with some generic value such as <code>unknown speaker</code> or <code>unknown position</code> or just the empty string.</p>

<p class="pagebreak-before">Pandas supplies the function <code>df.fillna()</code> for that purpose:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'speaker'</code><code class="p">]</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="s1">'unknown'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
</pre>

<p>But even the existing values can be problematic because the same speaker’s name <span class="keep-together">is sometimes</span> spelled differently or even ambiguously. The following statement <span class="keep-together">computes</span> the number of records per speaker for all documents containing <code>Bush</code> in the speaker column:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'speaker'</code><code class="p">]</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">contains</code><code class="p">(</code><code class="s1">'Bush'</code><code class="p">)][</code><code class="s1">'speaker'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
George W. Bush        4
Mr. George W. Bush    2
George Bush           1
Mr. George W Bush     1
Bush                  1
Name: speaker, dtype: int64
</pre>

<p>Any analysis on speaker names would produce the wrong results unless we resolve these ambiguities. So, we had better check the distinct values of categorical attributes. Knowing this, we will ignore the speaker information here.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Plotting Value Distributions"><div class="sect2" id="idm45634211338408">
<h2>Plotting Value Distributions</h2>

<p>One <a contenteditable="false" data-type="indexterm" data-primary="value distributions, plotting" id="ch1_term4"/><a contenteditable="false" data-type="indexterm" data-primary="box plots" id="idm45634218978520"/>way to visualize the five-number summary of a numerical distribution is a <a href="https://oreil.ly/7xZJ_">box plot</a>. It can be easily produced by Pandas’s built-in plot functionality. Let’s take a look at the box plot for the <code>length</code> column:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'length'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'box'</code><code class="p">,</code> <code class="n">vert</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
</pre>
<p><code>Out:</code></p>
<figure><div class="figure"><img src="Images/btap_01in01.jpg" width="1362" height="195"/>
<h6/>
</div></figure>

<p>As illustrated by this plot, 50% percent of the speeches (the box in the middle) have a length between roughly 12,000 and 22,000 characters, with the median at about 16,000 and a long tail with many outliers to the right. The distribution is obviously left-skewed. We can get some more details by plotting a histogram:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'length'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'hist'</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="mi">30</code><code class="p">)</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>
<figure><div class="figure"><img src="Images/btap_01in02.jpg" width="1385" height="346"/>
<h6/>
</div></figure>

<p>For the histogram, the value range of the <code>length</code> column is divided into 30 intervals of equal width, the <em>bins</em>. The y-axis shows the number of documents falling into each of these bins.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Comparing Value Distributions Across Categories"><div class="sect2" id="idm45634218980840">
<h2>Comparing Value Distributions Across Categories</h2>

<p>Peculiarities in the data often become visible when different subsets of the data are examined. A nice visualization to compare distributions across different categories is <a contenteditable="false" data-type="indexterm" data-primary="Seaborn library" id="idm45634218901320"/>Seaborn’s <a href="https://oreil.ly/jhlEE"><code>catplot</code></a>.</p>

<p>We <a contenteditable="false" data-type="indexterm" data-primary="box plots" id="idm45634218898568"/>show box and <a contenteditable="false" data-type="indexterm" data-primary="violin plots" id="idm45634218897304"/>violin plots to compare the distributions of the speech length of the five permanent members of the UN security council (<a data-type="xref" href="#fig-box-violin">Figure 1-2</a>). Thus, the category for the x-axis of <code>sns.catplot</code> is <code>country</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">where</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'country'</code><code class="p">]</code><code class="o">.</code><code class="n">isin</code><code class="p">([</code><code class="s1">'USA'</code><code class="p">,</code> <code class="s1">'FRA'</code><code class="p">,</code> <code class="s1">'GBR'</code><code class="p">,</code> <code class="s1">'CHN'</code><code class="p">,</code> <code class="s1">'RUS'</code><code class="p">])</code>
<code class="n">sns</code><code class="o">.</code><code class="n">catplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">df</code><code class="p">[</code><code class="n">where</code><code class="p">],</code> <code class="n">x</code><code class="o">=</code><code class="s2">"country"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"length"</code><code class="p">,</code> <code class="n">kind</code><code class="o">=</code><code class="s1">'box'</code><code class="p">)</code>
<code class="n">sns</code><code class="o">.</code><code class="n">catplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">df</code><code class="p">[</code><code class="n">where</code><code class="p">],</code> <code class="n">x</code><code class="o">=</code><code class="s2">"country"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"length"</code><code class="p">,</code> <code class="n">kind</code><code class="o">=</code><code class="s1">'violin'</code><code class="p">)</code>
</pre>

<figure><div id="fig-box-violin" class="figure"><img src="Images/btap_0102.jpg" width="1393" height="546"/>
<h6><span class="label">Figure 1-2. </span>Box plots (left) and violin plots (right) visualizing the distribution of speech lengths for selected countries.</h6>
</div></figure>

<p>The violin plot is the “smoothed” version of a box plot. Frequencies are visualized by the width of the violin body, while the box is still visible inside the violin. Both plots reveal that the dispersion of values, in this case the lengths of the speeches, for Russia is much larger than for Great Britain. But the existence of multiple peaks, as in Russia, only becomes apparent in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term4" id="idm45634218826776"/>violin plot.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Visualizing Developments Over Time"><div class="sect2" id="idm45634218825144">
<h2>Visualizing Developments Over Time</h2>

<p>If your <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="of developments over time" data-secondary-sortas="developments over time" id="idm45634218823352"/>data contains date or time attributes, it is always interesting to visualize some developments within the data over time. A first time series can be created by analyzing the number of speeches per year. We can use the Pandas <a contenteditable="false" data-type="indexterm" data-primary="size() grouping function" id="idm45634218821336"/>grouping function <code>size()</code> to return the number of rows per group. By simply appending <code>plot()</code>, we can visualize the resulting <code>DataFrame</code> (<a data-type="xref" href="#fig-timeline">Figure 1-3</a>, left):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'year'</code><code class="p">)</code><code class="o">.</code><code class="n">size</code><code class="p">()</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">title</code><code class="o">=</code><code class="s2">"Number of Countries"</code><code class="p">)</code>
</pre>

<p>The timeline reflects the development of the number of countries in the UN, as each country is eligible for only one speech per year. Actually, the UN has 193 members today. Interestingly, the speech length needed to decrease with more countries entering the debates, as the following analysis reveals (<a data-type="xref" href="#fig-timeline">Figure 1-3</a>, right):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'year'</code><code class="p">)</code><code class="o">.</code><code class="n">agg</code><code class="p">({</code><code class="s1">'length'</code><code class="p">:</code> <code class="s1">'mean'</code><code class="p">})</code> \
  <code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">title</code><code class="o">=</code><code class="s2">"Avg. Speech Length"</code><code class="p">,</code> <code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">30000</code><code class="p">))</code>
</pre>

<figure><div id="fig-timeline" class="figure"><img src="Images/btap_0103.jpg" width="1629" height="441"/>
<h6><span class="label">Figure 1-3. </span>Number of countries and average speech length over time.</h6>
</div></figure>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Pandas <a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634218528888"/><a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634218527752"/>dataframes not only can be easily visualized in Jupyter notebooks but also can be exported to Excel (<em>.xlsx</em>), HTML, CSV, LaTeX, and many other formats by built-in functions. There <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term5" id="idm45634218525720"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term6" id="idm45634218524344"/>is even a <code>to_clipboard()</code> function. Check the <a href="https://oreil.ly/HZDVN">documentation</a> for details.</p>
</div>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634218521544">
<h5>Resampling Time</h5>

<p>In the UN dataset, we already have yearly data; the integer column <code>year</code> contains <a contenteditable="false" data-type="indexterm" data-primary="time-series resampling" id="idm45634218519304"/>discrete values that we can use for grouping. But usually the <a contenteditable="false" data-type="indexterm" data-primary="resampling time" id="idm45634218518040"/><a contenteditable="false" data-type="indexterm" data-primary="datetime values" id="idm45634218516936"/><a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="datetime values in" id="idm45634218515832"/>dataset includes more fine-grained date or time values that need to be aggregated to an appropriate granularity for visualization. Depending on the scenario, this can range from hourly to yearly or even decades. Fortunately, Pandas has built-in functionality to access datetime values on different levels. For example, you can <a contenteditable="false" data-type="indexterm" data-primary="dt (datetime) functions" id="idm45634218513992"/>use the <code>dt</code> accessor of the underlying Pandas <code>Series</code> object to access certain properties such as <span class="keep-together"><code>dt.year</code></span> directly. The following table shows some examples:<sup><a data-type="noteref" id="idm45634218510984-marker" href="ch01.xhtml#idm45634218510984">1</a></sup></p>

<table class="border">
	<thead>
		<tr>
			<th>Datetime property</th>
			<th>Description</th>
			<th>Datetime property</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>dt.date</code></td>
			<td>Date part of datetime</td>
			<td><code>dt.hour</code></td>
			<td>Hour part of datetime</td>
		</tr>
		<tr>
			<td><code>dt.year</code></td>
			<td>Year</td>
			<td><code>dt.month</code></td>
			<td>Month within year as number</td>
		</tr>
		<tr>
			<td><code>dt.quarter</code></td>
			<td>Quarter within year as number</td>
			<td><code>dt.week</code></td>
			<td>Week within year as number</td>
		</tr>
	</tbody>
</table>

<p>Let’s assume the <code>datetime</code> column in our dateframe actually has the name <code>time</code> as recommended for our blueprints. Then you could just create an additional <code>year</code> column in any <code>DataFrame</code> with this command:</p>

<pre data-code-language="python" data-type="programlisting">
    <code class="n">df</code><code class="p">[</code><code class="s1">'year'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'time'</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">year</code></pre>

<p>Often you need combined values like <code>2020/Week 24</code> to plot a time series with appropriate labels. A flexible way to achieve this is to use <code>dt.strftime()</code>, which provides access to the common <a href="https://oreil.ly/KvMjG"><code>strftime</code></a> (string from time) functionality in Pandas:</p>

<pre data-code-language="python" data-type="programlisting">
    <code class="n">df</code><code class="p">[</code><code class="s1">'week'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'time'</code><code class="p">]</code><code class="o">.</code><code class="n">dt</code><code class="o">.</code><code class="n">strftime</code><code class="p">(</code><code class="s2">"</code><code class="si">%Y</code><code class="s2">/Week %W"</code><code class="p">)</code></pre>

<p>Pandas even has a <a contenteditable="false" data-type="indexterm" data-primary="resample() function" id="idm45634218445448"/>built-in function called <a href="https://oreil.ly/E0oOX"><code>resample()</code></a> for time-series resampling. However, it aggregates the data and is therefore not useful <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term3" id="idm45634218411320"/>when <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term2" id="idm45634218409848"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term40" id="idm45634218408440"/>working with text.</p>
</div></aside>
</div></section>
</div></section>

<section class="blueprint" data-type="sect1" data-pdf-bookmark="Blueprint: Building a Simple Text Preprocessing Pipeline"><div class="sect1" id="ch1-pipeline">
<h1>Blueprint: Building a Simple Text Preprocessing Pipeline</h1>

<p>The analysis of metadata such as categories, time, authors, and other attributes gives some first insights on the corpus. But it’s much more interesting to dig deeper into the actual content and explore frequent words in different subsets or time periods. In this section, we will develop a basic blueprint to <a contenteditable="false" data-type="indexterm" data-primary="data preprocessing" data-secondary="pipelines for" id="ch1_term7"/>prepare <a contenteditable="false" data-type="indexterm" data-primary="pipelines" id="ch1_term8"/>text <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="preprocessing pipeline for" id="ch1_term9"/>for a quick first analysis consisting of a simple sequence of steps (<a data-type="xref" href="#fig-simple-pipeline">Figure 1-4</a>). As the output of one operation forms the input of the next one, such a sequence is also called a <em>processing pipeline</em> that transforms the original text into a number of tokens.</p>

<figure><div id="fig-simple-pipeline" class="figure"><img src="Images/btap_0104.jpg" width="1352" height="113"/>
<h6><span class="label">Figure 1-4. </span>Simple preprocessing pipeline.</h6>
</div></figure>

<p>The pipeline presented here consists of three steps: case-folding into lowercase, tokenization, and stop word removal. These steps will be discussed in depth and extended in <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>, where we make use of spaCy. To keep it fast and simple here, we build our own <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with regular expressions" data-secondary-sortas="regular expressions" id="idm45634218394776"/>tokenizer based on regular expressions and show how to use an arbitrary stop word list.</p>

<section data-type="sect2" data-pdf-bookmark="Performing Tokenization with Regular Expressions"><div class="sect2" id="idm45634218392904">
<h2>Performing Tokenization with Regular Expressions</h2>

<p><em>Tokenization</em> <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="defined" id="idm45634218390856"/>is the process of extracting <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with initial text analyses" data-secondary-sortas="initial text analyses" id="idm45634218389320"/>words from a sequence of characters. <span class="keep-together">In Western </span>languages, words are often separated by whitespaces and punctuation <span class="keep-together">characters</span>. Thus, the <a contenteditable="false" data-type="indexterm" data-primary="str.split() tokenizer" id="idm45634218368744"/>simplest and fastest tokenizer is Python’s native <span class="keep-together"><code>str.split()</code></span> method, which splits on whitespace. A more flexible way is to use <a contenteditable="false" data-type="indexterm" data-primary="regular expressions" id="idm45634218366632"/>regular <span class="keep-together">expressions</span>.</p>

<p>Regular expressions and the <a contenteditable="false" data-type="indexterm" data-primary="regex library (Python)" id="idm45634218364168"/><a contenteditable="false" data-type="indexterm" data-primary="re library (Python)" id="idm45634218363064"/>Python libraries <code>re</code> and <code>regex</code> will be introduced in more detail in <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>. Here, we want to apply a simple pattern that <a contenteditable="false" data-type="indexterm" data-primary="matching words with simple pattern" id="idm45634218359912"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="defined" id="idm45634218358744"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="matching with simple pattern" id="idm45634218357368"/>matches words. Words in our definition consist of at least one letter as well as digits and hyphens. Pure numbers are skipped because they almost exclusively represent dates or speech or session identifiers in this corpus.</p>

<p>The frequently used expression <code>[A-Za-z]</code> is not a good option for matching letters because it misses accented letters like <em>ä</em> or <em>â</em>. Much better is the <a contenteditable="false" data-type="indexterm" data-primary="POSIX character classes" id="idm45634218353656"/>POSIX character class <code>\p{L}</code>, which selects all Unicode letters. Note that we need the <a href="https://oreil.ly/hJ6M2"><code>regex</code> library</a> instead of <code>re</code> to work with POSIX character classes. The following expression matches tokens <span class="keep-together">consisting</span> of at least one letter (<code>\p{L}</code>), preceded and followed by an arbitrary sequence of alphanumeric characters (<code>\w</code> includes digits, letters, and underscore) and <span class="keep-together">hyphens (<code>-</code>):</span></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">regex</code> <code class="kn">as</code> <code class="nn">re</code>

<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'[\w-]*\p{L}[\w-]*'</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
</pre>

<p>Let’s try it with a sample sentence from the corpus:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"Let's defeat SARS-CoV-2 together in 2020!"</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"|"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">tokens</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Let|s|defeat|SARS-CoV-2|together|in
</pre>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Treating Stop Words"><div class="sect2" id="idm45634218319272">
<h2>Treating Stop Words</h2>

<p>The most <a contenteditable="false" data-type="indexterm" data-primary="stop words" id="ch1_term11"/>frequent <a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="ch1_term12"/>words in text are common words such as determiners, auxiliary verbs, pronouns, adverbs, and so on. These words are called <em>stop words</em>. Stop words usually don’t carry much information but hide interesting content because of their high frequencies. Therefore, stop words are often removed before data analysis or model training.</p>

<p>In this section, we show how to discard stop words contained in a predefined list. Common stop word lists are available for many languages and are integrated in almost any NLP library. We will work with <a contenteditable="false" data-type="indexterm" data-primary="NLTK library" id="ch1_term10"/>NLTK’s list of stop words here, but you could use any list of words as a filter.<sup><a data-type="noteref" id="idm45634218281240-marker" href="ch01.xhtml#idm45634218281240">2</a></sup> For fast lookup, you should always convert a <span class="keep-together">list to</span> a set. Sets are hash-based structures like dictionaries with nearly constant <span class="keep-together">lookup time</span>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">nltk</code>

<code class="n">stopwords</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">nltk</code><code class="o">.</code><code class="n">corpus</code><code class="o">.</code><code class="n">stopwords</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="s1">'english'</code><code class="p">))</code>
</pre>

<p>Our approach to remove stop words from a given list, wrapped into the small function shown here, consists of a simple list comprehension. For the check, tokens are converted to lowercase as NLTK’s list contains only lowercase words:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">remove_stop</code><code class="p">(</code><code class="n">tokens</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">t</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">tokens</code> <code class="k">if</code> <code class="n">t</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">]</code>
</pre>

<p>Often you’ll need to add domain-specific stop words to the predefined list. For example, if you are analyzing emails, the terms <em>dear</em> and <em>regards</em> will probably appear in almost any document. On the other hand, you might want to treat some of the words in the predefined list not as stop words. We can add additional stop words <span class="keep-together">and exclude</span> others from the list using two of Python’s <a contenteditable="false" data-type="indexterm" data-primary="set operators (Python)" id="idm45634218213144"/>set operators, <code>|</code> (union/or) and <span class="keep-together"><code>-</code> (difference):</span></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">include_stopwords</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'dear'</code><code class="p">,</code> <code class="s1">'regards'</code><code class="p">,</code> <code class="s1">'must'</code><code class="p">,</code> <code class="s1">'would'</code><code class="p">,</code> <code class="s1">'also'</code><code class="p">}</code>
<code class="n">exclude_stopwords</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'against'</code><code class="p">}</code>

<code class="n">stopwords</code> <code class="o">|=</code> <code class="n">include_stopwords</code>
<code class="n">stopwords</code> <code class="o">-=</code> <code class="n">exclude_stopwords</code>
</pre>

<p>The stop word list from NLTK is conservative and contains only 179 words. Surprisingly, <em>would</em> is not considered a stop word, while <em>wouldn’t</em> is. This illustrates a common problem with predefined stop word lists: inconsistency. Be aware that removing stop words can significantly affect the performance of semantically targeted analyses, as explained in <a data-type="xref" href="#why-rm-stop-words-can-be-danger">“Why Removing Stop Words Can Be Dangerous”</a>.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="why-rm-stop-words-can-be-danger">
<h5>Why Removing Stop Words Can Be Dangerous</h5>

<p>Stop word removal is a coarse-grained rule-based method. Be careful with the stop word lists you are using, and make sure that you don’t delete valuable information. Look at this simple example: “I don’t like ice cream.”</p>

<p>Both NLTK and <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for stop words" data-secondary-sortas="stop words" id="idm45634218130632"/>spaCy have <em>I</em> and <em>don’t</em> (same as <em>do not</em>) in their stop word lists. If you remove those stop words, all that’s left is <em>like ice cream.</em> This kind of preprocessing would heavily distort any kind of sentiment analysis. <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="of stop words" data-secondary-sortas="stop words" id="idm45634218127096"/>TF-IDF weighting, as introduced later in this section, automatically underweighs frequently occurring words but keeps those terms in the vocabulary.</p>
</div></aside>

<p>In addition to or instead of a fixed list of stop words, it can be helpful to treat every word that appears in more than, say, 80% of the documents as a stop word. Such common words make it difficult to distinguish content. The <a contenteditable="false" data-type="indexterm" data-primary="max_df parameter" id="idm45634218124312"/>parameter <code>max_df</code> of the <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="max_df and min_df parameters of" id="idm45634218122632"/>scikit-learn vectorizers, as covered in <a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a>, does exactly this. Another method is to filter words based on the word type (part of speech). This <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term10" id="idm45634218119992"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term11" id="idm45634218118616"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term12" id="idm45634218117240"/>concept will be explained in <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Processing a Pipeline with One Line of Code"><div class="sect2" id="idm45634218318680">
<h2>Processing a Pipeline with One Line of Code</h2>

<p>Let’s get back to the <code>DataFrame</code> containing the documents of our corpus. We want to create a new column called <code>tokens</code> containing the lowercased, tokenized text without stop words for each document. For that, we use an extensible pattern for a processing pipeline. In our case, we will change all text to <a contenteditable="false" data-type="indexterm" data-primary="case-folding" id="idm45634218084904"/>lowercase, tokenize it, and remove stop words. Other operations can be added by simply extending the pipeline:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">pipeline</code> <code class="o">=</code> <code class="p">[</code><code class="nb">str</code><code class="o">.</code><code class="n">lower</code><code class="p">,</code> <code class="n">tokenize</code><code class="p">,</code> <code class="n">remove_stop</code><code class="p">]</code>

<code class="k">def</code> <code class="nf">prepare</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">pipeline</code><code class="p">):</code>
    <code class="n">tokens</code> <code class="o">=</code> <code class="n">text</code>
    <code class="k">for</code> <code class="n">transform</code> <code class="ow">in</code> <code class="n">pipeline</code><code class="p">:</code>
        <code class="n">tokens</code> <code class="o">=</code> <code class="n">transform</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">tokens</code>
</pre>

<p>If we put all this into a function, it becomes a <a contenteditable="false" data-type="indexterm" data-primary="map (Pandas)" id="idm45634218050104"/><a contenteditable="false" data-type="indexterm" data-primary="apply function" id="idm45634218049240"/>perfect use case for Pandas’s <code>map</code> or <code>apply</code> operation. Functions such as <code>map</code> and <code>apply</code>, which take other functions as parameters, are <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="higher-order functions of" id="idm45634218046248"/>called <em>higher-order functions</em> in mathematics and computer science.</p>

<table class="border" id="tab-map-apply">
	<caption><span class="label">Table 1-2. </span>Pandas higher-order functions</caption>
	<thead>
		<tr>
			<th>Function</th>
			<th>Description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><code>Series.map</code></td>
			<td>Works element by element on a Pandas <code>Series</code></td>
		</tr>
		<tr>
			<td><code>Series.apply</code></td>
			<td>Same as <code>map</code> but allows additional parameters</td>
		</tr>
		<tr class="pagebreak-before">
			<td><code>DataFrame.applymap</code></td>
			<td>Element by element on a Pandas <code>DataFrame</code> (same as <code>map</code> on <code>Series</code>)</td>
		</tr>
		<tr>
			<td><code>DataFrame.apply</code></td>
			<td>Works on rows or columns of a <code>DataFrame</code> and supports aggregation</td>
		</tr>
	</tbody>
</table>

<p>Pandas supports the different higher-order functions on series and <a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634218031448"/><a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634218030344"/>dataframes (<a data-type="xref" href="#tab-map-apply">Table 1-2</a>). These functions not only allow you to specify a series of functional data transformations in a comprehensible way, but they can also be easily parallelized. The Python package <a href="https://oreil.ly/qwPB4"><code>pandarallel</code></a>, for example, provides parallel versions of <code>map</code> and <code>apply</code>.</p>

<p>Scalable frameworks like <a href="https://spark.apache.org">Apache Spark</a> support similar operations on dataframes even more elegantly. In fact, the map and reduce operations in distributed programming are based on the same principle of functional programming. In addition, many programming languages, including Python and JavaScript, have a native map operation for lists or arrays.</p>

<p>Using one of Pandas’s higher-order operations, applying a functional transformation becomes a one-liner:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'tokens'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">prepare</code><code class="p">,</code> <code class="n">pipeline</code><code class="o">=</code><code class="n">pipeline</code><code class="p">)</code>
</pre>

<p>The <code>tokens</code> column now consists of Python lists containing the extracted tokens for each document. Of course, this additional column basically doubles memory consumption of the <code>DataFrame</code>, but it allows you to quickly access the <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with initial text analyses" data-secondary-sortas="initial text analyses" id="idm45634217972792"/>tokens directly for further analysis. Nevertheless, the following blueprints are designed in such a way that the tokenization can also be performed on the fly during analysis. In this way, performance can be traded for memory consumption: either tokenize once before analysis and consume memory or tokenize on the fly and wait.</p>

<p>We also add another column containing the length of the token list for summarizations later:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'num_tokens'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'tokens'</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="nb">len</code><code class="p">)</code>
</pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p><code>tqdm</code> (pronounced <em>taqadum</em> for “progress” in Arabic) is a great <a contenteditable="false" data-type="indexterm" data-primary="tqdm library" id="idm45634217924584"/>library for progress bars in Python. It supports conventional loops, e.g., by using <code>tqdm_range</code> instead of <code>range</code>, and it supports Pandas by providing <code>progress_map</code> and <code>progress_apply</code> operations on dataframes.<sup><a data-type="noteref" id="idm45634217921624-marker" href="ch01.xhtml#idm45634217921624">3</a></sup> Our accompanying notebooks on GitHub use these operations, but we stick to <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term7" id="idm45634217919944"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term8" id="idm45634217918568"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term9" id="idm45634217917192"/>plain Pandas here in the book.</p>
</div>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Blueprints for Word Frequency Analysis"><div class="sect1" id="idm45634218114312">
<h1>Blueprints for Word Frequency Analysis</h1>

<p>Frequently <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="of word frequency" data-secondary-sortas="word frequency" id="ch1_term13"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" id="ch1_term14"/>used words and phrases can give us some basic understanding of the discussed topics. However, word frequency analysis ignores the order and the context of the words. This is the idea of the famous <a contenteditable="false" data-type="indexterm" data-primary="bag-of-words models" id="idm45634217910584"/>bag-of-words model (see also <a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a>): all the words are thrown into a bag where they tumble into a jumble. The original arrangement in the text is lost; only the frequency of the terms is taken into account. This model does not work well for complex tasks such as sentiment analysis or question answering, but it works surprisingly well for classification and topic modeling. In addition, it’s a good starting point for understanding what the texts are all about.</p>

<p>In this section, we will develop a number of blueprints to calculate and visualize word frequencies. As raw frequencies overweigh unimportant but frequent words, we will also introduce TF-IDF at the end of the process. We will implement the frequency calculation by using a <code>Counter</code> because it is simple and extremely fast.</p>

<section class="blueprint pagebreak-after" data-type="sect2" data-pdf-bookmark="Blueprint: Counting Words with a Counter"><div class="sect2" id="idm45634217890216">
<h2>Blueprint: Counting Words with a Counter</h2>

<p>Python’s standard <a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="counter for" id="ch1_term17"/>library <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="counting words with counter for" id="ch1_term16"/>has a <a contenteditable="false" data-type="indexterm" data-primary="Counter (Python library)" id="ch1_term15"/>built-in class <code>Counter</code>, which does exactly what you might think: it counts things.<sup><a data-type="noteref" id="idm45634217882104-marker" href="ch01.xhtml#idm45634217882104">4</a></sup> The easiest way to work with a counter is to create it from a list of items, in our case strings representing the words or tokens. The resulting counter is basically a dictionary object containing those items as keys and their frequencies as values.</p>

<p>Let’s illustrate its functionality with a simple example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>

<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="s2">"She likes my cats and my cats like my sofa."</code><code class="p">)</code>

<code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">counter</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Counter({'my': 3, 'cats': 2, 'She': 1, 'likes': 1, 'and': 1, 'like': 1,
         'sofa': 1})
</pre>

<p>The counter requires a list as input, so any text needs to be tokenized in advance. What’s nice about the counter is that it can be incrementally updated with a list of tokens of a second document:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">more_tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="s2">"She likes dogs and cats."</code><code class="p">)</code>
<code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">more_tokens</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">counter</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Counter({'my': 3, 'cats': 3, 'She': 2, 'likes': 2, 'and': 2, 'like': 1,
         'sofa': 1, 'dogs': 1})
</pre>

<p>To find the <a contenteditable="false" data-type="indexterm" data-primary="speeches, UN" data-secondary="statistical analysis of" id="ch1_term41"/>most frequent words within a corpus, we need to create a counter from the list of all words in all documents. A naive approach would be to concatenate all documents into a single, giant list of tokens, but that does not scale for larger datasets. It is much more efficient to call the <code>update</code> function of the counter object for each single document.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>

<code class="n">df</code><code class="p">[</code><code class="s1">'tokens'</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">)</code>
</pre>

<p>We do a little trick here and <a contenteditable="false" data-type="indexterm" data-primary="map (Pandas)" id="idm45634217796296"/>put <code>counter.update</code> in the <code>map</code> function. The magic happens inside the <code>update</code> function under the hood. The whole <code>map</code> call runs extremely fast; it takes only about three seconds for the 7,500 UN speeches and scales linearly with the total number of tokens. The reason is that dictionaries in general and counters in particular are implemented as hash tables. A single counter is pretty compact compared to the whole corpus: it contains each word only once, along with its <span class="keep-together">frequency.</span></p>

<p>Now we can retrieve the most common words in the text with the respective counter function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="n">counter</code><code class="o">.</code><code class="n">most_common</code><code class="p">(</code><code class="mi">5</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[('nations', 124508),
 ('united', 120763),
 ('international', 117223),
 ('world', 89421),
 ('countries', 85734)]
</pre>

<p>For further processing and analysis, it is much more convenient to transform the counter into a <a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634217760248"/><a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634217759176"/>Pandas <code>DataFrame</code>, and this is what the following blueprint function finally does. The tokens make up the index of the <code>DataFrame</code>, while the frequency values are stored in a column named <code>freq</code>. The rows are sorted so that the most frequent words appear at the head:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'tokens'</code><code class="p">,</code> <code class="n">preprocess</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">min_freq</code><code class="o">=</code><code class="mi">2</code><code class="p">):</code>

    <code class="c1"># process tokens and update counter</code>
    <code class="k">def</code> <code class="nf">update</code><code class="p">(</code><code class="n">doc</code><code class="p">):</code>
        <code class="n">tokens</code> <code class="o">=</code> <code class="n">doc</code> <code class="k">if</code> <code class="n">preprocess</code> <code class="ow">is</code> <code class="bp">None</code> <code class="k">else</code> <code class="n">preprocess</code><code class="p">(</code><code class="n">doc</code><code class="p">)</code>
        <code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code>

    <code class="c1"># create counter and run through all data</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>
    <code class="n">df</code><code class="p">[</code><code class="n">column</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">update</code><code class="p">)</code>

    <code class="c1"># transform counter into a DataFrame</code>
    <code class="n">freq_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_dict</code><code class="p">(</code><code class="n">counter</code><code class="p">,</code> <code class="n">orient</code><code class="o">=</code><code class="s1">'index'</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">])</code>
    <code class="n">freq_df</code> <code class="o">=</code> <code class="n">freq_df</code><code class="o">.</code><code class="n">query</code><code class="p">(</code><code class="s1">'freq &gt;= @min_freq'</code><code class="p">)</code>
    <code class="n">freq_df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="s1">'token'</code>

    <code class="k">return</code> <code class="n">freq_df</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s1">'freq'</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
</pre>

<p>The function takes, as a first parameter, a Pandas <code>DataFrame</code> and takes the column name containing the tokens or the text as a second parameter. As we already stored the prepared tokens in the column <code>tokens</code> of the <code>DataFrame</code> containing the speeches, we can use the following two lines of code to compute the <code>DataFrame</code> with word frequencies and display the top five tokens:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">freq_df</code> <code class="o">=</code> <code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="n">freq_df</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
</pre>
<p><code>Out:</code></p>
<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th>token</th>
			<th>freq</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>nations</td>
			<td>124508</td>
		</tr>
		<tr>
			<td>united</td>
			<td>120763</td>
		</tr>
		<tr>
			<td>international</td>
			<td>117223</td>
		</tr>
		<tr>
			<td>world</td>
			<td>89421</td>
		</tr>
		<tr>
			<td>countries</td>
			<td>85734</td>
		</tr>
	</tbody>
</table>

<p>If we don’t want to use precomputed tokens for some special analysis, we could <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with initial text analyses" data-secondary-sortas="initial text analyses" id="idm45634217586424"/>tokenize the text on the fly with a custom preprocessing function as the third parameter. For example, we could generate and count all words with 10 or more characters with this on-the-fly tokenization of the text:</p>

<pre data-code-language="python" data-type="programlisting">
    <code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'text'</code><code class="p">,</code>
                <code class="n">preprocess</code><code class="o">=</code><code class="k">lambda</code> <code class="n">text</code><code class="p">:</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s-Affix">r</code><code class="s2">"\w{10,}"</code><code class="p">,</code> <code class="n">text</code><code class="p">))</code></pre>

<p>The last parameter <a contenteditable="false" data-type="indexterm" data-primary="count_words function" id="idm45634217490744"/>of <code>count_words</code> defines a minimum frequency of tokens to be included in the result. Its default is set to 2 to cut down the long tail <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term15" id="idm45634217489032"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term16" id="idm45634217487656"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term17" id="idm45634217486280"/>of hapaxes, i.e., tokens occurring only once.</p>
</div></section>

<section class="blueprint" data-type="sect2" data-pdf-bookmark="Blueprint: Creating a Frequency Diagram"><div class="sect2" id="idm45634217889272">
<h2>Blueprint: Creating a Frequency Diagram</h2>

<p>There are dozens of ways to <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="for producing tables and diagrams" data-secondary-sortas="producing tables and diagrams" id="idm45634217483064"/>produce tables and diagrams in Python. We prefer Pandas with its built-in plot functionality because it is easier to use than plain <a contenteditable="false" data-type="indexterm" data-primary="Matplotlib" id="idm45634217481208"/>Matplotlib. We assume a <code>DataFrame</code> <code>freq_df</code> generated by the previous blueprint for visualization. Creating a <a contenteditable="false" data-type="indexterm" data-primary="frequency diagrams" id="idm45634217479112"/><a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="frequency diagrams for" id="idm45634217478008"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="frequency diagrams for" id="idm45634217476616"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with frequency diagrams" data-secondary-sortas="frequency diagrams" id="idm45634217475224"/>frequency diagram based on such a <code>DataFrame</code> now becomes basically a one-liner. We add two more lines for formatting:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">ax</code> <code class="o">=</code> <code class="n">freq_df</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">15</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'barh'</code><code class="p">,</code> <code class="n">width</code><code class="o">=</code><code class="mf">0.95</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">invert_yaxis</code><code class="p">()</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s1">'Frequency'</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s1">'Token'</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="s1">'Top Words'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<figure><div class="figure"><img src="Images/btap_01in03.jpg" width="1523" height="592"/>
<h6/>
</div></figure>

<p>Using horizontal bars (<code>barh</code>) for word frequencies greatly improves readability because the words appear horizontally on the y-axis in a readable form. The y-axis is inverted to place the top words at the top of the chart. The axis labels and title can optionally be modified.</p>
</div></section>

<section class="blueprint" data-type="sect2" data-pdf-bookmark="Blueprint: Creating Word Clouds"><div class="sect2" id="idm45634217409864">
<h2>Blueprint: Creating Word Clouds</h2>

<p>Plots of frequency distributions like the ones shown previously give detailed information about the token frequencies. But it is quite difficult to compare frequency diagrams for different time periods, categories, authors, and so on. <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="word clouds for" id="ch1_term18"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word clouds " data-secondary-sortas="word clouds " id="ch1_term19"/><a contenteditable="false" data-type="indexterm" data-primary="word clouds " id="ch1_term20"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="word clouds for" id="ch1_term21"/>Word clouds, in contrast, visualize the frequencies by different font sizes. They are much easier to comprehend and to compare, but they lack the precision of tables and bar charts. You should keep in mind that long words or words with capital letters get unproportionally high attraction.</p>

<p>The <a contenteditable="false" data-type="indexterm" data-primary="wordcloud module (Python)" id="idm45634217400264"/>Python module <a href="https://oreil.ly/RV0r5"><code>wordcloud</code></a> generates nice word clouds from texts or counters. The simplest way to use it is to instantiate a word cloud object with some options, such as the maximum number of words and a stop word list, and then let the <code>wordcloud</code> module handle the tokenization and stop word removal. The following code shows how to generate a word cloud for the text of the 2015 US speech and display the resulting image with <a contenteditable="false" data-type="indexterm" data-primary="Matplotlib" id="idm45634217397352"/>Matplotlib:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">wordcloud</code> <code class="kn">import</code> <code class="n">WordCloud</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="n">text</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">query</code><code class="p">(</code><code class="s2">"year==2015 and country=='USA'"</code><code class="p">)[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>

<code class="n">wc</code> <code class="o">=</code> <code class="n">WordCloud</code><code class="p">(</code><code class="n">max_words</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)</code>
<code class="n">wc</code><code class="o">.</code><code class="n">generate</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">wc</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'bilinear'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
</pre>

<p>However, this works only for a single text and not a (potentially large) set of documents. For the latter use case, it is much faster to create a frequency counter first and then use the function <code>generate_from_frequencies()</code>.</p>

<p>Our blueprint is a little wrapper around this function to also support a Pandas <code>Series</code> containing frequency values as created by <code>count_words</code>. The <code>WordCloud</code> class already has a magnitude of options to fine-tune the result. We use some of them in the following function to demonstrate possible adjustments, but you should check the documentation for details:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">wordcloud</code><code class="p">(</code><code class="n">word_freq</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="bp">None</code><code class="p">):</code>

    <code class="n">wc</code> <code class="o">=</code> <code class="n">WordCloud</code><code class="p">(</code><code class="n">width</code><code class="o">=</code><code class="mi">800</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code>
                   <code class="n">background_color</code><code class="o">=</code> <code class="s2">"black"</code><code class="p">,</code> <code class="n">colormap</code><code class="o">=</code><code class="s2">"Paired"</code><code class="p">,</code>
                   <code class="n">max_font_size</code><code class="o">=</code><code class="mi">150</code><code class="p">,</code> <code class="n">max_words</code><code class="o">=</code><code class="n">max_words</code><code class="p">)</code>

    <code class="c1"># convert DataFrame into dict</code>
    <code class="k">if</code> <code class="nb">type</code><code class="p">(</code><code class="n">word_freq</code><code class="p">)</code> <code class="o">==</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">:</code>
        <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">(</code><code class="n">word_freq</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to_dict</code><code class="p">())</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">counter</code> <code class="o">=</code> <code class="n">word_freq</code>

    <code class="c1"># filter stop words in frequency counter</code>
    <code class="k">if</code> <code class="n">stopwords</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">:</code>
        <code class="n">counter</code> <code class="o">=</code> <code class="p">{</code><code class="n">token</code><code class="p">:</code><code class="n">freq</code> <code class="k">for</code> <code class="p">(</code><code class="n">token</code><code class="p">,</code> <code class="n">freq</code><code class="p">)</code> <code class="ow">in</code> <code class="n">counter</code><code class="o">.</code><code class="n">items</code><code class="p">()</code>
                              <code class="k">if</code> <code class="n">token</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">}</code>
    <code class="n">wc</code><code class="o">.</code><code class="n">generate_from_frequencies</code><code class="p">(</code><code class="n">counter</code><code class="p">)</code>

    <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="n">title</code><code class="p">)</code>

    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">wc</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'bilinear'</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
</pre>

<p>The function has two convenience parameters to filter words. <code>skip_n</code> skips the top <em>n</em> words of the list. <a contenteditable="false" data-type="indexterm" data-primary="UN General Debates dataset" id="idm45634217300376"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="UN General Debates" id="idm45634217299448"/>Obviously, in a UN corpus words like <em>united</em>, <em>nations</em>, or <em>international</em> are heading the list. It may be more interesting to visualize what comes next. The second filter is an (additional) list of stop words. Sometimes it is helpful to filter out specific frequent but uninteresting words for the visualization only.<sup><a data-type="noteref" id="idm45634217138552-marker" href="ch01.xhtml#idm45634217138552">5</a></sup></p>

<p>So, let’s take a look at the 2015 speeches (<a data-type="xref" href="#fig-2015-speeches">Figure 1-5</a>). The left word cloud visualizes the most frequent words unfiltered. The right word cloud instead treats the 50 most frequent words of the complete corpus as <a contenteditable="false" data-type="indexterm" data-primary="stop words" id="idm45634217135320"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="idm45634217134216"/>stop words:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">freq_2015_df</code> <code class="o">=</code> <code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'year'</code><code class="p">]</code><code class="o">==</code><code class="mi">2015</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_2015_df</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">],</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_2015_df</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">],</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="n">freq_df</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
</pre>

<figure><div id="fig-2015-speeches" class="figure"><img src="Images/btap_0105.jpg" width="1749" height="423"/>
<h6><span class="label">Figure 1-5. </span>Word clouds for the 2015 speeches including all words (left) and without the 50 most frequent words (right).</h6>
</div></figure>

<p>Clearly, the right word cloud without the most frequent words of the corpus gives a much better idea of the 2015 topics, but there are still frequent and unspecific words like <em>today</em> or <em>challenges</em>. We need a way to give less weight to those words, as shown in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term18" id="idm45634217089448"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term19" id="idm45634217088072"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term20" id="idm45634217086696"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term21" id="idm45634217085320"/>next section.</p>
</div></section>

<section class="blueprint" data-type="sect2" data-pdf-bookmark="Blueprint: Ranking with TF-IDF"><div class="sect2" id="idm45634217409240">
<h2>Blueprint: Ranking with TF-IDF</h2>

<p>As illustrated in <a data-type="xref" href="#fig-2015-speeches">Figure 1-5</a>, visualizing the most frequent words usually does not reveal much insight. Even if stop words are removed, the most common words are usually obvious domain-specific terms that are quite similar in any subset (slice) of the data. But we would like to give more importance to those words that appear more frequently in a given slice of the data than “usual.” Such a slice can be any subset of the corpus, e.g., a single speech, the speeches of a certain decade, or the speeches from one country.</p>

<p>We want to <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="TF-IDF ranking for" id="ch1_term22"/><a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="text data analysis with" id="ch1_term23"/>highlight words whose actual word frequency in a slice is higher than their total probability would suggest. There is a number of algorithms to measure the “surprise” factor of a word. One of the simplest but best working approaches is to complement the term frequency with the <a contenteditable="false" data-type="indexterm" data-primary="inverse document frequency (IDF)" id="ch1_term24"/>inverse document frequency (see sidebar).</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634217074696">
<h5>Inverse Document Frequency</h5>

<p>The <em>inverse document frequency</em> (IDF) is a weighting factor that measures the “unusualness” of a term in a corpus. It is often used to reduce the influence of common terms for data analysis or machine learning. To explain it, let’s first define the <em>document frequency</em> of a term <math alttext="t">
  <mi>t</mi>
</math>. Given a corpus (set of documents) <math alttext="upper C">
  <mi>C</mi>
</math>, the document frequency <math alttext="d f left-parenthesis t right-parenthesis">
  <mrow>
    <mi>d</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>)</mo>
  </mrow>
</math> is simply the number of documents <math alttext="d">
  <mi>d</mi>
</math> in <math alttext="upper C">
  <mi>C</mi>
</math> that contain the term <math alttext="t">
  <mi>t</mi>
</math>. Mathematically, it looks as follows:</p>

<div data-type="equation">
<p><math alttext="d f left-parenthesis t right-parenthesis equals StartAbsoluteValue StartSet d element-of upper C vertical-bar t element-of d EndSet EndAbsoluteValue" display="block">
  <mrow>
    <mi>d</mi>
    <mi>f</mi>
    <mrow>
      <mo>(</mo>
      <mi>t</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfenced separators="" open="|" close="|">
      <mo>{</mo>
      <mi>d</mi>
      <mo>∈</mo>
      <mi>C</mi>
      <mo>|</mo>
      <mi>t</mi>
      <mo>∈</mo>
      <mi>d</mi>
      <mo>}</mo>
    </mfenced>
  </mrow>
</math></p>
</div>

<p>Terms appearing in many documents have a high document frequency. Based on this, we can define the <em>inverse document frequency</em> <math alttext="i d f left-parenthesis t right-parenthesis">
  <mrow>
    <mi>i</mi>
    <mi>d</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>)</mo>
  </mrow>
</math> as follows:</p>

<div data-type="equation">
<p><math alttext="i d f left-parenthesis t right-parenthesis equals l o g left-parenthesis StartFraction StartAbsoluteValue upper C EndAbsoluteValue Over d f left-parenthesis t right-parenthesis EndFraction right-parenthesis" display="block">
  <mrow>
    <mi>i</mi>
    <mi>d</mi>
    <mi>f</mi>
    <mrow>
      <mo>(</mo>
      <mi>t</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mi>l</mi>
    <mi>o</mi>
    <mi>g</mi>
    <mfenced separators="" open="(" close=")">
      <mfrac><mrow><mo>|</mo><mi>C</mi><mo>|</mo></mrow> <mrow><mi>d</mi><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mfrac>
    </mfenced>
  </mrow>
</math></p>
</div>

<p>The logarithm is used for sublinear scaling. Otherwise, rare words would get extremely high IDF scores. Note that <math alttext="i d f left-parenthesis t right-parenthesis equals 0">
  <mrow>
    <mi>i</mi>
    <mi>d</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mn>0</mn>
  </mrow>
</math> for terms that appear in all documents, i.e., <math alttext="d f left-parenthesis t right-parenthesis equals StartAbsoluteValue upper C EndAbsoluteValue">
  <mrow>
    <mi>d</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mo>|</mo>
    <mi>C</mi>
    <mo>|</mo>
  </mrow>
</math>. To not completely ignore those terms, some libraries add a constant to the whole term.<sup><a data-type="noteref" id="idm45634216924552-marker" href="ch01.xhtml#idm45634216924552">6</a></sup> We add the term <code>0.1</code>, which is roughly the value of tokens appearing in 90% of the documents (<math alttext="l o g left-parenthesis 1 slash 0.9 right-parenthesis">
  <mrow>
    <mi>l</mi>
    <mi>o</mi>
    <mi>g</mi>
    <mo>(</mo>
    <mn>1</mn>
    <mo>/</mo>
    <mn>0</mn>
    <mo>.</mo>
    <mn>9</mn>
    <mo>)</mo>
  </mrow>
</math>).<sup><a data-type="noteref" id="idm45634216916280-marker" href="ch01.xhtml#idm45634216916280">7</a></sup></p>

<p>For the weighting of a term <math alttext="t">
  <mi>t</mi>
</math> in a set of documents <math alttext="upper D subset-of-or-equal-to upper C">
  <mrow>
    <mi>D</mi>
    <mo>⊆</mo>
    <mi>C</mi>
  </mrow>
</math>, we compute the TF-IDF-score as the product of the term frequency <math alttext="t f left-parenthesis t comma upper D right-parenthesis">
  <mrow>
    <mi>t</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>,</mo>
    <mi>D</mi>
    <mo>)</mo>
  </mrow>
</math> and the IDF of term <math alttext="t">
  <mi>t</mi>
</math>:</p>

<div data-type="equation">
<p><math alttext="t f i d f left-parenthesis t comma upper D right-parenthesis equals t f left-parenthesis t comma upper D right-parenthesis dot i d f left-parenthesis t right-parenthesis" display="block">
  <mrow>
    <mi>t</mi>
    <mi>f</mi>
    <mi>i</mi>
    <mi>d</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>,</mo>
    <mi>D</mi>
    <mo>)</mo>
    <mo>=</mo>
    <mi>t</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>,</mo>
    <mi>D</mi>
    <mo>)</mo>
    <mo>·</mo>
    <mi>i</mi>
    <mi>d</mi>
    <mi>f</mi>
    <mo>(</mo>
    <mi>t</mi>
    <mo>)</mo>
  </mrow>
</math></p>
</div>

<p>This score yields high values for terms appearing frequently in the selected document(s) <math alttext="upper D">
  <mi>D</mi>
</math> but rarely in the other documents of the corpus.</p>
</div></aside>

<p>Let’s define a function to compute the IDF for all terms in the corpus. It is <a contenteditable="false" data-type="indexterm" data-primary="count_words function" id="idm45634216887160"/>almost identical to <code>count_words</code>, except that each token is counted only once per document (<code>counter.update(set(tokens))</code>), and the IDF values are computed after counting. The parameter <code>min_df</code> serves as a filter for the long tail of infrequent words. The result of this function is again a <code>DataFrame</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">compute_idf</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'tokens'</code><code class="p">,</code> <code class="n">preprocess</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">2</code><code class="p">):</code>

    <code class="k">def</code> <code class="nf">update</code><code class="p">(</code><code class="n">doc</code><code class="p">):</code>
        <code class="n">tokens</code> <code class="o">=</code> <code class="n">doc</code> <code class="k">if</code> <code class="n">preprocess</code> <code class="ow">is</code> <code class="bp">None</code> <code class="k">else</code> <code class="n">preprocess</code><code class="p">(</code><code class="n">doc</code><code class="p">)</code>
        <code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="nb">set</code><code class="p">(</code><code class="n">tokens</code><code class="p">))</code>

    <code class="c1"># count tokens</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>
    <code class="n">df</code><code class="p">[</code><code class="n">column</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">update</code><code class="p">)</code>

    <code class="c1"># create DataFrame and compute idf</code>
    <code class="n">idf_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_dict</code><code class="p">(</code><code class="n">counter</code><code class="p">,</code> <code class="n">orient</code><code class="o">=</code><code class="s1">'index'</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'df'</code><code class="p">])</code>
    <code class="n">idf_df</code> <code class="o">=</code> <code class="n">idf_df</code><code class="o">.</code><code class="n">query</code><code class="p">(</code><code class="s1">'df &gt;= @min_df'</code><code class="p">)</code>
    <code class="n">idf_df</code><code class="p">[</code><code class="s1">'idf'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">)</code><code class="o">/</code><code class="n">idf_df</code><code class="p">[</code><code class="s1">'df'</code><code class="p">])</code><code class="o">+</code><code class="mf">0.1</code>
    <code class="n">idf_df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="s1">'token'</code>
    <code class="k">return</code> <code class="n">idf_df</code>
</pre>

<p>The IDF values need to be computed once for the entire corpus (do not use a subset here!) and can then be used in all kinds of analyses. We create a <code>DataFrame</code> containing the IDF values for each token (<code>idf_df</code>) with this function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">idf_df</code> <code class="o">=</code> <code class="n">compute_idf</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
</pre>

<p>As both the IDF and the frequency <code>DataFrame</code> have an index consisting of the tokens, we can simply multiply the columns of both <code>DataFrame</code>s to calculate the TF-IDF score for the terms:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">freq_df</code><code class="p">[</code><code class="s1">'tfidf'</code><code class="p">]</code> <code class="o">=</code> <code class="n">freq_df</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">]</code> <code class="o">*</code> <code class="n">idf_df</code><code class="p">[</code><code class="s1">'idf'</code><code class="p">]</code>
</pre>

<p>Let’s <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word clouds " data-secondary-sortas="word clouds " id="idm45634216655304"/><a contenteditable="false" data-type="indexterm" data-primary="word clouds " id="idm45634216653784"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="word clouds for" id="idm45634216755512"/>compare the word clouds based on word counts (term frequencies) alone and TF-IDF scores for the speeches of the first and last years in the corpus. We remove some more stop words that stand for the numbers of the respective debate sessions.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">freq_1970</code> <code class="o">=</code> <code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'year'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">1970</code><code class="p">])</code>
<code class="n">freq_2015</code> <code class="o">=</code> <code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'year'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2015</code><code class="p">])</code>

<code class="n">freq_1970</code><code class="p">[</code><code class="s1">'tfidf'</code><code class="p">]</code> <code class="o">=</code> <code class="n">freq_1970</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">]</code> <code class="o">*</code> <code class="n">idf_df</code><code class="p">[</code><code class="s1">'idf'</code><code class="p">]</code>
<code class="n">freq_2015</code><code class="p">[</code><code class="s1">'tfidf'</code><code class="p">]</code> <code class="o">=</code> <code class="n">freq_2015</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">]</code> <code class="o">*</code> <code class="n">idf_df</code><code class="p">[</code><code class="s1">'idf'</code><code class="p">]</code>

<code class="c1">#wordcloud(freq_df['freq'], title='All years', subplot=(1,3,1))</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_1970</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s1">'1970 - TF'</code><code class="p">,</code>
          <code class="n">stopwords</code><code class="o">=</code><code class="p">[</code><code class="s1">'twenty-fifth'</code><code class="p">,</code> <code class="s1">'twenty-five'</code><code class="p">])</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_2015</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s1">'2015 - TF'</code><code class="p">,</code>
          <code class="n">stopwords</code><code class="o">=</code><code class="p">[</code><code class="s1">'seventieth'</code><code class="p">])</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_1970</code><code class="p">[</code><code class="s1">'tfidf'</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s1">'1970 - TF-IDF'</code><code class="p">,</code>
          <code class="n">stopwords</code><code class="o">=</code><code class="p">[</code><code class="s1">'twenty-fifth'</code><code class="p">,</code> <code class="s1">'twenty-five'</code><code class="p">,</code> <code class="s1">'twenty'</code><code class="p">,</code> <code class="s1">'fifth'</code><code class="p">])</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_2015</code><code class="p">[</code><code class="s1">'tfidf'</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s1">'2015 - TF-IDF'</code><code class="p">,</code>
          <code class="n">stopwords</code><code class="o">=</code><code class="p">[</code><code class="s1">'seventieth'</code><code class="p">])</code>
</pre>

<p>The word clouds in <a data-type="xref" href="#fig-tf-idf">Figure 1-6</a> impressively demonstrate the power of TF-IDF weighting. While the most common words are almost identical in 1970 and 2015, the TF-IDF weighted visualizations emphasize the differences of political topics.</p>

<figure><div id="fig-tf-idf" class="figure"><img src="Images/btap_0106.jpg" width="1721" height="947"/>
<h6><span class="label">Figure 1-6. </span>Words weighted by plain counts (upper) and TF-IDF (lower) for speeches in two selected years.</h6>
</div></figure>

<p>The <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term22" id="idm45634216529208"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term23" id="idm45634216527416"/>experienced reader might wonder why we implemented functions to count words and compute IDF values ourselves instead of using the <a contenteditable="false" data-type="indexterm" data-primary="CountVectorizer" id="idm45634216525768"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="CountVectorizer of" id="idm45634216524664"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="TfidfVectorizer of" id="idm45634216523288"/><a contenteditable="false" data-type="indexterm" data-primary="TfidfVectorizer" id="idm45634216521912"/>classes <code>CountVectorizer</code> and <code>TfidfVectorizer</code> of scikit-learn. Actually, there two reasons. First, the vectorizers produce a vector with weighted term frequencies for each single document instead of arbitrary subsets of the dataset. Second, the results are matrices (good for machine learning) and not dataframes (good for slicing, aggregation, and visualization). We would have to write about the same number of code lines in the end to <span class="keep-together">produce</span> the results in <a data-type="xref" href="#fig-tf-idf">Figure 1-6</a> but miss the opportunity to introduce this important <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term24" id="idm45634216517624"/>concept from scratch. The scikit-learn vectorizers will be discussed in detail in <span class="keep-together"><a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a></span>.</p>
</div></section>
</div></section>

<section class="blueprint pagebreak-before less_space" data-type="sect1" data-pdf-bookmark="Blueprint: Finding a Keyword-in-Context"><div class="sect1" id="ch1-kwic">
<h1>Blueprint: Finding a Keyword-in-Context</h1>

<p>Word clouds and frequency diagrams are great tools to visually summarize textual data. However, they also often raise questions about why a certain term appears so prominently. For example, the <em>2015 TF-IDF</em> word cloud discussed earlier shows the terms <em>pv</em>, <em>sdgs</em>, or <em>sids</em>, and you probably do not know their meaning. To <a contenteditable="false" data-type="indexterm" data-primary="keyword-in-context (KWIC)" id="ch1_term25"/><a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="keyword-in-context (KWIC) for" id="ch1_term26"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="keyword-in-context (KWIC) for" id="ch1_term27"/>find that out, we need a way to inspect the actual occurrences of those words in the original, unprepared text. A simple yet clever way to do such an inspection is the keyword-in-context (KWIC) analysis. It produces a list of text fragments of equal length showing the left and right context of a keyword. Here is a sample of the KWIC list for <em>sdgs</em>, which gives us an explanation of that term:</p>

<pre data-type="programlisting">
5 random samples out of 73 contexts for 'sdgs':
 of our planet and its people. The   SDGs   are a tangible manifestation of th
nd, we are expected to achieve the   SDGs   and to demonstrate dramatic develo
ead by example in implementing the   SDGs   in Bangladesh. Attaching due impor
the Sustainable Development Goals (  SDGs  ). We applaud all the Chairs of the
new Sustainable Development Goals (  SDGs  ) aspire to that same vision. The A
</pre>

<p>Obviously, <em>sdgs</em> is the lowercased version of SDGs, which stands for “sustainable development goals.” With the same analysis we can learn that <em>sids</em> stands for “small island developing states.” That is important information to interpret the topics of 2015! <em>pv</em>, however, is a tokenization artifact. It is actually the remainder of citation references like <em>(A/70/PV.28)</em>, which stands for “Assembly 70, Process Verbal 28,” i.e., speech 28 of the 70th assembly.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Always look into the <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="recommendations for" id="idm45634216500488"/>details when you encounter tokens that you do not know or that do not make sense to you! Often they carry important information (like <em>sdgs</em>) that you as an analyst should be able to interpret. But you’ll also often find artifacts like <em>pv</em>. Those should be discarded if irrelevant or treated correctly.</p>
</div>

<p>KWIC analysis is implemented in <a contenteditable="false" data-type="indexterm" data-primary="NLTK library" id="idm45634216497096"/><a contenteditable="false" data-type="indexterm" data-primary="textacy library" id="idm45634216495992"/>NLTK and textacy. We will use textacy’s <a href="https://oreil.ly/-dSrA"><code>KWIC</code> function</a> because it is fast and works on the untokenized text. Thus, we can search for strings spanning multiple tokens like “climate change,” while NLTK cannot. Both NLTK and textacy’s KWIC functions work on a single document only. To extend <span class="keep-together">the analysis</span> to a number of documents in a <code>DataFrame</code>, we provide the following <span class="keep-together">function:</span></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">textacy.text_utils</code> <code class="kn">import</code> <code class="n">KWIC</code>

<code class="k">def</code> <code class="nf">kwic</code><code class="p">(</code><code class="n">doc_series</code><code class="p">,</code> <code class="n">keyword</code><code class="p">,</code> <code class="n">window</code><code class="o">=</code><code class="mi">35</code><code class="p">,</code> <code class="n">print_samples</code><code class="o">=</code><code class="mi">5</code><code class="p">):</code>

    <code class="k">def</code> <code class="nf">add_kwic</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
        <code class="n">kwic_list</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">KWIC</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">keyword</code><code class="p">,</code> <code class="n">ignore_case</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                              <code class="n">window_width</code><code class="o">=</code><code class="n">window</code><code class="p">,</code> <code class="n">print_only</code><code class="o">=</code><code class="bp">False</code><code class="p">))</code>

    <code class="n">kwic_list</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="n">doc_series</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">add_kwic</code><code class="p">)</code>

    <code class="k">if</code> <code class="n">print_samples</code> <code class="ow">is</code> <code class="bp">None</code> <code class="ow">or</code> <code class="n">print_samples</code><code class="o">==</code><code class="mi">0</code><code class="p">:</code>
        <code class="k">return</code> <code class="n">kwic_list</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="n">k</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">print_samples</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">kwic_list</code><code class="p">))</code>
        <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s2">"{k} random samples out of {len(kwic_list)} "</code> <code class="o">+</code> \
              <code class="n">f</code><code class="s2">"contexts for '{keyword}':"</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">sample</code> <code class="ow">in</code> <code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">kwic_list</code><code class="p">),</code> <code class="n">k</code><code class="p">):</code>
            <code class="k">print</code><code class="p">(</code><code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'[\n\t]'</code><code class="p">,</code> <code class="s1">' '</code><code class="p">,</code> <code class="n">sample</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code><code class="o">+</code><code class="s1">'  '</code><code class="o">+</code> \
                  <code class="n">sample</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">+</code><code class="s1">'  '</code><code class="o">+</code>\
                  <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'[\n\t]'</code><code class="p">,</code> <code class="s1">' '</code><code class="p">,</code> <code class="n">sample</code><code class="p">[</code><code class="mi">2</code><code class="p">]))</code>
</pre>

<p>The function iteratively collects the keyword contexts by <a contenteditable="false" data-type="indexterm" data-primary="map (Pandas)" id="idm45634216488920"/>applying the <code>add_kwic</code> function to each document with <code>map</code>. This trick, which we already used in the word count blueprints, is very efficient and enables KWIC analysis also for larger corpora. By default, the function returns a list of tuples of the form <code>(left context, keyword, right context)</code>. If <code>print_samples</code> is greater than 0, a random sample of the results is printed.<sup><a data-type="noteref" id="idm45634216293848-marker" href="ch01.xhtml#idm45634216293848">8</a></sup> <a contenteditable="false" data-type="indexterm" data-primary="sampling with results" id="idm45634216291992"/>Sampling is especially useful when you work with lots of documents because the first entries of the list would otherwise stem from a single or a very small number of documents.</p>

<p>The KWIC list for <em>sdgs</em> from earlier was <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term25" id="idm45634216289992"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term26" id="idm45634216288584"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term27" id="idm45634216287208"/>generated by this call:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">kwic</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'year'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2015</code><code class="p">][</code><code class="s1">'text'</code><code class="p">],</code> <code class="s1">'sdgs'</code><code class="p">,</code> <code class="n">print_samples</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
</pre>
</div></section>

<section class="blueprint" data-type="sect1" data-pdf-bookmark="Blueprint: Analyzing N-Grams"><div class="sect1" id="idm45634216250840">
<h1>Blueprint: Analyzing N-Grams</h1>

<p>Just knowing that climate is a frequent word does not tell us too much about the topic of discussion because, for example, <em>climate change</em> and <em>political climate</em> have completely different meanings. Even <em>change climate</em> is not the same as <em>climate change</em>. It can therefore be helpful to <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="n-grams for" id="ch1_term28"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="n-grams for" id="ch1_term29"/><a contenteditable="false" data-type="indexterm" data-primary="n-grams" id="ch1_term30"/>extend frequency analyses from single words to short sequences of two or three words.</p>

<p>Basically, we are looking for two types of <a contenteditable="false" data-type="indexterm" data-primary="word sequences" id="idm45634216205928"/>word sequences: compounds and collocations. <a contenteditable="false" data-type="indexterm" data-primary="compound phrases/names" id="idm45634216204312"/>A <em>compound</em> is a combination of two or more words with a specific meaning. In English, we find compounds in closed form, like <em>earthquake</em>; hyphenated form like <em>self-confident</em>; and open form like <em>climate change</em>. Thus, we may have to consider two tokens as a single semantic <a contenteditable="false" data-type="indexterm" data-primary="collocated words" id="idm45634216201192"/>unit. <em>Collocations</em>, in contrast, are words that are frequently used together. Often, they consist of an adjective or verb and a noun, like <em>red carpet</em> or <em>united nations</em>.</p>

<p>In text processing, we usually work with <a contenteditable="false" data-type="indexterm" data-primary="bigrams" id="ch1_term31"/>bigrams (sequences of length 2), sometimes even <a contenteditable="false" data-type="indexterm" data-primary="trigrams" id="idm45634216279208"/>trigrams (length 3). <em>n</em>-grams of size 1 are single words, also called <em>unigrams</em>. The reason to stick to <math alttext="n less-than-or-equal-to 3">
  <mrow>
    <mi>n</mi>
    <mo>≤</mo>
    <mn>3</mn>
  </mrow>
</math> is that the number of different n-grams increases exponentially with respect to <em>n</em>, while their frequencies decrease in the same way. By far the most trigrams appear only once in a corpus.</p>

<p>The following <a contenteditable="false" data-type="indexterm" data-primary="ngrams function" id="idm45634216273816"/>function produces elegantly the set of n-grams for a sequence of tokens:<sup><a data-type="noteref" id="idm45634216272552-marker" href="ch01.xhtml#idm45634216272552">9</a></sup></p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">ngrams</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">n</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">' '</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">sep</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">ngram</code><code class="p">)</code> <code class="k">for</code> <code class="n">ngram</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="p">[</code><code class="n">tokens</code><code class="p">[</code><code class="n">i</code><code class="p">:]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n</code><code class="p">)])]</code>

<code class="n">text</code> <code class="o">=</code> <code class="s2">"the visible manifestation of the global climate change"</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"|"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">ngrams</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="mi">2</code><code class="p">)))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
the visible|visible manifestation|manifestation of|of the|the global|
global climate|climate change
</pre>

<p>As you can see, most of the bigrams contain <a contenteditable="false" data-type="indexterm" data-primary="stop words" id="idm45634216093592"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="idm45634216092552"/>stop words like prepositions and determiners. Thus, it is advisable to build bigrams without stop words. But we need to be careful: if we remove the stop words first and then build the bigrams, we generate bigrams that don’t exist in the original text as a “manifestation global” in the example. Thus, we create the bigrams on all tokens but keep only those that do not contain any stop words with this modified <code>ngrams</code> function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">ngrams</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">n</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">' '</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="nb">set</code><code class="p">()):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">sep</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">ngram</code><code class="p">)</code> <code class="k">for</code> <code class="n">ngram</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="p">[</code><code class="n">tokens</code><code class="p">[</code><code class="n">i</code><code class="p">:]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n</code><code class="p">)])</code>
            <code class="k">if</code> <code class="nb">len</code><code class="p">([</code><code class="n">t</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">ngram</code> <code class="k">if</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">])</code><code class="o">==</code><code class="mi">0</code><code class="p">]</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Bigrams:"</code><code class="p">,</code> <code class="s2">"|"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">ngrams</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Trigrams:"</code><code class="p">,</code> <code class="s2">"|"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">ngrams</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Bigrams: visible manifestation|global climate|climate change
Trigrams: global climate change
</pre>

<p>Using this <code>ngrams</code> function, we can add a column containing all bigrams to our <code>DataFrame</code> and apply the word count blueprint to determine the top five bigrams:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'bigrams'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">prepare</code><code class="p">,</code> <code class="n">pipeline</code><code class="o">=</code><code class="p">[</code><code class="nb">str</code><code class="o">.</code><code class="n">lower</code><code class="p">,</code> <code class="n">tokenize</code><code class="p">])</code> \
                          <code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">ngrams</code><code class="p">,</code> <code class="n">n</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">stopwords</code><code class="o">=</code><code class="n">stopwords</code><code class="p">)</code>

<code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="s1">'bigrams'</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th>token</th>
			<th>freq</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>united nations</td>
			<td>103236</td>
		</tr>
		<tr>
			<td>international community</td>
			<td>27786</td>
		</tr>
		<tr>
			<td>general assembly</td>
			<td>27096</td>
		</tr>
		<tr>
			<td>security council</td>
			<td>20961</td>
		</tr>
		<tr>
			<td>human rights</td>
			<td>19856</td>
		</tr>
	</tbody>
</table>

<p>You may have noticed that we ignored sentence boundaries during <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with initial text analyses" data-secondary-sortas="initial text analyses" id="idm45634215864232"/>tokenization. Thus, we will generate nonsense bigrams with the last word of one sentence and the first word of the next. Those bigrams will not be very frequent, so they don’t really matter for data exploration. If we wanted to prevent this, we would need to identify sentence boundaries, which is much more complicated than word tokenization and not worth the effort here.</p>

<p>Now let’s <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="text data analysis with" id="idm45634215861688"/>extend our TF-IDF-based unigram analysis from the previous section and include bigrams. We add the bigram IDF values, compute the TF-IDF-weighted bigram frequencies for all speeches from 2015, and generate a word cloud from the resulting <code>DataFrame</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># concatenate existing IDF DataFrame with bigram IDFs</code>
<code class="n">idf_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">idf_df</code><code class="p">,</code> <code class="n">compute_idf</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="s1">'bigrams'</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">10</code><code class="p">)])</code>

<code class="n">freq_df</code> <code class="o">=</code> <code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'year'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2015</code><code class="p">],</code> <code class="s1">'bigrams'</code><code class="p">)</code>
<code class="n">freq_df</code><code class="p">[</code><code class="s1">'tfidf'</code><code class="p">]</code> <code class="o">=</code> <code class="n">freq_df</code><code class="p">[</code><code class="s1">'freq'</code><code class="p">]</code> <code class="o">*</code> <code class="n">idf_df</code><code class="p">[</code><code class="s1">'idf'</code><code class="p">]</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_df</code><code class="p">[</code><code class="s1">'tfidf'</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s1">'all bigrams'</code><code class="p">,</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
</pre>

<p>As we can see in the word cloud on the left of <a data-type="xref" href="#fig-bigrams">Figure 1-7</a>, <em>climate change</em> was a frequent bigram in 2015. But to understand the different contexts of <em>climate</em>, it may be interesting to take a look at the bigrams containing <em>climate</em> only. We can use a text filter on <em>climate</em> to achieve this and <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word clouds " data-secondary-sortas="word clouds " id="idm45634215763784"/><a contenteditable="false" data-type="indexterm" data-primary="word clouds " id="idm45634215762104"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="word clouds for" id="idm45634215761000"/>plot the result again as a word cloud (<a data-type="xref" href="#fig-bigrams">Figure 1-7</a>, right):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">where</code> <code class="o">=</code> <code class="n">freq_df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">contains</code><code class="p">(</code><code class="s1">'climate'</code><code class="p">)</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">freq_df</code><code class="p">[</code><code class="n">where</code><code class="p">][</code><code class="s1">'freq'</code><code class="p">],</code> <code class="n">title</code><code class="o">=</code><code class="s1">'"climate" bigrams'</code><code class="p">,</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
</pre>

<figure><div id="fig-bigrams" class="figure"><img src="Images/btap_0107.jpg" width="1738" height="465"/>
<h6><span class="label">Figure 1-7. </span>Word clouds for all bigrams and bigrams containing the word <em>climate</em>.</h6>
</div></figure>

<p>The <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term31" id="idm45634215734632"/>approach presented here creates and weights all n-grams that do not contain stop words. For a first analysis, the results look quite good. We just don’t care about the long tail of infrequent bigrams. More sophisticated but also computationally expensive algorithms to identify <a contenteditable="false" data-type="indexterm" data-primary="collocated words" id="idm45634215732792"/><a contenteditable="false" data-type="indexterm" data-primary="NLTK library" id="idm45634215731688"/>collocations are available, for example, in <a href="https://oreil.ly/uW-2A">NLTK’s collocation finder</a>. We <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term28" id="idm45634215729704"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term29" id="idm45634215728328"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term30" id="idm45634215726952"/>will show alternatives to identify meaningful phrases in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#ch-preparation">4</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch10.xhtml#ch-embeddings">10</a>.</p>
</div></section>

<section class="blueprint" data-type="sect1" data-pdf-bookmark="Blueprint: Comparing Frequencies Across Time Intervals and Categories"><div class="sect1" id="idm45634216214056">
<h1>Blueprint: Comparing Frequencies Across Time Intervals and Categories</h1>

<p>You <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="comparison across time intervals and categories with" id="ch1_term32"/>surely know <a href="http://trends.google.com">Google Trends</a>, where you can track the development of a number of search terms over time. This kind of trend analysis computes frequencies by day and visualizes them with a line chart. We want to track the development of certain keywords over the course of the years in our <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="UN General Debates" id="ch1_term35"/><a contenteditable="false" data-type="indexterm" data-primary="UN General Debates dataset" id="ch1_term36"/>UN Debates dataset to get an idea about the growing or shrinking importance of topics such as climate change, terrorism, or migration.</p>

<section data-type="sect2" data-pdf-bookmark="Creating Frequency Timelines"><div class="sect2" id="idm45634215715432">
<h2>Creating Frequency Timelines</h2>

<p>Our <a contenteditable="false" data-type="indexterm" data-primary="frequency timelines" id="ch1_term33"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="frequency timelines for" id="ch1_term34"/>approach is to calculate the frequencies of given keywords per document and then aggregate those frequencies using Pandas’s <code>groupby</code> function. The following function is for the first task. It extracts the counts of given keywords from a list of tokens:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">count_keywords</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">keywords</code><code class="p">):</code>
    <code class="n">tokens</code> <code class="o">=</code> <code class="p">[</code><code class="n">t</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">tokens</code> <code class="k">if</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">keywords</code><code class="p">]</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">(</code><code class="n">tokens</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">counter</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">k</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code> <code class="ow">in</code> <code class="n">keywords</code><code class="p">]</code>
</pre>

<p class="pagebreak-before">Let’s demonstrate the functionality with a small example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">keywords</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'nuclear'</code><code class="p">,</code> <code class="s1">'terrorism'</code><code class="p">,</code> <code class="s1">'climate'</code><code class="p">,</code> <code class="s1">'freedom'</code><code class="p">]</code>
<code class="n">tokens</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'nuclear'</code><code class="p">,</code> <code class="s1">'climate'</code><code class="p">,</code> <code class="s1">'climate'</code><code class="p">,</code> <code class="s1">'freedom'</code><code class="p">,</code> <code class="s1">'climate'</code><code class="p">,</code> <code class="s1">'freedom'</code><code class="p">]</code>

<code class="k">print</code><code class="p">(</code><code class="n">count_keywords</code><code class="p">(</code><code class="n">tokens</code><code class="p">,</code> <code class="n">keywords</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[1, 0, 3, 2]
</pre>

<p>As you can see, the function returns a list or vector of word counts. In fact, it’s a very simple count-vectorizer for keywords. If we apply this function to each document in our <code>DataFrame</code>, we <a contenteditable="false" data-type="indexterm" data-primary="count_keywords_by function" id="idm45634215525384"/>get a matrix of counts. The blueprint function <code>count_keywords_by</code>, shown next, does exactly this as a first step. The matrix is then again converted into a <code>DataFrame</code> that is finally aggregated and sorted by the supplied grouping column.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">count_keywords_by</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">by</code><code class="p">,</code> <code class="n">keywords</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'tokens'</code><code class="p">):</code>

    <code class="n">freq_matrix</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">column</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">count_keywords</code><code class="p">,</code> <code class="n">keywords</code><code class="o">=</code><code class="n">keywords</code><code class="p">)</code>
    <code class="n">freq_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_records</code><code class="p">(</code><code class="n">freq_matrix</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">keywords</code><code class="p">)</code>
    <code class="n">freq_df</code><code class="p">[</code><code class="n">by</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">by</code><code class="p">]</code> <code class="c1"># copy the grouping column(s)</code>

    <code class="k">return</code> <code class="n">freq_df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="n">by</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="p">)</code>
</pre>

<p>This function is very fast because it has to take care of the keywords only. Counting the four keywords from earlier in the UN corpus takes just two seconds on a laptop. Let’s take a look at the result:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">freq_df</code> <code class="o">=</code> <code class="n">count_keywords_by</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">by</code><code class="o">=</code><code class="s1">'year'</code><code class="p">,</code> <code class="n">keywords</code><code class="o">=</code><code class="n">keywords</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<table class="dataframe tex2jax_ignore">
	<thead>
		<tr>
			<th>nuclear</th>
			<th>terrorism</th>
			<th>climate</th>
			<th>freedom</th>
			<th>year</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>1970</th>
			<td>192</td>
			<td>7</td>
			<td>18</td>
			<td>128</td>
		</tr>
		<tr>
			<th>1971</th>
			<td>275</td>
			<td>9</td>
			<td>35</td>
			<td>205</td>
		</tr>
		<tr>
			<th>...</th>
			<td>...</td>
			<td>...</td>
			<td>...</td>
			<td>...</td>
		</tr>
		<tr>
			<th>2014</th>
			<td>144</td>
			<td>404</td>
			<td>654</td>
			<td>129</td>
		</tr>
		<tr>
			<th>2015</th>
			<td>246</td>
			<td>378</td>
			<td>662</td>
			<td>148</td>
		</tr>
	</tbody>
</table>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Even though we use only the attribute <code>year</code> as a grouping criterion in our examples, the blueprint function allows you to compare word frequencies across any discrete attribute, e.g., country, category, author—you name it. In fact, you could even specify a list of grouping attributes to compute, for example, counts per country and year.</p>
</div>

<p>The resulting <code>DataFrame</code> is already perfectly prepared for plotting as we have one data series per keyword. Using Pandas’s <code>plot</code> function, we get a nice line chart similar to Google Trends (<a data-type="xref" href="#fig-timeline_2">Figure 1-8</a>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">freq_df</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'line'</code><code class="p">)</code>
</pre>

<figure><div id="fig-timeline_2" class="figure"><img src="Images/btap_0108.jpg" width="1339" height="550"/>
<h6><span class="label">Figure 1-8. </span>Frequencies of selected words per year.</h6>
</div></figure>

<p>Note the peak of <em>nuclear</em> in the 1980s indicating the arms race and the high peak of terrorism in 2001. It is somehow remarkable that the topic <em>climate</em> already got some attention in the 1970s and 1980s. Has it really? Well, if you check with a KWIC analysis (<a data-type="xref" href="#ch1-kwic">“Blueprint: Finding a Keyword-in-Context”</a>), you’d find out that the word <em>climate</em> in those decades was almost exclusively used in a <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term33" id="idm45634215323624"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term34" id="idm45634215322248"/>figurative sense.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Creating Frequency Heatmaps"><div class="sect2" id="idm45634215714808">
<h2>Creating Frequency Heatmaps</h2>

<p>Say we want to <a contenteditable="false" data-type="indexterm" data-primary="heatmaps" id="idm45634215319144"/><a contenteditable="false" data-type="indexterm" data-primary="frequency heatmaps" id="ch1_term37"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="frequency heatmaps for" id="ch1_term38"/>analyze the historic developments of global crises like the cold war, terrorism, and climate change. We could pick a selection of significant words and visualize their timelines by line charts as in the previous example. But line charts become confusing if you have more than four or five lines. An alternative <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with heatmaps" data-secondary-sortas="heatmaps" id="ch1_term39"/>visualization without that limitation is a heatmap, as provided by the <a contenteditable="false" data-type="indexterm" data-primary="Seaborn library" id="idm45634215312520"/>Seaborn library. So, let’s add a few more keywords to our filter and display the result as a heatmap (<a data-type="xref" href="#fig-heatmap">Figure 1-9</a>).</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">keywords</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'terrorism'</code><code class="p">,</code> <code class="s1">'terrorist'</code><code class="p">,</code> <code class="s1">'nuclear'</code><code class="p">,</code> <code class="s1">'war'</code><code class="p">,</code> <code class="s1">'oil'</code><code class="p">,</code>
            <code class="s1">'syria'</code><code class="p">,</code> <code class="s1">'syrian'</code><code class="p">,</code> <code class="s1">'refugees'</code><code class="p">,</code> <code class="s1">'migration'</code><code class="p">,</code> <code class="s1">'peacekeeping'</code><code class="p">,</code>
            <code class="s1">'humanitarian'</code><code class="p">,</code> <code class="s1">'climate'</code><code class="p">,</code> <code class="s1">'change'</code><code class="p">,</code> <code class="s1">'sustainable'</code><code class="p">,</code> <code class="s1">'sdgs'</code><code class="p">]</code>

<code class="n">freq_df</code> <code class="o">=</code> <code class="n">count_keywords_by</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">by</code><code class="o">=</code><code class="s1">'year'</code><code class="p">,</code> <code class="n">keywords</code><code class="o">=</code><code class="n">keywords</code><code class="p">)</code>

<code class="c1"># compute relative frequencies based on total number of tokens per year</code>
<code class="n">freq_df</code> <code class="o">=</code> <code class="n">freq_df</code><code class="o">.</code><code class="n">div</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'year'</code><code class="p">)[</code><code class="s1">'num_tokens'</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(),</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="c1"># apply square root as sublinear filter for better contrast</code>
<code class="n">freq_df</code> <code class="o">=</code> <code class="n">freq_df</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">)</code>

<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">freq_df</code><code class="o">.</code><code class="n">T</code><code class="p">,</code>
            <code class="n">xticklabels</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">yticklabels</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">cbar</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"Reds"</code><code class="p">)</code>
</pre>

<figure><div id="fig-heatmap" class="figure"><img src="Images/btap_0109.jpg" width="1786" height="602"/>
<h6><span class="label">Figure 1-9. </span>Word frequencies over time as heatmap.</h6>
</div></figure>

<p>There are a few things to consider for this <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term41" id="idm45634215183576"/>kind of analysis:</p>

<dl>
	<dt>Prefer relative frequencies for any kind of comparison.</dt>
	<dd>Absolute <a contenteditable="false" data-type="indexterm" data-primary="absolute versus relative term frequencies" id="idm45634215180664"/>term frequencies are problematic if the total number of tokens per year or category is not stable. For example, absolute frequencies naturally go up if more countries are speaking year after year in our example.</dd>
	<dt>Be careful with the interpretation of frequency diagrams based on keyword lists.</dt>
	<dd>Although the chart looks like a distribution of topics, it is not! There may be other words representing the same topic but not included in the list. Keywords may also have different meanings (e.g., “climate of the discussion”). Advanced techniques such as topic modeling (<a data-type="xref" href="ch08.xhtml#ch-topicmodels">Chapter 8</a>) and word embeddings (<a data-type="xref" href="ch10.xhtml#ch-embeddings">Chapter 10</a>) can help here.</dd>
	<dt>Use sublinear scaling.</dt>
	<dd>As the <a contenteditable="false" data-type="indexterm" data-primary="sublinear scaling" id="idm45634215175016"/>frequency values differ greatly, it may be hard to see any change for less-frequent tokens. Therefore, you should scale the frequencies sublinearly (we applied the square root <code>np.sqrt</code>). The visual effect is similar to <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term13" id="idm45634215173144"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term14" id="idm45634215171768"/>lowering <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term32" id="idm45634215170264"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term35" id="idm45634215168856"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term36" id="idm45634215167480"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term37" id="idm45634215166104"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term38" id="idm45634215164728"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch1_term39" id="idm45634215163352"/>contrast.</dd>
</dl>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Closing Remarks"><div class="sect1" id="idm45634215161848">
<h1>Closing Remarks</h1>

<p>We demonstrated <a contenteditable="false" data-type="indexterm" data-primary="text data analysis, introduction" data-secondary="about" id="idm45634215160248"/>how to get started analyzing textual data. The process for text preparation and tokenization was kept simple to get quick results. In <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>, we will introduce more sophisticated methods and discuss the advantages and disadvantages of different approaches.</p>

<p>Data exploration should not only provide initial insights but actually help to develop confidence in your data. One thing you should keep in mind is that you should always identify the root cause for any strange tokens popping up. The KWIC analysis is a good tool to search for such tokens.</p>

<p>For a first analysis of the content, we introduced several blueprints for word frequency analysis. The weighting of terms is based either on term frequency alone or on the combination of term frequency and inverse document frequency (TF-IDF). These concepts will be picked up later in <a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a> because TF-IDF weighting is a standard method to vectorize documents for machine learning.</p>

<p>There are many aspects of textual analysis that we did not cover in this chapter:</p>

<ul>
	<li>Author-related information can help to identify influential writers, if that is one of your project goals. Authors can be distinguished by activity, social scores, writing style, etc.</li>
	<li>Sometimes it is interesting to compare authors or different corpora on the same topic by their <a contenteditable="false" data-type="indexterm" data-primary="readability scores" id="idm45634215152904"/><a contenteditable="false" data-type="indexterm" data-primary="textacy library" id="idm45634215151800"/>readability. The <a href="https://oreil.ly/FRZJb"><code>textacy</code> library</a> has a function called <code>textstats</code> that computes different readability scores and other statistics in a single pass over the text.</li>
	<li>An interesting tool to identify and <a contenteditable="false" data-type="indexterm" data-primary="Scattertext library" id="idm45634215148584"/>visualize distinguishing terms between categories (e.g., political parties) is Jason Kessler’s <a href="https://oreil.ly/R6Aw8"><code>Scattertext</code></a> library.</li>
	<li>Besides plain Python, you can also use interactive visual tools for data analysis. Microsoft’s PowerBI has a nice <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word clouds " data-secondary-sortas="word clouds " id="idm45634215145848"/><a contenteditable="false" data-type="indexterm" data-primary="word clouds " id="idm45634215144200"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="word clouds for" id="idm45634215143096"/>word cloud add-on and lots of other options to produce interactive charts. We mention it because it is free to use in the desktop version and supports Python and R for data preparation and visualization.</li>
	<li>For larger projects, we recommend setting up a search engine like <a href="https://oreil.ly/LqPvG">Apache SOLR</a>, <a href="https://elastic.co">Elasticsearch</a>, or <a href="https://oreil.ly/NCz1g">Tantivy</a>. Those platforms create specialized indexes (also using TF-IDF weighting) for fast full-text search. Python APIs are available for all of them.</li>
</ul>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45634218510984"><sup><a href="ch01.xhtml#idm45634218510984-marker">1</a></sup> See the <a href="https://oreil.ly/XjAKa">Pandas documentation</a> for a complete list.</p><p data-type="footnote" id="idm45634218281240"><sup><a href="ch01.xhtml#idm45634218281240-marker">2</a></sup> You can address spaCy’s list similarly with <code>spacy.lang.en.STOP_WORDS</code>.</p><p data-type="footnote" id="idm45634217921624"><sup><a href="ch01.xhtml#idm45634217921624-marker">3</a></sup> Check out the <a href="https://oreil.ly/gO_VN">documentation</a> for further details.</p><p data-type="footnote" id="idm45634217882104"><sup><a href="ch01.xhtml#idm45634217882104-marker">4</a></sup> The NLTK class <a href="https://oreil.ly/xQXUu"><code>FreqDist</code></a> is derived from <code>Counter</code> and adds some convenience <span class="keep-together">functions</span>.</p><p data-type="footnote" id="idm45634217138552"><sup><a href="ch01.xhtml#idm45634217138552-marker">5</a></sup> Note that the <code>wordcloud</code> module ignores the stop word list if <code>generate_from_frequencies</code> is called. Therefore, we apply an extra filter.</p><p data-type="footnote" id="idm45634216924552"><sup><a href="ch01.xhtml#idm45634216924552-marker">6</a></sup> For example, scikit-learn’s <code>TfIdfVectorizer</code> adds <code>+1</code>.</p><p data-type="footnote" id="idm45634216916280"><sup><a href="ch01.xhtml#idm45634216916280-marker">7</a></sup> Another option is to add +1 <!--span class="math-tex" data-type="tex">\(+1\)</span--> in the denominator to prevent a division by zero for unseen terms with <em>df</em>(<em>t</em>) = 0<!--span class="math-tex" data-type="tex">\(df(t)=0\)</span-->. This technique is called <em>smoothing</em>.</p><p data-type="footnote" id="idm45634216293848"><sup><a href="ch01.xhtml#idm45634216293848-marker">8</a></sup> The parameter <code>print_only</code> in textacy’s <code>KWIC</code> function works similarly but does not sample.</p><p data-type="footnote" id="idm45634216272552"><sup><a href="ch01.xhtml#idm45634216272552-marker">9</a></sup> See Scott Triglia’s <a href="https://oreil.ly/7WwTe">blog post</a> for an explanation.</p></div></div></section></div>



  </body></html>