<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 41. In Depth: Naive Bayes Classification" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0505-naive-bayes">
<h1><span class="label">Chapter 41. </span>In Depth: Naive Bayes Classification</h1>
<p><a data-primary="machine learning" data-secondary="naive Bayes classification" data-type="indexterm" id="ix_ch41-asciidoc0"/><a data-primary="naive Bayes classification" data-type="indexterm" id="ix_ch41-asciidoc1"/>The previous four chapters have given a general overview of the concepts
of machine learning. In the rest of <a data-type="xref" href="part05.xhtml#section-0500-machine-learning">Part V</a>, we will
be taking a closer look first at four algorithms for supervised
learning, and then at four algorithms for unsupervised learning. We
start here with our first supervised method, naive Bayes classification.</p>
<p>Naive Bayes models are a group of extremely fast and simple
classification algorithms that are often suitable for very
high-dimensional datasets. Because they are so fast and have so few
tunable parameters, they end up being useful as a quick-and-dirty
baseline for a classification problem. This chapter will provide an
intuitive explanation of how naive Bayes classifiers work, followed by a
few examples of them in action on some datasets.</p>
<section data-pdf-bookmark="Bayesian Classification" data-type="sect1"><div class="sect1" id="ch_0505-naive-bayes_bayesian-classification">
<h1>Bayesian Classification</h1>
<p><a data-primary="Bayesian classification" data-type="indexterm" id="idm45858738271984"/><a data-primary="naive Bayes classification" data-secondary="Bayesian classification and" data-type="indexterm" id="idm45858738271376"/>Naive Bayes classifiers are built on Bayesian classification methods.
<a data-primary="Bayes's theorem" data-type="indexterm" id="idm45858738270400"/>These rely on Bayes’s theorem, which is an equation
describing the relationship of conditional probabilities of statistical
quantities. In Bayesian classification, we’re interested in
finding the probability of a label <math alttext="upper L">
<mi>L</mi>
</math> given some observed
features, which we can write as <math alttext="upper P left-parenthesis upper L vertical-bar normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis">
<mrow>
<mi>P</mi>
<mo>(</mo>
<mi>L</mi>
<mspace width="3.33333pt"/>
<mo>|</mo>
<mspace width="3.33333pt"/>
<mi> features </mi>
<mo>)</mo>
</mrow>
</math>.
Bayes’s theorem tells us how to express this in terms of
quantities we can compute more directly:</p>
<div data-type="equation">
<math alttext="upper P left-parenthesis upper L vertical-bar normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis equals StartFraction upper P left-parenthesis normal f normal e normal a normal t normal u normal r normal e normal s vertical-bar upper L right-parenthesis upper P left-parenthesis upper L right-parenthesis Over upper P left-parenthesis normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis EndFraction" display="block">
<mrow>
<mi>P</mi>
<mrow>
<mo>(</mo>
<mi>L</mi>
<mspace width="3.33333pt"/>
<mo>|</mo>
<mspace width="3.33333pt"/>
<mi> features </mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mfrac><mrow><mi>P</mi><mo>(</mo><mi> features </mi><mspace width="3.33333pt"/><mo>|</mo><mspace width="3.33333pt"/><mi>L</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>L</mi><mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><mi> features </mi><mo>)</mo></mrow></mfrac>
</mrow>
</math>
</div>
<p class="pagebreak-before less_space">If we are trying to decide between two labels—let’s call
them <math alttext="upper L 1">
<msub><mi>L</mi> <mn>1</mn> </msub>
</math> and <math alttext="upper L 2">
<msub><mi>L</mi> <mn>2</mn> </msub>
</math>—then one way to make this
decision is to compute the ratio of the posterior probabilities for each
label:</p>
<div data-type="equation">
<math alttext="StartFraction upper P left-parenthesis upper L 1 vertical-bar normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis Over upper P left-parenthesis upper L 2 vertical-bar normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis EndFraction equals StartFraction upper P left-parenthesis normal f normal e normal a normal t normal u normal r normal e normal s vertical-bar upper L 1 right-parenthesis Over upper P left-parenthesis normal f normal e normal a normal t normal u normal r normal e normal s vertical-bar upper L 2 right-parenthesis EndFraction StartFraction upper P left-parenthesis upper L 1 right-parenthesis Over upper P left-parenthesis upper L 2 right-parenthesis EndFraction" display="block">
<mrow>
<mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi> <mn>1</mn> </msub><mspace width="3.33333pt"/><mo>|</mo><mspace width="3.33333pt"/><mi> features </mi><mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi> <mn>2</mn> </msub><mspace width="3.33333pt"/><mo>|</mo><mspace width="3.33333pt"/><mi> features </mi><mo>)</mo></mrow></mfrac>
<mo>=</mo>
<mfrac><mrow><mi>P</mi><mo>(</mo><mi> features </mi><mspace width="3.33333pt"/><mo>|</mo><mspace width="3.33333pt"/><msub><mi>L</mi> <mn>1</mn> </msub><mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><mi> features </mi><mspace width="3.33333pt"/><mo>|</mo><mspace width="3.33333pt"/><msub><mi>L</mi> <mn>2</mn> </msub><mo>)</mo></mrow></mfrac>
<mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi> <mn>1</mn> </msub><mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi> <mn>2</mn> </msub><mo>)</mo></mrow></mfrac>
</mrow>
</math>
</div>
<p>All we need now is some model by which we can compute
<math alttext="upper P left-parenthesis normal f normal e normal a normal t normal u normal r normal e normal s vertical-bar upper L Subscript i Baseline right-parenthesis">
<mrow>
<mi>P</mi>
<mo>(</mo>
<mi> features </mi>
<mspace width="3.33333pt"/>
<mo>|</mo>
<mspace width="3.33333pt"/>
<msub><mi>L</mi> <mi>i</mi> </msub>
<mo>)</mo>
</mrow>
</math> for each label. <a data-primary="generative models" data-type="indexterm" id="idm45858738218240"/>Such a model is
called a <em>generative model</em> because it specifies the hypothetical random
process that generates the data. Specifying this generative model for
each label is the main piece of the training of such a Bayesian
classifier. The general version of such a training step is a very
difficult task, but we can make it simpler through the use of some
simplifying assumptions about the form of this model.</p>
<p>This is where the “naive” in “naive Bayes” comes in: if we make very
naive assumptions about the generative model for each label, we can find
a rough approximation of the generative model for each class, and then
proceed with the Bayesian classification. Different types of naive Bayes
classifiers rest on different naive assumptions about the data, and we
will examine a few of these in the following sections.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code></pre>
</div></section>
<section data-pdf-bookmark="Gaussian Naive Bayes" data-type="sect1"><div class="sect1" id="ch_0505-naive-bayes_gaussian-naive-bayes">
<h1>Gaussian Naive Bayes</h1>
<p><a data-primary="Gaussian naive Bayes classification" data-type="indexterm" id="ix_ch41-asciidoc2"/><a data-primary="naive Bayes classification" data-secondary="Gaussian" data-type="indexterm" id="ix_ch41-asciidoc3"/>Perhaps the easiest naive Bayes classifier to understand is Gaussian
naive Bayes. With this classifier, the assumption is that <em>data from
each label is drawn from a simple Gaussian distribution</em>. Imagine that
we have the following data, shown in <a data-type="xref" href="#fig_0505-naive-bayes_files_in_output_5_0">Figure 41-1</a>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>
        <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">cluster_std</code><code class="o">=</code><code class="mf">1.5</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'RdBu'</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0505-naive-bayes_files_in_output_5_0">
<img alt="output 5 0" height="392" src="assets/output_5_0.png" width="600"/>
<h6><span class="label">Figure 41-1. </span>Data for Gaussian naive Bayes classification<sup><a data-type="noteref" href="ch41.xhtml#idm45858738121616" id="idm45858738121616-marker">1</a></sup></h6>
</div></figure>
<p>The simplest Gaussian model is to assume that the data is described by a
Gaussian distribution with no covariance between dimensions. This model
can be fit by computing the mean and standard deviation of the points
within each label, which is all we need to define such a distribution.
The result of this naive Gaussian assumption is shown in <a data-type="xref" href="#fig_images_in_0505-gaussian-nb">Figure 41-2</a>.</p>
<figure class="width-70"><div class="figure" id="fig_images_in_0505-gaussian-nb">
<img alt="05.05 gaussian NB" height="399" src="assets/05.05-gaussian-NB.png" width="600"/>
<h6><span class="label">Figure 41-2. </span>Visualization of the Gaussian naive Bayes model<sup><a data-type="noteref" href="ch41.xhtml#idm45858738117008" id="idm45858738117008-marker">2</a></sup></h6>
</div></figure>
<p>The ellipses here represent the Gaussian generative model for each
label, with larger probability toward the center of the ellipses. With
this generative model in place for each class, we have a simple recipe
to compute the likelihood <math alttext="upper P left-parenthesis normal f normal e normal a normal t normal u normal r normal e normal s vertical-bar upper L 1 right-parenthesis">
<mrow>
<mi>P</mi>
<mo>(</mo>
<mi> features </mi>
<mspace width="3.33333pt"/>
<mo>|</mo>
<mspace width="3.33333pt"/>
<msub><mi>L</mi> <mn>1</mn> </msub>
<mo>)</mo>
</mrow>
</math> for any
data point, and thus we can quickly compute the posterior ratio and
determine which label is the most probable for a given point.</p>
<p>This procedure is implemented in Scikit-Learn’s
<code>sklearn.naive_bayes.GaussianNB</code> estimator:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">GaussianNB</code><code class="p">()</code>
        <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">);</code></pre>
<p>Let’s generate some new data and predict the label:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">Xnew</code> <code class="o">=</code> <code class="p">[</code><code class="o">-</code><code class="mi">6</code><code class="p">,</code> <code class="o">-</code><code class="mi">14</code><code class="p">]</code> <code class="o">+</code> <code class="p">[</code><code class="mi">14</code><code class="p">,</code> <code class="mi">18</code><code class="p">]</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">2000</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
        <code class="n">ynew</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">Xnew</code><code class="p">)</code></pre>
<p>Now we can plot this new data to get an idea of where the decision
boundary is (see <a data-type="xref" href="#fig_0505-naive-bayes_files_in_output_13_0">Figure 41-3</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'RdBu'</code><code class="p">)</code>
        <code class="n">lim</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">()</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">Xnew</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">Xnew</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">ynew</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'RdBu'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="n">lim</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0505-naive-bayes_files_in_output_13_0">
<img alt="output 13 0" height="392" src="assets/output_13_0.png" width="600"/>
<h6><span class="label">Figure 41-3. </span>Visualization of the Gaussian naive Bayes classification</h6>
</div></figure>
<p>We see a slightly curved boundary in the classifications—in general, the
boundary produced by a Gaussian naive Bayes model will be quadratic.</p>
<p>A nice aspect of this Bayesian formalism is that it naturally allows for
probabilistic classification, which we can compute using the
<code>predict_proba</code> method:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">yprob</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">Xnew</code><code class="p">)</code>
        <code class="n">yprob</code><code class="p">[</code><code class="o">-</code><code class="mi">8</code><code class="p">:]</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">array</code><code class="p">([[</code><code class="mf">0.89</code><code class="p">,</code> <code class="mf">0.11</code><code class="p">],</code>
               <code class="p">[</code><code class="mf">1.</code>  <code class="p">,</code> <code class="mf">0.</code>  <code class="p">],</code>
               <code class="p">[</code><code class="mf">1.</code>  <code class="p">,</code> <code class="mf">0.</code>  <code class="p">],</code>
               <code class="p">[</code><code class="mf">1.</code>  <code class="p">,</code> <code class="mf">0.</code>  <code class="p">],</code>
               <code class="p">[</code><code class="mf">1.</code>  <code class="p">,</code> <code class="mf">0.</code>  <code class="p">],</code>
               <code class="p">[</code><code class="mf">1.</code>  <code class="p">,</code> <code class="mf">0.</code>  <code class="p">],</code>
               <code class="p">[</code><code class="mf">0.</code>  <code class="p">,</code> <code class="mf">1.</code>  <code class="p">],</code>
               <code class="p">[</code><code class="mf">0.15</code><code class="p">,</code> <code class="mf">0.85</code><code class="p">]])</code></pre>
<p>The columns give the posterior probabilities of the first and second
labels, respectively. If you are looking for estimates of uncertainty in
your classification, Bayesian approaches like this can be a good place
to start.</p>
<p>Of course, the final classification will only be as good as the model
assumptions that lead to it, which is why Gaussian naive Bayes often
does not produce very good results. Still, in many cases—especially as
the number of features becomes large—this assumption is not detrimental
enough to prevent Gaussian naive Bayes from being a reliable method.<a data-startref="ix_ch41-asciidoc3" data-type="indexterm" id="idm45858737798192"/><a data-startref="ix_ch41-asciidoc2" data-type="indexterm" id="idm45858737763312"/></p>
</div></section>
<section data-pdf-bookmark="Multinomial Naive Bayes" data-type="sect1"><div class="sect1" id="ch_0505-naive-bayes_multinomial-naive-bayes">
<h1>Multinomial Naive Bayes</h1>
<p><a data-primary="multinomial naive Bayes classification" data-type="indexterm" id="ix_ch41-asciidoc4"/><a data-primary="naive Bayes classification" data-secondary="multinomial" data-type="indexterm" id="ix_ch41-asciidoc5"/>The Gaussian assumption just described is by no means the only simple
assumption that could be used to specify the generative distribution for
each label. Another useful example is multinomial naive Bayes, where the
features are assumed to be generated from a simple multinomial
distribution. The multinomial distribution describes the probability of
observing counts among a number of categories, and thus multinomial
naive Bayes is most appropriate for features that represent counts or
count rates.</p>
<p>The idea is precisely the same as before, except that instead of
modeling the data distribution with the best-fit Gaussian, we model it
with a best-fit multinomial 
<span class="keep-together">distribution</span>.</p>
<section data-pdf-bookmark="Example: Classifying Text" data-type="sect2"><div class="sect2" id="ch_0505-naive-bayes_example-classifying-text">
<h2>Example: Classifying Text</h2>
<p><a data-primary="naive Bayes classification" data-secondary="text classification example" data-type="indexterm" id="ix_ch41-asciidoc6"/>One place where multinomial naive Bayes is often used is in text
classification, where the features are related to word counts or
frequencies within the documents to be classified. We discussed the
extraction of such features from text in
<a data-type="xref" href="ch40.xhtml#section-0504-feature-engineering">Chapter 40</a>; here we will
use the sparse word count features from the 20 Newsgroups corpus made
available through Scikit-Learn to show how we might classify these short
documents into categories.</p>
<p>Let’s download the data and take a look at the target names:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_20newsgroups</code>

        <code class="n">data</code> <code class="o">=</code> <code class="n">fetch_20newsgroups</code><code class="p">()</code>
        <code class="n">data</code><code class="o">.</code><code class="n">target_names</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="p">[</code><code class="s1">'alt.atheism'</code><code class="p">,</code>
         <code class="s1">'comp.graphics'</code><code class="p">,</code>
         <code class="s1">'comp.os.ms-windows.misc'</code><code class="p">,</code>
         <code class="s1">'comp.sys.ibm.pc.hardware'</code><code class="p">,</code>
         <code class="s1">'comp.sys.mac.hardware'</code><code class="p">,</code>
         <code class="s1">'comp.windows.x'</code><code class="p">,</code>
         <code class="s1">'misc.forsale'</code><code class="p">,</code>
         <code class="s1">'rec.autos'</code><code class="p">,</code>
         <code class="s1">'rec.motorcycles'</code><code class="p">,</code>
         <code class="s1">'rec.sport.baseball'</code><code class="p">,</code>
         <code class="s1">'rec.sport.hockey'</code><code class="p">,</code>
         <code class="s1">'sci.crypt'</code><code class="p">,</code>
         <code class="s1">'sci.electronics'</code><code class="p">,</code>
         <code class="s1">'sci.med'</code><code class="p">,</code>
         <code class="s1">'sci.space'</code><code class="p">,</code>
         <code class="s1">'soc.religion.christian'</code><code class="p">,</code>
         <code class="s1">'talk.politics.guns'</code><code class="p">,</code>
         <code class="s1">'talk.politics.mideast'</code><code class="p">,</code>
         <code class="s1">'talk.politics.misc'</code><code class="p">,</code>
         <code class="s1">'talk.religion.misc'</code><code class="p">]</code></pre>
<p>For simplicity here, we will select just a few of these categories and
download the training and testing sets:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">categories</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'talk.religion.misc'</code><code class="p">,</code> <code class="s1">'soc.religion.christian'</code><code class="p">,</code>
                      <code class="s1">'sci.space'</code><code class="p">,</code> <code class="s1">'comp.graphics'</code><code class="p">]</code>
        <code class="n">train</code> <code class="o">=</code> <code class="n">fetch_20newsgroups</code><code class="p">(</code><code class="n">subset</code><code class="o">=</code><code class="s1">'train'</code><code class="p">,</code> <code class="n">categories</code><code class="o">=</code><code class="n">categories</code><code class="p">)</code>
        <code class="n">test</code> <code class="o">=</code> <code class="n">fetch_20newsgroups</code><code class="p">(</code><code class="n">subset</code><code class="o">=</code><code class="s1">'test'</code><code class="p">,</code> <code class="n">categories</code><code class="o">=</code><code class="n">categories</code><code class="p">)</code></pre>
<p>Here is a representative entry from the data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="nb">print</code><code class="p">(</code><code class="n">train</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="mi">5</code><code class="p">][</code><code class="mi">48</code><code class="p">:])</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">Subject</code><code class="p">:</code> <code class="n">Federal</code> <code class="n">Hearing</code>
        <code class="n">Originator</code><code class="p">:</code> <code class="n">dmcgee</code><code class="nd">@uluhe</code>
        <code class="n">Organization</code><code class="p">:</code> <code class="n">School</code> <code class="n">of</code> <code class="n">Ocean</code> <code class="ow">and</code> <code class="n">Earth</code> <code class="n">Science</code> <code class="ow">and</code> <code class="n">Technology</code>
        <code class="n">Distribution</code><code class="p">:</code> <code class="n">usa</code>
        <code class="n">Lines</code><code class="p">:</code> <code class="mi">10</code>


        <code class="n">Fact</code> <code class="ow">or</code> <code class="n">rumor</code><code class="o">....</code><code class="err">?</code>  <code class="n">Madalyn</code> <code class="n">Murray</code> <code class="n">O</code><code class="s1">'Hare an atheist who eliminated the</code>
        <code class="n">use</code> <code class="n">of</code> <code class="n">the</code> <code class="n">bible</code> <code class="n">reading</code> <code class="ow">and</code> <code class="n">prayer</code> <code class="ow">in</code> <code class="n">public</code> <code class="n">schools</code> <code class="mi">15</code> <code class="n">years</code> <code class="n">ago</code> <code class="ow">is</code> <code class="n">now</code>
        <code class="n">going</code> <code class="n">to</code> <code class="n">appear</code> <code class="n">before</code> <code class="n">the</code> <code class="n">FCC</code> <code class="k">with</code> <code class="n">a</code> <code class="n">petition</code> <code class="n">to</code> <code class="n">stop</code> <code class="n">the</code> <code class="n">reading</code> <code class="n">of</code> <code class="n">the</code>
        <code class="n">Gospel</code> <code class="n">on</code> <code class="n">the</code> <code class="n">airways</code> <code class="n">of</code> <code class="n">America</code><code class="o">.</code>  <code class="n">And</code> <code class="n">she</code> <code class="ow">is</code> <code class="n">also</code> <code class="n">campaigning</code> <code class="n">to</code> <code class="n">remove</code>
        <code class="n">Christmas</code> <code class="n">programs</code><code class="p">,</code> <code class="n">songs</code><code class="p">,</code> <code class="n">etc</code> <code class="kn">from</code> <code class="nn">the</code> <code class="n">public</code> <code class="n">schools</code><code class="o">.</code>  <code class="n">If</code> <code class="n">it</code> <code class="ow">is</code> <code class="n">true</code>
        <code class="n">then</code> <code class="n">mail</code> <code class="n">to</code> <code class="n">Federal</code> <code class="n">Communications</code> <code class="n">Commission</code> <code class="mi">1919</code> <code class="n">H</code> <code class="n">Street</code> <code class="n">Washington</code> <code class="n">DC</code>
        <code class="mi">20054</code> <code class="n">expressing</code> <code class="n">your</code> <code class="n">opposition</code> <code class="n">to</code> <code class="n">her</code> <code class="n">request</code><code class="o">.</code>  <code class="n">Reference</code> <code class="n">Petition</code> <code class="n">number</code>

        <code class="mf">2493.</code></pre>
<p>In order to use this data for machine learning, we need to be able to
convert the content of each string into a vector of numbers. For this we
will use the TF-IDF vectorizer (introduced in
<a data-type="xref" href="ch40.xhtml#section-0504-feature-engineering">Chapter 40</a>), and create a
pipeline that attaches it to a multinomial naive Bayes classifier:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>
         <code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">MultinomialNB</code>
         <code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>

         <code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">TfidfVectorizer</code><code class="p">(),</code> <code class="n">MultinomialNB</code><code class="p">())</code></pre>
<p>With this pipeline, we can apply the model to the training data and
predict labels for the test data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">train</code><code class="o">.</code><code class="n">target</code><code class="p">)</code>
         <code class="n">labels</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">test</code><code class="o">.</code><code class="n">data</code><code class="p">)</code></pre>
<p>Now that we have predicted the labels for the test data, we can evaluate
them to learn about the performance of the estimator. For example,
let’s take a look at the confusion matrix between the true
and predicted labels for the test data (see <a data-type="xref" href="#fig_0505-naive-bayes_files_in_output_29_0">Figure 41-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>
         <code class="n">mat</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">test</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>
         <code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">mat</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">fmt</code><code class="o">=</code><code class="s1">'d'</code><code class="p">,</code> <code class="n">cbar</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code>
                     <code class="n">xticklabels</code><code class="o">=</code><code class="n">train</code><code class="o">.</code><code class="n">target_names</code><code class="p">,</code> <code class="n">yticklabels</code><code class="o">=</code><code class="n">train</code><code class="o">.</code><code class="n">target_names</code><code class="p">,</code>
                     <code class="n">cmap</code><code class="o">=</code><code class="s1">'Blues'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'true label'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'predicted label'</code><code class="p">);</code></pre>
<p>Evidently, even this very simple classifier can successfully separate
space discussions from computer discussions, but it gets confused
between discussions about religion and discussions about Christianity.
This is perhaps to be expected!</p>
<p>The cool thing here is that we now have the tools to determine the
category for <em>any</em> string, using the <code>predict</code> method of this pipeline.
Here’s a utility function that will return the prediction
for a single string:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">predict_category</code><code class="p">(</code><code class="n">s</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="n">train</code><code class="p">,</code> <code class="n">model</code><code class="o">=</code><code class="n">model</code><code class="p">):</code>
             <code class="n">pred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">s</code><code class="p">])</code>
             <code class="k">return</code> <code class="n">train</code><code class="o">.</code><code class="n">target_names</code><code class="p">[</code><code class="n">pred</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code></pre>
<p>Let’s try it out:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">predict_category</code><code class="p">(</code><code class="s1">'sending a payload to the ISS'</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="s1">'sci.space'</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">predict_category</code><code class="p">(</code><code class="s1">'discussing the existence of God'</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="s1">'soc.religion.christian'</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">predict_category</code><code class="p">(</code><code class="s1">'determining the screen resolution'</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="s1">'comp.graphics'</code></pre>
<figure><div class="figure" id="fig_0505-naive-bayes_files_in_output_29_0">
<img alt="output 29 0" height="565" src="assets/output_29_0.png" width="600"/>
<h6><span class="label">Figure 41-4. </span>Confusion matrix for the multinomial naive Bayes text classifier</h6>
</div></figure>
<p>Remember that this is nothing more sophisticated than a simple
probability model for the (weighted) frequency of each word in the
string; nevertheless, the result is striking. Even a very naive
algorithm, when used carefully and trained on a large set of
high-dimensional data, can be surprisingly effective<a data-startref="ix_ch41-asciidoc6" data-type="indexterm" id="idm45858736968352"/>.<a data-startref="ix_ch41-asciidoc5" data-type="indexterm" id="idm45858736967520"/><a data-startref="ix_ch41-asciidoc4" data-type="indexterm" id="idm45858736966816"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="When to Use Naive Bayes" data-type="sect1"><div class="sect1" id="ch_0505-naive-bayes_when-to-use-naive-bayes">
<h1>When to Use Naive Bayes</h1>
<p><a data-primary="naive Bayes classification" data-secondary="advantages/disadvantages" data-type="indexterm" id="idm45858736964848"/>Because naive Bayes classifiers make such stringent assumptions about
data, they will generally not perform as well as more complicated
models. That said, they have several advantages:</p>
<ul>
<li>
<p>They are fast for both training and prediction.</p>
</li>
<li>
<p>They provide straightforward probabilistic prediction.</p>
</li>
<li>
<p>They are often easily interpretable.</p>
</li>
<li>
<p>They have few (if any) tunable parameters.</p>
</li>
</ul>
<p>These advantages mean a naive Bayes classifier is often a good choice as
an initial baseline classification. If it performs suitably, then
congratulations: you have a very fast, very interpretable classifier for
your problem. If it does not perform well, then you can begin exploring
more sophisticated models, with some baseline knowledge of how well they
should perform.</p>
<p>Naive Bayes classifiers tend to perform especially well in the following
situations:</p>
<ul>
<li>
<p>When the naive assumptions actually match the data (very rare in
practice)</p>
</li>
<li>
<p>For very well-separated categories, when model complexity is less
important</p>
</li>
<li>
<p>For very high-dimensional data, when model complexity is less
important</p>
</li>
</ul>
<p>The last two points seem distinct, but they actually are related: as the
dimensionality of a dataset grows, it is much less likely for any two
points to be found close together (after all, they must be close in
<em>every single dimension</em> to be close overall). This means that clusters
in high dimensions tend to be more separated, on average, than clusters
in low dimensions, assuming the new dimensions actually add information.
For this reason, simplistic classifiers like the ones discussed here
tend to work as well or better than more complicated classifiers as the
dimensionality grows: once you have enough data, even a simple model can
be very powerful.<a data-startref="ix_ch41-asciidoc1" data-type="indexterm" id="idm45858736955168"/><a data-startref="ix_ch41-asciidoc0" data-type="indexterm" id="idm45858736954464"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858738121616"><sup><a href="ch41.xhtml#idm45858738121616-marker">1</a></sup> A full-color version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p><p data-type="footnote" id="idm45858738117008"><sup><a href="ch41.xhtml#idm45858738117008-marker">2</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/o0ENq">online appendix</a>.</p></div></div></section></div></body></html>