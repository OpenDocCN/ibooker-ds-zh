<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Experimental Design: The Basics"><div class="chapter" id="experimental_design_the_basics">
<h1><span class="label">Chapter 8. </span>Experimental Design: The Basics</h1>
<p>Let’s start our exploration of experimentation with a very simple experiment:<a contenteditable="false" data-type="indexterm" data-primary="AirCnC (Air Coach and Couch)" data-secondary="1-click booking" data-seealso="random assignment" data-secondary-sortas="one-click booking" id="idm45968154333304"/><a contenteditable="false" data-type="indexterm" data-primary="1-click booking" data-primary-sortas="one-click booking" data-seealso="random assignment" id="idm45968154331608"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="about 1-click booking site" data-secondary-sortas="about one-click booking" id="idm45968154330152"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="about business problem" id="idm45968154328696"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="about business problem" id="idm45968154327240"/> influenced by a leading online store, AirCnC’s management has decided that a “1-click booking” button is just what’s needed to boost AirCnC’s booking rate. As I discussed earlier, we’ll assign customers to our experimental groups one by one as they connect to the website. This is the simplest possible type of experiment, and many companies offer interfaces that allow you to create and start running A/B tests like this in a matter of minutes.</p>
<p>This straightforward experiment will be the opportunity to go through the process without getting bogged down in technical considerations:</p>
<ol>
<li><p>The first step is to plan the experiment. This is where the causal-behavioral perspective comes in, to help ensure that you have clearly defined criteria for success, and that you understand what it is you’re testing and how you expect it to impact your target metric.</p></li>
<li><p>Then, after reviewing the data and packages that we’ll use in the rest of the chapter, I’ll show you how to do the random assignment and determine the sample size for your experiment.</p></li>
<li><p>Finally, we’ll analyze the results of the experiment, which in such a simple case will be very quick.</p></li>
</ol>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The vocabulary of experimental design owes much to its statistical and scientific roots.<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="about terms used" id="idm45968154321352"/> I’ll talk of “control” and “treatment” groups as well as “interventions,” which may sound ominous or like overkill when we’re really discussing the position of a button on a website or the amount of a discount. When talking about experiments in general, there isn’t much I can do to use a simpler vocabulary; but when you’re talking to your business partners about a specific experiment, I would encourage you to stick with concrete terms relating to that experiment (e.g. “the old and the new creatives,” “the group with the lower discount and the group with the higher discount,” etc.).</p>
<p>Anecdote: once when I suggested an “intervention,” a business partner thought I meant that they weren’t doing their job well and I needed to intervene. Not the best start for a fruitful and trusting relationship. Meet people where they are and make an effort to speak their language instead of expecting them to know yours.</p>
</div>
<section data-type="sect1" data-pdf-bookmark="Planning the Experiment: Theory of Change"><div class="sect1" id="planning_the_experiment_theory_of_chang">
<h1>Planning the Experiment: Theory of Change</h1>
<p>Planning is a crucial step of experimental design. Experiments can fail for a variety of reasons,<a contenteditable="false" data-type="indexterm" data-primary="theory of change (ToC)" data-secondary="1-click booking site" data-secondary-sortas="one-click booking site" id="ch08plex3"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="planning the experiment" id="ch08plex2"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="planning the experiment" id="ch08plex"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="planning the experiment" id="ch08plex4"/> many of which you can’t control, such as the implementation going awry; but poor planning is both a frequent cause of failure and one you <em>can</em> control. Anyone who runs experiments for a living has horror stories of experiments that may or may not have been technically impeccable but were utterly pointless because people didn’t have clarity about what was tested.</p>
<p>At the end of the day, no process can save you if you’re just going through the motions without exercising your business acumen and common sense, but hopefully the formula I’m going to outline will help you make sure you have covered all your bases. <a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="planning the experiment" data-tertiary="theory of change" id="idm45968154308584"/><a contenteditable="false" data-type="indexterm" data-primary="theory of change (ToC)" id="idm45968154307128"/><a contenteditable="false" data-type="indexterm" data-primary="business goals" data-secondary="theory of change" id="idm45968154306152"/><a contenteditable="false" data-type="indexterm" data-primary="target metrics" data-secondary="theory of change" id="idm45968154304936"/><a contenteditable="false" data-type="indexterm" data-primary="metrics" data-see="target metrics" id="idm45968154303720"/><a contenteditable="false" data-type="indexterm" data-primary="interventions" data-secondary="theory of change" id="idm45968154302504"/>We’ll borrow a concept from nonprofit and governmental planning, namely the <em>theory of change</em> (ToC). In one sentence, your ToC should connect what you’re doing to your ultimate business goal and target metric through a behavioral change:</p>
<blockquote>
<p>Implementing [INTERVENTION] will help us achieve [BUSINESS GOAL], as measured by [TARGET METRIC], through [BEHAVIORAL LOGIC].</p>
</blockquote>
<p>I’ll elaborate on each of the four components in turn, but to give you a sense of where we’ll land, here’s what our final theory of change will look like:</p>
<blockquote>
<p>Implementing [a 1-click booking button] will help us achieve [higher revenue], as measured by [booking probability], through [a reduction in the duration of the booking process].</p>
</blockquote>
<p>This can be represented in CD format, as in <a data-type="xref" href="#theory_of_change_for_our_experiment">Figure 8-1</a>.</p>
<figure><div id="theory_of_change_for_our_experiment" class="figure">
<img src="Images/BEDA_0801.png" alt="Theory of change for our experiment" width="1441" height="139"/>
<h6><span class="label">Figure 8-1. </span>Theory of change for our experiment</h6>
</div></figure>
<p>Let’s first review our business goal and target metric, then our intervention, and finally our behavioral logic.</p>
<section data-type="sect2" data-pdf-bookmark="Business Goal and Target Metric"><div class="sect2" id="business_goal_and_target_metri">
<h2>Business Goal and Target Metric</h2>
<p>You might be surprised that I start with the business goal and<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="planning the experiment" data-tertiary="business goal" id="idm45968154292104"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="planning the experiment" data-tertiary="business goal and target metric" id="idm45968154290376"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="planning the experiment" data-tertiary="target metric" id="ch08-tm"/><a contenteditable="false" data-type="indexterm" data-primary="target metrics" data-secondary="1-click booking site" data-secondary-sortas="one-click booking site" id="ch08-tm2"/><a contenteditable="false" data-type="indexterm" data-primary="business goals" data-secondary="1-click booking site" data-secondary-sortas="one-click booking site" id="idm45968154285368"/><a contenteditable="false" data-type="indexterm" data-primary="goals" data-see="business goals" id="idm45968154283720"/> target metric instead of the definition of the intervention. After all, shouldn’t we know what we’re testing first? Unfortunately, a common cause of failure is the decision to test something (often the latest management fad or something that your boss’s boss read about) without a clear sense of what we’re trying to achieve.</p>
<section data-type="sect3" data-pdf-bookmark="Business goal"><div class="sect3" id="business_goal">
<h3>Business goal</h3>
<p>The first step is to determine the business goal for the experiment. Companies are usually trying to increase their profit, but just putting “higher profit” as your business goal would not be very helpful. Instead, I would recommend going one level deeper and using a variable such as revenue, costs, customer retention, etc., that is more concrete but of obvious benefit to the company. This may seem a trivial step, but it can actually surface disagreements about the goal of an experiment (e.g., is it to reduce costs or increase revenue?). Here, the business goal for the 1-click button experiment is higher revenue (<a data-type="xref" href="#our_business_goal_is_revenue">Figure 8-2</a>).</p>
<figure><div id="our_business_goal_is_revenue" class="figure">
<img src="Images/BEDA_0802.png" alt="Our business goal is revenue" width="1441" height="139"/>
<h6><span class="label">Figure 8-2. </span>Our business goal is revenue</h6>
</div></figure>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Target metric"><div class="sect3" id="target_metric">
<h3>Target metric</h3>
<p>The second step is to decide how you’ll measure success at the end of the experiment, i.e., your target metric. There is a trade-off at play here: on the one hand, you want to use a metric as close to profit as possible, such as dollars of additional revenue or decreased cost; on the other hand, you want to pick a metric as close to your intervention as possible, to reduce extraneous noise.</p>
<p>A compromise you’ll often have to make at this point is to <a contenteditable="false" data-type="indexterm" data-primary="target metrics" data-secondary="leading indicators for" id="idm45968154273704"/><a contenteditable="false" data-type="indexterm" data-primary="causes" data-secondary="leading indicators as" id="idm45968154272248"/><a contenteditable="false" data-type="indexterm" data-primary="LTV (lifetime value) as target metric" id="idm45968154270872"/>use “leading indicators”—basically causes of the variables you’re ultimately interested in. For example, you may ultimately care about your customers’ LTV (lifetime value, the total amount they’ll spend with your company) but have to settle for a three-month booking amount. Similarly, sign-up can be used as a leading indicator for usage, usage for amount purchased, etc. This will allow you to report results much earlier than if you used long-term business metrics, while still having a clear connection to your business goal.</p>
<p>However, if your target metric is an <a contenteditable="false" data-type="indexterm" data-primary="target metrics" data-secondary="operational versus financial" id="idm45968154268696"/>operational metric instead of a financial metric, things can get hairy. If the button shortens how long it takes someone to book a trip but otherwise doesn’t increase bookings in any way, is that a success or not? What about satisfaction with the booking experience and net promoter score? These may not translate directly into dollar figures, but at the same time it is not unreasonable to assume that improving them has a positive impact for your business. This is sometimes done informally, by picking operational metrics as targets for experimentation and assuming with some hand-waving that these will end up benefiting the company’s bottom line. Of course, armed with this book, we can do better. We can validate and measure the causal connection between a short-term operational metric and long-term business outcomes through an observational study or a dedicated experiment, as we’ll see later.</p>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Pitfalls of poor target metrics"><div class="sect3" id="pitfalls_of_poor_target_metrics">
<h3>Pitfalls of poor target metrics</h3>
<p>The goal here again is to make good business decisions. <a contenteditable="false" data-type="indexterm" data-primary="target metrics" data-secondary="poor target metrics" id="idm45968154264456"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="planning the experiment" data-tertiary="poor target metric pitfalls" id="idm45968154263000"/>I don’t want to be a fanatic of dollar figures, because it would be unduly restrictive and would exclude a large range of business improvements. At the same time, you want to make sure that you have a measurable target metric that you’ll be able to track. There are several potential pitfalls that you should avoid here.</p>
<p>The first one is picking something you can’t measure reliably. What would it mean to say, “The 1-click button makes the booking experience easier”? How would you measure it? Asking the product manager or the product owner to decide after the fact whether there has been an improvement is not a measurement. “Our customers rate the website as easier to navigate, as measured by a two-question survey at the end of a visit” may be an imperfect proxy but at least it can be measured. This is why it makes sense to express the business goal and the target metric separately: the former expresses your true intent, even if it’s not measurable, while the latter shows clearly what you intend to measure. This avoids misunderstandings and moving goal posts.</p>
<p>The second pitfall is to write down a laundry list of metrics, such as “success would be an improvement in booking rate, booking amount, customer satisfaction, or net promoter score,” or even worse, to wait until after seeing the results to determine the metrics for success—e.g., you were originally thinking that the experiment would improve customer experience, but when the results come, customer experience is flat and average website session duration has improved. <a contenteditable="false" data-type="indexterm" data-primary="false positive probability" data-secondary="target metric selection" id="idm45968154258536"/>The problem with that approach is that it increases the risk of false positives (calling the result a success when it was a random fluke).<sup><a data-type="noteref" id="ch01fn13-marker" href="ch08.xhtml#ch01fn13">1</a></sup> It’s okay however to have up to two or three target metrics that are clearly defined before the experiment as long as you take that into account when analyzing the results (more on that later). <a contenteditable="false" data-type="indexterm" data-primary="target metrics" data-secondary="weighted average of metrics" id="idm45968154255272"/><a contenteditable="false" data-type="indexterm" data-primary="weighted average of metrics" id="idm45968154253912"/><a contenteditable="false" data-type="indexterm" data-primary="target metrics" data-secondary="Overall Evaluation Criterion" id="idm45968154252792"/><a contenteditable="false" data-type="indexterm" data-primary="Overall Evaluation Criterion (OEC)" id="idm45968154251400"/>Some people advocate using a composite of multiple metrics (e.g., a weighted average), which is called an <em>Overall Evaluation Criterion</em> (OEC), but I personally feel that it often obscures things more than it helps. I would rather encourage you to clearly articulate your theory of change and how the various metrics are related to each other—for example, do you expect the 1-click button to improve booking rate <em>and</em> customer experience, or booking rate <em>through</em> customer experience?</p>
<p>To conclude, in the case of the 1-click button experiment, we could directly use booking revenue as our target metric, but we don’t expect that our intervention will modify the average amount per booking, so it makes more sense to use the probability that a customer will complete a booking (<a data-type="xref" href="#adding_the_target_metriccomma_the_proba">Figure 8-3</a>).<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-tm" id="idm45968154246824"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-tm2" id="idm45968154245448"/></p>
<figure><div id="adding_the_target_metriccomma_the_proba" class="figure">
<img src="Images/BEDA_0803.png" alt="Adding the target metric, the probability of completing a booking" width="1441" height="139"/>
<h6><span class="label">Figure 8-3. </span>Adding the target metric, the probability of completing a booking</h6>
</div></figure>
</div></section>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Intervention"><div class="sect2" id="intervention">
<h2>Intervention</h2>
<p>Once we have our business goal and target metric, we can work on defining our intervention.<a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="planning the experiment" data-tertiary="intervention" id="idm45968154239912"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="planning the experiment" data-tertiary="intervention" id="idm45968154238264"/><a contenteditable="false" data-type="indexterm" data-primary="interventions" data-secondary="1-click booking site" data-secondary-sortas="one-click booking site" id="idm45968154236616"/> Here the idea for the “1-click button” intervention comes from the company’s management (<a data-type="xref" href="#adding_the_interventioncomma_the_one_cl">Figure 8-4</a>), but it could also have come from UX or behavioral research: identifying issues and opportunities for improvement in a company’s processes, products, and services is indeed one of the main tasks of researchers in business but it’s outside the scope of this book.</p>
<figure><div id="adding_the_interventioncomma_the_one_cl" class="figure">
<img src="Images/BEDA_0804.png" alt="Adding the intervention, the 1-click button" width="1441" height="139"/>
<h6><span class="label">Figure 8-4. </span>Adding the intervention, the 1-click button</h6>
</div></figure>
<p>On the face of it, what could be simpler than a “1-click booking” button? Most of us have seen it implemented on the website of an online retailer or another and the idea seems perfectly straightforward. But there can be a large gap between a business idea, so used and familiar that everyone immediately feels they know what it is, and a specific implementation. If you think about the nitty-gritty details of how it would be implemented, there are actually a lot of questions to answer, each with multiple possible answers:</p>
<ul>
<li><p>At which point in the process does the button become available?</p></li>
<li><p>Where is the button located?</p></li>
<li><p>What does the button look like? Is it in the same colors as the other buttons on the page or does it stand out in a bright and rich color?</p></li>
<li><p>What is written on the button?</p></li>
<li><p>What information do we need about a customer to make 1-click booking available and how do we make sure we have it?</p></li>
<li><p>What happens after the customer clicks the button? What page are they taken to, and what, if any, actions do they still have to take?</p></li>
<li><p>Etc.</p></li>
</ul>
<p>Let’s say for example that the website’s color theme is pastel green and blue, to hint at nature and travel, and the new button is a shiny red. If that button increases booking, it might be because of the attractiveness of 1-click booking, but it could also be because customers otherwise struggle to see the normal booking button and give up. In that case, the cause of the increase in booking is really “a more visible booking button” rather than “a 1-click booking button,” but you can’t distinguish between the two because the 1-click button was also more visible. Unfortunately, you can’t ever just test one idea, as you’re always also testing the many aspects of how it is implemented.</p>
<p>The lesson here is that A/B testing is a powerful but <a contenteditable="false" data-type="indexterm" data-primary="A/B tests" data-secondary="powerful but narrow" data-seealso="experimental design" id="idm45968154223912"/>narrow tool. You need to be careful not to make—or let others make—grand statements about what a specific experiment says. It is definitely easier said than done, because business partners often want answers that are broad, clear-cut, and without fine print. With that said, even simply stating in your presentation that you’re testing a specific implementation and not a general idea can be useful, for example, “this experiment will test the impact of a 1-click button under such and such conditions, and its results should not be interpreted as applying to booking buttons more broadly.”</p>
<p>More generally, I would recommend you test the smallest<a contenteditable="false" data-type="indexterm" data-primary="interventions" data-secondary="testing smallest possible" id="idm45968154221032"/> possible intervention you can. In this case, you could try changing the color or position of the booking button before implementing the bigger change that is a 1-click button. You might get pushback from your business partners, who often want to run an “omnibus” test with multiple changes at once; in that case, make clear that you’re really just testing that none of the changes break the experience, as opposed to actually measuring impacts. A better alternative would be to test different implementations of the same concept in the experiment. If four slightly different 1-click button treatments have the same impact, you can more confidently draw conclusions regarding the general impact of 1-click booking; on the other hand, if they have very different impacts, then this suggests that implementation matters a lot and that you need to be very careful with your conclusions regarding how a specific implementation will generalize.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Behavioral Logic"><div class="sect2" id="behavioral_logic-id00001">
<h2>Behavioral Logic</h2>
<p>Once we know our business goal and target metrics and we have defined our intervention, <a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="planning the experiment" data-tertiary="behavioral logic" id="ch08-tcbl5"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="planning the experiment" data-tertiary="behavioral logic" id="ch08-tcbl4"/><a contenteditable="false" data-type="indexterm" data-primary="theory of change (ToC)" data-secondary="behavioral logic" id="ch08-tcbl2"/><a contenteditable="false" data-type="indexterm" data-primary="behavioral logic" data-secondary="random assignment" id="ch08-tcbl3"/>the last step is to connect the two through the behavioral logic of your theory of change: why and how would our intervention impact our target metric?</p>
<p>This is another surprisingly common source of failure for experiments: a problem has been identified and someone decides to implement the latest management fad that they have been thinking about recently, even though it’s unclear why it would help with this specific problem. Or someone decides that a more attractive and simpler user interface (UI) will increase purchase amounts. To have confidence in our experiment, you need to be able to articulate a reasonable behavioral story.<sup><a data-type="noteref" id="ch01fn14-marker" href="ch08.xhtml#ch01fn14">2</a></sup> In the case of 1-click booking, you might hypothesize that customers identify an attractive booking but give up before completing their booking because the booking process is cumbersome; the 1-click button would impact the probability of booking by shortening and simplifying the booking process. This is typically where your ToC comes together in a CD, in this case the one I showed you at the beginning of the chapter (<a data-type="xref" href="#the_complete_cd_for_our_theory_of_chang">Figure 8-5</a>).</p>
<figure><div id="the_complete_cd_for_our_theory_of_chang" class="figure">
<img src="Images/BEDA_0805.png" alt="The complete CD for our theory of change" width="1441" height="139"/>
<h6><span class="label">Figure 8-5. </span>The complete CD for our theory of change</h6>
</div></figure>
<p>Overall, articulating your behavioral logic has two benefits. First, it is often testable in itself. Do a large number of customers actually drop off between starting a booking and completing it? If so, the hypothesized logic makes sense from a behavioral data perspective. But if, for example, most customers leave without having started the booking process with a specific product, e.g., because they couldn’t find something that appealed to them or they were overwhelmed with the number of options, then AirCnC is trying to solve the wrong problem; it’s unlikely that offering 1-click booking will improve their numbers.</p>
<p>Ask yourself: what would confirm or refute our logic? What would the data look like if it were true or false? If you don’t have the necessary data at hand, it might be worth running a preliminary test, such as bringing in 10 people for user experience testing, e.g., observing them trying to use your website while they’re thinking out loud. This may not fully confirm or refute your logic, but it will probably give you some indication, at a fraction of the development cost for the solution considered. As the often-cited quote by Albert Einstein goes, “If I had an hour to solve a problem, I’d spend 55 minutes thinking about the problem and 5 minutes thinking about <span class="keep-together">solutions.”</span></p>
<p>The second benefit of articulating the behavioral logic of your intervention<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="planning the experiment" data-tertiary="best case scenario worth" id="idm45968154200344"/> is that it will generally give you a sense of the potential upside. What would the numbers look like, in the best-case scenario, if the problem at hand was solved? Assuming that all customers who drop off during the booking process would complete it with 1-click booking, how much would the booking rate increase? This is the best-case scenario from the perspective of the experiment because we’re assuming that it would fully solve the problem, which is unlikely in reality. If the increase in booking rate under this scenario would not pay for the implementation of the 1-click booking, then don’t even bother testing it.</p>
<p>Once you’ve validated that your best-case scenario would be profitable, you can start thinking about your most likely scenario. How much would we expect 1-click booking to improve the booking rate? There is undoubtedly a lot of subjectivity and uncertainty involved, but having articulated the behavioral mechanism at play, you can generally make reasonable guesses. Do you really expect that 75% of people are dropping off the booking process because it’s taking too long? In addition, it can be a worthwhile exercise by making explicit people’s assumptions and gut feelings. If the product manager and the UX researcher vastly disagree on the percentage of customers who drop off because the process is taking too long, you need to close that gap first. What does one of them know that the other doesn’t? Use your business sense and understanding of the processes. If most customers are dropping off at the exact same step in the process, e.g., payment, then it’s likely that there is something wrong with that specific step—people don’t all run out of patience at the exact same time. You can then compare the expected benefits with the cost of implementing the solution. Is it still worthwhile?</p>
<p>You can also approach this question from the other side: start by determining the breakeven point of the solution, i.e., the improvement in the target metric that would make the solution profitable to implement, and then consider whether that improvement is realistic from a behavioral standpoint. From a psychological perspective, it’s better to start with the expected result than with the breakeven point: if you start with the breakeven point, you’re more likely to anchor on it and find reasons to justify that it’s achievable. However, in many cases, you’ll know the breakeven point first, for example if it was calculated during a preliminary cost-benefit analysis; your company or business partners might also request it and refuse to think about the expected result first. Don’t worry too much about it. Regardless of whether you’re working with your expected or best-case result, we’ll need it to determine the minimum detectable effect for our experiment.</p>
<p>It’s important that your behavioral logic connects your proposed solution with your target metric. Don’t leave it to “this will improve the customer experience.” How will you know it? If your logic is solid, you should be able to express it in terms of a causal diagram, with at least some of the effects being observable.</p>
<p>A useful rule of thumb to help you articulate your logic is to break down your business metric into components. For example, revenue (or most of its variations) can be broken down into number of customers, probability/frequency of purchase, quantity purchased, and price paid. Determining which components are likely to be affected can allow you to better articulate the business case. If your business partners are concerned by a decrease in the number of customers and the proposed intervention would most likely only increase the quantity purchased, you need to clarify with them that they would still call it a win. This approach can also reduce the noise in your experiment; if the proposed intervention would most likely only increase the quantity purchased, you can focus on that metric and disregard somewhat random fluctuations in price paid.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08plex" id="idm45968154192904"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-tcbl2" id="idm45968154191432"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-tcbl3" id="idm45968154190056"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-tcbl4" id="idm45968154188680"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08plex2" id="idm45968154187304"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08plex3" id="idm45968154185928"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08plex4" id="idm45968154184552"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-tcbl5" id="idm45968154183176"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Data and Packages"><div class="sect1" id="data_and_libraries-id00023">
<h1>Data and Packages</h1>
<p>The <a href="https://oreil.ly/BehavioralDataAnalysisCh8">GitHub folder for this chapter</a> <a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="data and packages for examples" id="idm45968154179480"/><a contenteditable="false" data-type="indexterm" data-primary="GitHub" data-secondary="experimental design data" data-tertiary="1-click button site" data-tertiary-sortas="one-click button site" id="idm45968154178040"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="data and packages for example" id="idm45968154176104"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="data and packages for example" id="idm45968154174440"/>contains two CSV files with the variables listed in <a data-type="xref" href="#variables_in_our_data">Table 8-1</a>. In the table, the check mark (✓) indicates the variables present in that file, while the cross (☓) indicates the variables missing.</p>
<table class="border" id="variables_in_our_data">
<caption><span class="label">Table 8-1. </span>Variables in our data</caption>
<thead>
<tr>
<th/>
<th>Variable description</th>
<th>chap8-historical_data.csv</th>
<th>chap8-experimental_data.csv</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Gender</em></td>
<td>Categorical, “male”/ “female”</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td><em>Period</em></td>
<td>Month index, 1-32 in historical data, 33 in experimental data</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td><em>Seasonality</em></td>
<td>Annual seasonality, between 0 and 1</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td><em>Month</em></td>
<td>Month of year, 1-12</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td><em>Booked</em></td>
<td>Binary 0/1, target variable</td>
<td>✓</td>
<td>✓</td>
</tr>
<tr>
<td><em>Oneclick</em></td>
<td>Binary 0/1, experimental treatment</td>
<td>☓</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>In this chapter, we’ll use the following packages in addition to the standard ones called out in the Preface:<a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="power analysis" id="idm45968154152232"/><a contenteditable="false" data-type="indexterm" data-primary="power analysis" data-tertiary="package" id="idm45968154150856"/><a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="standardized effect size" id="idm45968154149480"/><a contenteditable="false" data-type="indexterm" data-primary="standardized effect size package" id="idm45968154148040"/></p>
<pre data-type="programlisting" data-code-language="r" class="pagebreak-before less_space"><code class="c1">## R</code>
<code class="nf">library</code><code class="p">(</code><code class="n">pwr</code><code class="p">)</code> <code class="c1"># For traditional power analysis</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="kn">import</code> <code class="nn">statsmodels.stats.proportion</code> <code class="kn">as</code> <code class="nn">ssprop</code> <code class="c1"># For the standardized effect size</code>
<code class="kn">import</code> <code class="nn">statsmodels.stats.power</code> <code class="kn">as</code> <code class="nn">ssp</code> <code class="c1"># For traditional power analysis</code></pre>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Determining Random Assignment and Sample Size/Power"><div class="sect1" id="determining_random_assignment_and_samp">
<h1>Determining Random Assignment and Sample Size/Power</h1>
<p>Once you’ve built and validated the theory of change of your experiment, <a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" id="ch08-raas"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" id="ch08-raas2"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="about" id="idm45968154098088"/><a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="about" id="idm45968154096440"/>the next step is to determine how you’ll do the random assignment and how big a sample size you’ll need.</p>
<p>In my experience, this is often a big step the first time you’re running an experiment in a certain environment. Taking a serious look at your historical data often yields surprising insights that can reshape an experiment. In addition, depending on the noise in your data and the expected impact size (small if you’ve done your job correctly of defining a narrow scope for your experiment), discovering how large a sample you’ll need can be humbling. I still remember the first time the numbers came back and I was told that we would need to run an experiment for almost a year.</p>
<section data-type="sect2" data-pdf-bookmark="Random Assignment"><div class="sect2" id="random_assignmen">
<h2>Random Assignment</h2>
<p>The theory of random assignment could not be simpler: whenever a customer reaches the relevant page, they should be<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="control versus treatment" id="idm45968154091688"/><a contenteditable="false" data-type="indexterm" data-primary="control versus treatment groups" id="idm45968154090072"/><a contenteditable="false" data-type="indexterm" data-primary="treatment versus control groups" id="idm45968154088952"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="control versus treatment" id="idm45968154087832"/> shown the current version of the page (called “control” in experimental jargon) with a certain probability and the version with the 1-click booking button (the “treatment”) with the opposite probability.</p>
<p>The most straightforward option is a 50%-50% allocation until you’ve reached your target sample size, but you may want to use a different split if you have a very high volume of transactions. Let’s imagine for example that you’re managing a website with 100 million visits a day, and you have determined that your necessary sample size is 2 million. You could simply go with 50%-50%, and be done with your experiment in about 30 minutes. However, if anything goes wrong with your treatment (e.g., a bug crashes the website, admittedly an extreme case), you’ll have 1 million unhappy customers on your hands before you know it. <a contenteditable="false" data-type="indexterm" data-primary="timing" data-secondary="experimental group assignment" id="idm45968154085016"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="timing of" id="idm45968154083624"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="timing of" id="idm45968154082248"/>In addition, maybe customers during these 30 minutes are not representative of your full customer base (e.g., China is asleep at that time and you get mostly American visitors or vice versa). In such a situation, it would be better to get the 1 million visits you want in your treatment group over the course of a more representative period, such as a week or a month (you don’t have to worry about the control group being bigger than 1 million). For a 100 million visits/day website, that would translate into splits of respectively 99.86%-0.14% (because 1 / (7 * 100) = 0.14%) and 99.97%-0.03% (because 1 / (30 * 100) = 0.03%). For the sake of simplicity, I’ll assume a 50%-50% split in the rest of the chapter.</p>
<section data-type="sect3" data-pdf-bookmark="Code implementation"><div class="sect3" id="code_implementation">
<h3>Code implementation</h3>
<p>From a coding perspective, assuming that you are not using a<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="code implementation" id="idm45968154060264"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="code implementation" id="idm45968154058808"/><a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="random assignment" id="idm45968154057592"/><a contenteditable="false" data-type="indexterm" data-primary="Python" data-secondary="random assignment" id="idm45968154056216"/> software that takes care of it for you, this can easily be implemented in R or Python:</p>
<ol>
<li><p>Whenever a new customer reaches the relevant page, we generate a random number between 0 and 1.</p></li>
<li><p>Then we assign the customer a group based on that random number: if K is the number of groups we want (including a control group), then all individuals with a random number less than 1/K are assigned to the first group; all individuals with a random number between 1/K and 2/K are assigned to the second group, and so on.</p></li>
</ol>
<p>Here, K is equal to 2, which translates into a very simple formula:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">K</code> <code class="o">&lt;-</code> <code class="m">2</code>
<code class="o">&gt;</code> <code class="n">assgnt</code> <code class="o">=</code> <code class="nf">runif</code><code class="p">(</code><code class="m">1</code><code class="p">,</code><code class="m">0</code><code class="p">,</code><code class="m">1</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="n">group</code> <code class="o">=</code> <code class="nf">ifelse</code><code class="p">(</code><code class="n">assgnt</code> <code class="o">&lt;=</code> <code class="m">1</code><code class="o">/</code><code class="n">K</code><code class="p">,</code> “<code class="n">control</code>”<code class="p">,</code> “<code class="n">treatment</code>”<code class="p">)</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="n">K</code> <code class="o">=</code> <code class="mi">2</code>
<code class="n">assgnt</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>
<code class="n">group</code> <code class="o">=</code> <code class="s2">"control"</code> <code class="k">if</code> <code class="n">assgnt</code> <code class="o">&lt;=</code> <code class="mi">1</code><code class="o">/</code><code class="n">K</code> <code class="k">else</code> <code class="s2">"treatment"</code></pre>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Pitfalls of random assignment"><div class="sect3" id="pitfalls_of_random_assignment">
<h3>Pitfalls of random assignment</h3>
<p>There are, however, a certain number of subtleties that can trip novice experimenters. I’ll cover two in this chapter: the timing and the level of the assignment.</p>
<section data-type="sect4" data-pdf-bookmark="Random assignment timing"><div class="sect4" id="random_assignment_timing">
<h4>Random assignment timing</h4>
<p>The first one is determining the right point in the process for random<a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="pitfalls of" id="idm45968153957688"/><a contenteditable="false" data-type="indexterm" data-primary="timing" data-secondary="experimental group assignment" id="idm45968153956312"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="timing of" id="idm45968153954968"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="timing of" id="idm45968153953592"/> assignment. Let’s say that whenever a customer gets on the first page of the website, you assign them to either the control or the treatment group. Many of these customers will never reach the point of making a booking and will therefore not see your booking interface. This would dramatically reduce the effectiveness of your experiment because you would in effect experiment only on a fraction of your <span class="keep-together">sample.</span></p>
<p>When determining which customers should be part of the experiment and when they should be assigned to an experimental group, you should reflect on how the treatment will be implemented if the experiment is a success. Your experimental design should include the same people who would see the treatment if it got implemented in business as usual and only them. For instance, visitors who leave the website before booking will still not see the button, whereas any future promotion or change to the booking page would be built in addition so to speak “on top of” the button, which would always be present. Therefore, nonbooking visitors should be excluded but customers with a promotion should be included.</p>
</div></section>
<section data-type="sect4" data-pdf-bookmark="Random assignment level"><div class="sect4" id="random_assignment_leve">
<h4>Random assignment level</h4>
<p>The second challenge is making sure that the random<a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="behavioral level of" id="idm45968153947608"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="behavioral level of" id="idm45968153946232"/><a contenteditable="false" data-type="indexterm" data-primary="behavioral levels" id="idm45968153944584"/> assignment is happening at the “right” behavioral level. I’ll explain what this means with an example. Let’s say that a visitor comes on the AirCnC website and starts a booking but then for whatever reason leaves the website (they got disconnected, it’s time for dinner, etc.) and comes back to it later. Should they see the same booking page? If they were offered 1-click booking the first time, should they still be offered it the second time?</p>
<p>The problem here is that there are really multiple levels that could potentially make sense. You could assign control or treatment at the level of a single website visit, of a booking, however many visits it takes, or at the level of a customer account (which may or may not be the same person if several people in a household use the same account). Unfortunately, there are no hard rules here, the right approach must be determined on a case-by-case basis by thinking about the conclusions you want to draw and what a permanent implementation would look like.</p>
<p>In many cases, it makes sense to do an assignment at the closest level you can to a human being: customer account if you can’t distinguish people in a household, or individual customer if they each have a subaccount, as is the case for example with Netflix. Human beings have persistent memories and alternating options for the same person can get confusing. Here, this would mean that our AirCnC customer should see the 1-click button for the whole duration of the experiment, regardless of how many visits and bookings they make during that time. Unfortunately, this means that you can’t just roll the dice metaphorically each time someone starts a booking on the website to determine their assignment; you need to keep track of whether they have been assigned in the past and if so to which group. <a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="cookies for" id="idm45968153940648"/><a contenteditable="false" data-type="indexterm" data-primary="cookies for random assignment" id="idm45968153939000"/><a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="cookies for" id="idm45968153937928"/>For a website experiment, this can be done through cookies (assuming the customer allows them!).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The level at which you’re making your random assignment <a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="behavioral levels" id="idm45968153935304"/><a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="experimental design" data-tertiary="behavioral levels" id="idm45968153933656"/>should also be the level at which you calculate your sample size. If you make assignments at the customer level and customers make an average of three visits per month, you’ll need a three times longer experiment than if you make assignments at the visit level. But the level you choose for your random assignment should determine your sample size, not the other way around!</p>
</div>
<p>Whatever level you choose, you’ll have to keep <a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="track for linking to outcomes" id="idm45968153904760"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="track for linking to outcomes" id="idm45968153903512"/>track of the assignment(s) to be able to link them to business outcomes later. This is why the best-in-class approach is to use centralized systems that record all assignments and connect them to customer IDs in a database, so that they can serve a consistent experience to customers over time.</p>
<p>More broadly, what these two challenges point at is that the implementation of a business experiment is almost always a complex technical affair. A variety of vendors now offer somewhat plug-and-play solutions that hide the complexity under the hood, especially for website experimentation. Whether you rely on them or on your internal tech people, you’ll need to understand how they’re doing the random assignment to make sure that you’re getting the experiment you want.</p>
<p>A good way to check that the system works correctly is to start with an A/A test, where there is a random assignment but the two groups see the same version of the page. This will allow you to check that there are indeed the same number of people in the two groups and that they don’t differ in any significant way.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-raas" id="idm45968153899752"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-raas2" id="idm45968153898376"/></p>
</div></section>
</div></section>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Sample Size and Power Analysis"><div class="sect2" id="sample_size_and_power_analysis">
<h2>Sample Size and Power Analysis</h2>
<p>Once we know what we’re going to test and how, we need to<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" id="ch08-saed"/><a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="experimental design" id="ch08-saed2"/><a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="about" id="idm45968153891896"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="about" id="idm45968153890520"/> determine our sample size. In some cases, as here with our 1-click booking experiment, we can choose our sample size: we can just decide how long we want to run the experiment. In other cases, our sample size might be defined for us, or at least its maximum. If we’re going to run a test across our whole customer or employee population, we can’t increase that population just for the sake of experimentation!</p>
<p>Regardless of the situation we’re in, we will look at our sample size in relation to other experimental variables, such as statistical significance, and not in a vacuum. Understanding how these variables are related is crucial to ensure that our experimental results are usable and that we’re drawing the right conclusions from them. Unfortunately, these are highly complex and nuanced statistical concepts and they have not been developed for business decisions.</p>
<p>In line with the spirit of this book, I’ll do my best to explain these statistical concepts and conventions in the context of business decisions. I’ll then share my reservations about the traditional conventions and offer my two cents regarding how to tweak them while remaining in the traditional framework. And finally, I’ll describe an alternative approach that has been slowly gaining momentum and which I think is superior, namely using computer simulations.</p>
<section data-type="sect3" data-pdf-bookmark="A little bit of statistics theory without math"><div class="sect3" id="a_little_bit_of_statistics_theory_witho">
<h3>A little bit of statistics theory without math</h3>
<p>When running an experiment such as the “1-click booking” button,<a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="about statistics behind" id="idm45968153884392"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="about statistics behind" id="idm45968153883016"/> our goal is to make the right decision: should we implement it or not? Unfortunately, even after running an experiment (or a hundred), we can never be 100% sure that we’re making the right decision, because we have only partial information. Certainly, if we ran an experiment for years on end, we might reach a point where there is only one chance in a million that we’re wrong, but never exactly zero. Moreover, we generally don’t want to run an experiment for years on end, when we could be running other experiments instead! Therefore, there’s a trade-off between the sample size of an experiment and our degree of certainty.</p>
<p>Because we can never know after the fact whether a specific decision was right or not, our approach will be to try to select a good sample size and a good decision rule before the experiment. What does “good” mean here? Well, the very best possible sample size and rule would be those that maximize our expected profit over time. The corresponding calculations are doable but require advanced methods that are beyond the scope of this book.<sup><a data-type="noteref" id="ch01fn15-marker" href="ch08.xhtml#ch01fn15">3</a></sup> Instead, we’ll rely on the following measures:</p>
<ul>
<li><p>Assuming that our 1-click button does increase our booking rate, what is the probability that we’ll rightly implement the button? <a contenteditable="false" data-type="indexterm" data-primary="true positive probability" id="idm45968153874056"/>This is called the “true positive” probability. On the other hand, if there is a positive effect and we wrongly conclude that there is none, it’s called a “false negative.”</p></li>
<li><p>Assuming that our 1-click button has no discernible effect (or, God forbid, a negative effect!) on our booking rate, what is the probability that we’ll wrongly implement the button? <a contenteditable="false" data-type="indexterm" data-primary="false positive probability" id="idm45968153871864"/>This probability is called the “false positive” probability. <a contenteditable="false" data-type="indexterm" data-primary="true negative probability" id="idm45968153870616"/>On the other hand, if there is no effect and we rightly conclude that there is no effect, it’s called a “true negative.”</p></li>
</ul>
<p>These various configurations are summarized in <a data-type="xref" href="#making_the_right_decision_in_the_right">Table 8-2</a>.</p>
<table class="border" id="making_the_right_decision_in_the_right">
<caption><span class="label">Table 8-2. </span>Making the right decision in the right situation</caption>
<thead>
<tr>
<th/>
<th/>
<th colspan="2">Do we implement the 1-click booking button?</th>
</tr>
</thead>
<tbody>
<tr>
<td/>
<td/>
<td><strong>YES</strong></td>
<td><strong>NO</strong></td>
</tr>
<tr>
<td><strong>Does the 1-click booking button increase the booking rate?</strong></td>
<td><strong>YES</strong></td>
<td>True positive</td>
<td>False negative</td>
</tr>
<tr>
<td/>
<td><strong>NO</strong></td>
<td>False positive</td>
<td>True negative</td>
</tr>
</tbody>
</table>
<p>We would want our true positive and our true negative rates to be as high as possible, and our false positive and false negative rates to be as low as possible. However, the simplicity of this table is deceptive, and it actually encompasses an infinite number of situations: when we say that the button increases the booking rate, it could mean that the increase is 1%, 2%, etc. On the other hand, when we say that the button does not increase the booking rate, it could mean that it has exactly zero effect, or that it is decreasing the booking rate by 1%, 2%, etc. <a contenteditable="false" data-type="indexterm" data-primary="effect sizes" data-secondary="true/false positives and negatives" id="idm45968153855368"/>All these effect sizes would have to be factored in to calculate the overall true positive and true negative rates, which would be too complicated. Instead, we will rely on two threshold values.</p>
<p>The first one is an impact of exactly zero for all subjects, also<a contenteditable="false" data-type="indexterm" data-primary="null hypotheses, sharp versus nonsharp" id="idm45968153853000"/><a contenteditable="false" data-type="indexterm" data-primary="sharp null hypothesis" id="idm45968153851848"/><a contenteditable="false" data-type="indexterm" data-primary="nonsharp null hypothesis" id="idm45968153850744"/> called the “sharp null hypothesis” (the nonsharp null hypothesis would be an <em>average</em> zero effect across subjects). <a contenteditable="false" data-type="indexterm" data-primary="false positive probability" data-secondary="statistical significance" id="idm45968153849080"/><a contenteditable="false" data-type="indexterm" data-primary="statistical significance (p-value)" data-secondary="false positive rate of zero impact" id="idm45968153847624"/><a contenteditable="false" data-type="indexterm" data-primary="p-value" data-secondary="false positive rate of zero impact" id="idm45968153846216"/>The false positive rate for this value is called the statistical significance of our experiment. Because a negative impact would be easier to catch than a null effect, the false positive rate for any negative value will be at least as large as the statistical significance, and larger negative effects will have higher false positive rates. <a contenteditable="false" data-type="indexterm" data-primary="statistical significance (p-value)" data-secondary="5% common convention" data-secondary-sortas="five percent common" id="idm45968153844344"/><a contenteditable="false" data-type="indexterm" data-primary="p-value" data-secondary="5% common convention" data-secondary-sortas="five percent common" id="idm45968153842680"/>The most common convention in academic research is to set the statistical significance at 5%, although in certain fields such as <a href="https://oreil.ly/U48vk">particle physics</a>, it can sometimes be as low as 0.00005%.</p>
<p>The second threshold value is set at some positive effect that we’re interested in measuring. For example, we might say that we want to choose a sample size so that we can be “reasonably sure” that we will capture a 1% increase in booking rate, but we’re OK with missing smaller effects than that. <a contenteditable="false" data-type="indexterm" data-primary="alternative hypothesis" id="idm45968153839432"/>This value is often called the “alternative hypothesis,” and <a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="experimental design" data-tertiary="statistical power" id="ch08-samsta"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="statistical power" id="ch08-samsta2"/><a contenteditable="false" data-type="indexterm" data-primary="statistical power" id="ch08-samsta3"/><a contenteditable="false" data-type="indexterm" data-primary="power analysis" id="ch08-samsta4"/>the true positive rate for this value is called the statistical power of our experiment. Because larger effects would be easier to catch, the true positive rate for any larger value will be at least as large as the statistical power, and larger positive effects will have higher true positive rates. “Reasonably sure” is traditionally taken to mean 80%. To reiterate, this doesn’t mean that your experiment “has a power of 80%” and that phrase is actually meaningless by itself: the experiment also has a power of 90% for a certain larger effect size, and a power of 70% for a certain smaller effect size, and so on.</p>
<p>Our table updated according to the traditional convention would therefore look like <a data-type="xref" href="#the_thresholds_used_in_the_traditional">Table 8-3</a>.</p>
<table class="border" id="the_thresholds_used_in_the_traditional">
<caption><span class="label">Table 8-3. </span>The thresholds used in the traditional statistical approach</caption>
<thead>
<tr>
<th/>
<th colspan="2">Do we implement the 1-click booking button?</th>
</tr>
</thead>
<tbody>
<tr>
<td/>
<td><strong>YES</strong></td>
<td><strong>NO</strong></td>
</tr>
<tr>
<td>The 1-click booking button increases the booking rate by more than 1%.</td>
<td>&gt; 80% (larger for larger effect sizes)</td>
<td>&lt; 20% (smaller for larger effect sizes)</td>
</tr>
<tr>
<td><strong>The 1-click booking button increases the booking rate by exactly 1%.</strong></td>
<td><strong>80% (statistical power)</strong></td>
<td><strong>20% (1 minus statistical power)</strong></td>
</tr>
<tr>
<td>The 1-click booking button increases the booking rate by less than 1%.</td>
<td>&lt; 80% (larger for larger effect sizes)</td>
<td>&gt; 20% (smaller for larger effect sizes)</td>
</tr>
<tr>
<td><strong>The 1-click booking button has exactly no impact on the booking rate.</strong></td>
<td><strong>5% (statistical significance)</strong></td>
<td><strong>95% (1 minus statistical significance)</strong></td>
</tr>
<tr>
<td>The 1-click booking button strictly decreases the booking rate.</td>
<td>&lt; 5% (smaller for larger negative effect sizes)</td>
<td>&gt; 95% (larger for larger negative effect sizes)</td>
</tr>
</tbody>
</table>
<p>I am no big fan of using an arbitrary number purely because it’s conventional, and you should feel free to adjust the “80% power” convention to fit your needs. Using a power of 80% for your relevant threshold effect size would mean that if the intervention had exactly that effect size, on average, you would have a 20% chance of not implementing the intervention because you wrongly got a negative result. For big and costly interventions that are hard to test, my opinion is that it’s too low and I would personally target 90% power. <a contenteditable="false" data-type="indexterm" data-primary="statistical power" data-secondary="sample size and" id="idm45968153812376"/>On the other hand, the higher the power you want, the larger your sample size will need to be. You may not want to spend half a year getting absolutely certain of the value of the 1-click button if in that time your competitor has completely revamped their website twice and is eating your lunch.</p>
<p>In my personal experience, one key but often ignored consideration for power analysis and sample size determination in the real world is organizational testing velocity:<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="how many experiments" id="idm45968153809912"/> how many experiments can you run in a year? In many companies, that number is constrained by someone’s time (either the analyst’s or the business partner’s), by the company’s planning cycle, by budget limits, etc., but not by the number of customers available. If you can realistically hope to plan, test, and implement only one intervention per year, do you really want to run a three-month experiment and then do nothing for the rest of the year? On the other hand, if you can run one experiment a week, do you really want to spend three months getting certain of a positive but mediocre impact instead of taking 12 chances at a big one? Therefore, after doing the math, you should always do a sanity check of your experiment duration based on your testing velocity and adjust it appropriately.</p>
<p>Regarding statistical significance, the conventional approach introduces <a contenteditable="false" data-type="indexterm" data-primary="statistical significance (p-value)" data-secondary="5% common convention" data-secondary-sortas="five percent common" id="idm45968153807176"/><a contenteditable="false" data-type="indexterm" data-primary="p-value" data-secondary="5% common convention" data-secondary-sortas="five percent common" id="idm45968153805512"/>an asymmetry between the control and the treatment with a statistical significance threshold of 95%. The bar of evidence the treatment has to pass to get implemented is much higher than for the control, which is implemented by default. Let’s say that you’re setting up a new marketing email campaign and you have two options to test. Why should one version be given the benefit of the doubt over the other? On the other hand, if you have a campaign that has been running for years and for which you have run hundreds of tests, the current version is probably extremely good and a 5% chance of wrongly abandoning it might be too high; the right threshold here might be 99% instead of 95%. More broadly, relying on a conventional value that is the same for all experiments feels to me like a missed opportunity to reflect on the respective costs of false positives and false negatives. In the case of the 1-click button, which is easily reversible and has minimal costs of implementation, I would target a statistical significance threshold of 90% as well.</p>
<p>To recap, from a statistical perspective, our experiment can be summarized by four values:<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="about four values of" id="idm45968153802168"/><a contenteditable="false" data-type="indexterm" data-primary="B.E.A.N. (beta, effect size, alpha, sample size N)" id="idm45968153800856"/><a contenteditable="false" data-type="indexterm" data-primary="beta (β) as statistical significance" id="idm45968153799656"/><a contenteditable="false" data-type="indexterm" data-primary="p-value" data-secondary="beta (β) as" id="idm45968153798536"/><a contenteditable="false" data-type="indexterm" data-primary="statistical significance (p-value)" data-secondary="beta (β) as" id="idm45968153797160"/><a contenteditable="false" data-type="indexterm" data-primary="effect sizes" data-secondary="B.E.A.N. (beta, effect size, alpha, sample size)" id="idm45968153795768"/><a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="N as" id="idm45968153794360"/><a contenteditable="false" data-type="indexterm" data-primary="statistical power" data-secondary="1–α as" id="idm45968153792984"/><a contenteditable="false" data-type="indexterm" data-primary="alternative hypothesis" id="idm45968153791608"/></p>
<ul>
<li><p>The statistical significance, often represented by the Greek letter beta (β)</p></li>
<li><p>The effect size chosen for the alternative hypothesis, a.k.a. the minimal detectable effect</p></li>
<li><p>The statistical power, often represented as 1 − α where α is the false negative rate for the chosen alternative effect size</p></li>
<li><p>The sample size of our experiment, represented by N</p></li>
</ul>
<p>These four variables are referred to as the B.E.A.N. (beta, effect size, alpha, sample size N), and determining them for an experiment is called a “power analysis.”<sup><a data-type="noteref" id="ch01fn16-marker" href="ch08.xhtml#ch01fn16">4</a></sup> For our 1-click button experiment, we have decided on the first three of them and we only have to determine the sample size. We’ll see next how to do it with traditional statistical formulas and then how to do it with computer simulations.</p>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Traditional power analysis"><div class="sect3" id="traditional_power_analysis">
<h3>Traditional power analysis</h3>
<p>Statisticians have developed formulas to determine the required<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="traditional power analysis" id="idm45968153783432"/><a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="experimental design" data-tertiary="traditional power analysis" id="idm45968153781640"/><a contenteditable="false" data-type="indexterm" data-primary="statistical power" data-secondary="traditional power analysis" data-seealso="power analysis" id="idm45968153779976"/><a contenteditable="false" data-type="indexterm" data-primary="power analysis" data-secondary="traditional power analysis" id="idm45968153778312"/> sample size for certain statistical tests. Given that we’ll rely on regression instead of tests, you might wonder why we would want to use these formulas. In my experience, these will give you values that are of the same order of magnitude as the “true” required sample size. That’s a quick and easy way to get reasonable starting values for your simulations if you have no idea whether your sample size should be 100 or 100,000 (in this particular example, we’ll end up with almost exactly the same sample size at the end of our simulations!).</p>
<p>The Test of Proportions is a standard test, and <a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="experimental design" data-tertiary="Test of Proportions for sample size" id="idm45968153775736"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="Test of Proportions for" id="idm45968153774072"/><a contenteditable="false" data-type="indexterm" data-primary="Test of Proportions for sample size" id="idm45968153772424"/>the formula to calculate the corresponding sample size is easily available in R and Python. Let’s first look at the R<a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="Test of Proportions for sample size" id="idm45968153771048"/> <span class="keep-together">formula.</span></p>
<p>With an average booking rate of 18.25% in our historical data, the chosen effect size of 1% would translate into an expected booking rate for our treatment group of 19.25%. For the standard values of the parameters—statistical significance = 0.05 and power = 0.8—the corresponding formula in R would be:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">effect_size</code> <code class="o">&lt;-</code> <code class="nf">ES.h</code><code class="p">(</code><code class="m">0.1925</code><code class="p">,</code><code class="m">0.1825</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">pwr.2p.test</code><code class="p">(</code><code class="n">h</code> <code class="o">=</code> <code class="n">effect_size</code><code class="p">,</code> <code class="n">n</code> <code class="o">=</code> <code class="kc">NULL</code><code class="p">,</code> <code class="n">sig.level</code> <code class="o">=</code> <code class="m">0.05</code><code class="p">,</code> <code class="n">power</code> <code class="o">=</code> <code class="m">0.8</code><code class="p">,</code>
                      <code class="n">alternative</code> <code class="o">=</code> <code class="s">"greater"</code><code class="p">)</code>


     <code class="n">Difference</code> <code class="n">of</code> <code class="n">proportion</code> <code class="n">power</code> <code class="n">calculation</code> <code class="n">for</code> <code class="n">binomial</code> <code class="nf">distribution</code>
<code class="nf">                      </code><code class="p">(</code><code class="n">arcsine</code> <code class="n">transformation</code><code class="p">)</code> 

              <code class="n">h</code> <code class="o">=</code> <code class="m">0.02562255</code>
              <code class="n">n</code> <code class="o">=</code> <code class="m">18834.47</code>
      <code class="n">sig.level</code> <code class="o">=</code> <code class="m">0.05</code>
          <code class="n">power</code> <code class="o">=</code> <code class="m">0.8</code>
    <code class="n">alternative</code> <code class="o">=</code> <code class="n">greater</code>

<code class="n">NOTE</code><code class="o">:</code> <code class="n">same</code> <code class="n">sample</code> <code class="n">sizes</code></pre>
<p>The syntax of all the functions for power analysis in the <code>pwr</code> package is the same, with the exception of the notation for the effect size, which changes from one formula to another:</p>
<ul>
<li><p><code>h</code> is the effect size, based on the increase in probability we want to be able to observe over the baseline probability.</p></li>
<li><p><code>n</code> is the sample size for each group.</p></li>
<li><p><code>sig.level</code> is the statistical significance.</p></li>
<li><p><code>power</code> is the statistical power, equal to 1 − α.</p></li>
</ul>
<p>When entering the formula, you should enter the values for three of these variables and set the remaining one to NULL. In the preceding formula, we’re calculating the sample size, so we set <code>n</code> = NULL.</p>
<p>Note that for a test of two proportions, the effect size for statistical purposes depends on the baseline rate; an increase of 5% from a baseline of 10% or 90% is more “important” than from a baseline of 50%. Fortunately, the package <code>pwr</code> provides the <code>ES.h</code><code>()</code> function, which translates the expected probability and the baseline probability into the right effect size for the formula.</p>
<p>Note also the parameter at the end of the formula: <code>alternative</code> indicates whether you want to run a one-sided (<code>greater</code> or <code>less</code>) or a two-sided (<code>two.sided</code>) test. As long as our treatment doesn’t increase our booking rate, we don’t really care whether it has the same booking rate or a lower booking rate compared to our control; either way, we won’t implement it. This implies that we can run a one-sided test instead of a two-sided test by setting <code>alternative = 'greater'</code>.</p>
<p>The code for Python is similar, using the<a contenteditable="false" data-type="indexterm" data-primary="Python" data-secondary="Test of Proportions for sample size" id="idm45968153685080"/> <code>proportion_effectsize()</code> function from the package <code>statsmodels.stats.proportion</code>:</p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python</code>
<code class="n">effect_size</code> <code class="o">=</code> <code class="n">ssprop</code><code class="o">.</code><code class="n">proportion_effectsize</code><code class="p">(</code><code class="mf">0.194</code><code class="p">,</code> <code class="mf">0.184</code><code class="p">)</code>
<code class="n">ssp</code><code class="o">.</code><code class="n">tt_ind_solve_power</code><code class="p">(</code><code class="n">effect_size</code> <code class="o">=</code> <code class="n">effect_size</code><code class="p">,</code> 
                       <code class="n">alpha</code> <code class="o">=</code> <code class="mf">0.05</code><code class="p">,</code> 
                       <code class="n">nobs1</code> <code class="o">=</code> <code class="bp">None</code><code class="p">,</code> 
                       <code class="n">alternative</code> <code class="o">=</code> <code class="s1">'larger'</code><code class="p">,</code> 
                       <code class="n">power</code><code class="o">=</code><code class="mf">0.8</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="mf">18950.818821558503</code></pre>
<p>The sample size returned by the formula is 18,800 per group (plus or minus some minor variation between R and Python), i.e., 37,600 total, which means we can achieve the necessary sample size in a bit less than four months. That was easy! Using a statistical significance of 0.1 and a power of 0.9 would yield a sample size of 20,000 per group, just a bit longer.</p>
<p>What does a total sample size of 40,000 for a statistical significance of 0.1 and a power of 0.9 mean in terms of the decision model I outlined in the previous section? Imagine the following:</p>
<ul>
<li><p>You run a very large number of experiments with a total sample size of 40,000, as described.</p></li>
<li><p>Your decision rule in each case is that you will implement the 1-click button if the statistics from the test of proportions has a p-value lower than 0.1.</p></li>
<li><p>In all of these experiments, the true effect size is 1%.</p></li>
</ul>
<p>Then you will find a significant positive result and implement the 1-click button in 90% (i.e., 0.9) of these experiments; in the remaining 10% of these experiments, you’ll get a null result and wrongly reject implementing the 1-click button.</p>
<p>There are some equivalent formulas for regression, but only for the simplest cases, and I find that even in those situations, their complexity vastly outweighs their usefulness. Nonetheless, as a conceptual step toward our simulation approach, let’s review what the traditional statistical approach would look like in terms of the decision model with regression. Let’s run a logistic regression on some mock-up data:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R (output not shown)</code>
<code class="n">exp_null_data</code> <code class="o">&lt;-</code> <code class="n">hist_data</code> <code class="o">%&gt;%</code>
  <code class="nf">slice_sample</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="m">20000</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code> <code class="o">=</code> <code class="nf">ifelse</code><code class="p">(</code><code class="nf">runif</code><code class="p">(</code><code class="m">20000</code><code class="p">)</code><code class="o">&gt;</code><code class="m">0.5</code><code class="p">,</code><code class="m">1</code><code class="p">,</code><code class="m">0</code><code class="p">))</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code> <code class="o">=</code> <code class="nf">factor</code><code class="p">(</code><code class="n">oneclick</code><code class="p">,</code> <code class="n">levels</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code><code class="m">1</code><code class="p">)))</code>
<code class="nf">summary</code><code class="p">(</code><code class="nf">glm</code><code class="p">(</code><code class="n">booked</code> <code class="o">~</code> <code class="n">oneclick</code> <code class="o">+</code> <code class="n">age</code> <code class="o">+</code> <code class="n">gender</code><code class="p">,</code> 
                     <code class="n">data</code> <code class="o">=</code> <code class="n">exp_null_data</code><code class="p">,</code> <code class="n">family</code> <code class="o">=</code> <code class="nf">binomial</code><code class="p">(</code><code class="n">link</code> <code class="o">=</code> <code class="s">"logit"</code><code class="p">)))</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python </code>
<code class="n">exp_null_data_df</code> <code class="o">=</code> <code class="n">hist_data_df</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2000</code><code class="p">)</code>
<code class="n">exp_null_data_df</code><code class="p">[</code><code class="s1">'oneclick'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code><code class="mi">2000</code><code class="p">)</code><code class="o">&gt;</code><code class="mf">0.5</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">)</code>
<code class="n">mod</code> <code class="o">=</code> <code class="n">smf</code><code class="o">.</code><code class="n">logit</code><code class="p">(</code><code class="s1">'booked ~ oneclick + age + gender'</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">exp_null_data_df</code><code class="p">)</code>
<code class="n">mod</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">disp</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>
<code class="o">...</code>
               <code class="n">coef</code>     <code class="n">std</code> <code class="n">err</code>    <code class="n">z</code>      <code class="n">P</code><code class="o">&gt;|</code><code class="n">z</code><code class="o">|</code> <code class="p">[</code><code class="mf">0.025</code>    <code class="mf">0.975</code><code class="p">]</code>
<code class="n">Intercept</code>      <code class="mf">9.5764</code>    <code class="mf">0.621</code>   <code class="mf">15.412</code>    <code class="mf">0.000</code>    <code class="mf">8.359</code>   <code class="mf">10.794</code>
<code class="n">gender</code><code class="p">[</code><code class="n">T</code><code class="o">.</code><code class="n">male</code><code class="p">]</code> <code class="mf">0.1589</code>    <code class="mf">0.136</code>    <code class="mf">1.167</code>    <code class="mf">0.243</code>    <code class="o">-</code><code class="mf">0.108</code>   <code class="mf">0.426</code>
<code class="n">oneclick</code>       <code class="mf">0.0496</code>    <code class="mf">0.136</code>    <code class="mf">0.365</code>    <code class="mf">0.715</code>    <code class="o">-</code><code class="mf">0.217</code>   <code class="mf">0.316</code>
<code class="n">age</code>           <code class="o">-</code><code class="mf">0.3017</code>    <code class="mf">0.017</code>  <code class="o">-</code><code class="mf">17.434</code>    <code class="mf">0.000</code>    <code class="o">-</code><code class="mf">0.336</code>  <code class="o">-</code><code class="mf">0.268</code>
<code class="o">...</code></pre>
<p>The traditional decision rule would be to consider the impact of the 1-click button to be significant and implement it if the corresponding coefficient (here approximately 0.0475) had a p-value less than 0.1. Because it’s approximately 0.28 with this mock-up data, we would consider the effect not significant and decline to implement the button (the actual numbers for you will vary randomly depending on your simulation).</p>
<p>Determining the sample size for our analysis based on this approach would entail determining the sample size such that in 90% of a large number of experiments where the true effect is 1%, we would get a p-value for the regression coefficient less than 0.1. But as I described in <a data-type="xref" href="ch07.xhtml#measuring_uncertainty_with_the_bootstra">Chapter 7</a>, this implicitly makes statistical assumptions about our data being normally distributed, which can be problematic, so we’ll use Bootstrap simulations instead as we’ll now see.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Using the sample size formula for the test of proportions can still be useful as a quick and fast first step, because its result should be on the same order of magnitude as the final sample size you’ll need. A total sample size of 40,000 for the test of proportions means that unless your other predictors have a crazy high predictive power, the order of magnitude for your required sample size is going to be 10,000, and not 1,000 or 100,000 (i.e., your sample size will have five figures). We will start our simulations with a tentative sample size of 20,000, and based on how much effective power it gives us, we’ll adjust that number upward or downward.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-samsta" id="idm45968153352520"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-samsta2" id="idm45968153351144"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-samsta3" id="idm45968153349768"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-samsta4" id="idm45968153348392"/></p>
</div>
</div></section>
<section data-type="sect3" data-pdf-bookmark="Power analysis without statistics: Bootstrap simulations"><div class="sect3" id="power_analysis_without_statistics_boots">
<h3>Power analysis without statistics: Bootstrap simulations</h3>
<p>The traditional statistical analysis made perfect sense when data<a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="sample size" data-tertiary="Bootstrap simulations for power analysis" id="ch08-btpow"/><a contenteditable="false" data-type="indexterm" data-primary="sample size" data-secondary="experimental design" data-tertiary="Bootstrap simulations for power analysis" id="ch08-btpow2"/><a contenteditable="false" data-type="indexterm" data-primary="Bootstrap" data-secondary="power analysis" id="ch08-btpow3"/><a contenteditable="false" data-type="indexterm" data-primary="power analysis" data-secondary="Bootstrap simulations" id="ch08-btpow4"/> was limited and calculations were done painstakingly by hand. I strongly believe that it has now outlived its usefulness: Bootstrap simulations offer an alternative that better reflects the realities and needs of applied data analysis. How wrong an experiment can get (e.g., saying that the treatment is 1% better than the control when in reality it’s 10% worse) is often a bigger concern for business partners than how likely it is that the difference is zero.<sup><a data-type="noteref" id="ch01fn17-marker" href="ch08.xhtml#ch01fn17">5</a></sup></p>
<section data-type="sect4" data-pdf-bookmark="Connecting simulations and statistical theory"><div class="sect4" id="connecting_simulations_and_statistical">
<h4>Connecting simulations and statistical theory</h4>
<p>When using Bootstrap simulations, our decision rule doesn’t rely on p-values. Instead, we implement the treatment if the Bootstrap confidence interval for the coefficient of interest is above a certain threshold, usually zero. If the assumptions of statistical power analysis are verified, Bootstrap simulations yield results that are very similar and intuitively connected:</p>
<ul>
<li><p>Under the sharp null hypothesis of no effect, we expect that a 90%-CI <a contenteditable="false" data-type="indexterm" data-primary="sharp null hypothesis" data-secondary="confidence intervals" id="idm45968153333528"/>will include zero 90% of the time, an 80%-CI will include zero 80% of the time, and so on. <a contenteditable="false" data-type="indexterm" data-primary="coverage of confidence intervals" id="idm45968153331912"/><a contenteditable="false" data-type="indexterm" data-primary="confidence intervals (CI)" data-secondary="coverage" id="idm45968153330840"/>This property, called the <em>coverage</em> of the CIs, implies that the percentage we use to define our CI is equivalent to statistical significance, i.e., a 90%-CI will have an approximately 5% false positive rate in each direction. In 5% of cases we’ll observe a CI that is strictly negative and in 5% of cases a CI that is strictly <span class="keep-together">positive.</span></p></li>
<li><p>Given an alternative hypothesis, i.e., a target effect size,<a contenteditable="false" data-type="indexterm" data-primary="alternative hypothesis" id="idm45968153327544"/> we can define our power as the percentage of simulations that yield a true positive. For example, if we set the effect of the 1-click button at 1%, simulate a large number of experiments, and observe that 75% of our Bootstrap CIs are strictly positive, then our power is 75%.</p></li>
</ul>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As we’ll see in the next chapter, if the assumptions of traditional statistical power analysis are <em>not</em> verified, the coverage of the Bootstrap CI may vary. That is, a 90%-CI may include zero more or less than 90% of the time. This effective coverage represents the real risk of false positives that needs to be set to the desired level of significance. This is just a heads-up; we’ll get into more details in <a data-type="xref" href="ch09.xhtml#stratified_randomizatio">Chapter 9</a>.</p>
</div>
<p>Simulations offer a very versatile but transparent way of determining the necessary sample size for any experiment, however weird the data or complex the business decision at hand. These advantages come from making you state explicitly how you’ll analyze your data and write the corresponding code before actually running the experiment, which provides an additional sanity check and opportunity to make adjustments. The counterpart to these benefits is that we’ll have to do more coding ourselves instead of relying on off-the-shelf formulas. I’ll attempt to limit the complexity of the code by breaking it into intuitive functions.</p>
</div></section>
<section data-type="sect4" data-pdf-bookmark="Writing our analysis code"><div class="sect4" id="writing_our_analysis_code">
<h4>Writing our analysis code</h4>
<p>Let’s first create a function that will output our metric of interest, namely the coefficient for <em>OneClick</em> in our logistic regression:<a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="Bootstrap" data-tertiary="power analysis" id="ch08-bspa"/><a contenteditable="false" data-type="indexterm" data-primary="Python" data-secondary="Bootstrap" data-tertiary="power analysis" id="ch08-bspa2"/><a contenteditable="false" data-type="indexterm" data-primary="power analysis" data-secondary="Bootstrap simulations" data-tertiary="code" id="ch08-bspa3"/></p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="c1">#Metric function</code>
<code class="n">log_reg_fun</code> <code class="o">&lt;-</code> <code class="nf">function</code><code class="p">(</code><code class="n">dat</code><code class="p">){</code>
  <code class="c1">#Running logistic regression</code>
  <code class="n">log_mod_exp</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="n">booked</code> <code class="o">~</code> <code class="n">oneclick</code> <code class="o">+</code> <code class="n">age</code> <code class="o">+</code> <code class="n">gender</code><code class="p">,</code> 
                     <code class="n">data</code> <code class="o">=</code> <code class="n">dat</code><code class="p">,</code> <code class="n">family</code> <code class="o">=</code> <code class="nf">binomial</code><code class="p">(</code><code class="n">link</code> <code class="o">=</code> <code class="s">"logit"</code><code class="p">))</code>
  <code class="n">summ</code> <code class="o">&lt;-</code> <code class="nf">summary</code><code class="p">(</code><code class="n">log_mod_exp</code><code class="p">)</code>
  <code class="n">metric</code> <code class="o">&lt;-</code> <code class="n">summ</code><code class="o">$</code><code class="n">coefficients</code><code class="p">[</code><code class="s">'oneclick1'</code><code class="p">,</code> <code class="s">'Estimate'</code><code class="p">]</code>
  <code class="nf">return</code><code class="p">(</code><code class="n">metric</code><code class="p">)}</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="k">def</code> <code class="nf">log_reg_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="p">):</code>
    <code class="n">model</code> <code class="o">=</code> <code class="n">smf</code><code class="o">.</code><code class="n">logit</code><code class="p">(</code><code class="s1">'booked ~ oneclick + age + gender'</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">dat_df</code><code class="p">)</code>
    <code class="n">res</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">disp</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
    <code class="n">coeff</code> <code class="o">=</code> <code class="n">res</code><code class="o">.</code><code class="n">params</code><code class="p">[</code><code class="s1">'oneclick'</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">coeff</code></pre>
<p>This is just a functional wrapper for our preceding analysis, and applying that function to our mock-up data set would return the same coefficient, approximately 0.0475.</p>
<p>Let’s then calculate Bootstrap CIs for this metric, reusing the function from <a data-type="xref" href="ch07.xhtml#measuring_uncertainty_with_the_bootstra">Chapter 7</a>:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="n">boot_CI_fun</code> <code class="o">&lt;-</code> <code class="nf">function</code><code class="p">(</code><code class="n">dat</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">){</code>
  <code class="c1"># Setting the number of bootstrap samples</code>
  <code class="n">B</code> <code class="o">&lt;-</code> <code class="m">100</code>
  
  <code class="n">boot_metric_fun</code> <code class="o">&lt;-</code> <code class="nf">function</code><code class="p">(</code><code class="n">dat</code><code class="p">,</code> <code class="n">J</code><code class="p">){</code>
    <code class="n">boot_dat</code> <code class="o">&lt;-</code> <code class="n">dat</code><code class="p">[</code><code class="n">J</code><code class="p">,]</code>
    <code class="nf">return</code><code class="p">(</code><code class="nf">metric_fun</code><code class="p">(</code><code class="n">boot_dat</code><code class="p">))}</code>
  <code class="n">boot.out</code> <code class="o">&lt;-</code> <code class="nf">boot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">dat</code><code class="p">,</code> <code class="n">statistic</code><code class="o">=</code><code class="n">boot_metric_fun</code><code class="p">,</code> <code class="n">R</code><code class="o">=</code><code class="n">B</code><code class="p">)</code>
  <code class="n">confint</code> <code class="o">&lt;-</code> <code class="nf">boot.ci</code><code class="p">(</code><code class="n">boot.out</code><code class="p">,</code> <code class="n">conf</code> <code class="o">=</code> <code class="m">0.90</code><code class="p">,</code> <code class="n">type</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="s">'perc'</code><code class="p">))</code>
  <code class="n">CI</code> <code class="o">&lt;-</code> <code class="n">confint</code><code class="o">$</code><code class="n">percent</code><code class="nf">[c</code><code class="p">(</code><code class="m">4</code><code class="p">,</code><code class="m">5</code><code class="p">)]</code>
  <code class="nf">return</code><code class="p">(</code><code class="n">CI</code><code class="p">)}</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="k">def</code> <code class="nf">boot_CI_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">,</code> <code class="n">B</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code> <code class="n">conf_level</code> <code class="o">=</code> <code class="mf">0.9</code><code class="p">):</code>
  <code class="c1">#Setting sample size</code>
  <code class="n">N</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dat_df</code><code class="p">)</code>
  <code class="n">conf_level</code> <code class="o">=</code> <code class="n">conf_level</code>
  <code class="n">coeffs</code> <code class="o">=</code> <code class="p">[]</code>
  
  <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">B</code><code class="p">):</code>
      <code class="n">sim_data_df</code> <code class="o">=</code> <code class="n">dat_df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">n</code><code class="o">=</code><code class="n">N</code><code class="p">,</code> <code class="n">replace</code> <code class="o">=</code> <code class="bp">True</code><code class="p">)</code>
      <code class="n">coeff</code> <code class="o">=</code> <code class="n">metric_fun</code><code class="p">(</code><code class="n">sim_data_df</code><code class="p">)</code>
      <code class="n">coeffs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">coeff</code><code class="p">)</code>
  
  <code class="n">coeffs</code><code class="o">.</code><code class="n">sort</code><code class="p">()</code>
  <code class="n">start_idx</code> <code class="o">=</code> <code class="nb">round</code><code class="p">(</code><code class="n">B</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">conf_level</code><code class="p">)</code> <code class="o">/</code> <code class="mi">2</code><code class="p">)</code>
  <code class="n">end_idx</code> <code class="o">=</code> <code class="o">-</code> <code class="nb">round</code><code class="p">(</code><code class="n">B</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">conf_level</code><code class="p">)</code> <code class="o">/</code> <code class="mi">2</code><code class="p">)</code>
  <code class="n">confint</code> <code class="o">=</code> <code class="p">[</code><code class="n">coeffs</code><code class="p">[</code><code class="n">start_idx</code><code class="p">],</code> <code class="n">coeffs</code><code class="p">[</code><code class="n">end_idx</code><code class="p">]]</code>  
  <code class="k">return</code><code class="p">(</code><code class="n">confint</code><code class="p">)</code></pre>
<p>Similarly, we’ll take as our decision rule that we’ll implement the button if and only if the Bootstrap 90%-CI is strictly positive (i.e., it doesn’t include zero):</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="n">decision_fun</code> <code class="o">&lt;-</code> <code class="nf">function</code><code class="p">(</code><code class="n">dat</code><code class="p">){</code>
  <code class="n">boot_CI</code> <code class="o">&lt;-</code> <code class="nf">boot_CI_fun</code><code class="p">(</code><code class="n">dat</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">)</code>
  <code class="n">decision</code> <code class="o">&lt;-</code> <code class="nf">ifelse</code><code class="p">(</code><code class="n">boot_CI</code><code class="p">[</code><code class="m">1</code><code class="p">]</code><code class="o">&gt;</code><code class="m">0</code><code class="p">,</code><code class="m">1</code><code class="p">,</code><code class="m">0</code><code class="p">)</code>
  <code class="nf">return</code><code class="p">(</code><code class="n">decision</code><code class="p">)}</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="k">def</code> <code class="nf">decision_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">,</code> <code class="n">B</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code> <code class="n">conf_level</code> <code class="o">=</code> <code class="mf">0.9</code><code class="p">):</code>
    <code class="n">boot_CI</code> <code class="o">=</code> <code class="n">boot_CI_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">,</code> <code class="n">B</code> <code class="o">=</code> <code class="n">B</code><code class="p">,</code> <code class="n">conf_level</code> <code class="o">=</code> <code class="n">conf_level</code><code class="p">)</code>
    <code class="n">decision</code> <code class="o">=</code> <code class="mi">1</code> <code class="k">if</code> <code class="n">boot_CI</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mi">0</code>  <code class="k">else</code> <code class="mi">0</code>
    <code class="k">return</code> <code class="n">decision</code></pre>
<p>This is equivalent to the decision rule of implementing the button if and only if the p-value is under the 0.10 threshold. You can check by yourself that applying this function to our mock-up data set returns 0 as it should.</p>
<p>The definition of the power of our experiment for a given effect size and a given sample size remains the same: it is the percentage of a large number of such experiments for which we would implement the button. Let’s now turn to simulating this large number of experiments!</p>
</div></section>
<section data-type="sect4" data-pdf-bookmark="Power simulation"><div class="sect4" id="power_simulation">
<h4>Power simulation</h4>
<p>We’ll then write our function to run a single simulation. The code works as follows (the callout numbers apply to both R and Python):</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code><code>
</code><code class="o">&gt;</code><code> </code><code class="n">single_sim_fun</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">function</code><code class="p">(</code><code class="n">dat</code><code class="p">,</code><code> </code><code class="n">metric_fun</code><code class="p">,</code><code> </code><code class="n">Nexp</code><code class="p">,</code><code> </code><code class="n">eff_size</code><code class="p">,</code><code> </code><code class="n">B</code><code> </code><code class="o">=</code><code> </code><code class="m">100</code><code class="p">,</code><code> 
                             </code><code class="n">conf.level</code><code> </code><code class="o">=</code><code> </code><code class="m">0.9</code><code class="p">)</code><code class="p">{</code><code>
    
    </code><code class="c1">#Adding predicted probability of booking </code><a class="co" id="comarker81" href="ch11.xhtml#c01"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
    </code><code class="n">hist_mod</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">glm</code><code class="p">(</code><code class="n">booked</code><code> </code><code class="o">~</code><code> </code><code class="n">age</code><code> </code><code class="o">+</code><code> </code><code class="n">gender</code><code> </code><code class="o">+</code><code> </code><code class="n">period</code><code class="p">,</code><code> 
                    </code><code class="n">family</code><code> </code><code class="o">=</code><code> </code><code class="nf">binomial</code><code class="p">(</code><code class="n">link</code><code> </code><code class="o">=</code><code> </code><code class="s">"</code><code class="s">logit"</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">dat</code><code class="p">)</code><code>
    </code><code class="n">sim_data</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">dat</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">pred_prob_bkg</code><code> </code><code class="o">=</code><code> </code><code class="n">hist_mod</code><code class="o">$</code><code class="n">fitted.values</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="c1">#Filtering down to desired sample size </code><a class="co" id="comarker82" href="ch11.xhtml#c02"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
      </code><code class="nf">slice_sample</code><code class="p">(</code><code class="n">n</code><code> </code><code class="o">=</code><code> </code><code class="n">Nexp</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="c1">#Random assignment of experimental groups </code><a class="co" id="comarker83" href="ch11.xhtml#c03"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code class="c1">         </code><code>
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code><code> </code><code class="o">=</code><code> </code><code class="nf">ifelse</code><code class="p">(</code><code class="nf">runif</code><code class="p">(</code><code class="n">Nexp</code><code class="p">,</code><code class="m">0</code><code class="p">,</code><code class="m">1</code><code class="p">)</code><code> </code><code class="o">&lt;=</code><code> </code><code class="m">1</code><code class="o">/</code><code class="m">2</code><code class="p">,</code><code> </code><code class="m">0</code><code class="p">,</code><code> </code><code class="m">1</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code><code> </code><code class="o">=</code><code> </code><code class="nf">factor</code><code class="p">(</code><code class="n">oneclick</code><code class="p">,</code><code> </code><code class="n">levels</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code><code class="m">1</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="c1"># Adding effect to treatment group  </code><a class="co" id="comarker84" href="ch11.xhtml#c04"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code class="c1">     </code><code>
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">pred_prob_bkg</code><code> </code><code class="o">=</code><code> </code><code class="nf">ifelse</code><code class="p">(</code><code class="n">oneclick</code><code> </code><code class="o">==</code><code> </code><code class="m">1</code><code class="p">,</code><code> 
                                    </code><code class="n">pred_prob_bkg</code><code> </code><code class="o">+</code><code> </code><code class="n">eff_size</code><code class="p">,</code><code> 
                                    </code><code class="n">pred_prob_bkg</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">booked</code><code> </code><code class="o">=</code><code> </code><code class="nf">ifelse</code><code class="p">(</code><code class="n">pred_prob_bkg</code><code> </code><code class="o">&gt;=</code><code> </code><code class="nf">runif</code><code class="p">(</code><code class="n">Nexp</code><code class="p">,</code><code class="m">0</code><code class="p">,</code><code class="m">1</code><code class="p">)</code><code class="p">,</code><code class="m">1</code><code class="p">,</code><code> </code><code class="m">0</code><code class="p">)</code><code class="p">)</code><code>
  
    </code><code class="c1">#Calculate the decision (we want it to be 1) </code><a class="co" id="comarker85" href="ch11.xhtml#c05"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code class="c1"> </code><code>
    </code><code class="n">decision</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">decision_fun</code><code class="p">(</code><code class="n">sim_data</code><code class="p">,</code><code> </code><code class="n">metric_fun</code><code class="p">,</code><code> </code><code class="n">B</code><code> </code><code class="o">=</code><code> </code><code class="n">B</code><code class="p">,</code><code> 
                           </code><code class="n">conf.level</code><code> </code><code class="o">=</code><code> </code><code class="n">conf.level</code><code class="p">)</code><code>
</code><code class="nf">return</code><code class="p">(</code><code class="n">decision</code><code class="p">)</code><code class="p">}</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python </code><code>
</code><code class="k">def</code><code> </code><code class="nf">single_sim_fun</code><code class="p">(</code><code class="n">Nexp</code><code class="p">,</code><code> </code><code class="n">dat_df</code><code class="p">,</code><code> </code><code class="n">metric_fun</code><code class="p">,</code><code> </code><code class="n">eff_size</code><code class="p">,</code><code> </code><code class="n">B</code><code> </code><code class="o">=</code><code> </code><code class="mi">100</code><code class="p">,</code><code>
</code><code>                   </code><code class="n">conf_level</code><code> </code><code class="o">=</code><code> </code><code class="mf">0.9</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code>
</code><code>    </code><code class="c1">#Adding predicted probability of booking </code><img src="Images/1.png" alt="1" width="12" height="12"/><code class="c1">     </code><code>
</code><code>    </code><code class="n">hist_model</code><code> </code><code class="o">=</code><code> </code><code class="n">smf</code><code class="o">.</code><code class="n">logit</code><code class="p">(</code><code class="s1">'</code><code class="s1">booked ~ age + gender + period</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">dat_df</code><code class="p">)</code><code>
</code><code>    </code><code class="n">res</code><code> </code><code class="o">=</code><code> </code><code class="n">hist_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">disp</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code>
</code><code>    </code><code class="n">sim_data_df</code><code> </code><code class="o">=</code><code> </code><code class="n">dat_df</code><code class="o">.</code><code class="n">copy</code><code class="p">(</code><code class="p">)</code><code>
</code><code>    </code><code class="n">sim_data_df</code><code class="p">[</code><code class="s1">'</code><code class="s1">pred_prob_bkg</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">res</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="p">)</code><code>
</code><code>    </code><code class="c1">#Filtering down to desired sample size </code><img src="Images/2.png" alt="2" width="12" height="12"/><code class="c1">  </code><code>
</code><code>    </code><code class="n">sim_data_df</code><code> </code><code class="o">=</code><code> </code><code class="n">sim_data_df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">Nexp</code><code class="p">)</code><code>
</code><code>    </code><code class="c1">#Random assignment of experimental groups </code><img src="Images/3.png" alt="3" width="12" height="12"/><code class="c1">            </code><code>
</code><code>    </code><code class="n">sim_data_df</code><code class="p">[</code><code class="s1">'</code><code class="s1">oneclick</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">size</code><code class="o">=</code><code class="n">Nexp</code><code class="p">)</code><code> </code><code class="o">&lt;</code><code class="o">=</code><code> </code><code class="mf">0.5</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">)</code><code>
</code><code>    </code><code class="c1"># Adding effect to treatment group </code><img src="Images/4.png" alt="4" width="12" height="12"/><code class="c1">                     </code><code>
</code><code>    </code><code class="n">sim_data_df</code><code class="p">[</code><code class="s1">'</code><code class="s1">pred_prob_bkg</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">sim_data_df</code><code class="o">.</code><code class="n">oneclick</code><code> </code><code class="o">==</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code>
</code><code>                                            </code><code class="n">sim_data_df</code><code class="o">.</code><code class="n">pred_prob_bkg</code><code> </code><code class="o">+</code><code> </code><code class="n">eff_size</code><code class="p">,</code><code> </code><code>
</code><code>                                            </code><code class="n">sim_data_df</code><code class="o">.</code><code class="n">pred_prob_bkg</code><code class="p">)</code><code>
</code><code>    </code><code class="n">sim_data_df</code><code class="p">[</code><code class="s1">'</code><code class="s1">booked</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">sim_data_df</code><code class="o">.</code><code class="n">pred_prob_bkg</code><code> </code><code class="o">&gt;</code><code class="o">=</code><code> </code><code>\
</code><code>                                     </code><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">size</code><code class="o">=</code><code class="n">Nexp</code><code class="p">)</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">)</code><code>
</code><code>    </code><code>
</code><code>    </code><code class="c1">#Calculate the decision (we want it to be 1) </code><img src="Images/5.png" alt="5" width="12" height="12"/><code class="c1">                   </code><code>
</code><code>    </code><code class="n">decision</code><code> </code><code class="o">=</code><code> </code><code class="n">decision_fun</code><code class="p">(</code><code class="n">sim_data_df</code><code class="p">,</code><code> </code><code class="n">metric_fun</code><code> </code><code class="o">=</code><code> </code><code class="n">metric_fun</code><code class="p">,</code><code> </code><code class="n">B</code><code> </code><code class="o">=</code><code> </code><code class="n">B</code><code class="p">,</code><code> </code><code>
</code><code>                            </code><code class="n">conf_level</code><code> </code><code class="o">=</code><code> </code><code class="n">conf_level</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">decision</code></pre>

<dl class="calloutlist">
 <dt><a class="co" id="c018" href="#comarker81"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
  <dd><p>Add the predicted probability of booking to the data.</p></dd> 
 <dt><a class="co" id="c028" href="#comarker82"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
  <dd><p>Filter down to the desired sample size.</p></dd> 
 <dt><a class="co" id="c038" href="#comarker83"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
  <dd><p>Assign experimental groups.</p></dd> 
 <dt><a class="co" id="c048" href="#comarker84"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
  <dd><p>Add effect to the treatment group.</p></dd>  
 <dt><a class="co" id="c058" href="#comarker85"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
  <dd><p>Apply the decision function and return its output.</p></dd> 
</dl>

<p>We can then write our power function for a certain effect size and sample size. This function repeatedly generates experimental data sets and then applies our decision function to them; it returns the fraction of them for which we would implement the button:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="n">power_sim_fun</code> <code class="o">&lt;-</code> <code class="nf">function</code><code class="p">(</code><code class="n">dat</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">,</code> <code class="n">Nexp</code><code class="p">,</code> <code class="n">eff_size</code><code class="p">,</code> <code class="n">Nsim</code><code class="p">,</code> 
                          <code class="n">B</code> <code class="o">=</code> <code class="m">100</code><code class="p">,</code> <code class="n">conf.level</code> <code class="o">=</code> <code class="m">0.9</code><code class="p">){</code>
  <code class="n">power_list</code> <code class="o">&lt;-</code> <code class="nf">vector</code><code class="p">(</code><code class="n">mode</code> <code class="o">=</code> <code class="s">"list"</code><code class="p">,</code> <code class="n">length</code> <code class="o">=</code> <code class="n">Nsim</code><code class="p">)</code>
  <code class="nf">for</code><code class="p">(</code><code class="n">i</code> <code class="n">in</code> <code class="m">1</code><code class="o">:</code><code class="n">Nsim</code><code class="p">){</code>
    <code class="n">power_list</code><code class="p">[[</code><code class="n">i</code><code class="p">]]</code> <code class="o">&lt;-</code> <code class="nf">single_sim_fun</code><code class="p">(</code><code class="n">dat</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">,</code> <code class="n">Nexp</code><code class="p">,</code> <code class="n">eff_size</code><code class="p">,</code> 
                                      <code class="n">B</code> <code class="o">=</code> <code class="n">B</code><code class="p">,</code> <code class="n">conf.level</code> <code class="o">=</code> <code class="n">conf.level</code><code class="p">)}</code>
  <code class="n">power</code> <code class="o">&lt;-</code> <code class="nf">mean</code><code class="p">(</code><code class="nf">unlist</code><code class="p">(</code><code class="n">power_list</code><code class="p">))</code>
  <code class="nf">return</code><code class="p">(</code><code class="n">power</code><code class="p">)}</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="k">def</code> <code class="nf">power_sim_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="p">,</code> <code class="n">metric_fun</code><code class="p">,</code> <code class="n">Nexp</code><code class="p">,</code> <code class="n">eff_size</code><code class="p">,</code> <code class="n">Nsim</code><code class="p">,</code> <code class="n">B</code> <code class="o">=</code> <code class="mi">100</code><code class="p">,</code> 
                  <code class="n">conf_level</code> <code class="o">=</code> <code class="mf">0.9</code><code class="p">):</code>
    <code class="n">power_lst</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">Nsim</code><code class="p">):</code>
        <code class="k">print</code><code class="p">(</code><code class="s2">"starting simulation number"</code><code class="p">,</code> <code class="n">i</code><code class="p">,</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code>
        <code class="n">power_lst</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">single_sim_fun</code><code class="p">(</code><code class="n">Nexp</code> <code class="o">=</code> <code class="n">Nexp</code><code class="p">,</code> <code class="n">dat_df</code> <code class="o">=</code> <code class="n">dat_df</code><code class="p">,</code> 
                                        <code class="n">metric_fun</code> <code class="o">=</code> <code class="n">metric_fun</code><code class="p">,</code> 
                                        <code class="n">eff_size</code> <code class="o">=</code> <code class="n">eff_size</code><code class="p">,</code> <code class="n">B</code> <code class="o">=</code> <code class="n">B</code><code class="p">,</code> 
                                        <code class="n">conf_level</code> <code class="o">=</code> <code class="n">conf_level</code><code class="p">))</code>
    <code class="n">power</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">power_lst</code><code class="p">)</code>
    <code class="k">return</code><code class="p">(</code><code class="n">power</code><code class="p">)</code></pre>
<p>How many data sets should you simulate? Twenty is a good starting point; it will give you a noisy estimate, but if you get a power of 0 or 1, you’ll know you have to adjust your sample size:</p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python (output not shown)</code>
<code class="n">power_sim_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="o">=</code><code class="n">hist_data_df</code><code class="p">,</code> <code class="n">metric_fun</code> <code class="o">=</code> <code class="n">log_reg_fun</code><code class="p">,</code> <code class="n">Nexp</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="mf">4e4</code><code class="p">),</code> 
              <code class="n">eff_size</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">Nsim</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code></pre>
<pre data-type="programlisting" data-code-language="r">
<code class="c1">## R</code>
<code class="o">&gt;</code> <code class="nf">set.seed</code><code class="p">(</code><code class="m">1234</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">power_sim_fun</code><code class="p">(</code><code class="n">dat</code><code class="o">=</code><code class="n">hist_data</code><code class="p">,</code> <code class="n">effect_size</code><code class="o">=</code><code class="m">0.01</code><code class="p">,</code> <code class="n">Nexp</code><code class="o">=</code><code class="m">4e4</code><code class="p">,</code> <code class="n">Nsim</code><code class="o">=</code><code class="m">20</code><code class="p">)</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.9</code></pre>
<p>This first estimate is 90% power; like I said, traditional formulas give you a reasonable ballpark to start your simulations. I then run the power simulation function with 30,000 and 50,000 rows for 100 simulations each, and finally 35,000 and 45,000 rows for 200 simulations each. Basically, as you get a narrower and narrower interval of sample sizes, you want to improve accuracy by increasing the number of simulations. <a data-type="xref" href="#power_simulations_for_various_sample_si">Figure 8-6</a> shows the results of my successive iterations.</p>
<figure><div id="power_simulations_for_various_sample_si" class="figure">
<img src="Images/BEDA_0806.png" alt="Power simulations for various sample sizes" width="1879" height="1158"/>
<h6><span class="label">Figure 8-6. </span>Power simulations for various sample sizes</h6>
</div></figure>
<p>As announced earlier, we get a power of 0.9 at about 40,000. We could keep running simulations if we needed to get more precise (e.g., should we get a sample size of 38,000? 41,000?) but that’s good enough for this example.</p>
<p>Now that we’ve determined our sample size, the last thing I like to do is plot the power curve for a few effect sizes at that sample size. This gives us a better sense of how likely overall we are to get a positive result, assuming the actual effect size is positive. It also allows you to better convey to your business partners that the power of your experiment is not just defined for one effect size. Here, we can see how the power of the experiment increases going from a 0.5% effect to a 2% effect (<a data-type="xref" href="#power_simulations_for_various_effect_si">Figure 8-7</a>).</p>
<figure><div id="power_simulations_for_various_effect_si" class="figure">
<img src="Images/BEDA_0807.png" alt="Power simulations for various effect sizes at N = 40,000, with 200 simulations per effect size, dashed line at power = 0.9" width="1898" height="1166"/>
<h6><span class="label">Figure 8-7. </span>Power simulations for various effect sizes at N = 40,000, with 200 simulations per effect size, dashed line at power = 0.9</h6>
</div></figure>
<p>With 200 simulations per effect size, the estimated values for power should be pretty accurate, although still not perfect, as illustrated by the lack of smoothness of the curve. In other words, the fact that we see a power of 1 for an effect size of 2% doesn’t mean that we literally have 100% power for that effect size, but very close to it.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Reminder: the simulated statistical significance of a Bootstrap CI should be pretty close to a normal CI if your variables are reasonably smoothly and normally distributed. For weirder data (multiple peaks, fat tails, etc.), this may not hold anymore, and you should definitely check that your simulated statistical significance is not widely off.</p>
</div>
<p>If you’d like, you can also run a simulation with an effect size of zero. This will give you the empirical statistical significance of your analysis. Because we’re using Bootstrap 90%-CIs, about 5% of these simulations should end up with the decision to (wrongly) implement the 1-click button, and this is what we observe here.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>These are not big data simulations by any means, but they take long enough (the longest one took about half an hour on my laptop) that you’ll want to improve the performance of your functions, get your code running while you do something else, or both. The <a href="https://oreil.ly/BehavioralDataAnalysis">code on GitHub</a> contains functions that I have optimized using the <code>Rfast</code> and <code>doParallel</code> packages in R and the <code>joblib</code> and <code>psutil</code> packages in Python.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-saed" id="idm45968151712088"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-saed2" id="idm45968151710680"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-btpow" id="idm45968151709304"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-btpow2" id="idm45968151707928"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-btpow3" id="idm45968151706552"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-btpow4" id="idm45968151705176"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-bspa" id="idm45968151703800"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-bspa2" id="idm45968151702424"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-bspa3" id="idm45968151701048"/></p>
</div>
</div></section>
</div></section>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Analyzing and Interpreting Experimental Results"><div class="sect1" id="analyzing_and_interpreting_experimenta">
<h1>Analyzing and Interpreting Experimental Results</h1>
<p>After running the experiment and collecting the corresponding<a contenteditable="false" data-type="indexterm" data-primary="random assignment" data-secondary="analyzing experimental results" id="ch08-oaai"/><a contenteditable="false" data-type="indexterm" data-primary="experimental design" data-secondary="random assignment" data-tertiary="analyzing experimental results" id="ch08-oaai2"/> data, you can analyze them. After all the simulated analyses you ran for the power estimation, the final analyses themselves should be a walk in the park. We run a logistic regression and determine the corresponding Bootstrap 90%-CI. Because of the random assignment, we know that our coefficient for the 1-click button is unconfounded—we don’t need to control for any confounder. However, by adding other variables that are also causes of the booking probability, we can reduce noise and significantly improve the precision of our estimation:</p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python code (output not shown)</code>
<code class="kn">import</code> <code class="nn">statsmodels.formula.api</code> <code class="kn">as</code> <code class="nn">smf</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">smf</code><code class="o">.</code><code class="n">logit</code><code class="p">(</code><code class="s1">'booked ~ age + gender + oneclick'</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">exp_data_df</code><code class="p">)</code>
<code class="n">res</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
<code class="n">res</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code></pre>
<pre data-type="programlisting" data-code-language="r">
<code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">log_mod_exp</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="n">booked</code> <code class="o">~</code> <code class="n">oneclick</code> <code class="o">+</code> <code class="n">age</code> <code class="o">+</code> <code class="n">gender</code><code class="p">,</code> 
                     <code class="n">data</code> <code class="o">=</code> <code class="n">exp_data</code><code class="p">,</code> <code class="n">family</code> <code class="o">=</code> <code class="nf">binomial</code><code class="p">(</code><code class="n">link</code> <code class="o">=</code> <code class="s">"logit"</code><code class="p">))</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="n">log_mod_exp</code><code class="p">)</code>

<code class="kc">...</code>
<code class="n">Coefficients</code><code class="o">:</code>
             <code class="n">Estimate</code> <code class="n">Std.</code> <code class="n">Error</code> <code class="n">z</code> <code class="n">value</code>    <code class="nf">Pr</code><code class="p">(</code><code class="o">&gt;|</code><code class="n">z</code><code class="o">|</code><code class="p">)</code>    
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>  <code class="m">11.94701</code>    <code class="m">0.22601</code>  <code class="m">52.861</code>     <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">oneclick1</code>     <code class="m">0.15784</code>    <code class="m">0.04702</code>   <code class="m">3.357</code>    <code class="m">0.000789</code> <code class="o">***</code>
<code class="n">age</code>          <code class="m">-0.39406</code>    <code class="m">0.00643</code> <code class="m">-61.282</code>     <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">genderfemale</code> <code class="m">-0.25420</code>    <code class="m">0.04905</code>  <code class="m">-5.182</code> <code class="m">0.000000219</code> <code class="o">***</code>
<code class="kc">...</code></pre>
<p>The coefficient for the 1-click button is equal to 0.15784, and the Bootstrap 90%-CI for it is approximately [0.073; 0.250]. Based on our decision rule, we would go ahead and implement the 1-click button.</p>
<p>The coefficients from a logistic regression are not directly interpretable, and I find that the recommended solution of using the odds ratio only helps marginally (in particular when you have moderation effects). My preferred rule of thumb is to generate two copies of the experimental data, one where the variable for the 1-click button is set to 1 for everyone, and the other where it is set to 0. By comparing the probability of booking predicted by our logistic model for these two data sets, we can calculate an “average” effect that is very close to the effect you would observe if implementing the treatment for everyone. It’s unscientific but useful:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R (output not shown)</code><code>
</code><code class="o">&gt;</code><code> </code><code class="n">diff_prob_fun</code><code> </code><code class="o">&lt;-</code><code> </code><code class="nf">function</code><code class="p">(</code><code class="n">dat</code><code class="p">,</code><code> </code><code class="n">reg_model</code><code> </code><code class="o">=</code><code> </code><code class="n">log_mod_exp</code><code class="p">)</code><code class="p">{</code><code>
    </code><code class="n">no_button</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">dat</code><code> </code><code class="o">%&gt;%</code><code> </code><a class="co" id="comarker881" href="ch11.xhtml#c01"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>                                  
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code><code> </code><code class="o">=</code><code> </code><code class="m">0</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>                                                      
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code><code> </code><code class="o">=</code><code> </code><code class="nf">factor</code><code class="p">(</code><code class="n">oneclick</code><code class="p">,</code><code> </code><code class="n">levels</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code><code> </code><code class="m">1</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="nf">select</code><code class="p">(</code><code class="n">age</code><code class="p">,</code><code> </code><code class="n">gender</code><code class="p">,</code><code> </code><code class="n">oneclick</code><code class="p">)</code><code>
    </code><code class="n">button</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">dat</code><code> </code><code class="o">%&gt;%</code><code> </code><a class="co" id="comarker882" href="ch11.xhtml#c02"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>                                        
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code><code> </code><code class="o">=</code><code> </code><code class="m">1</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code> 
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">oneclick</code><code> </code><code class="o">=</code><code> </code><code class="nf">factor</code><code class="p">(</code><code class="n">oneclick</code><code class="p">,</code><code> </code><code class="n">levels</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code><code> </code><code class="m">1</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="nf">select</code><code class="p">(</code><code class="n">age</code><code class="p">,</code><code> </code><code class="n">gender</code><code class="p">,</code><code> </code><code class="n">oneclick</code><code class="p">)</code><code>
    </code><code class="c1">#Adding the predictions of the model </code><code>
    </code><code class="n">no_button</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">no_button</code><code> </code><code class="o">%&gt;%</code><code> </code><a class="co" id="comarker883" href="ch11.xhtml#c03"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>                           
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">pred_mod</code><code> </code><code class="o">=</code><code> </code><code class="nf">predict</code><code class="p">(</code><code class="n">object</code><code class="o">=</code><code class="n">reg_model</code><code class="p">,</code><code> </code><code class="n">newdata</code><code> </code><code class="o">=</code><code> </code><code class="n">no_button</code><code class="p">,</code><code> 
                                </code><code class="n">type</code><code class="o">=</code><code class="s">"</code><code class="s">response"</code><code class="p">)</code><code class="p">)</code><code>
    </code><code class="n">button</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">button</code><code> </code><code class="o">%&gt;%</code><code>
      </code><code class="nf">mutate</code><code class="p">(</code><code class="n">pred_mod</code><code> </code><code class="o">=</code><code> </code><code class="nf">predict</code><code class="p">(</code><code class="n">object</code><code class="o">=</code><code class="n">reg_model</code><code class="p">,</code><code> </code><code class="n">newdata</code><code> </code><code class="o">=</code><code> </code><code class="n">button</code><code class="p">,</code><code> 
                                </code><code class="n">type</code><code class="o">=</code><code class="s">"</code><code class="s">response"</code><code class="p">)</code><code class="p">)</code><code>
    </code><code class="c1">#Calculating average difference in probabilities</code><code>
    </code><code class="n">diff</code><code> </code><code class="o">&lt;-</code><code> </code><code class="n">button</code><code class="o">$</code><code class="n">pred_mod</code><code> </code><code class="o">-</code><code> </code><code class="n">no_button</code><code class="o">$</code><code class="n">pred_mod</code><code> </code><a class="co" id="comarker884" href="ch11.xhtml#c04"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>   
    </code><code class="nf">return</code><code class="p">(</code><code class="nf">mean</code><code class="p">(</code><code class="n">diff</code><code class="p">)</code><code class="p">)</code><code class="p">}</code><code>
</code><code class="o">&gt;</code><code> </code><code class="nf">diff_prob_fun</code><code class="p">(</code><code class="n">exp_data</code><code class="p">,</code><code> </code><code class="n">reg_model</code><code> </code><code class="o">=</code><code> </code><code class="n">log_mod_exp</code><code class="p">)</code></pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code><code>
</code><code class="k">def</code><code> </code><code class="nf">diff_prob_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="p">,</code><code> </code><code class="n">reg_model</code><code> </code><code class="o">=</code><code> </code><code class="n">log_mod_exp</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code>
</code><code>    </code><code class="c1">#Creating new copies of data</code><code>
</code><code>    </code><code class="n">no_button_df</code><code> </code><code class="o">=</code><code> </code><code class="n">dat_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">age</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">gender</code><code class="s1">'</code><code class="p">]</code><code> </code><img src="Images/1.png" alt="1" width="12" height="12"/><code>
</code><code>    </code><code class="n">no_button_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">oneclick</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>    </code><code class="n">button_df</code><code> </code><code class="o">=</code><code> </code><code class="n">dat_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code class="s1">'</code><code class="s1">age</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">gender</code><code class="s1">'</code><code class="p">]</code><code> </code><img src="Images/2.png" alt="2" width="12" height="12"/><code>                       </code><code>
</code><code>    </code><code class="n">button_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">oneclick</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="mi">1</code><code>
</code><code>    </code><code>
</code><code>    </code><code class="c1">#Adding the predictions of the model </code><code>
</code><code>    </code><code class="n">no_button_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">pred_bkg_rate</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">res</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">no_button_df</code><code class="p">)</code><code> </code><img src="Images/3.png" alt="3" width="12" height="12"/><code>
</code><code>    </code><code class="n">button_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">pred_bkg_rate</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">res</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">button_df</code><code class="p">)</code><code>
</code><code>    </code><code>
</code><code>    </code><code class="n">diff</code><code> </code><code class="o">=</code><code> </code><code class="n">button_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code class="s1">'</code><code class="s1">pred_bkg_rate</code><code class="s1">'</code><code class="p">]</code><code> </code><code>\</code><code>  </code><img src="Images/4.png" alt="4" width="12" height="12"/><code>  </code><code>
</code><code>    </code><code class="o">-</code><code> </code><code class="n">no_button_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code class="s1">'</code><code class="s1">pred_bkg_rate</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">diff</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code>
</code><code class="n">diff_prob_fun</code><code class="p">(</code><code class="n">exp_data_df</code><code class="p">,</code><code> </code><code class="n">reg_model</code><code> </code><code class="o">=</code><code> </code><code class="n">log_mod_exp</code><code class="p">)</code><code>
</code><code class="mf">0.007129714313551981</code></pre>

<dl class="calloutlist">
 <dt><a class="co" id="c0188" href="#comarker881"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
  <dd><p>We create a data set called <code>no_button</code> for which we set the variable <code>oneclick</code> to zero for all rows (and convert it to factor so that the predict function later will work).</p></dd> 
 <dt><a class="co" id="c0288" href="#comarker882"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
  <dd><p>We create a data set called <code>button</code> for which we set the variable <code>oneclick</code> to one for all rows.</p></dd> 
 <dt><a class="co" id="c0388" href="#comarker883"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
  <dd><p>We calculate the predicted probability of booking in each case by using the <code>predict()</code> function with our model <code>log_mod_exp</code>.</p></dd> 
 <dt><a class="co" id="c0488" href="#comarker884"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
  <dd><p>We calculate the difference between the predicted probabilities.</p></dd>  
</dl>

<p>We can see that our average effect across our experimental population is about 0.712pp, positive but lower than our target of 1pp. As usual, let’s build the Bootstrap 90%-CI, which is approximately [0.705pp; 0.721pp]. This interval is very narrow and does not cross zero. Therefore, we can treat our result as empirically statistically significant at the 5% level. In this case, we can even get much more confident than that: the 99.8%-CI is approximately [0.697pp; 0.728pp], still far from zero, so we can treat our result as significant at the (1 − 0.998) / 2 = 0.1% level.</p>
<p>To cover all cases, let’s recap our decision rule (<a data-type="xref" href="#decision_rule_for_one_click_booking_but">Table 8-4</a>). This way, you’ll see what to do depending on whether the observed estimated effect is statistically significant or not and economically significant (taken here to mean a 1pp increase) or not. In the present case, I would implement the button, given that it has a strictly positive effect, and the cost of implementation is low.</p>
<table class="border" id="decision_rule_for_one_click_booking_but">
<caption><span class="label">Table 8-4. </span>Decision rule for 1-click booking button</caption>
<thead>
<tr>
<th/>
<th/>
<th colspan="3">Observed estimated effect</th>
</tr>
</thead>
<tbody>
<tr>
<td/>
<td/>
<td><strong>estimated effect &lt;= 0</strong></td>
<td><strong>0 &lt; estimated effect &lt; 1pp</strong></td>
<td><strong>1pp &lt;= estimated effect</strong></td>
</tr>
<tr>
<td><strong>Empirical statistical significance of observed results</strong></td>
<td><strong>“High” (the Bootstrap CI for 90% or higher doesn’t cross 0)</strong></td>
<td>Don’t implement button</td>
<td>Implement button or not, depending on estimated effect size, costs, and risk appetite <strong>(our case here)</strong></td>
<td>Implement button</td>
</tr>
<tr>
<td/>
<td><strong>“Low” (the Bootstrap CI for 90% crosses 0)</strong></td>
<td>Don’t implement button</td>
<td>Don’t implement button</td>
<td>Implement button or run new test, depending on confidence interval and risk appetite</td>
</tr>
</tbody>
</table>
<p>The last thing I will note is that the average effect across our experimental population, 0.712pp, is pretty far from the straightforward difference between our control group and treatment group, which is about 0.337pp. This is due to random differences between our two experimental groups. The average age in our control group is 40.63 years versus 40.78 in the treatment group. The proportion of men is also a bit higher in the treatment group. With very small effect sizes, these minute differences are enough to muddy a direct comparison of the two groups: our sample size is large enough that our two groups are identical within about 0.3pp, which is pretty close in absolute terms, but that’s about half of our experimental effect.</p>
<p>Unfortunately, there’s nothing we can do about that in this experiment, where customers are allocated randomly to the two groups as they come. But if we know our entire experimental sample at the beginning of the experiment, we can do significantly better by ensuring that the control group and the experimental group are as identical as possible through stratified randomization, as we’ll see in the next chapter.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-oaai" id="idm45968150992616"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch08-oaai2" id="idm45968150991400"/></p>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="conclusion-id00013">
<h1>Conclusion</h1>
<p>In this chapter, we saw how to design the simplest form of experiment, an online A/B test with straightforward randomization. I emphasized that a well-designed experiment is much more than just throwing out random different versions of a website or an email to customers. You need to determine your business goal and target metric, and then articulate how your intervention is connected to them through behavioral logic. Taken together, your business goal, target metric, intervention, and behavioral logic constitute the theory of change of your experiment.</p>
<p>We then turned to the quantitative aspects of experimental design. In this first chapter on experimentation, the random assignment was extremely simple, and I spent more time on the power analysis and sample size calculations. While there are statistical formulas available, I prefer to use regressions rather than statistical tests as analysis tools, and Bootstrap confidence intervals rather than p-values as a measure of the uncertainty around our estimated coefficient, which leads to using power simulations instead of formulas. In this case, the results of the two are almost identical, but in the next two chapters we’ll get into more complex designs where there are no formulas available.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="ch01fn13"><sup><a href="ch08.xhtml#ch01fn13-marker">1</a></sup> This is why pharmaceutical trials, as well as an increasing number of social science experiments, are preregistered. You can’t set out to test a drug for a heart condition and then decide after the fact that it is an effective drug against hair loss because the patients in the treatment group have seen their hairline move forward; you’d need to run a second experiment, itself preregistered, for the purpose of testing the new hypothesized effect.</p><p data-type="footnote" id="ch01fn14"><sup><a href="ch08.xhtml#ch01fn14-marker">2</a></sup> Wendel (2020) is a great resource to understand the obstacles and drivers of a behavior and build a strong behavioral logic.</p><p data-type="footnote" id="ch01fn15"><sup><a href="ch08.xhtml#ch01fn15-marker">3</a></sup> In case you’re wondering, you would need to use <a contenteditable="false" data-type="indexterm" data-primary="Downey, Allen" id="idm45968153878808"/><a contenteditable="false" data-type="indexterm" data-primary="Think Bayes (Downey)" id="idm45968153877704"/>Bayesian methods. Maybe I’ll get to that in the next edition of this book! In the meantime, <a class="orm:hideurl" href="https://www.oreilly.com/library/view/think-bayes/9781491945407/"><em>Think Bayes</em></a> (O’Reilly) by Allen Downey is one of the most approachable introductions I know to the topic.</p><p data-type="footnote" id="ch01fn16"><sup><a href="ch08.xhtml#ch01fn16-marker">4</a></sup> See Aberson (2019).</p><p data-type="footnote" id="ch01fn17"><sup><a href="ch08.xhtml#ch01fn17-marker">5</a></sup> Hubbard (2010) is a good resource if you want to think more about how to design useful measurements in business, even with very limited information.</p></div></div></section></div></body></html>