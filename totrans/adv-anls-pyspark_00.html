<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Preface" data-type="preface" epub:type="preface"><div class="preface" id="idm46507996832016">
<h1>Preface</h1>
<p>Apache Spark’s long lineage of predecessors,<a data-primary="Spark (Apache)" data-secondary="about" data-type="indexterm" id="idm46508000959040"/><a data-primary="Apache Spark" data-see="Spark" data-type="indexterm" id="idm46508005457008"/> from MPI (message passing interface) to MapReduce, made it possible to write programs that take advantage of massive resources while abstracting away the nitty-gritty details of distributed systems.  As much as data processing needs have motivated the development of these frameworks, in a way the field of big data has become so related to them that its scope is defined by what these frameworks can handle. Spark’s original promise was to take this a little further—to make writing distributed programs feel like writing regular programs.</p>
<p>The rise in Spark’s popularity coincided<a data-primary="big data" data-secondary="PyData ecosystem" data-type="indexterm" id="idm46508001079840"/><a data-primary="PyData ecosystem" data-secondary="PySpark API" data-type="indexterm" id="idm46508005273744"/><a data-primary="PySpark API" data-secondary="about" data-tertiary="PyData ecosystem" data-type="indexterm" id="idm46507996808864"/><a data-primary="Python" data-secondary="about PyData ecosystem" data-type="indexterm" id="idm46507992086192"/> with that of the Python data (PyData) ecosystem. So it makes sense that Spark’s Python API—PySpark—has significantly grown in popularity over the last few years. Although the PyData ecosystem has recently sprung up some distributed programming options, Apache Spark remains one of the most popular choices for working with large datasets across industries and domains. Thanks to recent efforts to integrate PySpark with the other PyData tools, learning the framework can help you boost your productivity significantly as a data science practitioner.</p>
<p>We think that the best way to teach data science is by example.  To that end, we have put together a book of applications, trying to touch on the interactions between the most common algorithms, datasets, and design patterns in large-scale analytics.  This book isn’t meant to be read cover to cover: page to a chapter that looks like something you’re trying to accomplish, or that simply ignites your interest, and start there.</p>
<section data-pdf-bookmark="Why Did We Write This Book Now?" data-type="sect1"><div class="sect1" id="idm46507993938704">
<h1>Why Did We Write This Book Now?</h1>
<p>Apache Spark experienced a major version<a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="version 3 features" data-type="indexterm" id="idm46508000990032"/><a data-primary="adaptive execution in Spark" data-type="indexterm" id="idm46507996805744"/> upgrade in 2020—version 3.0. One of the biggest improvements was the introduction of Spark Adaptive Execution. This feature takes away a big portion of the complexity around tuning and optimization. We do not refer to it in the book because it’s turned on by default in Spark 3.2 and later versions, and so you automatically get the benefits.</p>
<p>The ecosystem changes, combined with Spark’s latest major release, make this edition a timely one. Unlike previous editions of <em>Advanced Analytics with Spark</em>, which chose Scala, we will use Python. We’ll cover best practices and integrate with the wider Python data science ecosystem when appropriate. All chapters have been updated to use the latest PySpark API. Two new chapters have been added and multiple chapters have undergone major rewrites. We will not cover Spark’s streaming and graph libraries. With Spark in a new era of maturity and stability, we hope that these changes will preserve the book as a useful resource on analytics for years to come.</p>
</div></section>
<section data-pdf-bookmark="How This Book Is Organized" data-type="sect1"><div class="sect1" id="idm46507998094400">
<h1>How This Book Is Organized</h1>
<p><a data-type="xref" href="ch01.xhtml#Introduction">Chapter 1</a> places Spark and PySpark within the wider context of data science and big data analytics. After that, each chapter comprises a self-contained analysis using PySpark. <a data-type="xref" href="ch02.xhtml#introduction_to_data_anlysis_with_pyspark">Chapter 2</a> introduces the basics of data processing in PySpark and Python through a use case in data cleansing. The next few chapters delve into the meat and potatoes of machine learning with Spark, applying some of the most common algorithms in canonical applications. The remaining chapters are a bit more of a grab bag and apply Spark in slightly more exotic applications—for example, querying Wikipedia through latent semantic relationships in the text, analyzing genomics data, and identifying similar images.</p>
<p>This book is not about PySpark’s merits and disadvantages. There are a few other things that it is not about either. It introduces the Spark programming model and basics of Spark’s Python API, PySpark. However, it does not attempt to be a Spark reference or provide a comprehensive guide to all Spark’s nooks and crannies. It does not try to be a machine learning, statistics, or linear algebra reference, although many of the chapters provide some background on these before using them.</p>
<p>Instead, this book will help the reader get a feel for what it’s like to use PySpark for complex analytics on large datasets by covering the entire pipeline: not just building and evaluating models, but also cleansing, preprocessing, and exploring data, with attention paid to turning results into production applications. We believe that the best way to teach this is by example.</p>
<p>Here are examples of some tasks that will be tackled in this book:</p>
<dl>
<dt>Predicting forest cover</dt>
<dd>
<p>We predict type of forest cover using relevant features like location and soil type by using decision trees (see <a data-type="xref" href="ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests">Chapter 4</a>).</p>
</dd>
<dt>Querying Wikipedia for similar entries</dt>
<dd>
<p>We identify relationships between entries and query the Wikipedia corpus by using NLP (natural language processing) techniques (see <a data-type="xref" href="ch06.xhtml#understanding_wikipedia_with_lda_and_spark_nlp">Chapter 6</a>).</p>
</dd>
<dt>Understanding utilization of New York cabs</dt>
<dd>
<p>We compute average taxi waiting time as a function of location by performing temporal and geospatial analysis (see <a data-type="xref" href="ch07.xhtml#geospatial_and_temporal_data_analysis_on_taxi_trip_data">Chapter 7</a>).</p>
</dd>
<dt>Reduce risk for an investment portfolio</dt>
<dd>
<p>We estimate financial risk for an investment portfolio using the Monte Carlo simulation (see <a data-type="xref" href="ch09.xhtml#analyzing_genomics_data_and_and_the_bdg_project">Chapter 9</a>).</p>
</dd>
</dl>
<p>When possible, we attempt not to just provide a “solution,” but to demonstrate the full data science workflow, with all of its iterations, dead ends, and restarts. This book will be useful for getting more comfortable with Python, Spark, and machine learning and data analysis. However, these are in service of a larger goal, and we hope that most of all this book will teach you how to approach tasks like those described earlier. Each chapter, in about 20 measly pages, will try to get as close as possible to demonstrating how to build one piece of these data applications.</p>
</div></section>
<section data-pdf-bookmark="Conventions Used in This Book" data-type="sect1"><div class="sect1" id="idm46507996277392">
<h1>Conventions Used in This Book</h1>
<p>The following typographical conventions are used in this book:</p>
<dl>
<dt><em>Italic</em></dt>
<dd>
<p>Indicates new terms, URLs, email addresses, filenames, and file extensions.</p>
</dd>
<dt><code>Constant width</code></dt>
<dd>
<p>Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.</p>
</dd>
<dt><strong><code>Constant width bold</code></strong></dt>
<dd>
<p>Shows commands or other text that should be typed literally by the user.</p>
</dd>
<dt><em><code>Constant width italic</code></em></dt>
<dd>
<p>Shows text that should be replaced with user-supplied values or by values determined by context.</p>
</dd>
</dl>
<div data-type="tip">
<p>This element signifies a tip or suggestion.</p>
</div>
<div data-type="note" epub:type="note">
<p>This element signifies a general note.</p>
</div>
<div data-type="warning" epub:type="warning">
<p>This element indicates a warning or caution.</p>
</div>
</div></section>
<section data-pdf-bookmark="Using Code Examples" data-type="sect1"><div class="sect1" id="idm46507992488384">
<h1>Using Code Examples</h1>
<!--PROD: Please reach out to author to find out if they will be uploading code examples to oreilly.com or their own site (e.g., GitHub). If there is no code download, delete this whole section. If there is, when you email digidist with the link, let them know what you filled in for title_title (should be as close to book title as possible, i.e., learning_python_2e). This info will determine where digidist loads the files.-->
<p>Supplemental material (code examples, exercises, etc.) is available for download at <a href="https://github.com/sryza/aas"><em class="hyperlink">https://github.com/sryza/aas</em></a>.</p>
<p>If you have a technical question or a problem using the code examples, please send email to <a class="email" href="mailto:bookquestions@oreilly.com"><em>bookquestions@oreilly.com</em></a>.</p>
<p>This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission.</p>
<p>We appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “<em>Advanced Analytics with PySpark</em> by Akash Tandon, Sandy Ryza, Uri Laserson, Sean Owen, and Josh Wills (O’Reilly). Copyright 2022 Akash Tandon, 978-1-098-10365-1.”</p>
<p>If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at <a class="email" href="mailto:permissions@oreilly.com"><em>permissions@oreilly.com</em></a>.</p>
</div></section>
<section data-pdf-bookmark="O’Reilly Online Learning" data-type="sect1"><div class="sect1" id="idm46507993066240">
<h1>O’Reilly Online Learning</h1>
<div class="ormenabled" data-type="note" epub:type="note">
<p>For more than 40 years, <a class="orm:hideurl" href="https://oreilly.com"><em class="hyperlink">O’Reilly Media</em></a> has provided technology and business training, knowledge, and insight to help companies succeed.</p>
</div>
<p>Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit <a class="orm:hideurl" href="https://oreilly.com"><em>https://oreilly.com</em></a>.</p>
</div></section>
<section data-pdf-bookmark="How to Contact Us" data-type="sect1"><div class="sect1" id="idm46507997956496">
<h1>How to Contact Us</h1>
<p>Please address comments and questions concerning this book to the publisher:</p>
<ul class="simplelist">
<li>O’Reilly Media, Inc.</li>
<li>1005 Gravenstein Highway North</li>
<li>Sebastopol, CA 95472</li>
<li>800-998-9938 (in the United States or Canada)</li>
<li>707-829-0515 (international or local)</li>
<li>707-829-0104 (fax)</li>
</ul>
<p>We have a web page <a data-primary="online resources" data-secondary="book web page" data-type="indexterm" id="idm46507993989760"/><a data-primary="resources online" data-see="online resources" data-type="indexterm" id="idm46507992324544"/>for this book, where we list errata, examples, and any additional information. You can access this page at <a href="https://oreil.ly/adv-analytics-pyspark"><em class="hyperlink">https://oreil.ly/adv-analytics-pyspark</em></a>.</p>
<!--Don't forget to update the link above.-->
<p>Email <a class="email" href="mailto:bookquestions@oreilly.com"><em>bookquestions@oreilly.com</em></a> to comment or ask technical questions about this book.</p>
<p>For news and information about our books and courses, visit <a href="https://oreilly.com"><em class="hyperlink">https://oreilly.com</em></a>.</p>
<p>Find us on LinkedIn: <a href="https://linkedin.com/company/oreilly-media"><em class="hyperlink">https://linkedin.com/company/oreilly-media</em></a></p>
<p>Follow us on Twitter: <a href="https://twitter.com/oreillymedia"><em class="hyperlink">https://twitter.com/oreillymedia</em></a></p>
<p>Watch us on YouTube: <a href="https://youtube.com/oreillymedia"><em class="hyperlink">https://youtube.com/oreillymedia</em></a></p>
</div></section>
<section data-pdf-bookmark="Acknowledgments" data-type="sect1"><div class="sect1" id="idm46507997955872">
<h1>Acknowledgments</h1>
<p>It goes without saying that you wouldn’t be reading this book if it were not for the existence of Apache Spark and MLlib. We all owe thanks to the team that has built and open sourced it and the hundreds of contributors who have added to it.</p>
<p>We would like to thank everyone who spent a great deal of time reviewing the content of the previous editions of the book with expert eyes: Michael Bernico, Adam Breindel, Ian Buss, Parviz Deyhim, Jeremy Freeman, Chris Fregly, Debashish Ghosh, Juliet Hougland, Jonathan Keebler, Nisha Muktewar, Frank Nothaft, Nick Pentreath, Kostas Sakellis, Tom White, Marcelo Vanzin, and Juliet Hougland again. Thanks all! We owe you one.
This has greatly improved the structure and quality of the result.</p>
<p>Sandy also would like to thank Jordan Pinkus and Richard Wang for helping with some of the theory behind the risk chapter.</p>
<p>Thanks to Jeff Bleiel and O’Reilly for the experience and great support in getting this book published and into your hands.</p>
</div></section>
</div></section></div></body></html>