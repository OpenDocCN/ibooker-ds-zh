<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 3. Recommending Music and the Audioscrobbler Dataset" data-type="chapter" epub:type="chapter"><div class="chapter" id="recommending_music_and_the_audioscrobbler_data_set">
<h1><span class="label">Chapter 3. </span>Recommending Music and the Audioscrobbler Dataset</h1>
<p>The recommender engine is one of the most popular example of large-scale machine learning;<a data-primary="recommender engines" data-secondary="about" data-type="indexterm" id="idm46507985141248"/><a data-primary="music recommender engine" data-secondary="about recommender engines" data-type="indexterm" id="idm46507985140400"/> for example, most people are familiar with Amazon’s. It is a common denominator because recommender engines are everywhere, from social networks to video sites to online retailers. We can also directly observe them in action. We’re aware that a computer is picking tracks to play on Spotify, in much the same way we don’t necessarily notice that Gmail is deciding whether inbound email is spam.</p>
<p>The output of a recommender is more intuitively understandable than other machine learning algorithms. It’s exciting, even. For as much as we think that musical taste is personal and inexplicable, recommenders do a surprisingly good job of identifying tracks we didn’t know we would like. For domains like music or movies, where recommenders are often deployed, it’s comparatively easy to reason why a recommended piece of music fits with someone’s listening history. Not all clustering or classification algorithms match that description. For example, a support vector machine classifier is a set of coefficients, and it’s hard even for practitioners to articulate what the numbers mean when they make predictions.</p>
<p>It seems fitting to kick off the next three chapters, which will explore key machine learning algorithms on PySpark, with a chapter built around recommender engines, and recommending music in particular. It’s an accessible way to introduce real-world use of PySpark and MLlib and some basic machine learning ideas that will be developed in subsequent chapters.</p>
<p>In this chapter, we’ll implement a recommender system in PySpark. Specifically, we will use the Alternating Least Squares (ALS) algorithm on an open dataset provided by a music streaming service. We’ll start off by understanding the dataset and importing it in PySpark. Then we’ll discuss our motivation for choosing the ALS algorithm and its implementation in PySpark. This will be followed by data preparation and building our model using PySpark. We’ll finish up by making some user recommendations and discussing ways to improve our model through hyperparameter 
<span class="keep-together">selection</span>.</p>
<section data-pdf-bookmark="Setting Up the Data" data-type="sect1"><div class="sect1" id="idm46507985137520">
<h1>Setting Up the Data</h1>
<p>We will use a dataset published by Audioscrobbler.<a data-primary="recommender engines" data-secondary="data setup" data-type="indexterm" id="ch03-data"/><a data-primary="music recommender engine" data-secondary="data setup" data-type="indexterm" id="ch03-data2"/><a data-primary="Audioscrobbler dataset" data-type="indexterm" id="ch03-data3"/><a data-primary="datasets" data-secondary="Audioscrobbler" data-type="indexterm" id="ch03-data4"/><a data-primary="Last.fm streaming radio site" data-type="indexterm" id="idm46507985131072"/><a data-primary="online resources" data-secondary="Last.fm streaming radio site" data-type="indexterm" id="idm46507985130432"/><a data-primary="datasets" data-secondary="Last.fm scrobbling dataset" data-type="indexterm" id="idm46507985129520"/> Audioscrobbler was the first music recommendation system for <a href="http://www.last.fm">Last.fm</a>, one of the first internet
streaming radio sites, founded in 2002. Audioscrobbler provided an open API for “scrobbling,” or recording listeners’ song plays. Last.fm used this information to build a powerful music recommender engine. The system reached millions of users because third-party apps and sites could provide listening data back to the recommender engine.</p>
<p>At that time, research on recommender engines was mostly confined to learning from rating-like data. That is, recommenders were usually viewed as tools that operated on input like “Bob rates Prince 3.5 stars.” The Audioscrobbler dataset is interesting because it merely records plays: “Bob played a Prince track.” A play carries less information than a rating. Just because Bob played the track doesn’t mean he actually liked it. You or I may occasionally play a song by an artist we don’t care for, or even play an album and walk out of the room.</p>
<p>However, listeners rate music far less frequently than they play music. A dataset like this is therefore much larger, covers more users and artists, and contains more total information than a rating dataset, even if each individual data point carries less information. <a data-primary="implicit feedback data" data-type="indexterm" id="idm46507985127120"/>This type of data is often called <em>implicit feedback</em> data because the user-artist connections are implied as a side effect of other actions, and not given as explicit ratings or thumbs-up.</p>
<p>A snapshot of a dataset distributed by Last.fm in 2005 can<a data-primary="Last.fm streaming radio site" data-secondary="dataset snapshot link" data-type="indexterm" id="idm46507985125328"/><a data-primary="online resources" data-secondary="Last.fm streaming radio site" data-tertiary="dataset snapshot link" data-type="indexterm" id="idm46507985124384"/> be found <a href="https://oreil.ly/Z7sfL">online as a compressed archive</a>. Download the archive, and find within it several files. First, the dataset’s files need to be made available. If you are using a remote cluster, copy all three data files into storage. This chapter will assume that the files are available at <em>data/</em>.</p>
<p>Start <code>pyspark-shell</code>. Note that the computations in this chapter will take up more memory than simple applications. <a data-primary="memory" data-secondary="driver-memory for large local memory" data-type="indexterm" id="idm46507985120800"/><a data-primary="installing PySpark API" data-secondary="driver-memory for large local memory" data-type="indexterm" id="idm46507985119824"/><a data-primary="PySpark API" data-secondary="driver-memory for large local memory" data-type="indexterm" id="idm46507985118864"/><a data-primary="Python" data-secondary="PySpark API" data-tertiary="driver-memory for large local memory" data-type="indexterm" id="idm46507985117904"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="driver-memory for large local memory" data-type="indexterm" id="idm46507985116672"/>If you are running locally rather than on a cluster, for example, you will likely need to specify something like <code>--driver-memory 4g</code> to have enough memory to complete these computations. The main dataset is in the <em>user_artist_data.txt</em> file. It contains about 141,000 unique users, and 1.6 million unique artists. About 24.2 million users’ plays of artists are recorded, along with their counts. Let’s read this dataset into a DataFrame and have a look at it:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">raw_user_artist_path</code> <code class="o">=</code> <code class="s2">"data/audioscrobbler_data/user_artist_data.txt"</code>
<code class="n">raw_user_artist_data</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="n">raw_user_artist_path</code><code class="p">)</code>

<code class="n">raw_user_artist_data</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+-------------------+</code>
<code class="o">|</code>              <code class="n">value</code><code class="o">|</code>
<code class="o">+-------------------+</code>
<code class="o">|</code>       <code class="mi">1000002</code> <code class="mi">1</code> <code class="mi">55</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1000002</code> <code class="mi">1000006</code> <code class="mi">33</code><code class="o">|</code>
<code class="o">|</code>  <code class="mi">1000002</code> <code class="mi">1000007</code> <code class="mi">8</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1000002</code> <code class="mi">1000009</code> <code class="mi">144</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1000002</code> <code class="mi">1000010</code> <code class="mi">314</code><code class="o">|</code>
<code class="o">+-------------------+</code></pre>
<div data-type="note" epub:type="note">
<p>Machine learning tasks like ALS are likely<a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="data partitioned for processing" data-type="indexterm" id="idm46507985091120"/><a data-primary="repartition function" data-type="indexterm" id="idm46507985043856"/><a data-primary="data partitioned via repartition function" data-type="indexterm" id="idm46507985043184"/> to be more compute-intensive than simple text processing. It may be better to break the data into smaller pieces—more partitions—for processing. You can chain a call to <code>.repartition(n)</code> after reading the text file to specify a different and larger number of partitions. You might set this higher to match the number of cores in your cluster, for example.</p>
</div>
<p>The dataset also gives the names of each artist by ID in the <em>artist_data.txt</em> file. <a data-primary="string data" data-secondary="artist names misspelled" data-type="indexterm" id="idm46507985040880"/><a data-primary="names" data-secondary="artist names misspelled" data-type="indexterm" id="idm46507985039872"/>Note that when plays are scrobbled, the client application submits the name of the artist being played. This name could be misspelled or nonstandard, and this may only be detected later. For example, “The Smiths,” “Smiths, The,” and “the smiths” may appear as distinct artist IDs in the dataset even though they are plainly the same artist. So, the dataset also includes <em>artist_alias.txt</em>, which maps artist IDs that are known misspellings or variants to the canonical ID of that artist. Let’s read these two datasets into PySpark too:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">raw_artist_data</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="s2">"data/audioscrobbler_data/artist_data.txt"</code><code class="p">)</code>

<code class="n">raw_artist_data</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+--------------------+</code>
<code class="o">|</code>               <code class="n">value</code><code class="o">|</code>
<code class="o">+--------------------+</code>
<code class="o">|</code><code class="mi">1134999</code>\<code class="n">t06Crazy</code> <code class="o">...|</code>
<code class="o">|</code><code class="mi">6821360</code>\<code class="n">tPang</code> <code class="n">Nak</code><code class="o">...|</code>
<code class="o">|</code><code class="mi">10113088</code>\<code class="n">tTerfel</code><code class="p">,</code><code class="o">...|</code>
<code class="o">|</code><code class="mi">10151459</code>\<code class="n">tThe</code> <code class="n">Fla</code><code class="o">...|</code>
<code class="o">|</code><code class="mi">6826647</code>\<code class="n">tBodensta</code><code class="o">...|</code>
<code class="o">+--------------------+</code>
<code class="n">only</code> <code class="n">showing</code> <code class="n">top</code> <code class="mi">5</code> <code class="n">rows</code>
<code class="o">...</code>

<code class="n">raw_artist_alias</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="s2">"data/audioscrobbler_data/artist_alias.txt"</code><code class="p">)</code>

<code class="n">raw_artist_alias</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+-----------------+</code>
<code class="o">|</code>            <code class="n">value</code><code class="o">|</code>
<code class="o">+-----------------+</code>
<code class="o">|</code> <code class="mi">1092764</code>\<code class="n">t1000311</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1095122</code>\<code class="n">t1000557</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">6708070</code>\<code class="n">t1007267</code><code class="o">|</code>
<code class="o">|</code><code class="mi">10088054</code>\<code class="n">t1042317</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1195917</code>\<code class="n">t1042317</code><code class="o">|</code>
<code class="o">+-----------------+</code>
<code class="n">only</code> <code class="n">showing</code> <code class="n">top</code> <code class="mi">5</code> <code class="n">rows</code></pre>
<p>Now that we have a basic understanding of the datasets, we can discuss our requirements for a recommender algorithm and, subsequently, understand why the Alternating Least Squares algorithm is a good choice.<a data-startref="ch03-data" data-type="indexterm" id="idm46507984984368"/><a data-startref="ch03-data2" data-type="indexterm" id="idm46507984983760"/><a data-startref="ch03-data3" data-type="indexterm" id="idm46507984899440"/><a data-startref="ch03-data4" data-type="indexterm" id="idm46507984898768"/></p>
</div></section>
<section data-pdf-bookmark="Our Requirements for a Recommender System" data-type="sect1"><div class="sect1" id="idm46507985136928">
<h1>Our Requirements for a Recommender System</h1>
<p>We need to choose a recommender algorithm that is suitable for our data. Here are our considerations:<a data-primary="recommender engines" data-secondary="requirements for" data-type="indexterm" id="ch03-req"/><a data-primary="music recommender engine" data-secondary="requirements for" data-type="indexterm" id="ch03-req2"/></p>
<dl>
<dt>Implicit feedback</dt>
<dd>
<p>The data is <a data-primary="implicit feedback data" data-secondary="recommender engine requirements" data-type="indexterm" id="idm46507984853504"/>comprised entirely of interactions between users and artists’ songs. It contains no information about the users or about the artists other than their names. We need an algorithm that learns without access to user or artist attributes. These are typically called collaborative filtering algorithms. For example, deciding that two users might share similar tastes because they are the same age <em>is not</em> an example of collaborative filtering. Deciding that two users might both like the same song because they play many other songs that are the same <em>is</em> an example.</p>
</dd>
<dt>Sparsity</dt>
<dd>
<p>Our dataset looks large because<a data-primary="sparsity in recommender engine" data-type="indexterm" id="idm46507984850384"/> it contains tens of millions of play counts. But in a different sense, it is small and skimpy, because it is sparse. On average, each user has played songs from about 171 artists—out of 1.6 million. Some users have listened to only one artist. We need an algorithm that can provide decent recommendations to even these users. After all, every single listener must have started with just one play at some point!</p>
</dd>
<dt>Scalability and real-time predictions</dt>
<dd>
<p>Finally, we need<a data-primary="scalability" data-secondary="recommendation engine requirements" data-type="indexterm" id="idm46507984848336"/> an algorithm that scales, both in its ability to build large models and to create recommendations quickly. Recommendations are typically required in near real time—within a second, not tomorrow.</p>
</dd>
</dl>
<p>A broad class of algorithms that may be suitable is<a data-primary="latent factor models" data-type="indexterm" id="idm46507984846656"/><a data-primary="recommender engines" data-secondary="latent factor models" data-type="indexterm" id="idm46507984845952"/><a data-primary="music recommender engine" data-secondary="latent factor models" data-type="indexterm" id="idm46507984845008"/><a data-primary="observed interactions and latent factor models" data-type="indexterm" id="idm46507984844096"/> latent factor models. They try to explain <em>observed interactions</em> between large numbers of users and items through a relatively small number of <em>unobserved, underlying reasons</em>. For example, consider a customer who has bought albums by metal bands Megadeth and Pantera but also classical composer Mozart. It may be difficult to explain why exactly these albums were bought and nothing else. However, it’s probably a small window on a much larger set of tastes. Maybe the customer likes a coherent spectrum of music from metal to progressive rock to classical. That explanation is simpler and, as a bonus, suggests many other albums that would be of interest. In this example, “liking metal, progressive rock, and classical” are three latent factors that could explain tens of thousands of individual album preferences.</p>
<p>In our case, we will specifically use a type<a data-primary="matrix factorization models" data-type="indexterm" id="idm46507984841952"/><a data-primary="recommender engines" data-secondary="matrix factorization models" data-type="indexterm" id="idm46507984840928"/><a data-primary="music recommender engine" data-secondary="matrix factorization models" data-type="indexterm" id="idm46507984840016"/> of matrix factorization model. Mathematically, these algorithms treat the user and product data as if it were a large matrix <em>A</em>, where the entry at row <em>i</em> and column <em>j</em> exists if user <em>i</em> has played artist <em>j</em>. <em>A</em> is sparse: most entries of <em>A</em> are 0, because only a few of all possible user-artist combinations actually appear in the data. They factor <em>A</em> as the matrix product of two smaller matrices, <em>X</em> and <em>Y</em>. They are very skinny—both have many rows because <em>A</em> has many rows and columns, but both have just a few columns (<em>k</em>). The <em>k</em> columns correspond to the latent factors that are being used to explain the interaction data.</p>
<p>The factorization can only be approximate because <em>k</em> is small, as shown in <a data-type="xref" href="#ALSFactorization">Figure 3-1</a>.</p>
<figure><div class="figure" id="ALSFactorization">
<img alt="aaps 0301" height="609" src="assets/aaps_0301.png" width="1129"/>
<h6><span class="label">Figure 3-1. </span>Matrix factorization</h6>
</div></figure>
<p>These algorithms are sometimes called<a data-primary="missing data values" data-secondary="matrix completion algorithms" data-type="indexterm" id="idm46507984828864"/> matrix completion algorithms, because the original matrix <em>A</em>
may be quite sparse, but the product <em>XY</em><sup><em>T</em></sup> is dense. Very few, if any, entries are 0, and therefore the model is only an approximation of <em>A</em>. It is a model in the sense that it produces (“completes”) a value for even the many entries that are missing (that is, 0) in the original <em>A</em>.</p>
<p>This is a case where, happily, the linear algebra maps directly and elegantly to intuition.
These two matrices contain a row for each user and each artist. The rows have few values—<em>k</em>. Each value corresponds to a latent feature in the model. So the rows express how much users and artists associate with these <em>k</em> latent features, which might correspond to tastes or genres. And it is simply the product of a user-feature and feature-artist matrix that yields a complete estimation of the entire, dense user-artist interaction matrix. This product might be thought of as mapping items to their attributes and then weighting those by user attributes.</p>
<p>The bad news is that <em>A</em> = <em>XY</em><sup><em>T</em></sup> generally has no exact solution at all, because <em>X</em> and <em>Y</em>
aren’t large enough <a data-primary="rank" data-type="indexterm" id="idm46507984821232"/>(technically speaking, too low <a href="https://oreil.ly/OfVj4">rank</a>)
to perfectly represent <em>A</em>. This is actually a good thing. <em>A</em> is just a tiny sample of all interactions that <em>could</em> happen. In a way, we believe <em>A</em> is a terribly spotty and therefore hard-to-explain view of a simpler underlying reality that is well explained by just some small number of factors, <em>k</em>, of them. Think of a jigsaw puzzle depicting a cat. The final puzzle is simple to describe: a cat. When you’re holding just a few pieces, however, the picture you see is quite difficult to describe.</p>
<p><em>XY</em><sup><em>T</em></sup> should still be as close to <em>A</em> as possible. After all, it’s all we’ve got to go on.
It will not and should not reproduce it exactly. The bad news again is that this can’t be solved directly for both the best <em>X</em> and best <em>Y</em> at the same time. The good news is that it’s trivial to solve for the best <em>X</em> if <em>Y</em> is known, and vice versa. But neither is known beforehand!</p>
<p>Fortunately, there are algorithms that can escape this catch-22 and find a decent solution. One such algorithm that’s available in PySpark is the ALS algorithm.<a data-startref="ch03-req" data-type="indexterm" id="idm46507984813888"/><a data-startref="ch03-req2" data-type="indexterm" id="idm46507984813152"/></p>
<section data-pdf-bookmark="Alternating Least Squares Algorithm" data-type="sect2"><div class="sect2" id="idm46507984812352">
<h2>Alternating Least Squares Algorithm</h2>
<p>We will use the Alternating Least Squares algorithm<a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="recommender engine implicit data" data-type="indexterm" id="idm46507984810336"/><a data-primary="recommender engines" data-secondary="alternating least squares" data-type="indexterm" id="idm46507984809200"/><a data-primary="music recommender engine" data-secondary="alternating least squares" data-type="indexterm" id="idm46507984808240"/><a data-primary="implicit feedback data" data-secondary="“Collaborative Filtering for Implicit Feedback Datasets”" data-secondary-sortas="Collaborative Filtering for Implicit" data-type="indexterm" id="idm46507984807264"/><a data-primary="“Collaborative Filtering for Implicit Feedback Datasets” link" data-primary-sortas="Collaborative Filtering for Implicit" data-type="indexterm" id="idm46507984806064"/><a data-primary="“Large-Scale Parallel Collaborative Filtering for the Netflix Prize” link" data-primary-sortas="Large-Scale Parallel" data-type="indexterm" id="idm46507984805136"/><a data-primary="Netflix Prize competition" data-type="indexterm" id="idm46507984804224"/> to compute latent factors from our dataset. This type of approach was popularized around the time of the Netflix Prize competition by papers like <a href="https://oreil.ly/3pSzk">“Collaborative Filtering for Implicit Feedback Datasets”</a> and <a href="https://oreil.ly/LULpp">“Large-Scale Parallel Collaborative Filtering for the Netflix Prize”</a>. <a data-primary="MLlib component of Spark" data-secondary="Alternating Least Squares algorithm" data-type="indexterm" id="idm46507984801904"/>PySpark MLlib’s ALS implementation draws on ideas from both of these papers and is the only recommender algorithm currently implemented in Spark MLlib.</p>
<p>Here’s a code snippet (non-functional) to give you a peek at what lies ahead in the chapter:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.recommendation</code> <code class="kn">import</code> <code class="n">ALS</code>

<code class="n">als</code> <code class="o">=</code> <code class="n">ALS</code><code class="p">(</code><code class="n">maxIter</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">regParam</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">userCol</code><code class="o">=</code><code class="s2">"user"</code><code class="p">,</code>
          <code class="n">itemCol</code><code class="o">=</code><code class="s2">"artist"</code><code class="p">,</code> <code class="n">ratingCol</code><code class="o">=</code><code class="s2">"count"</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">als</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train</code><code class="p">)</code>

<code class="n">predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">test</code><code class="p">)</code></pre>
<p>With ALS, we will treat our input data as a large, sparse matrix <em>A</em>, and find out <em>X</em> and <em>Y</em>, as discussed in previous section. At the start, <em>Y</em> isn’t known, but it can be initialized to a matrix full of randomly chosen row vectors. Then simple linear algebra gives the best solution for <em>X</em>, given <em>A</em> and <em>Y</em>. In fact, it’s trivial to compute each row <em>i</em> of <em>X</em> separately as a function of <em>Y</em> and of one row of <em>A</em>. Because it can be done separately, it can be done in parallel, and that is an excellent property for large-scale computation:</p>
<div data-type="equation">
<math display="block">
<mrow>
<msub><mi>A</mi><mi>i</mi></msub><mi>Y</mi><mo>(</mo><msup><mi>Y</mi><mi>T</mi></msup><mi>Y</mi><msup><mo>)</mo><mi>–1</mi></msup> <mo>=</mo> <msub><mi>X</mi><mi>i</mi></msub>
</mrow>
</math>
</div>
<p>Equality can’t be achieved exactly, so in fact the goal is to minimize |<em>A</em><sub><em>i</em></sub><em>Y</em>(<em>Y</em><sup><em>T</em></sup><em>Y</em>)<sup><em>–1</em></sup> – <em>X</em><sub><em>i</em></sub>|, or the sum of squared differences between the two matrices’ entries. <a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="name origin" data-type="indexterm" id="idm46507984706032"/>This is where the “least squares” in the name comes from.
In practice, this is never solved by actually computing inverses, but faster and more directly via methods like the QR decomposition. This equation simply
elaborates on the theory of how the row vector is computed.</p>
<p>The same thing can be done to compute each <em>Y</em><sub><em>j</em></sub> from <em>X</em>. And again, to compute <em>X</em>
from <em>Y</em>, and so on. This is where the “alternating” part comes from. There’s just one small
problem: <em>Y</em> was made up—and random! <em>X</em> was computed optimally, yes, but gives a bogus solution
for <em>Y</em>. Fortunately, if this process is repeated, <em>X</em> and <em>Y</em> do eventually converge to
decent solutions.</p>
<p>When used to factor a matrix representing implicit data, there is a little more
complexity to the ALS factorization. It is not factoring the input matrix <em>A</em> directly, but
a matrix <em>P</em> of 0s and 1s, containing 1 where <em>A</em> contains a positive value and 0 elsewhere. The values in <em>A</em> are incorporated later as weights. This detail is beyond the scope of this book but is not necessary to understand how to use the algorithm.</p>
<p>Finally, the ALS algorithm can take<a data-primary="scalability" data-secondary="Alternating Least Squares algorithm" data-type="indexterm" id="idm46507984697456"/> advantage of the sparsity of the input data
as well. This, and its reliance on simple, optimized linear algebra and its data-parallel
nature, make it very fast at large scale.</p>
<p>Next, we will preprocess our dataset and make it suitable for use with the ALS algorithm.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Preparing the Data" data-type="sect1"><div class="sect1" id="idm46507984858400">
<h1>Preparing the Data</h1>
<p>The first step in building a model is to understand<a data-primary="recommender engines" data-secondary="alternating least squares" data-tertiary="preparing the data" data-type="indexterm" id="ch03-prep"/><a data-primary="music recommender engine" data-secondary="alternating least squares" data-tertiary="preparing the data" data-type="indexterm" id="ch03-prep2"/><a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="recommender engine implicit data" data-tertiary="data preparation" data-type="indexterm" id="ch03-prep3"/> the data that is available and parse or transform it into forms that are useful for analysis in Spark.</p>
<p>Spark MLlib’s ALS implementation does not strictly require numeric IDs for users and items, but is more efficient when the IDs are in fact representable as 32-bit integers. That is the case because under the hood the data is being represented using the JVM’s data type.  Does this dataset conform to this requirement already?</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">raw_user_artist_data</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+-------------------+</code>
<code class="o">|</code>              <code class="n">value</code><code class="o">|</code>
<code class="o">+-------------------+</code>
<code class="o">|</code>       <code class="mi">1000002</code> <code class="mi">1</code> <code class="mi">55</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1000002</code> <code class="mi">1000006</code> <code class="mi">33</code><code class="o">|</code>
<code class="o">|</code>  <code class="mi">1000002</code> <code class="mi">1000007</code> <code class="mi">8</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1000002</code> <code class="mi">1000009</code> <code class="mi">144</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1000002</code> <code class="mi">1000010</code> <code class="mi">314</code><code class="o">|</code>
<code class="o">|</code>  <code class="mi">1000002</code> <code class="mi">1000013</code> <code class="mi">8</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1000002</code> <code class="mi">1000014</code> <code class="mi">42</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1000002</code> <code class="mi">1000017</code> <code class="mi">69</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1000002</code> <code class="mi">1000024</code> <code class="mi">329</code><code class="o">|</code>
<code class="o">|</code>  <code class="mi">1000002</code> <code class="mi">1000025</code> <code class="mi">1</code><code class="o">|</code>
<code class="o">+-------------------+</code></pre>
<p>Each line of the file contains a user ID, an artist ID, and a play count, separated by spaces. <a data-primary="dataframes" data-secondary="split method" data-type="indexterm" id="idm46507984664048"/><a data-primary="split method on dataframes" data-type="indexterm" id="idm46507984680624"/>To compute statistics on the user ID, we split the line by space characters and parse the values as integers. The result is conceptually three “columns”: a user ID, artist ID, and count as <code>int</code>s. It makes sense to transform this to a dataframe with columns named “user”, “artist”, and “count” because it then becomes simple to compute simple statistics like the maximum and minimum:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">split</code><code class="p">,</code> <code class="nb">min</code><code class="p">,</code> <code class="nb">max</code>
<code class="kn">from</code> <code class="nn">pyspark.sql.types</code> <code class="kn">import</code> <code class="n">IntegerType</code><code class="p">,</code> <code class="n">StringType</code>

<code class="n">user_artist_df</code> <code class="o">=</code> <code class="n">raw_user_artist_data</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'user'</code><code class="p">,</code>
                                    <code class="n">split</code><code class="p">(</code><code class="n">raw_user_artist_data</code><code class="p">[</code><code class="s1">'value'</code><code class="p">],</code> <code class="s1">' '</code><code class="p">)</code><code class="o">.</code>\
                                    <code class="n">getItem</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code>\
                                    <code class="n">cast</code><code class="p">(</code><code class="n">IntegerType</code><code class="p">()))</code>
<code class="n">user_artist_df</code> <code class="o">=</code> <code class="n">user_artist_df</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'artist'</code><code class="p">,</code>
                                    <code class="n">split</code><code class="p">(</code><code class="n">raw_user_artist_data</code><code class="p">[</code><code class="s1">'value'</code><code class="p">],</code> <code class="s1">' '</code><code class="p">)</code><code class="o">.</code>\
                                    <code class="n">getItem</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code>\
                                    <code class="n">cast</code><code class="p">(</code><code class="n">IntegerType</code><code class="p">()))</code>
<code class="n">user_artist_df</code> <code class="o">=</code> <code class="n">user_artist_df</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'count'</code><code class="p">,</code>
                                    <code class="n">split</code><code class="p">(</code><code class="n">raw_user_artist_data</code><code class="p">[</code><code class="s1">'value'</code><code class="p">],</code> <code class="s1">' '</code><code class="p">)</code><code class="o">.</code>\
                                    <code class="n">getItem</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code>\
                                    <code class="n">cast</code><code class="p">(</code><code class="n">IntegerType</code><code class="p">()))</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s1">'value'</code><code class="p">)</code>

<code class="n">user_artist_df</code><code class="o">.</code><code class="n">select</code><code class="p">([</code><code class="nb">min</code><code class="p">(</code><code class="s2">"user"</code><code class="p">),</code> <code class="nb">max</code><code class="p">(</code><code class="s2">"user"</code><code class="p">),</code> <code class="nb">min</code><code class="p">(</code><code class="s2">"artist"</code><code class="p">),</code>\
                                    <code class="nb">max</code><code class="p">(</code><code class="s2">"artist"</code><code class="p">)])</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+---------+---------+-----------+-----------+</code>
<code class="o">|</code><code class="nb">min</code><code class="p">(</code><code class="n">user</code><code class="p">)</code><code class="o">|</code><code class="nb">max</code><code class="p">(</code><code class="n">user</code><code class="p">)</code><code class="o">|</code><code class="nb">min</code><code class="p">(</code><code class="n">artist</code><code class="p">)</code><code class="o">|</code><code class="nb">max</code><code class="p">(</code><code class="n">artist</code><code class="p">)</code><code class="o">|</code>
<code class="o">+---------+---------+-----------+-----------+</code>
<code class="o">|</code>       <code class="mi">90</code><code class="o">|</code>  <code class="mi">2443548</code><code class="o">|</code>          <code class="mi">1</code><code class="o">|</code>   <code class="mi">10794401</code><code class="o">|</code>
<code class="o">+---------+---------+-----------+-----------+</code></pre>
<p>The maximum user and artist IDs are 2443548 and 10794401, respectively (and their minimums are 90 and 1; no negative values). These are comfortably smaller than 2147483647. No additional transformation will be necessary to use these IDs.</p>
<p>It will be useful later in this example to know the artist names corresponding to the opaque numeric IDs. <code>raw_artist_data</code> contains the artist ID and name separated by a tab. PySpark’s split function accepts regular expression values for the <code>pattern</code> parameter. We can split using the whitespace character, <code>\s</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">col</code>

<code class="n">artist_by_id</code> <code class="o">=</code> <code class="n">raw_artist_data</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'id'</code><code class="p">,</code> <code class="n">split</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'value'</code><code class="p">),</code> <code class="s1">'\s+'</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code><code class="o">.</code>\
                                                <code class="n">getItem</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code>\
                                                <code class="n">cast</code><code class="p">(</code><code class="n">IntegerType</code><code class="p">()))</code>
<code class="n">artist_by_id</code> <code class="o">=</code> <code class="n">artist_by_id</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'name'</code><code class="p">,</code> <code class="n">split</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'value'</code><code class="p">),</code> <code class="s1">'\s+'</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code><code class="o">.</code>\
                                               <code class="n">getItem</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code>\
                                               <code class="n">cast</code><code class="p">(</code><code class="n">StringType</code><code class="p">()))</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s1">'value'</code><code class="p">)</code>

<code class="n">artist_by_id</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
<code class="o">...</code>
<code class="o">+--------+--------------------+</code>
<code class="o">|</code>      <code class="nb">id</code><code class="o">|</code>                <code class="n">name</code><code class="o">|</code>
<code class="o">+--------+--------------------+</code>
<code class="o">|</code> <code class="mi">1134999</code><code class="o">|</code>        <code class="mi">06</code><code class="n">Crazy</code> <code class="n">Life</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">6821360</code><code class="o">|</code>        <code class="n">Pang</code> <code class="n">Nakarin</code><code class="o">|</code>
<code class="o">|</code><code class="mi">10113088</code><code class="o">|</code><code class="n">Terfel</code><code class="p">,</code> <code class="n">Bartoli</code><code class="o">-</code> <code class="o">...|</code>
<code class="o">|</code><code class="mi">10151459</code><code class="o">|</code> <code class="n">The</code> <code class="n">Flaming</code> <code class="n">Sidebur</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">6826647</code><code class="o">|</code>   <code class="n">Bodenstandig</code> <code class="mi">3000</code><code class="o">|</code>
<code class="o">+--------+--------------------+</code></pre>
<p>This results in a dataframe with the artist ID and name as columns <code>id</code> and <code>name</code>.</p>
<p><code>raw_artist_alias</code> maps<a data-primary="string data" data-secondary="artist names misspelled" data-type="indexterm" id="idm46507984284256"/><a data-primary="names" data-secondary="artist names misspelled" data-type="indexterm" id="idm46507984283248"/> artist IDs that may be misspelled or nonstandard to the ID of the artist’s canonical name. This dataset is relatively small, containing about 200,000 entries. It contains two IDs per line, separated by a tab. We will parse this in a similar manner as we did <code>raw_artist_data</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">artist_alias</code> <code class="o">=</code> <code class="n">raw_artist_alias</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'artist'</code><code class="p">,</code>
                                          <code class="n">split</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'value'</code><code class="p">),</code> <code class="s1">'\s+'</code><code class="p">)</code><code class="o">.</code>\
                                                <code class="n">getItem</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code>\
                                                <code class="n">cast</code><code class="p">(</code><code class="n">IntegerType</code><code class="p">()))</code><code class="o">.</code>\
                                <code class="n">withColumn</code><code class="p">(</code><code class="s1">'alias'</code><code class="p">,</code>
                                            <code class="n">split</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'value'</code><code class="p">),</code> <code class="s1">'\s+'</code><code class="p">)</code><code class="o">.</code>\
                                            <code class="n">getItem</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code>\
                                            <code class="n">cast</code><code class="p">(</code><code class="n">StringType</code><code class="p">()))</code><code class="o">.</code>\
                                <code class="n">drop</code><code class="p">(</code><code class="s1">'value'</code><code class="p">)</code>

<code class="n">artist_alias</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
<code class="o">...</code>
<code class="o">+--------+-------+</code>
<code class="o">|</code>  <code class="n">artist</code><code class="o">|</code>  <code class="n">alias</code><code class="o">|</code>
<code class="o">+--------+-------+</code>
<code class="o">|</code> <code class="mi">1092764</code><code class="o">|</code><code class="mi">1000311</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1095122</code><code class="o">|</code><code class="mi">1000557</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">6708070</code><code class="o">|</code><code class="mi">1007267</code><code class="o">|</code>
<code class="o">|</code><code class="mi">10088054</code><code class="o">|</code><code class="mi">1042317</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">1195917</code><code class="o">|</code><code class="mi">1042317</code><code class="o">|</code>
<code class="o">+--------+-------+</code></pre>
<p>The first entry maps ID 1092764 to 1000311. We can look these up from the <code>artist_by_id</code> DataFrame:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">artist_by_id</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">artist_by_id</code><code class="o">.</code><code class="n">id</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="mi">1092764</code><code class="p">,</code> <code class="mi">1000311</code><code class="p">))</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>

<code class="o">+-------+--------------+</code>
<code class="o">|</code>     <code class="nb">id</code><code class="o">|</code>          <code class="n">name</code><code class="o">|</code>
<code class="o">+-------+--------------+</code>
<code class="o">|</code><code class="mi">1000311</code><code class="o">|</code> <code class="n">Steve</code> <code class="n">Winwood</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1092764</code><code class="o">|</code><code class="n">Winwood</code><code class="p">,</code> <code class="n">Steve</code><code class="o">|</code>
<code class="o">+-------+--------------+</code></pre>
<p>This entry evidently maps “Winwood, Steve” to “Steve Winwood,” which is in fact
the correct name for the artist.<a data-startref="ch03-prep" data-type="indexterm" id="idm46507983979072"/><a data-startref="ch03-prep2" data-type="indexterm" id="idm46507984019200"/><a data-startref="ch03-prep3" data-type="indexterm" id="idm46507984018592"/></p>
</div></section>
<section data-pdf-bookmark="Building a First Model" data-type="sect1"><div class="sect1" id="idm46507984695296">
<h1>Building a First Model</h1>
<p>Although the dataset is in nearly the right form<a data-primary="recommender engines" data-secondary="alternating least squares" data-tertiary="model" data-type="indexterm" id="ch03-model"/><a data-primary="music recommender engine" data-secondary="alternating least squares" data-tertiary="model" data-type="indexterm" id="ch03-model2"/><a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="recommender engine implicit data" data-tertiary="model" data-type="indexterm" id="ch03-model3"/> for use with Spark MLlib’s ALS implementation, it requires a small, extra transformation. The aliases dataset should be applied to convert all artist IDs to a canonical ID, if a different canonical ID exists:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">sql</code><code class="nn">.</code><code class="nn">functions</code><code> </code><code class="kn">import</code><code> </code><code class="n">broadcast</code><code class="p">,</code><code> </code><code class="n">when</code><code>
</code><code>
</code><code class="n">train_data</code><code> </code><code class="o">=</code><code> </code><code class="n">train_data</code><code> </code><code class="o">=</code><code> </code><code class="n">user_artist_df</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">broadcast</code><code class="p">(</code><code class="n">artist_alias</code><code class="p">)</code><code class="p">,</code><code>
</code><code>                                              </code><code class="s1">'</code><code class="s1">artist</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">how</code><code class="o">=</code><code class="s1">'</code><code class="s1">left</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code>\
</code><code class="n">train_data</code><code> </code><code class="o">=</code><code> </code><code class="n">train_data</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'</code><code class="s1">artist</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                                    </code><code class="n">when</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">alias</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">isNull</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">artist</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                    </code><code class="n">otherwise</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">alias</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_recommending_music_and_the_audioscrobbler_dataset_CO1-1" id="co_recommending_music_and_the_audioscrobbler_dataset_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code class="n">train_data</code><code> </code><code class="o">=</code><code> </code><code class="n">train_data</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'</code><code class="s1">artist</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">artist</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                             </code><code class="n">cast</code><code class="p">(</code><code class="n">IntegerType</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                             </code><code class="n">drop</code><code class="p">(</code><code class="s1">'</code><code class="s1">alias</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>
</code><code class="n">train_data</code><code class="o">.</code><code class="n">cache</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="n">train_data</code><code class="o">.</code><code class="n">count</code><code class="p">(</code><code class="p">)</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="mi">24296858</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_recommending_music_and_the_audioscrobbler_dataset_CO1-1" id="callout_recommending_music_and_the_audioscrobbler_dataset_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Get artist’s alias if it exists; otherwise, get original artist.</p></dd>
</dl>
<p>We <code>broadcast</code> the <code>artist_alias</code> DataFrame<a data-primary="broadcast joins of dataframes" data-type="indexterm" id="idm46507983845376"/><a data-primary="join function for dataframes" data-secondary="broadcast joins" data-type="indexterm" id="idm46507983844672"/><a data-primary="dataframes" data-secondary="joining" data-tertiary="broadcast joins" data-type="indexterm" id="idm46507983843760"/> created earlier. This makes Spark send and hold in memory just one copy for <em>each executor</em> in the cluster. When there are thousands of tasks and many execute
in parallel on each executor, this can save significant network traffic and memory. As a rule of thumb, it’s helpful to broadcast a significantly smaller dataset when performing a join with a very big dataset.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507983842032">
<h5>Broadcast Variables</h5>
<p>When Spark runs a stage, it creates<a data-primary="broadcast variables" data-type="indexterm" id="idm46507983840464"/><a data-primary="closure of function" data-type="indexterm" id="idm46507983839760"/><a data-primary="function data structure closure" data-type="indexterm" id="idm46507983839088"/> a binary representation of all the information needed to run tasks in that stage; this is called the <em>closure</em> of the function that needs to be executed.
This closure includes all the data structures on the driver referenced in the
function. Spark distributes the closure with every task that is sent to an executor on the cluster.</p>
<p>Broadcast variables are useful when many<a data-primary="broadcast variables" data-secondary="caching data" data-type="indexterm" id="idm46507983783728"/><a data-primary="caching of data by broadcast variables" data-type="indexterm" id="idm46507983782992"/> tasks need access to the same (immutable) data structure. They extend normal handling of task closures to enable:</p>
<ul>
<li>
<p>Caching data as raw Java objects on each executor, so they need not be deserialized for each task</p>
</li>
<li>
<p>Caching data across multiple jobs, stages, and tasks</p>
</li>
</ul>
<p>For example, consider a natural language processing application that requires a large dictionary of English words and has a <code>score</code> function that accepts a line of input and a dictionary. Broadcasting the dictionary means it is transferred to each executor only once.</p>
<p>DataFrame operations can at times also<a data-primary="broadcast joins of dataframes" data-secondary="broadcast hash joins" data-type="indexterm" id="idm46507983778784"/><a data-primary="join function for dataframes" data-secondary="broadcast joins" data-tertiary="broadcast hash joins" data-type="indexterm" id="idm46507983777904"/><a data-primary="dataframes" data-secondary="joining" data-tertiary="broadcast hash joins" data-type="indexterm" id="idm46507983776720"/> automatically take advantage of broadcasts when performing joins between a large and small table. Just broadcasting the small table is advantageous sometimes. This is called a <em>broadcast hash join</em>.</p>
</div></aside>
<p>The call to <code>cache</code> suggests to Spark<a data-primary="cache method on dataframes" data-type="indexterm" id="idm46507983774128"/><a data-primary="iteration in data analysis" data-secondary="caching dataframes" data-type="indexterm" id="idm46507983773424"/> that this DataFrame should be temporarily stored after being computed and, furthermore, kept in memory in the cluster. This is helpful because the ALS algorithm is iterative and will typically need to access this data 10 times or more. Without this, the DataFrame could be repeatedly recomputed from the original data each time it is accessed! The Storage tab in the Spark UI will show how much of the DataFrame is cached and how much memory it uses, as shown in <a data-type="xref" href="#Recommender_StorageTag">Figure 3-2</a>. This one consumes about 120 MB across the cluster.</p>
<figure><div class="figure" id="Recommender_StorageTag">
<img alt="aaps 0302" height="208" src="assets/aaps_0302.png" width="1138"/>
<h6><span class="label">Figure 3-2. </span>Storage tab in the Spark UI, showing cached DataFrame memory usage</h6>
</div></figure>
<p>When you use <code>cache</code> or <code>persist</code>, the DataFrame<a data-primary="actions for distributed computations" data-secondary="cache and persist methods" data-type="indexterm" id="idm46507983768080"/> is not fully cached until you trigger an action that goes through every record (e.g., <code>count</code>). If you use an action like <code>show(1)</code>, only one partition will be cached. That is because PySpark’s optimizer will figure out that you do not need to compute all the partitions just to retrieve one record.</p>
<p>Note that the label “Deserialized” in the UI in <a data-type="xref" href="#Recommender_StorageTag">Figure 3-2</a> is<a data-primary="RDDs (resilient distributed datasets)" data-secondary="Serialized versus Deserialized" data-type="indexterm" id="idm46507983765088"/><a data-primary="memory" data-secondary="Serialized versus Deserialized data" data-type="indexterm" id="idm46507983764176"/> actually only relevant for RDDs, where “Serialized” means data is stored in memory, not as objects, but as serialized bytes. However, DataFrame instances like this one perform their own “encoding” of common data types in memory separately.</p>
<p>Actually, 120 MB is surprisingly small. Given that there are about 24 million plays stored here, a quick back-of-the-envelope calculation suggests that this would mean that each user-artist-count entry consumes only 5 bytes on average. However, the three 32-bit integers alone ought to consume 12 bytes. This is one of the advantages of a DataFrame. <a data-primary="numeric data" data-secondary="memory representation optimized" data-type="indexterm" id="idm46507983762752"/>Because the types of data stored are primitive 32-bit integers, their representation can be optimized in memory internally.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507983761504">
<h5>Spark UI</h5>
<p>Spark UI is the web interface<a data-primary="Spark UI web interface" data-type="indexterm" id="idm46507983760032"/><a data-primary="web interface Spark UI" data-type="indexterm" id="idm46507983759328"/><a data-primary="Spark (Apache)" data-secondary="Spark UI web interface" data-type="indexterm" id="idm46507983758656"/><a data-primary="online resources" data-secondary="Spark" data-tertiary="Spark UI web interface" data-type="indexterm" id="idm46507983757712"/> of a running Spark application that monitors the status and resource consumption of your Spark cluster (<a data-type="xref" href="#SparkUI">Figure 3-3</a>). By default, it is available at <em>http://&lt;driver&gt;:4040</em>.</p>
<figure><div class="figure" id="SparkUI">
<img alt="aaps 0303" height="509" src="assets/aaps_0303.png" width="1073"/>
<h6><span class="label">Figure 3-3. </span>Spark UI</h6>
</div></figure>
<p>The Spark UI comes with the following tabs:</p>
<ul>
<li>
<p>Jobs</p>
</li>
<li>
<p>Stages</p>
</li>
<li>
<p>Storage with DataFrame size and memory use</p>
</li>
<li>
<p>Environment</p>
</li>
<li>
<p>Executors</p>
</li>
<li>
<p>SQL</p>
</li>
</ul>
<p>Some tabs, such as the ones related to streaming, are created lazily, i.e., when required.</p>
</div></aside>
<p>Finally, we can build a model:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.recommendation</code> <code class="kn">import</code> <code class="n">ALS</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">ALS</code><code class="p">(</code><code class="n">rank</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">maxIter</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">regParam</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
            <code class="n">implicitPrefs</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">userCol</code><code class="o">=</code><code class="s1">'user'</code><code class="p">,</code>
            <code class="n">itemCol</code><code class="o">=</code><code class="s1">'artist'</code><code class="p">,</code> <code class="n">ratingCol</code><code class="o">=</code><code class="s1">'count'</code><code class="p">)</code><code class="o">.</code> \
        <code class="n">fit</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code></pre>
<p>This constructs <code>model</code> as an <code>ALSModel</code> with some default configuration. The operation will likely take several minutes or more depending on your cluster. Compared to some machine learning models, whose final form may consist of just a few parameters or coefficients, this type of model is huge. It contains a feature vector of 10 values for each user and product in the model, and in this case there are more than 1.7 million of them. The model contains these large user-feature and product-feature matrices as DataFrames of their own.</p>
<p>The values in your results may be somewhat different. The final model depends on a randomly chosen
initial set of feature vectors. The default behavior of this and other components in MLlib, however, is to use the same set of random choices every time by defaulting to a fixed seed. This is unlike other libraries, where behavior of random elements is typically not fixed by default. So, here and elsewhere, a random seed is set with <code>(… seed=0,…)</code>.</p>
<p>To see some feature vectors, try the following, which displays just one row and does not truncate the wide display of the feature vector:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">model</code><code class="o">.</code><code class="n">userFactors</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">truncate</code> <code class="o">=</code> <code class="kc">False</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+---+-----------------------------------------------</code> <code class="o">...</code>
<code class="o">|</code><code class="nb">id</code> <code class="o">|</code><code class="n">features</code>                                        <code class="o">...</code>
<code class="o">+---+-----------------------------------------------</code> <code class="o">...</code>
<code class="o">|</code><code class="mi">90</code> <code class="o">|</code><code class="p">[</code><code class="mf">0.16020626</code><code class="p">,</code> <code class="mf">0.20717518</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1719469</code><code class="p">,</code> <code class="mf">0.06038466</code> <code class="o">...</code>
<code class="o">+---+-----------------------------------------------</code> <code class="o">...</code></pre>
<p>The other methods invoked on <code>ALS</code>, like <code>setAlpha</code>, set<a data-primary="hyperparameters" data-type="indexterm" id="idm46507983610128"/><a data-primary="recommender engines" data-secondary="hyperparameters" data-type="indexterm" id="idm46507983609456"/><a data-primary="music recommender engine" data-secondary="hyperparameters" data-type="indexterm" id="idm46507983601552"/><a data-primary="models" data-secondary="hyperparameters" data-type="indexterm" id="idm46507983600640"/> <em>hyperparameters</em> whose values can affect the quality of the recommendations that the model makes. These will be explained later. The more important first question is this: is the model any good? Does it produce good recommendations? That is what we will try to answer in the next section.<a data-startref="ch03-model" data-type="indexterm" id="idm46507983599152"/><a data-startref="ch03-model2" data-type="indexterm" id="idm46507983598448"/><a data-startref="ch03-model3" data-type="indexterm" id="idm46507983597776"/></p>
</div></section>
<section data-pdf-bookmark="Spot Checking Recommendations" data-type="sect1"><div class="sect1" id="idm46507984014512">
<h1>Spot Checking Recommendations</h1>
<p>We should first see if the artist recommendations<a data-primary="recommender engines" data-secondary="alternating least squares" data-tertiary="spot checking recommendations" data-type="indexterm" id="idm46507983595568"/><a data-primary="music recommender engine" data-secondary="alternating least squares" data-tertiary="spot checking recommendations" data-type="indexterm" id="idm46507983594384"/><a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="recommender engine implicit data" data-tertiary="spot checking recommendations" data-type="indexterm" id="idm46507983577696"/> make any intuitive sense, by
examining a user, plays, and recommendations for that user. Take, for example, user 2093760. First, let’s look at his or her plays to get a sense of the person’s tastes. Extract the IDs of artists that this user has listened to and print their names. This means searching the input for artist IDs played by this user and then filtering the set of artists by these IDs to print the names in order:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">user_id</code><code> </code><code class="o">=</code><code> </code><code class="mi">2093760</code><code>
</code><code>
</code><code class="n">existing_artist_ids</code><code> </code><code class="o">=</code><code> </code><code class="n">train_data</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">train_data</code><code class="o">.</code><code class="n">user</code><code> </code><code class="o">==</code><code> </code><code class="n">user_id</code><code class="p">)</code><code> </code><code>\</code><code> </code><a class="co" href="#callout_recommending_music_and_the_audioscrobbler_dataset_CO2-1" id="co_recommending_music_and_the_audioscrobbler_dataset_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>  </code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"</code><code class="s2">artist</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">collect</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_recommending_music_and_the_audioscrobbler_dataset_CO2-2" id="co_recommending_music_and_the_audioscrobbler_dataset_CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>
</code><code class="n">existing_artist_ids</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">i</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="n">existing_artist_ids</code><code class="p">]</code><code>
</code><code>
</code><code class="n">artist_by_id</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">id</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">existing_artist_ids</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_recommending_music_and_the_audioscrobbler_dataset_CO2-3" id="co_recommending_music_and_the_audioscrobbler_dataset_CO2-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code>
</code><code class="o">|</code><code>     </code><code class="nb">id</code><code class="o">|</code><code>           </code><code class="n">name</code><code class="o">|</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code>
</code><code class="o">|</code><code>   </code><code class="mi">1180</code><code class="o">|</code><code>     </code><code class="n">David</code><code> </code><code class="n">Gray</code><code class="o">|</code><code>
</code><code class="o">|</code><code>    </code><code class="mi">378</code><code class="o">|</code><code>  </code><code class="n">Blackalicious</code><code class="o">|</code><code>
</code><code class="o">|</code><code>    </code><code class="mi">813</code><code class="o">|</code><code>     </code><code class="n">Jurassic</code><code> </code><code class="mi">5</code><code class="o">|</code><code>
</code><code class="o">|</code><code class="mi">1255340</code><code class="o">|</code><code class="n">The</code><code> </code><code class="n">Saw</code><code> </code><code class="n">Doctors</code><code class="o">|</code><code>
</code><code class="o">|</code><code>    </code><code class="mi">942</code><code class="o">|</code><code>         </code><code class="n">Xzibit</code><code class="o">|</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_recommending_music_and_the_audioscrobbler_dataset_CO2-1" id="callout_recommending_music_and_the_audioscrobbler_dataset_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Find lines whose user is 2093760.</p></dd>
<dt><a class="co" href="#co_recommending_music_and_the_audioscrobbler_dataset_CO2-2" id="callout_recommending_music_and_the_audioscrobbler_dataset_CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Collect dataset of artist ID.</p></dd>
<dt><a class="co" href="#co_recommending_music_and_the_audioscrobbler_dataset_CO2-3" id="callout_recommending_music_and_the_audioscrobbler_dataset_CO2-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Filter in those artists.</p></dd>
</dl>
<p>The artists look like a mix of mainstream pop and hip-hop. A Jurassic 5 fan? Remember, it’s 2005.
In case you’re wondering, the Saw Doctors is a very Irish rock band popular in Ireland.</p>
<p>Now, it’s simple to make recommendations for a user, though computing them this way will take a few moments. It’s suitable for batch scoring but not real-time use cases:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">user_subset</code> <code class="o">=</code> <code class="n">train_data</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'user'</code><code class="p">)</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'user'</code><code class="p">)</code> <code class="o">==</code> <code class="n">user_id</code><code class="p">)</code><code class="o">.</code><code class="n">distinct</code><code class="p">()</code>
<code class="n">top_predictions</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">recommendForUserSubset</code><code class="p">(</code><code class="n">user_subset</code><code class="p">,</code> <code class="mi">5</code><code class="p">)</code>

<code class="n">top_predictions</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+-------+--------------------+</code>
<code class="o">|</code>   <code class="n">user</code><code class="o">|</code>     <code class="n">recommendations</code><code class="o">|</code>
<code class="o">+-------+--------------------+</code>
<code class="o">|</code><code class="mi">2093760</code><code class="o">|</code><code class="p">[{</code><code class="mi">2814</code><code class="p">,</code> <code class="mf">0.0294106</code><code class="o">...|</code>
<code class="o">+-------+--------------------+</code></pre>
<p>The resulting recommendations contain lists comprised of artist ID and, of course,<a data-primary="recommender engines" data-secondary="alternating least squares" data-tertiary="predictions" data-type="indexterm" id="idm46507983303248"/><a data-primary="music recommender engine" data-secondary="alternating least squares" data-tertiary="predictions" data-type="indexterm" id="idm46507983288256"/><a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="recommender engine implicit data" data-tertiary="predictions" data-type="indexterm" id="idm46507983287168"/><a data-primary="predictions" data-type="indexterm" id="idm46507983286016"/> “predictions.” For this type of ALS algorithm, the prediction is an opaque value normally between 0 and 1, where higher values mean a better recommendation. It is not a probability but can be thought of as an estimate of a 0/1 value indicating whether the user won’t or will interact with the artist, respectively.</p>
<p>After extracting the artist IDs for the recommendations, we can look up artist names in a similar way:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">top_predictions_pandas</code> <code class="o">=</code> <code class="n">top_predictions</code><code class="o">.</code><code class="n">toPandas</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="n">top_prediction_pandas</code><code class="p">)</code>
<code class="o">...</code>
      <code class="n">user</code>                                    <code class="n">recommendations</code>
<code class="mi">0</code>  <code class="mi">2093760</code>  <code class="p">[(</code><code class="mi">2814</code><code class="p">,</code> <code class="mf">0.029410675168037415</code><code class="p">),</code> <code class="p">(</code><code class="mi">1300642</code><code class="p">,</code> <code class="mf">0.028</code><code class="o">...</code>
<code class="o">...</code>

<code class="n">recommended_artist_ids</code> <code class="o">=</code> <code class="p">[</code><code class="n">i</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">top_predictions_pandas</code><code class="o">.</code>\
                                        <code class="n">recommendations</code><code class="p">[</code><code class="mi">0</code><code class="p">]]</code>

<code class="n">artist_by_id</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'id'</code><code class="p">)</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">recommended_artist_ids</code><code class="p">))</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+-------+----------+</code>
<code class="o">|</code>     <code class="nb">id</code><code class="o">|</code>      <code class="n">name</code><code class="o">|</code>
<code class="o">+-------+----------+</code>
<code class="o">|</code>   <code class="mi">2814</code><code class="o">|</code>   <code class="mi">50</code> <code class="n">Cent</code><code class="o">|</code>
<code class="o">|</code>   <code class="mi">4605</code><code class="o">|</code><code class="n">Snoop</code> <code class="n">Dogg</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1007614</code><code class="o">|</code>     <code class="n">Jay</code><code class="o">-</code><code class="n">Z</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1001819</code><code class="o">|</code>      <code class="mi">2</code><code class="n">Pac</code><code class="o">|</code>
<code class="o">|</code><code class="mi">1300642</code><code class="o">|</code>  <code class="n">The</code> <code class="n">Game</code><code class="o">|</code>
<code class="o">+-------+----------+</code></pre>
<p>The result is all hip-hop. This doesn’t look like a great set of recommendations, at first glance. While these are generally popular artists, they don’t appear to be personalized to this user’s listening habits.</p>
</div></section>
<section data-pdf-bookmark="Evaluating Recommendation Quality" data-type="sect1"><div class="sect1" id="idm46507983173392">
<h1>Evaluating Recommendation Quality</h1>
<p>Of course, that’s just one subjective judgment<a data-primary="recommender engines" data-secondary="evaluating recommender quality" data-type="indexterm" id="idm46507983114656"/><a data-primary="music recommender engine" data-secondary="evaluating recommender quality" data-type="indexterm" id="idm46507983113712"/> about one user’s results. It’s hard for anyone but that user to quantify how good the recommendations are. Moreover, it’s infeasible to have any human manually score even a small sample of the output to evaluate the results.</p>
<p>It’s reasonable to assume that users tend to play songs from artists who are appealing, and not play songs from artists who aren’t appealing. So, the plays for a user give a partial picture of “good” and “bad” artist recommendations. This is a problematic assumption but about the best that can be done without any other data. For example, presumably user 2093760 likes many more artists than the 5 listed previously, and of the 1.7 million other artists not played, a few are of interest, and not all are “bad” recommendations.</p>
<p>What if a recommender were evaluated on its ability to rank good artists high in a list of recommendations? This is one of several generic metrics that can be applied to a system that ranks things, like a recommender. The problem is that “good” is defined as “artists the user has listened to,” and the recommender system has already received all of this information as input. It could trivially return the user’s previously listened-to artists as top recommendations and score perfectly. But this is not useful, especially because the recommender’s role is to recommend artists that the user has never listened to.</p>
<p>To make this meaningful, some of the artist play data can be set aside and hidden from the ALS model-building process. Then, this held-out data can be interpreted as a collection of good recommendations for each user but one that the recommender has not already been given. The recommender is asked to rank all items in the model, and the ranks of the held-out artists are examined. Ideally, the recommender places all of them at or near the top of the list.</p>
<p>We can then compute the recommender’s score by comparing all held-out artists’ ranks to the rest.
(In practice, we compute this by examining only a sample of all such pairs, because a
potentially huge number of such pairs may exist.) The fraction of pairs where the held-out artist is
ranked higher is its score. A score of 1.0 is perfect, 0.0 is the worst possible score, and 0.5 is
the expected value achieved from randomly ranking artists.</p>
<p>This metric is directly related to an information retrieval concept<a data-primary="receiver operating characteristic (ROC) curve" data-type="indexterm" id="idm46507983110432"/><a data-primary="recommender engines" data-secondary="evaluating recommender quality" data-tertiary="receiver operating characteristic curve" data-type="indexterm" id="idm46507983109760"/><a data-primary="music recommender engine" data-secondary="evaluating recommender quality" data-tertiary="receiver operating characteristic curve" data-type="indexterm" id="idm46507983081008"/><a data-primary="receiver operating characteristic (ROC) curve" data-secondary="AUC metric" data-type="indexterm" id="idm46507983079920"/><a data-primary="recommender engines" data-secondary="evaluating recommender quality" data-tertiary="AUC metric" data-type="indexterm" id="idm46507983079072"/><a data-primary="music recommender engine" data-secondary="evaluating recommender quality" data-tertiary="AUC metric" data-type="indexterm" id="idm46507983077984"/><a data-primary="AUC (area under the curve) metric" data-type="indexterm" id="idm46507983076736"/> called the <a href="https://oreil.ly/Pt2bn">receiver operating characteristic (ROC) curve</a>. The metric in the preceding paragraph equals the area under this ROC curve and is indeed known as AUC, or area under the curve. AUC may be viewed as the probability that a randomly chosen good recommendation ranks above a randomly chosen bad recommendation.</p>
<p>The AUC metric is also used in<a data-primary="classifiers" data-secondary="AUC metric for evaluation" data-type="indexterm" id="idm46507983074752"/><a data-primary="MLlib component of Spark" data-secondary="BinaryClassificationMetrics class" data-type="indexterm" id="idm46507983073808"/><a data-primary="BinaryClassificationMetrics class" data-type="indexterm" id="idm46507983072832"/> the evaluation of classifiers. It is implemented, along with related
methods, in the MLlib class <code>BinaryClassificationMetrics</code>. For recommenders, we will compute AUC
<em>per user</em> and average the result. <a data-primary="mean AUC metric" data-type="indexterm" id="idm46507983071024"/><a data-primary="AUC (area under the curve) metric" data-secondary="mean AUC metric" data-type="indexterm" id="idm46507983070320"/>The resulting metric is slightly different and might be
called “mean AUC.” We will implement this, because it is not (quite) implemented in PySpark.</p>
<p>Other evaluation metrics that are relevant to systems that rank things are implemented in<a data-primary="RankingMetrics class" data-type="indexterm" id="idm46507983068688"/><a data-primary="mean average precision (MAP) metric" data-type="indexterm" id="idm46507983067984"/>
<code>RankingMetrics</code>. These include metrics like precision, recall, and <a href="https://oreil.ly/obbTT">mean average precision (MAP)</a>. MAP is also frequently used and focuses more narrowly on the quality of the top recommendations. However, AUC will be used here as a common and broad measure of the quality of the entire model output.</p>
<p>In fact, the process of holding out some data to select a model and evaluate its accuracy is common<a data-primary="models" data-secondary="scoring and model evaluation" data-tertiary="holding out data for" data-type="indexterm" id="idm46507983065504"/><a data-primary="data subsets for machine learning" data-secondary="holding out data for" data-type="indexterm" id="idm46507983064192"/>
practice in all of machine learning. <a data-primary="data subsets for machine learning" data-type="indexterm" id="idm46507983063104"/>Typically, data is divided into three subsets: training,
cross-validation (CV), and test sets. For simplicity in this initial example, only two sets will
be used: training and CV. This will be sufficient to choose a model. In <a data-type="xref" href="ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests">Chapter 4</a>,
this idea will be extended to include the test set.</p>
</div></section>
<section data-pdf-bookmark="Computing AUC" data-type="sect1"><div class="sect1" id="idm46507983115632">
<h1>Computing AUC</h1>
<p>An implementation of mean AUC<a data-primary="AUC (area under the curve) metric" data-secondary="computing AUC" data-type="indexterm" id="idm46507983059840"/><a data-primary="recommender engines" data-secondary="evaluating recommender quality" data-tertiary="computing AUC" data-type="indexterm" id="idm46507983058816"/><a data-primary="music recommender engine" data-secondary="evaluating recommender quality" data-tertiary="computing AUC" data-type="indexterm" id="idm46507983057584"/> is provided in the source code accompanying this book. It is not reproduced here, but is explained in some detail in comments in the source code. It accepts the CV set as the “positive” or “good” artists for each user and a prediction function. This function translates a dataframe containing each user-artist pair into a dataframe that also contains its estimated strength of interaction as a “prediction,” a number wherein higher values mean higher rank in the recommendations.</p>
<p>To use the input data, we must<a data-primary="data subsets for machine learning" data-secondary="computing AUC" data-type="indexterm" id="idm46507983055824"/> split it into a training set and a CV set. The ALS model will be trained on the training dataset only, and the CV set will be used to evaluate the model. Here, 90% of the data is used for training and the remaining 10% for cross-validation:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">area_under_curve</code><code class="p">(</code>
    <code class="n">positive_data</code><code class="p">,</code>
    <code class="n">b_all_artist_IDs</code><code class="p">,</code>
    <code class="n">predict_function</code><code class="p">):</code>
<code class="o">...</code>

<code class="n">all_data</code> <code class="o">=</code> <code class="n">user_artist_df</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">broadcast</code><code class="p">(</code><code class="n">artist_alias</code><code class="p">),</code> <code class="s1">'artist'</code><code class="p">,</code> <code class="n">how</code><code class="o">=</code><code class="s1">'left'</code><code class="p">)</code> \
    <code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'artist'</code><code class="p">,</code> <code class="n">when</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'alias'</code><code class="p">)</code><code class="o">.</code><code class="n">isNull</code><code class="p">(),</code> <code class="n">col</code><code class="p">(</code><code class="s1">'artist'</code><code class="p">))</code>\
    <code class="o">.</code><code class="n">otherwise</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s1">'alias'</code><code class="p">)))</code> \
    <code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'artist'</code><code class="p">,</code> <code class="n">col</code><code class="p">(</code><code class="s1">'artist'</code><code class="p">)</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">IntegerType</code><code class="p">()))</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s1">'alias'</code><code class="p">)</code>

<code class="n">train_data</code><code class="p">,</code> <code class="n">cv_data</code> <code class="o">=</code> <code class="n">all_data</code><code class="o">.</code><code class="n">randomSplit</code><code class="p">([</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.1</code><code class="p">],</code> <code class="n">seed</code><code class="o">=</code><code class="mi">54321</code><code class="p">)</code>
<code class="n">train_data</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code>
<code class="n">cv_data</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code>

<code class="n">all_artist_ids</code> <code class="o">=</code> <code class="n">all_data</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"artist"</code><code class="p">)</code><code class="o">.</code><code class="n">distinct</code><code class="p">()</code><code class="o">.</code><code class="n">count</code><code class="p">()</code>
<code class="n">b_all_artist_ids</code> <code class="o">=</code> <code class="n">broadcast</code><code class="p">(</code><code class="n">all_artist_ids</code><code class="p">)</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">ALS</code><code class="p">(</code><code class="n">rank</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">maxIter</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">regParam</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code>
            <code class="n">implicitPrefs</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">userCol</code><code class="o">=</code><code class="s1">'user'</code><code class="p">,</code>
            <code class="n">itemCol</code><code class="o">=</code><code class="s1">'artist'</code><code class="p">,</code> <code class="n">ratingCol</code><code class="o">=</code><code class="s1">'count'</code><code class="p">)</code> \
        <code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">train_data</code><code class="p">)</code>
<code class="n">area_under_curve</code><code class="p">(</code><code class="n">cv_data</code><code class="p">,</code> <code class="n">b_all_artist_ids</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">transform</code><code class="p">)</code></pre>
<p>Note that <code>areaUnderCurve</code> accepts a function as its third argument. Here, the <code>transform</code> method from
<code>ALSModel</code> is passed in, but it will shortly be swapped out for an alternative.</p>
<p>The result is about 0.879. Is this good? It is certainly higher than the 0.5 that is expected from making recommendations randomly, and it’s close to 1.0, which is the maximum possible score. Generally, an AUC over 0.9 would be considered high.</p>
<p>But is it an accurate evaluation? This evaluation could be repeated with a different 90% as the training set. The resulting AUC values’ average might be a better estimate of the algorithm’s performance on the dataset. In fact, one common practice is to divide the data into <em>k</em> subsets of similar size, use <em>k</em> – 1 subsets together for training, and evaluate on the remaining subset. We can repeat this <em>k</em> times, using a different set of subsets each time. <a data-primary="cross-validation, k-fold" data-type="indexterm" id="idm46507982907728"/><a data-primary="k-fold cross-validation" data-type="indexterm" id="idm46507982907056"/><a data-primary="MLlib component of Spark" data-secondary="CrossValidator API" data-type="indexterm" id="idm46507982906384"/><a data-primary="CrossValidator API" data-type="indexterm" id="idm46507982905472"/>This is called <a href="https://oreil.ly/DolrQ"><em>k-fold cross-validation</em></a>. This won’t be implemented in examples here, for simplicity, but some support for this technique exists in MLlib in its <code>CrossValidator</code> API. The validation API will be revisited in <a data-type="xref" href="ch04.xhtml#RandomDecisionForests">“Random Forests”</a>.</p>
<p>It’s helpful to benchmark this against a simpler approach. For example, consider recommending
the globally most-played artists to every user. This is not personalized, but it is simple and may be effective.
Define this simple prediction function and evaluate its AUC score:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="nb">sum</code> <code class="k">as</code> <code class="n">_sum</code>

<code class="k">def</code> <code class="nf">predict_most_listened</code><code class="p">(</code><code class="n">train</code><code class="p">):</code>
    <code class="n">listen_counts</code> <code class="o">=</code> <code class="n">train</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"artist"</code><code class="p">)</code>\
                    <code class="o">.</code><code class="n">agg</code><code class="p">(</code><code class="n">_sum</code><code class="p">(</code><code class="s2">"count"</code><code class="p">)</code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"prediction"</code><code class="p">))</code>\
                    <code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"artist"</code><code class="p">,</code> <code class="s2">"prediction"</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">all_data</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">listen_counts</code><code class="p">,</code> <code class="s2">"artist"</code><code class="p">,</code> <code class="s2">"left_outer"</code><code class="p">)</code><code class="o">.</code>\
                    <code class="n">select</code><code class="p">(</code><code class="s2">"user"</code><code class="p">,</code> <code class="s2">"artist"</code><code class="p">,</code> <code class="s2">"prediction"</code><code class="p">)</code>

<code class="n">area_under_curve</code><code class="p">(</code><code class="n">cv_data</code><code class="p">,</code> <code class="n">b_all_artist_ids</code><code class="p">,</code> <code class="n">predict_most_listened</code><code class="p">(</code><code class="n">train_data</code><code class="p">))</code></pre>
<p>The result is also about 0.880. This suggests that nonpersonalized recommendations are already fairly effective
according to this metric. However, we’d expect the “personalized” recommendations to score better in comparison.
Clearly, the model needs some tuning. Can it be made better?</p>
</div></section>
<section data-pdf-bookmark="Hyperparameter Selection" data-type="sect1"><div class="sect1" id="idm46507983060816">
<h1>Hyperparameter Selection</h1>
<p>So far, the hyperparameter values<a data-primary="hyperparameters" data-secondary="recommender engines" data-type="indexterm" id="ch03-hyper"/><a data-primary="recommender engines" data-secondary="hyperparameters" data-tertiary="hyperparameter selection" data-type="indexterm" id="ch03-hyper2"/><a data-primary="music recommender engine" data-secondary="hyperparameters" data-tertiary="hyperparameter selection" data-type="indexterm" id="ch03-hyper3"/><a data-primary="models" data-secondary="hyperparameters" data-tertiary="hyperparameter selection" data-type="indexterm" id="ch03-hyper4"/> used to build the <code>ALSModel</code> were simply given without comment.
They are not learned by the algorithm and must be chosen by the caller. The configured hyperparameters were:</p>
<dl>
<dt><code>setRank(10)</code></dt>
<dd>
<p>The number of latent factors in the model, or equivalently, the number of columns <em>k</em> in the user-feature
and product-feature matrices. In nontrivial cases, this is also their rank.</p>
</dd>
<dt><code>setMaxIter(5)</code></dt>
<dd>
<p>The number of iterations that the factorization runs. More iterations take more time but may
produce a better factorization.</p>
</dd>
<dt><code>setRegParam(0.01)</code></dt>
<dd>
<p>A standard overfitting parameter, also usually called <em>lambda</em>. Higher values resist overfitting, but values that are too
high hurt the factorization’s accuracy.</p>
</dd>
<dt><code>setAlpha(1.0)</code></dt>
<dd>
<p>Controls the relative weight of observed versus unobserved user-product interactions in the factorization.</p>
</dd>
</dl>
<p><code>rank</code>, <code>regParam</code>, and <code>alpha</code> can be considered <em>hyperparameters</em> to the model.
(<code>maxIter</code> is more of a constraint on resources used in the factorization.) These are not values that end up in the matrices inside the <code>ALSModel</code>—those are simply its <em>parameters</em> and are chosen by the algorithm. These hyperparameters are instead parameters to the process of building itself.</p>
<p>The values used in the preceding list are not necessarily optimal. Choosing good hyperparameter values is a common problem in machine learning. The most basic way to choose values is to simply try combinations of values and evaluate a metric for each of them, and choose the combination that produces the best value of the metric.</p>
<p>In the following example, eight possible combinations are tried: <code>rank</code> = 5 or 30, <code>regParam</code> = 4.0 or 0.0001, and <code>alpha</code> = 1.0 or 40.0. These values are still something of a guess, but are chosen to cover a broad range of parameter values. The results are printed in order by top AUC score:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pprint</code><code> </code><code class="kn">import</code><code> </code><code class="n">pprint</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">itertools</code><code> </code><code class="kn">import</code><code> </code><code class="n">product</code><code>
</code><code>
</code><code class="n">ranks</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="mi">5</code><code class="p">,</code><code> </code><code class="mi">30</code><code class="p">]</code><code>
</code><code class="n">reg_params</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="mf">4.0</code><code class="p">,</code><code> </code><code class="mf">0.0001</code><code class="p">]</code><code>
</code><code class="n">alphas</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="mf">1.0</code><code class="p">,</code><code> </code><code class="mf">40.0</code><code class="p">]</code><code>
</code><code class="n">hyperparam_combinations</code><code> </code><code class="o">=</code><code> </code><code class="nb">list</code><code class="p">(</code><code class="n">product</code><code class="p">(</code><code class="o">*</code><code class="p">[</code><code class="n">ranks</code><code class="p">,</code><code> </code><code class="n">reg_params</code><code class="p">,</code><code> </code><code class="n">alphas</code><code class="p">]</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code class="n">evaluations</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>
</code><code class="k">for</code><code> </code><code class="n">c</code><code> </code><code class="ow">in</code><code> </code><code class="n">hyperparam_combinations</code><code class="p">:</code><code>
</code><code>    </code><code class="n">rank</code><code> </code><code class="o">=</code><code> </code><code class="n">c</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>
</code><code>    </code><code class="n">reg_param</code><code> </code><code class="o">=</code><code> </code><code class="n">c</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code>
</code><code>    </code><code class="n">alpha</code><code> </code><code class="o">=</code><code> </code><code class="n">c</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code>
</code><code>    </code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">ALS</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setSeed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">setImplicitPrefs</code><code class="p">(</code><code class="n">true</code><code class="p">)</code><code class="o">.</code><code class="n">setRank</code><code class="p">(</code><code class="n">rank</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                  </code><code class="n">setRegParam</code><code class="p">(</code><code class="n">reg_param</code><code class="p">)</code><code class="o">.</code><code class="n">setAlpha</code><code class="p">(</code><code class="n">alpha</code><code class="p">)</code><code class="o">.</code><code class="n">setMaxIter</code><code class="p">(</code><code class="mi">20</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                  </code><code class="n">setUserCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">setItemCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">artist</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                  </code><code class="n">setRatingCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">count</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">setPredictionCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">prediction</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>        </code><code class="n">fit</code><code class="p">(</code><code class="n">trainData</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">auc</code><code> </code><code class="o">=</code><code> </code><code class="n">area_under_curve</code><code class="p">(</code><code class="n">cv_aata</code><code class="p">,</code><code> </code><code class="n">b_all_artist_ids</code><code class="p">,</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">transform</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">model</code><code class="o">.</code><code class="n">userFactors</code><code class="o">.</code><code class="n">unpersist</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_recommending_music_and_the_audioscrobbler_dataset_CO3-1" id="co_recommending_music_and_the_audioscrobbler_dataset_CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>    </code><code class="n">model</code><code class="o">.</code><code class="n">itemFactors</code><code class="o">.</code><code class="n">unpersist</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">evaluations</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">(</code><code class="n">auc</code><code class="p">,</code><code> </code><code class="p">(</code><code class="n">rank</code><code class="p">,</code><code> </code><code class="n">regParam</code><code class="p">,</code><code> </code><code class="n">alpha</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code class="n">evaluations</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">key</code><code class="o">=</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code class="n">x</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code> </code><a class="co" href="#callout_recommending_music_and_the_audioscrobbler_dataset_CO3-2" id="co_recommending_music_and_the_audioscrobbler_dataset_CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code class="n">pprint</code><code class="p">(</code><code class="n">evaluations</code><code class="p">)</code><code>
</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="p">(</code><code class="mf">0.8928367485129145</code><code class="p">,</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code><code class="mf">4.0</code><code class="p">,</code><code class="mf">40.0</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mf">0.891835487024326</code><code class="p">,</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code><code class="mf">1.0E-4</code><code class="p">,</code><code class="mf">40.0</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mf">0.8912376926662007</code><code class="p">,</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code><code class="mf">4.0</code><code class="p">,</code><code class="mf">1.0</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mf">0.889240668173946</code><code class="p">,</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mf">4.0</code><code class="p">,</code><code class="mf">40.0</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mf">0.8886268430389741</code><code class="p">,</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mf">4.0</code><code class="p">,</code><code class="mf">1.0</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mf">0.8883278461068959</code><code class="p">,</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mf">1.0E-4</code><code class="p">,</code><code class="mf">40.0</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mf">0.8825350012228627</code><code class="p">,</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mf">1.0E-4</code><code class="p">,</code><code class="mf">1.0</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mf">0.8770527940660278</code><code class="p">,</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code><code class="mf">1.0E-4</code><code class="p">,</code><code class="mf">1.0</code><code class="p">)</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_recommending_music_and_the_audioscrobbler_dataset_CO3-1" id="callout_recommending_music_and_the_audioscrobbler_dataset_CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Free up model resources immediately.</p></dd>
<dt><a class="co" href="#co_recommending_music_and_the_audioscrobbler_dataset_CO3-2" id="callout_recommending_music_and_the_audioscrobbler_dataset_CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Sort by first value (AUC), descending, and print.</p></dd>
</dl>
<p>The differences are small in absolute terms, but are still somewhat significant for AUC values. Interestingly, the parameter <code>alpha</code> seems consistently better at 40 than 1. (For the curious, 40 was a value proposed as a default in one of the original ALS papers <span class="keep-together">mentioned</span> earlier.) This can be interpreted as indicating that the model is better off focusing far more on what the user did listen to than what he or she did not listen to.</p>
<p>A higher <code>regParam</code> looks better too. <a data-primary="overfitting" data-secondary="hyperparameter selection and" data-type="indexterm" id="idm46507982293920"/><a data-primary="hyperparameters" data-secondary="overfitting" data-tertiary="hyperparameter selection and" data-type="indexterm" id="idm46507982292944"/>This suggests the model is somewhat susceptible to overfitting,
and so needs a higher <code>regParam</code> to resist trying to fit the sparse input given from each user too exactly.
Overfitting will be revisited in more detail in <a data-type="xref" href="ch04.xhtml#RandomDecisionForests">“Random Forests”</a>.</p>
<p>As expected, 5 features are pretty low for a model of this size, and it underperforms the model that uses 30 features to explain tastes. It’s possible that the best number of features is actually higher than 30 and that these values are alike in being too small.</p>
<p>Of course, this process can be repeated for different ranges of values or more values. It is a brute-force means of choosing hyperparameters. However, in a world where clusters with terabytes of memory and hundreds of cores are not uncommon, and with frameworks like Spark that can exploit parallelism and memory for speed, it becomes quite feasible.</p>
<p>It is not strictly required to understand what the hyperparameters mean, although it is helpful to know
what normal ranges of values are in order to start the search over a parameter space that is neither too
large nor too tiny.</p>
<p>This was a fairly manual way to loop over hyperparameters, build models, and evaluate them.
In <a data-type="xref" href="ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests">Chapter 4</a>, after learning more about the Spark ML API, we’ll find that
there is a more automated way to compute this using <code>Pipeline</code>s and <code>TrainValidationSplit</code>.<a data-startref="ch03-hyper" data-type="indexterm" id="idm46507981989040"/><a data-startref="ch03-hyper2" data-type="indexterm" id="idm46507981988304"/><a data-startref="ch03-hyper3" data-type="indexterm" id="idm46507981987632"/><a data-startref="ch03-hyper4" data-type="indexterm" id="idm46507981986960"/></p>
</div></section>
<section data-pdf-bookmark="Making Recommendations" data-type="sect1"><div class="sect1" id="idm46507982898048">
<h1>Making Recommendations</h1>
<p>Proceeding for the moment with the<a data-primary="recommender engines" data-secondary="making recommendations" data-type="indexterm" id="idm46507981984880"/><a data-primary="music recommender engine" data-secondary="making recommendations" data-type="indexterm" id="idm46507981983904"/> best set of hyperparameters, what does the new model recommend for user 2093760?</p>
<pre data-type="programlisting">+-----------+
|       name|
+-----------+
|  [unknown]|
|The Beatles|
|     Eminem|
|         U2|
|  Green Day|
+-----------+</pre>
<p>Anecdotally, this makes a bit more sense for this user, being dominated by pop rock instead of all hip-hop.<a data-primary="missing data values" data-secondary="[unknown] value" data-secondary-sortas="unknown" data-type="indexterm" id="idm46507981981664"/><a data-primary="iteration in data analysis" data-secondary="[unknown] value" data-secondary-sortas="unknown" data-type="indexterm" id="idm46507981980416"/><a data-primary="[unknown] value" data-primary-sortas="unknown" data-type="indexterm" id="idm46507981979232"/> <code>[unknown]</code> is plainly not an artist. Querying the original dataset reveals that it occurs 429,447 times, putting it nearly in the top 100! This is some default value for plays without an artist, maybe supplied by a certain scrobbling client. It is not useful information, and we should discard it from the input before starting again. It is an example of how the practice of data science is often iterative, with discoveries about the data occurring at every stage.</p>
<p>This model can be used to make recommendations for all users. This could be useful in a batch process that recomputes a model and recommendations for users every hour or even less, depending on
the size of the data and speed of the cluster.</p>
<p>At the moment, however, Spark MLlib’s ALS implementation does not support a method to recommend to all users. It is possible to recommend to one user at a time, as shown above, although each will launch a short-lived distributed job that takes a few seconds. This may be suitable for rapidly recomputing recommendations for small groups of users. Here, recommendations are made to 100 users taken from the data and printed:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">some_users</code><code> </code><code class="o">=</code><code> </code><code class="n">all_data</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"</code><code class="s2">user</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">distinct</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">limit</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code><code> </code><a class="co" href="#callout_recommending_music_and_the_audioscrobbler_dataset_CO4-1" id="co_recommending_music_and_the_audioscrobbler_dataset_CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code class="n">val</code><code> </code><code class="n">someRecommendations</code><code> </code><code class="o">=</code><code>
</code><code>  </code><code class="n">someUsers</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">userID</code><code> </code><code class="o">=</code><code class="o">&gt;</code><code> </code><code class="p">(</code><code class="n">userID</code><code class="p">,</code><code> </code><code class="n">makeRecommendations</code><code class="p">(</code><code class="n">model</code><code class="p">,</code><code> </code><code class="n">userID</code><code class="p">,</code><code> </code><code class="mi">5</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="n">someRecommendations</code><code class="o">.</code><code class="n">foreach</code><code> </code><code class="p">{</code><code> </code><code class="n">case</code><code> </code><code class="p">(</code><code class="n">userID</code><code class="p">,</code><code> </code><code class="n">recsDF</code><code class="p">)</code><code> </code><code class="o">=</code><code class="o">&gt;</code><code>
</code><code>  </code><code class="n">val</code><code> </code><code class="n">recommendedArtists</code><code> </code><code class="o">=</code><code> </code><code class="n">recsDF</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"</code><code class="s2">artist</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="k">as</code><code class="p">[</code><code class="n">Int</code><code class="p">]</code><code class="o">.</code><code class="n">collect</code><code class="p">(</code><code class="p">)</code><code>
</code><code>  </code><code class="n">println</code><code class="p">(</code><code class="n">s</code><code class="s2">"</code><code class="s2">$userID -&gt; $</code><code class="s2">{</code><code class="s2">recommendedArtists.mkString(</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">)}</code><code class="s2">"</code><code class="p">)</code><code>
</code><code class="p">}</code><code>
</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="mi">1000190</code><code> </code><code class="o">-</code><code class="o">&gt;</code><code> </code><code class="mi">6694932</code><code class="p">,</code><code> </code><code class="mi">435</code><code class="p">,</code><code> </code><code class="mi">1005820</code><code class="p">,</code><code> </code><code class="mi">58</code><code class="p">,</code><code> </code><code class="mi">1244362</code><code>
</code><code class="mi">1001043</code><code> </code><code class="o">-</code><code class="o">&gt;</code><code> </code><code class="mi">1854</code><code class="p">,</code><code> </code><code class="mi">4267</code><code class="p">,</code><code> </code><code class="mi">1006016</code><code class="p">,</code><code> </code><code class="mi">4468</code><code class="p">,</code><code> </code><code class="mi">1274</code><code>
</code><code class="mi">1001129</code><code> </code><code class="o">-</code><code class="o">&gt;</code><code> </code><code class="mi">234</code><code class="p">,</code><code> </code><code class="mi">1411</code><code class="p">,</code><code> </code><code class="mi">1307</code><code class="p">,</code><code> </code><code class="mi">189</code><code class="p">,</code><code> </code><code class="mi">121</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_recommending_music_and_the_audioscrobbler_dataset_CO4-1" id="callout_recommending_music_and_the_audioscrobbler_dataset_CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Subset of 100 distinct users</p></dd>
</dl>
<p>Here, the recommendations are just printed. <a data-primary="HBase (Apache)" data-type="indexterm" id="idm46507982333312"/><a data-primary="Apache HBase" data-type="indexterm" id="idm46507982332640"/><a data-primary="persistent storage" data-secondary="Apache HBase" data-type="indexterm" id="idm46507982331968"/>They could just as easily be written to an external store like <a href="https://oreil.ly/SQImy">HBase</a>, which provides fast lookup at runtime.</p>
</div></section>
<section data-pdf-bookmark="Where to Go from Here" data-type="sect1"><div class="sect1" id="idm46507982511872">
<h1>Where to Go from Here</h1>
<p>Naturally, it’s possible to spend more time tuning the model parameters and finding and fixing anomalies in the input, like the <code>[unknown]</code> artist. For example, a quick analysis of play counts reveals that user
2064012 played artist 4468 an astonishing 439,771 times! Artist 4468 is the implausibly successful alternate-metal band System of a Down,
which turned up earlier in recommendations. Assuming an average song length of 4 minutes, this is over 33 years of playing hits like “Chop Suey!” and “B.Y.O.B.” Because the band started making records in 1998, this would require playing four or five tracks at once for seven years. It must be spam or a data error, and another example of the types of real-world data problems that a production system would have to address.</p>
<p>ALS is not the only possible recommender algorithm, but at this time, it is the only one supported by
Spark MLlib. However, MLlib also supports a variant of ALS for non-implicit data. <a data-primary="Alternating Least Squares (ALS) algorithm" data-secondary="non-implicit data" data-type="indexterm" id="idm46507982509440"/><a data-primary="implicit feedback data" data-secondary="non-implicit data" data-type="indexterm" id="idm46507982508496"/><a data-primary="non-implicit data in Alternating Least Squares" data-type="indexterm" id="idm46507982507552"/>Its use is identical,
except that <code>ALS</code> is configured with <code>setImplicitPrefs(false)</code>. This is appropriate when data is rating-like,
rather than count-like. For example, it is appropriate when the dataset is user ratings of artists on a 1–5 scale.
The resulting <code>prediction</code> column returned from <code>ALSModel.transform</code> recommendation methods then really is
an estimated rating. In this case, the simple RMSE (root mean squared error) metric is appropriate for
evaluating the recommender.</p>
<p>Later, other recommender algorithms may be available in Spark MLlib or other libraries.</p>
<p>In production, recommender engines often need to make<a data-primary="scalability" data-secondary="recommendation engine requirements" data-type="indexterm" id="idm46507982504384"/><a data-primary="recommender engines" data-secondary="scalability" data-type="indexterm" id="idm46507982503440"/><a data-primary="music recommender engine" data-secondary="scalability" data-type="indexterm" id="idm46507982502496"/> recommendations in real time, because they are used in contexts like ecommerce sites where recommendations are requested frequently as customers browse product pages. Precomputing and storing recommendations is a reasonable way to make recommendations available at scale. One disadvantage of this approach is that it requires precomputing recommendations for all users who might need recommendations soon, which is potentially any of them. For example, if only 10,000 of 1 million users visit a site in a day, precomputing all million users’ recommendations each day is 99% wasted effort.</p>
<p>It would be nicer to compute recommendations on the fly, as needed. While we can compute recommendations for one user using the <code>ALSModel</code>, this is necessarily a distributed operation that takes several seconds, because <code>ALSModel</code> is uniquely large and therefore actually a distributed dataset. This is not true of other models, which afford much faster scoring.</p>
</div></section>
</div></section></div></body></html>