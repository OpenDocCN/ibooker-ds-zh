- en: Chapter 6\. Time Series Analysis with pandas
  prefs: []
  type: TYPE_NORMAL
- en: 'A time series is a series of data points along a time-based axis that plays
    a central role in many different scenarios: while traders use historical stock
    prices to calculate risk measures, the weather forecast is based on time series
    generated by sensors measuring temperature, humidity, and air pressure. And the
    digital marketing department relies on time series generated by web pages, e.g.,
    the source and number of page views per hour, and will use them to draw conclusions
    with regard to their marketing campaigns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Time series analysis is one of the main driving forces why data scientists
    and analysts have started to look for a better alternative to Excel. The following
    points summarize some of the reasons behind this move:'
  prefs: []
  type: TYPE_NORMAL
- en: Big datasets
  prefs: []
  type: TYPE_NORMAL
- en: Time series can quickly grow beyond Excel’s limit of roughly one million rows
    per sheet. For example, if you work with intraday stock prices on a tick data
    level, you’re often dealing with hundreds of thousands of records—per stock and
    day!
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Date and time
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen in [Chapter 3](index_split_010.html#filepos178328), Excel has
    various limitations when it comes to handling date and time, the backbone of time
    series. Missing support for time zones and a number format that is limited to
    milliseconds are some of them. pandas supports time zones and uses NumPy’s `datetime64[ns]`
    data type, which offers a resolution in up to nanoseconds.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: Missing functionality
  prefs: []
  type: TYPE_NORMAL
- en: Excel misses even basic tools to be able to work with time series data in a
    decent way. For example, if you want to turn a daily time series into a monthly
    time series, there is no easy way of doing this despite it being a very common
    task.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'DataFrames allow you to work with various time-based indices: `DatetimeIndex`
    is the most common one and represents an index with timestamps. Other index types,
    like `PeriodIndex`, are based on time intervals such as hours or months. In this
    chapter, however, we are only looking at `DatetimeIndex`, which I will introduce
    now in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: DatetimeIndex
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll learn how to construct a `DatetimeIndex`, how to filter
    such an index to a specific time range, and how to work with time zones.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DatetimeIndex
  prefs: []
  type: TYPE_NORMAL
- en: 'To construct a `DatetimeIndex`, pandas offers the `date_range` function. It
    accepts a start date, a frequency, and either the number of periods or the end
    date:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``1``]:``# Let''s start by importing the packages we use in this chapter``#
    and by setting the plotting backend to Plotly``import``pandas``as``pd``import``numpy``as``np``pd``.``options``.``plotting``.``backend``=``"plotly"`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``2``]:``# This creates a DatetimeIndex based on a start timestamp,``#
    number of periods and frequency ("D" = daily).``daily_index``=``pd``.``date_range``(``"2020-02-28"``,``periods``=``4``,``freq``=``"D"``)``daily_index`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[2]: DatetimeIndex([''2020-02-28'', ''2020-02-29'', ''2020-03-01'', ''2020-03-02''],
             dtype=''datetime64[ns]'', freq=''D'')`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``3``]:``# This creates a DatetimeIndex based on start/end timestamp.``#
    The frequency is set to "weekly on Sundays" ("W-SUN").``weekly_index``=``pd``.``date_range``(``"2020-01-01"``,``"2020-01-31"``,``freq``=``"W-SUN"``)``weekly_index`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[3]: DatetimeIndex([''2020-01-05'', ''2020-01-12'', ''2020-01-19'', ''2020-01-26''],
             dtype=''datetime64[ns]'', freq=''W-SUN'')`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``4``]:``# Construct a DataFrame based on the weekly_index. This could
    be``# the visitor count of a museum that only opens on Sundays.``pd``.``DataFrame``(``data``=``[``21``,``15``,``33``,``34``],``columns``=``[``"visitors"``],``index``=``weekly_index``)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[4]:             visitors         2020-01-05        21         2020-01-12       
    15         2020-01-19        33         2020-01-26        34`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s now return to the Microsoft stock time series from the last chapter.
    When you take a closer look at the data types of the columns, you will notice
    that the `Date` column has the type `object`, which means that pandas has interpreted
    the timestamps as strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``5``]:``msft``=``pd``.``read_csv``(``"csv/MSFT.csv"``)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``6``]:``msft``.``info``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`<class ''pandas.core.frame.DataFrame''> RangeIndex: 8622 entries, 0 to 8621
    Data columns (total 7 columns): #   Column     Non-Null Count  Dtype ---  ------    
    --------------  ----- 0   Date       8622 non-null   object 1   Open       8622
    non-null   float64 2   High       8622 non-null   float64 3   Low        8622
    non-null   float64 4   Close      8622 non-null   float64 5   Adj Close  8622
    non-null   float64 6   Volume     8622 non-null   int64 dtypes: float64(5), int64(1),
    object(1) memory usage: 471.6+ KB`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'There are two ways to fix this and turn it into a `datetime` data type. The
    first one is to run the `to_datetime` function on that column. Make sure to assign
    the transformed column back to the original DataFrame if you want to change it
    at the source:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``7``]:``msft``.``loc``[:,``"Date"``]``=``pd``.``to_datetime``(``msft``[``"Date"``])`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``8``]:``msft``.``dtypes`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[8]: Date         datetime64[ns]         Open                float64        
    High                float64         Low                 float64         Close              
    float64         Adj Close           float64         Volume                int64
            dtype: object`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The other possibility is to tell `read_csv` about the columns that contain
    timestamps by using the `parse_dates` argument. `parse_dates` expects a list of
    column names or indices. Also, you almost always want to turn timestamps into
    the index of the DataFrame since this will allow you to filter the data easily,
    as we will see in a moment. To spare yourself an extra `set_index` call, provide
    the column you would like to use as index via the `index_col` argument, again
    as column name or index:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``9``]:``msft``=``pd``.``read_csv``(``"csv/MSFT.csv"``,``index_col``=``"Date"``,``parse_dates``=``[``"Date"``])`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``10``]:``msft``.``info``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`<class ''pandas.core.frame.DataFrame''> DatetimeIndex: 8622 entries, 1986-03-13
    to 2020-05-27 Data columns (total 6 columns): #   Column     Non-Null Count  Dtype
    ---  ------     --------------  ----- 0   Open       8622 non-null   float64 1  
    High       8622 non-null   float64 2   Low        8622 non-null   float64 3  
    Close      8622 non-null   float64 4   Adj Close  8622 non-null   float64 5  
    Volume     8622 non-null   int64 dtypes: float64(5), int64(1) memory usage: 471.5
    KB`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As `info` reveals, you are now dealing with a DataFrame that has a `DatetimeIndex`.
    If you would need to change another data type (let’s say you wanted `Volume` to
    be a `float` instead of an `int`), you again have two options: either provide
    `dtype={"Volume": float}` as argument to the `read_csv` function, or apply the
    `astype` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``11``]:``msft``.``loc``[:,``"Volume"``]``=``msft``[``"Volume"``]``.``astype``(``"float"``)``msft``[``"Volume"``]``.``dtype`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[11]: dtype(''float64'')`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With time series, it’s always a good idea to make sure the index is sorted
    properly before starting your analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``12``]:``msft``=``msft``.``sort_index``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'And finally, if you need to access only parts of a `DatetimeIndex`, like the
    date part without the time, access the `date` attribute like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``13``]:``msft``.``index``.``date`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[13]: array([datetime.date(1986, 3, 13), datetime.date(1986, 3, 14),                
    datetime.date(1986, 3, 17), ..., datetime.date(2020, 5, 22),                 datetime.date(2020,
    5, 26), datetime.date(2020, 5, 27)],                dtype=object)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instead of `date`, you can also use parts of a date like `year`, `month`, `day`,
    etc. To access the same functionality on a regular column with data type `datetime`,
    you will have to use the `dt` attribute, e.g., `df["column_name"].dt.date`.
  prefs: []
  type: TYPE_NORMAL
- en: With a sorted `DatetimeIndex`, let’s see how we can filter the DataFrame to
    certain time periods!
  prefs: []
  type: TYPE_NORMAL
- en: Filtering a DatetimeIndex
  prefs: []
  type: TYPE_NORMAL
- en: 'If your DataFrame has a `DatetimeIndex`, there is an easy way to select rows
    from a specific time period by using `loc` with a string in the format `YYYY-MM-DD
    HH:MM:SS`. pandas will turn this string into a slice so it covers the whole period.
    For example, to select all rows from 2019, provide the year as a string, not a
    number:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``14``]:``msft``.``loc``[``"2019"``,``"Adj Close"``]`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[14]: Date          2019-01-02     99.099190          2019-01-03     95.453529
             2019-01-04     99.893005          2019-01-07    100.020401          2019-01-08   
    100.745613                           ...          2019-12-24    156.515396         
    2019-12-26    157.798309          2019-12-27    158.086731          2019-12-30   
    156.724243          2019-12-31    156.833633          Name: Adj Close, Length:
    252, dtype: float64`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s take this a step further and plot the data between June 2019 and May
    2020 (see [Figure 6-1](#filepos798528)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``15``]:``msft``.``loc``[``"2019-06"``:``"2020-05"``,``"Adj Close"``]``.``plot``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Adjusted close price for MSFT
  prefs: []
  type: TYPE_NORMAL
- en: Hover over the Plotly chart to read off the value as a tooltip and zoom in by
    drawing a rectangle with your mouse. Double-click the chart to get back to the
    default view.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the adjusted close price in the next section to learn about time zone
    handling.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Time Zones
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft is listed on the Nasdaq stock exchange. The Nasdaq is in New York
    and markets close at 4:00 p.m. To add this additional information to the DataFrame’s
    index, first add the closing hour to the date via `DateOffset`, then attach the
    correct time zone to the timestamps via `tz_localize`. Since the closing hour
    is only applicable to the close price, let’s create a new DataFrame with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``16``]:``# Add the time information to the date``msft_close``=``msft``.``loc``[:,``[``"Adj
    Close"``]]``.``copy``()``msft_close``.``index``=``msft_close``.``index``+``pd``.``DateOffset``(``hours``=``16``)``msft_close``.``head``(``2``)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[16]:                      Adj Close          Date          1986-03-13
    16:00:00   0.062205          1986-03-14 16:00:00   0.064427`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``17``]:``# Make the timestamps time-zone-aware``msft_close``=``msft_close``.``tz_localize``(``"America/New_York"``)``msft_close``.``head``(``2``)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[17]:                            Adj Close          Date          1986-03-13
    16:00:00-05:00   0.062205          1986-03-14 16:00:00-05:00   0.064427`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you want to convert the timestamps to UTC time zone, use the DataFrame method
    `tz_convert`. UTC stands for Coordinated Universal Time and is the successor of
    Greenwich Mean Time (GMT). Note how the closing hours change in UTC depending
    on whether daylight saving time (DST) is in effect or not in New York:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``18``]:``msft_close``=``msft_close``.``tz_convert``(``"UTC"``)``msft_close``.``loc``[``"2020-01-02"``,``"Adj
    Close"``]``# 21:00 without DST`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[18]: Date          2020-01-02 21:00:00+00:00    159.737595          Name:
    Adj Close, dtype: float64`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``19``]:``msft_close``.``loc``[``"2020-05-01"``,``"Adj Close"``]``# 20:00
    with DST`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[19]: Date          2020-05-01 20:00:00+00:00    174.085175          Name:
    Adj Close, dtype: float64`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Preparing time series like this will allow you to compare close prices from
    stock exchanges across different time zones even if the time info is missing or
    stated in the local time zone.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what a `DatetimeIndex` is, let’s try out a few common time
    series manipulations in the next section by calculating and comparing stock performance.
  prefs: []
  type: TYPE_NORMAL
- en: Common Time Series Manipulations
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll show you how to perform common time series analysis tasks
    such as calculating stock returns, plotting the performance of various stocks,
    and visualizing the correlation of their returns in a heatmap. We’ll also see
    how to change the frequency of time series and how to calculate rolling statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting and Percentage Changes
  prefs: []
  type: TYPE_NORMAL
- en: In finance, the log returns of stocks are often assumed to be normally distributed.
    By log returns, I mean the natural logarithm of the ratio of the current and previous
    price. To get a feeling for the distribution of the daily log returns, let’s plot
    a histogram. First, however, we need to calculate the log returns. In Excel, it’s
    typically done with a formula that involves cells from two rows, as shown in [Figure 6-2](#filepos810136).
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/00036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Calculating log returns in Excel
  prefs: []
  type: TYPE_NORMAL
- en: LOGARITHMS IN EXCEL AND PYTHON
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Excel uses `LN` to denote the natural logarithm and `LOG` for the logarithm
    with base 10\. Python’s math module and NumPy, however, use `log` for the natural
    logarithm and `log10` for the logarithm with base 10.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'With pandas, rather than having a formula accessing two different rows, you
    use the `shift` method to shift the values down by one row. This allows you to
    operate on a single row so your calculations can make use of vectorization. `shift`
    accepts a positive or negative integer that shifts the time series down or up
    by the respective number of rows. Let’s first see how `shift` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``20``]:``msft_close``.``head``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[20]:                            Adj Close          Date          1986-03-13
    21:00:00+00:00   0.062205          1986-03-14 21:00:00+00:00   0.064427         
    1986-03-17 21:00:00+00:00   0.065537          1986-03-18 21:00:00+00:00   0.063871
             1986-03-19 21:00:00+00:00   0.062760`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``21``]:``msft_close``.``shift``(``1``)``.``head``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[21]:                            Adj Close          Date          1986-03-13
    21:00:00+00:00        NaN          1986-03-14 21:00:00+00:00   0.062205         
    1986-03-17 21:00:00+00:00   0.064427          1986-03-18 21:00:00+00:00   0.065537
             1986-03-19 21:00:00+00:00   0.063871`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You are now able to write a single vector-based formula that is easy to read
    and understand. To get the natural logarithm, use NumPy’s `log` ufunc, which is
    applied to each element. Then we can plot a histogram (see [Figure 6-3](#filepos818582)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``22``]:``returns``=``np``.``log``(``msft_close``/``msft_close``.``shift``(``1``))``returns``=``returns``.``rename``(``columns``=``{``"Adj
    Close"``:``"returns"``})``returns``.``head``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[22]:                             returns          Date          1986-03-13
    21:00:00+00:00       NaN          1986-03-14 21:00:00+00:00  0.035097         
    1986-03-17 21:00:00+00:00  0.017082          1986-03-18 21:00:00+00:00 -0.025749
             1986-03-19 21:00:00+00:00 -0.017547`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``23``]:``# Plot a histogram with the daily log returns``returns``.``plot``.``hist``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00045.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Histogram plot
  prefs: []
  type: TYPE_NORMAL
- en: 'To get simple returns instead, use pandas’ built-in `pct_change` method. By
    default, it calculates the percentage change from the previous row, which is also
    the definition of simple returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``24``]:``simple_rets``=``msft_close``.``pct_change``()``simple_rets``=``simple_rets``.``rename``(``columns``=``{``"Adj
    Close"``:``"simple rets"``})``simple_rets``.``head``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[24]:                            simple rets          Date          1986-03-13
    21:00:00+00:00          NaN          1986-03-14 21:00:00+00:00     0.035721         
    1986-03-17 21:00:00+00:00     0.017229          1986-03-18 21:00:00+00:00    -0.025421
             1986-03-19 21:00:00+00:00    -0.017394`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So far, we have looked at just the Microsoft stock. In the next section, we’re
    going to load more time series so we can have a look at other DataFrame methods
    that require multiple time series.
  prefs: []
  type: TYPE_NORMAL
- en: Rebasing and Correlation
  prefs: []
  type: TYPE_NORMAL
- en: 'Things get slightly more interesting when we work with more than one time series.
    Let’s load a few additional adjusted close prices for Amazon (`AMZN`), Google
    (`GOOGL`), and Apple (`AAPL`), also downloaded from Yahoo! Finance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``25``]:``parts``=``[]``# List to collect individual DataFrames``for``ticker``in``[``"AAPL"``,``"AMZN"``,``"GOOGL"``,``"MSFT"``]:``#
    "usecols" allows us to only read in the Date and Adj Close``adj_close``=``pd``.``read_csv``(``f``"csv/{ticker}.csv"``,``index_col``=``"Date"``,``parse_dates``=``[``"Date"``],``usecols``=``[``"Date"``,``"Adj
    Close"``])``# Rename the column into the ticker symbol``adj_close``=``adj_close``.``rename``(``columns``=``{``"Adj
    Close"``:``ticker``})``# Append the stock''s DataFrame to the parts list``parts``.``append``(``adj_close``)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``26``]:``# Combine the 4 DataFrames into a single DataFrame``adj_close``=``pd``.``concat``(``parts``,``axis``=``1``)``adj_close`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[26]:                   AAPL         AMZN        GOOGL        MSFT         
    Date          1980-12-12    0.405683          NaN          NaN         NaN         
    1980-12-15    0.384517          NaN          NaN         NaN          1980-12-16   
    0.356296          NaN          NaN         NaN          1980-12-17    0.365115         
    NaN          NaN         NaN          1980-12-18    0.375698          NaN         
    NaN         NaN          ...                ...          ...          ...        
    ...          2020-05-22  318.890015  2436.879883  1413.239990  183.509995         
    2020-05-26  316.730011  2421.860107  1421.369995  181.570007          2020-05-27 
    318.109985  2410.389893  1420.280029  181.809998          2020-05-28  318.250000 
    2401.100098  1418.239990         NaN          2020-05-29  317.940002  2442.370117 
    1433.520020         NaN           [9950 rows x 4 columns]`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Did you see the power of `concat`? pandas has automatically aligned the individual
    time series along the dates. This is why you get `NaN` values for those stocks
    that don’t go back as far as Apple. And since `MSFT` has `NaN` values at the most
    recent dates, you may have guessed that I downloaded MSFT.csv two days before
    the other ones. Aligning time series by date is a typical operation that is very
    cumbersome to do with Excel and therefore also very error-prone. Dropping all
    rows that contain missing values will make sure that all stocks have the same
    amount of data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``27``]:``adj_close``=``adj_close``.``dropna``()``adj_close``.``info``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`<class ''pandas.core.frame.DataFrame''> DatetimeIndex: 3970 entries, 2004-08-19
    to 2020-05-27 Data columns (total 4 columns): #   Column  Non-Null Count  Dtype
    ---  ------  --------------  ----- 0   AAPL    3970 non-null   float64 1   AMZN   
    3970 non-null   float64 2   GOOGL   3970 non-null   float64 3   MSFT    3970 non-null  
    float64 dtypes: float64(4) memory usage: 155.1 KB`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s now rebase the prices so that all time series start at 100\. This allows
    us to compare their relative performance in a chart; see [Figure 6-4](#filepos838207).
    To rebase a time series, divide every value by its starting value and multiply
    by 100, the new base. If you did this in Excel, you would typically write a formula
    with a combination of absolute and relative cell references, then copy the formula
    for every row and every time series. In pandas, thanks to vectorization and broadcasting,
    you are dealing with a single formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``28``]:``# Use a sample from June 2019 - May 2020``adj_close_sample``=``adj_close``.``loc``[``"2019-06"``:``"2020-05"``,``:]``rebased_prices``=``adj_close_sample``/``adj_close_sample``.``iloc``[``0``,``:]``*``100``rebased_prices``.``head``(``2``)`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[28]:                   AAPL        AMZN      GOOGL        MSFT         
    Date          2019-06-03  100.000000  100.000000  100.00000  100.000000         
    2019-06-04  103.658406  102.178197  101.51626  102.770372`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``29``]:``rebased_prices``.``plot``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00053.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Rebased time series
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how independent the returns of the different stocks are, have a look
    at their correlations by using the `corr` method. Unfortunately, pandas doesn’t
    provide a built-in plot type to visualize the correlation matrix as a heatmap,
    so we need to use Plotly directly via its `plotly.express` interface (see [Figure 6-5](#filepos846256)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``30``]:``# Correlation of daily log returns``returns``=``np``.``log``(``adj_close``/``adj_close``.``shift``(``1``))``returns``.``corr``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[30]:            AAPL      AMZN     GOOGL      MSFT          AAPL   1.000000 
    0.424910  0.503497  0.486065          AMZN   0.424910  1.000000  0.486690  0.485725
             GOOGL  0.503497  0.486690  1.000000  0.525645          MSFT   0.486065 
    0.485725  0.525645  1.000000`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``31``]:``import``plotly.express``as``px`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``32``]:``fig``=``px``.``imshow``(``returns``.``corr``(),``x``=``adj_close``.``columns``,``y``=``adj_close``.``columns``,``color_continuous_scale``=``list``(``reversed``(``px``.``colors``.``sequential``.``RdBu``)),``zmin``=-``1``,``zmax``=``1``)``fig``.``show``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you want to understand how `imshow` works in detail, have a look at the [Plotly
    Express API docs](https://oreil.ly/O86li).
  prefs: []
  type: TYPE_NORMAL
- en: '![](images/00064.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Correlation heatmap
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have already learned quite a few things about time series,
    including how to combine and clean them and how to calculate returns and correlations.
    But what if you decide that daily returns are not a good base for your analysis
    and you want monthly returns? How you change the frequency of time series data
    is the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Resampling
  prefs: []
  type: TYPE_NORMAL
- en: 'A regular task with time series is up- and downsampling. Upsampling means that
    the time series is converted into one with a higher frequency, and downsampling
    means that it is converted into one with a lower frequency. On financial factsheets,
    you often show monthly or quarterly performance, for example. To turn the daily
    time series into a monthly one, use the `resample` method that accepts a frequency
    string like `M` for end-of-calendar-month or `BM` for end-of-business-month. You
    can find a list of all frequency strings in the [pandas docs](https://oreil.ly/zStpt).
    Similar to how `groupby` works, you then chain a method that defines how you are
    resampling. I am using `last` to always take the last observation of that month:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``33``]:``end_of_month``=``adj_close``.``resample``(``"M"``)``.``last``()``end_of_month``.``head``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[33]:                 AAPL       AMZN      GOOGL       MSFT          Date
             2004-08-31  2.132708  38.139999  51.236237  17.673630          2004-09-30 
    2.396127  40.860001  64.864868  17.900215          2004-10-31  3.240182  34.130001 
    95.415413  18.107374          2004-11-30  4.146072  39.680000  91.081078  19.344421
             2004-12-31  3.982207  44.290001  96.491493  19.279480`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instead of `last`, you can choose any other method that works on `groupby`,
    like `sum` or `mean`. There is also `ohlc`, which conveniently returns the open,
    high, low, and close values over that period. This may serve as the source to
    create the typical candlestick charts that are often used with stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'If that end-of-month time series would be all you have and you need to produce
    a weekly time series out of it, you have to upsample your time series. By using
    `asfreq`, you are telling pandas not to apply any transformation and hence you
    will see most of the values showing `NaN`. If you wanted to forward-fill the last
    known value instead, use the `ffill` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``34``]:``end_of_month``.``resample``(``"D"``)``.``asfreq``()``.``head``()``#
    No transformation`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[34]:                 AAPL       AMZN      GOOGL      MSFT          Date
             2004-08-31  2.132708  38.139999  51.236237  17.67363          2004-09-01      
    NaN        NaN        NaN       NaN          2004-09-02       NaN        NaN       
    NaN       NaN          2004-09-03       NaN        NaN        NaN       NaN         
    2004-09-04       NaN        NaN        NaN       NaN`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`In``[``35``]:``end_of_month``.``resample``(``"W-FRI"``)``.``ffill``()``.``head``()``#
    Forward fill`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`Out[35]:                 AAPL       AMZN      GOOGL       MSFT          Date
             2004-09-03  2.132708  38.139999  51.236237  17.673630          2004-09-10 
    2.132708  38.139999  51.236237  17.673630          2004-09-17  2.132708  38.139999 
    51.236237  17.673630          2004-09-24  2.132708  38.139999  51.236237  17.673630
             2004-10-01  2.396127  40.860001  64.864868  17.900215`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Downsampling data is one way of smoothing a time series. Calculating statistics
    over a rolling window is another way, as we will see next.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling Windows
  prefs: []
  type: TYPE_NORMAL
- en: 'When you calculate time series statistics, you often want a rolling statistic
    such as the moving average. The moving average looks at a subset of the time series
    (let’s say 25 days) and takes the mean from this subset before moving the window
    forward by one day. This will result in a new time series that is smoother and
    less prone to outliers. If you are into algorithmic trading, you may be looking
    at the intersection of the moving average with the stock price and take this (or
    some variation of it) as a trading signal. DataFrames have a `rolling` method,
    which accepts the number of observations as argument. You then chain it with the
    statistical method that you want to use—in the case of the moving average, it’s
    the `mean`. By looking at [Figure 6-6](#filepos859395), you are easily able to
    compare the original time series with the smoothed moving average:'
  prefs: []
  type: TYPE_NORMAL
- en: '`In``[``36``]:``# Plot the moving average for MSFT with data from 2019``msft19``=``msft``.``loc``[``"2019"``,``[``"Adj
    Close"``]]``.``copy``()``# Add the 25 day moving average as a new column to the
    DataFrame``msft19``.``loc``[:,``"25day average"``]``=``msft19``[``"Adj Close"``]``.``rolling``(``25``)``.``mean``()``msft19``.``plot``()`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](images/00072.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Moving average plot
  prefs: []
  type: TYPE_NORMAL
- en: Instead of `mean`, you can use many other statistical measures including `count`,
    `sum`, `median`, `min`, `max`, `std` (standard deviation), or `var` (variance).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have seen the most important functionality of pandas. It’s
    equally important, though, to understand where pandas has its limits, even though
    they may still be far away right now.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations with pandas
  prefs: []
  type: TYPE_NORMAL
- en: 'When your DataFrames start to get bigger, it’s a good idea to know the upper
    limit of what a DataFrame can hold. Unlike Excel, where you have a hard limit
    of roughly one million rows and 12,000 columns per sheet, pandas only has a soft
    limit: all data must fit into the available memory of your machine. If that’s
    not the case, there might be some easy fixes: only load those columns from your
    dataset that you need or delete intermediate results to free up some memory. If
    that doesn’t help, there are quite a few projects that will feel familiar to pandas
    users but work with big data. One of the projects, [Dask](https://dask.org), works
    on top of NumPy and pandas and allows you to work with big datasets by splitting
    it up into multiple pandas DataFrames and distributing the workload across multiple
    CPU cores or machines. Other big data projects that work with some sort of DataFrame
    are [Modin](https://oreil.ly/Wd8gi), [Koalas](https://oreil.ly/V13Be), [Vaex](https://vaex.io),
    [PySpark](https://oreil.ly/E7kmX), [cuDF](https://oreil.ly/zaeWz), [Ibis](https://oreil.ly/Gw4wn),
    and [PyArrow](https://oreil.ly/DQQGD). We will briefly touch on Modin in the next
    chapter but other than that, this is not something we are going to explore further
    in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis is the area where I feel Excel has fallen behind the most,
    so after reading this chapter, you probably understand why pandas has such a big
    success in finance, an industry that heavily relies on time series. We’ve seen
    how easy it is to work with time zones, resample time series, or produce correlation
    matrices, functionality that either isn’t supported in Excel or requires cumbersome
    workarounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing how to use pandas doesn’t mean you have to get rid of Excel, though,
    as the two worlds can play very nicely together: pandas DataFrames are a great
    way to transfer data from one world to the other, as we will see in the next part,
    which is about reading and writing Excel files in ways that bypass the Excel application
    entirely. This is very helpful as it means you can manipulate Excel files with
    Python on every operating system that Python supports, including Linux. To start
    this journey, the next chapter will show you how pandas can be used to automate
    tedious manual processes like the aggregation of Excel files into summary reports.'
  prefs: []
  type: TYPE_NORMAL
