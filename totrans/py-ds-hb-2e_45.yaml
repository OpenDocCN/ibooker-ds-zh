- en: Chapter 40\. Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapters outlined the fundamental ideas of machine learning, but
    all of the examples so far have assumed that you have numerical data in a tidy,
    `[n_samples, n_features]` format. In the real world, data rarely comes in such
    a form. With this in mind, one of the more important steps in using machine learning
    in practice is *feature engineering*: that is, taking whatever information you
    have about your problem and turning it into numbers that you can use to build
    your feature matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover a few common examples of feature engineering
    tasks: we’ll look at features for representing categorical data, text, and images.
    Additionally, we will discuss derived features for increasing model complexity
    and imputation of missing data. This process is commonly referred to as vectorization,
    as it involves converting arbitrary data into well-behaved vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One common type of nonnumerical data is *categorical* data. For example, imagine
    you are exploring some data on housing prices, and along with numerical features
    like “price” and “rooms,” you also have “neighborhood” information. For example,
    your data might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You might be tempted to encode this data with a straightforward numerical mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: But it turns out that this is not generally a useful approach in Scikit-Learn.
    The package’s models make the fundamental assumption that numerical features reflect
    algebraic quantities, so such a mapping would imply, for example, that *Queen
    Anne < Fremont < Wallingford*, or even that *Wallingford–Queen Anne = Fremont*,
    which (niche demographic jokes aside) does not make much sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, one proven technique is to use *one-hot encoding*, which effectively
    creates extra columns indicating the presence or absence of a category with a
    value of 1 or 0, respectively. When your data takes the form of a list of dictionaries,
    Scikit-Learn’s `DictVectorizer` will do this for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `neighborhood` column has been expanded into three separate
    columns representing the three neighborhood labels, and that each row has a 1
    in the column associated with its neighborhood. With these categorical features
    thus encoded, you can proceed as normal with fitting a Scikit-Learn model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the meaning of each column, you can inspect the feature names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one clear disadvantage of this approach: if your category has many
    possible values, this can *greatly* increase the size of your dataset. However,
    because the encoded data contains mostly zeros, a sparse output can be a very
    efficient solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Nearly all of the Scikit-Learn estimators accept such sparse inputs when fitting
    and evaluating models. Two additional tools that Scikit-Learn includes to support
    this type of encoding are `sklearn.preprocessing.OneHotEncoder` and `sklearn​.fea⁠ture_​extraction.FeatureHasher`.
  prefs: []
  type: TYPE_NORMAL
- en: Text Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common need in feature engineering is to convert text to a set of representative
    numerical values. For example, most automatic mining of social media data relies
    on some form of encoding the text as numbers. One of the simplest methods of encoding
    this type of data is by *word counts*: you take each snippet of text, count the
    occurrences of each word within it, and put the results in a table.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following set of three phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For a vectorization of this data based on word count, we could construct individual
    columns representing the words “problem,” “of,” “evil,” and so on. While doing
    this by hand would be possible for this simple example, the tedium can be avoided
    by using Scikit-Learn’s `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a sparse matrix recording the number of times each word appears;
    it is easier to inspect if we convert this to a `DataFrame` with labeled columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some issues with using a simple raw word count, however: it can lead
    to features that put too much weight on words that appear very frequently, and
    this can be suboptimal in some classification algorithms. One approach to fix
    this is known as *term frequency–inverse document frequency* (*TF–IDF*), which
    weights the word counts by a measure of how often they appear in the documents.
    The syntax for computing these features is similar to the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solid lines show the new results, while the fainter dashed lines show the
    results on the previous smaller dataset. It is clear from the validation curve
    that the larger dataset can support a much more complicated model: the peak here
    is probably around a degree of 6, but even a degree-20 model isn’t seriously overfitting
    the data—the validation and training scores remain very close.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For an example of using TF-IDF in a classification problem, see [Chapter 41](ch41.xhtml#section-0505-naive-bayes).
  prefs: []
  type: TYPE_NORMAL
- en: Image Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common need is to suitably encode images for machine learning analysis.
    The simplest approach is what we used for the digits data in [Chapter 38](ch38.xhtml#section-0502-introducing-scikit-learn):
    simply using the pixel values themselves. But depending on the application, such
    an approach may not be optimal.'
  prefs: []
  type: TYPE_NORMAL
- en: A comprehensive summary of feature extraction techniques for images is well
    beyond the scope of this chapter, but you can find excellent implementations of
    many of the standard approaches in the [Scikit-Image project](http://scikit-image.org).
    For one example of using Scikit-Learn and Scikit-Image together, see [Chapter 50](ch50.xhtml#section-0514-image-features).
  prefs: []
  type: TYPE_NORMAL
- en: Derived Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another useful type of feature is one that is mathematically derived from some
    input features. We saw an example of this in [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    when we constructed *polynomial features* from our input data. We saw that we
    could convert a linear regression into a polynomial regression not by changing
    the model, but by transforming the input!
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, this data clearly cannot be well described by a straight line
    (see [Figure 40-1](#fig_0504-feature-engineering_files_in_output_24_0)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![output 24 0](assets/output_24_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 40-1\. Data that is not well described by a straight line
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can still fit a line to the data using `LinearRegression` and get the optimal
    result, as shown in [Figure 40-2](#fig_0504-feature-engineering_files_in_output_26_0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![output 26 0](assets/output_26_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 40-2\. A poor straight-line fit
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But it’s clear that we need a more sophisticated model to describe the relationship
    between <math alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach to this is to transform the data, adding extra columns of features
    to drive more flexibility in the model. For example, we can add polynomial features
    to the data this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The derived feature matrix has one column representing <math alttext="x"><mi>x</mi></math>
    , a second column representing <math alttext="x squared"><msup><mi>x</mi> <mn>2</mn></msup></math>
    , and a third column representing <math alttext="x cubed"><msup><mi>x</mi> <mn>3</mn></msup></math>
    . Computing a linear regression on this expanded input gives a much closer fit
    to our data, as you can see in [Figure 40-3](#fig_0504-feature-engineering_files_in_output_30_0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 30 0](assets/output_30_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 40-3\. A linear fit to polynomial features derived from the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This idea of improving a model not by changing the model, but by transforming
    the inputs, is fundamental to many of the more powerful machine learning methods.
    We’ll explore this idea further in [Chapter 42](ch42.xhtml#section-0506-linear-regression)
    in the context of *basis function regression*. More generally, this is one motivational
    path to the powerful set of techniques known as *kernel methods*, which we will
    explore in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines).
  prefs: []
  type: TYPE_NORMAL
- en: Imputation of Missing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another common need in feature engineering is handling of missing data. We
    discussed the handling of missing data in `DataFrame` objects in [Chapter 16](ch16.xhtml#section-0304-missing-values),
    and saw that `NaN` is often is used to mark missing values. For example, we might
    have a dataset that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When applying a typical machine learning model to such data, we will need to
    first replace the missing values with some appropriate fill value. This is known
    as *imputation* of missing values, and strategies range from simple (e.g., replacing
    missing values with the mean of the column) to sophisticated (e.g., using matrix
    completion or a robust model to handle such data).
  prefs: []
  type: TYPE_NORMAL
- en: 'The sophisticated approaches tend to be very application-specific, and we won’t
    dive into them here. For a baseline imputation approach using the mean, median,
    or most frequent value, Scikit-Learn provides the `SimpleImputer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that in the resulting data, the two missing values have been replaced
    with the mean of the remaining values in the column. This imputed data can then
    be fed directly into, for example, a `LinearRegression` estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Feature Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With any of the preceding examples, it can quickly become tedious to do the
    transformations by hand, especially if you wish to string together multiple steps.
    For example, we might want a processing pipeline that looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Impute missing values using the mean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform features to quadratic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a linear regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To streamline this type of processing pipeline, Scikit-Learn provides a `Pipeline`
    object, which can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This pipeline looks and acts like a standard Scikit-Learn object, and will
    apply all the specified steps to any input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: All the steps of the model are applied automatically. Notice that for simplicity,
    in this demonstration we’ve applied the model to the data it was trained on; this
    is why it was able to perfectly predict the result (refer back to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    for further discussion).
  prefs: []
  type: TYPE_NORMAL
- en: For some examples of Scikit-Learn pipelines in action, see the following chapter
    on naive Bayes classification, as well as Chapters [42](ch42.xhtml#section-0506-linear-regression)
    and [43](ch43.xhtml#section-0507-support-vector-machines).
  prefs: []
  type: TYPE_NORMAL
