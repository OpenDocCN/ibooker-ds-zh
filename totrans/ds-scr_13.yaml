- en: Chapter 12\. k-Nearest Neighbors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 k-最近邻
- en: If you want to annoy your neighbors, tell the truth about them.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想要惹恼你的邻居，就告诉他们关于他们的真相。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pietro Aretino
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 皮耶特罗·阿雷蒂诺
- en: Imagine that you’re trying to predict how I’m going to vote in the next presidential
    election. If you know nothing else about me (and if you have the data), one sensible
    approach is to look at how my *neighbors* are planning to vote. Living in Seattle,
    as I do, my neighbors are invariably planning to vote for the Democratic candidate,
    which suggests that “Democratic candidate” is a good guess for me as well.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你试图预测我在下次总统选举中的投票方式。如果你对我一无所知（并且如果你有数据的话），一个明智的方法是看看我的*邻居*打算如何投票。像我一样住在西雅图，我的邻居们无一例外地计划投票给民主党候选人，这表明“民主党候选人”对我来说也是一个不错的猜测。
- en: Now imagine you know more about me than just geography—perhaps you know my age,
    my income, how many kids I have, and so on. To the extent my behavior is influenced
    (or characterized) by those things, looking just at my neighbors who are close
    to me among all those dimensions seems likely to be an even better predictor than
    looking at all my neighbors. This is the idea behind *nearest neighbors classification*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，你不仅了解我的地理位置，还知道我的年龄、收入、有几个孩子等等。在我行为受这些因素影响（或者说特征化）的程度上，只看那些在所有这些维度中与我接近的邻居，似乎比看所有邻居更有可能是一个更好的预测器。这就是*最近邻分类*背后的思想。
- en: The Model
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: 'Nearest neighbors is one of the simplest predictive models there is. It makes
    no mathematical assumptions, and it doesn’t require any sort of heavy machinery.
    The only things it requires are:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻是最简单的预测模型之一。它不做任何数学假设，也不需要任何重型设备。它唯一需要的是：
- en: Some notion of distance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某种距离的概念
- en: An assumption that points that are close to one another are similar
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个假设是相互接近的点是相似的。
- en: Most of the techniques we’ll see in this book look at the dataset as a whole
    in order to learn patterns in the data. Nearest neighbors, on the other hand,
    quite consciously neglects a lot of information, since the prediction for each
    new point depends only on the handful of points closest to it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将看到的大多数技术都是整体看待数据集以便学习数据中的模式。而最近邻则故意忽略了很多信息，因为对于每个新点的预测仅依赖于最接近它的少数几个点。
- en: What’s more, nearest neighbors is probably not going to help you understand
    the drivers of whatever phenomenon you’re looking at. Predicting my votes based
    on my neighbors’ votes doesn’t tell you much about what causes me to vote the
    way I do, whereas some alternative model that predicted my vote based on (say)
    my income and marital status very well might.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最近邻可能不会帮助你理解你正在研究的现象的驱动因素。根据我的邻居的投票预测我的投票并不能告诉你关于我为什么投票的原因，而一些基于（比如）我的收入和婚姻状况预测我的投票的替代模型可能会很好地做到这一点。
- en: In the general situation, we have some data points and we have a corresponding
    set of labels. The labels could be `True` and `False`, indicating whether each
    input satisfies some condition like “is spam?” or “is poisonous?” or “would be
    enjoyable to watch?” Or they could be categories, like movie ratings (G, PG, PG-13,
    R, NC-17). Or they could be the names of presidential candidates. Or they could
    be favorite programming languages.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下，我们有一些数据点和相应的标签集合。标签可以是`True`和`False`，表示每个输入是否满足某些条件，比如“是垃圾邮件？”或“有毒？”或“看起来有趣吗？”或者它们可以是类别，比如电影评级（G，PG，PG-13，R，NC-17）。或者它们可以是总统候选人的名字。或者它们可以是喜欢的编程语言。
- en: In our case, the data points will be vectors, which means that we can use the
    `distance` function from [Chapter 4](ch04.html#linear_algebra).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，数据点将是向量，这意味着我们可以使用[第四章](ch04.html#linear_algebra)中的`distance`函数。
- en: Let’s say we’ve picked a number *k* like 3 or 5\. Then, when we want to classify
    some new data point, we find the *k* nearest labeled points and let them vote
    on the new output.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择了一个像3或5这样的数字*k*。那么，当我们想要对一些新的数据点进行分类时，我们找到*k*个最近的带标签点，并让它们对新的输出进行投票。
- en: 'To do this, we’ll need a function that counts votes. One possibility is:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要一个计票的函数。一种可能性是：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'But this doesn’t do anything intelligent with ties. For example, imagine we’re
    rating movies and the five nearest movies are rated G, G, PG, PG, and R. Then
    G has two votes and PG also has two votes. In that case, we have several options:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不会处理带有智能的平局情况。例如，想象我们正在评分电影，而最近的五部电影分别被评为G、G、PG、PG和R。那么G有两票，PG也有两票。在这种情况下，我们有几个选项：
- en: Pick one of the winners at random.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机挑选一个赢家。
- en: Weight the votes by distance and pick the weighted winner.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过距离加权投票并选择加权赢家。
- en: Reduce *k* until we find a unique winner.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少*k*直到找到唯一的赢家。
- en: 'We’ll implement the third:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现第三种方法：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This approach is sure to work eventually, since in the worst case we go all
    the way down to just one label, at which point that one label wins.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法肯定最终会奏效，因为在最坏的情况下，我们最终只需一个标签，此时那个标签会获胜。
- en: 'With this function it’s easy to create a classifier:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数很容易创建一个分类器：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s take a look at how this works.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是如何工作的。
- en: 'Example: The Iris Dataset'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：鸢尾花数据集
- en: 'The *Iris* dataset is a staple of machine learning. It contains a bunch of
    measurements for 150 flowers representing three species of iris. For each flower
    we have its petal length, petal width, sepal length, and sepal width, as well
    as its species. You can download it from [*https://archive.ics.uci.edu/ml/datasets/iris*](https://archive.ics.uci.edu/ml/datasets/iris):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Iris* 数据集是机器学习的重要数据集。它包含了150朵花的测量数据，代表三种鸢尾花物种。对于每朵花，我们有它的花瓣长度、花瓣宽度、萼片长度和萼片宽度，以及它的物种。你可以从[*https://archive.ics.uci.edu/ml/datasets/iris*](https://archive.ics.uci.edu/ml/datasets/iris)下载：'
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The data is comma-separated, with fields:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是逗号分隔的，包含字段：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For example, the first row looks like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，第一行看起来像：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this section we’ll try to build a model that can predict the class (that
    is, the species) from the first four measurements.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将尝试构建一个模型，可以从前四个测量值预测类别（即物种）。
- en: 'To start with, let’s load and explore the data. Our nearest neighbors function
    expects a `LabeledPoint`, so let’s represent our data that way:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载并探索数据。我们的最近邻函数期望一个`LabeledPoint`，所以让我们用这种方式表示我们的数据：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We’d like to plot the measurements so we can see how they vary by species.
    Unfortunately, they are four-dimensional, which makes them tricky to plot. One
    thing we can do is look at the scatterplots for each of the six pairs of measurements
    ([Figure 12-1](#iris_scatterplots)). I won’t explain all the details, but it’s
    a nice illustration of more complicated things you can do with matplotlib, so
    it’s worth studying:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望绘制测量结果，以便查看它们按物种的变化。不幸的是，它们是四维的，这使得绘图变得棘手。我们可以做的一件事是查看每一对测量的散点图（[图 12-1](#iris_scatterplots)）。我不会解释所有的细节，但这是对matplotlib更复杂用法的很好示例，所以值得学习：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Iris scatterplots](assets/dsf2_1201.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![鸢尾花散点图](assets/dsf2_1201.png)'
- en: Figure 12-1\. Iris scatterplots
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1\. 鸢尾花散点图
- en: If you look at those plots, it seems like the measurements really do cluster
    by species. For example, looking at sepal length and sepal width alone, you probably
    couldn’t distinguish between *versicolor* and *virginica*. But once you add petal
    length and width into the mix, it seems like you should be able to predict the
    species based on the nearest neighbors.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看这些图，看起来测量结果确实按物种聚类。例如，仅看萼片长度和萼片宽度，你可能无法区分*鸢尾花*和*维吉尼亚*。但一旦加入花瓣长度和宽度，似乎你应该能够根据最近邻来预测物种。
- en: 'To start with, let’s split the data into a test set and a training set:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将数据分成测试集和训练集：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The training set will be the “neighbors” that we’ll use to classify the points
    in the test set. We just need to choose a value for *k*, the number of neighbors
    who get to vote. Too small (think *k* = 1), and we let outliers have too much
    influence; too large (think *k* = 105), and we just predict the most common class
    in the dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集将是我们用来分类测试集中点的“邻居”。我们只需选择一个*k*值，即获得投票权的邻居数。如果太小（考虑*k* = 1），我们让离群值影响过大；如果太大（考虑*k*
    = 105），我们只是预测数据集中最常见的类别。
- en: 'In a real application (and with more data), we might create a separate validation
    set and use it to choose *k*. Here we’ll just use *k* = 5:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实的应用中（和更多数据），我们可能会创建一个单独的验证集，并用它来选择*k*。在这里我们只使用*k* = 5：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: On this simple dataset, the model predicts almost perfectly. There’s one *versicolor*
    for which it predicts *virginica*, but otherwise it gets things exactly right.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的数据集上，模型几乎完美地预测了。有一个*鸢尾花*，它预测为*维吉尼亚*，但除此之外其他都是完全正确的。
- en: The Curse of Dimensionality
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度灾难
- en: The *k*-nearest neighbors algorithm runs into trouble in higher dimensions thanks
    to the “curse of dimensionality,” which boils down to the fact that high-dimensional
    spaces are *vast*. Points in high-dimensional spaces tend not to be close to one
    another at all. One way to see this is by randomly generating pairs of points
    in the *d*-dimensional “unit cube” in a variety of dimensions, and calculating
    the distances between them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '在高维空间中，“k”最近邻算法在处理高维数据时遇到麻烦，这要归因于“维度的诅咒”，其核心问题在于高维空间是*广阔的*。高维空间中的点往往彼此之间并不接近。通过在各种维度中随机生成“d”维“单位立方体”中的点对，并计算它们之间的距离，可以看出这一点。 '
- en: 'Generating random points should be second nature by now:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成随机点现在应该是驾轻就熟了：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'as is writing a function to generate the distances:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个生成距离的函数也是一样的：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For every dimension from 1 to 100, we’ll compute 10,000 distances and use those
    to compute the average distance between points and the minimum distance between
    points in each dimension ([Figure 12-2](#curse_of_dimensionality_graph)):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从 1 到 100 的每个维度，我们将计算 10,000 个距离，并使用这些距离计算点之间的平均距离以及每个维度中点之间的最小距离（参见[图 12-2](#curse_of_dimensionality_graph)）：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![The Curse of Dimensionality.](assets/dsf2_1202.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![维度的诅咒。](assets/dsf2_1202.png)'
- en: Figure 12-2\. The curse of dimensionality
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2。维度的诅咒
- en: 'As the number of dimensions increases, the average distance between points
    increases. But what’s more problematic is the ratio between the closest distance
    and the average distance ([Figure 12-3](#curse_of_dimensionality_graph2)):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度的增加，点之间的平均距离也增加。但更为问题的是最近距离与平均距离之间的比率（参见[图 12-3](#curse_of_dimensionality_graph2)）：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![The Curse of Dimensionality Again.](assets/dsf2_1203.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![再谈维度的诅咒。](assets/dsf2_1203.png)'
- en: Figure 12-3\. The curse of dimensionality again
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-3。再谈维度的诅咒
- en: In low-dimensional datasets, the closest points tend to be much closer than
    average. But two points are close only if they’re close in every dimension, and
    every extra dimension—even if just noise—is another opportunity for each point
    to be farther away from every other point. When you have a lot of dimensions,
    it’s likely that the closest points aren’t much closer than average, so two points
    being close doesn’t mean very much (unless there’s a lot of structure in your
    data that makes it behave as if it were much lower-dimensional).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在低维数据集中，最近的点往往比平均距离要接近得多。但是只有当两个点在每个维度上都接近时，这两个点才是接近的，而每增加一个维度——即使只是噪音——都是使每个点与其他每个点的距离更远的机会。当你有很多维度时，最接近的点可能并不比平均距离要接近，所以两个点接近并不意味着太多（除非你的数据具有使其表现得像低维度的大量结构）。
- en: A different way of thinking about the problem involves the sparsity of higher-dimensional
    spaces.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对问题的另一种思考方式涉及到更高维度空间的稀疏性。
- en: If you pick 50 random numbers between 0 and 1, you’ll probably get a pretty
    good sample of the unit interval ([Figure 12-4](#fifty_points_1d)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 0 和 1 之间随机选择 50 个数字，你可能会得到单位区间的一个相当好的样本（参见[图 12-4](#fifty_points_1d)）。
- en: '![50 Random Points in One Dimension.](assets/dsf2_1204.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![一维空间中的 50 个随机点。](assets/dsf2_1204.png)'
- en: Figure 12-4\. Fifty random points in one dimension
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-4。一维空间中的 50 个随机点
- en: If you pick 50 random points in the unit square, you’ll get less coverage ([Figure 12-5](#fifty_points_2d)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在单位正方形中随机选择 50 个点，你将得到更少的覆盖（参见[图 12-5](#fifty_points_2d)）。
- en: '![50 Random Points in Two Dimensions.](assets/dsf2_1205.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![二维空间中的 50 个随机点。](assets/dsf2_1205.png)'
- en: Figure 12-5\. Fifty random points in two dimensions
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-5。二维空间中的 50 个随机点
- en: And in three dimensions, less still ([Figure 12-6](#fifty_points_3d)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在三维空间中，覆盖更少（参见[图 12-6](#fifty_points_3d)）。
- en: matplotlib doesn’t graph four dimensions well, so that’s as far as we’ll go,
    but you can see already that there are starting to be large empty spaces with
    no points near them. In more dimensions—unless you get exponentially more data—those
    large empty spaces represent regions far from all the points you want to use in
    your predictions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: matplotlib 对于四维图表的呈现并不好，所以这是我们所能达到的最远的地方，但你已经可以看到开始出现大量空白区域，没有点靠近它们。在更多维度中——除非你得到指数级更多的数据——这些大量空白区域代表了远离所有你想要用于预测的点的区域。
- en: So if you’re trying to use nearest neighbors in higher dimensions, it’s probably
    a good idea to do some kind of dimensionality reduction first.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你尝试在更高维度中使用最近邻方法，最好先进行某种降维处理。
- en: '![50 Random Points in Three Dimensions.](assets/dsf2_1206.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![三维空间中的 50 个随机点。](assets/dsf2_1206.png)'
- en: Figure 12-6\. Fifty random points in three dimensions
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-6。三维空间中的 50 个随机点
- en: For Further Exploration
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: scikit-learn has many [nearest neighbor](https://scikit-learn.org/stable/modules/neighbors.html)
    models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn有许多[最近邻](https://scikit-learn.org/stable/modules/neighbors.html)模型。
