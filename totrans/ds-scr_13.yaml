- en: Chapter 12\. k-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to annoy your neighbors, tell the truth about them.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pietro Aretino
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Imagine that you’re trying to predict how I’m going to vote in the next presidential
    election. If you know nothing else about me (and if you have the data), one sensible
    approach is to look at how my *neighbors* are planning to vote. Living in Seattle,
    as I do, my neighbors are invariably planning to vote for the Democratic candidate,
    which suggests that “Democratic candidate” is a good guess for me as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine you know more about me than just geography—perhaps you know my age,
    my income, how many kids I have, and so on. To the extent my behavior is influenced
    (or characterized) by those things, looking just at my neighbors who are close
    to me among all those dimensions seems likely to be an even better predictor than
    looking at all my neighbors. This is the idea behind *nearest neighbors classification*.
  prefs: []
  type: TYPE_NORMAL
- en: The Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Nearest neighbors is one of the simplest predictive models there is. It makes
    no mathematical assumptions, and it doesn’t require any sort of heavy machinery.
    The only things it requires are:'
  prefs: []
  type: TYPE_NORMAL
- en: Some notion of distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An assumption that points that are close to one another are similar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the techniques we’ll see in this book look at the dataset as a whole
    in order to learn patterns in the data. Nearest neighbors, on the other hand,
    quite consciously neglects a lot of information, since the prediction for each
    new point depends only on the handful of points closest to it.
  prefs: []
  type: TYPE_NORMAL
- en: What’s more, nearest neighbors is probably not going to help you understand
    the drivers of whatever phenomenon you’re looking at. Predicting my votes based
    on my neighbors’ votes doesn’t tell you much about what causes me to vote the
    way I do, whereas some alternative model that predicted my vote based on (say)
    my income and marital status very well might.
  prefs: []
  type: TYPE_NORMAL
- en: In the general situation, we have some data points and we have a corresponding
    set of labels. The labels could be `True` and `False`, indicating whether each
    input satisfies some condition like “is spam?” or “is poisonous?” or “would be
    enjoyable to watch?” Or they could be categories, like movie ratings (G, PG, PG-13,
    R, NC-17). Or they could be the names of presidential candidates. Or they could
    be favorite programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the data points will be vectors, which means that we can use the
    `distance` function from [Chapter 4](ch04.html#linear_algebra).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we’ve picked a number *k* like 3 or 5\. Then, when we want to classify
    some new data point, we find the *k* nearest labeled points and let them vote
    on the new output.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we’ll need a function that counts votes. One possibility is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'But this doesn’t do anything intelligent with ties. For example, imagine we’re
    rating movies and the five nearest movies are rated G, G, PG, PG, and R. Then
    G has two votes and PG also has two votes. In that case, we have several options:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick one of the winners at random.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight the votes by distance and pick the weighted winner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce *k* until we find a unique winner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll implement the third:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This approach is sure to work eventually, since in the worst case we go all
    the way down to just one label, at which point that one label wins.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this function it’s easy to create a classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at how this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: The Iris Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The *Iris* dataset is a staple of machine learning. It contains a bunch of
    measurements for 150 flowers representing three species of iris. For each flower
    we have its petal length, petal width, sepal length, and sepal width, as well
    as its species. You can download it from [*https://archive.ics.uci.edu/ml/datasets/iris*](https://archive.ics.uci.edu/ml/datasets/iris):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is comma-separated, with fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the first row looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this section we’ll try to build a model that can predict the class (that
    is, the species) from the first four measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, let’s load and explore the data. Our nearest neighbors function
    expects a `LabeledPoint`, so let’s represent our data that way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We’d like to plot the measurements so we can see how they vary by species.
    Unfortunately, they are four-dimensional, which makes them tricky to plot. One
    thing we can do is look at the scatterplots for each of the six pairs of measurements
    ([Figure 12-1](#iris_scatterplots)). I won’t explain all the details, but it’s
    a nice illustration of more complicated things you can do with matplotlib, so
    it’s worth studying:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Iris scatterplots](assets/dsf2_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Iris scatterplots
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you look at those plots, it seems like the measurements really do cluster
    by species. For example, looking at sepal length and sepal width alone, you probably
    couldn’t distinguish between *versicolor* and *virginica*. But once you add petal
    length and width into the mix, it seems like you should be able to predict the
    species based on the nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, let’s split the data into a test set and a training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The training set will be the “neighbors” that we’ll use to classify the points
    in the test set. We just need to choose a value for *k*, the number of neighbors
    who get to vote. Too small (think *k* = 1), and we let outliers have too much
    influence; too large (think *k* = 105), and we just predict the most common class
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a real application (and with more data), we might create a separate validation
    set and use it to choose *k*. Here we’ll just use *k* = 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: On this simple dataset, the model predicts almost perfectly. There’s one *versicolor*
    for which it predicts *virginica*, but otherwise it gets things exactly right.
  prefs: []
  type: TYPE_NORMAL
- en: The Curse of Dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *k*-nearest neighbors algorithm runs into trouble in higher dimensions thanks
    to the “curse of dimensionality,” which boils down to the fact that high-dimensional
    spaces are *vast*. Points in high-dimensional spaces tend not to be close to one
    another at all. One way to see this is by randomly generating pairs of points
    in the *d*-dimensional “unit cube” in a variety of dimensions, and calculating
    the distances between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating random points should be second nature by now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'as is writing a function to generate the distances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For every dimension from 1 to 100, we’ll compute 10,000 distances and use those
    to compute the average distance between points and the minimum distance between
    points in each dimension ([Figure 12-2](#curse_of_dimensionality_graph)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![The Curse of Dimensionality.](assets/dsf2_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. The curse of dimensionality
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As the number of dimensions increases, the average distance between points
    increases. But what’s more problematic is the ratio between the closest distance
    and the average distance ([Figure 12-3](#curse_of_dimensionality_graph2)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![The Curse of Dimensionality Again.](assets/dsf2_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. The curse of dimensionality again
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In low-dimensional datasets, the closest points tend to be much closer than
    average. But two points are close only if they’re close in every dimension, and
    every extra dimension—even if just noise—is another opportunity for each point
    to be farther away from every other point. When you have a lot of dimensions,
    it’s likely that the closest points aren’t much closer than average, so two points
    being close doesn’t mean very much (unless there’s a lot of structure in your
    data that makes it behave as if it were much lower-dimensional).
  prefs: []
  type: TYPE_NORMAL
- en: A different way of thinking about the problem involves the sparsity of higher-dimensional
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: If you pick 50 random numbers between 0 and 1, you’ll probably get a pretty
    good sample of the unit interval ([Figure 12-4](#fifty_points_1d)).
  prefs: []
  type: TYPE_NORMAL
- en: '![50 Random Points in One Dimension.](assets/dsf2_1204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. Fifty random points in one dimension
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you pick 50 random points in the unit square, you’ll get less coverage ([Figure 12-5](#fifty_points_2d)).
  prefs: []
  type: TYPE_NORMAL
- en: '![50 Random Points in Two Dimensions.](assets/dsf2_1205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. Fifty random points in two dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: And in three dimensions, less still ([Figure 12-6](#fifty_points_3d)).
  prefs: []
  type: TYPE_NORMAL
- en: matplotlib doesn’t graph four dimensions well, so that’s as far as we’ll go,
    but you can see already that there are starting to be large empty spaces with
    no points near them. In more dimensions—unless you get exponentially more data—those
    large empty spaces represent regions far from all the points you want to use in
    your predictions.
  prefs: []
  type: TYPE_NORMAL
- en: So if you’re trying to use nearest neighbors in higher dimensions, it’s probably
    a good idea to do some kind of dimensionality reduction first.
  prefs: []
  type: TYPE_NORMAL
- en: '![50 Random Points in Three Dimensions.](assets/dsf2_1206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. Fifty random points in three dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn has many [nearest neighbor](https://scikit-learn.org/stable/modules/neighbors.html)
    models.
  prefs: []
  type: TYPE_NORMAL
