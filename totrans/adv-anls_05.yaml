- en: Chapter 4\. Correlation and Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you heard that ice cream consumption is linked to shark attacks? Apparently
    Jaws has a lethal appetite for mint chocolate chip. [Figure 4-1](#ice-cream-shark-attack)
    visualizes this proposed relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ice cream versus shark attacks](assets/aina_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. The proposed relationship between ice cream consumption and shark
    attacks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: “Not so,” you may retort. “This does not necessarily mean that shark attacks
    are triggered by ice cream consumption.”
  prefs: []
  type: TYPE_NORMAL
- en: “It could be,” you reason, “that as the outside temperature increases, more
    ice cream is consumed. People also spend more time near the ocean when the weather
    is warm, and *that* coincidence leads to more shark attacks.”
  prefs: []
  type: TYPE_NORMAL
- en: “Correlation Does Not Imply Causation”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve likely heard repeatedly that “correlation does not imply causation.”
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#foundations-of-inference), you learned that *causation*
    is a fraught expression in statistics. We really only reject the null hypothesis
    because we simply don’t have all the data to claim causality for sure. That semantic
    difference aside, does correlation have *anything* to do with causation? The standard
    expression somewhat oversimplifies their relationship; you’ll see why in this
    chapter using the tools of inferential statistics you picked up earlier.
  prefs: []
  type: TYPE_NORMAL
- en: This will be our final chapter conducted primarily in Excel. After that, you
    will have sufficiently grasped the framework of analytics to be ready to be move
    into R and Python.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve mostly been analyzing statistics one variable at a time: we’ve
    found the average reading score or the variance in housing prices, for example.
    This is known as *univariate* analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also done a bit of *bivariate* analysis. For example, we compared the
    frequencies of two categorical variables using a two-way frequency table. We also
    analyzed a continuous variable when grouped by multiple levels of a categorical
    variable, finding descriptive statistics for each group.
  prefs: []
  type: TYPE_NORMAL
- en: We will now calculate a bivariate measure of two continuous variables using
    correlation. More specifically, we will use the *Pearson correlation coefficient*
    to measure the strength of the *linear relationship* between two variables. Without
    a linear relationship, the Pearson correlation is unsuitable.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we know our data is linear? There are more rigorous ways to check,
    but, as usual, a visualization is a great start. In particular, we will use a
    *scatterplot* to depict all observations based on their x and y coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: If it appears a line could be drawn through the scatterplot that summarizes
    the overall pattern, then it’s a linear relationship and the Pearson correlation
    can be used. If you would need a curve or some other shape to summarize the pattern,
    then the opposite holds. [Figure 4-2](#linear-vs-non-linear) depicts one linear
    and two nonlinear relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear versus non-linear scatterplot relationships](assets/aina_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Linear versus nonlinear relationships
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In particular, [Figure 4-2](#linear-vs-non-linear) gives an example of a *positive*
    linear relationship: as values on the x-axis increase, so do the values on the
    y-axis (at a linear rate).'
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to have a *negative* correlation, where a negative line summarizes
    the relationship, or no correlation at all, in which a flat line summarizes it.
    These different types of linear relationships are shown in [Figure 4-3](#negative-positive-no-correlation).
    Remember, these all must be linear relationships for correlation to apply.
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation types](assets/aina_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Negative, zero, and positive correlations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once we’ve established that the data is linear, we can find the correlation
    coefficient. It always takes a value between –1 and 1, with –1 indicating a perfect
    negative linear relationship, 1 a perfect positive linear relationship, and 0
    no linear relationship at all. [Table 4-1](#correlation-interpretation) shows
    some rules of thumb for evaluating the strength of a correlation coefficient.
    These are not official standards by any means, but a useful jumping-off point
    for interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Interpretation of correlation coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '| Correlation coefficient | Interpretation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| –1.0 | Perfect negative linear relationship |'
  prefs: []
  type: TYPE_TB
- en: '| –0.7 | Strong negative relationship |'
  prefs: []
  type: TYPE_TB
- en: '| –0.5 | Moderate negative relationship |'
  prefs: []
  type: TYPE_TB
- en: '| –0.3 | Weak negative relationship |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | No linear relationship |'
  prefs: []
  type: TYPE_TB
- en: '| +0.3 | Weak positive relationship |'
  prefs: []
  type: TYPE_TB
- en: '| +0.5 | Moderate positive relationship |'
  prefs: []
  type: TYPE_TB
- en: '| +0.7 | Strong positive relationship |'
  prefs: []
  type: TYPE_TB
- en: '| +1.0 | Perfect positive linear relationship |'
  prefs: []
  type: TYPE_TB
- en: 'With the basic conceptual framework for correlations in mind, let’s do some
    analysis in Excel. We will be using a vehicle mileage dataset; you can find *mpg.xlsx*
    in the *mpg* subfolder of the book repository’s [*datasets* folder](https://oreil.ly/ygWQn).
    This is a new dataset, so take some time to learn about it: what types of variables
    are we working with? Summarize and visualize them using the tools covered in [Chapter 1](ch01.html#foundations-of-eda).
    To help with subsequent analysis, don’t forget to add an index column and convert
    the dataset into a table, which I will call *mpg*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Excel includes the `CORREL()` function to calculate the correlation coefficient
    between two arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use this function to find the correlation between `weight` and `mpg`
    in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This indeed returns a value between –1 and 1: –0.832\. (Do you remember how
    to interpret this?)'
  prefs: []
  type: TYPE_NORMAL
- en: A *correlation matrix* presents the correlations across all pairs of variables.
    Let’s build one using the Data Analysis ToolPak. From the ribbon, head to Data
    → Data Analysis → Correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that this is a measure of linear relationship between two *continuous*
    variables, so we should exclude categorical variables like *origin* and be judicious
    about including discrete variables like *cylinders* or *model.year*. The ToolPak
    insists on all variables being in a contiguous range, so I will cautiously include
    *cylinders*. [Figure 4-4](#insert-correlation-matrix-excel) shows what the ToolPak
    source menu should look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![Insert correlation matrix](assets/aina_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Inserting a correlation matrix in Excel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This results in a correlation matrix as shown in [Figure 4-5](#correlation-matrix-excel).
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation matrix](assets/aina_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Correlation matrix in Excel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can see the –0.83 in cell `B6`: it’s the intersection of *weight* and *mpg*.
    We would also see the same value in cell `F2`, but Excel left this half of the
    matrix blank, as it’s redundant information. All values along the diagonal are
    1, as any variable is perfectly correlated with itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Pearson correlation coefficient is only a suitable measure when the relationship
    between the two variables is linear.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve made a big leap in our assumptions about our variables by analyzing their
    correlations. Can you think of what that is? *We assumed their relationship is
    linear.* Let’s check that assumption with scatterplots. Unfortunately, there is
    no way in basic Excel to generate scatterplots of each pair of variables at once.
    For practice, consider plotting them all, but let’s try it with the *weight* and
    *mpg* variables. Highlight this data, then head to the ribbon and click Insert
    → Scatter.
  prefs: []
  type: TYPE_NORMAL
- en: I will add a custom chart title and relabel the axes to aid in interpretation.
    To change the chart title, double-click on it. To relabel the axes, click the
    perimeter of the chart and then select the plus sign that appears to expand the
    Chart Elements menu. (On Mac, click inside the chart and then Chart Design → Add
    Chart Element.) Select Axis Titles from the menu. [Figure 4-6](#weight-mileage-scatter)
    shows the resulting scatterplot. It’s not a bad idea to include units of measurement
    on the axes to help outsiders make sense of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-6](#weight-mileage-scatter) looks basically like a negative linear
    relationship, with a greater spread given lower weights and higher mileages. By
    default, Excel plotted the first variable in our data selection along the x-axis
    and the second along the y-axis. But why not the other way around? Try switching
    the order of these columns in your worksheet so that *weight* is in column `E`
    and *mpg* in column `F`, then insert a new scatterplot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![scatterplot of weight and mileage](assets/aina_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Scatterplot of weight and mileage
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 4-7](#mileage-weight-scatter) shows a mirror image of the relationship.
    Excel is a great tool, but as with any tool, you have to tell it what to do. Excel
    will calculate correlations regardless of whether the relationship is linear.
    It will also make you a scatterplot without concern for which variable should
    go on which axis.'
  prefs: []
  type: TYPE_NORMAL
- en: So, which scatterplot is “right?” Does it matter? By convention, the independent
    variable goes on the x-axis, and dependent on the y-axis. Take a moment to consider
    which is which. If you’re not sure, remember that the independent variable is
    generally the one measured first.
  prefs: []
  type: TYPE_NORMAL
- en: Our independent variable is *weight* because it was determined by the design
    and building of the car. *mpg* is the *dependent* variable because we assume it’s
    affected by the car’s weight. This puts *weight* on the x-axis and *mpg* on the
    y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: In business analytics, it is uncommon to have collected data just for the sake
    of statistical analysis; for example, the cars in our *mpg* dataset were built
    to generate revenue, not for a research study on the impact of weight on mileage.
    Because there are not always clear independent and dependent variables, we need
    to be all the more aware of *what* these variables are measuring, and *how* they
    are measured. This is why having some knowledge of the domain you are studying,
    or at least descriptions of your variables and how your observations were collected,
    is so valuable.
  prefs: []
  type: TYPE_NORMAL
- en: '![scatterplot of mileage and weight](assets/aina_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Scatterplot of mileage and weight
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From Correlation to Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Though it’s conventional to place the independent variable on the x-axis, it
    makes no difference to the related correlation coefficient. However, there is
    a big caveat here, and it relates to the earlier idea of using a line to summarize
    the relationship found by the scatterplot. This practice begins to diverge from
    correlation, and it’s one you may have heard of: *linear regression*.'
  prefs: []
  type: TYPE_NORMAL
- en: Correlation is agnostic to which variable you call independent and which you
    call dependent; that doesn’t factor into its definition as “the extent to which
    two variables move together linearly.”
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, linear regression is inherently affected by this relationship
    as “the estimated impact of a unit change of the independent variable *X* on the
    dependent variable *Y*.”
  prefs: []
  type: TYPE_NORMAL
- en: You are going to see that the line we fit through our scatterplot can be expressed
    as an equation; unlike the correlation coefficient, this equation depends on how
    we define our independent and dependent variables.
  prefs: []
  type: TYPE_NORMAL
- en: Like correlation, linear regression assumes that a linear relationship exists
    between the two variables. Other assumptions do exist and are important to consider
    when modeling data. For example, we do not want to have extreme observations that
    may disproportionately affect the overall trend of the linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of our demonstration, we will overlook this and other assumptions
    for now. These assumptions are often difficult to test using Excel; your knowledge
    of statistical programming will serve you well when looking into the deeper points
    of linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a deep breath; it’s time for another equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-1\. The equation for linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="upper Y equals beta 0 plus beta 1 times upper X plus epsilon"
    display="block"><mrow><mi>Y</mi> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mo>×</mo> <mi>X</mi> <mo>+</mo>
    <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The goal of [Equation 4-1](#linear-regression-equation) is to predict our dependent
    variable *Y*. That’s the left side. You may remember from school that a line can
    be broken into its *intercept* and *slope*. That’s where <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1 times upper X Subscript i"><mrow><msub><mi>β</mi>
    <mn>1</mn></msub> <mo>×</mo> <msub><mi>X</mi> <mi>i</mi></msub></mrow></math>
    , respectively, come in. In the second term, we multiply our independent variable
    by a slope *coefficient*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there will be a part of the relationship between our independent and
    dependent variable that can be explained not by the model per se but by some external
    influence. This is known as the model’s *error* and is indicated by <math alttext="epsilon
    Subscript i"><msub><mi>ε</mi> <mi>i</mi></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier we used the independent samples t-test to examine a significant difference
    in means between two groups. Here, we are measuring the linear influence of one
    continuous variable on another. We will do this by examining whether the *slope*
    of the fit regression line is statistically different than zero. That means our
    hypothesis test will work something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H0: There is no linear influence of our independent variable on our dependent
    variable. (The slope of the regression line equals zero.)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ha: There is a linear influence of our independent variable on our dependent
    variable. (The slope of the regression line does not equal zero.)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 4-8](#hypothesis-test) shows some examples of what significant and
    insignificant slopes might look like.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, we don’t have *all* the data, so we don’t know what the “true” slope
    would be for the population. Instead, we are inferring whether, given our sample,
    this slope would be statistically different from zero. We can use the same p-value
    methodology to estimate the slope’s significance that we did to find the difference
    in means of two groups. We will continue to conduct two-tailed tests at the 95%
    confidence interval. Let’s jump into finding the results using Excel.
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression slope hypothesis](assets/aina_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. Regression models with significant and insignificant slopes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linear Regression in Excel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this demo of linear regression on the *mpg* dataset in Excel, we test whether
    a car’s weight (*weight*) has a significant influence on its mileage (*mpg*).
    That means our hypotheses will be:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H0: There is no linear influence of weight on mileage.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Ha: There is a linear influence of weight on mileage.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Before getting started, it’s a good idea to write out the regression equation
    using the specific variables of interest, which I’ve done in [Equation 4-2](#mpg-regression-equation):'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-2\. Our regression equation for estimating mileage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="m p g equals beta 0 plus beta 1 times w e i g h t plus epsilon"
    display="block"><mrow><mi>m</mi> <mi>p</mi> <mi>g</mi> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mo>×</mo> <mi>w</mi>
    <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>+</mo> <mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with visualizing the results of the regression: we already have
    the scatterplot from [Figure 4-6](#weight-mileage-scatter), now it’s just a matter
    of overlaying or “fitting” the regression line onto it. Click on the perimeter
    of the plot to launch the “Chart Elements” menu. Click on “Trendline,” then “More
    Options” to the side. Click the radio button at the bottom of the “Format Trendline”
    screen reading “Display Equation on chart.”'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s click on the resulting equation on the graph to add bold formatting
    and increase its font size to 14\. We’ll make the trendline solid black and give
    it a 2.5-point width by clicking on it in the graph, then going to the paint bucket
    icon at the top of the Format Trendline menu. We now have the making of linear
    regression. Our scatterplot with trendline looks like [Figure 4-9](#scatter-plot-trendline).
    Excel also includes the *regression* equation we are looking for from [Equation
    4-2](#mpg-regression-equation) to estimate a car’s mileage based on its weight.
  prefs: []
  type: TYPE_NORMAL
- en: '![scatterplot regression equation in Excel](assets/aina_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. Scatterplot with trendline and regression equation for the effect
    of weight on mileage
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can place the intercept before the slope in our equation to get [Equation
    4-3](#excel-fit-regression-equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-3\. Equation 4-3\. Our fit regression equation for estimating mileage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="m p g equals 46.217 minus 0.0076 times w e i g h t" display="block"><mrow><mi>m</mi>
    <mi>p</mi> <mi>g</mi> <mo>=</mo> <mn>46.217</mn> <mo>-</mo> <mn>0.0076</mn> <mo>×</mo>
    <mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that Excel does not include the error term as part of the regression
    equation. Now that we’ve fit the regression line, we’ve quantified the difference
    between what values we expect from the equation and what values are found in the
    data. This difference is known as the *residual*, and we’ll come back to it later
    in this chapter. First, we’ll get back to what we set out to do: establish statistical
    significance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s great that Excel fit the line for us and gave us the resulting equation.
    But this does *not* give us enough information to conduct the hypothesis test:
    we still don’t know whether the line’s slope is statistically different than zero.
    To get this information, we will again use the Analysis ToolPak. From the ribbon,
    go to Data → Data Analysis → Regression. You’ll be asked to select your Y and
    X ranges; these are your dependent and independent variables, respectively. Make
    sure to indicate that your inputs include labels, as shown in [Figure 4-10](#regression-settings).'
  prefs: []
  type: TYPE_NORMAL
- en: '![ToolPak regression setup](assets/aina_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Menu settings for deriving a regression with the ToolPak
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This results in quite a lot of information, which is shown in [Figure 4-11](#regression-output-excel).
    Let’s step through it.
  prefs: []
  type: TYPE_NORMAL
- en: Ignore the first section in cells `A3:B8` for now; we will return to it later.
    Our second section in `A10:F14` is labeled ANOVA (short for *analysis of variance*).
    This tells us whether our regression performs significantly better with the coefficient
    of the slope included versus one with just the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression results](assets/aina_0411.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. Regression output
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 4-2](#intercept-vs-full) spells out what the competing equations are
    here.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-2\. Intercept-only versus full regression model
  prefs: []
  type: TYPE_NORMAL
- en: '| Incercept-only model | Model with coefficients |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *mpg* = 46.217 | *mpg* = 46.217 − 0.0076 × *weight* |'
  prefs: []
  type: TYPE_TB
- en: 'A statistically significant result indicates that our coefficients do improve
    the model. We can determine the results of the test from the p-value found in
    cell `F12` of [Figure 4-11](#regression-output-excel). Remember, this is scientific
    notation, so read the p-value as 6.01 times 10 to the power of –102: much smaller
    than 0.05\. We can conclude that *weight* is worth keeping as a coefficient in
    the regression model.'
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the third section in cells `A16:I18`; here is where we find
    what we were originally looking for. This range contains a lot of information,
    so let’s go column by column starting with the coefficients in cells `B17:B18`.
    These should look familiar as the intercept and slope of the line that were given
    in [Equation 4-3](#excel-fit-regression-equation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the standard error in `C17:C18`. We talked about this in [Chapter 3](ch03.html#foundations-of-inference):
    it’s a measure of variability across repeated samples and in this case can be
    thought of as a measure of our coefficients’ precision.'
  prefs: []
  type: TYPE_NORMAL
- en: We then have what Excel calls the “t Stat,” otherwise known as the t-statistic
    or test statistic, in `D17:D18`; this can be derived by dividing the coefficient
    by the standard error. We can compare it to our critical value of 1.96 to establish
    statistical significance at 95% confidence.
  prefs: []
  type: TYPE_NORMAL
- en: It’s more common, however, to interpret and report on the p-value, which gives
    the same information. We have two p-values to interpret. First, the intercept’s
    coefficient in `E17`. This tells us whether the intercept is significantly different
    than zero. The significance of the intercept is *not* part of our hypothesis test,
    so this information is irrelevant. (This is another good example of why we can’t
    always take Excel’s output at face value.)
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While most statistical packages (including Excel) report the p-value of the
    intercept, it’s usually not relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we want the p-value of *weight* in cell `E18`: this is related to
    the line’s slope. The p-value is well under 0.05, so we fail to reject the null
    and conclude that weight does likely influence mileage. In other words, the line’s
    slope is significantly different than zero. Just like with our earlier hypothesis
    tests, we will shy away from concluding that we’ve “proven” a relationship, or
    that more weight *causes* lower mileage. Again, we are making inferences about
    a population based on a sample, so uncertainty is inherent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output also gives us the 95% confidence interval for our intercept and
    slope in cells `F17:I18`. By default, this is stated twice: had we asked for a
    different confidence interval in the input menu, we’d have received both here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you’re getting the hang of interpreting the regression output, let’s
    try making a *point estimate* based on the equation line: what would we expect
    the mileage to be for a car weighing 3,021 pounds? Let’s plug it into our regression
    equation in [Equation 4-4](#regression-point-estimate):'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-4\. Equation 4-4\. Making a point estimate based on our equation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="m p g equals 46.217 minus 0.0076 times 3021" display="block"><mrow><mi>m</mi>
    <mi>p</mi> <mi>g</mi> <mo>=</mo> <mn>46.217</mn> <mo>-</mo> <mn>0.0076</mn> <mo>×</mo>
    <mn>3021</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on [Equation 4-4](#regression-point-estimate), we expect a car weighing
    3,021 pounds to get 23.26 miles per gallon. Take a look at the source dataset:
    there *is* an observation weighing 3,021 pounds (Ford Maverick, row `101` in the
    dataset) and it gets 18 miles per gallon, not 23.26\. *What gives?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This discrepancy is the *residual* that was mentioned earlier: it’s the difference
    between the values we estimated in the regression equation and those that are
    found in the actual data. I’ve included this and some other observations in [Figure 4-12](#residuals-labeled).
    The scatterpoints represent what values are actually found in the dataset, and
    the line represents what values we predicted with the regression.'
  prefs: []
  type: TYPE_NORMAL
- en: It stands to reason that we’d be motivated to minimize the difference between
    these values. Excel and most regression applications use *ordinary least squares*
    (OLS) to do this. Our goal in OLS is to minimize residuals, specifically, the
    *sum of squared residuals*, so that both negative and positive residuals are measured
    equally. The lower the sum of squared residuals, the less of a difference there
    is between our actual and expected values, and the better our regression equation
    is at making estimates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Residuals shown in Excel](assets/aina_0412.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-12\. Residuals as the differences between actual and predicted values
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We learned from the p-value of our slope that there is a significant relationship
    between independent and dependent variables. But this does not tell us how *much*
    of the variability in our dependent variable is explained by our independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that variability is at the heart of what we study as analysts; variables
    vary, and we want to study *why* they vary. Experiments let us do that, by understanding
    the relationship between an independent and dependent variable. But we won’t be
    able to explain everything about our dependent variable with our independent variable.
    There will always be some unexplained error.
  prefs: []
  type: TYPE_NORMAL
- en: '*R-squared*, or the coefficient of determination (which Excel refers to as
    *R-square*), expresses as a percentage how much variability in the dependent variable
    is explained by our regression model. For example, an R-squared of 0.4 indicates
    that 40% of variability in Y can be explained by the model. This means that 1
    minus R-squared is what variability *can’t* be explained by the model. If R-squared
    is 0.4, then 60% of Y’s variability is unaccounted for.'
  prefs: []
  type: TYPE_NORMAL
- en: Excel calculates R-squared for us in the first box of regression output; take
    a look back to cell `B5` in [Figure 4-11](#regression-output-excel). The square
    root of R-squared is multiple R, which is also seen in cell `B4` of the output.
    Adjusted R-square (cell `B6`) is used as a more conservative estimate of R-squared
    for a model with multiple independent variables. This measure is of interest when
    conducting *multiple* linear regression, which is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other ways than R-squared to measure the performance of regression:
    Excel includes one of them, the standard error of the regression, in its output
    (cell `B7` in [Figure 4-11](#regression-output-excel)). This measure tells us
    the average distance that observed values deviate from the regression line. Some
    analysts prefer this or other measures to R-squared for evaluating regression
    models, although R-squared remains a dominant choice. Regardless of preferences,
    the best evaluation often comes from evaluating multiple figures in their proper
    context, so there’s no need to swear by or swear off any one measure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Congratulations: you conducted and interpreted a complete regression analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rethinking Our Results: Spurious Relationships'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on their temporal ordering and our own logic, it’s nearly absolute in
    our mileage example that *weight* should be the independent variable and *mpg*
    the dependent. But what happens if we fit the regression line with these variables
    reversed? Go ahead and give it a try using the ToolPak. The resulting regression
    equation is shown in [Equation 4-5](#weight-mpg-regression-equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 4-5\. Equation 4-5\. A regression equation to estimate weight based
    on mileage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="w e i g h t equals 5101.1 minus 90.571 times m p g" display="block"><mrow><mi>w</mi>
    <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo> <mn>5101.1</mn>
    <mo>-</mo> <mn>90.571</mn> <mo>×</mo> <mi>m</mi> <mi>p</mi> <mi>g</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We can flip our independent and dependent variables and get the same correlation
    coefficient. But when we change them for regression, *our coefficients change*.
  prefs: []
  type: TYPE_NORMAL
- en: Were we to find out that *mpg* and *weight* were both influenced simultaneously
    by some outside variable, then neither of these models would be correct. And this
    is the same scenario that we’re faced with in ice cream consumption and shark
    attacks. It’s silly to say that ice cream consumption has any influence on shark
    attacks, because both of these are influenced by temperature, as [Figure 4-13](#ice-cream-shark-attack-confound)
    depicts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Ice cream versus shark attacks spurious relationship](assets/aina_0413.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4-13\. Ice cream consumption and shark attacks: a spurious relationship'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is called a *spurious* relationship. It’s frequently found in data, and
    it may not be as obvious as this example. Having some domain knowledge of the
    data you are studying can be invaluable for detecting spurious relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Variables can be correlated; there could even be evidence of a causal relationship.
    But the relationship might be driven by some variable you’ve not even accounted
    for.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember this old phrase?
  prefs: []
  type: TYPE_NORMAL
- en: Correlation doesn’t imply causation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Analytics is highly incremental: we usually layer one concept on top of the
    next to build increasingly complex analyses. For example, we’ll always start with
    descriptive statistics of the sample before attemping to infer parameters of the
    population. While correlation may not imply causation, causation is built on the
    foundations of correlation. That means a better way to summarize the relationship
    might be:'
  prefs: []
  type: TYPE_NORMAL
- en: Correlation is a necessary but not sufficient condition for causation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We’ve just scratched the surface of inferential statistics in this and previous
    chapters. A whole world of tests exists, but all of them use the same framework
    of *hypothesis testing* that we’ve used here. Get this process down, and you’ll
    be able to test for all sorts of different data relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Advancing into Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope you’ve seen and agree that Excel is a fantastic tool for learning statistics
    and analytics. You got a hands-on look at the statistical principles that power
    much of this work, and learned how to explore and test for relationships in real
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: That said, Excel can have diminishing returns when it comes to more advanced
    analytics. For example, we’ve been checking for properties like normality and
    linearity using visualizations; this is a good start, but there are more robust
    ways to test them (often, in fact, using statistical inference). These techniques
    often rely on matrix algebra and other computationally intensive operations that
    can be tedious to derive in Excel. While add-ins are available to make up for
    these shortcomings, they can be expensive and lack particular features. On the
    other hand, as open source tools R and Python are free, and include many app-like
    features called *packages* that serve nearly any use case. This environment will
    allow you to focus on the conceptual analysis of your data rather than raw computation,
    but you will need to learn how to program. These tools, and the analytics toolkit
    in general, will be the focus of [Chapter 5](ch05.html#data-analytics-stack).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practice your correlation and regression chops by analyzing the *ais* dataset
    found in the book repository’s [*datasets* folder](https://oreil.ly/hazKQ). This
    dataset includes height, weight, and blood readings from male and female Australian
    athletes of different sports.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the dataset, try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Produce a correlation matrix of the relevant variables in this dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visualize the relationship of *ht* and *wt*. Is this a linear relationship?
    If so, it is negative or positive?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of *ht* and *wt*, which do you presume is the independent and dependent variable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a significant influence of the independent variable on the dependent
    variable?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the slope of your fit regression line?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What percentage of the variance in the dependent variable is explained by the
    independent variable?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This dataset contains a variable for body mass index, *bmi*. If you are not
    familiar with this metric, take a moment to research how it’s calculated. Knowing
    this, would you want to analyze the relationship between *ht* and *bmi*? Don’t
    hesitate to lean on common sense here rather than just statistical reasoning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
