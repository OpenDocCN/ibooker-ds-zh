- en: Chapter 7\. Hypothesis and Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is the mark of a truly intelligent person to be moved by statistics.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: George Bernard Shaw
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What will we do with all this statistics and probability theory? The *science*
    part of data science frequently involves forming and testing *hypotheses* about
    our data and the processes that generate it.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Hypothesis Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, as data scientists, we’ll want to test whether a certain hypothesis is
    likely to be true. For our purposes, hypotheses are assertions like “this coin
    is fair” or “data scientists prefer Python to R” or “people are more likely to
    navigate away from the page without ever reading the content if we pop up an irritating
    interstitial advertisement with a tiny, hard-to-find close button” that can be
    translated into statistics about data. Under various assumptions, those statistics
    can be thought of as observations of random variables from known distributions,
    which allows us to make statements about how likely those assumptions are to hold.
  prefs: []
  type: TYPE_NORMAL
- en: In the classical setup, we have a *null hypothesis*, <math><msub><mi>H</mi>
    <mn>0</mn></msub></math> , that represents some default position, and some alternative
    hypothesis, <math><msub><mi>H</mi> <mn>1</mn></msub></math> , that we’d like to
    compare it with. We use statistics to decide whether we can reject <math><msub><mi>H</mi>
    <mn>0</mn></msub></math> as false or not. This will probably make more sense with
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Flipping a Coin'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine we have a coin and we want to test whether it’s fair. We’ll make the
    assumption that the coin has some probability *p* of landing heads, and so our
    null hypothesis is that the coin is fair—that is, that *p* = 0.5\. We’ll test
    this against the alternative hypothesis *p* ≠ 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, our test will involve flipping the coin some number, *n*, times
    and counting the number of heads, *X*. Each coin flip is a Bernoulli trial, which
    means that *X* is a Binomial(*n*,*p*) random variable, which (as we saw in [Chapter 6](ch06.html#probability))
    we can approximate using the normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever a random variable follows a normal distribution, we can use `normal_cdf`
    to figure out the probability that its realized value lies within or outside a
    particular interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also do the reverse—find either the nontail region or the (symmetric)
    interval around the mean that accounts for a certain level of likelihood. For
    example, if we want to find an interval centered at the mean and containing 60%
    probability, then we find the cutoffs where the upper and lower tails each contain
    20% of the probability (leaving 60%):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In particular, let’s say that we choose to flip the coin *n* = 1,000 times.
    If our hypothesis of fairness is true, *X* should be distributed approximately
    normally with mean 500 and standard deviation 15.8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We need to make a decision about *significance*—how willing we are to make a
    *type 1 error* (“false positive”), in which we reject <math><msub><mi>H</mi> <mn>0</mn></msub></math>
    even though it’s true. For reasons lost to the annals of history, this willingness
    is often set at 5% or 1%. Let’s choose 5%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the test that rejects <math><msub><mi>H</mi> <mn>0</mn></msub></math>
    if *X* falls outside the bounds given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Assuming *p* really equals 0.5 (i.e., <math><msub><mi>H</mi> <mn>0</mn></msub></math>
    is true), there is just a 5% chance we observe an *X* that lies outside this interval,
    which is the exact significance we wanted. Said differently, if <math><msub><mi>H</mi>
    <mn>0</mn></msub></math> is true, then, approximately 19 times out of 20, this
    test will give the correct result.
  prefs: []
  type: TYPE_NORMAL
- en: We are also often interested in the *power* of a test, which is the probability
    of not making a *type 2 error* (“false negative”), in which we fail to reject
    <math><msub><mi>H</mi> <mn>0</mn></msub></math> even though it’s false. In order
    to measure this, we have to specify what exactly <math><msub><mi>H</mi> <mn>0</mn></msub></math>
    being false *means*. (Knowing merely that *p* is *not* 0.5 doesn’t give us a ton
    of information about the distribution of *X*.) In particular, let’s check what
    happens if *p* is really 0.55, so that the coin is slightly biased toward heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In that case, we can calculate the power of the test with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Imagine instead that our null hypothesis was that the coin is not biased toward
    heads, or that <math><mrow><mi>p</mi> <mo>≤</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>
    . In that case we want a *one-sided test* that rejects the null hypothesis when
    *X* is much larger than 500 but not when *X* is smaller than 500\. So, a 5% significance
    test involves using `normal_probability_below` to find the cutoff below which
    95% of the probability lies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is a more powerful test, since it no longer rejects <math><msub><mi>H</mi>
    <mn>0</mn></msub></math> when *X* is below 469 (which is very unlikely to happen
    if <math><msub><mi>H</mi> <mn>1</mn></msub></math> is true) and instead rejects
    <math><msub><mi>H</mi> <mn>0</mn></msub></math> when *X* is between 526 and 531
    (which is somewhat likely to happen if <math><msub><mi>H</mi> <mn>1</mn></msub></math>
    is true).
  prefs: []
  type: TYPE_NORMAL
- en: p-Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative way of thinking about the preceding test involves *p-values*.
    Instead of choosing bounds based on some probability cutoff, we compute the probability—assuming
    <math><msub><mi>H</mi> <mn>0</mn></msub></math> is true—that we would see a value
    at least as extreme as the one we actually observed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our two-sided test of whether the coin is fair, we compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to see 530 heads, we would compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why did we use a value of `529.5` rather than using `530`? This is what’s called
    a [*continuity correction*](http://en.wikipedia.org/wiki/Continuity_correction).
    It reflects the fact that `normal_probability_between(529.5, 530.5, mu_0, sigma_0)`
    is a better estimate of the probability of seeing 530 heads than `normal_probability_between(530,
    531, mu_0, sigma_0)` is.
  prefs: []
  type: TYPE_NORMAL
- en: Correspondingly, `normal_probability_above(529.5, mu_0, sigma_0)` is a better
    estimate of the probability of seeing at least 530 heads. You may have noticed
    that we also used this in the code that produced [Figure 6-4](ch06.html#make_hist_result).
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to convince yourself that this is a sensible estimate is with a simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the *p*-value is greater than our 5% significance, we don’t reject the
    null. If we instead saw 532 heads, the *p*-value would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: which is smaller than the 5% significance, which means we would reject the null.
    It’s the exact same test as before. It’s just a different way of approaching the
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we would have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For our one-sided test, if we saw 525 heads we would compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'which means we wouldn’t reject the null. If we saw 527 heads, the computation
    would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: and we would reject the null.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Make sure your data is roughly normally distributed before using `normal_probability_above`
    to compute *p*-values. The annals of bad data science are filled with examples
    of people opining that the chance of some observed event occurring at random is
    one in a million, when what they really mean is “the chance, assuming the data
    is distributed normally,” which is fairly meaningless if the data isn’t.
  prefs: []
  type: TYPE_NORMAL
- en: There are various statistical tests for normality, but even plotting the data
    is a good start.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence Intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve been testing hypotheses about the value of the heads probability *p*,
    which is a *parameter* of the unknown “heads” distribution. When this is the case,
    a third approach is to construct a *confidence interval* around the observed value
    of the parameter.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can estimate the probability of the unfair coin by looking at
    the average value of the Bernoulli variables corresponding to each flip—1 if heads,
    0 if tails. If we observe 525 heads out of 1,000 flips, then we estimate *p* equals
    0.525.
  prefs: []
  type: TYPE_NORMAL
- en: 'How *confident* can we be about this estimate? Well, if we knew the exact value
    of *p*, the central limit theorem (recall [“The Central Limit Theorem”](ch06.html#central_limit_theorem))
    tells us that the average of those Bernoulli variables should be approximately
    normal, with mean *p* and standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we don’t know *p*, so instead we use our estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is not entirely justified, but people seem to do it anyway. Using the
    normal approximation, we conclude that we are “95% confident” that the following
    interval contains the true parameter *p*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a statement about the *interval*, not about *p*. You should understand
    it as the assertion that if you were to repeat the experiment many times, 95%
    of the time the “true” parameter (which is the same every time) would lie within
    the observed confidence interval (which might be different every time).
  prefs: []
  type: TYPE_NORMAL
- en: In particular, we do not conclude that the coin is unfair, since 0.5 falls within
    our confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'If instead we’d seen 540 heads, then we’d have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, “fair coin” doesn’t lie in the confidence interval. (The “fair coin” hypothesis
    doesn’t pass a test that you’d expect it to pass 95% of the time if it were true.)
  prefs: []
  type: TYPE_NORMAL
- en: p-Hacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A procedure that erroneously rejects the null hypothesis only 5% of the time
    will—by definition—5% of the time erroneously reject the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: What this means is that if you’re setting out to find “significant” results,
    you usually can. Test enough hypotheses against your dataset, and one of them
    will almost certainly appear significant. Remove the right outliers, and you can
    probably get your *p*-value below 0.05\. (We did something vaguely similar in
    [“Correlation”](ch05.html#correlation); did you notice?)
  prefs: []
  type: TYPE_NORMAL
- en: This is sometimes called [*p-hacking*](https://www.nature.com/news/scientific-method-statistical-errors-1.14700)
    and is in some ways a consequence of the “inference from *p*-values framework.”
    A good article criticizing this approach is [“The Earth Is Round”](http://www.iro.umontreal.ca/~dift3913/cours/papers/cohen1994_The_earth_is_round.pdf),
    by Jacob Cohen.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to do good *science*, you should determine your hypotheses before
    looking at the data, you should clean your data without the hypotheses in mind,
    and you should keep in mind that *p*-values are not substitutes for common sense.
    (An alternative approach is discussed in [“Bayesian Inference”](#bayesian_inference).)
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Running an A/B Test'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of your primary responsibilities at DataSciencester is experience optimization,
    which is a euphemism for trying to get people to click on advertisements. One
    of your advertisers has developed a new energy drink targeted at data scientists,
    and the VP of Advertisements wants your help choosing between advertisement A
    (“tastes great!”) and advertisement B (“less bias!”).
  prefs: []
  type: TYPE_NORMAL
- en: Being a *scientist*, you decide to run an *experiment* by randomly showing site
    visitors one of the two advertisements and tracking how many people click on each
    one.
  prefs: []
  type: TYPE_NORMAL
- en: If 990 out of 1,000 A-viewers click their ad, while only 10 out of 1,000 B-viewers
    click their ad, you can be pretty confident that A is the better ad. But what
    if the differences are not so stark? Here’s where you’d use statistical inference.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that <math><msub><mi>N</mi> <mn>A</mn></msub></math> people see ad
    A, and that <math><msub><mi>n</mi> <mn>A</mn></msub></math> of them click it.
    We can think of each ad view as a Bernoulli trial where <math><msub><mi>p</mi>
    <mn>A</mn></msub></math> is the probability that someone clicks ad A. Then (if
    <math><msub><mi>N</mi> <mn>A</mn></msub></math> is large, which it is here) we
    know that <math><mrow><msub><mi>n</mi> <mi>A</mi></msub> <mo>/</mo> <msub><mi>N</mi>
    <mi>A</mi></msub></mrow></math> is approximately a normal random variable with
    mean <math><msub><mi>p</mi> <mi>A</mi></msub></math> and standard deviation <math><mrow><msub><mi>σ</mi>
    <mi>A</mi></msub> <mo>=</mo> <msqrt><mrow><msub><mi>p</mi> <mi>A</mi></msub> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>p</mi> <mi>A</mi></msub> <mo>)</mo></mrow> <mo>/</mo>
    <msub><mi>N</mi> <mi>A</mi></msub></mrow></msqrt></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, <math><mrow><msub><mi>n</mi> <mi>B</mi></msub> <mo>/</mo> <msub><mi>N</mi>
    <mi>B</mi></msub></mrow></math> is approximately a normal random variable with
    mean <math><msub><mi>p</mi> <mi>B</mi></msub></math> and standard deviation <math><mrow><msub><mi>σ</mi>
    <mi>B</mi></msub> <mo>=</mo> <msqrt><mrow><msub><mi>p</mi> <mi>B</mi></msub> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>p</mi> <mi>B</mi></msub> <mo>)</mo></mrow> <mo>/</mo>
    <msub><mi>N</mi> <mi>B</mi></msub></mrow></msqrt></mrow></math> . We can express
    this in code as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If we assume those two normals are independent (which seems reasonable, since
    the individual Bernoulli trials ought to be), then their difference should also
    be normal with mean <math><mrow><msub><mi>p</mi> <mi>B</mi></msub> <mo>-</mo>
    <msub><mi>p</mi> <mi>A</mi></msub></mrow></math> and standard deviation <math><msqrt><mrow><msubsup><mi>σ</mi>
    <mi>A</mi> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mi>σ</mi> <mi>B</mi> <mn>2</mn></msubsup></mrow></msqrt></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is sort of cheating. The math only works out exactly like this if you *know*
    the standard deviations. Here we’re estimating them from the data, which means
    that we really should be using a *t*-distribution. But for large enough datasets,
    it’s close enough that it doesn’t make much of a difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means we can test the *null hypothesis* that <math><msub><mi>p</mi> <mi>A</mi></msub></math>
    and <math><msub><mi>p</mi> <mi>B</mi></msub></math> are the same (that is, that
    <math><mrow><msub><mi>p</mi> <mi>A</mi></msub> <mo>-</mo> <msub><mi>p</mi> <mi>B</mi></msub></mrow></math>
    is 0) by using the statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: which should approximately be a standard normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if “tastes great” gets 200 clicks out of 1,000 views and “less
    bias” gets 180 clicks out of 1,000 views, the statistic equals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The probability of seeing such a large difference if the means were actually
    equal would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'which is large enough that we can’t conclude there’s much of a difference.
    On the other hand, if “less bias” only got 150 clicks, we’d have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: which means there’s only a 0.003 probability we’d see such a large difference
    if the ads were equally effective.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The procedures we’ve looked at have involved making probability statements
    about our *tests*: e.g., “There’s only a 3% chance you’d observe such an extreme
    statistic if our null hypothesis were true.”'
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach to inference involves treating the unknown parameters
    themselves as random variables. The analyst (that’s you) starts with a *prior
    distribution* for the parameters and then uses the observed data and Bayes’s theorem
    to get an updated *posterior distribution* for the parameters. Rather than making
    probability judgments about the tests, you make probability judgments about the
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when the unknown parameter is a probability (as in our coin-flipping
    example), we often use a prior from the *Beta distribution*, which puts all its
    probability between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Generally speaking, this distribution centers its weight at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: and the larger `alpha` and `beta` are, the “tighter” the distribution is.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if `alpha` and `beta` are both 1, it’s just the uniform distribution
    (centered at 0.5, very dispersed). If `alpha` is much larger than `beta`, most
    of the weight is near 1\. And if `alpha` is much smaller than `beta`, most of
    the weight is near 0\. [Figure 7-1](#beta_priors) shows several different Beta
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Example Beta distributions.](assets/dsf2_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Example Beta distributions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Say we assume a prior distribution on *p*. Maybe we don’t want to take a stand
    on whether the coin is fair, and we choose `alpha` and `beta` to both equal 1\.
    Or maybe we have a strong belief that the coin lands heads 55% of the time, and
    we choose `alpha` equals 55, `beta` equals 45.
  prefs: []
  type: TYPE_NORMAL
- en: Then we flip our coin a bunch of times and see *h* heads and *t* tails. Bayes’s
    theorem (and some mathematics too tedious for us to go through here) tells us
    that the posterior distribution for *p* is again a Beta distribution, but with
    parameters `alpha + h` and `beta + t`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is no coincidence that the posterior distribution was again a Beta distribution.
    The number of heads is given by a Binomial distribution, and the Beta is the [*conjugate
    prior*](http://www.johndcook.com/blog/conjugate_prior_diagram/) to the Binomial
    distribution. This means that whenever you update a Beta prior using observations
    from the corresponding binomial, you will get back a Beta posterior.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you flip the coin 10 times and see only 3 heads. If you started with
    the uniform prior (in some sense refusing to take a stand about the coin’s fairness),
    your posterior distribution would be a Beta(4, 8), centered around 0.33\. Since
    you considered all probabilities equally likely, your best guess is close to the
    observed probability.
  prefs: []
  type: TYPE_NORMAL
- en: If you started with a Beta(20, 20) (expressing a belief that the coin was roughly
    fair), your posterior distribution would be a Beta(23, 27), centered around 0.46,
    indicating a revised belief that maybe the coin is slightly biased toward tails.
  prefs: []
  type: TYPE_NORMAL
- en: And if you started with a Beta(30, 10) (expressing a belief that the coin was
    biased to flip 75% heads), your posterior distribution would be a Beta(33, 17),
    centered around 0.66\. In that case you’d still believe in a heads bias, but less
    strongly than you did initially. These three different posteriors are plotted
    in [Figure 7-2](#beta_posteriors).
  prefs: []
  type: TYPE_NORMAL
- en: '![Posteriors arising from different priors.](assets/dsf2_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Posteriors arising from different priors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you flipped the coin more and more times, the prior would matter less and
    less until eventually you’d have (nearly) the same posterior distribution no matter
    which prior you started with.
  prefs: []
  type: TYPE_NORMAL
- en: For example, no matter how biased you initially thought the coin was, it would
    be hard to maintain that belief after seeing 1,000 heads out of 2,000 flips (unless
    you are a lunatic who picks something like a Beta(1000000,1) prior).
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s interesting is that this allows us to make probability statements about
    hypotheses: “Based on the prior and the observed data, there is only a 5% likelihood
    the coin’s heads probability is between 49% and 51%.” This is philosophically
    very different from a statement like “If the coin were fair, we would expect to
    observe data so extreme only 5% of the time.”'
  prefs: []
  type: TYPE_NORMAL
- en: Using Bayesian inference to test hypotheses is considered somewhat controversial—in
    part because the mathematics can get somewhat complicated, and in part because
    of the subjective nature of choosing a prior. We won’t use it any further in this
    book, but it’s good to know about.
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve barely scratched the surface of what you should know about statistical
    inference. The books recommended at the end of [Chapter 5](ch05.html#statistics)
    go into a lot more detail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coursera offers a [Data Analysis and Statistical Inference](https://www.coursera.org/course/statistics)
    course that covers many of these topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
