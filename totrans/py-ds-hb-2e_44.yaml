- en: Chapter 39\. Hyperparameters and Model Validation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第39章。超参数和模型验证
- en: 'In the previous chapter, we saw the basic recipe for applying a supervised
    machine learning model:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了应用监督机器学习模型的基本方法：
- en: Choose a class of model.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个模型类别。
- en: Choose model hyperparameters.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型超参数。
- en: Fit the model to the training data.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到训练数据中。
- en: Use the model to predict labels for new data.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型来预测新数据的标签。
- en: The first two pieces of this—the choice of model and choice of hyperparameters—are
    perhaps the most important part of using these tools and techniques effectively.
    In order to make informed choices, we need a way to *validate* that our model
    and our hyperparameters are a good fit to the data. While this may sound simple,
    there are some pitfalls that you must avoid to do this effectively.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个部分——模型的选择和超参数的选择——可能是有效使用这些工具和技术的最重要部分。为了做出明智的选择，我们需要一种验证模型和超参数是否与数据相匹配的方法。虽然这听起来很简单，但要有效地做到这一点，你必须避免一些陷阱。
- en: Thinking About Model Validation
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思考模型验证
- en: 'In principle, model validation is very simple: after choosing a model and its
    hyperparameters, we can estimate how effective it is by applying it to some of
    the training data and comparing the predictions to the known values.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，模型验证非常简单：在选择了模型和其超参数之后，我们可以通过将其应用于一些训练数据并将预测结果与已知值进行比较来估计其有效性。
- en: This section will first show a naive approach to model validation and why it
    fails, before exploring the use of holdout sets and cross-validation for more
    robust model evaluation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将首先展示一个关于模型验证的天真方法以及为什么它失败了，然后探讨使用保留集和交叉验证进行更健壮的模型评估。
- en: Model Validation the Wrong Way
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误的模型验证方法
- en: 'Let’s start with the naive approach to validation using the Iris dataset, which
    we saw in the previous chapter. We will start by loading the data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在上一章中看到的鸢尾花数据集中采用天真的验证方法开始。我们将从加载数据开始：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we choose a model and hyperparameters. Here we’ll use a *k*-nearest neighbors
    classifier with `n_neighbors=1`. This is a very simple and intuitive model that
    says “the label of an unknown point is the same as the label of its closest training
    point”:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择一个模型和超参数。在这里，我们将使用一个*n*最近邻分类器，其中`n_neighbors=1`。这是一个非常简单和直观的模型，它表示“未知点的标签与其最近训练点的标签相同”：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then we train the model, and use it to predict labels for data whose labels
    we already know:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们训练模型，并使用它来预测我们已经知道标签的数据的标签：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we compute the fraction of correctly labeled points:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算正确标记点的比例：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We see an accuracy score of 1.0, which indicates that 100% of points were correctly
    labeled by our model! But is this truly measuring the expected accuracy? Have
    we really come upon a model that we expect to be correct 100% of the time?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了一个准确度得分为1.0，这表明我们的模型100%正确标记了所有点！但这真的是测量预期准确度吗？我们真的找到了一个我们预计每次都会100%正确的模型吗？
- en: 'As you may have gathered, the answer is no. In fact, this approach contains
    a fundamental flaw: *it trains and evaluates the model on the same data*. Furthermore,
    this nearest neighbor model is an *instance-based* estimator that simply stores
    the training data, and predicts labels by comparing new data to these stored points:
    except in contrived cases, it will get 100% accuracy every time!'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经了解的那样，答案是否定的。事实上，这种方法包含一个根本性的缺陷：*它在相同的数据上训练和评估模型*。此外，这个最近邻模型是一个*基于实例*的估计器，它简单地存储训练数据，并通过将新数据与这些存储的点进行比较来预测标签：除了人为的情况外，它每次都会得到100%的准确度！
- en: 'Model Validation the Right Way: Holdout Sets'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确的模型验证方法：保留集
- en: 'So what can be done? A better sense of a model’s performance can be found by
    using what’s known as a *holdout set*: that is, we hold back some subset of the
    data from the training of the model, and then use this holdout set to check the
    model’s performance. This splitting can be done using the `train_test_split` utility
    in Scikit-Learn:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么可以做什么呢？通过使用所谓的*保留集*可以更好地了解模型的性能：也就是说，我们从模型的训练中保留一些数据子集，然后使用这个保留集来检查模型的性能。这种分割可以使用Scikit-Learn中的`train_test_split`工具来完成：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We see here a more reasonable result: the one-nearest-neighbor classifier is
    about 90% accurate on this holdout set. The holdout set is similar to unknown
    data, because the model has not “seen” it before.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到了一个更合理的结果：一对一最近邻分类器在这个保留集上的准确率约为90%。保留集类似于未知数据，因为模型以前没有“看到”它。
- en: Model Validation via Cross-Validation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过交叉验证进行模型验证
- en: One disadvantage of using a holdout set for model validation is that we have
    lost a portion of our data to the model training. In the preceding case, half
    the dataset does not contribute to the training of the model! This is not optimal,
    especially if the initial set of training data is small.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用留出法进行模型验证的一个缺点是我们已经失去了一部分数据用于模型训练。在前述情况下，一半的数据集并没有对模型的训练做出贡献！这并不是最优的，特别是如果初始训练数据集很小的情况下。
- en: One way to address this is to use *cross-validation*; that is, to do a sequence
    of fits where each subset of the data is used both as a training set and as a
    validation set. Visually, it might look something like [Figure 39-1](#fig_images_in_0503-2-fold-cv).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是使用*交叉验证*；也就是说，进行一系列拟合，其中每个数据子集既用作训练集又用作验证集。从视觉上看，可能会像是 [Figure 39-1](#fig_images_in_0503-2-fold-cv)
    这样。
- en: '![05.03 2 fold CV](assets/05.03-2-fold-CV.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![05.03 2 fold CV](assets/05.03-2-fold-CV.png)'
- en: Figure 39-1\. Visualization of two-fold cross-validation^([1](ch39.xhtml#idm45858742132032))
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图39-1\. 二折交叉验证的可视化^([1](ch39.xhtml#idm45858742132032))
- en: 'Here we do two validation trials, alternately using each half of the data as
    a holdout set. Using the split data from earlier, we could implement it like this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们进行了两个验证试验，交替使用数据的每一半作为留出集。使用之前的分割数据，我们可以这样实现：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What comes out are two accuracy scores, which we could combine (by, say, taking
    the mean) to get a better measure of the global model performance. This particular
    form of cross-validation is a *two-fold cross-validation*—that is, one in which
    we have split the data into two sets and used each in turn as a validation set.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 出现的是两个准确度分数，我们可以结合（比如取平均值）来获得更好的全局模型性能衡量标准。这种特定形式的交叉验证是*二折交叉验证*——即，我们将数据分为两组，轮流将每一组作为验证集。
- en: We could expand on this idea to use even more trials, and more folds in the
    data—for example, [Figure 39-2](#fig_images_in_0503-5-fold-cv) shows a visual
    depiction of five-fold cross-validation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以扩展这个想法，使用更多的试验和数据折叠——例如，[Figure 39-2](#fig_images_in_0503-5-fold-cv) 展示了五折交叉验证的可视化描述。
- en: '![05.03 5 fold CV](assets/05.03-5-fold-CV.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![05.03 5 fold CV](assets/05.03-5-fold-CV.png)'
- en: Figure 39-2\. Visualization of five-fold cross-validation^([2](ch39.xhtml#idm45858742031984))
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图39-2\. 五折交叉验证的可视化^([2](ch39.xhtml#idm45858742031984))
- en: 'Here we split the data into five groups and use each in turn to evaluate the
    model fit on the other four-fifths of the data. This would be rather tedious to
    do by hand, but we can use Scikit-Learn’s `cross_val_score` convenience routine
    to do it succinctly:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数据分为五组，依次使用每一组来评估模型在其余四分之四的数据上的拟合情况。这样手工操作将会相当乏味，但我们可以使用 Scikit-Learn
    的 `cross_val_score` 方便地完成：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Repeating the validation across different subsets of the data gives us an even
    better idea of the performance of the algorithm.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的数据子集上重复验证可以更好地了解算法的性能。
- en: 'Scikit-Learn implements a number of cross-validation schemes that are useful
    in particular situations; these are implemented via iterators in the `model_selection`
    module. For example, we might wish to go to the extreme case in which our number
    of folds is equal to the number of data points: that is, we train on all points
    but one in each trial. This type of cross-validation is known as *leave-one-out*
    cross validation, and can be used as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 实现了许多在特定情况下有用的交叉验证方案；这些通过 `model_selection` 模块中的迭代器实现。例如，我们可能希望使用极端情况，即我们的折数等于数据点的数量：也就是说，在每次试验中我们训练所有点但排除一个。这种交叉验证被称为*留一法*交叉验证，可以如下使用：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Because we have 150 samples, the leave-one-out cross-validation yields scores
    for 150 trials, and each score indicates either a successful (1.0) or an unsuccessful
    (0.0) prediction. Taking the mean of these gives an estimate of the error rate:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有150个样本，留一法交叉验证会产生150次试验的分数，每个分数表示预测成功（1.0）或失败（0.0）。对这些分数求平均值可以估计误差率：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Other cross-validation schemes can be used similarly. For a description of what
    is available in Scikit-Learn, use IPython to explore the `sklearn.model_selection`
    submodule, or take a look at Scikit-Learn’s [cross-validation documentation](https://oreil.ly/rITkn).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其他交叉验证方案可以类似地使用。要了解 Scikit-Learn 中提供的内容，请使用 IPython 探索 `sklearn.model_selection`
    子模块，或查看 Scikit-Learn 的[交叉验证文档](https://oreil.ly/rITkn)。
- en: Selecting the Best Model
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最佳模型
- en: Now that we’ve explored the basics of validation and cross-validation, we will
    go into a little more depth regarding model selection and selection of hyperparameters.
    These issues are some of the most important aspects of the practice of machine
    learning, but I find that this information is often glossed over in introductory
    machine learning tutorials.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了验证和交叉验证的基础知识，我们将更深入地讨论模型选择和超参数选择的问题。这些问题是机器学习实践中最重要的方面之一，但我发现这些信息在初学者机器学习教程中经常被忽略。
- en: 'Of core importance is the following question: *if our estimator is underperforming,
    how should we move forward?* There are several possible answers:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 核心问题是以下问题：*如果我们的估计器表现不佳，我们该如何前进？* 有几种可能的答案：
- en: Use a more complicated/more flexible model.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个更复杂/更灵活的模型。
- en: Use a less complicated/less flexible model.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个不那么复杂/不那么灵活的模型。
- en: Gather more training samples.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多训练样本。
- en: Gather more data to add features to each sample.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多数据以增加每个样本的特征。
- en: The answer to this question is often counterintuitive. In particular, sometimes
    using a more complicated model will give worse results, and adding more training
    samples may not improve your results! The ability to determine what steps will
    improve your model is what separates the successful machine learning practitioners
    from the unsuccessful.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个问题的答案通常是反直觉的。特别是，有时使用更复杂的模型会导致更差的结果，并且增加更多的训练样本可能不会改善您的结果！能够确定哪些步骤将改进您的模型是成功的机器学习从业者和不成功的区别。
- en: The Bias-Variance Trade-off
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差-方差折衷
- en: Fundamentally, finding “the best model” is about finding a sweet spot in the
    trade-off between *bias* and *variance*. Consider [Figure 39-3](#fig_images_in_0503-bias-variance),
    which presents two regression fits to the same dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，找到“最佳模型”就是在*偏差*和*方差*之间的折衷中找到一个甜蜜点。考虑[图39-3](#fig_images_in_0503-bias-variance)，它展示了对同一数据集的两个回归拟合。
- en: '![05.03 bias variance](assets/05.03-bias-variance.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![05.03 偏差-方差](assets/05.03-bias-variance.png)'
- en: Figure 39-3\. High-bias and high-variance regression models^([3](ch39.xhtml#idm45858741405920))
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图39-3\. 高偏差和高方差的回归模型^([3](ch39.xhtml#idm45858741405920))
- en: It is clear that neither of these models is a particularly good fit to the data,
    but they fail in different ways.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这两个模型都不特别适合数据，但它们以不同的方式失败。
- en: 'The model on the left attempts to find a straight-line fit through the data.
    Because in this case a straight line cannot accurately split the data, the straight-line
    model will never be able to describe this dataset well. Such a model is said to
    *underfit* the data: that is, it does not have enough flexibility to suitably
    account for all the features in the data. Another way of saying this is that the
    model has high bias.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的模型试图通过数据找到一条直线拟合。因为在这种情况下，一条直线不能准确地分割数据，所以直线模型永远无法很好地描述这个数据集。这样的模型被称为*欠拟合*数据：即它没有足够的灵活性来适当地考虑数据中的所有特征。另一种说法是，该模型具有高偏差。
- en: 'The model on the right attempts to fit a high-order polynomial through the
    data. Here the model fit has enough flexibility to nearly perfectly account for
    the fine features in the data, but even though it very accurately describes the
    training data, its precise form seems to be more reflective of the particular
    noise properties of the data than of the intrinsic properties of whatever process
    generated that data. Such a model is said to *overfit* the data: that is, it has
    so much flexibility that the model ends up accounting for random errors as well
    as the underlying data distribution. Another way of saying this is that the model
    has high variance.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 右边的模型试图通过数据拟合高阶多项式。在这里，模型拟合具有足够的灵活性来几乎完美地描述数据中的细微特征，但即使它非常精确地描述了训练数据，其精确形式似乎更反映了数据的特定噪声属性，而不是生成数据的任何过程的固有属性。这样的模型被称为*过拟合*数据：即它具有如此高的灵活性，以至于模型最终解释了随机误差以及底层数据分布。另一种说法是，该模型具有高方差。
- en: To look at this in another light, consider what happens if we use these two
    models to predict the *y*-values for some new data. In the plots in [Figure 39-4](#fig_images_in_0503-bias-variance-2),
    the red/lighter points indicate data that is omitted from the training set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个角度来看，考虑一下如果我们使用这两个模型来预测一些新数据的*y*值会发生什么。在[图39-4](#fig_images_in_0503-bias-variance-2)中的图表中，红色/浅色点表示从训练集中省略的数据。
- en: '![05.03 bias variance 2](assets/05.03-bias-variance-2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![05.03 偏差-方差 2](assets/05.03-bias-variance-2.png)'
- en: Figure 39-4\. Training and validation scores in high-bias and high-variance
    models^([4](ch39.xhtml#idm45858741397728))
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 39-4\. 高偏差和高方差模型中的训练和验证分数^([4](ch39.xhtml#idm45858741397728))
- en: 'The score here is the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    score, or [coefficient of determination](https://oreil.ly/2AtV8), which measures
    how well a model performs relative to a simple mean of the target values. <math
    alttext="upper R squared equals 1"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo>
    <mn>1</mn></mrow></math> indicates a perfect match, <math alttext="upper R squared
    equals 0"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>0</mn></mrow></math>
    indicates the model does no better than simply taking the mean of the data, and
    negative values mean even worse models. From the scores associated with these
    two models, we can make an observation that holds more generally:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的分数是<math alttext="上限 R 平方"><msup><mi>R</mi> <mn>2</mn></msup></math>分数，或者[确定系数](https://oreil.ly/2AtV8)，它衡量模型相对于目标值简单平均的表现。
    <math alttext="上限 R 平方 等于 1"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo>
    <mn>1</mn></mrow></math> 表示完美匹配， <math alttext="上限 R 平方 等于 0"><mrow><msup><mi>R</mi>
    <mn>2</mn></msup> <mo>=</mo> <mn>0</mn></mrow></math> 表示模型不比简单取数据均值更好，负值则表示更差的模型。从这两个模型相关的分数中，我们可以得出一个更普遍的观察：
- en: For high-bias models, the performance of the model on the validation set is
    similar to the performance on the training set.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高偏差模型，模型在验证集上的表现与在训练集上的表现类似。
- en: For high-variance models, the performance of the model on the validation set
    is far worse than the performance on the training set.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高方差模型，模型在验证集上的表现远远不及在训练集上的表现。
- en: 'If we imagine that we have some ability to tune the model complexity, we would
    expect the training score and validation score to behave as illustrated in [Figure 39-5](#fig_images_in_0503-validation-curve),
    often called a *validation curve*, and we see the following features:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以调整模型复杂度，我们会期望训练分数和验证分数表现如[图 39-5](#fig_images_in_0503-validation-curve)所示，通常称为*验证曲线*，我们可以看到以下特点：
- en: 'The training score is everywhere higher than the validation score. This is
    generally the case: the model will be a better fit to data it has seen than to
    data it has not seen.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练分数始终高于验证分数。一般情况下都是如此：模型对已见数据的拟合程度比对未见数据的拟合程度更好。
- en: For very low model complexity (a high-bias model), the training data is underfit,
    which means that the model is a poor predictor both for the training data and
    for any previously unseen data.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非常低的模型复杂度（即高偏差模型），训练数据欠拟合，这意味着该模型对于训练数据和任何之前未见数据的预测都很差。
- en: For very high model complexity (a high-variance model), the training data is
    overfit, which means that the model predicts the training data very well, but
    fails for any previously unseen data.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非常高的模型复杂度（即高方差模型），训练数据过拟合，这意味着模型对训练数据的预测非常好，但是对于任何之前未见数据都失败了。
- en: For some intermediate value, the validation curve has a maximum. This level
    of complexity indicates a suitable trade-off between bias and variance.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些中间值，验证曲线达到最大值。这种复杂度水平表明在偏差和方差之间有一个适当的权衡。
- en: The means of tuning the model complexity varies from model to model; when we
    discuss individual models in depth in later chapters, we will see how each model
    allows for such tuning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 调整模型复杂度的方法因模型而异；在后面的章节中深入讨论各个模型时，我们将看到每个模型如何允许此类调整。
- en: '![05.03 validation curve](assets/05.03-validation-curve.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![05.03 验证曲线](assets/05.03-validation-curve.png)'
- en: Figure 39-5\. A schematic of the relationship between model complexity, training
    score, and validation score^([5](ch39.xhtml#idm45858741372496))
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 39-5\. 模型复杂度、训练分数和验证分数之间关系的示意图^([5](ch39.xhtml#idm45858741372496))
- en: Validation Curves in Scikit-Learn
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Scikit-Learn 中的验证曲线](https://oreil.ly/2AtV8)'
- en: 'Let’s look at an example of using cross-validation to compute the validation
    curve for a class of models. Here we will use a *polynomial regression* model,
    a generalized linear model in which the degree of the polynomial is a tunable
    parameter. For example, a degree-1 polynomial fits a straight line to the data;
    for model parameters <math alttext="a"><mi>a</mi></math> and <math alttext="b"><mi>b</mi></math>
    :'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个使用交叉验证计算模型验证曲线的示例。这里我们将使用*多项式回归*模型，一个广义线性模型，其中多项式的次数是一个可调参数。例如，对于模型参数
    <math alttext="a"><mi>a</mi></math> 和 <math alttext="b"><mi>b</mi></math>：
- en: <math alttext="y equals a x plus b" display="block"><mrow><mi>y</mi> <mo>=</mo>
    <mi>a</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals a x plus b" display="block"><mrow><mi>y</mi> <mo>=</mo>
    <mi>a</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
- en: 'A degree-3 polynomial fits a cubic curve to the data; for model parameters
    <math alttext="a comma b comma c comma d"><mrow><mi>a</mi> <mo>,</mo> <mi>b</mi>
    <mo>,</mo> <mi>c</mi> <mo>,</mo> <mi>d</mi></mrow></math> :'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 三阶多项式对数据拟合出一个立方曲线；对于模型参数<math alttext="a comma b comma c comma d"><mrow><mi>a</mi>
    <mo>,</mo> <mi>b</mi> <mo>,</mo> <mi>c</mi> <mo>,</mo> <mi>d</mi></mrow></math>：
- en: <math alttext="y equals a x cubed plus b x squared plus c x plus d" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <mi>a</mi> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo> <mi>b</mi>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mi>c</mi> <mi>x</mi> <mo>+</mo>
    <mi>d</mi></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals a x cubed plus b x squared plus c x plus d" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <mi>a</mi> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo> <mi>b</mi>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mi>c</mi> <mi>x</mi> <mo>+</mo>
    <mi>d</mi></mrow></math>
- en: 'We can generalize this to any number of polynomial features. In Scikit-Learn,
    we can implement this with a linear regression classifier combined with the polynomial
    preprocessor. We will use a *pipeline* to string these operations together (we
    will discuss polynomial features and pipelines more fully in [Chapter 40](ch40.xhtml#section-0504-feature-engineering)):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这一概念推广到任意数量的多项式特征。在Scikit-Learn中，我们可以使用线性回归分类器结合多项式预处理器实现这一点。我们将使用*管道*将这些操作串联在一起（我们将在[第 40
    章](ch40.xhtml#section-0504-feature-engineering)中更全面地讨论多项式特征和管道）：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now let’s create some data to which we will fit our model:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一些数据来拟合我们的模型：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can now visualize our data, along with polynomial fits of several degrees
    (see [Figure 39-6](#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以可视化我们的数据，以及几个不同阶数的多项式拟合（见[图 39-6](#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0)）。
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The knob controlling model complexity in this case is the degree of the polynomial,
    which can be any nonnegative integer. A useful question to answer is this: what
    degree of polynomial provides a suitable trade-off between bias (underfitting)
    and variance (overfitting)?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下控制模型复杂度的旋钮是多项式的阶数，它可以是任何非负整数。一个有用的问题是：哪个多项式阶数提供了偏差（欠拟合）和方差（过拟合）之间的合适权衡点？
- en: '![output 33 0](assets/output_33_0.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![output 33 0](assets/output_33_0.png)'
- en: Figure 39-6\. Three different polynomial models fit to a dataset^([6](ch39.xhtml#idm45858741065984))
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 39-6\. 适合数据集的三个不同多项式模型^([6](ch39.xhtml#idm45858741065984))
- en: We can make progress in this by visualizing the validation curve for this particular
    data and model; this can be done straightforwardly using the `validation_curve`
    convenience routine provided by Scikit-Learn. Given a model, data, parameter name,
    and a range to explore, this function will automatically compute both the training
    score and the validation score across the range (see [Figure 39-7](#fig_0503-hyperparameters-and-model-validation_files_in_output_35_0)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过可视化特定数据和模型的验证曲线来取得进展；这可以通过Scikit-Learn提供的`validation_curve`便捷程序直接完成。给定一个模型、数据、参数名称和探索范围，该函数将自动计算跨范围的训练分数和验证分数（见[图 39-7](#fig_0503-hyperparameters-and-model-validation_files_in_output_35_0)）。
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 35 0](assets/output_35_0.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![output 35 0](assets/output_35_0.png)'
- en: Figure 39-7\. The validation curves for the data in [Figure 39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 39-7\. [Figure 39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)中数据的验证曲线
- en: 'This shows precisely the qualitative behavior we expect: the training score
    is everywhere higher than the validation score, the training score is monotonically
    improving with increased model complexity, and the validation score reaches a
    maximum before dropping off as the model becomes overfit.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地展示了我们预期的定性行为：训练分数始终高于验证分数，训练分数随着模型复杂度的增加而单调改善，并且验证分数在模型过拟合后达到最大值然后下降。
- en: From the validation curve, we can determine that the optimal trade-off between
    bias and variance is found for a third-order polynomial. We can compute and display
    this fit over the original data as follows (see [Figure 39-8](#fig_0503-hyperparameters-and-model-validation_files_in_output_37_0)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从验证曲线中，我们可以确定在三阶多项式下找到了偏差和方差之间的最佳权衡点。我们可以按以下方式计算并展示这个拟合结果在原始数据上的表现（见[图 39-8](#fig_0503-hyperparameters-and-model-validation_files_in_output_37_0)）。
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![output 37 0](assets/output_37_0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![output 37 0](assets/output_37_0.png)'
- en: Figure 39-8\. The cross-validated optimal model for the data in [Figure 39-6](#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0)
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 39-8\. [Figure 39-6](#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0)中数据的交叉验证最优模型
- en: Notice that finding this optimal model did not actually require us to compute
    the training score, but examining the relationship between the training score
    and validation score can give us useful insight into the performance of the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，找到这个最优模型实际上并不需要我们计算训练分数，但是检查训练分数和验证分数之间的关系可以为我们提供模型性能的有用见解。
- en: Learning Curves
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: One important aspect of model complexity is that the optimal model will generally
    depend on the size of your training data. For example, let’s generate a new dataset
    with five times as many points (see [Figure 39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂度的一个重要方面是，最优模型通常取决于训练数据的大小。例如，让我们生成一个数据集，其点数是之前的五倍（见[图39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)）。
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![output 40 0](assets/output_40_0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![output 40 0](assets/output_40_0.png)'
- en: Figure 39-9\. Data to demonstrate learning curves
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图39-9\. 展示学习曲线的数据
- en: Now let’s duplicate the preceding code to plot the validation curve for this
    larger dataset; for reference, we’ll overplot the previous results as well (see
    [Figure 39-10](#fig_0503-hyperparameters-and-model-validation_files_in_output_42_0)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们复制前述代码，为这个更大的数据集绘制验证曲线；为了参考，我们也会在前面的结果上进行叠加（见[图39-10](#fig_0503-hyperparameters-and-model-validation_files_in_output_42_0)）。
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The solid lines show the new results, while the fainter dashed lines show the
    results on the previous smaller dataset. It is clear from the validation curve
    that the larger dataset can support a much more complicated model: the peak here
    is probably around a degree of 6, but even a degree-20 model isn’t seriously overfitting
    the data—the validation and training scores remain very close.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 实线显示新结果，而较淡的虚线显示较小数据集的结果。从验证曲线可以明显看出，较大的数据集可以支持更复杂的模型：这里的高峰可能在6阶左右，但即使是20阶模型也不会严重过拟合数据——验证和训练分数仍然非常接近。
- en: '![output 42 0](assets/output_42_0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![output 42 0](assets/output_42_0.png)'
- en: Figure 39-10\. Learning curves for the polynomial model fit to data in [Figure 39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)^([7](ch39.xhtml#idm45858740554592))
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图39-10\. 多项式模型拟合数据的学习曲线
- en: 'So, the behavior of the validation curve has not one but two important inputs:
    the model complexity and the number of training points. We can gain further insight
    by exploring the behavior of the model as a function of the number of training
    points, which we can do by using increasingly larger subsets of the data to fit
    our model. A plot of the training/validation score with respect to the size of
    the training set is sometimes known as a *learning curve*.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，验证曲线的行为不仅仅取决于模型复杂度，还取决于训练点的数量。我们可以通过使用日益增大的数据子集来研究模型随训练点数量变化的行为，从而获得更深入的见解。有时，关于训练/验证分数与训练集大小的图称为*学习曲线*。
- en: 'The general behavior we would expect from a learning curve is this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望从学习曲线得到的一般行为是：
- en: 'A model of a given complexity will *overfit* a small dataset: this means the
    training score will be relatively high, while the validation score will be relatively
    low.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定复杂度的模型会*过拟合*小数据集：这意味着训练分数会相对较高，而验证分数则相对较低。
- en: 'A model of a given complexity will *underfit* a large dataset: this means that
    the training score will decrease, but the validation score will increase.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定复杂度的模型会*欠拟合*大数据集：这意味着训练分数会减少，但验证分数会增加。
- en: 'A model will never, except by chance, give a better score to the validation
    set than the training set: this means the curves should keep getting closer together
    but never cross.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了偶然情况外，模型永远不会给验证集比训练集更好的分数：这意味着曲线应该会越来越接近，但永远不会交叉。
- en: With these features in mind, we would expect a learning curve to look qualitatively
    like that shown in [Figure 39-11](#fig_images_in_0503-learning-curve).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些特征，我们期望学习曲线在质量上看起来像[图39-11](#fig_images_in_0503-learning-curve)所示。
- en: '![05.03 learning curve](assets/05.03-learning-curve.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![05.03 learning curve](assets/05.03-learning-curve.png)'
- en: Figure 39-11\. Schematic showing the typical interpretation of learning curves^([8](ch39.xhtml#idm45858740543504))
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图39-11\. 示意图展示学习曲线的典型解释
- en: The notable feature of the learning curve is the convergence to a particular
    score as the number of training samples grows. In particular, once you have enough
    points that a particular model has converged, *adding more training data will
    not help you!* The only way to increase model performance in this case is to use
    another (often more complex) model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线的显著特征是随着训练样本数量的增长而趋于特定分数。特别是，一旦您有足够的点使得特定模型收敛，*增加更多的训练数据将不会帮助您*！在这种情况下，提高模型性能的唯一方法是使用另一个（通常更复杂的）模型。
- en: Scikit-Learn offers a convenient utility for computing such learning curves
    from your models; here we will compute a learning curve for our original dataset
    with a second-order polynomial model and a ninth-order polynomial (see [Figure 39-12](#fig_0503-hyperparameters-and-model-validation_files_in_output_47_0)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![output 47 0](assets/output_47_0.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: Figure 39-12\. Learning curves for a low-complexity model (left) and a high-complexity
    model (right)^([9](ch39.xhtml#idm45858740396064))
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a valuable diagnostic, because it gives us a visual depiction of how
    our model responds to increasing amounts of training data. In particular, when
    the learning curve has already converged (i.e., when the training and validation
    curves are already close to each other) *adding more training data will not significantly
    improve the fit!* This situation is seen in the left panel, with the learning
    curve for the degree-2 model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The only way to increase the converged score is to use a different (usually
    more complicated) model. We see this in the right panel: by moving to a much more
    complicated model, we increase the score of convergence (indicated by the dashed
    line), but at the expense of higher model variance (indicated by the difference
    between the training and validation scores). If we were to add even more data
    points, the learning curve for the more complicated model would eventually converge.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Plotting a learning curve for your particular choice of model and dataset can
    help you to make this type of decision about how to move forward in improving
    your analysis.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The solid lines show the new results, while the fainter dashed lines show the
    results on the previous smaller dataset. It is clear from the validation curve
    that the larger dataset can support a much more complicated model: the peak here
    is probably around a degree of 6, but even a degree-20 model isn’t seriously overfitting
    the data—the validation and training scores remain very close.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Validation in Practice: Grid Search'
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding discussion is meant to give you some intuition into the trade-off
    between bias and variance, and its dependence on model complexity and training
    set size. In practice, models generally have more than one knob to turn, meaning
    plots of validation and learning curves change from lines to multidimensional
    surfaces. In these cases, such visualizations are difficult, and we would rather
    simply find the particular model that maximizes the validation score.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides some tools to make this kind of search more convenient:
    here we’ll consider the use of grid search to find the optimal polynomial model.
    We will explore a two-dimensional grid of model features, namely the polynomial
    degree and the flag telling us whether to fit the intercept. This can be set up
    using Scikit-Learn’s `GridSearchCV` meta-estimator:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Notice that like a normal estimator, this has not yet been applied to any data.
    Calling the `fit` method will fit the model at each grid point, keeping track
    of the scores along the way:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that the model is fit, we can ask for the best parameters as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Finally, if we wish, we can use the best model and show the fit to our data
    using code from before (see [Figure 39-13](#fig_0503-hyperparameters-and-model-validation_files_in_output_56_0)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![output 56 0](assets/output_56_0.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: Figure 39-13\. The best-fit model determined via an automatic grid search
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other options in `GridSearchCV` include the ability to specify a custom scoring
    function, to parallelize the computations, to do randomized searches, and more.
    For more information, see the examples in Chapters [49](ch49.xhtml#section-0513-kernel-density-estimation)
    and [50](ch50.xhtml#section-0514-image-features), or refer to Scikit-Learn’s [grid
    search documentation](https://oreil.ly/xft8j).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we began to explore the concept of model validation and hyperparameter
    optimization, focusing on intuitive aspects of the bias–variance trade-off and
    how it comes into play when fitting models to data. In particular, we found that
    the use of a validation set or cross-validation approach is vital when tuning
    parameters in order to avoid overfitting for more complex/flexible models.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In later chapters, we will discuss the details of particularly useful models,
    what tuning is available for these models, and how these free parameters affect
    model complexity. Keep the lessons of this chapter in mind as you read on and
    learn about these machine learning approaches!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch39.xhtml#idm45858742132032-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/jv0wb).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch39.xhtml#idm45858742031984-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/2BP2o).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch39.xhtml#idm45858741405920-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/j9G96).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch39.xhtml#idm45858741397728-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/YfwRC).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch39.xhtml#idm45858741372496-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/4AK15).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch39.xhtml#idm45858741065984-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch39.xhtml#idm45858740554592-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch39.xhtml#idm45858740543504-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/omZ1c).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch39.xhtml#idm45858740396064-marker)) A full-size version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
