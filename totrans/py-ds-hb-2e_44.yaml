- en: Chapter 39\. Hyperparameters and Model Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we saw the basic recipe for applying a supervised
    machine learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a class of model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose model hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model to the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the model to predict labels for new data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first two pieces of this—the choice of model and choice of hyperparameters—are
    perhaps the most important part of using these tools and techniques effectively.
    In order to make informed choices, we need a way to *validate* that our model
    and our hyperparameters are a good fit to the data. While this may sound simple,
    there are some pitfalls that you must avoid to do this effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking About Model Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In principle, model validation is very simple: after choosing a model and its
    hyperparameters, we can estimate how effective it is by applying it to some of
    the training data and comparing the predictions to the known values.'
  prefs: []
  type: TYPE_NORMAL
- en: This section will first show a naive approach to model validation and why it
    fails, before exploring the use of holdout sets and cross-validation for more
    robust model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Model Validation the Wrong Way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with the naive approach to validation using the Iris dataset, which
    we saw in the previous chapter. We will start by loading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we choose a model and hyperparameters. Here we’ll use a *k*-nearest neighbors
    classifier with `n_neighbors=1`. This is a very simple and intuitive model that
    says “the label of an unknown point is the same as the label of its closest training
    point”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we train the model, and use it to predict labels for data whose labels
    we already know:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we compute the fraction of correctly labeled points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We see an accuracy score of 1.0, which indicates that 100% of points were correctly
    labeled by our model! But is this truly measuring the expected accuracy? Have
    we really come upon a model that we expect to be correct 100% of the time?
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may have gathered, the answer is no. In fact, this approach contains
    a fundamental flaw: *it trains and evaluates the model on the same data*. Furthermore,
    this nearest neighbor model is an *instance-based* estimator that simply stores
    the training data, and predicts labels by comparing new data to these stored points:
    except in contrived cases, it will get 100% accuracy every time!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Validation the Right Way: Holdout Sets'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So what can be done? A better sense of a model’s performance can be found by
    using what’s known as a *holdout set*: that is, we hold back some subset of the
    data from the training of the model, and then use this holdout set to check the
    model’s performance. This splitting can be done using the `train_test_split` utility
    in Scikit-Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We see here a more reasonable result: the one-nearest-neighbor classifier is
    about 90% accurate on this holdout set. The holdout set is similar to unknown
    data, because the model has not “seen” it before.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Validation via Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One disadvantage of using a holdout set for model validation is that we have
    lost a portion of our data to the model training. In the preceding case, half
    the dataset does not contribute to the training of the model! This is not optimal,
    especially if the initial set of training data is small.
  prefs: []
  type: TYPE_NORMAL
- en: One way to address this is to use *cross-validation*; that is, to do a sequence
    of fits where each subset of the data is used both as a training set and as a
    validation set. Visually, it might look something like [Figure 39-1](#fig_images_in_0503-2-fold-cv).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.03 2 fold CV](assets/05.03-2-fold-CV.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-1\. Visualization of two-fold cross-validation^([1](ch39.xhtml#idm45858742132032))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here we do two validation trials, alternately using each half of the data as
    a holdout set. Using the split data from earlier, we could implement it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: What comes out are two accuracy scores, which we could combine (by, say, taking
    the mean) to get a better measure of the global model performance. This particular
    form of cross-validation is a *two-fold cross-validation*—that is, one in which
    we have split the data into two sets and used each in turn as a validation set.
  prefs: []
  type: TYPE_NORMAL
- en: We could expand on this idea to use even more trials, and more folds in the
    data—for example, [Figure 39-2](#fig_images_in_0503-5-fold-cv) shows a visual
    depiction of five-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.03 5 fold CV](assets/05.03-5-fold-CV.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-2\. Visualization of five-fold cross-validation^([2](ch39.xhtml#idm45858742031984))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here we split the data into five groups and use each in turn to evaluate the
    model fit on the other four-fifths of the data. This would be rather tedious to
    do by hand, but we can use Scikit-Learn’s `cross_val_score` convenience routine
    to do it succinctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Repeating the validation across different subsets of the data gives us an even
    better idea of the performance of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn implements a number of cross-validation schemes that are useful
    in particular situations; these are implemented via iterators in the `model_selection`
    module. For example, we might wish to go to the extreme case in which our number
    of folds is equal to the number of data points: that is, we train on all points
    but one in each trial. This type of cross-validation is known as *leave-one-out*
    cross validation, and can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we have 150 samples, the leave-one-out cross-validation yields scores
    for 150 trials, and each score indicates either a successful (1.0) or an unsuccessful
    (0.0) prediction. Taking the mean of these gives an estimate of the error rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Other cross-validation schemes can be used similarly. For a description of what
    is available in Scikit-Learn, use IPython to explore the `sklearn.model_selection`
    submodule, or take a look at Scikit-Learn’s [cross-validation documentation](https://oreil.ly/rITkn).
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Best Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve explored the basics of validation and cross-validation, we will
    go into a little more depth regarding model selection and selection of hyperparameters.
    These issues are some of the most important aspects of the practice of machine
    learning, but I find that this information is often glossed over in introductory
    machine learning tutorials.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of core importance is the following question: *if our estimator is underperforming,
    how should we move forward?* There are several possible answers:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a more complicated/more flexible model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a less complicated/less flexible model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gather more training samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gather more data to add features to each sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The answer to this question is often counterintuitive. In particular, sometimes
    using a more complicated model will give worse results, and adding more training
    samples may not improve your results! The ability to determine what steps will
    improve your model is what separates the successful machine learning practitioners
    from the unsuccessful.
  prefs: []
  type: TYPE_NORMAL
- en: The Bias-Variance Trade-off
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fundamentally, finding “the best model” is about finding a sweet spot in the
    trade-off between *bias* and *variance*. Consider [Figure 39-3](#fig_images_in_0503-bias-variance),
    which presents two regression fits to the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.03 bias variance](assets/05.03-bias-variance.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-3\. High-bias and high-variance regression models^([3](ch39.xhtml#idm45858741405920))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is clear that neither of these models is a particularly good fit to the data,
    but they fail in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model on the left attempts to find a straight-line fit through the data.
    Because in this case a straight line cannot accurately split the data, the straight-line
    model will never be able to describe this dataset well. Such a model is said to
    *underfit* the data: that is, it does not have enough flexibility to suitably
    account for all the features in the data. Another way of saying this is that the
    model has high bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model on the right attempts to fit a high-order polynomial through the
    data. Here the model fit has enough flexibility to nearly perfectly account for
    the fine features in the data, but even though it very accurately describes the
    training data, its precise form seems to be more reflective of the particular
    noise properties of the data than of the intrinsic properties of whatever process
    generated that data. Such a model is said to *overfit* the data: that is, it has
    so much flexibility that the model ends up accounting for random errors as well
    as the underlying data distribution. Another way of saying this is that the model
    has high variance.'
  prefs: []
  type: TYPE_NORMAL
- en: To look at this in another light, consider what happens if we use these two
    models to predict the *y*-values for some new data. In the plots in [Figure 39-4](#fig_images_in_0503-bias-variance-2),
    the red/lighter points indicate data that is omitted from the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.03 bias variance 2](assets/05.03-bias-variance-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-4\. Training and validation scores in high-bias and high-variance
    models^([4](ch39.xhtml#idm45858741397728))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The score here is the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    score, or [coefficient of determination](https://oreil.ly/2AtV8), which measures
    how well a model performs relative to a simple mean of the target values. <math
    alttext="upper R squared equals 1"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo>
    <mn>1</mn></mrow></math> indicates a perfect match, <math alttext="upper R squared
    equals 0"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>0</mn></mrow></math>
    indicates the model does no better than simply taking the mean of the data, and
    negative values mean even worse models. From the scores associated with these
    two models, we can make an observation that holds more generally:'
  prefs: []
  type: TYPE_NORMAL
- en: For high-bias models, the performance of the model on the validation set is
    similar to the performance on the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For high-variance models, the performance of the model on the validation set
    is far worse than the performance on the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we imagine that we have some ability to tune the model complexity, we would
    expect the training score and validation score to behave as illustrated in [Figure 39-5](#fig_images_in_0503-validation-curve),
    often called a *validation curve*, and we see the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training score is everywhere higher than the validation score. This is
    generally the case: the model will be a better fit to data it has seen than to
    data it has not seen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very low model complexity (a high-bias model), the training data is underfit,
    which means that the model is a poor predictor both for the training data and
    for any previously unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very high model complexity (a high-variance model), the training data is
    overfit, which means that the model predicts the training data very well, but
    fails for any previously unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For some intermediate value, the validation curve has a maximum. This level
    of complexity indicates a suitable trade-off between bias and variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The means of tuning the model complexity varies from model to model; when we
    discuss individual models in depth in later chapters, we will see how each model
    allows for such tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.03 validation curve](assets/05.03-validation-curve.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-5\. A schematic of the relationship between model complexity, training
    score, and validation score^([5](ch39.xhtml#idm45858741372496))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Validation Curves in Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at an example of using cross-validation to compute the validation
    curve for a class of models. Here we will use a *polynomial regression* model,
    a generalized linear model in which the degree of the polynomial is a tunable
    parameter. For example, a degree-1 polynomial fits a straight line to the data;
    for model parameters <math alttext="a"><mi>a</mi></math> and <math alttext="b"><mi>b</mi></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals a x plus b" display="block"><mrow><mi>y</mi> <mo>=</mo>
    <mi>a</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'A degree-3 polynomial fits a cubic curve to the data; for model parameters
    <math alttext="a comma b comma c comma d"><mrow><mi>a</mi> <mo>,</mo> <mi>b</mi>
    <mo>,</mo> <mi>c</mi> <mo>,</mo> <mi>d</mi></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals a x cubed plus b x squared plus c x plus d" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <mi>a</mi> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo> <mi>b</mi>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mi>c</mi> <mi>x</mi> <mo>+</mo>
    <mi>d</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generalize this to any number of polynomial features. In Scikit-Learn,
    we can implement this with a linear regression classifier combined with the polynomial
    preprocessor. We will use a *pipeline* to string these operations together (we
    will discuss polynomial features and pipelines more fully in [Chapter 40](ch40.xhtml#section-0504-feature-engineering)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s create some data to which we will fit our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can now visualize our data, along with polynomial fits of several degrees
    (see [Figure 39-6](#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The knob controlling model complexity in this case is the degree of the polynomial,
    which can be any nonnegative integer. A useful question to answer is this: what
    degree of polynomial provides a suitable trade-off between bias (underfitting)
    and variance (overfitting)?'
  prefs: []
  type: TYPE_NORMAL
- en: '![output 33 0](assets/output_33_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-6\. Three different polynomial models fit to a dataset^([6](ch39.xhtml#idm45858741065984))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can make progress in this by visualizing the validation curve for this particular
    data and model; this can be done straightforwardly using the `validation_curve`
    convenience routine provided by Scikit-Learn. Given a model, data, parameter name,
    and a range to explore, this function will automatically compute both the training
    score and the validation score across the range (see [Figure 39-7](#fig_0503-hyperparameters-and-model-validation_files_in_output_35_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 35 0](assets/output_35_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-7\. The validation curves for the data in [Figure 39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This shows precisely the qualitative behavior we expect: the training score
    is everywhere higher than the validation score, the training score is monotonically
    improving with increased model complexity, and the validation score reaches a
    maximum before dropping off as the model becomes overfit.'
  prefs: []
  type: TYPE_NORMAL
- en: From the validation curve, we can determine that the optimal trade-off between
    bias and variance is found for a third-order polynomial. We can compute and display
    this fit over the original data as follows (see [Figure 39-8](#fig_0503-hyperparameters-and-model-validation_files_in_output_37_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![output 37 0](assets/output_37_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-8\. The cross-validated optimal model for the data in [Figure 39-6](#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that finding this optimal model did not actually require us to compute
    the training score, but examining the relationship between the training score
    and validation score can give us useful insight into the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One important aspect of model complexity is that the optimal model will generally
    depend on the size of your training data. For example, let’s generate a new dataset
    with five times as many points (see [Figure 39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![output 40 0](assets/output_40_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-9\. Data to demonstrate learning curves
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now let’s duplicate the preceding code to plot the validation curve for this
    larger dataset; for reference, we’ll overplot the previous results as well (see
    [Figure 39-10](#fig_0503-hyperparameters-and-model-validation_files_in_output_42_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The solid lines show the new results, while the fainter dashed lines show the
    results on the previous smaller dataset. It is clear from the validation curve
    that the larger dataset can support a much more complicated model: the peak here
    is probably around a degree of 6, but even a degree-20 model isn’t seriously overfitting
    the data—the validation and training scores remain very close.'
  prefs: []
  type: TYPE_NORMAL
- en: '![output 42 0](assets/output_42_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-10\. Learning curves for the polynomial model fit to data in [Figure 39-9](#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0)^([7](ch39.xhtml#idm45858740554592))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, the behavior of the validation curve has not one but two important inputs:
    the model complexity and the number of training points. We can gain further insight
    by exploring the behavior of the model as a function of the number of training
    points, which we can do by using increasingly larger subsets of the data to fit
    our model. A plot of the training/validation score with respect to the size of
    the training set is sometimes known as a *learning curve*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The general behavior we would expect from a learning curve is this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A model of a given complexity will *overfit* a small dataset: this means the
    training score will be relatively high, while the validation score will be relatively
    low.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A model of a given complexity will *underfit* a large dataset: this means that
    the training score will decrease, but the validation score will increase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A model will never, except by chance, give a better score to the validation
    set than the training set: this means the curves should keep getting closer together
    but never cross.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these features in mind, we would expect a learning curve to look qualitatively
    like that shown in [Figure 39-11](#fig_images_in_0503-learning-curve).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.03 learning curve](assets/05.03-learning-curve.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-11\. Schematic showing the typical interpretation of learning curves^([8](ch39.xhtml#idm45858740543504))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The notable feature of the learning curve is the convergence to a particular
    score as the number of training samples grows. In particular, once you have enough
    points that a particular model has converged, *adding more training data will
    not help you!* The only way to increase model performance in this case is to use
    another (often more complex) model.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn offers a convenient utility for computing such learning curves
    from your models; here we will compute a learning curve for our original dataset
    with a second-order polynomial model and a ninth-order polynomial (see [Figure 39-12](#fig_0503-hyperparameters-and-model-validation_files_in_output_47_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![output 47 0](assets/output_47_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-12\. Learning curves for a low-complexity model (left) and a high-complexity
    model (right)^([9](ch39.xhtml#idm45858740396064))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is a valuable diagnostic, because it gives us a visual depiction of how
    our model responds to increasing amounts of training data. In particular, when
    the learning curve has already converged (i.e., when the training and validation
    curves are already close to each other) *adding more training data will not significantly
    improve the fit!* This situation is seen in the left panel, with the learning
    curve for the degree-2 model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The only way to increase the converged score is to use a different (usually
    more complicated) model. We see this in the right panel: by moving to a much more
    complicated model, we increase the score of convergence (indicated by the dashed
    line), but at the expense of higher model variance (indicated by the difference
    between the training and validation scores). If we were to add even more data
    points, the learning curve for the more complicated model would eventually converge.'
  prefs: []
  type: TYPE_NORMAL
- en: Plotting a learning curve for your particular choice of model and dataset can
    help you to make this type of decision about how to move forward in improving
    your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solid lines show the new results, while the fainter dashed lines show the
    results on the previous smaller dataset. It is clear from the validation curve
    that the larger dataset can support a much more complicated model: the peak here
    is probably around a degree of 6, but even a degree-20 model isn’t seriously overfitting
    the data—the validation and training scores remain very close.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validation in Practice: Grid Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding discussion is meant to give you some intuition into the trade-off
    between bias and variance, and its dependence on model complexity and training
    set size. In practice, models generally have more than one knob to turn, meaning
    plots of validation and learning curves change from lines to multidimensional
    surfaces. In these cases, such visualizations are difficult, and we would rather
    simply find the particular model that maximizes the validation score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides some tools to make this kind of search more convenient:
    here we’ll consider the use of grid search to find the optimal polynomial model.
    We will explore a two-dimensional grid of model features, namely the polynomial
    degree and the flag telling us whether to fit the intercept. This can be set up
    using Scikit-Learn’s `GridSearchCV` meta-estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that like a normal estimator, this has not yet been applied to any data.
    Calling the `fit` method will fit the model at each grid point, keeping track
    of the scores along the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the model is fit, we can ask for the best parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Finally, if we wish, we can use the best model and show the fit to our data
    using code from before (see [Figure 39-13](#fig_0503-hyperparameters-and-model-validation_files_in_output_56_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![output 56 0](assets/output_56_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 39-13\. The best-fit model determined via an automatic grid search
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other options in `GridSearchCV` include the ability to specify a custom scoring
    function, to parallelize the computations, to do randomized searches, and more.
    For more information, see the examples in Chapters [49](ch49.xhtml#section-0513-kernel-density-estimation)
    and [50](ch50.xhtml#section-0514-image-features), or refer to Scikit-Learn’s [grid
    search documentation](https://oreil.ly/xft8j).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we began to explore the concept of model validation and hyperparameter
    optimization, focusing on intuitive aspects of the bias–variance trade-off and
    how it comes into play when fitting models to data. In particular, we found that
    the use of a validation set or cross-validation approach is vital when tuning
    parameters in order to avoid overfitting for more complex/flexible models.
  prefs: []
  type: TYPE_NORMAL
- en: In later chapters, we will discuss the details of particularly useful models,
    what tuning is available for these models, and how these free parameters affect
    model complexity. Keep the lessons of this chapter in mind as you read on and
    learn about these machine learning approaches!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch39.xhtml#idm45858742132032-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/jv0wb).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch39.xhtml#idm45858742031984-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/2BP2o).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch39.xhtml#idm45858741405920-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/j9G96).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch39.xhtml#idm45858741397728-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/YfwRC).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch39.xhtml#idm45858741372496-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/4AK15).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch39.xhtml#idm45858741065984-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch39.xhtml#idm45858740554592-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch39.xhtml#idm45858740543504-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/omZ1c).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch39.xhtml#idm45858740396064-marker)) A full-size version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
