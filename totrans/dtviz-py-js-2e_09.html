<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 6. Heavyweight Scraping with Scrapy" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_heavy_scraping">
<h1><span class="label">Chapter 6. </span>Heavyweight Scraping with Scrapy</h1>
<p>As your scraping goals get more ambitious, hacking solutions with Beautiful Soup and requests can get very messy very fast. <a data-primary="Scrapy" data-type="indexterm" id="ix_Scrpy"/>Managing the scraped data as requests spawn more requests gets tricky, and if your requests are being made synchronously, things start to slow down rapidly.  A whole load of problems you probably hadn’t anticipated start to make themselves known. It’s at this point that you want to turn to a powerful, robust library that solves all these problems and more. And that’s where Scrapy comes in.<a data-primary="scraping data" data-secondary="heavyweight scraping with Scrapy" data-see="Scrapy" data-type="indexterm" id="idm45607786229504"/><a data-primary="Scrapy" data-secondary="about" data-type="indexterm" id="idm45607786228320"/></p>
<p>Where Beautiful Soup is a very handy little penknife for fast and dirty scraping, Scrapy is a  Python library that can do large-scale data scrapes with ease. It has all the things you’d expect, like built-in caching (with expiration times), asynchronous requests via Python’s Twisted web framework, user-agent randomization, and a whole lot more. The price for all this power is a fairly steep learning curve, which this chapter is intended to smooth, using a simple example.  I think Scrapy is a powerful addition to any dataviz toolkit and really opens up possibilities for web data <span class="keep-together">collection</span>.</p>
<p>In <a data-type="xref" href="ch05.xhtml#get_data_scraping">“Scraping Data”</a>, we managed to scrape a dataset containing all the Nobel Prize winners by name, year, and category. We did a speculative scrape of the winners’ linked biography pages, which showed that extracting the country of nationality was going to be difficult. In this chapter, we’ll set the bar on our Nobel Prize data a bit higher and aim to scrape objects of the form shown in <a data-type="xref" href="#scrapy_target_JSON">Example 6-1</a>.</p>
<div class="pagebreak-before less_space" data-type="example" id="scrapy_target_JSON">
<h5><span class="label">Example 6-1. </span>Our targeted Nobel JSON object</h5>
<pre data-code-language="python" data-type="programlisting"><code class="p">{</code>
  <code class="s2">"category"</code><code class="p">:</code> <code class="s2">"Physiology or Medicine"</code><code class="p">,</code>
  <code class="s2">"country"</code><code class="p">:</code> <code class="s2">"Argentina"</code><code class="p">,</code>
  <code class="s2">"date_of_birth': "</code><code class="mi">8</code> <code class="n">October</code> <code class="mi">1927</code><code class="s2">",</code>
  <code class="s2">"date_of_death': "</code><code class="mi">24</code> <code class="n">March</code> <code class="mi">2002</code><code class="s2">",</code>
  <code class="s2">"gender"</code><code class="p">:</code> <code class="s2">"male"</code><code class="p">,</code>
  <code class="s2">"link"</code><code class="p">:</code> <code class="s2">"http:\/\/en.wikipedia.org\/wiki\/C%C3%A9sar_Milstein"</code><code class="p">,</code>
  <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"C</code><code class="se">\u00e9</code><code class="s2">sar Milstein"</code><code class="p">,</code>
  <code class="s2">"place_of_birth"</code><code class="p">:</code> <code class="s2">"Bah</code><code class="se">\u00ed</code><code class="s2">a Blanca ,  Argentina"</code><code class="p">,</code>
  <code class="s2">"place_of_death"</code><code class="p">:</code> <code class="s2">"Cambridge , England"</code><code class="p">,</code>
  <code class="s2">"text"</code><code class="p">:</code> <code class="s2">"C</code><code class="se">\u00e9</code><code class="s2">sar Milstein , Physiology or Medicine, 1984"</code><code class="p">,</code>
  <code class="s2">"year"</code><code class="p">:</code> <code class="mi">1984</code>
<code class="p">}</code></pre></div>
<p>In addition to this data, we’ll aim to scrape prizewinners’ photos (where applicable) and some potted biographical data (see <a data-type="xref" href="#scrapy_targets">Figure 6-1</a>). We’ll be using the photos and body text to add a little character to our Nobel Prize visualization.</p>
<figure><div class="figure" id="scrapy_targets">
<img alt="dpj2 0601" height="710" src="assets/dpj2_0601.png" width="995"/>
<h6><span class="label">Figure 6-1. </span>Scraping targets for the prizewinners’ pages</h6>
</div></figure>
<section data-pdf-bookmark="Setting Up Scrapy" data-type="sect1"><div class="sect1" id="idm45607786172528">
<h1>Setting Up Scrapy</h1>
<p>Scrapy should be one of the Anaconda packages (see <a data-type="xref" href="ch01.xhtml#chapter_install">Chapter 1</a>), so you should already have it on hand. <a data-primary="Scrapy" data-secondary="setting up" data-type="indexterm" id="idm45607786169904"/>If that’s not the case, then you can install it with the following <code>conda</code> command line:</p>
<pre data-type="programlisting">$ conda install -c https://conda.anaconda.org/anaconda scrapy</pre>
<p>If you’re not using Anaconda, a quick <code>pip</code> install will do the job:<sup><a data-type="noteref" href="ch06.xhtml#idm45607786166832" id="idm45607786166832-marker">1</a></sup></p>
<pre data-type="programlisting">$ pip install scrapy</pre>
<p>With Scrapy installed, you should have access to the <code>scrapy</code> command. Unlike the vast majority of Python libraries, Scrapy is designed to be driven from the command line within the context of a scraping project, defined by configuration files, scraping spiders, pipelines, and so on.<a data-primary="projects" data-secondary="starting using Scrapy" data-type="indexterm" id="idm45607786121024"/> Let’s generate a fresh project for our Nobel Prize scraping, using the <code>startproject</code> option. This is going to generate a project folder, so make sure you run it from a suitable work <span class="keep-together">directory</span>:</p>
<pre data-code-language="bash" data-type="programlisting">$ scrapy startproject nobel_winners
New Scrapy project <code class="s1">'nobel_winners'</code> created <code class="k">in</code>:
    /home/kyran/workspace/.../scrapy/nobel_winners

You can start your first spider with:
    <code class="nb">cd</code> nobel_winners
    scrapy genspider example example.com</pre>
<p>As the output of <code>startproject</code> says, you’ll want to switch to the <em>nobel_winners</em> directory in order to start driving Scrapy.</p>
<p>Let’s take a look at the project’s directory tree:</p>
<pre data-code-language="bash" data-type="programlisting">nobel_winners
├── nobel_winners
│   ├── __init__.py
│   ├── items.py
│   ├── middlewares.py
│   ├── pipelines.py
│   ├── settings.py
│   └── spiders
│       └── __init__.py
└── scrapy.cfg</pre>
<p>As shown, the project directory has a subdirectory with the same name and a config file <em>scrapy.cfg</em>. The <em>nobel_winners</em> subdirectory is a Python module (containing an <em>__init__.py</em> file) with a few skeleton files and a <em>spiders</em> directory, which will contain your scrapers.</p>
</div></section>
<section data-pdf-bookmark="Establishing the Targets" data-type="sect1"><div class="sect1" id="idm45607786171904">
<h1>Establishing the Targets</h1>
<p>In <a data-type="xref" href="ch05.xhtml#get_data_scraping">“Scraping Data”</a>, we tried to scrape the Nobel winners’ nationalities from their biography pages but found they were missing or inconsistently labeled in many cases (see <a data-type="xref" href="ch05.xhtml#chapter_getting_data">Chapter 5</a>). <a data-primary="Scrapy" data-secondary="establishing targets" data-type="indexterm" id="ix_Scrpytrgt"/>Rather than get the country data indirectly, a little  Wikipedia searching shows a way through. There is a <a href="https://oreil.ly/p6pXm">page</a> that lists winners by country. The winners are presented in titled, ordered lists (see <a data-type="xref" href="#scrapy_wiki_list">Figure 6-2</a>), not in tabular form, which makes recovering our basic name, category, and year data a little harder. Also the data organization is not ideal (e.g., the country header titles and winner lists aren’t in useful, separate blocks). As we’ll see, a few well-structured Scrapy queries will easily net us the data we need.</p>
<p><a data-type="xref" href="#scrapy_wiki_list">Figure 6-2</a> shows the starting page for our first spider along with the key elements it will be targeting. A list of country name titles (A) is followed by an ordered list (B) of their Nobel Prize–winning <span class="keep-together">citizens</span>.</p>
<figure><div class="figure" id="scrapy_wiki_list">
<img alt="dpj2 0602" height="961" src="assets/dpj2_0602.png" width="1440"/>
<h6><span class="label">Figure 6-2. </span>Scraping Wikipedia’s Nobel Prizes by nationality</h6>
</div></figure>
<p>In order to scrape the list data, we need to <a data-primary="Chrome DevTools" data-secondary="using Elements tab to inspect scraping targets" data-type="indexterm" id="idm45607786075488"/>fire up our Chrome browser’s DevTools (see <a data-type="xref" href="ch04.xhtml#chrome_elements">“The Elements Tab”</a>) and inspect the target elements using the Elements tab and its inspector (magnifying glass). <a data-type="xref" href="#scrapy_wiki_list_source">Figure 6-3</a> shows the key HTML targets for our first spider: header titles (h2) containing a country name and followed by an ordered list (ol) of winners (li).</p>
<figure><div class="figure" id="scrapy_wiki_list_source">
<img alt="dpj2 0603" height="897" src="assets/dpj2_0603.png" width="1440"/>
<h6><span class="label">Figure 6-3. </span>Finding the HTML targets for the wikilist</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Targeting HTML with Xpaths" data-type="sect1"><div class="sect1" id="idm45607786070672">
<h1>Targeting HTML with Xpaths</h1>
<p>Scrapy uses <a href="https://oreil.ly/Y67BF">xpaths</a> to define its HTML targets. <a data-primary="Scrapy" data-secondary="establishing targets" data-startref="ix_Scrpytrgt" data-type="indexterm" id="idm45607786051680"/>Xpath is a syntax for describing parts of an X(HT)ML document, and while it can get rather complicated, the basics are straightforward and will often solve the job at hand.<a data-primary="HTML" data-secondary="targeting with xpaths" data-type="indexterm" id="ix_HTMLtrgt"/><a data-primary="Scrapy" data-secondary="targeting HTML with xpaths" data-type="indexterm" id="ix_Scrpyxpth"/><a data-primary="xpath" data-secondary="targeting HTML with" data-type="indexterm" id="ix_xpth"/></p>
<p>You can get the xpath of an HTML<a data-primary="Chrome DevTools" data-secondary="getting xpath of HTML elements" data-type="indexterm" id="idm45607786046624"/> element by using Chrome’s Elements tab to hover over the source and then right-clicking and selecting Copy XPath. For example, in the case of our Nobel Prize wikilist’s country names (h3 in <a data-type="xref" href="#scrapy_wiki_list_source">Figure 6-3</a>), selecting the xpath of Argentina (the first country) gives the following:</p>
<pre data-type="programlisting">//*[@id="mw-content-text"]/div[1]/h3[1]</pre>
<p>We can use the following xpath rules to decode it:</p>
<dl>
<dt><code>//E</code></dt>
<dd>
<p>Element <code>&lt;E&gt;</code> anywhere in the document (e.g., <code>//img</code> gets all images on the page)</p>
</dd>
<dt><code>//E[@id="foo"]</code></dt>
<dd>
<p>Select element <code>&lt;E&gt;</code> with ID <code>foo</code></p>
</dd>
<dt><code>//*[@id="foo"]</code></dt>
<dd>
<p>Select any element with ID <code>foo</code></p>
</dd>
<dt><code>//E/F[1]</code></dt>
<dd>
<p>First child element <code>&lt;F&gt;</code> of element <code>&lt;E&gt;</code></p>
</dd>
<dt><code>//E/*[1]</code></dt>
<dd>
<p>First child of element <code>&lt;E&gt;</code></p>
</dd>
</dl>
<p>Following these rules shows that our Argentinian title <code>//*[@id="mw-content-text"]/div[1]/h3[1]</code> is the first header (h2) child of the first <code>div</code> of the DOM element with ID <code>mw-content-text</code>. This is equivalent to the following HTML:</p>
<pre data-code-language="html" data-type="programlisting"><code class="p">&lt;</code><code class="nt">div</code> <code class="na">id</code><code class="o">=</code><code class="s">"mw-content-text"</code><code class="p">&gt;</code>
  <code class="p">&lt;</code><code class="nt">div</code><code class="p">&gt;</code>
    <code class="p">&lt;</code><code class="nt">h2</code><code class="p">&gt;</code>
        ...
    <code class="p">&lt;/</code><code class="nt">h2</code><code class="p">&gt;</code>
  <code class="p">&lt;/</code><code class="nt">div</code><code class="p">&gt;</code>
    ...
<code class="p">&lt;/</code><code class="nt">div</code><code class="p">&gt;</code></pre>
<p>Note that unlike Python, the xpaths don’t use a zero-based index but make the first member <em>1</em>.</p>
<section data-pdf-bookmark="Testing Xpaths with the Scrapy Shell" data-type="sect2"><div class="sect2" id="idm45607786007664">
<h2>Testing Xpaths with the Scrapy Shell</h2>
<p>Getting your xpath targeting right is crucial to good scraping and can involve a degree of iteration. <a data-primary="xpath" data-secondary="targeting HTML with" data-tertiary="testing xpaths with Scrapy shell" data-type="indexterm" id="ix_xpthtst"/><a data-primary="Scrapy" data-secondary="targeting HTML with xpaths" data-tertiary="testing xpaths with Scrapy shell" data-type="indexterm" id="ix_Scrpyxpthtst"/><a data-primary="command-line shell (Scrapy)" data-type="indexterm" id="idm45607786001968"/><a data-primary="shell (Scrapy), testing xpaths with" data-type="indexterm" id="ix_shellScrpy"/>Scrapy makes this process much easier by providing a command-line shell, which takes a URL and creates a response context in which you can try out your xpaths, like so:</p>
<pre data-code-language="bash" data-type="programlisting">$ scrapy shell
  https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_country


<code class="m">2021</code>-12-09 <code class="m">14</code>:31:06 <code class="o">[</code>scrapy.utils.log<code class="o">]</code> INFO: Scrapy <code class="m">2</code>.5.1 started
<code class="o">(</code>bot: nobel_winners<code class="o">)</code>
...

<code class="m">2021</code>-12-09 <code class="m">14</code>:31:07 <code class="o">[</code>scrapy.core.engine<code class="o">]</code> INFO: Spider opened
<code class="m">2021</code>-12-09 <code class="m">14</code>:31:07 <code class="o">[</code>scrapy.core.engine<code class="o">]</code> DEBUG: Crawled <code class="o">(</code><code class="m">200</code><code class="o">)</code>
&lt;GET https://en.wikip...List_of_Nobel_laureates_by_country&gt;
<code class="o">(</code>referer: None<code class="o">)</code>
<code class="o">[</code>s<code class="o">]</code> Available Scrapy objects:

<code class="o">[</code>s<code class="o">]</code>   crawler  &lt;scrapy.crawler.Crawler object at 0x3a8f510&gt;
<code class="o">[</code>s<code class="o">]</code>   item <code class="o">{}</code>
<code class="o">[</code>s<code class="o">]</code>   request    &lt;GET https://...Nobel_laureates_by_country&gt;
<code class="o">[</code>s<code class="o">]</code>   response   &lt;<code class="m">200</code> https://...Nobel_laureates_by_country&gt;
<code class="o">[</code>s<code class="o">]</code>   settings   &lt;scrapy.settings.Settings object at 0x34a98d0&gt;
<code class="o">[</code>s<code class="o">]</code>   spider     &lt;DefaultSpider <code class="s1">'default'</code> at 0x3f59190&gt;

<code class="o">[</code>s<code class="o">]</code> Useful shortcuts:
<code class="o">[</code>s<code class="o">]</code>   shelp<code class="o">()</code>   Shell <code class="nb">help</code> <code class="o">(</code>print this <code class="nb">help</code><code class="o">)</code>
<code class="o">[</code>s<code class="o">]</code>   fetch<code class="o">(</code>url<code class="o">[</code>, <code class="nv">redirect</code><code class="o">=</code>True<code class="o">])</code> Fetch URL and update <code class="nb">local</code> objects
<code class="o">(</code>by default, redirects are followed<code class="o">)</code>
<code class="o">[</code>s<code class="o">]</code>   fetch<code class="o">(</code>req<code class="o">)</code>                  Fetch a scrapy.Request and update
<code class="o">[</code>s<code class="o">]</code>   view<code class="o">(</code>response<code class="o">)</code>    View response <code class="k">in</code> a browser

In <code class="o">[</code><code class="m">1</code><code class="o">]</code>:</pre>
<p>Now we have an IPython-based shell with code-complete and syntax highlighting in which to try out our xpath targeting. Let’s grab all the <code>&lt;h3&gt;</code> headers on the wiki page:</p>
<pre data-type="programlisting">In [1]: h3s = response.xpath('//h3')</pre>
<p>The resulting <code>h3s</code> is a <a href="https://oreil.ly/zpbqa">SelectorList</a>, a specialized Python <code>list</code> object. Let’s see how many headers we have:</p>
<pre data-type="programlisting">In [2]: len(h3s)
Out[2]: 91</pre>
<p>We can grab the first <a href="https://oreil.ly/uBhdU"><code>Selector</code> object</a> and query its methods and properties in the Scrapy shell by pressing Tab after appending a dot:</p>
<pre data-type="programlisting">In [3] h3 = h3s[0]
In [4] h3.
attrib             get                re                 remove             ...
css                getall             re_first           remove_namespaces  ...
extract            namespaces         register_namespace response           ...</pre>
<p>You’ll often use the <code>extract</code> method to get the raw result of the xpath selector:</p>
<pre data-type="programlisting">In [5]: h3.extract()
Out[6]:
u'&lt;h3&gt;
  &lt;span class="mw-headline" id="Argentina"&gt;Argentina&lt;/span&gt;
  &lt;span class="mw-editsection"&gt;
  &lt;span class="mw-editsection-bracket"&gt;
  ...
  &lt;/h3&gt;'</pre>
<p>This shows that our country headers start on the first <code>&lt;h3&gt;</code> and contain a <code>span</code> with class <code>mw-headline</code>. We can use the presence of the <code>mw-headline</code> class as a filter for our country headers and the contents as our country label. Let’s try out an xpath, using the selector’s <code>text</code> method to extract the text from the <code>mw-headline</code> span. Note that we use the <code>xpath</code> method of the <code>&lt;h3&gt;</code> selector, which makes the xpath query relative to that element:</p>
<pre data-type="programlisting">In [7]: h3_arg = h3
In [8]: country = h3_arg.xpath(\
                         'span[@class="mw-headline"]/text()')\
.extract()
In [9]: country
Out[9]: ['Argentina']</pre>
<p>The <code>extract</code> method returns a list of possible matches, in our case the single <code>'Argentina'</code> string. By iterating through the <code>h3s</code> list, we can now get our country names.</p>
<p>Assuming we have a country’s <code>&lt;h3&gt;</code> header, we now need to get the <code>&lt;ol&gt;</code> ordered list of Nobel winners following it (<a data-type="xref" href="#scrapy_wiki_list">Figure 6-2</a>  B). Handily, the xpath <code>following-sibling</code> selector can do just that. Let’s grab the first ordered list after the Argentina header:</p>
<pre data-type="programlisting">In [10]: ol_arg = h3_arg.xpath('following-sibling::ol[1]')
Out[10]: ol_arg
[&lt;Selector xpath='following-sibling::ol[1]' data=u'&lt;ol&gt;&lt;li&gt;
&lt;a href="/wiki/C%C3%A9sar_Milst'&gt;]</pre>
<p>Looking at the truncated data for <code>ol_arg</code> shows that we have selected an ordered list.  Note that even though there’s only one <code>Selector</code>, <code>xpath</code> still returns a <code>SelectorList</code>.  For convenience, you’ll generally just select the first member directly:</p>
<pre data-type="programlisting">In [11]: ol_arg = h2_arg.xpath('following-sibling::ol[1]')[0]</pre>
<p>Now that we’ve got the ordered list, let’s get a list of its member <code>&lt;li&gt;</code> elements (as of mid 2022):</p>
<pre data-type="programlisting">In [12]: lis_arg = ol_arg.xpath('li')
In [13]: len(lis_arg)
Out[13]: 5</pre>
<p>Let’s examine one of those list elements using <code>extract</code>. As a first test, we’re looking to scrape the name of the winner and capture the list element’s text:</p>
<pre data-type="programlisting">In [14]: li = lis_arg[0] # select the first list element
In [15]: li.extract()
Out[15]:
'&lt;li&gt;&lt;a href="/wiki/C%C3%A9sar_Milstein"
         title="C\xe9sar Milstein"&gt;C\xe9sar Milstein&lt;/a&gt;,
         Physiology or Medicine, 1984&lt;/li&gt;'</pre>
<p>Extracting the list element shows a standard pattern: a hyperlinked name to the winner’s Wikipedia page followed by a comma-separated winning category and year. A robust way to get the winning name is just to select the text of the list element’s first <code>&lt;a&gt;</code> tag:</p>
<pre data-type="programlisting">In [16]: name = li.xpath('a//text()')[0].extract()
In [17]: name
Out[17]: 'César Milstein'</pre>
<p>It’s often useful to get all the text in, for example, a list element, stripping the various HTML <code>&lt;a&gt;</code>, <code>&lt;span&gt;</code>, and other tags. <code>descendant-or-self</code> gives us a handy way of doing this, producing a list of the descendants’ text:</p>
<pre data-type="programlisting">In [18]: list_text = li.xpath('descendant-or-self::text()')\
.extract()
In [19]: list_text
Out[19]: ['César Milstein', '*, Physiology or Medicine, 1984']</pre>
<p>We can get the full text by joining the list elements together:</p>
<pre data-type="programlisting">In [20]: ' '.join(list_text)
Out[20]: 'César Milstein *, Physiology or Medicine, 1984'</pre>
<p>Note that the first item of <code>list_text</code> is the winner’s name, giving us another way to access it if, for example, it were missing a hyperlink.</p>
<p>Now that we’ve established the xpaths to our scraping targets (the name and link text of the Nobel Prize winners), let’s incorporate them into our first Scrapy spider.</p>
</div></section>
<section data-pdf-bookmark="Selecting with Relative Xpaths" data-type="sect2"><div class="sect2" id="idm45607786005904">
<h2>Selecting with Relative Xpaths</h2>
<p>As just shown, Scrapy <code>xpath</code> selections return lists of selectors which, in turn,  have their own <code>xpath</code> methods.<a data-primary="selectors (xpath)" data-type="indexterm" id="idm45607785824448"/><a data-primary="Scrapy" data-secondary="targeting HTML with xpaths" data-tertiary="selecting with relative xpaths" data-type="indexterm" id="idm45607785823712"/><a data-primary="relative xpaths, selecting with" data-type="indexterm" id="idm45607785822416"/><a data-primary="xpath" data-secondary="targeting HTML with" data-tertiary="selecting with relative xpaths" data-type="indexterm" id="idm45607785821728"/> When using the <code>xpath</code> method, it’s important to be clear about relative and absolute selections. Let’s make the distinction clear using the Nobel page’s table of contents as an example.</p>
<p>The table of contents has the following structure:</p>
<pre data-code-language="html" data-type="programlisting"><code class="p">&lt;</code><code class="nt">div</code> <code class="na">id</code><code class="o">=</code><code class="s">'toc'</code><code class="err">...</code> <code class="p">&gt;</code>
  ...
   <code class="p">&lt;</code><code class="nt">ul</code> <code class="err">...</code> <code class="p">&gt;</code>
     <code class="p">&lt;</code><code class="nt">li</code> <code class="err">...</code> <code class="p">&gt;</code>
       <code class="p">&lt;</code><code class="nt">a</code> <code class="na">href</code><code class="o">=</code><code class="s">'Argentina'</code><code class="p">&gt;</code> ... <code class="p">&lt;/</code><code class="nt">a</code><code class="p">&gt;</code>
     <code class="p">&lt;/</code><code class="nt">li</code><code class="p">&gt;</code>
     ...
   <code class="p">&lt;/</code><code class="nt">ul</code><code class="p">&gt;</code>
  ...
<code class="p">&lt;/</code><code class="nt">div</code><code class="p">&gt;</code></pre>
<p>We can select the table of contents of the Nobel wiki page using a standard <code>xpath</code> query on the response, and getting the <code>div</code> with ID <code>toc</code>:</p>
<pre data-type="programlisting">In [21]: toc = response.xpath('//div[@id="toc"]')[0]</pre>
<p>If we want to get all the country <code>&lt;li&gt;</code> list tags, we can use a relative <code>xpath</code> on the selected <code>toc</code> div. Looking at the HTML in <a data-type="xref" href="#scrapy_wiki_list_source">Figure 6-3</a> shows that the unordered list <code>ul</code> of countries is the first list member of the second list item of the table of content’s top list. This list can be selected by the following equivalent xpaths, both selecting children of the current <code>toc</code> selection relatively:</p>
<pre data-type="programlisting">In [22]: lis = toc.xpath('.//ul/li[2]/ul/li')
In [23]: lis = toc.xpath('ul/li[2]/ul/li')
In [24]: len(lis)
Out[24]: 81 # the number of countries in the table of contents (July 2022)</pre>
<p>A common mistake is to use a nonrelative <code>xpath</code> selector on the current selection, which selects from the whole document, in this case getting all unordered (<code>&lt;ul&gt;</code>) <code>&lt;li&gt;</code> tags:</p>
<pre data-type="programlisting">In [25]: lis = toc.xpath('//ul/li')
In [26]: len(lis)
OUt[26]: 271</pre>
<p>Errors made from mistaking relative and nonrelative queries crop up a lot in the forums, so it’s good to be very aware of the distinction and watch those dots.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Getting the right xpath expression for your target element(s) can be a little tricky, and those difficult edge cases can demand a complex nest of clauses. The use of a well-written cheat sheet can be a great help here, and thankfully there are many good xpath ones.<a data-primary="HTML" data-secondary="targeting with xpaths" data-startref="ix_HTMLtrgt" data-type="indexterm" id="idm45607785783536"/><a data-primary="Scrapy" data-secondary="targeting HTML with xpaths" data-startref="ix_Scrpyxpth" data-type="indexterm" id="idm45607785782288"/><a data-primary="xpath" data-secondary="targeting HTML with" data-startref="ix_xpth" data-type="indexterm" id="idm45607785781104"/> A very nice selection can be found <a href="https://devhints.io/xpath">at devhints.io</a>.</p>
</div>
</div></section>
</div></section>
<section data-pdf-bookmark="A First Scrapy Spider" data-type="sect1"><div class="sect1" id="idm45607786070176">
<h1>A First Scrapy Spider</h1>
<p>Armed with a little xpath knowledge, let’s produce our first scraper aiming to get the country and link text for the winners (<a data-type="xref" href="#scrapy_wiki_list">Figure 6-2</a> A and B).<a data-primary="spiders (Scrapy)" data-type="indexterm" id="ix_spdr"/><a data-primary="Scrapy" data-secondary="first Scrapy spider" data-type="indexterm" id="ix_Scrpyspi"/></p>
<p>Scrapy calls its scrapers <em>spiders</em>, each of which is a Python module placed in the <em>spiders</em> directory of your project. We’ll call our first scraper <em>nwinner_list_spider.py</em>:</p>
<pre data-type="programlisting">.
├── nobel_winners
│   ├── __init__.py
│   ├── items.py
│   ├── middlewares.py
│   ├── pipelines.py
│   ├── settings.py
│   └── spiders
│       |── __init__.py
│       └── nwinners_list_spider.py &lt;---
└── scrapy.cfg</pre>
<p>Spiders are subclassed <code>scrapy.Spider</code> classes, and any placed in the <em>spiders</em> directory will be automatically detected by Scrapy and made accessible by name to the <code>scrapy</code> command.</p>
<p>The basic Scrapy spider shown in <a data-type="xref" href="#scrapy_spider">Example 6-2</a> follows a pattern you’ll be using with most of your spiders.<a data-primary="Item class (Scrapy)" data-type="indexterm" id="idm45607785729392"/> First, you subclass a Scrapy <code>item</code> to create fields for your scraped data (section A in <a data-type="xref" href="#scrapy_spider">Example 6-2</a>). You then create a named spider by subclassing <code>scrapy.Spider</code> (section B in <a data-type="xref" href="#scrapy_spider">Example 6-2</a>). You will use the spider’s name when calling <code>scrapy</code> from the command line. Each spider has a <code>parse</code> method, which deals with the HTTP requests to a list of start URLs contained in a <code>start_url</code> class attribute. In our case, the start URL is the Wikipedia page for Nobel laureates by country.</p>
<div data-type="example" id="scrapy_spider">
<h5><span class="label">Example 6-2. </span>A first Scrapy spider</h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># nwinners_list_spider.py</code><code>
</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">scrapy</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">re</code><code>
</code><code class="c1"># A. Define the data to be scraped</code><code>
</code><code class="k">class</code><code> </code><code class="nc">NWinnerItem</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Item</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">country</code><code> </code><code class="o">=</code><code> </code><code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">(</code><code class="p">)</code><code>
</code><code>    </code><code class="n">name</code><code> </code><code class="o">=</code><code> </code><code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">(</code><code class="p">)</code><code>
</code><code>    </code><code class="n">link_text</code><code> </code><code class="o">=</code><code> </code><code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="c1"># B Create a named spider</code><code>
</code><code class="k">class</code><code> </code><code class="nc">NWinnerSpider</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Spider</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">""" Scrapes the country and link text of the Nobel-winners. """</code><code>
</code><code>
</code><code>    </code><code class="n">name</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">nwinners_list</code><code class="s1">'</code><code>
</code><code>    </code><code class="n">allowed_domains</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">en.wikipedia.org</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>    </code><code class="n">start_urls</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">http://en.wikipedia.org ... of_Nobel_laureates_by_country</code><code class="s2">"</code><code>
</code><code>    </code><code class="p">]</code><code>
</code><code>    </code><code class="c1"># C A parse method to deal with the HTTP response</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">parse</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">response</code><code class="p">)</code><code class="p">:</code><code>
</code><code>
</code><code>         </code><code class="n">h3s</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">//h3</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO1-1" id="co_heavyweight_scraping_with_scrapy_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code>         </code><code class="k">for</code><code> </code><code class="n">h3</code><code> </code><code class="ow">in</code><code> </code><code class="n">h3s</code><code class="p">:</code><code>
</code><code>            </code><code class="n">country</code><code> </code><code class="o">=</code><code> </code><code class="n">h3</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">span[@class=</code><code class="s1">"</code><code class="s1">mw-headline</code><code class="s1">"</code><code class="s1">]</code><code class="s1">'</code><code>\
</code><code>            </code><code class="s1">'</code><code class="s1">text()</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO1-2" id="co_heavyweight_scraping_with_scrapy_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>            </code><code class="k">if</code><code> </code><code class="n">country</code><code class="p">:</code><code>
</code><code>                </code><code class="n">winners</code><code> </code><code class="o">=</code><code> </code><code class="n">h2</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">following-sibling::ol[1]</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO1-3" id="co_heavyweight_scraping_with_scrapy_CO1-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>                </code><code class="k">for</code><code> </code><code class="n">w</code><code> </code><code class="ow">in</code><code> </code><code class="n">winners</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">li</code><code class="s1">'</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                    </code><code class="n">text</code><code> </code><code class="o">=</code><code> </code><code class="n">w</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">descendant-or-self::text()</code><code class="s1">'</code><code class="p">)</code><code>\
</code><code>                    </code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code>
</code><code>                    </code><code class="k">yield</code><code> </code><code class="n">NWinnerItem</code><code class="p">(</code><code>
</code><code>                        </code><code class="n">country</code><code class="o">=</code><code class="n">country</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">name</code><code class="o">=</code><code class="n">text</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                        </code><code class="n">link_text</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1"> </code><code class="s1">'</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">text</code><code class="p">)</code><code>
</code><code>                        </code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO1-1" id="callout_heavyweight_scraping_with_scrapy_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Gets all the <code>&lt;h3&gt;</code> headers on the page, most of which will be our target country titles.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO1-2" id="callout_heavyweight_scraping_with_scrapy_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Where possible, gets the text of the <code>&lt;h3&gt;</code> element’s child <code>&lt;span&gt;</code> with class <code>mw-headline</code>.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO1-3" id="callout_heavyweight_scraping_with_scrapy_CO1-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Gets the list of country winners.</p></dd>
</dl></div>
<p>The <code>parse</code> method in <a data-type="xref" href="#scrapy_spider">Example 6-2</a> receives the response from an HTTP request to the Wikipedia Nobel Prize page and yields Scrapy items, which are then converted to JSON objects and appended to the output file, a JSON array of objects.<a data-primary="JSON" data-secondary="conversion of Scrapy items to" data-type="indexterm" id="idm45607785377056"/></p>
<p>Let’s run our first spider to make sure we’re correctly parsing and scraping our Nobel data. First, navigate to the <em>nobel_winners</em> root directory (containing the <em>scrapy.cfg</em> file) of the scraping project. Let’s see what scraping spiders are available:</p>
<pre data-type="programlisting">$ scrapy list
nwinners_list</pre>
<p>As expected, we have one <code>nwinners_list</code> spider sitting in the <em>spiders</em> directory. To start it scraping, we use the <code>crawl</code> command and direct the output to a <em>nwinners.json</em> file. By default, we get a lot of Python logging information accompanying the crawl:</p>
<pre data-type="programlisting">$ scrapy crawl nwinners_list -o nobel_winners.json
2021- ... [scrapy] INFO: Scrapy started (bot: nobel_winners)
...
2021- ... [nwinners_list] INFO: Closing spider (finished)
2021- ... [nwinners_list] INFO: Dumping Scrapy stats:
        {'downloader/request_bytes': 1147,
         'downloader/request_count': 4,
         'downloader/request_method_count/GET': 4,
         'downloader/response_bytes': 66459,
         ...
         'item_scraped_count': 1169, <a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO2-1" id="co_heavyweight_scraping_with_scrapy_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a>
2021- ...  [scrapy.core.engine] INFO: Spider closed (finished)</pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO2-1" id="callout_heavyweight_scraping_with_scrapy_CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>We scraped 1,169 Nobel winners from the page.</p></dd>
</dl>
<p>The output of the scrapy <code>crawl</code> shows 1,169 items successfully <span class="keep-together">scraped</span>. Let’s look at our JSON output file to make sure things have gone according to plan:</p>
<pre data-type="programlisting">$ head nobel_winners.json
[{"country": "Argentina",
  "link_text": "C\u00e9sar Milstein , Physiology or Medicine,"\
  " 1984",
  "name": "C\u00e9sar Milstein"},
 {"country": "Argentina",
  "link_text": "Adolfo P\u00e9rez Esquivel , Peace, 1980",
  "name": "Adolfo P\u00e9rez Esquivel"},
  ...</pre>
<p>As you can see, we have an array of JSON objects with the four key fields present and correct.</p>
<p>Now that we have a spider that successfully scrapes the list data for all the Nobel winners on the page, let’s start refining it to grab all the data we are targeting for our Nobel Prize visualization (see <a data-type="xref" href="#scrapy_target_JSON">Example 6-1</a> and <a data-type="xref" href="#scrapy_targets">Figure 6-1</a>).</p>
<p class="pagebreak-before">First, let’s add all the data we plan to scrape as fields to our <code>scrapy.Item</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="o">...</code>
<code class="k">class</code> <code class="nc">NWinnerItem</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Item</code><code class="p">):</code>
    <code class="n">name</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">link</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">year</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">category</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">country</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">gender</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">born_in</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">date_of_birth</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">date_of_death</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">place_of_birth</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">place_of_death</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
<code class="o">...</code></pre>
<p>It’s also sensible to simplify the code a bit and use a dedicated function, <code>process_winner_li</code>,  to process the winners’ link text. We’ll pass a link selector and country name to it and return a dictionary containing the scraped data:</p>
<pre data-code-language="python" data-type="programlisting"><code class="o">...</code>

<code class="k">def</code> <code class="nf">parse</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>

    <code class="n">h3s</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'//h3'</code><code class="p">)</code>

    <code class="k">for</code> <code class="n">h3</code> <code class="ow">in</code> <code class="n">h3s</code><code class="p">:</code>
        <code class="n">country</code> <code class="o">=</code> <code class="n">h3</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'span[@class="mw-headline"]/text()'</code><code class="p">)</code>\
        <code class="o">.</code><code class="n">extract</code><code class="p">()</code>
        <code class="k">if</code> <code class="n">country</code><code class="p">:</code>
            <code class="n">winners</code> <code class="o">=</code> <code class="n">h3</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'following-sibling::ol[1]'</code><code class="p">)</code>
            <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">winners</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'li'</code><code class="p">):</code>
                <code class="n">wdata</code> <code class="o">=</code> <code class="n">process_winner_li</code><code class="p">(</code><code class="n">w</code><code class="p">,</code> <code class="n">country</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
                <code class="o">...</code></pre>
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45607785259936">
<h5>Embracing Regexes</h5><blockquote>
<p>Some people, when confronted with a problem, think
“I know, I’ll use regular expressions.”   Now they have two problems.</p>
<p data-type="attribution">Jamie Zawinskie</p>
</blockquote>
<p>The preceding quote is a hoary old classic but does sum up how many people feel about <a href="https://oreil.ly/OfQls">regular expressions (regexes)</a>. <a data-primary="regular expressions" data-type="indexterm" id="idm45607785233984"/>Regexes use a sequence of characters to define a search expression used for string matching. Both Python and JavaScript have built-in handling of them.<a data-primary="regexes" data-see="regular expressions" data-type="indexterm" id="idm45607785233152"/></p>
<p>In Python, the <code>re</code> module provides a number of regex methods. <a data-primary="re module (Python)" data-type="indexterm" id="idm45607785231376"/>A common task might be to find all the email addresses in a document, recognizing email strings by the form <em>foo@bar.com</em>. Let’s create a regex to find them, breaking down the process:<sup><a data-type="noteref" href="ch06.xhtml#idm45607785183776" id="idm45607785183776-marker">2</a></sup></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">txt</code> <code class="o">=</code> <code class="s1">'Feel free to contact me at '</code>\
<code class="s1">' pyjdataviz@kyrandale.com with any feedback.'</code>

<code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="sa">r</code><code class="s1">'[\w\.-]+@[\w\.-]+'</code><code class="p">,</code> <code class="n">txt</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="p">[</code><code class="s1">'pyjdataviz@kyrandale.com'</code><code class="p">]</code></pre>
<p>The <code>findall</code> method takes a regex string (with an <em>r</em> prepended) as its first argument and the text to search as its second. The email search pattern uses the following rules:</p>
<table>
<tbody>
<tr>
<td><p>\w</p></td>
<td><p>Matches an alphanumeric string containing numbers and upper and lowercase letters (regex shorthand is [0-9a-zA-Z_])</p></td>
</tr>
<tr>
<td><p>\</p></td>
<td><p>Escapes a special character</p></td>
</tr>
<tr>
<td><p>\.</p></td>
<td><p>Matches a dot</p></td>
</tr>
<tr>
<td><p>-</p></td>
<td><p>Matches a hyphen</p></td>
</tr>
<tr>
<td><p>+</p></td>
<td><p>Matches one or more of the square-bracketed strings</p></td>
</tr>
</tbody>
</table>
<p>Taken together, these rules match any two strings connected by @ and containing alphanumeric characters or dots or hyphens. This is obviously a pretty broad pattern (e.g., <code>.@.</code> would provide a match) that you might want to refine. For example, you could use <code>r'[\w\.-]@gmail.com</code> if you were searching for only Gmail addresses.</p>
<p>Although the syntax of regexes can be challenging <a data-primary="regular expressions" data-secondary="pattern-matching in web scraping" data-type="indexterm" id="idm45607785146000"/>at first, the fact is that web scraping is often about pattern-matching messy and underspecified data, and a regex is pretty much tailor-made for many of the jobs that crop up.<a data-primary="pattern-matching" data-seealso="regular expressions" data-type="indexterm" id="idm45607785144928"/> You can probably hack your way around them, but embracing them a little will make your life that much easier, and the good news is that a little goes a long way. See <a data-type="xref" href="#scrapy_process_li">Example 6-3</a> for some examples.</p>
</div></aside>
<p>The <code>process_winner_li</code> method is shown in <a data-type="xref" href="#scrapy_process_li">Example 6-3</a>. A <code>wdata</code> dictionary is filled with information extracted from the winner’s <code>li</code> tag, using a couple of regexes to find the prize year and category.</p>
<div data-type="example" id="scrapy_process_li">
<h5><span class="label">Example 6-3. </span>Processing a winner’s list item</h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># ...</code><code>
</code><code class="kn">import</code><code> </code><code class="nn">re</code><code>
</code><code class="n">BASE_URL</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">http://en.wikipedia.org</code><code class="s1">'</code><code>
</code><code class="c1"># ...</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">process_winner_li</code><code class="p">(</code><code class="n">w</code><code class="p">,</code><code> </code><code class="n">country</code><code class="o">=</code><code class="kc">None</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">"""
    Process a winner's &lt;li&gt; tag, adding country of birth or
    nationality, as applicable.
    """</code><code>
</code><code>    </code><code class="n">wdata</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="p">}</code><code>
</code><code>    </code><code class="c1"># get the href link-address from the &lt;a&gt; tag</code><code>
</code><code>    </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">BASE_URL</code><code> </code><code class="o">+</code><code> </code><code class="n">w</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">a/@href</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO3-1" id="co_heavyweight_scraping_with_scrapy_CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code>    </code><code class="n">text</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1"> </code><code class="s1">'</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">w</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">descendant-or-self::text()</code><code class="s1">'</code><code class="p">)</code><code>\
</code><code>         </code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code>
</code><code>    </code><code class="c1"># get comma-delineated name and strip trailing whitespace</code><code>
</code><code>    </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">text</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'</code><code class="s1">,</code><code class="s1">'</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">strip</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">year</code><code> </code><code class="o">=</code><code> </code><code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s1">'</code><code class="s1">\</code><code class="s1">d</code><code class="si">{4}</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">text</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO3-2" id="co_heavyweight_scraping_with_scrapy_CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="n">year</code><code class="p">:</code><code>
</code><code>        </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">year</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="nb">int</code><code class="p">(</code><code class="n">year</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">)</code><code>
</code><code>    </code><code class="k">else</code><code class="p">:</code><code>
</code><code>        </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">year</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>        </code><code class="nb">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">Oops, no year in </code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">text</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">category</code><code> </code><code class="o">=</code><code> </code><code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">Physics|Chemistry|Physiology or Medicine|Literature|</code><code class="s1">'</code><code>\
</code><code>            </code><code class="s1">'</code><code class="s1">Peace|Economics</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                </code><code class="n">text</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO3-3" id="co_heavyweight_scraping_with_scrapy_CO3-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="n">category</code><code class="p">:</code><code>
</code><code>        </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">category</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">category</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>
</code><code>    </code><code class="k">else</code><code class="p">:</code><code>
</code><code>        </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">category</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">'</code><code>
</code><code>        </code><code class="nb">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">Oops, no category in </code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">text</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="n">country</code><code class="p">:</code><code>
</code><code>         </code><code class="k">if</code><code> </code><code class="n">text</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'</code><code class="s1">*</code><code class="s1">'</code><code class="p">)</code><code> </code><code class="o">!=</code><code> </code><code class="o">-</code><code class="mi">1</code><code class="p">:</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO3-4" id="co_heavyweight_scraping_with_scrapy_CO3-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code>
</code><code>             </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">country</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">'</code><code>
</code><code>             </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">born_in</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">country</code><code>
</code><code>         </code><code class="k">else</code><code class="p">:</code><code>
</code><code>             </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">country</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">country</code><code>
</code><code>             </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">born_in</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">'</code><code>
</code><code>
</code><code>
</code><code>    </code><code class="c1"># store a copy of the link's text-string for any manual corrections</code><code>
</code><code>    </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">text</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">text</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">wdata</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO3-1" id="callout_heavyweight_scraping_with_scrapy_CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>To grab the <code>href</code> attribute from the list item’s <code>&lt;a&gt;</code> tag (<code>&lt;li&gt;&lt;a href=<em>/wiki…​</em>&gt;[winner name]&lt;/a&gt;…​</code>), we use the xpath attribute referent @.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO3-2" id="callout_heavyweight_scraping_with_scrapy_CO3-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Here, we use <code>re</code>, Python’s built-in regex library, to find the four-digit year strings in the list item’s text.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO3-3" id="callout_heavyweight_scraping_with_scrapy_CO3-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Another use of the regex library to find the Nobel Prize category in the text.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO3-4" id="callout_heavyweight_scraping_with_scrapy_CO3-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>An asterisk following the winner’s name is used to indicate that the country is the winner’s by birth—​not nationality—​at the time of the prize (e.g., <code>"William Lawrence Bragg*, Physics, 1915"</code> in the list for Australia).</p></dd>
</dl></div>
<p><a data-type="xref" href="#scrapy_process_li">Example 6-3</a> returns all the winners’ data available on the main Wikipedia Nobels by Country page—that is, the name, year, category, country (country of birth or country of nationality when awarded the prize), and a link to the individual winners’ pages. We’ll need to use this last information to get those biographical pages and use them to scrape our remaining target data (see <a data-type="xref" href="#scrapy_target_JSON">Example 6-1</a> and <a data-type="xref" href="#scrapy_targets">Figure 6-1</a>).<a data-primary="spiders (Scrapy)" data-startref="ix_spdr" data-type="indexterm" id="idm45607784797904"/><a data-primary="Scrapy" data-secondary="first Scrapy spider" data-startref="ix_Scrpyspi" data-type="indexterm" id="idm45607784796928"/></p>
</div></section>
<section data-pdf-bookmark="Scraping the Individual Biography Pages" data-type="sect1"><div class="sect1" id="scrapy_indiv_bios">
<h1>Scraping the Individual Biography Pages</h1>
<p>The main Wikipedia Nobels by Country page gave us a lot of our target data, but the winner’s date of birth, date of death (where applicable), and gender are still to be scraped.<a data-primary="Scrapy" data-secondary="scraping individual biography pages for Nobel Prize winners" data-type="indexterm" id="ix_Scrpyscrpbios"/> It is hoped that this information is available, either implicitly or explicitly, on their biography pages (for nonorganization winners).  Now’s a good time to fire up Chrome’s Elements tab and take a look at those pages to work out how we’re going to extract the desired data.</p>
<p>We saw in the last chapter (<a data-type="xref" href="ch05.xhtml#chapter_getting_data">Chapter 5</a>) that the visible information boxes on individual’s pages are not a reliable source of information and are often missing entirely. Until recently,<sup><a data-type="noteref" href="ch06.xhtml#idm45607784791328" id="idm45607784791328-marker">3</a></sup> a hidden <code>persondata</code> table (see <a data-type="xref" href="#scrapy_persondata">Figure 6-4</a>) gave fairly reliable access to such information as place of birth, date of death, and the like. Unfortunately, this handy resource has been deprecated.<sup><a data-type="noteref" href="ch06.xhtml#idm45607784789392" id="idm45607784789392-marker">4</a></sup> The good news is that this is part of an attempt to improve the categorization of biographical information by giving it a dedicated space in <a href="https://oreil.ly/ICbBi">Wikidata</a>, Wikipedia’s central storage for its structured data.<a data-primary="Wikidata" data-type="indexterm" id="idm45607784703776"/></p>
<figure><div class="figure" id="scrapy_persondata">
<img alt="dpj2 0604" height="438" src="assets/dpj2_0604.png" width="1324"/>
<h6><span class="label">Figure 6-4. </span>A Nobel Prize winner’s hidden <code>persondata</code> table</h6>
</div></figure>
<p>Examining Wikipedia’s biography pages with Chrome’s Elements tab shows a link to the relevant Wikidata item (see <a data-type="xref" href="#scrapy_wikidata_link">Figure 6-5</a>), which takes you to the biographical data held at <a class="bare" href="https://www.wikidata.org"><em class="hyperlink">https://www.wikidata.org</em></a>. By following this link, we can scrape whatever we find there, which we hope will be the bulk of our target data—​significant dates and places (see <a data-type="xref" href="#scrapy_target_JSON">Example 6-1</a>).</p>
<figure><div class="figure" id="scrapy_wikidata_link">
<img alt="dpj2 0605" height="478" src="assets/dpj2_0605.png" width="698"/>
<h6><span class="label">Figure 6-5. </span>Hyperlink to the winner’s Wikidata</h6>
</div></figure>
<p class="pagebreak-before">Following the link to Wikidata shows a page containing fields for the data we are looking for, such as the date of birth of our prize winner. As <a data-type="xref" href="#scrapy_wikidata">Figure 6-6</a> shows, the properties are embedded in a nest of computer-generated HTML, with related codes, which we can use as a scraping identifier (e.g., date of birth has the code <code>P569</code>).</p>
<figure><div class="figure" id="scrapy_wikidata">
<img alt="dpj2 0606" height="512" src="assets/dpj2_0606.png" width="777"/>
<h6><span class="label">Figure 6-6. </span>Biographical properties at Wikidata</h6>
</div></figure>
<p>As <a data-type="xref" href="#scrapy_wikidata_xpath">Figure 6-7</a> shows, the actual data we want, in this case a date string, is contained in a further nested branch of HTML, within its respective property tag. By selecting the <code>div</code> and right-clicking, we can store the element’s xpath and use that to tell Scrapy how to get the data it contains.<a data-primary="xpath" data-secondary="getting xpath for Wikidata property" data-type="indexterm" id="idm45607784690960"/></p>
<figure><div class="figure" id="scrapy_wikidata_xpath">
<img alt="dpj2 0607" height="427" src="assets/dpj2_0607.png" width="763"/>
<h6><span class="label">Figure 6-7. </span>Getting the xpath for a Wikidata property</h6>
</div></figure>
<p>Now that we have the xpaths necessary to find our scraping targets, let’s put it all together and see how Scrapy chains requests, allowing for complex, multipage scraping operations.<a data-primary="Scrapy" data-secondary="scraping individual biography pages for Nobel Prize winners" data-startref="ix_Scrpyscrpbios" data-type="indexterm" id="idm45607784687984"/></p>
</div></section>
<section data-pdf-bookmark="Chaining Requests and Yielding Data" data-type="sect1"><div class="sect1" id="idm45607784795456">
<h1>Chaining Requests and Yielding Data</h1>
<p>In this section we’ll see how to chain Scrapy requests, allowing us to follow hyperlinks, scraping data as we go.<a data-primary="Scrapy" data-secondary="chaining requests and yielding data" data-type="indexterm" id="ix_Scrypchain"/> First, let’s enable Scrapy’s page caching. While experimenting with xpath targets, we want to limit the number of calls to Wikipedia, and it’s good manners to store our fetched pages. Unlike some datasets out there, our Nobel Prize winners change but once a year.<sup><a data-type="noteref" href="ch06.xhtml#idm45607784684416" id="idm45607784684416-marker">5</a></sup></p>
<section data-pdf-bookmark="Caching Pages" data-type="sect2"><div class="sect2" id="idm45607784683792">
<h2>Caching Pages</h2>
<p>As you might expect, Scrapy has a <a href="https://oreil.ly/ytYWP">sophisticated caching system</a> that gives you fine-grained control over your page caching (e.g., allowing you to choose between database or filesystem storage backends, how long before your pages are expired, etc.).  <a data-primary="caching" data-secondary="web page caching using Scrapy" data-type="indexterm" id="idm45607784681328"/><a data-primary="Scrapy" data-secondary="chaining requests and yielding data" data-tertiary="caching web pages" data-type="indexterm" id="idm45607784680480"/>It is implemented as <a href="https://oreil.ly/w8v7c">middleware</a> enabled in our project’s <code>settings.py</code> module. There are various options available but for the purposes of our Nobel scraping, simply setting <code>HTTPCACHE_ENABLED</code> to <code>True</code> will suffice:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># -*- coding: utf-8 -*-</code>

<code class="c1"># Scrapy settings for nobel_winners project</code>
<code class="c1">#</code>
<code class="c1"># This file contains only the most important settings by</code>
<code class="c1"># default. All the other settings are documented here:</code>
<code class="c1">#</code>
<code class="c1">#     http://doc.scrapy.org/en/latest/topics/settings.xhtml</code>
<code class="c1">#</code>

<code class="n">BOT_NAME</code> <code class="o">=</code> <code class="s1">'nobel_winners'</code>

<code class="n">SPIDER_MODULES</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'nobel_winners.spiders'</code><code class="p">]</code>
<code class="n">NEWSPIDER_MODULE</code> <code class="o">=</code> <code class="s1">'nobel_winners.spiders'</code>

<code class="c1"># Crawl responsibly by identifying yourself</code>
<code class="c1"># (and your website) on the user-agent</code>
<code class="c1">#USER_AGENT = 'nobel_winners (+http://www.yourdomain.com)'</code>

<code class="n">HTTPCACHE_ENABLED</code> <code class="o">=</code> <code class="kc">True</code></pre>
<p>Check out the full range of Scrapy middleware <a href="https://oreil.ly/9CMc4">in Scrapy’s documentation</a>.</p>
<p>Having ticked the caching box, let’s see how to chain Scrapy requests.</p>
</div></section>
<section data-pdf-bookmark="Yielding Requests" data-type="sect2"><div class="sect2" id="idm45607784670912">
<h2>Yielding Requests</h2>
<p>Our existing spider’s <code>parse</code> method cycles through the Nobel winners, using the <code>process_winner_li</code> method to scrape the country, name, year, category, and biography-hyperlink fields.<a data-primary="requests (Scrapy), chaining" data-type="indexterm" id="ix_reqScrpy"/><a data-primary="Scrapy" data-secondary="chaining requests and yielding data" data-tertiary="yielding requests" data-type="indexterm" id="ix_Scrpychainreq"/> We now want to use the biography hyperlinks to generate a Scrapy request that will fetch the bio pages and send them to a custom method for scraping.</p>
<p>Scrapy implements a Pythonic pattern for chaining requests, using Python’s <code>yield</code> statement to create a generator,<sup><a data-type="noteref" href="ch06.xhtml#idm45607784643296" id="idm45607784643296-marker">6</a></sup> allowing Scrapy to easily consume any extra page requests we make. <a data-type="xref" href="#scrapy_yield">Example 6-4</a> shows the pattern in action.</p>
<div data-type="example" id="scrapy_yield">
<h5><span class="label">Example 6-4. </span>Yielding a request with Scrapy</h5>
<pre data-code-language="python" data-type="programlisting"><code class="k">class</code><code> </code><code class="nc">NWinnerSpider</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Spider</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">name</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">nwinners_full</code><code class="s1">'</code><code>
</code><code>    </code><code class="n">allowed_domains</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">en.wikipedia.org</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>    </code><code class="n">start_urls</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">https://en.wikipedia.org/wiki/List_of_Nobel_laureates</code><code class="s2">"</code><code> </code><code>\
</code><code>        </code><code class="s2">"</code><code class="s2">_by_country</code><code class="s2">"</code><code>
</code><code>    </code><code class="p">]</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">parse</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">response</code><code class="p">)</code><code class="p">:</code><code>
</code><code>
</code><code>        </code><code class="n">h3s</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">//h3</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>        </code><code class="k">for</code><code> </code><code class="n">h3</code><code> </code><code class="ow">in</code><code> </code><code class="n">h3s</code><code class="p">:</code><code>
</code><code>            </code><code class="n">country</code><code> </code><code class="o">=</code><code> </code><code class="n">h3</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">span[@class=</code><code class="s1">"</code><code class="s1">mw-headline</code><code class="s1">"</code><code class="s1">]/text()</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>                      </code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code>
</code><code>            </code><code class="k">if</code><code> </code><code class="n">country</code><code class="p">:</code><code>
</code><code>                </code><code class="n">winners</code><code> </code><code class="o">=</code><code> </code><code class="n">h2</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">following-sibling::ol[1]</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>                </code><code class="k">for</code><code> </code><code class="n">w</code><code> </code><code class="ow">in</code><code> </code><code class="n">winners</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">li</code><code class="s1">'</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                    </code><code class="n">wdata</code><code> </code><code class="o">=</code><code> </code><code class="n">process_winner_li</code><code class="p">(</code><code class="n">w</code><code class="p">,</code><code> </code><code class="n">country</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                    </code><code class="n">request</code><code> </code><code class="o">=</code><code> </code><code class="n">scrapy</code><code class="o">.</code><code class="n">Request</code><code class="p">(</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO4-1" id="co_heavyweight_scraping_with_scrapy_CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>                        </code><code class="n">wdata</code><code class="p">[</code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                        </code><code class="n">callback</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">parse_bio</code><code class="p">,</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO4-2" id="co_heavyweight_scraping_with_scrapy_CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>                        </code><code class="n">dont_filter</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code>
</code><code>                    </code><code class="n">request</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">NWinnerItem</code><code class="p">(</code><code class="o">*</code><code class="o">*</code><code class="n">wdata</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO4-3" id="co_heavyweight_scraping_with_scrapy_CO4-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>                    </code><code class="k">yield</code><code> </code><code class="n">request</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO4-4" id="co_heavyweight_scraping_with_scrapy_CO4-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">parse_bio</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">response</code><code class="p">)</code><code class="p">:</code><code>
</code><code>        </code><code class="n">item</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">]</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO4-5" id="co_heavyweight_scraping_with_scrapy_CO4-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a><code>
</code><code>        </code><code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO4-1" id="callout_heavyweight_scraping_with_scrapy_CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Makes a request to the winner’s biography page, using the link (<code>wdata[<em>link</em>]</code>) scraped from <code>process_winner_li</code>.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO4-2" id="callout_heavyweight_scraping_with_scrapy_CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Sets the callback function to handle the response.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO4-3" id="callout_heavyweight_scraping_with_scrapy_CO4-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Creates a Scrapy <code>Item</code> to hold our Nobel data and initializes it with the data just scraped from <code>process_winner_li</code>. <a data-primary="Item class (Scrapy)" data-type="indexterm" id="idm45607784590992"/>This <code>Item</code> data is attached to the metadata of the request to allow any response access to it.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO4-4" id="callout_heavyweight_scraping_with_scrapy_CO4-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>By yielding the request, we make the <code>parse</code> method a generator of consumable requests.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO4-5" id="callout_heavyweight_scraping_with_scrapy_CO4-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a></dt>
<dd><p>This method handles the callback from our bio-link request. In order to add scraped data to our Scrapy <code>Item</code>, we first retrieve it from the <code>response</code> metadata.</p></dd>
</dl></div>
<p>Our investigation of the Wikipedia pages in <a data-type="xref" href="#scrapy_indiv_bios">“Scraping the Individual Biography Pages”</a> showed that we need to locate a winner’s Wikidata link from their biography page and use it to generate a request. We will then scrape the date, place, and gender data from the response.</p>
<p><a data-type="xref" href="#scrapy_wikidata_source">Example 6-5</a> shows <code>parse_bio</code> and <code>parse_wikidata</code>, the two methods used to scrape our winners’ biographical data. <code>parse_bio</code> uses the scraped Wikidata link to request the Wikidata page, yielding the <code>request</code> as it in turn was yielded in the <code>parse</code> method. At the end of the request chain, <code>parse_wikidata</code> retrieves the item and fills in any of the fields available from Wikidata, eventually yielding the item to Scrapy.</p>
<div data-type="example" id="scrapy_wikidata_source">
<h5><span class="label">Example 6-5. </span>Parsing the winners’ biography data</h5>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># ...</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">parse_bio</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">response</code><code class="p">)</code><code class="p">:</code><code>
</code><code>
</code><code>        </code><code class="n">item</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>        </code><code class="n">href</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s2">"</code><code class="s2">//li[@id=</code><code class="s2">'</code><code class="s2">t-wikibase</code><code class="s2">'</code><code class="s2">]/a/@href</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO5-1" id="co_heavyweight_scraping_with_scrapy_CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>               </code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code>
</code><code>        </code><code class="k">if</code><code> </code><code class="n">href</code><code class="p">:</code><code>
</code><code>            </code><code class="n">url</code><code> </code><code class="o">=</code><code> </code><code class="n">href</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO5-2" id="co_heavyweight_scraping_with_scrapy_CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>            </code><code class="n">wiki_code</code><code> </code><code class="o">=</code><code> </code><code class="n">url</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'</code><code class="s1">/</code><code class="s1">'</code><code class="p">)</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code>
</code><code>            </code><code class="n">request</code><code> </code><code class="o">=</code><code> </code><code class="n">scrapy</code><code class="o">.</code><code class="n">Request</code><code class="p">(</code><code class="n">href</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">,</code><code>\
</code><code>                          </code><code class="n">callback</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">parse_wikidata</code><code class="p">,</code><code>\</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO5-3" id="co_heavyweight_scraping_with_scrapy_CO5-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>                          </code><code class="n">dont_filter</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code>
</code><code>            </code><code class="n">request</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">item</code><code>
</code><code>            </code><code class="k">yield</code><code> </code><code class="n">request</code><code>
</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">parse_wikidata</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">response</code><code class="p">)</code><code class="p">:</code><code>
</code><code>
</code><code>        </code><code class="n">item</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>        </code><code class="n">property_codes</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO5-4" id="co_heavyweight_scraping_with_scrapy_CO5-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code>
</code><code>            </code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">date_of_birth</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">code</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">P569</code><code class="s1">'</code><code class="p">}</code><code class="p">,</code><code>
</code><code>            </code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">date_of_death</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">code</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">P570</code><code class="s1">'</code><code class="p">}</code><code class="p">,</code><code>
</code><code>            </code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">place_of_birth</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">code</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">P19</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">:</code><code class="kc">True</code><code class="p">}</code><code class="p">,</code><code>
</code><code>            </code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">place_of_death</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">code</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">P20</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">:</code><code class="kc">True</code><code class="p">}</code><code class="p">,</code><code>
</code><code>            </code><code class="p">{</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">gender</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">code</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">P21</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">:</code><code class="kc">True</code><code class="p">}</code><code>
</code><code>          </code><code class="p">]</code><code>
</code><code>
</code><code>        </code><code class="k">for</code><code> </code><code class="n">prop</code><code> </code><code class="ow">in</code><code> </code><code class="n">property_codes</code><code class="p">:</code><code>
</code><code>
</code><code>            </code><code class="n">link_html</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">'</code><code>
</code><code>            </code><code class="k">if</code><code> </code><code class="n">prop</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">)</code><code class="p">:</code><code>
</code><code>                </code><code class="n">link_html</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">/a</code><code class="s1">'</code><code>
</code><code>            </code><code class="c1"># select the div with a property-code id</code><code>
</code><code>            </code><code class="n">code_block</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">//*[@id=</code><code class="s1">"</code><code class="si">%s</code><code class="s1">"</code><code class="s1">]</code><code class="s1">'</code><code class="o">%</code><code class="p">(</code><code class="n">prop</code><code class="p">[</code><code class="s1">'</code><code class="s1">code</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code class="p">)</code><code>
</code><code>            </code><code class="c1"># continue if the code_block exists</code><code>
</code><code>            </code><code class="k">if</code><code> </code><code class="n">code_block</code><code class="p">:</code><code>
</code><code>            </code><code class="c1"># We can use the css selector, which has superior class</code><code>
</code><code>            </code><code class="c1"># selection</code><code>
</code><code>                </code><code class="n">values</code><code> </code><code class="o">=</code><code> </code><code class="n">code_block</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'</code><code class="s1">.wikibase-snakview-value</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>            </code><code class="c1"># the first value corresponds to the code property\</code><code>
</code><code>            </code><code class="c1"># (e.g., '10 August 1879')</code><code>
</code><code>                </code><code class="n">value</code><code> </code><code class="o">=</code><code> </code><code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>
</code><code>                </code><code class="n">prop_sel</code><code> </code><code class="o">=</code><code> </code><code class="n">value</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'</code><code class="s1">.</code><code class="si">%s</code><code class="s1">/text()</code><code class="s1">'</code><code class="o">%</code><code class="n">link_html</code><code class="p">)</code><code>
</code><code>                </code><code class="k">if</code><code> </code><code class="n">prop_sel</code><code class="p">:</code><code>
</code><code>                    </code><code class="n">item</code><code class="p">[</code><code class="n">prop</code><code class="p">[</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">]</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">prop_sel</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code>        </code><code class="k">yield</code><code> </code><code class="n">item</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO5-5" id="co_heavyweight_scraping_with_scrapy_CO5-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO5-1" id="callout_heavyweight_scraping_with_scrapy_CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Extracts the link to Wikidata identified in <a data-type="xref" href="#scrapy_wikidata_link">Figure 6-5</a>.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO5-2" id="callout_heavyweight_scraping_with_scrapy_CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Extracts the <code>wiki_code</code> from the URL, e.g., <a class="bare" href="http://wikidata.org/wiki/Q155525"><em class="hyperlink">http://wikidata.org/wiki/Q155525</em></a> → Q155525.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO5-3" id="callout_heavyweight_scraping_with_scrapy_CO5-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Uses the Wikidata link to generate a request with our spider’s <code>parse_wikidata</code> as a callback to deal with the response.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO5-4" id="callout_heavyweight_scraping_with_scrapy_CO5-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>These are the property codes we found earlier (see <a data-type="xref" href="#scrapy_wikidata">Figure 6-6</a>), with names corresponding to fields in our Scrapy item, <code>NWinnerItem</code>. Those with a <code>True</code> <code>link</code> attribute are contained in <code>&lt;a&gt;</code> tags.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO5-5" id="callout_heavyweight_scraping_with_scrapy_CO5-5"><img alt="5" height="12" src="assets/5.png" width="12"/></a></dt>
<dd><p>Finally we yield the item, which at this point should have all the target data available from Wikipedia.</p></dd>
</dl></div>
<p>With our request chain in place, let’s check that the spider is scraping our required data:</p>
<pre data-code-language="bash" data-type="programlisting">$ scrapy crawl nwinners_full
<code class="m">2021</code>-... <code class="o">[</code>scrapy<code class="o">]</code> ... started <code class="o">(</code>bot: nobel_winners<code class="o">)</code>
...
<code class="m">2021</code>-... <code class="o">[</code>nwinners_full<code class="o">]</code> DEBUG: Scraped from
         &lt;<code class="m">200</code> https://www.wikidata.org/wiki/Q155525&gt;
  <code class="o">{</code><code class="s1">'born_in'</code>: <code class="s1">''</code>,
   <code class="s1">'category'</code>: u<code class="s1">'Physiology or Medicine'</code>,
   <code class="s1">'date_of_birth'</code>: u<code class="s1">'8 October 1927'</code>,
   <code class="s1">'date_of_death'</code>: u<code class="s1">'24 March 2002'</code>,
   <code class="s1">'gender'</code>: u<code class="s1">'male'</code>,
   <code class="s1">'link'</code>: u<code class="s1">'http://en.wikipedia.org/wiki/C%C3%A9sar_Milstein'</code>,
   <code class="s1">'name'</code>: u<code class="s1">'C\xe9sar Milstein'</code>,
   <code class="s1">'country'</code>: u<code class="s1">'Argentina'</code>,
   <code class="s1">'place_of_birth'</code>: u<code class="s1">'Bah\xeda Blanca'</code>,
   <code class="s1">'place_of_death'</code>: u<code class="s1">'Cambridge'</code>,
   <code class="s1">'text'</code>: u<code class="s1">'C\xe9sar Milstein , Physiology or Medicine, 1984'</code>,
   <code class="s1">'year'</code>: <code class="m">1984</code><code class="o">}</code>
<code class="m">2021</code>-... <code class="o">[</code>nwinners_full<code class="o">]</code> DEBUG: Scraped from
         &lt;<code class="m">200</code> https://www.wikidata.org/wiki/Q193672&gt;
 <code class="o">{</code><code class="s1">'born_in'</code>: <code class="s1">''</code>,
  <code class="s1">'category'</code>: u<code class="s1">'Peace'</code>,
  <code class="s1">'date_of_birth'</code>: u<code class="s1">'1 November 1878'</code>,
  <code class="s1">'date_of_death'</code>: u<code class="s1">'5 May 1959'</code>,
  <code class="s1">'gender'</code>: u<code class="s1">'male'</code>,
  <code class="s1">'link'</code>: u<code class="s1">'http://en.wikipedia.org/wiki/Carlos_Saavedra_Lamas'</code>,
  ...</pre>
<p>Things are looking good. With the exception of the <code>born_in</code> field, which is dependent on a name in the main Wikipedia Nobel Prize winners list having an asterisk, we’re getting all the data we were targeting. This dataset is now ready to be cleaned by pandas in the coming chapter.</p>
<p>Now that we’ve scraped our basic biographical data for the Nobel Prize winners, let’s go scrape our remaining targets, some biographical body text, and a picture of the great man or woman, where <span class="keep-together">available</span>.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Scrapy Pipelines" data-type="sect1"><div class="sect1" id="scrapy_pipelines">
<h1>Scrapy Pipelines</h1>
<p>In order to add a little personality to our Nobel Prize visualization, it would be good to have a little biographical text and an image of the winner.<a data-primary="Scrapy" data-secondary="chaining requests and yielding data" data-startref="ix_Scrpychainreq" data-tertiary="yielding requests" data-type="indexterm" id="idm45607783847472"/><a data-primary="requests (Scrapy), chaining" data-startref="ix_reqScrpy" data-type="indexterm" id="idm45607783845984"/><a data-primary="Scrapy" data-secondary="chaining requests and yielding data" data-startref="ix_Scrypchain" data-type="indexterm" id="idm45607783845072"/> Wikipedia’s biographical pages generally provide these things, so let’s go about scraping them.<a data-primary="Scrapy" data-secondary="pipelines" data-type="indexterm" id="idm45607783843648"/><a data-primary="pipelines (Scrapy)" data-secondary="defining" data-type="indexterm" id="idm45607783842704"/></p>
<p>Up to now, our scraped data has been text strings. In order to scrape images in their various formats, we need to use a Scrapy <em>pipeline</em>. <a href="https://oreil.ly/maUyE">Pipelines</a> provide a way of postprocessing the items we have <span class="keep-together">scraped</span>, and you can define any number of them.  You can write your own or take advantage of those already provided by Scrapy, such as the <code>ImagesPipeline</code> we’ll be using.<a data-primary="ImagesPipeline (Scrapy)" data-type="indexterm" id="idm45607783856816"/></p>
<p>In its simplest form, a pipeline need only define a <code>process_item</code> method. This receives the scraped items and the spider object. Let’s write a little pipeline to reject genderless Nobel Prize winners (so we can omit prizes given to organizations rather than individuals) using our existing <code>nwinners_full</code> spider to deliver the items. First, we add a <code>DropNonPersons</code> pipeline to the <code>pipelines.py</code> module of our project:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># nobel_winners/nobel_winners/pipelines.py</code><code>
</code><code>
</code><code class="c1"># Define your item pipelines here</code><code>
</code><code class="c1">#</code><code>
</code><code class="c1"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</code><code>
</code><code class="c1"># See: http://doc.scrapy.org/en/latest/topics/item-pipeline.xhtml</code><code>
</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">scrapy</code><code class="nn">.</code><code class="nn">exceptions</code><code> </code><code class="kn">import</code><code> </code><code class="n">DropItem</code><code>
</code><code>
</code><code>
</code><code class="k">class</code><code> </code><code class="nc">DropNonPersons</code><code class="p">(</code><code class="nb">object</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">""" Remove non-person winners """</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">process_item</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">item</code><code class="p">,</code><code> </code><code class="n">spider</code><code class="p">)</code><code class="p">:</code><code>
</code><code>        </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">item</code><code class="p">[</code><code class="s1">'</code><code class="s1">gender</code><code class="s1">'</code><code class="p">]</code><code class="p">:</code><code>               </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO6-1" id="co_heavyweight_scraping_with_scrapy_CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>            </code><code class="k">raise</code><code> </code><code class="n">DropItem</code><code class="p">(</code><code class="s2">"</code><code class="s2">No gender for </code><code class="si">%s</code><code class="s2">"</code><code class="o">%</code><code class="n">item</code><code class="p">[</code><code class="s1">'</code><code class="s1">name</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="n">item</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO6-2" id="co_heavyweight_scraping_with_scrapy_CO6-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO6-1" id="callout_heavyweight_scraping_with_scrapy_CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>If our scraped item failed to find a gender property at Wikidata, it is probably an organization such as the Red Cross. Our visualization is focused on individual winners, so here we use <code>DropItem</code> to remove the item from our output stream.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO6-2" id="callout_heavyweight_scraping_with_scrapy_CO6-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>We need to return the item to further pipelines or for saving by Scrapy.</p></dd>
</dl>
<p>As mentioned in the <code>pipelines.py</code> header, in order to add this pipeline to the spiders of our project, we need to register it in the <code>settings.py</code> module by adding it to a <code>dict</code> of pipelines <a data-primary="pipelines (Scrapy)" data-secondary="registering to settings.py module for the project" data-type="indexterm" id="idm45607783565328"/>and setting it to active (<code>1</code>):</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># nobel_winners/nobel_winners/settings.py</code>

<code class="n">BOT_NAME</code> <code class="o">=</code> <code class="s1">'nobel_winners'</code>
<code class="n">SPIDER_MODULES</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'nobel_winners.spiders'</code><code class="p">]</code>
<code class="n">NEWSPIDER_MODULE</code> <code class="o">=</code> <code class="s1">'nobel_winners.spiders'</code>

<code class="n">HTTPCACHE_ENABLED</code> <code class="o">=</code> <code class="kc">True</code>
<code class="n">ITEM_PIPELINES</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'nobel_winners.pipelines.DropNonPersons'</code><code class="p">:</code><code class="mi">300</code><code class="p">}</code></pre>
<p>Now that we’ve got the basic workflow for our pipelines, let’s add a useful one to our project.</p>
</div></section>
<section data-pdf-bookmark="Scraping Text and Images with a Pipeline" data-type="sect1"><div class="sect1" id="scraping_bio">
<h1>Scraping Text and Images with a Pipeline</h1>
<p>We now want to scrape the winners’ biographies and photos (see <a data-type="xref" href="#scrapy_targets">Figure 6-1</a>), where available.<a data-primary="images" data-secondary="scraping with a Scrapy pipeline" data-type="indexterm" id="ix_imgscrp"/><a data-primary="pipelines (Scrapy)" data-secondary="scraping text and images with a pipeline" data-type="indexterm" id="ix_pipeScrpy"/><a data-primary="Scrapy" data-secondary="scraping text and images with a pipeline" data-type="indexterm" id="ix_Scrpypipe"/> We can scrape the biographical text using the same method as our last spider, but the photos are best dealt with by an image pipeline.</p>
<p>We could easily write our own pipeline to take a scraped image URL, request it from Wikipedia, and save to disk, but to do it properly requires a bit of care. For example, we would like to avoid reloading an image that was recently downloaded or hasn’t changed in the meantime. Some flexibility in specifying where to store the images is a useful feature. It would also be good to have the option of converting the images into a common format (e.g., JPG or PNG) or of generating thumbnails. Luckily, Scrapy provides an <code>ImagesPipeline</code> object with all this functionality and more. This is  one of its <a href="https://oreil.ly/y9vAT">media pipelines</a>, which includes a <code>FilesPipeline</code> for dealing with general files.<a data-primary="ImagesPipeline (Scrapy)" data-type="indexterm" id="idm45607783945952"/><a data-primary="FilesPipeline (Scrapy)" data-type="indexterm" id="idm45607783945248"/></p>
<p>We could add the image and biography-text scraping to our existing <code>nwinners_full</code> spider, but that’s starting to get a little large, and segregating this character data from the more formal categories makes sense. So we’ll create a new spider called <code>nwinners_minibio</code> that will reuse parts of the previous spider’s <code>parse</code> method in order to loop through the Nobel winners.</p>
<p>As usual, when creating a Scrapy spider, our first job is to get the xpaths for our scraping targets—​in this case, where available that’s the first part of the winners’ biographical text and a photograph of them.<a data-primary="xpath" data-secondary="getting xpaths for scraping targets" data-type="indexterm" id="idm45607783996896"/> To do this, we fire up Chrome Elements and explore the HTML source of the biography pages looking for the targets shown in <a data-type="xref" href="#scrapy_crick">Figure 6-8</a>.</p>
<figure><div class="figure" id="scrapy_crick">
<img alt="dpj2 0608" height="650" src="assets/dpj2_0608.png" width="879"/>
<h6><span class="label">Figure 6-8. </span>The target elements for our biography scraping: the first part of the biography (A) marked by a stop point (B), and the winner’s photograph (C)</h6>
</div></figure>
<div data-type="example" id="scrapy_bio_paras">
<h5><span class="label">Example 6-6. </span>Scraping the biographical text</h5>
<pre data-code-language="html" data-type="programlisting"><code class="p">&lt;</code><code class="nt">div</code> <code class="na">id</code><code class="o">=</code><code class="s">"mw-content-text"</code><code class="p">&gt;</code>
  <code class="p">&lt;</code><code class="nt">div</code> <code class="na">class</code><code class="o">=</code><code class="s">"mw-parser-output"</code><code class="p">&gt;</code>
    ...
    <code class="p">&lt;</code><code class="nt">table</code> <code class="na">class</code><code class="o">=</code><code class="s">"infobox biography vcard"</code><code class="p">&gt;</code>...<code class="p">&lt;/</code><code class="nt">table</code><code class="p">&gt;</code>
    /* target paragraphs: */
    <code class="p">&lt;</code><code class="nt">p</code><code class="p">&gt;</code>...<code class="p">&lt;/</code><code class="nt">p</code><code class="p">&gt;</code>
    <code class="p">&lt;</code><code class="nt">p</code><code class="p">&gt;</code>...<code class="p">&lt;/</code><code class="nt">p</code><code class="p">&gt;</code>
    <code class="p">&lt;</code><code class="nt">p</code><code class="p">&gt;</code>...<code class="p">&lt;/</code><code class="nt">p</code><code class="p">&gt;</code>
    <code class="p">&lt;</code><code class="nt">div</code> <code class="na">id</code><code class="o">=</code><code class="s">"toc"</code><code class="p">&gt;</code>...<code class="p">&lt;/</code><code class="nt">div</code><code class="p">&gt;</code>
  ...
  <code class="p">&lt;/</code><code class="nt">div</code><code class="p">&gt;</code>
<code class="p">&lt;/</code><code class="nt">div</code><code class="p">&gt;</code></pre></div>
<p>Investigating with Chrome Elements (see <a data-type="xref" href="#scrapy_bio_paras">Example 6-6</a>) shows the biographical text (<a data-type="xref" href="#scrapy_crick">Figure 6-8</a> A) is contained in child paragraphs of the <code>div</code> with class <code>mw-parser-output</code>, which is a child of the <code>div</code> with ID <code>mw-content-text</code>. The paragraphs are sandwiched between a <code>table</code> with class <code>infobox</code> and a table-of-contents <code>div</code> with ID <code>toc</code>. <a data-primary="selectors (xpath)" data-secondary="crafting for biographical scraping" data-type="indexterm" id="idm45607783482400"/><a data-primary="regular expressions" data-secondary="using in xpath selector" data-type="indexterm" id="idm45607783481424"/>We can use the xpath <code>following-sibling</code> and <code>preceding-sibling</code> operators to craft a selector that captures the target paragraphs:</p>
<pre data-code-language="python" data-type="programlisting"><code>  </code><code class="n">ps</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code>\
</code><code>    </code><code class="s1">'</code><code class="s1">//*[@id=</code><code class="s1">"</code><code class="s1">mw-content-text</code><code class="s1">"</code><code class="s1">]/div/table/following-sibling::p</code><code class="s1">'</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO7-1" id="co_heavyweight_scraping_with_scrapy_CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>    </code><code class="s1">'</code><code class="s1">[not(preceding-sibling::div[@id=</code><code class="s1">"</code><code class="s1">toc</code><code class="s1">"</code><code class="s1">])]</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO7-2" id="co_heavyweight_scraping_with_scrapy_CO7-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO7-1" id="callout_heavyweight_scraping_with_scrapy_CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>All paragraphs following the first table in the child <code>div</code> of the <code>div</code> with ID <code>mw-content-text</code>.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO7-2" id="callout_heavyweight_scraping_with_scrapy_CO7-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Exclude (not) all paragraphs that have a preceding sibling <code>div</code> with ID <code>toc</code>.</p></dd>
</dl>
<p>Testing this with the Scrapy shell shows it consistently captures the Nobel winners’ mini-bios.</p>
<p>Further exploration of the winners’ pages shows that their photos (<a data-type="xref" href="#scrapy_crick">Figure 6-8</a> C) are contained in a table of class <code>infobox</code> and are the only image tags (<code>&lt;img&gt;</code>) in that table:</p>
<pre data-code-language="html" data-type="programlisting"><code class="p">&lt;</code><code class="nt">table</code> <code class="na">class</code><code class="o">=</code><code class="s">"infobox biography vcard"</code><code class="p">&gt;</code>
  ...
        <code class="p">&lt;</code><code class="nt">img</code> <code class="na">alt</code><code class="o">=</code><code class="s">"Francis Crick crop.jpg"</code> <code class="na">src</code><code class="o">=</code><code class="s">"//upload..."</code> <code class="p">/&gt;</code>
  ...
<code class="p">&lt;/</code><code class="nt">table</code><code class="p">&gt;</code></pre>
<p>The xpath <code>'//table[contains(@class,"infobox")]//img/@src</code> will get the source address of the image.</p>
<p>As with our first spider, we first need to declare a Scrapy <code>Item</code> to hold our scraped data. <a data-primary="Item class (Scrapy)" data-secondary="declaring Item to hold scraped data" data-type="indexterm" id="idm45607783357392"/>We’ll scrape the bio link and name of the winner, which we can use as identifiers for the image and text. We also need somewhere to store our <code>image-urls</code> (though we will only scrape one bio image, I’ll cover the multiple-image use case), the resultant images references (a file path), and a <code>bio_image</code> field to store the particular image we’re interested in:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">scrapy</code>
<code class="kn">import</code> <code class="nn">re</code>

<code class="n">BASE_URL</code> <code class="o">=</code> <code class="s1">'http://en.wikipedia.org'</code>


<code class="k">class</code> <code class="nc">NWinnerItemBio</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Item</code><code class="p">):</code>
    <code class="n">link</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">name</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">mini_bio</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">image_urls</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">bio_image</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Field</code><code class="p">()</code>
<code class="o">...</code></pre>
<p>Now we reuse the scraping loop over our Nobel Prize winners (see <a data-type="xref" href="#scrapy_yield">Example 6-4</a> for details), this time yielding a request to our new <code>get_mini_bio</code> method, which will scrape the image URLs and bio text:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">NWinnerSpiderBio</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Spider</code><code class="p">):</code>

    <code class="n">name</code> <code class="o">=</code> <code class="s1">'nwinners_minibio'</code>
    <code class="n">allowed_domains</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'en.wikipedia.org'</code><code class="p">]</code>
    <code class="n">start_urls</code> <code class="o">=</code> <code class="p">[</code>
        <code class="s2">"https://en.wikipedia.org/wiki/List_of_Nobel_"</code> \
        <code class="s2">"laureates_by_country"</code>
    <code class="p">]</code>

    <code class="k">def</code> <code class="nf">parse</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>

        <code class="n">filename</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">url</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'/'</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
        <code class="n">h3s</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'//h3'</code><code class="p">)</code>

        <code class="k">for</code> <code class="n">h3</code> <code class="ow">in</code> <code class="n">h3s</code><code class="p">:</code>
            <code class="n">country</code> <code class="o">=</code> <code class="n">h3</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'span[@class="mw-headline"]'</code>\
            <code class="s1">'text()'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">()</code>
            <code class="k">if</code> <code class="n">country</code><code class="p">:</code>
                <code class="n">winners</code> <code class="o">=</code> <code class="n">h3</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'following-sibling::ol[1]'</code><code class="p">)</code>
                <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">winners</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'li'</code><code class="p">):</code>
                    <code class="n">wdata</code> <code class="o">=</code> <code class="p">{}</code>
                    <code class="n">wdata</code><code class="p">[</code><code class="s1">'link'</code><code class="p">]</code> <code class="o">=</code> <code class="n">BASE_URL</code> <code class="o">+</code> \
                    <code class="n">w</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code class="s1">'a/@href'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">()[</code><code class="mi">0</code><code class="p">]</code>
                    <code class="c1"># Process the winner's bio page with</code>
                    <code class="c1"># the get_mini_bio method</code>
                    <code class="n">request</code> <code class="o">=</code> <code class="n">scrapy</code><code class="o">.</code><code class="n">Request</code><code class="p">(</code><code class="n">wdata</code><code class="p">[</code><code class="s1">'link'</code><code class="p">],</code>
                                  <code class="n">callback</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">get_mini_bio</code><code class="p">)</code>
                    <code class="n">request</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s1">'item'</code><code class="p">]</code> <code class="o">=</code> <code class="n">NWinnerItemBio</code><code class="p">(</code><code class="o">**</code><code class="n">wdata</code><code class="p">)</code>
                    <code class="k">yield</code> <code class="n">request</code></pre>
<p>Our <code>get_mini_bio</code> method will add any available photo URLs to the <code>image_urls</code> list and add all paragraphs of the biography up to the <code>&lt;p&gt;&lt;/p&gt;</code> stop point to the item’s <code>mini_bio</code> field:</p>
<pre data-code-language="python" data-type="programlisting"><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">get_mini_bio</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">response</code><code class="p">)</code><code class="p">:</code><code>
</code><code>        </code><code class="sd">""" Get the winner's bio text and photo """</code><code>
</code><code>
</code><code>        </code><code class="n">BASE_URL_ESCAPED</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">http:</code><code class="s1">\</code><code class="s1">/</code><code class="s1">\</code><code class="s1">/en.wikipedia.org</code><code class="s1">'</code><code>
</code><code>        </code><code class="n">item</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">meta</code><code class="p">[</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>        </code><code class="n">item</code><code class="p">[</code><code class="s1">'</code><code class="s1">image_urls</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code>        </code><code class="n">img_src</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code>\
</code><code>            </code><code class="s1">'</code><code class="s1">//table[contains(@class,</code><code class="s1">"</code><code class="s1">infobox</code><code class="s1">"</code><code class="s1">)]//img/@src</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO8-1" id="co_heavyweight_scraping_with_scrapy_CO8-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>        </code><code class="k">if</code><code> </code><code class="n">img_src</code><code class="p">:</code><code>
</code><code>            </code><code class="n">item</code><code class="p">[</code><code class="s1">'</code><code class="s1">image_urls</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">http:</code><code class="s1">'</code><code> </code><code class="o">+</code><code>\
</code><code>             </code><code class="n">img_src</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code class="p">]</code><code>
</code><code>
</code><code>        </code><code class="n">ps</code><code> </code><code class="o">=</code><code> </code><code class="n">response</code><code class="o">.</code><code class="n">xpath</code><code class="p">(</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">//*[@id=</code><code class="s1">"</code><code class="s1">mw-content-text</code><code class="s1">"</code><code class="s1">]/div/table/</code><code class="s1">'</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">following-sibling::p[not(preceding-sibling::div[@id=</code><code class="s1">"</code><code class="s1">toc</code><code class="s1">"</code><code class="s1">])]</code><code class="s1">'</code><code class="p">)</code><code>\
</code><code>            </code><code class="o">.</code><code class="n">extract</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO8-2" id="co_heavyweight_scraping_with_scrapy_CO8-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>        </code><code class="c1"># Concatenate the biography paragraphs for a mini_bio string</code><code>
</code><code>        </code><code class="n">mini_bio</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">'</code><code>
</code><code>        </code><code class="k">for</code><code> </code><code class="n">p</code><code> </code><code class="ow">in</code><code> </code><code class="n">ps</code><code class="p">:</code><code>
</code><code>            </code><code class="n">mini_bio</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="n">p</code><code>
</code><code>        </code><code class="c1"># correct for wiki-links</code><code>
</code><code>        </code><code class="n">mini_bio</code><code> </code><code class="o">=</code><code> </code><code class="n">mini_bio</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s1">'</code><code class="s1">href=</code><code class="s1">"</code><code class="s1">/wiki</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">href=</code><code class="s1">"</code><code class="s1">'</code><code>
</code><code>                       </code><code class="o">+</code><code> </code><code class="n">BASE_URL</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="s1">/wiki</code><code class="s1">"</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO8-3" id="co_heavyweight_scraping_with_scrapy_CO8-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>        </code><code class="n">mini_bio</code><code> </code><code class="o">=</code><code> </code><code class="n">mini_bio</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s1">'</code><code class="s1">href=</code><code class="s1">"</code><code class="s1">#</code><code class="s1">'</code><code class="p">,</code><code>\
</code><code>         </code><code class="s1">'</code><code class="s1">href=</code><code class="s1">"</code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="n">item</code><code class="p">[</code><code class="s1">'</code><code class="s1">link</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="s1">#</code><code class="s1">"</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>        </code><code class="n">item</code><code class="p">[</code><code class="s1">'</code><code class="s1">mini_bio</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">mini_bio</code><code>
</code><code>        </code><code class="k">yield</code><code> </code><code class="n">item</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO8-1" id="callout_heavyweight_scraping_with_scrapy_CO8-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Targets the first (and only) image in the table of class <code>infobox</code> and gets its source (<code>src</code>) attribute (e.g., <code>&lt;img src='//upload.wikime⁠dia.org/​.../Max_Perutz.jpg'...</code>).</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO8-2" id="callout_heavyweight_scraping_with_scrapy_CO8-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Grab our mini-bio paragraphs in a sibling sandwich.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO8-3" id="callout_heavyweight_scraping_with_scrapy_CO8-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Replaces Wikipedia’s internal hrefs (e.g., <em>/wiki/…​</em>) with the full addresses our visualization will need.</p></dd>
</dl>
<p>With our bio-scraping spider defined, we need to create its complementary pipeline, which will take the image URLs scraped and convert them into saved images.<a data-primary="pipelines (Scrapy)" data-secondary="scraping images with image pipeline" data-type="indexterm" id="idm45607782825520"/> We’ll use Scrapy’s <a href="https://oreil.ly/MqUuX">images pipeline</a> for this job.</p>
<p>The <code>ImagesPipeline</code> shown in <a data-type="xref" href="#scrapy_images_pipeline">Example 6-7</a> has two main methods, <code>get_media_requests</code>, which generates the requests for the image URLs, and <code>item_completed</code>, called after the requests have been <span class="keep-together">consumed</span>.</p>
<div data-type="example" id="scrapy_images_pipeline">
<h5><span class="label">Example 6-7. </span>Scraping images with the image pipeline</h5>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code><code> </code><code class="nn">scrapy</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">itemadapter</code><code> </code><code class="kn">import</code><code> </code><code class="n">ItemAdapter</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">scrapy</code><code class="nn">.</code><code class="nn">pipelines</code><code class="nn">.</code><code class="nn">images</code><code> </code><code class="kn">import</code><code> </code><code class="n">ImagesPipeline</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">scrapy</code><code class="nn">.</code><code class="nn">exceptions</code><code> </code><code class="kn">import</code><code> </code><code class="n">DropItem</code><code>
</code><code>
</code><code class="k">class</code><code> </code><code class="nc">NobelImagesPipeline</code><code class="p">(</code><code class="n">ImagesPipeline</code><code class="p">)</code><code class="p">:</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">get_media_requests</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">item</code><code class="p">,</code><code> </code><code class="n">info</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO9-1" id="co_heavyweight_scraping_with_scrapy_CO9-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code>        </code><code class="k">for</code><code> </code><code class="n">image_url</code><code> </code><code class="ow">in</code><code> </code><code class="n">item</code><code class="p">[</code><code class="s1">'</code><code class="s1">image_urls</code><code class="s1">'</code><code class="p">]</code><code class="p">:</code><code>
</code><code>            </code><code class="k">yield</code><code> </code><code class="n">scrapy</code><code class="o">.</code><code class="n">Request</code><code class="p">(</code><code class="n">image_url</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">item_completed</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">results</code><code class="p">,</code><code> </code><code class="n">item</code><code class="p">,</code><code> </code><code class="n">info</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO9-2" id="co_heavyweight_scraping_with_scrapy_CO9-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>
</code><code>        </code><code class="n">image_paths</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">img</code><code class="p">[</code><code class="s1">'</code><code class="s1">path</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="k">for</code><code> </code><code class="n">ok</code><code class="p">,</code><code> </code><code class="n">img</code><code> </code><code class="ow">in</code><code> </code><code class="n">results</code><code> </code><code class="k">if</code><code> </code><code class="n">ok</code><code class="p">]</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO9-3" id="co_heavyweight_scraping_with_scrapy_CO9-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>
</code><code>        </code><code class="k">if</code><code> </code><code class="ow">not</code><code> </code><code class="n">image_paths</code><code class="p">:</code><code>
</code><code>            </code><code class="k">raise</code><code> </code><code class="n">DropItem</code><code class="p">(</code><code class="s2">"</code><code class="s2">Item contains no images</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>        </code><code class="n">adapter</code><code> </code><code class="o">=</code><code> </code><code class="n">ItemAdapter</code><code class="p">(</code><code class="n">item</code><code class="p">)</code><code> </code><a class="co" href="#callout_heavyweight_scraping_with_scrapy_CO9-4" id="co_heavyweight_scraping_with_scrapy_CO9-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code>
</code><code>        </code><code class="n">adapter</code><code class="p">[</code><code class="s1">'</code><code class="s1">bio_image</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">image_paths</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>
</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="n">item</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO9-1" id="callout_heavyweight_scraping_with_scrapy_CO9-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>This takes any image URLs scraped by our <em>nwinners_minibio</em> spider and generates an HTTP request for their content.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO9-2" id="callout_heavyweight_scraping_with_scrapy_CO9-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>After the image URL requests have been made, the results are delivered to the <code>item_completed</code> method.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO9-3" id="callout_heavyweight_scraping_with_scrapy_CO9-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>This Python list comprehension filters the list of result tuples (of form <code>[(True, Image), (False, Image) …​]</code>) for those that were successful and stores their file paths relative to the directory specified by the <code>IMAGES_STORE</code> variable in <code>settings.py</code>.</p></dd>
<dt><a class="co" href="#co_heavyweight_scraping_with_scrapy_CO9-4" id="callout_heavyweight_scraping_with_scrapy_CO9-4"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p>We use a Scrapy <a href="https://oreil.ly/8P6uq">item adapter</a>, which provides a common interface for working with supported item types.</p></dd>
</dl></div>
<p>Now that we have the spider and pipeline defined, we just need to add the pipeline to our <code>settings.py</code> module and set the <code>IMAGES_STORE</code> variable to the directory we want to save the images in:</p>
<pre data-code-language="python" data-type="programlisting"><code class="c1"># nobel_winners/nobel_winners/settings.py</code>

<code class="o">...</code>
<code class="n">ITEM_PIPELINES</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'nobel_winners.pipelines'</code>\
                  <code class="s1">'.NobelImagesPipeline'</code><code class="p">:</code><code class="mi">300</code><code class="p">}</code>
<code class="n">IMAGES_STORE</code> <code class="o">=</code> <code class="s1">'images'</code></pre>
<p>Let’s run our new spider from the <em>nobel_winners</em> root directory of our project, and check its output:</p>
<pre data-code-language="bash" data-type="programlisting">$ scrapy crawl nwinners_minibio -o minibios.json
...
<code class="m">2021</code>-12-13 <code class="m">17</code>:18:05 <code class="o">[</code>scrapy.core.scraper<code class="o">]</code> DEBUG: Scraped from
    &lt;<code class="m">200</code> https://en.wikipedia.org/wiki/C%C3%A9sar_Milstein&gt;

<code class="o">{</code><code class="s1">'bio_image'</code>: <code class="s1">'full/65ac9541c305ab4728ed889385d422a2321a117d.jpg'</code>,
 <code class="s1">'image_urls'</code>: <code class="o">[</code><code class="s1">'http://upload.wikimedia...</code>
<code class="s1">          150px-Milstein_lnp_restauraci%C3%B3n.jpg'</code><code class="o">]</code>,
 <code class="s1">'link'</code>: <code class="s1">'http://en.wikipedia.org/wiki/C%C3%A9sar_Milstein'</code>,
 <code class="s1">'mini_bio'</code>: <code class="s1">'&lt;p&gt;&lt;b&gt;César Milstein&lt;/b&gt;, &lt;a ...'</code>
             <code class="s1">'href="http://en.wikipedia.org/wiki/Order_of_the_...'</code>
             <code class="s1">'title="Order of the Companions of Honour"&gt;CH&lt;/a&gt;'</code>
             <code class="s1">'href="http://en.wikipedia.org/wiki/Royal_Society'</code>
             <code class="s1">'Society"&gt;FRS&lt;/a&gt;&lt;sup id="cite_ref-frs_2-1" class'</code>...&gt;
             <code class="s1">'href="http://en.wikipedia.org/wiki/C%C3%A9sar_Mi'</code>
             <code class="s1">'(8 October 1927 – 24 March 2002) was an &lt;a ...&gt;'</code>
             <code class="s1">'href="http://en.wikipedia.org/wiki/Argentine" '</code>

...</pre>
<p>The spider is correctly harvesting mini-bios and, using its image pipeline, photos of the Nobel winners.
The image was stored in <code>image_urls</code> and successfully processed, loading the JPG file stored in the <em>images</em> directory we specified with <code>IMAGE_STORE</code> with a relative path (<code>full/a5f763b828006e704cb291411b8b643bfb1886c.jpg</code>). The filename is, conveniently enough, a <a href="https://oreil.ly/SlSl2">SHA1 hash</a> of the image’s URL, which allows the image pipeline to check for existing images, enabling it to prevent redundant requests.</p>
<p>A quick listing of our images directory shows a nice array of Wikipedia Nobel Prize winner images, ready to be used in our web <span class="keep-together">visualization</span>:</p>
<pre data-code-language="bash" data-type="programlisting">$ <code class="o">(</code>nobel_winners<code class="o">)</code> tree images
images
└── full
    ├── 0512ae11141584da1262661992a1b05dfb20dd52.jpg
    ├── 092a92689118c16b15b1613751af422439df2850.jpg
    ├── 0b6a8ca56e6ff115b7d30087df9c21da09684db1.jpg
    ├── 1197aa95299a1fec983b3dbdeaeb97a1f7e545c9.jpg
    ├── 1f6fb8e9e2241733da47328291b25bd1a78fa588.jpg
    ├── 272cf1b089c7a28ea0109ad8655bc3ef1c03fb52.jpg
    ├── 28dcc7978d9d5710f0c29d6dfcf09caa7e13a1d0.jpg
    ...</pre>
<p>As we’ll see in <a data-type="xref" href="ch16.xhtml#chapter_building_viz">Chapter 16</a>, we will be placing these in the <em>static</em> folder of our web app, ready to be accessed via the winner’s <code>bio_image</code> field.</p>
<p>With our images and biography text to hand, we’ve successfully scraped all the targets we set ourselves at the beginning of the chapter (see <a data-type="xref" href="#scrapy_target_JSON">Example 6-1</a> and <a data-type="xref" href="#scrapy_targets">Figure 6-1</a>). Now, it’s time for a quick summary before moving on to clean this inevitably dirty data with help from pandas.<a data-primary="images" data-secondary="scraping with a Scrapy pipeline" data-startref="ix_imgscrp" data-type="indexterm" id="idm45607782496224"/></p>
<section data-pdf-bookmark="Specifying Pipelines with Multiple Spiders" data-type="sect2"><div class="sect2" id="idm45607782494912">
<h2>Specifying Pipelines with Multiple Spiders</h2>
<p>The pipelines enabled in <code>settings.py</code> are applied to all spiders in our Scrapy project. <a data-primary="pipelines (Scrapy)" data-secondary="scraping text and images with a pipeline" data-tertiary="specifying pipelines with multiple spiders" data-type="indexterm" id="idm45607782469936"/><a data-primary="spiders (Scrapy)" data-secondary="specifying pipelines with multiple spiders" data-type="indexterm" id="idm45607782468752"/><a data-primary="Scrapy" data-secondary="scraping text and images with a pipeline" data-tertiary="specifying pipelines with multiple spiders" data-type="indexterm" id="idm45607782467776"/>Often, if you have a number of spiders, you’ll want to be able to specify which pipelines are applied on a spider-by-spider basis. There are a <a href="https://oreil.ly/62Uzn">number of ways</a> to achieve this, but the best I’ve seen is to use the spiders’ <code>custom_settings</code> class property to set the <code>ITEM_PIPELINES</code> dictionary instead of setting it in <code>settings.py</code>. In the case of our <code>nwinners_minibio</code> spider, this means adapting the <code>NWinnerSpiderBio</code> class like so:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">NWinnerSpiderBio</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Spider</code><code class="p">):</code>
    <code class="n">name</code> <code class="o">=</code> <code class="s1">'nwinners_minibio'</code>
    <code class="n">allowed_domains</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'en.wikipedia.org'</code><code class="p">]</code>
    <code class="n">start_urls</code> <code class="o">=</code> <code class="p">[</code>
      <code class="s2">"http://en.wikipedia.org/wiki"</code>\
      <code class="s2">"List_of_Nobel_laureates_by_country"</code>
    <code class="p">]</code>

    <code class="n">custom_settings</code> <code class="o">=</code> <code class="p">{</code>
        <code class="s1">'ITEM_PIPELINES'</code><code class="p">:</code>\
        <code class="p">{</code><code class="s1">'nobel_winners.pipelines.NobelImagesPipeline'</code><code class="p">:</code><code class="mi">1</code><code class="p">}</code>
    <code class="p">}</code>

    <code class="c1"># ...</code></pre>
<p>Now the <code>NobelImagesPipeline</code> pipeline will only be applied while scraping the Nobel Prize winners’ biographies.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45607783966224">
<h1>Summary</h1>
<p>In this chapter, we produced two Scrapy spiders that managed to grab the simple statistical dataset of our Nobel Prize winners plus some biographical text (and, where available, a photograph, to add some color to the stats). Scrapy is a powerful library that takes care of everything you could need in a full-fledged scraper.<a data-primary="pipelines (Scrapy)" data-secondary="scraping text and images with a pipeline" data-startref="ix_pipeScrpy" data-type="indexterm" id="idm45607782360912"/><a data-primary="Scrapy" data-secondary="scraping text and images with a pipeline" data-startref="ix_Scrpypipe" data-type="indexterm" id="idm45607782359696"/> Although the workflow requires more effort to implement than doing some hacking with Beautiful Soup, Scrapy has far more power and comes into its own as your scraping ambitions increase. All Scrapy spiders follow the standard recipe demonstrated here, and the workflow should become routine after you program a few.</p>
<p>I hope this chapter has conveyed the rather hacky, iterative nature of scraping, and some of the quiet satisfaction that can be had when producing relatively clean data from the unpromising mound of stuff so often found on the web. The fact is that now and for the foreseeable future, the large majority of interesting data (the fuel for the art and science of data visualization) is trapped in a form that is unusable for the web-based visualizations that this book focuses on. Scraping is, in this sense, an emancipating endeavor.</p>
<p>The data we scraped, much of it human-edited, will certainly have some errors—​from badly formatted dates to categorical anomalies to missing fields. Making that data presentable is the focus of the next pandas-based chapters. But first, we need a little introduction to pandas and its building block, NumPy.<a data-primary="Scrapy" data-startref="ix_Scrpy" data-type="indexterm" id="idm45607782357584"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45607786166832"><sup><a href="ch06.xhtml#idm45607786166832-marker">1</a></sup> See <a href="https://oreil.ly/LamAt">the Scrapy install docs</a> for platform-specific details.</p><p data-type="footnote" id="idm45607785183776"><sup><a href="ch06.xhtml#idm45607785183776-marker">2</a></sup> There are some handy online tools for testing regexes, some of them programming-language-specific. <a href="http://www.pyregex.com">Pyregex</a> is a good Python one, with a handy cheat sheet included.</p><p data-type="footnote" id="idm45607784791328"><sup><a href="ch06.xhtml#idm45607784791328-marker">3</a></sup> The author got stung by this removal.</p><p data-type="footnote" id="idm45607784789392"><sup><a href="ch06.xhtml#idm45607784789392-marker">4</a></sup> See <a href="https://oreil.ly/pLVcE">Wikipedia</a> for an explanation.</p><p data-type="footnote" id="idm45607784684416"><sup><a href="ch06.xhtml#idm45607784684416-marker">5</a></sup> Strictly speaking, there are edits being made continually by the Wikipedia community, but the fundamental details should be stable until the next set of prizes.</p><p data-type="footnote" id="idm45607784643296"><sup><a href="ch06.xhtml#idm45607784643296-marker">6</a></sup> See <a href="https://oreil.ly/qgku4">Jeff Knupp’s blog, “Everything I Know About Python”</a>, for a nice rundown of Python generators and the use of <code>yield</code>.</p></div></div></section></div></body></html>