- en: Chapter 15\. Linear Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point in the book, we’ve covered the four stages of the data science
    lifecycle to different extents. We’ve talked about formulating questions and obtaining
    and cleaning data, and we’ve used exploratory data analysis to better understand
    the data. In this chapter, we extend the constant model introduced in [Chapter 4](ch04.html#ch-modeling)
    to the *linear model*. Linear models are a popular tool in the last stage of the
    lifecycle: understanding the world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing how to fit linear models opens the door to all kinds of useful data
    analyses. We can use these models to make *predictions*—for example, environmental
    scientists developed a linear model to predict air quality based on air sensor
    measurements and weather conditions (see [Chapter 12](ch12.html#ch-pa)). In that
    case study, understanding how measurements from two instruments varied enabled
    us to calibrate inexpensive sensors and improve their air quality readings. We
    can also use these models to make *inferences* about the form of a relationship
    between features—for example, in [Chapter 18](ch18.html#ch-donkey) we’ll see how
    veterinarians used a linear model to infer the coefficients for length and girth
    for a donkey’s weight: <math><mi>L</mi> <mi>e</mi> <mi>n</mi> <mi>g</mi> <mi>t</mi>
    <mi>h</mi>  <mo>+</mo>  <mn>2</mn> <mo>×</mo> <mi>G</mi> <mi>i</mi> <mi>r</mi>
    <mi>t</mi> <mi>h</mi>  <mo>−</mo>  <mn>175</mn></math> . In that case study, the
    model enables vets working in the field to prescribe medication for sick donkeys.
    Models can also help *describe relationships* and provide insights—for example,
    in this chapter we explore relationships between factors correlated with upward
    mobility, such as commute time, income inequality, and the quality of K–12 education.
    We carry out a descriptive analysis that follows an analysis social scientists
    have used to shape public conversation and inform policy recommendations.'
  prefs: []
  type: TYPE_NORMAL
- en: We start by describing the simple linear model, which summarizes the relationship
    between two features with a line. We explain how to fit this line to data using
    the loss minimization approach introduced in [Chapter 4](ch04.html#ch-modeling).
    Then we introduce the multiple linear model, which models one feature using multiple
    other features. To fit such a model, we use linear algebra and reveal the geometry
    behind fitting a linear model with squared error loss. Finally, we cover feature
    engineering techniques that let us include categorical features and transformed
    features when building models.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Linear Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like with the constant model, our goal is to approximate the signal in a feature
    by a constant. Now we have additional information from a second feature to help
    us. In short, we want to use information from a second feature to make a better
    model than the constant model. For example, we might describe the sale price of
    a house by its size or predict a donkey’s weight from its length. In each of these
    examples, we have an *outcome* feature (sale price, weight) that we want to explain,
    describe, or predict with the help of an *explanatory* feature (house size, length).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We use *outcome* to refer to the feature that we are trying to model and *explanatory*
    for the feature that we are using to explain the outcome. Different fields have
    adopted conventions for describing this relationship. Some call the outcome the
    dependent variable and the explanatory the independent variable. Others use response
    and covariate; regress and regressor; explained and explanatory; endogenous and
    exogenous. In machine learning, *target* and *features* or *predicted* and *predictors*
    are common. Unfortunately, many of these pairs connote a causal relationship.
    The notion of explaining or predicting is not necessarily meant to imply that
    one causes the other. Particularly confusing is the independent-dependent usage,
    and we recommend avoiding it.
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible model we might use is a line. Mathematically, that means we have
    an intercept, <math><msub><mi>θ</mi> <mn>0</mn></msub></math> , and a slope, <math><msub><mi>θ</mi>
    <mn>1</mn></msub></math> , and we use the explanatory feature <math><mi>x</mi></math>
    to approximate the outcome, <math><mi>y</mi></math> , by a point on the line:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>y</mi> <mo>≈</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: As <math><mi>x</mi></math> changes, the estimate for <math><mi>y</mi></math>
    changes but still falls on the line. Typically, the estimate isn’t perfect, and
    there is some error in using the model; that’s why we use the symbol <math><mo>≈</mo></math>
    to mean “approximately.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To find a line that does a good job of capturing the signal in the outcome,
    we use the same approach introduced in [Chapter 4](ch04.html#ch-modeling) and
    minimize the average squared loss. Specifically, we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the errors: <math><msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo> <mo>,</mo>  <mi>i</mi>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mi>n</mi></math>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Square the errors (i.e., use squared loss): <math><mo stretchy="false">[</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo>
    <mn>2</mn></msup></math>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Calculate the average loss over the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mn>1</mn> <mi>n</mi></mfrac> <munder><mo>∑</mo>
    <mrow><mi>i</mi></mrow></munder> <mo stretchy="false">[</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To fit the model, we find the slope and intercept that give us the smallest
    average loss; in other words, we minimize the *mean squared error*, or MSE for
    short. We call the minimizing values for the intercept and slope <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub></math> and <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the errors we calculate in step 1 are measured in the vertical direction,
    meaning for a specific <math><mi>x</mi></math> , the error is the vertical distance
    between the data point <math><mo stretchy="false">(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo stretchy="false">)</mo></math> and the point on the line <math><mo
    stretchy="false">(</mo> <mi>x</mi> <mo>,</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo stretchy="false">)</mo></math>
    . [Figure 15-1](#fig-slr) shows this notion. On the left is a scatterplot of points
    with a line used to estimate <math><mi>y</mi></math> from <math><mi>x</mi></math>
    . We have marked two specific points by squares and their corresponding approximations
    on the line by diamonds. The dotted segment from the actual point to the line
    shows the error. The plot on the right is a scatterplot of all the errors; for
    reference, we marked the errors corresponding to the two square points in the
    left plot with squares in the right plot as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15-1\. On the left is a scatterplot of <math><mo stretchy="false">(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo stretchy="false">)</mo></math> pairs and a line that we use to estimate <math><mi>y</mi></math>
    from <math><mi>x</mi></math> . Two specific points are represented by squares
    and their estimates by diamonds. On the right is a scatterplot of the errors:
    <math><msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math> .'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Later in this chapter, we derive the values <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub></math> and <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub></math> that minimize
    the mean squared error. We show that these are:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub></mtd> <mtd><mo>=</mo>
    <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow> <mo>−</mo>
    <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub>
    <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow></mtd></mtr>
    <mtr><mtd><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub></mtd> <mtd><mo>=</mo> <mi>r</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo>,</mo> <mrow><mrow><mi
    mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo> <mfrac><mrow><mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo></mrow> <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, <math><mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow></math> represents
    the values <math><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo>
    <msub><mi>x</mi> <mi>n</mi></msub></math> and <math><mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow></math>
    is similarly defined; <math><mi>r</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi
    mathvariant="bold">x</mi></mrow></mrow> <mo>,</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo></math> is the correlation coefficient of the <math><mo
    stretchy="false">(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo></math> pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting the two together, the equation for the line becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub> <mo>+</mo> <msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub> <mi>x</mi></mtd>
    <mtd><mo>=</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>+</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <mi>x</mi></mtd></mtr> <mtr><mtd><mo>=</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo>+</mo> <mi>r</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo>,</mo> <mrow><mrow><mi
    mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo> <mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo> <mfrac><mrow><mo stretchy="false">(</mo> <mi>x</mi>
    <mo>−</mo> <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mrow> <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This equation has a nice interpretation: for a given <math><mi>x</mi></math>
    value, we find how many standard deviations above (or below) average it is, and
    then we predict (or explain, depending on the setting) <math><mi>y</mi></math>
    to be <math><mi>r</mi></math> times as many standard deviations above (or below)
    its average.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We see from the expression for the optimal line that the *sample correlation
    coefficient* plays an important role. Recall that <math><mi>r</mi></math> measures
    the strength of the linear association and is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>r</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi
    mathvariant="bold">x</mi></mrow></mrow> <mo>,</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo> <mo>=</mo> <munder><mo>∑</mo> <mi>i</mi></munder>
    <mfrac><mrow><mo stretchy="false">(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>−</mo>
    <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow> <mo stretchy="false">)</mo></mrow>
    <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow>
    <mo stretchy="false">)</mo></mrow></mfrac> <mfrac><mrow><mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mrow> <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few important features of the correlation that help us fit linear
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '<math><mi>r</mi></math> is unitless. Notice that <math><mi>x</mi></math> ,
    <math><mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow></math>
    , and <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi
    mathvariant="bold">x</mi></mrow></mrow> <mo stretchy="false">)</mo></math> all
    have the same units, so the following ratio has no units (and likewise for the
    terms involving <math><msub><mi>y</mi> <mi>i</mi></msub></math> ):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mrow><mo stretchy="false">(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mrow> <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <math><mi>r</mi></math> is between <math><mo>−</mo> <mn>1</mn></math> and <math><mo>+</mo>
    <mn>1</mn></math> . Only when all of the points fall exactly along a line is the
    correlation either <math><mo>+</mo> <mn>1</mn></math> or <math><mo>−</mo> <mn>1</mn></math>
    , depending on whether the slope of the line is positive or negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math><mi>r</mi></math> measures the strength of a linear association, not whether
    or not the data have a linear association. The four scatterplots in [Figure 15-2](#fig-anscombequartet)
    all have the same correlation coefficient of about <math><mn>0.8</mn></math> (as
    well as the same averages and standard deviations), but only one plot, the one
    on the top left, has what we think of as a linear association with random errors
    about the line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/leds_1502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-2\. These four sets of points, known as Anscombe’s quartet, have the
    same correlation of 0.8, and the same means and standard deviations. The plot
    in the top left exhibits a linear association; top right shows a perfect nonlinear
    association; bottom left, with the exception of one point, is a perfect linear
    association; and bottom right, with the exception of one point, has no association.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Again, we do not expect the pairs of data points to fall exactly along a line,
    but we do expect the scatter of points to be reasonably described by the line,
    and we expect the deviations between <math><msub><mi>y</mi> <mi>i</mi></msub></math>
    and the estimate <math><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>0</mn></msub> <mo>+</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub></math> to be roughly symmetrically
    distributed about the line with no apparent patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models were introduced in [Chapter 12](ch12.html#ch-pa), where we used
    the relationship between measurements from high-quality air monitors operated
    by the Environmental Protection Agency and neighboring inexpensive air quality
    monitors to calibrate the inexpensive monitors for more accurate predictions.
    We revisit that example to make the notion of a simple linear model more concrete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: A Simple Linear Model for Air Quality'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall from [Chapter 12](ch12.html#ch-pa) that our aim is to use air quality
    measurements from the accurate Air Quality System (AQS) sensors operated by the
    US government to predict the measurements made by PurpleAir (PA) sensors. The
    pairs of data values come from neighboring instruments that measure the average
    daily concentration of particulate matter in the air on the same day. (The unit
    of measurement is an average count of particles under 2.5 mm in size per cubic
    liter of air in a 24-hour period.) In this section, we focus on air quality measurements
    at one location in Georgia. These are a subset of the data we examined in the
    case study in [Chapter 12](ch12.html#ch-pa). The measurements are daily averages
    from August 2019 to mid-November 2019:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | date | id | region | pm25aqs | pm25pa |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **5258** | 2019-08-02 | GA1 | Southeast | 8.65 | 16.19 |'
  prefs: []
  type: TYPE_TB
- en: '| **5259** | 2019-08-03 | GA1 | Southeast | 7.70 | 13.59 |'
  prefs: []
  type: TYPE_TB
- en: '| **5260** | 2019-08-04 | GA1 | Southeast | 6.30 | 10.30 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **5439** | 2019-10-18 | GA1 | Southeast | 6.30 | 12.94 |'
  prefs: []
  type: TYPE_TB
- en: '| **5440** | 2019-10-21 | GA1 | Southeast | 7.50 | 13.62 |'
  prefs: []
  type: TYPE_TB
- en: '| **5441** | 2019-10-30 | GA1 | Southeast | 5.20 | 14.55 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The feature `pm25aqs` contains measurements from the AQS sensor and `pm25pa`
    from the PurpleAir monitor. Since we are interested in studying how well the AQS
    measurements predict the PurpleAir measurements, our scatterplot places PurpleAir
    readings on the y-axis and AQS readings on the x-axis. We also add a trend line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This scatterplot shows a linear relationship between the measurements from
    these two kinds of instruments. The model that we want to fit has the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>P</mi> <mi>A</mi> <mo>≈</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>A</mi> <mi>Q</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math><mi>P</mi> <mi>A</mi></math> refers to the PurpleAir average daily
    measurement and <math><mi>A</mi> <mi>Q</mi></math> to its partner AQS measurement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since `pandas.Series` objects have built-in methods to compute standard deviations
    (SDs) and correlation coefficients, we can quickly define functions that calculate
    the best-fitting line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can fit the model by computing <math><msub><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub></math> and <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub></math> for these
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This model matches the trend line shown in the scatterplot. That’s not by accident.
    The parameter value for `trendline` in the call to `scatter()` is `"ols"`, which
    stands for *ordinary least squares*, another name for fitting a linear model by
    minimizing squared error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the errors. First, we find the predictions for PA measurements
    given the AQS measurements, and then we calculate the errors—the difference between
    the actual PA measurements and the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot these errors against the predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in02.png)'
  prefs: []
  type: TYPE_IMG
- en: An error of 0 means that the actual measurement falls on the fitted line; we
    also call this line the *least squares line* or the *regression line*. A positive
    value means it is above the line, and negative means it’s below. You might be
    wondering how good this model is and what it says about our data. We consider
    these topics next.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting Linear Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original scatterplot of paired measurements shows that the PurpleAir recordings
    are often quite a bit higher than the more accurate AQS measurements. Indeed,
    the equation for our simple line model has a slope of about 2.1\. We interpret
    the slope to mean that a change of 1 ppm measured by the AQS monitor is associated
    with a change of 2 ppm in the PA measurement, on average. So, if on one day the
    AQS sensor measures 10 ppm and on the next day it is 5 ppm higher, namely 15 ppm,
    then our prediction for the PA measurement increases from one day to the next
    by <math><mn>2</mn> <mo>×</mo> <mn>5</mn> <mo>=</mo> <mn>10</mn></math> ppm.
  prefs: []
  type: TYPE_NORMAL
- en: Any change in the PurpleAir reading is not caused by the change in the AQS reading.
    Rather, they both reflect the air quality, and our model captures the relationship
    between the two devices. Oftentimes, the term *prediction* is taken to mean *causation*,
    but that is not the case here. Instead, the prediction just refers to our use
    of the *linear association* between PA and AQS measurements.
  prefs: []
  type: TYPE_NORMAL
- en: As for the intercept in the model, we might expect it to be 0, since when there
    is no particulate matter in the air we would think that both instruments would
    measure 0 ppm. But for an AQS of 0, the model predicts <math><mo>−</mo> <mn>3.4</mn></math>
    ppm for PurpleAir, which doesn’t make sense. There can’t be negative amounts of
    particles in the air. This highlights the problem of using the model outside the
    range where measurements were taken. We observed AQS recordings between 3 and
    18 ppm, and in this range the model fits well. While it makes sense for the line
    to have an intercept of 0, such a model doesn’t fit well in a practical sense
    and the predictions tend to be much worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'George Box, a renowned statistician, famously said, “All models are wrong,
    but some are useful.” Here is a case where despite the intercept of the line not
    passing through 0, the simple linear model is useful in predicting air quality
    measurements for a PurpleAir sensor. Indeed, the correlation between our two features
    is very high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|   | pm25aqs | pm25pa |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **pm25aqs** | 1.00 | 0.92 |'
  prefs: []
  type: TYPE_TB
- en: '| **pm25pa** | 0.92 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: Aside from looking at correlation coefficients, there are other ways to assess
    the quality of a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the Fit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The earlier plot of the errors against the fitted values gives a visual assessment
    of the quality of the fit. (This plot is called a *residual plot* because the
    errors are sometimes referred to as *residuals*.) A good fit should show a cloud
    of points around the horizontal line at 0 with no clear pattern. When there is
    a pattern, we can usually conclude that the simple linear model doesn’t entirely
    capture the signal. We saw earlier that there are no apparent patterns in the
    residual plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of residual plot that can be useful is a plot of the residuals
    against a feature that is not in the model. If we see a pattern, then we may want
    to include this feature in the model, in addition to the feature(s) already in
    the model. Also, when the data have a time component, we want to check for patterns
    in the residuals over time. For these particular data, since the measurements
    are daily averages over a four-month period, we plot the error against the date
    the measurement is recorded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_15in03.png)'
  prefs: []
  type: TYPE_IMG
- en: It looks like there are a few consecutive days near the end of August and again
    near the end of September where the data are far below what is expected. Looking
    back at the original scatterplot (and the first residual plot), we can see two
    small clusters of horizontal points below the main point cloud. The plot we just
    made indicates that we should check the original data and any available information
    about the equipment to determine whether it was properly functioning on those
    days.
  prefs: []
  type: TYPE_NORMAL
- en: 'The residual plot can also give us a general sense of how accurate the model
    is in its predictions. Most of the errors lie between <math><mo>±</mo> <mn>6</mn></math>
    ppm of the line. And we find the standard deviation of the errors to be about
    2.8 ppm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In comparison, the standard deviation of the PurpleAir measurements is quite
    a bit larger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The model error may be further reduced if we find the monitor wasn’t working
    on those days in late August and September and so exclude them from the dataset.
    In any event, for situations where the air is quite clean, the error is relatively
    large, but in absolute terms it is inconsequential. We are typically more concerned
    about the case when there is air pollution, and in that case, an error of 2.8
    ppm seems reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the process of how to find this line, the process of *model
    fitting*. In the next section, we derive the intercept and slope by minimizing
    the mean squared error.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the Simple Linear Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We stated earlier in this chapter that when we minimize the average loss over
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mn>1</mn> <mi>n</mi></mfrac> <munder><mo>∑</mo>
    <mrow><mi>i</mi></mrow></munder> <mo stretchy="false">[</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'the best-fitting line has intercept and slope:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub></mtd> <mtd><mo>=</mo>
    <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow> <mo>−</mo>
    <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub>
    <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow></mtd></mtr>
    <mtr><mtd><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub></mtd> <mtd><mo>=</mo> <mi>r</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo>,</mo> <mrow><mrow><mi
    mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo> <mfrac><mrow><mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo></mrow> <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we use calculus to derive these results.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the simple linear model, the mean squared error is a function of two model
    parameters, the intercept and slope. This means that if we use calculus to find
    the minimizing parameter values, we need to find the partial derivatives of the
    MSE with respect to <math><msub><mi>θ</mi> <mn>0</mn></msub></math> and <math><msub><mi>θ</mi>
    <mn>1</mn></msub></math> . We can also find these minimizing values through other
    techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gradient descent*'
  prefs: []
  type: TYPE_NORMAL
- en: We can use numerical optimization techniques, such as gradient descent, when
    the loss function is more complex and it’s faster to find an approximate solution
    that’s pretty accurate (see [Chapter 20](ch20.html#ch-gd)).
  prefs: []
  type: TYPE_NORMAL
- en: '*Quadratic formula*'
  prefs: []
  type: TYPE_NORMAL
- en: Since the average loss is a quadratic function of <math><msub><mi>θ</mi> <mn>0</mn></msub></math>
    and <math><msub><mi>θ</mi> <mn>1</mn></msub></math> , we can use the quadratic
    formula (along with some algebra) to solve for the minimizing parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: '*Geometric argument*'
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we use a geometric interpretation of least squares to
    fit multiple linear models. This approach relates to the Pythagorean theorem and
    has several intuitive benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'We choose calculus to optimize the simple linear model since it is quick and
    straightforward. To begin, we take the partial derivatives of the sum of squared
    errors with respect to each parameter (we can ignore the e <math><mn>1</mn> <mrow><mo>/</mo></mrow>
    <mi>n</mi></math> in the MSE because it doesn’t affect the location of the minimum):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mfrac><mi>∂</mi> <mrow><mi>∂</mi>
    <msub><mi>θ</mi> <mn>0</mn></msub></mrow></mfrac> <munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder>
    <mo stretchy="false">[</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mo
    stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd> <mtd><mo>=</mo> <munder><mo>∑</mo>
    <mrow><mi>i</mi></mrow></munder> <mn>2</mn> <mo stretchy="false">(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>−</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo>
    <mo stretchy="false">(</mo> <mo>−</mo> <mn>1</mn> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><mfrac><mi>∂</mi> <mrow><mi>∂</mi> <msub><mi>θ</mi> <mn>1</mn></msub></mrow></mfrac>
    <munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder> <mo stretchy="false">[</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo>
    <mn>2</mn></msup> <mo>,</mo></mtd> <mtd><mo>=</mo> <munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder>
    <mn>2</mn> <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo>
    <msub><mi>θ</mi> <mn>0</mn></msub> <mo>−</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo> <mo stretchy="false">(</mo>
    <mo>−</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we set the partial derivatives equal to 0 and simplify a bit by multiplying
    both sides of the equations by <math><mo>−</mo> <mn>1</mn> <mrow><mo>/</mo></mrow>
    <mn>2</mn></math> to get:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mn>0</mn></mtd> <mtd><mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub> <mo>−</mo> <msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mo>=</mo> <munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mn>0</mn></msub> <mo>−</mo> <msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo> <msub><mi>x</mi> <mi>i</mi></msub></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'These equations are called the *normal equations*. In the first equation, we
    see that <math><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>0</mn></msub></math> can be represented as a function of <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>0</mn></msub> <mo>=</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging this value into the second equation gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mn>0</mn></mtd> <mtd><mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>+</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo>
    <msub><mi>x</mi> <mi>i</mi></msub></mtd></mtr> <mtr><mtd><mo>=</mo> <munder><mo>∑</mo>
    <mrow><mi>i</mi></mrow></munder> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo> <mo>−</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <mo stretchy="false">(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>−</mo> <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <msub><mi>x</mi> <mi>i</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub></mtd> <mtd><mo>=</mo> <mfrac><mrow><munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>y</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo stretchy="false">)</mo> <msub><mi>x</mi>
    <mi>i</mi></msub></mrow> <mrow><munder><mo>∑</mo> <mrow><mi>i</mi></mrow></munder>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>x</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo stretchy="false">)</mo> <msub><mi>x</mi>
    <mi>i</mi></msub></mrow></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'After some algebra, we can represent <math><msub><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mn>1</mn></msub></math> in terms of quantities
    that we are familiar with:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <mo>=</mo> <mi>r</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi
    mathvariant="bold">x</mi></mrow></mrow> <mo>,</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo> <mfrac><mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo></mrow>
    <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow>
    <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown earlier in this chapter, this representation says that a point on
    the fitted line at <math><mi>x</mi></math> can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>0</mn></msub> <mo>+</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub> <mi>x</mi> <mo>=</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>+</mo> <mi>r</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow>
    <mo>,</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow> <mo stretchy="false">)</mo>
    <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo stretchy="false">)</mo> <mfrac><mrow><mo stretchy="false">(</mo> <mi>x</mi>
    <mo>−</mo> <mrow><mover><mi>x</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mrow> <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mrow><mi mathvariant="bold">x</mi></mrow></mrow> <mo stretchy="false">)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We have derived the equation for the least squares line that we used in the
    previous section. There, we used the `pandas` built-in methods to compute <math><mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">x</mi></mrow>
    <mo stretchy="false">)</mo></math> , <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mi mathvariant="bold">y</mi></mrow> <mo stretchy="false">)</mo></math>
    , and <math><mi>r</mi> <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">x</mi></mrow>
    <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow> <mo stretchy="false">)</mo></math>
    , to easily calculate the equation for this line. However, in practice we recommend
    using the functionality provided in `scikit-learn` to do the model fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our fitted model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we provided `y` as an array and `x` as a dataframe to `LinearRegression`.
    We will soon see why when we fit multiple explanatory features in a model.
  prefs: []
  type: TYPE_NORMAL
- en: The `LinearRegression` method offers numerically stable algorithms to fit linear
    models by least squares. This is especially important when fitting multiple variables,
    which we introduce next.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, we’ve used a single input variable to predict an outcome
    variable. Now we introduce the *multiple linear model* that uses more than one
    feature to predict (or describe or explain) the outcome. Having multiple explanatory
    features can improve our model’s fit to the data and improve predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by generalizing from a simple linear model to one that includes a
    second explanatory variable, called <math><mi>v</mi></math> . This model is linear
    in both <math><mi>x</mi></math> and <math><mi>v</mi></math> , meaning that for
    a pair of values for <math><mi>x</mi></math> and <math><mi>v</mi></math> , we
    can describe, explain, or predict <math><mi>y</mi></math> by the linear combination:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>y</mi> <mo>≈</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>θ</mi>
    <mn>2</mn></msub> <mi>v</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that for a particular value of <math><mi>v</mi></math> , say <math><msup><mi>v</mi>
    <mo>⋆</mo></msup></math> , we could express the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>y</mi> <mo>≈</mo> <mo stretchy="false">(</mo> <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub> <msup><mi>v</mi>
    <mo>⋆</mo></msup> <mo stretchy="false">)</mo>  <mo>+</mo>  <msub><mi>θ</mi> <mn>1</mn></msub>
    <mi>x</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: In other words, when we hold <math><mi>v</mi></math> constant at <math><msup><mi>v</mi>
    <mo>⋆</mo></msup></math> , we have a simple linear relation between <math><mi>x</mi></math>
    and <math><mi>y</mi></math> with slope <math><msub><mi>θ</mi> <mn>1</mn></msub></math>
    and intercept <math><msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>2</mn></msub> <msup><mi>v</mi> <mo>⋆</mo></msup></math> . For a different
    value of <math><mi>v</mi></math> , say <math><msup><mi>v</mi> <mo>†</mo></msup></math>
    , we again have a simple linear relationship between <math><mi>x</mi></math> and
    <math><mi>y</mi></math> . The slope for <math><mi>x</mi></math> remains the same
    and the only change is the intercept, which is now <math><msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub> <msup><mi>v</mi> <mo>†</mo></msup></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: With multiple linear regression, we need to remember to interpret the coefficient
    <math><msub><mi>θ</mi> <mn>1</mn></msub></math> of <math><mi>x</mi></math> in
    the presence of the other variables in the model. Holding fixed the values of
    the other variables in the model (that’s just <math><mi>v</mi></math> in this
    case), an increase of 1 unit in <math><mi>x</mi></math> corresponds to a <math><msub><mi>θ</mi>
    <mn>1</mn></msub></math> change in <math><mi>y</mi></math> , on average. One way
    to visualize this kind of multiple linear relationship is to create facets of
    scatterplots of <math><mo stretchy="false">(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo stretchy="false">)</mo></math> where in each plot the values of <math><mi>v</mi></math>
    are roughly the same. We make such a scatterplot for the air quality measurements
    next, and provide examples of additional visualizations and statistics to examine
    when fitting a multiple linear model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scientists who studied the air quality monitors (see [Chapter 12](ch12.html#ch-pa))
    were looking for an improved model that incorporated weather factors. One weather
    variable they examined was a daily measurement for relative humidity. Let’s consider
    a two-variable linear model to explain the PurpleAir measurements based on the
    AQS sensor measurements and relative humidity. This model has the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>P</mi> <mi>A</mi> <mo>≈</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>A</mi> <mi>Q</mi> <mo>+</mo>
    <msub><mi>θ</mi> <mn>2</mn></msub> <mi>R</mi> <mi>H</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math><mi>P</mi> <mi>A</mi></math> , <math><mi>A</mi> <mi>Q</mi></math>
    , and <math><mi>R</mi> <mi>H</mi></math> refer to the variables: the PurpleAir
    average daily measurement, AQS measurement, and relative humidity, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a first step, we make a facet plot to compare the relationship between
    the two air quality measurements for fixed values of humidity. To do this, we
    transform relative humidity to a categorical variable so that each facet consists
    of observations with similar humidity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use this qualitative feature to subdivide the data into a two-by-two
    panel of scatterplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in04.png)'
  prefs: []
  type: TYPE_IMG
- en: These four plots show a linear relationship between the two sources of air quality
    measurements. And the slopes appear to be similar, which means that a multiple
    linear model may fit well. It’s difficult to see from these plots if the relative
    humidity affects the intercept much.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also want to examine the pairwise scatterplots between the three features.
    When two explanatory features are highly correlated, their coefficients in the
    model may be unstable. While linear relationships between three or more features
    may not show up in pairwise plots, it’s still a good idea to check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The relationship between humidity and air quality does not appear to be particularly
    strong. Another pairwise measure we should examine is the correlations between
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | pm25pa | pm25aqs | rh |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **pm25pa** | 1.00 | 0.95 | -0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| **pm25aqs** | 0.95 | 1.00 | -0.24 |'
  prefs: []
  type: TYPE_TB
- en: '| **rh** | -0.06 | -0.24 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: One small surprise is that relative humidity has a small negative correlation
    with the AQS measurement of air quality. This suggests that humidity might be
    helpful in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we derive the equation for the fit. But for now, we use
    the functionality in `LinearRegression` to fit the model. The only change from
    earlier is that we provide two columns for the explanatory variables (that’s why
    the `x` input is a dataframe):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The fitted multiple linear model, including the coefficient units, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The coefficient for humidity in the model adjusts the air quality prediction
    by 0.21 ppm for each percentage point of relative humidity. Notice that the coefficient
    for AQS differs from the simple linear model that we fitted earlier. This happens
    because the coefficient reflects the additional information coming from relative
    humidity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, to check the quality of the fit, we make residual plots of the predicted
    values and the errors. This time, we use `LinearRegression` to compute the predictions
    for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The residual plot appears to have no clear patterns, which indicates that the
    model fits pretty well. Notice also that the errors nearly all fall within –4
    and +4 ppm, a smaller range than in the simple linear model. And we find the standard
    deviation of the residuals is quite a bit smaller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The residual standard deviation has been reduced from 2.8 ppm in the one variable
    model to 1.8 ppm, a good size reduction.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation coefficient can’t capture the strength of a linear association
    model when we have more than one explanatory variable. Instead, we adapt the MSE
    to give us a sense of model fit. In the next section, we describe how to fit a
    multiple linear model and use the MSE to assess fit.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the Multiple Linear Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we considered the case of two explanatory variables;
    one of these we called <math><mi>x</mi></math> and the other <math><mi>v</mi></math>
    . Now we want to generalize the approach to <math><mi>p</mi></math> explanatory
    variables. The idea of choosing different letters to represent variables quickly
    fails us. Instead, we use a more formal and general approach that represents multiple
    predictors as a matrix, as depicted in [Figure 15-3](#fig-design-matrix). We call
    <math><mtext mathvariant="bold">X</mtext></math> the *design matrix*. Notice that
    <math><mtext mathvariant="bold">X</mtext></math> has shape <math><mi>n</mi> <mo>×</mo>
    <mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></math>
    . Each column of <math><mtext mathvariant="bold">X</mtext></math> represents a
    feature, and each row represents an observation. That is, <math><msub><mi>x</mi>
    <mrow><mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></msub></math> is the measurement
    taken on observation <math><mi>i</mi></math> for feature <math><mi>j</mi></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-3\. In this design matrix <math><mi>X</mi></math> , each row represents
    an observation/record and each column a feature/variable
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One technicality: the design matrix is defined as a mathematical matrix, not
    a dataframe, so you might notice that a matrix doesn’t include the column or row
    labels that a dataframe has.'
  prefs: []
  type: TYPE_NORMAL
- en: That said, we usually don’t have to worry about converting dataframes into matrices
    since most Python libraries for modeling treat dataframes of numbers as if they
    were matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given observation, say, the second row in <math><mtext mathvariant="bold">X</mtext></math>
    , we approximate the outcome <math><msub><mi>y</mi> <mn>2</mn></msub></math> by
    the linear combination:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mi>y</mi>
    <mn>2</mn></msub> <mo>≈</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mrow><mn>2</mn> <mo>,</mo> <mn>1</mn></mrow></msub>
    <mo>+</mo> <mo>…</mo> <mo>+</mo> <msub><mi>θ</mi> <mi>p</mi></msub> <msub><mi>x</mi>
    <mrow><mn>2</mn> <mo>,</mo> <mi>p</mi></mrow></msub></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s more convenient to express the linear approximation in matrix notation.
    To do this, we write the model parameters as a <math><mi>p</mi> <mo>+</mo> <mn>1</mn></math>
    column vector <math><mrow><mi mathvariant="bold-italic">θ</mi></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mrow><mi>θ</mi></mrow> <mo>=</mo> <mrow><mo>[</mo>
    <mtable columnalign="center" columnspacing="1em" rowspacing="4pt"><mtr><mtd><msub><mi>θ</mi>
    <mn>0</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>θ</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><mrow><mo>⋮</mo></mrow></mtd></mtr> <mtr><mtd><msub><mi>θ</mi> <mi>p</mi></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting these notational definitions together, we can write the vector of predictions
    for the entire dataset using matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext mathvariant="bold">X</mtext></mrow> <mrow><mi
    mathvariant="bold-italic">θ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If we check the dimensions of <math><mtext mathvariant="bold">X</mtext></math>
    and <math><mi mathvariant="bold-italic">θ</mi></math> , we can confirm that <math><mrow><mtext
    mathvariant="bold">X</mtext></mrow> <mrow><mi mathvariant="bold-italic">θ</mi></mrow></math>
    is an <math><mi>n</mi></math> -dimensional column vector. So the error in using
    this linear prediction can be expressed as the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="bold">e</mi></mrow> <mo>=</mo>
    <mrow><mi mathvariant="bold">y</mi></mrow> <mo>−</mo> <mrow><mtext mathvariant="bold">X</mtext></mrow>
    <mrow><mi mathvariant="bold-italic">θ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where the outcome variable is also represented as a column vector:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="bold">y</mi></mrow> <mo>=</mo>
    <mrow><mo>[</mo> <mtable columnalign="center" columnspacing="1em" rowspacing="4pt"><mtr><mtd><msub><mi>y</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mn>2</mn></msub></mtd></mtr>
    <mtr><mtd><mrow><mo>⋮</mo></mrow></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mi>n</mi></msub></mtd></mtr></mtable>
    <mo>]</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This matrix representation of the multiple linear model can help us find the
    model that minimizes mean squared error. Our goal is to find the model parameters
    <math><mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>,</mo>
    <msub><mi>θ</mi> <mn>1</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>θ</mi>
    <mi>p</mi></msub> <mo stretchy="false">)</mo></math> that minimize the mean squared
    error:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mn>1</mn> <mi>n</mi></mfrac> <munder><mo>∑</mo>
    <mi>i</mi></munder> <mo stretchy="false">[</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>−</mo> <mo stretchy="false">(</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo>
    <mn>1</mn></mrow></msub> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>θ</mi> <mi>p</mi></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>p</mi></mrow></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <mo fence="false" stretchy="false">‖</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo>−</mo> <mrow><mtext mathvariant="bold">X</mtext></mrow> <mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <msup><mo fence="false" stretchy="false">‖</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we use the notation <math><mo fence="false" stretchy="false">‖</mo> <mrow><mi
    mathvariant="bold">v</mi></mrow> <msup><mo fence="false" stretchy="false">‖</mo>
    <mn>2</mn></msup></math> for a vector <math><mrow><mi mathvariant="bold">v</mi></mrow></math>
    as a shorthand for the sum of each vector element squared: <math><mo fence="false"
    stretchy="false">‖</mo> <mrow><mi mathvariant="bold">v</mi></mrow> <msup><mo fence="false"
    stretchy="false">‖</mo> <mn>2</mn></msup> <mo>=</mo> <munder><mo>∑</mo> <mi>i</mi></munder>
    <msubsup><mi>v</mi> <mi>i</mi> <mn>2</mn></msubsup></math> . The square root,
    <math><msqrt><mo fence="false" stretchy="false">‖</mo> <mrow><mi mathvariant="bold">v</mi></mrow>
    <msup><mo fence="false" stretchy="false">‖</mo> <mn>2</mn></msup></msqrt></math>
    , corresponds to the length of the vector <math><mrow><mi mathvariant="bold">v</mi></mrow></math>
    and is also called the <math><msub><mi>ℓ</mi> <mn>2</mn></msub></math> norm of
    <math><mrow><mi mathvariant="bold">v</mi></mrow></math> . So, minimizing the mean
    squared error is the same thing as finding the shortest error vector.'
  prefs: []
  type: TYPE_NORMAL
- en: We can fit our model using calculus as we did for the simple linear model. However,
    this approach gets cumbersome, and instead we use a geometric argument that is
    more intuitive and easily leads to useful properties of the design matrix, errors,
    and predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to find the parameter vector, which we call <math><mrow><mover><mi
    mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math>
    , that minimizes our average squared loss—we want to make <math><mo fence="false"
    stretchy="false">‖</mo> <mrow><mi mathvariant="bold">y</mi></mrow> <mo>−</mo>
    <mrow><mtext mathvariant="bold">X</mtext></mrow> <mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <msup><mo fence="false" stretchy="false">‖</mo> <mn>2</mn></msup></math> as small
    as possible for a given <math><mtext mathvariant="bold">X</mtext></math> and <math><mrow><mi
    mathvariant="bold">y</mi></mrow></math> . The key insight is that we can restate
    this goal in a geometric way. Since the model predictions and the true outcomes
    are both vectors, we can think of them as vectors in a *vector space*. When we
    change our model parameters <math><mrow><mi mathvariant="bold-italic">θ</mi></mrow></math>
    , the model makes different predictions, but any prediction must be a linear combination
    of the column vectors of <math><mrow><mi mathvariant="bold">X</mi></mrow></math>
    ; that is, the prediction must be in what is called <math><mtext>span</mtext>
    <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">X</mi></mrow> <mo stretchy="false">)</mo></math>
    . This notion is illustrated in [Figure 15-4](#fig-spanx), where the shaded region
    consists of the possible linear models. Notice that <math><mrow><mi mathvariant="bold">y</mi></mrow></math>
    is not entirely captured in <math><mtext>span</mtext> <mo stretchy="false">(</mo>
    <mrow><mi mathvariant="bold">X</mi></mrow> <mo stretchy="false">)</mo></math>
    ; this is typically the case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-4\. In this simplified diagram, the space of all possible model prediction
    vectors <math><mtext>span</mtext> <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">X</mi></mrow>
    <mo stretchy="false">)</mo></math> is illustrated as a plane in three-dimensional
    space, and the observed <math><mrow><mi mathvariant="bold">y</mi></mrow></math>
    as a vector
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although the squared loss can’t be exactly zero because <math><mrow><mi mathvariant="bold">y</mi></mrow></math>
    isn’t in the <math><mtext>span</mtext> <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">X</mi></mrow>
    <mo stretchy="false">)</mo></math> , we can find the vector that lies as close
    to <math><mrow><mi mathvariant="bold">y</mi></mrow></math> as possible while still
    being in <math><mtext>span</mtext> <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">X</mi></mrow>
    <mo stretchy="false">)</mo></math> . This vector is called <math><mrow><mrow><mover><mi
    mathvariant="bold">y</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: The error is the vector <math><mrow><mi mathvariant="bold">e</mi></mrow> <mo>=</mo>
    <mrow><mi mathvariant="bold">y</mi></mrow> <mo>−</mo> <mrow><mrow><mover><mi mathvariant="bold">y</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></mrow></math> .
    Its length <math><mo fence="false" stretchy="false">‖</mo> <mrow><mi mathvariant="bold">e</mi></mrow>
    <mo fence="false" stretchy="false">‖</mo></math> represents the distance between
    the true outcome and our model’s prediction. Visually, <math><mrow><mi mathvariant="bold">e</mi></mrow></math>
    has the smallest magnitude when it is *perpendicular* to the <math><mtext>span</mtext>
    <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">X</mi></mrow> <mo stretchy="false">)</mo></math>
    , as shown in [Figure 15-5](#fig-error-vector-optimal). The proof of this fact
    is omitted, and we rely on the figures to convince you of it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-5\. The mean squared error is minimized when the prediction <math><mrow><mrow><mover><mi
    mathvariant="bold">y</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></mrow></math>
    lies in <math><mtext>span</mtext> <mo stretchy="false">(</mo> <mrow><mi mathvariant="bold">X</mi></mrow>
    <mo stretchy="false">)</mo></math> perpendicular to <math><mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow></math>
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The fact that the smallest error, <math><mrow><mi mathvariant="bold">e</mi></mrow></math>
    , must be perpendicular to <math><mrow><mrow><mover><mi mathvariant="bold">y</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></mrow></math> lets
    us derive a formula for <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left right" columnspacing="0em
    2em" displaystyle="true" rowspacing="3pt"><mtr><mtd><mtext mathvariant="bold">X</mtext>
    <mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow>
    <mo>+</mo> <mrow><mi mathvariant="bold">e</mi></mrow></mtd> <mtd><mo>=</mo> <mrow><mi
    mathvariant="bold">y</mi></mrow></mtd> <mtd><mo stretchy="false">(</mo> <mtext>the
    definition of </mtext> <mrow><mi mathvariant="bold">y</mi></mrow> <mo>,</mo> <mrow><mover><mrow><mi
    mathvariant="bold">y</mi></mrow> <mo stretchy="false">^</mo></mover></mrow> <mo>,</mo>
    <mrow><mi mathvariant="bold">e</mi></mrow> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><msup><mrow><mtext mathvariant="bold">X</mtext></mrow> <mi mathvariant="normal">⊤</mi></msup>
    <mtext mathvariant="bold">X</mtext> <mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo>+</mo> <msup><mrow><mtext mathvariant="bold">X</mtext></mrow>
    <mi mathvariant="normal">⊤</mi></msup> <mrow><mi mathvariant="bold">e</mi></mrow></mtd>
    <mtd><mo>=</mo> <msup><mrow><mtext mathvariant="bold">X</mtext></mrow> <mi mathvariant="normal">⊤</mi></msup>
    <mrow><mi mathvariant="bold">y</mi></mrow></mtd> <mtd><mo stretchy="false">(</mo>
    <mtext>left-multiply by </mtext> <msup><mrow><mtext mathvariant="bold">X</mtext></mrow>
    <mi mathvariant="normal">⊤</mi></msup> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><msup><mrow><mtext mathvariant="bold">X</mtext></mrow> <mi mathvariant="normal">⊤</mi></msup>
    <mtext mathvariant="bold">X</mtext> <mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></mtd> <mtd><mo>=</mo> <msup><mrow><mtext
    mathvariant="bold">X</mtext></mrow> <mi mathvariant="normal">⊤</mi></msup> <mrow><mi
    mathvariant="bold">y</mi></mrow></mtd> <mtd><mo stretchy="false">(</mo> <mrow><mi
    mathvariant="bold">e</mi></mrow> <mo>⊥</mo> <mtext>span</mtext> <mo stretchy="false">(</mo>
    <mtext mathvariant="bold">X</mtext> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold"
    stretchy="false">^</mo></mover></mrow></mtd> <mtd><mo>=</mo> <mo stretchy="false">(</mo>
    <msup><mrow><mtext mathvariant="bold">X</mtext></mrow> <mi mathvariant="normal">⊤</mi></msup>
    <mtext mathvariant="bold">X</mtext> <msup><mo stretchy="false">)</mo> <mrow><mo>−</mo>
    <mn>1</mn></mrow></msup> <msup><mrow><mtext mathvariant="bold">X</mtext></mrow>
    <mi mathvariant="normal">⊤</mi></msup> <mrow><mi mathvariant="bold">y</mi></mrow></mtd>
    <mtd><mo stretchy="false">(</mo> <mtext>left-multiply by </mtext> <mo stretchy="false">(</mo>
    <msup><mrow><mtext mathvariant="bold">X</mtext></mrow> <mi mathvariant="normal">⊤</mi></msup>
    <mtext mathvariant="bold">X</mtext> <msup><mo stretchy="false">)</mo> <mrow><mo>−</mo>
    <mn>1</mn></mrow></msup> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: This general approach to derive <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> for the multiple
    linear model also gives us <math><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>0</mn></msub></math> and <math><msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mn>1</mn></msub></math> for the simple linear model. If we set <math><mrow><mtext
    mathvariant="bold">X</mtext></mrow></math> to be the two-column matrix that contains
    the intercept column and one feature column, this formula for <math><mrow><mover><mi
    mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math>
    and some linear algebra gets the intercept and slope of the least-squares-fitted
    simple linear model. In fact, if <math><mrow><mtext mathvariant="bold">X</mtext></mrow></math>
    is simply a single column of <math><mn>1</mn></math> s, then we can use this formula
    to show that <math><mrow><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></mrow></math>
    is just the mean of <math><mrow><mi mathvariant="bold">y</mi></mrow></math> .
    This nicely ties back to the constant model that we introduced in [Chapter 4](ch04.html#ch-modeling).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While we can write a simple function to derive the <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> based on
    the formula
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo
    mathvariant="bold" stretchy="false">^</mo></mover></mrow> <mo>=</mo> <mo stretchy="false">(</mo>
    <msup><mrow><mtext mathvariant="bold">X</mtext></mrow> <mi mathvariant="normal">⊤</mi></msup>
    <mtext mathvariant="bold">X</mtext> <msup><mo stretchy="false">)</mo> <mrow><mo>−</mo>
    <mn>1</mn></mrow></msup> <msup><mrow><mtext mathvariant="bold">X</mtext></mrow>
    <mi mathvariant="normal">⊤</mi></msup> <mrow><mi mathvariant="bold">y</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: we recommend leaving the calculation of <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> to the optimally
    tuned methods provided in the `scikit-learn` and `statsmodels` libraries. They
    handle cases where the design matrix is sparse, highly co-linear, and not invertible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This solution for <math><mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo
    mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> (along with the
    pictures) reveals some useful properties of the fitted coefficients and the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: The residuals, <math><mrow><mi mathvariant="bold">e</mi></mrow></math> , are
    orthogonal to the predicted values, <math><mrow><mover><mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">^</mo></mover></mrow></math> .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average of the residuals is 0 if the model has an intercept term.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance of the residuals is just the MSE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These properties explain why we examine plots of the residuals against the predictions.
    When we fit a multiple linear model, we also plot the residuals against variables
    that we are considering adding to the model. If they showed a linear pattern,
    then we would consider adding them to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to examining the SD of the errors, the ratio of the MSE for a multiple
    linear model to the MSE for the constant model gives a measure of the model fit.
    This is called the *multiple <math><msup><mi>R</mi> <mn>2</mn></msup></math>*
    and is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>−</mo> <mfrac><mrow><mo fence="false" stretchy="false">‖</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo>−</mo> <mrow><mtext mathvariant="bold">X</mtext></mrow> <mrow><mrow><mover><mi
    mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></mrow>
    <msup><mo fence="false" stretchy="false">‖</mo> <mn>2</mn></msup></mrow> <mrow><mo
    fence="false" stretchy="false">‖</mo> <mrow><mrow><mi mathvariant="bold">y</mi></mrow></mrow>
    <mo>−</mo> <mrow><mover><mi>y</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <msup><mo fence="false" stretchy="false">‖</mo> <mn>2</mn></msup></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: As the model fits the data closer and closer, the multiple <math><msup><mi>R</mi>
    <mn>2</mn></msup></math> gets nearer to 1\. That might seem like a good thing,
    but there can be problems with this approach because <math><msup><mi>R</mi> <mn>2</mn></msup></math>
    continues to grow even as we add meaningless features to our model, as long as
    the features expand the <math><mtext>span</mtext> <mo stretchy="false">(</mo>
    <mtext mathvariant="bold">X</mtext> <mo stretchy="false">)</mo></math> . To account
    for the size of a model, we often adjust the numerator and denominator in <math><msup><mi>R</mi>
    <mn>2</mn></msup></math> by the number of fitted coefficients in the models. That
    is, we normalize the numerator by <math><mn>1</mn> <mrow><mo>/</mo></mrow> <mo
    stretchy="false">[</mo> <mi>n</mi> <mo>−</mo> <mo stretchy="false">(</mo> <mi>p</mi>
    <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo></math>
    and the denominator by <math><mn>1</mn> <mrow><mo>/</mo></mrow> <mo stretchy="false">(</mo>
    <mi>n</mi> <mo>−</mo> <mn>1</mn> <mo stretchy="false">)</mo></math> . Better approaches
    to selecting a model are covered in [Chapter 16](ch16.html#ch-risk).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we consider a social science example where there are many variables available
    to us for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Where Is the Land of Opportunity?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The US is called “the land of opportunity” because people believe that even
    those with few resources can end up wealthy in the US—economists call this notion
    “economic mobility.” In one study, economist Raj Chetty and colleagues did a [large-scale
    data analysis on economic mobility in the US](https://doi.org/10.1093/qje/qju022).
    His basic question was whether the US is a land of opportunity. To answer this
    somewhat vague question, Chetty needed a way to measure economic mobility.
  prefs: []
  type: TYPE_NORMAL
- en: Chetty had access to 2011–2012 federal income tax records for everyone born
    in the US between 1980 and 1982, along with their parents’ tax records filed in
    their birth year. They matched the 30-year-olds to their parents by finding the
    parents’ 1980–1982 tax records that listed them as dependents. In total, his dataset
    had about 10 million people. To measure economic mobility, Chetty grouped people
    born in a particular geographic region whose parents’ income was in the 25th income
    percentile in 1980–1982\. He then found the group’s average income percentile
    in 2011\. Chetty calls this average *absolute upward mobility* (AUM). If a region’s
    AUM is 25, then people born into the 25th percentile generally stay in the 25th
    percentile—they remain where their parents were when they were born. High AUM
    values mean that the region has more upward mobility. Those born into the 25th
    income percentile in these regions generally wind up in a higher income bracket
    than their parents. For reference, the US average AUM is about 41 at the time
    of this writing. Chetty calculated the AUM for regions called commuting zones
    (CZs), which are roughly on the same scale as counties.
  prefs: []
  type: TYPE_NORMAL
- en: While the granularity of the original data is at an individual level, the data
    Chetty analyzed has a granularity at the CZ level. Income records can’t be publicly
    available because of privacy laws, but the AUM for a commuting zone can be made
    available. However, even with the granularity of a commuting zone, not all commuting
    zones are included in the data set because with 40 features in the data, it might
    be possible to identify individuals in small CZs. This limitation points to a
    potential coverage bias. Measurement bias is another potential problem. For example,
    children born into the 25th income percentile who become extremely wealthy may
    not file income tax.
  prefs: []
  type: TYPE_NORMAL
- en: We also point out the limitations of working with data that are regional averages
    rather than individual measurements. The relationships found among features are
    often more highly correlated at the aggregate level than at the individual level.
    This phenomenon is called *ecological regression*, and interpretations of findings
    from aggregated data need to be made with care.
  prefs: []
  type: TYPE_NORMAL
- en: Chetty had a hunch that some places in the US have higher economic mobility
    than others. His analysis found this to be true. He found that some cities—such
    as San Jose, Calif.; Washington, DC; and Seattle—have higher mobility than others,
    such as Charlotte, N.C.; Milwaukee; and Atlanta. This means that, for example,
    people move from low to high income brackets in San Jose at a higher rate compared
    to Charlotte. Chetty used linear models to find that social and economic factors
    like segregation, income inequality, and local school systems are related to economic
    mobility.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this analysis, our outcome variable is the AUM for a commuting zone, since
    we are interested in finding features that correlate with AUM. There are many
    possible such features in Chetty’s data, but we first investigate one in particular:
    the fraction of people in a CZ who have a 15-minute or shorter commute to work.'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining Upward Mobility Using Commute Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin our investigation by loading the data into a dataframe called `cz_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | aum | travel_lt15 | gini | rel_tot | ... | taxrate | worked_14 | foreign
    | region |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 38.39 | 0.33 | 0.47 | 0.51 | ... | 0.02 | 3.75e-03 | 1.18e-02 | South
    |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 37.78 | 0.28 | 0.43 | 0.54 | ... | 0.02 | 4.78e-03 | 2.31e-02 | South
    |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 39.05 | 0.36 | 0.44 | 0.67 | ... | 0.01 | 2.89e-03 | 7.08e-03 | South
    |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **702** | 44.12 | 0.42 | 0.42 | 0.29 | ... | 0.02 | 4.82e-03 | 9.85e-02 |
    West |'
  prefs: []
  type: TYPE_TB
- en: '| **703** | 41.41 | 0.49 | 0.41 | 0.26 | ... | 0.01 | 4.39e-03 | 4.33e-02 |
    West |'
  prefs: []
  type: TYPE_TB
- en: '| **704** | 43.20 | 0.24 | 0.42 | 0.32 | ... | 0.02 | 3.67e-03 | 1.13e-01 |
    West |'
  prefs: []
  type: TYPE_TB
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Each row represents one commuting zone. The column `aum` has the average AUM
    for people born in the commuting zone in 1980–1982 to parents in the 25th income
    percentile. There are many columns in this dataframe, but for now we focus on
    the fraction of people in a CZ that have a 15-minute or shorter commute time,
    which is called `travel_lt15`. We plot AUM against this fraction to look at the
    relationship between the two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The scatterplot shows a rough linear association between AUM and commute time.
    Indeed, we find the correlation to be quite strong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '|   | aum | travel_lt15 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| aum | 1.00 | 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| travel_lt15 | 0.68 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s fit a simple linear model to explain AUM with commute time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The coefficients from the MSE minimization are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, an increase in upward mobility of a CZ is associated with an
    increase in the fraction of people with a short commute time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compare the SD of the AUM measurements to the SD of the residuals. This
    comparison gives us a sense of how useful the model is in explaining the AUM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The size of the errors about the regression line has decreased from the constant
    model by about 25%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we examine the residuals for lack of fit since it can be easier to see
    potential problems with the fit in a residual plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in08.png)'
  prefs: []
  type: TYPE_IMG
- en: It appears that the errors grow with AUM. We might try a transformation of the
    response variable, or fitting a model that is quadratic in the commute time fraction.
    We consider transformations and polynomials in the next section. First we see
    whether including additional variables offers a more accurate prediction of AUM.
  prefs: []
  type: TYPE_NORMAL
- en: Relating Upward Mobility Using Multiple Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In his original analysis, Chetty created several high-level features related
    to factors such as segregation, income, and K–12 education. We consider seven
    of Chetty’s predictors as we aim to build a more informative model for explaining
    AUM. These are described in [Table 15-1](#tbl-linear-predictors).
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-1\. Potential explanation for modeling AUM
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `travel_lt15` | Fraction of people with a ≤15-minute commute to work. |'
  prefs: []
  type: TYPE_TB
- en: '| `gini` | Gini coefficient, a measure of wealth inequality. Values are between
    0 and 1, where small values mean wealth is evenly distributed and large values
    mean more inequality. |'
  prefs: []
  type: TYPE_TB
- en: '| `rel_tot` | Fraction of people who self-reported as religious. |'
  prefs: []
  type: TYPE_TB
- en: '| `single_mom` | Fraction of children with a single mother. |'
  prefs: []
  type: TYPE_TB
- en: '| `taxrate` | Local tax rate. |'
  prefs: []
  type: TYPE_TB
- en: '| `worked_14` | Fraction of 14- to 16-year-olds who work. |'
  prefs: []
  type: TYPE_TB
- en: '| `foreign` | Fraction of people born outside the US. |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s first examine the correlations between AUM and the explanatory features
    and between the explanatory features themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | aum | travel_lt15 | gini | rel_tot | single_mom | taxrate | worked_14
    | foreign |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **aum** | 1.00 | 0.68 | -0.60 | 0.52 | -0.77 | 0.35 | 0.65 | -0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| **travel_lt15** | 0.68 | 1.00 | -0.56 | 0.40 | -0.42 | 0.34 | 0.60 | -0.19
    |'
  prefs: []
  type: TYPE_TB
- en: '| **gini** | -0.60 | -0.56 | 1.00 | -0.29 | 0.57 | -0.15 | -0.58 | 0.31 |'
  prefs: []
  type: TYPE_TB
- en: '| **rel_tot** | 0.52 | 0.40 | -0.29 | 1.00 | -0.31 | 0.08 | 0.28 | -0.11 |'
  prefs: []
  type: TYPE_TB
- en: '| **single_mom** | -0.77 | -0.42 | 0.57 | -0.31 | 1.00 | -0.26 | -0.60 | -0.04
    |'
  prefs: []
  type: TYPE_TB
- en: '| **taxrate** | 0.35 | 0.34 | -0.15 | 0.08 | -0.26 | 1.00 | 0.35 | 0.26 |'
  prefs: []
  type: TYPE_TB
- en: '| **worked_14** | 0.65 | 0.60 | -0.58 | 0.28 | -0.60 | 0.35 | 1.00 | -0.15
    |'
  prefs: []
  type: TYPE_TB
- en: '| **foreign** | -0.03 | -0.19 | 0.31 | -0.11 | -0.04 | 0.26 | -0.15 | 1.00
    |'
  prefs: []
  type: TYPE_TB
- en: We see that the fraction of single mothers in the commuting zone has the strongest
    correlation with AUM, which implies that it is also the single best feature to
    explain AUM. In addition, we see that several explanatory variables are highly
    correlated with each other; the Gini coefficient is highly correlated with the
    fraction of teenagers who work, the fraction of single mothers, and the fraction
    with less than a 15-minute commute. With such highly correlated features, we need
    to take care in interpreting the coefficients because several different models
    might equally explain AUM with the covariates standing in for one another.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The vector geometry perspective that we introduced earlier in this chapter can
    help us understand the problem. Recall that a feature corresponds to a column
    vector in <math><mi>n</mi></math> -dimensions, like <math><mrow><mi mathvariant="bold">x</mi></mrow></math>
    . With two highly correlated features, <math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mn>1</mn></msub></math> and <math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mn>2</mn></msub></math> , these vectors are nearly in alignment. So the projection
    of the response vector <math><mrow><mi mathvariant="bold">y</mi></mrow></math>
    onto one of these vectors is nearly the same as the projection onto the other.
    The situation gets even murkier when several features are correlated with one
    another.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we can consider all possible two-feature models to see which one has
    the smallest prediction error. Chetty derived 40 potential variables to use as
    predictors, which would have us checking <math><mo stretchy="false">(</mo> <mn>40</mn>
    <mo>×</mo> <mn>39</mn> <mo stretchy="false">)</mo> <mrow><mo>/</mo></mrow> <mn>2</mn>
    <mo>=</mo> <mn>780</mn></math> models. Fitting models, with all pairs, triples,
    and so on, of variables quickly grows out of control. And it can lead to finding
    spurious correlations (see [Chapter 17](ch17.html#ch-inf-pred-theory)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we keep things a bit simpler and examine just one two-variable model
    that includes the travel time and single-mother features. After that, we look
    at the model that has all seven numeric explanatory features in our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the coefficient for travel time is quite different than the coefficient
    for this variable in the simple linear model. That’s because the two features
    in our model are highly correlated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we compare the errors from the two fits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The SD of the residuals have been reduced by another 30%. Adding a second variable
    to the model seems worth the extra complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s again visually examine the residuals. We use the same scale on the y-axis
    to make it easier to compare this residual plot with the plot for the one-variable
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in09.png)'
  prefs: []
  type: TYPE_IMG
- en: The larger variability in the errors for higher AUM is even more evident. The
    implications are that the estimates, <math><mrow><mover><mi>y</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    , are unaffected, but their accuracy depends on AUM. This problem can be addressed
    with *weighted regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once again, we point out that data scientists from different backgrounds use
    different terminology to refer to the same concept. For example, the terminology
    that calls each row in the design matrix <math><mtext mathvariant="bold">X</mtext></math>
    an observation and each column a variable is more common among people with backgrounds
    in statistics. Others say that each column of the design matrix represents a *feature*
    or that each row represents a *record*. Also, we say that our overall process
    of fitting and interpreting models is called *modeling*, while others call it
    *machine learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s fit a multiple linear model that uses all seven variables to explain
    upward mobility. After fitting the model, we again plot the errors using the same
    y-axis scale as in the previous two residual plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model with seven features does not appear to be much better than the two-variable
    model. In fact, the standard deviation of the residuals has only decreased by 8%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compare the multiple <math><msup><mi>R</mi> <mn>2</mn></msup></math>
    for these three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The adjustment for the number of features in the model makes little difference
    for us since we have over 700 observations. Now we have confirmed our earlier
    findings that using two variables greatly improves the explanatory capability
    of the model, and the seven-variable model offers little improvement over the
    two-variable model. The small gain is likely not worth the added complexity of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: So far, our models have used only numeric predictor variables. But categorical
    data is often useful for model fitting as well. Additionally, in [Chapter 10](ch10.html#ch-eda)
    we transformed variables and created new variables from combinations of variables.
    We address how to incorporate these variables into linear models next.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering for Numeric Measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the models that we have fit so far in this chapter have used numeric
    features that were originally provided in the dataframe. In this section, we look
    at variables that are created from transformations of numeric features. Transforming
    variables to use in modeling is called *feature engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced feature engineering in Chapters [9](ch09.html#ch-wrangling) and
    [10](ch10.html#ch-eda). There, we transformed features so that they had symmetric
    distributions. Transformations can capture more kinds of patterns in the data
    and lead to better and more accurate models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s return to the dataset we used as an example in [Chapter 10](ch10.html#ch-eda):
    house sale prices in the San Francisco Bay Area. We restrict the data to houses
    sold in 2006, when sale prices were relatively stable, so we don’t need to account
    for trends in price.'
  prefs: []
  type: TYPE_NORMAL
- en: We wish to model sale price. Recall that visualizations in [Chapter 10](ch10.html#ch-eda)
    showed us that sale price was related to several features, like the size of the
    house, size of the lot, number of bedrooms, and location. We log-transformed both
    sale price and the size of the house to improve their relationship, and we saw
    that box plots of sale price by the number of bedrooms and box plots by city revealed
    interesting relationships too. In this section, we include transformed numeric
    features in a linear model. In the next section, we also add an ordinal feature
    (the number of bedrooms) and a nominal feature (the city) to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we’ll model sale price on house size. The correlation matrix tell
    us which of our numeric explanatory variables (original and transformed) is most
    strongly correlated with sale price:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | price | br | lsqft | bsqft | log_price | log_bsqft | log_lsqft | ppsf
    | log_ppsf |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **price** | 1.00 | 0.45 | 0.59 | 0.79 | 0.94 | 0.74 | 0.62 | 0.49 | 0.47
    |'
  prefs: []
  type: TYPE_TB
- en: '| **br** | 0.45 | 1.00 | 0.29 | 0.67 | 0.47 | 0.71 | 0.38 | -0.18 | -0.21 |'
  prefs: []
  type: TYPE_TB
- en: '| **lsqft** | 0.59 | 0.29 | 1.00 | 0.46 | 0.55 | 0.44 | 0.85 | 0.29 | 0.27
    |'
  prefs: []
  type: TYPE_TB
- en: '| **bsqft** | 0.79 | 0.67 | 0.46 | 1.00 | 0.76 | 0.96 | 0.52 | -0.08 | -0.10
    |'
  prefs: []
  type: TYPE_TB
- en: '| **log_price** | 0.94 | 0.47 | 0.55 | 0.76 | 1.00 | 0.78 | 0.62 | 0.51 | 0.52
    |'
  prefs: []
  type: TYPE_TB
- en: '| **log_bsqft** | 0.74 | 0.71 | 0.44 | 0.96 | 0.78 | 1.00 | 0.52 | -0.11 |
    -0.14 |'
  prefs: []
  type: TYPE_TB
- en: '| **log_lsqft** | 0.62 | 0.38 | 0.85 | 0.52 | 0.62 | 0.52 | 1.00 | 0.29 | 0.27
    |'
  prefs: []
  type: TYPE_TB
- en: '| **ppsf** | 0.49 | -0.18 | 0.29 | -0.08 | 0.51 | -0.11 | 0.29 | 1.00 | 0.96
    |'
  prefs: []
  type: TYPE_TB
- en: '| **log_ppsf** | 0.47 | -0.21 | 0.27 | -0.10 | 0.52 | -0.14 | 0.27 | 0.96 |
    1.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Sale price correlates most highly with house size, called `bsqft` for building
    square feet. We make a scatterplot of sale price against house size to confirm
    the association is linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_15in11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The relationship does look roughly linear, but the very large and expensive
    houses are far from the center of the distribution and can overly influence the
    model. As shown in [Chapter 10](ch10.html#ch-eda), the log transformation makes
    the distributions of price and size more symmetric (both are log base 10 to make
    it easier to convert the values into the original units):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_15in12.png)'
  prefs: []
  type: TYPE_IMG
- en: Ideally, a model that uses transformations should make sense in the context
    of the data. If we fit a simple linear model based on log(size), then when we
    examine the coefficient, we think in terms of a percentage increase. For example,
    a doubling of <math><mi>x</mi></math> increases the prediction by <math><mi>θ</mi>
    <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>2</mn> <mo stretchy="false">)</mo></math>
    , since <math><mi>θ</mi> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>2</mn>
    <mi>x</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mi>θ</mi> <mi>log</mi> <mo>⁡</mo>
    <mo stretchy="false">(</mo> <mn>2</mn> <mo stretchy="false">)</mo> <mo>+</mo>
    <mi>θ</mi> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>x</mi> <mo
    stretchy="false">)</mo></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by fitting a model that explains log-transformed price by the house’s
    log-transformed size. But first, we note that this model is still considered a
    linear model. If we represent sale price by <math><mi>y</mi></math> and house
    size by <math><mi>x</mi></math> , then the model is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>y</mi> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>log</mi> <mo>⁡</mo>
    <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: '(Note that we have ignored the approximation in this equation to make the linear
    relationship clearer.) This equation may not seem linear, but if we rename <math><mi>log</mi>
    <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>y</mi> <mo stretchy="false">)</mo></math>
    to <math><mi>w</mi></math> and <math><mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>x</mi> <mo stretchy="false">)</mo></math> to <math><mi>v</mi></math> , then
    we can express this “log–log” relationship as a linear model in <math><mi>w</mi></math>
    and <math><mi>v</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>w</mi>  <mo>=</mo>  <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>v</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Other examples of models that can be expressed as linear combinations of transformed
    features are:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>y</mi> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi></mtd></mtr>
    <mtr><mtd><mi>y</mi></mtd> <mtd><mo>=</mo>  <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>θ</mi>
    <mn>2</mn></msub> <msup><mi>x</mi> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mi>y</mi></mtd>
    <mtd><mo>=</mo>  <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub> <mi>z</mi>
    <mo>+</mo> <msub><mi>θ</mi> <mn>3</mn></msub> <mi>x</mi> <mi>z</mi></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, if we rename <math><mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>y</mi> <mo stretchy="false">)</mo></math> to <math><mi>w</mi></math> , <math><msup><mi>x</mi>
    <mn>2</mn></msup></math> to <math><mi>u</mi></math> , and <math><mi>x</mi> <mi>z</mi></math>
    as <math><mi>t</mi></math> , then we can express each of these models as linear
    in these renamed features. In order, the preceding models are now:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>w</mi></mtd> <mtd><mo>=</mo>  <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi></mtd></mtr>
    <mtr><mtd><mi>y</mi></mtd> <mtd><mo>=</mo>  <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>θ</mi>
    <mn>2</mn></msub> <mi>u</mi></mtd></mtr> <mtr><mtd><mi>y</mi></mtd> <mtd><mo>=</mo>  <msub><mi>θ</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo>
    <msub><mi>θ</mi> <mn>2</mn></msub> <mi>z</mi> <mo>+</mo> <msub><mi>θ</mi> <mn>3</mn></msub>
    <mi>t</mi></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: In short, we can think of models that include nonlinear transformations of features
    and/or combinations of features as linear in their derived features. In practice,
    we don’t rename the transformed features when we describe the model; instead,
    we write the model using the transformations of the original features because
    it’s important to keep track of them, especially when interpreting the coefficients
    and checking residual plots.
  prefs: []
  type: TYPE_NORMAL
- en: When we refer to these models, we include mention of the transformations. That
    is, we call a model *log–log* when both the outcome and explanatory variables
    are log-transformed; we say it’s *log–linear* when the outcome is log-transformed
    but not the explanatory variable; we describe a model as having *polynomial features*
    of, say, degree two when the first and second power transformations of the explanatory
    variable are included; and we say a model includes an *interaction term* between
    two explanatory features when the product of these two features is included in
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit a log–log model of price on size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The coefficients and predicted values from this model cannot be directly compared
    to a model fitted using linear features because the units are the log of dollars
    and log of square feet, not dollars and square feet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we examine the residuals and predicted values with a plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in13.png)'
  prefs: []
  type: TYPE_IMG
- en: The residual plot looks reasonable, but it contains thousands of points, which
    makes it hard to see curvature.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see if additional variables might be helpful, we can plot the residuals
    from the fitted model against a variable that is not in the model. If we see patterns,
    that indicates we might want to include this additional feature or a transformation
    of it. Earlier, we found that the distribution of price was related to the city
    where the house is located, so let’s examine the relationship between the residuals
    and city:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_15in14.png)'
  prefs: []
  type: TYPE_IMG
- en: This plot shows us that the distribution of errors appears shifted by city.
    Ideally, the median of each city’s box plot lines up with 0 on the y-axis. Instead,
    more than 75% of the houses sold in Piedmont have positive errors, meaning the
    actual sale price is above the predicted value. And at the other extreme, more
    than 75% of sale prices in Richmond fall below their predicted values. These patterns
    suggest that we should include city in the model. From a context point of view,
    it makes sense for location to impact sale price. In the next section, we show
    how to incorporate a nominal variable into a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering for Categorical Measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first model we ever fit was the constant model in [Chapter 4](ch04.html#ch-modeling).
    There, we minimized squared loss to find the best-fitting constant:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><munder><mo movablelimits="true">min</mo> <mi>c</mi></munder>
    <munder><mo>∑</mo> <mi>i</mi></munder> <mo stretchy="false">(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mi>c</mi> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of including a nominal feature in a model in a similar fashion.
    That is, we find the best-fitting constant to each subgroup of the data corresponding
    to a category:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><munder><mo movablelimits="true">min</mo>
    <mrow><msub><mi>c</mi> <mi>B</mi></msub></mrow></munder> <munder><mo>∑</mo> <mrow><mi>i</mi>
    <mo>∈</mo> <mtext>Berkeley</mtext></mrow></munder> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mi>c</mi> <mi>B</mi></msub>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd> <mtd><munder><mo movablelimits="true">min</mo>
    <mrow><msub><mi>c</mi> <mi>L</mi></msub></mrow></munder> <munder><mo>∑</mo> <mrow><mi>i</mi>
    <mo>∈</mo> <mtext>Lamorinda</mtext></mrow></munder> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mi>c</mi> <mi>L</mi></msub>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><munder><mo
    movablelimits="true">min</mo> <mrow><msub><mi>c</mi> <mi>P</mi></msub></mrow></munder>
    <munder><mo>∑</mo> <mrow><mi>i</mi> <mo>∈</mo> <mtext>Piedmont</mtext></mrow></munder>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mi>c</mi>
    <mi>P</mi></msub> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd> <mtd>   <munder><mo
    movablelimits="true">min</mo> <mrow><msub><mi>c</mi> <mi>R</mi></msub></mrow></munder>
    <munder><mo>∑</mo> <mrow><mi>i</mi> <mo>∈</mo> <mtext>Richmond</mtext></mrow></munder>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mi>c</mi>
    <mi>R</mi></msub> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Another way to describe this model is with *one-hot encoding*.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding takes a categorical feature and creates multiple numeric features
    that have only the values 0 or 1\. To one-hot encode a feature, we create new
    features, one for each unique category. In this case, since we have four cities—Berkeley,
    Lamorinda, Piedmont, and Richmond—we create four new features in a design matrix,
    called <math><msub><mi>X</mi> <mrow><mi>c</mi> <mi>i</mi> <mi>t</mi> <mi>y</mi></mrow></msub></math>
    . Each row in <math><msub><mi>X</mi> <mrow><mi>c</mi> <mi>i</mi> <mi>t</mi> <mi>y</mi></mrow></msub></math>
    contains one value of 1, and it appears in the column that corresponds to the
    city. All other columns contain 0 for that row. [Figure 15-6](#fig-one-hot2) illustrates
    this notion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-6\. One-hot encoding for a categorical feature with six rows (left)
    and its resulting design matrix (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we can concisely represent the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mi>θ</mi> <mi>B</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi>
    <mo>,</mo> <mi>B</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi> <mi>L</mi></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>L</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>P</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>P</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>R</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>R</mi></mrow></msub></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have indexed the columns of the design matrix by <math><mi>B</mi></math>
    , <math><mi>L</mi></math> , <math><mi>P</mi></math> , and <math><mi>R</mi></math>
    , rather than <math><mi>j</mi></math> , to make it clear that each column represents
    a column of 0s and 1s where, say, a 1 appears for <math><msub><mi>x</mi> <mrow><mi>i</mi>
    <mo>,</mo> <mi>P</mi></mrow></msub></math> if the <math><mi>i</mi></math> th house
    is located in Piedmont.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One-hot encoding creates features that have only 0-1 values. These features
    are also known as *dummy variable* or *indicator variable*. The term “dummy variable”
    is more common in econometrics, and the usage of “indicator variable” is more
    common in statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to minimize least square loss over <math><mi mathvariant="bold-italic">θ</mi></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mo fence="false" stretchy="false">‖</mo>
    <mrow><mi mathvariant="bold">y</mi></mrow> <mo>−</mo> <mtext mathvariant="bold">X</mtext>
    <mi mathvariant="bold-italic">θ</mi> <msup><mo fence="false" stretchy="false">‖</mo>
    <mn>2</mn></msup></mtd> <mtd><mo>=</mo> <munder><mo>∑</mo> <mi>i</mi></munder>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mi>θ</mi>
    <mi>B</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>B</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>L</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>L</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>P</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>P</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>R</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>R</mi></mrow></msub>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>i</mi> <mo>∈</mo> <mi>B</mi> <mi>e</mi> <mi>r</mi>
    <mi>k</mi> <mi>e</mi> <mi>l</mi> <mi>e</mi> <mi>y</mi></mrow></munder> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mi>θ</mi> <mi>B</mi></msub>
    <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>B</mi></mrow></msub> <msup><mo
    stretchy="false">)</mo> <mn>2</mn></msup>  <mo>+</mo>  <munder><mo>∑</mo> <mrow><mi>i</mi>
    <mo>∈</mo> <mi>L</mi> <mi>a</mi> <mi>m</mi> <mi>o</mi> <mi>r</mi> <mi>i</mi> <mi>n</mi>
    <mi>d</mi> <mi>a</mi></mrow></munder> <mo stretchy="false">(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <msub><mi>θ</mi> <mi>L</mi></msub> <msub><mi>x</mi>
    <mrow><mi>i</mi> <mo>,</mo> <mi>L</mi></mrow></msub> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></mtd></mtr> <mtr><mtd> <mo>+</mo>  <munder><mo>∑</mo> <mrow><mi>i</mi>
    <mo>∈</mo> <mi>P</mi> <mi>i</mi> <mi>e</mi> <mi>d</mi> <mi>m</mi> <mi>o</mi> <mi>n</mi>
    <mi>t</mi></mrow></munder> <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>−</mo> <msub><mi>θ</mi> <mi>P</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi>
    <mo>,</mo> <mi>P</mi></mrow></msub> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>  <mo>+</mo>  <munder><mo>∑</mo>
    <mrow><mi>i</mi> <mo>∈</mo> <mi>R</mi> <mi>i</mi> <mi>c</mi> <mi>h</mi> <mi>m</mi>
    <mi>o</mi> <mi>n</mi> <mi>d</mi></mrow></munder> <mo stretchy="false">(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <msub><mi>θ</mi> <mi>R</mi></msub> <msub><mi>x</mi>
    <mrow><mi>i</mi> <mo>,</mo> <mi>R</mi></mrow></msub> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math><mi mathvariant="bold-italic">θ</mi></math> is the column vector
    <math><mo stretchy="false">[</mo> <msub><mi>θ</mi> <mi>B</mi></msub> <mo>,</mo>
    <msub><mi>θ</mi> <mi>L</mi></msub> <mo>,</mo> <msub><mi>θ</mi> <mi>P</mi></msub>
    <mo>,</mo> <msub><mi>θ</mi> <mi>R</mi></msub> <mo stretchy="false">]</mo></math>
    . Notice that this minimization reduces to four minimizations, one for each city.
    That’s the idea that we started with at the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `OneHotEncoder` to create this design matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '|   | Berkeley | Lamorinda | Piedmont | Richmond |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **2664** | 0.0 | 0.0 | 0.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2665** | 0.0 | 0.0 | 0.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2666** | 0.0 | 0.0 | 0.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s fit a model using these one-hot encoded features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'And examine the multiple <math><msup><mi>R</mi> <mn>2</mn></msup></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If we only know the city where a house is located, the model does a reasonably
    good job of estimating its sale price. Here are the coefficients from the fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As expected from the box plots, the estimated sale price (in log $) depends
    on the city. But if we know the size of the house as well as the city, we should
    have an even better model. We saw earlier that the simple log–log model that explains
    sale price by house size fits reasonably well, so we expect that the city feature
    (as one-hot encoded variables) should further improve the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such a model looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mi>y</mi> <mi>i</mi></msub>  <mo>≈</mo>  <msub><mi>θ</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mi>B</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>B</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>L</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>L</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>P</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>P</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>R</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>R</mi></mrow></msub></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that this model describes the relationship between log(price), which
    is represented as <math><mi>y</mi></math> , and log(size), which is represented
    as <math><mi>x</mi></math> , as linear with the same coefficient for log(size)
    for each city. But the intercept term depends on the city:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left right" columnspacing="0em
    2em" displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mi>y</mi> <mi>i</mi></msub></mtd>
    <mtd><mo>≈</mo>  <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mi>B</mi></msub></mtd> <mtd><mtext>for houses in
    Berkeley</mtext></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mi>i</mi></msub></mtd>
    <mtd><mo>≈</mo>  <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mi>L</mi></msub></mtd> <mtd><mtext>for houses in
    Lamorinda</mtext></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mi>i</mi></msub></mtd>
    <mtd><mo>≈</mo>  <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mi>P</mi></msub></mtd> <mtd><mtext>for houses in
    Piedmont</mtext></mtd></mtr> <mtr><mtd><msub><mi>y</mi> <mi>i</mi></msub></mtd>
    <mtd><mo>≈</mo>  <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mi>R</mi></msub></mtd> <mtd><mtext>for houses in
    Richmond</mtext></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We next make a facet of scatterplots, one for each city, to see if this relationship
    roughly holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The shift is evident in the scatterplot. We concatenate our two design matrices
    together to fit the model that includes size and city:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '|   | log_bsqft | Berkeley | Lamorinda | Piedmont | Richmond |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 3.14 | 1.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 3.31 | 1.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2.96 | 1.0 | 0.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **2664** | 3.16 | 0.0 | 0.0 | 0.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2665** | 3.47 | 0.0 | 0.0 | 0.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2666** | 3.44 | 0.0 | 0.0 | 0.0 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s fit a model that incorporates the quantitative feature, the house
    size, and the qualitative feature, location (city):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The intercepts reflect which cities have more expensive houses, even taking
    into account the size of the house:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This fit, which includes the nominal variable `city` and the log-transformed
    house size, is better than both the simple log–log model with house size and the
    model that fits constants for each city.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that we dropped the intercept from the model so that each subgroup has
    its own intercept. However, a common practice is to remove one of the one-hot
    encoded features from the design matrix and keep the intercept. For example, if
    we drop the feature for Berkeley houses and add the intercept, then the model
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mi>θ</mi> <mn>0</mn></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>L</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>L</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>P</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>P</mi></mrow></msub>  <mo>+</mo>  <msub><mi>θ</mi>
    <mi>R</mi></msub> <msub><mi>x</mi> <mrow><mi>i</mi> <mo>,</mo> <mi>R</mi></mrow></msub></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The meaning of the coefficients for the dummy variables has changed in this
    representation. For example, consider this equation for a house in Berkeley and
    a house in Piedmont:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left right" columnspacing="0em
    2em" displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mi>θ</mi> <mn>0</mn></msub></mtd>
    <mtd><mo>+</mo>  <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub></mtd>
    <mtd><mtext>for a house in Berkeley</mtext></mtd></mtr> <mtr><mtd><msub><mi>θ</mi>
    <mn>0</mn></msub></mtd> <mtd><mo>+</mo>  <msub><mi>θ</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>θ</mi> <mi>P</mi></msub></mtd> <mtd><mtext>for
    a house in Piedmont</mtext></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: In this representation, the intercept <math><msub><mi>θ</mi> <mn>0</mn></msub></math>
    is for Berkeley houses, and the coefficient <math><msub><mi>θ</mi> <mi>P</mi></msub></math>
    measures the typical difference between a Piedmont house and a Berkeley house.
    In this representation, we can more easily compare <math><msub><mi>θ</mi> <mi>P</mi></msub></math>
    to 0 to see if these two cities have essentially the same average price.
  prefs: []
  type: TYPE_NORMAL
- en: If we include the intercept and all of the city variables, then the columns
    of the design matrix are linearly dependent, which means that we can’t solve for
    the coefficients. Our predictions will be the same in either case, but there will
    not be a unique solution to the minimization.
  prefs: []
  type: TYPE_NORMAL
- en: We also prefer the representation of the model that drops one dummy variable
    and includes an intercept term when we include one-hot encodings of two categorical
    variables. This practice maintains consistency in the interpretation of the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'We demonstrate how to build a model with two sets of dummy variables, using
    the `statsmodels` library. This library uses a formula language to describe the
    model to fit, so we don’t need to create the design matrix ourselves. We import
    the formula API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s first repeat our fit of the model with the nominal variable `city` and
    house size to show how to use the formula language and compare the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The string provided for the `formula` parameter describes the model to fit.
    The model has `log_price` as the outcome and fits a linear combination of `log_bsqft`
    and `city` as explanatory variables. Notice that we do not need to create dummy
    variables to fit the model. Conveniently, `smf.ols` does the one-hot encoding
    of the city feature for us. The fitted coefficients of the following model include
    an intercept term and drop the Berkeley indicator variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to drop the intercept, we can add –1 to the formula, which is a
    convention that indicates dropping the column of ones from the design matrix.
    In this particular example, the space spanned by all of the one-hot encoded features
    is equivalent to the space spanned by the 1 vector and all but one of the dummy
    variables, so the fit is the same. However, the coefficients are different as
    they reflect the different parameterization of the design matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, we can add interaction terms between the city and size variables
    to allow each city to have a different coefficient for size. We specify this in
    the formula by adding the term `log_bsqft:city`. We don’t go into details here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s fit a model with two categorical variables: the number of bedrooms
    and the city. Recall that we earlier reassigned the count of bedrooms that were
    above 6 to 6, which essentially collapses 6, 7, 8, … into the category 6+. We
    can see this relationship in the box plots of price (log $) by the number of bedrooms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_15in16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The relationship does not appear linear: for each additional bedroom, the sale
    price does not increase by the same amount. Given that the number of bedrooms
    is discrete, we can treat this feature as categorical, which allows each bedroom
    encoding to contribute a different amount to the cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: We have used the term `C(br)` in the formula to indicate that we want the number
    of bedrooms, which is numeric, to be treated like a categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the multiple <math><msup><mi>R</mi> <mn>2</mn></msup></math>
    from the fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The multiple <math><msup><mi>R</mi> <mn>2</mn></msup></math> has not increased
    even though we have added five more one-hot encoded features. The <math><msup><mi>R</mi>
    <mn>2</mn></msup></math> is adjusted for the number of parameters in the model
    and by this measure is no better than the earlier one that included only city
    and size.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduced feature engineering for qualitative features.
    We saw how the one-hot encoding technique lets us include categorical data in
    linear models and gives a natural interpretation for model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear models help us describe relationships between features. We discussed
    the simple linear model and extended it to linear models in multiple variables.
    Along the way, we applied mathematical techniques that are widely useful in modeling—calculus
    to minimize loss for the simple linear model and matrix geometry for the multiple
    linear model.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models may seem basic, but they are used for all sorts of tasks today.
    And they are flexible enough to allow us to include categorical features as well
    as nonlinear transformations of variables, such as log transformations, polynomials,
    and ratios. Linear models have the advantage of being broadly interpretable for
    nontechnical people, yet sophisticated enough to capture many common patterns
    in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be tempting to throw all of the variables available to us into a model
    to get the “best fit possible.” But we should keep in mind the geometry of least
    squares when fitting models. Recall that <math><mi>p</mi></math> explanatory variables
    can be thought of as <math><mi>p</mi></math> vectors in <math><mi>n</mi></math>
    -dimensional space, and if these vectors are highly correlated, then the projections
    onto this space will be similar to projections onto smaller spaces made up of
    fewer vectors. This implies that:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding more variables may not provide a large improvement in the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretation of the coefficients can be difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several models can be equally effective in predicting/explaining the response
    variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are concerned with making inferences, where we want to interpret/understand
    the model, then we should err on the side of simpler models. On the other hand,
    if our primary concern is the predictive ability of a model, then we tend not
    to concern ourselves with the number of coefficients and their interpretation.
    But this “black box” approach can lead to models that, say, overly depend on anomalous
    values in the data or models that are inadequate in other ways. So be careful
    with this approach, especially when the predictions may be harmful to people.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we used linear models in a descriptive way. We introduced a
    few notions for deciding when to include a feature in a model by examining residuals
    for patterns, comparing the size of standard errors and the change in the multiple
    <math><msup><mi>R</mi> <mn>2</mn></msup></math> . Oftentimes, we settled for a
    simpler model that was easier to interpret. In the next chapter, we look at other,
    more formal tools for choosing the features to include in a model.
  prefs: []
  type: TYPE_NORMAL
