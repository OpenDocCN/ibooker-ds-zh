- en: Chapter 17\. Theory for Inference and Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you want to generalize your findings beyond descriptions for your collection
    of data to a larger setting, the data needs to be representative of that larger
    world. For example, you may want to predict air quality at a future time based
    on a sensor reading ([Chapter 12](ch12.html#ch-pa)), test whether an incentive
    improves the productivity of contributors based on experimental findings ([Chapter 3](ch03.html#ch-theory-datadesign)),
    or construct an interval estimate for the amount of time you might spend waiting
    for a bus ([Chapter 5](ch05.html#ch-bus)). We touched on all of these scenarios
    in earlier chapters. In this chapter, we’ll formalize the framework for making
    predictions and inferences.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of this framework is the notion of a distribution, be it a population,
    empirical (aka sample), or probability distribution. Understanding the connections
    between these distributions is central to the basics of hypothesis testing, confidence
    intervals, prediction bands, and risk. We begin with a brief review of the urn
    model, introduced in [Chapter 3](ch03.html#ch-theory-datadesign), then we introduce
    formal definitions of hypothesis tests, confidence intervals, and prediction bands.
    We use simulation in our examples, including the bootstrap as a special case.
    We wrap up the chapter with formal definitions of expectation, variance, and standard
    error—essential concepts in the theory of testing, inference, and prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributions: Population, Empirical, Sampling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The population, sampling, and empirical distributions are important concepts
    that guide us when we make inferences about a model or predictions for new observations.
    [Figure 17-1](#triptych) provides a diagram that can help distinguish between
    them. The diagram uses the notions of population and access frame from [Chapter 2](ch02.html#ch-data-scope)
    and the urn model from [Chapter 3](ch03.html#ch-theory-datadesign). On the left
    is the population that we are studying, represented as marbles in an urn with
    one marble for each unit. We have simplified the situation to where the access
    frame and the population are the same; that is, we can access every unit in the
    population. (The problems that arise when this is not the case are covered in
    Chapters [2](ch02.html#ch-data-scope) and [3](ch03.html#ch-theory-datadesign).)
    The arrow from the urn to the sample represents the design, meaning the protocol
    for selecting the sample from the frame. The diagram shows this selection process
    as a chance mechanism, represented by draws from an urn filled with indistinguishable
    marbles. On the right side of the diagram, the collection of marbles constitutes
    our sample (the data we got).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1\. Diagram of the data generation process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have kept the diagram simple by considering measurements for just one feature.
    Below the urn in the diagram is the *population histogram* for that feature. The
    population histogram represents the distribution of values across the entire population.
    On the far right, the *empirical histogram* shows the distribution of values for
    our actual sample. Notice that these two distributions are similar in shape. This
    happens when our sampling mechanism produces representative samples.
  prefs: []
  type: TYPE_NORMAL
- en: We are often interested in a summary of the sample measurements, such as the
    mean, median, slope from a simple linear model, and so on. Typically, this summary
    statistic is an estimate for a population parameter, such as the population mean
    or median. The population parameter is shown as <math><msup><mi>θ</mi> <mo>∗</mo></msup></math>
    on the left of the diagram; on the right, the summary statistic, calculated from
    the sample, is <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: The chance mechanism that generates our sample might well produce a different
    set of data if we were to conduct our investigation over again. But if the protocols
    are well designed, we expect the sample to still resemble the population. In other
    words, we can infer the population parameter from the summary statistic calculated
    from the sample. The *sampling distribution* in the middle of the diagram is a
    *probability distribution* for the statistic. It shows the possible values that
    the statistic might take for different samples and their chances. In [Chapter 3](ch03.html#ch-theory-datadesign),
    we used simulation to estimate the sampling distribution in several examples.
    In this chapter, we revisit these and other examples from earlier chapters to
    formalize the analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'One last point about these three histograms: as introduced in [Chapter 10](ch10.html#ch-eda),
    the rectangles provide the fraction of observations in any bin. In the case of
    the population histogram, this is the fraction of the entire population; for the
    empirical histogram, the area represents the fraction in the sample; and for the
    sampling distribution, the area represents the chance the data generation mechanism
    would produce a sample statistic in this bin.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we typically don’t know the population distribution or parameter, and
    we try to infer the parameter or predict values for unseen units in the population.
    At other times, a conjecture about the population can be tested using the sample.
    Testing is the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of Hypothesis Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our experience, hypothesis testing is one of the more challenging areas
    of data science—challenging to learn and challenging to apply. This is not necessarily
    because hypothesis testing is deeply technical; rather, hypothesis testing can
    be counterintuitive because it makes use of contradictions. As the name suggests,
    we often start hypothesis testing with a *hypothesis*: a statement about the world
    that we would like to verify.'
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, we would directly prove our hypothesis is true. Unfortunately,
    we often don’t have access to all the information needed to determine the truth.
    For example, we might hypothesize that a new vaccine is effective, but contemporary
    medicine doesn’t yet understand all the details of the biology that govern vaccine
    efficacy. Instead, we turn to the tools of probability, random sampling, and data
    design.
  prefs: []
  type: TYPE_NORMAL
- en: One reason hypothesis testing can be confusing is that it’s a lot like “proof
    by contradiction,” where we assume the opposite of our hypothesis is true and
    try to show that the data we observe is inconsistent with that assumption. We
    approach the problem this way because often, something can be true for many reasons,
    but we only need a single example to contradict an assumption. We call this “opposite
    hypothesis” the *null hypothesis* and our original hypothesis the *alternative
    hypothesis*.
  prefs: []
  type: TYPE_NORMAL
- en: To make matters a bit more confusing, the tools of probability don’t directly
    prove or disprove things. Instead, they tell us how likely or unlikely something
    we observe is under assumptions, like the assumptions of the null hypothesis.
    That’s why it’s so important to design the data collection well.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the randomized clinical trial of the J&J vaccine ([Chapter 3](ch03.html#ch-theory-datadesign)),
    where 43,738 people enrolled in the trial were randomly split into two equal groups.
    The treatment group was given the vaccine and the control was given a fake vaccine,
    called a placebo. This random assignment created two groups that were similar
    in every way except for the vaccine.
  prefs: []
  type: TYPE_NORMAL
- en: In this trial, 117 people in the treatment group fell ill and 351 in the control
    group got sick. Since we want to provide convincing evidence that the vaccine
    works, we start with a null hypothesis that it doesn’t work, meaning it was just
    by chance that the random assignment led to so few illnesses in the treatment
    group. We can then use probability to calculate the chance of observing so few
    sick people in the treatment group. The probability calculations are based on
    the urn that has 43,738 marbles in it, with 468 marked 1 to denote a sick person.
    We then found that the probability of at most 117 marbles being drawn in 21,869
    draws with replacement from the urn was nearly zero. We take this as evidence
    to reject the null hypothesis in favor of the alternative hypothesis that the
    vaccine works. Because the J&J experiment was well designed, a rejection of the
    null leads us to conclude that the vaccine works. In other words, the truth of
    the hypothesis is left to us and how willing we are to be potentially wrong.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this section, we go over the four basic steps of a hypothesis
    test. We then provide two examples that continue two of the examples from [Chapter 3](ch03.html#ch-theory-datadesign),
    and delve deeper into the formalities for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four basic steps to hypothesis testing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Set up'
  prefs: []
  type: TYPE_NORMAL
- en: You have your data, and you want to test whether a particular model is reasonably
    consistent with the data. So you specify a statistic, <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> , such as the sample average,
    fraction of zeros in a sample, or fitted regression coefficient, with the goal
    of comparing your data’s statistic to what might have been produced under the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Model'
  prefs: []
  type: TYPE_NORMAL
- en: You spell out the model that you want to test in the form of a data generation
    mechanism, along with any specific assumptions about the population. This model
    typically includes specifying <math><msup><mi>θ</mi> <mo>∗</mo></msup></math>
    , which may be the population mean, the proportion of zeros, or a regression coefficient.
    The sampling distribution of the statistic under this model is referred to as
    the *null distribution*, and the model itself is called the *null hypothesis*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Compute'
  prefs: []
  type: TYPE_NORMAL
- en: How likely, according to the null model in step 2, is it to get data (and the
    resulting statistic) at least as extreme as what you actually got in step 1? In
    formal inference, this probability is called the <math><mi>p</mi></math> -*value*.
    To approximate the <math><mi>p</mi></math> -value, we often use the computer to
    generate a large number of repeated random trials using the assumptions in the
    model and find the fraction of samples that give a value of the statistic at least
    as extreme as our observed value. Other times, we can instead use mathematical
    theory to find the <math><mi>p</mi></math> -value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Interpret'
  prefs: []
  type: TYPE_NORMAL
- en: The <math><mi>p</mi></math> -value is used as a measure of surprise. If the
    model that you spelled out in step 2 is believable, how surprised should you be
    to get the data (and summary statistic) that you actually got? A moderately sized
    <math><mi>p</mi></math> -value means that the observed statistic is pretty much
    what you would expect to get for data generated by the null model. A tiny <math><mi>p</mi></math>
    -value raises doubts about the null model. In other words, if the model is correct
    (or approximately correct), then it would be very unusual to get such an extreme
    value of the test statistic from data generated by the model. In this case, either
    the null model is wrong or a very unlikely outcome has occurred. Statistical logic
    says to conclude that the pattern is real, that it is more than just coincidence.
    Then it’s up to you to explain why the data generation process led to such an
    unusual value. This is when a careful consideration of the scope is important.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s demonstrate these steps in the testing process with a couple of examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: A Rank Test to Compare Productivity of Wikipedia Contributors'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall the Wikipedia example from [Chapter 2](ch02.html#ch-data-scope), where
    a randomly selected set of 200 contributors were chosen from among the top 1%
    of contributors who were active in the past 30 days on the English-language Wikipedia
    and who had never received an award. These 200 contributors were divided at random
    into two groups of 100\. The contributors in one group, the treatment group, were
    each given an informal award, while no one in the other group was given one. All
    200 contributors were followed for 90 days and their activity on Wikipedia recorded.
  prefs: []
  type: TYPE_NORMAL
- en: It has been conjectured that informal awards have a reinforcing effect on volunteer
    work, and this experiment was designed to formally study this conjecture. We carry
    out a hypothesis test based on the rankings of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we read the data into a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|   | experiment | postproductivity |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **min** | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **25%** | 0.0 | 57.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **50%** | 0.5 | 250.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **75%** | 1.0 | 608.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **max** | 1.0 | 2344.0 |'
  prefs: []
  type: TYPE_TB
- en: 'The dataframe has 200 rows, one for each contributor. The feature `experiment`
    is either 0 or 1, depending on whether the contributor was in the control or treatment
    group, respectively, and `postproductivity` is a count of the edits made by the
    contributor in the 90 days after the awards were made. The gap between the quartiles
    (lower, middle, and upper) suggests the distribution of productivity is skewed.
    We make a histogram to confirm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_17in01.png)'
  prefs: []
  type: TYPE_IMG
- en: Indeed, the histogram of post-award productivity is highly skewed, with a spike
    near zero. The skewness suggests a statistic based on the ordering of the values
    from the two samples.
  prefs: []
  type: TYPE_NORMAL
- en: To compute our statistic, we order all productivity values (from both groups)
    from smallest to largest. The smallest value has rank 1, the second smallest rank
    2, and so on, up to the largest value, which has a rank of 200\. We use these
    ranks to compute our statistic, <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    , which is the average rank of the treatment group. We chose this statistic because
    it is insensitive to highly skewed distributions. For example, whether the largest
    value is 700 or 700,000, it still receives the same rank, namely 200\. If the
    informal award incentivizes contributors, then we would expect the average rank
    of the treatment group to be typically higher than the control.
  prefs: []
  type: TYPE_NORMAL
- en: The null model assumes that an informal award has *no* effect on productivity,
    and any difference observed between the treatment and control groups is due to
    the chance process in assigning contributors to groups. The null hypothesis is
    set up for the status quo to be rejected; that is, we hope to find a surprise
    in assuming no effect.
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis can be represented by 100 draws from an urn with 200 marbles,
    marked 1, 2, 3, …, 200\. In this case, the average rank would be <math><mo stretchy="false">(</mo>
    <mn>1</mn> <mo>+</mo> <mn>200</mn> <mo stretchy="false">)</mo> <mrow><mo>/</mo></mrow>
    <mn>2</mn> <mo>=</mo> <mn>100.5</mn></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `rankdata` method in `scipy.stats` to rank the 200 values and compute
    the sum of ranks in the treatment group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s confirm that the average rank of the 200 values is 100.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And find the average rank of the 100 productivity scores in the treatment group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The average rank in the treatment group is higher than expected, but we want
    to figure out if it is an unusually high value. We can use simulation to find
    the sampling distribution for this statistic to see if 113 is a routine value
    or a surprising one.
  prefs: []
  type: TYPE_NORMAL
- en: To carry out this simulation, we set up the urn as the `ranks` array from the
    data. Shuffling the 200 values in the array and taking the first 100 represents
    a randomly sampled treatment group. We write a function to shuffle the array of
    ranks and find the average of the first 100.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our simulation mixes the marbles in the urn, draws 100 times, computes the average
    rank for the 100 draws, and repeats this 100,000 times.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a histogram of the simulated averages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_17in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we expected, the sampling distribution of the average rank is centered on
    100 (100.5 actually) and is bell-shaped. The center of this distribution reflects
    the assumptions of the treatment having no effect. Our observed statistic is well
    outside the typical range of simulated average ranks, and we use this simulated
    sampling distribution to find the approximate <math><mi>p</mi></math> -value for
    observing a statistic at least as big as ours:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is a big surprise. Under the null, the chance of seeing an average rank
    at least as large as ours is about 5 in 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: This test raises doubt about the null model. Statistical logic has us conclude
    that the pattern is real. How do we interpret this? The experiment was well designed.
    The 200 contributors were selected at random from the top 1%, and then they were
    divided at random into two groups. These chance processes say that we can rely
    on the sample of 200 being representative of top contributors, and on the treatment
    and control groups being similar to each other in every way except for the application
    of the treatment (the award). Given the careful design, we conclude that informal
    awards have a positive effect on productivity for top contributors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier, we implemented a simulation to find the <math><mi>p</mi></math> -value
    for our observed statistic. In practice, rank tests are commonly used and made
    available in most statistical software:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The <math><mi>p</mi></math> -value here is twice the <math><mi>p</mi></math>
    -value we computed because we considered only values greater than the observed,
    whereas the `ranksums` test computed the the <math><mi>p</mi></math> -value for
    both sides of the distribution. In our example, we are only interested in an increase
    in productivity, and so use a one-sided <math><mi>p</mi></math> -value, which
    is half the reported value (0.0006) and close to our simulated value.
  prefs: []
  type: TYPE_NORMAL
- en: This somewhat unusual test statistic that uses ranks rather than the actual
    data values was developed in the 1950s and 1960s, before today’s era of powerful
    laptop computers. The mathematical properties of rank statistics is well developed
    and the sampling distribution is well behaved (it is symmetric and shaped like
    the bell curve even for small datasets). Rank tests remain popular for A/B testing
    where samples tend to be highly skewed, and it is common to carry out many, many
    tests where <math><mi>p</mi></math> -values can be computed rapidly from the normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The next example revisits the vaccine efficacy example from [Chapter 3](ch03.html#ch-theory-datadesign).
    There, we encountered a hypothesis test without actually calling it that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: A Test of Proportions for Vaccine Efficacy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The approval of a vaccine is subject to stricter requirements than the simple
    test we performed earlier where we compared the disease counts in the treatment
    group to those of the control group. The CDC requires stronger evidence of success
    based on a comparison of the proportion of sick individuals in each group. To
    explain, we express the sample proportion of sick people in the control and treatment
    groups as <math><msub><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mrow><mi>C</mi></mrow></msub></math> and <math><msub><mrow><mover><mi>p</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mi>T</mi></msub></math> , respectively,
    and use these proportions to compute vaccine efficacy:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo>=</mo> <mfrac><mrow><msub><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mrow><mi>C</mi></mrow></msub> <mo>−</mo> <msub><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mi>T</mi></msub></mrow> <msub><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mi>C</mi></msub></mfrac> <mo>=</mo> <mn>1</mn> <mo>−</mo> <mfrac><msub><mrow><mover><mi>p</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mi>T</mi></msub> <msub><mrow><mover><mi>p</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mi>C</mi></msub></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The observed value of vaccine efficacy in the J&J trial is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mn>1</mn> <mo>−</mo> <mfrac><mrow><mn>117</mn> <mrow><mo>/</mo></mrow>
    <mn>21869</mn></mrow> <mrow><mn>351</mn> <mrow><mo>/</mo></mrow> <mn>21869</mn></mrow></mfrac>
    <mo>=</mo> <mn>1</mn> <mo>−</mo> <mfrac><mn>117</mn> <mn>351</mn></mfrac> <mo>=</mo>
    <mn>0.667</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: If the treatment doesn’t work, the efficacy would be near 0\. The CDC sets a
    standard of 50% for vaccine efficacy, meaning that the efficacy has to exceed
    50% to be approved for distribution. In this situation, the null model assumes
    that vaccine efficacy is 50% ( <math><msup><mi>θ</mi> <mo>∗</mo></msup> <mo>=</mo>
    <mn>0.5</mn></math> ), and any difference of the observed value from the expected
    is due to the chance process in assigning people to groups. Again, we set the
    null hypothesis to be the status quo that the vaccine isn’t effective enough to
    warrant approval, and we hope to find a surprise and reject the null.
  prefs: []
  type: TYPE_NORMAL
- en: With a little algebra, the null model <math><mn>0.5</mn> <mo>=</mo> <mn>1</mn>
    <mo>−</mo> <msub><mi>p</mi> <mi>T</mi></msub> <mrow><mo>/</mo></mrow> <msub><mi>p</mi>
    <mi>C</mi></msub></math> reduces to <math><msub><mi>p</mi> <mi>T</mi></msub> <mo>=</mo>
    <mn>0.5</mn> <msub><mi>p</mi> <mi>C</mi></msub></math> . That is, the null hypothesis
    implies that the proportion of ill people among those receiving the treatment
    is at most half that of the control. Notice that the actual values for the two
    risks ( <math><msub><mi>p</mi> <mi>T</mi></msub></math> and <math><msub><mi>p</mi>
    <mi>C</mi></msub></math> ) are not assumed in the null. That is, the model doesn’t
    assume the treatment doesn’t work, but rather, that its efficacy is no larger
    than 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Our urn model in this situation is a bit different from what we set up in [Chapter 3](ch03.html#ch-theory-datadesign).
    The urn still has 43,738 marbles in it, corresponding to the enrollees in the
    experiment. But now each marble has two numbers on it, which for simplicity appear
    in a pair, such as <math><mo stretchy="false">(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo stretchy="false">)</mo></math> . The number on the left is the response if
    the person receives the treatment, and the number on the right corresponds to
    the response to no treatment (the control). As usual, 1 means they become ill
    and 0 means they stay healthy.
  prefs: []
  type: TYPE_NORMAL
- en: The null model assumes that the proportion of ones on the left of the pair is
    half the proportion on the right. Since we don’t know these two proportions, we
    can use the data to estimate them. There are three types of marbles in the urn
    <math><mo stretchy="false">(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn> <mo stretchy="false">)</mo></math>
    , <math><mo stretchy="false">(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo stretchy="false">)</mo></math>
    , and <math><mo stretchy="false">(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo stretchy="false">)</mo></math>
    . We assume that <math><mo stretchy="false">(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo stretchy="false">)</mo></math> , which corresponds to a person getting ill
    under treatment and not under control, is not possible. We observed 351 people
    getting sick in control and 117 in treatment. With the assumption that the treatment
    rate of illness is half that of the control, we can tray a scenario for the makeup
    of the urn. For example, we can study the case where 117 people in treatment didn’t
    get sick but would have if they were in the control group, so combined, all 585
    people ( <math><mn>351</mn> <mo>+</mo> <mn>117</mn> <mo>+</mo> <mn>117</mn></math>
    ) would get the virus if they didn’t receive the vaccine and half of them would
    not get the virus if they received treatment. [Table 17-1](#vacc-urn) shows these
    counts.
  prefs: []
  type: TYPE_NORMAL
- en: Table 17-1\. Vaccine trial urn
  prefs: []
  type: TYPE_NORMAL
- en: '| Label | Count |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (0, 0) | 43,152 |'
  prefs: []
  type: TYPE_TB
- en: '| (0, 1) | 293 |'
  prefs: []
  type: TYPE_TB
- en: '| (1, 0) | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| (1, 1) | 293 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 43,738 |'
  prefs: []
  type: TYPE_TB
- en: 'We can use these counts to carry out a simulation of the clinical trial and
    compute vaccine efficacy. As shown in [Chapter 3](ch03.html#ch-theory-datadesign),
    the multivariate hypergeometric function simulates draws from an urn when there
    are more than two kinds of marbles. We set up this urn and sampling process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can simulate the clinical trial 100,000 times and calculate the vaccine
    efficacy for each trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_17in03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sampling distribution is centered at 0.5, which agrees with our model assumptions.
    We see that 0.667 is far out in the tail of this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Only a tiny handful of the 100,000 simulations have a vaccine efficacy as large
    as the observed 0.667\. This is a rare event, and that’s why the CDC approved
    the Johnson & Johnson vaccine for distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In this example of hypothesis testing, we were not able to completely specify
    the model, and we had to provide approximate values for <math><msub><mi>p</mi>
    <mi>C</mi></msub></math> and <math><msub><mi>p</mi> <mi>T</mi></msub></math> based
    on our observed values of <math><msub><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mi>C</mi></msub></math> and <math><msub><mrow><mover><mi>p</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mi>T</mi></msub></math> . At times, the null model isn’t entirely specified,
    and we must rely on the data to set up the model. The next section introduces
    a general approach, called the bootstrap, to approximate the model using the data.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping for Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many hypothesis tests the assumptions of the null hypothesis lead to a complete
    specification of a hypothetical population and data design (see [Figure 17-1](#triptych)),
    and we use this specification to simulate the sampling distribution of a statistic.
    For example, the rank test for the Wikipedia experiment led us to sample the integers
    1, …, 200, which we easily simulated. Unfortunately, we can’t always specify the
    population and model completely. To remedy the situation, we substitute the data
    for the population. This substitution is at the heart of the notion of the bootstrap.
    [Figure 17-2](#boot-triptych) updates [Figure 17-1](#triptych) to reflect this
    idea; here the population distribution is replaced by the empirical distribution
    to create what is called the *bootstrap population*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-2\. Diagram of bootstrapping the data generation process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The rationale for the bootstrap goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Your sample looks like the population because it is a representative sample,
    so we replace the population with the sample and call it the bootstrap population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the same data generation process that produced the original sample to get
    a new sample, which is called a *bootstrap sample*, to reflect the change in the
    population. Calculate the statistic on the bootstrap sample in the same manner
    as before and call it the *bootstrap statistic*. The *bootstrap sampling distribution*
    of the bootstrap statistic should be similar in shape and spread to the true sampling
    distribution of the statistic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulate the data generation process many times, using the bootstrap population,
    to get bootstrap samples and their bootstrap statistics. The distribution of the
    simulated bootstrap statistics approximates the bootstrap sampling distribution
    of the bootstrap statistic, which itself approximates the original sampling distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a close look at [Figure 17-2](#boot-triptych) and compare it to [Figure 17-1](#triptych).
    Essentially, the bootstrap simulation involves two approximations: the original
    sample approximates the population, and the simulation approximates the sampling
    distribution. We have been using the second approximation in our examples so far;
    the approximation of the population by the sample is the core notion behind bootstrapping.
    Notice that in [Figure 17-2](#boot-triptych), the distribution of the bootstrap
    population (on the left) looks like the original sample histogram; the sampling
    distribution (in the middle) is still a probability distribution based on the
    same data generation process as in the original study, but it now uses the bootstrap
    population; and the sample distribution (on the right) is a histogram of one sample
    taken from the bootstrap population.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be wondering how to take a simple random sample from your bootstrap
    population and not wind up with the exact same sample each time. After all, if
    your sample has 100 units in it and you use it as your bootstrap population, then
    100 draws from the bootstrap population without replacement will take all of the
    units and give you the same bootstrap sample every time. There are two approaches
    to solving this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: When sampling from the bootstrap population, draw units from the bootstrap population
    with replacement. Essentially, if the original population is very large, then
    there is little difference between sampling with and without replacement. This
    is the more common approach by far.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Blow up the sample” to be the same size as the original population. That is,
    tally the fraction of each unique value in the sample, and add units to the bootstrap
    population so that it is the same size as the original population, while maintaining
    the proportions. For example, if the sample is size 30 and 1/3 of the sample values
    are 0, then a bootstrap population of 750 should include 250 zeros. Once you have
    this bootstrap population, use the original data generation procedure to take
    the bootstrap samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The example of vaccine efficacy used a bootstrap-like process, called the *parameterized
    bootstrap*. Our null model specified 0-1 urns, but we didn’t know how many 0s
    and 1s to put in the urn. We used the sample to determine the proportions of 0s
    and 1s; that is, the sample specified the parameters of the multivariate hypergeometric.
    Next, we use the example of calibrating air quality monitors to show how bootstrapping
    could be used to test a hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s a common mistake to think that the center of the bootstrap sampling distribution
    is the same as the center of the true sampling distribution. If the mean of the
    sample is not 0, then the mean of the bootstrap population is also not 0\. That’s
    why we use the spread of the bootstrap distribution, and not its center, in hypothesis
    testing. The next example shows how we might use the bootstrap to test a hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: The case study on calibrating air quality monitors (see [Chapter 12](ch12.html#ch-pa))
    fit a model to adjust the measurements from an inexpensive monitor to more accurately
    reflect true air quality. This adjustment included a term in the model related
    to humidity. The fitted coefficient was about <math><mn>0.2</mn></math> so that
    on days of high humidity the measurement is adjusted upward more than on days
    of low humidity. However, this coefficient is close to 0, and we might wonder
    whether including humidity in the model is really needed. In other words, we want
    to test the hypothesis that the coefficient for humidity in the linear model is
    0\. Unfortunately, we can’t fully specify the model, because it is based on measurements
    taken over a particular time period from a set of air monitors (both PurpleAir
    and those maintained by the EPA). This is where the bootstrap can help.
  prefs: []
  type: TYPE_NORMAL
- en: Our model makes the assumption that the air quality measurements taken resemble
    the population of measurements. Note that weather conditions, the time of year,
    and the location of the monitors make this statement a bit hand-wavy; what we
    mean here is that the measurements are similar to others taken under the same
    conditions as those when the original measurements were taken. Also, since we
    can imagine a virtually infinite supply of air quality measurements, we think
    of the procedure for generating measurements as draws with replacement from the
    urn. Recall that in [Chapter 2](ch02.html#ch-data-scope) we modeled the urn as
    repeated draws with replacement from an urn of measurement errors. This situation
    is a bit different because we are also including the other factors mentioned already
    (weather, season, location).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model is focused on the coefficient for humidity in the linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mtext>PA</mtext>
    <mo>≈</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mtext>AQ</mtext> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub> <mtext>RH</mtext></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, <math><mtext>PA</mtext></math> refers to the PurpleAir PM2.5 measurement,
    <math><mtext>RH</mtext></math> is the relative humidity, and <math><mtext>AQ</mtext></math>
    stands for the more exact measurement of PM2.5 made by the more accurate AQS monitors.
    The null hypothesis is <math><msub><mi>θ</mi> <mn>2</mn></msub> <mo>=</mo> <mn>0</mn></math>
    ; that is, the null model is the simpler model:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mtext>PA</mtext>
    <mo>≈</mo> <msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub>
    <mtext>AQ</mtext></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: To estimate <math><msub><mi>θ</mi> <mn>2</mn></msub></math> , we use the linear
    model fitting procedure from [Chapter 15](ch15.html#ch-linear).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our bootstrap population consists of the measurements from Georgia that we
    used in [Chapter 15](ch15.html#ch-linear). Now we sample rows from the dataframe
    (which is equivalent to our urn) with replacement using the chance mechanism `randint`.
    This function takes random samples with replacement from a set of integers. We
    use the random sample of indices to create the bootstrap sample from the dataframe.
    Then we fit the linear model and get the coefficient for humidity (our bootstrap
    statistic). The following `boot_stat` function performs this simulation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the design matrix and the outcome variable and check our `boot_stat`
    function once to test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'When we repeat this process 10,000 times, we get an approximation to the bootstrap
    sampling distribution of the bootstrap statistic (the fitted humidity coefficient):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We are interested in the shape and spread of this bootstrap sampling distribution
    (we know that the center will be close to the original coefficient of <math><mn>0.21</mn></math>
    ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_17in04.png)'
  prefs: []
  type: TYPE_IMG
- en: By design, the center of the bootstrap sampling distribution will be near <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> because the bootstrap population
    consists of the observed data. So, rather than compute the chance of a value at
    least as large as the observed statistic, we find the chance of a value at least
    as small as 0\. The hypothesized value of 0 is far from the sampling distribution.
  prefs: []
  type: TYPE_NORMAL
- en: None of the 10,000 simulated regression coefficients are as small as the hypothesized
    coefficient. Statistical logic leads us to reject the null hypothesis that we
    do not need to adjust the model for humidity.
  prefs: []
  type: TYPE_NORMAL
- en: The form of the hypothesis test we performed here looks different than the earlier
    tests because the sampling distribution of the statistic is not centered on the
    null. That is because we are using the bootstrap to create the sampling distribution.
    We are, in effect, using a confidence interval for the coefficient to test the
    hypothesis. In the next section we introduce interval estimates more generally,
    including those based on the bootstrap, and we connect the concepts of hypothesis
    testing and confidence intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of Confidence Intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen that modeling leads to estimates, such as the typical time that
    a bus is late ([Chapter 4](ch04.html#ch-modeling)), a humidity adjustment to an
    air quality measurement ([Chapter 15](ch15.html#ch-linear)), and an estimate of
    vaccine efficacy ([Chapter 2](ch02.html#ch-data-scope)). These examples are point
    estimates for unknown values, called *parameters*: the median lateness of the
    bus is 0.74 minutes; the humidity adjustment to air quality is 0.21 PM2.5 per
    humidity percentage point; and the ratio of COVID infection rates in vaccine efficacy
    is 0.67\. However, a different sample would have produced a different estimate.
    Simply providing a point estimate doesn’t give a sense of the estimate’s precision.
    Alternatively, an interval estimate can reflect the estimate’s accuracy. These
    intervals typically take one of two forms:'
  prefs: []
  type: TYPE_NORMAL
- en: A *bootstrap confidence interval* created from the percentiles of the bootstrap
    sampling distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *normal confidence interval* constructed using the standard error (SE) of
    the sampling distribution and additional assumptions about the distribution having
    the shape of a normal curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We describe these two types of intervals and then give an example. Recall that
    the sampling distribution (see [Figure 17-1](#triptych)) is a probability distribution
    that reflects the chance of observing different values of <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> . Confidence intervals are constructed
    from the spread of the sampling distribution of <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> , so the endpoints of the interval
    are random because they are based on <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    . These intervals are designed so that 95% of the time the interval covers <math><msup><mi>θ</mi>
    <mo>∗</mo></msup></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'As its name suggests, the percentile-based bootstrap confidence interval is
    created from the percentiles of the bootstrap sampling distribution. Specifically,
    we compute the quantiles of the sampling distribution of <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mi>B</mi></msub></math> , where <math><msub><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mi>B</mi></msub></math> is the bootstrapped
    statistic. For a 95th percentile interval, we identify the 2.5 and 97.5 quantiles,
    called <math><msub><mi>q</mi> <mrow><mn>2.5</mn> <mo>,</mo> <mi>B</mi></mrow></msub></math>
    and <math><msub><mi>q</mi> <mrow><mn>97.5</mn> <mo>,</mo> <mi>B</mi></mrow></msub></math>
    , respectively, where 95% of the time the bootstrapped statistic is in the interval:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mi>q</mi> <mrow><mn>2.5</mn> <mo>,</mo> <mi>B</mi></mrow></msub>
    <mo>≤</mo> <msub><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mi>B</mi></msub>  <mo>≤</mo>  <msub><mi>q</mi> <mrow><mn>97.5</mn> <mo>,</mo>
    <mi>B</mi></mrow></msub></math>
  prefs: []
  type: TYPE_NORMAL
- en: This bootstrap percentile  confidence interval is considered a quick-and-dirty
    interval. There are many alternatives that adjust for bias, take into consideration
    the shape of the distribution, and are better suited for small samples.
  prefs: []
  type: TYPE_NORMAL
- en: The percentile confidence interval does not rely on the sampling distribution
    having a particular shape or the center of the distribution being <math><msup><mi>θ</mi>
    <mo>∗</mo></msup></math> . In contrast, the normal confidence interval often doesn’t
    require bootstrapping to compute, but it does make additional assumptions about
    the shape of the sampling distribution of <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the normal confidence interval when the sampling distribution is well
    approximated by a normal curve. For a normal probability distribution, with center
    <math><mi>μ</mi></math> and spread <math><mi>σ</mi></math> , there is a 95% chance
    that a random value from this distribution is in the interval <math><mi>μ</mi>  <mo>±</mo>  <mn>1.96</mn>
    <mi>σ</mi></math> . Since the center of the sampling distribution is typically
    <math><msup><mi>θ</mi> <mo>∗</mo></msup></math> , the chance is 95% that for a
    randomly generated <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mo stretchy="false">|</mo> <mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mo>−</mo> <msup><mi>θ</mi> <mo>∗</mo></msup>
    <mrow><mo stretchy="false">|</mo></mrow> <mo>≤</mo> <mn>1.96</mn> <mi>S</mi> <mi>E</mi>
    <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math><mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo></math>
    is the spread of the sampling distribution of <math><mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow></math> . We use this inequality to make
    a 95% confidence interval for <math><msup><mi>θ</mi> <mo>∗</mo></msup></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mo stretchy="false">[</mo> <mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow>  <mo>−</mo>  <mn>1.96</mn> <mi>S</mi> <mi>E</mi>
    <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo> <mo>,</mo>    <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>  <mo>+</mo>  <mn>1.96</mn>
    <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: Confidence intervals of other sizes can be formed with different multiples of
    <math><mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo></math>
    , all based on the normal curve. For example, a 99% confidence interval is <math><mo>±</mo>
    <mn>2.58</mn> <mi>S</mi> <mi>E</mi></math> , and a one-sided upper 95% confidence
    interval is <math><mo stretchy="false">[</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>  <mo>−</mo>  <mn>1.64</mn>
    <mi>S</mi> <mi>E</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo
    stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo> <mo>,</mo>   <mi
    mathvariant="normal">∞</mi> <mo stretchy="false">]</mo></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The SD of a parameter estimate is often called the *standard error*, or SE,
    to distinguish it from the SD of a sample, population, or one draw from an urn.
    In this book, we don’t differentiate between them. We call them SDs.
  prefs: []
  type: TYPE_NORMAL
- en: We provide an example of each type of interval next.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter we tested the hypothesis that the coefficient for humidity
    in a linear model for air quality is 0\. The fitted coefficient for these data
    was <math><mn>0.21</mn></math> . Since the null model did not completely specify
    the data generation mechanism, we resorted to bootstrapping. That is, we used
    the data as the population, took a sample of 11,226 records with replacement from
    the bootstrap population, and fitted the model to find the bootstrap sample coefficient
    for humidity. Our simulation repeated this process 10,000 times to get an approximate
    bootstrap sampling distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the percentiles of this bootstrap sampling distribution to create
    a 99% confidence interval for <math><msup><mi>θ</mi> <mo>∗</mo></msup></math>
    . To do this, we find the quantiles, <math><msub><mi>q</mi> <mrow><mn>0.5</mn></mrow></msub></math>
    and <math><msub><mi>q</mi> <mrow><mn>99.5</mn></mrow></msub></math> , of the bootstrap
    sampling distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, since the histogram of the sampling distribution looks roughly
    normal in shape, we can create a 99% confidence interval based on the normal distribution.
    First, we find the standard error of <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    , which is just the standard deviation of the sampling distribution of <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a 99% confidence interval for <math><msup><mi>θ</mi> <mo>∗</mo></msup></math>
    is <math><mn>2.58</mn></math> SEs away from the observed <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> in either direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: These two intervals (bootstrap percentile and normal) are close but clearly
    not identical. We might expect this given the slight asymmetry in the bootstrapped
    sampling distribution.
  prefs: []
  type: TYPE_NORMAL
- en: There are other versions of the normal-based confidence interval that reflect
    the variability in estimating the standard error of the sampling distribution
    using the SD of the data. And there are still other confidence intervals for statistics
    that are percentiles, rather than averages. (Also note that for permutation tests,
    the bootstrap tends not to be as accurate as normal approximations.)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Confidence intervals can be easily misinterpreted as the chance that the parameter
    <math><msup><mi>θ</mi> <mo>∗</mo></msup></math> is in the interval. However, the
    confidence interval is created from one realization of the sampling distribution.
    The sampling distribution gives us a different probability statement; 95% of the
    time, an interval constructed in this way will contain <math><msup><mi>θ</mi>
    <mo>∗</mo></msup></math> . Unfortunately, we don’t know whether this particular
    time is one of those that happens 95 times in 100 or not. That is why the term
    *confidence* is used rather than *probability* or *chance*, and we say that we
    are 95% confident that the parameter is in our interval.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence intervals and hypothesis tests are related in the following way.
    If, say, a 95% confidence interval contains the hypothesized value <math><msup><mi>θ</mi>
    <mo>∗</mo></msup></math> , then the <math><mi>p</mi></math> -value for the test
    is less than 5%. That is, we can invert a confidence interval to create a hypothesis
    test. We used this technique in the previous section when we carried out the test
    that the coefficient for humidity in the air quality model is 0\. In this section,
    we have created a 99% confidence interval for the coefficient (based on the bootstrap
    percentiles), and since 0 does not belong to the interval, the <math><mi>p</mi></math>
    -value is less than 1% and statistical logic would lead us to conclude that the
    coefficient is not 0.
  prefs: []
  type: TYPE_NORMAL
- en: Another kind of interval estimate is the prediction interval. Prediction intervals
    focus on the variation in observations rather than the variation in an estimator.
    We explore these next.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of Prediction Intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Confidence intervals convey the accuracy of an estimator, but sometimes we
    want the accuracy of a prediction for a future observation. For example, someone
    might say: half the time my bus arrives three-quarters of a minute late at most,
    but how late might it get? As another example, the California Department of Fish
    and Wildlife sets the minimum catch size for Dungeness crabs at 146 mm, and a
    recreational fishing company might wonder how much bigger than 146 mm their customer’s
    catch might be when they bring them fishing. And for another example, a vet estimates
    the weight of a donkey to be 169 kg based on its length and girth and uses this
    estimate to administer medication. For the donkey’s safety, the vet is keen to
    know how different the donkey’s real weight might be from this estimate.'
  prefs: []
  type: TYPE_NORMAL
- en: What these examples have in common is an interest in the prediction of a future
    observation and the desire to quantify how far that future observation might be
    from this prediction. Just like with confidence intervals, we compute the statistic
    (the estimator) and use it in making the prediction, but now we’re interested
    in typical deviations of future observations from the prediction. In the following
    sections, we work through examples of prediction intervals based on quantiles,
    standard deviations, and those conditional on covariates. Along the way, we provide
    additional information about the typical variation of observations about a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Predicting Bus Lateness'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 4](ch04.html#ch-modeling) models the lateness of a Seattle bus in
    arriving at a particular stop. We observed that the distribution was highly skewed
    and chose to estimate the typical lateness by the median, which was 0.74 minutes.
    We reproduce the sample histogram from that chapter here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_17in05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The prediction problem addresses how late a bus might be. While the median
    is informative, it doesn’t provide information about the skewness of the distribution.
    That is, we don’t know how late the bus might be. The 75th percentile, or even
    the 95th percentile, would add useful information to consider. We compute those
    percentiles here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: From these statistics, we learn that while more than half the time the bus is
    not even a minute late, one-quarter of the time it’s almost four minutes late,
    and with some regularity it can happen that the bus is nearly 15 minutes late.
    These three values together help us make plans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Predicting Crab Size'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fishing for Dungeness crabs is highly regulated, including limiting the shell
    size to 146 mm in width for crabs caught for recreation. To better understand
    the distribution of shell size of Dungeness crabs, the California Department of
    Fish and Wildlife worked with commercial crab fishers from Northern California
    and Southern Oregon to capture, measure, and release crabs. Here is a histogram
    of crab shell sizes for the approximately 450 crabs caught:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_17in06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The distribution is somewhat skewed left, but the average and standard deviations
    are reasonable summary statistics of the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The average, 132 mm, is a good prediction for the typical size of a crab. However,
    it lacks information about how far an individual crab may vary from the average.
    The standard deviation can fill in this gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the variability of individual observations about the center
    of the distribution, we also take into account the variability in our estimate
    of the mean shell size. We can use the bootstrap to estimate this variability,
    or we can use probability theory (we do this in the next section) to show that
    the standard deviation of the estimator is <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo> <mrow><mo>/</mo></mrow>
    <msqrt><mi>n</mi></msqrt></math> . We also show, in the next section, that these
    two sources of variation combine as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msqrt><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mi>o</mi> <mi>p</mi> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>
    <mo>+</mo> <mfrac><mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi>
    <mi>o</mi> <mi>p</mi> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mrow>
    <mi>n</mi></mfrac></msqrt>  <mo>=</mo>  <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo> <msqrt><mn>1</mn>
    <mo>+</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac></msqrt></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We substitute <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>s</mi>
    <mi>a</mi> <mi>m</mi> <mi>p</mi> <mi>l</mi> <mi>e</mi> <mo stretchy="false">)</mo></math>
    for <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mi>o</mi>
    <mi>p</mi> <mo stretchy="false">)</mo></math> and apply this formula to our crabs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We see that including the SE of the sample average essentially doesn’t change
    the prediction error because the sample is so large. We conclude that crabs routinely
    differ from the typical size of 132 mm by 11 to 22 mm. This information is helpful
    in developing policies around crab fishing to maintain the health of the crab
    population and to set expectations for the recreational fisher.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Predicting the Incremental Growth of a Crab'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After Dungeness crabs mature, they continue to grow by casting off their shell
    and building a new, larger one to grow into each year; this process is called
    *molting*. The California Department of Fish and Wildlife wanted a better understanding
    of crab growth so that it could set better limits on fishing that would protect
    the crab population. The crabs caught in the study mentioned in the previous example
    were about to molt, and in addition to their size, the change in shell size from
    before to after molting was also recorded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '|   | shell | inc |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **shell** | 1.0 | -0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| **inc** | -0.6 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'These two measurements are negatively correlated, meaning that the larger the
    crab, the less they grow when they molt. We plot the growth increment against
    the shell size to determine whether the relationship between these variables is
    roughly linear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_17in07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The relationship appears linear, and we can fit a simple linear model to explain
    the growth increment by the pre-molt size of the shell. For this example, we use
    the `statsmodels` library, which provides prediction intervals with `get_prediction`.
    We first set up the design matrix and response variable, and then we use least
    squares to fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: When modeling, we create prediction intervals for given values of the explanatory
    variable. For example, if a newly caught crab is 120 mm across, then we use our
    fitted model to predict its shell’s growth.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous example, the variability of our prediction for an individual
    observation includes the variability in our estimate of the crab’s growth and
    the crab-to-crab variation in shell size. Again, we can use the bootstrap to estimate
    this variation, or we can use probability theory to show that these two sources
    of variation combine as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mi
    mathvariant="bold">e</mi></mrow> <mo stretchy="false">)</mo> <msqrt><mn>1</mn>
    <mo>+</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn></msub>
    <mo stretchy="false">(</mo> <msup><mtext mathvariant="bold">X</mtext> <mi mathvariant="normal">⊤</mi></msup>
    <mtext mathvariant="bold">X</mtext> <msup><mo stretchy="false">)</mo> <mrow><mo>−</mo>
    <mn>1</mn></mrow></msup> <msubsup><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn>
    <mi mathvariant="normal">⊤</mi></msubsup></msqrt></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here <math><mtext mathvariant="bold">X</mtext></math> is the design matrix
    that consists of the original data, <math><mrow><mi mathvariant="bold">e</mi></mrow></math>
    is the <math><mi>n</mi> <mo>×</mo> <mn>1</mn></math> column vector of residuals
    from the regression, and <math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mn>0</mn></msub></math> is the <math><mn>1</mn> <mo>×</mo> <mo stretchy="false">(</mo>
    <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></math> row vector
    of features for the new observation (in this example, these are <math><mrow><mo>[</mo>
    <mn>1</mn> <mo>,</mo> <mn>120</mn> <mo>]</mo></mrow></math> ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '|   | const | shell |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1 | 120 |'
  prefs: []
  type: TYPE_TB
- en: 'We use the `get_prediction` method in `statsmodels` to find a 95% prediction
    interval for a crab with a 120 mm shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '|   | mean | mean_se | mean_ci_lower | mean_ci_upper | obs_ci_lower | obs_ci_upper
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 15.86 | 0.12 | 15.63 | 16.08 | 12.48 | 19.24 |'
  prefs: []
  type: TYPE_TB
- en: 'Here we have both a confidence interval for the average growth increment for
    a crab with a 120 mm shell, [15.6, 16.1] and a prediction interval for the growth
    increment, [12.5, 19.2]. The prediction interval is quite a bit wider because
    it takes into account the variation in individual crabs. This variation is seen
    in the spread of the points about the regression line, which we approximate by
    the SD of the residuals. The correlation between shell size and growth increment
    means that the variation in a growth increment prediction for a particular shell
    size is smaller than the overall SD of the growth increment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The intervals provided by `get_prediction` rely on the normal approximation
    to the distribution of growth increment. That’s why the 95% prediction interval
    endpoints are roughly twice the residual SD away from the prediction. In the next
    section, we dive deeper into these calculations of standard deviations, estimators,
    and predictions. We also discuss some of the assumptions that we make in calculating
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Probability for Inference and Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hypothesis testing, confidence intervals, and prediction intervals rely on probability
    calculations computed from the sampling distribution and the data generation process.
    These probability frameworks also enable us to run simulation and bootstrap studies
    for a hypothetical survey, an experiment, or some other chance process in order
    to study its random behavior. For example, we found the sampling distribution
    for an average of ranks under the assumption that the treatment in a Wikipedia
    experiment was not effective. Using simulation, we quantified the typical deviations
    from the expected outcome and the distribution of the possible values for the
    summary statistic. The triptych in [Figure 17-1](#triptych) provided a diagram
    to guide us in the process; it helped keep straight the differences between the
    population, probability, and sample and also showed their connections. In this
    section, we bring more mathematical rigor to these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: We formally introduce the notions of expected value, standard deviation, and
    random variable, and we connect them to the concepts we have been using in this
    chapter for testing hypotheses and making confidence and prediction intervals.
    We begin with the specific example from the Wikipedia experiment, and then we
    generalize. Along the way, we connect this formalism to the triptych that we have
    used as our guide throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Formalizing the Theory for Average Rank Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall in the Wikipedia experiment that we pooled the post-award productivity
    values from the treatment and control groups and converted them into ranks, <math><mn>1</mn>
    <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>3</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>200</mn></math>
    , so the population is simply made up of the integers from 1 to 200\. [Figure 17-3](#triptychrank)
    is a diagram that represents this specific situation. Notice that the population
    distribution is flat and ranges from 1 to 200 (left side of [Figure 17-3](#triptychrank)).
    Also, the population summary (called *population parameter*) we use is the average
    rank:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msup><mi>θ</mi> <mo>∗</mo></msup>  <mo>=</mo>  <mtext>Avg</mtext>
    <mo stretchy="false">(</mo> <mtext>pop</mtext> <mo stretchy="false">)</mo>  <mo>=</mo>  <mfrac><mn>1</mn>
    <mn>200</mn></mfrac> <msubsup><mi mathvariant="normal">Σ</mi> <mrow><mi>k</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mrow><mn>200</mn></mrow></msubsup> <mi>k</mi>  <mo>=</mo>  <mn>100.5</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Another relevant summary is the spread about <math><msup><mi>θ</mi> <mo>∗</mo></msup></math>
    , defined as the population standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>SD</mtext> <mo stretchy="false">(</mo> <mtext>pop</mtext>
    <mo stretchy="false">)</mo>  <mo>=</mo>  <msqrt><mfrac><mn>1</mn> <mn>200</mn></mfrac>
    <msubsup><mi mathvariant="normal">Σ</mi> <mrow><mi>k</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mn>200</mn></mrow></msubsup> <mo stretchy="false">(</mo> <mi>k</mi> <mo>−</mo>
    <msup><mi>θ</mi> <mo>∗</mo></msup> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></msqrt>  <mo>=</mo>  <msqrt><mfrac><mn>1</mn>
    <mn>200</mn></mfrac> <msubsup><mi mathvariant="normal">Σ</mi> <mrow><mi>k</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mrow><mn>200</mn></mrow></msubsup> <mo stretchy="false">(</mo>
    <mi>k</mi> <mo>−</mo> <mn>100.5</mn> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></msqrt>  <mo>≈</mo>  <mn>57.7</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The SD(pop) represents the typical deviation of a rank from the population
    average. To calculate SD(pop) for this example takes some mathematical handiwork:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-3\. Diagram of the data generation process for the Wikipedia experiment;
    this is a special case where we know the population
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The observed sample consists of the integer ranks of the treatment group; we
    refer to these values as <math><msub><mi>k</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>k</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>k</mi> <mrow><mn>100</mn></mrow></msub><mo>.</mo></math>
    The sample distribution appears on the right in [Figure 17-3](#triptychrank) (each
    of the 100 integers appears once).
  prefs: []
  type: TYPE_NORMAL
- en: 'The parallel to the population average is the sample average, which is our
    statistic of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>Avg</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext>
    <mo stretchy="false">)</mo>  <mo>=</mo>  <mfrac><mn>1</mn> <mn>100</mn></mfrac>
    <msubsup><mi mathvariant="normal">Σ</mi> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mn>100</mn></mrow></msubsup> <msub><mi>k</mi> <mi>i</mi></msub>  <mo>=</mo>  <mrow><mover><mi>k</mi>
    <mo stretchy="false">¯</mo></mover></mrow>  <mo>=</mo>  <mn>113.7</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The <math><mtext>Avg</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext>
    <mo stretchy="false">)</mo></math> is the observed value for <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> . Similarly, the spread about
    <math><mtext>Avg</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext> <mo
    stretchy="false">)</mo></math> , called the standard deviation of the sample,
    represents the typical deviation of a rank in the sample from the sample average:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>SD</mtext> <mo stretchy="false">(</mo> <mtext>sample</mtext>
    <mo stretchy="false">)</mo>  <mo>=</mo>  <msqrt><mfrac><mn>1</mn> <mn>100</mn></mfrac>
    <msubsup><mi mathvariant="normal">Σ</mi> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mn>100</mn></mrow></msubsup> <mo stretchy="false">(</mo> <msub><mi>k</mi>
    <mi>i</mi></msub> <mo>−</mo> <mrow><mover><mi>k</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></msqrt>  <mo>=</mo>  <mn>553.</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice the parallel between the definitions of the sample statistic and the
    population parameter in the case where they are averages. The parallel between
    the two SDs is also noteworthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we turn to the data generation process: draw 100 marbles from the urn
    (with values <math><mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo>
    <mn>200</mn></math> ), without replacement, to create the treatment ranks. We
    represent the action of drawing the first marble from the urn, and the integer
    that we get, by the capital letter <math><msub><mi>Z</mi> <mn>1</mn></msub></math>
    . This <math><msub><mi>Z</mi> <mn>1</mn></msub></math> is called a *random variable*.
    It has a probability distribution determined by the urn model. That is, we can
    list all of the values that <math><msub><mi>Z</mi> <mn>1</mn></msub></math> might
    take and the probability associated with each:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mrow><mi mathvariant="double-struck">P</mi></mrow></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo>=</mo> <mi>k</mi>
    <mo stretchy="false">)</mo>  <mo>=</mo>  <mfrac><mn>1</mn> <mn>200</mn></mfrac>     <mtext> for </mtext>
    <mi>k</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>200</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the probability distribution of <math><msub><mi>Z</mi> <mn>1</mn></msub></math>
    is determined by a simple formula because all of the integers are equally likely
    to be drawn from the urn.
  prefs: []
  type: TYPE_NORMAL
- en: We often summarize the distribution of a random variable by its *expected value*
    and *standard deviation*. Like with the population and sample, these two quantities
    give us a sense of what to expect as an outcome and how far the actual value might
    be from what is expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, the expected value of <math><msub><mi>Z</mi> <mn>1</mn></msub></math>
    is simply:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">]</mo></mtd>
    <mtd><mo>=</mo> <mn>1</mn> <mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo>=</mo> <mn>1</mn>
    <mo stretchy="false">)</mo> <mo>+</mo> <mn>2</mn> <mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo>=</mo> <mn>2</mn>
    <mo stretchy="false">)</mo> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mn>200</mn> <mrow><mi
    mathvariant="double-struck">P</mi></mrow> <mo stretchy="false">(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>200</mn> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><mo>=</mo> <mn>1</mn> <mo>×</mo> <mfrac><mn>1</mn> <mn>200</mn></mfrac>
    <mo>+</mo> <mn>2</mn> <mo>×</mo> <mfrac><mn>1</mn> <mn>200</mn></mfrac> <mo>+</mo>
    <mo>⋯</mo> <mo>+</mo> <mn>200</mn> <mo>×</mo> <mfrac><mn>1</mn> <mn>200</mn></mfrac></mtd></mtr>
    <mtr><mtd><mo>=</mo> <mn>100.5</mn></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice that <math><mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">]</mo> <mo>=</mo> <msup><mi>θ</mi>
    <mo>∗</mo></msup></math> , the population average from the urn. The average value
    in a population and the expected value of a random variable that represents one
    draw at random from an urn that contains the population are always the same. This
    is more easily seen by expressing the population average as an average of the
    unique values in the population, weighted by the fraction of units that have that
    value. The expected value of a random variable of a draw at random from the population
    urn uses the exact same weights because they match the chance of selecting the
    particular value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term *expected value* can be a bit confusing because it need not be a possible
    value of the random variable. For example, <math><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">]</mo>
    <mo>=</mo> <mn>100.5</mn></math> , but only integers are possible values for <math><msub><mi>Z</mi>
    <mn>1</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the variance of <math><msub><mi>Z</mi> <mn>1</mn></msub></math> is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo></mtd>
    <mtd><mo>=</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <msub><mi>Z</mi> <mn>1</mn></msub> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mo>=</mo>
    <mo stretchy="false">[</mo> <mn>1</mn> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup> <mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo>=</mo> <mn>1</mn>
    <mo stretchy="false">)</mo> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo stretchy="false">[</mo>
    <mn>200</mn> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo
    stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup> <mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo>=</mo> <mn>200</mn>
    <mo stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mo>=</mo> <mo stretchy="false">(</mo>
    <mn>1</mn> <mo>−</mo> <mn>100.5</mn> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>
    <mo>×</mo> <mfrac><mn>1</mn> <mn>200</mn></mfrac> <mo>+</mo> <mo>⋯</mo> <mo>+</mo>
    <mo stretchy="false">(</mo> <mn>200</mn> <mo>−</mo> <mn>100.5</mn> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup> <mo>×</mo> <mfrac><mn>1</mn> <mn>200</mn></mfrac></mtd></mtr>
    <mtr><mtd><mo>=</mo> <mn>3333.25</mn></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we define the standard deviation of <math><msub><mi>Z</mi> <mn>1</mn></msub></math>
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>SD</mtext> <mo stretchy="false">(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo stretchy="false">)</mo> <mo>=</mo> <msqrt><mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo></msqrt>
    <mo>=</mo> <mn>57.7</mn></math>
  prefs: []
  type: TYPE_NORMAL
- en: We again point out that the standard deviation of <math><msub><mi>Z</mi> <mn>1</mn></msub></math>
    matches the <math><mtext>SD</mtext></math> (pop).
  prefs: []
  type: TYPE_NORMAL
- en: 'To describe the entire data generation process in [Figure 17-3](#triptychrank),
    we also define <math><msub><mi>Z</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>Z</mi>
    <mn>3</mn></msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>Z</mi> <mrow><mn>100</mn></mrow></msub></math>
    as the result of the remaining 99 draws from the urn. By symmetry, these random
    variables should all have the same probability distribution. That is, for any
    <math><mi>k</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>200</mn></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">P</mi></mrow> <mo
    stretchy="false">(</mo> <msub><mi>Z</mi> <mn>1</mn></msub> <mo>=</mo> <mi>k</mi>
    <mo stretchy="false">)</mo>  <mo>=</mo>  <mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mi>k</mi>
    <mo stretchy="false">)</mo>  <mo>=</mo>  <mo>⋯</mo>  <mo>=</mo>  <mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mo stretchy="false">(</mo> <msub><mi>Z</mi> <mrow><mn>100</mn></mrow></msub>
    <mo>=</mo> <mi>k</mi> <mo stretchy="false">)</mo>  <mo>=</mo>  <mfrac><mn>1</mn>
    <mn>200</mn></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: This implies that each <math><msub><mi>Z</mi> <mi>i</mi></msub></math> has the
    same expected value, 100.5, and standard deviation, 57.7\. However, these random
    variables are not independent. For example, if you know that <math><msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>17</mn></math> , then it is not possible for
    <math><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mn>17</mn></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'To complete the middle portion of [Figure 17-3](#triptychrank), which involves
    the sampling distribution of <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    , we express the average rank statistic as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>100</mn></mfrac> <msubsup><mi mathvariant="normal">Σ</mi>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mn>100</mn></mrow></msubsup>
    <msub><mi>Z</mi> <mi>i</mi></msub></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the expected value and SD of <math><msub><mi>Z</mi> <mn>1</mn></msub></math>
    and our knowledge of the data generation process to find the expected value and
    SD of <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    . We first find the expected value of <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mrow><mo>[</mo> <mfrac><mn>1</mn> <mn>100</mn></mfrac> <msubsup><mi mathvariant="normal">Σ</mi>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mn>100</mn></mrow></msubsup>
    <msub><mi>Z</mi> <mi>i</mi></msub> <mo>]</mo></mrow></mtd></mtr> <mtr><mtd><mo>=</mo>  <mfrac><mn>1</mn>
    <mn>100</mn></mfrac> <msubsup><mi mathvariant="normal">Σ</mi> <mrow><mi>i</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mrow><mn>100</mn></mrow></msubsup> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <msub><mi>Z</mi> <mi>i</mi></msub> <mo stretchy="false">]</mo></mtd></mtr>
    <mtr><mtd><mo>=</mo>  <mn>100.5</mn></mtd></mtr> <mtr><mtd><mo>=</mo>  <msup><mi>θ</mi>
    <mo>∗</mo></msup></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the expected value of the average of random draws from the
    population equals the population average. Here we provide formulas for the variance
    of the average in terms of the population variance, as well as the SD:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mrow><mo>[</mo> <mfrac><mn>1</mn> <mn>100</mn></mfrac> <msubsup><mi mathvariant="normal">Σ</mi>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mn>100</mn></mrow></msubsup>
    <msub><mi>Z</mi> <mi>i</mi></msub> <mo>]</mo></mrow></mtd></mtr> <mtr><mtd><mo>=</mo>  <mfrac><mrow><mn>200</mn>
    <mo>−</mo> <mn>100</mn></mrow> <mrow><mn>100</mn> <mo>−</mo> <mn>1</mn></mrow></mfrac>
    <mo>×</mo> <mfrac><mrow><mrow><mi mathvariant="double-struck">V</mi></mrow> <mo
    stretchy="false">(</mo> <msub><mi>Z</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mrow>
    <mn>100</mn></mfrac></mtd></mtr> <mtr><mtd><mo>=</mo>  <mn>16.75</mn></mtd></mtr>
    <mtr><mtd><mtext>SD</mtext> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <msqrt><mfrac><mn>100</mn>
    <mn>199</mn></mfrac></msqrt> <mfrac><mrow><mtext>SD</mtext> <mo stretchy="false">(</mo>
    <msub><mi>Z</mi> <mn>1</mn></msub> <mo stretchy="false">)</mo></mrow> <mn>10</mn></mfrac></mtd></mtr>
    <mtr><mtd><mo>=</mo>  <mn>4.1</mn></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: These computations relied on several properties of expected value and variance
    of a random variable and sums of random variables. Next, we provide properties
    of sums and averages of random variables that can be used to derive the formulas
    we just presented.
  prefs: []
  type: TYPE_NORMAL
- en: General Properties of Random Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, a *random variable* represents a numeric outcome of a chance event.
    In this book, we use capital letters like <math><mi>X</mi></math> or <math><mi>Y</mi></math>
    or <math><mi>Z</mi></math> to denote a random variable. The probability distribution
    for <math><mi>X</mi></math> is the specification <math><mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo stretchy="false">)</mo>
    <mo>=</mo> <msub><mi>p</mi> <mi>x</mi></msub></math> for all values <math><mi>x</mi></math>
    that the random variable takes on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the expected value of <math><mi>X</mi></math> is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">E</mi></mrow> <mo
    stretchy="false">[</mo> <mi>X</mi> <mo stretchy="false">]</mo> <mo>=</mo> <munder><mo>∑</mo>
    <mrow><mi>x</mi></mrow></munder> <mi>x</mi> <msub><mi>p</mi> <mi>x</mi></msub></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance <math><mi>X</mi></math> is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mrow><mi
    mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo>
    <mi>X</mi> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo
    stretchy="false">[</mo> <mi>X</mi> <mo stretchy="false">]</mo> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup> <mo stretchy="false">]</mo></mtd></mtr> <mtr><mtd><mo>=</mo>  <munder><mo>∑</mo>
    <mrow><mi>x</mi></mrow></munder> <mo stretchy="false">[</mo> <mi>x</mi> <mo>−</mo>
    <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">(</mo>
    <mi>X</mi> <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo> <mn>2</mn></msup>
    <msub><mi>p</mi> <mi>x</mi></msub></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: And the <math><mtext>SD</mtext> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo></math>
    is the square root of <math><mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although random variables can represent quantities that are either discrete
    (such as the number of children in a family drawn at random from a population)
    or continuous (such as the air quality measured by an air monitor), we address
    only random variables with discrete outcomes in this book. Since most measurements
    are made to a certain degree of precision, this simplification doesn’t limit us
    too much.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple formulas provide the expected value, variance, and standard deviation
    when we make scale and shift changes to random variables, such as <math><mi>a</mi>
    <mo>+</mo> <mi>b</mi> <mi>X</mi></math> for constants <math><mi>a</mi></math>
    and <math><mi>b</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo stretchy="false">)</mo></mtd>
    <mtd><mo>=</mo>  <mi>a</mi> <mo>+</mo> <mi>b</mi> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><mrow><mi mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo>
    <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo stretchy="false">)</mo></mtd>
    <mtd><mo>=</mo>  <msup><mi>b</mi> <mn>2</mn></msup> <mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo></mtd></mtr>
    <mtr><mtd><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>a</mi> <mo>+</mo>
    <mi>b</mi> <mi>X</mi> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mrow><mo
    stretchy="false">|</mo></mrow> <mi>b</mi> <mrow><mo stretchy="false">|</mo></mrow>
    <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: To convince yourself that these formulas make sense, think about how a distribution
    changes if you add a constant <math><mi>a</mi></math> to each value or scale each
    value by <math><mi>b</mi></math> . Adding <math><mi>a</mi></math> to each value
    would simply shift the distribution, which in turn would shift the expected value
    but not change the size of the deviations about the expected value. On the other
    hand, scaling the values by, say, 2 would spread the distribution out and essentially
    double both the expected value and the deviations from the expected value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are also interested in the properties of the sum of two or more random variables.
    Let’s consider two random variables, <math><mi>X</mi></math> and <math><mi>Y</mi></math>
    . Then:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">E</mi></mrow> <mo
    stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo>
    <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo>  <mo>=</mo>  <mi>a</mi> <mo>+</mo>
    <mi>b</mi> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">(</mo>
    <mi>X</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mi>c</mi> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'But to find the variance of <math><mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi>
    <mo>+</mo> <mi>c</mi> <mi>Y</mi></math> , we need to know how <math><mi>X</mi></math>
    and <math><mi>Y</mi></math> vary together, which is called the *joint distribution*
    of <math><mi>X</mi></math> and <math><mi>Y</mi></math> . The joint distribution
    of <math><mi>X</mi></math> and <math><mi>Y</mi></math> assigns probabilities to
    combinations of their outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">P</mi></mrow> <mo
    stretchy="false">(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo>,</mo> <mi>Y</mi>
    <mo>=</mo> <mi>y</mi> <mo stretchy="false">)</mo>  <mo>=</mo>  <msub><mi>p</mi>
    <mrow><mi>x</mi> <mo>,</mo> <mi>y</mi></mrow></msub></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'A summary of how <math><mi>X</mi></math> and <math><mi>Y</mi></math> vary together,
    called the *covariance*, is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>C</mi> <mi>o</mi> <mi>v</mi>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo>,</mo> <mi>Y</mi> <mo stretchy="false">)</mo></mtd>
    <mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mi>X</mi> <mo stretchy="false">]</mo> <mo stretchy="false">)</mo>
    <mo stretchy="false">(</mo> <mi>Y</mi> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mi>Y</mi> <mo stretchy="false">]</mo> <mo stretchy="false">)</mo>
    <mo stretchy="false">]</mo></mtd></mtr> <mtr><mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <mi>X</mi> <mi>Y</mi>
    <mo stretchy="false">)</mo> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo></mtd></mtr>
    <mtr><mtd><mo>=</mo>  <msub><mi mathvariant="normal">Σ</mi> <mrow><mi>x</mi> <mo>,</mo>
    <mi>y</mi></mrow></msub> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo>
    <mi>x</mi> <mi>y</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mi>X</mi> <mo stretchy="false">)</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo>
    <msub><mi>p</mi> <mrow><mi>x</mi> <mo>,</mo> <mi>y</mi></mrow></msub></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance enters into the calculation of <math><mrow><mo mathvariant="bold"
    stretchy="false">(</mo></mrow> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo>
    <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo></math> , as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">V</mi></mrow> <mo
    stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo>
    <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo>  <mo>=</mo>  <msup><mi>b</mi>
    <mn>2</mn></msup> <mrow><mi mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo>
    <mi>X</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mn>2</mn> <mi>b</mi> <mi>c</mi>
    <mi>C</mi> <mi>o</mi> <mi>v</mi> <mo stretchy="false">(</mo> <mi>X</mi> <mo>,</mo>
    <mi>Y</mi> <mo stretchy="false">)</mo> <mo>+</mo> <msup><mi>c</mi> <mn>2</mn></msup>
    <mrow><mi mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo>
    <mi>Y</mi> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In the special case where <math><mi>X</mi></math> and <math><mi>Y</mi></math>
    are independent, their joint distribution is simplified to <math><msub><mi>p</mi>
    <mrow><mi>x</mi> <mo>,</mo> <mi>y</mi></mrow></msub> <mo>=</mo> <msub><mi>p</mi>
    <mi>x</mi></msub> <msub><mi>p</mi> <mi>y</mi></msub></math> . And in this case,
    <math><mi>C</mi> <mi>o</mi> <mi>v</mi> <mo stretchy="false">(</mo> <mi>X</mi>
    <mo>,</mo> <mi>Y</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mn>0</mn></math>
    , so:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">V</mi></mrow> <mo
    stretchy="false">(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>X</mi> <mo>+</mo>
    <mi>c</mi> <mi>Y</mi> <mo stretchy="false">)</mo>  <mo>=</mo>  <msup><mi>b</mi>
    <mn>2</mn></msup> <mrow><mi mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo>
    <mi>X</mi> <mo stretchy="false">)</mo> <mo>+</mo> <msup><mi>c</mi> <mn>2</mn></msup>
    <mrow><mi mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo>
    <mi>Y</mi> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'These properties can be used to show that for random variables <math><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>…</mo>
    <mo>,</mo> <msub><mi>X</mi> <mn>n</mn></msub></math> that are independent with
    expected value <math><mi>μ</mi></math> and standard deviation <math><mi>σ</mi></math>
    , the average, <math><mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow></math>
    , has the following expected value, variance, and standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mi>μ</mi></mtd></mtr> <mtr><mtd><mrow><mi
    mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo> <mrow><mover><mi>X</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <msup><mi>σ</mi>
    <mn>2</mn></msup> <mrow><mo>/</mo></mrow> <mi>n</mi></mtd></mtr> <mtr><mtd><mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mi>σ</mi> <mrow><mo>/</mo></mrow>
    <msqrt><mi>n</mi></msqrt></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: This situation arises with the urn model where <math><msub><mi>X</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub><mi>X</mi> <mi>n</mi></msub></math> are
    the result of random draws with replacement. In this case, <math><mi>μ</mi></math>
    represents the average of the urn and <math><mi>σ</mi></math> the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when we make random draws from the urn without replacement, the <math><msub><mi>X</mi>
    <mi>i</mi></msub></math> are not independent. In this situation, <math><mrow><mover><mi>X</mi>
    <mo stretchy="false">¯</mo></mover></mrow></math> has the following expected value
    and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">(</mo> <mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mi>μ</mi></mtd></mtr> <mtr><mtd><mrow><mi
    mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo> <mrow><mover><mi>X</mi>
    <mo stretchy="false">¯</mo></mover></mrow> <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo>  <mfrac><mrow><mi>N</mi>
    <mo>−</mo> <mi>n</mi></mrow> <mrow><mi>N</mi> <mo>−</mo> <mn>1</mn></mrow></mfrac>
    <mo>×</mo> <mfrac><msup><mi>σ</mi> <mn>2</mn></msup> <mi>n</mi></mfrac></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice that while the expected value is the same as when the draws are without
    replacement, the variance and SD are smaller. These quantities are adjusted by
    <math><mo stretchy="false">(</mo> <mi>N</mi> <mo>−</mo> <mi>n</mi> <mo stretchy="false">)</mo>
    <mrow><mo>/</mo></mrow> <mo stretchy="false">(</mo> <mi>N</mi> <mo>−</mo> <mn>1</mn>
    <mo stretchy="false">)</mo></math> , which is called the *finite population correction
    factor*. We used this formula earlier to compute the <math><mi>S</mi> <mi>D</mi>
    <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></math> in our Wikipedia example.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to [Figure 17-3](#triptychrank), we see that the sampling distribution
    for <math><mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow></math>
    in the center of the diagram has an expectation that matches the population average;
    the SD decreases like <math><mn>1</mn> <mrow><mo>/</mo></mrow> <msqrt><mi>n</mi></msqrt></math>
    but even more quickly because we are drawing without replacement; and the distribution
    is shaped like a normal curve. We saw these properties earlier in our simulation
    study.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have outlined the general properties of random variables and their
    sums, we connect these ideas to testing, confidence, and prediction intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Behind Testing and Intervals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this chapter, probability is the underpinning
    of conducting a hypothesis test, providing a confidence interval for an estimator
    and a prediction interval for a future observation.
  prefs: []
  type: TYPE_NORMAL
- en: We now have the technical machinery to explain these concepts, which we have
    carefully defined in this chapter without the use of formal technicalities. This
    time we present the results in terms of random variables and their distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that a hypothesis test relies on a null model that provides the probability
    distribution for the statistic, <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    . The tests we carried out were essentially computing (sometimes approximately)
    the following probability. Given the assumptions of the null distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">P</mi></mrow> <mo
    stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo>≥</mo> <mtext>observed statistic</mtext> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Oftentimes, the random variable is normalized to make these computations easier
    and standard:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">P</mi></mrow> <mrow><mo>(</mo>
    <mfrac><mrow><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo>−</mo> <msup><mrow><mi>θ</mi></mrow> <mo>∗</mo></msup></mrow> <mrow><mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></mrow></mfrac> <mo>≥</mo> <mfrac><mrow><mtext>observed
    stat</mtext> <mo>−</mo> <msup><mi>θ</mi> <mo>∗</mo></msup></mrow> <mrow><mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></mrow></mfrac> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: When <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo></math>
    is not known, we have approximated it via simulation, and when we have a formula
    for <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo></math>
    in terms of <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mi>p</mi>
    <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo></math> , we substitute <math><mi>S</mi>
    <mi>D</mi> <mo stretchy="false">(</mo> <mi>s</mi> <mi>a</mi> <mi>m</mi> <mi>p</mi>
    <mo stretchy="false">)</mo></math> for <math><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mi>p</mi> <mi>o</mi> <mi>p</mi> <mo stretchy="false">)</mo></math> . This normalization
    is popular because it simplifies the null distribution. For example, if <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> has an approximate normal distribution,
    then the normalized version will have a standard normal distribution with center
    0 and SD 1\. These approximations are useful when a lot of hypothesis tests are
    being carried out, such as with A/B testing, since there is no need to simulate
    for every statistic because we can just use the normal curve probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability statement behind a confidence interval is quite similar to
    the probability calculations used in testing. In particular, to create a 95% confidence
    interval where the sampling distribution of the estimator is roughly normal, we
    standardize and use the probability:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mrow><mo>(</mo> <mfrac><mrow><mo stretchy="false">|</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo>−</mo> <msup><mi>θ</mi> <mo>∗</mo></msup>
    <mo stretchy="false">|</mo></mrow> <mrow><mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo></mrow></mfrac>
    <mo>≤</mo> <mn>1.96</mn> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">P</mi></mrow>
    <mrow><mo>(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo>−</mo> <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo> <mo>≤</mo>
    <msup><mi>θ</mi> <mo>∗</mo></msup> <mo>≤</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo>+</mo> <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo> <mo>)</mo></mrow></mtd></mtr>
    <mtr> <mtd><mo>≈</mo>  <mn>0.95</mn></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    is a random variable in the preceding probability statement and <math><msup><mi>θ</mi>
    <mo>∗</mo></msup></math> is considered a fixed unknown parameter value. The confidence
    interval is created by substituting the observed statistic for <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> and calling it a 95% confidence
    interval:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mo>[</mo> <mtext>observed stat</mtext> <mo>−</mo>
    <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo> <mo>,</mo>  <mtext>observed
    stat</mtext> <mo>+</mo> <mn>1.96</mn> <mi>S</mi> <mi>D</mi> <mo stretchy="false">(</mo>
    <mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">)</mo>
    <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Once the observed statistic is substituted for the random variable, then we
    say that we are 95% confident that the interval we have created contains the true
    value <math><msup><mi>θ</mi> <mo>∗</mo></msup></math> . In other words, in 100
    cases where we compute an interval in this way, we expect 95 of them to cover
    the population parameter that we are estimating.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we consider prediction intervals. The basic notion is to provide an interval
    that denotes the expected variation of a future observation about the estimator.
    In the simple case where the statistic is <math><mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow></math>
    and we have a hypothetical new observation <math><msub><mi>X</mi> <mn>0</mn></msub></math>
    that has the same expected value, say <math><mi>μ</mi></math> , and standard deviation,
    say <math><mi>σ</mi></math> , of the <math><msub><mi>X</mi> <mi>i</mi></msub></math>
    , then we find the expected variation of the squared loss:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub><mi>X</mi> <mn>0</mn></msub>
    <mo>−</mo> <mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup> <mo stretchy="false">]</mo></mtd>
    <mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo fence="false"
    stretchy="false">{</mo> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo>
    <msub><mi>X</mi> <mn>0</mn></msub> <mo>−</mo> <mi>μ</mi> <mo stretchy="false">)</mo>
    <mo>−</mo> <mo stretchy="false">(</mo> <mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo>−</mo> <mi>μ</mi> <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo>
    <mn>2</mn></msup> <mo fence="false" stretchy="false">}</mo></mtd></mtr> <mtr><mtd><mo>=</mo>  <mrow><mi
    mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo>+</mo> <mrow><mi mathvariant="double-struck">V</mi></mrow>
    <mo stretchy="false">(</mo> <mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mo>=</mo>  <msup><mi>σ</mi>
    <mn>2</mn></msup> <mo>+</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mrow><mo>/</mo></mrow>
    <mi>n</mi></mtd></mtr> <mtr><mtd><mo>=</mo>  <mi>σ</mi> <msqrt><mn>1</mn> <mo>+</mo>
    <mn>1</mn> <mrow><mo>/</mo></mrow> <mi>n</mi></msqrt></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice there are two parts to the variation: one due to the variation of <math><msub><mi>X</mi>
    <mn>0</mn></msub></math> and the other due to the approximation of <math><mrow><mi
    mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">(</mo> <msub><mi>X</mi>
    <mn>0</mn></msub> <mo stretchy="false">)</mo></math> by <math><mrow><mover><mi>X</mi>
    <mo stretchy="false">¯</mo></mover></mrow></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of more complex models, the variation in prediction also breaks
    down into two components: the inherent variation in the data about the model plus
    the variation in the sampling distribution due to the estimation of the model.
    Assuming the model is roughly correct, we can express it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="bold">Y</mi></mrow>  <mo>=</mo>  <mtext
    mathvariant="bold">X</mtext> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo>∗</mo></mrow></msup>
    <mo>+</mo> <mi mathvariant="bold-italic">ϵ</mi></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math><msup><mi mathvariant="bold-italic">θ</mi> <mo>∗</mo></msup></math>
    is a <math><mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo>
    <mo>×</mo> <mn>1</mn></math> column vector, <math><mtext mathvariant="bold">X</mtext></math>
    is an <math><mi>n</mi> <mo>×</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo>
    <mn>1</mn> <mo stretchy="false">)</mo></math> design matrix, and <math><mi mathvariant="bold-italic">ϵ</mi></math>
    consists of <math><mi>n</mi></math> independent random variables that each have
    expected value 0 and variance <math><msup><mi>σ</mi> <mn>2</mn></msup></math>
    . In this equation, <math><mrow><mi mathvariant="bold">Y</mi></mrow></math> is
    a vector of random variables, where the expected value of each variable is determined
    by the design matrix and the variance is <math><msup><mi>σ</mi> <mn>2</mn></msup></math>
    . That is, the variation about the line is constant in that it does not change
    with <math><mrow><mi mathvariant="bold">x</mi></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'When we create prediction intervals in regression, they are given a <math><mn>1</mn>
    <mo>×</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></math>
    row vector of covariates, called <math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mn>0</mn></msub></math> . Then the prediction is <math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mn>0</mn></msub> <mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold"
    stretchy="false">^</mo></mover></mrow></math> , where <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> is the estimated
    parameter vector based on the original <math><mrow><mi mathvariant="bold">y</mi></mrow></math>
    and design matrix <math><mtext mathvariant="bold">X</mtext></math> . The expected
    squared error in this prediction is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mo stretchy="false">(</mo> <msub><mi>Y</mi> <mn>0</mn></msub>
    <mo>−</mo> <mrow><msub><mi mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn></msub></mrow>
    <mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup> <mo stretchy="false">]</mo></mtd>
    <mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo fence="false"
    stretchy="false">{</mo> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo>
    <msub><mi>Y</mi> <mn>0</mn></msub> <mo>−</mo> <mrow><msub><mi mathvariant="bold">x</mi>
    <mn mathvariant="bold">0</mn></msub> <msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo mathvariant="bold">∗</mo></mrow></msup></mrow> <mo stretchy="false">)</mo>
    <mo>−</mo> <mo stretchy="false">(</mo> <mrow><msub><mi mathvariant="bold">x</mi>
    <mn mathvariant="bold">0</mn></msub></mrow> <mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow> <mo>−</mo> <mrow><msub><mi
    mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn></msub></mrow> <msup><mi
    mathvariant="bold-italic">θ</mi> <mrow><mo>∗</mo></mrow></msup> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup> <mo fence="false" stretchy="false">}</mo></mtd></mtr>
    <mtr><mtd><mo>=</mo>  <mrow><mi mathvariant="double-struck">V</mi></mrow> <mo
    stretchy="false">(</mo> <msub><mi>ϵ</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo>
    <mo>+</mo> <mrow><mi mathvariant="double-struck">V</mi></mrow> <mo stretchy="false">(</mo>
    <mrow><msub><mi mathvariant="bold">x</mi> <mn mathvariant="bold">0</mn></msub></mrow>
    <mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mo>=</mo>  <msup><mi>σ</mi>
    <mn>2</mn></msup> <mo stretchy="false">[</mo> <mn>1</mn> <mo>+</mo> <msub><mrow><mi
    mathvariant="bold">x</mi></mrow> <mn>0</mn></msub> <mo stretchy="false">(</mo>
    <msup><mtext mathvariant="bold">X</mtext> <mi mathvariant="normal">⊤</mi></msup>
    <mtext mathvariant="bold">X</mtext> <msup><mo stretchy="false">)</mo> <mrow><mo>−</mo>
    <mn>1</mn></mrow></msup> <msubsup><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn>
    <mi mathvariant="normal">⊤</mi></msubsup> <mo stretchy="false">]</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: We approximate the variance of <math><mi>ϵ</mi></math> with the variance of
    the residuals from the least squares fit.
  prefs: []
  type: TYPE_NORMAL
- en: The prediction intervals we create using the normal curve rely on the additional
    assumption that the distribution of the errors is approximately normal. This is
    a stronger assumption than we make for the confidence intervals. With confidence
    intervals, the probability distribution of <math><msub><mi>X</mi> <mi>i</mi></msub></math>
    need not look normal for <math><mrow><mover><mi>X</mi> <mo stretchy="false">¯</mo></mover></mrow></math>
    to have an approximate normal distribution. Similarly, the probability distribution
    of <math><mi mathvariant="bold-italic">ϵ</mi></math> in the linear model need
    not look normal for the estimator <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    to have an approximate normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We also assume that the linear model is approximately correct when making these
    prediction intervals. In [Chapter 16](ch16.html#ch-risk), we considered the case
    where the fitted model doesn’t match the model that has produced the data. We
    now have the technical machinery to derive the model bias-variance trade-off introduced
    in that chapter. It’s very similar to the prediction interval derivation with
    a couple of small twists.
  prefs: []
  type: TYPE_NORMAL
- en: Probability Behind Model Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 16](ch16.html#ch-risk), we introduced model under- and overfitting
    with mean squared error (MSE). We described a general setup where the data might
    be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>y</mi> <mo>=</mo> <mi>g</mi> <mo stretchy="false">(</mo>
    <mrow><mi mathvariant="bold">x</mi></mrow> <mo stretchy="false">)</mo> <mo>+</mo>
    <mrow><mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The <math><mi>ϵ</mi></math> are assumed to behave like random errors that have
    no trends or patterns, have constant variance, and are independent of one another.
    The *signal* in the model is the function <math><mi>g</mi> <mo stretchy="false">(</mo>
    <mo stretchy="false">)</mo></math> . The data are the <math><mo stretchy="false">(</mo>
    <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math> pairs, and
    we fit models by minimizing the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><munder><mo movablelimits="true">min</mo> <mrow><mi>f</mi>
    <mo>∈</mo> <mrow><mi mathvariant="script">F</mi></mrow></mrow></munder> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub><mrow><mi
    mathvariant="bold">x</mi></mrow> <mi>i</mi></msub> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here <math><mrow><mi mathvariant="script">F</mi></mrow></math> is the collection
    of models over which we are minimizing. This collection might be all polynomials
    of degree <math><mi>m</mi></math> or less, bent lines with a bend at point <math><mi>k</mi></math>
    , and so on. Note that <math><mi>g</mi></math> doesn’t have to be in the collection
    of functions that we are using to fit a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal in model selection is to land on a model that predicts a new observation
    well. For a new observation, we would like the expected loss to be small:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi mathvariant="double-struck">E</mi></mrow> <mo
    stretchy="false">[</mo> <msub><mi>y</mi> <mn>0</mn></msub> <mo>−</mo> <mi>f</mi>
    <mo stretchy="false">(</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn></msub>
    <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This expectation is with respect to the distribution of possible <math><mo
    stretchy="false">(</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo></math>
    and is called *risk*. Since we don’t know the population distribution of <math><mo
    stretchy="false">(</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn></msub>
    <mo>,</mo> <msub><mi>y</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo></math>
    , we can’t calculate the risk, but we can approximate it by the average loss over
    the data we have collected:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <msub><mi>y</mi> <mn>0</mn></msub> <mo>−</mo> <mi>f</mi>
    <mo stretchy="false">(</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn></msub>
    <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd>
    <mtd><mo>≈</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo>
    <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This approximation goes by the name of *empirical risk*. But hopefully you
    recognize it as the MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We fit models by minimizing the empirical risk (or MSE) over all possible models,
    <math><mrow><mi mathvariant="script">F</mi></mrow> <mo mathvariant="script">=</mo>
    <mo fence="false" stretchy="false">{</mo> <mi mathvariant="script">f</mi> <mo
    fence="false" stretchy="false">}</mo></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><munder><mo movablelimits="true">min</mo> <mrow><mi>f</mi>
    <mo>∈</mo> <mrow><mi mathvariant="script">F</mi></mrow></mrow></munder> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub><mrow><mi
    mathvariant="bold">x</mi></mrow> <mi>i</mi></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: The fitted model is called <math><mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    , a slightly more general representation of the linear model <math><mtext mathvariant="bold">X</mtext>
    <mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    . This technique is aptly called *empirical risk minimization*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 16](ch16.html#ch-risk), we saw problems arise when we used the
    empirical risk to both fit a model and evaluate the risk for a new observation.
    Ideally, we want to estimate the risk (expected loss):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi
    mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mn>0</mn></msub> <mo>−</mo> <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn></msub>
    <mo stretchy="false">)</mo> <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>
    <mo stretchy="false">]</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: where the expected value is over the new observation <math><mo stretchy="false">(</mo>
    <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mn>0</mn></msub> <mo>,</mo>
    <msub><mi>y</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo></math> and over
    <math><mrow><mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow></mrow></math>
    (which involves the original data <math><mo stretchy="false">(</mo> <msub><mtext
    mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo> <msub><mrow><mi>y</mi></mrow>
    <mi>i</mi></msub> <mo stretchy="false">)</mo></math> , <math><mi>i</mi> <mo>=</mo>
    <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mi>n</mi></math> ).
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the problem, we decompose this risk into three parts representing
    the model bias, the model variance, and the irreducible error from <math><mi>ϵ</mi></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left right" columnspacing="0em
    2em" displaystyle="true" rowspacing="3pt"><mtr><mtd><mrow><mi mathvariant="double-struck">E</mi></mrow></mtd>
    <mtd><mo stretchy="false">[</mo> <msub><mi>y</mi> <mn>0</mn></msub> <mo>−</mo>
    <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo>
    <mn>2</mn></msup></mtd></mtr> <mtr><mtd><mo>=</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo>+</mo> <msub><mi>ϵ</mi> <mn>0</mn></msub>
    <mo>−</mo> <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd> <mtd><mtext>definition</mtext>  <mtext>of</mtext>  <msub><mi>y</mi>
    <mn>0</mn></msub></mtd></mtr> <mtr><mtd><mo>=</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo>+</mo> <msub><mi>ϵ</mi> <mn>0</mn></msub>
    <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo>
    <mo>+</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo>
    <mo>−</mo> <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd> <mtd><mtext>adding</mtext>  <mo>±</mo>
    <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo></mtd></mtr>
    <mtr><mtd><mo>=</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <mi>g</mi> <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo
    stretchy="false">)</mo> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo>
    <mo stretchy="false">]</mo> <mo>−</mo> <mo stretchy="false">(</mo> <mrow><mover><mi>f</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo>
    <mo stretchy="false">]</mo> <mo stretchy="false">)</mo> <mo>+</mo> <msub><mi>ϵ</mi>
    <mn>0</mn></msub> <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd> <mtd><mtext>rearranging
    terms</mtext></mtd></mtr> <mtr><mtd><mo>=</mo> <mo stretchy="false">[</mo> <mi>g</mi>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo>
    <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup> <mo>+</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo>
    <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow> <mo stretchy="false">[</mo>
    <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup> <mo>+</mo> <msup><mi>σ</mi>
    <mn>2</mn></msup></mtd> <mtd><mtext>expanding the square</mtext></mtd></mtr> <mtr><mtd><mo>=</mo>    <msup><mtext>model
    bias</mtext> <mn>2</mn></msup>    <mo>+</mo>    <mtext>model variance</mtext>   <mo>+</mo>   <mtext>error</mtext></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'To derive the equality labeled “expanding the square,” we need to formally
    prove that the cross-product terms in the expansion are all 0\. This takes a bit
    of algebra and we don’t present it here. But the main idea is that the terms <math><msub><mi>ϵ</mi>
    <mn>0</mn></msub></math> and <math><mo stretchy="false">(</mo> <mrow><mover><mi>f</mi>
    <mo stretchy="false">^</mo></mover></mrow> <mo stretchy="false">(</mo> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo stretchy="false">)</mo> <mo>−</mo> <mrow><mi mathvariant="double-struck">E</mi></mrow>
    <mo stretchy="false">[</mo> <mrow><mover><mi>f</mi> <mo stretchy="false">^</mo></mover></mrow>
    <mo stretchy="false">(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo stretchy="false">]</mo>
    <mo stretchy="false">)</mo></math> are independent and both have the expected
    value 0\. The remaining three terms in the final equation—model bias, model variance,
    and irreducible error—are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Model bias
  prefs: []
  type: TYPE_NORMAL
- en: The first of the three terms in the final equation is model bias (squared).
    When the signal, <math><mi>g</mi></math> , does not belong to the model space,
    we have model bias. If the model space can approximate <math><mi>g</mi></math>
    well, then the bias is small. Note that this term is not present in our prediction
    intervals because we assumed that there is no (or minimal) model bias.
  prefs: []
  type: TYPE_NORMAL
- en: Model variance
  prefs: []
  type: TYPE_NORMAL
- en: The second term represents the variability in the fitted model that comes from
    the data. We have seen in earlier examples that high-degree polynomials can overfit,
    and so vary a lot from one set of data to the next. The more complex the model
    space, the greater the variability in the fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: Irreducible error
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last term is the variability in the error, the <math><msub><mi>ϵ</mi>
    <mn>0</mn></msub></math> , which is dubbed the “irreducible error.” This error
    sticks around whether we have underfit with a simple model (high bias) or overfit
    with a complex model (high variance).
  prefs: []
  type: TYPE_NORMAL
- en: This representation of the expected loss shows the bias-variance decomposition
    of a fitted model. Model selection aims to balance these two competing sources
    of error. The train-test split, cross-validation, and regularization introduced
    in [Chapter 16](ch16.html#ch-risk) are techniques to either mimic the expected
    loss for a new observation or penalize a model from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we have covered a lot of theory in this chapter, we have attempted to
    tie it to the basics of the urn model and the three distributions: population,
    sample, and sampling. We wrap up the chapter with a few cautions to keep in mind
    when performing hypothesis tests and when making confidence or prediction intervals.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we based our development of the theory behind inference
    and prediction on the urn model. The urn induced a probability distribution on
    the estimator, such as the sample mean and the least squares regression coefficients.
    We end this chapter with some cautions about these statistical procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw how the SD of an estimator has a factor of the square root of the sample
    size in the denominator. When samples are large, the SD can be quite small and
    can lead to rejecting a hypothesis or very narrow confidence intervals. When this
    happens it’s good to consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the difference that you have detected an important difference? That is, a
    <math><mi>p</mi></math> -value may be quite small, indicating a surprising result,
    but the actual effect observed may be unimportant. *Statistical significance*
    does not imply *practical significance*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep in mind that these calculations do not incorporate bias, such as non-response
    bias and measurement bias. The bias might well be larger than any difference due
    to chance variation in the sampling distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At times, we know the sample is not from a chance mechanism, but it can still
    be useful to carry out a hypothesis test. In this case, the null model would test
    whether the sample (and estimator) are as if they were at random. When this test
    is rejected, we confirm that something nonrandom has led to the observed data.
    This can be a useful conclusion: that the difference between what we expect and
    what we observed is not explained by chance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At other times, the sample consists of the complete population. When this happens,
    we might not need to make confidence intervals or hypothesis tests because we
    have observed all values in the population. That is, inference is not required.
    However, we can instead place a different interpretation on hypothesis tests:
    we can suppose that any relation observed between two features was randomly distributed
    without relation to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also saw how the bootstrap can be used when we don’t have enough information
    about the population. The bootstrap is a powerful technique, but it has limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that the original sample is large and random so that the sample resembles
    the population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the bootstrap process many times. Typically 10,000 replications is a
    reasonable number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bootstrap tends to have difficulties when:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimator is influenced by outliers.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameter is based on extreme values of the distribution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The sampling distribution of the statistic is far from bell shaped.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, we rely on the sampling distribution being approximately normal
    in shape. At times, the sampling distribution looks roughly normal but has thicker
    tails. In these situations, the family of <math><mi>t</mi></math> -distributions
    might be appropriate to use instead of the normal.
  prefs: []
  type: TYPE_NORMAL
- en: A model is usually only an approximation of underlying reality, and the precision
    of the statement that <math><msup><mi>θ</mi> <mo>∗</mo></msup></math> exactly
    equals 0 is at odds with this notion of a model. The inference depends on the
    correctness of our model. We can partially check the model assumptions, but some
    amount of doubt goes with any model. In fact, it often happens that the data suggest
    more than one possible model, and these models may even be contradictory.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, at times, the number of hypothesis tests or confidence intervals can
    be quite large, and we need to exercise caution to avoid spurious results. This
    problem is called <math><mi>p</mi></math> -hacking and is another example of the
    reproducibility crisis in science described in [Chapter 10](ch10.html#ch-eda).
    <math><mi>P</mi></math> -hacking is based on the notion that if we test, say,
    100 hypotheses, all of which are true, then we would expect to get a few surprise
    results and reject a few of these hypotheses. This phenomenon can happen in multiple
    linear regression when we have a large number of features in a model, and techniques
    have been developed to limit the dangers of these false discoveries.
  prefs: []
  type: TYPE_NORMAL
- en: We next recap the modeling process with a case study.
  prefs: []
  type: TYPE_NORMAL
