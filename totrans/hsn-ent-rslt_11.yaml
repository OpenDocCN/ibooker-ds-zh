- en: Chapter 11\. Further Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully the previous chapters have given you a solid practical understanding
    of how to resolve entities within your datasets and have equipped you to overcome
    some of the challenges you are likely to meet along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world data is messy and full of surprises, so joining it up is rarely straightforward.
    But it’s well worth spending the time to make the connections because the story
    becomes so much richer when we can bring together all the pieces of the jigsaw.
  prefs: []
  type: TYPE_NORMAL
- en: In this short closing chapter, I’ll talk about a few aspects of entity resolution
    that are worth considering when building a resilient production solution. I’ll
    also share some closing thoughts on the future of the art and science of entity
    resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Data Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with any analytic process, the importance of understanding the context and
    quality of your input data cannot be overstated. Quirks or misunderstandings in
    data that a traditional application could tolerate may fundamentally derail a
    matching process. Poor data can result in over- and underlinking, sometimes matching
    entities that do not represent the same person, with potentially serious consequences.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I’ll discuss the most important data-related issues to consider
    when performing entity resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book we have primarily used structured data to perform the matching
    process. When we encountered semi-unstructured data we used very simple rules
    of thumb to extract the attributes we needed. For example, in [Chapter 2](ch02.html#chapter_2),
    we somewhat arbitrarily split full name strings into `Firstname` and `Lastname`,
    and in [Chapter 6](ch06.html#chapter_6), we extracted only the postcode from the
    full address text. In the name of simplicity, we neglected potentially valuable
    data that could have enriched our matching process.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the state of the art in extracting meaning from unstructured text
    has developed considerably in the last few years. Advances in named entity recognition
    (NER) techniques for understanding sentence construction, and extracting entities
    in context, mean we can more easily link to unstructured content.
  prefs: []
  type: TYPE_NORMAL
- en: For example, there are several Python libraries available (such as usaddress,
    deepparse, and libpostal) that can parse addresses, extracting individual house
    number, street, and town attributes. The performance of these models depends on
    the availability of high-quality training data and so varies by country.
  prefs: []
  type: TYPE_NORMAL
- en: However, even the most sophisticated NER cannot make up for the absence of key
    attributes if they are not present in the source text. News articles, for example,
    will rarely provide a date of birth for their subjects, and financial transactions
    will not typically include a social security number.
  prefs: []
  type: TYPE_NORMAL
- en: Data Quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our illustrative examples, we have accepted much of our input data at face
    value and taken expedient shortcuts to prepare our data for matching. For example,
    as a shortcut, we simply dropped records that contained an attribute with a missing 
    value. Our process should be able to ignore (i.e., assign a zero match weight
    to that attribute) as opposed to discarding the whole record. For a production
    solution, a more rigorous approach to measuring and continuously improving data
    quality is vital. The better the data quality, the easier the matching task will
    be.
  prefs: []
  type: TYPE_NORMAL
- en: Additional data checks for completeness and validity (including identifying
    hidden and unexpected characters) will alert you to problems that may frustrate
    your matching process in unexpected ways and are challenging to diagnose further
    down the line.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal Equivalence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The entity resolution process relies on matching attributes to determine whether
    records refer to the same real-world entity. However, the attributes associated
    with an entity may change over time. Last names may change with marital status;
    phone numbers and email addresses may change as individuals swap service providers;
    passports, driving licenses, and other forms of identification are reissued with
    new identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: It sounds obvious, but this temporal aspect of entity resolution is often overlooked,
    so my advice is to be careful with datasets that contain data drawn from different
    time periods and ensure the model doesn’t place too much weight on attributes
    that are subject to change. Of course, where the entity is trying not to be identified, frequent
    attribute changes can be sign of a deliberate attempt to frustrate the entity
    resolution process.
  prefs: []
  type: TYPE_NORMAL
- en: Attribute Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#chapter_3), we explored some of the most commonly used
    techniques for approximate string matching. We considered edit distance and phonetic
    equivalence to determine a match between discrete name attributes. These approaches
    work well when we have a single token, such as a discrete element of a name, to
    compare. When we are faced with assessing the similarity between two strings with
    multiple words or tokens, such as addresses or the names of organizations, then
    there are other techniques we can consider.
  prefs: []
  type: TYPE_NORMAL
- en: Set Matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an entity is identified by a collection of terms, we can use set-based
    methods (such as Jaccard index^([1](ch11.html#id672))) to measure the degree of
    overlap between the tokens present in each set. More sophisticated methods, such
    as Monge-Elkan similarity, combine both set-based and edit distance techniques
    to perform the comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Recent advances in sentence embedding^([2](ch11.html#id674)) now allow us to
    translate the semantic meaning of a string of text into a vector (an array of
    quantities in number of dimensions). These vectorizing models, trained on vast
    repositories of open source data, are accessible through public interfaces, such
    as OpenAI’s embedding API. The semantic similarity of these text strings can then
    be assessed by techniques such as cosine similarity, which measures the angle
    between the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Vector-based approaches can also be applied to measure the similarity between
    individual words (represented as strings of single characters or multiple character
    n-grams) but they typically do not consider the sequence of these letters, which
    can be extremely important in matching, e.g., NAB (an Australian bank) versus
    NBA (a US basketball organization).
  prefs: []
  type: TYPE_NORMAL
- en: Geocoding Location Matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to matching the individual words, or tokens, that comprise an
    address is to convert the address into a set of geographic coordinates (latitude
    and longitude). We can then compare these values within a set straight line distance
    tolerance to determine if they refer to the same place. Clearly, for multioccupancy
    locations in close proximity (within a shared building or industrial estate, for
    example), this approach can produce a number of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing Google, Microsoft, and OpenStreetMap (via Nominatim)
    offer geocoding APIs that will perform the conversion, subject to pricing and
    usage policies. As an on-demand software as a service (SaaSso) offering, this
    approach may not be suitable for bulk address comparison or where the data is
    sensitive and cannot be shared with third parties.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating Comparisons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen, there are often several different techniques we can use to
    compare attributes, each with their strengths and weaknesses. In certain use cases
    it may be beneficial to evaluate a potential match using multiple approaches,
    for example, using both a Soundex comparison and an edit distance measurement
    to determine the most appropriate result.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that if parallel techniques are used on the same attributes,
    then the results will not respect the conditional independence assumption of the
    Fellegi-Sunter model and therefore may not perform well when using probabilistic
    tools such as Splink. In particular, the different measurements should be included
    at different comparison levels within a single comparison to avoid double counting.
    Alternatively, these different measurements could be preaggregated into a single
    score using a custom similarity function.
  prefs: []
  type: TYPE_NORMAL
- en: Post Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.html#chapter_10), we saw how we can group paired records
    into a single distinct cluster. We also introduced the challenge of determining
    which attribute values to use to describe our unified entity. The selection logic
    to choose which attribute value to promote is likely to be bespoke to your use
    case and may depend upon the relative trustworthiness and seniority of your datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Once a canonical entity view has been established, there is the opportunity
    to repeat the pairwise matching exercise, treating the newly consolidated entity
    as a new record. Additional records, previously too dissimilar to match, may now
    reach the equivalence threshold due to the concentration of attributes in the
    new composite entity.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the input records shown in [Table 11-1](#table-11-1).
    Records 1 and 2 may be assessed as referring to the same individual (based on
    equivalent first name and date of birth) but Record 3 does not have sufficient
    commonality with either to join that small cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-1\. Entity resolution—input
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute** | **Record 1** | ** Record 2** | **Record 3** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| First name | Michael  | Michael | M |'
  prefs: []
  type: TYPE_TB
- en: '| Last name | Shearer | Shear | Shearer |'
  prefs: []
  type: TYPE_TB
- en: '| Date of birth | 4/1/1970 | 4/1/1970 |   |'
  prefs: []
  type: TYPE_TB
- en: '| Place of birth |                          | Stow on the Wold | Stow on the
    Wold |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile number |   |   | 07700 900999 |'
  prefs: []
  type: TYPE_TB
- en: Having resolved Records 1 and 2 into a single entity, suppose we choose the
    value “Shearer” over “Shear” to represent the last name. Perhaps Record 1 was
    part of a dataset that was deemed of higher quality than that containing Record
    2\. Or perhaps we implemented a rule to select the more complete value. As shown
    in [Table 11-2](#table-11-2), we would then have a richer set of attributes to
    match against Record 3.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-2\. Entity resolution—pairwise clustering
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute** | **Cluster 1 Record 1 and 2** | **Record 3** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| First name | Michael  | M  |'
  prefs: []
  type: TYPE_TB
- en: '| Last name | Shearer | Shearer |'
  prefs: []
  type: TYPE_TB
- en: '| Date of birth | 4/1/1970 |   |'
  prefs: []
  type: TYPE_TB
- en: '| Place of birth | Stow on the Wold        | Stow on the Wold |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile number |   | 07700 900999 |'
  prefs: []
  type: TYPE_TB
- en: If an exact last name match and equivalent place of birth were deemed sufficient
    evidence, then we could conclude that Record 3 should now join the cluster of
    Records 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Table 11-3](#table-11-3), we have now resolved all three records
    into a single entity and, thanks to our additional step, added a phone number
    that we would otherwise not have linked to our entity.
  prefs: []
  type: TYPE_NORMAL
- en: Table 11-3\. Entity resolution—entity record resolution
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute** | **Cluster 1 Record 1, 2, and 3** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| First name | Michael  |'
  prefs: []
  type: TYPE_TB
- en: '| Last name | Shearer |'
  prefs: []
  type: TYPE_TB
- en: '| Date of birth | 4/1/1970 |'
  prefs: []
  type: TYPE_TB
- en: '| Place of birth | Stow on the Wold        |'
  prefs: []
  type: TYPE_TB
- en: '| Mobile number | **07700 900999** |'
  prefs: []
  type: TYPE_TB
- en: This shows how we can progressively build the confidence to join these records
    into a single clustered entity. This is an example of “bottom up,” or agglomerative
    hierarchical clustering. In this simple example, we exhaustively linked all three
    records, but in larger datasets there would have been many more candidates to
    compare and potentially cluster together in subsequent iterations.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.html#chapter_8), we saw how the Google Cloud Entity Reconciliation
    service uses this technique. The Google service specifies a number of iterations
    after which it terminates the clustering process. Clearly, this approach can be
    computationally intensive on large datasets and is not guaranteed to find the
    optimum solution.
  prefs: []
  type: TYPE_NORMAL
- en: Graphical Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After attribute comparison and pairwise match classification, the final steps
    in the entity resolution process overlap quite significantly with the field of
    graph analytics.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in [Chapter 10](ch10.html#chapter_10), the output of the clustering
    process can be presented as an entity graph of source records (nodes) and a set
    of matching attribute pairs (edges).  This representation may form part of a wider
    network graph showing how distinct entities are connected via shared relationships
    or common attributes. This representation is useful to allow inspection (and potential
    discounting) of the matches informed by the context of their wider network.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if matching confidence is high, or a simpler representation is
    required, the entity graph can resolve (or collapse) to a single node. That node
    can either list a canonical set of attributes or persist the alternative attribute
    values for closer examination. This curated network, or knowledge graph, provides
    a combined view of all the information about a given entity drawn from different
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that this is partly how Google search works today. Search
    results now present factual information from [Google’s Knowledge Graph](https://oreil.ly/H59UU),
    which contains over 500 million objects with over 3.5 billion facts about, and
    relationships between, these different objects. As we saw in [Chapter 8](ch08.html#chapter_8),
    you can now resolve your entities against Google’s objects using their reconciliation
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Real-Time Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have considered batch-based entity resolution of static datasets.
    This point-in-time approach allows us to compare, match, and cluster all relevant
    records to produce a set of resolved entities. This reference point can then be
    used for a period of time before becoming stale and the exercise repeated. In
    [Chapter 6](ch06.html#chapter_6), we saw how to pairwise match a new record against
    an existing dataset using a pretrained probabilistic model.
  prefs: []
  type: TYPE_NORMAL
- en: If an up-to-the-minute set of all resolved entities is required, then incrementally
    processing new records as they arrive brings with it some additional considerations.
    Depending upon the processing time window available, it may be challenging to
    recluster and generate new canonical entities based on the contents of a newly
    available record, or to reshape existing entities into a new configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The functional performance of an entity resolution solution may be evaluated
    by the extent to which records are matched when then they are truly distinct (overlinking)
    or remain unconnected when they refer to the same real-world entity (underlinking).
    The nature of the decisions and actions you propose to take based on the resolved
    dataset will determine the relative priority of these metrics. For example, in
    a high-risk situation, where the consequence of missing a link could be significant,
    you may wish to err toward overlinking. In a more speculative process, you may
    wish to lean toward underlinking to minimize unnecessary customer friction.
  prefs: []
  type: TYPE_NORMAL
- en: Systematic evaluation of the degree to which your solution is over- or underlinking
    is challenging. In the earlier chapters of this book, we had the benefit of a
    known population against which we could evaluate the precision, recall, and accuracy
    of our process. But in practice that is rarely the case. The need to resolve entities
    usually arises as a result of the lack of a common key or known population between
    datasets, thereby depriving the evaluator of ground truth against which to measure
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Smaller benchmark datasets, which can be affordably manually linked, are often
    used to predict performance at scale. However, these limited datasets can give
    a distorted view of the likely real-world outcomes. Larger datasets are more likely
    to contain distinct entities that have similar attributes (e.g., same name), increasing
    the rate of false positives. Care must also be taken to make sure the distribution
    of benchmark dataset is accounted for in the evaluation process. The ratio of
    matching to nonmatching records is often significantly higher in benchmark datasets
    that are constructed to check that the matching process finds the right matches
    (i.e., maximizes recall) but gives an overly optimistic view of the frequency
    of errors (i.e., overestimates precision). There is also a risk, particularly
    for the more sophisticated embedding-based approaches, of overfitting the entity
    resolution model because the benchmark data was represented in the training data,
    resulting in poor generalized performance.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of an entity resolution solution is a critical part
    of model development and improvement. It requires labeling data that can then
    be used to train more sophisticated models or to estimate performance metrics
    such as precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main approach types to data labeling and performance evaluation
    in entity resolution applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise approach
  prefs: []
  type: TYPE_NORMAL
- en: Labeling a set of record pairs as a match and not a match
  prefs: []
  type: TYPE_NORMAL
- en: Cluster-based approach
  prefs: []
  type: TYPE_NORMAL
- en: Identifying or using a set of known, fully resolved entities (clusters)
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using a pairwise approach we can estimate precision, i.e., how often we are
    correct when we declare a match, by simply sampling pairs of matched records and
    manually reviewing them. Once classified as true or false positives, we can calculate
    precision as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P r e c i s i o n equals StartFraction upper T r u e p
    o s i t i v e s Over left-parenthesis upper T r u e p o s i t i v e s plus upper
    F a l s e p o s i t i v e s right-parenthesis EndFraction"><mrow><mi>P</mi> <mi>r</mi>
    <mi>e</mi> <mi>c</mi> <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo>
    <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: To estimate recall is more challenging, as we essentially have to repeat the
    resolution exercise to identify records that should have been declared a match
    but were not. This can be more efficiently estimated by selecting a block of loosely
    matching records and then exhaustively reviewing all the potential pairs of records
    within this block. Of course, as with any blocking approach, we risk overlooking
    wildcard matches that didn’t make it into our loosely selected block.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, recall is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R e c a l l equals StartFraction upper T r u e p o s i
    t i v e s Over left-parenthesis upper T r u e p o s i t i v e s plus upper F a
    l s e n e g a t i v e s right-parenthesis EndFraction"><mrow><mi>R</mi> <mi>e</mi>
    <mi>c</mi> <mi>a</mi> <mi>l</mi> <mi>l</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Cluster-Based Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to the pairwise approach is to manually determine, for example,
    through the use of search tools, a true cluster view of those records that described
    the same real-world entity. We can then compare our pairwise predictions against
    this gold standard to assess our performance and improve our model. For example,
    consider the simple example shown in [Figure 11-1](#fig-11-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Cluster-based evaluation example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Within a population of four records, A to D, our model has paired records A
    + B and C + D. A true cluster view shows that A, B, and C all refer to the same
    entity but D is distinct. From this, we can assess the following and calculate
    our performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '| Record pair | Predicted | Actual | Result |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A - B | Match | Match | True positive |'
  prefs: []
  type: TYPE_TB
- en: '| A - C | Not match | Match | False negative |'
  prefs: []
  type: TYPE_TB
- en: '| A - D | Not match | Not match | True negative |'
  prefs: []
  type: TYPE_TB
- en: '| B - C | Not match | Match | False negative |'
  prefs: []
  type: TYPE_TB
- en: '| B - D | Not match | Not match | True negative |'
  prefs: []
  type: TYPE_TB
- en: '| C - D | Match | Not match | False positive |'
  prefs: []
  type: TYPE_TB
- en: The evaluation of entity resolution is an area of active research with toolsets
    emerging to assist with the process and produce actionable feedback to improve
    performance.^([3](ch11.html#id710))
  prefs: []
  type: TYPE_NORMAL
- en: Future of Entity Resolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Entity resolution is by definition a means to an end. By resolving entities,
    we seek to assemble all the relevant information from multiple sources to enable
    us to derive valuable insights and ultimately make better decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In an increasingly digital world, we have a shared responsibility to ensure
    that our data records accurately and comprehensively reflect society. If we have
    incorrect information, or only part of the picture, we risk drawing the wrong
    conclusions and taking the wrong actions. There is also a duty to respect individual
    privacy and to manage sensitive data accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: The art and science of entity resolution is evolving to balance these concerns.
    Entity resolution can enhance your ability to connect the dots in your data and
    show the bigger picture. Increasingly it can be done without unnecessarily sharing
    personal information. New machine learning algorithms, and techniques to more
    rigorously evaluate and optimize their performance, are now freely available.
  prefs: []
  type: TYPE_NORMAL
- en: Recent advancements in the scale and availability of large language models (LLMs)
    open up a breadth of information about how real-world entities are described and
    interrelated. The embedding technology that underpins these models provides a
    rich context to inform the matching processes. The increasing availability of
    managed entity resolution services and the ability to relate your entities to
    public knowledge repositories promises to make the matching process easier and
    the results richer.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you have enjoyed our shared journey through the challenges of entity
    resolution and that you feel ready to join the dots in your data. Who knows what
    you will find...
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch11.html#id672-marker)) For more details on the Jaccard index, visit
    the [Wikipedia page](https://oreil.ly/vJyir).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch11.html#id674-marker)) Further details on how to use the sent2vec library
    are available in the [PyPI documentation](https://oreil.ly/SUHrG).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch11.html#id710-marker)) For example, an open source Python package for
    the evaluation of entity resolution (ER) systems is available on [GitHub](https://github.com/Valires/er-evaluation).
  prefs: []
  type: TYPE_NORMAL
