- en: Chapter 16\. Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of people say there’s a fine line between genius and insanity. I don’t
    think there’s a fine line, I actually think there’s a yawning gulf.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bill Bailey
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#introduction), we briefly looked at the problem of
    trying to predict which DataSciencester users paid for premium accounts. Here
    we’ll revisit that problem.
  prefs: []
  type: TYPE_NORMAL
- en: The Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have an anonymized dataset of about 200 users, containing each user’s salary,
    her years of experience as a data scientist, and whether she paid for a premium
    account ([Figure 16-1](#logit_image)). As is typical with categorical variables,
    we represent the dependent variable as either 0 (no premium account) or 1 (premium
    account).
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, our data is a list of rows `[experience, salary, paid_account]`.
    Let’s turn it into the format we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'An obvious first attempt is to use linear regression and find the best model:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="paid account equals beta 0 plus beta 1 experience plus beta 2
    salary plus epsilon" display="block"><mrow><mtext>paid</mtext> <mtext>account</mtext>
    <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub>
    <mtext>experience</mtext> <mo>+</mo> <msub><mi>β</mi> <mn>2</mn></msub> <mtext>salary</mtext>
    <mo>+</mo> <mi>ε</mi></mrow></math>![Paid and Unpaid Users.](assets/dsf2_1601.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16-1\. Paid and unpaid users
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'And certainly, there’s nothing preventing us from modeling the problem this
    way. The results are shown in [Figure 16-2](#linear_regression_for_probabilities):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Using Linear Regression to Predict Premium Accounts.](assets/dsf2_1602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. Using linear regression to predict premium accounts
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But this approach leads to a couple of immediate problems:'
  prefs: []
  type: TYPE_NORMAL
- en: We’d like for our predicted outputs to be 0 or 1, to indicate class membership.
    It’s fine if they’re between 0 and 1, since we can interpret these as probabilities—an
    output of 0.25 could mean 25% chance of being a paid member. But the outputs of
    the linear model can be huge positive numbers or even negative numbers, which
    it’s not clear how to interpret. Indeed, here a lot of our predictions were negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The linear regression model assumed that the errors were uncorrelated with the
    columns of *x*. But here, the regression coefficient for `experience` is 0.43,
    indicating that more experience leads to a greater likelihood of a premium account.
    This means that our model outputs very large values for people with lots of experience.
    But we know that the actual values must be at most 1, which means that necessarily
    very large outputs (and therefore very large values of `experience`) correspond
    to very large negative values of the error term. Because this is the case, our
    estimate of `beta` is biased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we’d like instead is for large positive values of `dot(x_i, beta)` to correspond
    to probabilities close to 1, and for large negative values to correspond to probabilities
    close to 0\. We can accomplish this by applying another function to the result.
  prefs: []
  type: TYPE_NORMAL
- en: The Logistic Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the case of logistic regression, we use the *logistic function*, pictured
    in [Figure 16-3](#graph_of_logistic_function):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Logistic function.](assets/dsf2_1603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. The logistic function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As its input gets large and positive, it gets closer and closer to 1\. As its
    input gets large and negative, it gets closer and closer to 0\. Additionally,
    it has the convenient property that its derivative is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'which we’ll make use of in a bit. We’ll use this to fit a model:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals f left-parenthesis x Subscript
    i Baseline beta right-parenthesis plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mi>β</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where *f* is the `logistic` function.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that for linear regression we fit the model by minimizing the sum of
    squared errors, which ended up choosing the *β* that maximized the likelihood
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Here the two aren’t equivalent, so we’ll use gradient descent to maximize the
    likelihood directly. This means we need to calculate the likelihood function and
    its gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Given some *β*, our model says that each <math><msub><mi>y</mi> <mi>i</mi></msub></math>
    should equal 1 with probability <math><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow></math> and 0 with probability <math><mrow><mn>1</mn>
    <mo>-</mo> <mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi>
    <mo>)</mo></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the PDF for <math><msub><mi>y</mi> <mi>i</mi></msub></math>
    can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p left-parenthesis y Subscript i Baseline vertical-bar x Subscript
    i Baseline comma beta right-parenthesis equals f left-parenthesis x Subscript
    i Baseline beta right-parenthesis Superscript y Super Subscript i Baseline left-parenthesis
    1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis
    Superscript 1 minus y Super Subscript i" display="block"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>|</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>,</mo> <mi>β</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>f</mi> <msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi><mo>)</mo></mrow> <msub><mi>y</mi> <mi>i</mi></msub></msup>
    <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mrow><mn>1</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'since if <math><msub><mi>y</mi> <mi>i</mi></msub></math> is 0, this equals:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis"
    display="block"><mrow><mn>1</mn> <mo>-</mo> <mi>f</mi> <mo>(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and if <math><msub><mi>y</mi> <mi>i</mi></msub></math> is 1, it equals:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f left-parenthesis x Subscript i Baseline beta right-parenthesis"
    display="block"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mi>β</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that it’s actually simpler to maximize the *log likelihood*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="log upper L left-parenthesis beta vertical-bar x Subscript i
    Baseline comma y Subscript i Baseline right-parenthesis equals y Subscript i Baseline
    log f left-parenthesis x Subscript i Baseline beta right-parenthesis plus left-parenthesis
    1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus
    f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis"
    display="block"><mrow><mo form="prefix">log</mo> <mi>L</mi> <mrow><mo>(</mo> <mi>β</mi>
    <mo>|</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo form="prefix">log</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mi>β</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Because log is a strictly increasing function, any `beta` that maximizes the
    log likelihood also maximizes the likelihood, and vice versa. Because gradient
    descent minimizes things, we’ll actually work with the *negative* log likelihood,
    since maximizing the likelihood is the same as minimizing its negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we assume different data points are independent from one another, the overall
    likelihood is just the product of the individual likelihoods. That means the overall
    log likelihood is the sum of the individual log likelihoods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A little bit of calculus gives us the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: at which point we have all the pieces we need.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll want to split our data into a training set and a test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'after which we find that `beta` is approximately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'These are coefficients for the `rescale`d data, but we can transform them back
    to the original data as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, these are not as easy to interpret as linear regression coefficients.
    All else being equal, an extra year of experience adds 1.6 to the input of `logistic`.
    All else being equal, an extra $10,000 of salary subtracts 2.88 from the input
    of `logistic`.
  prefs: []
  type: TYPE_NORMAL
- en: The impact on the output, however, depends on the other inputs as well. If `dot(beta,
    x_i)` is already large (corresponding to a probability close to 1), increasing
    it even by a lot cannot affect the probability very much. If it’s close to 0,
    increasing it just a little might increase the probability quite a bit.
  prefs: []
  type: TYPE_NORMAL
- en: What we can say is that—all else being equal—people with more experience are
    more likely to pay for accounts. And that—all else being equal—people with higher
    salaries are less likely to pay for accounts. (This was also somewhat apparent
    when we plotted the data.)
  prefs: []
  type: TYPE_NORMAL
- en: Goodness of Fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We haven’t yet used the test data that we held out. Let’s see what happens
    if we predict *paid account* whenever the probability exceeds 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This gives a precision of 75% (“when we predict *paid account* we’re right 75%
    of the time”) and a recall of 80% (“when a user has a paid account we predict
    *paid account* 80% of the time”), which is not terrible considering how little
    data we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot the predictions versus the actuals ([Figure 16-4](#logistic_prediction_vs_actual)),
    which also shows that the model performs well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Logistic Regression Predicted vs Actual.](assets/dsf2_1604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. Logistic regression predicted versus actual
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The set of points where `dot(beta, x_i)` equals 0 is the boundary between our
    classes. We can plot this to see exactly what our model is doing ([Figure 16-5](#logit_image_part_two)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Paid and Unpaid Users With Decision Boundary.](assets/dsf2_1605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-5\. Paid and unpaid users with decision boundary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This boundary is a *hyperplane* that splits the parameter space into two half-spaces
    corresponding to *predict paid* and *predict unpaid*. We found it as a side effect
    of finding the most likely logistic model.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach to classification is to just look for the hyperplane
    that “best” separates the classes in the training data. This is the idea behind
    the *support vector machine*, which finds the hyperplane that maximizes the distance
    to the nearest point in each class ([Figure 16-6](#separating_hyperplane)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A Separating Hyperplane](assets/dsf2_1606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-6\. A separating hyperplane
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finding such a hyperplane is an optimization problem that involves techniques
    that are too advanced for us. A different problem is that a separating hyperplane
    might not exist at all. In our “who pays?” dataset there simply is no line that
    perfectly separates the paid users from the unpaid users.
  prefs: []
  type: TYPE_NORMAL
- en: We can sometimes get around this by transforming the data into a higher-dimensional
    space. For example, consider the simple one-dimensional dataset shown in [Figure 16-7](#svm_non_separable).
  prefs: []
  type: TYPE_NORMAL
- en: '![A Non-Separable One-Dimensional Data set](assets/dsf2_1607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-7\. A nonseparable one-dimensional dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s clear that there’s no hyperplane that separates the positive examples from
    the negative ones. However, look at what happens when we map this dataset to two
    dimensions by sending the point `x` to `(x, x**2)`. Suddenly it’s possible to
    find a hyperplane that splits the data ([Figure 16-8](#svm_separable)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Becomes Separable In Higher Dimensions](assets/dsf2_1608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. Dataset becomes separable in higher dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is usually called the *kernel trick* because rather than actually mapping
    the points into the higher-dimensional space (which could be expensive if there
    are a lot of points and the mapping is complicated), we can use a “kernel” function
    to compute dot products in the higher-dimensional space and use those to find
    a hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: It’s hard (and probably not a good idea) to *use* support vector machines without
    relying on specialized optimization software written by people with the appropriate
    expertise, so we’ll have to leave our treatment here.
  prefs: []
  type: TYPE_NORMAL
- en: For Further Investigation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn has modules for both [logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
    and [support vector machines](https://scikit-learn.org/stable/modules/svm.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) is the support vector
    machine implementation that scikit-learn is using behind the scenes. Its website
    has a variety of useful documentation about support vector machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
