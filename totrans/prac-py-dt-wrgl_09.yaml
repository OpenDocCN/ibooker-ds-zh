- en: Chapter 9\. Introduction to Data Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 数据分析简介
- en: So far, this book has focused mostly on the logistics of acquiring, assessing,
    transforming, and augmenting data. We’ve explored how to write code that can retrieve
    data from the internet, extract it from unfriendly formats, evaluate its completeness,
    and account for inconsistencies. We’ve even spent some time thinking about how
    to make sure that the tools we use to do all this—our Python scripts—are optimized
    to meet our needs, both now and in the future.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书主要关注获取、评估、转换和增强数据的各种细节。我们探讨了如何编写代码来从互联网获取数据，从不友好的格式中提取数据，评估其完整性，并解决不一致性问题。我们甚至花了一些时间考虑如何确保我们用来做所有这些事情的工具——我们的Python脚本——能够优化以满足我们当前和未来的需求。
- en: 'At this point, though, it’s time to revisit the *why* of all this work. Back
    in [“What Is “Data Wrangling”?”](ch01.html#describing_data_wrangling), I described
    the purpose of data wrangling as transforming “raw” data into something that can
    be used to generate insight and meaning. But unless we follow through with at
    least *some* degree of analysis, there’s no way to know if our wrangling efforts
    were sufficient—or what insights they might produce. In that sense, stopping our
    data wrangling work at the augmentation/transformation phase would be like setting
    up your mise en place and then walking out of the kitchen. You don’t spend hours
    carefully prepping vegetables and measuring ingredients unless you want to *cook*.
    And that’s what data analysis is: taking all that beautifully cleaned and prepared
    data and turning it into new insight and knowledge.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们需要重新审视这一切工作的*目的*。在[“数据整理是什么？”](ch01.html#describing_data_wrangling)一文中，我描述了数据整理的目的是将“原始”数据转化为可以生成洞察和意义的东西。但是，除非我们进行至少*一定程度*的分析，否则我们无法知道我们的整理工作是否足够，或者它们可能产生什么样的洞察。从这个意义上说，停留在增强/转换阶段的数据整理工作就像是摆放好你的厨房材料然后离开厨房一样。你不会花几个小时仔细准备蔬菜和测量配料，除非你打算*烹饪*。而数据分析就是这样：将所有精心清理和准备好的数据转化为新的洞察和知识。
- en: If you fear we’re slipping into abstractions again, don’t worry—the fundamentals
    of data analysis are simple and concrete enough. Like our data quality assessments,
    however, they are about one part technical effort to four parts judgment. Yes,
    the basics of data analysis involve reassuring, 2 + 2 = 4–style math, but the
    insights depend on *interpreting* the outputs of those very straightforward formulas.
    And that’s where you need logic and research—along with human judgment and expertise—to
    bridge the gap.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你担心我们再次陷入抽象，不用担心——数据分析的基础知识足够简单和具体。就像我们的数据质量评估一样，它们是技术努力的一部分，加上四分之三的判断力。是的，数据分析的基础包括令人放心的、2
    + 2 = 4风格的数学，但洞察力依赖于*解释*这些非常简单公式的输出。这就是你需要逻辑和研究的地方——以及人类的判断和专业知识——来弥合这一差距。
- en: Over the course of this chapter, then, we’ll be exploring the basics of data
    analysis—specifically, the simple measures of *central tendency* and *distribution*
    that help us give data meaningful context. We’ll also go over the rules of thumb
    for making appropriate inferences about data based on these measures and the role
    that both numerical and *visual* analysis play in helping us understand the trends
    and anomalies within our dataset. Toward the end of the chapter, we’ll address
    the limits of data analysis and why it *always* takes more than traditional “data”
    to get from the “what” to the *why*. Along the way, of course, we’ll see how Python
    can help us in all these tasks and why it’s the right tool for everything from
    quick calculations to essential visualizations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的过程中，我们将探讨数据分析的基础知识——特别是关于*中心趋势*和*分布*的简单度量，这些帮助我们为数据赋予有意义的背景。我们还将介绍根据这些度量进行适当推断的经验法则，以及数值和*视觉*分析在帮助我们理解数据集内趋势和异常中的作用。在章节的末尾，我们将讨论数据分析的局限性，以及为什么从“什么”到*为什么*总是需要比传统的“数据”更多。当然，在此过程中，我们还将看到Python如何帮助我们完成所有这些任务，以及为什么它是从快速计算到必要可视化的正确工具。
- en: Context Is Everything
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景是关键
- en: 'If I offered to sell you an apple right now for $0.50, would you buy it? For
    the sake of this example, let’s imagine that you like apples, and you’re feeling
    like a snack. Also, this is a beautiful apple: shiny, fragrant, and heavy in the
    hand. Let’s also suppose you’re confident that I’m not trying to harm you with
    this apple. It’s just a nice, fresh, apple, for $0.50\. Would you buy it?'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我现在以$0.50的价格向你出售一个苹果，你会买吗？为了这个例子，假设你喜欢苹果，而且你正想吃点零食。此外，这是一个漂亮的苹果：光滑、芬芳，并且手感沉重。还假设你确信我并没有企图用这个苹果伤害你。它只是一个漂亮、新鲜的苹果，售价为$0.50。你会买吗？
- en: 'For most people, the answer is: *it depends*. Depends on what? Lots of things.
    No matter how much you trust me (and my almost-too-perfect apple), if someone
    standing next to me was selling also-very-good-looking apples for $0.25 each,
    you might buy one of those. Why? Well, obviously they’re cheaper, and even my
    *incredibly* awesome apple probably isn’t *twice* as awesome as the next one.
    At the same time, if your cousin was standing on the other side of me selling
    tasty apples for $0.60 each, you might well buy one of those instead, just to
    show support for your cousin’s new apple-selling start-up.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对大多数人来说，答案是：*这取决于情况*。取决于什么呢？很多事情。无论你有多么相信我（以及我那几乎完美得过分的苹果），如果站在我旁边的人也在卖每个$0.25的好看苹果，你可能会买他的。为什么呢？显而易见，它们更便宜，即使我的*非常*棒的苹果可能也不会比旁边的那个好*两倍*。同时，如果你的表兄站在我另一边，卖每个$0.60的美味苹果，你可能会支持他的新苹果销售初创企业，而选择买他的。
- en: This might seem like a pretty complicated decision-making process for choosing
    a piece of fruit, but in reality we make these kinds of choices all the time,
    and the results are sometimes surprising. Economists like [Dan Ariely](https://danariely.com/all-about-dan)
    and [Tim Harford](https://timharford.com/articles/undercovereconomist) have conducted
    research illustrating things like how influential a “free” gift is, even if it
    creates an added cost, or how our satisfaction with our own pay can go down when
    we learn what people around us are earning.^([1](ch09.html#idm45143400237136))
    Most of our priorities and decisions depend on value judgment, and in order to
    make those effectively we need to know what our options are. Would I buy a fairy-tale-perfect
    apple for $0.50? Maybe. Probably not if I could get a really similar one at the
    next corner for half the price. But I probably would if I was in a hurry and had
    to walk a mile to get one otherwise. Though we all understand what we mean by
    it, a more precise way of saying “It depends” would be to say, “It depends on
    the *context*.”
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来是一个非常复杂的果选购决策过程，但实际上我们经常做出这种选择，并且结果有时令人意外。像[丹·阿里尔](https://danariely.com/all-about-dan)和[蒂姆·哈福德](https://timharford.com/articles/undercovereconomist)这样的经济学家已经进行了研究，说明了“免费”礼物的影响有多大，即使它会带来额外的成本，或者我们在了解周围人的收入后，我们对自己的工资满意度会降低。[^1]
    大多数的优先事项和决策都依赖于价值判断，为了有效地做出这些判断，我们需要了解我们的选择有哪些。如果我可以以$0.50买到童话般完美的苹果，我会买吗？也许吧。但如果我能在下一个拐角找到一个价格几乎相同的，那可能就不会了。但如果我赶时间，否则还得走一英里才能买到，我可能会选择买。尽管我们都明白“这取决于情况”的意思，更精确地说“这取决于*背景*”。
- en: The importance of context is why a data point in isolation is meaningless; even
    if it is factually “true,” a single data point can’t help us make decisions. Generating
    and acquiring new knowledge, in general, is about connecting new information to
    information we already know. In other words, the knowledge isn’t “in the data”
    itself but in its *relationship* to other things. Since we can’t exhaustively
    explore the context of every situation where decisions are required (including
    your concerns about your cousin’s new apple startup and the importance of supporting
    family efforts), we often have to restrict ourselves to examining those parts
    of the context that we can (or have chosen to) consistently measure and quantify.
    In other words, we turn to data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 背景的重要性在于，孤立的数据点毫无意义；即使它事实上“正确”，单个数据点也无法帮助我们做出决策。一般来说，产生和获取新知识是将新信息与我们已知的信息联系起来的过程。换句话说，知识不在于数据本身，而在于它与其他事物的*关系*。因为我们无法详尽地探索每个决策所需的背景（包括你对表兄新苹果初创企业的关切以及支持家庭努力的重要性），我们通常不得不限制自己，仅仅审视那些我们可以（或选择）持续测量和量化的背景部分。换句话说，我们求助于数据。
- en: How do we derive context from data? We do things like investigate its provenance,
    asking questions about who collected it and when and why—these answers help illuminate
    both what the data includes and what might be missing. And we look for ways to
    systematically compare each data point to the rest, to help understand how it
    conforms to—or breaks—any patterns that might exist across the dataset as a whole.
    Of course, none of this is likely to offer us definitive “answers,” but it will
    provide us with insights and ideas that we can share with others and use to inspire
    our *next* question about what’s happening in the world around us.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从数据中得出背景？我们进行诸如调查其来源的活动，询问谁收集了数据，何时收集的，以及为什么收集的—这些答案有助于阐明数据包含了什么以及可能缺失了什么。我们还寻找方法系统地比较每个数据点与其余部分，以帮助理解它如何符合—或打破—整个数据集可能存在的任何模式。当然，这些活动不太可能给我们提供确定的“答案”，但它们会为我们提供见解和思路，我们可以与他人分享并用来激发我们对周围世界正在发生的事情的*下一个*问题。
- en: Same but Different
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同样但不同
- en: 'While building context is essential for generating insight from data, how do
    we know *which* contexts matter? Given that there are infinite relationship *types*
    we could identify, even among the data points in a pretty small dataset, how do
    we decide which ones to pay attention to? Take, for example, the data in [Example 2-9](ch02.html#page_loop),
    which was just a simple (fictional) list of page counts:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成数据洞察力时，建立背景至关重要，但我们如何知道哪些背景*至关重要*呢？考虑到我们可以识别的无限关系*类型*，即使在一个相当小的数据集中，我们如何决定要关注哪些呢？例如，在[示例 2-9](ch02.html#page_loop)中的数据，它只是一个简单（虚构的）页面计数列表：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Even with just this handful of numbers, there are lots of types of “context”
    we can imagine: we could describe it in terms of even versus odd values, for example,
    or which ones can be evenly divided by 8\. The problem is, most of these relationships
    are not all that interesting. How can we tell which ones will be?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有这么几个数字，我们可以想象到很多种“背景”类型：例如，我们可以按照奇偶值描述它们，或者可以被8整除的值。问题是，这些关系大多数并不那么有趣。我们怎么知道哪些关系才是有趣的呢？
- en: 'It turns out, the human brain is pretty well wired to notice—and care about—two
    types of relationships in particular: sameness and differentness. Trends and anomalies
    in almost any type of stimulus—from [seeing patterns in clouds or lottery results](https://archive.is/20130121151738/http://dbskeptic.com/2007/11/04/apophenia-definition-and-analysis)
    to [quickly identifying a difference in orientation among similar objects](https://csc2.ncsu.edu/faculty/healey/PP)—tend
    to catch our attention. This means *trends* and *anomalies* are interesting, almost
    by definition. So a pretty good place to start when we want to build meaningful
    context for our data is with the ways that individual records in a given dataset
    are similar to—or different from—each other.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，人类大脑对两种特定的关系特别敏感和关注：相同和不同。几乎任何类型的刺激中的趋势和异常—从[在云朵或彩票结果中看到模式](https://archive.is/20130121151738/http://dbskeptic.com/2007/11/04/apophenia-definition-and-analysis)到[快速识别类似对象之间方向的差异](https://csc2.ncsu.edu/faculty/healey/PP)—都会引起我们的注意。这意味着*趋势*和*异常*本质上是有趣的。因此，当我们想要为我们的数据建立有意义的背景时，一个非常好的起点就是研究给定数据集中的个体记录在相互之间的相似性或差异性方面。
- en: What’s Typical? Evaluating Central Tendency
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型是什么？评估中心倾向
- en: What does it mean for something to be “average”? When we use that term in day-to-day
    life, it’s often a stand-in for “unremarkable,” “expected,” or “typical.” Given
    its specifically *un*extraordinary associations, then, in many cases “average”
    can also be a synonym for “boring.”
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 某物“平均”是什么意思？在我们日常生活中使用这个术语时，它通常是“平凡”，“预期”或“典型”的代名词。鉴于其特别*不*引人注目的联想，因此在许多情况下，“平均”也可以是“无聊”的同义词。
- en: When it comes to analyzing data, however, it turns out that what’s “average”
    is actually what interests us, because it’s a basis for comparison—and comparisons
    are one thing humans care about a lot. Remember the research on wages? As humans,
    we want to know how things that affect us compare to what is “typical” for other
    people. So even if we never hope to *be* “average,” in general we still want to
    know both what it is and how our own experience compares.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当涉及到分析数据时，事实证明“平均”值实际上是我们感兴趣的，因为它是比较的基础—而比较是人类非常关心的一件事。还记得有关工资的研究吗？作为人类，我们想知道影响我们的事物与其他人的“典型”相比如何。因此，即使我们永远不希望*成为*“平均”，总体而言，我们仍然想知道它是什么，以及我们自己的经验与之相比如何。
- en: What’s That Mean?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那意味着什么？
- en: 'You may already be familiar with the process for calculating the “average”
    of a set of numbers: add them all up and divide by how many you have. That particular
    measure of *central tendency* is more precisely known as the *arithmetic mean*,
    and it’s calculated just the way you remember. So for our `page_counts` variable,
    the math would be:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And this would give us (roughly) a mean value of:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a representation of the “typical” chapter length, this seems pretty reasonable:
    many of our chapters have page counts that are pretty close to 30, and there are
    even two chapters that have *precisely* 32 pages. So a mean per-chapter page count
    of just over 32 seems about right.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'While the mean may have served us well in this instance, however, there are
    many situations where using it as a measure of “typicality” can be deeply misleading.
    For example, let’s imagine one more, *really* long chapter (like, 100 pages long)
    got added to our book. Our method of calculating the mean would be the same:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'But now the mean value would be:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: All of a sudden, our “average” chapter length has increased by almost 6 pages,
    even though we only added a single new chapter, and fully *half* of our chapters
    are 28–34 pages long. Is a chapter that’s roughly 39 pages truly “typical” in
    this case? Not really.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: What we’re seeing even in this small example is that while in *some* cases the
    mean is a reasonable measure of “typicality,” it’s also heavily influenced by
    extreme values—and just one of these is enough to make it useless as a shorthand
    for what is “typical” of a dataset. But what are our alternatives?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Embrace the Median
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another way to think about “typical” values in a dataset is to figure out what
    is—quite literally—in the “middle.” In data analysis, the “middle” value in a
    series of records is known as the *median*, and we can find it with even *less*
    math than we used when calculating the mean: all you need to do is sort and count.
    For example, in our original set of chapter lengths, we would first sort the values
    from lowest to highest:^([2](ch09.html#idm45143398320448))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Which gives us:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, all we have to do is choose the “middle” value—that is the one that is
    positioned halfway between the beginning and end of the list. Since this is a
    nine-item list, that will be the value in the fifth position (leaving four items
    on either side). Thus, the *median* value of our `page_count` dataset is 32.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see what happens to the median when we add that extra-long chapter.
    Our sorted data will look like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And what about the median? Since the list now has an *even* number of items,
    we can just take the *two* “middle” values, add them together, and divide by two.^([3](ch09.html#idm45143399041152))
    In this case, that will be the values in positions 5 and 6, which are both 32\.
    So our median value is (32 + 32) / 2 = 32\. Even when we add our extra-long chapter,
    the median value is still the same!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Now at first you might be thinking, “Hold on, this feels wrong. A whole new
    chapter was added—a really *long* chapter, but the median value didn’t change
    *at all*. Shouldn’t it move, at least a little bit?”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 起初你可能会想：“等等，这感觉不对。增加了一个全新的章节——一个真的*很长*的章节，但中位数值竟然*一点都没有*变化。它不应该至少有些变动吗？”
- en: The real difference between the mean and the median is that the mean is powerfully
    affected by the specific *values* in dataset—as in how high or low they are—while
    the median is influenced mostly by the *frequency* with which certain values appear.
    In a sense, the median is much closer to a “one value, one vote” sort of approach,
    whereas the mean lets the most extreme values speak “loudest.” Since our current
    goal is to understand what values are “typical” in our dataset, the median will
    usually be the most representative choice.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 平均数和中位数之间的真正区别在于，平均数受数据集中具体*数值*的影响非常大——比如它们的高低——而中位数主要受到特定数值出现的*频率*的影响。在某种意义上，中位数更接近于“一值一票”的方法，而平均数则让最极端的数值“说得最响亮”。由于我们当前的目标是理解数据集中的“典型”数值，中位数通常会是最具代表性的选择。
- en: 'Think Different: Identifying Outliers'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的思考方式：识别异常值
- en: In [“Same but Different”](#samesame), I noted that human beings, in general,
    are interested in “sameness” and “differentness.” In looking at our two possible
    measures of central tendency, we were exploring ways in which values in a dataset
    are similar. But what about the ways in which they are different? If we look again
    at our original `page_count` list, we’d probably feel confident that a 32-page
    chapter is reasonably “typical,” and maybe even a 30-, 28-, or 34-page chapter,
    too. But what about a 12-page chapter? Or a 56-page chapter? They certainly don’t
    seem typical, but how do we know which values are different *enough* to be truly
    “unusual?”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“同中有异”](#samesame)中，我注意到人类总体上对“相同”和“不同”的兴趣。在研究我们两种可能的中心趋势测量方法时，我们探讨了数据集中数值相似的方式。但是数值在哪些方面不同呢？如果我们再次查看我们原始的`page_count`列表，我们可能会相信，32页的章节相对“典型”，甚至30页、28页或34页的章节也是如此。但是12页的章节呢？或者56页的章节呢？它们显然不太典型，但我们怎么知道哪些数值足够*不同*，以至于真正“异常”呢？
- en: 'This is where we have to start mixing math with human judgment. Measures of
    central tendency can be pretty unequivocally calculated, but determining which
    values in a dataset are truly unusual—that is, which are *anomalies* or *outliers*—cannot
    be definitively assessed with arithmetic alone. As datasets get larger and more
    complex, however, it gets harder for humans to interpret them effectively as sets
    of data points.^([4](ch09.html#idm45143398988128)) So how can we possibly apply
    human judgment to large datasets? We need to engage our largest and most comprehensive
    data-processing resource: the human visual system.^([5](ch09.html#idm45143398983968))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们必须开始将数学与人类判断结合起来的地方。中心趋势的度量可以被相当明确地计算出来，但确定数据集中哪些值真正异常——即哪些是*异常值*或*离群值*——不能仅凭算术来确定。然而，随着数据集变得越来越大和复杂，人类有效地将其解释为数据点集合变得更加困难。^([4](ch09.html#idm45143398988128))
    那么我们如何可能对大型数据集应用人类判断？我们需要利用我们最大和最全面的数据处理资源：人类视觉系统。^([5](ch09.html#idm45143398983968))
- en: Visualization for Data Analysis
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析的可视化
- en: The role of visualization in data work is twofold. On the one hand, visualization
    can be used to help us *analyze* and make sense of data; on the other, it can
    be used to *convey* the insights that we’ve generated from that analysis. Using
    data for the latter purpose—as a communication tool to share insights about our
    data with a broader audience—is something that we’ll explore in-depth in [Chapter 10](ch10.html#chapter10).
    Here, we’re going to focus on the ways in which visualization can offer us insight
    into the data we have.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工作中可视化的角色是双重的。一方面，可视化可以用来帮助我们*分析*和理解数据；另一方面，它可以用来*传达*我们从分析中获得的见解。利用数据进行后一目的——作为一种交流工具，与更广泛的观众分享有关数据的见解——是我们将在[第10章](ch10.html#chapter10)深入探讨的内容。在这里，我们将集中讨论可视化如何为我们提供对所拥有数据的洞察。
- en: In order to understand how visualization can help us identify extreme values
    in our data, we first need to look—quite literally—at the data itself. In this
    case, I don’t mean that we’re going to open up our CSV file and start reading
    through data records. Rather, we’re going to create a special kind of bar chart
    known as a *histogram*, in which each bar represents the number of times a particular
    value appears in our dataset. For example, we can see a very simple histogram
    of our (expanded) `page_count` data in [Figure 9-1](#page_count_histogram).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![A sample histogram](assets/ppdw_0901.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A basic histogram
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For a dataset as small as our `page_counts` example, a histogram can’t tell
    us much; in fact, a dataset with only 10 values arguably has too few data points
    for either the concepts of *central tendency* (like the *mean* and the *median*)
    or outliers to have much meaning. Even so, in [Figure 9-1](#page_count_histogram)
    you can see the *beginnings* of what looks like a pattern: the two chapters of
    equal length form a double-height spike, with the unique page-lengths of most
    of the remaining chapters becoming single-height bars clustered reasonably close
    it. *Way* out on the right end, meanwhile, is our 100-page chapter, with no other
    values anywhere near it. While we might have already been tempted to conclude
    that the 100-page chapter was an anomaly—or *outlier*—based on the math, this
    histogram certainly reinforces that interpretation.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in order to really appreciate the power of visualization for data
    analysis, we’ll want to look at a dataset with values we couldn’t even hope to
    “eyeball” the way that we did with our list of page counts. Fortunately, the Paycheck
    Protection Program (PPP) data that we worked through in [Chapter 6](ch06.html#chapter6)
    is *definitely* not lacking in this regard, since it contains hundreds of *thousands*
    of loan records. To see what we can learn about what is and isn’t “typical” in
    the loan amounts approved through the PPP, we’ll write a quick script to generate
    a histogram of the currently approved loan values in the PPP. Then, we’ll label
    that histogram with both the mean and median values in order to see how well each
    might serve as a potential measure of central tendency. After that, we’ll return
    to the question of identifying possible outliers in the approved loan amounts,
    using both our visualization of the data and some math to back it up.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: For this to work, we’ll once again be leveraging some powerful Python libraries—specifically,
    *matplotlib* and *seaborn*—both of which have functions for calculating and visualizing
    data. While *matplotlib* remains the foundational library for creating charts
    and graphs in Python, we’re also using *seaborn* for its helpful support of more
    advanced calculations and formats. Because the two are highly compatible (*seaborn*
    is actually built on *top* of *matplotlib*), this combination will offer us the
    flexibility we need both to quickly create the basic visualizations we need here
    and also customize them for presenting data effectively in [Chapter 10](ch10.html#chapter10).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为使其工作，我们将再次利用一些强大的 Python 库——具体来说是 *matplotlib* 和 *seaborn*——它们都具有用于计算和可视化数据的功能。虽然
    *matplotlib* 仍然是在 Python 中创建图表和图形的基础库，但我们还使用 *seaborn* 来支持更高级的计算和格式。由于这两者高度兼容（*seaborn*
    实际上是构建在 *matplotlib* 之上的），这种组合将为我们提供所需的灵活性，既可以快速创建这里需要的基本可视化，又可以定制它们以有效呈现数据在[第十章](ch10.html#chapter10)中的内容。
- en: For now, though, let’s focus on the analytical size of our visualization process.
    We’ll start this by creating a basic histogram of our PPP loan data using the
    `CurrentApprovalAmount` values. We’ll also add mean and median lines for more
    context, as shown in [Example 9-1](#ppp_loan_central_measures).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于我们的可视化过程的分析尺度。我们将从使用 `CurrentApprovalAmount` 值创建 PPP 贷款数据的基本直方图开始。我们还将添加均值和中位数线以提供更多背景信息，如[示例 9-1](#ppp_loan_central_measures)所示。
- en: Example 9-1\. ppp_loan_central_measures.py
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-1\. ppp_loan_central_measures.py
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO1-1)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO1-1)'
- en: The `CurrentApprovalAmount` column in our PPP data tells us the dollar amount
    of each loan that is currently approved (whether or not it has been disbursed).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 PPP 数据中，`CurrentApprovalAmount` 列告诉我们每笔贷款当前批准的金额（无论是否已发放）。
- en: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO1-2)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO1-2)'
- en: The `get_ylim()` method returns the lowest and highest y-axis value as a list.
    We’ll mostly be using this to set a legible length for our mean and median lines.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_ylim()` 方法以列表形式返回 y 轴的最低和最高值。我们主要将使用它来设置均值和中位数线的可读长度。'
- en: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO1-3)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO1-3)'
- en: We can add vertical lines anywhere on our histogram (or other visualization)
    by specifying the “x” position, “y” starting point, “y” ending point, color, and
    line style. Note that the units for “x” and “y” values are relative to the dataset,
    *not* the visual size of the chart.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过指定“x”位置、“y”起点、“y”终点、颜色和线型，在直方图（或其他可视化图表）上添加垂直线。请注意，“x”和“y”值的单位是相对于数据集的，*而不是*图表的视觉大小。
- en: '[![4](assets/4.png)](#co_introduction_to_data_analysis_CO1-4)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_introduction_to_data_analysis_CO1-4)'
- en: While calling the *matplotlib* `show()` method is not explicitly required in
    Jupyter notebooks, like `print()` statements, I prefer to include them for clarity
    and consistency. By default, the chart will render in “interactive” mode (in Jupyter,
    you’ll need to include the `%matplotlib notebook` “magic” command, as I have in
    the provided files), which allows us to zoom, pan, and otherwise explore our histogram
    in detail without writing more code.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 笔记本中，并非必须显式调用 *matplotlib* 的 `show()` 方法，就像 `print()` 语句一样，我更倾向于包含它们以确保清晰和一致性。默认情况下，图表将以“交互”模式呈现（在
    Jupyter 中，您需要包含 `%matplotlib notebook` “魔术”命令，正如我在提供的文件中所做的那样），这使我们能够放大、平移和详细探索直方图，而无需编写更多代码。
- en: Most likely, you’re looking at the chart that displayed as a result of running
    this script (which hopefully looks something like [Figure 9-2](#ppp_loan_histogram))
    and thinking, “Now what?” Admittedly, this initial visualization seems a bit lackluster—if
    not downright confusing. Fear not! If anything, take this as your first case study
    in why visualization for analysis and visualization for communication are *not*
    one and the same. Analytical visualizations like this one, for example, require
    far more effort to read, understand, and refine than *any* visualization we would
    want to use for general communications. For generating insight about our data,
    however, this is actually just the right place to start.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，您正在查看由运行此脚本生成的图表（希望看起来像[图 9-2](#ppp_loan_histogram)那样），并在想：“现在怎么办？”诚然，这种初始可视化看起来有些乏味，甚至有点令人困惑。不过不用担心！如果有什么的话，这可以作为你首次学习分析可视化与沟通可视化*不同*之处的案例研究。例如像这样的分析性可视化，需要更多的阅读、理解和改进工作，而不像我们希望用于一般沟通的*任何*可视化那样。然而，对于生成关于我们数据洞察的见解，这实际上是一个很好的起点。
- en: 'Before we forge ahead with our data analysis, though, let’s take a moment to
    appreciate the distinctly old-school—but incredibly useful—interface that Python
    has given us for this chart. Rather than just a static image, our Python script
    has *also* given us an entire toolbar (shown in [Figure 9-2](#ppp_loan_histogram))
    that we can use to interact with it: to zoom, pan, modify, and even save the output.
    While of course we can (and eventually will) customize the contents and aesthetics
    of this chart using code, the fact that we can effectively explore our data without
    having to constantly modify and rerun our code is a huge time-saver. To get a
    feel for what’s possible, take a few minutes to play with the controls yourself.
    When you’re ready to move on with our data analysis, just click the “home” icon
    to return the chart to its initial view and follow along in the sections that
    follow.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，在我们继续进行数据分析之前，让我们花点时间来欣赏Python为这张图表提供的非常老派但无比有用的界面。与其说仅仅是一个静态图像，我们的Python脚本还*同时*提供了一个完整的工具栏（显示在[图 9-2](#ppp_loan_histogram)中），我们可以用来与之交互：缩放、平移、修改甚至保存输出。当然，我们可以（并且最终会）使用代码自定义这张图表的内容和美观性，但事实上，我们可以有效地探索我们的数据，而无需不断修改和重新运行我们的代码，这大大节省了时间。为了了解可能的操作，请花几分钟自己玩弄控件。当您准备好继续进行数据分析时，只需点击“主页”图标将图表恢复到初始视图，并在接下来的章节中跟随进行。
- en: '![PPP loan histogram](assets/ppdw_0902.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![PPP 贷款直方图](assets/ppdw_0902.png)'
- en: Figure 9-2\. PPP loan histogram
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. PPP 贷款直方图
- en: What’s Our Data’s Shape? Understanding Histograms
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的数据的形状是什么？理解直方图
- en: 'When we were working with our data in a table-type format, we tended to think
    of its “shape” in terms of the number of rows and columns it had (this is actually
    *exactly* what the `pandas.shape` property of a DataFrame returns). In this context,
    the “shape” we’re interested in is the literal shape of the histogram, which will
    help us identify potentially interesting or important patterns or anomalies. Some
    of the first things we’ll look for in these instances are:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在表格类型的数据中工作时，我们倾向于从行数和列数的角度考虑其“形状”（这实际上正是DataFrame的`pandas.shape`属性返回的内容）。在这种情况下，我们感兴趣的“形状”是直方图的实际形状，它将帮助我们识别可能有趣或重要的模式或异常。在这些情况下，我们首先要查找的一些内容包括：
- en: Symmetry
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对称性
- en: Is our data vertically symmetrical? That is, could we draw a vertical line somewhere
    over our visualized data such that the pattern of bars on one side looks (roughly)
    like a reflection of those on the other?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据在垂直方向上对称吗？也就是说，我们能否在可视化数据的某个地方画一条垂直线，使得一侧的柱形图看起来（大致上）像另一侧的镜像？
- en: Density
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 密度
- en: Where are most of our data values clustered (if anywhere)? Are there multiple
    clusters or just one?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据值大多集中在哪里（如果有的话）？是否有多个集群还是只有一个？
- en: Unsurprisingly, these questions are about more than aesthetics. The shape of
    a dataset’s histogram illustrates what is typically described as its *distribution*.
    Because certain distributions have specific properties, we can use our data’s
    *distribution* to help us identify what is typical, what is unusual, and what
    deserves further scrutiny.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，这些问题不仅仅涉及美学。数据集直方图的形状反映了通常所描述的*分布*。由于某些分布具有特定的属性，我们可以利用数据的*分布*帮助我们识别典型、不寻常以及值得进一步审查的内容。
- en: The Significance of Symmetry
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对称性的意义
- en: In the natural world, symmetry is a common occurrence. Plants and animals tend
    to be symmetrical in many ways; for example, a dog’s face and an oak leaf both
    exhibit what’s known as *bilateral symmetry*—what we might describe as one side
    being a “mirror image” of the other. Across populations of living things, however,
    there is also often symmetry in the distribution of certain physical characteristics,
    like height or wing length. Our histogram lets us observe this symmetry firsthand,
    by illustrating the *frequency* of specific heights or wing lengths within a population.
    A classic example of this is shown in [Figure 9-3](#housefly_wings), which shows
    the length of housefly wings as measured by a team of biologists in the mid-20th
    century.^([6](ch09.html#idm45143397925760))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然界中，对称是常见的现象。植物和动物在许多方面都倾向于对称；例如，狗的脸和橡树叶都展示了所谓的*对称性*——我们可以描述为一侧是另一侧的“镜像”。然而，在生物群体中，某些物理特征的分布也经常表现出对称性，例如身高或翅膀长度。我们的直方图让我们可以直观地观察这种对称性，通过展示在人口中特定身高或翅膀长度的*频率*。一个经典的例子显示在[图 9-3](#housefly_wings)，显示了20世纪中叶由一组生物学家测量的家蝇翅膀长度。^([6](ch09.html#idm45143397925760))
- en: 'The symmetrical bell curve shown in [Figure 9-3](#housefly_wings) is also sometimes
    described as the “normal,” “standard,” or “Gaussian” distribution. If you’ve ever
    had an academic grade “curved,” this was the distribution that the grades in your
    cohort were being transformed to fit: one with very few scores at the top or bottom
    and most of them lumped in the middle.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对称的钟形曲线，如[图 9-3](#housefly_wings)，有时被描述为“正常”，“标准”或“高斯”分布。如果你曾经遇到过学术成绩“曲线调整”，那么你的同学们的成绩就是按照这种分布调整的：即在顶部或底部几乎没有分数，大部分集中在中间。
- en: 'The power of the Gaussian distribution is not just in its pretty shape, however;
    it’s in what that shape means we can *do*. Datasets that demonstrate Gaussian
    distributions can be both described and compared to one another in ways that nonsymmetrical
    distributions cannot, because we can meaningfully calculate two measures in particular:
    the *standard deviation*, which quantifies the numerical range of data values
    within which most of them can be found, and each value’s *z-score*, which describes
    its distance from the mean in terms of standard deviations. Because of the fundamental
    symmetry of Gaussian distributions, we can use the *standard deviation* and the
    *z-score* to compare two sets of functionally similar data *even if they use different
    scales*. For example, if student grades demonstrate a Gaussian distribution, we
    can calculate and compare individual students’ z-scores (that is, their performance
    relative to their cohort) even across different classes and instructors who may
    use different grading rubrics. In other words, even if the mean of student grades
    for one instructor is in the 90s and for another instructor in the 70s, if both
    sets of students’ grades are truly Gaussian in their distribution, we can still
    determine which students are doing the best or need the most help across cohorts—something
    the *nominal* grades (e.g., 74 or 92) could never tell us.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，高斯分布的力量不仅在于其漂亮的形状；更在于这种形状意味着我们可以*做*什么。展示高斯分布的数据集可以用一种非对称分布无法做到的方式进行描述和比较，因为我们可以有意义地计算两个特定的测量值：*标准偏差*，它量化数据值的数值范围，其中大多数值可以找到；以及每个值的*z-分数*，它描述了其在标准偏差方面与均值的距离。由于高斯分布的基本对称性，即使使用不同的比例尺度，我们也可以使用*标准偏差*和*z-分数*来比较两组功能上相似的数据。例如，如果学生成绩呈现高斯分布，我们可以计算和比较各个学生的z-分数（即他们相对于他们同学的表现）。这样，即使一个教师的学生成绩平均分在90分以上，另一个教师的在70分左右，如果两组学生的成绩都真正服从高斯分布，我们仍然可以确定哪些学生在各个同学群体中表现最好或最需要帮助——这是“名义”成绩（例如74或92）永远无法告诉我们的事情。
- en: '![Length of Housefly Wings](assets/ppdw_0903.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![家蝇翅膀长度](assets/ppdw_0903.png)'
- en: Figure 9-3\. Length of housefly wings
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-3\. 家蝇翅膀长度
- en: These characteristics also inform how we can think about measuring central tendency
    and outliers. For example, in a “perfect” Gaussian distribution, the mean and
    the median will have the same value. What’s more, a value’s z-score gives us a
    quick way of identifying how typical or unusual that particular value is, because
    the percentage of data values that we expect to have a given z-score is well-defined.
    Confused yet? Don’t worry. Just like any other complex data relationship, this
    all makes much more sense if we visualize it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征还告诉我们如何考虑测量集中趋势和异常值。例如，在“完美”的高斯分布中，均值和中位数将具有相同的值。此外，值的 z 分数为我们提供了一个快速的方式来识别特定值是典型还是不寻常，因为我们预期具有给定
    z 分数的数据值的百分比是明确定义的。混乱了吗？别担心。就像任何其他复杂的数据关系一样，如果我们将其可视化，这一切都会更加清晰。
- en: '![The Gaussian distribution, showing what percentage of values exist within
    1, 2 and 3 standard deviations (σ) from the mean.](assets/ppdw_0904.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![显示正态分布，展示了在均值附近 1、2 和 3 个标准差（σ）内的值的百分比。](assets/ppdw_0904.png)'
- en: Figure 9-4\. The Gaussian distribution, showing what percentage of values exist
    within 1, 2, and 3 standard deviations (σ) from the mean
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 显示正态分布，展示了在均值附近 1、2 和 3 个标准差（σ）内的值的百分比
- en: As you can see in [Figure 9-4](#gaussian_distro),^([7](ch09.html#idm45143397906944))
    if our data’s distribution is Gaussian, more than two-thirds of the data values
    (34.1% + 34.1% = 68.2%) can be found within one standard deviation (often designated
    as it is here, by the Greek letter σ) of the mean. Another 27.2% can be found
    between one and two standard deviations from the mean, and a final 4.2% can be
    found between two and three standard deviations from the mean. This means that
    for a Gaussian distribution, *99.7% of all values can be found within 3 standard
    deviations of the mean*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[图 9-4](#gaussian_distro)中所看到的那样，^([7](ch09.html#idm45143397906944))，如果我们数据的分布是高斯的，超过数据值的三分之二（34.1%
    + 34.1% = 68.2%）可以在均值的一个标准差（通常如此命名，以希腊字母 σ 表示）内找到。另外27.2%可以在均值的一个到两个标准差之间找到，最后的4.2%可以在均值的两到三个标准差之间找到。这意味着对于高斯分布，*99.7%的所有值都可以在均值的
    3 个标准差内找到*。
- en: So what? Well, remember that one of our fundamental objectives in data analysis
    is to understand what values are typical for our dataset and which ones are truly
    extreme. While the mean and the median offer a quick shorthand for a dataset’s
    “typical” value, measures like the standard deviation—and the z-scores we can
    calculate from it—help us systematically evaluate which values might or might
    not be truly unusual.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，有什么影响呢？嗯，要记住，在数据分析中，我们的一个基本目标是理解数据集的典型值和真正极端的值。虽然均值和中位数为数据集的“典型”值提供了一个快速的简写，但是像标准差以及我们可以从中计算的
    z 分数帮助我们系统地评估哪些值可能是真正不寻常的。
- en: Unsurprisingly, calculating these values using Python is quite straightforward.
    Using either *pandas* or the *statistics* library, we can quickly find the value
    of the standard deviation for our dataset (σ) and then use it to place lines over
    our histogram where the relevant z-score values are. For this example, we’ll build
    on the data used to generate [Figure 9-3](#housefly_wings), as shown in [Example 9-2](#wing_length_with_sd).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，使用Python计算这些值非常简单。使用*pandas*或*statistics*库，我们可以快速找到数据集（σ）的标准差的值，然后使用它在我们的直方图上放置线条，显示出相关的
    z 分数值。例如，我们将继续使用用于生成[图 9-3](#housefly_wings)的数据的例子，如[示例 9-2](#wing_length_with_sd)所示。
- en: Example 9-2\. wing_length_with_sd.py
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-2\. wing_length_with_sd.py
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO2-1)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO2-1)'
- en: Each “bin” is a range of *actual* data values that will be lumped together into
    a single histogram bar; the `kde` parameter is what adds a smoothed line to our
    visualization. This line approximates the pattern we would expect if our dataset
    had infinite data points.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个“bin”是一系列*实际*数据值，这些值将被合并到单个直方图条中；`kde`参数是为我们的可视化添加平滑线的。该线条近似于我们期望的模式，如果我们的数据集有无限数据点的话。
- en: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO2-2)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO2-2)'
- en: 'We could also have used the *pandas* `std()` method: `wing_data[''wing_length
    (0.1mm)''].std()`.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用*pandas*的`std()`方法：`wing_data['wing_length (0.1mm)'].std()`。
- en: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO2-3)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO2-3)'
- en: Recall that our loop will stop *before* the second value provided to `range()`,
    so to get three positive lines, we set the second value to `4`. By starting with
    a negative number, we actually *subtract* from the mean at first—which we want
    to do because we want to capture values both above *and* below the mean.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们的循环将在`range()`提供的第二个值*之前*停止，因此要获取三行正数，我们将第二个值设为`4`。通过从一个负数开始，我们实际上*从*平均值开始减去——这是我们想要的，因为我们希望捕获高于*和*低于平均值的值。
- en: As you review the output of [Example 9-2](#wing_length_with_sd) you might be
    thinking, “Great, we’ve drawn some lines on data about bugs. How will this help
    me interpret *real* data?” After all, the prototypically Gaussian distribution
    of this housefly wing-length data doesn’t look much like the output we got when
    we charted our PPP loan data, which was distinctly *a*symmetrical, as most of
    your data is likely to be.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当您审查[示例 9-2](https://example.org/wing_length_with_sd)的输出时，您可能会想到：“太好了，我们在关于虫子的数据上画了一些线。这将如何帮助我解释*真实*数据？”毕竟，这些家蝇翅长数据的原型高斯分布看起来与我们绘制PPP贷款数据时得到的输出不太相似，后者明显是*不*对称的，而您的大多数数据可能也是如此。
- en: 'So what do we do when our data distribution lacks symmetry? We already know
    how to find the “middle” of an asymmetric distribution like the one in [Example 9-1](#ppp_loan_central_measures):
    by calculating the median, rather than the mean. But what about identifying extreme
    values? Since asymmetric or *skewed* distributions aren’t, well, symmetric, there
    is no single “standard” deviation, nor can we use it to calculate z-scores. We
    can, however, still usefully subdivide an asymmetric dataset in a way that will
    let us generate insight about possibly unusual or extreme values.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们的数据分布缺乏对称性时该怎么办呢？我们已经知道如何找到不对称分布的“中间值”，就像在[示例 9-1](https://example.org/ppp_loan_central_measures)中那样：通过计算中位数，而不是平均数。但是如何识别极端值呢？由于不对称或*偏斜*的分布不是对称的，因此不存在单一的“标准”偏差，我们也不能用它来计算z分数。然而，我们仍然可以有用地将不对称的数据集分成一种方式，这样可以让我们洞察可能异常或极端值。
- en: Like finding the median, this subdivision process is really quite simple. First,
    we find the middle value of our sorted dataset—in other words, the median. Now
    we look at each half of the data records as if it were a standalone dataset and
    find *their* median values. The median of the lower half is traditionally labeled
    Q1, while the median of the upper half is traditionally labeled Q3\. At this point,
    we’ve split our dataset into four parts, or *quartiles*, each of which contains
    an equal number of data values.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 像寻找中位数一样，这种分割过程其实非常简单。首先，我们找到已排序数据集的中间值——也就是中位数。现在，我们将每一半数据记录视为一个独立的数据集，并找到*它们*的中位数值。传统上，下半部分的中位数被标记为Q1，而上半部分的中位数被标记为Q3。此时，我们已将数据集分成了四部分，或*四分位数*，每部分包含相同数量的数据值。
- en: What does this do for us? Well, remember that a big part of what z-scores tell
    us is the *percentage of data points that have similar values*. Looking at [Figure 9-4](#gaussian_distro),
    for example, we can see that a data point with a z-score of 0.75 is (as we would
    expect) less than one standard deviation from the mean—something we know will
    be true for roughly 68.2% of all the data values in the set as a whole. By dividing
    our data into quartiles, we have started along a similar path. For example, any
    value in our dataset that is *numerically* less than the value of Q1 is, by definition,
    smaller than at least 75% of all the data values we have.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们有什么帮助？嗯，记住z分数告诉我们的一大部分是*具有相似值的数据点的百分比*。例如，查看[图 9-4](https://example.org/gaussian_distro)，我们可以看到z分数为0.75的数据点（正如我们预期的那样）距离均值少于一个标准偏差——我们知道整体数据集中大约68.2%的所有数据值都会如此。通过将数据分成四分位数，我们已经开始了类似的路径。例如，在我们的数据集中，任何数值*数值上*小于Q1的值，根据定义，小于我们拥有的所有数据值的至少75%。
- en: Still, what we’re *really* looking for are ways to identify potentially unusual
    values. Being smaller—or larger—than 75% of all data values is something, but
    it’s hardly *extreme*. Identifying our quartile boundaries alone won’t quite be
    enough.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们*真正*寻找的是识别潜在异常值的方法。比起所有数据值的75%来说，更小或更大都算数，但这还远远不是*极端*的。仅仅识别我们的四分位数边界还不够。
- en: Fortunately, we can use our Q1 and Q3 values to calculate what’s known as the
    *lower bound* and *upper bound* of our dataset. If our data’s distribution was
    secretly Gaussian, these boundaries would line up almost perfectly with the values
    found at three standard deviations below and above the mean. While of course we’re
    using them precisely because our data *isn’t* Gaussian, I make the comparison
    to illustrate that we can use them to help identify extreme values in an asymmetrically
    distributed dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用我们的Q1和Q3值来计算数据集的*下限*和*上限*。如果我们的数据分布实际上是正态的，这些边界将几乎完美地与均值加三个标准差以下和以上的值对齐。当然，我们使用它们正是因为我们的数据*不是*正态分布，我做这个比较是为了说明我们可以用它们来帮助识别非对称分布数据集中的极端值。
- en: Like finding the median, calculating the upper and lower bounds is actually
    quite straightforward. We start by finding a value called the *interquartile range*
    (IQR)—a fancy-sounding name for the numerical difference between the values of
    Q3 and Q1\. We then multiply that value by 1.5 and subtract it from Q1 to get
    the lower bound, and add it to Q3 to get the upper bound. That’s it!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就像找到中位数一样，计算上限和下限实际上是非常简单的。我们首先找到一个称为*四分位数间距*（IQR）的值——这是Q3和Q1值之间的数值差异的花哨名称。然后我们将该值乘以1.5，并从Q1中减去它以获得下限，将其加到Q3以获得上限。就是这样！
- en: IQR (interquartile range) = Q3 – Q1
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: IQR（四分位数间距）= Q3 – Q1
- en: Lower bound = Q1 – (1.5 × IQR)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下限 = Q1 – （1.5 × IQR）
- en: Upper bound = Q3 + (1.5 × IQR)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上限 = Q3 + （1.5 × IQR）
- en: On a Gaussian distribution, our upper and lower bound values will be about three
    standard deviations above or below the mean—but does this mean that every value
    beyond our upper and lower bounds is automatically an *outlier*? No. But finding
    these boundaries does let us narrow down where we might start *looking* for outliers.
    And just as importantly, these measures help us understand what values are *not*
    outliers, even if they might seem, numerically, to be pretty different from the
    “typical” or “expected” value provided by the median or mean.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在正态分布中，我们的上限和下限值大约是平均值的三倍标准差之上或之下，但这是否意味着超出我们上限和下限的每个值都自动成为*异常值*？不是。但是找到这些边界确实帮助我们缩小可能需要*寻找*异常值的范围。同样重要的是，这些测量帮助我们了解哪些值*不是*异常值，即使它们在数值上看起来与中位数或平均数提供的“典型”或“预期”值有很大差异。
- en: As an example, let’s return to our PPP loan data. A $1 million loan seems like
    a lot, even if—as we are—you’re only looking at loans that were over $150,000
    to begin with. But is a $1 million loan truly *unusual*? This is where our measures
    of central tendency and spread—in this case, the median, quartiles, and lower
    and upper bound values—can really help us out. Let’s take a look at what our histogram
    looks like with these values added, as shown in [Example 9-3](#ppp_loan_central_and_dist),
    and see what we think.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们回到PPP贷款数据。100万美元的贷款似乎很多，即使——像我们现在这样——你只看的是起始额超过15万美元的贷款。但是100万美元的贷款真的*不寻常*吗？这就是我们的中心趋势和离散度测量（在这种情况下是中位数、四分位数以及下限和上限值）真正能帮助我们的地方。让我们看看我们添加了这些值后直方图的样子，如[示例 9-3](#ppp_loan_central_and_dist)所示，然后看看我们的想法。
- en: Example 9-3\. ppp_loan_central_and_dist.py
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-3\. ppp_loan_central_and_dist.py
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see from the zoom-in view of the resulting graph (shown in [Figure 9-5](#PPP_loan_distribution)),
    there’s really no support for the claim that a loan of $1 million is out of the
    ordinary; that amount falls well below the upper bound we’ve calculated for this
    dataset. So even though a loan of that amount is larger than three-quarters of
    all loans approved so far (because the $1 million mark, currently labeled as 1.0
    1e6 on the graph’s x-axis, is to the *right* of our Q3 line), it’s still not *so*
    much that any loan of $1 million is likely to be worth investigating further.
    At least, that’s probably not where we’d want to start.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从所得图表的放大视图中可以看出（见[图 9-5](#PPP_loan_distribution)），实际上并没有支持声称100万美元贷款是不寻常的证据；这笔金额远低于我们为该数据集计算的上界。因此，即使这笔金额超过了迄今批准的所有贷款的四分之三（因为当前标记为图表
    x 轴上的 1.0 1e6 的 100 万美元标记位于我们的 Q3 线右侧），但这仍然不足以使任何100万美元的贷款很可能值得进一步调查。至少，这可能不是我们想要开始的地方。
- en: '![PPP Current Loan Amount histogram with median, quartiles, and bounds in black,
    and mean plotted in gray.](assets/ppdw_0905.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![PPP 当前贷款金额直方图，其中中位数、四分位数和边界用黑色标出，均值用灰色标出。](assets/ppdw_0905.png)'
- en: Figure 9-5\. Detail of PPP Current Loan Amount histogram with median, quartiles,
    and bounds in black and mean plotted in gray
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-5. PPP当前贷款金额直方图的详细信息，中位数、四分位数和边界以黑色标出，平均值以灰色标出
- en: So where should we look next for potentially interesting patterns in the data?
    *Right in front of us*, at the graph we already have. Because while we *could*
    start looking for more complex statistical measures to calculate and evaluate,
    even this basic visualization is showing some intriguing patterns in the data.
    The first one worth noting—if only to reassure ourselves about our choice of statistical
    measures—is that the mean of this dataset is at nearly the same position in the
    distribution as our Q3 value. If we had any concern about selecting the median
    over the mean as a measure of central tendency for this dataset, that fact should
    set it to rest. The other thing that we can see—in the data view shown in [Figure 9-5](#PPP_loan_distribution)
    and if we were to scroll farther to the right—is that there are curious little
    spikes in our data, indicating that a particular loan amount was approved with
    relatively high frequency.^([8](ch09.html#idm45143397217488)) Given how clearly
    these stand out from the data patterns immediately around them, we should probably
    look at those values next.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们应该在数据中的哪里寻找可能有趣的模式呢？*就在我们面前*，在我们已经有的图表上。因为虽然我们*可以*开始寻找更复杂的统计指标来计算和评估，但即使这基本的可视化也显示出数据中一些引人注目的模式。首先值得注意的是——即使只是为了让我们对我们选择的统计指标感到放心——这个数据集的平均值几乎与我们的第三四分位数处于相同的分布位置。如果我们对在这个数据集中选择中位数而不是平均值作为中心倾向度量有任何疑虑，这个事实应该让我们放心。我们还可以看到的另一件事情——在[图9-5](#PPP_loan_distribution)中展示的数据视图中，如果我们向右滚动更远，我们会看到数据中有些奇怪的小尖峰，表明某个特定的贷款金额被相对频繁地批准了。鉴于它们在周围数据模式中的显著突出，我们可能接下来应该看一看这些数值。
- en: Counting “Clusters”
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算“集群”
- en: Imagine you’re walking down a crowded street and you notice a group of people
    gathered on the corner across from you. What do you do? On a busy avenue where
    most pedestrians are concerned with getting from one place to another, more than
    one or two people stopped in the same place at the same time is enough to signal
    that *something* is going on. Whether “something” turns out to be a busker playing
    music, a vendor of especially popular snacks, or a box full of kittens,^([9](ch09.html#idm45143397194032))
    the fact remains that our visual system is drawn to anomalies, and that’s precisely
    because deviating from a trend indicates that something at least a little bit
    out of the ordinary is going on.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在走在拥挤的街道上，你注意到对面角落聚集了一群人。你会怎么做？在一个大多数行人只关心从一个地方到另一个地方的繁忙大街上，即使有一两个人在同一时间停在同一个地方，也足以表明*有事情发生*。无论“事情”最终是否是一个弹奏音乐的艺人，一个销售特别受欢迎小吃的摊贩，还是一个盒子里装满小猫咪的盒子，我们的视觉系统都被异常所吸引，这正是因为偏离趋势表明至少有些不寻常的事情正在发生。
- en: This is also why visualizing data is such a valuable tool for analyzing it—our
    eyes and brain are wired to both quickly perceive patterns and to just as quickly
    notice deviations from them. Sometimes the reason for a pattern is easy to guess,
    sometimes less so. But in any predictable pattern—whether it’s the flow of people
    on a street, a bell-shaped data distribution, or one that forms a smoothly sloping
    curve—something that breaks that pattern is worth investigating.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么数据可视化是分析数据的如此有价值的工具——我们的眼睛和大脑都被设计成能够快速感知模式，并能迅速注意到与这些模式的偏离。有时候模式形成的原因很容易猜到，有时候则不那么简单。但在任何可预测的模式中——无论是街上人群的流动，钟形的数据分布，还是平滑倾斜的曲线——任何打破这种模式的现象都值得调查。
- en: In the case of [Figure 9-5](#PPP_loan_distribution), we can see a range of such
    pattern violations. The first is the sharp line at the lefthand side of the graph,
    which serves as a good reminder that our dataset contains *only* approved loan
    amounts of $150,000 or more, rather than all of the loans that have been approved
    within the program. In case we had lost sight of that, the obvious and hard cutoff
    our data shows at the lefthand edge is a good reminder.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图9-5](#PPP_loan_distribution)的情况下，我们可以看到一系列这种模式违背的范围。首先是图表左侧的清晰线条，这是一个很好的提醒，即我们的数据集仅包含批准的高于$150,000的贷款金额，而不是所有已批准的贷款。如果我们忽视了这一点，数据在左侧边缘的明显和硬性截止是一个很好的提醒。
- en: 'Bu there is also another set of pattern violations: little spikes in our histogram
    around particular points along the x-axis, at data values like $2 million. Where
    are these coming from? While we can’t say for sure, scanning our histogram reveals
    that similar spikes appear at roughly $500,000 intervals, especially as the loan
    amounts increase. To some degree, these are probably the result of a tendency
    toward “round” numbers: if you’re going to ask for $1,978,562.34, why not just
    “round it up” to $2 million? Of course, that would still be $21,437.66 more than
    maybe you need—and to most of us it is a *lot* of money. Given that PPP loans
    are intended to support specific costs, it does seem a little strange that *so*
    many loans—nearly 2,000 of them, based on our graph—would happen to work out to
    precisely $2 million.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 但也存在另一组模式违规：在我们的柱状图周围出现小尖峰，位于x轴上特定点附近，如$2百万的数据值。这些是从哪里来的呢？虽然我们不能确定，但扫描我们的柱状图显示，类似的尖峰出现在大约$500,000的间隔处，特别是当贷款金额增加时。在某种程度上，这些可能是“圆”数字的结果：如果您要求$1,978,562.34，为什么不将其“四舍五入”为$2百万呢？当然，这仍然比您可能需要的多$21,437.66——对大多数人来说这是*很多*钱。鉴于PPP贷款旨在支持特定成本，确实有点奇怪，*那么多*贷款——根据我们的图表，近2000笔——竟然恰好达到了$2百万。
- en: So what’s going on? This is where we need to do some additional research to
    effectively interpret what we’re seeing in the data. Based on my experience, my
    first step would be to look through the rules for PPP loans to see if I can work
    out why $2 million might be such a popular amount to request. For example, is
    $2 million a minimum or maximum allowed amount based on specific attributes of
    the business or what it’s requesting support for?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那么到底发生了什么呢？这就是我们需要进行一些额外研究以有效解释我们在数据中看到的内容的地方。根据我的经验，我的第一步将是查阅PPP贷款的规则，看看我是否能弄清楚为什么$2百万可能是如此受欢迎的请求金额。例如，$2百万是基于业务的特定属性或所请求支持的最低还是最高允许金额吗？
- en: 'A little bit of searching around the Small Business Administration’s (SBA)
    [website](https://sba.gov/funding-programs/loans/covid-19-relief-options/paycheck-protection-program/second-draw-ppp-loan)
    seems to offer at least part of the answer:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一点搜索，小企业管理局（SBA）的[网站](https://sba.gov/funding-programs/loans/covid-19-relief-options/paycheck-protection-program/second-draw-ppp-loan)似乎提供了部分答案：
- en: For most borrowers, the maximum loan amount of a Second Draw PPP loan is 2.5x
    the average monthly 2019 or 2020 payroll costs up to $2 million. For borrowers
    in the Accommodation and Food Services sector (use NAICS 72 to confirm), the maximum
    loan amount for a Second Draw PPP loan is 3.5x the average monthly 2019 or 2020
    payroll costs up to $2 million.
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于大多数借款人，第二次提款PPP贷款的最高贷款金额是2019年或2020年平均月工资成本的2.5倍，最高为$2百万。对于住宿和餐饮服务部门的借款人（使用NAICS
    72确认），第二次提款PPP贷款的最高贷款金额为2019年或2020年平均月工资成本的3.5倍，最高为$2百万。
- en: Since $2 million is a ceiling for essentially *all* types of businesses applying
    for so-called second-draw (or second-round) PPP loans—including those that might
    have initially qualified for *more* money—it makes sense that the cluster of loans
    approved for *precisely* $2 million is so large.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$2百万是几乎所有申请所谓的第二次提款（或第二轮）PPP贷款的各种类型企业的上限，包括那些最初可能有资格获得更多资金的企业，因此有道理，获批的$2百万贷款的集群如此之大。
- en: This “answer,” of course, just leads to more questions. According to the documentation,
    $2 million was the upper limit for second-round PPP loans; first-round loans could
    be up to $10 million. If so many businesses were requesting the upper limit for
    *second*-round loans, it indicates that many businesses 1) have already received
    a first-round loan, and 2) their first-round loan may have been even *larger*
    than $2 million, since they would have had to round *down* to $2 million if they
    qualified for more than that in the first round. In other words, we might expect
    those businesses that requested *precisely* $2 million in second-round loans are
    among those that were approved for the largest total amounts of PPP loan relief.
    And of course, if they *did* get some of the largest pots of money, we (and probably
    a lot of other people!) certainly want to know about it.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个“答案”只会带来更多问题。 根据文件，第二轮 PPP 贷款的上限为 2 百万美元； 第一轮贷款最多可以达到 1,000 万美元。 如果有这么多企业请求第二轮贷款的上限，这表明许多企业
    1）已经获得了第一轮贷款，并且 2）他们的第一轮贷款可能甚至比 2 百万美元更大，因为如果他们在第一轮中符合更高的贷款金额，那么他们必须将其调整到 2 百万美元以下。
    换句话说，我们可能期望那些在第二轮贷款中请求*精确* 2 百万美元的企业是那些获得最大 PPP 贷款救济总额批准的企业之一。 当然，如果他们确实获得了一些最大的资金池，我们（可能还有许多其他人！）肯定想知道这件事。
- en: The $2 Million Question
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2 百万美元问题**'
- en: In order to understand what (if any) shared characteristics there may be among
    companies that requested $2 million for their second-round PPP loans, we first
    have to effectively isolate their records within our dataset. How might we do
    this? Well, we know that we’re interested in companies approved for more than
    one loan, which means that their `BorrowerName` *should* appear more than once
    in our data. We *also* know that no second-round loans were issued before January
    13, 2021\. By combining these two insights, we can probably use our data wrangling
    skills to do a decent job of identifying the companies that requested precisely
    $2 million for their second-round loan.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解那些请求第二轮 PPP 贷款 2 百万美元的公司之间可能存在的共同特征，我们首先需要有效地在我们的数据集中隔离它们的记录。 我们该如何做呢？ 嗯，我们知道我们对获得多笔贷款的公司感兴趣，这意味着他们的
    `BorrowerName` *应该* 在我们的数据中出现多次。 我们还知道在 2021 年 1 月 13 日之前没有发放第二轮贷款。 通过结合这两个观点，我们可能可以利用我们的数据整理技能来比较好地识别请求第二轮贷款
    2 百万美元的公司。
- en: 'In order to accomplish this, we’ll do a couple of key transformations on our
    dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将对我们的数据集进行一些关键的转换：
- en: We’ll create a new column for each loan, containing the label `first_round`,
    or `maybe_second`, based on whether it was issued before January 13, 2021\. While
    we can’t be sure that all loans after that date were “second round,” we *can*
    be sure that all loans *before* that date were “first round.”
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将为每笔贷款创建一个新列，包含 `first_round` 或 `maybe_second` 标签，具体取决于它是否在 2021 年 1 月 13
    日之前发放。 虽然我们不能确定所有在那之后发放的贷款都是“第二轮”，但我们*可以*确定在那之前发放的所有贷款都是“第一轮”。
- en: Look for duplicate entries in our dataset. Each approved loan creates a separate
    record, so if the same business was approved for two loans, that means its information
    would appear twice in the records.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的数据集中查找重复条目。 每笔批准的贷款都会创建一个单独的记录，因此如果同一家企业获得了两笔贷款的批准，那么其信息在记录中将出现两次。
- en: The logic here is that if we find a given business name twice in our data *and*
    those records have different “round” labels, it probably indicates a business
    that has, in fact, been approved for two separate loans.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的逻辑是，如果我们在我们的数据中找到了给定的企业名称两次，*并且*这些记录具有不同的“轮次”标签，这可能表明实际上这是一家已获批两笔独立贷款的企业。
- en: As usual, we’re going to call in the help of some Python libraries to get this
    work done. We’ll need to use *pandas*, as usual, but we’re also going to use another
    library called *numpy* that has lots of useful array/list functions (*pandas*
    actually relies heavily on *numpy* under the hood). I’m also going to pull in
    *seaborn* and *matplotlib* again so that we have the option of generating visualizations
    to help us evaluate our evolving dataset as we go along.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们将调用一些 Python 库来完成这项工作。 我们需要像往常一样使用 *pandas*，但我们还将使用另一个名为 *numpy* 的库，它具有许多有用的数组/列表函数（*pandas*
    在幕后实际上严重依赖于 *numpy*）。 我还将再次引入 *seaborn* 和 *matplotlib*，以便我们在进行数据集演变评估时有生成可视化的选项。
- en: Although what we’re trying to do with this data is conceptually pretty straightforward,
    the wrangling involved in performing this analysis takes a fair number of steps,
    as you can see in [Example 9-4](#who_got_2_loans_by_date).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在处理这些数据时的概念是相当直观的，但在执行此分析时所涉及的整理工作却需要进行相当多的步骤，正如您在[示例 9-4](#who_got_2_loans_by_date)中所看到的。
- en: Example 9-4\. who_got_2_loans_by_date.py
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-4\. who_got_2_loans_by_date.py
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO3-1)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO3-1)'
- en: This file was generated by running our fingerprinting process on `BorrowerName`,
    as described in [“Finding a Fingerprint”](#finding_a_fingerprint).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件是通过对`BorrowerName`运行我们的指纹识别过程生成的，详见[“找到指纹”](#finding_a_fingerprint)。
- en: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO3-2)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO3-2)'
- en: We want to know which loans were approved *before* January 13, 2021\. The fastest
    way to do this will be to convert our `DateApproved` strings to “real” dates and
    compare them to that.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想知道在2021年1月13日之前批准的贷款。这样做的最快方法是将我们的`DateApproved`字符串转换为“真实”日期，并将其与之比较。
- en: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO3-3)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO3-3)'
- en: The pandas `cut()` function lets us create a new column by applying boundaries
    and labels to an existing one. In this case, we label each record according to
    whether it was approved before or after January 13, 2021.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: pandas的`cut()`函数允许我们通过对现有列应用边界和标签来创建新列。在这种情况下，我们根据是否在2021年1月13日之前批准来为每条记录标记。
- en: '[![4](assets/4.png)](#co_introduction_to_data_analysis_CO3-4)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_introduction_to_data_analysis_CO3-4)'
- en: We do this for convenience so we can use the `describe()` method.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们可以使用`describe()`方法。
- en: '[![5](assets/5.png)](#co_introduction_to_data_analysis_CO3-5)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_introduction_to_data_analysis_CO3-5)'
- en: We expect the maximum value in this table to be `2`, since no business is allowed
    to get more than two loans under the PPP.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计此表中的最大值将为`2`，因为根据PPP的规定，任何企业都不得获得超过两笔贷款。
- en: 'If you run the code from [Example 9-4](#who_got_2_loans_by_date) and nothing
    happens for a minute, don’t despair. On my Chromebook, this script takes about
    40 to 90 seconds to execute (depending on how many other Linux apps I’m running
    alongside).^([10](ch09.html#idm45143397083520)) When it’s finished, however, your
    output will look something like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行[示例 9-4](#who_got_2_loans_by_date)中的代码，一分钟内什么都没有发生，不要泄气。在我的Chromebook上，这个脚本大约需要40到90秒的时间来执行（取决于我同时运行的其他Linux应用程序数量）。^([10](ch09.html#idm45143397083520))
    但是，执行完成后，您的输出将类似于以下内容：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: From this first effort something seems…off. The output from our `.describe()`
    command gives us a quick way of getting almost all the summary statistics we’re
    interested in (the Q1, median, and Q3 are labeled here according to the percentage
    of values that would appear to their left on a histogram—so 25%, 50%, and 75%,
    respectively). These values suggest that fewer than 25% of all businesses have
    received more than one loan (otherwise the 75% value would be greater than 1),
    which makes sense. But the max value is troubling, since the PPP rules don’t appear
    to allow a single business to receive more than two loans, much less 12! Let’s
    take a closer look by adding the code shown in [Example 9-5](#who_got_2_loans_by_date_contd)
    to what we wrote in [Example 9-4](#who_got_2_loans_by_date) and see what we find.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次初步尝试来看，似乎有些问题。我们的`.describe()`命令的输出为我们提供了一个快速获取几乎所有我们感兴趣的摘要统计信息的方法（这里标记的Q1、中位数和Q3根据直方图上出现在其左侧的值的百分比进行标记，分别为25%、50%和75%）。这些值表明，少于25%的所有企业获得了多于一笔贷款（否则75%的值将大于1），这是有道理的。但是最大值令人担忧，因为PPP规则似乎不允许单个企业获得超过两笔贷款，更不用说12笔了！让我们通过在我们在[示例 9-4](#who_got_2_loans_by_date)中编写的内容中添加[示例 9-5](#who_got_2_loans_by_date_contd)中显示的代码来仔细查看。
- en: Example 9-5\. who_got_2_loans_by_date.py (continued)
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 9-5\. who_got_2_loans_by_date.py（续）
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we get the additional output shown here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了以下额外的输出：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This suggests that there are only a (relative) handful of businesses that may
    have been approved for more than two loans, and we can probably attribute those
    cases to a combination of our chosen fingerprinting approach (a combination of
    `BorrowerName`, `BorrowerCity`, and `BorrowerState`) along with the possibility
    that there are multiple instances of a single franchise in the same city that
    applied for PPP funds.^([11](ch09.html#idm45143396792864)) In any case, there
    are few enough of them that they are unlikely to change the outcome of our analysis
    considerably, so we won’t focus on tracking down their details right now. At least
    the second piece of output showing that 72,060 individual businesses got *exactly*
    two loans seems reasonable so far, since this is definitely less than 25% of our
    total dataset, and therefore aligns with the summary statistics we got from our
    `Loan Count` DataFrame (because the value of Q3 was still 1, meaning that fewer
    than 25% of all business names appeared in our dataset more than once).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is still just an estimate; it would be much better if we had
    a more official count of second-round loans to work with. As noted at the end
    of [Chapter 6](ch06.html#chapter6), the Small Business Administration *did* actually
    release [an official data dictionary](https://data.sba.gov/dataset/ppp-foia/resource/aab8e9f9-36d1-42e1-b3ba-e59c79f1d7f0)
    for the PPP loan data, and while it doesn’t contain all of the information we
    might hope, it *does* indicate that the `ProcessingMethod` field distinguishes
    between first-round (`PPP`) and second-round (`PPS`) loans. Let’s look at our
    data this way and compare it to our name-matching-based estimate by adding the
    code in [Example 9-6](#who_got_2_loans_by_date_contd2) further down in our file.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-6\. who_got_2_loans_by_date.py (continued again)
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Rerunning our script yields the additional output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Wow! Even with our possibly too-lax fingerprinting method, we still failed to
    find more than 300,000 businesses with both of their loans. What do we do?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: First of all, recognize that this isn’t even an unusual situation. We’re dealing
    with around 750,000 data records, each one of which is a combination of data entry
    done by multiple individuals, including the borrower, the lender, and possibly
    the SBA. The fact that there are still so many discrepancies is not really surprising
    (I illustrate some of them in the following sidebar), but all is not lost. Remember
    that our original interest was in those businesses that got precisely $2 million
    for their second-round loan, which is likely to be just a fraction of all the
    businesses that got two loans. We can still move ahead with that part of the analysis
    in order to (1) test how effective our date-based estimate of second-round loans
    was, and (2) see what we can learn about that specific subset of businesses that
    got exactly $2 million in the second round.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’re going to use the information from the `PaymentProcessingMethod`
    column to *validate* our earlier work using name-matching and date-based loan
    round estimates. To do this, we’re going to merge our `Loan Count` DataFrame back
    onto our original dataset. Then we’ll select only the $2M loans that we *estimate*,
    based on their date, were second-round loans. Finally, we’ll compare that number
    of loans with the number of $2M loans we *know* were second draw, based on their
    `ProcessingMethod` value of `PPS`. Obviously, this will mean adding yet more code
    to our file, as shown in [Example 9-7](#who_got_2_loans_by_date_contd3).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-7\. who_got_2_loans_by_date.py (continued more)
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Adding this code to our main files give us another few lines of output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If we compare these results to previous ones, it looks like we’re doing a bit
    better. Across *all* loans, we appear to have matched up 72,060 out of 103,949
    actual second-round loans, or about 70%. For those organizations approved for
    $2M in second-round loans, we’ve found 1,115 out of 1,459, or about 80%.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: So what can we say about businesses that got $2M in the second round? We can’t
    say anything with 100% confidence unless and until we find matches for those 284
    companies whose `BorrowerNameFingerprint` isn’t the same between their first-
    and second-round loans. But we can still look at our 80% sample and see what we
    discover. To do this, I’m going to take the following steps:^([13](ch09.html#idm45143396564848))
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Find all the unique `BorrowerNameFingerprint` values for businesses that definitely
    got $2M second-round loans.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a DataFrame (`biz_names_df`) based on this list and fill it out with
    the flag value `2Mil2ndRnd`.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge that DataFrame back onto my dataset and use the flag value to pull *all*
    loan records for those businesses (both first and second round).
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do some basic analyses of how much money those businesses were approved for
    across both rounds, and visualize those amounts, comparing the official second-round
    designation (that is, `ProcessingMethod == 'PPS'`) with our derived, date-based
    category.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And of course, now that I’ve written out in a list the steps my script should
    take (this is exactly the kind of thing that you’d want to put in your data diary
    and/or program outline), it’s just a matter of coding it up below our existing
    work; for clarity I’ve put it into a second script file, the complete code of
    which is shown in [Example 9-8](#who_got_2M_with_viz).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-8\. who_got_2M_with_viz.py
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Running this script will give us all the output from previous examples, along
    with yet a few more lines of additional output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'At first, it looks like something’s off, because we might have expected our
    total number of loans to be 2 × 1,175 = 2,350\. But remember that we matched up
    loans based on whether they got approved for *exactly* $2M in round two, *and*
    we failed to match 284 loans on `BorrowerNameFingerprint`. This means we have
    *all* second-round loans but are missing 284 first-round loans in these numbers.
    In other words, we’d *expect* to have (2 × 1,175) + 284 = 2,634—and we do! Good!
    It’s always nice when *something* matches up. This means that our “total” figure,
    while still not 100% accurate, is a somewhat reasonable estimate of the *minimum*
    total loan amount this group of businesses were approved for in PPP funds: around
    $6 billion dollars.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s take a look at the visualization shown in [Figure 9-6](#two_loan_business_amounts_by_round),
    which is a view of the graphic generated by the script in which we can compare
    how our `Loan Round` classification matches up against the designated `PPS` loans.
    This is a rough (but still useful) way to validate our work—and the results look
    pretty good!^([14](ch09.html#idm45143396503936))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Dollar amount of most approved loans for businesses that received two PPP
    loans, by loan round.](assets/ppdw_0906.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. Dollar amount of most approved loans for businesses that received
    two PPP loans, by loan round
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Interestingly, though, [Figure 9-6](#two_loan_business_amounts_by_round) also
    illustrates something else: it seems that a fair number of the businesses approved
    for $2M in second-round loans violate our earlier hypothesis that companies approved
    for $2M in second-round loans were approved for *more* than that amount in their
    first-round loans, when the limits were higher. As usual, in answering one question,
    we’ve generated another! And of course the work we’ve already done would give
    us a head start down the road to answering it. Before we let loose on our next
    round of question-and-answer, though, we need to talk about one more essential
    component of data analysis and interpretation: *proportionality*.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Proportional Response
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you go out to eat with some friends. You’ve eaten recently, so you just
    order a drink, but your three friends are ravenous and each order a full meal.
    How do you decide who owes what when the check arrives? Most of us would agree
    that the most sensible thing to do would be to calculate—or at least estimate—what
    *proportion* of the total bill each person’s order accounted for, and then have
    each person pay that, along with that same proportion of, say, the tax and tip.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: The same sort of logic applies when we’re analyzing data. In [“The $2 Million
    Question”](#two_million_question), we looked at the *total* funds that had been
    approved for a certain subset of businesses through the PPP, and while $6B sounds
    like a lot, we should arguably be more interested in how those businesses *used*
    that money, rather than the absolute number of dollars they got. Since the PPP
    was designed to keep people on the payroll, one thing we might want to know is
    how much money those businesses received *in relation to how many jobs they preserved*,
    a process I think of as *rationalizing* the data.^([15](ch09.html#idm45143395830240))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the process of rationalizing our data is extremely simple: we
    calculate the *ratio* between two data values by dividing one number by the other.
    For example, if we want to know how many dollars per job the companies identified
    in [“The $2 Million Question”](#two_million_question) spent, we can (after some
    sanity checking) divide the value in `PAYROLL_PROCEED` by the value in `JobsReported`
    for each record, as shown in [Example 9-9](#dollars_per_job_2M_rnd2).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-9\. dollars_per_job_2M_rnd2.py
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO4-1)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that a couple of businesses didn’t report *any* jobs, which will
    break our calculation. Since there are only two records guilty of this, we’ll
    just drop them, using their *pandas*-assigned row labels.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: While the text output here confirms that we’re looking at the same set of loans
    that we examined in [“The $2 Million Question”](#two_million_question), our rationalized
    data highlights some noteworthy anomalies in some of the first-round loans, where
    a handful of companies appear to have had loans approved that allocated more for
    payroll than the $100,000-per-job limit allowed, as shown in [Figure 9-7](#dollars_per_job).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Detail of dollars per job of companies approved for $2M in second-round loans](assets/ppdw_0907.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Detail of dollars per job of companies approved for $2M in second-round
    loans
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What do we make of this? You may notice that by now we’ve veered a rather long
    way from the question we posed back in [“The Pandemic and the PPP”](ch06.html#pandemic_and_ppp),
    where we started out trying to assess whether the PPP had helped “save” American
    businesses. While that focus helped us work our way through our data quality evaluations,
    doing some contextual analysis has opened up a number of new questions and directions—which
    I think you’ll find is a pretty common occurrence when it comes to data wrangling.
    Hopefully, though, this will just encourage you to keep working with new datasets
    to see what else you can find!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After all these analyses, we’ve learned a few new things—some of which are
    specific to this dataset but many of which are far more generally applicable:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: A relatively small number of companies were approved for the maximum allowable
    second-round amount in the PPP loan program. While many of them had filed for
    much more than that for their first-round loan, some did not.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A handful of companies that were approved for a $2M second-round loan claimed
    more than $100,000 per reported job in their first-round loan.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human-entered data is always a mess. That’s why data cleaning is an ongoing,
    iterative process. Documenting your work is essential to being able to defend
    your results.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, our introductory data analysis left us with far more questions than answers.
    At this point, there’s only one way we’re going to find out more: talking to people.
    Sure, some of the patterns we uncovered [look questionable](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3906395),
    but we have far too many unknowns to make any well-supported claims at this point.
    For example, many of the $2M second-round loans had yet to be disbursed when this
    data was released, so some companies might have actually gotten or used far less
    than that. Since the PPP rules only require that a minimum percentage of a loan
    is spent on payroll in order to be forgivable, companies that appear to have been
    approved for too much may have simply used the difference on other allowable expenses,
    like mortgage interest or health care costs. In other words, while we can learn
    a little bit from this type of numerical data analysis, it will never be enough
    to tell us the whole story—either the how or the why. That is something for which
    we need direct human input.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve done that work, and are clear about what insights we want to share,
    we are ready to begin thinking about the most effective way to convey what we’ve
    learned to others. Just as our data analysis relies on both data *and* human judgment
    and input, the most effective data communications almost always involve a balance
    between words and visualizations. As we’ll see in the next chapter, by crafting
    both our words *and* our visualizations with care, we can better ensure that our
    intended message truly gets heard.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.html#idm45143400237136-marker)) See [*Predictably Irrational*](https://danariely.com/books/predictably-irrational/)
    by Dan Ariely (Harper) for more information.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.html#idm45143398320448-marker)) You can technically also sort from
    highest to lowest, but starting with lower values is conventional and will make
    things easier in the long run.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch09.html#idm45143399041152-marker)) There are actually multiple methods
    of choosing the median value for an even number of data points, but as long as
    you’re consistent, any of them is fine. Anecdotally, this is the approach that
    feels most intuitive and that I’ve seen used most often.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch09.html#idm45143398988128-marker)) While the precise estimates for the
    number of items that humans can hold in *working memory* differ, researchers *do*
    agree that this capacity has limits. See [*https://ncbi.nlm.nih.gov/pmc/articles/PMC2864034*](https://ncbi.nlm.nih.gov/pmc/articles/PMC2864034)
    and [*https://pnas.org/content/113/27/7459*](https://pnas.org/content/113/27/7459).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch09.html#idm45143398983968-marker)) Although there is a long way to go,
    there is some [exciting research being done on tactile graphics](https://dl.acm.org/doi/abs/10.1145/3373625.3418027)
    to reduce the vision dependency of these approaches, especially for folks who
    are blind or visually impaired. See [*http://shape.stanford.edu/research/constructiveVizAccess/assets20-88.pdf*](http://shape.stanford.edu/research/constructiveVizAccess/assets20-88.pdf).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch09.html#idm45143397925760-marker)) See “A Morphometric Analysis of Ddt-Resistant
    and Non-Resistant House Fly Strains” by Robert R. Sokal and Preston E. Hunter,
    [*https://doi.org/10.1093/aesa/48.6.499*](https://doi.org/10.1093/aesa/48.6.499);
    the relevant data is provided there.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch09.html#idm45143397906944-marker)) M. W. Toews, “CC BY 2.5,” [*https://creativecommons.org/licenses/by/2.5*](https://creativecommons.org/licenses/by/2.5),
    via [Wikimedia Commons](https://en.wikipedia.org/wiki/Normal_distribution#/media/File:Standard_deviation_diagram.svg).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch09.html#idm45143397217488-marker)) Specifically relative to the loan
    amounts just above or below these values.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch09.html#idm45143397194032-marker)) It happens.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch09.html#idm45143397083520-marker)) If it’s *too* many, the output will
    say `Killed`. This is a sign you either need to close some apps or maybe move
    into the cloud.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch09.html#idm45143396792864-marker)) See [*https://sba.gov/document/support-faq-ppp-borrowers-lenders*](https://sba.gov/document/support-faq-ppp-borrowers-lenders)
    and [*https://sba.gov/document/support-sba-franchise-directory*](https://sba.gov/document/support-sba-franchise-directory)
    for more information.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch09.html#idm45143396754128-marker)) Though you can find it in the file
    *ppp_fingerprint_borrowers.py*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch09.html#idm45143396564848-marker)) Note that I’m intentionally doing
    this in a *slightly* roundabout way in order to demonstrate a few more data-wrangling
    and visualization strategies, but feel free to rework this code to be more efficient
    as an exercise!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch09.html#idm45143396503936-marker)) If we compare the results numerically,
    we’ll find they’re identical, at least for our subset of companies approved for
    $2M in the second round.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch09.html#idm45143395830240-marker)) This term has more specific meanings
    in the business and statistics/data-science worlds, but *proportionalizing* just
    sounds kind of awkward. Plus, it better matches the actual calculation process!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
