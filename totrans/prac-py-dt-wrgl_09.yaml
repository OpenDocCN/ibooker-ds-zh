- en: Chapter 9\. Introduction to Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, this book has focused mostly on the logistics of acquiring, assessing,
    transforming, and augmenting data. We’ve explored how to write code that can retrieve
    data from the internet, extract it from unfriendly formats, evaluate its completeness,
    and account for inconsistencies. We’ve even spent some time thinking about how
    to make sure that the tools we use to do all this—our Python scripts—are optimized
    to meet our needs, both now and in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, though, it’s time to revisit the *why* of all this work. Back
    in [“What Is “Data Wrangling”?”](ch01.html#describing_data_wrangling), I described
    the purpose of data wrangling as transforming “raw” data into something that can
    be used to generate insight and meaning. But unless we follow through with at
    least *some* degree of analysis, there’s no way to know if our wrangling efforts
    were sufficient—or what insights they might produce. In that sense, stopping our
    data wrangling work at the augmentation/transformation phase would be like setting
    up your mise en place and then walking out of the kitchen. You don’t spend hours
    carefully prepping vegetables and measuring ingredients unless you want to *cook*.
    And that’s what data analysis is: taking all that beautifully cleaned and prepared
    data and turning it into new insight and knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: If you fear we’re slipping into abstractions again, don’t worry—the fundamentals
    of data analysis are simple and concrete enough. Like our data quality assessments,
    however, they are about one part technical effort to four parts judgment. Yes,
    the basics of data analysis involve reassuring, 2 + 2 = 4–style math, but the
    insights depend on *interpreting* the outputs of those very straightforward formulas.
    And that’s where you need logic and research—along with human judgment and expertise—to
    bridge the gap.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, then, we’ll be exploring the basics of data
    analysis—specifically, the simple measures of *central tendency* and *distribution*
    that help us give data meaningful context. We’ll also go over the rules of thumb
    for making appropriate inferences about data based on these measures and the role
    that both numerical and *visual* analysis play in helping us understand the trends
    and anomalies within our dataset. Toward the end of the chapter, we’ll address
    the limits of data analysis and why it *always* takes more than traditional “data”
    to get from the “what” to the *why*. Along the way, of course, we’ll see how Python
    can help us in all these tasks and why it’s the right tool for everything from
    quick calculations to essential visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Context Is Everything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If I offered to sell you an apple right now for $0.50, would you buy it? For
    the sake of this example, let’s imagine that you like apples, and you’re feeling
    like a snack. Also, this is a beautiful apple: shiny, fragrant, and heavy in the
    hand. Let’s also suppose you’re confident that I’m not trying to harm you with
    this apple. It’s just a nice, fresh, apple, for $0.50\. Would you buy it?'
  prefs: []
  type: TYPE_NORMAL
- en: 'For most people, the answer is: *it depends*. Depends on what? Lots of things.
    No matter how much you trust me (and my almost-too-perfect apple), if someone
    standing next to me was selling also-very-good-looking apples for $0.25 each,
    you might buy one of those. Why? Well, obviously they’re cheaper, and even my
    *incredibly* awesome apple probably isn’t *twice* as awesome as the next one.
    At the same time, if your cousin was standing on the other side of me selling
    tasty apples for $0.60 each, you might well buy one of those instead, just to
    show support for your cousin’s new apple-selling start-up.'
  prefs: []
  type: TYPE_NORMAL
- en: This might seem like a pretty complicated decision-making process for choosing
    a piece of fruit, but in reality we make these kinds of choices all the time,
    and the results are sometimes surprising. Economists like [Dan Ariely](https://danariely.com/all-about-dan)
    and [Tim Harford](https://timharford.com/articles/undercovereconomist) have conducted
    research illustrating things like how influential a “free” gift is, even if it
    creates an added cost, or how our satisfaction with our own pay can go down when
    we learn what people around us are earning.^([1](ch09.html#idm45143400237136))
    Most of our priorities and decisions depend on value judgment, and in order to
    make those effectively we need to know what our options are. Would I buy a fairy-tale-perfect
    apple for $0.50? Maybe. Probably not if I could get a really similar one at the
    next corner for half the price. But I probably would if I was in a hurry and had
    to walk a mile to get one otherwise. Though we all understand what we mean by
    it, a more precise way of saying “It depends” would be to say, “It depends on
    the *context*.”
  prefs: []
  type: TYPE_NORMAL
- en: The importance of context is why a data point in isolation is meaningless; even
    if it is factually “true,” a single data point can’t help us make decisions. Generating
    and acquiring new knowledge, in general, is about connecting new information to
    information we already know. In other words, the knowledge isn’t “in the data”
    itself but in its *relationship* to other things. Since we can’t exhaustively
    explore the context of every situation where decisions are required (including
    your concerns about your cousin’s new apple startup and the importance of supporting
    family efforts), we often have to restrict ourselves to examining those parts
    of the context that we can (or have chosen to) consistently measure and quantify.
    In other words, we turn to data.
  prefs: []
  type: TYPE_NORMAL
- en: How do we derive context from data? We do things like investigate its provenance,
    asking questions about who collected it and when and why—these answers help illuminate
    both what the data includes and what might be missing. And we look for ways to
    systematically compare each data point to the rest, to help understand how it
    conforms to—or breaks—any patterns that might exist across the dataset as a whole.
    Of course, none of this is likely to offer us definitive “answers,” but it will
    provide us with insights and ideas that we can share with others and use to inspire
    our *next* question about what’s happening in the world around us.
  prefs: []
  type: TYPE_NORMAL
- en: Same but Different
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While building context is essential for generating insight from data, how do
    we know *which* contexts matter? Given that there are infinite relationship *types*
    we could identify, even among the data points in a pretty small dataset, how do
    we decide which ones to pay attention to? Take, for example, the data in [Example 2-9](ch02.html#page_loop),
    which was just a simple (fictional) list of page counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Even with just this handful of numbers, there are lots of types of “context”
    we can imagine: we could describe it in terms of even versus odd values, for example,
    or which ones can be evenly divided by 8\. The problem is, most of these relationships
    are not all that interesting. How can we tell which ones will be?'
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out, the human brain is pretty well wired to notice—and care about—two
    types of relationships in particular: sameness and differentness. Trends and anomalies
    in almost any type of stimulus—from [seeing patterns in clouds or lottery results](https://archive.is/20130121151738/http://dbskeptic.com/2007/11/04/apophenia-definition-and-analysis)
    to [quickly identifying a difference in orientation among similar objects](https://csc2.ncsu.edu/faculty/healey/PP)—tend
    to catch our attention. This means *trends* and *anomalies* are interesting, almost
    by definition. So a pretty good place to start when we want to build meaningful
    context for our data is with the ways that individual records in a given dataset
    are similar to—or different from—each other.'
  prefs: []
  type: TYPE_NORMAL
- en: What’s Typical? Evaluating Central Tendency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What does it mean for something to be “average”? When we use that term in day-to-day
    life, it’s often a stand-in for “unremarkable,” “expected,” or “typical.” Given
    its specifically *un*extraordinary associations, then, in many cases “average”
    can also be a synonym for “boring.”
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to analyzing data, however, it turns out that what’s “average”
    is actually what interests us, because it’s a basis for comparison—and comparisons
    are one thing humans care about a lot. Remember the research on wages? As humans,
    we want to know how things that affect us compare to what is “typical” for other
    people. So even if we never hope to *be* “average,” in general we still want to
    know both what it is and how our own experience compares.
  prefs: []
  type: TYPE_NORMAL
- en: What’s That Mean?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may already be familiar with the process for calculating the “average”
    of a set of numbers: add them all up and divide by how many you have. That particular
    measure of *central tendency* is more precisely known as the *arithmetic mean*,
    and it’s calculated just the way you remember. So for our `page_counts` variable,
    the math would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And this would give us (roughly) a mean value of:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As a representation of the “typical” chapter length, this seems pretty reasonable:
    many of our chapters have page counts that are pretty close to 30, and there are
    even two chapters that have *precisely* 32 pages. So a mean per-chapter page count
    of just over 32 seems about right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the mean may have served us well in this instance, however, there are
    many situations where using it as a measure of “typicality” can be deeply misleading.
    For example, let’s imagine one more, *really* long chapter (like, 100 pages long)
    got added to our book. Our method of calculating the mean would be the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'But now the mean value would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: All of a sudden, our “average” chapter length has increased by almost 6 pages,
    even though we only added a single new chapter, and fully *half* of our chapters
    are 28–34 pages long. Is a chapter that’s roughly 39 pages truly “typical” in
    this case? Not really.
  prefs: []
  type: TYPE_NORMAL
- en: What we’re seeing even in this small example is that while in *some* cases the
    mean is a reasonable measure of “typicality,” it’s also heavily influenced by
    extreme values—and just one of these is enough to make it useless as a shorthand
    for what is “typical” of a dataset. But what are our alternatives?
  prefs: []
  type: TYPE_NORMAL
- en: Embrace the Median
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another way to think about “typical” values in a dataset is to figure out what
    is—quite literally—in the “middle.” In data analysis, the “middle” value in a
    series of records is known as the *median*, and we can find it with even *less*
    math than we used when calculating the mean: all you need to do is sort and count.
    For example, in our original set of chapter lengths, we would first sort the values
    from lowest to highest:^([2](ch09.html#idm45143398320448))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, all we have to do is choose the “middle” value—that is the one that is
    positioned halfway between the beginning and end of the list. Since this is a
    nine-item list, that will be the value in the fifth position (leaving four items
    on either side). Thus, the *median* value of our `page_count` dataset is 32.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see what happens to the median when we add that extra-long chapter.
    Our sorted data will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And what about the median? Since the list now has an *even* number of items,
    we can just take the *two* “middle” values, add them together, and divide by two.^([3](ch09.html#idm45143399041152))
    In this case, that will be the values in positions 5 and 6, which are both 32\.
    So our median value is (32 + 32) / 2 = 32\. Even when we add our extra-long chapter,
    the median value is still the same!
  prefs: []
  type: TYPE_NORMAL
- en: Now at first you might be thinking, “Hold on, this feels wrong. A whole new
    chapter was added—a really *long* chapter, but the median value didn’t change
    *at all*. Shouldn’t it move, at least a little bit?”
  prefs: []
  type: TYPE_NORMAL
- en: The real difference between the mean and the median is that the mean is powerfully
    affected by the specific *values* in dataset—as in how high or low they are—while
    the median is influenced mostly by the *frequency* with which certain values appear.
    In a sense, the median is much closer to a “one value, one vote” sort of approach,
    whereas the mean lets the most extreme values speak “loudest.” Since our current
    goal is to understand what values are “typical” in our dataset, the median will
    usually be the most representative choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think Different: Identifying Outliers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Same but Different”](#samesame), I noted that human beings, in general,
    are interested in “sameness” and “differentness.” In looking at our two possible
    measures of central tendency, we were exploring ways in which values in a dataset
    are similar. But what about the ways in which they are different? If we look again
    at our original `page_count` list, we’d probably feel confident that a 32-page
    chapter is reasonably “typical,” and maybe even a 30-, 28-, or 34-page chapter,
    too. But what about a 12-page chapter? Or a 56-page chapter? They certainly don’t
    seem typical, but how do we know which values are different *enough* to be truly
    “unusual?”
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where we have to start mixing math with human judgment. Measures of
    central tendency can be pretty unequivocally calculated, but determining which
    values in a dataset are truly unusual—that is, which are *anomalies* or *outliers*—cannot
    be definitively assessed with arithmetic alone. As datasets get larger and more
    complex, however, it gets harder for humans to interpret them effectively as sets
    of data points.^([4](ch09.html#idm45143398988128)) So how can we possibly apply
    human judgment to large datasets? We need to engage our largest and most comprehensive
    data-processing resource: the human visual system.^([5](ch09.html#idm45143398983968))'
  prefs: []
  type: TYPE_NORMAL
- en: Visualization for Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The role of visualization in data work is twofold. On the one hand, visualization
    can be used to help us *analyze* and make sense of data; on the other, it can
    be used to *convey* the insights that we’ve generated from that analysis. Using
    data for the latter purpose—as a communication tool to share insights about our
    data with a broader audience—is something that we’ll explore in-depth in [Chapter 10](ch10.html#chapter10).
    Here, we’re going to focus on the ways in which visualization can offer us insight
    into the data we have.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand how visualization can help us identify extreme values
    in our data, we first need to look—quite literally—at the data itself. In this
    case, I don’t mean that we’re going to open up our CSV file and start reading
    through data records. Rather, we’re going to create a special kind of bar chart
    known as a *histogram*, in which each bar represents the number of times a particular
    value appears in our dataset. For example, we can see a very simple histogram
    of our (expanded) `page_count` data in [Figure 9-1](#page_count_histogram).
  prefs: []
  type: TYPE_NORMAL
- en: '![A sample histogram](assets/ppdw_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A basic histogram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For a dataset as small as our `page_counts` example, a histogram can’t tell
    us much; in fact, a dataset with only 10 values arguably has too few data points
    for either the concepts of *central tendency* (like the *mean* and the *median*)
    or outliers to have much meaning. Even so, in [Figure 9-1](#page_count_histogram)
    you can see the *beginnings* of what looks like a pattern: the two chapters of
    equal length form a double-height spike, with the unique page-lengths of most
    of the remaining chapters becoming single-height bars clustered reasonably close
    it. *Way* out on the right end, meanwhile, is our 100-page chapter, with no other
    values anywhere near it. While we might have already been tempted to conclude
    that the 100-page chapter was an anomaly—or *outlier*—based on the math, this
    histogram certainly reinforces that interpretation.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in order to really appreciate the power of visualization for data
    analysis, we’ll want to look at a dataset with values we couldn’t even hope to
    “eyeball” the way that we did with our list of page counts. Fortunately, the Paycheck
    Protection Program (PPP) data that we worked through in [Chapter 6](ch06.html#chapter6)
    is *definitely* not lacking in this regard, since it contains hundreds of *thousands*
    of loan records. To see what we can learn about what is and isn’t “typical” in
    the loan amounts approved through the PPP, we’ll write a quick script to generate
    a histogram of the currently approved loan values in the PPP. Then, we’ll label
    that histogram with both the mean and median values in order to see how well each
    might serve as a potential measure of central tendency. After that, we’ll return
    to the question of identifying possible outliers in the approved loan amounts,
    using both our visualization of the data and some math to back it up.
  prefs: []
  type: TYPE_NORMAL
- en: For this to work, we’ll once again be leveraging some powerful Python libraries—specifically,
    *matplotlib* and *seaborn*—both of which have functions for calculating and visualizing
    data. While *matplotlib* remains the foundational library for creating charts
    and graphs in Python, we’re also using *seaborn* for its helpful support of more
    advanced calculations and formats. Because the two are highly compatible (*seaborn*
    is actually built on *top* of *matplotlib*), this combination will offer us the
    flexibility we need both to quickly create the basic visualizations we need here
    and also customize them for presenting data effectively in [Chapter 10](ch10.html#chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: For now, though, let’s focus on the analytical size of our visualization process.
    We’ll start this by creating a basic histogram of our PPP loan data using the
    `CurrentApprovalAmount` values. We’ll also add mean and median lines for more
    context, as shown in [Example 9-1](#ppp_loan_central_measures).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-1\. ppp_loan_central_measures.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `CurrentApprovalAmount` column in our PPP data tells us the dollar amount
    of each loan that is currently approved (whether or not it has been disbursed).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `get_ylim()` method returns the lowest and highest y-axis value as a list.
    We’ll mostly be using this to set a legible length for our mean and median lines.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We can add vertical lines anywhere on our histogram (or other visualization)
    by specifying the “x” position, “y” starting point, “y” ending point, color, and
    line style. Note that the units for “x” and “y” values are relative to the dataset,
    *not* the visual size of the chart.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introduction_to_data_analysis_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: While calling the *matplotlib* `show()` method is not explicitly required in
    Jupyter notebooks, like `print()` statements, I prefer to include them for clarity
    and consistency. By default, the chart will render in “interactive” mode (in Jupyter,
    you’ll need to include the `%matplotlib notebook` “magic” command, as I have in
    the provided files), which allows us to zoom, pan, and otherwise explore our histogram
    in detail without writing more code.
  prefs: []
  type: TYPE_NORMAL
- en: Most likely, you’re looking at the chart that displayed as a result of running
    this script (which hopefully looks something like [Figure 9-2](#ppp_loan_histogram))
    and thinking, “Now what?” Admittedly, this initial visualization seems a bit lackluster—if
    not downright confusing. Fear not! If anything, take this as your first case study
    in why visualization for analysis and visualization for communication are *not*
    one and the same. Analytical visualizations like this one, for example, require
    far more effort to read, understand, and refine than *any* visualization we would
    want to use for general communications. For generating insight about our data,
    however, this is actually just the right place to start.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we forge ahead with our data analysis, though, let’s take a moment to
    appreciate the distinctly old-school—but incredibly useful—interface that Python
    has given us for this chart. Rather than just a static image, our Python script
    has *also* given us an entire toolbar (shown in [Figure 9-2](#ppp_loan_histogram))
    that we can use to interact with it: to zoom, pan, modify, and even save the output.
    While of course we can (and eventually will) customize the contents and aesthetics
    of this chart using code, the fact that we can effectively explore our data without
    having to constantly modify and rerun our code is a huge time-saver. To get a
    feel for what’s possible, take a few minutes to play with the controls yourself.
    When you’re ready to move on with our data analysis, just click the “home” icon
    to return the chart to its initial view and follow along in the sections that
    follow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PPP loan histogram](assets/ppdw_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. PPP loan histogram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What’s Our Data’s Shape? Understanding Histograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we were working with our data in a table-type format, we tended to think
    of its “shape” in terms of the number of rows and columns it had (this is actually
    *exactly* what the `pandas.shape` property of a DataFrame returns). In this context,
    the “shape” we’re interested in is the literal shape of the histogram, which will
    help us identify potentially interesting or important patterns or anomalies. Some
    of the first things we’ll look for in these instances are:'
  prefs: []
  type: TYPE_NORMAL
- en: Symmetry
  prefs: []
  type: TYPE_NORMAL
- en: Is our data vertically symmetrical? That is, could we draw a vertical line somewhere
    over our visualized data such that the pattern of bars on one side looks (roughly)
    like a reflection of those on the other?
  prefs: []
  type: TYPE_NORMAL
- en: Density
  prefs: []
  type: TYPE_NORMAL
- en: Where are most of our data values clustered (if anywhere)? Are there multiple
    clusters or just one?
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, these questions are about more than aesthetics. The shape of
    a dataset’s histogram illustrates what is typically described as its *distribution*.
    Because certain distributions have specific properties, we can use our data’s
    *distribution* to help us identify what is typical, what is unusual, and what
    deserves further scrutiny.
  prefs: []
  type: TYPE_NORMAL
- en: The Significance of Symmetry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the natural world, symmetry is a common occurrence. Plants and animals tend
    to be symmetrical in many ways; for example, a dog’s face and an oak leaf both
    exhibit what’s known as *bilateral symmetry*—what we might describe as one side
    being a “mirror image” of the other. Across populations of living things, however,
    there is also often symmetry in the distribution of certain physical characteristics,
    like height or wing length. Our histogram lets us observe this symmetry firsthand,
    by illustrating the *frequency* of specific heights or wing lengths within a population.
    A classic example of this is shown in [Figure 9-3](#housefly_wings), which shows
    the length of housefly wings as measured by a team of biologists in the mid-20th
    century.^([6](ch09.html#idm45143397925760))
  prefs: []
  type: TYPE_NORMAL
- en: 'The symmetrical bell curve shown in [Figure 9-3](#housefly_wings) is also sometimes
    described as the “normal,” “standard,” or “Gaussian” distribution. If you’ve ever
    had an academic grade “curved,” this was the distribution that the grades in your
    cohort were being transformed to fit: one with very few scores at the top or bottom
    and most of them lumped in the middle.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The power of the Gaussian distribution is not just in its pretty shape, however;
    it’s in what that shape means we can *do*. Datasets that demonstrate Gaussian
    distributions can be both described and compared to one another in ways that nonsymmetrical
    distributions cannot, because we can meaningfully calculate two measures in particular:
    the *standard deviation*, which quantifies the numerical range of data values
    within which most of them can be found, and each value’s *z-score*, which describes
    its distance from the mean in terms of standard deviations. Because of the fundamental
    symmetry of Gaussian distributions, we can use the *standard deviation* and the
    *z-score* to compare two sets of functionally similar data *even if they use different
    scales*. For example, if student grades demonstrate a Gaussian distribution, we
    can calculate and compare individual students’ z-scores (that is, their performance
    relative to their cohort) even across different classes and instructors who may
    use different grading rubrics. In other words, even if the mean of student grades
    for one instructor is in the 90s and for another instructor in the 70s, if both
    sets of students’ grades are truly Gaussian in their distribution, we can still
    determine which students are doing the best or need the most help across cohorts—something
    the *nominal* grades (e.g., 74 or 92) could never tell us.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Length of Housefly Wings](assets/ppdw_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Length of housefly wings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These characteristics also inform how we can think about measuring central tendency
    and outliers. For example, in a “perfect” Gaussian distribution, the mean and
    the median will have the same value. What’s more, a value’s z-score gives us a
    quick way of identifying how typical or unusual that particular value is, because
    the percentage of data values that we expect to have a given z-score is well-defined.
    Confused yet? Don’t worry. Just like any other complex data relationship, this
    all makes much more sense if we visualize it.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Gaussian distribution, showing what percentage of values exist within
    1, 2 and 3 standard deviations (σ) from the mean.](assets/ppdw_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. The Gaussian distribution, showing what percentage of values exist
    within 1, 2, and 3 standard deviations (σ) from the mean
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in [Figure 9-4](#gaussian_distro),^([7](ch09.html#idm45143397906944))
    if our data’s distribution is Gaussian, more than two-thirds of the data values
    (34.1% + 34.1% = 68.2%) can be found within one standard deviation (often designated
    as it is here, by the Greek letter σ) of the mean. Another 27.2% can be found
    between one and two standard deviations from the mean, and a final 4.2% can be
    found between two and three standard deviations from the mean. This means that
    for a Gaussian distribution, *99.7% of all values can be found within 3 standard
    deviations of the mean*.
  prefs: []
  type: TYPE_NORMAL
- en: So what? Well, remember that one of our fundamental objectives in data analysis
    is to understand what values are typical for our dataset and which ones are truly
    extreme. While the mean and the median offer a quick shorthand for a dataset’s
    “typical” value, measures like the standard deviation—and the z-scores we can
    calculate from it—help us systematically evaluate which values might or might
    not be truly unusual.
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, calculating these values using Python is quite straightforward.
    Using either *pandas* or the *statistics* library, we can quickly find the value
    of the standard deviation for our dataset (σ) and then use it to place lines over
    our histogram where the relevant z-score values are. For this example, we’ll build
    on the data used to generate [Figure 9-3](#housefly_wings), as shown in [Example 9-2](#wing_length_with_sd).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-2\. wing_length_with_sd.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Each “bin” is a range of *actual* data values that will be lumped together into
    a single histogram bar; the `kde` parameter is what adds a smoothed line to our
    visualization. This line approximates the pattern we would expect if our dataset
    had infinite data points.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also have used the *pandas* `std()` method: `wing_data[''wing_length
    (0.1mm)''].std()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that our loop will stop *before* the second value provided to `range()`,
    so to get three positive lines, we set the second value to `4`. By starting with
    a negative number, we actually *subtract* from the mean at first—which we want
    to do because we want to capture values both above *and* below the mean.
  prefs: []
  type: TYPE_NORMAL
- en: As you review the output of [Example 9-2](#wing_length_with_sd) you might be
    thinking, “Great, we’ve drawn some lines on data about bugs. How will this help
    me interpret *real* data?” After all, the prototypically Gaussian distribution
    of this housefly wing-length data doesn’t look much like the output we got when
    we charted our PPP loan data, which was distinctly *a*symmetrical, as most of
    your data is likely to be.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what do we do when our data distribution lacks symmetry? We already know
    how to find the “middle” of an asymmetric distribution like the one in [Example 9-1](#ppp_loan_central_measures):
    by calculating the median, rather than the mean. But what about identifying extreme
    values? Since asymmetric or *skewed* distributions aren’t, well, symmetric, there
    is no single “standard” deviation, nor can we use it to calculate z-scores. We
    can, however, still usefully subdivide an asymmetric dataset in a way that will
    let us generate insight about possibly unusual or extreme values.'
  prefs: []
  type: TYPE_NORMAL
- en: Like finding the median, this subdivision process is really quite simple. First,
    we find the middle value of our sorted dataset—in other words, the median. Now
    we look at each half of the data records as if it were a standalone dataset and
    find *their* median values. The median of the lower half is traditionally labeled
    Q1, while the median of the upper half is traditionally labeled Q3\. At this point,
    we’ve split our dataset into four parts, or *quartiles*, each of which contains
    an equal number of data values.
  prefs: []
  type: TYPE_NORMAL
- en: What does this do for us? Well, remember that a big part of what z-scores tell
    us is the *percentage of data points that have similar values*. Looking at [Figure 9-4](#gaussian_distro),
    for example, we can see that a data point with a z-score of 0.75 is (as we would
    expect) less than one standard deviation from the mean—something we know will
    be true for roughly 68.2% of all the data values in the set as a whole. By dividing
    our data into quartiles, we have started along a similar path. For example, any
    value in our dataset that is *numerically* less than the value of Q1 is, by definition,
    smaller than at least 75% of all the data values we have.
  prefs: []
  type: TYPE_NORMAL
- en: Still, what we’re *really* looking for are ways to identify potentially unusual
    values. Being smaller—or larger—than 75% of all data values is something, but
    it’s hardly *extreme*. Identifying our quartile boundaries alone won’t quite be
    enough.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we can use our Q1 and Q3 values to calculate what’s known as the
    *lower bound* and *upper bound* of our dataset. If our data’s distribution was
    secretly Gaussian, these boundaries would line up almost perfectly with the values
    found at three standard deviations below and above the mean. While of course we’re
    using them precisely because our data *isn’t* Gaussian, I make the comparison
    to illustrate that we can use them to help identify extreme values in an asymmetrically
    distributed dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Like finding the median, calculating the upper and lower bounds is actually
    quite straightforward. We start by finding a value called the *interquartile range*
    (IQR)—a fancy-sounding name for the numerical difference between the values of
    Q3 and Q1\. We then multiply that value by 1.5 and subtract it from Q1 to get
    the lower bound, and add it to Q3 to get the upper bound. That’s it!
  prefs: []
  type: TYPE_NORMAL
- en: IQR (interquartile range) = Q3 – Q1
  prefs: []
  type: TYPE_NORMAL
- en: Lower bound = Q1 – (1.5 × IQR)
  prefs: []
  type: TYPE_NORMAL
- en: Upper bound = Q3 + (1.5 × IQR)
  prefs: []
  type: TYPE_NORMAL
- en: On a Gaussian distribution, our upper and lower bound values will be about three
    standard deviations above or below the mean—but does this mean that every value
    beyond our upper and lower bounds is automatically an *outlier*? No. But finding
    these boundaries does let us narrow down where we might start *looking* for outliers.
    And just as importantly, these measures help us understand what values are *not*
    outliers, even if they might seem, numerically, to be pretty different from the
    “typical” or “expected” value provided by the median or mean.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s return to our PPP loan data. A $1 million loan seems like
    a lot, even if—as we are—you’re only looking at loans that were over $150,000
    to begin with. But is a $1 million loan truly *unusual*? This is where our measures
    of central tendency and spread—in this case, the median, quartiles, and lower
    and upper bound values—can really help us out. Let’s take a look at what our histogram
    looks like with these values added, as shown in [Example 9-3](#ppp_loan_central_and_dist),
    and see what we think.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-3\. ppp_loan_central_and_dist.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the zoom-in view of the resulting graph (shown in [Figure 9-5](#PPP_loan_distribution)),
    there’s really no support for the claim that a loan of $1 million is out of the
    ordinary; that amount falls well below the upper bound we’ve calculated for this
    dataset. So even though a loan of that amount is larger than three-quarters of
    all loans approved so far (because the $1 million mark, currently labeled as 1.0
    1e6 on the graph’s x-axis, is to the *right* of our Q3 line), it’s still not *so*
    much that any loan of $1 million is likely to be worth investigating further.
    At least, that’s probably not where we’d want to start.
  prefs: []
  type: TYPE_NORMAL
- en: '![PPP Current Loan Amount histogram with median, quartiles, and bounds in black,
    and mean plotted in gray.](assets/ppdw_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Detail of PPP Current Loan Amount histogram with median, quartiles,
    and bounds in black and mean plotted in gray
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So where should we look next for potentially interesting patterns in the data?
    *Right in front of us*, at the graph we already have. Because while we *could*
    start looking for more complex statistical measures to calculate and evaluate,
    even this basic visualization is showing some intriguing patterns in the data.
    The first one worth noting—if only to reassure ourselves about our choice of statistical
    measures—is that the mean of this dataset is at nearly the same position in the
    distribution as our Q3 value. If we had any concern about selecting the median
    over the mean as a measure of central tendency for this dataset, that fact should
    set it to rest. The other thing that we can see—in the data view shown in [Figure 9-5](#PPP_loan_distribution)
    and if we were to scroll farther to the right—is that there are curious little
    spikes in our data, indicating that a particular loan amount was approved with
    relatively high frequency.^([8](ch09.html#idm45143397217488)) Given how clearly
    these stand out from the data patterns immediately around them, we should probably
    look at those values next.
  prefs: []
  type: TYPE_NORMAL
- en: Counting “Clusters”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you’re walking down a crowded street and you notice a group of people
    gathered on the corner across from you. What do you do? On a busy avenue where
    most pedestrians are concerned with getting from one place to another, more than
    one or two people stopped in the same place at the same time is enough to signal
    that *something* is going on. Whether “something” turns out to be a busker playing
    music, a vendor of especially popular snacks, or a box full of kittens,^([9](ch09.html#idm45143397194032))
    the fact remains that our visual system is drawn to anomalies, and that’s precisely
    because deviating from a trend indicates that something at least a little bit
    out of the ordinary is going on.
  prefs: []
  type: TYPE_NORMAL
- en: This is also why visualizing data is such a valuable tool for analyzing it—our
    eyes and brain are wired to both quickly perceive patterns and to just as quickly
    notice deviations from them. Sometimes the reason for a pattern is easy to guess,
    sometimes less so. But in any predictable pattern—whether it’s the flow of people
    on a street, a bell-shaped data distribution, or one that forms a smoothly sloping
    curve—something that breaks that pattern is worth investigating.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of [Figure 9-5](#PPP_loan_distribution), we can see a range of such
    pattern violations. The first is the sharp line at the lefthand side of the graph,
    which serves as a good reminder that our dataset contains *only* approved loan
    amounts of $150,000 or more, rather than all of the loans that have been approved
    within the program. In case we had lost sight of that, the obvious and hard cutoff
    our data shows at the lefthand edge is a good reminder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bu there is also another set of pattern violations: little spikes in our histogram
    around particular points along the x-axis, at data values like $2 million. Where
    are these coming from? While we can’t say for sure, scanning our histogram reveals
    that similar spikes appear at roughly $500,000 intervals, especially as the loan
    amounts increase. To some degree, these are probably the result of a tendency
    toward “round” numbers: if you’re going to ask for $1,978,562.34, why not just
    “round it up” to $2 million? Of course, that would still be $21,437.66 more than
    maybe you need—and to most of us it is a *lot* of money. Given that PPP loans
    are intended to support specific costs, it does seem a little strange that *so*
    many loans—nearly 2,000 of them, based on our graph—would happen to work out to
    precisely $2 million.'
  prefs: []
  type: TYPE_NORMAL
- en: So what’s going on? This is where we need to do some additional research to
    effectively interpret what we’re seeing in the data. Based on my experience, my
    first step would be to look through the rules for PPP loans to see if I can work
    out why $2 million might be such a popular amount to request. For example, is
    $2 million a minimum or maximum allowed amount based on specific attributes of
    the business or what it’s requesting support for?
  prefs: []
  type: TYPE_NORMAL
- en: 'A little bit of searching around the Small Business Administration’s (SBA)
    [website](https://sba.gov/funding-programs/loans/covid-19-relief-options/paycheck-protection-program/second-draw-ppp-loan)
    seems to offer at least part of the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: For most borrowers, the maximum loan amount of a Second Draw PPP loan is 2.5x
    the average monthly 2019 or 2020 payroll costs up to $2 million. For borrowers
    in the Accommodation and Food Services sector (use NAICS 72 to confirm), the maximum
    loan amount for a Second Draw PPP loan is 3.5x the average monthly 2019 or 2020
    payroll costs up to $2 million.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Since $2 million is a ceiling for essentially *all* types of businesses applying
    for so-called second-draw (or second-round) PPP loans—including those that might
    have initially qualified for *more* money—it makes sense that the cluster of loans
    approved for *precisely* $2 million is so large.
  prefs: []
  type: TYPE_NORMAL
- en: This “answer,” of course, just leads to more questions. According to the documentation,
    $2 million was the upper limit for second-round PPP loans; first-round loans could
    be up to $10 million. If so many businesses were requesting the upper limit for
    *second*-round loans, it indicates that many businesses 1) have already received
    a first-round loan, and 2) their first-round loan may have been even *larger*
    than $2 million, since they would have had to round *down* to $2 million if they
    qualified for more than that in the first round. In other words, we might expect
    those businesses that requested *precisely* $2 million in second-round loans are
    among those that were approved for the largest total amounts of PPP loan relief.
    And of course, if they *did* get some of the largest pots of money, we (and probably
    a lot of other people!) certainly want to know about it.
  prefs: []
  type: TYPE_NORMAL
- en: The $2 Million Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to understand what (if any) shared characteristics there may be among
    companies that requested $2 million for their second-round PPP loans, we first
    have to effectively isolate their records within our dataset. How might we do
    this? Well, we know that we’re interested in companies approved for more than
    one loan, which means that their `BorrowerName` *should* appear more than once
    in our data. We *also* know that no second-round loans were issued before January
    13, 2021\. By combining these two insights, we can probably use our data wrangling
    skills to do a decent job of identifying the companies that requested precisely
    $2 million for their second-round loan.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to accomplish this, we’ll do a couple of key transformations on our
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll create a new column for each loan, containing the label `first_round`,
    or `maybe_second`, based on whether it was issued before January 13, 2021\. While
    we can’t be sure that all loans after that date were “second round,” we *can*
    be sure that all loans *before* that date were “first round.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look for duplicate entries in our dataset. Each approved loan creates a separate
    record, so if the same business was approved for two loans, that means its information
    would appear twice in the records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The logic here is that if we find a given business name twice in our data *and*
    those records have different “round” labels, it probably indicates a business
    that has, in fact, been approved for two separate loans.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we’re going to call in the help of some Python libraries to get this
    work done. We’ll need to use *pandas*, as usual, but we’re also going to use another
    library called *numpy* that has lots of useful array/list functions (*pandas*
    actually relies heavily on *numpy* under the hood). I’m also going to pull in
    *seaborn* and *matplotlib* again so that we have the option of generating visualizations
    to help us evaluate our evolving dataset as we go along.
  prefs: []
  type: TYPE_NORMAL
- en: Although what we’re trying to do with this data is conceptually pretty straightforward,
    the wrangling involved in performing this analysis takes a fair number of steps,
    as you can see in [Example 9-4](#who_got_2_loans_by_date).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-4\. who_got_2_loans_by_date.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This file was generated by running our fingerprinting process on `BorrowerName`,
    as described in [“Finding a Fingerprint”](#finding_a_fingerprint).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_data_analysis_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We want to know which loans were approved *before* January 13, 2021\. The fastest
    way to do this will be to convert our `DateApproved` strings to “real” dates and
    compare them to that.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introduction_to_data_analysis_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The pandas `cut()` function lets us create a new column by applying boundaries
    and labels to an existing one. In this case, we label each record according to
    whether it was approved before or after January 13, 2021.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introduction_to_data_analysis_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We do this for convenience so we can use the `describe()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_introduction_to_data_analysis_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: We expect the maximum value in this table to be `2`, since no business is allowed
    to get more than two loans under the PPP.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the code from [Example 9-4](#who_got_2_loans_by_date) and nothing
    happens for a minute, don’t despair. On my Chromebook, this script takes about
    40 to 90 seconds to execute (depending on how many other Linux apps I’m running
    alongside).^([10](ch09.html#idm45143397083520)) When it’s finished, however, your
    output will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From this first effort something seems…off. The output from our `.describe()`
    command gives us a quick way of getting almost all the summary statistics we’re
    interested in (the Q1, median, and Q3 are labeled here according to the percentage
    of values that would appear to their left on a histogram—so 25%, 50%, and 75%,
    respectively). These values suggest that fewer than 25% of all businesses have
    received more than one loan (otherwise the 75% value would be greater than 1),
    which makes sense. But the max value is troubling, since the PPP rules don’t appear
    to allow a single business to receive more than two loans, much less 12! Let’s
    take a closer look by adding the code shown in [Example 9-5](#who_got_2_loans_by_date_contd)
    to what we wrote in [Example 9-4](#who_got_2_loans_by_date) and see what we find.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-5\. who_got_2_loans_by_date.py (continued)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we get the additional output shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This suggests that there are only a (relative) handful of businesses that may
    have been approved for more than two loans, and we can probably attribute those
    cases to a combination of our chosen fingerprinting approach (a combination of
    `BorrowerName`, `BorrowerCity`, and `BorrowerState`) along with the possibility
    that there are multiple instances of a single franchise in the same city that
    applied for PPP funds.^([11](ch09.html#idm45143396792864)) In any case, there
    are few enough of them that they are unlikely to change the outcome of our analysis
    considerably, so we won’t focus on tracking down their details right now. At least
    the second piece of output showing that 72,060 individual businesses got *exactly*
    two loans seems reasonable so far, since this is definitely less than 25% of our
    total dataset, and therefore aligns with the summary statistics we got from our
    `Loan Count` DataFrame (because the value of Q3 was still 1, meaning that fewer
    than 25% of all business names appeared in our dataset more than once).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this is still just an estimate; it would be much better if we had
    a more official count of second-round loans to work with. As noted at the end
    of [Chapter 6](ch06.html#chapter6), the Small Business Administration *did* actually
    release [an official data dictionary](https://data.sba.gov/dataset/ppp-foia/resource/aab8e9f9-36d1-42e1-b3ba-e59c79f1d7f0)
    for the PPP loan data, and while it doesn’t contain all of the information we
    might hope, it *does* indicate that the `ProcessingMethod` field distinguishes
    between first-round (`PPP`) and second-round (`PPS`) loans. Let’s look at our
    data this way and compare it to our name-matching-based estimate by adding the
    code in [Example 9-6](#who_got_2_loans_by_date_contd2) further down in our file.
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-6\. who_got_2_loans_by_date.py (continued again)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Rerunning our script yields the additional output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Wow! Even with our possibly too-lax fingerprinting method, we still failed to
    find more than 300,000 businesses with both of their loans. What do we do?
  prefs: []
  type: TYPE_NORMAL
- en: First of all, recognize that this isn’t even an unusual situation. We’re dealing
    with around 750,000 data records, each one of which is a combination of data entry
    done by multiple individuals, including the borrower, the lender, and possibly
    the SBA. The fact that there are still so many discrepancies is not really surprising
    (I illustrate some of them in the following sidebar), but all is not lost. Remember
    that our original interest was in those businesses that got precisely $2 million
    for their second-round loan, which is likely to be just a fraction of all the
    businesses that got two loans. We can still move ahead with that part of the analysis
    in order to (1) test how effective our date-based estimate of second-round loans
    was, and (2) see what we can learn about that specific subset of businesses that
    got exactly $2 million in the second round.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’re going to use the information from the `PaymentProcessingMethod`
    column to *validate* our earlier work using name-matching and date-based loan
    round estimates. To do this, we’re going to merge our `Loan Count` DataFrame back
    onto our original dataset. Then we’ll select only the $2M loans that we *estimate*,
    based on their date, were second-round loans. Finally, we’ll compare that number
    of loans with the number of $2M loans we *know* were second draw, based on their
    `ProcessingMethod` value of `PPS`. Obviously, this will mean adding yet more code
    to our file, as shown in [Example 9-7](#who_got_2_loans_by_date_contd3).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-7\. who_got_2_loans_by_date.py (continued more)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding this code to our main files give us another few lines of output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If we compare these results to previous ones, it looks like we’re doing a bit
    better. Across *all* loans, we appear to have matched up 72,060 out of 103,949
    actual second-round loans, or about 70%. For those organizations approved for
    $2M in second-round loans, we’ve found 1,115 out of 1,459, or about 80%.
  prefs: []
  type: TYPE_NORMAL
- en: So what can we say about businesses that got $2M in the second round? We can’t
    say anything with 100% confidence unless and until we find matches for those 284
    companies whose `BorrowerNameFingerprint` isn’t the same between their first-
    and second-round loans. But we can still look at our 80% sample and see what we
    discover. To do this, I’m going to take the following steps:^([13](ch09.html#idm45143396564848))
  prefs: []
  type: TYPE_NORMAL
- en: Find all the unique `BorrowerNameFingerprint` values for businesses that definitely
    got $2M second-round loans.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a DataFrame (`biz_names_df`) based on this list and fill it out with
    the flag value `2Mil2ndRnd`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge that DataFrame back onto my dataset and use the flag value to pull *all*
    loan records for those businesses (both first and second round).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do some basic analyses of how much money those businesses were approved for
    across both rounds, and visualize those amounts, comparing the official second-round
    designation (that is, `ProcessingMethod == 'PPS'`) with our derived, date-based
    category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And of course, now that I’ve written out in a list the steps my script should
    take (this is exactly the kind of thing that you’d want to put in your data diary
    and/or program outline), it’s just a matter of coding it up below our existing
    work; for clarity I’ve put it into a second script file, the complete code of
    which is shown in [Example 9-8](#who_got_2M_with_viz).
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-8\. who_got_2M_with_viz.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this script will give us all the output from previous examples, along
    with yet a few more lines of additional output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'At first, it looks like something’s off, because we might have expected our
    total number of loans to be 2 × 1,175 = 2,350\. But remember that we matched up
    loans based on whether they got approved for *exactly* $2M in round two, *and*
    we failed to match 284 loans on `BorrowerNameFingerprint`. This means we have
    *all* second-round loans but are missing 284 first-round loans in these numbers.
    In other words, we’d *expect* to have (2 × 1,175) + 284 = 2,634—and we do! Good!
    It’s always nice when *something* matches up. This means that our “total” figure,
    while still not 100% accurate, is a somewhat reasonable estimate of the *minimum*
    total loan amount this group of businesses were approved for in PPP funds: around
    $6 billion dollars.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s take a look at the visualization shown in [Figure 9-6](#two_loan_business_amounts_by_round),
    which is a view of the graphic generated by the script in which we can compare
    how our `Loan Round` classification matches up against the designated `PPS` loans.
    This is a rough (but still useful) way to validate our work—and the results look
    pretty good!^([14](ch09.html#idm45143396503936))
  prefs: []
  type: TYPE_NORMAL
- en: '![Dollar amount of most approved loans for businesses that received two PPP
    loans, by loan round.](assets/ppdw_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. Dollar amount of most approved loans for businesses that received
    two PPP loans, by loan round
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Interestingly, though, [Figure 9-6](#two_loan_business_amounts_by_round) also
    illustrates something else: it seems that a fair number of the businesses approved
    for $2M in second-round loans violate our earlier hypothesis that companies approved
    for $2M in second-round loans were approved for *more* than that amount in their
    first-round loans, when the limits were higher. As usual, in answering one question,
    we’ve generated another! And of course the work we’ve already done would give
    us a head start down the road to answering it. Before we let loose on our next
    round of question-and-answer, though, we need to talk about one more essential
    component of data analysis and interpretation: *proportionality*.'
  prefs: []
  type: TYPE_NORMAL
- en: Proportional Response
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you go out to eat with some friends. You’ve eaten recently, so you just
    order a drink, but your three friends are ravenous and each order a full meal.
    How do you decide who owes what when the check arrives? Most of us would agree
    that the most sensible thing to do would be to calculate—or at least estimate—what
    *proportion* of the total bill each person’s order accounted for, and then have
    each person pay that, along with that same proportion of, say, the tax and tip.
  prefs: []
  type: TYPE_NORMAL
- en: The same sort of logic applies when we’re analyzing data. In [“The $2 Million
    Question”](#two_million_question), we looked at the *total* funds that had been
    approved for a certain subset of businesses through the PPP, and while $6B sounds
    like a lot, we should arguably be more interested in how those businesses *used*
    that money, rather than the absolute number of dollars they got. Since the PPP
    was designed to keep people on the payroll, one thing we might want to know is
    how much money those businesses received *in relation to how many jobs they preserved*,
    a process I think of as *rationalizing* the data.^([15](ch09.html#idm45143395830240))
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the process of rationalizing our data is extremely simple: we
    calculate the *ratio* between two data values by dividing one number by the other.
    For example, if we want to know how many dollars per job the companies identified
    in [“The $2 Million Question”](#two_million_question) spent, we can (after some
    sanity checking) divide the value in `PAYROLL_PROCEED` by the value in `JobsReported`
    for each record, as shown in [Example 9-9](#dollars_per_job_2M_rnd2).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 9-9\. dollars_per_job_2M_rnd2.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_data_analysis_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that a couple of businesses didn’t report *any* jobs, which will
    break our calculation. Since there are only two records guilty of this, we’ll
    just drop them, using their *pandas*-assigned row labels.
  prefs: []
  type: TYPE_NORMAL
- en: While the text output here confirms that we’re looking at the same set of loans
    that we examined in [“The $2 Million Question”](#two_million_question), our rationalized
    data highlights some noteworthy anomalies in some of the first-round loans, where
    a handful of companies appear to have had loans approved that allocated more for
    payroll than the $100,000-per-job limit allowed, as shown in [Figure 9-7](#dollars_per_job).
  prefs: []
  type: TYPE_NORMAL
- en: '![Detail of dollars per job of companies approved for $2M in second-round loans](assets/ppdw_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Detail of dollars per job of companies approved for $2M in second-round
    loans
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What do we make of this? You may notice that by now we’ve veered a rather long
    way from the question we posed back in [“The Pandemic and the PPP”](ch06.html#pandemic_and_ppp),
    where we started out trying to assess whether the PPP had helped “save” American
    businesses. While that focus helped us work our way through our data quality evaluations,
    doing some contextual analysis has opened up a number of new questions and directions—which
    I think you’ll find is a pretty common occurrence when it comes to data wrangling.
    Hopefully, though, this will just encourage you to keep working with new datasets
    to see what else you can find!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After all these analyses, we’ve learned a few new things—some of which are
    specific to this dataset but many of which are far more generally applicable:'
  prefs: []
  type: TYPE_NORMAL
- en: A relatively small number of companies were approved for the maximum allowable
    second-round amount in the PPP loan program. While many of them had filed for
    much more than that for their first-round loan, some did not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A handful of companies that were approved for a $2M second-round loan claimed
    more than $100,000 per reported job in their first-round loan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human-entered data is always a mess. That’s why data cleaning is an ongoing,
    iterative process. Documenting your work is essential to being able to defend
    your results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, our introductory data analysis left us with far more questions than answers.
    At this point, there’s only one way we’re going to find out more: talking to people.
    Sure, some of the patterns we uncovered [look questionable](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3906395),
    but we have far too many unknowns to make any well-supported claims at this point.
    For example, many of the $2M second-round loans had yet to be disbursed when this
    data was released, so some companies might have actually gotten or used far less
    than that. Since the PPP rules only require that a minimum percentage of a loan
    is spent on payroll in order to be forgivable, companies that appear to have been
    approved for too much may have simply used the difference on other allowable expenses,
    like mortgage interest or health care costs. In other words, while we can learn
    a little bit from this type of numerical data analysis, it will never be enough
    to tell us the whole story—either the how or the why. That is something for which
    we need direct human input.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve done that work, and are clear about what insights we want to share,
    we are ready to begin thinking about the most effective way to convey what we’ve
    learned to others. Just as our data analysis relies on both data *and* human judgment
    and input, the most effective data communications almost always involve a balance
    between words and visualizations. As we’ll see in the next chapter, by crafting
    both our words *and* our visualizations with care, we can better ensure that our
    intended message truly gets heard.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.html#idm45143400237136-marker)) See [*Predictably Irrational*](https://danariely.com/books/predictably-irrational/)
    by Dan Ariely (Harper) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.html#idm45143398320448-marker)) You can technically also sort from
    highest to lowest, but starting with lower values is conventional and will make
    things easier in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch09.html#idm45143399041152-marker)) There are actually multiple methods
    of choosing the median value for an even number of data points, but as long as
    you’re consistent, any of them is fine. Anecdotally, this is the approach that
    feels most intuitive and that I’ve seen used most often.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch09.html#idm45143398988128-marker)) While the precise estimates for the
    number of items that humans can hold in *working memory* differ, researchers *do*
    agree that this capacity has limits. See [*https://ncbi.nlm.nih.gov/pmc/articles/PMC2864034*](https://ncbi.nlm.nih.gov/pmc/articles/PMC2864034)
    and [*https://pnas.org/content/113/27/7459*](https://pnas.org/content/113/27/7459).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch09.html#idm45143398983968-marker)) Although there is a long way to go,
    there is some [exciting research being done on tactile graphics](https://dl.acm.org/doi/abs/10.1145/3373625.3418027)
    to reduce the vision dependency of these approaches, especially for folks who
    are blind or visually impaired. See [*http://shape.stanford.edu/research/constructiveVizAccess/assets20-88.pdf*](http://shape.stanford.edu/research/constructiveVizAccess/assets20-88.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch09.html#idm45143397925760-marker)) See “A Morphometric Analysis of Ddt-Resistant
    and Non-Resistant House Fly Strains” by Robert R. Sokal and Preston E. Hunter,
    [*https://doi.org/10.1093/aesa/48.6.499*](https://doi.org/10.1093/aesa/48.6.499);
    the relevant data is provided there.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch09.html#idm45143397906944-marker)) M. W. Toews, “CC BY 2.5,” [*https://creativecommons.org/licenses/by/2.5*](https://creativecommons.org/licenses/by/2.5),
    via [Wikimedia Commons](https://en.wikipedia.org/wiki/Normal_distribution#/media/File:Standard_deviation_diagram.svg).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch09.html#idm45143397217488-marker)) Specifically relative to the loan
    amounts just above or below these values.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch09.html#idm45143397194032-marker)) It happens.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch09.html#idm45143397083520-marker)) If it’s *too* many, the output will
    say `Killed`. This is a sign you either need to close some apps or maybe move
    into the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch09.html#idm45143396792864-marker)) See [*https://sba.gov/document/support-faq-ppp-borrowers-lenders*](https://sba.gov/document/support-faq-ppp-borrowers-lenders)
    and [*https://sba.gov/document/support-sba-franchise-directory*](https://sba.gov/document/support-sba-franchise-directory)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch09.html#idm45143396754128-marker)) Though you can find it in the file
    *ppp_fingerprint_borrowers.py*.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch09.html#idm45143396564848-marker)) Note that I’m intentionally doing
    this in a *slightly* roundabout way in order to demonstrate a few more data-wrangling
    and visualization strategies, but feel free to rework this code to be more efficient
    as an exercise!
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch09.html#idm45143396503936-marker)) If we compare the results numerically,
    we’ll find they’re identical, at least for our subset of companies approved for
    $2M in the second round.
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch09.html#idm45143395830240-marker)) This term has more specific meanings
    in the business and statistics/data-science worlds, but *proportionalizing* just
    sounds kind of awkward. Plus, it better matches the actual calculation process!
  prefs: []
  type: TYPE_NORMAL
