- en: Chapter 16\. Model Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far when we fit models, we have used a few strategies to decide which features
    to include:'
  prefs: []
  type: TYPE_NORMAL
- en: Assess model fit with residual plots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect the statistical model to a physical model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep the model simple.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare improvements in the standard deviation of the residuals and in the MSE
    between increasingly complex models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, when we examined the one-variable model of upward mobility in [Chapter 15](ch15.html#ch-linear),
    we found curvature in the residual plot. Adding a second variable greatly improved
    the fit in terms of average loss (MSE and, relatedly, multiple <math><msup><mi>R</mi>
    <mn>2</mn></msup></math> ), but some curvature remained in the residuals. A seven-variable
    model made little improvement over the two-variable model in terms of a decrease
    in MSE, so although the two-variable model still showed some patterns in the residuals,
    we opted for this simpler model.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, when we model the weight of a donkey in [Chapter 18](ch18.html#ch-donkey),
    we will take guidance from a physical model. We’ll ignore the donkey’s appendages
    and draw on the similarity between a barrel and a donkey’s body to begin fitting
    a model that explains weight by its length and girth (comparable to a barrel’s
    height and circumference). We’ll then continue to adjust that model by adding
    categorical features related to the donkey’s physical condition and age, collapsing
    categories, and excluding other possible features to keep the model simple.
  prefs: []
  type: TYPE_NORMAL
- en: The decisions we make in building these models are based on judgment calls,
    and in this chapter we augment these with more formal criteria. To begin, we provide
    an example that shows why it’s typically not a good idea to include too many features
    in a model. This phenomenon, called *overfitting*, often leads to models that
    follow the data too closely and capture some of the noise in the data. Then, when
    new observations come along, the predictions are worse than those from a simpler
    model. The remainder of the chapter provides techniques, such as the train-test
    split, cross-validation, and regularization, for limiting the impact of overfitting.
    These techniques are especially helpful when there are a large number of potential
    features to include in a model. We also provide a synthetic example, where we
    know the true model, to explain the concepts of model variance and bias and how
    they relate to over- and underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we have many features available to include in a model, choosing which ones
    to include or exclude rapidly gets complicated. In the upward mobility example
    in [Chapter 15](ch15.html#ch-linear), we chose two of the seven variables to fit
    the model, but there are 21 pairs of features that we could have examined and
    fitted for a two-variable model. And there are over one hundred models to choose
    from if we consider all one-, two-, …, seven-variable models. It can be hard to
    examine hundreds of residual plots to decide how simple is simple enough, and
    to settle on a model. Unfortunately, the notion of minimizing MSE isn’t entirely
    helpful either. With each variable that we add to a model, the MSE typically gets
    smaller. Recall from the geometric perspective of model fitting ([Chapter 15](ch15.html#ch-linear))
    that adding a feature to a model adds an <math><mi>n</mi></math> -dimensional
    vector to the feature space, and the error between the outcome vector and its
    projection into the space spanned by the explanatory variables shrinks. We might
    view this as a good thing because our model fits the data more closely, but there
    is a danger in overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting happens when the model follows the data too closely and picks up
    the variability in the random noise in the outcome. When this happens, new observations
    are not well-predicted. An example helps clarify this idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Energy Consumption'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we examine a [dataset you can download](https://oreil.ly/ngD4G)
    that contains information from utility bills for a private residence in Minnesota.
    We have records of the monthly gas usage in a home (cubic feet) and the average
    temperature that month (degrees Fahrenheit).^([1](ch16.html#id1643)) We first
    read in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|   | temp | ccf |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 29 | 166 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 31 | 179 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 15 | 224 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **96** | 76 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| **97** | 55 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| **98** | 39 | 91 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will begin by looking at a scatterplot of gas consumption as a function
    of temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in01.png)'
  prefs: []
  type: TYPE_IMG
- en: The relationship shows curvature (left plot), but when we try to straighten
    it with a log transformation (right plot), a different curvature arises in the
    low-temperature region. Additionally, there are two unusual points. When we refer
    back to the documentation, we find that these points represent recording errors,
    so we remove them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if a quadratic curve can capture the relationship between gas usage
    and temperature. Polynomials are still considered linear models. They are linear
    in their polynomial features. For example, we can express a quadratic model as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub> <msup><mi>x</mi>
    <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is linear in the features <math><mi>x</mi></math> and <math><msup><mi>x</mi>
    <mn>2</mn></msup></math> , and in matrix notation we can write this model as <math><mrow><mtext
    mathvariant="bold">X</mtext></mrow> <mrow><mi mathvariant="bold-italic">θ</mi></mrow></math>
    , where <math><mtext mathvariant="bold">X</mtext></math> is the design matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mrow><mo>⌈</mo> <mtable columnalign="center" columnspacing="1em"
    rowspacing="4pt"><mtr><mtd><mn>1</mn></mtd> <mtd><msub><mi>x</mi> <mn>1</mn></msub></mtd>
    <mtd><msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><msub><mi>x</mi> <mn>2</mn></msub></mtd> <mtd><msubsup><mi>x</mi> <mn>2</mn>
    <mn>2</mn></msubsup></mtd></mtr> <mtr><mtd><mrow><mo>⋮</mo></mrow></mtd> <mtd><mrow><mo>⋮</mo></mrow></mtd>
    <mtd><mrow><mo>⋮</mo></mrow></mtd></mtr> <mtr><mtd><mn>1</mn></mtd> <mtd><msub><mi>x</mi>
    <mi>n</mi></msub></mtd> <mtd><msubsup><mi>x</mi> <mi>n</mi> <mn>2</mn></msubsup></mtd></mtr></mtable>
    <mo>⌉</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create the polynomial features of the design matrix with the `Polynomial​Fea⁠tures`
    tool in `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the parameter `include_bias` to `False` because we plan to fit the polynomial
    with the `LinearRegression` method in `scikit-learn`, and by default it includes
    the constant term in the model. We fit the polynomial with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a quick idea as to the quality of the fit, let’s overlay the fitted
    quadratic on the scatterplot and also look at the residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in02.png)'
  prefs: []
  type: TYPE_IMG
- en: The quadratic captures the curve in the data quite well, but the residuals show
    a slight upward trend in the temperature range of 70°F to 80°F, which indicates
    some lack of fit. There is also some funneling in the residuals, where the variability
    in gas consumption is greater in the colder months. We might expect this behavior
    since we have only the monthly average temperature.
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, we fit a few more models with higher-degree polynomials and
    collectively examine the fitted curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We use the polynomial features in this section to demonstrate over-fitting,
    but directly fitting the <math><mi>x</mi> <mo>,</mo> <msup><mi>x</mi> <mn>2</mn></msup>
    <mo>,</mo> <msup><mi>x</mi> <mn>3</mn></msup> <mo>,</mo> <mo>…</mo></math> polynomials
    is not advisable in practice. Unfortunately, these polynomial features tend to
    be highly correlated. For example, the correlation between <math><mi>x</mi></math>
    and <math><msup><mi>x</mi> <mn>2</mn></msup></math> for the energy data is 0.98\.
    Highly correlated features give unstable coefficients, where a small change in
    an x-value can lead to a large change in the coefficients of the polynomial. Also,
    when the x-values are large, the normal equations are poorly conditioned and the
    coefficients can be difficult to interpret and compare.
  prefs: []
  type: TYPE_NORMAL
- en: A better practice is to use polynomials that have been constructed to be orthogonal
    to one another. These polynomials fill the same space as the original polynomials,
    but they are uncorrelated with one another and give a more stable fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s place all of the polynomial fits on the same graph so that we can see
    how the higher-degree polynomials bend more and more strangely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also visualize the different polynomial fits in separate facets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in04.png)'
  prefs: []
  type: TYPE_IMG
- en: The degree 1 curve (the straight line) in the upper-left facet misses the curved
    pattern in the data. The degree 2 curve begins to capture it, and the degree 3
    curve looks like an improvement, but notice the upward bend at the right side
    of the plot. The polynomials of degrees 6, 8, and 12 follow the data increasingly
    closely, as they get increasingly curvy. These polynomials seem to fit spurious
    bumps in the data. Altogether, these six curves illustrate under- and overfitting.
    The fitted line in the upper left underfits and misses the curvature entirely.
    And the degree 12 polynomial in the bottom right definitely overfits with a wiggly
    pattern that we don’t think makes sense in this context.
  prefs: []
  type: TYPE_NORMAL
- en: In general, as we add more features, models get more complex and the MSE drops,
    but at the same time, the fitted model grows increasingly erratic and sensitive
    to the data. When we overfit, the model follows the data too closely, and predictions
    are poor for new observations. One simple technique to assess a fitted model is
    to compute the MSE on new data, data that were not used in building the model.
    Since we don’t typically have the capacity to acquire more data, we set aside
    some of the original data to evaluate the fitted model. This technique is the
    topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Train-Test Split
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we want to use all of our data in building a model, we also want to
    get a sense of how the model behaves with new data. We often do not have the luxury
    of collecting additional data to assess a model, so instead we set aside a portion
    of our data, called the *test set*, to stand in for new data. The remainder of
    the data is called the *train set*, and we use this portion to build the model.
    Then, after we have chosen a model, we pull out the test set and see how well
    the model (fitted on the train set) predicts the outcomes in the test set. [Figure 16-1](#train-test-diagram)
    demonstrates this idea.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-1\. The train-test split divides the data into two parts: the train
    set is used to build the model and the test set evaluates that model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Typically, the test set consists of 10% to 25% of the data. What might not be
    clear from the diagram is that this division into two parts is often made at random,
    so the train and test sets are similar to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can describe this process using the notion introduced in [Chapter 15](ch15.html#ch-linear).
    The design matrix, <math><mtext mathvariant="bold">X</mtext></math> , and outcome,
    <math><mrow><mi mathvariant="bold">y</mi></mrow></math> , are each divided into
    two parts; the design matrix, labeled <math><msub><mtext mathvariant="bold">X</mtext>
    <mi>T</mi></msub></math> , and corresponding outcomes, <math><msub><mrow><mi mathvariant="bold">y</mi></mrow>
    <mi>T</mi></msub></math> , form the train set. We minimize the average squared
    loss over <math><mi mathvariant="bold-italic">θ</mi></math> with these data:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><munder><mo movablelimits="true">min</mo> <mrow><mi mathvariant="bold-italic">θ</mi></mrow></munder>
    <mo fence="false" stretchy="false">‖</mo> <msub><mrow><mi mathvariant="bold">y</mi></mrow>
    <mi>T</mi></msub> <mo>−</mo> <mrow><msub><mtext mathvariant="bold">X</mtext> <mi>T</mi></msub></mrow>
    <mrow><mrow><mi mathvariant="bold-italic">θ</mi></mrow></mrow> <msup><mo fence="false"
    stretchy="false">‖</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The coefficient, <math><msub><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow> <mi>T</mi></msub></math>
    , that minimizes the training error is used to predict outcomes for the test set,
    which is labeled <math><msub><mtext mathvariant="bold">X</mtext> <mi>S</mi></msub></math>
    and <math><msub><mrow><mi mathvariant="bold">y</mi></mrow> <mi>S</mi></msub></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mo fence="false" stretchy="false">‖</mo> <msub><mrow><mi
    mathvariant="bold">y</mi></mrow> <mi>S</mi></msub> <mo>−</mo> <mrow><msub><mtext
    mathvariant="bold">X</mtext> <mi>S</mi></msub></mrow> <mrow><msub><mrow><mover><mi
    mathvariant="bold-italic">θ</mi> <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow>
    <mi>T</mi></msub></mrow> <msup><mo fence="false" stretchy="false">‖</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: Since <math><msub><mtext mathvariant="bold">X</mtext> <mi>S</mi></msub></math>
    and <math><msub><mrow><mi mathvariant="bold">y</mi></mrow> <mi>S</mi></msub></math>
    are not used to build the model, they give a reasonable estimate of the loss we
    might expect for a new observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We demonstrate the train-test split with our polynomial model for gas consumption
    from the previous section. To do this, we carry out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data at random into two parts, the train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit several polynomial models to the train set and choose one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the MSE on the test set for the chosen polynomial (with coefficients
    fitted on the train set).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the first step, we divide the data with the `train_test_split` method in
    `scikit-learn` and set aside 22 observations for model evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous section, we fit models of gas consumption to various polynomials
    in temperature. But this time, we use only the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We find the MSE for each of these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the change in MSE, we plot MSE for each fitted polynomial against
    its degree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_16in05.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the training error decreases with the additional model complexity.
    We saw earlier that the higher-order polynomials showed a wiggly behavior that
    we don’t think reflects the underlying structure in the data. With this in mind,
    we might choose a model that is simpler but shows a large reduction in MSE. That
    could be degree 3, 4, or 5\. Let’s go with degree 3 since the difference between
    these three models in terms of MSE is quite small and it’s the simplest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have chosen our model, we provide an independent assessment of
    its MSE using the test set. We prepare the design matrix for the test set and
    use the degree 3 polynomial fitted on the train set to predict the outcome for
    each row in the test set. Lastly, we compute the MSE for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The MSE for this model is quite a bit larger than the MSE computed on the training
    data. This demonstrates the problem with using the same data to fit and evaluate
    a model: the MSE doesn’t adequately reflect the MSE for a new observation. To
    further demonstrate the problem with overfitting, we compute the error for the
    test for each of these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, we do not look at the test set until we have committed to a model.
    Alternating between fitting a model on the train set and evaluating it on the
    test set can lead to overfitting. But for demonstration purposes, we plot the
    MSE on the test set for all of the polynomial models we fitted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in06.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the MSE for the test set is larger than the MSE for the train set
    for all models, not just the model that we selected. More importantly, notice
    how the MSE for the test set initially decreases as the model goes from underfitting
    to one that follows the curvature in the data a bit better. Then, as the model
    grows in complexity, the MSE for the test set increases. These more complex models
    overfit the training data and lead to large errors in predicting the test set.
    An idealization of this phenomenon is captured in the diagram in [Figure 16-2](#train-test-overfit).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. As the model grows in complexity, the train set error shrinks
    and the test set error increases
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The test data provides an assessment of the prediction error for new observations.
    It is crucial to use the test set only once, after we have committed to a model.
    Otherwise, we fall into the trap of using the same data to choose and evaluate
    the model. When choosing the model, we fell back on the simplicity argument because
    we were aware that increasingly complex models tend to overfit. However, we can
    extend the train-test method to help select the model as well. This is the topic
    of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use the train-test paradigm to help us choose a model. The idea is to
    further divide the train set into separate parts where we fit the model on one
    part and evaluate it on another. This approach is called *cross-validation*. We
    describe one version, called <math><mi>k</mi></math> *-fold cross-validation*.
    [Figure 16-3](#cvdiagram) shows the idea behind this division of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. An example of fivefold cross-validation in which the train set
    is divided into five parts that are used in turn to validate models built on the
    remainder of the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Cross-validation can help select the general form of a model. By this we mean
    the degree of the polynomial, the number of features in the model, or a cutoff
    for a regularization penalty (covered in the next section). The basic steps behind
    <math><mi>k</mi></math> -fold cross-validation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the train set into <math><mi>k</mi></math> parts of roughly the same
    size; each part is called a *fold*. Use the same technique that was used to create
    the train and test sets to make the folds. Typically, we divide the data at random.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set one fold aside to act as a test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit all models on the remainder of the training data (the training data less
    the particular fold).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the fold you set aside to evaluate all of these models.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat this process for a total of <math><mi>k</mi></math> times, where each
    time you set aside one fold, use the rest of the train set to fit the models,
    and evaluate them on the fold that was set aside.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the error in fitting each model across the folds, and choose the model
    with the smallest error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These fitted models will not have identical coefficients across folds. As an
    example, when we fit a polynomial of, say, degree 3, we average the MSE across
    the <math><mi>k</mi></math> folds to get an average MSE for the <math><mi>k</mi></math>
    fitted polynomials of degree 3\. We then compare the MSEs and choose the degree
    of the polynomial with the lowest MSE. The actual coefficients for the <math><mi>x</mi></math>
    , <math><msup><mi>x</mi> <mn>2</mn></msup></math> , and <math><msup><mi>x</mi>
    <mn>3</mn></msup></math> terms in the cubic polynomial are not the same in each
    of the <math><mi>k</mi></math> fits. Once the polynomial degree is selected, we
    refit the model using all of the training data and evaluate it on the test set.
    (We haven’t used the test set in any of the earlier steps to select the model.)
  prefs: []
  type: TYPE_NORMAL
- en: Typically, we use 5 or 10 folds. Another popular choice puts one observation
    in each fold. This special case is called *leave-one-out cross-validation*. Its
    popularity stems from the simplicity in adjusting a least squares fit to have
    one fewer observation.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, <math><mi>k</mi></math> -fold cross-validation takes some computation
    time since we typically have to refit each model from scratch for each fold. The
    `scikit-learn` library provides a convenient [`sklearn.model_selection.KFold`](https://oreil.ly/tnHTv)
    class to implement <math><mi>k</mi></math> -fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: To give you an idea of how k-fold cross-validation works, we’ll demonstrate
    the technique on the gas consumption example. However, this time we’ll fit a different
    type of model. In the original scatterplot of the data, it looks like the points
    fall along two connected line segments. In cold temperatures, the relationship
    between gas consumption and temperature looks roughly linear with a negative slope
    of about <math><mo>−</mo> <mn>4</mn></math> cubic ft/degree, and in warmer months,
    the relationship appears nearly flat. So, rather than fitting a polynomial, we
    can fit a bent line to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by fitting a line with a bend at 65 degrees. To do this, we create
    a feature that enables the points with temperatures above 65°F to have a different
    slope. The model is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>y</mi> <mo>=</mo> <msub><mi>θ</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>θ</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>θ</mi>
    <mn>2</mn></msub> <mo stretchy="false">(</mo> <mi>x</mi> <mo>−</mo> <mn>65</mn>
    <msup><mo stretchy="false">)</mo> <mo>+</mo></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, <math><mo stretchy="false">(</mo>  <msup><mo stretchy="false">)</mo>
    <mo>+</mo></msup></math> stands for “positive part,” so when <math><mi>x</mi></math>
    is less than 65 it evaluates to 0, and when <math><mi>x</mi></math> is 65 or greater
    it is just <math><mi>x</mi> <mo>−</mo> <mn>65</mn></math> . We create this new
    feature and add it to the design matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we fit the model with these two features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s overlay this fitted “curve” on the scatterplot to see how well it captures
    the shape of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This model appears to fit the data much better than a polynomial. But many
    bent line models are possible. The line might bend at 55 degrees or 60 degrees,
    and so on. We can use <math><mi>k</mi></math> -fold cross-validation to choose
    the temperature value at which the line bends. Let’s consider models with bends
    at <math><mn>40</mn> <mo>,</mo> <mn>41</mn> <mo>,</mo> <mn>42</mn> <mo>,</mo>
    <mo>…</mo> <mo>,</mo> <mn>68</mn> <mo>,</mo> <mn>69</mn></math> degrees. For each
    of these, we need to create the additional feature to enable the line to bend
    there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '|   | temp | ccf | temp40p | temp41p | ... | temp66p | temp67p | temp68p |
    temp69p |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 29 | 166 | 0 | 0 | ... | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 31 | 179 | 0 | 0 | ... | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 15 | 224 | 0 | 0 | ... | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **96** | 76 | 11 | 36 | 35 | ... | 10 | 9 | 8 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| **97** | 55 | 32 | 15 | 14 | ... | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **98** | 39 | 91 | 0 | 0 | ... | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step in cross-validation is to create our train and test sets. Like
    before, we choose 22 observations at random to be placed in the test set. That
    leaves 75 for the train set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can divide the train set into folds. We use three folds so that we have
    25 observations in each fold. For each fold, we fit 30 models, one for each bend
    in the line. For this step, we divide the data with the `KFold` method in `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we find the mean validation error across the three folds and plot them
    against the location of the bend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_16in08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The MSE looks quite flat for 57 to 60 degrees. The minimum occurs at 58, so
    we choose that model. To assess this model on the test set, we first fit the bent
    line model at 58 degrees on the entire train set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use the fitted model to predict gas consumption for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s overlay the bent-line fit on the scatterplot and examine the residuals
    to get an idea as to the quality of the fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in09.png)'
  prefs: []
  type: TYPE_IMG
- en: The fitted curve looks reasonable, and the residuals are much smaller than those
    from the polynomial fit.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For teaching purposes in this section, we use `KFold` to manually split up the
    training data into three folds, then find the model validation errors using a
    loop. In practice, we suggest using `sklearn.model_selection.GridSearchCV` with
    an `sklearn.pipeline.Pipeline` object, which can automatically break the data
    into training and validation sets and find the model that has the lowest average
    validation error across the folds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using cross-validation to manage model complexity has a couple of critical
    limitations: typically it requires the complexity to vary discretely, and there
    may not be a natural way to order the models. Rather than changing the dimensions
    of a sequence of models, we can fit a large model and apply constraints on the
    size of the coefficients. This notion is called regularization and is the topic
    of the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We just saw how cross-validation can help find a dimension for a fitted model
    that balances under- and overfitting. Rather than selecting the dimension of the
    model, we can build a model with all of the features, but restrict the size of
    the coefficients. We keep from overfitting by adding to the MSE a penalty term
    on the size of the coefficients. The penalty, called a *regularization term*,
    is <math><mi>λ</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>p</mi></mrow></munderover> <msubsup><mi>θ</mi> <mi>j</mi> <mn>2</mn></msubsup></math>
    . We fit the model by minimizing the combination of mean squared error plus this
    penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <msub><mrow><mi
    mathvariant="bold">x</mi></mrow> <mi>i</mi></msub> <mi mathvariant="bold-italic">θ</mi>
    <msup><mo stretchy="false">)</mo> <mn>2</mn></msup>  <mo>+</mo>  <mi>λ</mi> <munderover><mo>∑</mo>
    <mrow><mi>j</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>p</mi></mrow></munderover>
    <msubsup><mi>θ</mi> <mi>j</mi> <mn>2</mn></msubsup></math>
  prefs: []
  type: TYPE_NORMAL
- en: When the *regularization parameter*, <math><mi>λ</mi></math> , is large, it
    penalizes large coefficients. (We typically choose it by cross-validation.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Penalizing the square of the coefficients is called <math><msub><mi>L</mi>
    <mn>2</mn></msub></math> regularization, or *ridge regression*. Another popular
    regularization penalizes the absolute size of the coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">(</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mi>i</mi></msub> <mi mathvariant="bold-italic">θ</mi> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup>  <mo>+</mo>  <mi>λ</mi> <munderover><mo>∑</mo> <mrow><mi>j</mi>
    <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>p</mi></mrow></munderover> <mrow><mo stretchy="false">|</mo></mrow>
    <msub><mi>θ</mi> <mi>j</mi></msub> <mo stretchy="false">|</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: This <math><msub><mi>L</mi> <mn>1</mn></msub></math> regularized linear model
    is also called *lasso regression* (lasso stands for Least Absolute Shrinkage and
    Selection Operator).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an idea about how regularization works, let’s think about the extreme
    cases: when <math><mi>λ</mi></math> is really large and when it’s close to 0 (
    <math><mi>λ</mi></math> is never negative). With a big regularization parameter,
    the coefficients are heavily penalized, so they shrink. On the other hand, when
    <math><mi>λ</mi></math> is tiny, the coefficients aren’t restricted. In fact,
    when <math><mi>λ</mi></math> is 0, we’re back in the world of ordinary least squares.
    A couple of issues crop up when we think about controlling the size of the coefficients
    through regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: We do not want to regularize the intercept term. This way, a large penalty fits
    a constant model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When features have very different scales, the penalty can impact them differently,
    with large-valued features being penalized more than others. To avoid this, we
    standardize all of the features to have mean 0 and variance 1 before fitting the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at an example with 35 features.
  prefs: []
  type: TYPE_NORMAL
- en: Model Bias and Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we provide a different way to think about the problem of over-
    and underfitting. We carry out a simulation study where we generate synthetic
    data from a model of our design. This way, we know the true model, and we can
    see how close we get to the truth when we fit models to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We concoct a general model of data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>y</mi> <mo>=</mo> <mi>g</mi> <mo stretchy="false">(</mo>
    <mrow><mi mathvariant="bold">x</mi></mrow> <mo stretchy="false">)</mo> <mo>+</mo>
    <mrow><mi>ϵ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This expression makes it easy to see the two components of the model: the signal
    <math><mi>g</mi> <mo stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo></math>
    and the noise <math><mi>ϵ</mi></math> . In our model, we assume the noise has
    no trend or pattern, constant variance, and each observation’s noise is independent
    of the others’.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s take <math><mi>g</mi> <mo stretchy="false">(</mo> <mi>x</mi>
    <mo stretchy="false">)</mo> <mo>=</mo> <mi>sin</mi> <mo>⁡</mo> <mo stretchy="false">(</mo>
    <mi>x</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mn>0.3</mn> <mi>x</mi></math>
    and the noise from a normal curve with center 0 and SD = 0.2\. We can generate
    data from this model with the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s generate 50 data points <math><mo stretchy="false">(</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
    , <math><mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>…</mo> <mo>,</mo> <mn>50</mn></math>
    , from this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot our data, and since we know the true signal, we can find the errors
    and plot them too:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in10.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot on the left shows <math><mi>g</mi></math> as a dashed curve. We can
    also see that the <math><mo stretchy="false">(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo stretchy="false">)</mo></math> pairs form a scatter of dots about this curve.
    The righthand plot shows the errors, <math><mi>y</mi> <mo>−</mo> <mi>g</mi> <mo
    stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo></math> , for the
    50 points. Notice that they do not form a pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we fit a model to the data, we minimize the mean squared error. Let’s
    write this minimization in generality:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><munder><mo movablelimits="true">min</mo> <mrow><mi>f</mi>
    <mo>∈</mo> <mrow><mi mathvariant="script">F</mi></mrow></mrow></munder> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">[</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mi>f</mi> <mo stretchy="false">(</mo> <msub><mrow><mi
    mathvariant="bold">x</mi></mrow> <mi>i</mi></msub> <mo stretchy="false">)</mo>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></math>
  prefs: []
  type: TYPE_NORMAL
- en: The minimization is over the collection of functions <math><mrow><mi mathvariant="script">F</mi></mrow></math>
    . We have seen in this chapter that this collection of functions might be polynomials
    of 12 degrees, or simply bent lines. An important point is that the true model,
    <math><mi>g</mi></math> , doesn’t have to be one of the functions in the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take <math><mrow><mi mathvariant="script">F</mi></mrow></math> to be the
    collection of second-degree polynomials; in other words, functions that can be
    expressed as <math><msub><mi>θ</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>θ</mi>
    <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>θ</mi> <mn>2</mn></msub> <msup><mi>x</mi>
    <mn>2</mn></msup></math> . Since <math><mi>g</mi> <mo stretchy="false">(</mo>
    <mi>x</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mi>sin</mi> <mo>⁡</mo> <mo
    stretchy="false">(</mo> <mi>x</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mn>0.3</mn>
    <mi>x</mi></math> , it doesn’t belong to the collection of functions that we are
    optimizing over.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit a polynomial to our 50 data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we know the true model is not quadratic (because we built it). Let’s
    plot the data and the fitted curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in11.png)'
  prefs: []
  type: TYPE_IMG
- en: The quadratic doesn’t fit the data well, and it doesn’t represent the underlying
    curve well either because the set of models that we are choosing from (second-order
    polynomials) can’t capture the curvature in <math><mi>g</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'If we repeat this process and generate another 50 points from the true model
    and fit a second-degree polynomial to these data, then the fitted coefficients
    of the quadratic will change because it depends on the new set of data. We can
    repeat this process many times, and average the fitted curves. This average curve
    will resemble the typical best fit of a second-degree polynomial to 50 points
    from our true model. To demonstrate this notion, let’s generate 25 sets of 50
    data points and fit a quadratic to each dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can show on a plot all 25 fitted models along with the true function, <math><mi>g</mi></math>
    , and the average of the fitted curves, <math><mrow><mover><mi>f</mi> <mo stretchy="false">¯</mo></mover></mrow></math>
    . To do this, we use transparency for the 25 fitted models to distinguish overlapping
    curves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_16in12.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the 25 fitted quadratics vary with the data. This concept is
    called *model variation*. The average of the 25 quadratics is represented by the
    solid black line. The difference between the average quadratic and the true curve
    is called the *model bias*.
  prefs: []
  type: TYPE_NORMAL
- en: When the signal, <math><mi>g</mi></math> , does not belong to the model space,
    <math><mrow><mi mathvariant="script">F</mi></mrow></math> , we have model bias.
    If the model space can approximate <math><mi>g</mi></math> well, then the bias
    is small. For instance, a 10-degree polynomial can get pretty close to the <math><mi>g</mi></math>
    used in our example. On the other hand, we have seen earlier in this chapter that
    higher-degree polynomials can overfit the data and vary a lot trying to get close
    to the data. The more complex the model space, the greater the variability in
    the fitted model. Underfitting with too simple a model can lead to high model
    bias (the difference between <math><mi>g</mi></math> and <math><mrow><mover><mi>f</mi>
    <mo stretchy="false">¯</mo></mover></mrow></math> ), and overfitting with too
    complex a model can result in high model variance (the fluctuations of <math><mrow><mover><mi>f</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> around <math><mrow><mover><mi>f</mi>
    <mo stretchy="false">¯</mo></mover></mrow></math> ). This notion is called the
    *bias-variance trade-off*. Model selection aims to balance these competing sources
    of a lack of fit.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that problems arise when we minimize mean squared error
    to both fit a model and evaluate it. The train-test split helps us get around
    this problem, where we fit a model with the train set and evaluate our fitted
    model on test data that have been set aside.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to not “overuse” the test set, so we keep it separate until we
    have committed to a model. To help us commit, we might use cross-validation, which
    imitates the division of data into test and train sets. Again, it’s important
    to cross-validate using only the train set and keep the original test set away
    from any model selection process.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization takes a different approach and penalizes the mean squared error
    to keep the model from fitting the data too closely. In regularization, we use
    all of the data available to fit the model, but shrink the size of the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bias-variance trade-off allows us to more precisely describe the modeling
    phenomena that we have seen in this chapter: underfitting relates to model bias;
    overfitting results in model variance. In [Figure 16-4](#model-bias-variance-diagram),
    the x-axis measures model complexity and the y-axis measures these two components
    of model misfit: model bias and model variance. Notice that as the complexity
    of the model being fit increases, model bias decreases and model variance increases.
    Thinking in terms of test error, we have seen this error first decrease and then
    increase as the model variance outweighs the decrease in model bias. To select
    a useful model, we must strike a balance between model bias and variance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_1604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. Bias-variance trade-off
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Collecting more observations reduces bias if the model can fit the population
    process exactly. If the model is inherently incapable of modeling the population
    (as in our synthetic example), even infinite data cannot get rid of model bias.
    In terms of variance, collecting more data also reduces variance. One recent trend
    in data science is to select a model with low bias and high intrinsic variance
    (such as a neural network) but to collect many data points so that the model variance
    is low enough to make accurate predictions. While effective in practice, collecting
    enough data for these models tends to require large amounts of time and money.
  prefs: []
  type: TYPE_NORMAL
- en: Creating more features, whether useful or not, typically increases model variance.
    Models with many parameters have many possible combinations of parameters and
    therefore have higher variance than models with few parameters. On the other hand,
    adding a useful feature to the model, such as a quadratic feature when the underlying
    process is quadratic, reduces bias. But even adding a useless feature rarely increases
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: Being aware of the bias-variance trade-off can help you do a better job of fitting
    models. And using techniques like the train-test split, cross-validation, and
    regularization can ameliorate this issue.
  prefs: []
  type: TYPE_NORMAL
- en: Another part of modeling considers the variation in the fitted coefficients
    and curve. We might want to provide a confidence interval for a coefficient or
    a prediction band for a future observation. These intervals and bands give a sense
    of the accuracy of the fitted model. We discuss this notion next.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch16.html#id1643-marker)) These data are from Daniel T. Kaplan (CreateSpace
    Independent Publishing Platform, 2009).
  prefs: []
  type: TYPE_NORMAL
