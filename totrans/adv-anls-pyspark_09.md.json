["```py\nenum Strand {\n  Forward,\n  Reverse,\n  Independent\n}\n\nrecord SequenceFeature {\n  string featureId;\n  string featureType; ![1](assets/1.png)\n  string chromosome;\n  long startCoord;\n  long endCoord;\n  Strand strand;\n  double value;\n  map<string> attributes;\n}\n```", "```py\npip3 install bdgenomics.adam\n```", "```py\nadam-submit\n...\n\nUsing ADAM_MAIN=org.bdgenomics.adam.cli.ADAMMain\nUsing spark-submit=/home/analytical-monk/miniconda3/envs/pyspark/bin/spark-submit\n\n       e        888~-_         e            e    e\n      d8b       888   \\       d8b          d8b  d8b\n     /Y88b      888    |     /Y88b        d888bdY88b\n    /  Y88b     888    |    /  Y88b      / Y88Y Y888b\n   /____Y88b    888   /    /____Y88b    /   YY   Y888b\n  /      Y88b   888_-~    /      Y88b  /          Y888b\n\nUsage: adam-submit [<spark-args> --] <adam-args>\n\nChoose one of the following commands:\n\nADAM ACTIONS\n          countKmers : Counts the k-mers/q-mers from a read dataset...\n     countSliceKmers : Counts the k-mers/q-mers from a slice dataset...\n transformAlignments : Convert SAM/BAM to ADAM format and optionally...\n   transformFeatures : Convert a file with sequence features into...\n  transformGenotypes : Convert a file with genotypes into correspondi...\n  transformSequences : Convert a FASTA file as sequences into corresp...\n     transformSlices : Convert a FASTA file as slices into correspond...\n   transformVariants : Convert a file with variants into correspondin...\n         mergeShards : Merges the shards of a fil...\n            coverage : Calculate the coverage from a given ADAM fil...\nCONVERSION OPERATION\n          adam2fastq : Convert BAM to FASTQ file\n  transformFragments : Convert alignments into fragment records\nPRIN\n               print : Print an ADAM formatted fil\n            flagstat : Print statistics on reads in an ADAM file...\n                view : View certain reads from an alignment-record file.\n```", "```py\n# Note: this file is 16 GB\ncurl -O ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/phase3/data\\\n/HG00103/alignment/HG00103.mapped.ILLUMINA.bwa.GBR\\\n.low_coverage.20120522.bam\n\n# or using Aspera instead (which is *much* faster)\nascp -i path/to/asperaweb_id_dsa.openssh -QTr -l 10G \\\nanonftp@ftp.ncbi.nlm.nih.gov:/1000genomes/ftp/phase3/data\\\n/HG00103/alignment/HG00103.mapped.ILLUMINA.bwa.GBR\\\n.low_coverage.20120522.bam .\n```", "```py\nmv HG00103.mapped.ILLUMINA.bwa.GBR\\\n.low_coverage.20120522.bam data/genomics\n```", "```py\nadam-submit \\\n  --master yarn \\ ![1](assets/1.png)\n  --deploy-mode client \\\n  --driver-memory 8G \\\n  --num-executors 6 \\\n  --executor-cores 4 \\\n  --executor-memory 12G \\\n  -- \\\n  transform \\ ![2](assets/2.png)\n  data/genomics/HG00103.mapped.ILLUMINA.bwa.GBR\\ .low_coverage.20120522.bam \\\n  data/genomics/HG00103\n```", "```py\n$ du -sh data/genomics/HG00103*bam\n16G  data/genomics/HG00103\\. [...] .bam\n\n$ du -sh data/genomics/HG00103/\n13G  data/genomics/HG00103\n```", "```py\npyadam\n\n...\n\n[...]\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _  / __/   _/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n      /_/\n\nUsing Python version 3.6.12 (default, Sep  8 2020 23:10:56)\nSpark context Web UI available at http://192.168.29.60:4040\nSpark context available as 'sc'.\nSparkSession available as 'spark'.\n\n>>>\n```", "```py\n!pyspark --conf spark.serializer=org.apache.spark.\nserializer.KryoSerializer --conf spark.kryo.registrator=\norg.bdgenomics.adam.serialization.ADAMKryoRegistrator\n--jars `find-adam-assembly.sh` --driver-class-path\n`find-adam-assembly.sh`\n```", "```py\nfrom bdgenomics.adam.adamContext import ADAMContext\n\nac = ADAMContext(spark)\n\nreadsData = ac.loadAlignments(\"data/HG00103\")\n\nreadsDataDF = readsData.toDF()\nreadsDataDF.show(1, vertical=True)\n\n...\n\n-RECORD 0-----------------------------------------\n referenceName             | hs37d5\n start                     | 21810734\n originalStart             | null\n end                       | 21810826\n mappingQuality            | 0\n readName                  | SRR062640.14600566\n sequence                  | TCCATTCCACTCAGTTT...\n qualityScores             | /MOONNCRQPIQIKRGL...\n cigar                     | 92M8S\n originalCigar             | null\n basesTrimmedFromStart     | 0\n basesTrimmedFromEnd       | 0\n readPaired                | true\n properPair                | false\n readMapped                | false\n mateMapped                | true\n failedVendorQualityChecks | false\n duplicateRead             | false\n readNegativeStrand        | false\n mateNegativeStrand        | false\n primaryAlignment          | true\n secondaryAlignment        | false\n supplementaryAlignment    | false\n mismatchingPositions      | null\n originalQualityScores     | null\n readGroupId               | SRR062640\n readGroupSampleId         | HG00103\n mateAlignmentStart        | 21810734\n mateReferenceName         | hs37d5\n insertSize                | null\n readInFragment            | 1\n attributes                | RG:Z:SRR062640\\tX...\nonly showing top 1 row\n```", "```py\nreadsData.toDF().count()\n...\n160397565\n```", "```py\nunique_chr = readsDataDF.select('referenceName').distinct().collect()\nunique_chr = [u.referenceName for u in unique_chr]\n\nunique_chr.sort()\n...\n1\n10\n11\n12\n[...]\nGL000249.1\nMT\nNC_007605\nX\nY\nhs37d5\n```", "```py\nreadsData = ac.loadAlignments(\"data/HG00103\") ![1](assets/1.png)\n\nreadsDataDF = readsData.toDF() ![2](assets/2.png)\n\nunique_chr = readsDataDF.select('referenceName').distinct(). \\ ![3](assets/3.png)\n              collect() ![4](assets/4.png)\n```", "```py\nfrom pyspark.sql import functions as fun\ncftr_reads = readsDataDF.where(\"referenceName == 7\").\\\n              where(fun.col(\"start\") <= 117149189).\\\n              where(fun.col(\"end\") > 117149189)\n\ncftr_reads.count()\n...\n\n9\n```", "```py\nmkdir data/genomics/dnase\n\ncurl -O -L \"https://www.encodeproject.org/ \\\n              files/ENCFF001UVC/@@download/ENCFF001UVC.bed.gz\" | \\\n              gunzip > data/genomics/dnase/GM12878.DNase.narrowPeak ![1](assets/1.png)\ncurl -O -L \"https://www.encodeproject.org/ \\\n              files/ENCFF001UWQ/@@download/ENCFF001UWQ.bed.gz\" | \\\n              gunzip > data/genomics/dnase/K562.DNase.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n              files/ENCFF001WEI/@@download/ENCFF001WEI.bed.gz\" | \\\n              gunzip > data/genomics/dnase/BJ.DNase.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n              files/ENCFF001UVQ/@@download/ENCFF001UVQ.bed.gz\" | \\\n              gunzip > data/genomics/dnase/HEK293.DNase.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n            files/ENCFF001SOM/@@download/ENCFF001SOM.bed.gz\" | \\\n            gunzip > data/genomics/dnase/H54.DNase.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n            files/ENCFF001UVU/@@download/ENCFF001UVU.bed.gz\" | \\\n            gunzip > data/genomics/dnase/HepG2.DNase.narrowPeak\n\n[...]\n```", "```py\nmkdir data/genomics/chip-seq\n\ncurl -O -L \"https://www.encodeproject.org/ \\\n files/ENCFF001VED/@@download/ENCFF001VED.bed.gz\" | \\\n            gunzip > data/genomics/chip-seq/GM12878.ChIP-seq.CTCF.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n files/ENCFF001VMZ/@@download/ENCFF001VMZ.bed.gz\" | \\\n            gunzip > data/genomics/chip-seq/K562.ChIP-seq.CTCF.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n files/ENCFF001XMU/@@download/ENCFF001XMU.bed.gz\" | \\\n            gunzip > data/genomics/chip-seq/BJ.ChIP-seq.CTCF.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n files/ENCFF001XQU/@@download/ENCFF001XQU.bed.gz\" | \\\n            gunzip > data/genomics/chip-seq/HEK293.ChIP-seq.CTCF.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n files/ENCFF001USC/@@download/ENCFF001USC.bed.gz\" | \\\n            gunzip> data/genomics/chip-seq/H54.ChIP-seq.CTCF.narrowPeak\ncurl -O -L \"https://www.encodeproject.org/ \\\n files/ENCFF001XRC/@@download/ENCFF001XRC.bed.gz\" | \\\n            gunzip> data/genomics/chip-seq/HepG2.ChIP-seq.CTCF.narrowPeak\n\ncurl -s -L \"http://ftp.ebi.ac.uk/pub/databases/gencode/\\\n Gencode_human/release_18/gencode.v18.annotation.gtf.gz\" | \\\n            gunzip > data/genomics/gencode.v18.annotation.gtf\n[...]\n```", "```py\ncell_lines = [\"GM12878\", \"K562\", \"BJ\", \"HEK293\", \"H54\", \"HepG2\"]\nfor cell in cell_lines:\n## For each cell line…\n  ## …generate a suitable DataFrame\n## Concatenate the DataFrames and carry through into MLlib, for example\n```", "```py\nlocal_prefix = \"data/genomics\"\nimport pyspark.sql.functions as fun\n\n## UDF for finding closest transcription start site\n## naive; exercise for reader: make this faster\ndef distance_to_closest(loci, query):\n  return min([abs(x - query) for x in loci])\ndistance_to_closest_udf = fun.udf(distance_to_closest)\n\n## build in-memory structure for computing distance to TSS\n## we are essentially implementing a broadcast join here\ntss_data = ac.loadFeatures(\"data/genomics/gencode.v18.annotation.gtf\")\ntss_df = tss_data.toDF().filter(fun.col(\"featureType\") == 'transcript')\nb_tss_df = spark.sparkContext.broadcast(tss_df.groupBy('referenceName').\\\n                agg(fun.collect_list(\"start\").alias(\"start_sites\")))\n```", "```py\ncurrent_cell_line = cell_lines[0]\n\ndnase_path = f'data/genomics/dnase/{current_cell_line}.DNase.narrowPeak'\ndnase_data = ac.loadFeatures(dnase_path) ![1](assets/1.png)\ndnase_data.toDF().columns ![2](assets/2.png)\n...\n['featureId', 'sampleId', 'name', 'source', 'featureType', 'referenceName',\n'start', 'end', 'strand', 'phase', 'frame', 'score', 'geneId', 'transcriptId',\n'exonId', 'proteinId', 'aliases', 'parentIds', 'target', 'gap', 'derivesFrom',\n'notes', 'dbxrefs', 'ontologyTerms', 'circular', 'attributes']\n\n...\n\nchip_seq_path = f'data/genomics/chip-seq/ \\\n                  {current_cell_line}.ChIP-seq.CTCF.narrowPeak'\nchipseq_data = ac.loadFeatures(chipseq_path) ![1](assets/1.png)\n```", "```py\ndnase_with_label = dnase_data.leftOuterShuffleRegionJoin(chipseq_data)\ndnase_with_label_df = dnase_with_label.toDF()\n...\n\n-RECORD 0----------------------------------------------------------------------..\n _1  | {null, null, chr1.1, null, null, chr1, 713841, 714424, INDEPENDENT, null..\n _2  | {null, null, null, null, null, chr1, 713945, 714492, INDEPENDENT, null, ..\n-RECORD 1----------------------------------------------------------------------..\n _1  | {null, null, chr1.2, null, null, chr1, 740179, 740374, INDEPENDENT, null..\n _2  | {null, null, null, null, null, chr1, 740127, 740310, INDEPENDENT, null, ..\n-RECORD 2----------------------------------------------------------------------..\n _1  | {null, null, chr1.3, null, null, chr1, 762054, 763213, INDEPENDENT, null..\n _2  | null...\nonly showing top 3 rows\n...\n\ndnase_with_label_df = dnase_with_label_df.\\\n                        withColumn(\"label\", \\\n                                    ~fun.col(\"_2\").isNull())\ndnase_with_label_df.show(5)\n```", "```py\n## build final training DF\ntraining_df = dnase_with_label_df.withColumn(\n    \"contig\", fun.col(\"_1\").referenceName).withColumn(\n    \"start\", fun.col(\"_1\").start).withColumn(\n    \"end\", fun.col(\"_1\").end).withColumn(\n    \"tf\", fun.lit(\"CTCF\")).withColumn(\n    \"cell_line\", fun.lit(current_cell_line)).drop(\"_1\", \"_2\")\n\ntraining_df = training_df.join(b_tss_df,\n                               training_df.contig == b_tss_df.referenceName,\n                               \"inner\") ![1](assets/1.png)\n\ntraining_df.withColumn(\"closest_tss\",\n                      fun.least(distance_to_closest_udf(fun.col(\"start_sites\"),\n                                                        fun.col(\"start\")),\n                          distance_to_closest_udf(fun.col(\"start_sites\"),\n                                                  fun.col(\"end\")))) ![2](assets/2.png)\n```", "```py\npreTrainingData = data_by_cellLine.union(...)\npreTrainingData.cache()\n\npreTrainingData.count()\npreTrainingData.filter(fun.col(\"label\") == true).count()\n```"]