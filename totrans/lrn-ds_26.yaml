- en: Chapter 20\. Numerical Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point in the book, our modeling procedure should feel familiar: we
    define a model, choose a loss function, and fit the model by minimizing the average
    loss over our training data. We’ve seen several techniques to minimize loss. For
    example, we used both calculus and a geometric argument in [Chapter 15](ch15.html#ch-linear)
    to find a simple expression for fitting linear models using squared loss.'
  prefs: []
  type: TYPE_NORMAL
- en: But empirical loss minimization isn’t always so straightforward. Lasso regression,
    with the addition of the <math><msub><mi>L</mi> <mn>1</mn></msub></math> penalty
    to the average squared loss, no longer has a closed-form solution, and logistic
    regression uses cross-entropy loss to fit a nonlinear model. In these cases, we
    use *numerical optimization* to fit the model, where we systematically choose
    parameter values to evaluate the average loss in search of the minimizing value.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we introduced loss functions in [Chapter 4](ch04.html#ch-modeling), we
    performed a simple numerical optimization to find the minimizer of the average
    loss. We created a grid of <math><mi>θ</mi></math> values and evaluated the average
    loss at all points in the grid (see [Figure 20-1](#grid-diagram)). The grid point
    with the smallest average loss we took as the best fit. Unfortunately, this sort
    of grid search quickly becomes impractical, for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: For complex models with many features, the grid becomes unwieldy. With only
    four features and a grid of 100 values for each feature, we must evaluate the
    average loss at <math><msup><mn>100</mn> <mn>4</mn></msup> <mo>=</mo> <mn>100,000,000</mn></math>
    grid points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The range of parameter values to search over must be specified in advance to
    create the grid, and when we don’t have a good sense of the range, we need to
    start with a wide grid and possibly repeat the grid search over narrower ranges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a large number of observations, the evaluation of the average loss over
    the grid points can be slow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/leds_2001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-1\. Searching over a grid of points can be computationally slow or
    inexact
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this chapter, we introduce numerical optimization techniques that take advantage
    of the shape and smoothness of the loss function in the search for the minimizing
    parameter values. We first introduce the basic idea behind the technique of gradient
    descent, then we give an example and describe the properties of the loss function
    that make gradient descent work, and finally, we provide a few extensions of gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient descent is based on the notion that for many loss functions, the function
    is roughly linear in small neighborhoods of the parameter. [Figure 20-2](#gd-diagram)
    gives a diagram of the basic idea.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_2002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-2\. The technique of gradient descent moves in small increments toward
    the minimizing parameter value
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the diagram, we have drawn the tangent line to the loss curve <math><mi>L</mi></math>
    at some point <math><mrow><mi>θ</mi></mrow></math> to the left of the minimizing
    value, <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    . Notice that the slope of the tangent line is negative. A short step to the right
    of <math><mrow><mi>θ</mi></mrow></math> to <math><mrow><mi>θ</mi></mrow> <mo>+</mo>
    <mtext>s</mtext></math> , for some small amount <math><mtext>s</mtext></math>
    , gives a point on the tangent line close to the loss at <math><mrow><mi>θ</mi></mrow>
    <mo>+</mo> <mtext>s</mtext></math> , and this loss is smaller than <math><mi>L</mi>
    <mo stretchy="false">(</mo> <mrow><mover><mi>θ</mi> <mo stretchy="false">~</mo></mover></mrow>
    <mo stretchy="false">)</mo></math> . That is, since the slope, <math><mi>b</mi></math>
    , is negative, and the tangent line approximates the loss function in a neighborhood
    of <math><mrow><mi>θ</mi></mrow></math> , we have:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>L</mi> <mo stretchy="false">(</mo> <mrow><mi>θ</mi></mrow>
    <mo>+</mo> <mtext>s</mtext> <mo stretchy="false">)</mo> <mo>≈</mo> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi>θ</mi> <mo stretchy="false">)</mo> <mo>+</mo>
    <mi>b</mi> <mo>×</mo> <mtext>s</mtext> <mo><</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: So, taking a small step to the right of this <math><mrow><mi>θ</mi></mrow></math>
    decreases the loss. On the other hand, on the other side of <math><mrow><mover><mi>θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> in the diagram in [Figure 20-2](#gd-diagram),
    the slope is positive, and taking a small step to the left decreases the loss.
  prefs: []
  type: TYPE_NORMAL
- en: When we take repeated small steps in the direction indicated by whether the
    slope of the tangent line is positive or negative at each new step, this leads
    to smaller and smaller values of the average loss and eventually brings us to
    the minimizing value <math><mrow><mover><mi>θ</mi> <mo stretchy="false">^</mo></mover></mrow></math>
    (or very close to it). This is the basic idea behind gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, to minimize <math><mi>L</mi> <mo stretchy="false">(</mo> <mi
    mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> for a general
    vector of parameters, <math><mi mathvariant="bold-italic">θ</mi></math> , the
    gradient (first-order partial derivative) determines the direction and size of
    the step to take. If we write the gradient, <math><msub><mi mathvariant="normal">∇</mi>
    <mi>θ</mi></msub> <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> , as simply <math><mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> , then
    gradient descent says the increment or step is <math><mo>−</mo> <mi>α</mi> <mi>g</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    for some small positive <math><mi>α</mi></math> . Then the average loss at the
    new position is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtable columnalign="right left" columnspacing="0em"
    displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo>+</mo> <mo stretchy="false">(</mo> <mo>−</mo> <mi>α</mi> <mi>g</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>≈</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mi>α</mi> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <msup><mo stretchy="false">)</mo> <mi>T</mi></msup>
    <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></mtd></mtr> <mtr><mtd><mo><</mo> <mi>L</mi> <mo stretchy="false">(</mo>
    <mi>θ</mi> <mo stretchy="false">)</mo></mtd></mtr></mtable></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> is a <math><mi>p</mi> <mo>×</mo> <mn>1</mn></math>
    vector and <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <msup><mo stretchy="false">)</mo> <mi>T</mi></msup> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> is positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps in the gradient descent algorithm go as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a starting value, called <math><msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mn>0</mn> <mo stretchy="false">)</mo></mrow></msup></math>
    (a common choice is <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mn>0</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <mn>0</mn></math> ).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mi>g</mi> <mo
    stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 until <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup></math>
    doesn’t change (or changes little) between iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The quantity <math><mi>α</mi></math> is called the *learning rate*. Setting
    <math><mi>α</mi></math> can be tricky. It needs to be small enough to not overshoot
    the minimum but large enough to arrive at the minimum in reasonably few steps
    (see [Figure 20-3](#gd-learning-rate)). There are many strategies for setting
    <math><mi>α</mi></math> . For example, it can be useful to decrease <math><mi>α</mi></math>
    over time. When <math><mi>α</mi></math> changes between iterations, we use the
    notation <math><msup><mi>α</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo
    stretchy="false">)</mo></mrow></msup></math> to indicate that the learning rate
    varies during the search.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_2003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-3\. A small learning rate requires many steps to converge (left),
    and a large learning rate can diverge (right); choosing the learning rate well
    leads to fast convergence on the minimizing value (middle)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The gradient descent algorithm is simple yet powerful since we can use it for
    many types of models and many types of loss functions. It is the computational
    tool of choice for fitting many models, including linear regression on large datasets
    and logistic regression. We demonstrate the algorithm to fit a constant to the
    bus delay data (from [Chapter 4](ch04.html#ch-modeling)) next.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing Huber Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Huber loss* combines absolute loss and squared loss to get a function that
    is differentiable (like squared loss) and less sensitive to outliers (like absolute
    loss):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mi>n</mi></munderover> <mrow><mo>{</mo> <mtable columnalign="left left" columnspacing="1em"
    rowspacing=".2em"><mtr><mtd><mfrac><mn>1</mn> <mn>2</mn></mfrac> <mo stretchy="false">(</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <msup><mo stretchy="false">)</mo>
    <mn>2</mn></msup></mtd> <mtd><mo stretchy="false">|</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow> <mo>≤</mo> <mi>γ</mi></mtd></mtr>
    <mtr><mtd><mi>γ</mi> <mo stretchy="false">(</mo> <mrow><mo stretchy="false">|</mo></mrow>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow>
    <mo>−</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mi>γ</mi> <mo stretchy="false">)</mo></mtd>
    <mtd><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Huber loss is differentiable, we can use gradient descent. We first find
    the gradient of the average Huber loss:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi>θ</mi> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo> <mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <munderover><mo>∑</mo>
    <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mi>n</mi></munderover> <mrow><mo>{</mo>
    <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em"><mtr><mtd><mo>−</mo>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi>
    <mo stretchy="false">)</mo></mtd> <mtd><mo stretchy="false">|</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi> <mrow><mo stretchy="false">|</mo></mrow>
    <mo>≤</mo> <mi>γ</mi></mtd></mtr> <mtr><mtd><mo>−</mo> <mi>γ</mi> <mo>⋅</mo> <mtext>sign</mtext>
    <mo stretchy="false">(</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>−</mo> <mi>θ</mi>
    <mo stretchy="false">)</mo></mtd> <mtd><mtext>otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We create the functions `huber_loss` and `grad_huber_loss` to compute the average
    loss and its gradient. We write these functions to have signatures that enable
    us to specify the parameter as well as the observed data that we average over
    and the transition point of the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we write a simple implementation of gradient descent. The signature of
    our function includes the loss function, its gradient, and the data to average
    over. We also supply the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that the bus delays dataset consists of over 1,000 measurements of how
    many minutes late the northbound C-line buses are in arriving at the stop at 3rd
    Avenue and Pike Street in Seattle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In [Chapter 4](ch04.html#ch-modeling), we fit a constant model to these data
    for absolute loss and squared loss. We found that absolute loss yielded the median
    and square the mean of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we use the gradient descent algorithm to find the minimizing constant model
    for Huber loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The optimizing constant for Huber loss is close to the value that minimizes
    absolute loss. This comes from the shape of the Huber loss function. It is linear
    in the tails and so is not affected by outliers like with absolute loss and unlike
    with squared loss.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We wrote our `minimize` function to demonstrate the idea behind the algorithm.
    In practice, you will want to use well-tested, numerically sound implementations
    of an optimization algorithm. For example, the `scipy` package has a `minimize`
    method that we can use to find the minimizer of average loss, and we don’t even
    need to compute the gradient. This algorithm is likely to be much faster than
    any one that we might write. In fact, we used it in [Chapter 18](ch18.html#ch-donkey)
    when we created our own asymmetric modification of quadratic loss for the special
    case where we wanted the loss to be greater for errors on one side of the minimum
    than the other.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, we typically stop the algorithm when <math><msup><mi>θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    doesn’t change much between iterations. In our function, we stop when <math><msup><mi>θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>−</mo> <msup><mi>θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    is less than 0.001\. It is also common to stop the search after a large number
    of steps, such as 1,000\. If the algorithm has not arrived at the minimizing value
    after 1,000 iterations, then the algorithm might be diverging because the learning
    rate is too large or the minimum might exist in the limit at <math><mo>±</mo>
    <mi mathvariant="normal">∞</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient descent gives us a general way to minimize average loss when we cannot
    easily solve for the minimizing value analytically or when the minimization is
    computationally expensive. The algorithm relies on two important properties of
    the average loss function: it is both convex and differentiable in <math><mi mathvariant="bold-italic">θ</mi></math>
    . We discuss how the algorithm relies on these properties next.'
  prefs: []
  type: TYPE_NORMAL
- en: Convex and Differentiable Loss Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As its name suggests, the gradient descent algorithm requires the function being
    minimized to be differentiable. The gradient, <math><msub><mi mathvariant="normal">∇</mi>
    <mi>θ</mi></msub> <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> , allows us to make a linear approximation
    to the average loss in small neighborhoods of <math><mi mathvariant="bold-italic">θ</mi></math>
    . This approximation gives us the direction (and size) of the step, and as long
    as we don’t overshoot the minimum, <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="bold" stretchy="false">^</mo></mover></mrow></math> , we are
    bound to eventually reach it. Well, as long as the loss function is also convex.
  prefs: []
  type: TYPE_NORMAL
- en: The step-by-step search for the minimum also relies on the loss function being
    convex. The function in the diagram on the left in [Figure 20-4](#gd-convex) is
    convex, but the function on the right is not. The function on the right has a
    local minimum, and depending on where the algorithm starts, it might converge
    to this local minimum and miss the real minimum entirely. The property of convexity
    avoids this problem. A *convex function* avoids the problem of local minima. So,
    with an appropriate step size, gradient descent finds the globally optimal <math><mi>θ</mi></math>
    for any convex, differentiable function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_2004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-4\. With nonconvex functions (right), gradient descent might locate
    a local minimum rather than a global minimum, which is not possible with convex
    functions (left)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Formally, a function <math><mi>f</mi></math> is convex if for any two input
    values, <math><msub><mi mathvariant="bold-italic">θ</mi> <mi>a</mi></msub></math>
    and <math><msub><mi mathvariant="bold-italic">θ</mi> <mi>b</mi></msub></math>
    , and any <math><mi>q</mi></math> between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mi>q</mi> <mi>f</mi> <mo stretchy="false">(</mo> <msub><mi
    mathvariant="bold-italic">θ</mi> <mi>a</mi></msub> <mo stretchy="false">)</mo>
    <mo>+</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>q</mi> <mo stretchy="false">)</mo>
    <mi>f</mi> <mo stretchy="false">(</mo> <msub><mi mathvariant="bold-italic">θ</mi>
    <mi>b</mi></msub> <mo stretchy="false">)</mo> <mo>≥</mo> <mi>f</mi> <mo stretchy="false">(</mo>
    <mi>q</mi> <msub><mi mathvariant="bold-italic">θ</mi> <mi>a</mi></msub> <mo>+</mo>
    <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>q</mi> <mo stretchy="false">)</mo>
    <msub><mi mathvariant="bold-italic">θ</mi> <mi>b</mi></msub> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: This inequality implies that any line segment that connects two points of the
    function must reside on or above the function itself. Heuristically, this means
    that whenever we take a small enough step to the right when the gradient is negative
    or to the left when the gradient is positive, we will head in the direction of
    the function’s minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formal definition of convexity gives us a precise way to determine whether
    a function is convex. And we can use this definition to connect the convexity
    of the average loss <math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> to the loss function <math><mrow><mi mathvariant="script">l</mi></mrow>
    <mo mathvariant="script" stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="script" stretchy="false">)</mo></math> . We have so far in this
    chapter simplified the representation of <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> by not
    mentioning the data. Recall:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mrow><mi mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math><mtext mathvariant="bold">X</mtext></math> is an <math><mi>n</mi>
    <mo>×</mo> <mi>p</mi></math> design matrix and <math><msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mi>i</mi></msub></math> is the <math><mi>i</mi></math> th row of the design matrix,
    which corresponds to the <math><mi>i</mi></math> th observation in the dataset.
    This means that the gradient can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi>i</mi></msub>
    <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If <math><mrow><mi mathvariant="script">l</mi></mrow> <mo mathvariant="script"
    stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo mathvariant="script">,</mo>
    <msub><mrow><mi mathvariant="bold">x</mi></mrow> <mi mathvariant="script">i</mi></msub>
    <mo mathvariant="script">,</mo> <msub><mi mathvariant="script">y</mi> <mi mathvariant="script">i</mi></msub>
    <mo mathvariant="script" stretchy="false">)</mo></math> is a convex function of
    <math><mi mathvariant="bold-italic">θ</mi></math> , then the average loss is also
    convex. And similarly for the derivative: the derivative of <math><mrow><mi mathvariant="script">l</mi></mrow>
    <mo mathvariant="script" stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo mathvariant="script">,</mo> <msub><mrow><mi mathvariant="bold">x</mi></mrow>
    <mi mathvariant="script">i</mi></msub> <mo mathvariant="script">,</mo> <msub><mi
    mathvariant="script">y</mi> <mi mathvariant="script">i</mi></msub> <mo mathvariant="script"
    stretchy="false">)</mo></math> is averaged over the data to evaluate the derivative
    of <math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mrow><mi mathvariant="bold">y</mi></mrow>
    <mo stretchy="false">)</mo></math> . We walk through a proof of the convexity
    property in the exercises.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, with a large amount of data, calculating <math><msup><mi>θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    can be computationally expensive since it involves the average of the gradient
    <math><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow></math> over all the <math><mo stretchy="false">(</mo>
    <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo stretchy="false">)</mo></math> . We next consider variants
    of gradient descent that can be computationally faster because they don’t average
    over all of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Variants of Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two variants of gradient descent, stochastic gradient descent and mini-batch
    gradient descent, use subsets of the data when computing the gradient of the average
    loss and are useful for optimization problems with large datasets. A third alternative,
    Newton’s method, assumes the loss function is twice differentiable and uses a
    quadratic approximation to the loss function, rather than the linear approximation
    used in gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that gradient descent takes steps based on the gradient. At step <math><mi>t</mi></math>
    , we move from <math><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math> to:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mi>L</mi>
    <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'And since <math><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></math> can be expressed as the average gradient of
    the loss function <math><mi mathvariant="script">l</mi></math> , we have:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>,</mo>
    <mtext mathvariant="bold">X</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext>
    <mo stretchy="false">)</mo></mtd> <mtd><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: This representation of the gradient of the average loss in terms of the average
    of the gradient of loss at each point in the data shows why the algorithm is also
    called *batch gradient descent*. Two variants to batch gradient descent use smaller
    amounts of the data rather than the complete “batch.” The first, stochastic gradient
    descent, uses only one observation in each step of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although batch gradient descent can often find an optimal <math><mi mathvariant="bold-italic">θ</mi></math>
    in relatively few iterations, each iteration can take a long time to compute if
    the dataset contains many observations. To get around this difficulty, stochastic
    gradient descent approximates the overall gradient by a single, randomly chosen
    data point. Since this observation is chosen randomly, we expect that using the
    gradient at randomly chosen observations will, on average, move in the correct
    direction and so eventually converge to the minimizing parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, to conduct stochastic gradient descent, we replace the average gradient
    with the gradient at a single data point. So, the updated formula is just:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <msup><mrow><mi
    mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: In this formula, the <math><msup><mi>i</mi> <mrow><mi>t</mi> <mi>h</mi></mrow></msup></math>
    observations <math><mo stretchy="false">(</mo> <msub><mtext mathvariant="bold">x</mtext>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
    are chosen randomly from the data. Choosing the points randomly is critical to
    the success of stochastic gradient descent. If the points are not chosen randomly,
    the algorithm may produce significantly worse results than batch gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: We most commonly run stochastic gradient descent by randomly shuffling all of
    the data points and using each point in its shuffled order until we complete one
    entire pass through the data. If the algorithm hasn’t converged yet, then we reshuffle
    the points and run another pass through the data. Each *iteration* of stochastic
    gradient descent looks at one data point; each complete pass through the data
    is called an *epoch*.
  prefs: []
  type: TYPE_NORMAL
- en: Since stochastic descent only examines a single data point at a time, at times
    it takes steps away from the minimizer, <math><mrow><mover><mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">^</mo></mover></mrow></math> , but on average these steps
    are in the right direction. And since the algorithm computes an update much more
    quickly than batch gradient descent, it can make significant progress toward the
    optimal <math><mrow><mover><mi mathvariant="bold-italic">θ</mi> <mo mathvariant="bold"
    stretchy="false">^</mo></mover></mrow></math> by the time batch gradient descent
    finishes a single update.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-Batch Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As its name suggests, *mini-batch gradient descent* strikes a balance between
    batch gradient descent and stochastic gradient descent by increasing the number
    of observations selected at random in each iteration. In mini-batch gradient descent,
    we average the gradient of the loss function at a few data points instead of at
    a single point or all the points. We let <math><mrow><mi mathvariant="script">B</mi></mrow></math>
    represent the mini-batch of data points that are randomly sampled from the dataset,
    and we define the algorithm’s next step as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup>
    <mo>=</mo> <msup><mrow><mi mathvariant="bold-italic">θ</mi></mrow> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo>−</mo> <mi>α</mi> <mo>⋅</mo>
    <mfrac><mn>1</mn> <mrow><mo stretchy="false">|</mo> <mrow><mi mathvariant="script">B</mi></mrow>
    <mo stretchy="false">|</mo></mrow></mfrac> <munder><mo>∑</mo> <mrow><mrow><mi>i</mi>
    <mo>∈</mo> <mrow><mi mathvariant="script">B</mi></mrow></mrow></mrow></munder>
    <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub> <mrow><mi
    mathvariant="script">l</mi></mrow> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo>,</mo> <msub><mtext mathvariant="bold">x</mtext> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: As with stochastic gradient descent, we perform mini-batch gradient descent
    by randomly shuffling the data. Then we split the data into consecutive mini-batches
    and iterate through the batches in sequence. After each epoch, we reshuffle our
    data and select new mini-batches.
  prefs: []
  type: TYPE_NORMAL
- en: While we have made the distinction between stochastic and mini-batch gradient
    descent, *stochastic gradient descent* is sometimes used as an umbrella term that
    encompasses the selection of a mini-batch of any size.
  prefs: []
  type: TYPE_NORMAL
- en: Another common optimization technique is Newton’s method.
  prefs: []
  type: TYPE_NORMAL
- en: Newton’s Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Newton’s method uses the second derivative to optimize the loss. The basic
    idea is to approximate the average loss, <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> , in small
    neighborhoods of <math><mi mathvariant="bold-italic">θ</mi></math> , with a quadratic
    curve rather than a linear approximation. The approximation looks as follows for
    a small step <math><mrow><mi mathvariant="bold">s</mi></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo>+</mo> <mrow><mi
    mathvariant="bold">s</mi></mrow> <mo stretchy="false">)</mo> <mo>≈</mo> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo>
    <mo>+</mo> <mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <msup><mo stretchy="false">)</mo> <mi>T</mi></msup> <mrow><mi mathvariant="bold">s</mi></mrow>
    <mo>+</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <msup><mrow><mi mathvariant="bold">s</mi></mrow>
    <mi>T</mi></msup> <mi>H</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mrow><mi mathvariant="bold">s</mi></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math><mi>g</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mo>=</mo> <msub><mi mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow></msub>
    <mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo
    stretchy="false">)</mo></math> is the gradient and <math><mi>H</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo> <mo>=</mo> <msubsup><mi
    mathvariant="normal">∇</mi> <mrow><mi>θ</mi></mrow> <mn>2</mn></msubsup> <mi>L</mi>
    <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math>
    is the Hessian of <math><mi>L</mi> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo></math> . More specifically, <math><mi>H</mi></math>
    is a <math><mi>p</mi> <mo>×</mo> <mi>p</mi></math> matrix of second-order partial
    derivatives in <math><mi mathvariant="bold-italic">θ</mi></math> with <math><mi>i</mi></math>
    , <math><mi>j</mi></math> elements:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><msub><mi>H</mi>
    <mrow><mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></msub> <mo>=</mo> <mfrac><mrow><msup><mi>∂</mi>
    <mn>2</mn></msup> <mrow><mi mathvariant="script">l</mi></mrow></mrow> <mrow><mi>∂</mi>
    <msub><mi>θ</mi> <mi>i</mi></msub> <mi>∂</mi> <msub><mi>θ</mi> <mi>j</mi></msub></mrow></mfrac></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This quadratic approximation to <math><mi>L</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo>+</mo> <mrow><mi mathvariant="bold">s</mi></mrow>
    <mo stretchy="false">)</mo></math> has a minimum at <math><mrow><mi mathvariant="bold">s</mi></mrow>
    <mo>=</mo> <mo>−</mo> <mo stretchy="false">[</mo> <msup><mi>H</mi> <mrow><mo>−</mo>
    <mn>1</mn></mrow></msup> <mo stretchy="false">(</mo> <mi mathvariant="bold-italic">θ</mi>
    <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> <mi>g</mi> <mo stretchy="false">(</mo>
    <mi mathvariant="bold-italic">θ</mi> <mo stretchy="false">)</mo></math> . (Convexity
    implies that <math><mi>H</mi></math> is a symmetric square matrix that can be
    inverted.) Then a step in the algorithm moves from <math><msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup></math>
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo>
    <mi>t</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo></mrow></msup> <mo>=</mo>
    <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo stretchy="false">(</mo> <mi>t</mi>
    <mo stretchy="false">)</mo></mrow></msup> <mo>+</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow> <mrow><mi>n</mi></mrow></munderover>
    <mo>−</mo> <mo stretchy="false">[</mo> <msup><mi>H</mi> <mrow><mo>−</mo> <mn>1</mn></mrow></msup>
    <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi> <mrow><mo
    stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup> <mo
    stretchy="false">]</mo> <mi>g</mi> <mo stretchy="false">(</mo> <msup><mi mathvariant="bold-italic">θ</mi>
    <mrow><mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo></mrow></msup>
    <mo stretchy="false">)</mo></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 20-5](#newton-diagram) gives the idea behind Newton’s method of optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/leds_2005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20-5\. Newton’s method uses a local quadratic approximation to the curve
    to take steps toward the minimizing value of a convex, twice-differentiable function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This technique converges quickly if the approximation is accurate and the steps
    are small. Otherwise, Newton’s method can diverge, which often happens if the
    function is nearly flat in a dimension. When the function is relatively flat,
    the derivative is near zero and its inverse can be quite large. Large steps can
    move to <math><mi mathvariant="bold-italic">θ</mi></math> that are far from where
    the approximation is accurate. (Unlike with gradient descent, there is no learning
    rate that keeps steps small.)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced several techniques for numerical optimization
    that take advantage of the shape and smoothness of the loss function in the search
    for the minimizing parameter values. We first introduced gradient descent, which
    relies on the differentiability of loss function. Gradient descent, also called
    batch gradient descent, iteratively improves model parameters until the model
    achieves minimal loss. Since batch gradient descent is computationally intractable
    with large datasets, we often instead use stochastic gradient descent to fit models.
  prefs: []
  type: TYPE_NORMAL
- en: Mini-batch gradient descent is most optimal when running on a graphical processing
    unit (GPU) chip found in some computers. Since computations on these types of
    hardware can be executed in parallel, using a mini-batch can increase the accuracy
    of the gradient without increasing computation time. Depending on the memory size
    of the GPU, the mini-batch size is often set between 10 and 100 observations.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if the loss function is twice differentiable, then Newton’s method
    can converge very quickly, even though it is more expensive to compute one step
    in the iteration. A hybrid approach is also popular, beginning with gradient descent
    (of some kind) and then switching the algorithm to Newton’s method. This approach
    can avoid divergence and be faster than gradient descent alone. Typically, the
    second-order approximation used by Newton’s method is more appropriate near the
    optimum and converges quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, another option is to set the step size adaptively. Additionally, setting
    different learning rates for different features can be important if they are of
    different scale or vary in frequency. For example, word counts can differ a lot
    across common words and rare words.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic regression model introduced in [Chapter 19](ch19.html#ch-logistic)
    is fitted using numerical optimization methods like those described in this chapter.
    We wrap up with one final case study that uses logistic regression to fit a complex
    model with thousands of features.
  prefs: []
  type: TYPE_NORMAL
