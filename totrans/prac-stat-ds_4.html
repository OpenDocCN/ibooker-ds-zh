<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Regression and Prediction"><div class="chapter" id="Regression">
<h1><span class="label">Chapter 4. </span>Regression and Prediction</h1>


<p>Perhaps the most common goal in statistics is to answer the question “Is the variable <em>X</em> (or more likely, <math alttext="upper X 1 comma ellipsis comma upper X Subscript p Baseline">
  <mrow>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi> </msub>
  </mrow>
</math>) associated with a variable <em>Y</em>, and if so, what is the relationship and can we use it to predict <em>Y</em>?”<a data-type="indexterm" data-primary="prediction" data-secondary="using regression" id="idm46522856647384"/><a data-type="indexterm" data-primary="regression" id="ix_regr"/></p>

<p>Nowhere is the nexus between statistics and data science stronger than in the realm of prediction—specifically, the prediction of an outcome (target) variable based on the values of other “predictor” variables. This process of training a model on data where the outcome is known, for subsequent application to data where the<a data-type="indexterm" data-primary="supervised learning" id="idm46522856644824"/> outcome is not known, is termed <em>supervised learning</em>. Another important connection between data science and statistics is in the area of <em>anomaly detection</em>, where regression diagnostics originally intended for data analysis and improving the regression model can be used to detect unusual records.<a data-type="indexterm" data-primary="anomaly detection" id="idm46522856643032"/></p>






<section data-type="sect1" data-pdf-bookmark="Simple Linear Regression"><div class="sect1" id="idm46522856642200">
<h1>Simple Linear Regression</h1>

<p id="SimpleLinearRegression">Simple linear regression provides a model of the relationship between the magnitude of one variable and that of a second—for example, as <em>X</em> increases, <em>Y</em> also increases.<a data-type="indexterm" data-primary="linear regression" data-secondary="simple" id="ix_linregsim"/><a data-type="indexterm" data-primary="regression" data-secondary="simple linear regression" id="ix_regrsimlin"/>
Or as <em>X</em> increases, <em>Y</em> decreases.<sup><a data-type="noteref" id="idm46522856635400-marker" href="ch04.xhtml#idm46522856635400">1</a></sup>
Correlation is another way to measure how two variables are related—see the section <a data-type="xref" href="ch01.xhtml#Correlations">“Correlation”</a>.
The difference is that while correlation measures the <em>strength</em> of an association between two variables,
regression quantifies the <em>nature</em> of the relationship.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856640536">
<h5>Key Terms for Simple Linear Regression</h5><dl>
<dt class="horizontal"><strong><em>Response</em></strong></dt>
<dd>
<p>The variable we are trying to predict.<a data-type="indexterm" data-primary="response" id="idm46522856630024"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>dependent variable, <em>Y</em> variable, target, outcome</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Independent variable</em></strong></dt>
<dd>
<p>The variable used to predict the response.<a data-type="indexterm" data-primary="independent variables" id="idm46522856625256"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p><em>X</em> variable, feature, attribute, predictor</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Record</em></strong></dt>
<dd>
<p>The vector of predictor and outcome values for a specific individual or case.<a data-type="indexterm" data-primary="records" id="idm46522856593896"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>row, case, instance, example</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Intercept</em></strong></dt>
<dd>
<p>The intercept of the regression line—that is, the predicted value when <math alttext="upper X equals 0">
  <mrow>
    <mi>X</mi>
    <mo>=</mo>
    <mn>0</mn>
  </mrow>
</math>.<a data-type="indexterm" data-primary="intercept" id="idm46522856587048"/><a data-type="indexterm" data-primary="regression coefficients" id="idm46522856586312"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p><math alttext="b 0">
  <msub><mi>b</mi> <mn>0</mn> </msub>
</math>, <math alttext="beta 0">
  <msub><mi>β</mi> <mn>0</mn> </msub>
</math></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Regression coefficient</em></strong></dt>
<dd>
<p>The slope of the regression line.</p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>slope, <math alttext="b 1">
  <msub><mi>b</mi> <mn>1</mn> </msub>
</math>, <math alttext="beta 1">
  <msub><mi>β</mi> <mn>1</mn> </msub>
</math>, parameter estimates, weights</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Fitted values</em></strong></dt>
<dd>
<p>The estimates <math alttext="ModifyingAbove upper Y With caret Subscript i">
  <msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
</math> obtained from the regression line.<a data-type="indexterm" data-primary="fitted values" id="idm46522856567592"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>predicted values</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Residuals</em></strong></dt>
<dd>
<p>The difference between the observed values and the fitted values.<a data-type="indexterm" data-primary="residuals" id="idm46522856563288"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>errors</p>
</dd>
</dl>
</dd>
</dl>
<dl class="pagebreak-before">
<dt><strong><em>Least squares</em></strong></dt>
<dd>
<p>The method of fitting a regression by minimizing the sum of squared residuals.<a data-type="indexterm" data-primary="least squares regression" id="idm46522856558856"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>ordinary least squares, OLS</p>
</dd>
</dl>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="The Regression Equation"><div class="sect2" id="idm46522856555896">
<h2>The Regression Equation</h2>

<p>Simple linear regression estimates how much <em>Y</em> will change when <em>X</em> changes by a <span class="keep-together">certain amount</span>.<a data-type="indexterm" data-primary="regression" data-secondary="simple linear regression" data-tertiary="regression equation" id="ix_regrsimlinequ"/><a data-type="indexterm" data-primary="linear regression" data-secondary="simple" data-tertiary="regression equation" id="ix_linregrsimequ"/>
With the correlation coefficient, the variables <em>X</em> and <em>Y</em> are interchangeable.
With regression, we are trying to predict the <em>Y</em> variable from <em>X</em> using a linear relationship (i.e., a line):</p>
<div data-type="equation">
<math display="block" alttext="upper Y equals b 0 plus b 1 upper X">
  <mrow>
    <mi>Y</mi>
    <mo>=</mo>
    <msub><mi>b</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>b</mi> <mn>1</mn> </msub>
    <mi>X</mi>
  </mrow>
</math>
</div>

<p>We read this as “Y equals b<sub>1</sub> times X, plus a constant b<sub>0</sub>.”
The symbol <math alttext="b 0">
  <msub><mi>b</mi> <mn>0</mn> </msub>
</math> is known as the <em>intercept</em> (or constant), and the symbol  <math alttext="b 1">
  <msub><mi>b</mi> <mn>1</mn> </msub>
</math> as the <em>slope</em> for <em>X</em>.  Both appear in <em>R</em> output as <em>coefficients</em>, though in general use the term <em>coefficient</em> is often reserved for <math alttext="b 1">
  <msub><mi>b</mi> <mn>1</mn> </msub>
</math>.<a data-type="indexterm" data-primary="intercept" id="idm46522856530312"/><a data-type="indexterm" data-primary="slope" data-seealso="regression coefficients" id="idm46522856529576"/><a data-type="indexterm" data-primary="coefficients" data-secondary="in simple linear regression" id="idm46522856528632"/>
The <em>Y</em> variable is known as the <em>response</em> or <em>dependent</em> variable since it depends on <em>X</em>.<a data-type="indexterm" data-primary="response" id="idm46522856525752"/><a data-type="indexterm" data-primary="dependent variables" data-seealso="response" id="idm46522856525016"/>
The <em>X</em> variable is known as the <em>predictor</em> or <em>independent</em> variable.
The machine learning community tends to use other terms,
calling <em>Y</em> the <em>target</em> and <em>X</em> a <em>feature</em> vector. Throughout this book, we will use the terms <em>predictor</em> and <em>feature</em> interchangeably.<a data-type="indexterm" data-primary="features" data-seealso="predictor variables" id="idm46522856519928"/><a data-type="indexterm" data-primary="predictor variables" id="idm46522856518920"/><a data-type="indexterm" data-primary="independent variables" data-seealso="predictor variables" id="idm46522856518248"/></p>

<p>Consider the scatterplot in <a data-type="xref" href="#cotton">Figure 4-1</a> displaying the number of years a worker was exposed to cotton dust (<code>Exposure</code>) versus a measure
of lung capacity (<code>PEFR</code> or “peak expiratory flow rate”).
How is <code>PEFR</code> related to <code>Exposure</code>?
It’s hard to tell based just on the picture.</p>

<figure><div id="cotton" class="figure">
<img src="Images/psd2_0401.png" alt="images/lung_scatter.png" width="1178" height="1188"/>
<h6><span class="label">Figure 4-1. </span>Cotton exposure versus lung capacity</h6>
</div></figure>

<p>Simple linear regression tries to find the “best” line to predict the response <code>PEFR</code> as a function of the predictor variable <code>Exposure</code>:</p>
<div data-type="equation">
<math display="block" alttext="PEFR equals b 0 plus b 1 Exposure">
  <mrow>
    <mtext>PEFR</mtext>
    <mo>=</mo>
    <msub><mi>b</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>b</mi> <mn>1</mn> </msub>
    <mtext>Exposure</mtext>
  </mrow>
</math>
</div>

<p>The <code>lm</code> function in <em>R</em> can be used to fit a linear regression:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">model</code> <code class="o">&lt;-</code> <code class="nf">lm</code><code class="p">(</code><code class="n">PEFR</code> <code class="o">~</code> <code class="n">Exposure</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">lung</code><code class="p">)</code></pre>

<p><code>lm</code> stands for <em>linear model</em>, and <a data-type="indexterm" data-primary="linear model (lm)" id="idm46522856491816"/>the <code>~</code> symbol denotes that <code>PEFR</code> is predicted by <code>Exposure</code>. With this model definition, the intercept is automatically included and fitted. If you want to exclude the intercept from the model, you need to write the model definition as follows:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">PEFR</code> <code class="o">~</code> <code class="n">Exposure</code> <code class="o">-</code> <code class="m">1</code></pre>

<p class="pagebreak-before">Printing the <code>model</code> object produces the following output:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">Call</code><code class="o">:</code>
<code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">PEFR</code> <code class="o">~</code> <code class="n">Exposure</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">lung</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>     <code class="n">Exposure</code>
    <code class="m">424.583</code>       <code class="m">-4.185</code></pre>

<p>The intercept, or <math alttext="b 0">
  <msub><mi>b</mi> <mn>0</mn> </msub>
</math>, is 424.583 and can be interpreted as the predicted <code>PEFR</code> for a worker with zero years exposure.
The <a data-type="indexterm" data-primary="regression coefficients" id="idm46522856411400"/>regression coefficient, or <math alttext="b 1">
  <msub><mi>b</mi> <mn>1</mn> </msub>
</math>, can be interpreted as follows: for each additional year that a worker is exposed to cotton dust, the worker’s <code>PEFR</code> measurement is reduced by –4.185.</p>

<p>In <em>Python</em>, we can use <code>LinearRegression</code> from the <code>scikit-learn</code> package. (the <code>statsmodels</code> package has a linear regression implementation that is more similar to <em>R</em> (<code>sm.OLS</code>); we will use it later in this chapter):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'Exposure'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'PEFR'</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">lung</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">lung</code><code class="p">[</code><code class="n">outcome</code><code class="p">])</code>

<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Intercept: {model.intercept_:.3f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Coefficient Exposure: {model.coef_[0]:.3f}'</code><code class="p">)</code></pre>

<p>The regression line from this model is displayed in <a data-type="xref" href="#lung_model">Figure 4-2</a>.</p>

<figure class="width-75"><div id="lung_model" class="figure">
<img src="Images/psd2_0402.png" alt="images/lung_model.png" width="1178" height="1190"/>
<h6><span class="label">Figure 4-2. </span>Slope and intercept for the regression fit to the lung data</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Fitted Values and Residuals"><div class="sect2" id="idm46522856555272">
<h2>Fitted Values and Residuals</h2>

<p>Important concepts in regression analysis are the <em>fitted values</em> (the predictions) and  <em>residuals</em> (prediction errors).<a data-type="indexterm" data-primary="regression" data-secondary="simple linear regression" data-tertiary="regression equation" data-startref="ix_regrsimlinequ" id="idm46522856331784"/><a data-type="indexterm" data-primary="linear regression" data-secondary="simple" data-tertiary="regression equation" data-startref="ix_linregrsimequ" id="idm46522856330264"/>
In general, the data<a data-type="indexterm" data-primary="fitted values" data-secondary="in simple linear regression" id="idm46522856328648"/><a data-type="indexterm" data-primary="residuals" data-secondary="in simple linear regression" id="idm46522856327704"/><a data-type="indexterm" data-primary="regression" data-secondary="simple linear regression" data-tertiary="fitted values and residuals" id="idm46522856326792"/><a data-type="indexterm" data-primary="linear regression" data-secondary="simple" data-tertiary="fitted values and residuals" id="idm46522856325544"/><a data-type="indexterm" data-primary="prediction" data-secondary="fitted values and residuals in simple linear regression" id="idm46522856324312"/><a data-type="indexterm" data-primary="errors" data-secondary="prediction errors" data-seealso="residuals" id="idm46522856323272"/> doesn’t fall exactly on a line, so the regression equation should include an explicit error term <math alttext="e Subscript i">
  <msub><mi>e</mi> <mi>i</mi> </msub>
</math>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>Y</mi> <mi>i</mi> </msub>
    <mo>=</mo>
    <msub><mi>b</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>b</mi> <mn>1</mn> </msub>
    <msub><mi>X</mi> <mi>i</mi> </msub>
    <mo>+</mo>
    <msub><mi>e</mi> <mi>i</mi> </msub>
  </mrow>
</math>
</div>

<p>The fitted values, also <a data-type="indexterm" data-primary="predicted values" data-seealso="fitted values" id="idm46522856310296"/>referred to as the <em>predicted values</em>,
are typically denoted by  <math alttext="ModifyingAbove upper Y With caret Subscript i">
  <msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
</math> (Y-hat).
These are given by:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
    <mo>=</mo>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
    <msub><mi>X</mi> <mi>i</mi> </msub>
  </mrow>
</math>
</div>

<p>The notation <math alttext="ModifyingAbove b With caret Subscript 0">
  <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
</math> and  <math alttext="ModifyingAbove b With caret Subscript 1">
  <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
</math> indicates that the coefficients are estimated versus known.</p>
<div data-type="tip" id="Hat_notation"><h1>Hat Notation: Estimates Versus Known Values</h1>
<p>The “hat” notation is used to differentiate between estimates and known values.<a data-type="indexterm" data-primary="hat notation, estimates versus known values" id="idm46522856263800"/><a data-type="indexterm" data-primary="estimates" data-secondary="hat notation and" id="idm46522856263000"/>
So the symbol <math alttext="ModifyingAbove b With caret">
  <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
</math> (“b-hat”) is an estimate of the unknown parameter <math alttext="b">
  <mi>b</mi>
</math>.
Why do statisticians differentiate between the estimate and the true value?
The estimate has uncertainty, whereas the true value is fixed.<sup><a data-type="noteref" id="idm46522856258456-marker" href="ch04.xhtml#idm46522856258456">2</a></sup></p>
</div>

<p>We compute the residuals <math alttext="ModifyingAbove e With caret Subscript i">
  <msub><mover accent="true"><mi>e</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
</math> by subtracting the <em>predicted</em> values from the original data:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mover accent="true"><mi>e</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
    <mo>=</mo>
    <msub><mi>Y</mi> <mi>i</mi> </msub>
    <mo>-</mo>
    <msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
  </mrow>
</math>
</div>

<p>In <em>R</em>, we can obtain the fitted values and residuals using the functions <code>predict</code> and <code>residuals</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">fitted</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
<code class="n">resid</code> <code class="o">&lt;-</code> <code class="nf">residuals</code><code class="p">(</code><code class="n">model</code><code class="p">)</code></pre>

<p>With <code>scikit-learn</code>’s <code>LinearRegression</code> model, we use the <code>predict</code> method on the training data to get the <code>fitted</code> values and subsequently the <code>residuals</code>. As we will see, this is a general pattern that all models in <code>scikit-learn</code> follow:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fitted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">lung</code><code class="p">[</code><code class="n">predictors</code><code class="p">])</code>
<code class="n">residuals</code> <code class="o">=</code> <code class="n">lung</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code> <code class="o">-</code> <code class="n">fitted</code></pre>

<p><a data-type="xref" href="#residuals">Figure 4-3</a> illustrates the residuals from the regression line fit to the lung data.
The residuals are the length of the vertical dashed lines from the data to the line.</p>

<figure><div id="residuals" class="figure">
<img src="Images/psd2_0403.png" alt="images/lung_residuals.png" width="1178" height="1188"/>
<h6><span class="label">Figure 4-3. </span>Residuals from a regression line (to accommodate all the data, the y-axis scale differs from <a data-type="xref" href="#lung_model">Figure 4-2</a>, hence the apparently different slope)</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Least Squares"><div class="sect2" id="OLS">
<h2>Least Squares</h2>

<p>How is the model fit to the data?
When there is a clear relationship, you could imagine fitting the line by hand.<a data-type="indexterm" data-primary="residual sum of squares (RSS)" id="idm46522856209496"/><a data-type="indexterm" data-primary="regression" data-secondary="simple linear regression" data-tertiary="least squares" id="idm46522856208824"/><a data-type="indexterm" data-primary="linear regression" data-secondary="simple" data-tertiary="least squares" id="idm46522856185080"/><a data-type="indexterm" data-primary="RSS" data-see="residual sum of squares" id="idm46522856183864"/>
In practice, the regression line is the estimate that minimizes the sum of squared residual values, also called the <em>residual sum of squares</em> or <em>RSS</em>:</p>
<div data-type="equation">
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mi>R</mi>
          <mi>S</mi>
          <mi>S</mi>
        </mrow>
      </mtd>
      <mtd>
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
          <msup><mfenced separators="" open="(" close=")"><msub><mi>Y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub></mfenced> <mn>2</mn> </msup>
        </mrow>
      </mtd>
    </mtr>
    <mtr>
      <mtd/>
      <mtd>
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </munderover>
          <msup><mfenced separators="" open="(" close=")"><msub><mi>Y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn> </msub><mo>-</mo><msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub><msub><mi>X</mi> <mi>i</mi> </msub></mfenced> <mn>2</mn> </msup>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>The estimates <math alttext="ModifyingAbove b With caret Subscript 0">
  <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
</math> and  <math alttext="ModifyingAbove b With caret Subscript 1">
  <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
</math> are the values that minimize RSS.</p>

<p>The method of minimizing the sum of the squared residuals is termed <em>least squares</em> regression, or <em>ordinary least squares</em> (OLS) regression.<a data-type="indexterm" data-primary="least squares regression" id="idm46522856118312"/><a data-type="indexterm" data-primary="ordinary least squares (OLS) regression" id="idm46522856117560"/>
It is often attributed to Carl Friedrich Gauss, the German mathematician, but was first published by<a data-type="indexterm" data-primary="Gauss, Carl Friedrich" id="idm46522856116632"/> the French mathematician Adrien-Marie Legendre in 1805.<a data-type="indexterm" data-primary="Legendre, Adrien-Marie" id="idm46522856115832"/>
Least squares regression can be computed quickly and easily with any standard statistical software.</p>

<p>Historically, computational convenience is one reason for the widespread use of least squares in regression.
With the advent of big data, computational speed is still an important factor.
Least squares, like the mean (see <a data-type="xref" href="ch01.xhtml#Median">“Median and Robust Estimates”</a>), are sensitive to outliers, although this tends to be a significant problem only in small or moderate-sized data sets.
See <a data-type="xref" href="#regression_outliers">“Outliers”</a> for a discussion of outliers in regression.</p>
<div data-type="note" epub:type="note"><h1>Regression Terminology</h1>
<p>When analysts and researchers use the term <em>regression</em> by itself, they are typically referring to linear regression; the focus is usually on developing a linear model to explain the relationship between predictor variables and a numeric outcome variable.<a data-type="indexterm" data-primary="regression" data-secondary="meanings of term" id="idm46522856110616"/>
In its formal statistical sense, regression also includes nonlinear models that yield a functional relationship between predictors and outcome variables.
In the machine learning community, the term is also occasionally used loosely to refer to the use of any predictive model that produces a predicted numeric outcome (as opposed to classification methods that predict a binary or categorical outcome).</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Prediction Versus Explanation (Profiling)"><div class="sect2" id="idm46522856108840">
<h2>Prediction Versus Explanation (Profiling)</h2>

<p>Historically, a primary use of regression was to illuminate a supposed linear relationship between predictor variables and an outcome variable.<a data-type="indexterm" data-primary="prediction" data-secondary="prediction vs. explanation in simple linear regression" id="idm46522856107176"/><a data-type="indexterm" data-primary="explanation (profiling), prediction versus" id="idm46522856106104"/><a data-type="indexterm" data-primary="linear regression" data-secondary="prediction vs. explanation" id="idm46522856105400"/><a data-type="indexterm" data-primary="regression" data-secondary="simple linear regression" data-tertiary="prediction vs. explanation" id="idm46522856104488"/>
The goal has been to understand a relationship and explain it using the data that the regression was fit to.
In this case, the primary focus is on the estimated slope of the regression equation, <math alttext="ModifyingAbove b With caret">
  <mover accent="true"><mi>b</mi> <mo>^</mo></mover>
</math>.
Economists want to know the relationship between consumer spending and GDP growth.
Public health officials might want to understand whether a public information campaign is effective in promoting safe sex practices.
In such cases, the focus is not on predicting individual cases but rather on understanding the overall relationship among variables.</p>

<p>With the advent of big data,
regression is widely used to form a model to predict individual outcomes for new data (i.e., a predictive model) rather than explain data in hand.
In this instance, the main items of interest are the fitted values <math alttext="ModifyingAbove upper Y With caret">
  <mover accent="true"><mi>Y</mi> <mo>^</mo></mover>
</math>.
In marketing, regression can be used to predict the change in revenue in response to the size of an ad campaign.
Universities use regression to predict students’ GPA based on their SAT scores.</p>

<p>A regression model that fits the data well is set up such that changes in <em>X</em> lead to changes in <em>Y</em>.<a data-type="indexterm" data-primary="causation, regression and" id="idm46522856096296"/><a data-type="indexterm" data-primary="regression" data-secondary="causation caution" id="idm46522856095544"/>
However, by itself, the regression equation does not prove the direction of causation.
Conclusions about causation must come from a broader understanding about the relationship.
For example, a regression equation might show a definite relationship between number of clicks on a web ad and number of conversions.
It is our knowledge of the marketing process, not the regression equation, that leads us to the conclusion that clicks on the ad lead to sales, and not vice versa.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856093848">
<h5>Key Ideas</h5>
<ul>
<li>
<p>The regression equation models the relationship between a response  variable <em>Y</em> and a predictor variable <em>X</em> as a line.</p>
</li>
<li>
<p>A regression model yields fitted values and residuals—predictions of the response and the errors of the predictions.</p>
</li>
<li>
<p>Regression models are typically fit by the method of least squares.</p>
</li>
<li>
<p>Regression is used both for prediction and explanation.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522856087752">
<h2>Further Reading</h2>

<p>For an in-depth treatment of prediction versus explanation, see Galit Shmueli’s article <a href="https://oreil.ly/4fVUY">“To Explain or to Predict?”</a>.<a data-type="indexterm" data-primary="linear regression" data-secondary="simple" data-startref="ix_linregsim" id="idm46522856085624"/><a data-type="indexterm" data-primary="regression" data-secondary="simple linear regression" data-startref="ix_regrsimlin" id="idm46522856084376"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Multiple Linear Regression"><div class="sect1" id="MultipleLinearRegression">
<h1>Multiple Linear Regression</h1>

<p>When there are multiple predictors, the equation<a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" id="ix_regrMLR"/><a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" id="ix_linregmul"/> is simply extended to accommodate them:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>Y</mi>
    <mo>=</mo>
    <msub><mi>b</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>b</mi> <mn>1</mn> </msub>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>+</mo>
    <msub><mi>b</mi> <mn>2</mn> </msub>
    <msub><mi>X</mi> <mn>2</mn> </msub>
    <mo>+</mo>
    <mo>...</mo>
    <mo>+</mo>
    <msub><mi>b</mi> <mi>p</mi> </msub>
    <msub><mi>X</mi> <mi>p</mi> </msub>
    <mo>+</mo>
    <mi>e</mi>
  </mrow>
</math>
</div>

<p>Instead of a line, we now have a linear model—the relationship between each coefficient and its variable (feature) is linear.<a data-type="indexterm" data-primary="linear model (lm)" id="idm46522856063288"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856062456">
<h5>Key Terms for Multiple Linear Regression</h5><dl>
<dt class="horizontal"><strong><em>Root mean squared error</em></strong></dt>
<dd>
<p>The square root of the average squared error of the regression (this is the most widely used metric to compare regression models).<a data-type="indexterm" data-primary="root mean squared error (RMSE)" id="idm46522856059144"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>RMSE<a data-type="indexterm" data-primary="RMSE" data-see="root mean squared error" id="idm46522856057000"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Residual standard error</em></strong></dt>
<dd>
<p>The same as the root mean squared error, but adjusted for degrees of freedom.<a data-type="indexterm" data-primary="residual standard error (RSE)" id="idm46522856053928"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>RSE<a data-type="indexterm" data-primary="RSE" data-see="residual standard error" id="idm46522856051736"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>R-squared</em></strong></dt>
<dd>
<p>The proportion of variance explained by the model, from 0 to 1.<a data-type="indexterm" data-primary="R-squared" id="idm46522856048664"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>coefficient of determination, <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>t-statistic</em></strong></dt>
<dd>
<p>The coefficient for a predictor, divided by the standard error of the coefficient, giving a metric to compare the importance of variables in the model.<a data-type="indexterm" data-primary="t-statistic" id="idm46522856042376"/> See <a data-type="xref" href="ch03.xhtml#tTest">“t-Tests”</a>.</p>
</dd>
<dt class="horizontal"><strong><em>Weighted regression</em></strong></dt>
<dd>
<p>Regression with the records having different weights.<a data-type="indexterm" data-primary="weighted regression" id="idm46522856038808"/></p>
</dd>
</dl>
</div></aside>

<p>All of the other concepts in simple linear regression,
such as <a data-type="indexterm" data-primary="fitted values" data-secondary="in multiple linear regression" id="idm46522856037592"/>fitting by least squares and the definition of fitted values and residuals, extend to the multiple linear regression setting.<a data-type="indexterm" data-primary="residuals" data-secondary="in multiple linear regression" id="idm46522856036280"/>
For example, the fitted values are given by:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
    <mo>=</mo>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn> </msub>
    <msub><mi>X</mi> <mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow> </msub>
    <mo>+</mo>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>2</mn> </msub>
    <msub><mi>X</mi> <mrow><mn>2</mn><mo>,</mo><mi>i</mi></mrow> </msub>
    <mo>+</mo>
    <mo>...</mo>
    <mo>+</mo>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>p</mi> </msub>
    <msub><mi>X</mi> <mrow><mi>p</mi><mo>,</mo><mi>i</mi></mrow> </msub>
  </mrow>
</math>
</div>








<section data-type="sect2" data-pdf-bookmark="Example: King County Housing Data"><div class="sect2" id="KingCountyHousingData">
<h2>Example: King County Housing Data</h2>

<p>An example of using multiple linear regression is in estimating the value of houses.<a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="example, estimating value of houses" id="ix_linregmulex"/><a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="example, estimating value of houses" id="ix_regrMLRex"/>
County assessors must estimate the value of a house for the purposes of assessing taxes.
Real estate professionals and home buyers consult popular websites such as <a href="https://zillow.com">Zillow</a> to ascertain a fair price.
Here are a few rows of housing data from King County (Seattle), Washington, from the <code>house data.frame</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">head</code><code class="p">(</code><code class="n">house</code><code class="p">[,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'AdjSalePrice'</code><code class="p">,</code> <code class="s">'SqFtTotLiving'</code><code class="p">,</code> <code class="s">'SqFtLot'</code><code class="p">,</code> <code class="s">'Bathrooms'</code><code class="p">,</code>
               <code class="s">'Bedrooms'</code><code class="p">,</code> <code class="s">'BldgGrade'</code><code class="p">)])</code>
<code class="n">Source</code><code class="o">:</code> <code class="n">local</code> <code class="n">data</code> <code class="n">frame</code> <code class="p">[</code><code class="m">6</code> <code class="n">x</code> <code class="m">6</code><code class="p">]</code>

  <code class="n">AdjSalePrice</code> <code class="n">SqFtTotLiving</code> <code class="n">SqFtLot</code> <code class="n">Bathrooms</code> <code class="n">Bedrooms</code> <code class="nf">BldgGrade</code>
<code class="nf">         </code><code class="p">(</code><code class="n">dbl</code><code class="p">)</code>         <code class="p">(</code><code class="n">int</code><code class="p">)</code>   <code class="p">(</code><code class="n">int</code><code class="p">)</code>     <code class="p">(</code><code class="n">dbl</code><code class="p">)</code>    <code class="p">(</code><code class="n">int</code><code class="p">)</code>     <code class="p">(</code><code class="n">int</code><code class="p">)</code>
<code class="m">1</code>       <code class="m">300805</code>          <code class="m">2400</code>    <code class="m">9373</code>      <code class="m">3.00</code>        <code class="m">6</code>         <code class="m">7</code>
<code class="m">2</code>      <code class="m">1076162</code>          <code class="m">3764</code>   <code class="m">20156</code>      <code class="m">3.75</code>        <code class="m">4</code>        <code class="m">10</code>
<code class="m">3</code>       <code class="m">761805</code>          <code class="m">2060</code>   <code class="m">26036</code>      <code class="m">1.75</code>        <code class="m">4</code>         <code class="m">8</code>
<code class="m">4</code>       <code class="m">442065</code>          <code class="m">3200</code>    <code class="m">8618</code>      <code class="m">3.75</code>        <code class="m">5</code>         <code class="m">7</code>
<code class="m">5</code>       <code class="m">297065</code>          <code class="m">1720</code>    <code class="m">8620</code>      <code class="m">1.75</code>        <code class="m">4</code>         <code class="m">7</code>
<code class="m">6</code>       <code class="m">411781</code>           <code class="m">930</code>    <code class="m">1012</code>      <code class="m">1.50</code>        <code class="m">2</code>         <code class="m">8</code></pre>

<p>The <code>head</code> method of <code>pandas</code> data frame lists the top rows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">subset</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'AdjSalePrice'</code><code class="p">,</code> <code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="s1">'SqFtLot'</code><code class="p">,</code> <code class="s1">'Bathrooms'</code><code class="p">,</code>
          <code class="s1">'Bedrooms'</code><code class="p">,</code> <code class="s1">'BldgGrade'</code><code class="p">]</code>
<code class="n">house</code><code class="p">[</code><code class="n">subset</code><code class="p">]</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p>The goal is to predict the sales price from the other variables.
The <code>lm</code> function handles the multiple regression case simply by including more terms on the righthand side of the equation; the argument <code>na.action=na.omit</code> causes the model to drop records that have missing values:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">house_lm</code> <code class="o">&lt;-</code> <code class="nf">lm</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
               <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code><code class="p">,</code>
               <code class="n">data</code><code class="o">=</code><code class="n">house</code><code class="p">,</code> <code class="n">na.action</code><code class="o">=</code><code class="n">na.omit</code><code class="p">)</code></pre>

<p><code>scikit-learn</code>’s <code>LinearRegression</code> can be used for multiple linear regression as well:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="s1">'SqFtLot'</code><code class="p">,</code> <code class="s1">'Bathrooms'</code><code class="p">,</code> <code class="s1">'Bedrooms'</code><code class="p">,</code> <code class="s1">'BldgGrade'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'AdjSalePrice'</code>

<code class="n">house_lm</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">house_lm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">])</code></pre>

<p>Printing <code>house_lm</code> object produces the following output:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">house_lm</code>

<code class="n">Call</code><code class="o">:</code>
<code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
    <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house</code><code class="p">,</code> <code class="n">na.action</code> <code class="o">=</code> <code class="n">na.omit</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
  <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>  <code class="n">SqFtTotLiving</code>        <code class="n">SqFtLot</code>      <code class="n">Bathrooms</code>
   <code class="m">-5.219e+05</code>      <code class="m">2.288e+02</code>     <code class="m">-6.047e-02</code>     <code class="m">-1.944e+04</code>
     <code class="n">Bedrooms</code>      <code class="n">BldgGrade</code>
   <code class="m">-4.777e+04</code>      <code class="m">1.061e+05</code></pre>

<p>For a <code>LinearRegression</code> model, intercept and coefficients are the fields <code>intercept_</code> and <code>coef_</code> of the fitted model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Intercept: {house_lm.intercept_:.3f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Coefficients:'</code><code class="p">)</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">coef</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">predictors</code><code class="p">,</code> <code class="n">house_lm</code><code class="o">.</code><code class="n">coef_</code><code class="p">):</code>
    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">' {name}: {coef}'</code><code class="p">)</code></pre>

<p>The interpretation of<a data-type="indexterm" data-primary="coefficients" data-secondary="in multiple linear regression" id="idm46522855617240"/> the coefficients is as with simple linear regression:
the predicted value <math alttext="ModifyingAbove upper Y With caret">
  <mover accent="true"><mi>Y</mi> <mo>^</mo></mover>
</math> changes by the coefficient <math alttext="b Subscript j">
  <msub><mi>b</mi> <mi>j</mi> </msub>
</math> for each unit change in <math alttext="upper X Subscript j">
  <msub><mi>X</mi> <mi>j</mi> </msub>
</math> assuming all the other variables, <math alttext="upper X Subscript k">
  <msub><mi>X</mi> <mi>k</mi> </msub>
</math> for <math alttext="k not-equals j">
  <mrow>
    <mi>k</mi>
    <mo>≠</mo>
    <mi>j</mi>
  </mrow>
</math>,
remain the same.
For example,
adding an extra finished square foot to a house increases the estimated value by roughly $229;
adding 1,000 finished square feet implies the value will increase by $228,800.<a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="example, estimating value of houses" data-startref="ix_linregmulex" id="idm46522855571832"/><a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="example, estimating value of houses" data-startref="ix_regrMLRex" id="idm46522855570248"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Assessing the Model"><div class="sect2" id="RMSE">
<h2>Assessing the Model</h2>

<p>The most important performance metric from a data science perspective is <em>root mean squared error</em>, or <em>RMSE</em>.<a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="assessing the model" id="idm46522855565432"/><a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="assessing the model" id="idm46522855564264"/><a data-type="indexterm" data-primary="root mean squared error (RMSE)" id="idm46522855563080"/>
RMSE is the square root of the average squared error in the predicted <math alttext="ModifyingAbove y With caret Subscript i">
  <msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
</math> values:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>R</mi>
    <mi>M</mi>
    <mi>S</mi>
    <mi>E</mi>
    <mo>=</mo>
    <msqrt>
      <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub></mfenced> <mn>2</mn> </msup></mrow> <mi>n</mi></mfrac>
    </msqrt>
  </mrow>
</math>
</div>

<p>This measures the overall accuracy of the model and is a basis for comparing it to other models (including models fit using machine learning techniques).
Similar to RMSE is <a data-type="indexterm" data-primary="residual standard error (RSE)" id="idm46522855526856"/>the <em>residual standard error</em>, or <em>RSE</em>.
In this case we have <em>p</em> predictors, and the RSE is given by:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>R</mi>
    <mi>S</mi>
    <mi>E</mi>
    <mo>=</mo>
    <msqrt>
      <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub></mfenced> <mn>2</mn> </msup></mrow> <mfenced separators="" open="(" close=")"><mi>n</mi><mo>-</mo><mi>p</mi><mo>-</mo><mn>1</mn></mfenced></mfrac>
    </msqrt>
  </mrow>
</math>
</div>

<p>The only difference is that the denominator is the degrees of freedom, as opposed to number of records (see  <a data-type="xref" href="ch03.xhtml#DOF">“Degrees of Freedom”</a>).
In practice, for linear regression, the difference between RMSE and RSE is very small, particularly for big data applications.</p>

<p>The <code>summary</code> function in <em>R</em> computes RSE as well as other metrics for a regression model:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">summary</code><code class="p">(</code><code class="n">house_lm</code><code class="p">)</code>

<code class="n">Call</code><code class="o">:</code>
<code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
    <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house</code><code class="p">,</code> <code class="n">na.action</code> <code class="o">=</code> <code class="n">na.omit</code><code class="p">)</code>

<code class="n">Residuals</code><code class="o">:</code>
     <code class="n">Min</code>       <code class="m">1</code><code class="n">Q</code>   <code class="n">Median</code>       <code class="m">3</code><code class="n">Q</code>      <code class="n">Max</code>
<code class="m">-1199479</code>  <code class="m">-118908</code>   <code class="m">-20977</code>    <code class="m">87435</code>  <code class="m">9473035</code>

<code class="n">Coefficients</code><code class="o">:</code>
                <code class="n">Estimate</code> <code class="n">Std.</code> <code class="n">Error</code> <code class="n">t</code> <code class="n">value</code> <code class="nf">Pr</code><code class="p">(</code><code class="o">&gt;|</code><code class="n">t</code><code class="o">|</code><code class="p">)</code>
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>   <code class="m">-5.219e+05</code>  <code class="m">1.565e+04</code> <code class="m">-33.342</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">SqFtTotLiving</code>  <code class="m">2.288e+02</code>  <code class="m">3.899e+00</code>  <code class="m">58.694</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">SqFtLot</code>       <code class="m">-6.047e-02</code>  <code class="m">6.118e-02</code>  <code class="m">-0.988</code>    <code class="m">0.323</code>
<code class="n">Bathrooms</code>     <code class="m">-1.944e+04</code>  <code class="m">3.625e+03</code>  <code class="m">-5.363</code> <code class="m">8.27e-08</code> <code class="o">***</code>
<code class="n">Bedrooms</code>      <code class="m">-4.777e+04</code>  <code class="m">2.490e+03</code> <code class="m">-19.187</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="n">BldgGrade</code>      <code class="m">1.061e+05</code>  <code class="m">2.396e+03</code>  <code class="m">44.277</code>  <code class="o">&lt;</code> <code class="m">2e-16</code> <code class="o">***</code>
<code class="o">---</code>
<code class="n">Signif.</code> <code class="n">codes</code><code class="o">:</code>  <code class="m">0</code> ‘<code class="o">***</code>’ <code class="m">0.001</code> ‘<code class="o">**</code>’ <code class="m">0.01</code> ‘<code class="o">*</code>’ <code class="m">0.05</code> ‘<code class="n">.’</code> <code class="m">0.1</code> ‘ ’ <code class="m">1</code>

<code class="n">Residual</code> <code class="n">standard</code> <code class="n">error</code><code class="o">:</code> <code class="m">261300</code> <code class="n">on</code> <code class="m">22681</code> <code class="n">degrees</code> <code class="n">of</code> <code class="n">freedom</code>
<code class="n">Multiple</code> <code class="n">R</code><code class="o">-</code><code class="n">squared</code><code class="o">:</code>  <code class="m">0.5406</code><code class="p">,</code>	<code class="n">Adjusted</code> <code class="n">R</code><code class="o">-</code><code class="n">squared</code><code class="o">:</code>  <code class="m">0.5405</code>
<code class="bp">F</code><code class="o">-</code><code class="n">statistic</code><code class="o">:</code>  <code class="m">5338</code> <code class="n">on</code> <code class="m">5</code> <code class="n">and</code> <code class="m">22681</code> <code class="n">DF</code><code class="p">,</code>  <code class="n">p</code><code class="o">-</code><code class="n">value</code><code class="o">:</code> <code class="o">&lt;</code> <code class="m">2.2e-16</code></pre>

<p><code>scikit-learn</code> provides a number of metrics for regression and classification.
Here, we use <code>mean_squared_error</code> to get RMSE and
<code>r2_score</code> for the coefficient of <span class="keep-together">determination</span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fitted</code> <code class="o">=</code> <code class="n">house_lm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">])</code>
<code class="n">RMSE</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">],</code> <code class="n">fitted</code><code class="p">))</code>
<code class="n">r2</code> <code class="o">=</code> <code class="n">r2_score</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">],</code> <code class="n">fitted</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'RMSE: {RMSE:.0f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'r2: {r2:.4f}'</code><code class="p">)</code></pre>

<p>Use <code>statsmodels</code> to get a more detailed analysis of the regression model in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">OLS</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">],</code> <code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">const</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
<code class="n">results</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code></pre>

<p>The <code>pandas</code> method <code>assign</code>, as used here, adds a constant column with value 1 to the predictors.
This is required to model the intercept.</p>

<p>Another useful metric that <a data-type="indexterm" data-primary="coefficient of determination" id="idm46522855283816"/><a data-type="indexterm" data-primary="R-squared" id="idm46522855283144"/>you will see in software output is the <em>coefficient of determination</em>, also called the <em>R-squared</em> statistic or <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math>.
R-squared ranges from 0 to 1 and measures the proportion of variation in the data that is accounted for in the model.
It is useful mainly in explanatory uses of regression where you want to assess how well the model fits the data.
The formula for <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math> is:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msup><mi>R</mi> <mn>2</mn> </msup>
    <mo>=</mo>
    <mn>1</mn>
    <mo>-</mo>
    <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi> </msub></mfenced> <mn>2</mn> </msup></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover></mfenced> <mn>2</mn> </msup></mrow></mfrac>
  </mrow>
</math>
</div>

<p>The denominator is proportional to the variance of <em>Y</em>.<a data-type="indexterm" data-primary="adjusted R-squared" id="idm46522855257656"/>
The output from <em>R</em> also reports an <em>adjusted R-squared</em>, which adjusts for the degrees of freedom, effectively penalizing the addition of more predictors to a model.  Seldom is this significantly different from <em>R-squared</em> in multiple regression with large data sets.</p>

<p>Along with the estimated coefficients, <em>R</em> and <code>statsmodels</code> report the standard error <a data-type="indexterm" data-primary="t-statistic" id="idm46522855113416"/>of the coefficients (SE) and a <em>t-statistic</em>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>t</mi> <mi>b</mi> </msub>
    <mo>=</mo>
    <mfrac><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mrow><mtext>SE</mtext><mfenced separators="" open="(" close=")"><mover accent="true"><mi>b</mi> <mo>^</mo></mover></mfenced></mrow></mfrac>
  </mrow>
</math>
</div>

<p>The t-statistic—and <a data-type="indexterm" data-primary="p-values" data-secondary="t-statistic and" id="idm46522855103928"/>its mirror image, the p-value—measures the extent to which a coefficient is “statistically significant”—that is, outside the range of what a random chance arrangement of predictor and target variable might produce.
The higher the <span class="keep-together">t-statistic</span> (and the lower the p-value), the more significant the predictor.
Since parsimony is a valuable model feature, it is useful to have a tool like this to guide choice of variables to include as predictors (see <a data-type="xref" href="#StepwiseRegression">“Model Selection and Stepwise Regression”</a>).</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>In addition to the t-statistic, <em>R</em> and other packages will often report a <em>p-value</em>
(<code>Pr(&gt;|t|)</code> in the <em>R</em> output) and <em>F-statistic</em>. Data scientists do not generally get too involved with the interpretation of these statistics, nor with the issue of statistical significance.<a data-type="indexterm" data-primary="F-statistic" id="idm46522855097256"/>
Data scientists primarily focus on the t-statistic as a useful guide for whether to include a predictor in a model or not.<a data-type="indexterm" data-primary="predictor variables" data-secondary="t-statistic and" id="idm46522855096280"/><a data-type="indexterm" data-primary="data science" data-secondary="t-statistic and" id="idm46522855095336"/>
High t-statistics (which go with p-values near 0) indicate a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped.
See <a data-type="xref" href="ch03.xhtml#p-value">“p-Value”</a> for more discussion.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Cross-Validation"><div class="sect2" id="CrossValidation">
<h2>Cross-Validation</h2>

<p>Classic statistical regression metrics (<em>R<sup>2</sup></em>, F-statistics, and p-values) are all “in-sample” metrics—they are applied to the same data that was used to fit the model.<a data-type="indexterm" data-primary="cross validation" id="idm46522855090936"/><a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="cross validation" id="idm46522855090264"/><a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="cross validation" id="idm46522855089048"/>
Intuitively, you can see that it would make a lot of sense to set aside some of the original data, not use it to fit the model, and then apply the model to the set-aside (holdout) data to see how well it does.
Normally, you would use a majority of the data to fit the model and use a smaller portion to test the model.<a data-type="indexterm" data-primary="out-of-sample validation" id="idm46522855087352"/></p>

<p>This idea of “out-of-sample” validation is not new, but it did not really take hold until larger data sets became more prevalent; with a small data set, analysts typically want to use all the data and fit the best possible model.</p>

<p>Using a holdout sample, though, leaves you subject to some uncertainty that arises simply from variability in the small holdout sample.
How different would the assessment be if you selected a different holdout sample?</p>

<p>Cross-validation extends the idea of a holdout sample to multiple sequential holdout samples.<a data-type="indexterm" data-primary="k-fold cross-validation" id="idm46522855084840"/> The algorithm for basic <em>k-fold cross-validation</em> is as follows:</p>
<ol>
<li>
<p>Set aside <em>1/k</em> of the data as a holdout sample.</p>
</li>
<li>
<p>Train the model on the remaining data.</p>
</li>
<li>
<p>Apply (score) the model to the <em>1/k</em> holdout, and record needed model assessment metrics.</p>
</li>
<li>
<p>Restore the first <em>1/k</em> of the data, and set aside the next <em>1/k</em> (excluding any records that got picked the first time).</p>
</li>
<li>
<p>Repeat steps 2 and 3.</p>
</li>
<li>
<p>Repeat until each record has been used in the holdout portion.</p>
</li>
<li>
<p>Average or otherwise combine the model assessment metrics.</p>
</li>

</ol>

<p>The division of the data into the training sample and the holdout sample is also called <a data-type="indexterm" data-primary="folds" id="idm46522855074504"/>a <em>fold</em>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Model Selection and Stepwise Regression"><div class="sect2" id="StepwiseRegression">
<h2>Model Selection and Stepwise Regression</h2>

<p>In some problems,
many variables could be used as predictors in a regression.<a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="model selection and stepwise regression" id="ix_linregmulmod"/><a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="model selection and stepwise regression" id="ix_regrMLRmodsel"/>
For example, to<a data-type="indexterm" data-primary="feature selection" data-secondary="in stepwise regression" id="idm46522855067640"/> predict house value,
additional variables such as the basement size or year built could be used.
In <em>R</em>, these are easy to add to the regression equation:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">house_full</code> <code class="o">&lt;-</code> <code class="nf">lm</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
                 <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">PropertyType</code> <code class="o">+</code> <code class="n">NbrLivingUnits</code> <code class="o">+</code>
                 <code class="n">SqFtFinBasement</code> <code class="o">+</code> <code class="n">YrBuilt</code> <code class="o">+</code> <code class="n">YrRenovated</code> <code class="o">+</code>
                 <code class="n">NewConstruction</code><code class="p">,</code>
               <code class="n">data</code><code class="o">=</code><code class="n">house</code><code class="p">,</code> <code class="n">na.action</code><code class="o">=</code><code class="n">na.omit</code><code class="p">)</code></pre>

<p>In <em>Python</em>, we need to convert the categorical and boolean variables into numbers:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="s1">'SqFtLot'</code><code class="p">,</code> <code class="s1">'Bathrooms'</code><code class="p">,</code> <code class="s1">'Bedrooms'</code><code class="p">,</code> <code class="s1">'BldgGrade'</code><code class="p">,</code>
              <code class="s1">'PropertyType'</code><code class="p">,</code> <code class="s1">'NbrLivingUnits'</code><code class="p">,</code> <code class="s1">'SqFtFinBasement'</code><code class="p">,</code> <code class="s1">'YrBuilt'</code><code class="p">,</code>
              <code class="s1">'YrRenovated'</code><code class="p">,</code> <code class="s1">'NewConstruction'</code><code class="p">]</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">X</code><code class="p">[</code><code class="s1">'NewConstruction'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code> <code class="k">if</code> <code class="n">nc</code> <code class="k">else</code> <code class="mi">0</code> <code class="k">for</code> <code class="n">nc</code> <code class="ow">in</code> <code class="n">X</code><code class="p">[</code><code class="s1">'NewConstruction'</code><code class="p">]]</code>

<code class="n">house_full</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">OLS</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">],</code> <code class="n">X</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">const</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">house_full</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
<code class="n">results</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code></pre>

<p>Adding more variables, however, does not necessarily mean we have a better model.<a data-type="indexterm" data-primary="Occam's razor" id="idm46522855022824"/>
Statisticians use the principle of <em>Occam’s razor</em> to guide the choice of a model:
all things being equal, a simpler model should be used in preference to a more complicated model.</p>

<p>Including additional variables always reduces RMSE and increases <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math> for the training data.
Hence, these are not appropriate to help guide the model choice.
One approach to including model complexity is to use the adjusted <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math>:</p>
<div data-type="equation">
<math>
  <mstyle displaystyle="true">
    <mrow class="MJX-TeXAtom-ORD">
      <msubsup>
        <mi>R</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>a</mi>
          <mi>d</mi>
          <mi>j</mi>
        </mrow>
        <mn>2</mn>
      </msubsup>
      <mo>=</mo>
      <mn>1</mn>
      <mo>−<!-- − --></mo>
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>−<!-- − --></mo>
      <msup>
        <mi>R</mi>
        <mn>2</mn>
      </msup>
      <mo stretchy="false">)</mo>
      <mfrac>
        <mrow>
          <mi>n</mi>
          <mo>−<!-- − --></mo>
          <mn>1</mn>
        </mrow>
        <mrow>
          <mi>n</mi>
          <mo>−<!-- − --></mo>
          <mi>P</mi>
          <mo>−<!-- − --></mo>
          <mn>1</mn>
        </mrow>
      </mfrac>
    </mrow>
  </mstyle>
</math>
</div>

<p>Here, <em>n</em> is the number of records and <em>P</em> is the number of variables in the model.</p>

<p>In the 1970s,
Hirotugu Akaike,
the eminent Japanese statistician,
developed a metric called <em>AIC</em> (Akaike’s Information Criteria) that penalizes adding terms to a model.<a data-type="indexterm" data-primary="AIC (Akaike's Information Criteria)" id="idm46522854883112"/>
In the case of regression, AIC has the form:</p>
<ul class="simplelist">
  <li>AIC = 2<em>P</em> + <em>n</em> log(<code>RSS</code>/<em>n</em>)</li>
</ul>

<p>where <em>P</em> is the number of variables and <em>n</em> is the number of records.
The goal is to find the model that minimizes AIC;
models with <em>k</em> more extra variables are penalized by 2<em>k</em>.</p>
<div data-type="warning" epub:type="warning"><h1>AIC, BIC, and Mallows Cp</h1>
<p id="AIC">The formula for AIC may seem a bit mysterious,
but in fact it is based on asymptotic results in information theory.
There are several variants to AIC:</p>
<dl>
<dt>AICc</dt>
<dd>
<p>A version of AIC corrected for small sample sizes.<a data-type="indexterm" data-primary="AICc" id="idm46522854873416"/></p>
</dd>
<dt>BIC or Bayesian information criteria</dt>
<dd>
<p>Similar to AIC, with a stronger penalty for including additional variables to the model.<a data-type="indexterm" data-primary="Bayesian information criteria (BIC)" id="idm46522854871352"/><a data-type="indexterm" data-primary="Mallows Cp" id="idm46522854870584"/></p>
</dd>
<dt>Mallows Cp</dt>
<dd>
<p>A variant of AIC developed by Colin Mallows.</p>
</dd>
</dl>

<p>These are typically reported as in-sample metrics (i.e., on the training data), and data scientists using holdout data for model assessment do not need to worry about the differences among them or the underlying theory behind them.</p>
</div>

<p>How do we find the model that minimizes AIC or maximizes adjusted <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math>?
One way is to search through all possible models,
an approach called <em>all subset regression</em>.<a data-type="indexterm" data-primary="all subset regression" id="idm46522854864568"/>
This is computationally expensive and is not feasible for problems with large data and many variables.
An attractive alternative is to use <em>stepwise regression</em>. <a data-type="indexterm" data-primary="stepwise regression" id="idm46522854863160"/>It could start with a full model and successively drop variables that don’t contribute meaningfully.<a data-type="indexterm" data-primary="backward elimination" id="idm46522854862184"/> This is called <em>backward elimination</em>.<a data-type="indexterm" data-primary="forward selection" id="idm46522854860968"/> Alternatively one could start with a constant model and successively add variables (<em>forward selection</em>). As a third option we can also successively add and drop predictors to find a model that lowers AIC or adjusted <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math>.
The  <code>MASS</code> in <em>R</em> package by Venebles and Ripley  offers a stepwise regression function called <code>stepAIC</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">MASS</code><code class="p">)</code>
<code class="n">step</code> <code class="o">&lt;-</code> <code class="nf">stepAIC</code><code class="p">(</code><code class="n">house_full</code><code class="p">,</code> <code class="n">direction</code><code class="o">=</code><code class="s">"both"</code><code class="p">)</code>
<code class="n">step</code>

<code class="n">Call</code><code class="o">:</code>
<code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code> <code class="n">Bedrooms</code> <code class="o">+</code>
    <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">PropertyType</code> <code class="o">+</code> <code class="n">SqFtFinBasement</code> <code class="o">+</code> <code class="n">YrBuilt</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house</code><code class="p">,</code>
    <code class="n">na.action</code> <code class="o">=</code> <code class="n">na.omit</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
              <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>              <code class="n">SqFtTotLiving</code>
                <code class="m">6.179e+06</code>                  <code class="m">1.993e+02</code>
                <code class="n">Bathrooms</code>                   <code class="n">Bedrooms</code>
                <code class="m">4.240e+04</code>                 <code class="m">-5.195e+04</code>
                <code class="n">BldgGrade</code>  <code class="n">PropertyTypeSingle</code> <code class="n">Family</code>
                <code class="m">1.372e+05</code>                  <code class="m">2.291e+04</code>
    <code class="n">PropertyTypeTownhouse</code>            <code class="n">SqFtFinBasement</code>
                <code class="m">8.448e+04</code>                  <code class="m">7.047e+00</code>
                  <code class="n">YrBuilt</code>
               <code class="m">-3.565e+03</code></pre>

<p><code>scikit-learn</code> has no implementation for stepwise regression. We implemented functions <code>stepwise_selection</code>, <code>forward_selection</code>, and <code>backward_elimination</code> in our <code>dmba</code> package:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">train_model</code><code class="p">(</code><code class="n">variables</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" id="co_regression_and_prediction_CO1-1" href="#callout_regression_and_prediction_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">variables</code><code class="p">)</code><code> </code><code class="o">==</code><code> </code><code class="mi">0</code><code class="p">:</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="bp">None</code><code>
</code><code>    </code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="n">LinearRegression</code><code class="p">(</code><code class="p">)</code><code>
</code><code>    </code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">[</code><code class="n">variables</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">y</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">model</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">score_model</code><code class="p">(</code><code class="n">model</code><code class="p">,</code><code> </code><code class="n">variables</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" id="co_regression_and_prediction_CO1-2" href="#callout_regression_and_prediction_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>    </code><code class="k">if</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">variables</code><code class="p">)</code><code> </code><code class="o">==</code><code> </code><code class="mi">0</code><code class="p">:</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="n">AIC_score</code><code class="p">(</code><code class="n">y</code><code class="p">,</code><code> </code><code class="p">[</code><code class="n">y</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code class="p">]</code><code> </code><code class="o">*</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">y</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">model</code><code class="p">,</code><code> </code><code class="n">df</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">AIC_score</code><code class="p">(</code><code class="n">y</code><code class="p">,</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">[</code><code class="n">variables</code><code class="p">]</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">model</code><code class="p">)</code><code>
</code><code>
</code><code class="n">best_model</code><code class="p">,</code><code> </code><code class="n">best_variables</code><code> </code><code class="o">=</code><code> </code><code class="n">stepwise_selection</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code><code> </code><code class="n">train_model</code><code class="p">,</code><code>
</code><code>                                                </code><code class="n">score_model</code><code class="p">,</code><code> </code><code class="n">verbose</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code>
</code><code>
</code><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'</code><code class="s1">Intercept: {best_model.intercept_:.3f}</code><code class="s1">'</code><code class="p">)</code><code>
</code><code class="k">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">Coefficients:</code><code class="s1">'</code><code class="p">)</code><code>
</code><code class="k">for</code><code> </code><code class="n">name</code><code class="p">,</code><code> </code><code class="n">coef</code><code> </code><code class="ow">in</code><code> </code><code class="nb">zip</code><code class="p">(</code><code class="n">best_variables</code><code class="p">,</code><code> </code><code class="n">best_model</code><code class="o">.</code><code class="n">coef_</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'</code><code class="s1"> {name}: {coef}</code><code class="s1">'</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_regression_and_prediction_CO1-1" href="#co_regression_and_prediction_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Define a function that returns a fitted model for a given set of variables.</p></dd>
<dt><a class="co" id="callout_regression_and_prediction_CO1-2" href="#co_regression_and_prediction_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Define a function that returns a score for a given model and set of variables. In this case, we use the <code>AIC_score</code> implemented in the <code>dmba</code> package.</p></dd>
</dl>

<p>The function chose a model in which several variables were dropped from <code>house_full</code>:
<code>SqFtLot</code>, <code>NbrLivingUnits</code>, <code>YrRenovated</code>, and <code>NewConstruction</code>.</p>

<p>Simpler yet are <em>forward selection</em> and <em>backward selection</em>.<a data-type="indexterm" data-primary="forward selection" id="idm46522854542424"/><a data-type="indexterm" data-primary="backward selection" id="idm46522854541688"/>  In forward selection, you start with no predictors and add them one by one, at each step adding the predictor that has the largest contribution to <math alttext="upper R squared">
  <msup><mi>R</mi> <mn>2</mn> </msup>
</math>, and stopping when the contribution is no longer statistically significant.<a data-type="indexterm" data-primary="backward elimination" id="idm46522854538888"/>  In backward selection, or <em>backward elimination</em>, you start with the full model and take away predictors that are not statistically significant until you are left with a model in which all predictors are statistically significant.</p>

<p><em>Penalized regression</em> is similar in spirit to AIC.<a data-type="indexterm" data-primary="penalized regression" id="idm46522854536904"/>
Instead of explicitly searching through a discrete set of models, the model-fitting equation incorporates a constraint that penalizes the model for too many variables (parameters).
Rather than eliminating predictor variables entirely—as with stepwise, forward, and backward selection—penalized regression applies the penalty by reducing coefficients, in some cases to near zero.
Common penalized regression methods are <em>ridge regression</em> and <em>lasso regression</em>.<a data-type="indexterm" data-primary="ridge regression" id="idm46522854534792"/><a data-type="indexterm" data-primary="lasso regression" id="idm46522854534056"/></p>

<p>Stepwise regression and all subset regression are <em>in-sample</em> methods to assess and tune models.<a data-type="indexterm" data-primary="in-sample validation methods" id="idm46522854532536"/>
This means the model selection is possibly subject to overfitting (fitting the noise in the data) and may not perform as well when applied to new data.
One common approach to avoid this is to use cross-validation to validate the models.
In linear regression, overfitting is typically not a major issue, due to the simple (linear) global structure imposed on the data.
For more sophisticated types of models, particularly iterative procedures that respond to local data structure, cross-validation is a very important tool;
see <a data-type="xref" href="#CrossValidation">“Cross-Validation”</a> for details.<a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="model selection and stepwise regression" data-startref="ix_linregmulmod" id="idm46522854530296"/><a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="model selection and stepwise regression" data-startref="ix_regrMLRmodsel" id="idm46522854528760"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Weighted Regression"><div class="sect2" id="WeightedRegression">
<h2>Weighted Regression</h2>

<p>Weighted regression is used by statisticians for a variety of purposes;
in particular, it is important for analysis of complex surveys.<a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="weighted regression" id="ix_regrMLRwgt"/><a data-type="indexterm" data-primary="weighted regression" id="ix_wgtreg"/><a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="weighted regression" id="ix_linregmulwgt"/>
Data scientists may find weighted regression useful in two cases:</p>

<ul>
<li>
<p>Inverse-variance weighting when different observations have been measured with different precision; the higher variance ones receiving lower weights.</p>
</li>
<li>
<p>Analysis of data where rows represent multiple cases; the weight variable encodes how many original observations each row represents.</p>
</li>
</ul>

<p>For example, with the housing data,
older sales are less reliable than more recent sales.
Using the <code>DocumentDate</code> to determine the year of the sale, we can compute a <code>Weight</code> as the number of years since 2005 (the beginning of the data):</p>

<p><em>R</em></p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">lubridate</code><code class="p">)</code>
<code class="n">house</code><code class="o">$</code><code class="n">Year</code> <code class="o">=</code> <code class="nf">year</code><code class="p">(</code><code class="n">house</code><code class="o">$</code><code class="n">DocumentDate</code><code class="p">)</code>
<code class="n">house</code><code class="o">$</code><code class="n">Weight</code> <code class="o">=</code> <code class="n">house</code><code class="o">$</code><code class="n">Year</code> <code class="o">-</code> <code class="m">2005</code></pre>

<p><em>Python</em></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">house</code><code class="p">[</code><code class="s1">'Year'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="nb">int</code><code class="p">(</code><code class="n">date</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'-'</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code> <code class="k">for</code> <code class="n">date</code> <code class="ow">in</code> <code class="n">house</code><code class="o">.</code><code class="n">DocumentDate</code><code class="p">]</code>
<code class="n">house</code><code class="p">[</code><code class="s1">'Weight'</code><code class="p">]</code> <code class="o">=</code> <code class="n">house</code><code class="o">.</code><code class="n">Year</code> <code class="o">-</code> <code class="mi">2005</code></pre>

<p>We can compute a weighted regression with the <code>lm</code> function using the  <code>weight</code> <span class="keep-together">argument</span>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">house_wt</code> <code class="o">&lt;-</code> <code class="nf">lm</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
                 <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code><code class="p">,</code>
               <code class="n">data</code><code class="o">=</code><code class="n">house</code><code class="p">,</code> <code class="n">weight</code><code class="o">=</code><code class="n">Weight</code><code class="p">)</code>
<code class="nf">round</code><code class="p">(</code><code class="nf">cbind</code><code class="p">(</code><code class="n">house_lm</code><code class="o">=</code><code class="n">house_lm</code><code class="o">$</code><code class="n">coefficients</code><code class="p">,</code>
            <code class="n">house_wt</code><code class="o">=</code><code class="n">house_wt</code><code class="o">$</code><code class="n">coefficients</code><code class="p">),</code> <code class="n">digits</code><code class="o">=</code><code class="m">3</code><code class="p">)</code>

                 <code class="n">house_lm</code>    <code class="nf">house_wt</code>
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>   <code class="m">-521871.368</code> <code class="m">-584189.329</code>
<code class="n">SqFtTotLiving</code>     <code class="m">228.831</code>     <code class="m">245.024</code>
<code class="n">SqFtLot</code>            <code class="m">-0.060</code>      <code class="m">-0.292</code>
<code class="n">Bathrooms</code>      <code class="m">-19442.840</code>  <code class="m">-26085.970</code>
<code class="n">Bedrooms</code>       <code class="m">-47769.955</code>  <code class="m">-53608.876</code>
<code class="n">BldgGrade</code>      <code class="m">106106.963</code>  <code class="m">115242.435</code></pre>

<p>The coefficients in the weighted regression are slightly different from the original regression.</p>

<p>Most models in <code>scikit-learn</code> accept weights as the keyword argument <code>sample_weight</code> in the call of the <code>fit</code> method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="s1">'SqFtLot'</code><code class="p">,</code> <code class="s1">'Bathrooms'</code><code class="p">,</code> <code class="s1">'Bedrooms'</code><code class="p">,</code> <code class="s1">'BldgGrade'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'AdjSalePrice'</code>

<code class="n">house_wt</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">house_wt</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">],</code> <code class="n">sample_weight</code><code class="o">=</code><code class="n">house</code><code class="o">.</code><code class="n">Weight</code><code class="p">)</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522854250808">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Multiple linear regression models the relationship between a response  variable <em>Y</em> and multiple predictor variables <math alttext="upper X 1 comma ellipsis comma upper X Subscript p Baseline">
  <mrow>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi> </msub>
  </mrow>
</math>.</p>
</li>
<li>
<p>The most important metrics to evaluate a model are root mean squared error (RMSE) and R-squared (<em>R</em><sup>2</sup>).</p>
</li>
<li>
<p>The standard error of the coefficients can be used to measure the reliability of a variable’s contribution to a model.</p>
</li>
<li>
<p>Stepwise regression is a way to automatically determine which variables should be included in the model.</p>
</li>
<li>
<p>Weighted regression is used to give certain records more or less weight in  fitting the equation.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522854526984">
<h2>Further Reading</h2>

<p>An excellent treatment of cross-validation and resampling can be found in <em>An Introduction to Statistical Learning</em> by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (Springer, 2013).<a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-tertiary="weighted regression" data-startref="ix_regrMLRwgt" id="idm46522854214104"/><a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-tertiary="weighted regression" data-startref="ix_linregmulwgt" id="idm46522854212648"/><a data-type="indexterm" data-primary="weighted regression" data-startref="ix_wgtreg" id="idm46522854211160"/><a data-type="indexterm" data-primary="regression" data-secondary="multiple linear regression" data-startref="ix_regrMLR" id="idm46522854210216"/><a data-type="indexterm" data-primary="linear regression" data-secondary="multiple" data-startref="ix_linregmul" id="idm46522854209032"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Prediction Using Regression"><div class="sect1" id="RegressionPrediction">
<h1>Prediction Using Regression</h1>

<p>The primary purpose of regression in data science is prediction.<a data-type="indexterm" data-primary="regression" data-secondary="prediction with" id="ix_regrpred"/><a data-type="indexterm" data-primary="prediction" data-secondary="using regression" id="ix_predregr"/>
This is useful to keep in mind, since regression, being an old and established statistical method, comes with baggage that is more relevant to its traditional role as a tool for explanatory modeling than to prediction.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522854202952">
<h5>Key Terms for Prediction Using Regression</h5><dl>
<dt class="horizontal"><strong><em>Prediction interval</em></strong></dt>
<dd>
<p>An uncertainty interval around an individual predicted value.<a data-type="indexterm" data-primary="extrapolation" data-secondary="defined" id="idm46522854200232"/></p>
</dd>
<dt class="horizontal"><strong><em>Extrapolation</em></strong></dt>
<dd>
<p>Extension of a model beyond the range of the data used to fit it.</p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="The Dangers of Extrapolation"><div class="sect2" id="Extrapolation">
<h2>The Dangers of Extrapolation</h2>

<p>Regression models should not be used to extrapolate beyond the range of the data (leaving aside the use of regression for time series forecasting.).<a data-type="indexterm" data-primary="prediction" data-secondary="using regression" data-tertiary="dangers of extrapolation" id="idm46522854195208"/><a data-type="indexterm" data-primary="regression" data-secondary="prediction with" data-tertiary="dangers of extrapolation" id="idm46522854193896"/><a data-type="indexterm" data-primary="extrapolation" data-secondary="dangers of" id="idm46522854192664"/>
The model is valid only for predictor values for which the data has sufficient values
(even in the case that sufficient data is available, there could be other problems—see <a data-type="xref" href="#RegressionDiagnostics">“Regression Diagnostics”</a>).
As an extreme case,
suppose <code>model_lm</code> is used to predict the value of a 5,000-square-foot empty lot.
In such a case, all the predictors related to the building would have a value of 0, and the regression equation would yield an absurd prediction of –521,900 + 5,000 × –.0605 = –$522,202.
Why did this happen?
The data contains only parcels with buildings—there are no records corresponding to vacant land.
Consequently, the model has no information to tell it how to predict the sales price for vacant land.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Confidence and Prediction Intervals"><div class="sect2" id="RegressionCIs">
<h2>Confidence and Prediction Intervals</h2>

<p>Much of statistics involves understanding and measuring variability (uncertainty).<a data-type="indexterm" data-primary="prediction" data-secondary="using regression" data-tertiary="confidence and prediction intervals" id="ix_predregrCIPI"/><a data-type="indexterm" data-primary="regression" data-secondary="prediction with" data-tertiary="confidence and prediction intervals" id="ix_regrpredCIPI"/><a data-type="indexterm" data-primary="prediction intervals" id="idm46522854105240"/><a data-type="indexterm" data-primary="confidence intervals" id="idm46522854104568"/>
The t-statistics and p-values reported in regression output deal with this in a formal way, which is sometimes useful for variable selection (see <a data-type="xref" href="#RMSE">“Assessing the Model”</a>).
More useful metrics are confidence intervals, which are uncertainty intervals placed around regression coefficients and predictions.<a data-type="indexterm" data-primary="regression coefficients" data-secondary="confidence intervals and" id="idm46522854102664"/>
An easy way to understand this is via the bootstrap (see <a data-type="xref" href="ch02.xhtml#bootstrap">“The Bootstrap”</a> for more details about the general bootstrap procedure).
The most common regression confidence intervals encountered in software output are those for regression parameters (coefficients).<a data-type="indexterm" data-primary="coefficients" data-secondary="confidence intervals and" id="idm46522854100568"/>
Here is a bootstrap algorithm for generating <a data-type="indexterm" data-primary="bootstrap" data-secondary="confidence interval generation" id="ix_bootCI"/>confidence intervals for regression parameters (coefficients) for a data set with <em>P</em> predictors and <em>n</em> records (rows):</p>
<ol>
<li>
<p>Consider each row (including outcome variable) as a single “ticket” and place all the <em>n</em> tickets in a box.</p>
</li>
<li>
<p>Draw a ticket at random, record the values, and replace it in the box.</p>
</li>
<li>
<p>Repeat step 2 <em>n</em> times; you now have one bootstrap resample.</p>
</li>
<li>
<p>Fit a regression to the bootstrap sample, and record the estimated coefficients.</p>
</li>
<li>
<p>Repeat steps 2 through 4, say, 1,000 times.</p>
</li>
<li>
<p>You now have 1,000 bootstrap values for each coefficient; find the appropriate percentiles for each one (e.g., 5th and 95th for a 90% confidence interval).</p>
</li>

</ol>

<p>You can use the <code>Boot</code> function in <em>R</em> to generate actual bootstrap confidence intervals for the coefficients, or you can simply use the formula-based intervals that are a routine <em>R</em> output.
The conceptual meaning and interpretation are the same, and not of central importance to data scientists, because they concern the regression coefficients.  Of greater interest to data scientists are intervals around predicted <em>y</em> values (<math alttext="ModifyingAbove upper Y With caret Subscript i">
  <msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
</math>). The uncertainty around <math alttext="ModifyingAbove upper Y With caret Subscript i">
  <msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
</math> comes from two sources:</p>

<ul>
<li>
<p>Uncertainty about what the relevant predictor variables and their coefficients are (see the preceding bootstrap algorithm)</p>
</li>
<li>
<p>Additional error inherent in individual data points</p>
</li>
</ul>

<p>The individual data point error can be thought of as follows: even if we knew for certain what the regression equation was (e.g., if we had a huge number of records to fit it), the <em>actual</em> outcome values for a given set of predictor values will vary.
For example, several houses—each with 8 rooms, a 6,500-square-foot lot, 3 bathrooms, and a basement—might have different values.
We can model this individual error with the residuals from the fitted values.
The bootstrap algorithm for modeling both the regression model error and the individual data point error would look as follows:</p>
<ol>
<li>
<p>Take a bootstrap sample from the data (spelled out in greater detail earlier).</p>
</li>
<li>
<p>Fit the regression, and predict the new value.</p>
</li>
<li>
<p>Take a single residual at random from the original regression fit, add it to the predicted value, and record the result.</p>
</li>
<li>
<p>Repeat steps 1 through 3, say, 1,000 times.</p>
</li>
<li>
<p>Find the 2.5th and the 97.5th percentiles of the results.</p>
</li>

</ol>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522854072888">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Extrapolation beyond the range of the data can lead to error.</p>
</li>
<li>
<p>Confidence intervals quantify uncertainty around regression coefficients.</p>
</li>
<li>
<p>Prediction intervals quantify uncertainty in individual predictions.</p>
</li>
<li>
<p>Most software, <em>R</em> included, will produce prediction and confidence intervals in default or specified output, using formulas.</p>
</li>
<li>
<p>The bootstrap can also be used to produce prediction and confidence intervals; the interpretation and idea are the same.<a data-type="indexterm" data-primary="bootstrap" data-secondary="confidence interval generation" data-startref="ix_bootCI" id="idm46522854066520"/></p>
</li>
</ul>
</div></aside>
<div data-type="warning" epub:type="warning"><h1>Prediction Interval or Confidence Interval?</h1>
<p>A prediction interval pertains to uncertainty around a single value, while a confidence interval pertains to a mean or other statistic calculated from multiple values.<a data-type="indexterm" data-primary="confidence intervals" data-secondary="prediction intervals versus" id="idm46522854063576"/><a data-type="indexterm" data-primary="prediction intervals" data-secondary="confidence intervals versus" id="idm46522854062632"/>
Thus, a prediction interval will typically be much wider than a confidence interval for the same value.
We model this individual value error in the bootstrap model by selecting an individual residual to tack on to the predicted value.
Which should you use?
That depends on the context and the purpose of the analysis, but, in general, data scientists are interested in specific individual predictions, so a prediction interval would be more appropriate.
Using a confidence interval when you should be using a prediction interval will greatly underestimate the uncertainty in a given predicted value.</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Factor Variables in Regression"><div class="sect1" id="FactorsRegression">
<h1>Factor Variables in Regression</h1>

<p><em>Factor</em> variables, also termed <em>categorical</em> variables,
take on a limited number of discrete values.<a data-type="indexterm" data-primary="prediction" data-secondary="using regression" data-tertiary="confidence and prediction intervals" data-startref="ix_predregrCIPI" id="idm46522854057928"/><a data-type="indexterm" data-primary="regression" data-secondary="prediction with" data-tertiary="confidence and prediction intervals" data-startref="ix_regrpredCIPI" id="idm46522854056392"/><a data-type="indexterm" data-primary="regression" data-secondary="prediction with" data-startref="ix_regrpred" id="idm46522854054888"/><a data-type="indexterm" data-primary="prediction" data-secondary="using regression" data-startref="ix_predregr" id="idm46522854053672"/><a data-type="indexterm" data-primary="regression" data-secondary="factor variables in" id="ix_regrfacV"/><a data-type="indexterm" data-primary="factor variables" id="ix_facvar"/><a data-type="indexterm" data-primary="categorical variables" data-seealso="factor variables" id="idm46522854050296"/>
For example,  a loan purpose can be “debt consolidation,” “wedding,” “car,” and so on.
The binary (yes/no) variable, also called an <em>indicator</em> variable, is a special case of a factor variable.<a data-type="indexterm" data-primary="binary variables" id="idm46522854048680"/><a data-type="indexterm" data-primary="indicator variables" id="idm46522854047976"/>
Regression requires numerical inputs, so factor variables need to be recoded to use in the model.<a data-type="indexterm" data-primary="numeric variables" data-secondary="conversion of factor variables to, in regression" id="idm46522854047064"/>
The most common approach is to convert a variable
into a set of binary <em>dummy</em> variables.<a data-type="indexterm" data-primary="dummy variables" id="idm46522854045512"/></p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522854058680">
<h5>Key Terms for Factor Variables</h5><dl>
<dt class="horizontal"><strong><em>Dummy variables</em></strong></dt>
<dd>
<p>Binary 0–1 variables derived by recoding factor data for use in regression and other models.</p>
</dd>
<dt class="horizontal"><strong><em>Reference coding</em></strong></dt>
<dd>
<p>The most common type of coding used by statisticians, in which one level of a factor is used as a reference and other factors are compared to that level.<a data-type="indexterm" data-primary="reference coding" id="idm46522854039544"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>treatment coding</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>One hot encoder</em></strong></dt>
<dd>
<p>A common type of coding used in the machine learning community in which all factor levels are retained.<a data-type="indexterm" data-primary="one hot encoding" id="idm46522854035128"/>
While useful for certain machine learning algorithms, this approach is not appropriate for multiple linear regression.</p>
</dd>
<dt class="horizontal"><strong><em>Deviation coding</em></strong></dt>
<dd>
<p>A type of coding that compares each level against the overall mean as opposed to the reference level.<a data-type="indexterm" data-primary="deviation coding" id="idm46522854032248"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>sum contrasts</p>
</dd>
</dl>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Dummy Variables Representation"><div class="sect2" id="idm46522854029432">
<h2>Dummy Variables Representation</h2>

<p>In the King County housing data,  there is a factor variable for the property type; a small subset<a data-type="indexterm" data-primary="dummy variables" data-secondary="representation of factor variables in regression" id="idm46522854027224"/><a data-type="indexterm" data-primary="regression" data-secondary="factor variables in" data-tertiary="dummy variables representation" id="idm46522854026280"/><a data-type="indexterm" data-primary="factor variables" data-secondary="dummy variable representations" id="idm46522854025000"/> of six records is shown below:</p>

<p><em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">head</code><code class="p">(</code><code class="n">house</code><code class="p">[,</code> <code class="s">'PropertyType'</code><code class="p">])</code>
<code class="n">Source</code><code class="o">:</code> <code class="n">local</code> <code class="n">data</code> <code class="n">frame</code> <code class="p">[</code><code class="m">6</code> <code class="n">x</code> <code class="m">1</code><code class="p">]</code>

   <code class="nf">PropertyType</code>
<code class="nf">         </code><code class="p">(</code><code class="n">fctr</code><code class="p">)</code>
<code class="m">1</code>     <code class="n">Multiplex</code>
<code class="m">2</code> <code class="n">Single</code> <code class="n">Family</code>
<code class="m">3</code> <code class="n">Single</code> <code class="n">Family</code>
<code class="m">4</code> <code class="n">Single</code> <code class="n">Family</code>
<code class="m">5</code> <code class="n">Single</code> <code class="n">Family</code>
<code class="m">6</code>     <code class="n">Townhouse</code></pre>

<p><em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">house</code><code class="o">.</code><code class="n">PropertyType</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p>There are three possible values: <code>Multiplex</code>, <code>Single Family</code>, and <code>Townhouse</code>.
To use this factor variable, we need to convert it to a set of binary variables.
We do this by creating a binary variable for each possible value of the factor variable.
To do this in <em>R</em>, we use the <code>model.matrix</code> function:<sup><a data-type="noteref" id="idm46522853979800-marker" href="ch04.xhtml#idm46522853979800">3</a></sup></p>

<pre data-type="programlisting" data-code-language="r"><code class="n">prop_type_dummies</code> <code class="o">&lt;-</code> <code class="nf">model.matrix</code><code class="p">(</code><code class="o">~</code><code class="n">PropertyType</code> <code class="m">-1</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">house</code><code class="p">)</code>
<code class="nf">head</code><code class="p">(</code><code class="n">prop_type_dummies</code><code class="p">)</code>
  <code class="n">PropertyTypeMultiplex</code> <code class="n">PropertyTypeSingle</code> <code class="n">Family</code> <code class="n">PropertyTypeTownhouse</code>
<code class="m">1</code>                     <code class="m">1</code>                         <code class="m">0</code>                     <code class="m">0</code>
<code class="m">2</code>                     <code class="m">0</code>                         <code class="m">1</code>                     <code class="m">0</code>
<code class="m">3</code>                     <code class="m">0</code>                         <code class="m">1</code>                     <code class="m">0</code>
<code class="m">4</code>                     <code class="m">0</code>                         <code class="m">1</code>                     <code class="m">0</code>
<code class="m">5</code>                     <code class="m">0</code>                         <code class="m">1</code>                     <code class="m">0</code>
<code class="m">6</code>                     <code class="m">0</code>                         <code class="m">0</code>                     <code class="m">1</code></pre>

<p>The function <code>model.matrix</code> converts a data frame into a matrix suitable to a linear model. The factor variable <code>PropertyType</code>, which has three distinct levels, is represented as a matrix with three columns.<a data-type="indexterm" data-primary="one hot encoding" id="idm46522853856472"/>
In the machine learning community, this representation is referred to as <em>one hot encoding</em> (see <a data-type="xref" href="ch06.xhtml#OneHotEncoder">“One Hot Encoder”</a>).</p>

<p>In <em>Python</em>, we can convert categorical variables to dummies using the <code>pandas</code> method <code>get_dummies</code>:<a data-type="indexterm" data-primary="categorical variables" data-secondary="converting to dummy variables" id="idm46522853852856"/></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="s1">'</code><code class="s1">PropertyType</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" id="co_regression_and_prediction_CO2-1" href="#callout_regression_and_prediction_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="s1">'</code><code class="s1">PropertyType</code><code class="s1">'</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" id="co_regression_and_prediction_CO2-2" href="#callout_regression_and_prediction_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_regression_and_prediction_CO2-1" href="#co_regression_and_prediction_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>By default, returns one hot encoding of the categorical variable.</p></dd>
<dt><a class="co" id="callout_regression_and_prediction_CO2-2" href="#co_regression_and_prediction_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The keyword argument <code>drop_first</code> will return <em>P</em> – 1 columns. Use this to avoid the problem of multicollinearity.</p></dd>
</dl>

<p>In certain machine learning algorithms, such as nearest neighbors and tree models,  one hot encoding is the standard way to represent factor variables (for example, see <a data-type="xref" href="ch06.xhtml#TreeModels">“Tree Models”</a>).</p>

<p>In the regression setting, a factor variable with <em>P</em> distinct levels is usually represented by a matrix with only <em>P</em> – 1 columns.
This is because a regression model typically includes an intercept term.
With an intercept, once you have defined the values for <span class="keep-together"><em>P</em> – 1</span> binaries,
the value for the <em>P</em>th is known and could be<a data-type="indexterm" data-primary="multicollinearity errors" id="idm46522853763096"/> considered redundant.
Adding the <em>P</em>th  column will cause a multicollinearity error (see <a data-type="xref" href="#Multicollinearity">“Multicollinearity”</a>).</p>

<p>The default representation in <em>R</em> is to use the first <a data-type="indexterm" data-primary="reference coding" id="idm46522853760376"/>factor level as a <em>reference</em> and interpret the remaining levels relative to that factor:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">lm</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
       <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">PropertyType</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">house</code><code class="p">)</code>

<code class="n">Call</code><code class="o">:</code>
<code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
    <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">PropertyType</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
              <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>              <code class="n">SqFtTotLiving</code>
               <code class="m">-4.468e+05</code>                  <code class="m">2.234e+02</code>
                  <code class="n">SqFtLot</code>                  <code class="n">Bathrooms</code>
               <code class="m">-7.037e-02</code>                 <code class="m">-1.598e+04</code>
                 <code class="n">Bedrooms</code>                  <code class="n">BldgGrade</code>
               <code class="m">-5.089e+04</code>                  <code class="m">1.094e+05</code>
<code class="n">PropertyTypeSingle</code> <code class="n">Family</code>      <code class="n">PropertyTypeTownhouse</code>
               <code class="m">-8.468e+04</code>                 <code class="m">-1.151e+05</code></pre>

<p>The method <code>get_dummies</code> takes the optional keyword argument <code>drop_first</code> to exclude the first factor as <em>reference</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="s1">'SqFtLot'</code><code class="p">,</code> <code class="s1">'Bathrooms'</code><code class="p">,</code> <code class="s1">'Bedrooms'</code><code class="p">,</code>
              <code class="s1">'BldgGrade'</code><code class="p">,</code> <code class="s1">'PropertyType'</code><code class="p">]</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">house_lm_factor</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">house_lm_factor</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">])</code>

<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Intercept: {house_lm_factor.intercept_:.3f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Coefficients:'</code><code class="p">)</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">coef</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">house_lm_factor</code><code class="o">.</code><code class="n">coef_</code><code class="p">):</code>
    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">' {name}: {coef}'</code><code class="p">)</code></pre>

<p>The output from the <em>R</em> regression shows two coefficients corresponding to <code>PropertyType</code>: <code>PropertyTypeSingle Family</code> and <code>PropertyTypeTownhouse</code>.
There is no coefficient of <code>Multiplex</code> since it is implicitly defined when  <code>PropertyTypeSingle Family == 0</code> and <code>PropertyTypeTownhouse == 0</code>.
The coefficients are interpreted as relative to <code>Multiplex</code>, so a home that is <code>Single Family</code> is worth almost $85,000 less, and a home that is <code>Townhouse</code> is worth over $150,000 less.<sup><a data-type="noteref" id="idm46522853557496-marker" href="ch04.xhtml#idm46522853557496">4</a></sup></p>
<div data-type="warning" epub:type="warning"><h1>Different Factor Codings</h1>
<p>There are several different ways to encode factor variables, known as <em>contrast coding</em> systems.<a data-type="indexterm" data-primary="factor variables" data-secondary="different codings" id="idm46522853554488"/><a data-type="indexterm" data-primary="contrast coding" id="idm46522853553480"/>
For example, <em>deviation coding</em>, also known as <em>sum contrasts</em>, compares each level against the overall mean.<a data-type="indexterm" data-primary="deviation coding" id="idm46522853551752"/><a data-type="indexterm" data-primary="sum contrasts" id="idm46522853551048"/><a data-type="indexterm" data-primary="polynomial coding" id="idm46522853550376"/> Another contrast is <em>polynomial coding</em>, which is appropriate for ordered factors; see the section <a data-type="xref" href="#OrderedFactorsRegression">“Ordered Factor Variables”</a>.
With the exception of ordered factors,
data scientists will generally not encounter any type of coding besides reference coding or one hot encoder.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Factor Variables with Many Levels"><div class="sect2" id="FactorVariablesManyLevels">
<h2>Factor Variables with Many Levels</h2>

<p>Some factor variables can produce a huge number of binary dummies—zip codes are a factor variable, and there are 43,000 zip codes in the US.<a data-type="indexterm" data-primary="factor variables" data-secondary="with many levels" id="ix_facvarlvl"/><a data-type="indexterm" data-primary="regression" data-secondary="factor variables in" data-tertiary="with many levels" id="ix_regrfacVlvl"/> In such cases, it is useful to explore the data, and the relationships between predictor variables and the outcome, to determine whether useful information is contained in the categories.
If so, you must further decide whether it is useful to retain all factors, or whether the levels should be consolidated.</p>

<p>In King County, there are 80 zip codes with a house sale:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">table</code><code class="p">(</code><code class="n">house</code><code class="o">$</code><code class="n">ZipCode</code><code class="p">)</code>

<code class="m">98001</code> <code class="m">98002</code> <code class="m">98003</code> <code class="m">98004</code> <code class="m">98005</code> <code class="m">98006</code> <code class="m">98007</code> <code class="m">98008</code> <code class="m">98010</code> <code class="m">98011</code> <code class="m">98014</code> <code class="m">98019</code>
  <code class="m">358</code>   <code class="m">180</code>   <code class="m">241</code>   <code class="m">293</code>   <code class="m">133</code>   <code class="m">460</code>   <code class="m">112</code>   <code class="m">291</code>    <code class="m">56</code>   <code class="m">163</code>    <code class="m">85</code>   <code class="m">242</code>
<code class="m">98022</code> <code class="m">98023</code> <code class="m">98024</code> <code class="m">98027</code> <code class="m">98028</code> <code class="m">98029</code> <code class="m">98030</code> <code class="m">98031</code> <code class="m">98032</code> <code class="m">98033</code> <code class="m">98034</code> <code class="m">98038</code>
  <code class="m">188</code>   <code class="m">455</code>    <code class="m">31</code>   <code class="m">366</code>   <code class="m">252</code>   <code class="m">475</code>   <code class="m">263</code>   <code class="m">308</code>   <code class="m">121</code>   <code class="m">517</code>   <code class="m">575</code>   <code class="m">788</code>
<code class="m">98039</code> <code class="m">98040</code> <code class="m">98042</code> <code class="m">98043</code> <code class="m">98045</code> <code class="m">98047</code> <code class="m">98050</code> <code class="m">98051</code> <code class="m">98052</code> <code class="m">98053</code> <code class="m">98055</code> <code class="m">98056</code>
   <code class="m">47</code>   <code class="m">244</code>   <code class="m">641</code>     <code class="m">1</code>   <code class="m">222</code>    <code class="m">48</code>     <code class="m">7</code>    <code class="m">32</code>   <code class="m">614</code>   <code class="m">499</code>   <code class="m">332</code>   <code class="m">402</code>
<code class="m">98057</code> <code class="m">98058</code> <code class="m">98059</code> <code class="m">98065</code> <code class="m">98068</code> <code class="m">98070</code> <code class="m">98072</code> <code class="m">98074</code> <code class="m">98075</code> <code class="m">98077</code> <code class="m">98092</code> <code class="m">98102</code>
    <code class="m">4</code>   <code class="m">420</code>   <code class="m">513</code>   <code class="m">430</code>     <code class="m">1</code>    <code class="m">89</code>   <code class="m">245</code>   <code class="m">502</code>   <code class="m">388</code>   <code class="m">204</code>   <code class="m">289</code>   <code class="m">106</code>
<code class="m">98103</code> <code class="m">98105</code> <code class="m">98106</code> <code class="m">98107</code> <code class="m">98108</code> <code class="m">98109</code> <code class="m">98112</code> <code class="m">98113</code> <code class="m">98115</code> <code class="m">98116</code> <code class="m">98117</code> <code class="m">98118</code>
  <code class="m">671</code>   <code class="m">313</code>   <code class="m">361</code>   <code class="m">296</code>   <code class="m">155</code>   <code class="m">149</code>   <code class="m">357</code>     <code class="m">1</code>   <code class="m">620</code>   <code class="m">364</code>   <code class="m">619</code>   <code class="m">492</code>
<code class="m">98119</code> <code class="m">98122</code> <code class="m">98125</code> <code class="m">98126</code> <code class="m">98133</code> <code class="m">98136</code> <code class="m">98144</code> <code class="m">98146</code> <code class="m">98148</code> <code class="m">98155</code> <code class="m">98166</code> <code class="m">98168</code>
  <code class="m">260</code>   <code class="m">380</code>   <code class="m">409</code>   <code class="m">473</code>   <code class="m">465</code>   <code class="m">310</code>   <code class="m">332</code>   <code class="m">287</code>    <code class="m">40</code>   <code class="m">358</code>   <code class="m">193</code>   <code class="m">332</code>
<code class="m">98177</code> <code class="m">98178</code> <code class="m">98188</code> <code class="m">98198</code> <code class="m">98199</code> <code class="m">98224</code> <code class="m">98288</code> <code class="m">98354</code>
  <code class="m">216</code>   <code class="m">266</code>   <code class="m">101</code>   <code class="m">225</code>   <code class="m">393</code>     <code class="m">3</code>     <code class="m">4</code>     <code class="m">9</code></pre>

<p>The <code>value_counts</code> method of <code>pandas</code> data frames returns the same information:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="s1">'ZipCode'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">())</code><code class="o">.</code><code class="n">transpose</code><code class="p">()</code></pre>

<p><code>ZipCode</code> is  an important variable, since it is a proxy for the effect of location on the value of a house.
Including all levels requires 79 coefficients corresponding to 79 degrees of freedom.
The original model <code>house_lm</code> has only 5 degrees of freedom; see <a data-type="xref" href="#RMSE">“Assessing the Model”</a>.
Moreover,  several zip codes have only one sale.
In some problems, you can consolidate a zip code using the first two or three digits, <span class="keep-together">corresponding</span> to a submetropolitan geographic region.
For King County, almost all of the sales occur in 980xx or 981xx, so this doesn’t help.</p>

<p>An alternative approach is to group the zip codes according to another variable, such as sale price.
Even better is to form zip code groups using the residuals from an initial model.
The following <code>dplyr</code> code in <em>R</em> consolidates the 80 zip codes into five groups based on the median of the residual from the  <code>house_lm</code> regression:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">zip_groups</code> <code class="o">&lt;-</code> <code class="n">house</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="n">resid</code> <code class="o">=</code> <code class="nf">residuals</code><code class="p">(</code><code class="n">house_lm</code><code class="p">))</code> <code class="o">%&gt;%</code>
  <code class="nf">group_by</code><code class="p">(</code><code class="n">ZipCode</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">summarize</code><code class="p">(</code><code class="n">med_resid</code> <code class="o">=</code> <code class="nf">median</code><code class="p">(</code><code class="n">resid</code><code class="p">),</code>
            <code class="n">cnt</code> <code class="o">=</code> <code class="nf">n</code><code class="p">())</code> <code class="o">%&gt;%</code>
  <code class="nf">arrange</code><code class="p">(</code><code class="n">med_resid</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="n">cum_cnt</code> <code class="o">=</code> <code class="nf">cumsum</code><code class="p">(</code><code class="n">cnt</code><code class="p">),</code>
         <code class="n">ZipGroup</code> <code class="o">=</code> <code class="nf">ntile</code><code class="p">(</code><code class="n">cum_cnt</code><code class="p">,</code> <code class="m">5</code><code class="p">))</code>
<code class="n">house</code> <code class="o">&lt;-</code> <code class="n">house</code> <code class="o">%&gt;%</code>
  <code class="nf">left_join</code><code class="p">(</code><code class="nf">select</code><code class="p">(</code><code class="n">zip_groups</code><code class="p">,</code> <code class="n">ZipCode</code><code class="p">,</code> <code class="n">ZipGroup</code><code class="p">),</code> <code class="n">by</code><code class="o">=</code><code class="s">'ZipCode'</code><code class="p">)</code></pre>

<p>The median residual is computed for each zip, and the <code>ntile</code> function is used to split the zip codes, sorted by the median, into five groups. See <a data-type="xref" href="#ConfoundingVariables">“Confounding Variables”</a> for an example of how this is used as a term in a regression improving upon the original fit.</p>

<p>In <em>Python</em> we can calculate this information as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">zip_groups</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">([</code>
    <code class="o">*</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code>
        <code class="s1">'ZipCode'</code><code class="p">:</code> <code class="n">house</code><code class="p">[</code><code class="s1">'ZipCode'</code><code class="p">],</code>
        <code class="s1">'residual'</code> <code class="p">:</code> <code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code> <code class="o">-</code> <code class="n">house_lm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">]),</code>
    <code class="p">})</code>
    <code class="o">.</code><code class="n">groupby</code><code class="p">([</code><code class="s1">'ZipCode'</code><code class="p">])</code>
    <code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="p">{</code>
        <code class="s1">'ZipCode'</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">0</code><code class="p">],</code>
        <code class="s1">'count'</code><code class="p">:</code> <code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="p">),</code>
        <code class="s1">'median_residual'</code><code class="p">:</code> <code class="n">x</code><code class="o">.</code><code class="n">residual</code><code class="o">.</code><code class="n">median</code><code class="p">()</code>
    <code class="p">})</code>
<code class="p">])</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="s1">'median_residual'</code><code class="p">)</code>
<code class="n">zip_groups</code><code class="p">[</code><code class="s1">'cum_count'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">zip_groups</code><code class="p">[</code><code class="s1">'count'</code><code class="p">])</code>
<code class="n">zip_groups</code><code class="p">[</code><code class="s1">'ZipGroup'</code><code class="p">]</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">qcut</code><code class="p">(</code><code class="n">zip_groups</code><code class="p">[</code><code class="s1">'cum_count'</code><code class="p">],</code> <code class="mi">5</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code>
                                 <code class="n">retbins</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>

<code class="n">to_join</code> <code class="o">=</code> <code class="n">zip_groups</code><code class="p">[[</code><code class="s1">'ZipCode'</code><code class="p">,</code> <code class="s1">'ZipGroup'</code><code class="p">]]</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s1">'ZipCode'</code><code class="p">)</code>
<code class="n">house</code> <code class="o">=</code> <code class="n">house</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">to_join</code><code class="p">,</code> <code class="n">on</code><code class="o">=</code><code class="s1">'ZipCode'</code><code class="p">)</code>
<code class="n">house</code><code class="p">[</code><code class="s1">'ZipGroup'</code><code class="p">]</code> <code class="o">=</code> <code class="n">house</code><code class="p">[</code><code class="s1">'ZipGroup'</code><code class="p">]</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="s1">'category'</code><code class="p">)</code></pre>

<p>The concept of using the residuals to help guide the regression fitting is a fundamental <a data-type="indexterm" data-primary="regression" data-secondary="factor variables in" data-tertiary="with many levels" data-startref="ix_regrfacVlvl" id="idm46522853186296"/><a data-type="indexterm" data-primary="factor variables" data-secondary="with many levels" data-startref="ix_facvarlvl" id="idm46522853184936"/>step in the modeling process; see <a data-type="xref" href="#RegressionDiagnostics">“Regression Diagnostics”</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Ordered Factor Variables"><div class="sect2" id="OrderedFactorsRegression">
<h2>Ordered Factor Variables</h2>

<p>Some factor variables reflect levels of a factor; these are termed <em>ordered factor variables</em> or <em>ordered categorical variables</em>.<a data-type="indexterm" data-primary="factor variables" data-secondary="ordered" id="idm46522852949624"/><a data-type="indexterm" data-primary="regression" data-secondary="factor variables in" data-tertiary="ordered factor variables" id="idm46522852948648"/><a data-type="indexterm" data-primary="ordered factor variables" id="idm46522852947416"/>
For example, the loan grade could be A, B, C, and so on—each grade carries more risk than the prior grade.
Often, ordered factor variables can be converted to numerical values and used as is.<a data-type="indexterm" data-primary="numeric variables" data-secondary="converting ordered factor variables to" id="idm46522852946392"/>
For example,
the variable <code>BldgGrade</code> is an ordered factor variable.
Several of the types of grades are shown in <a data-type="xref" href="#BldgGrade">Table 4-1</a>.
While the grades have specific meaning,
the numeric value is ordered from low to high, corresponding to higher-grade homes.
With the regression model <code>house_lm</code>,
fit in <a data-type="xref" href="#MultipleLinearRegression">“Multiple Linear Regression”</a>,
<code>BldgGrade</code> was treated as a numeric variable.</p>
<table id="BldgGrade">
<caption><span class="label">Table 4-1. </span>Building grades and numeric equivalents</caption>
<thead>
<tr>
<th>Value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>1</p></td>
<td><p>Cabin</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>Substandard</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>Fair</p></td>
</tr>
<tr>
<td><p>10</p></td>
<td><p>Very good</p></td>
</tr>
<tr>
<td><p>12</p></td>
<td><p>Luxury</p></td>
</tr>
<tr>
<td><p>13</p></td>
<td><p>Mansion</p></td>
</tr>
</tbody>
</table>

<p>Treating ordered factors as a numeric variable preserves the information contained in the ordering that would be lost if it were  converted to a factor.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522852928680">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Factor variables need to be converted into numeric variables for use in a <span class="keep-together">regression</span>.</p>
</li>
<li>
<p>The most common method to encode a factor variable with P distinct values is to represent them using P – 1 dummy variables.</p>
</li>
<li>
<p>A factor variable with many levels, even in very big data sets, may need to be consolidated into a variable with fewer levels.</p>
</li>
<li>
<p>Some factors have levels that are ordered and can be represented as a single numeric variable.<a data-type="indexterm" data-primary="factor variables" data-startref="ix_facvar" id="idm46522852923000"/><a data-type="indexterm" data-primary="regression" data-secondary="factor variables in" data-startref="ix_regrfacV" id="idm46522852922024"/></p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Interpreting the Regression Equation"><div class="sect1" id="InterpretingRegression">
<h1>Interpreting the Regression Equation</h1>

<p>In data science, the most important use of regression is to predict some dependent (outcome) variable.<a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" id="ix_regrequa"/>
In some cases, however, gaining insight from the equation itself to understand the nature of the relationship between the predictors and the outcome can be of value.
This section provides guidance on examining the regression equation and interpreting it.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522852916728">
<h5>Key Terms for Interpreting the Regression Equation</h5><dl>
<dt class="horizontal"><strong><em>Correlated variables</em></strong></dt>
<dd>
<p>When the predictor variables are highly correlated, it is difficult to interpret the individual coefficients.</p>
</dd>
<dt class="horizontal"><strong><em>Multicollinearity</em></strong></dt>
<dd>
<p>When the predictor variables have perfect, or near-perfect, correlation, the regression can be unstable or impossible to compute.<a data-type="indexterm" data-primary="multicollinearity" id="idm46522852911512"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>collinearity</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Confounding variables</em></strong></dt>
<dd>
<p>An important predictor that, when omitted, leads to spurious relationships in a regression equation.<a data-type="indexterm" data-primary="confounding variables" id="idm46522852907096"/></p>
</dd>
<dt class="horizontal"><strong><em>Main effects</em></strong></dt>
<dd>
<p>The relationship between a predictor and the outcome variable, independent of other variables.<a data-type="indexterm" data-primary="main effects" id="idm46522852904472"/></p>
</dd>
<dt class="horizontal"><strong><em>Interactions</em></strong></dt>
<dd>
<p>An interdependent relationship between two or more predictors and the response.<a data-type="indexterm" data-primary="interactions" id="idm46522852901960"/></p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Correlated Predictors"><div class="sect2" id="CorrelatedPredictors">
<h2>Correlated Predictors</h2>

<p>In multiple regression, the predictor variables are often correlated with each other.<a data-type="indexterm" data-primary="predictor variables" data-secondary="correlated" id="idm46522852899032"/><a data-type="indexterm" data-primary="correlated predictor variables" id="idm46522852898056"/><a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" data-tertiary="correlated predictor variables" id="idm46522852897416"/>
As an example,
examine the regression coefficients for the model <code>step_lm</code>, fit in <a data-type="xref" href="#StepwiseRegression">“Model Selection and Stepwise Regression”</a>.</p>

<p><em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">step_lm</code><code class="o">$</code><code class="nf">coefficients</code>
<code class="nf">              </code><code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>             <code class="n">SqFtTotLiving</code>                 <code class="n">Bathrooms</code>
             <code class="m">6.178645e+06</code>              <code class="m">1.992776e+02</code>              <code class="m">4.239616e+04</code>
                 <code class="n">Bedrooms</code>                 <code class="n">BldgGrade</code> <code class="n">PropertyTypeSingle</code> <code class="n">Family</code>
            <code class="m">-5.194738e+04</code>              <code class="m">1.371596e+05</code>              <code class="m">2.291206e+04</code>
    <code class="n">PropertyTypeTownhouse</code>           <code class="n">SqFtFinBasement</code>                   <code class="n">YrBuilt</code>
             <code class="m">8.447916e+04</code>              <code class="m">7.046975e+00</code>             <code class="m">-3.565425e+03</code></pre>

<p class="pagebreak-before"><em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Intercept: {best_model.intercept_:.3f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Coefficients:'</code><code class="p">)</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">coef</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">best_variables</code><code class="p">,</code> <code class="n">best_model</code><code class="o">.</code><code class="n">coef_</code><code class="p">):</code>
    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">' {name}: {coef}'</code><code class="p">)</code></pre>

<p>The coefficient for  <code>Bedrooms</code> is  negative!
This implies that adding a bedroom to a house will reduce its value.
How can this be?
This is because the predictor variables are correlated: larger houses tend to have more bedrooms, and it is the size that drives house value, not the number of bedrooms.
Consider two homes of the exact same size:
it is reasonable to expect that a home with more but smaller bedrooms would be considered less desirable.</p>

<p>Having correlated predictors can make it difficult to interpret the sign and value of regression coefficients (and can inflate the standard error of the estimates).<a data-type="indexterm" data-primary="regression coefficients" data-secondary="correlated predictors and" id="idm46522852747736"/>  The variables for bedrooms, house size, and number of bathrooms are all correlated.
This is illustrated by the following example in <em>R</em>, which fits another regression removing the variables <code>SqFtTotLiving</code>, <code>SqFtFinBasement</code>, and <code>Bathrooms</code> from the equation:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">update</code><code class="p">(</code><code class="n">step_lm</code><code class="p">,</code> <code class="n">. </code><code class="o">~</code> <code class="n">. </code><code class="o">-</code> <code class="n">SqFtTotLiving</code> <code class="o">-</code> <code class="n">SqFtFinBasement</code> <code class="o">-</code> <code class="n">Bathrooms</code><code class="p">)</code>

<code class="n">Call</code><code class="o">:</code>
<code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">PropertyType</code> <code class="o">+</code>
    <code class="n">YrBuilt</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house</code><code class="p">,</code> <code class="n">na.action</code> <code class="o">=</code> <code class="n">na.omit</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
              <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>                   <code class="n">Bedrooms</code>
                  <code class="m">4913973</code>                      <code class="m">27151</code>
                <code class="n">BldgGrade</code>  <code class="n">PropertyTypeSingle</code> <code class="n">Family</code>
                   <code class="m">248998</code>                     <code class="m">-19898</code>
    <code class="n">PropertyTypeTownhouse</code>                    <code class="n">YrBuilt</code>
                   <code class="m">-47355</code>                      <code class="m">-3212</code></pre>

<p>The <code>update</code> function can be used to add or remove variables from a model. Now the coefficient for bedrooms is positive—in line with what we would expect (though it is really acting as a proxy for house size, now that those variables have been removed).</p>

<p>In <em>Python</em>, there is no equivalent to <em>R</em>’s <code>update</code> function. We need to refit the model with the modified predictor list:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'Bedrooms'</code><code class="p">,</code> <code class="s1">'BldgGrade'</code><code class="p">,</code> <code class="s1">'PropertyType'</code><code class="p">,</code> <code class="s1">'YrBuilt'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'AdjSalePrice'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">reduced_lm</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">reduced_lm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">])</code></pre>

<p>Correlated variables are only one issue with interpreting regression coefficients.
In <code>house_lm</code>, there is no variable to account for the location of the home,
and the model is mixing together very different types of regions.<a data-type="indexterm" data-primary="confounding variables" id="idm46522852630136"/>
Location may be a <em>confounding</em> variable;
see <a data-type="xref" href="#ConfoundingVariables">“Confounding Variables”</a> for further discussion.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Multicollinearity"><div class="sect2" id="Multicollinearity">
<h2>Multicollinearity</h2>

<p>An extreme case of correlated variables produces multicollinearity—a condition in which there is redundance among the predictor variables.<a data-type="indexterm" data-primary="predictor variables" data-secondary="redundancy in" id="idm46522852626184"/><a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" data-tertiary="multicollinearity" id="idm46522852625208"/><a data-type="indexterm" data-primary="multicollinearity" id="idm46522852624024"/>
Perfect multicollinearity occurs when one predictor variable can be expressed as a linear combination of others.
Multicollinearity occurs when:</p>

<ul>
<li>
<p>A variable is included multiple times by error.</p>
</li>
<li>
<p><em>P</em> dummies, instead of <em>P</em> – 1 dummies, are created from a factor variable (see <a data-type="xref" href="#FactorsRegression">“Factor Variables in Regression”</a>).</p>
</li>
<li>
<p>Two variables are nearly perfectly correlated with one another.</p>
</li>
</ul>

<p>Multicollinearity in regression must be addressed—variables should be removed until the multicollinearity is gone.
A regression does not have a well-defined solution in the presence of perfect multicollinearity.
Many software packages, including <em>R</em> and <em>Python</em>,
automatically handle certain types of multicollinearity.
For example, if <code>SqFtTotLiving</code> is included twice in the regression of the <code>house</code> data, the results are the same as for the <code>house_lm</code> model.
In the  case of nonperfect multicollinearity, the software may obtain a solution, but the results may be unstable.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Multicollinearity is not such a problem for nonlinear regression methods like trees, clustering, and nearest-neighbors, and in such methods it may be advisable to retain <em>P</em> dummies (instead of <em>P</em> – 1).
That said, even in those methods, nonredundancy in predictor variables is still a virtue.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Confounding Variables"><div class="sect2" id="ConfoundingVariables">
<h2>Confounding Variables</h2>

<p>With correlated variables, the problem is one of commission:
including different variables that have a similar predictive relationship with the response.<a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" data-tertiary="confounding variables" id="ix_regrequaConfV"/><a data-type="indexterm" data-primary="confounding variables" id="ix_confvar"/>
With <em>confounding variables</em>, the problem is one of omission:
an important variable is not included in the regression equation.
Naive interpretation of the equation coefficients can lead to invalid conclusions.</p>

<p>Take, for example, the King County regression equation <code>house_lm</code> from <a data-type="xref" href="#KingCountyHousingData">“Example: King County Housing Data”</a>.
The regression coefficients of <code>SqFtLot</code>, <code>Bathrooms</code>, and <code>Bedrooms</code> are all negative.
The original regression model does not contain a variable to represent location—a very important predictor of house price.
To model location, include a variable <code>ZipGroup</code> that categorizes the zip code into one of five groups, from least expensive (1) to most expensive (5):<sup><a data-type="noteref" id="idm46522852564136-marker" href="ch04.xhtml#idm46522852564136">5</a></sup></p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
    <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">PropertyType</code> <code class="o">+</code> <code class="n">ZipGroup</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house</code><code class="p">,</code>
    <code class="n">na.action</code> <code class="o">=</code> <code class="n">na.omit</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
              <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>              <code class="n">SqFtTotLiving</code>
               <code class="m">-6.666e+05</code>                  <code class="m">2.106e+02</code>
                  <code class="n">SqFtLot</code>                  <code class="n">Bathrooms</code>
                <code class="m">4.550e-01</code>                  <code class="m">5.928e+03</code>
                 <code class="n">Bedrooms</code>                  <code class="n">BldgGrade</code>
               <code class="m">-4.168e+04</code>                  <code class="m">9.854e+04</code>
<code class="n">PropertyTypeSingle</code> <code class="n">Family</code>      <code class="n">PropertyTypeTownhouse</code>
                <code class="m">1.932e+04</code>                 <code class="m">-7.820e+04</code>
                <code class="n">ZipGroup2</code>                  <code class="n">ZipGroup3</code>
                <code class="m">5.332e+04</code>                  <code class="m">1.163e+05</code>
                <code class="n">ZipGroup4</code>                  <code class="n">ZipGroup5</code>
                <code class="m">1.784e+05</code>                  <code class="m">3.384e+05</code></pre>

<p>The same model in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="s1">'SqFtLot'</code><code class="p">,</code> <code class="s1">'Bathrooms'</code><code class="p">,</code> <code class="s1">'Bedrooms'</code><code class="p">,</code>
              <code class="s1">'BldgGrade'</code><code class="p">,</code> <code class="s1">'PropertyType'</code><code class="p">,</code> <code class="s1">'ZipGroup'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'AdjSalePrice'</code>

<code class="n">X</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">house</code><code class="p">[</code><code class="n">predictors</code><code class="p">],</code> <code class="n">drop_first</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">confounding_lm</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">confounding_lm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">house</code><code class="p">[</code><code class="n">outcome</code><code class="p">])</code>

<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Intercept: {confounding_lm.intercept_:.3f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Coefficients:'</code><code class="p">)</code>
<code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">coef</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">,</code> <code class="n">confounding_lm</code><code class="o">.</code><code class="n">coef_</code><code class="p">):</code>
    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">' {name}: {coef}'</code><code class="p">)</code></pre>

<p><code>ZipGroup</code> is clearly an important variable:
a home in the most expensive zip code group is estimated to have a higher sales price by almost $340,000.
The coefficients of <code>SqFtLot</code> and <code>Bathrooms</code> are now positive, and
adding a bathroom increases the sale price by $5,928.</p>

<p>The coefficient for <code>Bedrooms</code> is still negative.
While this is unintuitive, this is a well-known phenomenon in real estate.
For homes of the same livable area and number of bathrooms,
having more and therefore smaller bedrooms is associated with less valuable homes.<a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" data-tertiary="confounding variables" data-startref="ix_regrequaConfV" id="idm46522852336664"/><a data-type="indexterm" data-primary="confounding variables" data-startref="ix_confvar" id="idm46522852335176"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Interactions and Main Effects"><div class="sect2" id="Interactions">
<h2>Interactions and Main Effects</h2>

<p>Statisticians like to distinguish between <em>main effects</em>,
or independent variables,
and the <em>interactions</em> between the main effects.<a data-type="indexterm" data-primary="main effects" data-secondary="interactions and" id="ix_maineff"/><a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" data-tertiary="interactions and main effects" id="ix_regrequaIME"/><a data-type="indexterm" data-primary="interactions" data-secondary="and main effects" id="ix_interME"/><a data-type="indexterm" data-primary="predictor variables" data-secondary="main effects and interactions" id="ix_predvarMEI"/>
Main effects are what are often referred to as the <em>predictor variables</em> in the regression equation.
An implicit assumption when only main effects are used in a model is that the relationship between a predictor variable and the response is independent of the other predictor variables.
This is often not the case.</p>

<p>For example, the model fit to the King County Housing Data in <a data-type="xref" href="#ConfoundingVariables">“Confounding Variables”</a> includes several variables as main effects,
including <code>ZipCode</code>.
Location in real estate is everything, and it is natural to presume that the relationship between, say, house size and the sale price depends on location.
A big house built in a low-rent district is not going to retain the same value as a big house built in an expensive area.
You include interactions between variables in <em>R</em> using the <code>*</code> operator.
For the King County data, the following fits an interaction between <code>SqFtTotLiving</code> and <code>ZipGroup</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">*</code> <code class="n">ZipGroup</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code>
    <code class="n">Bathrooms</code> <code class="o">+</code> <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">PropertyType</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house</code><code class="p">,</code>
    <code class="n">na.action</code> <code class="o">=</code> <code class="n">na.omit</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
              <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>              <code class="n">SqFtTotLiving</code>
               <code class="m">-4.853e+05</code>                  <code class="m">1.148e+02</code>
                <code class="n">ZipGroup2</code>                  <code class="n">ZipGroup3</code>
               <code class="m">-1.113e+04</code>                  <code class="m">2.032e+04</code>
                <code class="n">ZipGroup4</code>                  <code class="n">ZipGroup5</code>
                <code class="m">2.050e+04</code>                 <code class="m">-1.499e+05</code>
                  <code class="n">SqFtLot</code>                  <code class="n">Bathrooms</code>
                <code class="m">6.869e-01</code>                 <code class="m">-3.619e+03</code>
                 <code class="n">Bedrooms</code>                  <code class="n">BldgGrade</code>
               <code class="m">-4.180e+04</code>                  <code class="m">1.047e+05</code>
<code class="n">PropertyTypeSingle</code> <code class="n">Family</code>      <code class="n">PropertyTypeTownhouse</code>
                <code class="m">1.357e+04</code>                 <code class="m">-5.884e+04</code>
  <code class="n">SqFtTotLiving</code><code class="o">:</code><code class="n">ZipGroup2</code>    <code class="n">SqFtTotLiving</code><code class="o">:</code><code class="n">ZipGroup3</code>
                <code class="m">3.260e+01</code>                  <code class="m">4.178e+01</code>
  <code class="n">SqFtTotLiving</code><code class="o">:</code><code class="n">ZipGroup4</code>    <code class="n">SqFtTotLiving</code><code class="o">:</code><code class="n">ZipGroup5</code>
                <code class="m">6.934e+01</code>                  <code class="m">2.267e+02</code></pre>

<p>The resulting model has four new terms:
<code>SqFtTotLiving:ZipGroup2</code>, <code>SqFtTotLiving:ZipGroup3</code>, and so on.</p>

<p>In <em>Python</em>, we need to use the <code>statsmodels</code> package to train linear regression models with interactions. This package was designed similar to <em>R</em> and allows defining models using a formula interface:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">smf</code><code class="o">.</code><code class="n">ols</code><code class="p">(</code><code class="n">formula</code><code class="o">=</code><code class="s1">'AdjSalePrice ~ SqFtTotLiving*ZipGroup + SqFtLot + '</code> <code class="o">+</code>
     <code class="s1">'Bathrooms + Bedrooms + BldgGrade + PropertyType'</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">house</code><code class="p">)</code>
<code class="n">results</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>
<code class="n">results</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code></pre>

<p>The <code>statsmodels</code> package takes care of categorical variables (e.g., <code>ZipGroup[T.1]</code>, <code>PropertyType[T.Single Family]</code>) and interaction terms (e.g., <code>SqFtTotLiving:ZipGroup[T.1]</code>).</p>

<p>Location and house size appear to have a strong interaction.
For a home in the lowest <code>ZipGroup</code>,
the slope is the same as the slope for the main effect <code>SqFtTotLiving</code>,
which<a data-type="indexterm" data-primary="reference coding" id="idm46522852194232"/> is $118 per square foot (this is because <em>R</em> uses <em>reference</em> coding for factor variables; see <a data-type="xref" href="#FactorsRegression">“Factor Variables in Regression”</a>).
For a home in the highest <code>ZipGroup</code>,
the slope is the sum of the main effect plus <code>SqFtTotLiving:ZipGroup5</code>,
or $115 + $227 = $342 per square foot.
In other words, adding a square foot in the most expensive zip code group boosts the predicted sale price by a factor of almost three, compared to the average boost from adding a square foot.</p>
<div data-type="tip"><h1>Model Selection with Interaction Terms</h1>
<p>In problems involving many variables,
it can be challenging to decide which interaction terms should be included in the model.
Several different approaches are commonly taken:</p>

<ul>
<li>
<p>In some problems,
prior knowledge and intuition can guide the choice of which interaction terms to include in the model.</p>
</li>
<li>
<p>Stepwise selection (see <a data-type="xref" href="#StepwiseRegression">“Model Selection and Stepwise Regression”</a>) can be used to sift through the various models.<a data-type="indexterm" data-primary="penalized regression" id="idm46522852186136"/></p>
</li>
<li>
<p>Penalized regression can automatically fit to a large set of possible interaction terms.<a data-type="indexterm" data-primary="tree models" id="idm46522852184520"/></p>
</li>
<li>
<p>Perhaps the most common approach is to use <em>tree models</em>,
as well as their descendants, <em>random forest</em> and <em>gradient boosted trees</em>.
This class of models automatically searches for optimal interaction terms; see <a data-type="xref" href="ch06.xhtml#TreeModels">“Tree Models”</a>.</p>
</li>
</ul>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522852180248">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Because of correlation between predictors, care must be taken in the interpretation of the coefficients in multiple linear regression.</p>
</li>
<li>
<p>Multicollinearity can cause numerical instability in fitting the regression <span class="keep-together">equation</span>.</p>
</li>
<li>
<p>A confounding variable is an important predictor that is omitted from a model and can lead to a regression equation with spurious relationships.</p>
</li>
<li>
<p>An interaction term between two variables is needed if the relationship between the variables and the response is interdependent.<a data-type="indexterm" data-primary="predictor variables" data-secondary="main effects and interactions" data-startref="ix_predvarMEI" id="idm46522852174424"/><a data-type="indexterm" data-primary="main effects" data-secondary="interactions and" data-startref="ix_maineff" id="idm46522852173112"/><a data-type="indexterm" data-primary="interactions" data-secondary="and main effects" data-startref="ix_interME" id="idm46522852171896"/><a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" data-tertiary="interactions and main effects" data-startref="ix_regrequaIME" id="idm46522852170680"/><a data-type="indexterm" data-primary="regression" data-secondary="interpreting the regression equation" data-startref="ix_regrequa" id="idm46522852169160"/></p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Regression Diagnostics"><div class="sect1" id="RegressionDiagnostics">
<h1>Regression Diagnostics</h1>

<p>In explanatory modeling (i.e., in a research context), various steps, in addition to the metrics mentioned previously (see <a data-type="xref" href="#RMSE">“Assessing the Model”</a>), are taken to assess how well the model fits the data;
most are based on analysis of the residuals.<a data-type="indexterm" data-primary="regression" data-secondary="diagnostics" id="ix_regrdiag"/>
These steps do not directly address predictive accuracy, but they can provide useful insight in a predictive setting.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522852083192">
<h5>Key Terms for Regression Diagnostics</h5><dl>
<dt class="horizontal"><strong><em>Standardized residuals</em></strong></dt>
<dd>
<p>Residuals divided by the standard error of the residuals.<a data-type="indexterm" data-primary="standardized residuals" data-secondary="defined" id="idm46522852080088"/></p>
</dd>
<dt class="horizontal"><strong><em>Outliers</em></strong></dt>
<dd>
<p>Records (or outcome values) that are distant from the rest of the data (or the predicted outcome).</p>
</dd>
<dt class="horizontal"><strong><em>Influential value</em></strong></dt>
<dd>
<p>A value or record whose presence or absence makes a big difference in the regression equation.<a data-type="indexterm" data-primary="influential values" id="idm46522852075240"/></p>
</dd>
<dt class="horizontal"><strong><em>Leverage</em></strong></dt>
<dd>
<p>The degree of influence that a single record has on a regression equation.<a data-type="indexterm" data-primary="leverage" data-secondary="defined" id="idm46522852072728"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>hat-value<a data-type="indexterm" data-primary="hat-value" id="idm46522852070184"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Non-normal residuals</em></strong></dt>
<dd>
<p>Non-normally distributed residuals can invalidate some technical requirements of regression but are usually not a concern in data science.<a data-type="indexterm" data-primary="non-normal residuals" id="idm46522852067224"/><a data-type="indexterm" data-primary="heteroskedasticity" id="idm46522852066520"/></p>
</dd>
<dt class="horizontal"><strong><em>Heteroskedasticity</em></strong></dt>
<dd>
<p>When some ranges of the outcome experience residuals with higher variance (may indicate a predictor missing from the equation).</p>
</dd>
<dt class="horizontal"><strong><em>Partial residual plots</em></strong></dt>
<dd>
<p>A diagnostic plot to illuminate the relationship between the outcome variable and a single predictor.</p>
<dl>
<dt>Synonym</dt>
<dd>
<p>added variables plot<a data-type="indexterm" data-primary="partial residual plots" id="idm46522852060440"/></p>
</dd>
</dl>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Outliers"><div class="sect2" id="regression_outliers">
<h2>Outliers</h2>

<p>Generally speaking, an extreme value, also called an <em>outlier</em>, is one that is distant from most of the other observations.<a data-type="indexterm" data-primary="regression" data-secondary="diagnostics" data-tertiary="outliers" id="idm46522852056584"/><a data-type="indexterm" data-primary="outliers" data-secondary="in regression diagnostics" id="ix_outl"/>
Just as outliers need to be handled for estimates of location and variability
(see <a data-type="xref" href="ch01.xhtml#Location">“Estimates of Location”</a> and <a data-type="xref" href="ch01.xhtml#Variability">“Estimates of Variability”</a>),
outliers can cause problems with regression models.
In regression, an outlier is a record whose actual <em>y</em> value is distant from the predicted value.
You can detect outliers by examining the <em>standardized residual</em>, which is the residual divided by the standard error of the residuals.<a data-type="indexterm" data-primary="standardized residuals" data-secondary="examining to detect outliers" id="idm46522852051464"/></p>

<p>There is no statistical theory that separates outliers from nonoutliers.  Rather, there are (arbitrary) rules of thumb for how distant from the bulk of the data an observation needs to be in order to be called an outlier.
For example,
with the boxplot, outliers are those data points that are too far above or below the box boundaries (see <a data-type="xref" href="ch01.xhtml#Boxplots">“Percentiles and Boxplots”</a>), where “too far” = “more than 1.5 times the interquartile range.”
In regression, the standardized residual is the metric that is typically used to determine whether a record is classified as an outlier.
Standardized residuals can be interpreted as “the number of standard errors away from the regression line.”</p>

<p>Let’s fit a regression to the King County house sales data for all sales in zip code 98105 in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">house_98105</code> <code class="o">&lt;-</code> <code class="n">house</code><code class="p">[</code><code class="n">house</code><code class="o">$</code><code class="n">ZipCode</code> <code class="o">==</code> <code class="m">98105</code><code class="p">,]</code>
<code class="n">lm_98105</code> <code class="o">&lt;-</code> <code class="nf">lm</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="n">SqFtTotLiving</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code>
                 <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">house_98105</code><code class="p">)</code></pre>

<p>In <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">house_98105</code> <code class="o">=</code> <code class="n">house</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">house</code><code class="p">[</code><code class="s1">'ZipCode'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">98105</code><code class="p">,</code> <code class="p">]</code>

<code class="n">predictors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="s1">'SqFtLot'</code><code class="p">,</code> <code class="s1">'Bathrooms'</code><code class="p">,</code> <code class="s1">'Bedrooms'</code><code class="p">,</code> <code class="s1">'BldgGrade'</code><code class="p">]</code>
<code class="n">outcome</code> <code class="o">=</code> <code class="s1">'AdjSalePrice'</code>

<code class="n">house_outlier</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">OLS</code><code class="p">(</code><code class="n">house_98105</code><code class="p">[</code><code class="n">outcome</code><code class="p">],</code>
                       <code class="n">house_98105</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code><code class="o">.</code><code class="n">assign</code><code class="p">(</code><code class="n">const</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code>
<code class="n">result_98105</code> <code class="o">=</code> <code class="n">house_outlier</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code></pre>

<p>We extract the standardized residuals in <em>R</em> using the <code>rstandard</code> function and obtain the index of the smallest residual using the <code>order</code> function:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">sresid</code> <code class="o">&lt;-</code> <code class="nf">rstandard</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">)</code>
<code class="n">idx</code> <code class="o">&lt;-</code> <code class="nf">order</code><code class="p">(</code><code class="n">sresid</code><code class="p">)</code>
<code class="n">sresid</code><code class="p">[</code><code class="n">idx</code><code class="p">[</code><code class="m">1</code><code class="p">]]</code>
    <code class="m">20429</code>
<code class="m">-4.326732</code></pre>

<p class="pagebreak-before">In <code>statsmodels</code>, use <code>OLSInfluence</code> to analyze the residuals:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">influence</code> <code class="o">=</code> <code class="n">OLSInfluence</code><code class="p">(</code><code class="n">result_98105</code><code class="p">)</code>
<code class="n">sresiduals</code> <code class="o">=</code> <code class="n">influence</code><code class="o">.</code><code class="n">resid_studentized_internal</code>
<code class="n">sresiduals</code><code class="o">.</code><code class="n">idxmin</code><code class="p">(),</code> <code class="n">sresiduals</code><code class="o">.</code><code class="n">min</code><code class="p">()</code></pre>

<p>The biggest overestimate from the model is more than four standard errors above the regression line, corresponding to an overestimate of $757,754.
The original data record corresponding to this outlier is as follows in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">house_98105</code><code class="p">[</code><code class="n">idx</code><code class="p">[</code><code class="m">1</code><code class="p">],</code> <code class="nf">c</code><code class="p">(</code><code class="s">'AdjSalePrice'</code><code class="p">,</code> <code class="s">'SqFtTotLiving'</code><code class="p">,</code> <code class="s">'SqFtLot'</code><code class="p">,</code>
              <code class="s">'Bathrooms'</code><code class="p">,</code> <code class="s">'Bedrooms'</code><code class="p">,</code> <code class="s">'BldgGrade'</code><code class="p">)]</code>

<code class="n">AdjSalePrice</code> <code class="n">SqFtTotLiving</code> <code class="n">SqFtLot</code> <code class="n">Bathrooms</code> <code class="n">Bedrooms</code> <code class="nf">BldgGrade</code>
<code class="nf">         </code><code class="p">(</code><code class="n">dbl</code><code class="p">)</code>         <code class="p">(</code><code class="n">int</code><code class="p">)</code>   <code class="p">(</code><code class="n">int</code><code class="p">)</code>     <code class="p">(</code><code class="n">dbl</code><code class="p">)</code>    <code class="p">(</code><code class="n">int</code><code class="p">)</code>     <code class="p">(</code><code class="n">int</code><code class="p">)</code>
<code class="m">20429</code>   <code class="m">119748</code>          <code class="m">2900</code>    <code class="m">7276</code>         <code class="m">3</code>        <code class="m">6</code>         <code class="m">7</code></pre>

<p>In <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">outlier</code> <code class="o">=</code> <code class="n">house_98105</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sresiduals</code><code class="o">.</code><code class="n">idxmin</code><code class="p">(),</code> <code class="p">:]</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'AdjSalePrice'</code><code class="p">,</code> <code class="n">outlier</code><code class="p">[</code><code class="n">outcome</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="n">outlier</code><code class="p">[</code><code class="n">predictors</code><code class="p">])</code></pre>

<p>In this case, it appears that there is something wrong with the record:
a house of that size typically sells for much more than $119,748 in that zip code.
<a data-type="xref" href="#StatutoryDeed">Figure 4-4</a> shows an excerpt from the statutory deed from this sale: it is clear that the sale involved only partial interest in the property.
In this case, the outlier corresponds to a sale that is anomalous and should not be included in the regression.
Outliers could also be the result of other problems,
such as a “fat-finger” data entry or a mismatch of units (e.g., reporting a sale in thousands of dollars rather than simply in dollars).</p>

<figure><div id="StatutoryDeed" class="figure">
<img src="Images/psd2_0404.png" alt="Statutory warranty deed for the largest negative residual" width="1006" height="236"/>
<h6><span class="label">Figure 4-4. </span>Statutory warrany deed for the largest negative residual</h6>
</div></figure>

<p>For big data problems, outliers are generally not a problem in fitting the regression to be used in predicting new data.
However, outliers are central to anomaly detection, where finding outliers is the whole point.
The outlier could also correspond to a case of fraud or an accidental action.
In any case, detecting  outliers can be a critical business need.<a data-type="indexterm" data-primary="outliers" data-secondary="in regression diagnostics" data-startref="ix_outl" id="idm46522851707624"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Influential Values"><div class="sect2" id="idm46522852058600">
<h2>Influential Values</h2>

<p>A value whose absence would significantly change the regression equation is termed an <em>influential observation</em>.<a data-type="indexterm" data-primary="influential values" id="idm46522851704072"/><a data-type="indexterm" data-primary="regression" data-secondary="diagnostics" data-tertiary="influential values" id="idm46522851703336"/>
In regression,
such a value need not be associated with a large residual.
As an example,
consider the regression lines in <a data-type="xref" href="#InfluenceExample">Figure 4-5</a>.
The solid line corresponds to the regression with all the data, while the dashed line corresponds to the regression with the point in the upper-right corner removed.
Clearly, that data value has a huge influence on the regression even though it is not associated with a large outlier (from the full regression).
This data value is considered to have high <em>leverage</em> on the regression.<a data-type="indexterm" data-primary="leverage" data-secondary="influential values in regression" id="idm46522851700296"/></p>

<p>In addition to standardized residuals (see <a data-type="xref" href="#regression_outliers">“Outliers”</a>),
statisticians have developed several metrics to determine the influence of a single record on a regression.<a data-type="indexterm" data-primary="hat-value" id="idm46522851698104"/>
A common measure of leverage is the <em>hat-value</em>;
values above <math alttext="2 left-parenthesis upper P plus 1 right-parenthesis slash n">
  <mrow>
    <mn>2</mn>
    <mo>(</mo>
    <mi>P</mi>
    <mo>+</mo>
    <mn>1</mn>
    <mo>)</mo>
    <mo>/</mo>
    <mi>n</mi>
  </mrow>
</math> indicate a high-leverage data value.<sup><a data-type="noteref" id="idm46522851692056-marker" href="ch04.xhtml#idm46522851692056">6</a></sup></p>

<figure class="width-75"><div id="InfluenceExample" class="figure">
<img src="Images/psd2_0405.png" alt="An example of an influential data point in regression" width="1118" height="1119"/>
<h6><span class="label">Figure 4-5. </span>An example of an influential data point in regression</h6>
</div></figure>

<p>Another metric is <em>Cook’s distance</em>, which defines influence as a combination of leverage and residual size.<a data-type="indexterm" data-primary="Cook's distance" id="idm46522851637960"/>
A rule of thumb is that an observation has high influence if Cook’s distance exceeds <math alttext="4 slash left-parenthesis n minus upper P minus 1 right-parenthesis">
  <mrow>
    <mn>4</mn>
    <mo>/</mo>
    <mo>(</mo>
    <mi>n</mi>
    <mo>-</mo>
    <mi>P</mi>
    <mo>-</mo>
    <mn>1</mn>
    <mo>)</mo>
  </mrow>
</math>.</p>

<p>An <em>influence plot</em> or <em>bubble plot</em> combines standardized residuals, the hat-value, and Cook’s distance in a single plot.<a data-type="indexterm" data-primary="influence plots" id="idm46522851630024"/><a data-type="indexterm" data-primary="bubble plots" id="idm46522851629320"/>
<a data-type="xref" href="#InfluencePlot">Figure 4-6</a> shows the influence plot for the King County house data
and can be created by the following <em>R</em> code:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">std_resid</code> <code class="o">&lt;-</code> <code class="nf">rstandard</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">)</code>
<code class="n">cooks_D</code> <code class="o">&lt;-</code> <code class="nf">cooks.distance</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">)</code>
<code class="n">hat_values</code> <code class="o">&lt;-</code> <code class="nf">hatvalues</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">)</code>
<code class="nf">plot</code><code class="p">(</code><code class="nf">subset</code><code class="p">(</code><code class="n">hat_values</code><code class="p">,</code> <code class="n">cooks_D</code> <code class="o">&gt;</code> <code class="m">0.08</code><code class="p">),</code> <code class="nf">subset</code><code class="p">(</code><code class="n">std_resid</code><code class="p">,</code> <code class="n">cooks_D</code> <code class="o">&gt;</code> <code class="m">0.08</code><code class="p">),</code>
     <code class="n">xlab</code><code class="o">=</code><code class="s">'hat_values'</code><code class="p">,</code> <code class="n">ylab</code><code class="o">=</code><code class="s">'std_resid'</code><code class="p">,</code>
     <code class="n">cex</code><code class="o">=</code><code class="m">10</code><code class="o">*</code><code class="nf">sqrt</code><code class="p">(</code><code class="nf">subset</code><code class="p">(</code><code class="n">cooks_D</code><code class="p">,</code> <code class="n">cooks_D</code> <code class="o">&gt;</code> <code class="m">0.08</code><code class="p">)),</code> <code class="n">pch</code><code class="o">=</code><code class="m">16</code><code class="p">,</code> <code class="n">col</code><code class="o">=</code><code class="s">'lightgrey'</code><code class="p">)</code>
<code class="nf">points</code><code class="p">(</code><code class="n">hat_values</code><code class="p">,</code> <code class="n">std_resid</code><code class="p">,</code> <code class="n">cex</code><code class="o">=</code><code class="m">10</code><code class="o">*</code><code class="nf">sqrt</code><code class="p">(</code><code class="n">cooks_D</code><code class="p">))</code>
<code class="nf">abline</code><code class="p">(</code><code class="n">h</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">-2.5</code><code class="p">,</code> <code class="m">2.5</code><code class="p">),</code> <code class="n">lty</code><code class="o">=</code><code class="m">2</code><code class="p">)</code></pre>

<p>Here is the <em>Python</em> code to create a similar figure:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">influence</code> <code class="o">=</code> <code class="n">OLSInfluence</code><code class="p">(</code><code class="n">result_98105</code><code class="p">)</code>
<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">axhline</code><code class="p">(</code><code class="o">-</code><code class="mf">2.5</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'C1'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">axhline</code><code class="p">(</code><code class="mf">2.5</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'--'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'C1'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">influence</code><code class="o">.</code><code class="n">hat_matrix_diag</code><code class="p">,</code> <code class="n">influence</code><code class="o">.</code><code class="n">resid_studentized_internal</code><code class="p">,</code>
           <code class="n">s</code><code class="o">=</code><code class="mi">1000</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">influence</code><code class="o">.</code><code class="n">cooks_distance</code><code class="p">[</code><code class="mi">0</code><code class="p">]),</code>
           <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'hat values'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'studentized residuals'</code><code class="p">)</code></pre>

<p>There are apparently several data points that exhibit large influence in the regression. Cook’s distance can be computed using the function <code>cooks.distance</code>, and you can use <code>hatvalues</code> to compute the diagnostics. The hat values are plotted on the x-axis, the residuals are plotted on the y-axis, and the size of the points is related to the value of Cook’s distance.</p>

<figure><div id="InfluencePlot" class="figure">
<img src="Images/psd2_0406.png" alt="A plot to determine which observations have high influence" width="1178" height="1188"/>
<h6><span class="label">Figure 4-6. </span>A plot to determine which observations have high influence; points with Cook’s distance greater than 0.08 are highlighted in grey</h6>
</div></figure>

<p><a data-type="xref" href="#InfluenceTable">Table 4-2</a> compares the regression with the full data set and with highly influential data points removed (Cook’s distance &gt; 0.08).</p>

<p>The<a data-type="indexterm" data-primary="regression coefficients" data-secondary="comparison with full data and with influential data removed" id="idm46522851380792"/> regression coefficient for <code>Bathrooms</code> changes quite dramatically.<sup><a data-type="noteref" id="idm46522851379304-marker" href="ch04.xhtml#idm46522851379304">7</a></sup></p>
<table id="InfluenceTable">
<caption><span class="label">Table 4-2. </span>Comparison of regression coefficients with the full data and with influential data removed</caption>
<thead>
<tr>
<th/>
<th>Original</th>
<th>Influential removed</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>(Intercept)</p></td>
<td><p>–772,550</p></td>
<td><p>–647,137</p></td>
</tr>
<tr>
<td><p>SqFtTotLiving</p></td>
<td><p>210</p></td>
<td><p>230</p></td>
</tr>
<tr>
<td><p>SqFtLot</p></td>
<td><p>39</p></td>
<td><p>33</p></td>
</tr>
<tr>
<td><p>Bathrooms</p></td>
<td><p>2282</p></td>
<td><p>–16,132</p></td>
</tr>
<tr>
<td><p>Bedrooms</p></td>
<td><p>–26,320</p></td>
<td><p>–22,888</p></td>
</tr>
<tr>
<td><p>BldgGrade</p></td>
<td><p>130,000</p></td>
<td><p>114,871</p></td>
</tr>
</tbody>
</table>

<p>For purposes of fitting a regression that reliably predicts future data, identifying influential observations is useful only in smaller data sets.
For regressions involving  many records,
it is unlikely that  any one observation will carry sufficient weight to cause extreme influence on the fitted equation
(although the regression may still have big outliers).
For purposes of anomaly detection, though, identifying influential observations can be very useful.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Heteroskedasticity, Non-Normality, and Correlated Errors"><div class="sect2" id="idm46522851705816">
<h2>Heteroskedasticity, Non-Normality, and Correlated Errors</h2>

<p>Statisticians pay considerable attention to the distribution of the residuals.<a data-type="indexterm" data-primary="regression" data-secondary="diagnostics" data-tertiary="heteroskedasticity, non-normality, and correlated errors" id="ix_regrdiagHNCE"/><a data-type="indexterm" data-primary="ordinary least squares (OLS) regression" id="idm46522851356296"/>
It turns out that ordinary least squares (see <a data-type="xref" href="#OLS">“Least Squares”</a>) are unbiased, and in some cases are the “optimal” estimator,
under a wide range of  distributional assumptions.
This means that in most problems, data scientists do not need to be too concerned with the distribution of the residuals.</p>

<p>The distribution of the residuals is relevant mainly for the validity of formal statistical inference (hypothesis tests and p-values), which is of minimal importance to data scientists concerned mainly with predictive accuracy.<a data-type="indexterm" data-primary="non-normal residuals" id="idm46522851353848"/>  Normally distributed errors are a sign that the model is complete; errors that are not normally distributed indicate the model may be missing something.
For formal inference to be fully valid, the residuals are assumed to be normally distributed, have the same variance, and be independent.
One area where this may be of concern to data scientists is the standard calculation of confidence intervals for predicted values,
which are based upon the assumptions about the residuals (see <a data-type="xref" href="#RegressionCIs">“Confidence and Prediction Intervals”</a>).</p>

<p><em>Heteroskedasticity</em> is the lack of constant residual variance across the range of the predicted values.<a data-type="indexterm" data-primary="heteroskedasticity" id="idm46522851351016"/>
In other words, errors are greater for some portions of the range than for others.
Visualizing the data is a convenient way to analyze residuals.</p>

<p>The following code in <em>R</em> plots the absolute residuals versus the predicted values for the <code>lm_98105</code> regression fit in <a data-type="xref" href="#regression_outliers">“Outliers”</a>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">df</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">resid</code> <code class="o">=</code> <code class="nf">residuals</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">),</code> <code class="n">pred</code> <code class="o">=</code> <code class="nf">predict</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">))</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">pred</code><code class="p">,</code> <code class="nf">abs</code><code class="p">(</code><code class="n">resid</code><code class="p">)))</code> <code class="o">+</code> <code class="nf">geom_point</code><code class="p">()</code> <code class="o">+</code> <code class="nf">geom_smooth</code><code class="p">()</code></pre>

<p><a data-type="xref" href="#HouseHetero">Figure 4-7</a> shows the resulting plot.
Using <code>geom_smooth</code>, it is easy to superpose a smooth of the absolute residuals.  The function calls the <code>loess</code> method (locally estimated scatterplot smoothing) to produce a smoothed estimate of the relationship between the variables on the x-axis and y-axis in a scatterplot (see <a data-type="xref" class="pagenum" href="#ScatterplotSmoothers">“Scatterplot Smoothers”</a>).</p>

<p>In <em>Python</em>, the <code>seaborn</code> package has the <code>regplot</code> function to create a similar figure:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">sns</code><code class="o">.</code><code class="n">regplot</code><code class="p">(</code><code class="n">result_98105</code><code class="o">.</code><code class="n">fittedvalues</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">abs</code><code class="p">(</code><code class="n">result_98105</code><code class="o">.</code><code class="n">resid</code><code class="p">),</code>
            <code class="n">scatter_kws</code><code class="o">=</code><code class="p">{</code><code class="s1">'alpha'</code><code class="p">:</code> <code class="mf">0.25</code><code class="p">},</code> <code class="n">line_kws</code><code class="o">=</code><code class="p">{</code><code class="s1">'color'</code><code class="p">:</code> <code class="s1">'C1'</code><code class="p">},</code>
            <code class="n">lowess</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'predicted'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'abs(residual)'</code><code class="p">)</code></pre>

<figure class="width-75"><div id="HouseHetero" class="figure">
<img src="Images/psd2_0407.png" alt="A plot of the absolute value of the residuals versus the predicted values" width="1157" height="1167"/>
<h6><span class="label">Figure 4-7. </span>A plot of the absolute value of the residuals versus the predicted values</h6>
</div></figure>

<p>Evidently, the variance of the residuals tends to increase for higher-valued homes but is also large for lower-valued homes.<a data-type="indexterm" data-primary="heteroskedastic errors" id="idm46522851183320"/>
This plot indicates that <code>lm_98105</code> has <em>heteroskedastic</em> errors.</p>
<div data-type="tip"><h1>Why Would a Data Scientist Care About Heteroskedasticity?</h1>
<p>Heteroskedasticity indicates that prediction errors differ for different ranges of the predicted value, and may suggest an incomplete model.<a data-type="indexterm" data-primary="data science" data-secondary="value of heteroskedasticity for" id="idm46522851180200"/><a data-type="indexterm" data-primary="heteroskedasticity" data-secondary="value to data science" id="idm46522851179256"/>  For example, the heteroskedasticity in <code>lm_98105</code> may indicate that the regression has left something unaccounted for in high- and low-range homes.</p>
</div>

<p><a data-type="xref" href="#HistRegression">Figure 4-8</a> is a histogram of the standardized residuals for the <code>lm_98105</code> regression.<a data-type="indexterm" data-primary="standardized residuals" data-secondary="histogram of, for housing data regression" id="idm46522851175928"/>
The distribution has decidedly longer tails than the normal distribution and exhibits mild skewness toward larger residuals.</p>

<figure class="width-75"><div id="HistRegression" class="figure">
<img src="Images/psd2_0408.png" alt="A histogram of the residuals from the regression of the housing data" width="1152" height="1178"/>
<h6><span class="label">Figure 4-8. </span>A histogram of the residuals from the regression of the housing data</h6>
</div></figure>

<p>Statisticians may also check the assumption that the errors are independent.
This is particularly true for data that is collected over time or space.
The <em>Durbin-Watson</em> statistic <a data-type="indexterm" data-primary="Durbin-Watson statistic" id="idm46522851171368"/>can be used to detect if there is significant autocorrelation in a regression involving time series data.  If the errors from a regression model are correlated, then this information can be useful in making short-term forecasts and should be built into the model. See <em>Practical Time Series Forecasting with R</em>, 2nd ed., by Galit Shmueli and Kenneth Lichtendahl (Axelrod Schnall, 2018) to learn more about how to build autocorrelation information into regression models for time series data.  If longer-term forecasts or explanatory models are the goal, excess autocorrelated data at the microlevel may distract.  In that case, smoothing, or less granular collection of data in the first place, may be in order.</p>

<p>Even though a regression may violate one of the distributional assumptions, should we care?
Most often in data science, the interest is primarily in predictive accuracy, so some review of heteroskedasticity may be in order.  You may discover that there is some signal in the data that your model has not captured. However, satisfying distributional assumptions simply for the sake of validating formal statistical inference (p-values, F-statistics, etc.) is not that important for the data scientist.</p>
<div data-type="note" epub:type="note" id="ScatterplotSmoothers"><h1>Scatterplot Smoothers</h1>
<p>Regression is about modeling the relationship between the response and predictor variables.<a data-type="indexterm" data-primary="scatterplot smoothers" id="idm46522851166840"/>
In evaluating a regression model, it is useful to use a <em>scatterplot smoother</em> to visually highlight relationships between two variables.</p>

<p>For example, in <a data-type="xref" href="#HouseHetero">Figure 4-7</a>, a smooth of the relationship between the absolute residuals and the predicted value shows that the variance of the residuals depends on the value of the residual.
In this case, the <code>loess</code> function was used; <code>loess</code> works by repeatedly fitting a series of local regressions to contiguous subsets to come up with a smooth.
While <code>loess</code> is probably the most commonly used smoother,
other scatterplot smoothers are available in <em>R</em>, such as super smooth (<code>supsmu</code>) and kernel smoothing (<code>ksmooth</code>). In <em>Python</em>, we can find additional smoothers in <code>scipy</code> (<code>wiener</code> or <code>sav</code>) and <code>statsmodels</code> (<code>kernel_regression</code>).
For the purposes of evaluating a regression model, there is typically no need to worry about the details of these scatterplot smooths.<a data-type="indexterm" data-primary="regression" data-secondary="diagnostics" data-tertiary="heteroskedasticity, non-normality, and correlated errors" data-startref="ix_regrdiagHNCE" id="idm46522851158136"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Partial Residual Plots and Nonlinearity"><div class="sect2" id="PartialResidualPlots">
<h2>Partial Residual Plots and Nonlinearity</h2>

<p><em>Partial residual plots</em> are a way to visualize how well the estimated fit explains the relationship between a predictor and the outcome.<a data-type="indexterm" data-primary="regression" data-secondary="diagnostics" data-tertiary="partial residual plots and nonlinearity" id="idm46522851154120"/><a data-type="indexterm" data-primary="partial residual plots" id="idm46522851152856"/>
The basic idea of a partial residual plot is to isolate the relationship between a predictor variable and the response, <em>taking into account all of the other predictor variables</em>.<a data-type="indexterm" data-primary="predictor variables" data-secondary="isolating relationship between response and" id="idm46522851151528"/><a data-type="indexterm" data-primary="response" data-secondary="isolating relationship between predictor variable and" id="idm46522851150520"/>
A partial residual might be thought of as a “synthetic outcome” value, combining the prediction based on a single predictor with the actual residual from the full regression equation.
A partial residual for predictor <math alttext="upper X Subscript i">
  <msub><mi>X</mi> <mi>i</mi> </msub>
</math> is the ordinary residual plus the regression term associated with <math alttext="upper X Subscript i">
  <msub><mi>X</mi> <mi>i</mi> </msub>
</math>:</p>
<div data-type="equation">
<math display="block" alttext="Partial residual equals Residual plus ModifyingAbove b With caret Subscript i Baseline upper X Subscript i Baseline">
  <mrow>
    <mtext>Partial</mtext>
    <mspace width="4.pt"/>
    <mtext>residual</mtext>
    <mo>=</mo>
    <mtext>Residual</mtext>
    <mo>+</mo>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
    <msub><mi>X</mi> <mi>i</mi> </msub>
  </mrow>
</math>
</div>

<p>where <math alttext="ModifyingAbove b With caret Subscript i">
  <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
</math> is the estimated regression coefficient.
The <code>predict</code> function in <em>R</em> has an option to return the individual regression terms <math alttext="ModifyingAbove b With caret Subscript i Baseline upper X Subscript i">
  <mrow>
    <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>i</mi> </msub>
    <msub><mi>X</mi> <mi>i</mi> </msub>
  </mrow>
</math>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">terms</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">,</code> <code class="n">type</code><code class="o">=</code><code class="s">'terms'</code><code class="p">)</code>
<code class="n">partial_resid</code> <code class="o">&lt;-</code> <code class="nf">resid</code><code class="p">(</code><code class="n">lm_98105</code><code class="p">)</code> <code class="o">+</code> <code class="n">terms</code></pre>

<p>The partial residual plot displays the <math alttext="upper X Subscript i">
  <msub><mi>X</mi> <mi>i</mi> </msub>
</math> predictor on the x-axis and the partial residuals on the y-axis.
Using <code>ggplot2</code> makes it easy to superpose a smooth of the partial residuals:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">df</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">SqFtTotLiving</code> <code class="o">=</code> <code class="n">house_98105</code><code class="p">[,</code> <code class="s">'SqFtTotLiving'</code><code class="p">],</code>
                 <code class="n">Terms</code> <code class="o">=</code> <code class="n">terms</code><code class="p">[,</code> <code class="s">'SqFtTotLiving'</code><code class="p">],</code>
                 <code class="n">PartialResid</code> <code class="o">=</code> <code class="n">partial_resid</code><code class="p">[,</code> <code class="s">'SqFtTotLiving'</code><code class="p">])</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="n">PartialResid</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">shape</code><code class="o">=</code><code class="m">1</code><code class="p">)</code> <code class="o">+</code> <code class="nf">scale_shape</code><code class="p">(</code><code class="n">solid</code> <code class="o">=</code> <code class="kc">FALSE</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">geom_smooth</code><code class="p">(</code><code class="n">linetype</code><code class="o">=</code><code class="m">2</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">geom_line</code><code class="p">(</code><code class="nf">aes</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="n">Terms</code><code class="p">))</code></pre>

<p>The <code>statsmodels</code> package has the method <code>sm.graphics.plot_ccpr</code> that creates a similar partial residual plot:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">sm</code><code class="o">.</code><code class="n">graphics</code><code class="o">.</code><code class="n">plot_ccpr</code><code class="p">(</code><code class="n">result_98105</code><code class="p">,</code> <code class="s1">'SqFtTotLiving'</code><code class="p">)</code></pre>

<p>The <em>R</em> and <em>Python</em> graphs differ by a constant shift. In <em>R</em>, a constant is added so that the mean of the terms is zero.</p>

<p>The resulting plot is shown in <a data-type="xref" href="#HousePartialResid">Figure 4-9</a>.
The partial residual is an estimate of the contribution that <code>SqFtTotLiving</code> adds to the sales price.
The relationship between <code>SqFtTotLiving</code> and the sales price is evidently nonlinear (dashed line).
The regression line (solid line) underestimates the sales price for homes less than 1,000 square feet and overestimates the price for homes between 2,000 and 3,000 square feet.
There are too few data points above 4,000 square feet to draw conclusions for those homes.</p>

<figure class="width-75"><div id="HousePartialResid" class="figure">
<img src="Images/psd2_0409.png" alt="A partial residual plot of for the variable SqFtTotLiving" width="1156" height="1156"/>
<h6><span class="label">Figure 4-9. </span>A partial residual plot for the variable <code>SqFtTotLiving</code></h6>
</div></figure>

<p>This <a data-type="indexterm" data-primary="partial residual plots" data-secondary="nonlinearity and" id="idm46522850955928"/>nonlinearity makes sense in this case: adding 500 feet in a small home makes a much bigger difference than adding 500 feet in a large home. This suggests that, instead of a simple linear term for <code>SqFtTotLiving</code>, a nonlinear term should be considered (see <a data-type="xref" href="#NonlinearTerms">“Polynomial and Spline Regression”</a>).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522850911352">
<h5>Key Ideas</h5>
<ul>
<li>
<p>While outliers can cause problems for small data sets, the primary interest with outliers is to identify problems with the data, or locate anomalies.</p>
</li>
<li>
<p>Single records (including regression outliers) can have a big influence on a regression equation with small data, but this effect washes out in big data.</p>
</li>
<li>
<p>If the regression model is used for formal inference (p-values and the like), then certain assumptions about the distribution of the residuals should be checked.
In general, however, the distribution of residuals is not critical in data science.</p>
</li>
<li>
<p>The partial residuals plot can be used to qualitatively assess the fit for each regression term, possibly leading to alternative model specification.</p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Polynomial and Spline Regression"><div class="sect1" id="NonlinearTerms">
<h1>Polynomial and Spline Regression</h1>

<p>The relationship between the response and a predictor variable isn’t necessarily linear.<a data-type="indexterm" data-primary="regression" data-secondary="diagnostics" data-startref="ix_regrdiag" id="idm46522850903768"/>
The response to the dose of a drug is often nonlinear: doubling the dosage generally doesn’t lead to a doubled response.<a data-type="indexterm" data-primary="regression" data-secondary="polynomial and spline regression" id="ix_regrpoly"/>
The demand for a product isn’t a linear function of marketing dollars spent; at some point, demand is likely to be saturated.
There are many ways that regression can be extended to capture these nonlinear effects.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522850900568">
<h5>Key Terms for Nonlinear Regression</h5><dl>
<dt class="horizontal"><strong><em>Polynomial regression</em></strong></dt>
<dd>
<p>Adds polynomial terms (squares, cubes, etc.) to a regression.</p>
</dd>
<dt class="horizontal"><strong><em>Spline regression</em></strong></dt>
<dd>
<p>Fitting a smooth curve with a series of polynomial segments.<a data-type="indexterm" data-primary="spline regression" id="idm46522850946696"/></p>
</dd>
<dt class="horizontal"><strong><em>Knots</em></strong></dt>
<dd>
<p>Values that separate spline segments.<a data-type="indexterm" data-primary="knots" id="idm46522850944184"/></p>
</dd>
<dt class="horizontal"><strong><em>Generalized additive models</em></strong></dt>
<dd>
<p>Spline models with automated selection of knots.<a data-type="indexterm" data-primary="generalized additive models (GAM)" id="idm46522850941704"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>GAM</p>
</dd>
</dl>
</dd>
</dl>
</div></aside>
<div data-type="tip"><h1>Nonlinear Regression</h1>
<p>When statisticians talk about <em>nonlinear regression</em>,
they are referring to models that can’t be fit using least squares.
What kind of models are nonlinear?<a data-type="indexterm" data-primary="nonlinear regression" id="idm46522850937176"/>
Essentially all models where the response cannot be expressed as a linear combination of the predictors or some transform of the predictors.
Nonlinear regression models are  harder and computationally more intensive to fit, since they require numerical optimization.
For this reason, it is generally preferred to use a linear model if possible.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Polynomial"><div class="sect2" id="Polynomial">
<h2>Polynomial</h2>

<p><em>Polynomial regression</em> involves including polynomial terms in a regression equation.<a data-type="indexterm" data-primary="polynomial regression" id="idm46522850880728"/><a data-type="indexterm" data-primary="regression" data-secondary="polynomial and spline regression" data-tertiary="polynomial" id="idm46522850879944"/>
The use of polynomial regression dates back almost to the development of regression itself with a paper by Gergonne in 1815.
For example, a quadratic regression between the response <em>Y</em> and the predictor <em>X</em> would take the form:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>Y</mi>
    <mo>=</mo>
    <msub><mi>b</mi> <mn>0</mn> </msub>
    <mo>+</mo>
    <msub><mi>b</mi> <mn>1</mn> </msub>
    <mi>X</mi>
    <mo>+</mo>
    <msub><mi>b</mi> <mn>2</mn> </msub>
    <msup><mi>X</mi> <mn>2</mn> </msup>
    <mo>+</mo>
    <mi>e</mi>
  </mrow>
</math>
</div>

<p>Polynomial regression can be fit in <em>R</em> through the <code>poly</code> function.
For example, the following fits a quadratic polynomial for <code>SqFtTotLiving</code> with the King County housing data:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">lm</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code>  <code class="nf">poly</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="m">2</code><code class="p">)</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code>
                <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code> <code class="n">Bedrooms</code><code class="p">,</code>
                    <code class="n">data</code><code class="o">=</code><code class="n">house_98105</code><code class="p">)</code>

<code class="n">Call</code><code class="o">:</code>
<code class="nf">lm</code><code class="p">(</code><code class="n">formula</code> <code class="o">=</code> <code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="nf">poly</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="m">2</code><code class="p">)</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code>
   <code class="n">BldgGrade</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code> <code class="n">Bedrooms</code><code class="p">,</code> <code class="n">data</code> <code class="o">=</code> <code class="n">house_98105</code><code class="p">)</code>

<code class="n">Coefficients</code><code class="o">:</code>
           <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code>  <code class="nf">poly</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="m">2</code><code class="p">)</code><code class="m">1</code>  <code class="nf">poly</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="m">2</code><code class="p">)</code><code class="m">2</code>
            <code class="m">-402530.47</code>               <code class="m">3271519.49</code>                <code class="m">776934.02</code>
               <code class="n">SqFtLot</code>                <code class="n">BldgGrade</code>                <code class="n">Bathrooms</code>
                 <code class="m">32.56</code>                <code class="m">135717.06</code>                 <code class="m">-1435.12</code>
              <code class="n">Bedrooms</code>
              <code class="m">-9191.94</code></pre>

<p>In <code>statsmodels</code>, we add the squared term to the model definition using <code>I(SqFtTotLiving**2)</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model_poly</code><code> </code><code class="o">=</code><code> </code><code class="n">smf</code><code class="o">.</code><code class="n">ols</code><code class="p">(</code><code class="n">formula</code><code class="o">=</code><code class="s1">'</code><code class="s1">AdjSalePrice ~  SqFtTotLiving + </code><code class="s1">'</code><code> </code><code class="o">+</code><code>
</code><code>                </code><code class="s1">'</code><code class="s1">+ I(SqFtTotLiving**2) + </code><code class="s1">'</code><code> </code><code class="o">+</code><code>
</code><code>                </code><code class="s1">'</code><code class="s1">SqFtLot + Bathrooms + Bedrooms + BldgGrade</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">data</code><code class="o">=</code><code class="n">house_98105</code><code class="p">)</code><code>
</code><code class="n">result_poly</code><code> </code><code class="o">=</code><code> </code><code class="n">model_poly</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="p">)</code><code>
</code><code class="n">result_poly</code><code class="o">.</code><code class="n">summary</code><code class="p">(</code><code class="p">)</code><code>  </code><a class="co" id="co_regression_and_prediction_CO3-1" href="#callout_regression_and_prediction_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_regression_and_prediction_CO3-1" href="#co_regression_and_prediction_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The intercept and the polynomial coefficients are different compared to <em>R</em>. This is due to different implementations. The remaining coefficients and the predictions are equivalent.</p></dd>
</dl>

<p>There are now two coefficients associated with <code>SqFtTotLiving</code>:
one for the linear term and one for the quadratic term.</p>

<p>The partial residual plot (see <a data-type="xref" href="#PartialResidualPlots">“Partial Residual Plots and Nonlinearity”</a>) indicates some curvature in the regression equation associated with <code>SqFtTotLiving</code>.
The fitted line more closely matches the smooth (see <a data-type="xref" href="#Splines">“Splines”</a>) of the partial residuals as compared to a linear fit (see <a data-type="xref" href="#PolynomialRegressionPlot">Figure 4-10</a>).</p>

<p>The <code>statsmodels</code> implementation works only for linear terms. The accompanying source code gives an implementation that will work for polynomial regression as well.</p>

<figure class="width-75"><div id="PolynomialRegressionPlot" class="figure">
<img src="Images/psd2_0410.png" alt="A polynomial regression fit for the variable SqFtTotLiving (solid line) versus a smooth (dashed line)" width="1156" height="1156"/>
<h6><span class="label">Figure 4-10. </span>A polynomial regression fit for the variable <code>SqFtTotLiving</code> (solid line) versus a smooth (dashed line; see the following section about splines)</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Splines"><div class="sect2" id="Splines">
<h2>Splines</h2>

<p>Polynomial regression captures only a certain amount of curvature in a nonlinear relationship.<a data-type="indexterm" data-primary="regression" data-secondary="polynomial and spline regression" data-tertiary="splines" id="idm46522850772360"/><a data-type="indexterm" data-primary="spline regression" id="idm46522850776600"/>
Adding in higher-order terms, such as a cubic quartic polynomial, often leads to undesirable “wiggliness” in the regression equation.
An alternative, and often superior, approach to modeling nonlinear relationships is to use <em>splines</em>.
<em>Splines</em> provide a way to smoothly interpolate between fixed points.
Splines were originally used by draftsmen to draw a smooth curve, particularly in ship and aircraft building.</p>

<p>The splines were created by bending a thin piece of wood using weights, referred to as “ducks”; see <a data-type="xref" href="#SplineDucks">Figure 4-11</a>.</p>

<figure><div id="SplineDucks" class="figure">
<img src="Images/psd2_0411.png" alt="Splines were originally created using bendable wood and ``ducks,'' and were used as a draftsman's tool to fit curves. Photo courtesy of Bob Perry." width="295" height="243"/>
<h6><span class="label">Figure 4-11. </span>Splines were originally created using bendable wood and “ducks” and were used as a draftsman’s tool to fit curves (photo courtesy of Bob Perry)</h6>
</div></figure>

<p>The technical definition of a spline is a series of piecewise continuous polynomials.  They were first developed during World War II at the US Aberdeen Proving Grounds by I. J. Schoenberg, a Romanian mathematician.
The polynomial pieces are smoothly connected at a series of fixed points in a predictor variable, referred to as <em>knots</em>.<a data-type="indexterm" data-primary="knots" id="idm46522850660152"/>
Formulation of splines is much more complicated than polynomial regression;
statistical software usually handles the details of fitting a spline.
The <em>R</em> package <code>splines</code> includes the function <code>bs</code> to create a <em>b-spline</em> (basis spline) term in a regression model.<a data-type="indexterm" data-primary="b-spline (basis spline)" id="idm46522850657400"/>
For example, the following adds a b-spline term to the house regression model:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">splines</code><code class="p">)</code>
<code class="n">knots</code> <code class="o">&lt;-</code> <code class="nf">quantile</code><code class="p">(</code><code class="n">house_98105</code><code class="o">$</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="n">p</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">.25</code><code class="p">,</code> <code class="m">.5</code><code class="p">,</code> <code class="m">.75</code><code class="p">))</code>
<code class="n">lm_spline</code> <code class="o">&lt;-</code> <code class="nf">lm</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="nf">bs</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="n">knots</code><code class="o">=</code><code class="n">knots</code><code class="p">,</code> <code class="n">degree</code><code class="o">=</code><code class="m">3</code><code class="p">)</code> <code class="o">+</code>
  <code class="n">SqFtLot</code> <code class="o">+</code> <code class="n">Bathrooms</code> <code class="o">+</code> <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code><code class="p">,</code>  <code class="n">data</code><code class="o">=</code><code class="n">house_98105</code><code class="p">)</code></pre>

<p>Two parameters need to be specified: the degree of the polynomial and the location of the knots.
In this case, the predictor <code>SqFtTotLiving</code> is included in the model using a cubic spline (<code>degree=3</code>).
By default, <code>bs</code> places knots at the boundaries;
in addition, knots were also placed at the lower quartile, the median quartile, and the upper <span class="keep-together">quartile</span>.</p>

<p>The <code>statsmodels</code> formula interface supports the use of splines in a similar way to <em>R</em>. Here, we specify the <em>b-spline</em> using <code>df</code>, the degrees of freedom. This will create <code>df</code> – <code>degree</code> = 6 – 3 = 3 internal knots with positions calculated in the same way as in the <em>R</em> code above:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">formula</code> <code class="o">=</code> <code class="s1">'AdjSalePrice ~ bs(SqFtTotLiving, df=6, degree=3) + '</code> <code class="o">+</code>
          <code class="s1">'SqFtLot + Bathrooms + Bedrooms + BldgGrade'</code>
<code class="n">model_spline</code> <code class="o">=</code> <code class="n">smf</code><code class="o">.</code><code class="n">ols</code><code class="p">(</code><code class="n">formula</code><code class="o">=</code><code class="n">formula</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">house_98105</code><code class="p">)</code>
<code class="n">result_spline</code> <code class="o">=</code> <code class="n">model_spline</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code></pre>

<p>In contrast to a linear term, for which the coefficient has a direct meaning, the coefficients for a spline term are not interpretable.
Instead, it is more useful to use the visual display to reveal the nature of the spline fit.<a data-type="indexterm" data-primary="partial residual plots" data-secondary="for spline regression" id="idm46522850561592"/>
<a data-type="xref" href="#SplineRegressionPlot">Figure 4-12</a> displays the partial residual plot from the regression.
In contrast to the polynomial model,
the spline model more closely matches the smooth,
demonstrating the greater flexibility of splines.
In this case, the line more closely fits the data.
Does this mean the spline regression is a better model?
Not necessarily: it doesn’t make economic sense that very small homes (less than 1,000 square feet) would have higher value than slightly larger homes. This is possibly an artifact of a confounding variable; see <a data-type="xref" href="#ConfoundingVariables">“Confounding Variables”</a>.</p>

<figure class="width-75"><div id="SplineRegressionPlot" class="figure">
<img src="Images/psd2_0412.png" alt="A spline regression fit for the variable SqFtTotLiving (solid line) compared to a smooth (dashed line)" width="1156" height="1156"/>
<h6><span class="label">Figure 4-12. </span>A spline regression fit for the variable <code>SqFtTotLiving</code> (solid line) compared to a smooth (dashed line)</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Generalized Additive Models"><div class="sect2" id="GAMS">
<h2>Generalized Additive Models</h2>

<p>Suppose you suspect a nonlinear <a data-type="indexterm" data-primary="regression" data-secondary="polynomial and spline regression" data-tertiary="generalized additive models" id="ix_regrpolyGAM"/><a data-type="indexterm" data-primary="generalized additive models (GAM)" id="ix_GAMs"/>relationship between the response and a predictor variable,
either by a priori knowledge or by examining the regression diagnostics.
Polynomial terms may not be flexible enough to capture the relationship, and spline terms require specifying the knots.
<em>Generalized additive models</em>, or <em>GAM</em>, are a flexible modeling technique that can be used to automatically fit a spline regression.
The <code>mgcv</code> package in <em>R</em> can be used to fit a GAM model to the housing data:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">mgcv</code><code class="p">)</code>
<code class="n">lm_gam</code> <code class="o">&lt;-</code> <code class="nf">gam</code><code class="p">(</code><code class="n">AdjSalePrice</code> <code class="o">~</code> <code class="nf">s</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">)</code> <code class="o">+</code> <code class="n">SqFtLot</code> <code class="o">+</code>
                    <code class="n">Bathrooms</code> <code class="o">+</code>  <code class="n">Bedrooms</code> <code class="o">+</code> <code class="n">BldgGrade</code><code class="p">,</code>
                    <code class="n">data</code><code class="o">=</code><code class="n">house_98105</code><code class="p">)</code></pre>

<p>The term <code>s(SqFtTotLiving)</code> tells the <code>gam</code> function to find the “best” knots for a spline term (see <a data-type="xref" href="#GAMPlot">Figure 4-13</a>).</p>

<figure class="width-75"><div id="GAMPlot" class="figure">
<img src="Images/psd2_0413.png" alt="A GAM regression fit for the variable SqFtTotLiving (solid line) compared to a smooth (dashed line)" width="1156" height="1156"/>
<h6><span class="label">Figure 4-13. </span>A GAM regression fit for the variable <code>SqFtTotLiving</code> (solid line) compared to a smooth (dashed line)</h6>
</div></figure>

<p>In <em>Python</em>, we can use the <code>pyGAM</code> package. It provides methods for regression and classification. Here, we use <code>LinearGAM</code> to create a regression model:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictors</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">SqFtTotLiving</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">SqFtLot</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">Bathrooms</code><code class="s1">'</code><code class="p">,</code><code>  </code><code class="s1">'</code><code class="s1">Bedrooms</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">BldgGrade</code><code class="s1">'</code><code class="p">]</code><code>
</code><code class="n">outcome</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">AdjSalePrice</code><code class="s1">'</code><code>
</code><code class="n">X</code><code> </code><code class="o">=</code><code> </code><code class="n">house_98105</code><code class="p">[</code><code class="n">predictors</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code>
</code><code class="n">y</code><code> </code><code class="o">=</code><code> </code><code class="n">house_98105</code><code class="p">[</code><code class="n">outcome</code><code class="p">]</code><code>
</code><code>
</code><code class="n">gam</code><code> </code><code class="o">=</code><code> </code><code class="n">LinearGAM</code><code class="p">(</code><code class="n">s</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="n">n_splines</code><code class="o">=</code><code class="mi">12</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><code class="n">l</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><code class="n">l</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><code class="n">l</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code> </code><code class="o">+</code><code> </code><code class="n">l</code><code class="p">(</code><code class="mi">4</code><code class="p">)</code><code class="p">)</code><code>  </code><a class="co" id="co_regression_and_prediction_CO4-1" href="#callout_regression_and_prediction_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">gam</code><code class="o">.</code><code class="n">gridsearch</code><code class="p">(</code><code class="n">X</code><code class="p">,</code><code> </code><code class="n">y</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_regression_and_prediction_CO4-1" href="#co_regression_and_prediction_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The default value for <code>n_splines</code> is 20. This leads to overfitting for larger <code>SqFtTotLiving</code> values. A value of 12 leads to a more reasonable fit.</p></dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522850365368">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Outliers in a regression are records with a large residual.</p>
</li>
<li>
<p>Multicollinearity can cause numerical instability in fitting the regression <span class="keep-together">equation</span>.</p>
</li>
<li>
<p>A confounding variable is an important predictor that is omitted from a model and can lead to a regression equation with spurious relationships.</p>
</li>
<li>
<p>An interaction term between two variables is needed if the effect of one variable depends on the level or magnitude of the other.</p>
</li>
<li>
<p>Polynomial regression can fit nonlinear relationships between predictors and the outcome variable.</p>
</li>
<li>
<p>Splines are series of polynomial segments strung together, joining at knots.</p>
</li>
<li>
<p>We can automate the process of specifying the knots in splines using generalized additive models (GAM).<a data-type="indexterm" data-primary="regression" data-secondary="polynomial and spline regression" data-tertiary="generalized additive models" data-startref="ix_regrpolyGAM" id="idm46522850372008"/><a data-type="indexterm" data-primary="generalized additive models (GAM)" data-startref="ix_GAMs" id="idm46522850378936"/></p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522850555080">
<h2>Further Reading</h2>

<ul>
<li>
<p>For more on spline models and GAMs, see <em>The Elements of Statistical Learning</em>, 2nd ed., by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), and its shorter cousin based on <em>R</em>, <em>An Introduction to Statistical Learning</em> by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (2013); both are Springer books.</p>
</li>
<li>
<p>To learn more about using regression models for time series forecasting, see <em>Practical Time Series Forecasting with R</em> by Galit Shmueli and Kenneth Lichtendahl (Axelrod Schnall, 2018).<a data-type="indexterm" data-primary="regression" data-secondary="polynomial and spline regression" data-startref="ix_regrpoly" id="idm46522850387944"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46522850385864">
<h1>Summary</h1>

<p>Perhaps no other statistical method has seen greater use over the years than  regression—the process of establishing a relationship between multiple predictor variables and an outcome variable. The fundamental form is linear: each predictor variable has a coefficient that describes a linear relationship between the predictor and the outcome.  More advanced forms of regression, such as polynomial and spline regression, permit the relationship to be nonlinear.  In classical statistics, the emphasis is on finding a good fit to the observed data to explain or describe some phenomenon, and the strength of this fit is how traditional <em>in-sample</em> metrics are used to assess the model.  In data science, by contrast, the goal is typically to predict values for new data, so metrics based on predictive accuracy for out-of-sample data are used.  Variable<a data-type="indexterm" data-primary="regression" data-startref="ix_regr" id="idm46522850391400"/> selection methods are used to reduce dimensionality and create more compact  <span class="keep-together">models</span>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46522856635400"><sup><a href="ch04.xhtml#idm46522856635400-marker">1</a></sup> This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck; used by permission.</p><p data-type="footnote" id="idm46522856258456"><sup><a href="ch04.xhtml#idm46522856258456-marker">2</a></sup> In Bayesian statistics, the true value is assumed to be a random variable with a specified distribution. In the Bayesian context, instead of estimates of unknown parameters, there are posterior and prior distributions.</p><p data-type="footnote" id="idm46522853979800"><sup><a href="ch04.xhtml#idm46522853979800-marker">3</a></sup> The <code>-1</code> argument in the <code>model.matrix</code> produces one hot encoding representation (by removing the intercept, hence the “-”).  Otherwise, the default in <em>R</em> is to produce a matrix with <em>P</em> – 1 columns with the first factor level as a reference.</p><p data-type="footnote" id="idm46522853557496"><sup><a href="ch04.xhtml#idm46522853557496-marker">4</a></sup> This is unintuitive, but can be explained by the impact of location as a confounding variable; see <a data-type="xref" href="#ConfoundingVariables">“Confounding Variables”</a>.</p><p data-type="footnote" id="idm46522852564136"><sup><a href="ch04.xhtml#idm46522852564136-marker">5</a></sup> There are 80 zip codes in King County, several with just a handful of sales. An alternative to directly using zip code as a factor variable, <code>ZipGroup</code> clusters similar zip codes into a single group. See <a data-type="xref" href="#FactorVariablesManyLevels">“Factor Variables with Many Levels”</a> for details.</p><p data-type="footnote" id="idm46522851692056"><sup><a href="ch04.xhtml#idm46522851692056-marker">6</a></sup> The term <em>hat-value</em> comes from the notion of the hat matrix in regression. Multiple linear regression can be expressed by the formula <math alttext="ModifyingAbove upper Y With caret equals upper H upper Y">
  <mrow>
    <mover accent="true"><mi>Y</mi> <mo>^</mo></mover>
    <mo>=</mo>
    <mi>H</mi>
    <mi>Y</mi>
  </mrow>
</math> where <math alttext="upper H">
  <mi>H</mi>
</math> is the hat matrix. The hat-values correspond to the diagonal of <math alttext="upper H">
  <mi>H</mi>
</math>.</p><p data-type="footnote" id="idm46522851379304"><sup><a href="ch04.xhtml#idm46522851379304-marker">7</a></sup> The coefficient for <code>Bathrooms</code> becomes negative, which is unintuitive. Location has not been taken into account, and the zip code 98105 contains areas of disparate types of homes. See <a data-type="xref" href="#ConfoundingVariables">“Confounding Variables”</a> for a discussion of confounding variables.</p></div></div></section></div>



  </body></html>