["```py\n# a simple example of reading data from a .csv file with Python\n# using the \"csv\" library.\n# the source data was sampled from the Citi Bike system data:\n# https://drive.google.com/file/d/17b461NhSjf_akFWvjgNXQfqgh9iFxCu_/\n# which can be found here:\n# https://s3.amazonaws.com/tripdata/index.html\n\n# import the `csv` library ![1](assets/1.png)\nimport csv\n\n# open the `202009CitibikeTripdataExample.csv` file in read (\"r\") mode\n# this file should be in the same folder as our Python script or notebook\nsource_file = open(\"202009CitibikeTripdataExample.csv\",\"r\") ![2](assets/2.png)\n\n# pass our `source_file` as an ingredient to the `csv` library's\n# DictReader \"recipe\".\n# store the result in a variable called `citibike_reader`\ncitibike_reader = csv.DictReader(source_file)\n\n# the DictReader method has added some useful information to our data,\n# like a `fieldnames` property that lets us access all the values\n# in the first or \"header\" row\nprint(citibike_reader.fieldnames) ![3](assets/3.png)\n\n# let's just print out the first 5 rows\nfor i in range(0,5): ![4](assets/4.png)\n    print (next(citibike_reader))\n```", "```py\n['tripduration', 'starttime', 'StartDate', 'stoptime', 'start station id',\n'start station name', 'start station latitude', 'start station longitude', 'end\nstation id', 'end station name', 'end station latitude', 'end station\nlongitude', 'bikeid', 'usertype', 'birth year', 'gender']\n{'tripduration': '4225', 'starttime': '2020-09-01 00:00:01.0430', 'StartDate':\n'2020-09-01', 'stoptime': '2020-09-01 01:10:26.6350', 'start station id':\n'3508', 'start station name': 'St Nicholas Ave & Manhattan Ave', 'start station\nlatitude': '40.809725', 'start station longitude': '-73.953149', 'end station\nid': '116', 'end station name': 'W 17 St & 8 Ave', 'end station latitude': '40.\n74177603', 'end station longitude': '-74.00149746', 'bikeid': '44317',\n'usertype': 'Customer', 'birth year': '1979', 'gender': '1'}\n ...\n{'tripduration': '1193', 'starttime': '2020-09-01 00:00:12.2020', 'StartDate':\n'2020-09-01', 'stoptime': '2020-09-01 00:20:05.5470', 'start station id':\n'3081', 'start station name': 'Graham Ave & Grand St', 'start station\nlatitude': '40.711863', 'start station longitude': '-73.944024', 'end station\nid': '3048', 'end station name': 'Putnam Ave & Nostrand Ave', 'end station\nlatitude': '40.68402', 'end station longitude': '-73.94977', 'bikeid': '26396',\n'usertype': 'Customer', 'birth year': '1969', 'gender': '0'}\n```", "```py\n# a simple example of reading data from a .tsv file with Python, using\n# the `csv` library. The source data was downloaded as a .tsv file\n# from Jed Shugerman's Google Sheet on prosecutor politicians: ![1](assets/1.png)\n# https://docs.google.com/spreadsheets/d/1E6Z-jZWbrKmit_4lG36oyQ658Ta6Mh25HCOBaz7YVrA\n\n# import the `csv` library\nimport csv\n\n# open the `ShugermanProsecutorPoliticians-SupremeCourtJustices.tsv` file\n# in read (\"r\") mode.\n# this file should be in the same folder as our Python script or notebook\ntsv_source_file = open(\"ShugermanProsecutorPoliticians-SupremeCourtJustices.tsv\",\"r\")\n\n# pass our `tsv_source_file` as an ingredient to the csv library's\n# DictReader \"recipe.\"\n# store the result in a variable called `politicians_reader`\npoliticians_reader = csv.DictReader(tsv_source_file, delimiter='\\t')\n\n# the DictReader method has added some useful information to our data,\n# like a `fieldnames` property that lets us access all the values\n# in the first or \"header\" row\nprint(politicians_reader.fieldnames)\n\n# we'll use the `next()` function to print just the first row of data\nprint (next(politicians_reader))\n```", "```py\n['', 'Justice', 'Term Start/End', 'Party', 'State', 'Pres Appt', 'Other Offices\nHeld', 'Relevant Prosecutorial Background']\n{'': '40', 'Justice': 'William Strong', 'Term Start/End': '1870-1880', 'Party':\n'D/R', 'State': 'PA', 'Pres Appt': 'Grant', 'Other Offices Held': 'US House,\nSupr Court of PA, elect comm for elec of 1876', 'Relevant Prosecutorial\nBackground': 'lawyer'}\n```", "```py\n# a simple example of reading data from a .tsv file with Python, using\n# the `csv` library. The source data was downloaded as a .tsv file\n# from Jed Shugerman's Google Sheet on prosecutor politicians:\n# https://docs.google.com/spreadsheets/d/1E6Z-jZWbrKmit_4lG36oyQ658Ta6Mh25HCOBaz7YVrA\n# the original .tsv file was renamed with a file extension of .txt\n\n# import the `csv` library\nimport csv\n\n# open the `ShugermanProsecutorPoliticians-SupremeCourtJustices.txt` file\n# in read (\"r\") mode.\n# this file should be in the same folder as our Python script or notebook\ntxt_source_file = open(\"ShugermanProsecutorPoliticians-SupremeCourtJustices.txt\",\"r\")\n\n# pass our txt_source_file as an ingredient to the csv library's DictReader\n# \"recipe\" and store the result in a variable called `politicians_reader`\n# add the \"delimiter\" parameter and specify the tab character, \"\\t\"\npoliticians_reader = csv.DictReader(txt_source_file, delimiter='\\t') ![1](assets/1.png)\n\n# the DictReader function has added useful information to our data,\n# like a label that shows us all the values in the first or \"header\" row\nprint(politicians_reader.fieldnames)\n\n# we'll use the `next()` function to print just the first row of data\nprint (next(politicians_reader))\n```", "```py\npip install openpyxl\npip install pyexcel-ods\npip install xlrd==2.0.1\n```", "```py\n# an example of reading data from an .xlsx file with Python, using the \"openpyxl\"\n# library. First, you'll need to pip install the openpyxl library:\n# https://pypi.org/project/openpyxl/\n# the source data can be composed and downloaded from:\n# https://fred.stlouisfed.org/series/U6RATE\n\n# specify the \"chapter\" you want to import from the \"openpyxl\" library\n# in this case, \"load_workbook\"\nfrom openpyxl import load_workbook\n\n# import the `csv` library, to create our output file\nimport csv\n\n# pass our filename as an ingredient to the `openpyxl` library's\n# `load_workbook()` \"recipe\"\n# store the result in a variable called `source_workbook`\nsource_workbook = load_workbook(filename = 'fredgraph.xlsx')\n\n# an .xlsx workbook can have multiple sheets\n# print their names here for reference\nprint(source_workbook.sheetnames) ![1](assets/1.png)\n\n# loop through the worksheets in `source_workbook`\nfor sheet_num, sheet_name in enumerate(source_workbook.sheetnames): ![2](assets/2.png)\n\n    # create a variable that points to the current worksheet by\n    # passing the current value of `sheet_name` to `source_workbook`\n    current_sheet = source_workbook[sheet_name]\n\n    # print `sheet_name`, just to see what it is\n    print(sheet_name)\n\n    # create an output file called \"xlsx_\"+sheet_name\n    output_file = open(\"xlsx_\"+sheet_name+\".csv\",\"w\") ![3](assets/3.png)\n\n    # use this csv library's \"writer\" recipe to easily write rows of data\n    # to `output_file`, instead of reading data *from* it\n    output_writer = csv.writer(output_file)\n\n    # loop through every row in our sheet\n    for row in current_sheet.iter_rows(): ![4](assets/4.png)\n\n        # we'll create an empty list where we'll put the actual\n        # values of the cells in each row\n        row_cells = [] ![5](assets/5.png)\n\n        # for every cell (or column) in each row....\n        for cell in row:\n\n            # let's print what's in here, just to see how the code sees it\n            print(cell, cell.value)\n\n            # add the values to the end of our list with the `append()` method\n            row_cells.append(cell.value)\n\n        # write our newly (re)constructed data row to the output file\n        output_writer.writerow(row_cells) ![6](assets/6.png)\n\n    # officially close the `.csv` file we just wrote all that data to\n    output_file.close()\n```", "```py\n# an example of reading data from an .ods file with Python, using the\n# \"pyexcel_ods\" library. First, you'll need to pip install the library:\n# https://pypi.org/project/pyexcel-ods/\n\n# specify the \"chapter\" of the \"pyexcel_ods\" library you want to import,\n# in this case, `get_data`\nfrom pyexcel_ods import get_data\n\n# import the `csv` library, to create our output file\nimport csv\n\n# pass our filename as an ingredient to the `pyexcel_ods` library's\n# `get_data()` \"recipe\"\n# store the result in a variable called `source_workbook`\nsource_workbook = get_data(\"fredgraph.ods\")\n\n# an `.ods` workbook can have multiple sheets\nfor sheet_name, sheet_data in source_workbook.items(): ![1](assets/1.png)\n\n    # print `sheet_name`, just to see what it is\n    print(sheet_name)\n\n    # create \"ods_\"+sheet_name+\".csv\" as an output file for the current sheet\n    output_file = open(\"ods_\"+sheet_name+\".csv\",\"w\")\n\n    # use this csv library's \"writer\" recipe to easily write rows of data\n    # to `output_file`, instead of reading data *from* it\n    output_writer = csv.writer(output_file)\n\n    # now, we need to loop through every row in our sheet\n    for row in sheet_data: ![2](assets/2.png)\n\n        # use the `writerow` recipe to write each `row`\n        # directly to our output file\n        output_writer.writerow(row) ![3](assets/3.png)\n\n    # officially close the `.csv` file we just wrote all that data to\n    output_file.close()\n```", "```py\n# a simple example of reading data from a .xls file with Python\n# using the \"xrld\" library. First, pip install the xlrd library:\n# https://pypi.org/project/xlrd/2.0.1/\n\n# import the \"xlrd\" library\nimport xlrd\n\n# import the `csv` library, to create our output file\nimport csv\n\n# pass our filename as an ingredient to the `xlrd` library's\n# `open_workbook()` \"recipe\"\n# store the result in a variable called `source_workbook`\nsource_workbook = xlrd.open_workbook(\"fredgraph.xls\") ![1](assets/1.png)\n\n# an `.xls` workbook can have multiple sheets\nfor sheet_name in source_workbook.sheet_names():\n\n    # create a variable that points to the current worksheet by\n    # passing the current value of `sheet_name` to the `sheet_by_name` recipe\n    current_sheet = source_workbook.sheet_by_name(sheet_name)\n\n    # print `sheet_name`, just to see what it is\n    print(sheet_name)\n\n    # create \"xls_\"+sheet_name+\".csv\" as an output file for the current sheet\n    output_file = open(\"xls_\"+sheet_name+\".csv\",\"w\")\n\n    # use the `csv` library's \"writer\" recipe to easily write rows of data\n    # to `output_file`, instead of reading data *from* it\n    output_writer = csv.writer(output_file)\n\n    # now, we need to loop through every row in our sheet\n    for row_num, row in enumerate(current_sheet.get_rows()): ![2](assets/2.png)\n\n        # each row is already a list, but we need to use the `row_value()`\n        # method to access them\n        # then we can use the `writerow` recipe to write them\n        # directly to our output file\n        output_writer.writerow(current_sheet.row_values(row_num)) ![3](assets/3.png)\n\n    # officially close the `.csv` file we just wrote all that data to\n    output_file.close()\n```", "```py\n------------------------------\nVariable   Columns   Type\n------------------------------\nID            1-11   Character\nLATITUDE     13-20   Real\nLONGITUDE    22-30   Real\nELEVATION    32-37   Real\nSTATE        39-40   Character\nNAME         42-71   Character\nGSN FLAG     73-75   Character\nHCN/CRN FLAG 77-79   Character\nWMO ID       81-85   Character\n------------------------------\n```", "```py\n# an example of reading data from a fixed-width file with Python.\n# the source file for this example comes from NOAA and can be accessed here:\n# https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n# the metadata for the file can be found here:\n# https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n\n# import the `csv` library, to create our output file\nimport csv\n\nfilename = \"ghcnd-stations\"\n\n# reading from a basic text file doesn't require any special libraries\n# so we'll just open the file in read format (\"r\") as usual\nsource_file = open(filename+\".txt\", \"r\")\n\n# the built-in \"readlines()\" method does just what you'd think:\n# it reads in a text file and converts it to a list of lines\nstations_list = source_file.readlines()\n\n# create an output file for our transformed data\noutput_file = open(filename+\".csv\",\"w\")\n\n# use the `csv` library's \"writer\" recipe to easily write rows of data\n# to `output_file`, instead of reading data *from* it\noutput_writer = csv.writer(output_file)\n\n# create the header list\nheaders = [\"ID\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"STATE\",\"NAME\",\"GSN_FLAG\",\n           \"HCNCRN_FLAG\",\"WMO_ID\"] ![1](assets/1.png)\n\n# write our headers to the output file\noutput_writer.writerow(headers)\n\n# loop through each line of our file (multiple \"sheets\" are not possible)\nfor line in stations_list:\n\n    # create an empty list, to which we'll append each set of characters that\n    # makes up a given \"column\" of data\n    new_row = []\n\n    # ID: positions 1-11\n    new_row.append(line[0:11]) ![2](assets/2.png)\n\n    # LATITUDE: positions 13-20\n    new_row.append(line[12:20])\n\n    # LONGITUDE: positions 22-30\n    new_row.append(line[21:30])\n\n    # ELEVATION: positions 32-37\n    new_row.append(line[31:37])\n\n    # STATE: positions 39-40\n    new_row.append(line[38:40])\n\n    # NAME: positions 42-71\n    new_row.append(line[41:71])\n\n    # GSN_FLAG: positions 73-75\n    new_row.append(line[72:75])\n\n    # HCNCRN_FLAG: positions 77-79\n    new_row.append(line[76:79])\n\n    # WMO_ID: positions 81-85\n    new_row.append(line[80:85])\n\n    # now all that's left is to use the\n    # `writerow` function to write new_row to our output file\n    output_writer.writerow(new_row)\n\n# officially close the `.csv` file we just wrote all that data to\noutput_file.close()\n```", "```py\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <mainDoc>\n    <!--This is a comment-->\n    <elements>\n        <element1>This is some text in the document.</element1>\n        <element2>This is some other data in the document.</element2>\n        <element3 someAttribute=\"aValue\" />\n    </elements>\n    <someElement anAttribute=\"anotherValue\">More content</someElement>\n</mainDoc>\n```", "```py\n<mainDoc>\n```", "```py\n <outerElement>\n    <!-- Notice that that the `innerElement1` is closed\n before the `innerElement2` tag is opened -->\n    <innerElement1>Some content</innerElement1>\n    <innerElement2>More content</innerElement2>\n </outerElement>\n```", "```py\n <outerElement>\n    <!-- NOPE! The `innerElement2` tag was opened\n before the `innerElement1` tag was closed -->\n    <innerElement1>Some content<innerElement2>More content</innerElement1>\n    </innerElement2>\n </outerElement>\n```", "```py\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <mainDoc>\n    <!--`mainDoc` is the *root* element, and `elements` is its *child*-->\n    <elements>\n        <!-- `elements` is the *parent* of `element1`, `element2`, and\n `element3`, which are *siblings* of one another -->\n        <element1>This is text data in the document.</element1>\n        <element2>This is some other data in the document.</element2>\n        <element3 someAttribute=\"aValue\" />\n    </elements>\n    <!-- `someElement` is also a *child* of `mainDoc`,\n and a *sibling* of `elements` -->\n    <someElement anAttribute=\"anotherValue\">More content</someElement>\n</mainDoc>\n```", "```py\n <element3 someAttribute=\"aValue\" />\n```", "```py\n<aBook>\n    <bookURL url=\"https://www.oreilly.com/library/view/practical-python-data/\n 9781492091493\"/>\n    <bookAbstract>\n    There are awesome discoveries to be made and valuable stories to be\n    told in datasets--and this book will help you uncover them.\n    </bookAbstract>\n    <pubDate date=\"2022-02-01\" />\n</aBook>\n```", "```py\n<aBook>\n    <bookURL>\n        https://www.oreilly.com/library/view/practical-python-data/9781492091493\n    </bookURL>\n    <bookAbstract>\n        There are awesome discoveries to be made and valuable stories to be\n        told in datasets--and this book will help you uncover them.\n    </bookAbstract>\n    <pubDate>2022-02-01</pubDate>\n</aBook>\n```", "```py\npip install lxml\n```", "```py\n# an example of reading data from an .xml file with Python, using the \"lxml\"\n# library.\n# first, you'll need to pip install the lxml library:\n# https://pypi.org/project/lxml/\n# a helpful tutorial can be found here: https://lxml.de/tutorial.html\n# the data used here is an instance of\n# https://api.stlouisfed.org/fred/series/observations?series_id=U6RATE& \\\n# api_key=YOUR_API_KEY_HERE\n\n# specify the \"chapter\" of the `lxml` library you want to import,\n# in this case, `etree`, which stands for \"ElementTree\"\nfrom lxml import etree\n\n# import the `csv` library, to create our output file\nimport csv\n\n# choose a filename\nfilename = \"U6_FRED_data\" ![1](assets/1.png)\n\n# open our data file in read format, using \"rb\" as the \"mode\"\nxml_source_file = open(filename+\".xml\",\"rb\") ![2](assets/2.png)\n\n# pass our xml_source_file as an ingredient to the `lxml` library's\n# `etree.parse()` method and store the result in a variable called `xml_doc`\nxml_doc = etree.parse(xml_source_file)\n\n# start by getting the current xml document's \"root\" element\ndocument_root = xml_doc.getroot() ![3](assets/3.png)\n\n# let's print it out to see what it looks like\nprint(etree.tostring(document_root)) ![4](assets/4.png)\n\n# confirm that `document_root` is a well-formed XML element\nif etree.iselement(document_root):\n\n    # create our output file, naming it \"xml_\"+filename+\".csv\n    output_file = open(\"xml_\"+filename+\".csv\",\"w\")\n\n    # use the `csv` library's \"writer\" recipe to easily write rows of data\n    # to `output_file`, instead of reading data *from* it\n    output_writer = csv.writer(output_file)\n\n    # grab the first element of our xml document (using `document_root[0]`)\n    # and write its attribute keys as column headers to our output file\n    output_writer.writerow(document_root[0].attrib.keys()) ![5](assets/5.png)\n\n    # now, we need to loop through every element in our XML file\n      for child in document_root: ![6](assets/6.png)\n\n        # now we'll use the `.values()` method to get each element's values\n        # as a list and then use that directly with the `writerow` recipe\n        output_writer.writerow(child.attrib.values())\n\n    # officially close the `.csv` file we just wrote all that data to\n    output_file.close()\n```", "```py\n# an example of reading data from an .xml file with Python, using the \"lxml\"\n# library.\n# first, you'll need to pip install the lxml library:\n# https://pypi.org/project/lxml/\n# the data used here is an instance of\n# http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\n\n# specify the \"chapter\" of the `lxml` library you want to import,\n# in this case, `etree`, which stands for \"ElementTree\"\nfrom lxml import etree\n\n# import the `csv` library, to create our output file\nimport csv\n\n# choose a filename, for simplicity\nfilename = \"BBC News - Science & Environment XML Feed\"\n\n# open our data file in read format, using \"rb\" as the \"mode\"\nxml_source_file = open(filename+\".xml\",\"rb\")\n\n# pass our xml_source_file as an ingredient to the `lxml` library's\n# `etree.parse()` method and store the result in a variable called `xml_doc`\nxml_doc = etree.parse(xml_source_file)\n\n# start by getting the current xml document's \"root\" element\ndocument_root = xml_doc.getroot()\n\n# if the document_root is a well-formed XML element\nif etree.iselement(document_root):\n\n    # create our output file, naming it \"rss_\"+filename+\".csv\"\n    output_file = open(\"rss_\"+filename+\".csv\",\"w\")\n\n    # use the `csv` library's \"writer\" recipe to easily write rows of data\n    # to `output_file`, instead of reading data *from* it\n    output_writer = csv.writer(output_file)\n\n    # document_root[0] is the \"channel\" element\n    main_channel = document_root[0]\n\n    # the `find()` method returns *only* the first instance of the element name\n    article_example = main_channel.find('item') ![1](assets/1.png)\n\n    # create an empty list in which to store our future column headers\n    tag_list = []\n\n    for child in article_example.iterdescendants(): ![2](assets/2.png)\n\n        # add each tag to our would-be header list\n        tag_list.append(child.tag) ![3](assets/3.png)\n\n        # if the current tag has any attributes\n        if child.attrib: ![4](assets/4.png)\n\n            # loop through the attribute keys in the tag\n            for attribute_name in child.attrib.keys(): ![5](assets/5.png)\n\n                # append the attribute name to our `tag_list` column headers\n                tag_list.append(attribute_name)\n\n    # write the contents of `tag_list` to our output file as column headers\n    output_writer.writerow(tag_list) ![6](assets/6.png)\n\n    # now we want to grab *every* <item> element in our file\n    # so we use the `findall()` method instead of `find()`\n    for item in main_channel.findall('item'):\n\n        # empty list for holding our new row's content\n        new_row = []\n\n        # now we'll use our list of tags to get the contents of each element\n        for tag in tag_list:\n\n            # if there is anything in the element with a given tag name\n            if item.findtext(tag):\n\n                # append it to our new row\n                new_row.append(item.findtext(tag))\n\n            # otherwise, make sure it's the \"isPermaLink\" attribute\n            elif tag == \"isPermaLink\":\n\n                # grab its value from the <guid> element\n                # and append it to our row\n                new_row.append(item.find('guid').get(\"isPermaLink\"))\n\n        # write the new row to our output file!\n        output_writer.writerow(new_row)\n\n    # officially close the `.csv` file we just wrote all that data to\n    output_file.close()\n```", "```py\n{\n\"author\": \"Susan E. McGregor\",\n\"book\": {\n    \"bookURL\": \"https://www.oreilly.com/library/view/practical-python-data/\n 9781492091493/\",\n    \"bookAbstract\": \"There are awesome discoveries to be made and valuable\n stories to be told in datasets--and this book will help you uncover\n them.\",\n    \"pubDate\": \"2022-02-01\"\n},\n\"papers\": [{\n    \"paperURL\": \"https://www.usenix.org/conference/usenixsecurity15/\n technical-sessions/presentation/mcgregor\",\n    \"paperTitle\": \"Investigating the computer security practices and needs\n of journalists\",\n    \"pubDate\": \"2015-08-12\"\n},\n    {\n    \"paperURL\": \"https://www.aclweb.org/anthology/W18-5104.pdf\",\n    \"paperTitle\": \"Predictive embeddings for hate speech detection on\n twitter\",\n    \"pubDate\": \"2018-10-31\"\n}\n    ]\n}\n```", "```py\n# a simple example of reading data from a .json file with Python,\n# using the built-in \"json\" library. The data used here is an instance of\n# https://api.stlouisfed.org/fred/series/observations?series_id=U6RATE& \\\n# file_type=json&api_key=YOUR_API_KEY_HERE\n\n# import the `json` library, since that's our source file format\nimport json\n\n# import the `csv` library, to create our output file\nimport csv\n\n# choose a filename\nfilename = \"U6_FRED_data\"\n\n# open the file in read format (\"r\") as usual\njson_source_file = open(filename+\".json\",\"r\")\n\n# pass the `json_source_file` as an ingredient to the json library's `load()`\n# method and store the result in a variable called `json_data`\njson_data = json.load(json_source_file)\n\n# create our output file, naming it \"json_\"+filename\noutput_file = open(\"json_\"+filename+\".csv\",\"w\")\n\n# use the `csv` library's \"writer\" recipe to easily write rows of data\n# to `output_file`, instead of reading data *from* it\noutput_writer = csv.writer(output_file)\n\n# grab the first element (at position \"0\"), and use its keys as the column headers\noutput_writer.writerow(list(json_data[\"observations\"][0].keys())) ![1](assets/1.png)\n\nfor obj in json_data[\"observations\"]: ![2](assets/2.png)\n\n    # we'll create an empty list where we'll put the actual values of each object\n    obj_values = []\n\n    # for every `key` (which will become a column), in each object\n    for key, value in obj.items(): ![3](assets/3.png)\n\n        # let's print what's in here, just to see how the code sees it\n        print(key,value)\n\n        # add the values to our list\n        obj_values.append(value)\n\n    # now we've got the whole row, write the data to our output file\n    output_writer.writerow(obj_values)\n\n# officially close the `.csv` file we just wrote all that data to\noutput_file.close()\n```", "```py\nsudo apt install poppler-utils\n```", "```py\npip install pdf2image\n```", "```py\nsudo apt-get install tesseract-ocr\n```", "```py\npip install pytesseract\n```", "```py\npip install opencv-python\n```", "```py\n# a basic example of reading data from a .pdf file with Python,\n# using `pdf2image` to convert it to images, and then using the\n# openCV and `tesseract` libraries to extract the text\n\n# the source data was downloaded from:\n# https://files.stlouisfed.org/files/htdocs/publications/page1-econ/2020/12/01/ \\\n# unemployment-insurance-a-tried-and-true-safety-net_SE.pdf\n\n# the built-in `operating system` or `os` Python library will let us create\n# a new folder in which to store our converted images and output text\nimport os\n\n# we'll import the `convert_from_path` \"chapter\" of the `pdf2image` library\nfrom pdf2image import convert_from_path\n\n# the built-in `glob`library offers a handy way to loop through all the files\n# in a folder that have a certain file extension, for example\nimport glob\n\n# `cv2` is the actual library name for `openCV`\nimport cv2\n\n# and of course, we need our Python library for interfacing\n# with the tesseract OCR process\nimport pytesseract\n\n# we'll use the pdf name to name both our generated images and text files\npdf_name = \"SafetyNet\"\n\n# our source pdf is in the same folder as our Python script\npdf_source_file = pdf_name+\".pdf\"\n\n# as long as a folder with the same name as the pdf does not already exist\nif os.path.isdir(pdf_name) == False:\n\n    # create a new folder with that name\n    target_folder = os.mkdir(pdf_name)\n\n# store all the pages of the PDF in a variable\npages = convert_from_path(pdf_source_file, 300) ![1](assets/1.png)\n\n# loop through all the converted pages, enumerating them so that the page\n# number can be used to label the resulting images\nfor page_num, page in enumerate(pages):\n\n    # create unique filenames for each page image, combining the\n    # folder name and the page number\n    filename = os.path.join(pdf_name,\"p\"+str(page_num)+\".png\") ![2](assets/2.png)\n\n    # save the image of the page in system\n    page.save(filename, 'PNG')\n\n# next, go through all the files in the folder that end in `.png`\nfor img_file in glob.glob(os.path.join(pdf_name, '*.png')): ![3](assets/3.png)\n\n    # replace the slash in the image's filename with a dot\n    temp_name = img_file.replace(\"/\",\".\")\n\n    # pull the unique page name (e.g. `p2`) from the `temp_name`\n    text_filename = temp_name.split(\".\")[1] ![4](assets/4.png)\n\n    # now! create a new, writable file, also in our target folder, that\n    # has the same name as the image, but is a `.txt` file\n    output_file = open(os.path.join(pdf_name,text_filename+\".txt\"), \"w\")\n\n    # use the `cv2` library to interpret our image\n    img = cv2.imread(img_file)\n\n    # create a new variable to hold the results of using pytesseract's\n    # `image_to_string()` function, which will do just that\n    converted_text = pytesseract.image_to_string(img)\n\n    # write our extracted text to our output file\n    output_file.write(converted_text)\n\n    # close the output file\n    output_file.close()\n```", "```py\nsudo apt install default-jre\n```"]