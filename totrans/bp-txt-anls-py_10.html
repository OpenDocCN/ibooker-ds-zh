<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Exploring Semantic Relationships with Word Embeddings"><div class="chapter" id="ch-embeddings">
<h1><span class="label">Chapter 10. </span>Exploring Semantic Relationships with Word Embeddings</h1>

<p>The concept of similarity is fundamental to all machine learning tasks. In <a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a>, we explained how to compute text similarity based on the <a contenteditable="false" data-type="indexterm" data-primary="bag-of-words models" id="ch10_term1"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="bag-of-words models for" id="ch10_term2"/>bag-of-words model. Given two TF-IDF vectors for documents, their cosine similarity can be easily computed, and we can use this information to search, cluster, or classify similar documents. </p>

<p>However, the concept of similarity in the bag-of-words model is completely based on the number of common words in two documents. If documents do not share any tokens, the <a contenteditable="false" data-type="indexterm" data-primary="dot product calculation" id="idm45634182167544"/>dot product of the document vectors and hence the cosine similarity will be zero. Consider the following two comments about a new movie, which could be found on a social platform:</p>

<blockquote>
  <p>“What a wonderful movie.”</p>
  <p>“The film is great.”</p>
</blockquote>

<p>Obviously, the <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="about" id="idm45634182164376"/>comments have a similar meaning even though they use completely different words. In this chapter, we will introduce word embeddings as a means to capture the semantics of words and use them to explore semantic similarities within a corpus. </p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634182162392">
<h1>What You’ll Learn and What We’ll Build</h1>

<p>For our <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="market research on cars" id="idm45634182160856"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for semantic analysis" data-secondary-sortas="semantic analysis" id="idm45634182159448"/>use case we assume that we are market researchers and want to use texts about cars to better understand some relationships in the car market. Specifically, we want to explore similarities among car brands and models. For example, which models of brand A are most similar to a given model of brand B? </p>

<p>Our corpus <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="use case and Reddit dataset for" id="idm45634182124200"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reddit Self-Posts" id="idm45634182122936"/><a contenteditable="false" data-type="indexterm" data-primary="Reddit Self-Posts " id="idm45634182121528"/>consists of the 20 subreddits in the autos category of the Reddit Self-Posts dataset, which was already used in <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>. Each of these subreddits contains 1,000 posts on cars and motorcycles with brands such as Mercedes, Toyota, Ford, and Harley-Davidson. Since those posts are questions, answers, and comments written <span class="keep-together">by users,</span> we will actually get an idea of what these users implicitly <em>consider</em> as being <span class="keep-together">similar</span>.</p>

<p>We will use <a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for semantic embeddings" data-secondary-sortas="semantic embeddings" id="idm45634182116872"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="Gensim for" id="idm45634182115192"/>the <a href="https://oreil.ly/HaYkR">Gensim library</a> again, which was introduced in <a data-type="xref" href="ch08.xhtml#ch-topicmodels">Chapter 8</a>. It provides a nice API to train different types of embeddings and to use those models for semantic reasoning.</p>

<p>After studying this chapter, you will be able to use word embeddings for semantic analysis. You will know how to use pretrained embeddings, how to train your own embeddings, how to compare different models, and how to visualize them. You can find the source code for this chapter along with some of the images in our <a href="https://oreil.ly/W1ztU">GitHub repository</a>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="The Case for Semantic Embeddings"><div class="sect1" id="idm45634182110312">
<h1>The Case for Semantic Embeddings</h1>

<p>In the previous chapters, we used the <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="limitations of" id="idm45634182108792"/><a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="vectorization with" id="idm45634182107448"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with TF-IDF weighting" data-secondary-sortas="TF-IDF weighting" id="idm45634182106168"/>TF-IDF vectorization for our models. It is easy to compute, but it has some severe disadvantages:</p>

<ul>
  <li>The document vectors have a very high dimensionality that is defined by the size of the vocabulary. Thus, the vectors are extremely sparse; i.e., most entries are zero. </li>
  <li>It does not work well for short texts like Twitter messages, service comments, and similar content because the probability for common words is low for short texts. </li>
  <li>Advanced applications such as sentiment analysis, question answering, or machine translation require capturing the real meaning of the words to work correctly. </li>
</ul>

<p>Still, the bag-of-words model works surprisingly well for tasks such as classification or topic modeling, but only if the texts are sufficiently long and <a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="data for" id="idm45634182101480"/>enough training data is available. Remember that similarity in the bag-of-words model is solely based on the existence of significant common words.</p>

<p>An <em>embedding</em>, in contrast, is a dense numerical vector representation of an object that captures some kind of <em>semantic</em> similarity. When <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term1_2" id="idm45634182098200"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term2" id="idm45634182096792"/>we talk of embeddings in the context of text analysis, we have to <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="vs. document embedding" id="idm45634182095288"/>distinguish word and document embeddings. A <em>word embedding</em> is a vector representation for a single word, <a contenteditable="false" data-type="indexterm" data-primary="document embedding" id="idm45634182093192"/>while a <em>document embedding</em> is a vector representing a document. By <em>document</em> we mean any sequence of words, be it a short phrase, a sentence, a paragraph, or even a long article. In this chapter, we will focus on dense vector representations for words.</p>

<section data-type="sect2" data-pdf-bookmark="Word Embeddings"><div class="sect2" id="idm45634182090840">
<h2>Word Embeddings</h2>

<p>The target <a contenteditable="false" data-type="indexterm" data-primary="embedding" id="idm45634182089336"/>of an <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="basic algorithm for" id="idm45634182088072"/><a contenteditable="false" data-type="indexterm" data-primary="vector dimensions of word embeddings" id="idm45634182086696"/>embedding algorithm can be defined as follows: given a dimensionality <math alttext="d">
  <mi>d</mi>
</math>, find vector representations for words such that words with similar meanings have similar vectors. The dimensionality <math alttext="d">
  <mi>d</mi>
</math> is a hyperparameter of any word embedding algorithm. It is typically set to a value between 50 and 300.</p>

<p>The dimensions themselves have no predefined or human-understandable meaning. Instead, the model learns latent relations among the words from the text. <a data-type="xref" href="#fig-embeddings">Figure 10-1</a> (left) illustrates the concept. We have five-dimensional vectors for each word. Each of these dimensions represents some relation among the words so that words similar in that aspect have similar values in this dimension. Dimension names shown are possible interpretations of those values. </p>

<figure><div id="fig-embeddings" class="figure">
<img src="Images/btap_1001.jpg" width="1235" height="625"/>
<h6><span class="label">Figure 10-1. </span>Dense vector representations captioning semantic similarity of words (left) can be used to answer analogy questions (right). We gave the vector dimensions hypothetical names like “Royalty” to show possible interpretations.<sup><a data-type="noteref" id="idm45634182078904-marker" href="ch10.xhtml#idm45634182078904">1</a></sup>
</h6>
</div></figure>

<p>The basic idea for training is that <a contenteditable="false" data-type="indexterm" data-primary="distributional hypothesis" id="idm45634182076456"/>words occurring in similar contexts have similar meanings. This is called the <em>distributional hypothesis</em>. Take, for example, the following sentences describing <em>tesgüino</em>:<sup><a data-type="noteref" id="idm45634182074280-marker" href="ch10.xhtml#idm45634182074280">2</a></sup> </p>

<ul class="less-space-list">
<li>A bottle of ___ is on the table.</li>
<li>Everybody likes ___ .</li>
<li>Don’t have ___ before you drive.</li>
<li>We make ___ out of corn. </li>
</ul>

<p>Even without knowing the word <em>tesgüino</em>, you get a pretty good understanding of its meaning by analyzing typical contexts. You could also identify semantically similar words because you know it’s an alcoholic beverage.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Analogy Reasoning with Word Embeddings"><div class="sect2" id="idm45634182069528">
<h2>Analogy Reasoning with Word Embeddings</h2>

<p>What’s really amazing is that <a contenteditable="false" data-type="indexterm" data-primary="word vectors" data-secondary="in semantic analysis" data-secondary-sortas="semantic analysis" id="idm45634182067832"/>word vectors built this way <a contenteditable="false" data-type="indexterm" data-primary="analogy reasoning" id="idm45634182066008"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="analogy reasoning with" id="idm45634182064904"/>allow us to detect analogies like “queen is to king like woman is to man” with <a contenteditable="false" data-type="indexterm" data-primary="vector algebra" id="idm45634182063336"/>vector algebra (<a data-type="xref" href="#fig-embeddings">Figure 10-1</a>, right). Let <math alttext="v left-parenthesis w right-parenthesis">
  <mrow>
    <mi>v</mi>
    <mo>(</mo>
    <mi>w</mi>
    <mo>)</mo>
  </mrow>
</math> be the word embedding for a word <math alttext="w">
  <mi>w</mi>
</math>. Then the analogy can be expressed mathematically like this: </p>

<div data-type="equation">
  <p><math alttext="v left-parenthesis q u e e n right-parenthesis minus v left-parenthesis k i n g right-parenthesis almost-equals v left-parenthesis w o m a n right-parenthesis minus v left-parenthesis m a n right-parenthesis" display="block">
  <mrow>
    <mi>v</mi>
    <mo>(</mo>
    <mi>q</mi>
    <mi>u</mi>
    <mi>e</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mo>)</mo>
    <mo>-</mo>
    <mi>v</mi>
    <mo>(</mo>
    <mi>k</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mo>)</mo>
    <mo>≈</mo>
    <mi>v</mi>
    <mo>(</mo>
    <mi>w</mi>
    <mi>o</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mo>)</mo>
    <mo>-</mo>
    <mi>v</mi>
    <mo>(</mo>
    <mi>m</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mo>)</mo>
  </mrow>
</math></p>
</div>

<p>If this approximate equation holds, we can reformulate the analogy as a question: What is to <em>king</em> like “woman” is to “man”? Or mathematically:<sup><a data-type="noteref" id="idm45634182038552-marker" href="ch10.xhtml#idm45634182038552">3</a></sup></p>


<div data-type="equation">
  <p><math alttext="v left-parenthesis w o m a n right-parenthesis plus left-bracket v left-parenthesis k i n g right-parenthesis minus v left-parenthesis m a n right-parenthesis right-bracket almost-equals question-mark" display="block">
  <mrow>
    <mi>v</mi>
    <mrow>
      <mo>(</mo>
      <mi>w</mi>
      <mi>o</mi>
      <mi>m</mi>
      <mi>a</mi>
      <mi>n</mi>
      <mo>)</mo>
    </mrow>
    <mo>+</mo>
    <mfenced separators="" open="[" close="]"><mi>v</mi> <mo>(</mo> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>)</mo> <mo>-</mo> <mi>v</mi> <mo>(</mo> <mi>m</mi> <mi>a</mi> <mi>n</mi> <mo>)</mo></mfenced>
    <mo>≈</mo>
    <mo>?</mo>
  </mrow>
</math></p>
</div>


<p>This allows some kind of fuzzy reasoning to answer analogy questions like this one: “Given that Paris is the capital of France, what is the capital of Germany?” Or in a market research scenario as the one we will explore: “Given that F-150 is a pickup truck from Ford, what is the similar model from Toyota?”</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Types of Embeddings"><div class="sect2" id="idm45634182020744">
<h2>Types of Embeddings</h2>

<p>Several algorithms have been developed to <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="training options for" id="ch10_term4"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="for word embeddings" data-secondary-sortas="word embeddings" id="ch10_term1_2"/>train word embeddings. <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="Gensim for" id="idm45634182015288"/><a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for semantic embeddings" data-secondary-sortas="semantic embeddings" id="idm45634182013912"/>Gensim allows you to train <a contenteditable="false" data-type="indexterm" data-primary="Word2Vec embedding" id="ch10_term3"/>Word2Vec and <a contenteditable="false" data-type="indexterm" data-primary="FastText" id="idm45634182010632"/>FastText embeddings. <a contenteditable="false" data-type="indexterm" data-primary="GloVe (global vectors) embedding" id="idm45634182009368"/>GloVe embeddings can be used for similarity queries but not trained with Gensim. We introduce the basic ideas of these algorithms and briefly explain the more advanced but also more complex contextualized embedding methods. You will find the references to the original papers and further explanations at the end of this chapter. </p>

<section data-type="sect3" data-pdf-bookmark="Word2Vec"><div class="sect3" id="idm45634182007784">
<h3>Word2Vec</h3>

<p>Even though there have been approaches for word embeddings before, the work of <a contenteditable="false" data-type="indexterm" data-primary="Mikolov, Tomáš" id="idm45634182006056"/>Tomáš Mikolov at Google (Mikolov et al., 2013) marks a milestone because it dramatically outperformed previous approaches, especially on analogy tasks such as the ones just explained.
There exist <a contenteditable="false" data-type="indexterm" data-primary="CBOW (continuous bag-of-words) model" id="idm45634182004616"/><a contenteditable="false" data-type="indexterm" data-primary="skip-gram model" id="idm45634182003544"/>two variants of Word2Vec, the <em>continuous bag-of-words model</em> (CBOW) and the <em>skip-gram model</em> (see <a data-type="xref" href="#fig-word2vec">Figure 10-2</a>).</p><figure><div id="fig-word2vec" class="figure">
<img src="Images/btap_1002.jpg" width="1306" height="538"/>
<h6><span class="label">Figure 10-2. </span>Continuous bag-of words (left) versus skip-gram model (right).</h6></div></figure>

<p>Both algorithms use a sliding window over the text, defined by a target word <math alttext="w Subscript t">
  <msub><mi>w</mi> <mi>t</mi> </msub>
</math> and the size of the context window <math alttext="c">
  <mi>c</mi>
</math>. In the example, <math alttext="c equals 2">
  <mrow>
    <mi>c</mi>
    <mo>=</mo>
    <mn>2</mn>
  </mrow>
</math>, i.e., the training samples consist of the five words <math alttext="w Subscript t minus 2 Baseline comma ellipsis comma w Subscript t plus 2 Baseline">
  <mrow>
    <msub><mi>w</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <msub><mi>w</mi> <mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow> </msub>
  </mrow>
</math>. One such training sample is printed in bold: <strong>... is trying <em>things</em> to see ...</strong>. In the CBOW architecture (left), the model is trained to predict the target words from their context words. Here, a training sample consists of the sum or average of the one-hot encoded vectors of the context words and the target word as the label. In contrast, the skip-gram model (right) is trained to predict the context words given the target word. In this case, each target word generates a separate training sample for each context word; there is no vector averaging. Thus, skip-gram trains slower (much slower for large window sizes!) but often gives better results for infrequent words.</p>

<p>Both embedding algorithms <a contenteditable="false" data-type="indexterm" data-primary="neural networks" id="idm45634181984152"/>use a simple single-layer neural network and some tricks for fast and scalable training. The learned embeddings are actually defined by the weight matrix of the hidden layer. Thus, if you want to learn 100-dimensional vector representations, the hidden layer has to consist of 100 neurons. The <a contenteditable="false" data-type="indexterm" data-primary="one-hot vectorizer" id="idm45634181982584"/>input and output words are represented by one-hot vectors. The dimensionality of the embeddings and size of the context window <math alttext="c">
  <mi>c</mi>
</math> are <a contenteditable="false" data-type="indexterm" data-primary="hyperparameters" id="idm45634181980008"/>hyperparameters in all of the embedding methods presented here. We will explore their impact on the embeddings later in this chapter.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="GloVe"><div class="sect3" id="idm45634181978472">
  <h3>GloVe</h3>

<p>The <a href="https://oreil.ly/7hIGW">GloVe (global vectors) approach</a>, <a contenteditable="false" data-type="indexterm" data-primary="GloVe (global vectors) embedding" id="ch10_term7"/>developed in 2014 by <a contenteditable="false" data-type="indexterm" data-primary="Stanford University" id="idm45634181974728"/>Stanford’s NLP group, uses a global co-occurrence matrix to compute word vectors instead of a prediction task (Pennington et al., 2014). A co-occurrence matrix for a vocabulary of size <math alttext="upper V">
  <mi>V</mi>
</math> has the dimensionality <math alttext="upper V times upper V">
  <mrow>
    <mi>V</mi>
    <mo>×</mo>
    <mi>V</mi>
  </mrow>
</math>. Each cell <math alttext="left-parenthesis i comma j right-parenthesis">
  <mrow>
    <mo>(</mo>
    <mi>i</mi>
    <mo>,</mo>
    <mi>j</mi>
    <mo>)</mo>
  </mrow>
</math> in the matrix contains the number of co-occurrences of the words <math alttext="w Subscript i">
  <msub><mi>w</mi> <mi>i</mi> </msub>
</math> and <math alttext="w Subscript j">
  <msub><mi>w</mi> <mi>j</mi> </msub>
</math> based again on a fixed context window size. The embeddings are derived using a matrix factorization technique similar to those used for topic modeling or dimensionality reduction.</p>

<p>The model is called <em>global</em> because the co-occurrence matrix captures global corpus statistics in contrast to Word2Vec, which uses only the local context window for its prediction task. GloVe does not generally perform better than Word2Vec but produces similarly good results with some differences depending on the training data and the task (see Levy et al., 2014, for a discussion).</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="FastText"><div class="sect3" id="idm45634181960136">
<h3>FastText</h3>

<p>The third <a contenteditable="false" data-type="indexterm" data-primary="FastText" id="idm45634181958808"/>model we introduce was developed again by a team with <a contenteditable="false" data-type="indexterm" data-primary="Mikolov, Tomáš" id="idm45634181957096"/>Tomáš Mikolov, this time at Facebook (Joulin et al., 2017). The main <a contenteditable="false" data-type="indexterm" data-primary="out-of-vocabulary words" id="idm45634181955864"/>motivation was to handle out-of-vocabulary words. Both Word2Vec and GloVe produce word embeddings only for words contained in the training corpus. <a href="https://fasttext.cc">FastText</a>, in contrast, uses subword information in the form of character n-grams to derive vector representations. The character trigrams for <em>fasttext</em> are, for example, <em>fas</em>, <em>ast</em>, <em>stt</em>, <em>tte</em>, <em>tex</em>, and <em>ext</em>. The lengths of n-grams used (minimum and maximum) are hyperparameters of the model. </p>

<p>Any word vector is built from the embeddings of its <a contenteditable="false" data-type="indexterm" data-primary="character n-grams" id="idm45634181950264"/><a contenteditable="false" data-type="indexterm" data-primary="n-grams" id="idm45634181949160"/>character n-grams. And that does work even for words previously unseen by the model because most of the character n-grams have embeddings. For example, the vector for <em>fasttext</em> will be similar to <em>fast</em> and <em>text</em> because of the common n-grams. Thus, FastText is pretty good at finding embeddings for misspelled words that are usually out-of-vocabulary.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Deep contextualized embeddings"><div class="sect3" id="idm45634181946024">
<h3>Deep contextualized embeddings</h3>

<p>The semantic meaning of a word often depends on its <a contenteditable="false" data-type="indexterm" data-primary="contextualized embeddings" id="idm45634181944664"/><a contenteditable="false" data-type="indexterm" data-primary="deep contextualized embeddings" id="idm45634181943592"/>context. Think of different meanings of the word <em>right</em> in “I am right” and “Please turn right.”<sup><a data-type="noteref" id="idm45634181941800-marker" href="ch10.xhtml#idm45634181941800">4</a></sup> All three models  (Word2Vec, GloVe, and FastText) have just one vector representation per word; they cannot distinguish between context-dependent semantics. </p>

<p>Contextualized embeddings like <a contenteditable="false" data-type="indexterm" data-primary="ELMo (Embedding from Language Models)" id="idm45634181939048"/><em>Embedding from Language Models</em> (ELMo) take the context, i.e., the preceding and following  words, into account (Peters et al., 2018). There is not one word vector stored for each word that can simply be looked up. Instead, ELMo passes the whole sentence through a <a contenteditable="false" data-type="indexterm" data-primary="LSTM (long short-term memory neural network)" id="idm45634181937256"/><a contenteditable="false" data-type="indexterm" data-primary="neural networks" id="idm45634181936088"/>multilayer bidirectional long short-term memory neural network (LSTM) and assembles the vectors for each word from weights of the internal layers. Recent models such as <a contenteditable="false" data-type="indexterm" data-primary="BERT (Bidirectional Encoder Representations from Transformers) model" id="idm45634181934664"/>BERT and its successors improve the approach by using attention transformers instead of bidirectional LSTMs. The main benefit of all these models is <a contenteditable="false" data-type="indexterm" data-primary="transfer learning" id="idm45634181933224"/>transfer learning: the ability to use a pretrained language model and fine-tune it for specific downstream tasks such as classification or question answering. We will cover this concept in more detail in <a data-type="xref" href="ch11.xhtml#ch-sentiment">Chapter 11</a>.</p>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Using Similarity Queries on Pretrained Models"><div class="sect1" id="idm45634181930680">
<h1>Blueprint: Using Similarity Queries on <span class="keep-together">Pretrained Models</span></h1>

<p>After all this theory, let’s start some practice. For our first examples, we use <a contenteditable="false" data-type="indexterm" data-primary="pretrained models" data-secondary="loading process for" id="idm45634181928376"/><a contenteditable="false" data-type="indexterm" data-primary="pretrained models" data-secondary="for similarity queries" data-secondary-sortas="similarity queries" id="ch10_term8"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="with similarity queries on pretrained models" data-secondary-sortas="similarity queries on pretrained models" id="ch10_term9"/><a contenteditable="false" data-type="indexterm" data-primary="similarity queries for semantic analysis" id="ch10_term10"/>pretrained embeddings. These have the advantage that somebody else spent the training effort, usually on a large corpus like Wikipedia or news articles. In our blueprint, we will check out available models, load one of them, and do some reasoning with word vectors.</p>

<section data-type="sect2" data-pdf-bookmark="Loading a Pretrained Model"><div class="sect2" id="idm45634181921144">
<h2>Loading a Pretrained Model</h2>

<p>Several models are publicly available for download.<sup><a data-type="noteref" id="idm45634181919512-marker" href="ch10.xhtml#idm45634181919512">5</a></sup> We will describe later how to load custom models, but here we will use Gensim’s convenient downloader API instead. </p>

<p>Per the default, Gensim stores models under <code>~/gensim-data</code>. If you want to change this to a custom path, you can set the environment variable <code>GENSIM_DATA_DIR</code> before importing the downloader API. We will store all models in the local directory <code>models</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">os</code>
<code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s1">'GENSIM_DATA_DIR'</code><code class="p">]</code> <code class="o">=</code> <code class="s1">'./models'</code>
</pre>

<p>Now let’s take a look at the available models. The following lines transform the dictionary returned by <code>api.info()['models']</code> into a <code>DataFrame</code> to get a nicely formatted list and show the first five of a total of 13 entries:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">gensim.downloader</code> <code class="kn">as</code> <code class="nn">api</code>

<code class="n">info_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_dict</code><code class="p">(</code><code class="n">api</code><code class="o">.</code><code class="n">info</code><code class="p">()[</code><code class="s1">'models'</code><code class="p">],</code> <code class="n">orient</code><code class="o">=</code><code class="s1">'index'</code><code class="p">)</code>
<code class="n">info_df</code><code class="p">[[</code><code class="s1">'file_size'</code><code class="p">,</code> <code class="s1">'base_dataset'</code><code class="p">,</code> <code class="s1">'parameters'</code><code class="p">]]</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
</pre>



<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th/>
      <th>file_size</th>
      <th>base_dataset</th>
      <th>parameters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fasttext-wiki-news-subwords-300</th>
      <td>1005007116</td>
      <td>Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)</td>
      <td>{'dimension’: 300}</td>
    </tr>
    <tr>
      <th>conceptnet-numberbatch-17-06-300</th>
      <td>1225497562</td>
      <td>ConceptNet, word2vec, GloVe, and OpenSubtitles 2016</td>
      <td>{'dimension’: 300}</td>
    </tr>
    <tr>
      <th>word2vec-ruscorpora-300</th>
      <td>208427381</td>
      <td>Russian National Corpus (about 250M words)</td>
      <td>{'dimension’: 300, ‘window_size’: 10}</td>
    </tr>
    <tr class="pagebreak-before">
      <th>word2vec-google-news-300</th>
      <td>1743563840</td>
      <td>Google News (about 100 billion words)</td>
      <td>{'dimension’: 300}</td>
    </tr>
    <tr>
      <th>glove-wiki-gigaword-50</th>
      <td>69182535</td>
      <td>Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)</td>
      <td>{'dimension’: 50}</td>
    </tr>
  </tbody>
</table>


<p>We will <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term3" id="idm45634181822120"/>use the <em>glove-wiki-gigaword-50</em> model. This model with 50-dimensional word vectors is small in size but still quite comprehensive and fully sufficient for our purposes. It was trained on roughly 6 billion lowercased tokens. <code>api.load</code> downloads the model if required and then loads it into <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term4" id="idm45634181819512"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term7" id="idm45634181818136"/>memory:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code> <code class="o">=</code> <code class="n">api</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"glove-wiki-gigaword-50"</code><code class="p">)</code>
</pre>

<p>The file we downloaded actually does not contain a full GloVe model but just the plain word vectors. As the internal states of the model are not included, such reduced models cannot be trained further.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Similarity Queries"><div class="sect2" id="idm45634181920520">
<h2>Similarity Queries</h2>

<p>Given a <a contenteditable="false" data-type="indexterm" data-primary="pretrained models" data-secondary="training of" id="ch10_term11"/><a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="of pretrained models" data-secondary-sortas="pretrained models" id="ch10_term12"/>model, the vector for a single word like <em>king</em> can be accessed simply via the property <code>model.wv['king']</code> or even more simply by the shortcut <code>model['king']</code>. Let’s take a look at the first 10 components of the 50-dimensional vectors for <em>king</em> and <em>queen</em>.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">v_king</code> <code class="o">=</code> <code class="n">model</code><code class="p">[</code><code class="s1">'king'</code><code class="p">]</code>
<code class="n">v_queen</code> <code class="o">=</code> <code class="n">model</code><code class="p">[</code><code class="s1">'queen'</code><code class="p">]</code>

<code class="k">print</code><code class="p">(</code><code class="s2">"Vector size:"</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">vector_size</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"v_king  ="</code><code class="p">,</code> <code class="n">v_king</code><code class="p">[:</code><code class="mi">10</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"v_queen ="</code><code class="p">,</code> <code class="n">v_queen</code><code class="p">[:</code><code class="mi">10</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"similarity:"</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">similarity</code><code class="p">(</code><code class="s1">'king'</code><code class="p">,</code> <code class="s1">'queen'</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Vector size: 50
v_king  = [ 0.5   0.69 -0.6  -0.02  0.6  -0.13 -0.09  0.47 -0.62 -0.31]
v_queen = [ 0.38  1.82 -1.26 -0.1   0.36  0.6  -0.18  0.84 -0.06 -0.76]
similarity: 0.7839043
</pre>

<p>As expected, the values are similar in many dimensions, resulting in a high similarity score of over 0.78. So <em>queen</em> is quite similar to <em>king</em>, but is it the most similar word? Well, let’s <a contenteditable="false" data-type="indexterm" data-primary="most_similar (Gensim)" id="idm45634181706344"/>check the three words most similar to <em>king</em> with a call to the respective function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="s1">'king'</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[('prince', 0.824), ('queen', 0.784), ('ii', 0.775)]
</pre>

<p>In fact, the male <em>prince</em> is more similar than <em>queen</em>, but <em>queen</em> is second in the list, followed by the roman numeral II, because many kings have been named “the <span class="keep-together">second</span>.”</p>

<p>Similarity scores on word vectors are generally computed by <a contenteditable="false" data-type="indexterm" data-primary="cosine similarity" id="ch10_term13"/>cosine similarity, which was introduced in <a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a>. Gensim <a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for semantic embeddings" data-secondary-sortas="semantic embeddings" id="ch10_term14"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="Gensim for" id="ch10_term15"/>provides several variants of similarity functions. For <a contenteditable="false" data-type="indexterm" data-primary="cosine_similarity function" id="idm45634181686872"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="cosine_similarity function of" id="idm45634181685800"/>example, the <code>cosine_similarities</code> method computes the similarity between a word vector and an array of other word vectors. Let’s compare <em>king</em> to some more words:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">v_lion</code> <code class="o">=</code> <code class="n">model</code><code class="p">[</code><code class="s1">'lion'</code><code class="p">]</code>
<code class="n">v_nano</code> <code class="o">=</code> <code class="n">model</code><code class="p">[</code><code class="s1">'nanotechnology'</code><code class="p">]</code>

<code class="n">model</code><code class="o">.</code><code class="n">cosine_similarities</code><code class="p">(</code><code class="n">v_king</code><code class="p">,</code> <code class="p">[</code><code class="n">v_queen</code><code class="p">,</code> <code class="n">v_lion</code><code class="p">,</code> <code class="n">v_nano</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
array([ 0.784,  0.478, -0.255], dtype=float32)
</pre>

<p>Based on the <a contenteditable="false" data-type="indexterm" data-primary="Wikipedia articles" id="ch10_term17"/><a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="data for" id="ch10_term19"/>training data for the model (Wikipedia and Gigaword), the model assumes the word <em>king</em> to be similar to <em>queen</em>, still a little similar to <em>lion</em>, but not at all similar to <em>nanotechnology</em>. Note, that in contrast to nonnegative TF-IDF vectors, word embeddings can also be negative in some dimensions. Thus, the similarity values range from <math alttext="plus 1">
  <mrow>
    <mo>+</mo>
    <mn>1</mn>
  </mrow>
</math> to <math alttext="negative 1">
  <mrow>
    <mo>-</mo>
    <mn>1</mn>
  </mrow>
</math>. </p>

<p>The <a contenteditable="false" data-type="indexterm" data-primary="most_similar (Gensim)" id="idm45634181594968"/>function <code>most_similar()</code> used earlier allows also two parameters, <code>positive</code> and <code>negative</code>, each a list of vectors. If <math alttext="p o s i t i v e equals left-bracket p o s 1 comma ellipsis comma p o s Subscript n Baseline right-bracket">
  <mrow>
    <mi>p</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>v</mi>
    <mi>e</mi>
    <mo>=</mo>
    <mo>[</mo>
    <mi>p</mi>
    <mi>o</mi>
    <msub><mi>s</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <mi>p</mi>
    <mi>o</mi>
    <msub><mi>s</mi> <mi>n</mi> </msub>
    <mo>]</mo>
  </mrow>
</math> and <math alttext="n e g a t i v e equals left-bracket n e g 1 comma ellipsis comma n e g Subscript m Baseline right-bracket">
  <mrow>
    <mi>n</mi>
    <mi>e</mi>
    <mi>g</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>v</mi>
    <mi>e</mi>
    <mo>=</mo>
    <mo>[</mo>
    <mi>n</mi>
    <mi>e</mi>
    <msub><mi>g</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <mo>⋯</mo>
    <mo>,</mo>
    <mi>n</mi>
    <mi>e</mi>
    <msub><mi>g</mi> <mi>m</mi> </msub>
    <mo>]</mo>
  </mrow>
</math>, then this function finds the word vectors most similar to <math alttext="sigma-summation Underscript i equals 1 Overscript n Endscripts p o s Subscript i minus sigma-summation Underscript j equals 1 Overscript m Endscripts n e g Subscript j">
  <mrow>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup>
    <mi>p</mi>
    <mi>o</mi>
    <msub><mi>s</mi> <mi>i</mi> </msub>
    <mo>-</mo>
    <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi> </msubsup>
    <mi>n</mi>
    <mi>e</mi>
    <msub><mi>g</mi> <mi>j</mi> </msub>
  </mrow>
</math>.</p>

<p>Thus, we can formulate our analogy query about the royals in Gensim this way:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="n">positive</code><code class="o">=</code><code class="p">[</code><code class="s1">'woman'</code><code class="p">,</code> <code class="s1">'king'</code><code class="p">],</code> <code class="n">negative</code><code class="o">=</code><code class="p">[</code><code class="s1">'man'</code><code class="p">],</code> <code class="n">topn</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[('queen', 0.852), ('throne', 0.766), ('prince', 0.759)]
</pre>

<p>And the question for the German capital:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="n">positive</code><code class="o">=</code><code class="p">[</code><code class="s1">'paris'</code><code class="p">,</code> <code class="s1">'germany'</code><code class="p">],</code> <code class="n">negative</code><code class="o">=</code><code class="p">[</code><code class="s1">'france'</code><code class="p">],</code> <code class="n">topn</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[('berlin', 0.920), ('frankfurt', 0.820), ('vienna', 0.818)]
</pre>

<p>We can also leave out the negative list to find the word closest to the sum of <em>france</em> and <em>capital</em>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="n">positive</code><code class="o">=</code><code class="p">[</code><code class="s1">'france'</code><code class="p">,</code> <code class="s1">'capital'</code><code class="p">],</code> <code class="n">topn</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>

<pre data-type="programlisting">
[('paris', 0.784)]
</pre>

<p>It is indeed <code>paris</code>! That’s really amazing and shows the great power of word vectors. However, as always in machine learning, the models are not perfect. They can learn only what’s in the data. Thus, by far not all similarity queries yield such staggering results, as the following example demonstrates:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="n">positive</code><code class="o">=</code><code class="p">[</code><code class="s1">'greece'</code><code class="p">,</code> <code class="s1">'capital'</code><code class="p">],</code> <code class="n">topn</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[('central', 0.797), ('western', 0.757), ('region', 0.750)]
</pre>

<p>Obviously, there has not been enough training data for the model to derive the relation between Athens and Greece.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>

<p>Gensim also offers a <a contenteditable="false" data-type="indexterm" data-primary="most_similar (Gensim)" id="idm45634181376664"/>variant of cosine similarity, <code>most_similar_cosmul</code>. This is supposed to work better for analogy queries than the one shown earlier because it smooths the effects of one large similarity term dominating the equation (Levy et al., 2015). For the previous examples, however, the returned words would be the same, but the similarity scores would <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term13" id="idm45634181374680"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term14" id="idm45634181373304"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term15" id="idm45634181371928"/>be higher.</p></div>

<p>If you train embeddings with redacted texts from Wikipedia and news articles, your model will be able to capture factual relations like capital-country quite well. But what about the market research question comparing products of different brands? Usually you won’t find this information on Wikipedia but rather on up-to-date social platforms where people <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term17" id="idm45634181369640"/>discuss products. If you train embeddings on user comments from a social platform, your model will learn word associations from user discussions. This way, it becomes a representation of what people <em>think</em> about a relationship, independent of whether this is objectively true. This is an interesting side effect you should be aware of. Often you want to capture exactly this application specific bias, and this is what we are going to do next. But be aware that every training corpus contains some bias, which may also lead to unwanted side effects (see <a data-type="xref" data-xrefstyle="select:nopage" href="#man_computer">“Man Is to Computer Programmer as Woman Is to Homemaker”</a>).</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="man_computer">
<h5>Man Is to Computer Programmer as Woman Is to Homemaker</h5>

<p>Most state-of-the-art approaches for NLP tasks ranging from classification to machine translation use semantic embeddings for better results. Thus, the quality of the embeddings has a direct impact on the quality of the final model. Unfortunately, <a contenteditable="false" data-type="indexterm" data-primary="bias in training data" id="idm45634181363848"/><a contenteditable="false" data-type="indexterm" data-primary="debiasing training data" id="idm45634181362744"/>machine learning algorithms have the tendency to amplify biases present in the training data. This is also true for word embeddings. </p>

<p>Bolukbasi et al., showed that “even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent.” This is problematic, <span class="keep-together">as a</span> common approach is to use pretrained word embeddings for downstream <span class="keep-together">tasks such</span> as classification. Thus, debiasing training data is a hot topic in <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term8" id="idm45634181359160"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term9" id="idm45634181341432"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term10" id="idm45634181340056"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term11" id="idm45634181338680"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term12" id="idm45634181337304"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term19" id="idm45634181335928"/>research <span class="keep-together">nowadays</span>.</p></div></aside>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Blueprints for Training and Evaluating Your Own Embeddings"><div class="sect1" id="idm45634181812568">
<h1>Blueprints for Training and Evaluating Your Own Embeddings</h1>

<p>In this section, we will train and evaluate domain-specific embeddings <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="market research on cars" id="idm45634181332728"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reddit Self-Posts" id="idm45634181331352"/><a contenteditable="false" data-type="indexterm" data-primary="Reddit Self-Posts " id="idm45634181329976"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="use case and Reddit dataset for" id="idm45634181328872"/>on 20,000 user posts on autos from the Reddit Selfposts dataset. Before we start training, we have to consider the options for data preparation as they always have a significant impact on the usefulness of a model for a specific task.</p>

<section data-type="sect2" class="pagebreak-after" data-pdf-bookmark="Data Preparation"><div class="sect2" id="idm45634181327048">
<h2>Data Preparation</h2>

<p>Gensim requires <a contenteditable="false" data-type="indexterm" data-primary="data preprocessing" data-secondary="for word embeddings" data-secondary-sortas="word embeddings" id="ch10_term20"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="data preparation for" id="ch10_term21"/><a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for semantic embeddings" data-secondary-sortas="semantic embeddings" id="idm45634181321544"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="Gensim for" id="idm45634181319896"/>sequences of tokens as input for the training. Besides tokenization there are some other aspects to consider for data preparation. Based on the distributional hypothesis, words frequently appearing together or in similar context will get similar vectors. Thus, we should make sure that co-occurrences are actually identified as such. If you do not have very many training sentences, as in our example here, you should include these steps in your preprocessing:</p>

<ol>
  <li>Clean text from <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="in data preparation" data-secondary-sortas="data preparation" id="idm45634181317208"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="tokenizers for" id="idm45634181315528"/>unwanted tokens (symbols, tags, etc.).</li>
  <li>Put all words into lowercase.</li>
  <li>Use <a contenteditable="false" data-type="indexterm" data-primary="lemmatization" id="idm45634181313192"/>lemmas.</li>
</ol>

<p>All this keeps the vocabulary small and training times short. Of course, inflected and uppercase words <a contenteditable="false" data-type="indexterm" data-primary="out-of-vocabulary words" id="idm45634181311240"/>will be out-of-vocabulary if we prune our training data according to these rules. This is not a problem for semantic reasoning on nouns as we want to do, but it could be, if we wanted to analyze, for example, emotions. In addition, you should consider these token categories: </p>
<dl>
<dt>Stop words</dt>
<dd>Stop words <a contenteditable="false" data-type="indexterm" data-primary="stop words" id="idm45634181308536"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="idm45634181307400"/>can carry valuable information about the context of non-stop words. Thus, we prefer to keep the stop words.</dd>
<dt>Numbers</dt>
<dd>Depending on the application, numbers can be valuable or just noise. In our example, we are looking at auto data and definitely want to keep tokens like <code>328</code> because it’s a BMW model name. You should keep numbers if they carry relevant information. </dd>
</dl>

<p>Another question is whether we should split on sentences or just keep the posts as they are. Consider the imaginary post “I like the BMW 328. But the Mercedes C300 is also great.” Should these sentences be treated like two different posts for our similarity task? Probably not. Thus, we will treat the list of all lemmas in one user post as a single “sentence” for training.</p>

<p>We already prepared the lemmas for the 20,000 Reddit posts on autos in <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>. Therefore, we can skip that part of data preparation here and just load the lemmas into a <a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634181301768"/><a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634181300664"/>Pandas <code>DataFrame</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">db_name</code> <code class="o">=</code> <code class="s2">"reddit-selfposts.db"</code>
<code class="n">con</code> <code class="o">=</code> <code class="n">sqlite3</code><code class="o">.</code><code class="n">connect</code><code class="p">(</code><code class="n">db_name</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_sql</code><code class="p">(</code><code class="s2">"select subreddit, lemmas, text from posts_nlp"</code><code class="p">,</code> <code class="n">con</code><code class="p">)</code>
<code class="n">con</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>

<code class="n">df</code><code class="p">[</code><code class="s1">'lemmas'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'lemmas'</code><code class="p">]</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">split</code><code class="p">()</code> <code class="c1"># lower case tokens</code>
<code class="n">sents</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'lemmas'</code><code class="p">]</code> <code class="c1"># our training "sentences"</code>
</pre>

<section data-type="sect3" data-pdf-bookmark="Phrases"><div class="sect3" id="idm45634185603256">
<h3>Phrases</h3>

<p>Especially in English, the <a contenteditable="false" data-type="indexterm" data-primary="compound phrases/names" id="ch10_term22"/><a contenteditable="false" data-type="indexterm" data-primary="phrase detection" id="ch10_term23"/>meaning of a word may change if the word is part of a compound phrase. Take, for example, <em>timing belt</em>, <em>seat belt</em>, or <em>rust belt</em>. All of these compounds have different meanings, even though all of them can be found in our corpus. So, it may better to treat such compounds as single tokens.</p>

<p>We can use any algorithm to detect such phrases, for example, spaCy’s detection of noun chunks (see <a data-type="xref" href="ch04.xhtml#ch4-spacy">“Linguistic Processing with spaCy”</a>). A number of statistical algorithms also exist to identify such collocations, such as extraordinary frequent n-grams. The original <a contenteditable="false" data-type="indexterm" data-primary="Word2Vec embedding" id="idm45634181231032"/>Word2Vec paper (Mikolov et al., 2013) uses a <a contenteditable="false" data-type="indexterm" data-primary="pointwise mutual information (PMI)" id="idm45634181229736"/>simple but effective algorithm based on <em>pointwise mutual information</em> (PMI), which basically measures the statistical dependence between the occurrences of two words.</p>

<p>For the model that we are now training, we use <a contenteditable="false" data-type="indexterm" data-primary="normalized pointwise mutual information (NPMI)" id="idm45634181227336"/>an advanced version called <em>normalized pointwise mutual information</em> (NPMI) because it gives more robust results. And given its limited value range from <math alttext="negative 1">
  <mrow>
    <mo>-</mo>
    <mn>1</mn>
  </mrow>
</math> to <math alttext="plus 1">
  <mrow>
    <mo>+</mo>
    <mn>1</mn>
  </mrow>
</math>, it is also easier to tune. The NPMI threshold in our initial run is set to a rather low value of 0.3. We choose a hyphen as a delimiter to connect the words in a phrase. This generates compound tokens like <em>harley-davidson</em>, which will be found in the text anyway. The default underscore delimiter would result in a different token:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models.phrases</code> <code class="kn">import</code> <code class="n">Phrases</code><code class="p">,</code> <code class="n">npmi_scorer</code>

<code class="n">phrases</code> <code class="o">=</code> <code class="n">Phrases</code><code class="p">(</code><code class="n">sents</code><code class="p">,</code> <code class="n">min_count</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">threshold</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
                  <code class="n">delimiter</code><code class="o">=</code><code class="s-Affix">b</code><code class="s1">'-'</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="n">npmi_scorer</code><code class="p">)</code>
</pre>

<p class="pagebreak-before">With this phrase model we can identify some interesting compound words:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">sent</code> <code class="o">=</code> <code class="s2">"I had to replace the timing belt in my mercedes c300"</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
<code class="n">phrased</code> <code class="o">=</code> <code class="n">phrases</code><code class="p">[</code><code class="n">sent</code><code class="p">]</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'|'</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">phrased</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
I|had|to|replace|the|timing-belt|in|my|mercedes-c300
</pre>

<p><em>timing-belt</em> is good, but we do not want to build compounds for combinations of brands and model names, like <em>mercedes c300</em>. Thus, we will analyze the phrase model to find a good threshold. Obviously, the chosen value was too low. The following code exports all phrases found in our corpus together with their scores and converts the result to a <code>DataFrame</code> for easy inspection:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">phrase_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">phrases</code><code class="o">.</code><code class="n">export_phrases</code><code class="p">(</code><code class="n">sents</code><code class="p">),</code>
                         <code class="n">columns</code> <code class="o">=</code><code class="p">[</code><code class="s1">'phrase'</code><code class="p">,</code> <code class="s1">'score'</code><code class="p">])</code>
<code class="n">phrase_df</code> <code class="o">=</code> <code class="n">phrase_df</code><code class="p">[[</code><code class="s1">'phrase'</code><code class="p">,</code> <code class="s1">'score'</code><code class="p">]]</code><code class="o">.</code><code class="n">drop_duplicates</code><code class="p">()</code> \
            <code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s1">'score'</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code><code class="o">.</code><code class="n">reset_index</code><code class="p">(</code><code class="n">drop</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">phrase_df</code><code class="p">[</code><code class="s1">'phrase'</code><code class="p">]</code> <code class="o">=</code> <code class="n">phrase_df</code><code class="p">[</code><code class="s1">'phrase'</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">p</code><code class="p">:</code> <code class="n">p</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s1">'utf-8'</code><code class="p">))</code>
</pre>

<p>Now we can check what would be a good threshold for <em>mercedes</em>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">phrase_df</code><code class="p">[</code><code class="n">phrase_df</code><code class="p">[</code><code class="s1">'phrase'</code><code class="p">]</code><code class="o">.</code><code class="n">str</code><code class="o">.</code><code class="n">contains</code><code class="p">(</code><code class="s1">'mercedes'</code><code class="p">)]</code>
</pre>



<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th/>
      <th>phrase</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>83</th>
      <td>mercedes benz</td>
      <td>0.80</td>
    </tr>
    <tr>
      <th>1417</th>
      <td>mercedes c300</td>
      <td>0.47</td>
    </tr>
  </tbody>
</table>


<p>As we can see, it should be larger than 0.5 and less than 0.8. Checking with a few other brands like <em>bmw</em>, <em>ford</em>, or <em>harley davidson</em> lets us identify 0.7 as a good threshold to identify compound vendor names but keep brands and models separate. In fact, with the rather stringent threshold of 0.7, the phrase model still keeps many relevant word combinations, for example, <em>street glide</em> (Harley-Davidson), <em>land cruiser</em> (Toyota), <em>forester xt</em> (Subaru), <em>water pump</em>, <em>spark plug</em>, or <em>timing belt</em>.</p>

<p>We rebuild our phraser and create a new column in our <code>DataFrame</code> with single tokens for compound words:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">phrases</code> <code class="o">=</code> <code class="n">Phrases</code><code class="p">(</code><code class="n">sents</code><code class="p">,</code> <code class="n">min_count</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">threshold</code><code class="o">=</code><code class="mf">0.7</code><code class="p">,</code>
                  <code class="n">delimiter</code><code class="o">=</code><code class="s-Affix">b</code><code class="s1">'-'</code><code class="p">,</code> <code class="n">scoring</code><code class="o">=</code><code class="n">npmi_scorer</code><code class="p">)</code>

<code class="n">df</code><code class="p">[</code><code class="s1">'phrased_lemmas'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'lemmas'</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">s</code><code class="p">:</code> <code class="n">phrases</code><code class="p">[</code><code class="n">s</code><code class="p">])</code>
<code class="n">sents</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'phrased_lemmas'</code><code class="p">]</code>
</pre>

<p>The result of our data preparation steps are sentences consisting of lemmas and phrases. We will now train different embedding models and check which insights we can <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term20" id="idm45634180973528"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term21" id="idm45634180885416"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term22" id="idm45634180884040"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term23" id="idm45634180882664"/>gain from them. </p>
</div></section>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Training Models with Gensim"><div class="sect2" id="idm45634181326712">
<h2>Blueprint: Training Models with Gensim</h2>

<p>Word2Vec and <a contenteditable="false" data-type="indexterm" data-primary="FastText" id="idm45634180879112"/><a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for semantic embeddings" data-secondary-sortas="semantic embeddings" id="idm45634180877976"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="Gensim for" id="idm45634180876328"/><a contenteditable="false" data-type="indexterm" data-primary="Word2Vec embedding" id="idm45634180874984"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="training of" id="idm45634180873880"/><a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="of machine learning models" data-secondary-sortas="machine learning models" id="idm45634180872504"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="training process for" id="idm45634180870888"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="for word embeddings" data-secondary-sortas="word embeddings" id="ch10_term24"/>FastText embeddings can be conveniently trained by Gensim. The following call to <code>Word2Vec</code> trains 100-dimensional Word2Vec embeddings on the corpus with a window size of 2, i.e., target word <span class="keep-together">±2 context</span> words. Some other <a contenteditable="false" data-type="indexterm" data-primary="hyperparameters" id="idm45634180866280"/>relevant hyperparameters are passed as well for illustration. We use the <a contenteditable="false" data-type="indexterm" data-primary="skip-gram model" id="idm45634180865016"/>skip-gram algorithm and train the network in four threads for <span class="keep-together">five epochs:</span></p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">Word2Vec</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">Word2Vec</code><code class="p">(</code><code class="n">sents</code><code class="p">,</code>       <code class="c1"># tokenized input sentences</code>
                 <code class="n">size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>    <code class="c1"># size of word vectors (default 100)</code>
                 <code class="n">window</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>    <code class="c1"># context window size (default 5)</code>
                 <code class="n">sg</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>        <code class="c1"># use skip-gram (default 0 = CBOW)</code>
                 <code class="n">negative</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>  <code class="c1"># number of negative samples (default 5)</code>
                 <code class="n">min_count</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="c1"># ignore infrequent words (default 5)</code>
                 <code class="n">workers</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>   <code class="c1"># number of threads (default 3)</code>
                 <code class="nb">iter</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>      <code class="c1"># number of epochs (default 5)</code>
</pre>

<p>This takes about 30 seconds on an i7 laptop for the 20,000 sentences, so it is quite fast. More samples and more epochs, as well as longer vectors and larger context windows, will increase the training time. For example, training 100-dimensional vectors with a context window size of 30 requires about 5 minutes in this setting for skip-gram. The <a contenteditable="false" data-type="indexterm" data-primary="CBOW (continuous bag-of-words) model" id="idm45634180782344"/>CBOW training time, in contrast, is rather independent of the context window size. </p>

<p>The following call saves the full model to disk. <em>Full model</em> means the complete neural network, including all internal states. This way, the model can be loaded again and trained further:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s1">'./models/autos_w2v_100_2_full.bin'</code><code class="p">)</code>
</pre>

<p>The choice of the algorithm as well as those hyperparameters have quite an impact on the resulting models. Therefore, we provide a blueprint to train and inspect different models. A parameter grid defines which algorithm variant (CBOW or skip-gram) and window sizes will be trained for Word2Vec or FastText. We could also vary vector size here, but that parameter does not have such a big impact. In our experience, 50- or 100-dimensional vectors work well on smaller corpora. So, we fix the vector size to 100 in our experiments:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">Word2Vec</code><code class="p">,</code> <code class="n">FastText</code>

<code class="n">model_path</code> <code class="o">=</code> <code class="s1">'./models'</code>
<code class="n">model_prefix</code> <code class="o">=</code> <code class="s1">'autos'</code>

<code class="n">param_grid</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'w2v'</code><code class="p">:</code> <code class="p">{</code><code class="s1">'variant'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'cbow'</code><code class="p">,</code> <code class="s1">'sg'</code><code class="p">],</code> <code class="s1">'window'</code><code class="p">:</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">30</code><code class="p">]},</code>
              <code class="s1">'ft'</code><code class="p">:</code> <code class="p">{</code><code class="s1">'variant'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'sg'</code><code class="p">],</code> <code class="s1">'window'</code><code class="p">:</code> <code class="p">[</code><code class="mi">5</code><code class="p">]}}</code>
<code class="n">size</code> <code class="o">=</code> <code class="mi">100</code>

<code class="k">for</code> <code class="n">algo</code><code class="p">,</code> <code class="n">params</code> <code class="ow">in</code> <code class="n">param_grid</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
    <code class="k">for</code> <code class="n">variant</code> <code class="ow">in</code> <code class="n">params</code><code class="p">[</code><code class="s1">'variant'</code><code class="p">]:</code>
        <code class="n">sg</code> <code class="o">=</code> <code class="mi">1</code> <code class="k">if</code> <code class="n">variant</code> <code class="o">==</code> <code class="s1">'sg'</code> <code class="k">else</code> <code class="mi">0</code>
        <code class="k">for</code> <code class="n">window</code> <code class="ow">in</code> <code class="n">params</code><code class="p">[</code><code class="s1">'window'</code><code class="p">]:</code>
            <code class="k">if</code> <code class="n">algo</code> <code class="o">==</code> <code class="s1">'w2v'</code><code class="p">:</code>
                <code class="n">model</code> <code class="o">=</code> <code class="n">Word2Vec</code><code class="p">(</code><code class="n">sents</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">size</code><code class="p">,</code> <code class="n">window</code><code class="o">=</code><code class="n">window</code><code class="p">,</code> <code class="n">sg</code><code class="o">=</code><code class="n">sg</code><code class="p">)</code>
            <code class="k">else</code><code class="p">:</code>
                <code class="n">model</code> <code class="o">=</code> <code class="n">FastText</code><code class="p">(</code><code class="n">sents</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="n">size</code><code class="p">,</code> <code class="n">window</code><code class="o">=</code><code class="n">window</code><code class="p">,</code> <code class="n">sg</code><code class="o">=</code><code class="n">sg</code><code class="p">)</code>

            <code class="n">file_name</code> <code class="o">=</code> <code class="n">f</code><code class="s2">"{model_path}/{model_prefix}_{algo}_{variant}_{window}"</code>
            <code class="n">model</code><code class="o">.</code><code class="n">wv</code><code class="o">.</code><code class="n">save_word2vec_format</code><code class="p">(</code><code class="n">file_name</code> <code class="o">+</code> <code class="s1">'.bin'</code><code class="p">,</code> <code class="n">binary</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
</pre>

<p>As we just want to analyze the similarities within our corpus, we do not save the complete models here but just the plain word vectors. These are represented by the class <code>KeyedVectors</code> and can be accessed by the model property <code>model.wv</code>. This generates much smaller files and is fully sufficient for our purpose. </p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>

<p>Beware of information loss! When you reload models <a contenteditable="false" data-type="indexterm" data-primary="word vectors" data-secondary="in semantic analysis" data-secondary-sortas="semantic analysis" id="idm45634180771112"/>consisting only of the word vectors, they cannot be trained further. Moreover, FastText models <a contenteditable="false" data-type="indexterm" data-primary="out-of-vocabulary words" id="idm45634180574792"/>lose the ability to derive embeddings for out-of-vocabulary words.</p></div>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Evaluating Different Models"><div class="sect2" id="idm45634180880568">
<h2>Blueprint: Evaluating Different Models</h2>

<p>Actually, it is quite hard to algorithmically identify the best hyperparameters for a domain-specific task and corpus. Thus, it is not a bad idea <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="evaluating results of" id="ch10_term25"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning models" data-secondary="evaluation of" id="ch10_term27"/>to inspect the models manually and check how they perform to identify some already-known relationships.</p>

<p>The saved files containing only the word vectors are small (about 5 MB each), so we can load many of them into memory and run some comparisons. We use a subset of five models to illustrate our findings. The models are stored in a dictionary indexed by the model name. You could add any models you’d like to compare, even the pretrained GloVe model from earlier:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">KeyedVectors</code>

<code class="n">names</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'autos_w2v_cbow_2'</code><code class="p">,</code> <code class="s1">'autos_w2v_sg_2'</code><code class="p">,</code>
         <code class="s1">'autos_w2v_sg_5'</code><code class="p">,</code> <code class="s1">'autos_w2v_sg_30'</code><code class="p">,</code> <code class="s1">'autos_ft_sg_5'</code><code class="p">]</code>
<code class="n">models</code> <code class="o">=</code> <code class="p">{}</code>

<code class="k">for</code> <code class="n">name</code> <code class="ow">in</code> <code class="n">names</code><code class="p">:</code>
    <code class="n">file_name</code> <code class="o">=</code> <code class="n">f</code><code class="s2">"{model_path}/{name}.bin"</code>
    <code class="n">models</code><code class="p">[</code><code class="n">name</code><code class="p">]</code> <code class="o">=</code> <code class="n">KeyedVectors</code><code class="o">.</code><code class="n">load_word2vec_format</code><code class="p">(</code><code class="n">file_name</code><code class="p">,</code> <code class="n">binary</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
</pre>

<p>We provide a small blueprint function for the comparison. It takes a list of models and a word and produces a <code>DataFrame</code> with the most similar words according to each model:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">compare_models</code><code class="p">(</code><code class="n">models</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>

    <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">model</code> <code class="ow">in</code> <code class="n">models</code><code class="p">:</code>
        <code class="n">df</code><code class="p">[</code><code class="n">name</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">f</code><code class="s2">"{word} {score:.3f}"</code>
                    <code class="k">for</code> <code class="n">word</code><code class="p">,</code> <code class="n">score</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="o">**</code><code class="n">kwargs</code><code class="p">)]</code>
    <code class="n">df</code><code class="o">.</code><code class="n">index</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">index</code> <code class="o">+</code> <code class="mi">1</code> <code class="c1"># let row index start at 1</code>
    <code class="k">return</code> <code class="n">df</code>
</pre>

<p>Now let’s see what effect the parameters have on our computed models. As we are going to analyze the car market, we check out the words most similar to <em>bmw</em>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">compare_models</code><code class="p">([(</code><code class="n">n</code><code class="p">,</code> <code class="n">models</code><code class="p">[</code><code class="n">n</code><code class="p">])</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">names</code><code class="p">],</code> <code class="n">positive</code><code class="o">=</code><code class="s1">'bmw'</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
</pre>



<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th/>
      <th>autos_w2v_cbow_2</th>
      <th>autos_w2v_sg_2</th>
      <th>autos_w2v_sg_5</th>
      <th>autos_w2v_sg_30</th>
      <th>autos_ft_sg_5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>mercedes 0.873</td>
      <td>mercedes 0.772</td>
      <td>mercedes 0.808</td>
      <td>xdrive 0.803</td>
      <td>bmws 0.819</td>
    </tr>
    <tr>
      <th>2</th>
      <td>lexus 0.851</td>
      <td>benz 0.710</td>
      <td>335i 0.740</td>
      <td>328i 0.797</td>
      <td>bmwfs 0.789</td>
    </tr>
    <tr>
      <th>3</th>
      <td>vw 0.807</td>
      <td>porsche 0.705</td>
      <td>328i 0.736</td>
      <td>f10 0.762</td>
      <td>m135i 0.774</td>
    </tr>
    <tr>
      <th>4</th>
      <td>benz 0.806</td>
      <td>lexus 0.704</td>
      <td>benz 0.723</td>
      <td>335i 0.760</td>
      <td>335i 0.773</td>
    </tr>
    <tr>
      <th>5</th>
      <td>volvo 0.792</td>
      <td>merc 0.695</td>
      <td>x-drive 0.708</td>
      <td>535i 0.755</td>
      <td>mercedes_benz 0.765</td>
    </tr>
    <tr>
      <th>6</th>
      <td>harley 0.783</td>
      <td>mercede 0.693</td>
      <td>135i 0.703</td>
      <td>bmws 0.745</td>
      <td>mercedes 0.760</td>
    </tr>
    <tr>
      <th>7</th>
      <td>porsche 0.781</td>
      <td>mercedes-benz 0.680</td>
      <td>mercede 0.690</td>
      <td>x-drive 0.740</td>
      <td>35i 0.747</td>
    </tr>
    <tr>
      <th>8</th>
      <td>subaru 0.777</td>
      <td>audi 0.675</td>
      <td>e92 0.685</td>
      <td>5-series 0.736</td>
      <td>merc 0.747</td>
    </tr>
    <tr>
      <th>9</th>
      <td>mb 0.769</td>
      <td>335i 0.670</td>
      <td>mercedes-benz 0.680</td>
      <td>550i 0.728</td>
      <td>135i 0.746</td>
    </tr>
    <tr>
      <th>10</th>
      <td>volkswagen 0.768</td>
      <td>135i 0.662</td>
      <td>merc 0.679</td>
      <td>435i 0.726</td>
      <td>435i 0.744</td>
    </tr>
  </tbody>
</table>


<p>Interestingly, the first models with the small window size of 2 produce mainly other car brands, while the model with window size 30 produces basically lists of different BMW models. In fact, shorter windows emphasize paradigmatic relations<a contenteditable="false" data-type="indexterm" data-primary="paradigmatic relations" id="idm45634180255608"/>, i.e., words that can be substituted for each other in a sentence. In our case, this would be brands as we are searching for words similar to <em>bmw</em>. Larger <a contenteditable="false" data-type="indexterm" data-primary="syntagmatic relations" id="idm45634180253768"/>windows capture more syntagmatic relations, where words are similar if they frequently show up in the same context. Window size 5, which is the default, produced a mix of both. For our data, paradigmatic relations are best represented by the CBOW model, while syntagmatic relations require a large window size and are therefore better captured by the <a contenteditable="false" data-type="indexterm" data-primary="skip-gram model" id="idm45634180252136"/>skip-gram model. The <a contenteditable="false" data-type="indexterm" data-primary="FastText" id="idm45634180250904"/>outputs of the FastText model demonstrate its property that similarly spelled words get similar scores. </p>

<section data-type="sect3" data-pdf-bookmark="Looking for similar concepts"><div class="sect3" id="idm45634180249512">
<h3>Looking for similar concepts</h3>

<p>The CBOW vectors with window size 2 are pretty precise on paradigmatic relations. Starting from some known terms, we can use such a <a contenteditable="false" data-type="indexterm" data-primary="similar concepts" id="idm45634180247848"/>model to identify the central terms and concepts of a domain. <a data-type="xref" href="#tab-most-sim">Table 10-1</a> shows the output of <a contenteditable="false" data-type="indexterm" data-primary="most_similar (Gensim)" id="idm45634180245656"/>some similarity queries on model <code>autos_w2v_cbow_2</code>. The column <code>concept</code> was added by us to highlight what kind of words we would expect as output.</p>

<table id="tab-most-sim"><caption><span class="label">Table 10-1. </span>Most similar neighbors for selected words using the CBOW model with window size 2</caption><thead>
<tr>
<th>Word</th>

<th>Concept</th>

<th>Most Similar</th></tr>
</thead>
<tbody>
<tr>
<td>toyota</td>

<td>car brand</td>

<td>ford mercedes nissan certify dodge mb bmw lexus chevy honda</td></tr>
<tr>
<td>camry</td>

<td>car model</td>

<td>corolla f150 f-150 c63 is300 ranger 335i 535i 328i rx</td></tr>
<tr>
<td>spark-plug</td>

<td>car part</td>

<td>water-pump gasket thermostat timing-belt tensioner throttle-body serpentine-belt radiator intake-manifold fluid</td></tr>
<tr>
<td>washington</td>

<td>location</td>

<td>oregon southwest ga ottawa san_diego valley portland mall chamber county</td></tr>
</tbody>
</table>


<p>Of course, the answers are not always correct with regard to our expectations; they are just similar words. For example, the list for <em>toyota</em> contains not only car brands but also several models. In real-life projects, however, domain experts from the business department can easily identify the wrong terms and still find interesting new associations. But manual curation is definitely necessary when you work with word embeddings this way.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Analogy reasoning on our own models"><div class="sect3" id="idm45634180232072">
<h3>Analogy reasoning on our own models</h3>

<p>Now let’s find out how our different models are capable of <a contenteditable="false" data-type="indexterm" data-primary="analogy reasoning" id="idm45634180230600"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="analogy reasoning with" id="idm45634180229496"/>detecting analogous concepts. We want to find out if Toyota has a product comparable to Ford’s F-150 pickup truck. So our question is: What is to “toyota” as “f150” is to “ford”? We use our function <code>compare_models</code> from earlier and transpose the result to compare the results of <code>wv.most_similar()</code> for different models:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">compare_models</code><code class="p">([(</code><code class="n">n</code><code class="p">,</code> <code class="n">models</code><code class="p">[</code><code class="n">n</code><code class="p">])</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">names</code><code class="p">],</code>
               <code class="n">positive</code><code class="o">=</code><code class="p">[</code><code class="s1">'f150'</code><code class="p">,</code> <code class="s1">'toyota'</code><code class="p">],</code> <code class="n">negative</code><code class="o">=</code><code class="p">[</code><code class="s1">'ford'</code><code class="p">],</code> <code class="n">topn</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>
</pre>

<p><code>Out:</code></p>

<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th/>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>autos_w2v_cbow_2</th>
      <td>f-150 0.850</td>
      <td>328i 0.824</td>
      <td>s80 0.820</td>
      <td>93 0.819</td>
      <td>4matic 0.817</td>
    </tr>
    <tr>
      <th>autos_w2v_sg_2</th>
      <td>f-150 0.744</td>
      <td>f-250 0.727</td>
      <td>dodge-ram 0.716</td>
      <td>tacoma 0.713</td>
      <td>ranger 0.708</td>
    </tr>
    <tr>
      <th>autos_w2v_sg_5</th>
      <td>tacoma 0.724</td>
      <td>tundra 0.707</td>
      <td>f-150 0.664</td>
      <td>highlander 0.644</td>
      <td>4wd 0.631</td>
    </tr>
    <tr>
      <th>autos_w2v_sg_30</th>
      <td>4runner 0.742</td>
      <td>tacoma 0.739</td>
      <td>4runners 0.707</td>
      <td>4wd 0.678</td>
      <td>tacomas 0.658</td>
    </tr>
    <tr>
      <th>autos_ft_sg_5</th>
      <td>toyotas 0.777</td>
      <td>toyo 0.762</td>
      <td>tacoma 0.748</td>
      <td>tacomas 0.745</td>
      <td>f150s 0.744</td>
    </tr>
  </tbody>
</table>


<p>In reality, the Toyota Tacoma is a direct competitor to the F-150 as well as the Toyota Tundra. With that in mind, the skip-gram model with the window size 5 gives the best results.<sup><a data-type="noteref" id="idm45634180172056-marker" href="ch10.xhtml#idm45634180172056">6</a></sup> In fact, if you exchange <em>toyota</em> for <em>gmc</em>, you get the <em>sierra</em>, and if you ask for <em>chevy</em>, you get <em>silverado</em> as the most similar to this model. All of these are competing full-size pickup trucks. This also works quite well for other brands and models, but of course it works best for those models that are heavily <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term24" id="idm45634180168552"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term25" id="idm45634180167176"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term27" id="idm45634180165800"/>discussed in the Reddit forum.</p>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Blueprints for Visualizing Embeddings"><div class="sect1" id="idm45634180572968">
<h1>Blueprints for Visualizing Embeddings</h1>

<p>If we explore our corpus on the basis of word embeddings, as we do in this chapter, we are not interested in the actual similarity scores because the whole concept is inherently fuzzy. What <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word embeddings" data-secondary-sortas="word embeddings" id="ch10_term31"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="visualizations for" id="ch10_term32"/>we want to understand are semantic relations based on the concepts of closeness and similarity. Therefore, visual representations can be extremely helpful for the exploration of word embeddings and their relations. In this section, we will first visualize embeddings by using different dimensionality reduction techniques. After that, we will show how to visually explore the semantic neighborhood of given keywords. As we will see, this type of data exploration can reveal quite interesting relationships between domain-specific terms.</p>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Applying Dimensionality Reduction"><div class="sect2" id="idm45634180133208">
<h2>Blueprint: Applying Dimensionality Reduction</h2>

<p>High-dimensional vectors <a contenteditable="false" data-type="indexterm" data-primary="dimensionality reduction" id="ch10_term28"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="dimensionality reduction for" id="ch10_term29"/>can be visualized by projecting the data into two or three dimensions. If the projection works well, it is possible to visually detect clusters of related terms and get a much deeper understanding of semantic concepts in the corpus. We will look for clusters of related words and explore the semantic neighborhood of certain keywords in the model with window size 30, which favors syntagmatic relations. Thus, we expect to see a “BMW” cluster with BMW terms, a “Toyota” cluster with Toyota terms, and so on. </p>

<p>Dimensionality reduction also has many use cases in the area of machine learning. Some learning algorithms have problems with high-dimensional and often sparse data. Dimensionality reduction techniques such as PCA, t-SNE, or UMAP (see <a data-type="xref" href="#drt">“Dimensionality Reduction Techniques”</a>) try to preserve or even highlight important aspects of the data distribution by the projection. The general idea <span class="keep-together">is to</span> project the data in a way that objects close to each other in high-dimensional <span class="keep-together">space are</span> close in the projection and, similarly, distant objects remain distant. In our <span class="keep-together">examples</span>, we <a contenteditable="false" data-type="indexterm" data-primary="UMAP algorithm" id="ch10_term30"/>will use the UMAP algorithm because it provides the best results for <span class="keep-together">visualization</span>. But as the <code>umap</code> library implements the <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="with visualization of results" data-secondary-sortas="visualization of results" id="idm45634180121128"/>scikit-learn estimator interface, you can easily replace the UMAP reducer with scikit-learn’s <code>PCA</code> or <code>TSNE</code> classes.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="drt">
<h5>Dimensionality Reduction Techniques</h5>

<p>There are many different algorithms for dimensionality reduction. Frequently used for visualization are PCA, t-SNE, and UMAP.</p>

<p><em>Principal <a contenteditable="false" data-type="indexterm" data-primary="PCA (principal component analysis)" id="idm45634180116184"/><a contenteditable="false" data-type="indexterm" data-primary="principal component analysis (PCA)" id="idm45634180115032"/>Component Analysis (PCA)</em> performs a linear projection of the data such that most of the variance in the data points is preserved. Mathematically, it is based on the eigenvectors of the largest eigenvalues of the covariance matrix (the principal components). PCA only takes the global data distribution into account. Independent of local structures, all data points are transformed in the same way. Except for the number of dimensions in the target space (<code>n_components</code>), PCA has no hyperparameters to tune. </p>

<p>Nonlinear <a contenteditable="false" data-type="indexterm" data-primary="nonlinear algorithms" id="idm45634180112136"/>algorithms such as <a contenteditable="false" data-type="indexterm" data-primary="t-SNE algorithm" id="idm45634180110872"/>t-SNE and UMAP try to balance local and global aspects in the mapping. Thus, different areas of the original space get projected differently dependent on the local data distribution. <a contenteditable="false" data-type="indexterm" data-primary="hyperparameters" id="idm45634180109416"/>Both algorithms provide hyperparameters that need to be carefully selected to produce good results. For t-SNE this is the <code>perplexity</code> (roughly the effective number of nearest neighbors of each point). For UMAP you need to specify the size of the local neighborhood (<code>n_neighbors</code>) and in addition the minimum distance of points in the projection (<code>min_dist</code>). t-SNE, published in 2008, is very popular but has some severe limitations. It preserves the local structure much better than the global structure, it does not scale well, and it works practically only for two or three dimensions. UMAP, published in 2018, is faster and retains the global data structure much better.</p></div></aside>

<p>The following code block contains the <a contenteditable="false" data-type="indexterm" data-primary="two-dimensional projections" id="ch10_term33"/>basic operations to project the embeddings into two-dimensional space with UMAP, as shown in <a data-type="xref" href="#fig-umap-all">Figure 10-3</a>. After the selection of the embedding models and the words to plot (in this case we take the whole vocabulary), we instantiate the UMAP dimensionality reducer with target dimensionality <code>n_components=2</code>. Instead of the standard Euclidean distance metric, we use the cosine as usual. The embeddings are then projected to 2D by calling <code>reducer.fit_transform(wv)</code>. </p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">umap</code> <code class="kn">import</code> <code class="n">UMAP</code>

<code class="n">model</code> <code class="o">=</code> <code class="n">models</code><code class="p">[</code><code class="s1">'autos_w2v_sg_30'</code><code class="p">]</code>
<code class="n">words</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">vocab</code>
<code class="n">wv</code> <code class="o">=</code> <code class="p">[</code><code class="n">model</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">]</code>

<code class="n">reducer</code> <code class="o">=</code> <code class="n">UMAP</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">metric</code><code class="o">=</code><code class="s1">'cosine'</code><code class="p">,</code> <code class="n">n_neighbors</code> <code class="o">=</code> <code class="mi">15</code><code class="p">,</code> <code class="n">min_dist</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="n">reduced_wv</code> <code class="o">=</code> <code class="n">reducer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">wv</code><code class="p">)</code>
</pre>

<figure><div id="fig-umap-all" class="figure">
<img src="Images/btap_1003.jpg" width="1408" height="528"/>
<h6><span class="label">Figure 10-3. </span>Two-dimensional UMAP projections of all word embeddings of our model. A few words and their most similar neighbors are highlighted to explain some of the clusters in this scatter plot. </h6></div></figure>

<p>We use <a contenteditable="false" data-type="indexterm" data-primary="Plotly Express" id="ch10_term34"/>Plotly Express here for visualization instead of Matplotlib because it has two nice features. First, it produces interactive plots. When you hover with the mouse over a point, the respective word will be displayed. Moreover, you can zoom in and out and select regions. The second nice feature of Plotly Express is its simplicity. All you need to prepare is a <code>DataFrame</code> with the coordinates and the metadata to be displayed. Then you just instantiate the chart, in this case <a contenteditable="false" data-type="indexterm" data-primary="scatter plots" id="idm45634180035640"/>the scatter plot (<code>px.scatter</code>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">plotly.express</code> <code class="kn">as</code> <code class="nn">px</code>

<code class="n">plot_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_records</code><code class="p">(</code><code class="n">reduced_wv</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'x'</code><code class="p">,</code> <code class="s1">'y'</code><code class="p">])</code>
<code class="n">plot_df</code><code class="p">[</code><code class="s1">'word'</code><code class="p">]</code> <code class="o">=</code> <code class="n">words</code>
<code class="n">params</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'hover_data'</code><code class="p">:</code> <code class="p">{</code><code class="n">c</code><code class="p">:</code> <code class="bp">False</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">plot_df</code><code class="o">.</code><code class="n">columns</code><code class="p">},</code>
          <code class="s1">'hover_name'</code><code class="p">:</code> <code class="s1">'word'</code><code class="p">}</code>

<code class="n">fig</code> <code class="o">=</code> <code class="n">px</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">plot_df</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s2">"x"</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s2">"y"</code><code class="p">,</code> <code class="n">opacity</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">size_max</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="o">**</code><code class="n">params</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
</pre>

<p>You can find a more general blueprint function <code>plot_embeddings</code> in the <code>embeddings</code> package in our <a href="https://oreil.ly/gX6Ti">GitHub repository</a>. It allows you to choose the dimensionality reduction algorithm and highlight selected search words with their most similar neighbors in the low-dimensional projection. For the plot in <a data-type="xref" href="#fig-umap-all">Figure 10-3</a> we inspected some clusters manually beforehand and then explicitly named a few typical search words to colorize the clusters.<sup><a data-type="noteref" id="idm45634179916184-marker" href="ch10.xhtml#idm45634179916184">7</a></sup> In the interactive view, you could see the words when you hover over the points. </p>

<p class="pagebreak-before">Here is the code to produce this diagram:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">blueprints.embeddings</code> <code class="kn">import</code> <code class="n">plot_embeddings</code>

<code class="n">search</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'ford'</code><code class="p">,</code> <code class="s1">'lexus'</code><code class="p">,</code> <code class="s1">'vw'</code><code class="p">,</code> <code class="s1">'hyundai'</code><code class="p">,</code>
          <code class="s1">'goodyear'</code><code class="p">,</code> <code class="s1">'spark-plug'</code><code class="p">,</code> <code class="s1">'florida'</code><code class="p">,</code> <code class="s1">'navigation'</code><code class="p">]</code>

<code class="n">plot_embeddings</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">search</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">show_all</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code>
                <code class="n">algo</code><code class="o">=</code><code class="s1">'umap'</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">15</code><code class="p">,</code> <code class="n">min_dist</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
</pre>

<p>For data exploration, it might be more interesting to visualize only the set of search words and their most similar neighbors, without all other points. <a data-type="xref" href="#fig-umap-selected-2d">Figure 10-4</a> shows an example generated by the following lines. Displayed are the search words and their top 10 most similar neighbors:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">search</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'ford'</code><code class="p">,</code> <code class="s1">'bmw'</code><code class="p">,</code> <code class="s1">'toyota'</code><code class="p">,</code> <code class="s1">'tesla'</code><code class="p">,</code> <code class="s1">'audi'</code><code class="p">,</code> <code class="s1">'mercedes'</code><code class="p">,</code> <code class="s1">'hyundai'</code><code class="p">]</code>

<code class="n">plot_embeddings</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">search</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">show_all</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
    <code class="n">algo</code><code class="o">=</code><code class="s1">'umap'</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">15</code><code class="p">,</code> <code class="n">min_dist</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">spread</code><code class="o">=</code><code class="mi">25</code><code class="p">)</code>
</pre>

<figure><div id="fig-umap-selected-2d" class="figure">
<img src="Images/btap_1004.jpg" width="1408" height="528"/>
<h6><span class="label">Figure 10-4. </span>Two-dimensional UMAP projection of selected keywords words and their most similar neighbors.</h6></div></figure>

<p><a data-type="xref" href="#fig-umap-selected-3d">Figure 10-5</a> shows the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term33" id="idm45634179727176"/>same keywords but with many more similar neighbors <a contenteditable="false" data-type="indexterm" data-primary="three-dimensional plot" id="idm45634179725576"/>as a three-dimensional plot. It is nice that Plotly allows you to rotate and zoom into the point cloud. This way it is easy to investigate interesting areas. Here is the call to generate <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term34" id="idm45634179724136"/>that diagram:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">plot_embeddings</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">search</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code> <code class="n">n_dims</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code>
    <code class="n">algo</code><code class="o">=</code><code class="s1">'umap'</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">15</code><code class="p">,</code> <code class="n">min_dist</code><code class="o">=.</code><code class="mi">1</code><code class="p">,</code> <code class="n">spread</code><code class="o">=</code><code class="mi">40</code><code class="p">)</code>
</pre>


<p>To visualize analogies such as <em>tacoma is to toyota like f150 is to ford</em>, you should <a contenteditable="false" data-type="indexterm" data-primary="PCA (principal component analysis)" id="idm45634179666712"/><a contenteditable="false" data-type="indexterm" data-primary="principal component analysis (PCA)" id="idm45634179665736"/>use the linear PCA transformation. Both <a contenteditable="false" data-type="indexterm" data-primary="nonlinear algorithms" id="idm45634179664504"/>UMAP and <a contenteditable="false" data-type="indexterm" data-primary="t-SNE algorithm" id="idm45634179663272"/>t-SNE distort the original space in a nonlinear manner. Therefore, the direction of difference vectors in the projected space can be totally unrelated to the original direction. Even PCA distorts because of shearing, but the effect is not as <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term28" id="idm45634179661752"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term29" id="idm45634179660376"/>strong as in UMAP or t-SNE. </p>


<figure><div id="fig-umap-selected-3d" class="figure">
  <img src="Images/btap_1005.jpg" width="1004" height="487"/>
  <h6><span class="label">Figure 10-5. </span>Three-dimensional UMAP projection of selected keywords and their most similar neighbors.</h6></div></figure>

</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Using the TensorFlow Embedding Projector"><div class="sect2" id="idm45634180132616">
<h2>Blueprint: Using the TensorFlow Embedding Projector</h2>

<p>A nice alternative to a self-implemented visualization function <a contenteditable="false" data-type="indexterm" data-primary="TensorFlow Embedding Projector" id="idm45634179655896"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="TensorFlow Embedding Projector for" id="idm45634179654824"/>is the TensorFlow Embedding Projector. It also supports PCA, t-SNE, and UMAP and offers some convenient options for data filtering and highlighting. You don’t even have to install TensorFlow to use it because there is an <a href="https://oreil.ly/VKLxe">online version available</a>. A few datasets are already loaded as a demo.</p>

<p>To display our own word embeddings with the TensorFlow Embedding Projector, we need to create two files with tabulator-separated values: one file with the word vectors and an optional file with metadata for the embeddings, which in our case are simply the words. This can be achieved with a few lines of code: </p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">csv</code>

<code class="n">name</code> <code class="o">=</code> <code class="s1">'autos_w2v_sg_30'</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">models</code><code class="p">[</code><code class="n">name</code><code class="p">]</code>

<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">f</code><code class="s1">'{model_path}/{name}_words.tsv'</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code> <code class="k">as</code> <code class="n">tsvfile</code><code class="p">:</code>
    <code class="n">tsvfile</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">vocab</code><code class="p">))</code>

<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">f</code><code class="s1">'{model_path}/{name}_vecs.tsv'</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code> <code class="k">as</code> <code class="n">tsvfile</code><code class="p">:</code>
    <code class="n">writer</code> <code class="o">=</code> <code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">tsvfile</code><code class="p">,</code> <code class="n">delimiter</code><code class="o">=</code><code class="s1">'</code><code class="se">\t</code><code class="s1">'</code><code class="p">,</code>
                        <code class="n">dialect</code><code class="o">=</code><code class="n">csv</code><code class="o">.</code><code class="n">unix_dialect</code><code class="p">,</code> <code class="n">quoting</code><code class="o">=</code><code class="n">csv</code><code class="o">.</code><code class="n">QUOTE_MINIMAL</code><code class="p">)</code>
    <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">vocab</code><code class="p">:</code>
        <code class="n">_</code> <code class="o">=</code> <code class="n">writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">model</code><code class="p">[</code><code class="n">w</code><code class="p">]</code><code class="o">.</code><code class="n">tolist</code><code class="p">())</code>
</pre>

<p>Now we can load our embeddings into the projector and navigate through the 3D visualization. For the detection of clusters, you should use UMAP or t-SNE. <a data-type="xref" href="#fig-tf-projector">Figure 10-6</a> shows a cutout of the UMAP projection for our embeddings. In the projector, you can click any data point or search for a word and get the first 100 neighbors highlighted. We chose <em>harley</em> as a starting point to explore the terms related to Harley-Davidson. As you can see, this kind of visualization can be extremely helpful when exploring important terms of a domain and <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term30" id="idm45634179528584"/>their semantic relationship.</p>

<figure><div id="fig-tf-projector" class="figure">
<img src="Images/btap_1006.jpg" width="993" height="571"/>
<h6><span class="label">Figure 10-6. </span>Visualization of embeddings with TensorFlow Embedding Projector.</h6></div></figure>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Constructing a Similarity Tree"><div class="sect2" id="idm45634179525416">
<h2>Blueprint: Constructing a Similarity Tree</h2>

<p>The words with their <a contenteditable="false" data-type="indexterm" data-primary="similarity trees, constructing" id="ch10_term35"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="similarity trees, constructing" id="ch10_term36"/>similarity relations can be interpreted as a network graph in the following way: the words represent the nodes of the graph, and an edge is created whenever two nodes are “very” similar. The criterion for this could be either that the nodes are among their top-n most-similar neighbors or a threshold for the similarity score. However, most of the words in the vicinity of a word are similar not only to that word but also to each other. Thus, the complete network graph even for a small subset of words would have too many edges for comprehensible visualization. Therefore, we start with a slightly different approach and create a subgraph of this network, a similarity tree. <a data-type="xref" href="#fig-sim-tree-noise">Figure 10-7</a> shows such a similarity tree for the root word <em>noise</em>.</p>

<figure><div id="fig-sim-tree-noise" class="figure">
<img src="Images/btap_1007.jpg" width="1359" height="570"/>
<h6><span class="label">Figure 10-7. </span>Similarity tree of words most similar to noise.</h6></div></figure>

<p>We provide two blueprint functions to create such visualizations. The <a contenteditable="false" data-type="indexterm" data-primary="sim_tree function" id="ch10_term37"/>first one, <code>sim_tree</code>, generates the similarity tree starting from a root word. The second one, <code>plot_tree</code>, creates the plot. We use <a contenteditable="false" data-type="indexterm" data-primary="NetworkX graph library (Python)" id="ch10_term39"/>Python’s graph library <code>networkx</code> in both <span class="keep-together">functions</span>.</p>

<p>Let’s first look at <code>sim_tree</code>. Starting from a root word, we look for the top-n most-similar neighbors. They are added to the graph with the according edges. Then we do the same for each of these newly discovered neighbors, and their neighbors, and so on, until a maximum distance to the root node is reached. Internally, we use a queue (<code>collections.deque</code>) to implement a breadth-first search. The edges are weighted by similarity, which is used later to style the line width:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">networkx</code> <code class="kn">as</code> <code class="nn">nx</code>
<code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">deque</code>

<code class="k">def</code> <code class="nf">sim_tree</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">word</code><code class="p">,</code> <code class="n">top_n</code><code class="p">,</code> <code class="n">max_dist</code><code class="p">):</code>

    <code class="n">graph</code> <code class="o">=</code> <code class="n">nx</code><code class="o">.</code><code class="n">Graph</code><code class="p">()</code>
    <code class="n">graph</code><code class="o">.</code><code class="n">add_node</code><code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">dist</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

    <code class="n">to_visit</code> <code class="o">=</code> <code class="n">deque</code><code class="p">([</code><code class="n">word</code><code class="p">])</code>
    <code class="k">while</code> <code class="nb">len</code><code class="p">(</code><code class="n">to_visit</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>
        <code class="n">source</code> <code class="o">=</code> <code class="n">to_visit</code><code class="o">.</code><code class="n">popleft</code><code class="p">()</code> <code class="c1"># visit next node</code>
        <code class="n">dist</code> <code class="o">=</code> <code class="n">graph</code><code class="o">.</code><code class="n">nodes</code><code class="p">[</code><code class="n">source</code><code class="p">][</code><code class="s1">'dist'</code><code class="p">]</code><code class="o">+</code><code class="mi">1</code>

        <code class="k">if</code> <code class="n">dist</code> <code class="o">&lt;=</code> <code class="n">max_dist</code><code class="p">:</code> <code class="c1"># discover new nodes</code>
            <code class="k">for</code> <code class="n">target</code><code class="p">,</code> <code class="n">sim</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">most_similar</code><code class="p">(</code><code class="n">source</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="n">top_n</code><code class="p">):</code>
                <code class="k">if</code> <code class="n">target</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">graph</code><code class="p">:</code>
                    <code class="n">to_visit</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">target</code><code class="p">)</code>
                    <code class="n">graph</code><code class="o">.</code><code class="n">add_node</code><code class="p">(</code><code class="n">target</code><code class="p">,</code> <code class="n">dist</code><code class="o">=</code><code class="n">dist</code><code class="p">)</code>
                    <code class="n">graph</code><code class="o">.</code><code class="n">add_edge</code><code class="p">(</code><code class="n">source</code><code class="p">,</code> <code class="n">target</code><code class="p">,</code> <code class="n">sim</code><code class="o">=</code><code class="n">sim</code><code class="p">,</code> <code class="n">dist</code><code class="o">=</code><code class="n">dist</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">graph</code>
</pre>

<p>The function <code>plot_tree</code> consists of just a few calls to create the layout and to draw the nodes and edges with some styling. We <a contenteditable="false" data-type="indexterm" data-primary="Graphviz" id="idm45634179505848"/>used Graphviz’s <code>twopi</code> layout to create the snowflake-like positioning of nodes. A few details have been left out here for the sake of simplicity, but you can find the <a href="https://oreil.ly/W-zbu">full code on GitHub</a>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">networkx.drawing.nx_pydot</code> <code class="kn">import</code> <code class="n">graphviz_layout</code>

<code class="k">def</code> <code class="nf">plot_tree</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">node_size</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">font_size</code><code class="o">=</code><code class="mi">12</code><code class="p">):</code>

    <code class="n">pos</code> <code class="o">=</code> <code class="n">graphviz_layout</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">prog</code><code class="o">=</code><code class="s1">'twopi'</code><code class="p">,</code> <code class="n">root</code><code class="o">=</code><code class="nb">list</code><code class="p">(</code><code class="n">graph</code><code class="o">.</code><code class="n">nodes</code><code class="p">)[</code><code class="mi">0</code><code class="p">])</code>

    <code class="n">colors</code> <code class="o">=</code> <code class="p">[</code><code class="n">graph</code><code class="o">.</code><code class="n">nodes</code><code class="p">[</code><code class="n">n</code><code class="p">][</code><code class="s1">'dist'</code><code class="p">]</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">graph</code><code class="p">]</code> <code class="c1"># colorize by distance</code>
    <code class="n">nx</code><code class="o">.</code><code class="n">draw_networkx_nodes</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">pos</code><code class="p">,</code> <code class="n">node_size</code><code class="o">=</code><code class="n">node_size</code><code class="p">,</code> <code class="n">node_color</code><code class="o">=</code><code class="n">colors</code><code class="p">,</code>
                           <code class="n">cmap</code><code class="o">=</code><code class="s1">'Set1'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.4</code><code class="p">)</code>
    <code class="n">nx</code><code class="o">.</code><code class="n">draw_networkx_labels</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">pos</code><code class="p">,</code> <code class="n">font_size</code><code class="o">=</code><code class="n">font_size</code><code class="p">)</code>

    <code class="k">for</code> <code class="p">(</code><code class="n">n1</code><code class="p">,</code> <code class="n">n2</code><code class="p">,</code> <code class="n">sim</code><code class="p">)</code> <code class="ow">in</code> <code class="n">graph</code><code class="o">.</code><code class="n">edges</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="s1">'sim'</code><code class="p">):</code>
         <code class="n">nx</code><code class="o">.</code><code class="n">draw_networkx_edges</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">pos</code><code class="p">,</code> <code class="p">[(</code><code class="n">n1</code><code class="p">,</code> <code class="n">n2</code><code class="p">)],</code> <code class="n">width</code><code class="o">=</code><code class="n">sim</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.2</code><code class="p">)</code>

    <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
</pre>

<p><a data-type="xref" href="#fig-sim-tree-noise">Figure 10-7</a> was generated with these functions using this parametrization:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code> <code class="o">=</code> <code class="n">models</code><code class="p">[</code><code class="s1">'autos_w2v_sg_2'</code><code class="p">]</code>
<code class="n">graph</code> <code class="o">=</code> <code class="n">sim_tree</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="s1">'noise'</code><code class="p">,</code> <code class="n">top_n</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">max_dist</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">plot_tree</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">node_size</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">font_size</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code>
</pre>

<p>It shows the most similar words to <em>noise</em> and their most similar words up to an imagined distance of 3 to <em>noise</em>.
The visualization suggests that we created a kind of a taxonomy, but actually we didn’t. We just chose to include only a subset of the possible edges in our graph to highlight the relationships between a “parent” word and its most similar “child” words. The approach ignores possible edges among siblings or to grandparents. The visual presentation nevertheless helps to explore the specific vocabulary of an application domain around the root word. However, <a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for semantic embeddings" data-secondary-sortas="semantic embeddings" id="idm45634179134680"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="Gensim for" id="idm45634179133128"/><a contenteditable="false" data-type="indexterm" data-primary="Poincaré embeddings" id="idm45634179131784"/><a contenteditable="false" data-type="indexterm" data-primary="hierarchical relationships" id="idm45634179130680"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="hierarchical relationships of" id="idm45634179129608"/>Gensim also implements <a href="https://oreil.ly/mff7p">Poincaré embeddings</a> for learning hierarchical relationships among words.</p>

<p>The model with the small context window of 2 used for this figure brings out the different kinds and synonyms of noises. If we choose a large context window instead, we get more concepts related to the root word. <a data-type="xref" href="#fig-sim-tree-plug">Figure 10-8</a> was created with these parameters:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">model</code> <code class="o">=</code> <code class="n">models</code><code class="p">[</code><code class="s1">'autos_w2v_sg_30'</code><code class="p">]</code>
<code class="n">graph</code> <code class="o">=</code> <code class="n">sim_tree</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="s1">'spark-plug'</code><code class="p">,</code> <code class="n">top_n</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> <code class="n">max_dist</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">plot_tree</code><code class="p">(</code><code class="n">graph</code><code class="p">,</code> <code class="n">node_size</code><code class="o">=</code><code class="mi">500</code><code class="p">,</code> <code class="n">font_size</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code>
</pre>

<figure><div id="fig-sim-tree-plug" class="figure">
<img src="Images/btap_1008.jpg" width="1365" height="570"/>
<h6><span class="label">Figure 10-8. </span>Similarity tree of words most similar to spark-plug’s most similar words.</h6></div></figure>

<p>Here, we chose <em>spark-plug</em> as root word and selected the model with window size 30. The generated diagram gives a nice overview about domain-specific terms related to <em>spark-plugs</em>. For example, the codes like <em>p0302</em>, etc., are the standardized OBD2 error codes for misfires in the different cylinders. </p>

<p>Of course, these charts also bring up some the weaknesses of our data preparation. We see four nodes for <em>spark-plug</em>, <em>sparkplug</em>, <em>spark</em>, and <em>plugs</em>, all of which are representing the same concept. If we wanted to have a single embedding for all of these, we would have to merge the different forms of writing <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term31" id="idm45634178966776"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term32" id="idm45634178965400"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term35" id="idm45634178964024"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term36" id="idm45634178962648"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term37" id="idm45634178961272"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch10_term39" id="idm45634178959896"/>into a single token.</p>

</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Closing Remarks"><div class="sect1" id="idm45634179524824">
<h1>Closing Remarks</h1>

<p>Exploring the <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="about" id="idm45634178956952"/>similar neighbors of certain key terms in domain-specific models can be a valuable technique to discover latent semantic relationships among words in a domain-specific corpus. Even though the whole concept of word similarity is inherently fuzzy, we produced really interesting and interpretable results by training a simple neural network on just 20,000 user posts about cars.</p>

<p>As in most machine learning tasks, the <a contenteditable="false" data-type="indexterm" data-primary="data preprocessing" data-secondary="for word embeddings" data-secondary-sortas="word embeddings" id="idm45634178954632"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="data preparation for" id="idm45634178952984"/>quality of the results is strongly influenced by data preparation. Depending on the task you are going to achieve, you should decide consciously which kind of normalization and pruning you apply to the original texts. In many cases, using lemmas and lowercase words produces good embeddings for similarity reasoning. Phrase detection can be helpful, not only to improve the result but also to identify possibly important compound terms in your application domain.</p>

<p>We used <a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for semantic embeddings" data-secondary-sortas="semantic embeddings" id="idm45634178950392"/><a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="Gensim for" id="idm45634178948712"/>Gensim to train, store, and analyze our embeddings. Gensim is very popular, but you may also want to check possibly faster alternatives like <a href="https://oreil.ly/UlRzX">(Py)Magnitude</a> or <a href="https://oreil.ly/TwM4h">finalfusion</a>. Of course, you can also use <a contenteditable="false" data-type="indexterm" data-primary="TensorFlow Embedding Projector" id="idm45634178945672"/>TensorFlow and <a contenteditable="false" data-type="indexterm" data-primary="PyTorch" id="idm45634178944424"/>PyTorch to train different kinds of embeddings.</p>

<p>Today, semantic embeddings are fundamental for all complex machine learning tasks. However, for tasks such as sentiment analysis or paraphrase detection, you don’t need embeddings for words but for sentences or <a contenteditable="false" data-type="indexterm" data-primary="document embedding" id="idm45634178942648"/>complete documents. Many different approaches have been published to create document embeddings (Wolf, 2018; Palachy, 2019). A common approach is to compute the average of the word vectors in a sentence. Some of <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for word vectors" data-secondary-sortas="word vectors" id="idm45634178941416"/><a contenteditable="false" data-type="indexterm" data-primary="word vectors" data-secondary="in semantic analysis" data-secondary-sortas="semantic analysis" id="idm45634178915112"/>spaCy’s models include <a href="https://oreil.ly/zI1wm">word vectors</a> in their vocabulary and allow the computation of document similarities based on average word vectors out of the box. However, averaging word vectors only works reasonably well for single sentences or very short documents. In addition, the whole approach is limited to the bag-of-words idea where the word order is not considered.</p>

<p>State-of-the-art models utilize both the power of semantic embeddings and the word order. We will use such a model in the next chapter for <a contenteditable="false" data-type="indexterm" data-primary="word embeddings for semantic analysis" data-secondary="further reading on" id="idm45634178911848"/>sentiment classification.</p></div></section>


<section data-type="sect1" data-pdf-bookmark="Further Reading"><div class="sect1" id="idm45634178910280">
<h1>Further Reading</h1>
<ul class="author-date-bib">
<li>

<p>Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. <em>Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings</em>. 2016. <a href="https://arxiv.org/abs/1607.06520"><em>https://arxiv.org/abs/1607.06520</em></a>.</p>
</li>
<li>

<p>Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomáš Mikolov. <em>Bag of Tricks for Efficient Text Classification</em>. 2017. <a href="https://www.aclweb.org/anthology/E17-2068"><em>https://www.aclweb.org/anthology/E17-2068</em></a>.</p>
</li>
<li>

<p>Levy, Omer, Yoav Goldberg, and Ido Dagan. <em>Improving Distributional Similarity with Lessons Learned from Word Embeddings</em>. <a href="https://www.aclweb.org/anthology/Q15-1016"><em>https://www.aclweb.org/anthology/Q15-1016</em></a>.</p>
</li>
<li>

<p>McCormick, Chris. <em>Word2Vec Tutorial</em>. <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model"><em>http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model</em></a> and
  <a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling"><em>http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling</em></a>.</p>
</li>
<li>

<p>Mikolov, Tomáš, Kai Chen, Greg Corrado, and Jeffrey Dean. <em>Efficient Estimation of Word Representations in Vector Space</em>. 2013. <a href="https://arxiv.org/abs/1301.3781"><em>https://arxiv.org/abs/1301.3781</em></a>.</p>
</li>
<li>

<p>Mikolov, Tomáš, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. <em>Distributed Representations of Words and Phrases and Their Compositionality</em>. 2013. <a href="https://arxiv.org/abs/1310.4546"><em>https://arxiv.org/abs/1310.4546</em></a>.</p>
</li>
<li>

<p>Palachy, Shay. <em>Beyond Word Embedding: Key Ideas in Document Embedding</em>. <a href="https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html"><em>https://www.kdnuggets.com/2019/10/beyond-word-embedding-document-embedding.html</em></a>.</p>
</li>
<li>

<p>Pennington, Jeffrey, Richard Socher, and Christopher Manning. <em>Glove: Global Vectors for Word Representation</em>. 2014. <a href="https://nlp.stanford.edu/pubs/glove.pdf"><em>https://nlp.stanford.edu/pubs/glove.pdf</em></a>.</p>
</li>
<li>

<p>Peters, Matthew E., Mark Neumann, Mohit Iyyer, et al. <em>Deep contextualized word representations</em>. 2018. <a href="https://arxiv.org/abs/1802.05365"><em>https://arxiv.org/abs/1802.05365</em></a>.</p>
</li>
<li>

<p>Wolf, Thomas. <em>The Current Best of Universal Word Embeddings and Sentence Embeddings</em>. 2018. <a href="https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a"><em>https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a</em></a>.</p>
</li>
</ul></div></section>

<div data-type="footnotes"><p data-type="footnote" id="idm45634182078904"><sup><a href="ch10.xhtml#idm45634182078904-marker">1</a></sup> Inspired by Adrian Colyer’s <a href="https://oreil.ly/8iMPF">“The Amazing Power of Word Vectors” blog post</a>.</p><p data-type="footnote" id="idm45634182074280"><sup><a href="ch10.xhtml#idm45634182074280-marker">2</a></sup> This frequently cited example originally came from the linguist Eugene Nida in 1975.</p><p data-type="footnote" id="idm45634182038552"><sup><a href="ch10.xhtml#idm45634182038552-marker">3</a></sup> Jay Alammar’s blog post entitled <a href="https://oreil.ly/TZNTT">“The Illustrated Word2Vec”</a> gives a wonderful visual explanation of this equation.</p><p data-type="footnote" id="idm45634181941800"><sup><a href="ch10.xhtml#idm45634181941800-marker">4</a></sup> Words having the same pronunciation but different meanings are called <em>homonyms</em>. If they are spelled identically, they are called <em>homographs</em>.</p><p data-type="footnote" id="idm45634181919512"><sup><a href="ch10.xhtml#idm45634181919512-marker">5</a></sup> For example, from <a href="https://oreil.ly/two0R">RaRe Technologies</a> and <a href="https://oreil.ly/4DwDy">3Top</a>.</p><p data-type="footnote" id="idm45634180172056"><sup><a href="ch10.xhtml#idm45634180172056-marker">6</a></sup> If you run the code yourself, the results may be slightly different than the ones printed in the book because of random initialization.</p><p data-type="footnote" id="idm45634179916184"><sup><a href="ch10.xhtml#idm45634179916184-marker">7</a></sup> You’ll find the colorized figures in the electronic versions and on <a href="https://oreil.ly/MWJLd">GitHub</a>.</p></div></div></section></div>



  </body></html>