["```py\n$ mkdir stocks && cd stocks\n$ url=\"https://raw.githubusercontent.com/ \\\n sryza/aas/master/ch09-risk/data/stocks.zip\"\n$ wget $url\n$ unzip stocks.zip\n```", "```py\n$ cd .. && mkdir factors && cd factors\n$ url2 = \"https://raw.githubusercontent.com/ \\\n sryza/aas/master/ch09-risk/data/factors.zip\"\n$ wget $url2\n$ unzip factors.zip\n$ ls factors\n...\n\nNASDAQ%3ATLT.csv  NYSEARCA%3ACRED.csv  NYSEARCA%3AGLD.csv\n```", "```py\n$ !head -n 5 data/factors/NASDAQ%3ATLT.csv\n...\n\nDate,Open,High,Low,Close,Volume\n31-Dec-13,102.29,102.55,101.17,101.86,7219195\n30-Dec-13,102.15,102.58,102.08,102.51,4491711\n27-Dec-13,102.07,102.31,101.69,101.81,4755262\n26-Dec-13,102.35,102.36,102.01,102.10,4645323\n24-Dec-13,103.23,103.35,102.80,102.83,4897009\n```", "```py\n$ !head -n 5 data/stocks/GOOGL.csv\n...\n\nDate,Open,High,Low,Close,Volume\n31-Dec-13,556.68,561.06,553.68,560.92,1358300\n30-Dec-13,560.73,560.81,555.06,555.28,1236709\n27-Dec-13,560.56,560.70,557.03,559.76,1570140\n26-Dec-13,557.56,560.06,554.90,559.29,1338507\n24-Dec-13,558.04,558.18,554.60,556.48,734170\n```", "```py\n$ pyspark --driver-memory 4g\n```", "```py\nstocks = spark.read.csv(\"data/stocks/\", header='true', inferSchema='true')\n\nstocks.show(2)\n...\n\n+----------+----+----+----+-----+------+\n|      Date|Open|High| Low|Close|Volume|\n+----------+----+----+----+-----+------+\n|2013-12-31|4.40|4.48|3.92| 4.07|561247|\n|2013-12-30|3.93|4.42|3.90| 4.38|550358|\n+----------+----+----+----+-----+------+\n```", "```py\nfrom pyspark.sql import functions as fun\n\nstocks = stocks.withColumn(\"Symbol\", fun.input_file_name()).\\\n                withColumn(\"Symbol\",\n                  fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).\\\n                withColumn(\"Symbol\",\n                  fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n\nstocks.show(2)\n...\n+---------+-------+-------+-------+-------+------+------+\n|     Date|   Open|   High|    Low|  Close|Volume|Symbol|\n+---------+-------+-------+-------+-------+------+------+\n|31-Dec-13|1884.00|1900.00|1880.00| 1900.0|   546|  CLDN|\n|30-Dec-13|1889.00|1900.00|1880.00| 1900.0|  1656|  CLDN|\n+---------+-------+-------+-------+-------+------+------+\n```", "```py\nfactors = spark.read.csv(\"data/factors\", header='true', inferSchema='true')\nfactors = factors.withColumn(\"Symbol\", fun.input_file_name()).\\\n                  withColumn(\"Symbol\",\n                    fun.element_at(fun.split(\"Symbol\", \"/\"), -1)).\\\n                  withColumn(\"Symbol\",\n                    fun.element_at(fun.split(\"Symbol\", \"\\.\"), 1))\n```", "```py\nfrom pyspark.sql import Window\n\nstocks = stocks.withColumn('count', fun.count('Symbol').\\\n                over(Window.partitionBy('Symbol'))).\\\n                filter(fun.col('count') > 260*5 + 10)\n```", "```py\nstocks = stocks.withColumn('Date',\n                  fun.to_date(fun.to_timestamp(fun.col('Date'),\n                                              'dd-MM-yy')))\nstocks.printSchema()\n...\nroot\n |-- Date: date (nullable = true)\n |-- Open: string (nullable = true)\n |-- High: string (nullable = true)\n |-- Low: string (nullable = true)\n |-- Close: double (nullable = true)\n |-- Volume: string (nullable = true)\n |-- Symbol: string (nullable = true)\n |-- count: long (nullable = false)\n```", "```py\nfrom datetime import datetime\n\nstocks = stocks.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\\n                filter(fun.col('Date') <= datetime(2014, 10, 23))\n```", "```py\nfactors = factors.withColumn('Date',\n                              fun.to_date(fun.to_timestamp(fun.col('Date'),\n                                                          'dd-MMM-yy')))\n\nfactors = factors.filter(fun.col('Date') >= datetime(2009, 10, 23)).\\\n                  filter(fun.col('Date') <= datetime(2014, 10, 23))\n```", "```py\nstocks_pd_df = stocks.toPandas()\nfactors_pd_df = factors.toPandas()\n\nfactors_pd_df.head(5)\n...\n \tDate \tOpen \tHigh \tLow \tClose \tVolume \tSymbol\n0 \t2013-12-31 \t102.29 \t102.55 \t101.17 \t101.86 \t7219195\n    NASDAQ%253ATLT\n1 \t2013-12-30 \t102.15 \t102.58 \t102.08 \t102.51 \t4491711\n    NASDAQ%253ATLT\n2 \t2013-12-27 \t102.07 \t102.31 \t101.69 \t101.81 \t4755262\n    NASDAQ%253ATLT\n3 \t2013-12-26 \t102.35 \t102.36 \t102.01 \t102.10 \t4645323\n    NASDAQ%253ATLT\n4 \t2013-12-24 \t103.23 \t103.35 \t102.80 \t102.83 \t4897009\n    NASDAQ%253ATLT\n```", "```py\nn_steps = 10\ndef my_fun(x):\n    return ((x.iloc[-1] - x.iloc[0]) / x.iloc[0])\n\nstock_returns = stocks_pd_df.groupby('Symbol').Close.\\\n                            rolling(window=n_steps).apply(my_fun)\nfactors_returns = factors_pd_df.groupby('Symbol').Close.\\\\\n                            rolling(window=n_steps).apply(my_fun)\n\nstock_returns = stock_returns.reset_index().\\\n                              sort_values('level_1').\\\n                              reset_index()\nfactors_returns = factors_returns.reset_index().\\\n                                  sort_values('level_1').\\\n                                  reset_index()\n```", "```py\n# Create combined stocks DF\nstocks_pd_df_with_returns = stocks_pd_df.\\\n                              assign(stock_returns = \\\n                                    stock_returns['Close'])\n\n# Create combined factors DF\nfactors_pd_df_with_returns = factors_pd_df.\\\n                              assign(factors_returns = \\\n                                    factors_returns['Close'],\n                                    factors_returns_squared = \\\n                                    factors_returns['Close']**2)\n\nfactors_pd_df_with_returns = factors_pd_df_with_returns.\\\n                                pivot(index='Date',\n                                      columns='Symbol',\n                                      values=['factors_returns', \\\n                                              'factors_returns_squared']) ![1](assets/1.png)\n\nfactors_pd_df_with_returns.columns = factors_pd_df_with_returns.\\\n                                        columns.\\\n                                        to_series().\\\n                                        str.\\\n                                        join('_').\\\n                                        reset_index()[0]  ![2](assets/2.png)\n\nfactors_pd_df_with_returns = factors_pd_df_with_returns.\\\n                                reset_index()\n\nprint(factors_pd_df_with_returns.head(1))\n...\n0        Date  factors_returns_NASDAQ%253ATLT  \\ 0  2009-10-23                         0.01834\n\n0  factors_returns_NYSEARCA%253ACRED\n0                          -0.006594\n\n0 factors_returns_NYSEARCA%253AGLD  \\ 0                       - 0.032623\n\n0  factors_returns_squared_NASDAQ%253ATLT  \\ 0                                0.000336\n\n0  factors_returns_squared_NYSEARCA%253ACRED  \\ 0                                   0.000043\n\n0  factors_returns_squared_NYSEARCA%253AGLD\n0                                  0.001064\n...\n\nprint(factors_pd_df_with_returns.columns)\n...\nIndex(['Date', 'factors_returns_NASDAQ%253ATLT',\n       'factors_returns_NYSEARCA%253ACRED', 'factors_returns_NYSEARCA%253AGLD',\n       'factors_returns_squared_NASDAQ%253ATLT',\n       'factors_returns_squared_NYSEARCA%253ACRED',\n       'factors_returns_squared_NYSEARCA%253AGLD'],\n      dtype='object', name=0)\n...\n```", "```py\nfrom sklearn.linear_model import LinearRegression\n\n# For each stock, create input DF for linear regression training\n\nstocks_factors_combined_df = pd.merge(stocks_pd_df_with_returns,\n                                      factors_pd_df_with_returns,\n                                      how=\"left\", on=\"Date\")\n\nfeature_columns = list(stocks_factors_combined_df.columns[-6:])\n\nwith pd.option_context('mode.use_inf_as_na', True):\n    stocks_factors_combined_df = stocks_factors_combined_df.\\\n                                    dropna(subset=feature_columns \\\n                                            + ['stock_returns'])\n\ndef find_ols_coef(df):\n    y = df[['stock_returns']].values\n    X = df[feature_columns]\n\n    regr = LinearRegression()\n    regr_output = regr.fit(X, y)\n\n    return list(df[['Symbol']].values[0]) + \\\n                list(regr_output.coef_[0])\n\ncoefs_per_stock = stocks_factors_combined_df.\\\n                      groupby('Symbol').\\\n                      apply(find_ols_coef)\n\ncoefs_per_stock = pd.DataFrame(coefs_per_stock).reset_index()\ncoefs_per_stock.columns = ['symbol', 'factor_coef_list']\n\ncoefs_per_stock = pd.DataFrame(coefs_per_stock.\\\n                                factor_coef_list.tolist(),\n                                index=coefs_per_stock.index,\n                                columns = ['Symbol'] + feature_columns)\n\ncoefs_per_stock\n```", "```py\nsamples = factors_returns.loc[factors_returns.Symbol == \\\n                              factors_returns.Symbol.unique()[0]]['Close']\nsamples.plot.kde()\n```", "```py\nf_1 = factors_returns.loc[factors_returns.Symbol == \\\n                          factors_returns.Symbol.unique()[0]]['Close']\nf_2 = factors_returns.loc[factors_returns.Symbol == \\\n                          factors_returns.Symbol.unique()[1]]['Close']\nf_3 = factors_returns.loc[factors_returns.Symbol == \\\n                          factors_returns.Symbol.unique()[2]]['Close']\n\npd.DataFrame({'f1': list(f_1), 'f2': list(f_2), 'f3': list(f_3)}).corr()\n...\n\n         f1 \t   f2 \t    f3\nf1 \t1.000000 \t0.530550 \t0.074578\nf2 \t0.530550 \t1.000000 \t0.206538\nf3 \t0.074578 \t0.206538 \t1.000000\n```", "```py\nfactors_returns_cov = pd.DataFrame({'f1': list(f_1),\n                                    'f2': list(f_2[:-1]),\n                                    'f3': list(f_3[:-2])})\\\n                                    .cov().to_numpy()\nfactors_returns_mean = pd.DataFrame({'f1': list(f_1),\n                                     'f2': list(f_2[:-1]),\n                                     'f3': list(f_3[:-2])}).\\\n                                     mean()\n```", "```py\nfrom numpy.random import multivariate_normal\n\nmultivariate_normal(factors_returns_mean, factors_returns_cov)\n...\narray([ 0.02234821,  0.01838763, -0.01107748])\n```", "```py\nrandom_seed = 1496\ninstruments_dF = ...\ndef trialLossesForInstrument(seed, instrument):\n  ...\n\ninstruments_DF.rdd.\\\n  flatMap(trialLossesForInstrument(random_seed, _)).\\\n  reduceByKey(_ + _)\n```", "```py\nb_coefs_per_stock = spark.sparkContext.broadcast(coefs_per_stock)\nb_feature_columns = spark.sparkContext.broadcast(feature_columns)\nb_factors_returns_mean = spark.sparkContext.broadcast(factors_returns_mean)\nb_factors_returns_cov = spark.sparkContext.broadcast(factors_returns_cov)\n```", "```py\nfrom pyspark.sql.types import IntegerType\n\nparallelism = 1000\nnum_trials = 1000000\nbase_seed = 1496\n\nseeds = [b for b in range(base_seed,\n                          base_seed + parallelism)]\nseedsDF = spark.createDataFrame(seeds, IntegerType())\n\nseedsDF = seedsDF.repartition(parallelism)\n```", "```py\nimport random\n\nfrom pyspark.sql.types import LongType, ArrayType\nfrom pyspark.sql.functions import udf\n\ndef calculate_trial_return(x):\n#     return x\n    trial_return_list = []\n\n    for i in range(num_trials/parallelism):\n        random_int = random.randint(0, num_trials*num_trials)\n\n        seed(x)\n\n        random_factors = multivariate_normal(b_factors_returns_mean.value,\n          b_factors_returns_cov.value)\n\n        coefs_per_stock_df = b_coefs_per_stock.value\n        returns_per_stock = coefs_per_stock_df[b_feature_columns.value] *\n          (list(random_factors) + list(random_factors**2))\n\n        trial_return_list.append(float(returns_per_stock.sum(axis=1).sum()/b_coefs_\n          per_stock.value.size))\n\n    return trial_return_list\n\nudf_return = udf(calculate_trial_return, ArrayType(DoubleType()))\n```", "```py\nfrom pyspark.sql.functions import col, explode\n\ntrials = seedsDF.withColumn(\"trial_return\", udf_return(col(\"value\")))\ntrials = trials.select('value', explode('trial_return')) ![1](assets/1.png)\n\ntrials.cache()\n```", "```py\ntrials.approxQuantile('trial_return', [0.05], 0.0)\n...\n-0.010831826593164014\n```", "```py\ntrials.orderBy(col('trial_return').asc()).\\\n  limit(int(trials.count()/20)).\\\n  agg(fun.avg(col(\"trial_return\"))).show()\n...\n+--------------------+\n|   avg(trial_return)|\n+--------------------+\n|-0.09002629251426077|\n+--------------------+\n```", "```py\ntrials.plot.kde()\n```"]