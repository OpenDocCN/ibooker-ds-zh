- en: Chapter 6\. Logistic Regression and Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we are going to cover *logistic regression*, a type of regression
    that predicts a probability of an outcome given one or more independent variables.
    This in turn can be used for *classification*, which is predicting categories
    rather than real numbers as we did with linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: We are not always interested in representing variables as *continuous*, where
    they can represent an infinite number of real decimal values. There are situations
    where we would rather variables be *discrete*, or representative of whole numbers,
    integers, or booleans (1/0, true/false). Logistic regression is trained on an
    output variable that is discrete (a binary 1 or 0) or a categorical number (which
    is a whole number). It does output a continuous variable in the form of probability,
    but that can be converted into a discrete value with a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is easy to implement and fairly resilient against outliers
    and other data challenges. Many machine learning problems can best be solved with
    logistic regression, offering more practicality and performance than other types
    of supervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Just like we did in [Chapter 5](ch05.xhtml#ch05) when we covered linear regression,
    we will attempt to walk the line between statistics and machine learning, using
    tools and analysis from both disciplines. Logistic regression will integrate many
    concepts we have learned from this book, from probability to linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine there was a small industrial accident and you are trying to understand
    the impact of chemical exposure. You have 11 patients who were exposed for differing
    numbers of hours to this chemical (please note this is fabricated data). Some
    have shown symptoms (value of 1) and others have not shown symptoms (value of
    0). Let’s plot them in [Figure 6-1](#ipsJABMLqW), where the x-axis is hours of
    exposure and the y-axis is whether or not (1 or 0) they have showed symptoms.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0601](Images/emds_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Plotting whether patients showed symptoms (1) or not (0) over *x*
    hours of exposure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At what length of time do patients start showing symptoms? Well it is easy to
    see at almost four hours, we immediately transition from patients not showing
    symptoms (0) to showing symptoms (1). In [Figure 6-2](#LTFTchtBPw), we see the
    same data with a predictive curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0602](Images/emds_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. After four hours, we see a clear jump where patients start showing
    symptoms
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Doing a cursory analysis on this sample, we can say that there is nearly 0%
    probability a patient exposed for fewer than four hours will show symptoms, but
    there is 100% probability for greater than four hours. Between these two groups,
    there is an immediate jump to showing symptoms at approximately four hours.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, nothing is ever this clear-cut in the real world. Let’s say you gathered
    more data, where the middle of the range has a mix of patients showing symptoms
    versus not showing symptoms as shown in [Figure 6-3](#nlRTQIvpKn).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0603](Images/emds_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. A mix of patients who show symptoms (1) and do not show symptoms
    (0) exists in the middle
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The way to interpret this is the probability of patients showing symptoms gradually
    increases with each hour of exposure. Let’s visualize this with a *logistic function*,
    or an S-shaped curve where the output variable is squeezed between 0 and 1, as
    shown in [Figure 6-4](#nDHvgvHkOC).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0604](Images/emds_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Fitting a logistic function to the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because of this overlap of points in the middle, there is no distinct cutoff
    when patients show symptoms but rather a gradual transition from 0% probability
    to 100% probability (0 and 1). This example demonstrates how a *logistic regression*
    results in a curve indicating a probability of belonging to the true category
    (a patient showed symptoms) across an independent variable (hours of exposure).
  prefs: []
  type: TYPE_NORMAL
- en: We can repurpose a logistic regression to not just predict a probability for
    given input variables but also add a threshold to predict whether it belongs to
    that category. For example, if I get a new patient and find they have been exposed
    for six hours, I predict a 71.1% chance they will show symptoms as traced in [Figure 6-5](#WgtUGPOfFr).
    If my threshold is at least 50% probability to show symptoms, I will simply classify
    that the patient will show symptoms.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0605](Images/emds_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. We can expect a patient exposed for six hours to be 71.1% likely
    to have symptoms, and because that’s greater than a threshold of 50% we predict
    they will show symptoms
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Performing a Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So how do we perform a logistic regression? Let’s first take a look at the logistic
    function and explore the math behind it.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *logistic function* is an S-shaped curve (also known as a *sigmoid curve*)
    that, for a given set of input variables, produces an output variable between
    0 and 1\. Because the output variable is between 0 and 1 it can be used to represent
    a probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the logistic function that outputs a probability *y* for one input
    variable *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note this formula uses Euler’s number <math alttext="e"><mi>e</mi></math> ,
    which we covered in [Chapter 1](ch01.xhtml#ch01). The *x* variable is the independent/input
    variable. <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> and
    <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> are the coefficients
    we need to solve for.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math> are packaged inside an exponent resembling
    a linear function, which you may recall looks identical to <math alttext="y equals
    m x plus b"><mrow><mi>y</mi> <mo>=</mo> <mi>m</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
    or <math alttext="y equals beta 0 plus beta 1 x"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math>
    . This is not a coincidence; logistic regression actually has a close relationship
    to linear regression, which we will discuss later in this chapter. <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> indeed is the intercept (which we
    call <math alttext="b"><mi>b</mi></math> in a simple linear regression) and <math
    alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> is the slope for *x*
    (which we call <math alttext="m"><mi>m</mi></math> in a simple linear regression).
    This linear function in the exponent is known as the log-odds function, but for
    now just know this whole logistic function produces this S-shaped curve we need
    to output a shifting probability across an x-value.
  prefs: []
  type: TYPE_NORMAL
- en: To declare the logistic function in Python, use the `exp()` function from the
    `math` package to declare the *e* exponent as shown in [Example 6-1](#ikVeOTGjpQ).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. The logistic function in Python for one independent variable
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s plot to see what it looks like, and assume *Β*[0] = –2.823 and *Β*[1]
    = 0.62\. We will use SymPy in [Example 6-2](#WsJKESbiCj) and the output graph
    is shown in [Figure 6-6](#DllsJpEMCJ).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-2\. Using SymPy to plot a logistic function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![emds 0606](Images/emds_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. A logistic function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In some textbooks, you may alternatively see the logistic function declared
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p equals StartFraction e Superscript beta 0 plus beta 1 x Baseline
    Over 1 plus e Superscript beta 0 plus beta Baseline 1 x Baseline EndFraction"
    display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><msup><mi>e</mi> <mrow><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></msup>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo><mi>β</mi><mn>1</mn><mi>x</mi></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Do not fret about it, because it is the same function, just algebraically expressed
    differently. Note like linear regression we can also extend logistic regression
    to more than one input variable ( <math alttext="x 1 comma x 2 comma period period
    period x Subscript n Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <msub><mi>x</mi> <mi>n</mi></msub></mrow></math> ), as shown in this formula.
    We just add more <math alttext="beta Subscript x"><msub><mi>β</mi> <mi>x</mi></msub></math>
    coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p equals StartFraction 1 Over 1 plus e Superscript minus left-parenthesis
    beta 0 plus beta 1 x 1 plus beta 2 x 2 plus period period period beta Super Subscript
    n Superscript x Super Subscript n Superscript right-parenthesis Baseline EndFraction"
    display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>β</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the Logistic Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you fit the logistic curve to a given training dataset? First, the data
    can have any mix of decimal, integer, and binary variables, but the output variable
    must be binary (0 or 1). When we actually do prediction, the output variable will
    be between 0 and 1, resembling a probability.
  prefs: []
  type: TYPE_NORMAL
- en: The data provides our input and output variable values, but we need to solve
    for the <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math
    alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> coefficients to fit
    our logistic function. Recall how we used least squares in [Chapter 5](ch05.xhtml#ch05).
    However, this does not apply here. Instead we use *maximum likelihood estimation*,
    which, as the name suggests, maximizes the likelihood a given logistic curve would
    output the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the maximum likelihood estimation, there really is no closed form
    equation like in linear regression. We can still use gradient descent, or have
    a library do it for us. Let’s cover both of these approaches starting with the
    library SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: Using SciPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The nice thing about SciPy is the models often have a standardized set of functions
    and APIs, meaning in many cases you can copy/paste your code and can then reuse
    it between models. In [Example 6-3](#rOMnaFDjgv) you will see a logistic regression
    performed on our patient data. If you compare it to our linear regression code
    in [Chapter 5](ch05.xhtml#ch05), you will see it has nearly identical code in
    importing, separating, and fitting our data. The main difference is I use a `LogisticRegression()`
    for my model instead of a `LinearRegression()`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-3\. Using a plain logistic regression in SciPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After running the model in SciPy, I get a logistic regression where *β*[0] =
    –3.17576395 and *β*[1] = 0.69267212\. When I plot this, it should look pretty
    good as shown in [Figure 6-7](#VEMwqkqrSk).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0607](Images/emds_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Plotting the logistic regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are a couple of things to note here. When I created the `LogisticRegression()`
    model, I specified no `penalty` argument, which chooses a regularization technique
    like `l1` or `l2`. While this is beyond the scope of this book, I have included
    brief insights in the following note “Learning About SciPy Parameters” so that
    you have helpful references on hand.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I am going to `flatten()` the coefficient and intercept, which come
    out as multidimensional matrices but with one element. *Flattening* means collapsing
    a matrix of numbers into lesser dimensions, particularly when there are fewer
    elements than there are dimensions. For example, I use `flatten()` here to take
    a single number nested into a two-dimensional matrix and pull it out as a single
    value. I then have my <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Learning About SciPy Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SciPy offers a lot of options in its regression and classification models. Unfortunately,
    there is not enough bandwidth or pages to cover them as this is not a book focusing
    exclusively on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, the SciPy docs are well-written and the page on logistic regression
    is found [here](https://oreil.ly/eL8hZ).
  prefs: []
  type: TYPE_NORMAL
- en: If a lot of terms are unfamiliar, such as regularization and `l1` and `l2` penalties,
    there are other great O’Reilly books exploring these topics. One of the more helpful
    texts I have found is *Hands-On Machine Learning with Scikit-Learn, Keras, and
    TensorFlow* by Aurélien Géron.
  prefs: []
  type: TYPE_NORMAL
- en: Using Maximum Likelihood and Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As I have done throughout this book, I aim to provide insights on building techniques
    from scratch even if libraries can do it for us. There are several ways to fit
    a logistic regression ourselves, but all methods typically turn to maximum likelihood
    estimation (MLE). MLE maximizes the likelihood a given logistic curve would output
    the observed data. It is different than sum of squares, but we can still apply
    gradient descent or stochastic gradient descent to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll try to streamline the mathematical jargon and minimize the linear algebra
    here. Essentially, the idea is to find the <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    coefficients that bring our logistic curve to those points as closely as possible,
    indicating it is most likely to have produced those points. If you recall from
    [Chapter 2](ch02.xhtml#ch02) when we studied probability, we combine probabilities
    (or likelihoods) of multiple events by multiplying them together. In this application,
    we are calculating the likelihood we would see all these points for a given logistic
    regression curve.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the idea of joint probabilities, each patient has a likelihood they
    would show symptoms *based on the fitted logistic function* as shown in [Figure 6-8](#qVWERlmlnt).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0608](Images/emds_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Every input value has a corresponding likelihood on the logistic
    curve
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We fetch each likelihood off the logistic regression curve above or below each
    point. If the point is below the logistic regression curve, we need to subtract
    the resulting probability from 1.0 because we want to maximize the false cases
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Given coefficients *β*[0] = –3.17576395 and *β*[1] = 0.69267212, [Example 6-4](#aScjqhrufa)
    shows how we calculate the joint likelihood for this data in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-4\. Calculating the joint likelihood of observing all the points for
    a given logistic regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s a mathematical trick we can do to compress that `if` expression. As
    we covered in [Chapter 1](ch01.xhtml#ch01), when you set any number to the power
    of 0 it will always be 1\. Take a look at this formula and note the handling of
    true (1) and false (0) cases in the exponents:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>joint likelihood</mtext> <mo>=</mo> <munderover><mo>∏</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <msub><mi>y</mi>
    <mi>i</mi></msub></msup> <mo>×</mo> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <mrow><mn>1.0</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: To do this in Python, compress everything inside that `for` loop into [Example 6-5](#hwJLCkrAPp).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-5\. Compressing the joint likelihood calculation without an `if` expression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: What exactly did I do? Notice that there are two halves to this expression,
    one for when <math alttext="y equals 1"><mrow><mi>y</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    and the other where <math alttext="y equals 0"><mrow><mi>y</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    . When any number is raised to exponent 0, it will result in 1\. Therefore, whether
    *y* is 1 or 0, it will cause the opposite condition on the other side to evaluate
    to 1 and have no effect in multiplication. We get to express our `if` expression
    but do it completely in a mathematical expression. We cannot do derivatives on
    expressions that use `if`, so this will be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Note that computers can get overwhelmed multiplying several small decimals together,
    known as *floating point underflow*. This means that as decimals get smaller and
    smaller, which can happen in multiplication, the computer runs into limitations
    keeping track of that many decimal places. There is a clever mathematical hack
    to get around this. You can take the `log()` of each decimal you are multiplying
    and instead add them together. This is thanks to the additive properties of logarithms
    we covered in [Chapter 1](ch01.xhtml#ch01). This is more numerically stable, and
    you can then call the `exp()` function to convert the total sum back to get the
    product.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s revise our code to use logarithmic addition instead of multiplication
    (see [Example 6-6](#dNMlqaFFPW)). Note that the `log()` function will default
    to base *e* and while any base technically works, this is preferable because <math
    alttext="e Superscript x"><msup><mi>e</mi> <mi>x</mi></msup></math> is the derivative
    of itself and will computationally be more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-6\. Using logarithmic addition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To express the preceding Python code in mathematical notation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>joint likelihood</mtext> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mi mathvariant="italic">log</mi>
    <mrow><mo>(</mo> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow> <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>)</mo></mrow> <msub><mi>y</mi> <mi>i</mi></msub></msup> <mo>×</mo> <msup><mrow><mo>(</mo><mn>1.0</mn><mo>-</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <mrow><mn>1.0</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Would you like to calculate the partial derivatives for <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi>
    <mn>1</mn></msub></math> in the preceding expression? I didn’t think so. It’s
    a beast. Goodness, expressing that function in SymPy alone is a mouthful! Look
    at this in [Example 6-7](#QsBpdpBWda).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-7\. Expressing a joint likelihood for logistic regression in SymPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So let’s just allow SymPy to do the partial derivatives for us, for <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi>
    <mn>1</mn></msub></math> respectively. We will then immediately compile and use
    them for gradient descent, as shown in [Example 6-8](#aVIANjbpTT).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-8\. Using gradient descent on logistic regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: After calculating the partial derivatives for <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    , we substitute the x- and y-values as well as the number of data points *n*.
    Then we use `lambdify()` to compile the derivative function for efficiency (it
    uses NumPy behind the scenes). After that, we perform gradient descent like we
    did in [Chapter 5](ch05.xhtml#ch05), but since we are trying to maximize rather
    than minimize, we add each adjustment to <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    rather than subtract like in least squares.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Example 6-8](#aVIANjbpTT), we got *β*[0] = –3.17575 and *β*[1]
    = 0.692667\. This is highly comparable to the coefficient values we got in SciPy
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned to do in [Chapter 5](ch05.xhtml#ch05), we can also use stochastic
    gradient descent and only sample one or a handful of records on each iteration.
    This would extend the benefits of increasing computational speed and performance
    as well as prevent overfitting. It would be redundant to cover it again here,
    so we will keep moving on.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariable Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s try an example that uses logistic regression on multiple input variables.
    [Table 6-1](#ijGQdDBEmd) shows a sample of a few records from a fictitious dataset
    containing some employment-retention data (full dataset is [here](https://bit.ly/3aqsOMO)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Sample of employment-retention data
  prefs: []
  type: TYPE_NORMAL
- en: '| SEX | AGE | PROMOTIONS | YEARS_EMPLOYED | DID_QUIT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 32 | 3 | 7 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 34 | 2 | 5 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 29 | 2 | 5 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 42 | 4 | 10 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 43 | 4 | 10 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'There are 54 records in this dataset. Let’s say we want to use it to predict
    whether other employees are going to quit and logistic regression can be utilized
    here (although none of this is a good idea, and I will elaborate why later). Recall
    we can support more than one input variable as shown in this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals StartFraction 1 Over 1 plus e Superscript minus left-parenthesis
    beta 0 plus beta 1 x 1 plus beta 2 x 2 plus period period period beta Super Subscript
    n Superscript x Super Subscript n Superscript right-parenthesis Baseline EndFraction"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>β</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: I will create <math alttext="beta"><mi>β</mi></math> coefficients for each of
    the variables `sex`, `age`, `promotions`, and `years_employed`. The output variable
    `did_quit` is binary, and that is going to drive the logistic regression outcome
    we are predicting. Because we are dealing with multiple dimensions, it is going
    to be hard to visualize the curvy hyperplane that is our logistic curve. So we
    will steer clear from visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make it interesting. We will use scikit-learn but make an interactive
    shell we can test employees with. [Example 6-9](#GOoIdgKATe) shows the code, and
    when we run it, a logistic regression will be performed, and then we can type
    in new employees to predict whether they quit or not. What can go wrong? Nothing,
    I’m sure. We are only making predictions on people’s personal attributes and making
    decisions accordingly. I’m sure it will be fine.
  prefs: []
  type: TYPE_NORMAL
- en: (If it was not clear, I’m being very tongue in cheek).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-9\. Doing a multivariable logistic regression on employee data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 6-9](#PtmqbJijiV) shows the result whether an employee is predicted
    to quit. The employee is a sex “1,” age is 34, had 1 promotion, and has been at
    the company for 5 years. Sure enough, the prediction is “WILL LEAVE.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0609](Images/emds_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Making a prediction whether a 34-year-old employee with 1 promotion
    and 5 years, employment will quit
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the `predict_proba()` function will output two values, the first being
    the probability of 0 (false) and the second being 1 (true).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that the coefficients for `sex`, `age`, `promotions`, and `years_employed`
    are displayed in that order. By the weight of the coefficients, you can see that
    `sex` and `age` play very little role in the prediction (they both have a weight
    near 0). However, `promotions` and `years_employed` have significant weights of
    `–2.504` and `0.97`. Here’s a secret with this toy dataset: I fabricated it so
    that an employee quits if they do not get a promotion roughly every two years.
    Sure enough, my logistic regression picked up this pattern and you can try it
    out with other employees as well. However, if you venture outside the ranges of
    data it was trained on, the predictions will likely start falling apart (e.g.,
    if put in a 70-year-old employee who hasn’t been promoted in three years, it’s
    hard to say what this model will do since it has no data around that age).'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, real life is not always this clean. An employee who has been at a
    company for eight years and has never gotten a promotion is likely comfortable
    with their role and not leaving anytime soon. If that is the case, variables like
    age then might play a role and get weighted. Then of course we can get concerned
    about other relevant variables that are not being captured. See the following
    warning to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Be Careful Making Classifications on People!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A quick and surefire way to shoot yourself in the foot is to collect data on
    people and use it to make predictions haphazardly. Not only can data privacy concerns
    come about, but legal and PR issues can emerge if the model is found to be discriminatory.
    Input variables like race and gender can become weighted from machine learning
    training. After that, undesirable outcomes are inflicted on those demographics
    like not being hired or being denied loans. More extreme applications include
    being falsely flagged by surveillance systems or being denied criminal parole.
    Note too that seemingly benign variables like commute time have been found to
    correlate with discriminatory variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, a number of articles have been citing machine learning
    discrimination as an issue:'
  prefs: []
  type: TYPE_NORMAL
- en: Katyanna Quach, [“Teen turned away from roller rink after AI wrongly identifies
    her as banned troublemaker”](https://oreil.ly/boUcW), *The Register*, July 16,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kashmir Hill, [“Wrongfully Accused by an Algorithm”](https://oreil.ly/dOJyI),
    *New York Times*, June 24, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As data privacy laws continue to evolve, it is advisable to err on the side
    of caution and engineer personal data carefully. Think about what automated decisions
    will be propagated and how that can cause harm. Sometimes it is better to just
    leave a “problem” alone and keep doing it manually.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, on this employee-retention example, think about where this data came
    from. Yes, I made up this dataset but in the real world you always want to question
    what process created the data. Over what period of time did this sample come from?
    How far back do we go looking for employees who quit? What constitutes an employee
    who stayed? Are they current employees at this point in time? How do we know they
    are not about to quit, making them a false negative? Data scientists easily fall
    into traps analyzing only what data says, but not questioning where it came from
    and what assumptions are built into it.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to get answers to these questions is to understand what the predictions
    are being used for. Is it to decide when to give people promotions to retain them?
    Can this create a circular bias promoting people with a set of attributes? Will
    that bias be reaffirmed when those promotions start becoming the new training
    data?
  prefs: []
  type: TYPE_NORMAL
- en: These are all important questions, and perhaps even inconvenient ones that cause
    unwanted scope to creep into the project. If this scrutiny is not welcomed by
    your team or leadership on a project, consider empowering yourself with a different
    role where curiosity becomes a strength.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Log-Odds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, it is time to discuss the logistic regression and what it is
    mathematically made of. This can be a bit dizzying so take your time here. If
    you get overwhelmed, you can always revisit this section later.
  prefs: []
  type: TYPE_NORMAL
- en: Starting in the 1900s, it has always been of interest to mathematicians to take
    a linear function and scale its output to fall between 0 and 1, and therefore
    be useful for predicting probability. The log-odds, also called the logit function,
    lends itself to logistic regression for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember earlier I pointed out the exponent value <math alttext="beta 0 plus
    beta 1 x"><mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></math> is a linear function? Look at our logistic
    function again:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This linear function being raised to *e* is known as the *log-odds* function,
    which takes the logarithm of the odds for the event of interest. Your response
    might be, “Wait, I don’t see any `log()` or odds. I just see a linear function!”
    Bear with me, I will show the hidden math.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s use our logistic regression from earlier where *Β*[0]
    = -3.17576395 and *Β*[1] = 0.69267212\. What is the probability of showing symptoms
    after six hours, where <math alttext="x equals 6"><mrow><mi>x</mi> <mo>=</mo>
    <mn>6</mn></mrow></math> ? We already know how to do this: plug these values into
    our logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>6</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.727161542928554</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We plug in these values and output a probability of 0.72716\. But let’s look
    at this from an odds perspective. Recall in [Chapter 2](ch02.xhtml#ch02) we learned
    how to calculate odds from a probability:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>odds</mtext> <mo>=</mo> <mfrac><mi>p</mi>
    <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math><math display="block"><mrow><mtext>odds</mtext>
    <mo>=</mo> <mfrac><mrow><mn>.72716</mn></mrow> <mrow><mn>1</mn><mo>-</mo><mn>.72716</mn></mrow></mfrac>
    <mo>=</mo> <mn>2.66517246407876</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: So at six hours, a patient is 2.66517 times more likely to show symptoms than
    not show symptoms.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we wrap the odds function in a natural logarithm (a logarithm with base
    *e*), we call this the *logit function*. The output of this formula is what we
    call the *log-odds*, named…shockingly…because we take the logarithm of the odds:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>logit</mtext> <mo>=</mo> <mi>log</mi> <mo>(</mo>
    <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac> <mo>)</mo></mrow></math><math
    display="block"><mrow><mtext>logit</mtext> <mo>=</mo> <mi>log</mi> <mo>(</mo>
    <mfrac><mrow><mn>.72716</mn></mrow> <mrow><mn>1</mn><mo>-</mo><mn>.72716</mn></mrow></mfrac>
    <mo>)</mo> <mo>=</mo> <mn>0.98026877</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Our log-odds at six hours is 0.9802687\. What does this mean and why do we care?
    When we are in “log-odds land” it is easier to compare one set of odds against
    another. We treat anything greater than 0 as favoring odds an event will happen,
    whereas anything less than 0 is against an event. A log-odds of –1.05 is linearly
    the same distance from 0 as 1.05\. In plain odds, though, the equivalents are
    0.3499 and 2.857, respectively, which is not as interpretable. That is the convenience
    of log-odds.
  prefs: []
  type: TYPE_NORMAL
- en: Odds and Logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logarithms and odds have an interesting relationship. Odds are against an event
    when it is between 0.0 and 1.0, but anything greater than 1.0 favors the event
    and extends into positive infinity. This lack of symmetry is awkward. However,
    logarithms rescale an odds so that it is completely linear, where a log-odds of
    0.0 means fair odds. A log-odds of –1.05 is linearly the same distance from 0
    as 1.05, thus make comparing odds much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Josh Starmer has a [great video](https://oreil.ly/V0H8w) talking about this
    relationship between odds and logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall I said the linear function in our logistic regression formula <math
    alttext="beta 0 plus beta 1 x"><mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math> is our log-odds function.
    Check this out:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math>
    <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <mo>-</mo> <mn>3.17576395</mn>
    <mo>+</mo> <mn>0.69267212</mn> <mo>(</mo> <mn>6</mn> <mo>)</mo></mrow></math>
    <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <mn>0.98026877</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s the same value 0.98026877 as our previous calculation, the odds of our
    logistic regression at *x* = 6 and then taking the `log()` of it! So what is the
    link? What ties all this together? Given a probability from a logistic regression
    *p* and input variable *x*, it is this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="l o g left-parenthesis StartFraction p Over 1 minus p EndFraction
    right-parenthesis equals beta 0 plus beta 1 x" display="block"><mrow><mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Let’s plot the log-odds line alongside the logistic regression, as shown in
    [Figure 6-10](#PnVjoMFBlR).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0610](Images/emds_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The log-odds line is converted into a logistic function that outputs
    a probability
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Every logistic regression is actually backed by a linear function, and that
    linear function is a log-odds function. Note in [Figure 6-10](#PnVjoMFBlR) that
    when the log-odds is 0.0 on the line, then the probability of the logistic curve
    is at 0.5\. This makes sense because when our odds are fair at 1.0, the probability
    is going to be 0.50 as shown in the logistic regression, and the log-odds are
    going to be 0 as shown by the line.
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit we get looking at the logistic regression from an odds perspective
    is we can compare the effect between one x-value and another. Let’s say I want
    to understand how much my odds change between six hours and eight hours of exposure
    to the chemical. I can take the odds at six hours and then eight hours, and then
    ratio the two odds against each other in an *odds ratio*. This is not to be confused
    with a plain odds which, yes, is a ratio, but it is not an odds ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first find the probabilities of symptoms for six hours and eight hours,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math><math
    display="block"><mrow><msub><mi>p</mi> <mn>6</mn></msub> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>6</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.727161542928554</mn></mrow></math><math display="block"><mrow><msub><mi>p</mi>
    <mn>8</mn></msub> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow> <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>8</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.914167258137741</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s convert those into odds, which we will declare as <math alttext="o
    Subscript x"><msub><mi>o</mi> <mi>x</mi></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>o</mi> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math><math
    display="block"><mrow><msub><mi>o</mi> <mn>6</mn></msub> <mo>=</mo> <mfrac><mrow><mn>0.727161542928554</mn></mrow>
    <mrow><mn>1</mn><mo>-</mo><mn>0.727161542928554</mn></mrow></mfrac> <mo>=</mo>
    <mn>2.66517246407876</mn></mrow></math><math display="block"><mrow><msub><mi>o</mi>
    <mn>8</mn></msub> <mo>=</mo> <mfrac><mrow><mn>0.914167258137741</mn></mrow> <mrow><mn>1</mn><mo>-</mo><mn>0.914167258137741</mn></mrow></mfrac>
    <mo>=</mo> <mn>10.6505657200694</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, set the two odds against each other as an odds ratio, where the odds
    for eight hours is the numerator and the odds for six hours is in the denominator.
    We get a value of approximately 3.996, meaning that our odds of showing symptoms
    increases by nearly a factor of four with an extra two hours of exposure:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>odds ratio</mtext> <mo>=</mo> <mfrac><mrow><mn>10.6505657200694</mn></mrow>
    <mrow><mn>2.66517246407876</mn></mrow></mfrac> <mo>=</mo> <mn>3.99620132040906</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: You will find this odds ratio value of 3.996 holds across any two-hour range,
    like 2 hours to 4 hours, 4 hours to 6 hours, 8 hours to 10 hours, and so forth.
    As long as it’s a two-hour gap, you will find that odds ratio stays consistent.
    It will differ for other range lengths.
  prefs: []
  type: TYPE_NORMAL
- en: R-Squared
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered quite a few statistical metrics for linear regression in [Chapter 5](ch05.xhtml#ch05),
    and we will try to do the same for logistic regression. We still worry about many
    of the same problems as in linear regression, including overfitting and variance.
    As a matter of fact, we can borrow and adapt several metrics from linear regression
    and apply them to logistic regression. Let’s start with <math alttext="upper R
    squared"><msup><mi>R</mi> <mn>2</mn></msup></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Just like linear regression, there is an <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> for a given logistic regression. If you recall from [Chapter 5](ch05.xhtml#ch05),
    the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    indicates how well a given independent variable explains a dependent variable.
    Applying this to our chemical exposure problem, it makes sense we want to measure
    how much chemical exposure hours explains showing symptoms.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is not really a consensus on the best way to calculate the <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> on a logistic regression,
    but a popular technique known as McFadden’s Pseudo <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> closely mimics the <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> used in linear regression. We will use this technique
    in the following examples and here is the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R squared equals StartFraction left-parenthesis log likelihood
    right-parenthesis minus left-parenthesis log likelihood fit right-parenthesis
    Over left-parenthesis log likelihood right-parenthesis EndFraction" display="block"><mrow><msup><mi>R</mi>
    <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mo>)</mo><mo>-</mo><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mtext>fit</mtext><mo>)</mo></mrow>
    <mrow><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to calculate the “log likelihood fit” and “log likelihood”
    so we can calculate the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: We cannot use residuals here like in linear regression, but we can project the
    outcomes back onto the logistic curve as shown in [Figure 6-11](#nTgHGJUoVu),
    and look up their corresponding likelihoods between 0.0 and 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0611](Images/emds_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. Projecting the output values back onto the logistic curve
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can then take the `log()` of each of those likelihoods and sum them together.
    This will be the log likelihood of the fit ([Example 6-10](#oFlUCSCfPU)). Just
    like we did calculating maximum likelihood, we will convert the “false” likelihoods
    by subtracting from 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-10\. Calculating the log likelihood of the fit
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Using some clever binary multiplication and Python comprehensions, we can consolidate
    that `for` loop and `if` expression into one line that returns the `log_likelihood_fit`.
    Similar to what we did in the maximum likelihood formula, we can use some binary
    subtraction between the true and false cases to mathematically eliminate one or
    the other. In this case, we multiply by 0 and therefore apply either the true
    or the false case, but not both, to the sum accordingly ([Example 6-11](#etUMOEwqlo)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-11\. Consolidating our log likelihood logic into a single line
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to express the likelihood of the fit in mathematic notation, this
    is what it would look like. Note that <math alttext="f left-parenthesis x Subscript
    i Baseline right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is the logistic function for a given input variable <math
    alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>log likelihood fit</mtext> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mi mathvariant="italic">log</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>×</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <mi mathvariant="italic">log</mi> <mrow><mo>(</mo> <mn>1.0</mn> <mo>-</mo> <mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'As calculated in Examples [6-10](#oFlUCSCfPU) and [6-11](#etUMOEwqlo), we have
    -9.9461 as our log likelihood of the fit. We need one more datapoint to calculate
    the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    : the log likelihood that estimates without using any input variables and simply
    uses the number of true cases divided by all cases (effectively leaving only the
    intercept). Note we can count the number of symptomatic cases by summing all the
    y-values together <math alttext="sigma-summation y Subscript i"><mrow><mo>∑</mo>
    <msub><mi>y</mi> <mi>i</mi></msub></mrow></math> , because only the 1s and not
    the 0s will count into the sum. Here is the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="log likelihood equals StartFraction sigma-summation y Subscript
    i Baseline Over n EndFraction times y Subscript i Baseline plus left-parenthesis
    1 minus StartFraction sigma-summation y Subscript i Baseline Over n EndFraction
    right-parenthesis times left-parenthesis 1 minus y Subscript i Baseline right-parenthesis"
    display="block"><mrow><mtext>log</mtext> <mtext>likelihood</mtext> <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><msub><mi>y</mi> <mi>i</mi></msub></mrow> <mi>n</mi></mfrac>
    <mo>×</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><mo>∑</mo><msub><mi>y</mi> <mi>i</mi></msub></mrow> <mi>n</mi></mfrac>
    <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here is the expanded Python equivalent of this formula applied in [Example 6-12](#pBKtDOrDKa).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-12\. Log likelihood of patients
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To consolidate this logic and reflect the formula, we can compress that `for`
    loop and `if` expression into a single line, using some binary multiplication
    logic to handle both true and false cases ([Example 6-13](#pvdQHolacp)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-13\. Consolidating the log likelihood into a single line
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, just plug these values in and get your <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mtext>(log
    likelihood)</mtext><mo>-</mo><mtext>(log likelihood fit)</mtext></mrow> <mrow><mo>(</mo><mtext>log
    likelihood</mtext><mo>)</mo></mrow></mfrac></mrow></math><math display="block"><mrow><msup><mi>R</mi>
    <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mo>-</mo><mn>0.5596</mn><mo>-</mo><mo>(</mo><mo>-</mo><mn>9.9461</mn><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>0.5596</mn></mrow></mfrac> <msup><mi>R</mi> <mn>2</mn></msup>
    <mo>=</mo> <mn>0.306456</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: And here is the Python code shown in [Example 6-14](#gbVRCTIbOV), calculating
    the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    in its entirety.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-14\. Calculating the <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> for a logistic regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: OK, so we got an <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    = 0.306456, so do hours of chemical exposure explain whether someone shows symptoms?
    As we learned in [Chapter 5](ch05.xhtml#ch05) on linear regression, a poor fit
    will be closer to an <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    of 0.0 and a greater fit will be closer to 1.0\. Therefore, we can conclude that
    hours of exposure is mediocre for predicting symptoms, as the <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> is 0.30645\. There must be
    variables other than time exposure that better predict if someone will show symptoms.
    This makes sense because we have a large mix of patients showing symptoms versus
    not showing symptoms for most of our observed data, as shown in [Figure 6-12](#IPKLkNGRGb).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0612](Images/emds_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. Our data has a mediocre <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> of 0.30645 because there is a lot of variance in the
    middle of our curve
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But if we did have a clean divide in our data, where 1 and 0 outcomes are cleanly
    separated as shown in [Figure 6-13](#JPVUIsSBQP), we would have a perfect <math
    alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> of 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0613](Images/emds_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. This logistic regression has a perfect <math alttext="upper R
    squared"><msup><mi>R</mi> <mn>2</mn></msup></math> of 1.0 because there is a clean
    divide in outcomes predicted by hours of exposure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: P-Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like linear regression, we are not done just because we have an <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> . We need to investigate
    how likely we would have seen this data by chance rather than because of an actual
    relationship. This means we need a p-value.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we will need to learn a new probability distribution called the
    *chi-square distribution*, annotated as <math alttext="chi squared"><msup><mi>χ</mi>
    <mn>2</mn></msup></math> distribution. It is continuous and used in several areas
    of statistics, including this one!
  prefs: []
  type: TYPE_NORMAL
- en: If we take each value in a standard normal distribution (mean of 0 and standard
    deviation of 1) and square it, that will give us the <math alttext="chi squared"><msup><mi>χ</mi>
    <mn>2</mn></msup></math> distribution with one degree of freedom. For our purposes,
    the degrees of freedom will depend on how many parameters <math alttext="n"><mi>n</mi></math>
    are in our logistic regression, which will be <math alttext="n minus 1"><mrow><mi>n</mi>
    <mo>-</mo> <mn>1</mn></mrow></math> . You can see examples of different degrees
    of freedom in [Figure 6-14](#pmQLIIsNqF).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0614](Images/emds_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-14\. A <math alttext="chi squared"><msup><mi>χ</mi> <mn>2</mn></msup></math>
    distribution with differing degrees of freedom
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since we have two parameters (hours of exposure and whether symptoms were shown),
    our degree of freedom will be 1 because <math alttext="2 minus 1 equals 1"><mrow><mn>2</mn>
    <mo>-</mo> <mn>1</mn> <mo>=</mo> <mn>1</mn></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need the log likelihood fit and log likelihood as calculated in the
    previous subsection on R². Here is the formula that will produce the <math alttext="chi
    squared"><msup><mi>χ</mi> <mn>2</mn></msup></math> value we need to look up:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="chi squared equals 2 left-parenthesis log likelihood fit right-parenthesis
    minus left-parenthesis log likelihood right-parenthesis" display="block"><mrow><msup><mi>χ</mi>
    <mn>2</mn></msup> <mo>=</mo> <mn>2</mn> <mrow><mo>(</mo> <mtext>log</mtext> <mtext>likelihood</mtext>
    <mtext>fit</mtext> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mtext>log</mtext>
    <mtext>likelihood</mtext> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We then take that value and look up the probability from the <math alttext="chi
    squared"><msup><mi>χ</mi> <mn>2</mn></msup></math> distribution. That will give
    us our p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p hyphen value equals chi left-parenthesis 2 left-parenthesis
    left-parenthesis log likelihood fit right-parenthesis minus left-parenthesis log
    likelihood right-parenthesis right-parenthesis" display="block"><mrow><mtext>p-value</mtext>
    <mo>=</mo> <mtext>chi</mtext> <mo>(</mo> <mn>2</mn> <mo>(</mo> <mo>(</mo> <mtext>log</mtext>
    <mtext>likelihood</mtext> <mtext>fit</mtext> <mo>)</mo> <mo>-</mo> <mo>(</mo>
    <mtext>log</mtext> <mtext>likelihood</mtext> <mo>)</mo> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 6-15](#PPMuoBavDD) shows our p-value for a given fitted logistic regression.
    We use SciPy’s `chi2` module to use the chi-square distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-15\. Calculating a p-value for a given logistic regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So we have a p-value of 0.00166, and if our threshold for signifiance is .05,
    we say this data is statistically significant and was not by random chance.
  prefs: []
  type: TYPE_NORMAL
- en: Train/Test Splits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As covered in [Chapter 5](ch05.xhtml#ch05) on linear regression, we can use
    train/test splits as a way to validate machine learning algorithms. This is the
    more machine learning approach to assessing the performance of a logistic regression.
    While it is a good idea to rely on traditional statistical metrics like <math
    alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> and p-values,
    when you are dealing with more variables, this becomes less practical. This is
    where train/test splits come in handy once again. To review, [Figure 6-15](#HgfhjgkCrp2)
    visualizes a three-fold cross-validation alternating a testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0615](Images/emds_0615.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-15\. A three-fold cross-validation alternating each third of the dataset
    as a testing dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Example 6-16](#RHQQqqghLN) we perform a logistic regression on the employee-retention
    dataset, but we split the data into thirds. We then alternate each third as the
    testing data. Finally, we summarize the three accuracies with an average and standard
    deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-16\. Performing a logistic regression with three-fold cross-validation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can also use random-fold validation, leave-one-out cross-validation, and
    all the other folding variants we performed in [Chapter 5](ch05.xhtml#ch05). With
    that out of the way, let’s talk about why accuracy is a bad measure for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose a model observed people with the name “Michael” quit their job. The
    reason why first and last names are captured as input variables is indeed questionable,
    as it is doubtful someone’s name has any impact on whether they quit. However,
    to simplify the example, let’s go with it. The model then predicts that any person
    named “Michael” will quit their job.
  prefs: []
  type: TYPE_NORMAL
- en: Now this is where accuracy falls apart. I have one hundred employees, including
    one named “Michael” and another named “Sam.” Michael is wrongly predicted to quit,
    and it is Sam that ends up quitting. What’s the accuracy of my model? It is 98%
    because there were only two wrong predictions out of one hundred employees as
    visualized in [Figure 6-16](#SouBMsENVQ).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0616](Images/emds_0616.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-16\. The employee named “Michael” is predicted to quit, but it’s actually
    another employee that does, giving us 98% accuracy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Especially for imbalanced data where the event of interest (e.g., a quitting
    employee) is rare, the accuracy metric is horrendously misleading for classification
    problems. If a vendor, consultant, or data scientist ever tries to sell you a
    classification system on claims of accuracy, ask for a confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: A *confusion matrix* is a grid that breaks out the predictions against the actual
    outcomes showing the true positives, true negatives, false positives (type I error),
    and false negatives (type II error). Here is a confusion matrix presented in [Figure 6-17](#sVsaaKoGnR).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0617](Images/emds_0617.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-17\. A simple confusion matrix
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generally, we want the diagonal values (top-left to bottom-right) to be higher
    because these reflect correct classifications. We want to evaluate how many employees
    who were predicted to quit actually did quit (true positives). Conversely, we
    also want to evaluate how many employees who were predicted to stay actually did
    stay (true negatives).
  prefs: []
  type: TYPE_NORMAL
- en: The other cells reflect wrong predictions, where an employee predicted to quit
    ended up staying (false positive), and where an employee predicted to stay ends
    up quitting (false negative).
  prefs: []
  type: TYPE_NORMAL
- en: What we need to do is dice up that accuracy metric into more specific accuracy
    metrics targeting different parts of the confusion matrix. Let’s look at [Figure 6-18](#mbgQPeoPAQ),
    which adds some useful measures.
  prefs: []
  type: TYPE_NORMAL
- en: From the confusion matrix, we can derive all sorts of useful metrics beyond
    just accuracy. We can easily see that precision (how accurate positive predictions
    were) and sensitivity (rate of identified positives) are 0, meaning this machine
    learning model fails entirely at positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0618](Images/emds_0618.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-18\. Adding useful metrics to the confusion matrix
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Example 6-17](#jsciVCfjkQ) shows how to use the confusion matrix API in SciPy
    on a logistic regression with a train/test split. Note that the confusion matrix
    is only applied to the testing dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-17\. Creating a confusion matrix for a testing dataset in SciPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Bayes’ Theorem and Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do you recall Bayes’ Theorem in [Chapter 2](ch02.xhtml#ch02)? You can use Bayes’
    Theorem to bring in outside information to further validate findings on a confusion
    matrix. [Figure 6-19](#AKsSBkvTTJ) shows a confusion matrix of one thousand patients
    tested for a disease.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0619](Images/emds_0619.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-19\. A confusion matrix for a medical test identifying a disease
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We are told that for patients that have a health risk, 99% will be identified
    successfully (sensitivity). Using the confusion matrix, we can see this mathematically
    checks out:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>sensitivity</mtext> <mo>=</mo> <mfrac><mn>198</mn>
    <mrow><mn>198</mn><mo>+</mo><mn>2</mn></mrow></mfrac> <mo>=</mo> <mn>.99</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if we flip the condition? What percentage of those who tested positive
    have the health risk (precision)? While we are flipping a conditional probability,
    we do not have to use Bayes’ Theorem here because the confusion matrix gives us
    all the numbers we need:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>precision</mtext> <mo>=</mo> <mfrac><mn>198</mn>
    <mrow><mn>198</mn><mo>+</mo><mn>50</mn></mrow></mfrac> <mo>=</mo> <mn>.798</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: OK, so 79.8% is not terrible, and that’s the percentage of people who tested
    positive that actually have the disease. But ask yourself this…what are we assuming
    about our data? Is it representative of the population?
  prefs: []
  type: TYPE_NORMAL
- en: Some quick research found 1% of the population actually has the disease. There
    is an opportunity to use Bayes’ Theorem here. We can account for the proportion
    of the population that actually has the disease and incorporate that into our
    confusion matrix findings. We then discover something significant.
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>At Risk if Positive</mtext>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mtext>Positive
    if At Risk</mtext><mo>)</mo><mo>×</mo><mi>P</mi><mtext>(At Risk)</mtext></mrow>
    <mrow><mi>P</mi><mo>(</mo><mtext>Positive</mtext><mo>)</mo></mrow></mfrac></mrow></math><math
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>At Risk if Positive</mtext>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mn>.99</mn><mo>×</mo><mn>.01</mn></mrow>
    <mrow><mn>.248</mn></mrow></mfrac></mrow></math><math display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>At Risk if Positive</mtext> <mo>)</mo> <mo>=</mo> <mn>.0339</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: When we account for the fact that only 1% of the population is at risk, and
    20% of our test patients are at risk, the probability of being at risk given a
    positive test is 3.39%! How did it drop from 99%? This just shows how easily we
    can get duped by probabilities that are high only in a specific sample like the
    vendor’s one thousand test patients. So if this test has only a 3.39% probability
    of successfully identifying a true positive, we probably should not use it.
  prefs: []
  type: TYPE_NORMAL
- en: Receiver Operator Characteristics/Area Under Curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are evaluating different machine learning configurations, we may end
    up with dozens, hundreds, or thousands of confusion matrices. These can be tedious
    to review, so we can summarize all of them with a *receiver operator characteristic
    (ROC) curve* as shown in [Figure 6-20](#QCgoMOWuNR). This allows us to see each
    testing instance (each represented by a black dot) and find an agreeable balance
    between true positives and false positives.
  prefs: []
  type: TYPE_NORMAL
- en: We can also compare different machine learning models by creating separate ROC
    curves for each. For example, if in [Figure 6-21](#VNjNDUrmFa) our top curve represents
    a logistic regression and the bottom curve represents a decision tree (a machine
    learning technique we did not cover in this book), we can see the performance
    of them side by side. The *area under the curve (AUC)* is a good metric for choosing
    which model to use. Since the top curve (logistic regression) has a greater area,
    this suggests it is a superior model.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0620](Images/emds_0620.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-20\. A receiver operator characteristic curve
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![emds 0621](Images/emds_0621.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-21\. Comparing two models by their area under the curve (AUC) with
    their respective ROC curves
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To use the AUC as a scoring metric, change the `scoring` parameter in the scikit-learn
    API to use `roc_auc` as shown for a cross-validation in [Example 6-18](#OIHKJooDId).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-18\. Using the AUC as the scikit-learn parameter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Class Imbalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is one last thing to cover before we close this chapter. As we saw earlier
    when discussing confusion matrices, *class imbalance*, which happens when data
    is not equally represented across every outcome class, is a problem in machine
    learning. Unfortunately, many problems of interest are imbalanced, such as disease
    prediction, security breaches, fraud detection, and so on. Class imbalance is
    still an open problem with no great solution. However, there are a few techniques
    you can try.
  prefs: []
  type: TYPE_NORMAL
- en: First, you can do obvious things like collect more data or try different models
    as well as use confusion matrices and ROC/AUC curves. All of this will help track
    poor predictions and proactively catch errors.
  prefs: []
  type: TYPE_NORMAL
- en: Another common technique is to duplicate samples in the minority class until
    it is equally represented in the dataset. You can do this in scikit-learn as shown
    in [Example 6-19](#NUGnKQArce) when doing your train-test splits. Pass the `stratify`
    option with the column containing the class values, and it will attempt to equally
    represent the data of each class.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-19\. Using the `stratify` option in scikit-learn to balance classes
    in the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: There is also a family of algorithms called SMOTE, which generate synthetic
    samples of the minority class. What would be most ideal though is to tackle the
    problem in a way that uses anomaly-detection models, which are deliberately designed
    for seeking out a rare event. These seek outliers, however, and are not necessarily
    a classification since they are unsupervised algorithms. All these techniques
    are beyond the scope of this book but are worth mentioning as they *might* provide
    better solutions to a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is the workhorse model for predicting probabilities and
    classifications on data. Logistic regressions can predict more than one category
    rather than just a true/false. You just build separate logistic regressions modeling
    whether or not it belongs to that category, and the model that produces the highest
    probability is the one that wins. You may discover that scikit-learn, for the
    most part, will do this for you and detect when your data has more than two classes.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we covered not just how to fit a logistic regression using
    gradient descent and scikit-learn but also statistical and machine learning approaches
    to validation. On the statistical front we covered the <math alttext="upper R
    squared"><msup><mi>R</mi> <mn>2</mn></msup></math> and p-value, and in machine
    learning we explored train/test splits, confusion matrices, and ROC/AUC.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about logistic regression, probably the best resource
    to jump-start further is Josh Starmer’s StatQuest playlist on Logistic Regression.
    I have to credit Josh’s work in assisting some portions of this chapter, particularly
    in how to calculate <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    and p-values for logistic regression. If nothing else, watch his videos for the
    [fantastic opening jingles](https://oreil.ly/tueJJ)!
  prefs: []
  type: TYPE_NORMAL
- en: As always, you will find yourself walking between the two worlds of statistics
    and machine learning. Many books and resources right now cover logistic regression
    from a machine learning perspective, but try to seek out statistics resources
    too. There are advantages and disadvantages to both schools of thought, and you
    can win only by being adaptable to both!
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dataset of three input variables `RED`, `GREEN`, and `BLUE` as well as an
    output variable `LIGHT_OR_DARK_FONT_IND` is provided [here](https://bit.ly/3imidqa).
    It will be used to predict whether a light/dark font (0/1 respectively) will work
    for a given background color (specified by RGB values).
  prefs: []
  type: TYPE_NORMAL
- en: Perform a logistic regression on the preceding data, using three-fold cross-validation
    and accuracy as your metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Produce a confusion matrix comparing the predictions and actual data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick a few different background colors (you can use an RGB tool like [this one](https://bit.ly/3FHywrZ))
    and see if the logistic regression sensibly chooses a light (0) or dark (1) font
    for each one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the preceding exercises, do you think logistic regression is effective
    for predicting a light or dark font for a given background color?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers are in [Appendix B](app02.xhtml#exercise_answers).
  prefs: []
  type: TYPE_NORMAL
