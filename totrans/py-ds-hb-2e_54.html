<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 49. In Depth: Kernel Density Estimation" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0513-kernel-density-estimation">
<h1><span class="label">Chapter 49. </span>In Depth: Kernel Density Estimation</h1>
<p><a data-primary="kernel density estimation (KDE)" data-type="indexterm" id="ix_ch49-asciidoc0"/><a data-primary="machine learning" data-secondary="kernel density estimation" data-type="indexterm" id="ix_ch49-asciidoc1"/>In <a data-type="xref" href="ch48.xhtml#section-0512-gaussian-mixtures">Chapter 48</a> we covered Gaussian mixture models, which are a
kind of hybrid between a clustering estimator and a density estimator.
Recall that a density estimator is an algorithm that takes a
<math alttext="upper D">
<mi>D</mi>
</math>-dimensional dataset and produces an estimate of the
<math alttext="upper D">
<mi>D</mi>
</math>-dimensional probability distribution that data is drawn
from. <a data-primary="Gaussian mixture models (GMMs)" data-secondary="kernel density estimation and" data-type="indexterm" id="idm45858719806416"/>The GMM algorithm accomplishes this by representing the density as
a weighted sum of Gaussian distributions. <em>Kernel density estimation</em>
(KDE) is in some senses an algorithm that takes the mixture-of-Gaussians
idea to its logical extreme: it uses a mixture consisting of one
Gaussian component <em>per point</em>, resulting in an essentially
nonparametric estimator of density. In this chapter, we will explore the
motivation and uses of KDE.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>
<section data-pdf-bookmark="Motivating Kernel Density Estimation: Histograms" data-type="sect1"><div class="sect1" id="ch_0513-kernel-density-estimation_motivating-kernel-density-estimation-histograms">
<h1>Motivating Kernel Density Estimation: Histograms</h1>
<p><a data-primary="histograms" data-secondary="kernel density estimation and" data-type="indexterm" id="ix_ch49-asciidoc2"/><a data-primary="kernel density estimation (KDE)" data-secondary="histograms and" data-type="indexterm" id="ix_ch49-asciidoc3"/>As <a data-primary="density estimator" data-secondary="histogram as" data-type="indexterm" id="idm45858719757520"/>mentioned previously, a density estimator is an algorithm that seeks
to model the probability distribution that generated a dataset. For
one-dimensional data, you are probably already familiar with one simple
density estimator: the histogram. A histogram divides the data into
discrete bins, counts the number of points that fall in each bin, and
then visualizes the results in an intuitive manner.</p>
<p class="pagebreak-before less_space">For example, let’s create some data that is drawn from two
normal distributions:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">make_data</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="n">f</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">rseed</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
            <code class="n">rand</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="n">rseed</code><code class="p">)</code>
            <code class="n">x</code> <code class="o">=</code> <code class="n">rand</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">N</code><code class="p">)</code>
            <code class="n">x</code><code class="p">[</code><code class="nb">int</code><code class="p">(</code><code class="n">f</code> <code class="o">*</code> <code class="n">N</code><code class="p">):]</code> <code class="o">+=</code> <code class="mi">5</code>
            <code class="k">return</code> <code class="n">x</code>

        <code class="n">x</code> <code class="o">=</code> <code class="n">make_data</code><code class="p">(</code><code class="mi">1000</code><code class="p">)</code></pre>
<p>We have previously seen that the standard count-based histogram can be
created with the <code>plt.hist</code> function. By specifying the <code>density</code>
parameter of the histogram, we end up with a normalized histogram where
the height of the bins does not reflect counts, but instead reflects
probability density (see <a data-type="xref" href="#fig_0513-kernel-density-estimation_files_in_output_6_0">Figure 49-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">hist</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code> <code class="n">density</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0513-kernel-density-estimation_files_in_output_6_0">
<img alt="output 6 0" height="383" src="assets/output_6_0.png" width="600"/>
<h6><span class="label">Figure 49-1. </span>Data drawn from a combination of normal distributions</h6>
</div></figure>
<p>Notice that for equal binning, this normalization simply changes the
scale on the y-axis, leaving the relative heights essentially the same
as in a histogram built from counts. This normalization is chosen so
that the total area under the histogram is equal to 1, as we can confirm
by looking at the output of the histogram function:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">density</code><code class="p">,</code> <code class="n">bins</code><code class="p">,</code> <code class="n">patches</code> <code class="o">=</code> <code class="n">hist</code>
        <code class="n">widths</code> <code class="o">=</code> <code class="n">bins</code><code class="p">[</code><code class="mi">1</code><code class="p">:]</code> <code class="o">-</code> <code class="n">bins</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
        <code class="p">(</code><code class="n">density</code> <code class="o">*</code> <code class="n">widths</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="mf">1.0</code></pre>
<p class="pagebreak-before less_space">One of the issues with using a histogram as a density estimator is that
the choice of bin size and location can lead to representations that
have qualitatively different features. For example, if we look at a
version of this data with only 20 points, the choice of how to draw the
bins can lead to an entirely different interpretation of the data!
Consider this example, visualized in <a data-type="xref" href="#fig_0513-kernel-density-estimation_files_in_output_11_0">Figure 49-2</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">x</code> <code class="o">=</code> <code class="n">make_data</code><code class="p">(</code><code class="mi">20</code><code class="p">)</code>
        <code class="n">bins</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code>
                               <code class="n">sharex</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">sharey</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                               <code class="n">subplot_kw</code><code class="o">=</code><code class="p">{</code><code class="s1">'xlim'</code><code class="p">:(</code><code class="o">-</code><code class="mi">4</code><code class="p">,</code> <code class="mi">9</code><code class="p">),</code>
                                           <code class="s1">'ylim'</code><code class="p">:(</code><code class="o">-</code><code class="mf">0.02</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">)})</code>
        <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">wspace</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">offset</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">([</code><code class="mf">0.0</code><code class="p">,</code> <code class="mf">0.6</code><code class="p">]):</code>
            <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="n">bins</code> <code class="o">+</code> <code class="n">offset</code><code class="p">,</code> <code class="n">density</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
            <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">full_like</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.01</code><code class="p">),</code> <code class="s1">'|k'</code><code class="p">,</code>
                       <code class="n">markeredgewidth</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0513-kernel-density-estimation_files_in_output_11_0">
<img alt="output 11 0" height="200" src="assets/output_11_0.png" width="600"/>
<h6><span class="label">Figure 49-2. </span>The problem with histograms: the bin locations can affect interpretation</h6>
</div></figure>
<p>On the left, the histogram makes clear that this is a bimodal
distribution. On the right, we see a unimodal distribution with a long
tail. Without seeing the preceding code, you would probably not guess
that these two histograms were built from the same data. With that in
mind, how can you trust the intuition that histograms confer? And how
might we improve on this?</p>
<p>Stepping back, we can think of a histogram as a stack of blocks, where
we stack one block within each bin on top of each point in the dataset.
Let’s view this directly (see <a data-type="xref" href="#fig_0513-kernel-density-estimation_files_in_output_13_1">Figure 49-3</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">()</code>
        <code class="n">bins</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code>
        <code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">full_like</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1</code><code class="p">),</code> <code class="s1">'|k'</code><code class="p">,</code>
                <code class="n">markeredgewidth</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
        <code class="k">for</code> <code class="n">count</code><code class="p">,</code> <code class="n">edge</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="n">np</code><code class="o">.</code><code class="n">histogram</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">bins</code><code class="p">)):</code>
            <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">count</code><code class="p">):</code>
                <code class="n">ax</code><code class="o">.</code><code class="n">add_patch</code><code class="p">(</code><code class="n">plt</code><code class="o">.</code><code class="n">Rectangle</code><code class="p">(</code>
                    <code class="p">(</code><code class="n">edge</code><code class="p">,</code> <code class="n">i</code><code class="p">),</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">ec</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">))</code>
        <code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code>
        <code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="mf">0.2</code><code class="p">,</code> <code class="mi">8</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="p">(</code><code class="o">-</code><code class="mf">0.2</code><code class="p">,</code> <code class="mf">8.0</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0513-kernel-density-estimation_files_in_output_13_1">
<img alt="output 13 1" height="401" src="assets/output_13_1.png" width="600"/>
<h6><span class="label">Figure 49-3. </span>Histogram as stack of blocks</h6>
</div></figure>
<p>The problem with our two binnings stems from the fact that the height of
the block stack often reflects not the actual density of points nearby,
but coincidences of how the bins align with the data points. This
misalignment between points and their blocks is a potential cause of the
poor histogram results seen here. But what if, instead of stacking the
blocks aligned with the <em>bins</em>, we were to stack the blocks aligned with
the <em>points they represent</em>? If we do this, the blocks won’t
be aligned, but we can add their contributions at each location along
the x-axis to find the result. Let’s try this (see <a data-type="xref" href="#fig_0513-kernel-density-estimation_files_in_output_15_0">Figure 49-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">x_d</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">2000</code><code class="p">)</code>
        <code class="n">density</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">((</code><code class="nb">abs</code><code class="p">(</code><code class="n">xi</code> <code class="o">-</code> <code class="n">x_d</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.5</code><code class="p">)</code> <code class="k">for</code> <code class="n">xi</code> <code class="ow">in</code> <code class="n">x</code><code class="p">)</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">x_d</code><code class="p">,</code> <code class="n">density</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">full_like</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1</code><code class="p">),</code> <code class="s1">'|k'</code><code class="p">,</code> <code class="n">markeredgewidth</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">([</code><code class="o">-</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.2</code><code class="p">,</code> <code class="mi">8</code><code class="p">]);</code></pre>
<figure><div class="figure" id="fig_0513-kernel-density-estimation_files_in_output_15_0">
<img alt="output 15 0" height="401" src="assets/output_15_0.png" width="600"/>
<h6><span class="label">Figure 49-4. </span>A “histogram” where blocks center on each individual point; this is an example of a kernel density estimate</h6>
</div></figure>
<p>The result looks a bit messy, but it’s a much more robust
reflection of the actual data characteristics than is the standard
histogram. Still, the rough edges are not aesthetically pleasing, nor
are they reflective of any true properties of the data. In order to
smooth them out, we might decide to replace the blocks at each location
with a smooth function, like a Gaussian. Let’s use a
standard normal curve at each point instead of a block (see <a data-type="xref" href="#fig_0513-kernel-density-estimation_files_in_output_17_0">Figure 49-5</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">scipy.stats</code> <code class="kn">import</code> <code class="n">norm</code>
        <code class="n">x_d</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code>
        <code class="n">density</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">norm</code><code class="p">(</code><code class="n">xi</code><code class="p">)</code><code class="o">.</code><code class="n">pdf</code><code class="p">(</code><code class="n">x_d</code><code class="p">)</code> <code class="k">for</code> <code class="n">xi</code> <code class="ow">in</code> <code class="n">x</code><code class="p">)</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">x_d</code><code class="p">,</code> <code class="n">density</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">full_like</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1</code><code class="p">),</code> <code class="s1">'|k'</code><code class="p">,</code> <code class="n">markeredgewidth</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">([</code><code class="o">-</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.2</code><code class="p">,</code> <code class="mi">5</code><code class="p">]);</code></pre>
<figure><div class="figure" id="fig_0513-kernel-density-estimation_files_in_output_17_0">
<img alt="output 17 0" height="401" src="assets/output_17_0.png" width="600"/>
<h6><span class="label">Figure 49-5. </span>A kernel density estimate with a Gaussian kernel</h6>
</div></figure>
<p>This smoothed-out plot, with a Gaussian distribution contributed at the
location of each input point, gives a much more accurate idea of the
shape of the data distribution, and one that has much less variance
(i.e., changes much less in response to differences in sampling).</p>
<p>What we’ve landed on in the last two plots is
what’s called kernel density estimation in one dimension: we
have placed a “kernel”—a square or top hat–shaped kernel in the
former, a Gaussian kernel in the latter—at the location of each point,
and used their sum as an estimate of density. With this intuition in
mind, we’ll now explore kernel density estimation in more
detail.<a data-startref="ix_ch49-asciidoc3" data-type="indexterm" id="idm45858718916112"/><a data-startref="ix_ch49-asciidoc2" data-type="indexterm" id="idm45858718915408"/></p>
</div></section>
<section data-pdf-bookmark="Kernel Density Estimation in Practice" data-type="sect1"><div class="sect1" id="ch_0513-kernel-density-estimation_kernel-density-estimation-in-practice">
<h1>Kernel Density Estimation in Practice</h1>
<p><a data-primary="kernel (defined)" data-type="indexterm" id="idm45858718913472"/>The <a data-primary="kernel density estimation (KDE)" data-secondary="in practice" data-secondary-sortas="practice" data-type="indexterm" id="ix_ch49-asciidoc4"/>free parameters of kernel density estimation are the <em>kernel</em>, which
specifies the shape of the distribution placed at each point, and <a data-primary="kernel bandwidth" data-secondary="defined" data-type="indexterm" id="idm45858718887408"/>the
<em>kernel bandwidth</em>, which controls the size of the kernel at each point.
In practice, there are many kernels you might use for kernel density
estimation: in particular, the Scikit-Learn KDE implementation supports
six kernels, which you can read about in the
<a href="https://oreil.ly/2Ae4a">“Density
Estimation” section</a> of the documentation.</p>
<p>While there are several versions of KDE implemented in Python (notably
in the SciPy and <code>statsmodels</code> packages), I prefer to use
Scikit-Learn’s version because of its efficiency and
flexibility. It is implemented in the <code>sklearn.neighbors.KernelDensity</code>
estimator, which handles KDE in multiple dimensions with one of six
kernels and one of a couple dozen distance metrics. Because KDE can be
fairly computationally intensive, the Scikit-Learn estimator uses a
tree-based algorithm under the hood and can trade off computation time
for accuracy using the <code>atol</code> (absolute tolerance) and <code>rtol</code> (relative
tolerance) parameters. The kernel bandwidth can be determined using
Scikit-Learn’s standard cross-validation tools, as we will
soon see.</p>
<p>Let’s first show a simple example of replicating the
previous plot using the Scikit-Learn <code>KernelDensity</code> estimator (see <a data-type="xref" href="#fig_0513-kernel-density-estimation_files_in_output_20_0">Figure 49-6</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KernelDensity</code>

         <code class="c1"># instantiate and fit the KDE model</code>
         <code class="n">kde</code> <code class="o">=</code> <code class="n">KernelDensity</code><code class="p">(</code><code class="n">bandwidth</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="s1">'gaussian'</code><code class="p">)</code>
         <code class="n">kde</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="kc">None</code><code class="p">])</code>

         <code class="c1"># score_samples returns the log of the probability density</code>
         <code class="n">logprob</code> <code class="o">=</code> <code class="n">kde</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">x_d</code><code class="p">[:,</code> <code class="kc">None</code><code class="p">])</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">fill_between</code><code class="p">(</code><code class="n">x_d</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">logprob</code><code class="p">),</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">full_like</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.01</code><code class="p">),</code> <code class="s1">'|k'</code><code class="p">,</code> <code class="n">markeredgewidth</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mf">0.02</code><code class="p">,</code> <code class="mf">0.22</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0513-kernel-density-estimation_files_in_output_20_0">
<img alt="output 20 0" height="383" src="assets/output_20_0.png" width="600"/>
<h6><span class="label">Figure 49-6. </span>A kernel density estimate computed with Scikit-Learn</h6>
</div></figure>
<p>The result here is normalized such that the area under the curve is
equal to 1.<a data-startref="ix_ch49-asciidoc4" data-type="indexterm" id="idm45858718796464"/></p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Selecting the Bandwidth via Cross-Validation" data-type="sect1"><div class="sect1" id="ch_0513-kernel-density-estimation_selecting-the-bandwidth-via-cross-validation">
<h1>Selecting the Bandwidth via Cross-Validation</h1>
<p><a data-primary="bias–variance trade-off" data-secondary="kernel bandwidth and" data-type="indexterm" id="idm45858718793776"/><a data-primary="kernel bandwidth" data-secondary="selection via cross-validation" data-type="indexterm" id="idm45858718792832"/><a data-primary="kernel density estimation (KDE)" data-secondary="bandwidth selection via cross-validation" data-type="indexterm" id="idm45858718791920"/>The final estimate produced by a KDE procedure can be quite sensitive to
the choice of bandwidth, which is the knob that controls the
bias–variance trade-off in the estimate of density. Too narrow a
bandwidth leads to a high-variance estimate (i.e., overfitting), where
the presence or absence of a single point makes a large difference. Too
wide a bandwidth leads to a high-bias estimate (i.e., underfitting),
where the structure in the data is washed out by the wide kernel.</p>
<p>There is a long history in statistics of methods to quickly estimate the
best bandwidth based on rather stringent assumptions about the data: if
you look up the KDE implementations in the SciPy and <code>statsmodels</code>
packages, for example, you will see implementations based on some of
these rules.</p>
<p>In machine learning contexts, we’ve seen that such
hyperparameter tuning often is done empirically via a cross-validation
approach. With this in mind, Scikit-Learn’s <code>KernelDensity</code>
estimator is designed such that it can be used directly within the
package’s standard grid search tools. Here we will use
<code>GridSearchCV</code> to optimize the bandwidth for the preceding dataset.
Because we are looking at such a small dataset, we will use
leave-one-out cross-validation, which minimizes the reduction in
training set size for each cross-validation trial:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>
         <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">LeaveOneOut</code>

         <code class="n">bandwidths</code> <code class="o">=</code> <code class="mi">10</code> <code class="o">**</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code>
         <code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">KernelDensity</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s1">'gaussian'</code><code class="p">),</code>
                             <code class="p">{</code><code class="s1">'bandwidth'</code><code class="p">:</code> <code class="n">bandwidths</code><code class="p">},</code>
                             <code class="n">cv</code><code class="o">=</code><code class="n">LeaveOneOut</code><code class="p">())</code>
         <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="kc">None</code><code class="p">]);</code></pre>
<p>Now we can find the choice of bandwidth that maximizes the score (which
in this case defaults to the log-likelihood):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">grid</code><code class="o">.</code><code class="n">best_params_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="p">{</code><code class="s1">'bandwidth'</code><code class="p">:</code> <code class="mf">1.1233240329780276</code><code class="p">}</code></pre>
<p>The optimal bandwidth happens to be very close to what we used in the
example plot earlier, where the bandwidth was 1.0 (i.e., the default
width of <code>scipy.stats.norm</code>).</p>
</div></section>
<section data-pdf-bookmark="Example: Not-so-Naive Bayes" data-type="sect1"><div class="sect1" id="ch_0513-kernel-density-estimation_example-not-so-naive-bayes">
<h1>Example: Not-so-Naive Bayes</h1>
<p><a data-primary="Bayesian classification" data-type="indexterm" id="ix_ch49-asciidoc5"/><a data-primary="kernel density estimation (KDE)" data-secondary="Bayesian generative classification with" data-type="indexterm" id="ix_ch49-asciidoc6"/>This example looks at Bayesian generative classification with KDE, and
demonstrates how to use the Scikit-Learn architecture to create a custom
estimator.</p>
<p>In <a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a> we
explored naive Bayesian classification, in which we create a simple
generative model for each class, and use these models to build a fast
classifier. For Gaussian naive Bayes, the generative model is a simple
axis-aligned Gaussian. With a density estimation algorithm like KDE, we
can remove the “naive” element and perform the same classification
with a more sophisticated generative model for each class.
It’s still Bayesian classification, but it’s no
longer naive.</p>
<p>The general approach for generative classification is this:</p>
<ol>
<li>
<p>Split the training data by label.</p>
</li>
<li>
<p>For each set, fit a KDE to obtain a generative model of the
data.
This allows you, for any observation <math alttext="x">
<mi>x</mi>
</math> and
label <math alttext="y">
<mi>y</mi>
</math>, to compute a likelihood <math alttext="upper P left-parenthesis x vertical-bar y right-parenthesis">
<mrow>
<mi>P</mi>
<mo>(</mo>
<mi>x</mi>
<mspace width="3.33333pt"/>
<mo>|</mo>
<mspace width="3.33333pt"/>
<mi>y</mi>
<mo>)</mo>
</mrow>
</math>.</p>
</li>
<li>
<p>From the number of examples of each class in the training set, compute
the <em>class prior</em>, <math alttext="upper P left-parenthesis y right-parenthesis">
<mrow>
<mi>P</mi>
<mo>(</mo>
<mi>y</mi>
<mo>)</mo>
</mrow>
</math>.</p>
</li>
<li>
<p>For an unknown point <math alttext="x">
<mi>x</mi>
</math>, the posterior probability for
each class is <math alttext="upper P left-parenthesis y vertical-bar x right-parenthesis proportional-to upper P left-parenthesis x vertical-bar y right-parenthesis upper P left-parenthesis y right-parenthesis">
<mrow>
<mi>P</mi>
<mo>(</mo>
<mi>y</mi>
<mspace width="3.33333pt"/>
<mo>|</mo>
<mspace width="3.33333pt"/>
<mi>x</mi>
<mo>)</mo>
<mo>∝</mo>
<mi>P</mi>
<mo>(</mo>
<mi>x</mi>
<mspace width="3.33333pt"/>
<mo>|</mo>
<mspace width="3.33333pt"/>
<mi>y</mi>
<mo>)</mo>
<mi>P</mi>
<mo>(</mo>
<mi>y</mi>
<mo>)</mo>
</mrow>
</math>.
The
class that maximizes this posterior is the label assigned to the point.</p>
</li>
</ol>
<p>The algorithm is straightforward and intuitive to understand; the more
difficult piece is couching it within the Scikit-Learn framework in
order to make use of the grid search and cross-validation architecture.</p>
<p>This is the code that implements the algorithm within the Scikit-Learn
framework; we will step through it following the code block:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.base</code> <code class="kn">import</code> <code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">ClassifierMixin</code>


         <code class="k">class</code> <code class="nc">KDEClassifier</code><code class="p">(</code><code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">ClassifierMixin</code><code class="p">):</code>
             <code class="sd">"""Bayesian generative classification based on KDE</code>

<code class="sd">             Parameters</code>
<code class="sd">             ----------</code>
<code class="sd">             bandwidth : float</code>
<code class="sd">                 the kernel bandwidth within each class</code>
<code class="sd">             kernel : str</code>
<code class="sd">                 the kernel name, passed to KernelDensity</code>
<code class="sd">             """</code>
             <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">bandwidth</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="s1">'gaussian'</code><code class="p">):</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">bandwidth</code> <code class="o">=</code> <code class="n">bandwidth</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">kernel</code> <code class="o">=</code> <code class="n">kernel</code>

             <code class="k">def</code> <code class="nf">fit</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">classes_</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">y</code><code class="p">))</code>
                 <code class="n">training_sets</code> <code class="o">=</code> <code class="p">[</code><code class="n">X</code><code class="p">[</code><code class="n">y</code> <code class="o">==</code> <code class="n">yi</code><code class="p">]</code> <code class="k">for</code> <code class="n">yi</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">classes_</code><code class="p">]</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">models_</code> <code class="o">=</code> <code class="p">[</code><code class="n">KernelDensity</code><code class="p">(</code><code class="n">bandwidth</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">bandwidth</code><code class="p">,</code>
                                               <code class="n">kernel</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">Xi</code><code class="p">)</code>
                                 <code class="k">for</code> <code class="n">Xi</code> <code class="ow">in</code> <code class="n">training_sets</code><code class="p">]</code>
                 <code class="bp">self</code><code class="o">.</code><code class="n">logpriors_</code> <code class="o">=</code> <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">Xi</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
                                    <code class="k">for</code> <code class="n">Xi</code> <code class="ow">in</code> <code class="n">training_sets</code><code class="p">]</code>
                 <code class="k">return</code> <code class="bp">self</code>

             <code class="k">def</code> <code class="nf">predict_proba</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
                 <code class="n">logprobs</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">model</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
                                      <code class="k">for</code> <code class="n">model</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">models_</code><code class="p">])</code><code class="o">.</code><code class="n">T</code>
                 <code class="n">result</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">logprobs</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">logpriors_</code><code class="p">)</code>
                 <code class="k">return</code> <code class="n">result</code> <code class="o">/</code> <code class="n">result</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

             <code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
                 <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">classes_</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="mi">1</code><code class="p">)]</code></pre>
<section data-pdf-bookmark="Anatomy of a Custom Estimator" data-type="sect2"><div class="sect2" id="ch_0513-kernel-density-estimation_anatomy-of-a-custom-estimator">
<h2>Anatomy of a Custom Estimator</h2>
<p><a data-primary="kernel density estimation (KDE)" data-secondary="custom estimator" data-type="indexterm" id="ix_ch49-asciidoc7"/>Let’s step through this code and discuss the essential
features:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">sklearn.base</code> <code class="kn">import</code> <code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">ClassifierMixin</code>

<code class="k">class</code> <code class="nc">KDEClassifier</code><code class="p">(</code><code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">ClassifierMixin</code><code class="p">):</code>
    <code class="sd">"""Bayesian generative classification based on KDE</code>

<code class="sd">    Parameters</code>
<code class="sd">    ----------</code>
<code class="sd">    bandwidth : float</code>
<code class="sd">        the kernel bandwidth within each class</code>
<code class="sd">    kernel : str</code>
<code class="sd">        the kernel name, passed to KernelDensity</code>
<code class="sd">    """</code></pre>
<p>Each estimator in Scikit-Learn is a class, and it is most convenient for
this class to inherit from the <code>BaseEstimator</code> class as well as the
appropriate mixin, which provides standard functionality. For example,
here the <code>BaseEstimator</code> contains (among other things) the logic
necessary to clone/copy an estimator for use in a cross-validation
procedure, and <code>ClassifierMixin</code> defines a default <code>score</code> method used
by such routines. We also provide a docstring, which will be captured by
IPython’s help functionality (see
<a data-type="xref" href="ch01.xhtml#section-0101-help-and-documentation">Chapter 1</a>).</p>
<p>Next comes the class initialization method:</p>
<pre data-code-language="python" data-type="programlisting">    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">bandwidth</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="s1">'gaussian'</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">bandwidth</code> <code class="o">=</code> <code class="n">bandwidth</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">kernel</code> <code class="o">=</code> <code class="n">kernel</code></pre>
<p>This is the actual code that is executed when the object is instantiated
with 
<span class="keep-together"><code>KDEClassifier</code></span>. In Scikit-Learn, it is important that
<em>initialization contains no operations</em> other than assigning the passed
values by name to <code>self</code>. This is due to the logic contained in
<code>BaseEstimator</code> required for cloning and modifying estimators for
cross-validation, grid search, and other functions. Similarly, all
arguments to <code>__init__</code> should be explicit: i.e., <code>*args</code> or <code>**kwargs</code>
should be avoided, as they will not be correctly handled within
cross-validation routines.</p>
<p class="pagebreak-before less_space">Next comes the <code>fit</code> method, where we handle training data:</p>
<pre data-code-language="python" data-type="programlisting">    <code class="k">def</code> <code class="nf">fit</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">classes_</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sort</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">y</code><code class="p">))</code>
        <code class="n">training_sets</code> <code class="o">=</code> <code class="p">[</code><code class="n">X</code><code class="p">[</code><code class="n">y</code> <code class="o">==</code> <code class="n">yi</code><code class="p">]</code> <code class="k">for</code> <code class="n">yi</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">classes_</code><code class="p">]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">models_</code> <code class="o">=</code> <code class="p">[</code><code class="n">KernelDensity</code><code class="p">(</code><code class="n">bandwidth</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">bandwidth</code><code class="p">,</code>
                                      <code class="n">kernel</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">kernel</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">Xi</code><code class="p">)</code>
                        <code class="k">for</code> <code class="n">Xi</code> <code class="ow">in</code> <code class="n">training_sets</code><code class="p">]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">logpriors_</code> <code class="o">=</code> <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">Xi</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
                           <code class="k">for</code> <code class="n">Xi</code> <code class="ow">in</code> <code class="n">training_sets</code><code class="p">]</code>
        <code class="k">return</code> <code class="bp">self</code></pre>
<p>Here we find the unique classes in the training data, train a
<code>KernelDensity</code> model for each class, and compute the class priors based
on the number of input samples. Finally, <code>fit</code> should always return
<code>self</code> so that we can chain commands. For example:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">label</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>
<p>Notice that each persistent result of the fit is stored with a trailing
underscore (e.g., <code>self.logpriors_</code>). This is a convention used in
Scikit-Learn so that you can quickly scan the members of an estimator
(using IPython’s tab completion) and see exactly which
members are fit to training data.</p>
<p>Finally, we have the logic for predicting labels on new data:</p>
<pre data-code-language="python" data-type="programlisting">    <code class="k">def</code> <code class="nf">predict_proba</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="n">logprobs</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">([</code><code class="n">model</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
                              <code class="k">for</code> <code class="n">model</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">models_</code><code class="p">])</code><code class="o">.</code><code class="n">T</code>
        <code class="n">result</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">logprobs</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">logpriors_</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">result</code> <code class="o">/</code> <code class="n">result</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">keepdims</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

    <code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">classes_</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">),</code> <code class="mi">1</code><code class="p">)]</code></pre>
<p>Because this is a probabilistic classifier, we first implement
<code>predict_proba</code>, which returns an array of class probabilities of shape
<code>[n_samples, n_classes]</code>. Entry <code>[i, j]</code> of this array is the posterior
probability that sample <code>i</code> is a member of class <code>j</code>, computed by
multiplying the likelihood by the class prior and normalizing.</p>
<p>The <code>predict</code> method uses these probabilities and simply returns the
class with the largest probability.</p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Using Our Custom Estimator" data-type="sect2"><div class="sect2" id="ch_0513-kernel-density-estimation_using-our-custom-estimator">
<h2>Using Our Custom Estimator</h2>
<p>Let’s try this custom estimator on a problem we have seen
before: the classification of handwritten digits. Here we will load the
digits and compute the cross-validation score for a range of candidate
bandwidths using the <code>GridSearchCV</code> meta-estimator (refer back to
<a data-type="xref" href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">Chapter 39</a>):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
         <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

         <code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>

         <code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">KDEClassifier</code><code class="p">(),</code>
                             <code class="p">{</code><code class="s1">'bandwidth'</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">logspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">100</code><code class="p">)})</code>
         <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">);</code></pre>
<p>Next we can plot the cross-validation score as a function of bandwidth
(see <a data-type="xref" href="#fig_0513-kernel-density-estimation_files_in_output_37_1">Figure 49-7</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">()</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">semilogx</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">grid</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s1">'param_bandwidth'</code><code class="p">]),</code>
                     <code class="n">grid</code><code class="o">.</code><code class="n">cv_results_</code><code class="p">[</code><code class="s1">'mean_test_score'</code><code class="p">])</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">title</code><code class="o">=</code><code class="s1">'KDE Model Performance'</code><code class="p">,</code> <code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                <code class="n">xlabel</code><code class="o">=</code><code class="s1">'bandwidth'</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s1">'accuracy'</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'best param: </code><code class="si">{</code><code class="n">grid</code><code class="o">.</code><code class="n">best_params_</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s1">'accuracy = </code><code class="si">{</code><code class="n">grid</code><code class="o">.</code><code class="n">best_score_</code><code class="si">}</code><code class="s1">'</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">best</code> <code class="n">param</code><code class="p">:</code> <code class="p">{</code><code class="s1">'bandwidth'</code><code class="p">:</code> <code class="mf">6.135907273413174</code><code class="p">}</code>
         <code class="n">accuracy</code> <code class="o">=</code> <code class="mf">0.9677298050139276</code></pre>
<figure><div class="figure" id="fig_0513-kernel-density-estimation_files_in_output_37_1">
<img alt="output 37 1" height="427" src="assets/output_37_1.png" width="600"/>
<h6><span class="label">Figure 49-7. </span>Validation curve for the KDE-based Bayesian classifier</h6>
</div></figure>
<p>This indicates that our KDE classifier reaches a cross-validation
accuracy of over 96%, compared to around 80% for the naive Bayes
classifier:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.naive_bayes</code> <code class="kn">import</code> <code class="n">GaussianNB</code>
         <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code>
         <code class="n">cross_val_score</code><code class="p">(</code><code class="n">GaussianNB</code><code class="p">(),</code> <code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="mf">0.8069281956050759</code></pre>
<p>One benefit of such a generative classifier is interpretability of
results: for each unknown sample, we not only get a probabilistic
classification, but a <em>full model</em> of the distribution of points we are
comparing it to! If desired, this offers an intuitive window into the
reasons for a particular classification that algorithms like SVMs and
random forests tend to obscure.</p>
<p>If you would like to take this further, here are some ideas for
improvements that could be made to our KDE classifier model:</p>
<ul>
<li>
<p>You could allow the bandwidth in each class to vary independently.</p>
</li>
<li>
<p>You could optimize these bandwidths not based on their prediction
score, but on the likelihood of the training data under the generative
model within each class (i.e. use the scores from <code>KernelDensity</code> itself
rather than the global prediction accuracy).</p>
</li>
</ul>
<p>Finally, if you want some practice building your own estimator, you
might tackle building a similar Bayesian classifier using Gaussian
mixture models<a data-startref="ix_ch49-asciidoc7" data-type="indexterm" id="idm45858717603312"/> instead of KDE<a data-startref="ix_ch49-asciidoc6" data-type="indexterm" id="idm45858717602480"/><a data-startref="ix_ch49-asciidoc5" data-type="indexterm" id="idm45858717601776"/>.<a data-startref="ix_ch49-asciidoc1" data-type="indexterm" id="idm45858717600976"/><a data-startref="ix_ch49-asciidoc0" data-type="indexterm" id="idm45858717571936"/></p>
</div></section>
</div></section>
</div></section></div></body></html>